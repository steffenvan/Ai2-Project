<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<note confidence="0.835974333333333">
Proceedings of the Workshop on Speech-to-Speech Translation:
Algorithms and Systems, Philadelphia, July 2002, pp. 23-30.
Association for Computational Linguistics.
</note>
<bodyText confidence="0.9975990625">
in the English sentences.
Since the algorithm does not rely on a spe-
cific feature of Japanese&apos; or trained tokeniz-
ers, it is applicable to parallel corpora consist-
ing of English and another language without
trivial word boundaries. We expect that this
kind of corpora will be available for many lan-
guages in the near future because texts in a
local language will be translated into English
and vice versa, considering that English is the
&amp;quot;common&amp;quot; language of the world.
The remaining part of the paper is orga-
nized as follows. Section 2 introduces the
problem. Section 3 presents our algorithm.
Section 4 reports the results of the experi-
ments and the discussion.
</bodyText>
<sectionHeader confidence="0.9581" genericHeader="abstract">
2 Problem Setting
</sectionHeader>
<subsectionHeader confidence="0.727711">
2.1 Aligned Corpus
</subsectionHeader>
<bodyText confidence="0.999989777777778">
Input to our algorithm is an English-Japanese
parallel aligned corpus, where every sentence
in English is paired with its translation in
Japanese. An example of pairs is shown
in Figure 1. Although this paper uses an
English-Japanese corpus, the proposed algo-
rithm is applicable to any language pair, one
from a language with trivial word boundaries
and the other from a language without them.
</bodyText>
<subsectionHeader confidence="0.9958855">
2.2 Finding Translation Pairs from
Untokenized Corpus
</subsectionHeader>
<bodyText confidence="0.998875">
Our goal is to find translation pairs from the
parallel corpus described above. This goal is
achieved by performing the following two:
</bodyText>
<listItem confidence="0.9701158">
1. Identifying words of aligned sentences,
what we call segmentation or tokeniza-
tion and
2. Finding translation pairs from the seg-
mented corpora, what we call extraction.
</listItem>
<bodyText confidence="0.883372058823529">
Existing approaches, most of which are for
pairs of Western-European languages, con-
centrate on the second task, since words in
these languages are already separated by ex-
plicit markers, such as spaces. For East-Asian
&apos;e.g., character classes
languages, (e.g., Chinese, Japanese, and Ko-
rean), however, the first task is equally impor-
tant, since these languages have no explicit
word boundaries in their written form.
Let us consider a hypothetical language,
Lx, instead of Japanese. Lx has 10 words,
as shown in Figure 2, and its written form
does not have explicit word separators.
Alphabet A, B, C, D, E, F
Lexicon A, B, BC, CDE, CEF, DE,
DF, EF, F, FB
</bodyText>
<figureCaption confidence="0.97571">
Figure 2: Hypothetical language L.
</figureCaption>
<bodyText confidence="0.999114666666667">
Then, suppose our goal is to find English-
L2 translation pairs, e.g., &amp;quot;paper-BC&amp;quot;, from a
parallel corpus shown in Figure 3.
</bodyText>
<figure confidence="0.4596675">
English
E1: Here is a paper.
E2: He wrote a paper.
E3: He read the paper.
</figure>
<figureCaption confidence="0.997871">
Figure 3: Hypothetical aligned corpus.
</figureCaption>
<bodyText confidence="0.999256">
Since we presuppose that no external infor-
mation is available besides the lexicon, a sen-
tence in Lx can be segmented in many ways,
as shown in Figure 4.
</bodyText>
<equation confidence="0.8833015">
possible segmentations
LO1 A/l3/CDE, A/BC/DE
L02 FB/CEF, F/BC/EF, F/B/CEF
L03 FB/C/DF, F/BC/DF
</equation>
<figureCaption confidence="0.724151">
Figure 4: Possible Segmentations for LO1 -
</figureCaption>
<bodyText confidence="0.917071">
L03. (&amp;quot;/&amp;quot; shows a word boundary.)
The problem tackled in this paper is how to
find a translation of each English noun from
possible words in the Lx, or Japanese, side of
the parallel corpus.
</bodyText>
<sectionHeader confidence="0.986774" genericHeader="keywords">
3 Algorithm
</sectionHeader>
<bodyText confidence="0.9955656">
Our algorithm is an extension of unsupervised
segmentation using Hidden-Markov-Models
(HMMs) (Takeuchi and Matsumoto, 1995).
The original segmentation algorithm tries to
estimate HAIM parameters in such a way that
</bodyText>
<figure confidence="0.922528571428571">
Lx
LOLABCDE
L02:FBCEF
L03:FBCDF
English Japanese
Please show me your passport. iN &lt;
Tea or coffee? e t thN
</figure>
<figureCaption confidence="0.999856">
Figure 1: Sample Aligned Corpus
</figureCaption>
<bodyText confidence="0.999047916666666">
the total entropy of the (unsegmented) train-
ing corpus is maximized.
This approach has a great advantage since
it is free from language dependent heuristics
and since in such applications as speech recog-
nition, the estimated parameters sometimes
achieve even better performance than those
estimated by supervised training. The prob-
lem, however, is that it cannot definitely guar-
antee that the resulting word boundaries are
semantically correct.
In order to solve this problem, our segmen-
tation algorithm tries to include semantic in-
formation by using the aligned English sen-
tences. More specifically, we extended the
original unsupervised segmentation algorithm
based on the following assumption:
If a Japanese word in a segmentation
candidate has its translated word
in the aligned English sentence, the
Japanese word gets higher probabil-
ity according to the strength of their
translation relation.
We have no bilingual dictionary, which is
the output of our algorithm, so the transla-
tion relation between two words should be
calculated somehow from the parallel corpus,
e.g., by using mutual information. This cal-
culation, however, requires solving the initial
problem of segmenting Japanese sentences.
To get around this chicken-and-egg prob-
lem, our algorithm iteratively estimates trans-
lation relations, beginning with tentatively
segmenting Japanese sentences without us-
ing bilingual information, then estimating
translation relations using the tentative seg-
mentation. Next, the algorithm re-segments
Japanese sentences considering the transla-
tion relations, resulting in updated transla-
tion relations.
In order to cope with ambiguous segmenta-
tion, our algorithm estimates translation rela-
tions using every word in possible segmenta-
tions by assuming that each word&apos;s occurrence
count is not a natural number but the prob-
ability of the word calculated by the HMM
segmentation model.
This process is equivalent to relating an En-
glish word and Japanese word if they co-occur
in many translation pairs.
For example, the sample Lx corpus shown
in Figure 3 includes three English sentences
that share the word &amp;quot;paper&amp;quot;. The word &amp;quot;BC&amp;quot;
is a candidate for the translation of &amp;quot;paper&amp;quot;
since it appears in the set of candidates for
every sentence in Lx (as shown in 4).
The remaining part of this section first ex-
plains unsupervised learning of an n-gram
model, then extends it for handling a parallel
corpus.
</bodyText>
<subsectionHeader confidence="0.812109333333333">
3.1 Unsupervised Sentence
Tokenization using Markov
Models
</subsectionHeader>
<bodyText confidence="0.9985245">
Let us concentrate on tokenizing a Japanese
monolingual sentence.
In the statistical framework, the most likely
word-sequence TV for a sentence S is for-
malized as follows (Takeuchi and Matsumoto,
1995), (Nagata, 1994):
= arg max PO V ), (1)
where P(TV) is the probability of a word se-
quence TV = wi, , tun whose surface string
is equal to that of S. This formula is trans-
formed to the following form, known as the
n-gram model, by approximating the proba-
bility of each word depending on N preceding
words.
</bodyText>
<equation confidence="0.8388798">
1W I
arg max 11 P(wiltm , . , wi_1)
1
= arg max 11 p(wilwi_N+i, • • • , wi_1P)
1
</equation>
<bodyText confidence="0.999029125">
A larger N will make the model stronger,
but a larger amount of data is required for
estimating probabilities. Thus, a value of 2
or 3 is usually used. Hereafter, we fix N to 2
for simplicity.
In order to implement a tokenizer using the
n-gram model, we must solve the following
problems.
</bodyText>
<listItem confidence="0.999261333333333">
1. How to efficiently find IF for given n-
gram probabilities P(wi wi_i).
2. How to estimate n-gram probabilities.
</listItem>
<bodyText confidence="0.999822">
Since the first problem can be solved by us-
ing the dynamic programming technique, we
concentrate on the second problem.
The standard way to answer the second
question is to find the probability distribu-
tions that can generate the given corpus with
highest probability. Formally, this is achieved
by finding the probability distributions that
maximize the log-likelihood log(L) of the
training corpus. If the training corpus is al-
ready segmented, log(L) is easily calculated
as shown:
</bodyText>
<equation confidence="0.997682">
log(L) = log P(w,lh,), (3)
w,EC orpws
</equation>
<bodyText confidence="0.999936285714286">
where h, is the history of the word, w,. In the
bigram case, h, is simply the preceding word,
w,.
In our problem, however, the corpus has
no explicit segmentation. Thus, we consider
every possible segmentation for each string as
follows:
</bodyText>
<equation confidence="0.9351255">
log(L) = E higp(wp,s1+), (4)
wp,, EC orpws
</equation>
<bodyText confidence="0.999322615384615">
where wp,s corresponds to a word whose sur-
face form is s, starting from the position p,
and + represents the context of ws in this
case, the previous word.
The probability distributions P that max-
imize the above formula cannot be deter-
mined analytically but by an iterative algo-
rithm called the Forward-Backward Algorithm
or the Baum-Welch Algorithm as roughly de-
scribed below 2.
Note that the algorithm presupposes pos-
sible segmentations and P(wp,s1h) are repre-
sented by a lattice.
</bodyText>
<listItem confidence="0.997172285714286">
• Step 0 (Initialization)
Assign P(wp,s I +) to every word in the
lattice.
Since P(wp,s I +) is unknown, we use the
0-gram (uniform) probability as the ini-
tial value.
• Step 1
</listItem>
<bodyText confidence="0.93088825">
For each sentence, estimate posterior
(bigram) probability for each word in
each (Japanese) sentence Jk, written as
P(wp,s, 4).
Posterior probabilities are calculated by
using forward and backward probabilities
(Jurafsky and Martin, 2000), (Manning
and Schuetze, 1999).
</bodyText>
<listItem confidence="0.987841">
• Step 2
</listItem>
<bodyText confidence="0.9789272">
Re-estimate the bigram (prior) probabili-
ties based on the maximum likelihood es-
timation, regarding the posterior proba-
bility of each word as the real-valued oc-
currence frequency. Formally,
</bodyText>
<equation confidence="0.9701315">
P(wp,s, +IA)
P(wp,s p(+14) (5)
</equation>
<listItem confidence="0.962881">
• Step 3
</listItem>
<bodyText confidence="0.947071142857143">
If some condition is satisfied, then stop,
otherwise go to step 2. The condition
could be the iteration count or total en-
tropy.
2 Detailed explanations are found in textbooks (Ra-
biner and Juang, 1993), (Jurafsky and Martin, 2000),
(Manning and Schuetze, 1999)
</bodyText>
<subsectionHeader confidence="0.997329">
3.2 Including Aligned Sentences
</subsectionHeader>
<bodyText confidence="0.9998747">
Since the algorithm in the previous section
tries to segment each sentence to maximize
the log-likelihood, it is not necessarily optimal
for finding translation pairs.
In this section, we modify the algorithm un-
der the assumption presented in Section 2.1.
We begin by considering that the probability
of a sentence in Japanese, Jk, is conditioned
by the aligned sentence, Ek. Thus, the log-
likelihood of this model is
</bodyText>
<equation confidence="0.9597454">
E(nlEk)=
EE(w.d ( ))E(Aln,Ek)
A
P(W.d MP(a(wj)1 j). (10)
A
</equation>
<bodyText confidence="0.99898725">
Instead of summing up every A, which
means to consider all the words in Ek, we ap-
proximate the formula by using the maximum
value 3
</bodyText>
<equation confidence="0.9972455">
log(L) = E logP(JklEk). (6)
(Jk,Ek)EC orpws
</equation>
<bodyText confidence="0.986295333333333">
We can rewrite this formula in such a way
that each Japanese word depends on both the
preceding word(s) and the aligned sentence.
</bodyText>
<equation confidence="0.919879">
P(Jk Ek) = H E(wp,s +,Ek) (7)
Wp, EJk
</equation>
<bodyText confidence="0.999848666666667">
It is, however, hard to estimate a reliable
value for P(wp,s +, E) since it includes the
entire sentence, Ek. One possibility is to use
Maximum Entropy Markov Models (MEMM)
(McCallum et al., 2000). In this paper, we
take a simpler approach.
First, we decompose the above conditional
probability into a monolingual part and a
bilingual part.
</bodyText>
<subsectionHeader confidence="0.408785">
E(wp,s I +,Ek)= E(wp,s I +)E(wp,s I Ek). (8)
</subsectionHeader>
<bodyText confidence="0.9992444">
The monolingual part is exactly same as
the previous n-gram probability. The bilin-
gual part, P(wp,s I Ek), is expressed as the
sum of all possible alignment A (Brown et al.,
1990):
</bodyText>
<equation confidence="0.968596">
E(nlEk)= ( , (9)
</equation>
<bodyText confidence="0.993232333333333">
Since A just specifies a(wj), i.e., the trans-
lation of wj, the above formula can be approx-
imated as follows:
</bodyText>
<equation confidence="0.979749666666667">
P(WjlEk) 1)(W:7 lEk)
We EEk
max P(tvj we)P(we I wj) (11)
</equation>
<bodyText confidence="0.98381">
We estimate P(tvj we)P(we wj) by
counting numbers of co-occurring words:
</bodyText>
<equation confidence="0.988223666666667">
C2(w3, we)
P(w3 we)P(we w3) = (12)
C3(2113)Ce(We)&apos;
</equation>
<bodyText confidence="0.93537125">
where C(w3, we) is the number of times tv3
and we co-occur in one aligned pair, C3(2113) is
the frequency of tvj in the Japanese part of the
aligned corpus and Ce(we) is the frequency of
we in the English part.
To estimate P(w, I +, E), we use the algo-
rithm shown in Section 3.1 replacing P(w,
+) with P(71) +,Ek).
</bodyText>
<listItem confidence="0.974041">
1. Step 0 (same as in Section 3.1)
2. Step 1 (same as in Section 3.1)
3. Step 2
Re-estimate P(wp,s I +„5,) in the follow-
ing form:
</listItem>
<equation confidence="0.42028">
E(wp,s I +, se) = E(wp,s I +)E(wp,slEk),(13)
</equation>
<bodyText confidence="0.9342872">
where P(wp,s I +) is given by (5) and
P(wp,s1Ek) is given by (11). Note that
C3(2V3) is calculated by summing the pos-
terior probabilities of tv3 in the entire
Japanese segmentation candidates.
</bodyText>
<listItem confidence="0.811595">
4. Step 3 (same as in Section 3.1)
</listItem>
<footnote confidence="0.931798">
3This roughly corresponds to using the Viterbi
Alignment instead of all the possible alignments.
</footnote>
<subsectionHeader confidence="0.892627">
3.3 Bilingual Dictionary Construction
</subsectionHeader>
<bodyText confidence="0.9915936">
After segmentation is completed, translation
pairs can be found by simply retrieving such
combinations of w and e where P(w3, we)
and/or C(tv3, we) are over a certain thresh-
old.
The noun recall is how many nouns in the
correctly segmented sentences are identified
as words (i.e., nouns) by the tokenizer.
# of nouns correctly identified by the system
total # of nouns
</bodyText>
<sectionHeader confidence="0.990631" genericHeader="introduction">
4 Evaluation Experiments
</sectionHeader>
<bodyText confidence="0.999827571428572">
We applied the proposed algorithm to a
Japanese-English corpus of travel expressions.
The corpus includes 200 k sentence pairs that
are considered to be frequently used in trav-
eling abroad. This corpus is divided into two
non-overlapping sets: a training set with 190
k sentences and a test set with 0.5 k sentences
</bodyText>
<equation confidence="0.416374">
4 .
</equation>
<bodyText confidence="0.9999586">
The lexicon of Japanese is compiled by ac-
cumulating all the entries of the dictionary of
ChaSen (Matsumoto et al., 1997), a Japanese
morphological analyzer. Every word in the
English part of the corpus was assigned a
part-of-speech. Using the part-of-speech, we
restricted ourselves to finding translations of
1101111s.
In the following experiments, we modified
the formula (8) as:
</bodyText>
<equation confidence="0.997189">
P(wp,s Se) =
(1 — A)P(wp,s +) AP(wp,slEk)(14)
</equation>
<bodyText confidence="0.9999495">
where A determines the weight of the transla-
tion probability.
</bodyText>
<subsectionHeader confidence="0.999419">
4.1 Segmentation Accuracy
</subsectionHeader>
<bodyText confidence="0.999319833333333">
First, we evaluated the segmentation perfor-
mance in terms of the entropy, word accuracy
and noun recall for the test corpus. The rela-
tion between total entropy and the iteration
count is shown in Figure 5.
The word accuracy is defined as follows:
</bodyText>
<equation confidence="0.993066">
Ins + Del + Sub
ace =
T otalW ordC ount
</equation>
<bodyText confidence="0.997885">
where Ins, Del, and Sub are, respectively,
counts of insertion, deletion, and substitution
words as compared with the reference data.
</bodyText>
<footnote confidence="0.721363">
4The remaining sentences are reserved.
</footnote>
<bodyText confidence="0.999697911764706">
The values for different A are shown in Ta-
ble 1. For comparison, results of supervised
tokenization (Nagata, 1994)5 were included in
the table.
Since the unsupervised algorithm tries to
find segmentation such that the total entropy
is minimized6, the total entropy for each re-
sult is smaller than that of supervised learn-
ing. This shows that our method can pro-
duce segmentations useful for building lan-
guage models for speech recognition etc.
On the other hand, word accuracies, which
are roughly linear to segmentation accuracies,
are worse than the results of supervised to-
kenization, as expected. Many errors were
found in functional words and suffixes. Func-
tional words are often wrongly concatenated
to adjacent functional words when they occur
frequently. The unsupervised tokenizer often
separates an inflexive suffix of a verb from the
main part, although the reference segmenta-
tion treats them as one unit.
The noun recall for each run is significantly
greater than the word accuracy. This shows
that translation relations are useful for ob-
taining correct word boundaries and identify-
ing nouns.
Considering the fact that the number of
functional words is limited and a small
training corpus improves segmentation accu-
racy(Takenclti and Matsumoto, 1995), the
best method would be to combine the super-
vised method with our algorithm which is ef-
fective for content words.
</bodyText>
<subsectionHeader confidence="0.999528">
4.2 Correctness of Translation Pairs
</subsectionHeader>
<bodyText confidence="0.9886922">
Next, we investigated how correctly our algo-
rithm could compile a bilingual dictionary. In
this evaluation, we collected every bilingual
pair, j,e, that satisfied the following condi-
tion:
</bodyText>
<footnote confidence="0.9990745">
5We ignored part-of-speech .
6More accurately, &amp;quot;locally minimized&amp;quot;.
</footnote>
<figure confidence="0.9360485">
—Unsupervised
- - Supervised
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 20 29 30
Iteration
</figure>
<figureCaption confidence="0.988407">
Figure 5: Entropy vs. Iteration Count
</figureCaption>
<tableCaption confidence="0.684591">
Table 1: Word Accuracy
</tableCaption>
<figure confidence="0.97837247826087">
450000
440000
430000
„
2=2
-c&amp;quot;73 410000
400000
390000
300000
A total entropy word accuracy noun recall
0.0 385801.85 68.0
0.25 386569.83 70.2
0.5 386983.16 70.5
0.75 388118.42 71.9
0.90 388751.05 72.2
supervised 397785.54 98.7
86.3
87.8
89.2
89.7
90.1
98.1
C(w , we) &gt; 111(1 = 1, , 5).
</figure>
<bodyText confidence="0.998122666666667">
This condition states that the co-
occurrence frequency of the translation
pair should be greater than or equal to a
threshold, 11I.
We compared our algorithm with the fol-
lowing two settings.
</bodyText>
<listItem confidence="0.524988">
1. Dictionary Only (DIC)
</listItem>
<bodyText confidence="0.999531">
As a baseline, we stopped our procedure
after the first iteration. This means that
every possible segmentation is equally
likely.
</bodyText>
<sectionHeader confidence="0.579798" genericHeader="method">
2. Supervised Segmentation (SUP)
</sectionHeader>
<bodyText confidence="0.969053588235294">
This case also stops the procedure before
re-estimation, but instead of uniform dis-
tribution for initialization, we used prob-
abilities estimated from a manually seg-
mented training corpus.
The correctness of every translation pair
produced by the above methods was evalu-
ated by professional translators. The results
are shown in Table 2, where accuracy is de-
fined as:
# of correctpairs
total # of outputs
This table shows that our algorithm is more
accurate and found more bilingual pairs than
the method using every possible word and is
slightly less accurate than the method using
supervised segmentation.
</bodyText>
<sectionHeader confidence="0.992093" genericHeader="method">
5 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.99989125">
In this paper, we proposed an algorithm that
extracts translation pairs from an English-
Japanese parallel raw corpus. The unique
feature of our algorithm is that it does not
</bodyText>
<tableCaption confidence="0.991524">
Table 2: Accuracy of Translation Pairs
</tableCaption>
<table confidence="0.94539875">
DICT = dictionary only, UNS = proposed, SUP = supervised
(min. occ.) correct partially correct wrong # found
DIC/UNS/SUP DIC/UNS/SUP DIC/UNS/SUP DIC/UNS/SUP
5 74.5/76.3/76.7 14.5/15.5/16.4 11.1/ 8.2/ 6.9 1545/1788/1837
4 73.8/75.4/76.2 15.0/15.9/16.8 11.2/ 8.7/ 7.0 1769/2012/2068
3 73.1/73.4/75.0 15.4/17.3/17.7 12.0/ 9.3/ 7.3 2072/2398/2423
2 66.9/69.0/70.9 17.2/19.0/18.8 15.9/12.0/10.3 2709/3174/3240
1 52.0/57.0/57.5 17.9/20.0/20.3 30.1/23.0/22.2 5431/5860/5884
</table>
<bodyText confidence="0.999852777777778">
require any separate tokenizer for Japanese.
This means that the algorithm can find trans-
lation pairs from any parallel corpus consist-
ing of a language with trivial word boundaries
and a language without them. Experimen-
tal results showed that the algorithm achieved
only a 0.4 points lower accuracy than super-
vised segmentation.
The proposed algorithm can be elaborated
in many ways. One direction would be to
use a better formula for the degree of trans-
lation relation. Another direction would be
to improve the method of combining an n-
gram probability and a translation proba-
bility. The maximum entropy approach as
stated before (McCallum et al., 2000) for cou-
pling two probabilities in a principled way is
promising.
</bodyText>
<sectionHeader confidence="0.998789" genericHeader="conclusions">
6 Acknowledgment
</sectionHeader>
<bodyText confidence="0.999857">
The research reported here was supported in
part by a contract with the Telecommunica-
tions Advancement Organization of Japan en-
titled, &amp;quot;A study of speech dialogue translation
technology based on a large corpus&amp;quot;.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9987195">
P. Brown, J. Cocke, V. Della Pietra, F. Jelinek,
R.L. Mercer, and P. C. Roosin. 1990. A statis-
tical approach to language translation. Compu-
tational Linguistics, 16(2):79-85.
Ido Dagan, Kenneth Church, and William Gale.
1993. Robust word alignment for machine aided
translation. In Proceedings of the Workshop on
Very Large Corpora, pages 1-8.
Pascale Fung. 1995. Compiling bilingual lexicon
entries from a non-parallel English-Chinese cor-
pus. In David Yarovsky and Kenneth Church,
editors, Proceedings of the Third Workshop on
Very Large Corpora, pages 173-183, Somerset,
New Jersey. Association for Computational Lin-
guistics.
Daniel S. Jurafsky and James H. Martin. 2000.
SPEECH and LANGUAGE PROCESSING.
Prentice Hall.
Christoper D. Manning and Hinrich Schuetze.
1999. Foundations of Statistical Natural Lan-
guage Processing. The MIT Press.
Y. Matsumoto, A. K itauchi, T. Yamashita,
Y. Hirano, 0. Imaichi, and T. Imamura.
1997. Japanese morphological analysis system
ChaSen manual. Technical Report NAIST-IS-
TR97007, Nara Institute of Technology.
Andrew AlcCallum, Dayne Freitag, and Fernando
Pereira. 2000. Alaximum entropy Alarkov mod-
els for information extraction and segmenta-
tion. In Proceedings of 17th International Conf.
on Machine Learning, pages 591-598. Morgan
Kaufmann, San Francisco, CA.
Alasaaki Nagata. 1994. A stochastic Japanese
morphological analyzer using a forward-dp
backward-a* n-best search algorithm. In Pro-
ceedings of COLING-94, pages 201-207.
Lawrence Rabiner and Biing-Hwang Juang. 1993.
Foundations of Speech Recognition. Prentice
Hall.
Kouichi Takeuchi and Yuji Matsumoto. 1995.
HMM parameter learning for Japanese mor-
phological analyzer. In Proceedings of PA-
CLING95, pages 163-172.
Dekai Wu and Xuanyin Xia. 1994. Learning
an English-Chinese lexicon from a parallel cor-
pus. In Proceedings of the First Conference of
the Association for Machine Translation in the
Americas (AMTA-9-4).
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000001">
<note confidence="0.876061">Proceedings of the Workshop on Speech-to-Speech Translation: Algorithms and Systems, Philadelphia, July 2002, pp. 23-30. Association for Computational Linguistics.</note>
<abstract confidence="0.988737533057851">in the English sentences. Since the algorithm does not rely on a specific feature of Japanese&apos; or trained tokenizers, it is applicable to parallel corpora consisting of English and another language without trivial word boundaries. We expect that this kind of corpora will be available for many languages in the near future because texts in a local language will be translated into English and vice versa, considering that English is the &amp;quot;common&amp;quot; language of the world. The remaining part of the paper is organized as follows. Section 2 introduces the problem. Section 3 presents our algorithm. Section 4 reports the results of the experiments and the discussion. Problem 2.1 Aligned Corpus Input to our algorithm is an English-Japanese parallel aligned corpus, where every sentence in English is paired with its translation in Japanese. An example of pairs is shown in Figure 1. Although this paper uses an English-Japanese corpus, the proposed algorithm is applicable to any language pair, one from a language with trivial word boundaries and the other from a language without them. 2.2 Finding Translation Pairs from Untokenized Corpus Our goal is to find translation pairs from the parallel corpus described above. This goal is achieved by performing the following two: 1. Identifying words of aligned sentences, we call tokeniza- 2. Finding translation pairs from the segcorpora, what we call Existing approaches, most of which are for pairs of Western-European languages, concentrate on the second task, since words in these languages are already separated by explicit markers, such as spaces. For East-Asian &apos;e.g., character classes languages, (e.g., Chinese, Japanese, and Korean), however, the first task is equally important, since these languages have no explicit word boundaries in their written form. Let us consider a hypothetical language, of Japanese. 10 words, as shown in Figure 2, and its written form does not have explicit word separators. Alphabet A, B, C, D, E, F Lexicon A, B, BC, CDE, CEF, DE, DF, EF, F, FB 2: Hypothetical language Then, suppose our goal is to find English- L2 translation pairs, e.g., &amp;quot;paper-BC&amp;quot;, from a parallel corpus shown in Figure 3. English E1: Here is a paper. E2: He wrote a paper. E3: He read the paper. Figure 3: Hypothetical aligned corpus. Since we presuppose that no external information is available besides the lexicon, a senin be segmented in many ways, as shown in Figure 4. possible segmentations LO1 A/l3/CDE, A/BC/DE L02 FB/CEF, F/BC/EF, F/B/CEF L03 FB/C/DF, F/BC/DF 4: Possible Segmentations for - (&amp;quot;/&amp;quot; a word boundary.) The problem tackled in this paper is how to find a translation of each English noun from words in the Japanese, side of the parallel corpus. 3 Algorithm Our algorithm is an extension of unsupervised segmentation using Hidden-Markov-Models (HMMs) (Takeuchi and Matsumoto, 1995). The original segmentation algorithm tries to estimate HAIM parameters in such a way that LOLABCDE L02:FBCEF L03:FBCDF English Japanese show me your passport. or coffee? t Figure 1: Sample Aligned Corpus the total entropy of the (unsegmented) training corpus is maximized. This approach has a great advantage since it is free from language dependent heuristics and since in such applications as speech recognition, the estimated parameters sometimes achieve even better performance than those estimated by supervised training. The problem, however, is that it cannot definitely guarantee that the resulting word boundaries are semantically correct. In order to solve this problem, our segmentation algorithm tries to include semantic information by using the aligned English sentences. More specifically, we extended the original unsupervised segmentation algorithm based on the following assumption: If a Japanese word in a segmentation candidate has its translated word in the aligned English sentence, the Japanese word gets higher probability according to the strength of their translation relation. We have no bilingual dictionary, which is the output of our algorithm, so the translation relation between two words should be calculated somehow from the parallel corpus, e.g., by using mutual information. This calculation, however, requires solving the initial problem of segmenting Japanese sentences. To get around this chicken-and-egg problem, our algorithm iteratively estimates translation relations, beginning with tentatively segmenting Japanese sentences without using bilingual information, then estimating translation relations using the tentative segmentation. Next, the algorithm re-segments Japanese sentences considering the translation relations, resulting in updated translation relations. In order to cope with ambiguous segmentation, our algorithm estimates translation relations using every word in possible segmentations by assuming that each word&apos;s occurrence count is not a natural number but the probability of the word calculated by the HMM segmentation model. This process is equivalent to relating an English word and Japanese word if they co-occur in many translation pairs. example, the sample shown in Figure 3 includes three English sentences that share the word &amp;quot;paper&amp;quot;. The word &amp;quot;BC&amp;quot; is a candidate for the translation of &amp;quot;paper&amp;quot; since it appears in the set of candidates for sentence in shown in 4). The remaining part of this section first explains unsupervised learning of an n-gram model, then extends it for handling a parallel corpus. 3.1 Unsupervised Sentence Tokenization using Markov Models Let us concentrate on tokenizing a Japanese monolingual sentence. In the statistical framework, the most likely TV for a sentence formalized as follows (Takeuchi and Matsumoto, 1995), (Nagata, 1994): = arg max PO V ), (1) P(TV) is the probability of a word se- TV = wi, , whose surface equal to that of formula is transformed to the following form, known as the model, approximating the probability of each word depending on N preceding words. max , . , 1 arg max • • • , wi_1P) 1 A larger N will make the model stronger, but a larger amount of data is required for estimating probabilities. Thus, a value of 2 or 3 is usually used. Hereafter, we fix N to 2 for simplicity. In order to implement a tokenizer using the n-gram model, we must solve the following problems. 1. How to efficiently find IF for given nprobabilities P(wi 2. How to estimate n-gram probabilities. Since the first problem can be solved by using the dynamic programming technique, we concentrate on the second problem. The standard way to answer the second question is to find the probability distributions that can generate the given corpus with highest probability. Formally, this is achieved by finding the probability distributions that the log-likelihood the training corpus. If the training corpus is alsegmented, easily calculated as shown: = log P(w,lh,), w,EC orpws the history of the word, w,. In the case, simply the preceding word, w,. In our problem, however, the corpus has no explicit segmentation. Thus, we consider every possible segmentation for each string as follows: E wp,, EC orpws corresponds to a word whose surform is from the position + represents the context of in this case, the previous word. probability distributions maximize the above formula cannot be determined analytically but by an iterative algocalled the Algorithm the Algorithm roughly debelow Note that the algorithm presupposes possegmentations and P(wp,s1h)are represented by a lattice. • Step 0 (Initialization) I +) to every word in the lattice. I +) unknown, we use the 0-gram (uniform) probability as the initial value. • Step 1 For each sentence, estimate posterior (bigram) probability for each word in (Japanese) sentence as Posterior probabilities are calculated by and backward probabilities (Jurafsky and Martin, 2000), (Manning and Schuetze, 1999). • Step 2 Re-estimate the bigram (prior) probabilities based on the maximum likelihood estimation, regarding the posterior probability of each word as the real-valued occurrence frequency. Formally, • Step 3 If some condition is satisfied, then stop, otherwise go to step 2. The condition could be the iteration count or total entropy.</abstract>
<note confidence="0.729411">2Detailed explanations are found in textbooks (Rabiner and Juang, 1993), (Jurafsky and Martin, 2000), (Manning and Schuetze, 1999)</note>
<abstract confidence="0.993325930817611">3.2 Including Aligned Sentences Since the algorithm in the previous section tries to segment each sentence to maximize the log-likelihood, it is not necessarily optimal for finding translation pairs. In this section, we modify the algorithm under the assumption presented in Section 2.1. We begin by considering that the probability a sentence in Japanese, conditioned the aligned sentence, the loglikelihood of this model is E(nlEk)= ( A j). (10) A Instead of summing up every A, which to consider all the words in approximate the formula by using the maximum 3 (6) We can rewrite this formula in such a way that each Japanese word depends on both the preceding word(s) and the aligned sentence. Ek) H +,Ek) Wp, EJk It is, however, hard to estimate a reliable for +, E) it includes the sentence, possibility is to use Maximum Entropy Markov Models (MEMM) (McCallum et al., 2000). In this paper, we take a simpler approach. First, we decompose the above conditional probability into a monolingual part and a bilingual part. The monolingual part is exactly same as the previous n-gram probability. The bilinpart, I expressed as the sum of all possible alignment A (Brown et al., 1990): A just specifies the transof above formula can be approximated as follows: We EEk I wj) estimate we)P(we wj) counting numbers of co-occurring words: w3) = (12) is the number of times co-occur in one aligned pair, frequency of in the Japanese part of the corpus and is the frequency of in the English part. estimate E), use the algoshown in Section 3.1 replacing with +,Ek). 1. Step 0 (same as in Section 3.1) 2. Step 1 (same as in Section 3.1) 3. Step 2 I +„5,) in the following form: +, I +) given by (5) and given by (11). Note that calculated by summing the posprobabilities of in the entire Japanese segmentation candidates. 4. Step 3 (same as in Section 3.1) roughly corresponds to using the Viterbi Alignment instead of all the possible alignments. 3.3 Bilingual Dictionary Construction After segmentation is completed, translation pairs can be found by simply retrieving such of w and are over a certain threshold. recall is many nouns in the correctly segmented sentences are identified as words (i.e., nouns) by the tokenizer. total # of nouns 4 Evaluation Experiments We applied the proposed algorithm to a Japanese-English corpus of travel expressions. corpus includes sentence pairs that are considered to be frequently used in traveling abroad. This corpus is divided into two sets: a set with sentences and a set 0.5 k sentences 4 . The lexicon of Japanese is compiled by accumulating all the entries of the dictionary of ChaSen (Matsumoto et al., 1997), a Japanese morphological analyzer. Every word in the English part of the corpus was assigned a part-of-speech. Using the part-of-speech, we restricted ourselves to finding translations of 1101111s. In the following experiments, we modified the formula (8) as: P(wp,s Se) = A)P(wp,s +) AP(wp,slEk)(14) where A determines the weight of the translation probability. 4.1 Segmentation Accuracy First, we evaluated the segmentation performance in terms of the entropy, word accuracy and noun recall for the test corpus. The relation between total entropy and the iteration count is shown in Figure 5. accuracy defined as follows: Ins + Del + Sub ace = T otalW ordC ount Del, respectively, counts of insertion, deletion, and substitution words as compared with the reference data. remaining sentences are reserved. The values for different A are shown in Table 1. For comparison, results of supervised (Nagata, were included in the table. Since the unsupervised algorithm tries to find segmentation such that the total entropy the total entropy for each result is smaller than that of supervised learning. This shows that our method can produce segmentations useful for building language models for speech recognition etc. On the other hand, word accuracies, which are roughly linear to segmentation accuracies, are worse than the results of supervised tokenization, as expected. Many errors were found in functional words and suffixes. Functional words are often wrongly concatenated to adjacent functional words when they occur frequently. The unsupervised tokenizer often separates an inflexive suffix of a verb from the main part, although the reference segmentation treats them as one unit. The noun recall for each run is significantly greater than the word accuracy. This shows that translation relations are useful for obtaining correct word boundaries and identifying nouns. Considering the fact that the number of functional words is limited and a small training corpus improves segmentation accuracy(Takenclti and Matsumoto, 1995), the best method would be to combine the supervised method with our algorithm which is effective for content words. 4.2 Correctness of Translation Pairs Next, we investigated how correctly our algorithm could compile a bilingual dictionary. In this evaluation, we collected every bilingual satisfied the following condition: ignored part-of-speech . accurately, &amp;quot;locally minimized&amp;quot;.</abstract>
<note confidence="0.9006575">Unsupervised -Supervised 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 20 29 30 Iteration Figure 5: Entropy vs. Iteration Count Table 1: Word Accuracy</note>
<address confidence="0.957147666666667">450000 440000 430000</address>
<affiliation confidence="0.378137"></affiliation>
<address confidence="0.8225318">2=2 -c&amp;quot;73 410000 400000 390000 300000</address>
<abstract confidence="0.939790470588235">A total entropy word accuracy noun recall 0.0 385801.85 68.0 0.25 386569.83 70.2 0.5 386983.16 70.5 0.75 388118.42 71.9 0.90 388751.05 72.2 supervised 397785.54 98.7 86.3 87.8 89.2 89.7 90.1 98.1 &gt; 111(1 = 1, , 5). This condition states that the cooccurrence frequency of the translation pair should be greater than or equal to a threshold, 11I. We compared our algorithm with the following two settings. 1. Dictionary Only (DIC) As a baseline, we stopped our procedure after the first iteration. This means that every possible segmentation is equally likely. 2. Supervised Segmentation (SUP) This case also stops the procedure before re-estimation, but instead of uniform distribution for initialization, we used probabilities estimated from a manually segmented training corpus. The correctness of every translation pair produced by the above methods was evaluated by professional translators. The results are shown in Table 2, where accuracy is defined as: total # of outputs This table shows that our algorithm is more accurate and found more bilingual pairs than the method using every possible word and is slightly less accurate than the method using supervised segmentation. 5 Concluding Remarks In this paper, we proposed an algorithm that extracts translation pairs from an English- Japanese parallel raw corpus. The unique feature of our algorithm is that it does not Table 2: Accuracy of Translation Pairs DICT = dictionary only, UNS = proposed, SUP = supervised (min. occ.) correct partially wrong # found DIC/UNS/SUP DIC/UNS/SUP DIC/UNS/SUP DIC/UNS/SUP</abstract>
<phone confidence="0.697737">5 74.5/76.3/76.7 14.5/15.5/16.4 11.1/ 8.2/ 6.9 1545/1788/1837 4 73.8/75.4/76.2 15.0/15.9/16.8 11.2/ 8.7/ 7.0 1769/2012/2068 3 73.1/73.4/75.0 15.4/17.3/17.7 12.0/ 9.3/ 7.3 2072/2398/2423 2 66.9/69.0/70.9 17.2/19.0/18.8 15.9/12.0/10.3 2709/3174/3240 1 52.0/57.0/57.5 17.9/20.0/20.3 30.1/23.0/22.2 5431/5860/5884</phone>
<abstract confidence="0.998253041666667">require any separate tokenizer for Japanese. This means that the algorithm can find translation pairs from any parallel corpus consisting of a language with trivial word boundaries and a language without them. Experimental results showed that the algorithm achieved only a 0.4 points lower accuracy than supervised segmentation. The proposed algorithm can be elaborated in many ways. One direction would be to use a better formula for the degree of translation relation. Another direction would be to improve the method of combining an ngram probability and a translation probability. The maximum entropy approach as stated before (McCallum et al., 2000) for coupling two probabilities in a principled way is promising. 6 Acknowledgment The research reported here was supported in part by a contract with the Telecommunications Advancement Organization of Japan entitled, &amp;quot;A study of speech dialogue translation technology based on a large corpus&amp;quot;.</abstract>
<title confidence="0.818116">References</title>
<author confidence="0.838098">P Brown</author>
<author confidence="0.838098">J Cocke</author>
<author confidence="0.838098">V Della Pietra</author>
<author confidence="0.838098">F Jelinek</author>
<note confidence="0.701916510638298">R.L. Mercer, and P. C. Roosin. 1990. A statisapproach to language translation. Compu- Linguistics, Ido Dagan, Kenneth Church, and William Gale. 1993. Robust word alignment for machine aided In of the Workshop on Corpora, 1-8. Pascale Fung. 1995. Compiling bilingual lexicon from a non-parallel English-Chinese corpus. In David Yarovsky and Kenneth Church, of the Third Workshop on Large Corpora, 173-183, Somerset, New Jersey. Association for Computational Linguistics. Daniel S. Jurafsky and James H. Martin. 2000. SPEECH and LANGUAGE PROCESSING. Prentice Hall. Christoper D. Manning and Hinrich Schuetze. of Statistical Natural Lan- Processing. MIT Press. Y. Matsumoto, A. K itauchi, T. Yamashita, Y. Hirano, 0. Imaichi, and T. Imamura. morphological analysis system manual. Report NAIST-IS- TR97007, Nara Institute of Technology. Andrew AlcCallum, Dayne Freitag, and Fernando Pereira. 2000. Alaximum entropy Alarkov models for information extraction and segmenta- In of 17th International Conf. Machine Learning, 591-598. Morgan Kaufmann, San Francisco, CA. Alasaaki Nagata. 1994. A stochastic Japanese morphological analyzer using a forward-dp n-best search algorithm. In Proof COLING-94, 201-207. Lawrence Rabiner and Biing-Hwang Juang. 1993. of Speech Recognition. Hall. Kouichi Takeuchi and Yuji Matsumoto. 1995. HMM parameter learning for Japanese moranalyzer. In of PA- 163-172. Dekai Wu and Xuanyin Xia. 1994. Learning an English-Chinese lexicon from a parallel cor- In of the First Conference of the Association for Machine Translation in the Americas (AMTA-9-4).</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>J Cocke</author>
<author>V Della Pietra</author>
<author>F Jelinek</author>
<author>R L Mercer</author>
<author>P C Roosin</author>
</authors>
<title>A statistical approach to language translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<pages>16--2</pages>
<contexts>
<context position="10487" citStr="Brown et al., 1990" startWordPosition="1722" endWordPosition="1725">igned sentence. P(Jk Ek) = H E(wp,s +,Ek) (7) Wp, EJk It is, however, hard to estimate a reliable value for P(wp,s +, E) since it includes the entire sentence, Ek. One possibility is to use Maximum Entropy Markov Models (MEMM) (McCallum et al., 2000). In this paper, we take a simpler approach. First, we decompose the above conditional probability into a monolingual part and a bilingual part. E(wp,s I +,Ek)= E(wp,s I +)E(wp,s I Ek). (8) The monolingual part is exactly same as the previous n-gram probability. The bilingual part, P(wp,s I Ek), is expressed as the sum of all possible alignment A (Brown et al., 1990): E(nlEk)= ( , (9) Since A just specifies a(wj), i.e., the translation of wj, the above formula can be approximated as follows: P(WjlEk) 1)(W:7 lEk) We EEk max P(tvj we)P(we I wj) (11) We estimate P(tvj we)P(we wj) by counting numbers of co-occurring words: C2(w3, we) P(w3 we)P(we w3) = (12) C3(2113)Ce(We)&apos; where C(w3, we) is the number of times tv3 and we co-occur in one aligned pair, C3(2113) is the frequency of tvj in the Japanese part of the aligned corpus and Ce(we) is the frequency of we in the English part. To estimate P(w, I +, E), we use the algorithm shown in Section 3.1 replacing P(</context>
</contexts>
<marker>Brown, Cocke, Pietra, Jelinek, Mercer, Roosin, 1990</marker>
<rawString>P. Brown, J. Cocke, V. Della Pietra, F. Jelinek, R.L. Mercer, and P. C. Roosin. 1990. A statistical approach to language translation. Computational Linguistics, 16(2):79-85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Kenneth Church</author>
<author>William Gale</author>
</authors>
<title>Robust word alignment for machine aided translation.</title>
<date>1993</date>
<booktitle>In Proceedings of the Workshop on Very Large Corpora,</booktitle>
<pages>1--8</pages>
<marker>Dagan, Church, Gale, 1993</marker>
<rawString>Ido Dagan, Kenneth Church, and William Gale. 1993. Robust word alignment for machine aided translation. In Proceedings of the Workshop on Very Large Corpora, pages 1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
</authors>
<title>Compiling bilingual lexicon entries from a non-parallel English-Chinese corpus.</title>
<date>1995</date>
<booktitle>Proceedings of the Third Workshop on Very Large Corpora,</booktitle>
<pages>173--183</pages>
<editor>In David Yarovsky and Kenneth Church, editors,</editor>
<publisher>Association for Computational Linguistics.</publisher>
<location>Somerset, New Jersey.</location>
<marker>Fung, 1995</marker>
<rawString>Pascale Fung. 1995. Compiling bilingual lexicon entries from a non-parallel English-Chinese corpus. In David Yarovsky and Kenneth Church, editors, Proceedings of the Third Workshop on Very Large Corpora, pages 173-183, Somerset, New Jersey. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel S Jurafsky</author>
<author>James H Martin</author>
</authors>
<date>2000</date>
<booktitle>SPEECH and LANGUAGE PROCESSING.</booktitle>
<publisher>Prentice Hall.</publisher>
<contexts>
<context position="8540" citStr="Jurafsky and Martin, 2000" startWordPosition="1391" endWordPosition="1394">ve algorithm called the Forward-Backward Algorithm or the Baum-Welch Algorithm as roughly described below 2. Note that the algorithm presupposes possible segmentations and P(wp,s1h) are represented by a lattice. • Step 0 (Initialization) Assign P(wp,s I +) to every word in the lattice. Since P(wp,s I +) is unknown, we use the 0-gram (uniform) probability as the initial value. • Step 1 For each sentence, estimate posterior (bigram) probability for each word in each (Japanese) sentence Jk, written as P(wp,s, 4). Posterior probabilities are calculated by using forward and backward probabilities (Jurafsky and Martin, 2000), (Manning and Schuetze, 1999). • Step 2 Re-estimate the bigram (prior) probabilities based on the maximum likelihood estimation, regarding the posterior probability of each word as the real-valued occurrence frequency. Formally, P(wp,s, +IA) P(wp,s p(+14) (5) • Step 3 If some condition is satisfied, then stop, otherwise go to step 2. The condition could be the iteration count or total entropy. 2 Detailed explanations are found in textbooks (Rabiner and Juang, 1993), (Jurafsky and Martin, 2000), (Manning and Schuetze, 1999) 3.2 Including Aligned Sentences Since the algorithm in the previous se</context>
</contexts>
<marker>Jurafsky, Martin, 2000</marker>
<rawString>Daniel S. Jurafsky and James H. Martin. 2000. SPEECH and LANGUAGE PROCESSING. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoper D Manning</author>
<author>Hinrich Schuetze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>1999</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="8570" citStr="Manning and Schuetze, 1999" startWordPosition="1395" endWordPosition="1398">rd-Backward Algorithm or the Baum-Welch Algorithm as roughly described below 2. Note that the algorithm presupposes possible segmentations and P(wp,s1h) are represented by a lattice. • Step 0 (Initialization) Assign P(wp,s I +) to every word in the lattice. Since P(wp,s I +) is unknown, we use the 0-gram (uniform) probability as the initial value. • Step 1 For each sentence, estimate posterior (bigram) probability for each word in each (Japanese) sentence Jk, written as P(wp,s, 4). Posterior probabilities are calculated by using forward and backward probabilities (Jurafsky and Martin, 2000), (Manning and Schuetze, 1999). • Step 2 Re-estimate the bigram (prior) probabilities based on the maximum likelihood estimation, regarding the posterior probability of each word as the real-valued occurrence frequency. Formally, P(wp,s, +IA) P(wp,s p(+14) (5) • Step 3 If some condition is satisfied, then stop, otherwise go to step 2. The condition could be the iteration count or total entropy. 2 Detailed explanations are found in textbooks (Rabiner and Juang, 1993), (Jurafsky and Martin, 2000), (Manning and Schuetze, 1999) 3.2 Including Aligned Sentences Since the algorithm in the previous section tries to segment each se</context>
</contexts>
<marker>Manning, Schuetze, 1999</marker>
<rawString>Christoper D. Manning and Hinrich Schuetze. 1999. Foundations of Statistical Natural Language Processing. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Matsumoto</author>
<author>A K itauchi</author>
<author>T Yamashita</author>
<author>Y Hirano</author>
</authors>
<title>Japanese morphological analysis system ChaSen manual.</title>
<date>1997</date>
<tech>Technical Report NAIST-ISTR97007,</tech>
<institution>Nara Institute of Technology.</institution>
<contexts>
<context position="12482" citStr="Matsumoto et al., 1997" startWordPosition="2075" endWordPosition="2078">s in the correctly segmented sentences are identified as words (i.e., nouns) by the tokenizer. # of nouns correctly identified by the system total # of nouns 4 Evaluation Experiments We applied the proposed algorithm to a Japanese-English corpus of travel expressions. The corpus includes 200 k sentence pairs that are considered to be frequently used in traveling abroad. This corpus is divided into two non-overlapping sets: a training set with 190 k sentences and a test set with 0.5 k sentences 4 . The lexicon of Japanese is compiled by accumulating all the entries of the dictionary of ChaSen (Matsumoto et al., 1997), a Japanese morphological analyzer. Every word in the English part of the corpus was assigned a part-of-speech. Using the part-of-speech, we restricted ourselves to finding translations of 1101111s. In the following experiments, we modified the formula (8) as: P(wp,s Se) = (1 — A)P(wp,s +) AP(wp,slEk)(14) where A determines the weight of the translation probability. 4.1 Segmentation Accuracy First, we evaluated the segmentation performance in terms of the entropy, word accuracy and noun recall for the test corpus. The relation between total entropy and the iteration count is shown in Figure 5</context>
</contexts>
<marker>Matsumoto, itauchi, Yamashita, Hirano, 1997</marker>
<rawString>Y. Matsumoto, A. K itauchi, T. Yamashita, Y. Hirano, 0. Imaichi, and T. Imamura. 1997. Japanese morphological analysis system ChaSen manual. Technical Report NAIST-ISTR97007, Nara Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew AlcCallum</author>
<author>Dayne Freitag</author>
<author>Fernando Pereira</author>
</authors>
<title>Alaximum entropy Alarkov models for information extraction and segmentation.</title>
<date>2000</date>
<booktitle>In Proceedings of 17th International Conf. on Machine Learning,</booktitle>
<pages>591--598</pages>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, CA.</location>
<marker>AlcCallum, Freitag, Pereira, 2000</marker>
<rawString>Andrew AlcCallum, Dayne Freitag, and Fernando Pereira. 2000. Alaximum entropy Alarkov models for information extraction and segmentation. In Proceedings of 17th International Conf. on Machine Learning, pages 591-598. Morgan Kaufmann, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alasaaki Nagata</author>
</authors>
<title>A stochastic Japanese morphological analyzer using a forward-dp backward-a* n-best search algorithm.</title>
<date>1994</date>
<booktitle>In Proceedings of COLING-94,</booktitle>
<pages>201--207</pages>
<contexts>
<context position="6068" citStr="Nagata, 1994" startWordPosition="961" endWordPosition="962">three English sentences that share the word &amp;quot;paper&amp;quot;. The word &amp;quot;BC&amp;quot; is a candidate for the translation of &amp;quot;paper&amp;quot; since it appears in the set of candidates for every sentence in Lx (as shown in 4). The remaining part of this section first explains unsupervised learning of an n-gram model, then extends it for handling a parallel corpus. 3.1 Unsupervised Sentence Tokenization using Markov Models Let us concentrate on tokenizing a Japanese monolingual sentence. In the statistical framework, the most likely word-sequence TV for a sentence S is formalized as follows (Takeuchi and Matsumoto, 1995), (Nagata, 1994): = arg max PO V ), (1) where P(TV) is the probability of a word sequence TV = wi, , tun whose surface string is equal to that of S. This formula is transformed to the following form, known as the n-gram model, by approximating the probability of each word depending on N preceding words. 1W I arg max 11 P(wiltm , . , wi_1) 1 = arg max 11 p(wilwi_N+i, • • • , wi_1P) 1 A larger N will make the model stronger, but a larger amount of data is required for estimating probabilities. Thus, a value of 2 or 3 is usually used. Hereafter, we fix N to 2 for simplicity. In order to implement a tokenizer usi</context>
<context position="13452" citStr="Nagata, 1994" startWordPosition="2234" endWordPosition="2235">probability. 4.1 Segmentation Accuracy First, we evaluated the segmentation performance in terms of the entropy, word accuracy and noun recall for the test corpus. The relation between total entropy and the iteration count is shown in Figure 5. The word accuracy is defined as follows: Ins + Del + Sub ace = T otalW ordC ount where Ins, Del, and Sub are, respectively, counts of insertion, deletion, and substitution words as compared with the reference data. 4The remaining sentences are reserved. The values for different A are shown in Table 1. For comparison, results of supervised tokenization (Nagata, 1994)5 were included in the table. Since the unsupervised algorithm tries to find segmentation such that the total entropy is minimized6, the total entropy for each result is smaller than that of supervised learning. This shows that our method can produce segmentations useful for building language models for speech recognition etc. On the other hand, word accuracies, which are roughly linear to segmentation accuracies, are worse than the results of supervised tokenization, as expected. Many errors were found in functional words and suffixes. Functional words are often wrongly concatenated to adjace</context>
</contexts>
<marker>Nagata, 1994</marker>
<rawString>Alasaaki Nagata. 1994. A stochastic Japanese morphological analyzer using a forward-dp backward-a* n-best search algorithm. In Proceedings of COLING-94, pages 201-207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Rabiner</author>
<author>Biing-Hwang Juang</author>
</authors>
<title>Foundations of Speech Recognition.</title>
<date>1993</date>
<publisher>Prentice Hall.</publisher>
<contexts>
<context position="9010" citStr="Rabiner and Juang, 1993" startWordPosition="1468" endWordPosition="1472">panese) sentence Jk, written as P(wp,s, 4). Posterior probabilities are calculated by using forward and backward probabilities (Jurafsky and Martin, 2000), (Manning and Schuetze, 1999). • Step 2 Re-estimate the bigram (prior) probabilities based on the maximum likelihood estimation, regarding the posterior probability of each word as the real-valued occurrence frequency. Formally, P(wp,s, +IA) P(wp,s p(+14) (5) • Step 3 If some condition is satisfied, then stop, otherwise go to step 2. The condition could be the iteration count or total entropy. 2 Detailed explanations are found in textbooks (Rabiner and Juang, 1993), (Jurafsky and Martin, 2000), (Manning and Schuetze, 1999) 3.2 Including Aligned Sentences Since the algorithm in the previous section tries to segment each sentence to maximize the log-likelihood, it is not necessarily optimal for finding translation pairs. In this section, we modify the algorithm under the assumption presented in Section 2.1. We begin by considering that the probability of a sentence in Japanese, Jk, is conditioned by the aligned sentence, Ek. Thus, the loglikelihood of this model is E(nlEk)= EE(w.d ( ))E(Aln,Ek) A P(W.d MP(a(wj)1 j). (10) A Instead of summing up every A, w</context>
</contexts>
<marker>Rabiner, Juang, 1993</marker>
<rawString>Lawrence Rabiner and Biing-Hwang Juang. 1993. Foundations of Speech Recognition. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kouichi Takeuchi</author>
<author>Yuji Matsumoto</author>
</authors>
<title>HMM parameter learning for Japanese morphological analyzer.</title>
<date>1995</date>
<booktitle>In Proceedings of PACLING95,</booktitle>
<pages>163--172</pages>
<contexts>
<context position="3160" citStr="Takeuchi and Matsumoto, 1995" startWordPosition="512" endWordPosition="515">ned corpus. Since we presuppose that no external information is available besides the lexicon, a sentence in Lx can be segmented in many ways, as shown in Figure 4. possible segmentations LO1 A/l3/CDE, A/BC/DE L02 FB/CEF, F/BC/EF, F/B/CEF L03 FB/C/DF, F/BC/DF Figure 4: Possible Segmentations for LO1 - L03. (&amp;quot;/&amp;quot; shows a word boundary.) The problem tackled in this paper is how to find a translation of each English noun from possible words in the Lx, or Japanese, side of the parallel corpus. 3 Algorithm Our algorithm is an extension of unsupervised segmentation using Hidden-Markov-Models (HMMs) (Takeuchi and Matsumoto, 1995). The original segmentation algorithm tries to estimate HAIM parameters in such a way that Lx LOLABCDE L02:FBCEF L03:FBCDF English Japanese Please show me your passport. iN &lt; Tea or coffee? e t thN Figure 1: Sample Aligned Corpus the total entropy of the (unsegmented) training corpus is maximized. This approach has a great advantage since it is free from language dependent heuristics and since in such applications as speech recognition, the estimated parameters sometimes achieve even better performance than those estimated by supervised training. The problem, however, is that it cannot definit</context>
<context position="6052" citStr="Takeuchi and Matsumoto, 1995" startWordPosition="957" endWordPosition="960">rpus shown in Figure 3 includes three English sentences that share the word &amp;quot;paper&amp;quot;. The word &amp;quot;BC&amp;quot; is a candidate for the translation of &amp;quot;paper&amp;quot; since it appears in the set of candidates for every sentence in Lx (as shown in 4). The remaining part of this section first explains unsupervised learning of an n-gram model, then extends it for handling a parallel corpus. 3.1 Unsupervised Sentence Tokenization using Markov Models Let us concentrate on tokenizing a Japanese monolingual sentence. In the statistical framework, the most likely word-sequence TV for a sentence S is formalized as follows (Takeuchi and Matsumoto, 1995), (Nagata, 1994): = arg max PO V ), (1) where P(TV) is the probability of a word sequence TV = wi, , tun whose surface string is equal to that of S. This formula is transformed to the following form, known as the n-gram model, by approximating the probability of each word depending on N preceding words. 1W I arg max 11 P(wiltm , . , wi_1) 1 = arg max 11 p(wilwi_N+i, • • • , wi_1P) 1 A larger N will make the model stronger, but a larger amount of data is required for estimating probabilities. Thus, a value of 2 or 3 is usually used. Hereafter, we fix N to 2 for simplicity. In order to implement</context>
</contexts>
<marker>Takeuchi, Matsumoto, 1995</marker>
<rawString>Kouichi Takeuchi and Yuji Matsumoto. 1995. HMM parameter learning for Japanese morphological analyzer. In Proceedings of PACLING95, pages 163-172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
<author>Xuanyin Xia</author>
</authors>
<title>Learning an English-Chinese lexicon from a parallel corpus.</title>
<date>1994</date>
<booktitle>In Proceedings of the First Conference of the Association for Machine Translation in the Americas</booktitle>
<pages>9--4</pages>
<marker>Wu, Xia, 1994</marker>
<rawString>Dekai Wu and Xuanyin Xia. 1994. Learning an English-Chinese lexicon from a parallel corpus. In Proceedings of the First Conference of the Association for Machine Translation in the Americas (AMTA-9-4).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>