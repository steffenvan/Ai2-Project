<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.022925">
<title confidence="0.998959">
Combination of Recurrent Neural Networks and Factored Language
Models for Code-Switching Language Modeling
</title>
<author confidence="0.981071">
Heike Adel Ngoc Thang Vu Tanja Schultz
</author>
<affiliation confidence="0.989075">
Institute for Anthropomatics, Karlsruhe Institute of Technology (KIT)
</affiliation>
<email confidence="0.993672">
heike.adel@student.kit.edu thang.vu@kit.edu tanja.schultz@kit.edu
</email>
<sectionHeader confidence="0.993805" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999656692307692">
In this paper, we investigate the appli-
cation of recurrent neural network lan-
guage models (RNNLM) and factored
language models (FLM) to the task of
language modeling for Code-Switching
speech. We present a way to integrate part-
of-speech tags (POS) and language in-
formation (LID) into these models which
leads to significant improvements in terms
of perplexity. Furthermore, a comparison
between RNNLMs and FLMs and a de-
tailed analysis of perplexities on the dif-
ferent backoff levels are performed. Fi-
nally, we show that recurrent neural net-
works and factored language models can
be combined using linear interpolation to
achieve the best performance. The final
combined language model provides 37.8%
relative improvement in terms of perplex-
ity on the SEAME development set and
a relative improvement of 32.7% on the
evaluation set compared to the traditional
n-gram language model.
Index Terms: multilingual speech processing,
code switching, language modeling, recurrent
neural networks, factored language models
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999906205882353">
Code-Switching (CS) speech is defined as speech
that contains more than one language (’code’). It
is a common phenomenon in multilingual com-
munities (Auer, 1999a). For the automated pro-
cessing of spoken communication in these sce-
narios, a speech recognition system must be able
to handle code switches. However, the compo-
nents of speech recognition systems are usually
trained on monolingual data. Furthermore, there
is a lack of bilingual training data. While there
have been promising research results in the area
of acoustic modeling, only few approaches so far
address Code-Switching in the language model.
Recently, it has been shown that recurrent neu-
ral network language models (RNNLMs) can im-
prove perplexity and error rates in speech recogni-
tion systems in comparison to traditional n-gram
approaches (Mikolov et al., 2010; Mikolov et al.,
2011). One reason for that is their ability to han-
dle longer contexts. Furthermore, the integration
of additional features as input is rather straight-
forward due to their structure. On the other hand,
factored language models (FLMs) have been used
successfully for languages with rich morphology
due to their ability to process syntactical features,
such as word stems or part-of-speech tags (Bilmes
and Kirchhoff, 2003; El-Desoky et al., 2010).
The main contribution of this paper is the appli-
cation of RNNLMs and FLMs to the challenging
task of Code-Switching. Furthermore, the two dif-
ferent models are combined using linear interpo-
lation. In addition, a comparison between them is
provided including a detailed analysis to explain
their results.
</bodyText>
<sectionHeader confidence="0.999833" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999927866666667">
For this work, three different topics are investi-
gated and combined: linguistic investigation of
Code-Switching, recurrent neural network lan-
guage modeling and factored language models.
In (Muysken, 2000; Poplack, 1978; Bokamba,
1989), it is observed that code switches occur at
positions in an utterance where they do not violate
the syntactical rules of the involved languages. On
the one hand, Code-Switching can be regarded as
a speaker dependent phenomenon (Auer, 1999b;
Vu, Adel et al., 2013). On the other hand, par-
ticular Code-Switching patterns are shared across
speakers (Poplack, 1980). It can be observed that
part-of-speech tags may predict Code-Switching
points more reliable than words themselves. The
</bodyText>
<page confidence="0.982994">
206
</page>
<bodyText confidence="0.971312448979592">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 206–211,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
authors of (Solorio et al., 2008a) predict Code-
Switching points using several linguistic features,
such as word form, language ID, part-of-speech
tags or the position of the word relative to the
phrase (BIO). The best result is obtained by com-
bining those features. In (Chan et.al., 2006), four
different kinds of n-gram language models are
compared to predict Code-Switching. It is dis-
covered that clustering all foreign words into their
part-of-speech classes leads to the best perfor-
mance.
In the last years, neural networks have been used
for a variety of tasks, including language model-
ing (Mikolov et al., 2010). Recurrent neural net-
works are able to handle long-term contexts since
the input vector does not only contain the cur-
rent word but also the previous hidden layer. It
is shown that these networks outperform tradi-
tional language models, such as n-grams which
only contain very limited histories. In (Mikolov
et al., 2011), the network is extended by factoriz-
ing the output layer into classes to accelerate the
training and testing processes. The input layer
can be augmented to model features, such as part-
of-speech tags (Shi et al., 2011; Adel, Vu et al.,
2013). In (Adel, Vu et al., 2013), recurrent neural
networks are applied to Code-Switching speech. It
is shown that the integration of POS tags into the
neural network, which predicts the next language
as well as the next word, leads to significant per-
plexity reductions.
A factored language model refers to a word as a
vector of features, such as the word itself, morpho-
logical classes, POS tags or word stems. Hence, it
provides another possibility to integrate syntacti-
cal features into the language modeling process.
In (Bilmes and Kirchhoff, 2003), it is shown that
factored language models are able to outperform
standard n-gram techniques in terms of perplexity.
In the same paper, generalized parallel backoff is
introduced. This technique can be used to general-
ize traditional backoff methods and to improve the
performance of factored language models. Due to
the integration of various features, it is possible to
handle rich morphology in languages like Arabic
or Turkish (Duh and Kirchhoff, 2004; El-Desoky
et al., 2010).
</bodyText>
<sectionHeader confidence="0.789547" genericHeader="method">
3 Code-Switching Language Modeling
</sectionHeader>
<subsectionHeader confidence="0.997889">
3.1 Motivation
</subsectionHeader>
<bodyText confidence="0.999990625">
Since there is a lack of Code-Switching data, lan-
guage modeling is a challenging task. Traditional
n-gram approaches may not provide reliable esti-
mates. Hence, more general features than words
should be integrated into the language models.
Therefore, we apply recurrent neural networks and
factored language models. As features, we use
part-of-speech tags and language identifiers.
</bodyText>
<subsectionHeader confidence="0.999759">
3.2 Using Recurrent Neural Networks As
Language Model
</subsectionHeader>
<bodyText confidence="0.9998428">
This section describes the structure of the recur-
rent neural network (RNNLM) that we use as
Code-Switching language model. It has been pro-
posed in (Adel, Vu et al., 2013) and is illustrated
in figure 1.
</bodyText>
<figureCaption confidence="0.9929195">
Figure 1: RNNLM for Code-Switching
(based upon a figure in (Mikolov et al., 2011))
</figureCaption>
<bodyText confidence="0.999975375">
Vector w(t), which represents the current word us-
ing 1-of-N coding, forms the input of the recur-
rent neural network. Thus, its dimension equals
the size of the vocabulary. Vector s(t) con-
tains the state of the network and is called ’hid-
den layer’. The network is trained using back-
propagation through time (BPTT), an extension of
the back-propagation algorithm for recurrent neu-
ral networks. With BPTT, the error is propagated
through recurrent connections back in time for a
specific number of time steps t. Hence, the net-
work is able to remember information for several
time steps. The matrices U1, U2, V , and W con-
tain the weights for the connections between the
layers. These weights are learned during the train-
ing phase. Moreover, the output layer is factorized
</bodyText>
<equation confidence="0.8587795">
W(t) y(t)
f(t)
U1
U2
S(t)
W
V
C(t)
</equation>
<page confidence="0.974992">
207
</page>
<bodyText confidence="0.999988714285714">
into classes which provide language information.
In this work, four classes are used: English, Man-
darin, other languages and particles. Vector c(t)
contains the probabilities for each class and vector
y(t) provides the probabilities for each word given
its class. Hence, the probability P(wi|history) is
computed as shown in equation 1.
</bodyText>
<equation confidence="0.999873">
P(wi|history) = P(ci|s(t))P(wi|ci,s(t)) (1)
</equation>
<bodyText confidence="0.999976714285714">
It is intended to not only predict the next word but
also the next language. Hence according to equa-
tion 1, the probability of the next language is com-
puted first and then the probability of each word
given the language. Furthermore, a vector f(t)
is added to the input layer. It provides features
(in this work part-of-speech tags) corresponding
to the current word. Thus, not only the current
word is activated but also its features. Since the
POS tags are integrated into the input layer, they
are also propagated into the hidden layer and back-
propagated into its history s(t). Hence, not only
the previous feature is stored in the history but also
features from several time steps in the past.
</bodyText>
<subsectionHeader confidence="0.999876">
3.3 Using Factored Language Models
</subsectionHeader>
<bodyText confidence="0.999558185185185">
Factored language models (FLM) are another ap-
proach to integrate syntactical features, such as
part-of-speech tags or language identifiers into the
language modeling process. Each word is re-
garded as a sequence of features which are used
for the computation of the n-gram probabilities.
If a particular sequence of features has not been
detected in the training data, backoff techniques
will be applied. For our task of Code-Switching,
we develop two different models: One model with
only part-of-speech tags as features and one model
including also language information tags. Un-
fortunately, the number of possible parameters is
rather high: Different feature combinations from
different time steps can be used to predict the
next word (conditioning factors), different back-
off paths and different smoothing methods may
be applied. To detect useful parameters, the ge-
netic algorithm described in (Duh and Kirchhoff,
2004) is used. It is an evolution-inspired technique
that encodes the parameters of an FLM as binary
strings (genes). First, an initializing set of genes is
generated. Then, a loop follows that evaluates the
fitness of the genes and mutates them until their
average fitness is not improved any more. As fit-
ness value, the inverse perplexity of the FLM cor-
responding to the gene on the development set is
</bodyText>
<figureCaption confidence="0.995716">
Figure 2: Backoff graph of the FLM
</figureCaption>
<bodyText confidence="0.91980265">
used. Hence, parameter solutions with lower per-
plexities are preferred in the selection of the genes
for the following iteration. In (Duh and Kirch-
hoff, 2004), it is shown that this genetic method
outperforms both knowledge-based and random-
ized choices. For the case of part-of-speech tags
as features, the method results in three condition-
ing factors: the previous word Wt−1 and the two
previous POS tags Pt−1 and Pt−2. The backoff
graph obtained by the algorithm is illustrated in
figure 2. According to the result of the genetic al-
gorithm, different smoothing methods are used at
different backoff levels: For the backoff from three
factors to two factors, Kneser-Ney discounting is
applied. If the probabilities for the factor combi-
nation Wt−1Pt−2 could not be estimated reliably,
absolute discounting is used. In all other cases,
Witten-Bell discounting is applied. An overview
of the different smoothing methods can be found
in (Rosenfeld, 2000).
</bodyText>
<sectionHeader confidence="0.998407" genericHeader="evaluation">
4 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.942658">
4.1 Data Corpus
</subsectionHeader>
<bodyText confidence="0.999309">
SEAME (South East Asia Mandarin-English) is a
conversational Mandarin-English Code-Switching
speech corpus recorded from Singaporean and
Malaysian speakers (D.C. Lyu et al., 2011). It
was used for the research project ’Code-Switch’
jointly performed by Nanyang Technological Uni-
versity (NTU) and Karlsruhe Institute of Technol-
ogy (KIT). The recordings consist of spontanously
spoken interviews and conversations of about 63
hours of audio data. For this task, we deleted all
hesitations and divided the transcribed words into
four categories: English words, Mandarin words,
particles (Singaporean and Malaysian discourse
particles) and others (other languages). These cat-
egories are used as language information in the
language models. The average number of Code-
Switching points between Mandarin and English
</bodyText>
<figure confidence="0.752286714285714">
Wt-1 Pt-1 Pt-2
unigram
Wt-1 Pt-2
Pt-2
Wt-1
Wt-1 Pt-1
Pt-1
</figure>
<page confidence="0.994526">
208
</page>
<bodyText confidence="0.999489692307692">
is 2.6 per utterance and the duration of monolin-
gual segments is quite short: The average dura-
tion of English and Mandarin segments is only
0.67 seconds and 0.81 seconds respectively. In to-
tal, the corpus contains 9,210 unique English and
7,471 unique Mandarin vocabulary words. We di-
vided the corpus into three disjoint sets (training,
development and test set) and assigned the data
based on several criteria (gender, speaking style,
ratio of Singaporean and Malaysian speakers, ra-
tio of the four categories, and the duration in each
set). Table 1 lists the statistics of the corpus in
these sets.
</bodyText>
<table confidence="0.99917">
Train set Dev set Eval set
# Speakers 139 8 8
Duration(hrs) 59.2 2.1 1.5
# Utterances 48,040 1,943 1,018
# Token 525,168 23,776 11,294
</table>
<tableCaption confidence="0.99988">
Table 1: Statistics of the SEAME corpus
</tableCaption>
<subsectionHeader confidence="0.994557">
4.2 POS Tagger for Code-Switching Speech
</subsectionHeader>
<bodyText confidence="0.9999618">
To be able to assign part-of-speech tags to our
bilingual text corpus, we apply the POS tagger
described in (Schultz et al., 2010) and (Adel, Vu
et al., 2013). It consists of two different mono-
lingual (Stanford log-linear) taggers (Toutanova
et al., 2003; Toutanova et al., 2000) and a com-
bination of their results. While (Solorio et al.,
2008b) passes the whole Code-Switching text to
both monolingual taggers and combines their re-
sults using different heuristics, in this work, the
text is splitted into different languages first. The
tagging process is illustrated in figure 3.
Mandarin is determined as matrix language (the
main language of an utterance) and English as em-
bedded language. If three or more words of the
embedded language are detected, they are passed
to the English tagger. The rest of the text is passed
to the Mandarin tagger, even if it contains foreign
words. The idea is to provide the tagger as much
context as possible. Since most English words in
the Mandarin segments are falsely tagged as nouns
by the Mandarin tagger, a postprocessing step is
applied. It passes all foreign words of the Man-
darin segments to the English tagger in order to
replace the wrong tags with the correct ones.
</bodyText>
<figureCaption confidence="0.999033">
Figure 3: Tagging of Code-Switching speech
</figureCaption>
<subsectionHeader confidence="0.981512">
4.3 Evaluation
</subsectionHeader>
<bodyText confidence="0.999895571428572">
For evaluation, we compute the perplexity of each
language model on the SEAME development and
evaluation set und perform an analysis of the dif-
ferent back-off levels to understand in detail the
behavior of each language model. A traditional 3-
gram LM trained with the SEAME transcriptions
serves as baseline.
</bodyText>
<subsectionHeader confidence="0.755028">
4.3.1 LM Performance
</subsectionHeader>
<bodyText confidence="0.981889333333333">
The language models are evaluated in terms of per-
plexity. Table 2 presents the results on the devel-
opment and test set.
</bodyText>
<table confidence="0.984421666666667">
Model dev set test set
Baseline 3-gram 285.87 285.25
FLM (pos) 263.57 271.57
FLM (pos + lid) 263.84 276.99
RNNLM (pos) 233.50 268.05
RNNLM (pos + lid) 219.85 239.21
</table>
<tableCaption confidence="0.99773">
Table 2: Perplexity results
</tableCaption>
<bodyText confidence="0.999668375">
It can be noticed that both the RNNLM and the
FLM model outperform the traditional 3-gram
model. Hence, adding syntactical features im-
proves the word prediction. For the FLM, it leads
to no improvement to add the language identifier
as feature. In contrast, clustering the words into
their languages on the output layer of the RNNLM
leads to lower perplexities.
</bodyText>
<figure confidence="0.970970181818182">
„Matrix language“ = Mandarin
„Embedded language“ = English
Language islands
(&gt; 2 embedded
words)
POS
tagger for
English
Output
Postprocessing:
CS-text
English
segments
in
remaining
text
POS
tagger for
Mandarin
Remaining
text
Output
</figure>
<page confidence="0.98688">
209
</page>
<subsectionHeader confidence="0.953436">
4.3.2 Backoff Level Analysis
</subsectionHeader>
<bodyText confidence="0.999991666666667">
To understand the different results of the RNNLM
and the FLM, an analysis similar to the one de-
scribed in (Oparin et al., 2012) is performed. For
each word, the backoff-level of the n-gram model
is observed. Then, a level-dependent perplexity is
computed for each model as shown in equation 2.
</bodyText>
<equation confidence="0.979824">
PPLk = 10 n&apos;k Ewklog10P(wk|hk) (2)
</equation>
<bodyText confidence="0.998301833333333">
In the equation, k denotes the backoff-level, Nk
the number of words on this level, wk the current
word and hk its history. Table 3 shows how often
each backoff-level is used and presents the level-
dependent perplexities of each model on the de-
velopment set.
</bodyText>
<table confidence="0.9990542">
1-gram 2-gram 3-gram
# occurences 6894 11628 6226
Baseline 3-gram 5,786.24 165.82 28.28
FLM (pos) 4,950.31 147.70 30.99
RNNLM 3,231.02 151.67 21.24
</table>
<tableCaption confidence="0.999293">
Table 3: Backoff-level-dependent PPLs
</tableCaption>
<bodyText confidence="0.9998525">
In case of backoff to the 2-gram, the FLM pro-
vides the best perplexity, while for the 3-gram and
backoff to the 1-gram, the RNNLM performs best.
This may be correlated with the better over-all per-
plexity of the RNNLM in comparison to the FLM.
Nevertheless, the backoff to the 2-gram is used
about twice as often as the backoff to the 1-gram
or the 3-gram.
</bodyText>
<subsectionHeader confidence="0.993732">
4.4 LM Interpolation
</subsectionHeader>
<bodyText confidence="0.999993166666667">
The different results of RNNLM and FLM show
that they provide different estimates of the next
word. Thus, a combination of them may reduce
the perplexities of table 2. Hence, we apply lin-
ear interpolation to the probabilities of each two
models as shown in equation 3.
</bodyText>
<equation confidence="0.999894">
P(wjh) = A·PM1(wjh)+(1−A)·PM2(wjh) (3)
</equation>
<bodyText confidence="0.999125444444445">
The equation shows the computation of the pob-
ability for word w given its history h. PM1 de-
notes the probability provided by the first model
and PM2 the probability from the second model.
Table 4 shows the results of this experiment. The
weights are optimized on the development set.
The interpolation of RNNLM and FLM leads to
the best results. This may be caused by the supe-
rior backoff-level-dependent PPLs in comparison
</bodyText>
<table confidence="0.972085">
PPL PPL
Model weight on dev on eval
FLM + 3-gram 0.7, 0.3 211.13 227.57
RNNLM + 3-gram 0.8, 0.2 206.49 227.08
RNNLM + FLM 0.6, 0.4 177.79 192.08
</table>
<tableCaption confidence="0.998819">
Table 4: Perplexities after interpolation
</tableCaption>
<bodyText confidence="0.9998726">
to the 3-gram model. While the RNNLM performs
better for the 3-gram and for the backoff to the 1-
gram, the FLM performs the best in case of back-
off to the 2-gram which is used more often than
the other levels (table 3).
</bodyText>
<sectionHeader confidence="0.999407" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9999665625">
In this paper, we presented two different methods
for language modeling of Code-Switching speech:
Recurrent neural networks and factored language
models. We integrated part-of-speech tags and
language information to improve the performance
of the language models. In addition, we ana-
lyzed their behavior on the different backoff lev-
els. While the FLM performed better in case of
backoff to the 2-gram, the RNNLM led to a bet-
ter over-all performance. Finally, the models were
combined using linear interpolation. The com-
bined language model provided 37.8% relative im-
provement in terms of perplexity on the SEAME
development set and a relative improvement of
32.7% on the evaluation set compared to the tra-
ditional n-gram LM.
</bodyText>
<sectionHeader confidence="0.998567" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.992400222222222">
H. Adel, N.T. Vu, F. Kraus, T. Schlippe, and T. Schultz.
2013 Recurrent Neural Network Language Model-
ing for Code Switching Conversational Speech In:
Proceedings of ICASSP 2013.
P. Auer 1999 Code-Switching in Conversation , Rout-
ledge.
P. Auer 1999 From codeswitching via language mixing
to fused lects toward a dynamic typology of bilin-
gual speech In: International Journal of Bilingual-
ism, vol. 3, no. 4, pp. 309-332.
J.A. Bilmes and K. Kirchhoff. 2003 Factored Lan-
guage Models and Generalized Parallel Backoff In:
Proceedings of NAACL, 2003.
E.G. Bokamba 1989 Are there syntactic constraints on
code-mixing? In: World Englishes, vol. 8, no. 3, pp.
277-292.
J.Y.C. Chan, PC Ching, T. Lee, and H. Cao 2006
Automatic speech recognition of Cantonese-English
</reference>
<page confidence="0.993172">
210
</page>
<reference confidence="0.999092487804878">
code-mixing utterances In: Proceeding of Inter-
speech 2006.
K. Duh and K. Kirchhoff. 2004. Automatic Learning
of Language Model Structure, pg 148. In: Proceed-
ings of the 20th international conference on Compu-
tational Linguistics.
A. El-Desoky, R. Schl¨uter, H.Ney 2010 A Hybrid Mor-
phologically Decomposed Factored Language Mod-
els for Arabic LVCSR In: NAACL 2010.
D.C. Lyu, T.P. Tan, E.S. Cheng, H. Li 2011 An Anal-
ysis of Mandarin-English Code-Switching Speech
Corpus: SEAME In: Proceedings of Interspeech
2011.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993 Building a large annotated corpus of english:
The penn treebank In: Computational Linguistics,
vol. 19, no. 2, pp. 313330.
T. Mikolov, M. Karafiat, L. Burget, J. Jernocky and S.
Khudanpur. 2010 Recurrent Neural Network based
Language Model In: Proceedings of Interspeech
2010.
T. Mikolov, S. Kombrink, L. Burget, J. Jernocky and
S. Khudanpur. 2011 Extensions of Recurrent Neu-
ral Network Language Model In: Proceedings of
ICASSP 2011.
P. Muysken 2000 Bilingual speech: A typology of
code-mixing In: Cambridge University Press, vol.
11.
I. Oparin, M. Sundermeyer, H. Ney, J.-L. Gauvain
2012 Performance analysis of Neural Networks
in combination with n-gram language models In:
ICASSP, 2012.
S. Poplack 1978 Syntactic structure and social func-
tion of code-switching , Centro de Estudios Puertor-
riquenos, City University of New York.
S. Poplack 1980 Sometimes ill start a sentence in
spanish y termino en espanol: toward a typology of
code-switching In: Linguistics, vol. 18, no. 7-8, pp.
581-618.
R. Rosenfeld 2000 Two decades of statistical language
modeling: Where do we go from here? In: Proceed-
ings of the IEEE 88.8 (2000): 1270-1278.
T. Schultz, P. Fung, and C. Burgmer, 2010 Detecting
code-switch events based on textual features.
Y. Shi, P. Wiggers, M. Jonker 2011 Towards Recurrent
Neural Network Language Model with Linguistics
and Contextual Features In: Proceedings of Inter-
speech 2011.
T. Solorio, Y. Liu 2008 Part-of-speech tagging for
English-Spanish code-switched text In: Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing. Association for Computa-
tional Linguistics, 2008.
T. Solorio, Y. Liu 2008 Learning to predict code-
switching points In: Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing. Association for Computational Linguis-
tics, 2008.
K. Toutanova, C.D. Manning 2000 Enriching the
knowledge sources used in a maximum entropy part-
of-speech tagger In: Proceedings of the 2000 Joint
SIGDAT conference on Empirical methods in natu-
ral language processing and very large corpora: held
in conjunction with the 38th Annual Meeting of the
Association for Computational Linguistics, vol. 13.
K. Toutanova, D. Klein, C.D. Manning, and Y. Singer
2003 Feature-rich part-of-speech tagging with a
cyclic dependency network In: Proceedings of
NAACL 2003.
N.T. Vu, D.C. Lyu, J. Weiner, D. Telaar, T. Schlippe, F.
Blaicher, E.S. Chng, T. Schultz, H. Li 2012 A First
Speech Recognition System For Mandarin-English
Code-Switch Conversational Speech In: Proceed-
ings of Interspeech 2012.
N.T. Vu, H. Adel, T. Schultz 2013 An Investigation of
Code-Switching Attitude Dependent Language Mod-
eling In: In Statistical Language and Speech Pro-
cessing, First International Conference, 2013.
N. Xue, F. Xia, F.D. Chiou, and M. Palmer 2005 The
penn chinese treebank: Phrase structure annotation
of a large corpusk In: Natural Language Engineer-
ing, vol. 11, no. 2, pp. 207.
</reference>
<page confidence="0.998786">
211
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.494294">
<title confidence="0.998812">Combination of Recurrent Neural Networks and Factored Models for Code-Switching Language Modeling</title>
<author confidence="0.961097">Heike Adel Ngoc Thang Vu Tanja Schultz</author>
<affiliation confidence="0.99803">Institute for Anthropomatics, Karlsruhe Institute of Technology (KIT)</affiliation>
<email confidence="0.998779">heike.adel@student.kit.eduthang.vu@kit.edutanja.schultz@kit.edu</email>
<abstract confidence="0.979336481481482">In this paper, we investigate the application of recurrent neural network language models (RNNLM) and factored language models (FLM) to the task of language modeling for Code-Switching speech. We present a way to integrate partof-speech tags (POS) and language information (LID) into these models which leads to significant improvements in terms of perplexity. Furthermore, a comparison between RNNLMs and FLMs and a detailed analysis of perplexities on the different backoff levels are performed. Finally, we show that recurrent neural networks and factored language models can be combined using linear interpolation to achieve the best performance. The final combined language model provides 37.8% relative improvement in terms of perplexity on the SEAME development set and a relative improvement of 32.7% on the evaluation set compared to the traditional n-gram language model. multilingual speech processing, code switching, language modeling, recurrent neural networks, factored language models</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Adel</author>
<author>N T Vu</author>
<author>F Kraus</author>
<author>T Schlippe</author>
<author>T Schultz</author>
</authors>
<title>Recurrent Neural Network Language Modeling for Code Switching Conversational Speech In:</title>
<date>2013</date>
<booktitle>Proceedings of ICASSP</booktitle>
<contexts>
<context position="3431" citStr="Adel et al., 2013" startWordPosition="513" endWordPosition="516"> addition, a comparison between them is provided including a detailed analysis to explain their results. 2 Related Work For this work, three different topics are investigated and combined: linguistic investigation of Code-Switching, recurrent neural network language modeling and factored language models. In (Muysken, 2000; Poplack, 1978; Bokamba, 1989), it is observed that code switches occur at positions in an utterance where they do not violate the syntactical rules of the involved languages. On the one hand, Code-Switching can be regarded as a speaker dependent phenomenon (Auer, 1999b; Vu, Adel et al., 2013). On the other hand, particular Code-Switching patterns are shared across speakers (Poplack, 1980). It can be observed that part-of-speech tags may predict Code-Switching points more reliable than words themselves. The 206 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 206–211, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics authors of (Solorio et al., 2008a) predict CodeSwitching points using several linguistic features, such as word form, language ID, part-of-speech tags or the position of the word relative to</context>
</contexts>
<marker>Adel, Vu, Kraus, Schlippe, Schultz, 2013</marker>
<rawString>H. Adel, N.T. Vu, F. Kraus, T. Schlippe, and T. Schultz. 2013 Recurrent Neural Network Language Modeling for Code Switching Conversational Speech In: Proceedings of ICASSP 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Auer</author>
</authors>
<date>1999</date>
<booktitle>Code-Switching in Conversation ,</booktitle>
<publisher>Routledge.</publisher>
<contexts>
<context position="1480" citStr="Auer, 1999" startWordPosition="211" endWordPosition="212">be combined using linear interpolation to achieve the best performance. The final combined language model provides 37.8% relative improvement in terms of perplexity on the SEAME development set and a relative improvement of 32.7% on the evaluation set compared to the traditional n-gram language model. Index Terms: multilingual speech processing, code switching, language modeling, recurrent neural networks, factored language models 1 Introduction Code-Switching (CS) speech is defined as speech that contains more than one language (’code’). It is a common phenomenon in multilingual communities (Auer, 1999a). For the automated processing of spoken communication in these scenarios, a speech recognition system must be able to handle code switches. However, the components of speech recognition systems are usually trained on monolingual data. Furthermore, there is a lack of bilingual training data. While there have been promising research results in the area of acoustic modeling, only few approaches so far address Code-Switching in the language model. Recently, it has been shown that recurrent neural network language models (RNNLMs) can improve perplexity and error rates in speech recognition syste</context>
<context position="3406" citStr="Auer, 1999" startWordPosition="510" endWordPosition="511">interpolation. In addition, a comparison between them is provided including a detailed analysis to explain their results. 2 Related Work For this work, three different topics are investigated and combined: linguistic investigation of Code-Switching, recurrent neural network language modeling and factored language models. In (Muysken, 2000; Poplack, 1978; Bokamba, 1989), it is observed that code switches occur at positions in an utterance where they do not violate the syntactical rules of the involved languages. On the one hand, Code-Switching can be regarded as a speaker dependent phenomenon (Auer, 1999b; Vu, Adel et al., 2013). On the other hand, particular Code-Switching patterns are shared across speakers (Poplack, 1980). It can be observed that part-of-speech tags may predict Code-Switching points more reliable than words themselves. The 206 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 206–211, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics authors of (Solorio et al., 2008a) predict CodeSwitching points using several linguistic features, such as word form, language ID, part-of-speech tags or the positio</context>
</contexts>
<marker>Auer, 1999</marker>
<rawString>P. Auer 1999 Code-Switching in Conversation , Routledge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Auer</author>
</authors>
<title>From codeswitching via language mixing to fused lects toward a dynamic typology of bilingual speech In:</title>
<date>1999</date>
<journal>International Journal of Bilingualism,</journal>
<volume>3</volume>
<pages>309--332</pages>
<contexts>
<context position="1480" citStr="Auer, 1999" startWordPosition="211" endWordPosition="212">be combined using linear interpolation to achieve the best performance. The final combined language model provides 37.8% relative improvement in terms of perplexity on the SEAME development set and a relative improvement of 32.7% on the evaluation set compared to the traditional n-gram language model. Index Terms: multilingual speech processing, code switching, language modeling, recurrent neural networks, factored language models 1 Introduction Code-Switching (CS) speech is defined as speech that contains more than one language (’code’). It is a common phenomenon in multilingual communities (Auer, 1999a). For the automated processing of spoken communication in these scenarios, a speech recognition system must be able to handle code switches. However, the components of speech recognition systems are usually trained on monolingual data. Furthermore, there is a lack of bilingual training data. While there have been promising research results in the area of acoustic modeling, only few approaches so far address Code-Switching in the language model. Recently, it has been shown that recurrent neural network language models (RNNLMs) can improve perplexity and error rates in speech recognition syste</context>
<context position="3406" citStr="Auer, 1999" startWordPosition="510" endWordPosition="511">interpolation. In addition, a comparison between them is provided including a detailed analysis to explain their results. 2 Related Work For this work, three different topics are investigated and combined: linguistic investigation of Code-Switching, recurrent neural network language modeling and factored language models. In (Muysken, 2000; Poplack, 1978; Bokamba, 1989), it is observed that code switches occur at positions in an utterance where they do not violate the syntactical rules of the involved languages. On the one hand, Code-Switching can be regarded as a speaker dependent phenomenon (Auer, 1999b; Vu, Adel et al., 2013). On the other hand, particular Code-Switching patterns are shared across speakers (Poplack, 1980). It can be observed that part-of-speech tags may predict Code-Switching points more reliable than words themselves. The 206 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 206–211, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics authors of (Solorio et al., 2008a) predict CodeSwitching points using several linguistic features, such as word form, language ID, part-of-speech tags or the positio</context>
</contexts>
<marker>Auer, 1999</marker>
<rawString>P. Auer 1999 From codeswitching via language mixing to fused lects toward a dynamic typology of bilingual speech In: International Journal of Bilingualism, vol. 3, no. 4, pp. 309-332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Bilmes</author>
<author>K Kirchhoff</author>
</authors>
<title>Factored Language Models and Generalized Parallel Backoff In:</title>
<date>2003</date>
<booktitle>Proceedings of NAACL,</booktitle>
<contexts>
<context position="2588" citStr="Bilmes and Kirchhoff, 2003" startWordPosition="382" endWordPosition="385">t recurrent neural network language models (RNNLMs) can improve perplexity and error rates in speech recognition systems in comparison to traditional n-gram approaches (Mikolov et al., 2010; Mikolov et al., 2011). One reason for that is their ability to handle longer contexts. Furthermore, the integration of additional features as input is rather straightforward due to their structure. On the other hand, factored language models (FLMs) have been used successfully for languages with rich morphology due to their ability to process syntactical features, such as word stems or part-of-speech tags (Bilmes and Kirchhoff, 2003; El-Desoky et al., 2010). The main contribution of this paper is the application of RNNLMs and FLMs to the challenging task of Code-Switching. Furthermore, the two different models are combined using linear interpolation. In addition, a comparison between them is provided including a detailed analysis to explain their results. 2 Related Work For this work, three different topics are investigated and combined: linguistic investigation of Code-Switching, recurrent neural network language modeling and factored language models. In (Muysken, 2000; Poplack, 1978; Bokamba, 1989), it is observed that</context>
<context position="5566" citStr="Bilmes and Kirchhoff, 2003" startWordPosition="854" endWordPosition="857">l features, such as partof-speech tags (Shi et al., 2011; Adel, Vu et al., 2013). In (Adel, Vu et al., 2013), recurrent neural networks are applied to Code-Switching speech. It is shown that the integration of POS tags into the neural network, which predicts the next language as well as the next word, leads to significant perplexity reductions. A factored language model refers to a word as a vector of features, such as the word itself, morphological classes, POS tags or word stems. Hence, it provides another possibility to integrate syntactical features into the language modeling process. In (Bilmes and Kirchhoff, 2003), it is shown that factored language models are able to outperform standard n-gram techniques in terms of perplexity. In the same paper, generalized parallel backoff is introduced. This technique can be used to generalize traditional backoff methods and to improve the performance of factored language models. Due to the integration of various features, it is possible to handle rich morphology in languages like Arabic or Turkish (Duh and Kirchhoff, 2004; El-Desoky et al., 2010). 3 Code-Switching Language Modeling 3.1 Motivation Since there is a lack of Code-Switching data, language modeling is a</context>
</contexts>
<marker>Bilmes, Kirchhoff, 2003</marker>
<rawString>J.A. Bilmes and K. Kirchhoff. 2003 Factored Language Models and Generalized Parallel Backoff In: Proceedings of NAACL, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E G Bokamba</author>
</authors>
<title>Are there syntactic constraints on code-mixing? In:</title>
<date>1989</date>
<journal>World Englishes,</journal>
<volume>8</volume>
<pages>277--292</pages>
<contexts>
<context position="3167" citStr="Bokamba, 1989" startWordPosition="471" endWordPosition="472">ch tags (Bilmes and Kirchhoff, 2003; El-Desoky et al., 2010). The main contribution of this paper is the application of RNNLMs and FLMs to the challenging task of Code-Switching. Furthermore, the two different models are combined using linear interpolation. In addition, a comparison between them is provided including a detailed analysis to explain their results. 2 Related Work For this work, three different topics are investigated and combined: linguistic investigation of Code-Switching, recurrent neural network language modeling and factored language models. In (Muysken, 2000; Poplack, 1978; Bokamba, 1989), it is observed that code switches occur at positions in an utterance where they do not violate the syntactical rules of the involved languages. On the one hand, Code-Switching can be regarded as a speaker dependent phenomenon (Auer, 1999b; Vu, Adel et al., 2013). On the other hand, particular Code-Switching patterns are shared across speakers (Poplack, 1980). It can be observed that part-of-speech tags may predict Code-Switching points more reliable than words themselves. The 206 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 206–211, Sofia, Bu</context>
</contexts>
<marker>Bokamba, 1989</marker>
<rawString>E.G. Bokamba 1989 Are there syntactic constraints on code-mixing? In: World Englishes, vol. 8, no. 3, pp. 277-292.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Y C Chan</author>
<author>PC Ching</author>
<author>T Lee</author>
<author>H Cao</author>
</authors>
<title>Automatic speech recognition of Cantonese-English code-mixing utterances In: Proceeding of Interspeech</title>
<date>2006</date>
<marker>Chan, Ching, Lee, Cao, 2006</marker>
<rawString>J.Y.C. Chan, PC Ching, T. Lee, and H. Cao 2006 Automatic speech recognition of Cantonese-English code-mixing utterances In: Proceeding of Interspeech 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Duh</author>
<author>K Kirchhoff</author>
</authors>
<title>Automatic Learning of Language Model Structure, pg 148. In:</title>
<date>2004</date>
<booktitle>Proceedings of the 20th international conference on Computational Linguistics.</booktitle>
<contexts>
<context position="6021" citStr="Duh and Kirchhoff, 2004" startWordPosition="925" endWordPosition="928">es, POS tags or word stems. Hence, it provides another possibility to integrate syntactical features into the language modeling process. In (Bilmes and Kirchhoff, 2003), it is shown that factored language models are able to outperform standard n-gram techniques in terms of perplexity. In the same paper, generalized parallel backoff is introduced. This technique can be used to generalize traditional backoff methods and to improve the performance of factored language models. Due to the integration of various features, it is possible to handle rich morphology in languages like Arabic or Turkish (Duh and Kirchhoff, 2004; El-Desoky et al., 2010). 3 Code-Switching Language Modeling 3.1 Motivation Since there is a lack of Code-Switching data, language modeling is a challenging task. Traditional n-gram approaches may not provide reliable estimates. Hence, more general features than words should be integrated into the language models. Therefore, we apply recurrent neural networks and factored language models. As features, we use part-of-speech tags and language identifiers. 3.2 Using Recurrent Neural Networks As Language Model This section describes the structure of the recurrent neural network (RNNLM) that we us</context>
<context position="9663" citStr="Duh and Kirchhoff, 2004" startWordPosition="1513" endWordPosition="1516">cular sequence of features has not been detected in the training data, backoff techniques will be applied. For our task of Code-Switching, we develop two different models: One model with only part-of-speech tags as features and one model including also language information tags. Unfortunately, the number of possible parameters is rather high: Different feature combinations from different time steps can be used to predict the next word (conditioning factors), different backoff paths and different smoothing methods may be applied. To detect useful parameters, the genetic algorithm described in (Duh and Kirchhoff, 2004) is used. It is an evolution-inspired technique that encodes the parameters of an FLM as binary strings (genes). First, an initializing set of genes is generated. Then, a loop follows that evaluates the fitness of the genes and mutates them until their average fitness is not improved any more. As fitness value, the inverse perplexity of the FLM corresponding to the gene on the development set is Figure 2: Backoff graph of the FLM used. Hence, parameter solutions with lower perplexities are preferred in the selection of the genes for the following iteration. In (Duh and Kirchhoff, 2004), it is </context>
</contexts>
<marker>Duh, Kirchhoff, 2004</marker>
<rawString>K. Duh and K. Kirchhoff. 2004. Automatic Learning of Language Model Structure, pg 148. In: Proceedings of the 20th international conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A El-Desoky</author>
<author>R Schl¨uter</author>
</authors>
<title>A Hybrid Morphologically Decomposed Factored Language Models for Arabic LVCSR In: NAACL</title>
<date>2010</date>
<marker>El-Desoky, Schl¨uter, 2010</marker>
<rawString>A. El-Desoky, R. Schl¨uter, H.Ney 2010 A Hybrid Morphologically Decomposed Factored Language Models for Arabic LVCSR In: NAACL 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Lyu</author>
<author>T P Tan</author>
<author>E S Cheng</author>
<author>H Li</author>
</authors>
<title>An Analysis of Mandarin-English Code-Switching Speech Corpus: SEAME In:</title>
<date>2011</date>
<booktitle>Proceedings of Interspeech</booktitle>
<contexts>
<context position="11271" citStr="Lyu et al., 2011" startWordPosition="1770" endWordPosition="1773">thing methods are used at different backoff levels: For the backoff from three factors to two factors, Kneser-Ney discounting is applied. If the probabilities for the factor combination Wt−1Pt−2 could not be estimated reliably, absolute discounting is used. In all other cases, Witten-Bell discounting is applied. An overview of the different smoothing methods can be found in (Rosenfeld, 2000). 4 Experiments and Results 4.1 Data Corpus SEAME (South East Asia Mandarin-English) is a conversational Mandarin-English Code-Switching speech corpus recorded from Singaporean and Malaysian speakers (D.C. Lyu et al., 2011). It was used for the research project ’Code-Switch’ jointly performed by Nanyang Technological University (NTU) and Karlsruhe Institute of Technology (KIT). The recordings consist of spontanously spoken interviews and conversations of about 63 hours of audio data. For this task, we deleted all hesitations and divided the transcribed words into four categories: English words, Mandarin words, particles (Singaporean and Malaysian discourse particles) and others (other languages). These categories are used as language information in the language models. The average number of CodeSwitching points </context>
</contexts>
<marker>Lyu, Tan, Cheng, Li, 2011</marker>
<rawString>D.C. Lyu, T.P. Tan, E.S. Cheng, H. Li 2011 An Analysis of Mandarin-English Code-Switching Speech Corpus: SEAME In: Proceedings of Interspeech 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>M A Marcinkiewicz</author>
<author>B Santorini</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank In:</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<pages>313330</pages>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini. 1993 Building a large annotated corpus of english: The penn treebank In: Computational Linguistics, vol. 19, no. 2, pp. 313330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mikolov</author>
<author>M Karafiat</author>
<author>L Burget</author>
<author>J Jernocky</author>
<author>S Khudanpur</author>
</authors>
<title>Recurrent Neural Network based Language Model In:</title>
<date>2010</date>
<booktitle>Proceedings of Interspeech</booktitle>
<contexts>
<context position="2151" citStr="Mikolov et al., 2010" startWordPosition="314" endWordPosition="317">ation in these scenarios, a speech recognition system must be able to handle code switches. However, the components of speech recognition systems are usually trained on monolingual data. Furthermore, there is a lack of bilingual training data. While there have been promising research results in the area of acoustic modeling, only few approaches so far address Code-Switching in the language model. Recently, it has been shown that recurrent neural network language models (RNNLMs) can improve perplexity and error rates in speech recognition systems in comparison to traditional n-gram approaches (Mikolov et al., 2010; Mikolov et al., 2011). One reason for that is their ability to handle longer contexts. Furthermore, the integration of additional features as input is rather straightforward due to their structure. On the other hand, factored language models (FLMs) have been used successfully for languages with rich morphology due to their ability to process syntactical features, such as word stems or part-of-speech tags (Bilmes and Kirchhoff, 2003; El-Desoky et al., 2010). The main contribution of this paper is the application of RNNLMs and FLMs to the challenging task of Code-Switching. Furthermore, the tw</context>
<context position="4458" citStr="Mikolov et al., 2010" startWordPosition="669" endWordPosition="672">ics authors of (Solorio et al., 2008a) predict CodeSwitching points using several linguistic features, such as word form, language ID, part-of-speech tags or the position of the word relative to the phrase (BIO). The best result is obtained by combining those features. In (Chan et.al., 2006), four different kinds of n-gram language models are compared to predict Code-Switching. It is discovered that clustering all foreign words into their part-of-speech classes leads to the best performance. In the last years, neural networks have been used for a variety of tasks, including language modeling (Mikolov et al., 2010). Recurrent neural networks are able to handle long-term contexts since the input vector does not only contain the current word but also the previous hidden layer. It is shown that these networks outperform traditional language models, such as n-grams which only contain very limited histories. In (Mikolov et al., 2011), the network is extended by factorizing the output layer into classes to accelerate the training and testing processes. The input layer can be augmented to model features, such as partof-speech tags (Shi et al., 2011; Adel, Vu et al., 2013). In (Adel, Vu et al., 2013), recurrent</context>
</contexts>
<marker>Mikolov, Karafiat, Burget, Jernocky, Khudanpur, 2010</marker>
<rawString>T. Mikolov, M. Karafiat, L. Burget, J. Jernocky and S. Khudanpur. 2010 Recurrent Neural Network based Language Model In: Proceedings of Interspeech 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mikolov</author>
<author>S Kombrink</author>
<author>L Burget</author>
<author>J Jernocky</author>
<author>S Khudanpur</author>
</authors>
<title>Extensions of Recurrent Neural Network Language Model In:</title>
<date>2011</date>
<booktitle>Proceedings of ICASSP</booktitle>
<contexts>
<context position="2174" citStr="Mikolov et al., 2011" startWordPosition="318" endWordPosition="321">os, a speech recognition system must be able to handle code switches. However, the components of speech recognition systems are usually trained on monolingual data. Furthermore, there is a lack of bilingual training data. While there have been promising research results in the area of acoustic modeling, only few approaches so far address Code-Switching in the language model. Recently, it has been shown that recurrent neural network language models (RNNLMs) can improve perplexity and error rates in speech recognition systems in comparison to traditional n-gram approaches (Mikolov et al., 2010; Mikolov et al., 2011). One reason for that is their ability to handle longer contexts. Furthermore, the integration of additional features as input is rather straightforward due to their structure. On the other hand, factored language models (FLMs) have been used successfully for languages with rich morphology due to their ability to process syntactical features, such as word stems or part-of-speech tags (Bilmes and Kirchhoff, 2003; El-Desoky et al., 2010). The main contribution of this paper is the application of RNNLMs and FLMs to the challenging task of Code-Switching. Furthermore, the two different models are </context>
<context position="4778" citStr="Mikolov et al., 2011" startWordPosition="722" endWordPosition="725">gram language models are compared to predict Code-Switching. It is discovered that clustering all foreign words into their part-of-speech classes leads to the best performance. In the last years, neural networks have been used for a variety of tasks, including language modeling (Mikolov et al., 2010). Recurrent neural networks are able to handle long-term contexts since the input vector does not only contain the current word but also the previous hidden layer. It is shown that these networks outperform traditional language models, such as n-grams which only contain very limited histories. In (Mikolov et al., 2011), the network is extended by factorizing the output layer into classes to accelerate the training and testing processes. The input layer can be augmented to model features, such as partof-speech tags (Shi et al., 2011; Adel, Vu et al., 2013). In (Adel, Vu et al., 2013), recurrent neural networks are applied to Code-Switching speech. It is shown that the integration of POS tags into the neural network, which predicts the next language as well as the next word, leads to significant perplexity reductions. A factored language model refers to a word as a vector of features, such as the word itself,</context>
<context position="6818" citStr="Mikolov et al., 2011" startWordPosition="1049" endWordPosition="1052">n-gram approaches may not provide reliable estimates. Hence, more general features than words should be integrated into the language models. Therefore, we apply recurrent neural networks and factored language models. As features, we use part-of-speech tags and language identifiers. 3.2 Using Recurrent Neural Networks As Language Model This section describes the structure of the recurrent neural network (RNNLM) that we use as Code-Switching language model. It has been proposed in (Adel, Vu et al., 2013) and is illustrated in figure 1. Figure 1: RNNLM for Code-Switching (based upon a figure in (Mikolov et al., 2011)) Vector w(t), which represents the current word using 1-of-N coding, forms the input of the recurrent neural network. Thus, its dimension equals the size of the vocabulary. Vector s(t) contains the state of the network and is called ’hidden layer’. The network is trained using backpropagation through time (BPTT), an extension of the back-propagation algorithm for recurrent neural networks. With BPTT, the error is propagated through recurrent connections back in time for a specific number of time steps t. Hence, the network is able to remember information for several time steps. The matrices U</context>
</contexts>
<marker>Mikolov, Kombrink, Burget, Jernocky, Khudanpur, 2011</marker>
<rawString>T. Mikolov, S. Kombrink, L. Burget, J. Jernocky and S. Khudanpur. 2011 Extensions of Recurrent Neural Network Language Model In: Proceedings of ICASSP 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Muysken</author>
</authors>
<title>Bilingual speech: A typology of code-mixing In:</title>
<date>2000</date>
<volume>11</volume>
<publisher>Cambridge University Press,</publisher>
<contexts>
<context position="3136" citStr="Muysken, 2000" startWordPosition="467" endWordPosition="468"> as word stems or part-of-speech tags (Bilmes and Kirchhoff, 2003; El-Desoky et al., 2010). The main contribution of this paper is the application of RNNLMs and FLMs to the challenging task of Code-Switching. Furthermore, the two different models are combined using linear interpolation. In addition, a comparison between them is provided including a detailed analysis to explain their results. 2 Related Work For this work, three different topics are investigated and combined: linguistic investigation of Code-Switching, recurrent neural network language modeling and factored language models. In (Muysken, 2000; Poplack, 1978; Bokamba, 1989), it is observed that code switches occur at positions in an utterance where they do not violate the syntactical rules of the involved languages. On the one hand, Code-Switching can be regarded as a speaker dependent phenomenon (Auer, 1999b; Vu, Adel et al., 2013). On the other hand, particular Code-Switching patterns are shared across speakers (Poplack, 1980). It can be observed that part-of-speech tags may predict Code-Switching points more reliable than words themselves. The 206 Proceedings of the 51st Annual Meeting of the Association for Computational Lingui</context>
</contexts>
<marker>Muysken, 2000</marker>
<rawString>P. Muysken 2000 Bilingual speech: A typology of code-mixing In: Cambridge University Press, vol. 11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Oparin</author>
<author>M Sundermeyer</author>
<author>H Ney</author>
<author>J-L</author>
</authors>
<title>Performance analysis of Neural Networks in combination with n-gram language models In: ICASSP,</title>
<date>2012</date>
<publisher>Gauvain</publisher>
<contexts>
<context position="15450" citStr="Oparin et al., 2012" startWordPosition="2456" endWordPosition="2459">proves the word prediction. For the FLM, it leads to no improvement to add the language identifier as feature. In contrast, clustering the words into their languages on the output layer of the RNNLM leads to lower perplexities. „Matrix language“ = Mandarin „Embedded language“ = English Language islands (&gt; 2 embedded words) POS tagger for English Output Postprocessing: CS-text English segments in remaining text POS tagger for Mandarin Remaining text Output 209 4.3.2 Backoff Level Analysis To understand the different results of the RNNLM and the FLM, an analysis similar to the one described in (Oparin et al., 2012) is performed. For each word, the backoff-level of the n-gram model is observed. Then, a level-dependent perplexity is computed for each model as shown in equation 2. PPLk = 10 n&apos;k Ewklog10P(wk|hk) (2) In the equation, k denotes the backoff-level, Nk the number of words on this level, wk the current word and hk its history. Table 3 shows how often each backoff-level is used and presents the leveldependent perplexities of each model on the development set. 1-gram 2-gram 3-gram # occurences 6894 11628 6226 Baseline 3-gram 5,786.24 165.82 28.28 FLM (pos) 4,950.31 147.70 30.99 RNNLM 3,231.02 151.6</context>
</contexts>
<marker>Oparin, Sundermeyer, Ney, J-L, 2012</marker>
<rawString>I. Oparin, M. Sundermeyer, H. Ney, J.-L. Gauvain 2012 Performance analysis of Neural Networks in combination with n-gram language models In: ICASSP, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Poplack</author>
</authors>
<title>Syntactic structure and social function of code-switching ,</title>
<date>1978</date>
<institution>Centro de Estudios Puertorriquenos, City University of New York.</institution>
<contexts>
<context position="3151" citStr="Poplack, 1978" startWordPosition="469" endWordPosition="470">or part-of-speech tags (Bilmes and Kirchhoff, 2003; El-Desoky et al., 2010). The main contribution of this paper is the application of RNNLMs and FLMs to the challenging task of Code-Switching. Furthermore, the two different models are combined using linear interpolation. In addition, a comparison between them is provided including a detailed analysis to explain their results. 2 Related Work For this work, three different topics are investigated and combined: linguistic investigation of Code-Switching, recurrent neural network language modeling and factored language models. In (Muysken, 2000; Poplack, 1978; Bokamba, 1989), it is observed that code switches occur at positions in an utterance where they do not violate the syntactical rules of the involved languages. On the one hand, Code-Switching can be regarded as a speaker dependent phenomenon (Auer, 1999b; Vu, Adel et al., 2013). On the other hand, particular Code-Switching patterns are shared across speakers (Poplack, 1980). It can be observed that part-of-speech tags may predict Code-Switching points more reliable than words themselves. The 206 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 20</context>
</contexts>
<marker>Poplack, 1978</marker>
<rawString>S. Poplack 1978 Syntactic structure and social function of code-switching , Centro de Estudios Puertorriquenos, City University of New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Poplack</author>
</authors>
<title>Sometimes ill start a sentence in spanish y termino en espanol: toward a typology of code-switching In:</title>
<date>1980</date>
<journal>Linguistics,</journal>
<volume>18</volume>
<pages>7--8</pages>
<contexts>
<context position="3529" citStr="Poplack, 1980" startWordPosition="529" endWordPosition="530">s. 2 Related Work For this work, three different topics are investigated and combined: linguistic investigation of Code-Switching, recurrent neural network language modeling and factored language models. In (Muysken, 2000; Poplack, 1978; Bokamba, 1989), it is observed that code switches occur at positions in an utterance where they do not violate the syntactical rules of the involved languages. On the one hand, Code-Switching can be regarded as a speaker dependent phenomenon (Auer, 1999b; Vu, Adel et al., 2013). On the other hand, particular Code-Switching patterns are shared across speakers (Poplack, 1980). It can be observed that part-of-speech tags may predict Code-Switching points more reliable than words themselves. The 206 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 206–211, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics authors of (Solorio et al., 2008a) predict CodeSwitching points using several linguistic features, such as word form, language ID, part-of-speech tags or the position of the word relative to the phrase (BIO). The best result is obtained by combining those features. In (Chan et.al., 2006)</context>
</contexts>
<marker>Poplack, 1980</marker>
<rawString>S. Poplack 1980 Sometimes ill start a sentence in spanish y termino en espanol: toward a typology of code-switching In: Linguistics, vol. 18, no. 7-8, pp. 581-618.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rosenfeld</author>
</authors>
<title>Two decades of statistical language modeling: Where do we go from here? In:</title>
<date>2000</date>
<booktitle>Proceedings of the IEEE</booktitle>
<volume>88</volume>
<pages>1270--1278</pages>
<contexts>
<context position="11048" citStr="Rosenfeld, 2000" startWordPosition="1742" endWordPosition="1743">itioning factors: the previous word Wt−1 and the two previous POS tags Pt−1 and Pt−2. The backoff graph obtained by the algorithm is illustrated in figure 2. According to the result of the genetic algorithm, different smoothing methods are used at different backoff levels: For the backoff from three factors to two factors, Kneser-Ney discounting is applied. If the probabilities for the factor combination Wt−1Pt−2 could not be estimated reliably, absolute discounting is used. In all other cases, Witten-Bell discounting is applied. An overview of the different smoothing methods can be found in (Rosenfeld, 2000). 4 Experiments and Results 4.1 Data Corpus SEAME (South East Asia Mandarin-English) is a conversational Mandarin-English Code-Switching speech corpus recorded from Singaporean and Malaysian speakers (D.C. Lyu et al., 2011). It was used for the research project ’Code-Switch’ jointly performed by Nanyang Technological University (NTU) and Karlsruhe Institute of Technology (KIT). The recordings consist of spontanously spoken interviews and conversations of about 63 hours of audio data. For this task, we deleted all hesitations and divided the transcribed words into four categories: English words</context>
</contexts>
<marker>Rosenfeld, 2000</marker>
<rawString>R. Rosenfeld 2000 Two decades of statistical language modeling: Where do we go from here? In: Proceedings of the IEEE 88.8 (2000): 1270-1278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Schultz</author>
<author>P Fung</author>
<author>C Burgmer</author>
</authors>
<title>Detecting code-switch events based on textual features.</title>
<date>2010</date>
<contexts>
<context position="12908" citStr="Schultz et al., 2010" startWordPosition="2034" endWordPosition="2037"> (training, development and test set) and assigned the data based on several criteria (gender, speaking style, ratio of Singaporean and Malaysian speakers, ratio of the four categories, and the duration in each set). Table 1 lists the statistics of the corpus in these sets. Train set Dev set Eval set # Speakers 139 8 8 Duration(hrs) 59.2 2.1 1.5 # Utterances 48,040 1,943 1,018 # Token 525,168 23,776 11,294 Table 1: Statistics of the SEAME corpus 4.2 POS Tagger for Code-Switching Speech To be able to assign part-of-speech tags to our bilingual text corpus, we apply the POS tagger described in (Schultz et al., 2010) and (Adel, Vu et al., 2013). It consists of two different monolingual (Stanford log-linear) taggers (Toutanova et al., 2003; Toutanova et al., 2000) and a combination of their results. While (Solorio et al., 2008b) passes the whole Code-Switching text to both monolingual taggers and combines their results using different heuristics, in this work, the text is splitted into different languages first. The tagging process is illustrated in figure 3. Mandarin is determined as matrix language (the main language of an utterance) and English as embedded language. If three or more words of the embedde</context>
</contexts>
<marker>Schultz, Fung, Burgmer, 2010</marker>
<rawString>T. Schultz, P. Fung, and C. Burgmer, 2010 Detecting code-switch events based on textual features.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Shi</author>
<author>P Wiggers</author>
<author>M Jonker</author>
</authors>
<title>Towards Recurrent Neural Network Language Model with Linguistics and Contextual Features In:</title>
<date>2011</date>
<booktitle>Proceedings of Interspeech</booktitle>
<contexts>
<context position="4995" citStr="Shi et al., 2011" startWordPosition="759" endWordPosition="762">used for a variety of tasks, including language modeling (Mikolov et al., 2010). Recurrent neural networks are able to handle long-term contexts since the input vector does not only contain the current word but also the previous hidden layer. It is shown that these networks outperform traditional language models, such as n-grams which only contain very limited histories. In (Mikolov et al., 2011), the network is extended by factorizing the output layer into classes to accelerate the training and testing processes. The input layer can be augmented to model features, such as partof-speech tags (Shi et al., 2011; Adel, Vu et al., 2013). In (Adel, Vu et al., 2013), recurrent neural networks are applied to Code-Switching speech. It is shown that the integration of POS tags into the neural network, which predicts the next language as well as the next word, leads to significant perplexity reductions. A factored language model refers to a word as a vector of features, such as the word itself, morphological classes, POS tags or word stems. Hence, it provides another possibility to integrate syntactical features into the language modeling process. In (Bilmes and Kirchhoff, 2003), it is shown that factored l</context>
</contexts>
<marker>Shi, Wiggers, Jonker, 2011</marker>
<rawString>Y. Shi, P. Wiggers, M. Jonker 2011 Towards Recurrent Neural Network Language Model with Linguistics and Contextual Features In: Proceedings of Interspeech 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Solorio</author>
<author>Y Liu</author>
</authors>
<title>Part-of-speech tagging for English-Spanish code-switched text In:</title>
<date>2008</date>
<booktitle>Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,</booktitle>
<marker>Solorio, Liu, 2008</marker>
<rawString>T. Solorio, Y. Liu 2008 Part-of-speech tagging for English-Spanish code-switched text In: Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Solorio</author>
<author>Y Liu</author>
</authors>
<title>Learning to predict codeswitching points In:</title>
<date>2008</date>
<booktitle>Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,</booktitle>
<marker>Solorio, Liu, 2008</marker>
<rawString>T. Solorio, Y. Liu 2008 Learning to predict codeswitching points In: Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>C D Manning</author>
</authors>
<title>Enriching the knowledge sources used in a maximum entropy partof-speech tagger In:</title>
<date>2000</date>
<booktitle>Proceedings of the 2000 Joint SIGDAT conference on Empirical methods in natural language processing and very large corpora: held in conjunction with the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<volume>13</volume>
<marker>Toutanova, Manning, 2000</marker>
<rawString>K. Toutanova, C.D. Manning 2000 Enriching the knowledge sources used in a maximum entropy partof-speech tagger In: Proceedings of the 2000 Joint SIGDAT conference on Empirical methods in natural language processing and very large corpora: held in conjunction with the 38th Annual Meeting of the Association for Computational Linguistics, vol. 13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>D Klein</author>
<author>C D Manning</author>
<author>Y Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network In:</title>
<date>2003</date>
<booktitle>Proceedings of NAACL</booktitle>
<contexts>
<context position="13032" citStr="Toutanova et al., 2003" startWordPosition="2054" endWordPosition="2057">gaporean and Malaysian speakers, ratio of the four categories, and the duration in each set). Table 1 lists the statistics of the corpus in these sets. Train set Dev set Eval set # Speakers 139 8 8 Duration(hrs) 59.2 2.1 1.5 # Utterances 48,040 1,943 1,018 # Token 525,168 23,776 11,294 Table 1: Statistics of the SEAME corpus 4.2 POS Tagger for Code-Switching Speech To be able to assign part-of-speech tags to our bilingual text corpus, we apply the POS tagger described in (Schultz et al., 2010) and (Adel, Vu et al., 2013). It consists of two different monolingual (Stanford log-linear) taggers (Toutanova et al., 2003; Toutanova et al., 2000) and a combination of their results. While (Solorio et al., 2008b) passes the whole Code-Switching text to both monolingual taggers and combines their results using different heuristics, in this work, the text is splitted into different languages first. The tagging process is illustrated in figure 3. Mandarin is determined as matrix language (the main language of an utterance) and English as embedded language. If three or more words of the embedded language are detected, they are passed to the English tagger. The rest of the text is passed to the Mandarin tagger, even </context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>K. Toutanova, D. Klein, C.D. Manning, and Y. Singer 2003 Feature-rich part-of-speech tagging with a cyclic dependency network In: Proceedings of NAACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N T Vu</author>
<author>D C Lyu</author>
<author>J Weiner</author>
<author>D Telaar</author>
<author>T Schlippe</author>
<author>F Blaicher</author>
<author>E S Chng</author>
<author>T Schultz</author>
<author>H Li</author>
</authors>
<title>A First Speech Recognition System For Mandarin-English Code-Switch Conversational Speech In:</title>
<date>2012</date>
<booktitle>Proceedings of Interspeech</booktitle>
<marker>Vu, Lyu, Weiner, Telaar, Schlippe, Blaicher, Chng, Schultz, Li, 2012</marker>
<rawString>N.T. Vu, D.C. Lyu, J. Weiner, D. Telaar, T. Schlippe, F. Blaicher, E.S. Chng, T. Schultz, H. Li 2012 A First Speech Recognition System For Mandarin-English Code-Switch Conversational Speech In: Proceedings of Interspeech 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N T Vu</author>
<author>H Adel</author>
<author>T Schultz</author>
</authors>
<title>An Investigation of Code-Switching Attitude Dependent Language Modeling In:</title>
<date>2013</date>
<booktitle>In Statistical Language and Speech Processing, First International Conference,</booktitle>
<contexts>
<context position="5019" citStr="Vu et al., 2013" startWordPosition="764" endWordPosition="767">sks, including language modeling (Mikolov et al., 2010). Recurrent neural networks are able to handle long-term contexts since the input vector does not only contain the current word but also the previous hidden layer. It is shown that these networks outperform traditional language models, such as n-grams which only contain very limited histories. In (Mikolov et al., 2011), the network is extended by factorizing the output layer into classes to accelerate the training and testing processes. The input layer can be augmented to model features, such as partof-speech tags (Shi et al., 2011; Adel, Vu et al., 2013). In (Adel, Vu et al., 2013), recurrent neural networks are applied to Code-Switching speech. It is shown that the integration of POS tags into the neural network, which predicts the next language as well as the next word, leads to significant perplexity reductions. A factored language model refers to a word as a vector of features, such as the word itself, morphological classes, POS tags or word stems. Hence, it provides another possibility to integrate syntactical features into the language modeling process. In (Bilmes and Kirchhoff, 2003), it is shown that factored language models are able </context>
<context position="6704" citStr="Vu et al., 2013" startWordPosition="1029" endWordPosition="1032">otivation Since there is a lack of Code-Switching data, language modeling is a challenging task. Traditional n-gram approaches may not provide reliable estimates. Hence, more general features than words should be integrated into the language models. Therefore, we apply recurrent neural networks and factored language models. As features, we use part-of-speech tags and language identifiers. 3.2 Using Recurrent Neural Networks As Language Model This section describes the structure of the recurrent neural network (RNNLM) that we use as Code-Switching language model. It has been proposed in (Adel, Vu et al., 2013) and is illustrated in figure 1. Figure 1: RNNLM for Code-Switching (based upon a figure in (Mikolov et al., 2011)) Vector w(t), which represents the current word using 1-of-N coding, forms the input of the recurrent neural network. Thus, its dimension equals the size of the vocabulary. Vector s(t) contains the state of the network and is called ’hidden layer’. The network is trained using backpropagation through time (BPTT), an extension of the back-propagation algorithm for recurrent neural networks. With BPTT, the error is propagated through recurrent connections back in time for a specific</context>
<context position="12936" citStr="Vu et al., 2013" startWordPosition="2040" endWordPosition="2043">set) and assigned the data based on several criteria (gender, speaking style, ratio of Singaporean and Malaysian speakers, ratio of the four categories, and the duration in each set). Table 1 lists the statistics of the corpus in these sets. Train set Dev set Eval set # Speakers 139 8 8 Duration(hrs) 59.2 2.1 1.5 # Utterances 48,040 1,943 1,018 # Token 525,168 23,776 11,294 Table 1: Statistics of the SEAME corpus 4.2 POS Tagger for Code-Switching Speech To be able to assign part-of-speech tags to our bilingual text corpus, we apply the POS tagger described in (Schultz et al., 2010) and (Adel, Vu et al., 2013). It consists of two different monolingual (Stanford log-linear) taggers (Toutanova et al., 2003; Toutanova et al., 2000) and a combination of their results. While (Solorio et al., 2008b) passes the whole Code-Switching text to both monolingual taggers and combines their results using different heuristics, in this work, the text is splitted into different languages first. The tagging process is illustrated in figure 3. Mandarin is determined as matrix language (the main language of an utterance) and English as embedded language. If three or more words of the embedded language are detected, the</context>
</contexts>
<marker>Vu, Adel, Schultz, 2013</marker>
<rawString>N.T. Vu, H. Adel, T. Schultz 2013 An Investigation of Code-Switching Attitude Dependent Language Modeling In: In Statistical Language and Speech Processing, First International Conference, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Xue</author>
<author>F Xia</author>
<author>F D Chiou</author>
<author>M Palmer</author>
</authors>
<title>The penn chinese treebank: Phrase structure annotation of a large corpusk In:</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<volume>11</volume>
<pages>207</pages>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>N. Xue, F. Xia, F.D. Chiou, and M. Palmer 2005 The penn chinese treebank: Phrase structure annotation of a large corpusk In: Natural Language Engineering, vol. 11, no. 2, pp. 207.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>