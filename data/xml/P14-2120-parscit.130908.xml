<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005133">
<title confidence="0.998547">
Recognizing Implied Predicate-Argument Relationships
in Textual Inference
</title>
<author confidence="0.996526">
Asher Stern
</author>
<affiliation confidence="0.988689">
Computer Science Department
Bar-Ilan University
</affiliation>
<email confidence="0.995471">
astern7@cs.biu.ac.il
</email>
<sectionHeader confidence="0.993809" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999890571428571">
We investigate recognizing implied
predicate-argument relationships which
are not explicitly expressed in syntactic
structure. While prior works addressed
such relationships as an extension to se-
mantic role labeling, our work investigates
them in the context of textual inference
scenarios. Such scenarios provide prior
information, which substantially eases
the task. We provide a large and freely
available evaluation dataset for our task
setting, and propose methods to cope with
it, while obtaining promising results in
empirical evaluations.
</bodyText>
<sectionHeader confidence="0.845861" genericHeader="categories and subject descriptors">
1 Motivation and Task
</sectionHeader>
<bodyText confidence="0.999673565217392">
This paper addresses a typical sub-task in tex-
tual inference scenarios, of recognizing implied
predicate-argument relationships which are not
expressed explicitly through syntactic structure.
Consider the following example:
The crucial role Vioxx plays in Merck’s port-
folio was apparent last week when Merck’s
shares plunged 27 percent to 33 dollars after the
withdrawal announcement.
While a human reader understands that the
withdrawal refers to Vioxx, and hence an im-
plied predicate-argument relationship holds be-
tween them, this relationship is not expressed in
the syntactic structure, and will be missed by syn-
tactic parsers or standard semantic role labelers.
This paper targets such types of implied rela-
tionships in textual inference scenarios. Partic-
ularly, we investigate the setting of Recognizing
Textual Entailment (RTE) as a typical scenario of
textual inference. We suggest, however, that the
same challenge, as well as the solutions proposed
in our work, are applicable, with proper adap-
tations, to other textual-inference scenarios, like
</bodyText>
<author confidence="0.641161">
Ido Dagan
</author>
<affiliation confidence="0.839623">
Computer Science Department
Bar-Ilan University
</affiliation>
<email confidence="0.968112">
dagan@cs.biu.ac.il
</email>
<bodyText confidence="0.990544275">
Question Answering, and Information Extraction
(see Section 6).
An RTE problem instance is composed of two
text fragments, termed Text and Hypothesis, as in-
put. The task is to recognize whether a human
reading the Text would infer that the Hypothesis
is most likely true (Dagan et al., 2006). For our
problem, consider a positive Text Hypothesis pair,
where the Text is example (i) above and the Hy-
pothesis is:
(ii) Merck withdrew Vioxx.
A common approach for recognizing textual en-
tailment is to verify that all the textual elements
of the Hypothesis are covered, or aligned, by el-
ements of the Text. These elements typically in-
clude lexical terms as well as relationships be-
tween them. In our example, the Hypothesis lexi-
cal terms (“Merck”, “withdrew” and “Vioxx”) are
indeed covered by the Text. Yet, the predicate-
argument relationships (e.g., “withdrawal-Vioxx”)
are not expressed in the text explicitly. In such
a case, an RTE system has to verify that the
predicate-argument relationships which are ex-
plicitly expressed in the Hypothesis, are implied
from the Text discourse. Such cases are quite fre-
quent (∼17%) in the settings of our dataset, de-
scribed in Section 3.
Consequently, we define the task of recognizing
implied predicate-argument relationships, with il-
lustrating examples in Table 1, as follows. The
input includes a Text and a Hypothesis. Two terms
in the Hypothesis, predicate and argument, are
marked, where a predicate-argument relationship
between them is explicit in the Hypothesis syntac-
tic structure. Two terms in the Text, candidate-
predicate and candidate-argument, aligned to the
Hypothesis predicate and argument, are marked
as well. However, no predicate-argument rela-
tionship between them is expressed syntactically.
The task is to recognize whether the predicate-
</bodyText>
<equation confidence="0.473643">
(i)
</equation>
<page confidence="0.964493">
739
</page>
<note confidence="0.4453015">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 739–744,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<table confidence="0.999326217391305">
# Hypothesis Text Y/N
1 Merck [withdrew]pred [Vioxx]arg The crucial role [Vioxx]cand-arg plays in Merck’s Y
from the market. portfolio was apparent last week when Merck’s
shares plunged 27 percent to 33 dollars after the
[withdrawal]cand-pred announcement.
2 Barbara Cummings heard the tale Sheehan’s [protest]cand-arg is misguided and is hurting N
of a woman who was coming troop morale... .
to Crawford to [join]pred Cindy Sheehan never wanted Casey to [join]cand-pred the mil-
Sheehans [protest]arg. itary.
3 Casey Sheehan was [killed]pred in 5 days after he arrived in [Iraq]cand-arg last year, Casey Y
[Iraq]arg. Sheehan was [killed]cand-pred.
4 Hurricane Rita [threatened]pred Hurricane Rita was upgraded from a tropical storm as Y
[New Orleans]arg. it [threatened]cand-pred the southeastern United States,
forcing an alert in southern Florida and scuttling plans
to repopulate [New Orleans]cand-arg after Hurricane
Katrina turned it into a ghost city 3 weeks earlier.
5 Alberto Gonzales defends A senior official defended the [Patriot Act]cand-arg . . . Y
[renewal]pred of the [Patriot ... President Bush has urged Congress to
Act]arg to Congress. [renew]cand-pred the law ...
6 The [train]arg [crash]pred injured At least 10 people were killed ... in the [crash]cand-pred Y
nearly 200 people. ...
Alvarez is accused of ... causing the derailment of one
[train]cand-arg . . .
</table>
<tableCaption confidence="0.997987">
Table 1: Example task instances from our dataset. The last column specifies the Yes/No annotation,
</tableCaption>
<bodyText confidence="0.996684578947369">
indicating whether the sought predicate-argument relationship is implied in the Text. For illustration, a
dashed line indicates an explicit argument that is related to the candidate argument through some kind of
discourse reference. Pred, arg and cand abbreviate predicate, argument and candidate respectively.
argument relationship, as expressed in the Hypoth-
esis, holds implicitly also in the Text.
To address this task, we provide a large and
freely available annotated dataset, and propose
methods for coping with it. A related task, de-
scribed in the next section, deals with such implied
predicate-argument relationships as an extension
to Semantic Role Labeling. While the results re-
ported so far on that annotation task were rela-
tively low, we suggest that the task itself may be
more complicated than what is actually required
in textual inference scenarios. On the other hand,
the results obtained for our task, which does fit
textual inference scenarios, are promising, and en-
courage utilizing algorithms for this task in actual
inference systems.
</bodyText>
<sectionHeader confidence="0.99294" genericHeader="method">
2 Prior Work
</sectionHeader>
<bodyText confidence="0.9842134">
The most notable work targeting implied
predicate-argument relationships is the 2010
SemEval task of Linking Events and Their Par-
ticipants in Discourse (Ruppenhofer et al., 2009).
This task extends Semantic Role Labeling to cases
in which a core argument of a predicate is missing
in the syntactic structure but a filler for the
corresponding semantic role appears elsewhere
and can be inferred from discourse. For example,
in the following sentence the semantic role goal is
unfilled:
(iii) He arrived (0Goal) at 8pm.
Yet, we can expect to find an implied filler for
goal elsewhere in the document.
The SemEval task, termed henceforth as Im-
plied SRL, involves three major sub-tasks. First,
for each predicate, the unfilled roles, termed Null
Instantiations (NI), should be detected. Second,
each NI should be classified as Definite NI (DNI),
meaning that the role filler must exist in the dis-
course, or Indefinite NI otherwise. Third, the DNI
fillers should be found (DNI linking).
Later works that followed the SemEval chal-
lenge include (Silberer and Frank, 2012) and
(Roth and Frank, 2013), which proposed auto-
</bodyText>
<page confidence="0.970377">
740
</page>
<bodyText confidence="0.999953444444444">
matic dataset generation methods and features
which capture discourse phenomena. Their high-
est result was 12% F1-score. Another work is the
probabilistic model of Laparra and Rigau (2012),
which is trained by properties captured not only
from implicit arguments but also from explicit
ones, resulting in 19% F1-score. Another notable
work is (Gerber and Chai, 2012), which was lim-
ited to ten carefully selected nominal predicates.
</bodyText>
<subsectionHeader confidence="0.999049">
2.1 Annotations vs. Recognition
</subsectionHeader>
<bodyText confidence="0.999992181818182">
Comparing to the implied SRL task, our task may
better fit the needs of textual inference. First, some
relatively complex steps of the implied SRL task
are avoided in our setting, while on the other hand
it covers more relevant cases.
More concretely, in textual inference the can-
didate predicate and argument are typically iden-
tified, as they are aligned by the RTE system to
a predicate and an argument of the Hypothesis.
Thus, the only remaining challenge is to verify
that the sought relationship is implied in the text.
Therefore, the sub-tasks of identifying and classi-
fying DNIs can be avoided.
On the other hand, in some cases the candi-
date argument is not a DNI, but is still required
in textual inference. One type of such cases are
non-core arguments, which cannot be Definite NIs.
However, textual inference deals with non-core ar-
guments as well (see example 3 in Table 1).
Another case is when an implied predicate-
argument relationship holds even though the cor-
responding role is already filled by another argu-
ment, hence not an NI. Consider example 4 of Ta-
ble 1. While the object of “threatened” is filled (in
the Text) by “southeastern United States”, a hu-
man reader also infers the “threatened-New Or-
leans” relationship. Such cases might follow a
meronymy relation between the filler (“southeast-
ern United States”) and the candidate argument
(“New Orleans”), or certain types of discourse (co-
)references (e.g., example 5 in Table 1), or some
other linguistic phenomena. Either way, they are
crucial for textual inference, while not being NIs.
</bodyText>
<sectionHeader confidence="0.999476" genericHeader="method">
3 Dataset
</sectionHeader>
<bodyText confidence="0.999988358974359">
This section describes a semi-automatic method
for extracting candidate instances of implied
predicate-argument relationship from an RTE
dataset. This extraction process directly follows
our task formalization. Given a Text Hypothe-
sis pair, we locate a predicate-argument relation-
ship in the Hypothesis, where both the predicate
and the argument appear also in the Text, while
the relationship between them is not expressed in
its syntactic structure. This process is performed
automatically, based on syntactic parsing (see be-
low). Then, a human reader annotates each in-
stance as “Yes” – meaning that the implied rela-
tionship indeed holds in the Text, or “No” other-
wise. Example instances, constructed by this pro-
cess, are shown in Table 1.
In this work we used lemma-level lexical
matching, as well as nominalization matching, to
align the Text predicates and arguments to the Hy-
pothesis. We note that more advanced match-
ing, e.g., by utilizing knowledge resources (like
WordNet), can be performed as well. To identify
explicit predicate-argument relationships we uti-
lized dependency parsing by the Easy-First parser
(Goldberg and Elhadad, 2010). Nominalization
matching (e.g., example 1 of Table 1) was per-
formed with Nomlex (Macleod et al., 1998).
By applying this method on the RTE-6 dataset
(Bentivogli et al., 2010), we constructed a
dataset of 4022 instances, where 2271 (56%)
are annotated as positive instances, and 1751
as negative ones. This dataset is significantly
larger than prior datasets for the implied SRL
task. To calculate inter-annotator agreement, the
first author also annotated 185 randomly-selected
instances. We have reached high agreement score
of 0.80 Kappa. The dataset is freely available at
www.cs.biu.ac.il/˜nlp/resources/
downloads/implied-relationships.
</bodyText>
<sectionHeader confidence="0.993601" genericHeader="method">
4 Recognition Algorithm
</sectionHeader>
<bodyText confidence="0.999997">
We defined 15 features, summarized in Table 2,
which capture local and discourse phenomena.
These features do not depend on manually built
resources, and hence are portable to resource-poor
languages. Some features were proposed in prior
works, and are marked by G&amp;C (Gerber and Chai,
2012) or S&amp;F (Silberer and Frank, 2012). Our best
results were obtained with the Random Forests
learning algorithm (Breiman, 2001). The first two
features are described in the next subsection, while
the others are explained in the table itself.
</bodyText>
<subsectionHeader confidence="0.997393">
4.1 Statistical discourse features
</subsectionHeader>
<bodyText confidence="0.999782">
Statistical features in prior works mostly cap-
ture general properties of the predicate and the
</bodyText>
<page confidence="0.994595">
741
</page>
<table confidence="0.9008101">
# Category Feature Prev. work
1 statistical co-occurring predicate (explained in subsection 4.1) New
discourse
2 co-occurring argument (explained in subsection 4.1) New
3 local co-reference: whether an explicit argument of p co-refers with a. New
discourse
4 last known location: If the NE of a is “location”, and it is the last New
location mentioned before p in the document.
5 argument prominence: The frequency of the lemma of a in a two- S&amp;F
sentence windows of p, relative to all entities in that window.
6 predicate frequency in document: The frequency of p in the docu- G&amp;C
ment, relative to all predicates appear in the document.
7 local statistical argument frequency: The Unigram-model likelihood of a New
candidate in English documents, calculated from a large corpus.
properties
8 definite NP: Whether a is a definite NP G&amp;C
9 indefinite NP: Whether a is an indefinite NP G&amp;C
10 quantified predicate: Whether p is quantified (i.e., by expressions G&amp;C
like “every ... ”, “a good deal of ... ”, etc.)
11 NE mismatch: Whether a is a named entity but the corresponding New
argument in the hypothesis is not, or vice versa.
12 predicate- predicate-argument frequency: The likelihood of a to be an argu- similar feature
argument ment of p (formally: Pr(a|p)) in a large corpus. in G&amp;C
relatedness
14
15
13 sentence distance: The distance between p and a in sentences. G&amp;C, S&amp;F
mention distance: The distance between p and a in entity-mentions. S&amp;F
shared head-predicate: Whether p and a are themselves arguments G&amp;C
of another predicate.
</table>
<tableCaption confidence="0.999718">
Table 2: Algorithmic features. p and a denote the candidate predicate and argument respectively.
</tableCaption>
<bodyText confidence="0.999928605263158">
argument, like selectional preferences, lexical
similarities, etc. On the contrary, our statis-
tical features follow the intuition that explicit
predicate-argument relationships in the discourse
provide plausible indication that an implied
relationship holds as well. In our experiments
we collected the statistics from Reuters corpus
RCV1 (trec.nist.gov/data/reuters/
reuters.html), which contains more than
806,000 documents.
We defined two features: Co-occurring predi-
cate and Co-occurring argument. Let p and a be
the candidate predicate and the argument in the
text. While they are not connected syntactically,
each of them often has an explicit relationships
with other terms in the text, that might support the
sought (implied) relationship between a and p.
More concretely, a is often an explicit argument
of another predicate p&apos;. For example, example 6 in
Table 1 includes the explicit relationship “derail-
ment of train”, which might indicate the implied
relationship “crash of train”. Hence p=“crash”,
a=“train” and p&apos;=“derailment”. The Co-occurring
predicate feature estimates the probability that a
document would contain a as an argument of p,
given that a appears elsewhere in that document
as an argument of p&apos;, based on explicit predicate-
argument relationships in a large corpus.
Similarly, the Co-occurring argument feature
captures cases where p has another explicit argu-
ment, a&apos;. This is exemplified in example 5 of
Table 1, where p=“renew”, a=“Patriot Act” and
a&apos;=“law”. Accordingly, the feature quantifies the
probability that a document including the relation-
ship p-a&apos; would also include the relationship p-a.
More details about these features can be found
in the first author’s Ph.D. thesis at www.cs.biu.
ac.il/˜nlp/publications/theses/
</bodyText>
<sectionHeader confidence="0.999909" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.9997995">
We tested our method in a cross-validation setting,
and obtained high result as shown in the first row
of Table 3. Since our task and dataset are novel,
there is no direct baseline with which we can com-
pare this result. As a reference point we mention
the majority class proportion, and also report a
configuration in which only features adopted from
prior works (G&amp;C and S&amp;F) are utilized. This
</bodyText>
<page confidence="0.990507">
742
</page>
<table confidence="0.999906">
Configuration Accuracy % A %
Full algorithm 81.0 –
Union of prior work 78.0 3.0
Major category (all true) 56.5 24.5
Ablation tests
no statistical discourse 79.9 1.1
no local discourse 79.3 1.7
no local candidate properties 79.2 1.8
no predicate-argument relatedness 79.7 1.3
</table>
<tableCaption confidence="0.9982925">
Table 3: Accuracy of our method, followed by
baselines and ablation tests.
</tableCaption>
<table confidence="0.99996325">
Configuration (input) Recall Precision F1 %
Explicit only 44.6 44.3 44.4
Human annotations 50.9 43.4 46.8
Algorithm recognition 48.5 42.3 45.2
</table>
<tableCaption confidence="0.999338">
Table 4: RTE-6 Experiment
</tableCaption>
<bodyText confidence="0.999849709677419">
comparison shows that the contribution of our new
features (3%) is meaningful, which is also statis-
tically significant with p &lt; 0.01 using Bootstrap
Resampling test (Koehn, 2004). The high results
show that this task is feasible, and its solutions
can be adopted as a component in textual infer-
ence systems. The positive contribution of each
feature category is shown in ablation tests.
An additional experiment tests the contribution
of recognizing implied predicate-argument rela-
tionships for overall RTE, specifically on the RTE-
6 dataset. For the scope of this experiment we de-
veloped a simple RTE system, which uses the F1
optimized logistic regression classifier of Jansche
(2005) with two features: lexical coverage and
predicate-argument relationships coverage. We
ran three configurations for the second feature,
where in the first only syntactically expressed re-
lationships are used, in the second all the implied
relationships, as detected by a human annotator,
are added, and in the third only the implied rela-
tionships detected by our algorithm are added.
The results, presented in Table 4, first demon-
strate the full potential of the implied relation-
ship recognition task to improve textual entail-
ment recognition (Human annotation vs. Explicit
only). One third of this potential improvement is
achieved by our algorithm1. Note that all these re-
sults are higher than the median result in the RTE-
6 challenge (36.14%). While the delta in the F1
score is small in absolute terms, such magnitudes
</bodyText>
<footnote confidence="0.955104">
1Following the relatively modest size of the RTE dataset,
the Algorithm vs. Explicit result is not statistically significant
(p ^_ 0.1). However, the Human annotation vs. Explicit
result is statistically significant with p &lt; 0.01.
</footnote>
<note confidence="0.7415985">
are typical in RTE for most resources and tools
(see (Bentivogli et al., 2010)).
</note>
<sectionHeader confidence="0.983247" genericHeader="conclusions">
6 Discussion and Conclusions
</sectionHeader>
<bodyText confidence="0.999853186046512">
We formulated the task of recognizing implied
predicate-argument relationships within textual in-
ference scenarios. We compared this task to the
labeling task of SemEval 2010, where no prior in-
formation about candidate arguments in the text is
available. We point out that in textual inference
scenarios the candidate predicate and argument
are given by the Hypothesis, while the challenge
is only to verify that a predicate-argument rela-
tionship between these candidates is implied from
the given Text. Accordingly, some complex steps
necessitated in the SemEval task can be avoided,
while additional relevant cases are covered.
Moreover, we have shown that this simpler task
is more feasibly solvable, where our 15 features
achieved more than 80% accuracy.
While our dataset and algorithm were presented
in the context of RTE, the same challenge and
methods are applicable to other textual inference
tasks as well. Consider, for example, the Ques-
tion Answering (QA) task. Typically QA sys-
tems detect a candidate predicate that matches the
question’s predicate. Similarly, candidate argu-
ments, which match either the expected answer
type or other arguments in the question are de-
tected too. Consequently, our methods which ex-
ploit the availability of the candidate predicate and
argument can be adapted to this scenario as well.
Similarly, a typical approach for Event Extrac-
tion (a sub task of Information Extraction) is to
start by applying an entity extractor, which identi-
fies argument candidates. Accordingly, candidate
predicate and arguments are detected in this sce-
nario too, while the remaining challenge is to as-
sess the likelihood that a predicate-argument rela-
tionship holds between them.
Following this observation, we propose future
work of applying our methods to other tasks. An
additional direction for future work is to further
develop new methods for our task, possibly by
incorporating SRL resources and/or linguistically
oriented rules, in order to improve the results we
achieved so far.
</bodyText>
<sectionHeader confidence="0.998297" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9982285">
This work was partially supported by the EC-
funded project EXCITEMENT (FP7ICT-287923).
</bodyText>
<page confidence="0.994677">
743
</page>
<note confidence="0.995755">
Josef Ruppenhofer, Caroline Sporleder, Roser
Morante, Collin Baker, and Martha Palmer. 2009.
Semeval-2010 task 10: Linking events and their
participants in discourse. In The NAACL-HLT
2009 Workshop on Semantic Evaluations: Recent
Achievements and Future Directions (SEW-09).
</note>
<sectionHeader confidence="0.594916" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994433428571429">
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2010. The sixth
pascal recognizing textual entailment challenge. In
Proccidings of TAC.
Leo Breiman. 2001. Random forests. Machine Learn-
ing, 45(1).
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment
challenge. Machine Learning Challenges. Evaluat-
ing Predictive Uncertainty, Visual Object Classifi-
cation, and Recognising Tectual Entailment, pages
177–190.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies rep-
resentation. In proceedings of COLING 2008 Work-
shop on Cross-framework and Cross-domain Parser
Evaluation.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of ACL.
Matthew Gerber and Joyce Y. Chai. 2012. Seman-
tic role labeling of implicit arguments for nominal
predicates. Computational Linguistics.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Proceedings of NAACL.
Aria Haghighi and Dan Klein. 2009. Simple coref-
erence resolution with rich syntactic and semantic
features. In Proceedings of EMNLP.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update.
SIGKDD Explorations, 11(1).
Martin Jansche. 2005. Maximum expected f-measure
training of logistic regression models. In Proceed-
ings of EMNLP.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP.
Egoitz Laparra and German Rigau. 2012. Exploiting
explicit annotations and semantic types for implicit
argument resolution. In Proceedings of IEEE-ICSC.
Catherine Macleod, Ralph Grishman, Adam Meyers,
Leslie Barrett, and Ruth Reeves. 1998. Nomlex: A
lexicon of nominalizations. In Proceedings of EU-
RALEX.
Michael Roth and Anette Frank. 2013. Automatically
identifying implicit arguments to improve argument
linking and coherence modeling. In Proceedings of
*SEM.
Josef Ruppenhofer, Caroline Sporleder, Roser
Morante, Collin Baker, and Martha Palmer. 2010.
Semeval-2010 task 10: Linking events and their
participants in discourse. In Proceedings of the 5th
International Workshop on Semantic Evaluation.
Carina Silberer and Anette Frank. 2012. Casting im-
plicit role linking as an anaphora resolution task. In
Proceedings of *SEM.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of NAACL.
</reference>
<page confidence="0.998305">
744
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.996491">Recognizing Implied Predicate-Argument in Textual Inference</title>
<author confidence="0.994335">Asher</author>
<affiliation confidence="0.995795">Computer Science</affiliation>
<author confidence="0.468476">Bar-Ilan</author>
<email confidence="0.875815">astern7@cs.biu.ac.il</email>
<abstract confidence="0.962992178571428">We investigate recognizing implied predicate-argument relationships which are not explicitly expressed in syntactic structure. While prior works addressed such relationships as an extension to semantic role labeling, our work investigates them in the context of textual inference scenarios. Such scenarios provide prior information, which substantially eases the task. We provide a large and freely available evaluation dataset for our task setting, and propose methods to cope with it, while obtaining promising results in empirical evaluations. 1 Motivation and Task This paper addresses a typical sub-task in textual inference scenarios, of recognizing implied predicate-argument relationships which are not expressed explicitly through syntactic structure. Consider the following example: crucial role Vioxxplays in Merck’s portfolio was apparent last week when Merck’s shares plunged 27 percent to 33 dollars after the withdrawal announcement. While a human reader understands that the withdrawal refers to Vioxx, and hence an implied predicate-argument relationship holds between them, this relationship is not expressed in the syntactic structure, and will be missed by syntactic parsers or standard semantic role labelers. This paper targets such types of implied relationships in textual inference scenarios. Particwe investigate the setting of Entailment (RTE) a typical scenario of textual inference. We suggest, however, that the same challenge, as well as the solutions proposed in our work, are applicable, with proper adaptations, to other textual-inference scenarios, like Ido Computer Science Bar-Ilan dagan@cs.biu.ac.il and Extraction (see Section 6). An RTE problem instance is composed of two fragments, termed as input. The task is to recognize whether a human reading the Text would infer that the Hypothesis is most likely true (Dagan et al., 2006). For our problem, consider a positive Text Hypothesis pair, where the Text is example (i) above and the Hypothesis is: withdrew Vioxx. A common approach for recognizing textual entailment is to verify that all the textual elements the Hypothesis are or by elements of the Text. These elements typically include lexical terms as well as relationships between them. In our example, the Hypothesis lexical terms (“Merck”, “withdrew” and “Vioxx”) are indeed covered by the Text. Yet, the predicateargument relationships (e.g., “withdrawal-Vioxx”) are not expressed in the text explicitly. In such a case, an RTE system has to verify that the predicate-argument relationships which are exexpressed in the Hypothesis, are from the Text discourse. Such cases are quite frein the settings of our dataset, described in Section 3. Consequently, we define the task of recognizing implied predicate-argument relationships, with illustrating examples in Table 1, as follows. The includes a a Two terms the Hypothesis, are marked, where a predicate-argument relationship between them is explicit in the Hypothesis syntacstructure. Two terms in the Text, candidatealigned to the Hypothesis predicate and argument, are marked as well. However, no predicate-argument relationship between them is expressed syntactically. task is to recognize whether the predicate- (i)</abstract>
<note confidence="0.81342125">739 of the 52nd Annual Meeting of the Association for Computational Linguistics (Short pages 739–744, Maryland, USA, June 23-25 2014. Association for Computational Linguistics 1 from the market. crucial role plays in Merck’s portfolio was apparent last week when Merck’s shares plunged 27 percent to 33 dollars after the Y</note>
<abstract confidence="0.963461908227848">2 Barbara Cummings heard the tale of a woman who was coming Crawford to Cindy is misguided and is hurting troop morale... . N never wanted Casey to the mil-itary. 3 Sheehan was in days after he arrived in last year, Casey was Y 4 Rita Hurricane Rita was upgraded from a tropical storm as the southeastern United States, forcing an alert in southern Florida and scuttling plans repopulate [New after Hurricane Katrina turned it into a ghost city 3 weeks earlier. Y 5 Alberto Gonzales defends senior official defended the [Patriot . . . Y the ... President Bush has urged Congress Congress. law ... 6 injured nearly 200 people. least 10 people were killed ... in the ... Y Alvarez is accused of ... causing the derailment of one . . Table 1: Example task instances from our dataset. The last column specifies the Yes/No annotation, indicating whether the sought predicate-argument relationship is implied in the Text. For illustration, a dashed line indicates an explicit argument that is related to the candidate argument through some kind of discourse reference. Pred, arg and cand abbreviate predicate, argument and candidate respectively. argument relationship, as expressed in the Hypothesis, holds implicitly also in the Text. To address this task, we provide a large and freely available annotated dataset, and propose methods for coping with it. A related task, described in the next section, deals with such implied predicate-argument relationships as an extension Role While the results reported so far on that annotation task were relatively low, we suggest that the task itself may be more complicated than what is actually required in textual inference scenarios. On the other hand, the results obtained for our task, which does fit textual inference scenarios, are promising, and encourage utilizing algorithms for this task in actual inference systems. 2 Prior Work The most notable work targeting implied predicate-argument relationships is the 2010 task of Events and Their Parin Discourse et al., 2009). task extends Role Labeling cases in which a core argument of a predicate is missing in the syntactic structure but a filler for the corresponding semantic role appears elsewhere and can be inferred from discourse. For example, the following sentence the semantic role unfilled: arrivedat 8pm. Yet, we can expect to find an implied filler for in the document. SemEval task, termed henceforth as Iminvolves three major sub-tasks. First, each predicate, the unfilled roles, termed should be detected. Second, NI should be classified as NI meaning that the role filler must exist in the disor NI Third, the DNI fillers should be found (DNI linking). Later works that followed the SemEval challenge include (Silberer and Frank, 2012) and and Frank, 2013), which proposed auto- 740 matic dataset generation methods and features which capture discourse phenomena. Their highest result was 12% F1-score. Another work is the probabilistic model of Laparra and Rigau (2012), which is trained by properties captured not only from implicit arguments but also from explicit ones, resulting in 19% F1-score. Another notable work is (Gerber and Chai, 2012), which was limited to ten carefully selected nominal predicates. 2.1 Annotations vs. Recognition Comparing to the implied SRL task, our task may better fit the needs of textual inference. First, some relatively complex steps of the implied SRL task are avoided in our setting, while on the other hand it covers more relevant cases. More concretely, in textual inference the candidate predicate and argument are typically identified, as they are aligned by the RTE system to a predicate and an argument of the Hypothesis. Thus, the only remaining challenge is to verify that the sought relationship is implied in the text. Therefore, the sub-tasks of identifying and classifying DNIs can be avoided. On the other hand, in some cases the candidate argument is not a DNI, but is still required in textual inference. One type of such cases are arguments, which cannot be However, textual inference deals with non-core arguments as well (see example 3 in Table 1). Another case is when an implied predicateargument relationship holds even though the corresponding role is already filled by another argument, hence not an NI. Consider example 4 of Table 1. While the object of “threatened” is filled (in the Text) by “southeastern United States”, a human reader also infers the “threatened-New Orleans” relationship. Such cases might follow a meronymy relation between the filler (“southeastern United States”) and the candidate argument (“New Orleans”), or certain types of discourse (co- )references (e.g., example 5 in Table 1), or some other linguistic phenomena. Either way, they are crucial for textual inference, while not being NIs. 3 Dataset This section describes a semi-automatic method for extracting candidate instances of implied predicate-argument relationship from an RTE dataset. This extraction process directly follows task formalization. Given a Text Hypothesis pair, we locate a predicate-argument relationship in the Hypothesis, where both the predicate and the argument appear also in the Text, while the relationship between them is not expressed in its syntactic structure. This process is performed automatically, based on syntactic parsing (see below). Then, a human reader annotates each instance as “Yes” – meaning that the implied relationship indeed holds in the Text, or “No” otherwise. Example instances, constructed by this process, are shown in Table 1. In this work we used lemma-level lexical matching, as well as nominalization matching, to align the Text predicates and arguments to the Hypothesis. We note that more advanced matching, e.g., by utilizing knowledge resources (like WordNet), can be performed as well. To identify relationships we utilized dependency parsing by the Easy-First parser (Goldberg and Elhadad, 2010). Nominalization matching (e.g., example 1 of Table 1) was performed with Nomlex (Macleod et al., 1998). By applying this method on the RTE-6 dataset (Bentivogli et al., 2010), we constructed a dataset of 4022 instances, where 2271 (56%) are annotated as positive instances, and 1751 as negative ones. This dataset is significantly larger than prior datasets for the implied SRL task. To calculate inter-annotator agreement, the first author also annotated 185 randomly-selected instances. We have reached high agreement score of 0.80 Kappa. The dataset is freely available at www.cs.biu.ac.il/˜nlp/resources/ 4 Recognition Algorithm We defined 15 features, summarized in Table 2, which capture local and discourse phenomena. These features do not depend on manually built resources, and hence are portable to resource-poor languages. Some features were proposed in prior works, and are marked by G&amp;C (Gerber and Chai, 2012) or S&amp;F (Silberer and Frank, 2012). Our best were obtained with the Forests learning algorithm (Breiman, 2001). The first two features are described in the next subsection, while the others are explained in the table itself. 4.1 Statistical discourse features Statistical features in prior works mostly capture general properties of the predicate and the 741 1 discourse co-occurring predicate (explained in subsection 4.1) New 2 co-occurring argument (explained in subsection 4.1) New 3 local discourse whether an explicit argument of with New 4 known location: If the NE of “location”, and it is the last mentioned before the document. New 5 prominence: The frequency of the lemma of a twowindows of relative to all entities in that window. S&amp;F 6 frequency in document: The frequency of the document, relative to all predicates appear in the document. G&amp;C 7 local candidate properties argument frequency: The Unigram-model likelihood of in English documents, calculated from a large corpus. New 8 NP: Whether a definite NP G&amp;C 9 NP: Whether an indefinite NP G&amp;C 10 predicate: Whether quantified (i.e., by expressions like “every ... ”, “a good deal of ... ”, etc.) G&amp;C 11 mismatch: Whether a named entity but the corresponding argument in the hypothesis is not, or vice versa. New 12 predicate-argument relatedness frequency: The likelihood of be an arguof in a large corpus. similar feature in G&amp;C 14 15 13 distance: The distance between sentences. G&amp;C, S&amp;F distance: The distance between entity-mentions. S&amp;F head-predicate: Whether themselves arguments of another predicate. G&amp;C 2: Algorithmic features. the candidate predicate and argument respectively. argument, like selectional preferences, lexical similarities, etc. On the contrary, our statisfeatures follow the intuition that predicate-argument relationships in the discourse plausible indication that an relationship holds as well. In our experiments we collected the statistics from Reuters corpus which contains more than 806,000 documents. defined two features: predi- Let the candidate predicate and the argument in the text. While they are not connected syntactically, each of them often has an explicit relationships in the text, that might support the (implied) relationship between concretely, often an another predicate For example, example 6 in Table 1 includes the explicit relationship “derailment of train”, which might indicate the implied “crash of train”. Hence and The estimates the probability that a would contain an argument of that elsewhere in that document an argument of based on explicit predicateargument relationships in a large corpus. the argument cases where another argu- This is exemplified in example 5 of 1, where Act” and Accordingly, the feature quantifies the probability that a document including the relationwould also include the relationship More details about these features can be found the first author’s Ph.D. thesis at ac.il/˜nlp/publications/theses/ 5 Results We tested our method in a cross-validation setting, and obtained high result as shown in the first row of Table 3. Since our task and dataset are novel, there is no direct baseline with which we can compare this result. As a reference point we mention the majority class proportion, and also report a configuration in which only features adopted from prior works (G&amp;C and S&amp;F) are utilized. This 742 Configuration Accuracy % Full algorithm 81.0 – Union of prior work 78.0 3.0 Major category (all true) 56.5 24.5 Ablation tests no statistical discourse 79.9 1.1 no local discourse 79.3 1.7 no local candidate properties 79.2 1.8 no predicate-argument relatedness 79.7 1.3 Table 3: Accuracy of our method, followed by baselines and ablation tests. Configuration (input) Recall Precision F1 % Explicit only 44.6 44.3 44.4 Human annotations 50.9 43.4 46.8 Algorithm recognition 48.5 42.3 45.2 Table 4: RTE-6 Experiment comparison shows that the contribution of our new features (3%) is meaningful, which is also statissignificant with &lt; (Koehn, 2004). The high results show that this task is feasible, and its solutions can be adopted as a component in textual inference systems. The positive contribution of each feature category is shown in ablation tests. An additional experiment tests the contribution of recognizing implied predicate-argument relationships for overall RTE, specifically on the RTE- 6 dataset. For the scope of this experiment we developed a simple RTE system, which uses the F1 optimized logistic regression classifier of Jansche (2005) with two features: lexical coverage and predicate-argument relationships coverage. We ran three configurations for the second feature, where in the first only syntactically expressed relationships are used, in the second all the implied relationships, as detected by a human annotator, are added, and in the third only the implied relationships detected by our algorithm are added. The results, presented in Table 4, first demonstrate the full potential of the implied relationship recognition task to improve textual entailment recognition (Human annotation vs. Explicit only). One third of this potential improvement is by our Note that all these results are higher than the median result in the RTE- 6 challenge (36.14%). While the delta in the F1 score is small in absolute terms, such magnitudes the relatively modest size of the RTE dataset, the Algorithm vs. Explicit result is not statistically significant However, the Human annotation vs. Explicit is statistically significant with &lt; are typical in RTE for most resources and tools (see (Bentivogli et al., 2010)). 6 Discussion and Conclusions We formulated the task of recognizing implied predicate-argument relationships within textual inference scenarios. We compared this task to the labeling task of SemEval 2010, where no prior information about candidate arguments in the text is available. We point out that in textual inference scenarios the candidate predicate and argument are given by the Hypothesis, while the challenge is only to verify that a predicate-argument relationship between these candidates is implied from the given Text. Accordingly, some complex steps necessitated in the SemEval task can be avoided, while additional relevant cases are covered. Moreover, we have shown that this simpler task is more feasibly solvable, where our 15 features achieved more than 80% accuracy. While our dataset and algorithm were presented in the context of RTE, the same challenge and methods are applicable to other textual inference as well. Consider, for example, the Ques- Answering (QA) Typically QA systems detect a candidate predicate that matches the question’s predicate. Similarly, candidate arguments, which match either the expected answer type or other arguments in the question are detected too. Consequently, our methods which exploit the availability of the candidate predicate and argument can be adapted to this scenario as well. a typical approach for Extracsub task of is to start by applying an entity extractor, which identifies argument candidates. Accordingly, candidate predicate and arguments are detected in this scenario too, while the remaining challenge is to assess the likelihood that a predicate-argument relationship holds between them. Following this observation, we propose future work of applying our methods to other tasks. An additional direction for future work is to further develop new methods for our task, possibly by incorporating SRL resources and/or linguistically oriented rules, in order to improve the results we achieved so far.</abstract>
<note confidence="0.873103888888889">Acknowledgments This work was partially supported by the ECfunded project EXCITEMENT (FP7ICT-287923). 743 Josef Ruppenhofer, Caroline Sporleder, Roser Morante, Collin Baker, and Martha Palmer. 2009. Semeval-2010 task 10: Linking events and their in discourse. In NAACL-HLT 2009 Workshop on Semantic Evaluations: Recent</note>
<title confidence="0.990462">and Future Directions References</title>
<author confidence="0.98498">Luisa Bentivogli</author>
<author confidence="0.98498">Peter Clark</author>
<author confidence="0.98498">Ido Dagan</author>
<author confidence="0.98498">Hoa Trang</author>
<abstract confidence="0.858912282051282">Dang, and Danilo Giampiccolo. 2010. The sixth pascal recognizing textual entailment challenge. In of Breiman. 2001. Random forests. Learn- 45(1). Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The pascal recognising textual entailment Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classifiand Recognising Tectual pages 177–190. Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The stanford typed dependencies rep- In of COLING 2008 Workshop on Cross-framework and Cross-domain Parser Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs In of Matthew Gerber and Joyce Y. Chai. 2012. Semantic role labeling of implicit arguments for nominal Yoav Goldberg and Michael Elhadad. 2010. An efficient algorithm for easy-first non-directional depenparsing. In of Aria Haghighi and Dan Klein. 2009. Simple coreference resolution with rich syntactic and semantic In of Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The weka data mining software: An update. 11(1). Martin Jansche. 2005. Maximum expected f-measure of logistic regression models. In Proceedof Philipp Koehn. 2004. Statistical significance tests for translation evaluation. In of Egoitz Laparra and German Rigau. 2012. Exploiting explicit annotations and semantic types for implicit resolution. In of</abstract>
<author confidence="0.6600785">A Nomlex</author>
<affiliation confidence="0.524453">of nominalizations. In of EU-</affiliation>
<author confidence="0.574443">Automatically</author>
<abstract confidence="0.448063923076923">identifying implicit arguments to improve argument and coherence modeling. In of Josef Ruppenhofer, Caroline Sporleder, Roser Morante, Collin Baker, and Martha Palmer. 2010. Semeval-2010 task 10: Linking events and their in discourse. In of the 5th Workshop on Semantic Carina Silberer and Anette Frank. 2012. Casting implicit role linking as an anaphora resolution task. In of Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network.</abstract>
<note confidence="0.6262165">of 744</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Luisa Bentivogli</author>
<author>Peter Clark</author>
<author>Ido Dagan</author>
<author>Hoa Trang Dang</author>
<author>Danilo Giampiccolo</author>
</authors>
<title>The sixth pascal recognizing textual entailment challenge.</title>
<date>2010</date>
<booktitle>In Proccidings of TAC.</booktitle>
<contexts>
<context position="10903" citStr="Bentivogli et al., 2010" startWordPosition="1680" endWordPosition="1683">cted by this process, are shown in Table 1. In this work we used lemma-level lexical matching, as well as nominalization matching, to align the Text predicates and arguments to the Hypothesis. We note that more advanced matching, e.g., by utilizing knowledge resources (like WordNet), can be performed as well. To identify explicit predicate-argument relationships we utilized dependency parsing by the Easy-First parser (Goldberg and Elhadad, 2010). Nominalization matching (e.g., example 1 of Table 1) was performed with Nomlex (Macleod et al., 1998). By applying this method on the RTE-6 dataset (Bentivogli et al., 2010), we constructed a dataset of 4022 instances, where 2271 (56%) are annotated as positive instances, and 1751 as negative ones. This dataset is significantly larger than prior datasets for the implied SRL task. To calculate inter-annotator agreement, the first author also annotated 185 randomly-selected instances. We have reached high agreement score of 0.80 Kappa. The dataset is freely available at www.cs.biu.ac.il/˜nlp/resources/ downloads/implied-relationships. 4 Recognition Algorithm We defined 15 features, summarized in Table 2, which capture local and discourse phenomena. These features d</context>
<context position="18205" citStr="Bentivogli et al., 2010" startWordPosition="2821" endWordPosition="2824">rove textual entailment recognition (Human annotation vs. Explicit only). One third of this potential improvement is achieved by our algorithm1. Note that all these results are higher than the median result in the RTE6 challenge (36.14%). While the delta in the F1 score is small in absolute terms, such magnitudes 1Following the relatively modest size of the RTE dataset, the Algorithm vs. Explicit result is not statistically significant (p ^_ 0.1). However, the Human annotation vs. Explicit result is statistically significant with p &lt; 0.01. are typical in RTE for most resources and tools (see (Bentivogli et al., 2010)). 6 Discussion and Conclusions We formulated the task of recognizing implied predicate-argument relationships within textual inference scenarios. We compared this task to the labeling task of SemEval 2010, where no prior information about candidate arguments in the text is available. We point out that in textual inference scenarios the candidate predicate and argument are given by the Hypothesis, while the challenge is only to verify that a predicate-argument relationship between these candidates is implied from the given Text. Accordingly, some complex steps necessitated in the SemEval task </context>
</contexts>
<marker>Bentivogli, Clark, Dagan, Dang, Giampiccolo, 2010</marker>
<rawString>Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang Dang, and Danilo Giampiccolo. 2010. The sixth pascal recognizing textual entailment challenge. In Proccidings of TAC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
</authors>
<title>Random forests.</title>
<date>2001</date>
<booktitle>Machine Learning,</booktitle>
<volume>45</volume>
<issue>1</issue>
<contexts>
<context position="11810" citStr="Breiman, 2001" startWordPosition="1812" endWordPosition="1813">y-selected instances. We have reached high agreement score of 0.80 Kappa. The dataset is freely available at www.cs.biu.ac.il/˜nlp/resources/ downloads/implied-relationships. 4 Recognition Algorithm We defined 15 features, summarized in Table 2, which capture local and discourse phenomena. These features do not depend on manually built resources, and hence are portable to resource-poor languages. Some features were proposed in prior works, and are marked by G&amp;C (Gerber and Chai, 2012) or S&amp;F (Silberer and Frank, 2012). Our best results were obtained with the Random Forests learning algorithm (Breiman, 2001). The first two features are described in the next subsection, while the others are explained in the table itself. 4.1 Statistical discourse features Statistical features in prior works mostly capture general properties of the predicate and the 741 # Category Feature Prev. work 1 statistical co-occurring predicate (explained in subsection 4.1) New discourse 2 co-occurring argument (explained in subsection 4.1) New 3 local co-reference: whether an explicit argument of p co-refers with a. New discourse 4 last known location: If the NE of a is “location”, and it is the last New location mentioned</context>
</contexts>
<marker>Breiman, 2001</marker>
<rawString>Leo Breiman. 2001. Random forests. Machine Learning, 45(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The pascal recognising textual entailment challenge. Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment,</title>
<date>2006</date>
<pages>177--190</pages>
<contexts>
<context position="2157" citStr="Dagan et al., 2006" startWordPosition="305" endWordPosition="308">zing Textual Entailment (RTE) as a typical scenario of textual inference. We suggest, however, that the same challenge, as well as the solutions proposed in our work, are applicable, with proper adaptations, to other textual-inference scenarios, like Ido Dagan Computer Science Department Bar-Ilan University dagan@cs.biu.ac.il Question Answering, and Information Extraction (see Section 6). An RTE problem instance is composed of two text fragments, termed Text and Hypothesis, as input. The task is to recognize whether a human reading the Text would infer that the Hypothesis is most likely true (Dagan et al., 2006). For our problem, consider a positive Text Hypothesis pair, where the Text is example (i) above and the Hypothesis is: (ii) Merck withdrew Vioxx. A common approach for recognizing textual entailment is to verify that all the textual elements of the Hypothesis are covered, or aligned, by elements of the Text. These elements typically include lexical terms as well as relationships between them. In our example, the Hypothesis lexical terms (“Merck”, “withdrew” and “Vioxx”) are indeed covered by the Text. Yet, the predicateargument relationships (e.g., “withdrawal-Vioxx”) are not expressed in the</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The pascal recognising textual entailment challenge. Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment, pages 177–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>The stanford typed dependencies representation.</title>
<date>2008</date>
<booktitle>In proceedings of COLING 2008 Workshop on Cross-framework and Cross-domain Parser Evaluation.</booktitle>
<marker>de Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The stanford typed dependencies representation. In proceedings of COLING 2008 Workshop on Cross-framework and Cross-domain Parser Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Gerber</author>
<author>Joyce Y Chai</author>
</authors>
<title>Semantic role labeling of implicit arguments for nominal predicates. Computational Linguistics.</title>
<date>2012</date>
<contexts>
<context position="7914" citStr="Gerber and Chai, 2012" startWordPosition="1197" endWordPosition="1200">role filler must exist in the discourse, or Indefinite NI otherwise. Third, the DNI fillers should be found (DNI linking). Later works that followed the SemEval challenge include (Silberer and Frank, 2012) and (Roth and Frank, 2013), which proposed auto740 matic dataset generation methods and features which capture discourse phenomena. Their highest result was 12% F1-score. Another work is the probabilistic model of Laparra and Rigau (2012), which is trained by properties captured not only from implicit arguments but also from explicit ones, resulting in 19% F1-score. Another notable work is (Gerber and Chai, 2012), which was limited to ten carefully selected nominal predicates. 2.1 Annotations vs. Recognition Comparing to the implied SRL task, our task may better fit the needs of textual inference. First, some relatively complex steps of the implied SRL task are avoided in our setting, while on the other hand it covers more relevant cases. More concretely, in textual inference the candidate predicate and argument are typically identified, as they are aligned by the RTE system to a predicate and an argument of the Hypothesis. Thus, the only remaining challenge is to verify that the sought relationship i</context>
<context position="11685" citStr="Gerber and Chai, 2012" startWordPosition="1791" endWordPosition="1794">ger than prior datasets for the implied SRL task. To calculate inter-annotator agreement, the first author also annotated 185 randomly-selected instances. We have reached high agreement score of 0.80 Kappa. The dataset is freely available at www.cs.biu.ac.il/˜nlp/resources/ downloads/implied-relationships. 4 Recognition Algorithm We defined 15 features, summarized in Table 2, which capture local and discourse phenomena. These features do not depend on manually built resources, and hence are portable to resource-poor languages. Some features were proposed in prior works, and are marked by G&amp;C (Gerber and Chai, 2012) or S&amp;F (Silberer and Frank, 2012). Our best results were obtained with the Random Forests learning algorithm (Breiman, 2001). The first two features are described in the next subsection, while the others are explained in the table itself. 4.1 Statistical discourse features Statistical features in prior works mostly capture general properties of the predicate and the 741 # Category Feature Prev. work 1 statistical co-occurring predicate (explained in subsection 4.1) New discourse 2 co-occurring argument (explained in subsection 4.1) New 3 local co-reference: whether an explicit argument of p c</context>
</contexts>
<marker>Gerber, Chai, 2012</marker>
<rawString>Matthew Gerber and Joyce Y. Chai. 2012. Semantic role labeling of implicit arguments for nominal predicates. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Michael Elhadad</author>
</authors>
<title>An efficient algorithm for easy-first non-directional dependency parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="10728" citStr="Goldberg and Elhadad, 2010" startWordPosition="1651" endWordPosition="1654">(see below). Then, a human reader annotates each instance as “Yes” – meaning that the implied relationship indeed holds in the Text, or “No” otherwise. Example instances, constructed by this process, are shown in Table 1. In this work we used lemma-level lexical matching, as well as nominalization matching, to align the Text predicates and arguments to the Hypothesis. We note that more advanced matching, e.g., by utilizing knowledge resources (like WordNet), can be performed as well. To identify explicit predicate-argument relationships we utilized dependency parsing by the Easy-First parser (Goldberg and Elhadad, 2010). Nominalization matching (e.g., example 1 of Table 1) was performed with Nomlex (Macleod et al., 1998). By applying this method on the RTE-6 dataset (Bentivogli et al., 2010), we constructed a dataset of 4022 instances, where 2271 (56%) are annotated as positive instances, and 1751 as negative ones. This dataset is significantly larger than prior datasets for the implied SRL task. To calculate inter-annotator agreement, the first author also annotated 185 randomly-selected instances. We have reached high agreement score of 0.80 Kappa. The dataset is freely available at www.cs.biu.ac.il/˜nlp/r</context>
</contexts>
<marker>Goldberg, Elhadad, 2010</marker>
<rawString>Yoav Goldberg and Michael Elhadad. 2010. An efficient algorithm for easy-first non-directional dependency parsing. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Simple coreference resolution with rich syntactic and semantic features.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Haghighi, Klein, 2009</marker>
<rawString>Aria Haghighi and Dan Klein. 2009. Simple coreference resolution with rich syntactic and semantic features. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The weka data mining software: An update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<volume>11</volume>
<issue>1</issue>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The weka data mining software: An update. SIGKDD Explorations, 11(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Jansche</author>
</authors>
<title>Maximum expected f-measure training of logistic regression models.</title>
<date>2005</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="17075" citStr="Jansche (2005)" startWordPosition="2643" endWordPosition="2644">ingful, which is also statistically significant with p &lt; 0.01 using Bootstrap Resampling test (Koehn, 2004). The high results show that this task is feasible, and its solutions can be adopted as a component in textual inference systems. The positive contribution of each feature category is shown in ablation tests. An additional experiment tests the contribution of recognizing implied predicate-argument relationships for overall RTE, specifically on the RTE6 dataset. For the scope of this experiment we developed a simple RTE system, which uses the F1 optimized logistic regression classifier of Jansche (2005) with two features: lexical coverage and predicate-argument relationships coverage. We ran three configurations for the second feature, where in the first only syntactically expressed relationships are used, in the second all the implied relationships, as detected by a human annotator, are added, and in the third only the implied relationships detected by our algorithm are added. The results, presented in Table 4, first demonstrate the full potential of the implied relationship recognition task to improve textual entailment recognition (Human annotation vs. Explicit only). One third of this po</context>
</contexts>
<marker>Jansche, 2005</marker>
<rawString>Martin Jansche. 2005. Maximum expected f-measure training of logistic regression models. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="16568" citStr="Koehn, 2004" startWordPosition="2563" endWordPosition="2564"> category (all true) 56.5 24.5 Ablation tests no statistical discourse 79.9 1.1 no local discourse 79.3 1.7 no local candidate properties 79.2 1.8 no predicate-argument relatedness 79.7 1.3 Table 3: Accuracy of our method, followed by baselines and ablation tests. Configuration (input) Recall Precision F1 % Explicit only 44.6 44.3 44.4 Human annotations 50.9 43.4 46.8 Algorithm recognition 48.5 42.3 45.2 Table 4: RTE-6 Experiment comparison shows that the contribution of our new features (3%) is meaningful, which is also statistically significant with p &lt; 0.01 using Bootstrap Resampling test (Koehn, 2004). The high results show that this task is feasible, and its solutions can be adopted as a component in textual inference systems. The positive contribution of each feature category is shown in ablation tests. An additional experiment tests the contribution of recognizing implied predicate-argument relationships for overall RTE, specifically on the RTE6 dataset. For the scope of this experiment we developed a simple RTE system, which uses the F1 optimized logistic regression classifier of Jansche (2005) with two features: lexical coverage and predicate-argument relationships coverage. We ran th</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Egoitz Laparra</author>
<author>German Rigau</author>
</authors>
<title>Exploiting explicit annotations and semantic types for implicit argument resolution.</title>
<date>2012</date>
<booktitle>In Proceedings of IEEE-ICSC.</booktitle>
<contexts>
<context position="7736" citStr="Laparra and Rigau (2012)" startWordPosition="1169" endWordPosition="1172">ks. First, for each predicate, the unfilled roles, termed Null Instantiations (NI), should be detected. Second, each NI should be classified as Definite NI (DNI), meaning that the role filler must exist in the discourse, or Indefinite NI otherwise. Third, the DNI fillers should be found (DNI linking). Later works that followed the SemEval challenge include (Silberer and Frank, 2012) and (Roth and Frank, 2013), which proposed auto740 matic dataset generation methods and features which capture discourse phenomena. Their highest result was 12% F1-score. Another work is the probabilistic model of Laparra and Rigau (2012), which is trained by properties captured not only from implicit arguments but also from explicit ones, resulting in 19% F1-score. Another notable work is (Gerber and Chai, 2012), which was limited to ten carefully selected nominal predicates. 2.1 Annotations vs. Recognition Comparing to the implied SRL task, our task may better fit the needs of textual inference. First, some relatively complex steps of the implied SRL task are avoided in our setting, while on the other hand it covers more relevant cases. More concretely, in textual inference the candidate predicate and argument are typically </context>
</contexts>
<marker>Laparra, Rigau, 2012</marker>
<rawString>Egoitz Laparra and German Rigau. 2012. Exploiting explicit annotations and semantic types for implicit argument resolution. In Proceedings of IEEE-ICSC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catherine Macleod</author>
<author>Ralph Grishman</author>
<author>Adam Meyers</author>
<author>Leslie Barrett</author>
<author>Ruth Reeves</author>
</authors>
<title>Nomlex: A lexicon of nominalizations.</title>
<date>1998</date>
<booktitle>In Proceedings of EURALEX.</booktitle>
<contexts>
<context position="10831" citStr="Macleod et al., 1998" startWordPosition="1668" endWordPosition="1671">deed holds in the Text, or “No” otherwise. Example instances, constructed by this process, are shown in Table 1. In this work we used lemma-level lexical matching, as well as nominalization matching, to align the Text predicates and arguments to the Hypothesis. We note that more advanced matching, e.g., by utilizing knowledge resources (like WordNet), can be performed as well. To identify explicit predicate-argument relationships we utilized dependency parsing by the Easy-First parser (Goldberg and Elhadad, 2010). Nominalization matching (e.g., example 1 of Table 1) was performed with Nomlex (Macleod et al., 1998). By applying this method on the RTE-6 dataset (Bentivogli et al., 2010), we constructed a dataset of 4022 instances, where 2271 (56%) are annotated as positive instances, and 1751 as negative ones. This dataset is significantly larger than prior datasets for the implied SRL task. To calculate inter-annotator agreement, the first author also annotated 185 randomly-selected instances. We have reached high agreement score of 0.80 Kappa. The dataset is freely available at www.cs.biu.ac.il/˜nlp/resources/ downloads/implied-relationships. 4 Recognition Algorithm We defined 15 features, summarized i</context>
</contexts>
<marker>Macleod, Grishman, Meyers, Barrett, Reeves, 1998</marker>
<rawString>Catherine Macleod, Ralph Grishman, Adam Meyers, Leslie Barrett, and Ruth Reeves. 1998. Nomlex: A lexicon of nominalizations. In Proceedings of EURALEX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Roth</author>
<author>Anette Frank</author>
</authors>
<title>Automatically identifying implicit arguments to improve argument linking and coherence modeling.</title>
<date>2013</date>
<booktitle>In Proceedings of *SEM.</booktitle>
<contexts>
<context position="7524" citStr="Roth and Frank, 2013" startWordPosition="1137" endWordPosition="1140">l is unfilled: (iii) He arrived (0Goal) at 8pm. Yet, we can expect to find an implied filler for goal elsewhere in the document. The SemEval task, termed henceforth as Implied SRL, involves three major sub-tasks. First, for each predicate, the unfilled roles, termed Null Instantiations (NI), should be detected. Second, each NI should be classified as Definite NI (DNI), meaning that the role filler must exist in the discourse, or Indefinite NI otherwise. Third, the DNI fillers should be found (DNI linking). Later works that followed the SemEval challenge include (Silberer and Frank, 2012) and (Roth and Frank, 2013), which proposed auto740 matic dataset generation methods and features which capture discourse phenomena. Their highest result was 12% F1-score. Another work is the probabilistic model of Laparra and Rigau (2012), which is trained by properties captured not only from implicit arguments but also from explicit ones, resulting in 19% F1-score. Another notable work is (Gerber and Chai, 2012), which was limited to ten carefully selected nominal predicates. 2.1 Annotations vs. Recognition Comparing to the implied SRL task, our task may better fit the needs of textual inference. First, some relativel</context>
</contexts>
<marker>Roth, Frank, 2013</marker>
<rawString>Michael Roth and Anette Frank. 2013. Automatically identifying implicit arguments to improve argument linking and coherence modeling. In Proceedings of *SEM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Ruppenhofer</author>
<author>Caroline Sporleder</author>
<author>Roser Morante</author>
<author>Collin Baker</author>
<author>Martha Palmer</author>
</authors>
<title>Semeval-2010 task 10: Linking events and their participants in discourse.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation.</booktitle>
<marker>Ruppenhofer, Sporleder, Morante, Baker, Palmer, 2010</marker>
<rawString>Josef Ruppenhofer, Caroline Sporleder, Roser Morante, Collin Baker, and Martha Palmer. 2010. Semeval-2010 task 10: Linking events and their participants in discourse. In Proceedings of the 5th International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carina Silberer</author>
<author>Anette Frank</author>
</authors>
<title>Casting implicit role linking as an anaphora resolution task.</title>
<date>2012</date>
<booktitle>In Proceedings of *SEM.</booktitle>
<contexts>
<context position="7497" citStr="Silberer and Frank, 2012" startWordPosition="1132" endWordPosition="1135"> sentence the semantic role goal is unfilled: (iii) He arrived (0Goal) at 8pm. Yet, we can expect to find an implied filler for goal elsewhere in the document. The SemEval task, termed henceforth as Implied SRL, involves three major sub-tasks. First, for each predicate, the unfilled roles, termed Null Instantiations (NI), should be detected. Second, each NI should be classified as Definite NI (DNI), meaning that the role filler must exist in the discourse, or Indefinite NI otherwise. Third, the DNI fillers should be found (DNI linking). Later works that followed the SemEval challenge include (Silberer and Frank, 2012) and (Roth and Frank, 2013), which proposed auto740 matic dataset generation methods and features which capture discourse phenomena. Their highest result was 12% F1-score. Another work is the probabilistic model of Laparra and Rigau (2012), which is trained by properties captured not only from implicit arguments but also from explicit ones, resulting in 19% F1-score. Another notable work is (Gerber and Chai, 2012), which was limited to ten carefully selected nominal predicates. 2.1 Annotations vs. Recognition Comparing to the implied SRL task, our task may better fit the needs of textual infer</context>
<context position="11719" citStr="Silberer and Frank, 2012" startWordPosition="1797" endWordPosition="1800"> implied SRL task. To calculate inter-annotator agreement, the first author also annotated 185 randomly-selected instances. We have reached high agreement score of 0.80 Kappa. The dataset is freely available at www.cs.biu.ac.il/˜nlp/resources/ downloads/implied-relationships. 4 Recognition Algorithm We defined 15 features, summarized in Table 2, which capture local and discourse phenomena. These features do not depend on manually built resources, and hence are portable to resource-poor languages. Some features were proposed in prior works, and are marked by G&amp;C (Gerber and Chai, 2012) or S&amp;F (Silberer and Frank, 2012). Our best results were obtained with the Random Forests learning algorithm (Breiman, 2001). The first two features are described in the next subsection, while the others are explained in the table itself. 4.1 Statistical discourse features Statistical features in prior works mostly capture general properties of the predicate and the 741 # Category Feature Prev. work 1 statistical co-occurring predicate (explained in subsection 4.1) New discourse 2 co-occurring argument (explained in subsection 4.1) New 3 local co-reference: whether an explicit argument of p co-refers with a. New discourse 4 l</context>
</contexts>
<marker>Silberer, Frank, 2012</marker>
<rawString>Carina Silberer and Anette Frank. 2012. Casting implicit role linking as an anaphora resolution task. In Proceedings of *SEM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In Proceedings of NAACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>