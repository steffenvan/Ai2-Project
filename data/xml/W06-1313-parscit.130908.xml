<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000010">
<title confidence="0.992237">
An Information State-Based Dialogue Manager for Call for Fire Dialogues
</title>
<author confidence="0.965386">
Antonio Roque and David Traum
</author>
<affiliation confidence="0.877024">
USC Institute for Creative Technologies
13274 Fiji Way, Marina Del Rey, CA 90292
</affiliation>
<email confidence="0.998684">
roque@ict.usc.edu, traum@ict.usc.edu
</email>
<sectionHeader confidence="0.9939" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999973333333333">
We present a dialogue manager for “Call
for Fire” training dialogues. We describe
the training environment, the domain, the
features of its novel information state-
based dialogue manager, the system it is a
part of, and preliminary evaluation results.
</bodyText>
<sectionHeader confidence="0.994318" genericHeader="keywords">
1 Overview
</sectionHeader>
<bodyText confidence="0.999704242424243">
Dialogue systems are built for many different pur-
poses, including information gathering (e.g., (Aust
et al., 1995)), performing simple transactions (e.g,
(Walker and Hirschman, 2000)), collaborative in-
teraction (e.g., (Allen et al., 1996)), tutoring (e.g.,
(Rose et al., 2003)), and training (e.g. (Traum
and Rickel, 2002)). Aspects of the purpose, as
well as features of the domain itself (e.g., train
timetables, air flight bookings, schedule mainte-
nance, physics, and platoon-level military opera-
tions) will have a profound effect on the nature of
the dialogue which a system will need to engage
in. Issues such as initiative, error correction, flex-
ibility in phrasing and dialogue structure may de-
pend crucially on these factors.
The information state approach to dialogue
managers (Larsson and Traum, 2000) has been an
attempt to cast some of these differences within
the same framework. In this approach, a theory
of dialogue is constructed by providing informa-
tion structure elements, a set of dialogue moves
that can be recognized and produced and are used
to modify the nature of these elements, a set of
update rules that govern the dynamics of how the
information is changed as dialogue moves are per-
formed, and an update strategy. Many differ-
ent dialogue systems have been built according to
this general approach (e.g., (Cooper and Larsson,
1999; Matheson et al., 2000; Lemon et al., 2001;
Johnston et al., 2002; Traum and Rickel, 2002;
Purver, 2002)).
In this paper, we present an information-state
based dialogue manager for a new domain: train-
ing call for fire dialogues. Like other dialogue sys-
tems used as role-players in training applications,
the structure of the dialogue is not completely free
for a dialogue designer to specify based on issues
of dialogue efficiency. The dialogue system must
conform as much as possible to the type of dia-
logue that a trainee would actually encounter in the
types of interaction he or she is being trained for.
In particular, for military radio dialogues, much
of the protocol for interaction is specified by con-
vention (e.g., (Army, 2001)). Still, there is a fair
amount of flexibility in how other aspects of the
dialogue progress.
This dialogue manager is part of a system we
call Radiobot-CFF. Radiobots are a general class
of dialogue systems meant to speak over the ra-
dio in military simulations. Our most extended
effort to date is the Radiobot-CFF system, which
engages in “call for fire” dialogues to train ar-
tillery observers within a virtual reality training
simulation. Our dialogue system can operate ac-
cording to three different use cases, depending on
how much control a human operator/trainer would
like to exercise over the dialogue. There is a fully
automatic mode in which the Radiobot-CFF sys-
tem engages unassisted in dialogue with the user, a
semi-automatic mode in which the Radiobot-CFF
system fills in forms (which can be edited) and the
operator can approve or change communication
with a simulator or trainee, and a passive mode
in which the operator is engaging in the dialogue
and the Radiobot-CFF system is just observing.
In section 2, we describe the training applica-
</bodyText>
<page confidence="0.986933">
88
</page>
<note confidence="0.5646125">
Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 88–95,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999874272727273">
tion that our dialogue system has been embedded
in as well as the system itself. In section 3, we de-
scribe some aspects of “call for fire dialogues”, es-
pecially the differences in initiative and purposes
of different phases in the dialogue. In section 4,
we describe the information-state based dialogue
model we have developed for this domain. This in-
cludes dialogue moves, information components,
and update rules. We describe some error handling
capabilities in section 5, and evaluation results in
section 6.
</bodyText>
<sectionHeader confidence="0.992895" genericHeader="introduction">
2 Testbed
</sectionHeader>
<bodyText confidence="0.977163081081081">
Our current testbed, Radiobot-CFF, has been
developed in a military training environment,
JFETS-UTM, at the U.S. Army base in in Ft. Sill,
Oklahoma. JFETS-UTM trains soldiers to make
Calls for Fire (CFFs), in which a Forward Ob-
server (FO) team locates an enemy target and re-
quests an artillery fire mission by radio from a Fire
Direction Center (FDC). The training room resem-
bles a battle-scarred apartment in a Middle East-
ern country. A window shows a virtual city dis-
played by a rear-projected computer screen, and
the soldiers use binoculars with computer displays
at their ends to search for targets.
Ordinarily, two trainers control a UTM session.
One communicates with the FO via a simulated
radio, and the other decides what the artillery fire
should be and inputs it to a GUI for the simu-
lator. It is our goal to replace those two train-
ers with one trainer focusing on assessment while
Radiobot-CFF handles the radio communications
and interfaces with the virtual world.
Radiobot-CFF is composed of several pipelined
components. A Speech Recognition component
is implemented using the SONIC speech recogni-
tion system (Pellom, 2001) with custom language
and acoustic models. An Interpreter component
tags the ASR output with its its dialogue move
and parameter labels using two separate Condi-
tional Random Field (Sha and Pereira, 2003; Mc-
Callum, 2002) taggers trained on hand-annotated
utterances. A Dialogue Manager processes the
tagged output, sending a reply to the FO (via a
template-based Generator) and, when necessary, a
message to the artillery simulator FireSim XXI1 to
make decisions on what type of fire to send. The
reply to FO and messages to simulator are medi-
ated by GUIs where the trainer can intervene if
</bodyText>
<footnote confidence="0.963779">
1http://sill-www.army.mil/blab/sims/FireSimXXI.htm
</footnote>
<bodyText confidence="0.40459">
need be.
</bodyText>
<sectionHeader confidence="0.988983" genericHeader="method">
3 Call for Fire Dialogues
</sectionHeader>
<bodyText confidence="0.998593333333333">
Call for Fire procedures are specified in an Army
field manual (Army, 2001) with variations based
on a unit’s standard operating procedure. Mes-
sages are brief and followed by confirmations,
where any misunderstandings are immediately
corrected. A typical CFF is shown in Figure 1.
</bodyText>
<figure confidence="0.88362848275862">
1 FO steel one niner this is gator niner one adjust
fire polar over
2 FDC gator nine one this is steel one nine adjust fire
polar out
3 FO direction five niner four zero distance four
eight zero over
4 FDC direction five nine four zero distance four eight
zero out
5 FO one b m p in the open i c m in effect over
6 FDC one b m p in the open i c m in effect out
7 FDC message to observer kilo alpha high explo-
sive four rounds adjust fire target number al-
pha bravo one zero zero zero over
8 FO m t o kilo alpha four rounds target number al-
pha bravo one out
9 FDC shot over
10 FO shot out
11 FDC splash over
12 FO splash out
13 FO right five zero fire for effect out over
14 FDC right five zero fire for effect out
15 FDC shot over
16 FO shot out
17 FDC rounds complete over
18 FO rounds complete out
19 FO end of mission one b m p suppressed zero ca-
sualties over
20 FDC end of mission one b m p suppressed zero ca-
sualties out
</figure>
<figureCaption confidence="0.999993">
Figure 1: Example Dialogue with Radiobot-CFF
</figureCaption>
<bodyText confidence="0.998229555555556">
CFFs can generally be divided into three
phases. In the first phase (utterances 1-6 in Fig-
ure 1) the FOs identify themselves and important
information about the CFF, including their coor-
dinates, the kind of fire they are requesting, the
location of the target, and the kind of target. In
utterance 1 in Figure 1 the FO performs an identi-
fication, giving his own call sign and that of the
FDC he is calling, and also specifies a method
of fire (“adjust fire”) and a method of targeting
(“polar”.) Note that when speakers expect a reply,
they end their utterance with “over” as in utter-
ance 1, otherwise with “out” as in the confirmation
in utterance 2. In utterance 3 the FO gives target
coordinates, and in utterance 5 the FO identifies
the target as a BMP (a type of light tank) and re-
quests ICM rounds (“improved conventional mu-
nitions”.) These turns typically follow one another
</bodyText>
<page confidence="0.998836">
89
</page>
<bodyText confidence="0.999871486486486">
in quick sequence.
In the second phase of a CFF, (utterances 7-12
in Figure 1), after the FDC decides what kind of
fire they will send, they inform the FO in a mes-
sage to observer (MTO) as in utterance 7. This
includes the units that will fire (“kilo alpha”), the
kind of ammunition (“high explosive”), the num-
ber of rounds and method of fire (“4 rounds ad-
just fire”), and the target number (“alpha bravo one
zero zero zero”). CFFs are requests rather than or-
ders, and they may be denied in full or in part. In
this example, the FO’s request for ICM rounds was
denied in favor of High Explosive rounds. Next
the FDC informs the FO when the fire mission has
been shot, as in utterance 9, and when the fire is
about to land, as in utterance 11. Each of these are
confirmed by the FO.
In the third phase, (utterances 13-20 in Fig-
ure 1) the FO regains dialogue initiative. Depend-
ing on the observed results, the FO may request
that the fire be repeated with an adjust in location
or method of fire. In utterance 13 the FO requests
that the shot be re-sent to a location 50 meters to
the right of the previous shot as a “fire for effect”
all-out bombardment rather than an “adjust fire”
targeting fire. This is followed by the abbreviated
FDC-initiated phase of utterances 15-18. In utter-
ance 19 the FO ends the mission, describing the
results and number of casualties.
Besides the behavior shown, at any turn either
participant may request or initiate an intelligence
report or request the status of a mission. Further-
more, after receiving an MTO the FO may imme-
diately begin another fire mission and thus have
multiple missions active; subsequent adjusts are
disambiguated with the target numbers assigned
during the MTOs.
</bodyText>
<sectionHeader confidence="0.983646" genericHeader="method">
4 Dialogue Manager
</sectionHeader>
<bodyText confidence="0.999931857142857">
We have constructed an Information State-based
dialogue manager (Larsson and Traum, 2000) on
this domain consisting of a set of dialogue moves,
a set of informational components with appropri-
ate formal representations, and a set of update
rules with an update strategy. We describe each
of these in turn.
</bodyText>
<subsectionHeader confidence="0.98687">
4.1 Dialogue Moves
</subsectionHeader>
<bodyText confidence="0.998532777777778">
We defined a set of dialogue moves to represent
the incoming FO utterances based on a study of
transcripts of human-controlled JFETS-UTM ses-
sions, Army manuals, and the needs of the simu-
lator. As shown in Figure 2 these are divided into
three groups: those that provide information about
the FO or the fire mission, those that confirm in-
formation that the FDC has transmitted, and those
that make requests.
</bodyText>
<figure confidence="0.993497136363636">
Mission Information:
Observer Coordinates
Situation Report
Identification
Warning Order
Method of Control
Method of Engagement
Target Location
Target Description
End of Mission
Confirming Information:
Message to Observer
Shot
Splash
Rounds Complete
Intel Report
Other Requests:
Radio Check
Say Again
Status
Standby
Command
</figure>
<figureCaption confidence="0.999698">
Figure 2: FO Dialogue Moves
</figureCaption>
<bodyText confidence="0.999487541666667">
The dialogue moves that provide information
include those in which the FOs transmit their Ob-
server Coordinates (grid location on a map), a
generic Situation Report, or one of the various
components of a fire mission request ranging from
call sign Identification to final End of Mission.
The dialogue moves that confirm information in-
clude those that confirm the MTO and other FDC-
initiated utterances, or a general report on scenario
Intel. The final group includes requests to check
radio functionality, to repeat the previous utter-
ance, for status of a shot, to stand by for transmis-
sion of information, and finally a set of commands
such as “check fire” requesting cancellation of a
submitted fire mission.
Each of these dialogue moves contains informa-
tion important to the dialogue manager. This in-
formation is captured by the parameters of the di-
alogue move, which are enumerated in Figure 3.
Each parameter is listed with the dialogue move
it usually occurs with, but this assignment is not
strict. For example, “number of enemies” param-
eters occur in Target Description as well as End of
Mission dialogue moves.
</bodyText>
<page confidence="0.966166">
90
</page>
<figure confidence="0.985387037037037">
Identification-related:
fdc—id
fo—id
Warning Order-related:
method—of—fire
method—of—control
method—of—engagement
method—of—location
Target Location-related:
grid—location
direction
distance
attitude
left—right
left—right—adjust
add—drop
add—drop—adjust
known—point
End Of Mission-related:
target—type
target—description
number—of—enemies
disposition
Other:
command
detail—of—request
target—number
</figure>
<figureCaption confidence="0.9833816">
Figure 3: Dialogue Move Parameters
Figure 4 shows how the dialogue moves and pa-
rameters act to identify the components of an FO
utterance. The example is based on utterance 1 in
Figure 1; the Identification move has two param-
</figureCaption>
<bodyText confidence="0.935621357142857">
eters representing the call signs of the FDC and
the FO, and the Warning Order has two parame-
ters representing the method of fire and method of
location. Parameters need to be identified to con-
firm back to the FO, and in some cases to be sent
to the simulator and for use in updating the infor-
mation state. In the example in Figure 4, the fact
that the requested method of fire is an “adjust fire”
will be sent to the simulator, and the fact that a
method of fire has been given will be updated in
the information state.
Identification: steel one nine this is gator niner one
fdc id: steel one nine
fo id: gator niner one
</bodyText>
<listItem confidence="0.648987">
Warning Order: adjust fire polar
method of fire: adjust fire
method of location: polar
</listItem>
<figureCaption confidence="0.9538065">
Figure 4: Example Dialogue Moves and Parame-
ters
</figureCaption>
<subsectionHeader confidence="0.897567">
4.2 Informational Components
</subsectionHeader>
<bodyText confidence="0.999876311111111">
The Radiobot-CFF dialogue manager’s informa-
tion state consists of five classes of informational
components, defined by their role in the dia-
logue and their level of accessibility to the user.
These are the Fire Mission Decision components,
the Fire Mission Value components, the Post-Fire
Value components, the Disambiguation compo-
nents, and the Update Rule Processing compo-
nents.
By dividing the components into multiple
classes we separate those that are simulator-
specific from more general aspects of the domain.
Decisions to fire are based on general con-
straints of the domain, whereas the exact com-
ponents to include in a message to simulator will
be simulator-specific. Also, the components have
been designed such that there is almost no over-
lap in the update rules that modify them (see sec-
tion 4.3). This reduces the complexity involved
in editing or adding rules; although there are over
100 rules in the information state, there are few
unanticipated side-effects when rules are altered.
The first class of components are the Fire Mis-
sion Decision components, which are used to de-
termine whether enough information has been col-
lected to send fire. These components are boolean
flags, updated by rules based on incoming dia-
logue moves and parameters. Figure 5 shows the
values of these components after utterance 3 in
Figure 1 has been processed. The FO has given a
warning order, and a target location (which can ei-
ther be given through a grid location, or through a
combination of direction and distance values, and
observer coordinates), so the appropriate compo-
nents are “true”. After the FO gives a target de-
scription, that component will be true as well, and
an update rule will recognize that enough informa-
tion has been gathered to send a fire mission.
has warning order? true
has target location? true
has grid location? false
has polar direction? true
has polar distance? true
has polar obco? true
has target descr? false
</bodyText>
<figureCaption confidence="0.954237">
Figure 5: Fire Mission Decision Components
</figureCaption>
<bodyText confidence="0.998776">
The second class of information state compo-
nents is the set of Fire Mission Value components,
which track the value of various information el-
</bodyText>
<page confidence="0.997548">
91
</page>
<bodyText confidence="0.999970166666667">
ements necessary for requesting a fire mission.
These are specific to the FireSim XXI simulator.
Figure 6 shows the values after utterance 3 in Fig-
ure 1. Components such as “direction value” take
number values, and components such as “method
of fire” take values from a finite set of possibilities.
Several of these components, such as “attitude”
have defaults that are rarely changed. Once the
dialogue manager or human trainer decides that it
has enough information to request fire, these com-
ponents are translated into a simulator command
and sent to the simulator.
</bodyText>
<figure confidence="0.465436272727273">
method of control: adjust fire
method of fire: adjust fire
method of engagement: none given
target type: -
grid value: -
direction value: 5940
distance value: 480
length: 0
width: 100
attitude: 0
observer coordinate value: 45603595
</figure>
<figureCaption confidence="0.975992923076923">
Figure 7: GUI
Figure 6: Fire Mission Value Components
Fire Mission Value components are also directly
modifiable by the trainer. Figure 7 shows the GUI
which the trainer can use to take control of the
session, edit any of the Fire Mission Value com-
ponents, and relinquish control of the session back
to Radiobot-CFF. This allows the trainer to correct
any mistakes that the Radiobot may have made or
test the trainee’s adaptability by sending the fire
to an unexpected location. The example shown in
Figure 7 is after utterance 5 of Figure 1; the sys-
tem is running in semi-automated mode and the
</figureCaption>
<bodyText confidence="0.991761571428571">
dialogue manager has decided that it has enough
information to send a fire. The trainer may send
the message or edit it and then send it. A second
GUI, not shown, allows the trainer to take con-
trol of the outgoing speech of the Radiobot, and,
in semi-automated mode, either confirm the send-
ing of a suggested output utterance, alter it before
sending, or author new text for the radiobot to say.
The third class of components is the Post-Fire
Value components, which are also exposed to the
trainer for modification. The example shown in
Figure 8 is from after utterance 13 in Figure 1; the
FO has requested an “adjust fire” with an indica-
tor of “fire for effect” and a right adjustment of 50.
At this point in the dialogue the FO could have in-
stead chosen to end the mission. If the initial fire
had been a “fire for effect” it could have been re-
peated, rather than following up an initial “adjust
fire.” The adjust fire stage does not have any de-
cision components because typically the adjust in-
formation is given in one move.
</bodyText>
<figureCaption confidence="0.8393997">
adjust fire: true
shift indicator: fire for effect
repeat FFE: false
left-right adjustment: 50
add-drop adjustment: 0
vertical adjustment: 0
end of mission: false
disposition: -
number of casualties: -
Figure 8: Post-Fire Value Components
</figureCaption>
<bodyText confidence="0.999986857142857">
The fourth class, Disambiguation components,
are used by many rules to disambiguate local in-
formation based on global dialogue features. The
example shown in Figure 9 is from the dialogue
in Figure 1, after utterance 1. The “mission is
polar” component helps determine the method of
target location if speech recognition erroneously
detects both polar and grid coordinates. Target
numbers allow the FOs to handle multiple mis-
sions at the same time (e.g., starting a new call for
fire, before the previous mission has been com-
pleted). The “missions active” component tracks
how many missions are currently being discussed.
The “phase” refers to the state of a three-state FSA
</bodyText>
<page confidence="0.99006">
92
</page>
<bodyText confidence="0.922009086956522">
that tracks which of the three subdialogue phases
(described in section 3) the dialogue is in for the
most recently-discussed mission.
An example of the use of the Disambiguation
components is to determine whether the phrase
“fire for effect” refers to an adjustment of a pre-
vious mission or the initiation of a new mission.
In utterance 13 in Figure 1, “fire for effect” refers
to an adjustment of a CFF that began with an “ad-
just fire” in utterance 1. However, the FO could
have started that CFF by calling for a “fire for ef-
fect”. Furthermore the FO could have started a
second CFF in utterance 13 rather than doing an
adjust, and might have specified “fire for effect”.
By using a rule to check the phase of the mission
the move can be disambiguated to understand that
it is referring to an adjustment, rather than the ini-
tiation of a new fire mission.
mission is polar?: true
target number: 0
missions active: 0
last method of fire: adjust
phase: Info-Gathering
</bodyText>
<figureCaption confidence="0.999408">
Figure 9: Disambiguation Components
</figureCaption>
<bodyText confidence="0.997620666666667">
The last class of components, shown in Fig-
ure 10, is closely tied to the update rule processing,
and is therefore described in the following section.
</bodyText>
<table confidence="0.561828625">
current reply: gator nine one this is
steel one nine
previous reply: -
understood? true
send EOM? false
send repeat? false
send repeat adjust? false
send repeat ffe? false
</table>
<figureCaption confidence="0.715658">
Figure 10: Update Rule Processing Components
</figureCaption>
<subsectionHeader confidence="0.999605">
4.3 Update Rules
</subsectionHeader>
<bodyText confidence="0.99596546031746">
Update rules update the informational compo-
nents, build a message to send to the FO, build
a message to send to the simulator, and decide
whether a message should actually be sent to the
FO or simulator.
As an example of rule application, consider the
processing of utterance 1 in Figure 1. Figure 4
shows the moves and parameters for this utterance.
When the dialogue manager processes this utter-
ance, a set of rules associated with the Identifi-
cation move are applied, which starts building a
response to the FO. This response is built in the
“current reply” Update Rule Processing compo-
nent. Figure 10 shows a reply in the process of
being built: a rule has recognized that an Identifi-
cation move is being given, and has filled in slots
in a template with the necessary information and
added it to the “current reply” component.
Next, the update rules will recognize that a
Warning Order is being given, and will identify
that it is an “adjust fire” method of fire, and up-
date the “has warning order” decision component,
the “method of control” and “method of fire” value
components, and the “last method of fire” disam-
biguation component. As part of this, the appro-
priate fields of the GUIs will be filled in to allow
the trainer to override the FO’s request if need be.
Another rule will then fill in the slots of a template
to add “adjust fire polar” to the current reply, and
later another rule will add “out”, thus finishing the
reply to the FO. After the reply is finished, it will
place it in the “previous reply” component, for ref-
erence if the FO requests a repeat of the previous
utterance.
Certain rules are specified as achieving compre-
hension — that is, if they are applied, the “under-
stood” variable for that turn is set. If no reply has
been built but the move has been understood, then
no reply needs to be sent. This happens, for ex-
ample, for each of utterances 8, 10, and 12 in Fig-
ure 1: because they are confirmations of utterances
that the FDC has initiated, they do not need to be
replied to. Similarly, no reply needs to be sent if
no reply has been built and the incoming message
is empty or only contains one or two words in-
dicative of an open mic and background noise. Fi-
nally, if no reply has been built and the move has
not been understood, then the FO is prompted to
repeat the message.
As described above, the Fire Mission Decision
components are used to determine whether to send
a fire mission. For other communications with the
simulator, a simpler approach is possible. The de-
cisions to send an end of mission, a repeat fire, or a
repeat fire with the ’adjust’ or ’fire for effect’ spec-
ification can be made with update rules acting on
a single boolean, and so these are also part of the
Update Rule Processing Components as shown in
Figure 10.
Finally, the application of rules follows a spe-
cific strategy. A given utterance may contain one
or more dialogue moves, each with a set of rules
specific to it. The dialogue manager applies the
</bodyText>
<page confidence="0.995857">
93
</page>
<bodyText confidence="0.999545875">
appropriate rules to each dialogue move in the
utterance before applying the rules that send the
FO messages or simulator commands, as shown in
Figure 11. Rules for producing replies and simula-
tor commands are delayed until the end of process-
ing an utterance to allow for utterances that may
contain self-corrections or relevant details later in
the turn.
</bodyText>
<tableCaption confidence="0.958989">
for each dialogue move in utterance
apply rules for that dialogue move
end for
apply rules to send reply to FO
apply rules to send simulator commands
</tableCaption>
<figureCaption confidence="0.955261">
Figure 11: Update Strategy for Rules
</figureCaption>
<sectionHeader confidence="0.99696" genericHeader="method">
5 Error Handling
</sectionHeader>
<bodyText confidence="0.999663416666667">
Radiobot-CFF is able to handle various kind of
problematic input in a number of ways. It can han-
dle partially correct information, as in Figure 12.
Speech recognition errors caused the “three casu-
alties” information to be lost, but the update rules
were able to handle the essential part of the FO
contribution: that the mission was ended, and that
the target was neutralized. The domain is forgiv-
ing in this particular example, although a strict
trainer might want to intervene by the GUI and
insist that the FO re-submit the end of mission re-
port.
</bodyText>
<figureCaption confidence="0.748174444444444">
FO Said: end of mission target
neutralized estimate three
casualties over
ASR Output: in end of mission target
neutralized as the make three
catch a these over
Radiobot: end of mission target
neutralized out
Figure 12: Error Correction
</figureCaption>
<bodyText confidence="0.999445">
In other cases, such as when giving number co-
ordinates, all information must be fully grounded.
An example of this is in Figure 13, where the num-
ber “five” is lost by the speech recognition. In
this case, the domain-appropriate response is to
prompt for a repetition.
</bodyText>
<table confidence="0.672207666666667">
FO Said: right five zero over
ASR Output: right by zero over
Radiobot: say again over
</table>
<figureCaption confidence="0.9475">
Figure 13: Error Correction - Prompt
</figureCaption>
<sectionHeader confidence="0.995982" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.9997605">
We conducted an evaluation of the Radiobot-CFF
system in fully-automated, semi-automated, and
human-controlled conditions. The system per-
formed well in a number of measures; for exam-
ple, Table 1 shows the scores for median time-to-
fire and task-completion rates. Additional mea-
sures and further details are available in (Robinson
et al., 2006).
</bodyText>
<tableCaption confidence="0.997493">
Table 1: Example Evaluation Measures
</tableCaption>
<table confidence="0.996616">
Measure Human Semi Fully
Time To Fire 106.2 s 139.4 s 104.3 s
Task Compl. 100% 97.5% 85.9%
</table>
<bodyText confidence="0.998168583333333">
Of particular relevance here, we performed an
evaluation of the dialogue manager, using the eval-
uation corpus of 17 missions run on 8 sessions, a
total of 408 FO utterances. We took transcribed
recordings of the FO utterances, ran them through
the Interpreter, and corrected them. For each ses-
sion, we ran corrected Interpreter output through
the Dialogue Manager to print out the values of the
informational components at the end of every turn.
We then corrected those, and compared the cor-
rections to the uncorrected values to receive preci-
sion, accuracy, and f-scores of 0.99 each.2
</bodyText>
<sectionHeader confidence="0.998153" genericHeader="conclusions">
7 Summary
</sectionHeader>
<bodyText confidence="0.999995571428571">
We presented a dialogue manager which can en-
gage in Call for Fire training dialogues, and de-
scribed the environment and system in which it
works. It has an information state-based design
with several components accessible to a human
operator, and may be controlled either fully, in
part, or not at all by that human operator.
</bodyText>
<sectionHeader confidence="0.998541" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.997654615384615">
This work has been sponsored by the U.S. Army
Research, Development, and Engineering Com-
mand (RDECOM). Statements and opinions ex-
pressed do not necessarily reflect the position or
the policy of the United States Government, and
no official endorsement should be inferred.
2In this preliminary evaluation, the Interpreter and infor-
mational component corrections were all done by a single
coder; also, the coder was correcting the informational com-
ponent output rather than entering informational component
information from blank, thus any errors of omission on the
part of the coder would work in favor of the system perfor-
mance.
</bodyText>
<page confidence="0.997539">
94
</page>
<bodyText confidence="0.9999895">
We would like to thank Charles Hernandez and
Janet Sutton of the Army Research Laboratory,
and Bill Millspaugh and the Depth &amp; Simultane-
ous Attack Battle Lab in Fort Sill, Oklahoma, for
their efforts on this project. We would also like to
thank the other members of the Radiobots project.
</bodyText>
<sectionHeader confidence="0.999192" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999819955223881">
James F. Allen, Bradford W. Miller, Eric K. Ringger,
and Teresa Sikorski. 1996. A robust system for nat-
ural spoken dialogue. In Proceedings of the 1996
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-96), pages 62–70.
Department of the Army. 2001. Tactics, techniques
and procedures for observed fire and fire support at
battalion task force and below. Technical Report FM
3-09.30 (6-30), Department of the Army.
H. Aust, M. Oerder, F. Siede, and V. Steinbiss. 1995. A
spoken language enquiry system for automatic train
timetable information. Philips Journal of Research,
49(4):399–418.
Robin Cooper and Staffan Larsson. 1999. Dialogue
moves and information states. In H.C. Bunt and
E. C. G. Thijsse, editors, Proceedings of the Third
International Workshop on Computational Seman-
tics.
Michael Johnston, Srinivas Bangalore, Gunaranjan
Vasireddy, Amanda Stent, Patrick Ehlen, Mari-
lyn Walker, Steve Whittaker, and Preetam Maloor.
2002. Match: An architecture for multimodal dia-
logue systems. In Proceedings of the 40th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 376–383.
Staffan Larsson and David Traum. 2000. Information
state and dialogue management in the TRINDI dia-
logue move engine toolkit. Natural Language En-
gineering, 6:323–340, September. Special Issue on
Spoken Language Dialogue System Engineering.
Oliver Lemon, Anne Bracy, Alexander Gruenstein, and
Stanley Peters. 2001. The witas mult-modal dia-
logue system i. In Proc. European Conf. on Speech
Communication and Tech- nology, pages 559–1562.
Colin Matheson, Massimo Poesio, and David Traum.
2000. Modelling grounding and discourse obliga-
tions using update rules. In Proceedings of the First
Conference of the North American Chapter of the
Association for Computational Linguistics.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Bryan Pellom. 2001. Sonic: The university of col-
orado continuous speech recognizer. Technical Re-
port TR-CSLR-2001-01, University of Colorado.
Matthew Purver. 2002. Processing unknown words
in a dialogue system. In Proceedings of the 3rd
ACL SIGdial Workshop on Discourse and Dialogue,
pages 174–183. Association for Computational Lin-
guistics, July.
Susan Robinson, Antonio Roque, Ashish Vaswani, and
David Traum. 2006. Evaluation of a spoken dia-
logue system for military call for fire training. To
Appear.
C. Rose, D. Litman, D. Bhembe, K. Forbes, S. Silli-
man, R. Srivastava, and K. van Lehn. 2003. A com-
parison of tutor and student behavior in speech ver-
sus text based tutoring.
F. Sha and F. Pereira. 2003. Shallow parsing with con-
ditional random fields.
David R. Traum and Jeff Rickel. 2002. Embodied
agents for multi-party dialogue in immersive virtual
worlds. In Proceedings of the first International
Joint conference on Autonomous Agents and Mul-
tiagent systems, pages 766–773.
M. Walker and L. Hirschman. 2000. Evaluation for
darpa communicator spoken dialogue systems.
</reference>
<page confidence="0.999068">
95
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.999896">An Information State-Based Dialogue Manager for Call for Fire Dialogues</title>
<author confidence="0.947615">Roque</author>
<affiliation confidence="0.996893">USC Institute for Creative</affiliation>
<address confidence="0.997787">13274 Fiji Way, Marina Del Rey, CA</address>
<email confidence="0.999815">roque@ict.usc.edu,traum@ict.usc.edu</email>
<abstract confidence="0.998606983193278">We present a dialogue manager for “Call for Fire” training dialogues. We describe the training environment, the domain, the features of its novel information statebased dialogue manager, the system it is a part of, and preliminary evaluation results. 1 Overview Dialogue systems are built for many different purposes, including information gathering (e.g., (Aust et al., 1995)), performing simple transactions (e.g, (Walker and Hirschman, 2000)), collaborative interaction (e.g., (Allen et al., 1996)), tutoring (e.g., (Rose et al., 2003)), and training (e.g. (Traum and Rickel, 2002)). Aspects of the purpose, as well as features of the domain itself (e.g., train timetables, air flight bookings, schedule maintenance, physics, and platoon-level military operations) will have a profound effect on the nature of the dialogue which a system will need to engage in. Issues such as initiative, error correction, flexibility in phrasing and dialogue structure may depend crucially on these factors. The information state approach to dialogue managers (Larsson and Traum, 2000) has been an attempt to cast some of these differences within the same framework. In this approach, a theory of dialogue is constructed by providing information structure elements, a set of dialogue moves that can be recognized and produced and are used to modify the nature of these elements, a set of update rules that govern the dynamics of how the information is changed as dialogue moves are performed, and an update strategy. Many different dialogue systems have been built according to this general approach (e.g., (Cooper and Larsson, 1999; Matheson et al., 2000; Lemon et al., 2001; Johnston et al., 2002; Traum and Rickel, 2002; Purver, 2002)). In this paper, we present an information-state based dialogue manager for a new domain: training call for fire dialogues. Like other dialogue systems used as role-players in training applications, the structure of the dialogue is not completely free for a dialogue designer to specify based on issues of dialogue efficiency. The dialogue system must conform as much as possible to the type of dialogue that a trainee would actually encounter in the types of interaction he or she is being trained for. In particular, for military radio dialogues, much of the protocol for interaction is specified by convention (e.g., (Army, 2001)). Still, there is a fair amount of flexibility in how other aspects of the dialogue progress. This dialogue manager is part of a system we call Radiobot-CFF. Radiobots are a general class of dialogue systems meant to speak over the radio in military simulations. Our most extended effort to date is the Radiobot-CFF system, which engages in “call for fire” dialogues to train artillery observers within a virtual reality training simulation. Our dialogue system can operate according to three different use cases, depending on how much control a human operator/trainer would like to exercise over the dialogue. There is a fully automatic mode in which the Radiobot-CFF system engages unassisted in dialogue with the user, a semi-automatic mode in which the Radiobot-CFF system fills in forms (which can be edited) and the operator can approve or change communication with a simulator or trainee, and a passive mode in which the operator is engaging in the dialogue and the Radiobot-CFF system is just observing. section 2, we describe the training applica- 88 of the 7th SIGdial Workshop on Discourse and pages July 2006. Association for Computational Linguistics tion that our dialogue system has been embedded in as well as the system itself. In section 3, we describe some aspects of “call for fire dialogues”, especially the differences in initiative and purposes of different phases in the dialogue. In section 4, we describe the information-state based dialogue model we have developed for this domain. This includes dialogue moves, information components, and update rules. We describe some error handling capabilities in section 5, and evaluation results in section 6. 2 Testbed Our current testbed, Radiobot-CFF, has been developed in a military training environment, JFETS-UTM, at the U.S. Army base in in Ft. Sill, Oklahoma. JFETS-UTM trains soldiers to make Calls for Fire (CFFs), in which a Forward Observer (FO) team locates an enemy target and requests an artillery fire mission by radio from a Fire Direction Center (FDC). The training room resembles a battle-scarred apartment in a Middle Eastern country. A window shows a virtual city displayed by a rear-projected computer screen, and the soldiers use binoculars with computer displays at their ends to search for targets. Ordinarily, two trainers control a UTM session. One communicates with the FO via a simulated radio, and the other decides what the artillery fire should be and inputs it to a GUI for the simulator. It is our goal to replace those two trainers with one trainer focusing on assessment while Radiobot-CFF handles the radio communications and interfaces with the virtual world. Radiobot-CFF is composed of several pipelined components. A Speech Recognition component is implemented using the SONIC speech recognition system (Pellom, 2001) with custom language and acoustic models. An Interpreter component tags the ASR output with its its dialogue move and parameter labels using two separate Conditional Random Field (Sha and Pereira, 2003; Mc- Callum, 2002) taggers trained on hand-annotated utterances. A Dialogue Manager processes the tagged output, sending a reply to the FO (via a template-based Generator) and, when necessary, a to the artillery simulator FireSim to make decisions on what type of fire to send. The reply to FO and messages to simulator are mediated by GUIs where the trainer can intervene if need be. 3 Call for Fire Dialogues Call for Fire procedures are specified in an Army field manual (Army, 2001) with variations based on a unit’s standard operating procedure. Messages are brief and followed by confirmations, where any misunderstandings are immediately corrected. A typical CFF is shown in Figure 1. 1 FO steel one niner this is gator niner one adjust fire polar over 2 FDC gator nine one this is steel one nine adjust polar out 3 FO direction five niner four zero distance eight zero over 4 FDC direction five nine four zero distance four zero out 5 FO one b m p in the open i c m in effect over 6 FDC one b m p in the open i c m in effect out FDC message to observer kilo alpha high explosive four rounds adjust fire target number alpha bravo one zero zero zero over FO m t o kilo alpha four rounds target number pha bravo one out 9 FDC shot over 10 FO shot out 11 FDC splash over 12 FO splash out 13 FO right five zero fire for effect out over 14 FDC right five zero fire for effect out 15 FDC shot over 16 FO shot out 17 FDC rounds complete over 18 FO rounds complete out FO end of mission one b m p suppressed zero casualties over 20 FDC end of mission one b m p suppressed zero casualties out Figure 1: Example Dialogue with Radiobot-CFF CFFs can generally be divided into three In the first phase (utterances 1-6 in Figure 1) the FOs identify themselves and important information about the CFF, including their coordinates, the kind of fire they are requesting, the location of the target, and the kind of target. In utterance 1 in Figure 1 the FO performs an identification, giving his own call sign and that of the FDC he is calling, and also specifies a method of fire (“adjust fire”) and a method of targeting (“polar”.) Note that when speakers expect a reply, they end their utterance with “over” as in utterance 1, otherwise with “out” as in the confirmation in utterance 2. In utterance 3 the FO gives target coordinates, and in utterance 5 the FO identifies the target as a BMP (a type of light tank) and requests ICM rounds (“improved conventional munitions”.) These turns typically follow one another 89 in quick sequence. In the second phase of a CFF, (utterances 7-12 in Figure 1), after the FDC decides what kind of fire they will send, they inform the FO in a message to observer (MTO) as in utterance 7. This includes the units that will fire (“kilo alpha”), the kind of ammunition (“high explosive”), the number of rounds and method of fire (“4 rounds adjust fire”), and the target number (“alpha bravo one zero zero zero”). CFFs are requests rather than orders, and they may be denied in full or in part. In this example, the FO’s request for ICM rounds was denied in favor of High Explosive rounds. Next the FDC informs the FO when the fire mission has been shot, as in utterance 9, and when the fire is about to land, as in utterance 11. Each of these are confirmed by the FO. In the third phase, (utterances 13-20 in Figure 1) the FO regains dialogue initiative. Depending on the observed results, the FO may request that the fire be repeated with an adjust in location or method of fire. In utterance 13 the FO requests that the shot be re-sent to a location 50 meters to the right of the previous shot as a “fire for effect” all-out bombardment rather than an “adjust fire” targeting fire. This is followed by the abbreviated FDC-initiated phase of utterances 15-18. In utterance 19 the FO ends the mission, describing the results and number of casualties. Besides the behavior shown, at any turn either participant may request or initiate an intelligence report or request the status of a mission. Furthermore, after receiving an MTO the FO may immediately begin another fire mission and thus have multiple missions active; subsequent adjusts are disambiguated with the target numbers assigned during the MTOs. 4 Dialogue Manager We have constructed an Information State-based dialogue manager (Larsson and Traum, 2000) on this domain consisting of a set of dialogue moves, a set of informational components with appropriate formal representations, and a set of update rules with an update strategy. We describe each of these in turn. 4.1 Dialogue Moves We defined a set of dialogue moves to represent the incoming FO utterances based on a study of of human-controlled JFETS-UTM sessions, Army manuals, and the needs of the simulator. As shown in Figure 2 these are divided into three groups: those that provide information about the FO or the fire mission, those that confirm information that the FDC has transmitted, and those that make requests.</abstract>
<title confidence="0.929766">Mission Information: Observer Coordinates Situation Report Identification Warning Order Method of Control Method of Engagement Target Location Target Description End of Mission Confirming Information: Message to Observer Shot Splash Rounds Complete Intel Report Other Requests:</title>
<author confidence="0.8290995">Radio Check Say Again</author>
<note confidence="0.58010125">Status Standby Command Figure 2: FO Dialogue Moves</note>
<abstract confidence="0.995201583333333">The dialogue moves that provide information include those in which the FOs transmit their Observer Coordinates (grid location on a map), a generic Situation Report, or one of the various components of a fire mission request ranging from call sign Identification to final End of Mission. The dialogue moves that confirm information include those that confirm the MTO and other FDCinitiated utterances, or a general report on scenario Intel. The final group includes requests to check radio functionality, to repeat the previous utterance, for status of a shot, to stand by for transmission of information, and finally a set of commands such as “check fire” requesting cancellation of a submitted fire mission. Each of these dialogue moves contains information important to the dialogue manager. This information is captured by the parameters of the dialogue move, which are enumerated in Figure 3. Each parameter is listed with the dialogue move it usually occurs with, but this assignment is not strict. For example, “number of enemies” parameters occur in Target Description as well as End of Mission dialogue moves.</abstract>
<date confidence="0.434062">90</date>
<title confidence="0.798759666666667">Identification-related: Warning Order-related: Target Location-related:</title>
<abstract confidence="0.985631306010929">direction distance attitude End Of Mission-related: number—of—enemies disposition Other: command Figure 3: Dialogue Move Parameters Figure 4 shows how the dialogue moves and parameters act to identify the components of an FO utterance. The example is based on utterance 1 in Figure 1; the Identification move has two parameters representing the call signs of the FDC and the FO, and the Warning Order has two parameters representing the method of fire and method of location. Parameters need to be identified to confirm back to the FO, and in some cases to be sent to the simulator and for use in updating the information state. In the example in Figure 4, the fact that the requested method of fire is an “adjust fire” will be sent to the simulator, and the fact that a method of fire has been given will be updated in the information state. one nine this is gator niner one id: one nine id: niner one Order: fire polar of fire: of location: Figure 4: Example Dialogue Moves and Parameters 4.2 Informational Components The Radiobot-CFF dialogue manager’s information state consists of five classes of informational components, defined by their role in the dialogue and their level of accessibility to the user. These are the Fire Mission Decision components, the Fire Mission Value components, the Post-Fire Value components, the Disambiguation components, and the Update Rule Processing components. By dividing the components into multiple classes we separate those that are simulatorspecific from more general aspects of the domain. Decisions to fire are based on general constraints of the domain, whereas the exact components to include in a message to simulator will be simulator-specific. Also, the components have been designed such that there is almost no overlap in the update rules that modify them (see section 4.3). This reduces the complexity involved in editing or adding rules; although there are over 100 rules in the information state, there are few unanticipated side-effects when rules are altered. The first class of components are the Fire Mission Decision components, which are used to determine whether enough information has been collected to send fire. These components are boolean flags, updated by rules based on incoming dialogue moves and parameters. Figure 5 shows the values of these components after utterance 3 in Figure 1 has been processed. The FO has given a warning order, and a target location (which can either be given through a grid location, or through a combination of direction and distance values, and observer coordinates), so the appropriate components are “true”. After the FO gives a target description, that component will be true as well, and an update rule will recognize that enough information has been gathered to send a fire mission. has warning order? true has target location? true has grid location? false has polar direction? true has polar distance? true has polar obco? true has target descr? false Figure 5: Fire Mission Decision Components The second class of information state components is the set of Fire Mission Value components, track the value of various information el- 91 ements necessary for requesting a fire mission. These are specific to the FireSim XXI simulator. Figure 6 shows the values after utterance 3 in Figure 1. Components such as “direction value” take number values, and components such as “method of fire” take values from a finite set of possibilities. Several of these components, such as “attitude” have defaults that are rarely changed. Once the dialogue manager or human trainer decides that it has enough information to request fire, these components are translated into a simulator command and sent to the simulator. method of control: adjust fire method of fire: adjust fire method of engagement: none given target type: grid value: direction value: 5940 distance value: 480 length: 0 width: 100 attitude: 0 observer coordinate value: 45603595 Figure 7: GUI Figure 6: Fire Mission Value Components Fire Mission Value components are also directly modifiable by the trainer. Figure 7 shows the GUI which the trainer can use to take control of the session, edit any of the Fire Mission Value components, and relinquish control of the session back to Radiobot-CFF. This allows the trainer to correct any mistakes that the Radiobot may have made or test the trainee’s adaptability by sending the fire to an unexpected location. The example shown in Figure 7 is after utterance 5 of Figure 1; the system is running in semi-automated mode and the dialogue manager has decided that it has enough information to send a fire. The trainer may send the message or edit it and then send it. A second GUI, not shown, allows the trainer to take control of the outgoing speech of the Radiobot, and, in semi-automated mode, either confirm the sending of a suggested output utterance, alter it before sending, or author new text for the radiobot to say. The third class of components is the Post-Fire Value components, which are also exposed to the trainer for modification. The example shown in Figure 8 is from after utterance 13 in Figure 1; the FO has requested an “adjust fire” with an indicator of “fire for effect” and a right adjustment of 50. At this point in the dialogue the FO could have instead chosen to end the mission. If the initial fire had been a “fire for effect” it could have been repeated, rather than following up an initial “adjust fire.” The adjust fire stage does not have any decision components because typically the adjust information is given in one move. adjust fire: true shift indicator: fire for effect repeat FFE: false left-right adjustment: 50 add-drop adjustment: 0 vertical adjustment: 0 end of mission: false disposition: number of casualties: - Figure 8: Post-Fire Value Components The fourth class, Disambiguation components, are used by many rules to disambiguate local information based on global dialogue features. The example shown in Figure 9 is from the dialogue in Figure 1, after utterance 1. The “mission is polar” component helps determine the method of target location if speech recognition erroneously detects both polar and grid coordinates. Target numbers allow the FOs to handle multiple missions at the same time (e.g., starting a new call for fire, before the previous mission has been completed). The “missions active” component tracks how many missions are currently being discussed. The “phase” refers to the state of a three-state FSA 92 that tracks which of the three subdialogue phases (described in section 3) the dialogue is in for the most recently-discussed mission. An example of the use of the Disambiguation components is to determine whether the phrase “fire for effect” refers to an adjustment of a previous mission or the initiation of a new mission. In utterance 13 in Figure 1, “fire for effect” refers to an adjustment of a CFF that began with an “adjust fire” in utterance 1. However, the FO could have started that CFF by calling for a “fire for effect”. Furthermore the FO could have started a second CFF in utterance 13 rather than doing an adjust, and might have specified “fire for effect”. By using a rule to check the phase of the mission the move can be disambiguated to understand that it is referring to an adjustment, rather than the initiation of a new fire mission. mission is polar?: true target number: 0 missions active: 0 last method of fire: adjust phase: Info-Gathering Figure 9: Disambiguation Components The last class of components, shown in Figure 10, is closely tied to the update rule processing, and is therefore described in the following section. current reply: gator nine one this is steel one nine previous reply: understood? true send EOM? false send repeat? false send repeat adjust? false send repeat ffe? false Figure 10: Update Rule Processing Components 4.3 Update Rules Update rules update the informational components, build a message to send to the FO, build a message to send to the simulator, and decide whether a message should actually be sent to the FO or simulator. As an example of rule application, consider the processing of utterance 1 in Figure 1. Figure 4 shows the moves and parameters for this utterance. When the dialogue manager processes this utterance, a set of rules associated with the Identification move are applied, which starts building a response to the FO. This response is built in the “current reply” Update Rule Processing component. Figure 10 shows a reply in the process of being built: a rule has recognized that an Identification move is being given, and has filled in slots in a template with the necessary information and added it to the “current reply” component. Next, the update rules will recognize that a Warning Order is being given, and will identify that it is an “adjust fire” method of fire, and update the “has warning order” decision component, the “method of control” and “method of fire” value components, and the “last method of fire” disambiguation component. As part of this, the appropriate fields of the GUIs will be filled in to allow the trainer to override the FO’s request if need be. Another rule will then fill in the slots of a template to add “adjust fire polar” to the current reply, and later another rule will add “out”, thus finishing the reply to the FO. After the reply is finished, it will place it in the “previous reply” component, for reference if the FO requests a repeat of the previous utterance. Certain rules are specified as achieving comprehension — that is, if they are applied, the “understood” variable for that turn is set. If no reply has been built but the move has been understood, then no reply needs to be sent. This happens, for example, for each of utterances 8, 10, and 12 in Figure 1: because they are confirmations of utterances that the FDC has initiated, they do not need to be replied to. Similarly, no reply needs to be sent if no reply has been built and the incoming message is empty or only contains one or two words indicative of an open mic and background noise. Finally, if no reply has been built and the move has not been understood, then the FO is prompted to repeat the message. As described above, the Fire Mission Decision components are used to determine whether to send a fire mission. For other communications with the simulator, a simpler approach is possible. The decisions to send an end of mission, a repeat fire, or a repeat fire with the ’adjust’ or ’fire for effect’ specification can be made with update rules acting on a single boolean, and so these are also part of the Update Rule Processing Components as shown in Figure 10. Finally, the application of rules follows a specific strategy. A given utterance may contain one or more dialogue moves, each with a set of rules specific to it. The dialogue manager applies the 93 appropriate rules to each dialogue move in the utterance before applying the rules that send the FO messages or simulator commands, as shown in Figure 11. Rules for producing replies and simulator commands are delayed until the end of processing an utterance to allow for utterances that may contain self-corrections or relevant details later in the turn. for each dialogue move in utterance apply rules for that dialogue move end for apply rules to send reply to FO apply rules to send simulator commands Figure 11: Update Strategy for Rules 5 Error Handling Radiobot-CFF is able to handle various kind of problematic input in a number of ways. It can handle partially correct information, as in Figure 12. Speech recognition errors caused the “three casualties” information to be lost, but the update rules were able to handle the essential part of the FO contribution: that the mission was ended, and that the target was neutralized. The domain is forgiving in this particular example, although a strict trainer might want to intervene by the GUI and insist that the FO re-submit the end of mission report. FO Said: end of mission target neutralized estimate three casualties over ASR Output: in end of mission target neutralized as the make three catch a these over Radiobot: end of mission neutralized out Figure 12: Error Correction In other cases, such as when giving number coordinates, all information must be fully grounded. An example of this is in Figure 13, where the number “five” is lost by the speech recognition. In this case, the domain-appropriate response is to prompt for a repetition. FO Said: right five zero over ASR Output: right by zero over Radiobot: say again over Figure 13: Error Correction - Prompt 6 Evaluation We conducted an evaluation of the Radiobot-CFF system in fully-automated, semi-automated, and human-controlled conditions. The system performed well in a number of measures; for example, Table 1 shows the scores for median time-tofire and task-completion rates. Additional measures and further details are available in (Robinson et al., 2006). 1: Evaluation Measures Measure Human Semi Fully Time To Fire 106.2 s 139.4 s 104.3 s Task Compl. 100% 97.5% 85.9% Of particular relevance here, we performed an evaluation of the dialogue manager, using the evaluation corpus of 17 missions run on 8 sessions, a total of 408 FO utterances. We took transcribed recordings of the FO utterances, ran them through the Interpreter, and corrected them. For each session, we ran corrected Interpreter output through the Dialogue Manager to print out the values of the informational components at the end of every turn. We then corrected those, and compared the corrections to the uncorrected values to receive preciaccuracy, and f-scores of 0.99 7 Summary We presented a dialogue manager which can engage in Call for Fire training dialogues, and described the environment and system in which it works. It has an information state-based design with several components accessible to a human operator, and may be controlled either fully, in part, or not at all by that human operator. 8 Acknowledgements This work has been sponsored by the U.S. Army Research, Development, and Engineering Command (RDECOM). Statements and opinions expressed do not necessarily reflect the position or the policy of the United States Government, and no official endorsement should be inferred. this preliminary evaluation, the Interpreter and informational component corrections were all done by a single coder; also, the coder was correcting the informational component output rather than entering informational component information from blank, thus any errors of omission on the part of the coder would work in favor of the system performance. 94 We would like to thank Charles Hernandez and Janet Sutton of the Army Research Laboratory, and Bill Millspaugh and the Depth &amp; Simultaneous Attack Battle Lab in Fort Sill, Oklahoma, for their efforts on this project. We would also like to thank the other members of the Radiobots project.</abstract>
<title confidence="0.906709">References</title>
<author confidence="0.985893">James F Allen</author>
<author confidence="0.985893">Bradford W Miller</author>
<author confidence="0.985893">Eric K Ringger</author>
<note confidence="0.6894272">and Teresa Sikorski. 1996. A robust system for natspoken dialogue. In of the 1996 Annual Meeting of the Association for Computa- Linguistics pages 62–70. Department of the Army. 2001. Tactics, techniques</note>
<abstract confidence="0.900444222222222">and procedures for observed fire and fire support at battalion task force and below. Technical Report FM 3-09.30 (6-30), Department of the Army. H. Aust, M. Oerder, F. Siede, and V. Steinbiss. 1995. A spoken language enquiry system for automatic train information. Journal of 49(4):399–418. Robin Cooper and Staffan Larsson. 1999. Dialogue moves and information states. In H.C. Bunt and</abstract>
<note confidence="0.5218322">C. G. Thijsse, editors, of the Third International Workshop on Computational Seman- Michael Johnston, Srinivas Bangalore, Gunaranjan Vasireddy, Amanda Stent, Patrick Ehlen, Marilyn Walker, Steve Whittaker, and Preetam Maloor. 2002. Match: An architecture for multimodal diasystems. In of the 40th Annual Meeting of the Association for Computational Linpages 376–383. Staffan Larsson and David Traum. 2000. Information</note>
<abstract confidence="0.867211133333333">state and dialogue management in the TRINDI diamove engine toolkit. Language En- 6:323–340, September. Special Issue on Spoken Language Dialogue System Engineering. Oliver Lemon, Anne Bracy, Alexander Gruenstein, and Stanley Peters. 2001. The witas mult-modal diasystem i. In European Conf. on Speech and Techpages 559–1562. Colin Matheson, Massimo Poesio, and David Traum. 2000. Modelling grounding and discourse obligausing update rules. In of the First Conference of the North American Chapter of the for Computational Kachites McCallum. 2002. Mallet: A machine learning for language toolkit.</abstract>
<web confidence="0.843172">http://mallet.cs.umass.edu.</web>
<note confidence="0.810209714285714">Bryan Pellom. 2001. Sonic: The university of colorado continuous speech recognizer. Technical Report TR-CSLR-2001-01, University of Colorado. Matthew Purver. 2002. Processing unknown words a dialogue system. In of the 3rd SIGdial Workshop on Discourse and pages 174–183. Association for Computational Linguistics, July. Susan Robinson, Antonio Roque, Ashish Vaswani, and David Traum. 2006. Evaluation of a spoken dialogue system for military call for fire training. To Appear. C. Rose, D. Litman, D. Bhembe, K. Forbes, S. Silliman, R. Srivastava, and K. van Lehn. 2003. A com-</note>
<abstract confidence="0.914845181818182">parison of tutor and student behavior in speech versus text based tutoring. F. Sha and F. Pereira. 2003. Shallow parsing with conditional random fields. David R. Traum and Jeff Rickel. 2002. Embodied agents for multi-party dialogue in immersive virtual In of the first International Joint conference on Autonomous Agents and Mulpages 766–773. M. Walker and L. Hirschman. 2000. Evaluation for darpa communicator spoken dialogue systems.</abstract>
<intro confidence="0.817277">95</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James F Allen</author>
<author>Bradford W Miller</author>
<author>Eric K Ringger</author>
<author>Teresa Sikorski</author>
</authors>
<title>A robust system for natural spoken dialogue.</title>
<date>1996</date>
<booktitle>In Proceedings of the 1996 Annual Meeting of the Association for Computational Linguistics (ACL-96),</booktitle>
<pages>62--70</pages>
<contexts>
<context position="729" citStr="Allen et al., 1996" startWordPosition="101" endWordPosition="104">itute for Creative Technologies 13274 Fiji Way, Marina Del Rey, CA 90292 roque@ict.usc.edu, traum@ict.usc.edu Abstract We present a dialogue manager for “Call for Fire” training dialogues. We describe the training environment, the domain, the features of its novel information statebased dialogue manager, the system it is a part of, and preliminary evaluation results. 1 Overview Dialogue systems are built for many different purposes, including information gathering (e.g., (Aust et al., 1995)), performing simple transactions (e.g, (Walker and Hirschman, 2000)), collaborative interaction (e.g., (Allen et al., 1996)), tutoring (e.g., (Rose et al., 2003)), and training (e.g. (Traum and Rickel, 2002)). Aspects of the purpose, as well as features of the domain itself (e.g., train timetables, air flight bookings, schedule maintenance, physics, and platoon-level military operations) will have a profound effect on the nature of the dialogue which a system will need to engage in. Issues such as initiative, error correction, flexibility in phrasing and dialogue structure may depend crucially on these factors. The information state approach to dialogue managers (Larsson and Traum, 2000) has been an attempt to cas</context>
</contexts>
<marker>Allen, Miller, Ringger, Sikorski, 1996</marker>
<rawString>James F. Allen, Bradford W. Miller, Eric K. Ringger, and Teresa Sikorski. 1996. A robust system for natural spoken dialogue. In Proceedings of the 1996 Annual Meeting of the Association for Computational Linguistics (ACL-96), pages 62–70.</rawString>
</citation>
<citation valid="true">
<title>the Army.</title>
<date>2001</date>
<tech>Technical Report FM 3-09.30 (6-30),</tech>
<institution>Department of</institution>
<marker>2001</marker>
<rawString>Department of the Army. 2001. Tactics, techniques and procedures for observed fire and fire support at battalion task force and below. Technical Report FM 3-09.30 (6-30), Department of the Army.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Aust</author>
<author>M Oerder</author>
<author>F Siede</author>
<author>V Steinbiss</author>
</authors>
<title>A spoken language enquiry system for automatic train timetable information.</title>
<date>1995</date>
<journal>Philips Journal of Research,</journal>
<volume>49</volume>
<issue>4</issue>
<marker>Aust, Oerder, Siede, Steinbiss, 1995</marker>
<rawString>H. Aust, M. Oerder, F. Siede, and V. Steinbiss. 1995. A spoken language enquiry system for automatic train timetable information. Philips Journal of Research, 49(4):399–418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robin Cooper</author>
<author>Staffan Larsson</author>
</authors>
<title>Dialogue moves and information states.</title>
<date>1999</date>
<booktitle>Proceedings of the Third International Workshop on Computational Semantics.</booktitle>
<editor>In H.C. Bunt and E. C. G. Thijsse, editors,</editor>
<contexts>
<context position="1849" citStr="Cooper and Larsson, 1999" startWordPosition="284" endWordPosition="287">The information state approach to dialogue managers (Larsson and Traum, 2000) has been an attempt to cast some of these differences within the same framework. In this approach, a theory of dialogue is constructed by providing information structure elements, a set of dialogue moves that can be recognized and produced and are used to modify the nature of these elements, a set of update rules that govern the dynamics of how the information is changed as dialogue moves are performed, and an update strategy. Many different dialogue systems have been built according to this general approach (e.g., (Cooper and Larsson, 1999; Matheson et al., 2000; Lemon et al., 2001; Johnston et al., 2002; Traum and Rickel, 2002; Purver, 2002)). In this paper, we present an information-state based dialogue manager for a new domain: training call for fire dialogues. Like other dialogue systems used as role-players in training applications, the structure of the dialogue is not completely free for a dialogue designer to specify based on issues of dialogue efficiency. The dialogue system must conform as much as possible to the type of dialogue that a trainee would actually encounter in the types of interaction he or she is being tra</context>
</contexts>
<marker>Cooper, Larsson, 1999</marker>
<rawString>Robin Cooper and Staffan Larsson. 1999. Dialogue moves and information states. In H.C. Bunt and E. C. G. Thijsse, editors, Proceedings of the Third International Workshop on Computational Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Johnston</author>
<author>Srinivas Bangalore</author>
<author>Gunaranjan Vasireddy</author>
<author>Amanda Stent</author>
<author>Patrick Ehlen</author>
<author>Marilyn Walker</author>
<author>Steve Whittaker</author>
<author>Preetam Maloor</author>
</authors>
<title>Match: An architecture for multimodal dialogue systems.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>376--383</pages>
<contexts>
<context position="1915" citStr="Johnston et al., 2002" startWordPosition="296" endWordPosition="299">m, 2000) has been an attempt to cast some of these differences within the same framework. In this approach, a theory of dialogue is constructed by providing information structure elements, a set of dialogue moves that can be recognized and produced and are used to modify the nature of these elements, a set of update rules that govern the dynamics of how the information is changed as dialogue moves are performed, and an update strategy. Many different dialogue systems have been built according to this general approach (e.g., (Cooper and Larsson, 1999; Matheson et al., 2000; Lemon et al., 2001; Johnston et al., 2002; Traum and Rickel, 2002; Purver, 2002)). In this paper, we present an information-state based dialogue manager for a new domain: training call for fire dialogues. Like other dialogue systems used as role-players in training applications, the structure of the dialogue is not completely free for a dialogue designer to specify based on issues of dialogue efficiency. The dialogue system must conform as much as possible to the type of dialogue that a trainee would actually encounter in the types of interaction he or she is being trained for. In particular, for military radio dialogues, much of the</context>
</contexts>
<marker>Johnston, Bangalore, Vasireddy, Stent, Ehlen, Walker, Whittaker, Maloor, 2002</marker>
<rawString>Michael Johnston, Srinivas Bangalore, Gunaranjan Vasireddy, Amanda Stent, Patrick Ehlen, Marilyn Walker, Steve Whittaker, and Preetam Maloor. 2002. Match: An architecture for multimodal dialogue systems. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 376–383.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Staffan Larsson</author>
<author>David Traum</author>
</authors>
<title>Information state and dialogue management in the TRINDI dialogue move engine toolkit.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<booktitle>Special Issue on Spoken Language Dialogue System Engineering.</booktitle>
<pages>6--323</pages>
<contexts>
<context position="1302" citStr="Larsson and Traum, 2000" startWordPosition="191" endWordPosition="194">llaborative interaction (e.g., (Allen et al., 1996)), tutoring (e.g., (Rose et al., 2003)), and training (e.g. (Traum and Rickel, 2002)). Aspects of the purpose, as well as features of the domain itself (e.g., train timetables, air flight bookings, schedule maintenance, physics, and platoon-level military operations) will have a profound effect on the nature of the dialogue which a system will need to engage in. Issues such as initiative, error correction, flexibility in phrasing and dialogue structure may depend crucially on these factors. The information state approach to dialogue managers (Larsson and Traum, 2000) has been an attempt to cast some of these differences within the same framework. In this approach, a theory of dialogue is constructed by providing information structure elements, a set of dialogue moves that can be recognized and produced and are used to modify the nature of these elements, a set of update rules that govern the dynamics of how the information is changed as dialogue moves are performed, and an update strategy. Many different dialogue systems have been built according to this general approach (e.g., (Cooper and Larsson, 1999; Matheson et al., 2000; Lemon et al., 2001; Johnston</context>
<context position="10098" citStr="Larsson and Traum, 2000" startWordPosition="1716" endWordPosition="1719">s followed by the abbreviated FDC-initiated phase of utterances 15-18. In utterance 19 the FO ends the mission, describing the results and number of casualties. Besides the behavior shown, at any turn either participant may request or initiate an intelligence report or request the status of a mission. Furthermore, after receiving an MTO the FO may immediately begin another fire mission and thus have multiple missions active; subsequent adjusts are disambiguated with the target numbers assigned during the MTOs. 4 Dialogue Manager We have constructed an Information State-based dialogue manager (Larsson and Traum, 2000) on this domain consisting of a set of dialogue moves, a set of informational components with appropriate formal representations, and a set of update rules with an update strategy. We describe each of these in turn. 4.1 Dialogue Moves We defined a set of dialogue moves to represent the incoming FO utterances based on a study of transcripts of human-controlled JFETS-UTM sessions, Army manuals, and the needs of the simulator. As shown in Figure 2 these are divided into three groups: those that provide information about the FO or the fire mission, those that confirm information that the FDC has t</context>
</contexts>
<marker>Larsson, Traum, 2000</marker>
<rawString>Staffan Larsson and David Traum. 2000. Information state and dialogue management in the TRINDI dialogue move engine toolkit. Natural Language Engineering, 6:323–340, September. Special Issue on Spoken Language Dialogue System Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Lemon</author>
<author>Anne Bracy</author>
<author>Alexander Gruenstein</author>
<author>Stanley Peters</author>
</authors>
<title>The witas mult-modal dialogue system i. In</title>
<date>2001</date>
<booktitle>Proc. European Conf. on Speech Communication and Tech- nology,</booktitle>
<pages>559--1562</pages>
<contexts>
<context position="1892" citStr="Lemon et al., 2001" startWordPosition="292" endWordPosition="295">rs (Larsson and Traum, 2000) has been an attempt to cast some of these differences within the same framework. In this approach, a theory of dialogue is constructed by providing information structure elements, a set of dialogue moves that can be recognized and produced and are used to modify the nature of these elements, a set of update rules that govern the dynamics of how the information is changed as dialogue moves are performed, and an update strategy. Many different dialogue systems have been built according to this general approach (e.g., (Cooper and Larsson, 1999; Matheson et al., 2000; Lemon et al., 2001; Johnston et al., 2002; Traum and Rickel, 2002; Purver, 2002)). In this paper, we present an information-state based dialogue manager for a new domain: training call for fire dialogues. Like other dialogue systems used as role-players in training applications, the structure of the dialogue is not completely free for a dialogue designer to specify based on issues of dialogue efficiency. The dialogue system must conform as much as possible to the type of dialogue that a trainee would actually encounter in the types of interaction he or she is being trained for. In particular, for military radio</context>
</contexts>
<marker>Lemon, Bracy, Gruenstein, Peters, 2001</marker>
<rawString>Oliver Lemon, Anne Bracy, Alexander Gruenstein, and Stanley Peters. 2001. The witas mult-modal dialogue system i. In Proc. European Conf. on Speech Communication and Tech- nology, pages 559–1562.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Matheson</author>
<author>Massimo Poesio</author>
<author>David Traum</author>
</authors>
<title>Modelling grounding and discourse obligations using update rules.</title>
<date>2000</date>
<booktitle>In Proceedings of the First Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1872" citStr="Matheson et al., 2000" startWordPosition="288" endWordPosition="291">oach to dialogue managers (Larsson and Traum, 2000) has been an attempt to cast some of these differences within the same framework. In this approach, a theory of dialogue is constructed by providing information structure elements, a set of dialogue moves that can be recognized and produced and are used to modify the nature of these elements, a set of update rules that govern the dynamics of how the information is changed as dialogue moves are performed, and an update strategy. Many different dialogue systems have been built according to this general approach (e.g., (Cooper and Larsson, 1999; Matheson et al., 2000; Lemon et al., 2001; Johnston et al., 2002; Traum and Rickel, 2002; Purver, 2002)). In this paper, we present an information-state based dialogue manager for a new domain: training call for fire dialogues. Like other dialogue systems used as role-players in training applications, the structure of the dialogue is not completely free for a dialogue designer to specify based on issues of dialogue efficiency. The dialogue system must conform as much as possible to the type of dialogue that a trainee would actually encounter in the types of interaction he or she is being trained for. In particular</context>
</contexts>
<marker>Matheson, Poesio, Traum, 2000</marker>
<rawString>Colin Matheson, Massimo Poesio, and David Traum. 2000. Modelling grounding and discourse obligations using update rules. In Proceedings of the First Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="5678" citStr="McCallum, 2002" startWordPosition="911" endWordPosition="913">ry fire should be and inputs it to a GUI for the simulator. It is our goal to replace those two trainers with one trainer focusing on assessment while Radiobot-CFF handles the radio communications and interfaces with the virtual world. Radiobot-CFF is composed of several pipelined components. A Speech Recognition component is implemented using the SONIC speech recognition system (Pellom, 2001) with custom language and acoustic models. An Interpreter component tags the ASR output with its its dialogue move and parameter labels using two separate Conditional Random Field (Sha and Pereira, 2003; McCallum, 2002) taggers trained on hand-annotated utterances. A Dialogue Manager processes the tagged output, sending a reply to the FO (via a template-based Generator) and, when necessary, a message to the artillery simulator FireSim XXI1 to make decisions on what type of fire to send. The reply to FO and messages to simulator are mediated by GUIs where the trainer can intervene if 1http://sill-www.army.mil/blab/sims/FireSimXXI.htm need be. 3 Call for Fire Dialogues Call for Fire procedures are specified in an Army field manual (Army, 2001) with variations based on a unit’s standard operating procedure. Mes</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bryan Pellom</author>
</authors>
<title>Sonic: The university of colorado continuous speech recognizer.</title>
<date>2001</date>
<tech>Technical Report TR-CSLR-2001-01,</tech>
<institution>University of Colorado.</institution>
<contexts>
<context position="5459" citStr="Pellom, 2001" startWordPosition="877" endWordPosition="878">ers use binoculars with computer displays at their ends to search for targets. Ordinarily, two trainers control a UTM session. One communicates with the FO via a simulated radio, and the other decides what the artillery fire should be and inputs it to a GUI for the simulator. It is our goal to replace those two trainers with one trainer focusing on assessment while Radiobot-CFF handles the radio communications and interfaces with the virtual world. Radiobot-CFF is composed of several pipelined components. A Speech Recognition component is implemented using the SONIC speech recognition system (Pellom, 2001) with custom language and acoustic models. An Interpreter component tags the ASR output with its its dialogue move and parameter labels using two separate Conditional Random Field (Sha and Pereira, 2003; McCallum, 2002) taggers trained on hand-annotated utterances. A Dialogue Manager processes the tagged output, sending a reply to the FO (via a template-based Generator) and, when necessary, a message to the artillery simulator FireSim XXI1 to make decisions on what type of fire to send. The reply to FO and messages to simulator are mediated by GUIs where the trainer can intervene if 1http://si</context>
</contexts>
<marker>Pellom, 2001</marker>
<rawString>Bryan Pellom. 2001. Sonic: The university of colorado continuous speech recognizer. Technical Report TR-CSLR-2001-01, University of Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Purver</author>
</authors>
<title>Processing unknown words in a dialogue system.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd ACL SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>174--183</pages>
<contexts>
<context position="1954" citStr="Purver, 2002" startWordPosition="304" endWordPosition="305">ese differences within the same framework. In this approach, a theory of dialogue is constructed by providing information structure elements, a set of dialogue moves that can be recognized and produced and are used to modify the nature of these elements, a set of update rules that govern the dynamics of how the information is changed as dialogue moves are performed, and an update strategy. Many different dialogue systems have been built according to this general approach (e.g., (Cooper and Larsson, 1999; Matheson et al., 2000; Lemon et al., 2001; Johnston et al., 2002; Traum and Rickel, 2002; Purver, 2002)). In this paper, we present an information-state based dialogue manager for a new domain: training call for fire dialogues. Like other dialogue systems used as role-players in training applications, the structure of the dialogue is not completely free for a dialogue designer to specify based on issues of dialogue efficiency. The dialogue system must conform as much as possible to the type of dialogue that a trainee would actually encounter in the types of interaction he or she is being trained for. In particular, for military radio dialogues, much of the protocol for interaction is specified </context>
</contexts>
<marker>Purver, 2002</marker>
<rawString>Matthew Purver. 2002. Processing unknown words in a dialogue system. In Proceedings of the 3rd ACL SIGdial Workshop on Discourse and Dialogue, pages 174–183. Association for Computational Linguistics, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Robinson</author>
<author>Antonio Roque</author>
<author>Ashish Vaswani</author>
<author>David Traum</author>
</authors>
<title>Evaluation of a spoken dialogue system for military call for fire training.</title>
<date>2006</date>
<note>To Appear.</note>
<contexts>
<context position="25551" citStr="Robinson et al., 2006" startWordPosition="4331" endWordPosition="4334">ure 13, where the number “five” is lost by the speech recognition. In this case, the domain-appropriate response is to prompt for a repetition. FO Said: right five zero over ASR Output: right by zero over Radiobot: say again over Figure 13: Error Correction - Prompt 6 Evaluation We conducted an evaluation of the Radiobot-CFF system in fully-automated, semi-automated, and human-controlled conditions. The system performed well in a number of measures; for example, Table 1 shows the scores for median time-tofire and task-completion rates. Additional measures and further details are available in (Robinson et al., 2006). Table 1: Example Evaluation Measures Measure Human Semi Fully Time To Fire 106.2 s 139.4 s 104.3 s Task Compl. 100% 97.5% 85.9% Of particular relevance here, we performed an evaluation of the dialogue manager, using the evaluation corpus of 17 missions run on 8 sessions, a total of 408 FO utterances. We took transcribed recordings of the FO utterances, ran them through the Interpreter, and corrected them. For each session, we ran corrected Interpreter output through the Dialogue Manager to print out the values of the informational components at the end of every turn. We then corrected those,</context>
</contexts>
<marker>Robinson, Roque, Vaswani, Traum, 2006</marker>
<rawString>Susan Robinson, Antonio Roque, Ashish Vaswani, and David Traum. 2006. Evaluation of a spoken dialogue system for military call for fire training. To Appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Rose</author>
<author>D Litman</author>
<author>D Bhembe</author>
<author>K Forbes</author>
<author>S Silliman</author>
<author>R Srivastava</author>
<author>K van Lehn</author>
</authors>
<title>A comparison of tutor and student behavior in speech versus text based tutoring.</title>
<date>2003</date>
<marker>Rose, Litman, Bhembe, Forbes, Silliman, Srivastava, van Lehn, 2003</marker>
<rawString>C. Rose, D. Litman, D. Bhembe, K. Forbes, S. Silliman, R. Srivastava, and K. van Lehn. 2003. A comparison of tutor and student behavior in speech versus text based tutoring.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sha</author>
<author>F Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<contexts>
<context position="5661" citStr="Sha and Pereira, 2003" startWordPosition="907" endWordPosition="910">ecides what the artillery fire should be and inputs it to a GUI for the simulator. It is our goal to replace those two trainers with one trainer focusing on assessment while Radiobot-CFF handles the radio communications and interfaces with the virtual world. Radiobot-CFF is composed of several pipelined components. A Speech Recognition component is implemented using the SONIC speech recognition system (Pellom, 2001) with custom language and acoustic models. An Interpreter component tags the ASR output with its its dialogue move and parameter labels using two separate Conditional Random Field (Sha and Pereira, 2003; McCallum, 2002) taggers trained on hand-annotated utterances. A Dialogue Manager processes the tagged output, sending a reply to the FO (via a template-based Generator) and, when necessary, a message to the artillery simulator FireSim XXI1 to make decisions on what type of fire to send. The reply to FO and messages to simulator are mediated by GUIs where the trainer can intervene if 1http://sill-www.army.mil/blab/sims/FireSimXXI.htm need be. 3 Call for Fire Dialogues Call for Fire procedures are specified in an Army field manual (Army, 2001) with variations based on a unit’s standard operati</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>F. Sha and F. Pereira. 2003. Shallow parsing with conditional random fields.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Traum</author>
<author>Jeff Rickel</author>
</authors>
<title>Embodied agents for multi-party dialogue in immersive virtual worlds.</title>
<date>2002</date>
<booktitle>In Proceedings of the first International Joint conference on Autonomous Agents and Multiagent systems,</booktitle>
<pages>766--773</pages>
<contexts>
<context position="813" citStr="Traum and Rickel, 2002" startWordPosition="114" endWordPosition="117">ct.usc.edu, traum@ict.usc.edu Abstract We present a dialogue manager for “Call for Fire” training dialogues. We describe the training environment, the domain, the features of its novel information statebased dialogue manager, the system it is a part of, and preliminary evaluation results. 1 Overview Dialogue systems are built for many different purposes, including information gathering (e.g., (Aust et al., 1995)), performing simple transactions (e.g, (Walker and Hirschman, 2000)), collaborative interaction (e.g., (Allen et al., 1996)), tutoring (e.g., (Rose et al., 2003)), and training (e.g. (Traum and Rickel, 2002)). Aspects of the purpose, as well as features of the domain itself (e.g., train timetables, air flight bookings, schedule maintenance, physics, and platoon-level military operations) will have a profound effect on the nature of the dialogue which a system will need to engage in. Issues such as initiative, error correction, flexibility in phrasing and dialogue structure may depend crucially on these factors. The information state approach to dialogue managers (Larsson and Traum, 2000) has been an attempt to cast some of these differences within the same framework. In this approach, a theory of</context>
</contexts>
<marker>Traum, Rickel, 2002</marker>
<rawString>David R. Traum and Jeff Rickel. 2002. Embodied agents for multi-party dialogue in immersive virtual worlds. In Proceedings of the first International Joint conference on Autonomous Agents and Multiagent systems, pages 766–773.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Walker</author>
<author>L Hirschman</author>
</authors>
<title>Evaluation for darpa communicator spoken dialogue systems.</title>
<date>2000</date>
<contexts>
<context position="673" citStr="Walker and Hirschman, 2000" startWordPosition="93" endWordPosition="96">r Call for Fire Dialogues Antonio Roque and David Traum USC Institute for Creative Technologies 13274 Fiji Way, Marina Del Rey, CA 90292 roque@ict.usc.edu, traum@ict.usc.edu Abstract We present a dialogue manager for “Call for Fire” training dialogues. We describe the training environment, the domain, the features of its novel information statebased dialogue manager, the system it is a part of, and preliminary evaluation results. 1 Overview Dialogue systems are built for many different purposes, including information gathering (e.g., (Aust et al., 1995)), performing simple transactions (e.g, (Walker and Hirschman, 2000)), collaborative interaction (e.g., (Allen et al., 1996)), tutoring (e.g., (Rose et al., 2003)), and training (e.g. (Traum and Rickel, 2002)). Aspects of the purpose, as well as features of the domain itself (e.g., train timetables, air flight bookings, schedule maintenance, physics, and platoon-level military operations) will have a profound effect on the nature of the dialogue which a system will need to engage in. Issues such as initiative, error correction, flexibility in phrasing and dialogue structure may depend crucially on these factors. The information state approach to dialogue manag</context>
</contexts>
<marker>Walker, Hirschman, 2000</marker>
<rawString>M. Walker and L. Hirschman. 2000. Evaluation for darpa communicator spoken dialogue systems.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>