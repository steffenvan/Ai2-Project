<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000022">
<note confidence="0.817035">
In: Proceedings of CoNLL-2000 and LLL-2000, pages 43-48, Lisbon, Portugal, 2000.
</note>
<title confidence="0.991332">
Memory-Based Learning for Article Generation
</title>
<author confidence="0.996693">
Guido Minnen* Francis Bondt
</author>
<affiliation confidence="0.986395">
Cognitive and Computing Sciences MT Research Group
University of Sussex NTT Communication Science Labs
</affiliation>
<address confidence="0.506615">
Falmer BN1 9QH, Brighton, UK 2-4 Hikari-dai, Kyoto 619-0237, JAPAN
Guido .Minnen@cogs .susx.ac.uk bondOcslab.kecl.ntt . co . jp
Ann Copestake
</address>
<email confidence="0.204619">
CSLI
</email>
<affiliation confidence="0.727254">
Stanford University
</affiliation>
<address confidence="0.384347">
Stanford CA 94305-2150, USA
</address>
<email confidence="0.356065">
aaacsli . stanf ord edu
</email>
<sectionHeader confidence="0.980676" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999875">
Article choice can pose difficult problems in ap-
plications such as machine translation and auto-
mated summarization. In this paper, we investi-
gate the use of corpus data to collect statistical
generalizations about article use in English in
order to be able to generate articles automati-
cally to supplement a symbolic generator. We
use data from the Penn Treebank as input to a
memory-based learner (TiMBL 3.0; Daelemans
et al., 2000) which predicts whether to gener-
ate an article with respect to an English base
noun phrase. We discuss competitive results ob-
tained using a variety of lexical, syntactic and
semantic features that play an important role in
automated article generation.
</bodyText>
<sectionHeader confidence="0.998522" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.980793518518519">
Article choice can pose difficult problems in nat-
ural language applications. Machine transla-
tion (MT) is an example of such an applica-
tion. When translating from a source language
that lacks articles, such as Japanese or Rus-
sian, to one that requires them, such as English
or German, the system must somehow generate
the source language articles (Bond and Ogura,
1998). Similarly in automated summarization:
when sentences or fragments are combined or
reduced, it is possible that the form of a noun
phrase (NP) is changed such that a change of
the article associated with the NP&apos;s head be-
comes necessary. For example, consider the sen-
tences A talk will be given on Friday about NLP;
The talk will last for one hour which might get
summarized as Friday&apos;s NLP talk will last one
hour. However, given the input sentences, it is
not clear how to decide not to generate an arti-
cle for the subject NP in the output sentence.
Another important application is in the field
known as augmentative and alternative com-
munication (AAC). In particular, people who
have lost the ability to speak sometimes use
a text-to-speech generator as a prosthetic de-
vice. But most disabilities which affect speech,
such as stroke or amyotrophic lateral sclerosis
(ALS or Lou Gehrig&apos;s disease), also cause some
more general motor impairment, which means
that prosthesis users cannot achieve a text in-
put rate comparable to normal typing speeds
even if they are able to use a keyboard. Many
have to rely on a slower physical interface (head-
stick, head-pointer, eye-tracker etc). We are at-
tempting to use a range of NLP technology to
improve text input speed for such users. Article
choice is particularly important for this applica-
tion: many AAC users drop articles and resort
to a sort of telegraphese, but this causes degra-
dation in comprehension of synthetic speech and
contributes to its perception as unnatural and
robot-like. Our particular goal is to be able to
use an article generator in conjunction with a
symbolic generator for AAC (Copestake, 1997;
Carroll et al., 1999).
In this paper we investigate the use of corpus
data to collect statistical generalizations about
article use in English so as to be able to gen-
erate them automatically. We use data from
the Penn Treebank as input to a memory-based
learner (TiMBL 3.0; Daelemans et al., 2000)
that is used to predict whether to generate the
or a/an or no article.&apos; We discuss a variety
of lexical, syntactic and semantic features that
</bodyText>
<affiliation confidence="0.531809">
Visiting CSLI, Stanford University (2000). 1We assume a postprocessor to determine whether to
t Visiting CSLI, Stanford University (1999-2000). generate a or an as described in Minnen et al. (2000).
</affiliation>
<page confidence="0.999908">
43
</page>
<bodyText confidence="0.9999419">
play an important role in automated article gen-
eration, and compare our results with other re-
searchers&apos;.
The paper is structured as follows. Section 2
relates our work to that of others. Section 3
introduces the features we use. Section 4 intro-
duces the learning method we use. We discuss
our results in Section 5 and suggest some di-
rections for future research, then conclude with
some final remarks in Section 6.
</bodyText>
<sectionHeader confidence="0.998753" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999989954545455">
There has been considerable research on gen-
erating articles in machine translation sys-
tems (Gawroriska, 1990; Murata and Nagao,
1993; Bond and Ogura, 1998; Heine, 1998).
These systems use hand-written rules and lex-
ical information to generate articles. The best
cited results, 88% accuracy, are quoted by Heine
(1998) which were obtained with respect to a
very small corpus of 1,000 sentences in a re-
stricted domain.
Knight and Chander (1994) present an ap-
proach that uses decision trees to determine
whether to generate the or a/an. They do not
consider the possibility that no article should
be generated. On the basis of a corpus of 400K
NP instances derived from the Wall Street Jour-
nal, they construct decision trees for the 1,600
most frequent nouns by considering over 30,000
lexical, syntactic and semantic features. They
achieve an accuracy of 81% with respect to these
nouns. By guessing the for the remainder of the
nouns, they achieve an overall accuracy of 78%.
</bodyText>
<sectionHeader confidence="0.9667615" genericHeader="method">
3 Features Determining Automated
Article Generation
</sectionHeader>
<bodyText confidence="0.999757857142857">
We have extracted 300K base noun phrases
(NPs) from the Penn Treebank Wall Street
Journal data (Bies et al., 1995) using the tgrep
tool. The distribution of these NP instances
with respect to articles is as follows: the 20.6%,
a/an 9.4% and 70.0% with no article.
We experimented with a range of features:
</bodyText>
<listItem confidence="0.992716142857143">
1. Head of the NP: We consider as the head
of the NP the rightmost noun in the NP. If an
NP does not contain a noun, we take the last
word in the NP as its head.
2. Part-of-speech (PoS) tag of the head of
the NP: PoS labels were taken from the Penn
Treebank. We list the tags that occurred with
</listItem>
<figure confidence="0.5980215">
(PP-DIR to/TO
(NP the/DT problem/NN))
</figure>
<figureCaption confidence="0.9961775">
Figure 1: An example of a prepositional phrase
annotated with a functional tag
</figureCaption>
<bodyText confidence="0.605012">
the heads of the NPs in Table 1.
</bodyText>
<table confidence="0.999206952380953">
PoS Tag the a/an no
NN 42,806 27,160 53,855
NNS 10,705 446 58,118
NNP 6,938 271 47,721
NNPS 536 2 1,329
CD 382 180 13,368
DT 18 0 3,045
PRP 0 0 21,214
PRP$ 0 0 25
EX 0 0 1,073
IN 0 1 502
JJ 388 143 931
JJR 11 1 310
JJS 184 0 282
RB 15 41 498
VBG 43 12 210
VB 0 1 89
WDT 2 0 4,812
WP 0 0 2,759
Misc. 40 8 269
Total: 62,068 28,266 210,410
</table>
<tableCaption confidence="0.8329575">
Table 1: Distribution of NP instances in Wall
Street Journal data (300,744 NPs in all)
</tableCaption>
<listItem confidence="0.914445857142857">
3. Functional tag of the head of the NP: In
the Penn Treebank each syntactic category can
be associated with up to four functional tags as
listed in Table 2. We consider the sequence of
functional tags associated with the category of
the NP as a feature; if a constituent has no func-
tional tag, we give the feature the value NONE.
4. Category of the constituent embedding the
NP: We looked at the category of the embedding
constituent. See Figure 1: The category of the
constituent embedding the NP the problem is
PP.
5. Functional tag of the constituent
embedding the NP: If the category of the con-
</listItem>
<bodyText confidence="0.730326">
stituent embedding the NP is associated with
one or more functional tags, they are used as
features. The functional tag of the constituent
embedding the problem in Figure 1 is DIR.
</bodyText>
<listItem confidence="0.990812">
6. Other determiners of the NP: We looked
at the presence of a determiner in the NP. By
definition, an NP in the Penn Treebank can only
</listItem>
<page confidence="0.992699">
44
</page>
<figure confidence="0.77143456">
Functional Marks:
Tag (ft) Text categories
HLN headlines and datelines
LST list markers
TTL titles
Grammatical functions
CLF true clefts
NOM non NPs that function as NPs
ADV clausal and NP adverbials
LGS logical subjects in passives
PRD nonVP predicates
SUBJ surface subject
TPC topicalized/fronted constituents
CLR closely related
BNF beneficiary of action
DTV dative object
Semantic roles
VOC vocatives
DIR direction and trajectory
LOC location
MNR manner
PRP purpose and reason
TMP temporal phrases
PUT locative complement of put
EXT spatial extent of activity
</figure>
<tableCaption confidence="0.5668575">
Table 2: Functional tags and their mean-
ing (Santorini, 1990)
</tableCaption>
<bodyText confidence="0.999891745454546">
have one determiner (Bies et al., 1995), so we
expect it to be a good predictor of situations
where we should not generate an article.
7. Head countability preferences of the head
of the NP: In case the head of an NP is a noun
we also use its countability as a feature. We an-
ticipate that this is a useful feature because sin-
gular indefinite countable nouns normally take
the article a/n, whereas singular indefinite un-
countable nouns normally take no article: a dog
vs water. We looked up the countability from
the transfer lexicon used in the Japanese-to-
English machine translation system ALT-J/E
(Ikehara et al., 1991). We used six values for
the countability feature: FC (fully countable) for
nouns that have both singular and plural forms
and can be directly modified by numerals and
modifiers such as many; UC (uncountable) for
nouns that have no plural form and can be mod-
ified by much; Sc (strongly countable) for nouns
that are more often countable than uncount-
able; WC (weakly countable) for nouns that are
more often uncountable than countable; and PT
(pluralia tantum) for nouns that only have plu-
ral forms, such as for example, scissors (Bond
et al., 1994). Finally, we used the value UNKNOWN
if the lexicon did not provide countability infor-
mation for a noun or if the head of the NP was
not a noun. 41.4% of the NP instances received
the value UNKNOWN for this feature.
8. Semantic classes of the head of the NP: If
the head of the NP is a noun we also take into
account its semantic classification in a large se-
mantic hierarchy. The underlying idea is that
the semantic class of the noun can be used as a
way to back off in case of unknown head nouns.
The 2,710 node semantic hierarchy we used was
also developed in the context of the ALT-J/E
system (Ikehara et al., 1991). Edges in this hi-
erarchy represent IS-A or HAS-A relationships.
In case the semantic classes associated with two
nodes stand in the IS-A relation, the semantic
class associated with the node highest in the hi-
erarchy subsumes the semantic class associated
with the other node.
Each of the nodes in this part of the hierarchy
is represented by a boolean feature which is set
to 1 if that node lies on the path from the root
of the hierarchy to a particular semantic class.
Thus, for example, the semantic features of a
noun in the semantic class organization con-
sists of a vector of 30 features where the features
corresponding to the nodes noun, concrete,
agent and organization are set to 1 and all
other features are set to 0.2
</bodyText>
<sectionHeader confidence="0.999349" genericHeader="method">
4 Memory-based learning
</sectionHeader>
<bodyText confidence="0.9999668">
We used the Tilburg memory based learner
TiMBL 3.0.1 (Daelemans et al., 2000) to learn
from examples for generating articles using the
features discussed above. Memory-based learn-
ing reads all training instances into memory and
classifies test instances by extrapolating a class
from the most similar instance(s) in memory.
Daelemans et al. (1999) have shown that
for typical natural language tasks, this ap-
proach has the advantage that it also extrap-
olates from exceptional and low-frequency in-
stances. In addition, as a result of automat-
ically weighing features in the similarity func-
tion used to determine the class of a test in-
stance, it allows the user to incorporate large
</bodyText>
<footnote confidence="0.882553">
2If a noun has multiple senses, we collapse them by
taking the semantic classes of a noun to be the union of
the semantic classes of all its senses.
</footnote>
<page confidence="0.998635">
45
</page>
<bodyText confidence="0.9935595">
numbers of features from heterogeneous sources:
When data is sparse, feature weighing embod-
ies a smoothing-by-similarity effect (Zavrel and
Daelemans, 1997).
</bodyText>
<sectionHeader confidence="0.964052" genericHeader="evaluation">
5 Evaluation and Discussion
</sectionHeader>
<bodyText confidence="0.987626777777778">
We tested the features discussed in section 3
with respect to a number of different memory-
based learning methods as implemented in the
TiMBL system (Daelemans et al., 2000).
We considered two different learning algo-
rithms. The first, ml is a k-nearest neighbour
algorithm.3. This can be used with two differ-
ent metrics to judge the distance between the
examples: overlap and modified value difference
metric (MVDM). TiMBL automatically learns
weights for the features, using one of five dif-
ferent weighting methods: no weighting, gain
ratio, information gain, chi-squared and shared
variance. The second algorithm, IGTREE, stores
examples in a tree which is pruned according
to the weightings. This makes it much faster
and of comparable accuracy. The results for
these different methods, for k = 1, 4, 16 are dis-
played in Table 3. ml is tested with leave-one-
out cross-validation, IGTREE with ten-fold cross
validation.
The best results were (82.6%) for ml with
the MVDM metric, and either no weighting or
weighting by gain ratio. IGTREE did not per-
form as well. We investigated more values of k,
from 1 to 200, and found they had little influ-
ence on the accuracy results with k = 4 or 5
performing slightly better.
We also tested each of the features described
in Section 3 in isolation and then all together.
We used the best performing algorithm from our
earlier experiment: m 1 with MVDM, gain ratio
and k = 4. The results of this are given in
Table 4.
When interpreting these results it is impor-
tant to recall the figures provided in Table 1.
The most common article, for any PoS, was no
and for many PoS, including pronouns, gener-
ating no article is always correct. There is more
variation in NPs headed by common nouns and
adjectives, and a little in NPs headed by proper
nouns. Our baseline therefore consists of never
3Strictly speaking, it is a k nearest distance algo-
rithm, which looks at all examples in the nearest k dis-
tances, the number of which may be greater than k.
</bodyText>
<subsectionHeader confidence="0.800167">
Feature Accuracy
</subsectionHeader>
<bodyText confidence="0.595321444444444">
head 80.3%
head&apos;s part-of-speech 70.0%
NP&apos;s functional tag 70.5%
embedding category 70.0%
embedding functional tag 70.0%
determiner present or not 70.0%
head&apos;s countability 70.0%
head&apos;s semantic classes 72.9%
hline
</bodyText>
<tableCaption confidence="0.993823">
Table 4: Accuracy results by feature
</tableCaption>
<bodyText confidence="0.997529047619047">
generating an article: this will be right in 70.0%
of all cases.
Looking at the figures in Table 4, we see that
many of the features investigated did not im-
prove results above the baseline. Using the head
of the NP itself to predict the article gave the
best results of any single feature, raising the ac-
curacy to 79.4%. The functional tag of the head
of the NP itself improved results slightly. The
use of the semantic classes (72.1%) clearly im-
proves the results over the baseline thereby indi-
cating that they capture useful generalizations.
The results from testing the features in com-
bination are shown in Table 5. Interestingly,
features which were not useful on their own,
proved useful in combination with the head
noun. The most useful features appear to be the
category of the embedding constituent (81.1%)
and the presence or absence of a determiner
(80.9%). Combining all the features gave an
accuracy of 82.9%.
</bodyText>
<subsectionHeader confidence="0.99204">
Feature Accuracy
</subsectionHeader>
<bodyText confidence="0.889853111111111">
head+its part-of-speech 80.8%
head+functional tag of NP 81.1%
head+embedding category 80.8%
head+embedding functional tag 81.4%
head+determiner present or not 81.7%
head+countability 80.8%
head+semantic classes 80.8%
hline all features 83.6%
all features-semantic classes 83.6%
</bodyText>
<tableCaption confidence="0.977152">
Table 5: Accuracy with combined features
</tableCaption>
<bodyText confidence="0.9999495">
Our best results (82.6%), which used all fea-
tures are significantly better than the baseline
of generating no articles (70.0%) or using only
the head of the NP for training (79.4%). We
</bodyText>
<page confidence="0.998939">
46
</page>
<table confidence="0.999906111111111">
Algorithm None Gain ratio Feature Weighting Shared variance
k Information gain x2
IB 1 1 83.5% 83.5% 83.3% 83.2% 83.3%
(MVDM) 4 83.5% 83.6% 83.3% 83.3% 83.3%
16 83.6% 83.5% 83.2% 83.2% 83.2%
ml 1 83.1% 83.5% 83.3% 83.2% 83.3%
(overlap) 4 82.9% 83.1% 83.1% 83.1% 83.1%
16 82.9% 83.0% 82.9% 82.9% 82.9%
IGTREE 82.9% 82.5% 82.4% 82.6%
</table>
<tableCaption confidence="0.999848">
Table 3: Accuracy results broken down with respect to memory-based learning methods used
</tableCaption>
<bodyText confidence="0.999779157894737">
also improve significantly upon earlier results of
78% as reported by Knight and Chander (1994),
which in any case is a simpler task since it only
involved choice between the and a/an. Further,
our results are competitive with state of the art
rule-based systems. Because different corpora
are used to obtain the various results reported
in the literature and the problem is often de-
fined differently, detailed comparison is difficult.
However, the accuracy achieved appears to ap-
proach the accuracy results achieved with hand-
written rules.
In order to test the effect of the size of the
training data, we tested used the best perform-
ing algorithm from our earlier experiment (ml
with MVDM, gain ratio and k = 4) on various
subsets of the corpus: the first 10%, the first
20%, the first 30% and so on to the whole cor-
pus. The results are given in Table 6.
</bodyText>
<table confidence="0.989628666666667">
Size Accuracy
10% 80.95%
20% 81.67%
30% 82.14%
40% 82.45%
50% 82.69%
60% 83.04%
70% 83.17%
80% 83.24%
90% 83.45%
100% 83.58%
(100% is 300,744 NPs)
</table>
<tableCaption confidence="0.999178">
Table 6: Accuracy versus Size of Training Data
</tableCaption>
<bodyText confidence="0.9991348">
The accuracy is still improving even with
300,744 NPs, an even larger corpus should give
even better results. It is important to keep in
mind that we, like most other researchers, have
been training and testing on a relatively homo-
geneous corpus. Furthermore, we took as given
information about the number of the NP. In
many applications we will have neither a large
amount of homogeneous training data nor infor-
mation about number.
</bodyText>
<subsectionHeader confidence="0.739103">
5.1 Future Work
</subsectionHeader>
<bodyText confidence="0.99994159375">
In the near future we intend to further ex-
tend our approach in various directions. First,
we plan to investigate other lexical and syn-
tactic features that might further improve our
results, such as the existence of pre-modifiers
like superlative and comparative adjectives, and
post-modifiers like prepositional phrases, rela-
tive clauses, and so on. We would also like to in-
vestigate the effect of additional discourse-based
features such as one that incorporates informa-
tion about whether the referent of a noun phrase
has been mentioned before.
Second, we intend to make sure that the fea-
tures we are using in training and testing will
be available in the applications we consider. For
example, in machine translation, the input noun
phrase may be all dogs, whereas the output
could be either all dogs or all the dogs. At
present, words such as all, both, half in our in-
put are tagged as pre-determiners if there is a
following determiner (it can only be the or a
possessive), and determiners if there is no arti-
cle. To train for a realistic application we need
to collapse the determiner and pre-determiner
inputs together in our training data.
Furthermore, we are interested in training
on corpora with less markup, like the British
National Corpus (Burnard, 1995) or even no
markup at all. By running a PoS tagger and
then an NP chunker, we should be able to get
a lot more training data, and thus significantly
improve our coverage. If we can use plain text
</bodyText>
<page confidence="0.997699">
47
</page>
<bodyText confidence="0.999647666666667">
to train on, then it will be easier to adapt our
tool quickly to new domains, for which there are
unlikely to be fully marked up corpora.
</bodyText>
<sectionHeader confidence="0.982698" genericHeader="conclusions">
6 Concluding remarks
</sectionHeader>
<bodyText confidence="0.9999595">
We described a memory-based approach to au-
tomated article generation that uses a variety of
lexical, syntactic and semantic features as pro-
vided by the Penn Treebank Wall Street Jour-
nal data and a large hand-encoded MT dictio-
nary. With this approach we achieve an accu-
racy of 82.6%. We believe that this approach
is an encouraging first step towards a statistical
device for automated article generation that can
be used in a range of applications such as speech
prosthesis, machine translation and automated
summarization.
</bodyText>
<sectionHeader confidence="0.998369" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999298">
The authors would like to thank the Stanford NLP
reading group, the LinG0 project at CSLI, Timo-
thy Baldwin, Kevin Knight, Chris Manning, Wal-
ter Daelemans and two anonymous reviewers for
their helpful comments. This project is in part sup-
ported by the National Science Foundation under
grant number IRI-9612682.
</bodyText>
<sectionHeader confidence="0.999149" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999916049382716">
Ann Bies, Mark Fergusona, Karen Katz, and Robert
MacIntyre, 1995. Bracketing Guidelines for Tree-
bank II Style. Penn Treebank Project, University
of Pennsylvania.
Francis Bond and Kentaro Ogura. 1998. Reference
in Japanese-to-English machine translation. Ma-
chine Translation, 13(2-3):107-134.
Francis Bond, Kentaro Ogura, and Satoru Ikehara.
1994. Countability and number in Japanese-to-
English machine translation. In 15th Interna-
tional Conference on Computational Linguistics:
COLING-94, pages 32-38, Kyoto. (http: //xxx.
lanl.gov/abs/cmp-lg/9511001).
Lou Burnard. 1995. User reference guide for the
British National Corpus. Technical report, Ox-
ford University Computing Services.
John Carroll, Ann Copestake, Dan Flickinger, and
Victor Poznanski. 1999. An efficient chart gen-
erator for (semi-)lexicalist grammars. In Proceed-
ings of the 7th European Workshop on Natural
Language Generation (EWNLG&apos;99), pages 86-95,
Toulouse, France.
Ann Copestake. 1997. Augmented and alternative
NLP techniques for augmentative and alternative
communication. In Proceedings of the ACL work-
shop on Natural Language Processing for Commu-
nication Aids, pages 37-42, Madrid.
Walter Daelemans, Antal van den Bosch, and Jakub
Zavrel. 1999. Forgetting exceptions is harmful in
language learning. Machine Learning, 34.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot,
and Antal van den Bosch. 2000. TiMBL: Tilburg
memory based learner, version 3.0, reference
guide. ILK Technical Report 00-01, ILK, Tilburg,
The Netherlands. (ILK-0001; http: //ilk .kub.
nl).
Barbara Gawroriska. 1990. &amp;quot;Translation Great
Problem&amp;quot; on the problem of inserting articles
when translating from Russian into Swedish. In
13th International Conference on Computational
Linguistics: COLING-90, Helsinki.
Julia E. Heine. 1998. Definiteness predictions for
Japanese noun phrases. In 36th Annual Meeting
of the Association for Computational Linguistics
and 17th International Conference on Computa-
tional Linguistics: COLING/ACL-98, pages 519-
525, Montreal, Canada.
Satoru Ikehara, Satoshi Shirai, Akio Yokoo, and Hi-
romi Nakaiwa. 1991. Toward an MT system with-
out pre-editing - effects of new methods in ALT-
J/E-. In Third Machine Translation Summit:
MT Summit III, pages 101-106, Washington DC.
(http: //xxx . lanl .gov/abs/cmp-1g/9510008).
Kevin Knight and Ishwar Chander. 1994. Au-
tomated postediting of documents. In Proceed-
ings of the 12th National Conference on Artificial
Intelligence: AAAI-94, pages 779-784, Seattle.
(http: //xxx.lanl.gov/abs/cmp-lg/9407028).
Guido Minnen, John Carroll, and Darren Pearce.
2000. Robust, applied morphological generation.
In Proceedings of the first International Natural
Language Genration Conference, Mitzpe Ramon,
Israel.
Masaki Murata and Makoto Nagao. 1993. Deter-
mination of referential property and number of
nouns in Japanese sentences for machine transla-
tion into English. In Fifth International Confer-
ence on Theoretical and Methodological Issues in
Machine Translation: TMI-93, pages 218-25, Ky-
oto, July. (http://xxx.lanl.gov/abs/cmp-lg/
9405019).
Beatrice Santorini. 1990. Part-of-speech tagging
guidelines for the Penn Treebank Project. Tech-
nical Report MS-CIS-90-47, Department of Com-
puter and Information Science, University of
Pennsylvania.
Jakub Zavrel and Walter Daelemans. 1997.
Memory-based learning: Using similarity for
smoothing. In Proceedings of the 35th Annual
Meeting of the Association for Computational
Linguistics, Madrid, Spain.
</reference>
<page confidence="0.999353">
48
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.359167">
<note confidence="0.958377">of CoNLL-2000 and LLL-2000, 43-48, Lisbon, Portugal, 2000.</note>
<title confidence="0.998952">Memory-Based Learning for Article Generation</title>
<author confidence="0.999651">Guido Minnen Francis Bondt</author>
<affiliation confidence="0.999285">Cognitive and Computing Sciences MT Research Group University of Sussex NTT Communication Science</affiliation>
<address confidence="0.996825">9QH, Brighton, UK 2-4 Hikari-dai, Kyoto 619-0237, JAPAN</address>
<author confidence="0.9182895">co</author>
<affiliation confidence="0.76597">Stanford</affiliation>
<address confidence="0.786065">Stanford CA 94305-2150,</address>
<email confidence="0.50029">.ordedu</email>
<abstract confidence="0.9991599375">Article choice can pose difficult problems in applications such as machine translation and automated summarization. In this paper, we investigate the use of corpus data to collect statistical generalizations about article use in English in order to be able to generate articles automatically to supplement a symbolic generator. We use data from the Penn Treebank as input to a memory-based learner (TiMBL 3.0; Daelemans et al., 2000) which predicts whether to generate an article with respect to an English base noun phrase. We discuss competitive results obtained using a variety of lexical, syntactic and semantic features that play an important role in automated article generation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ann Bies</author>
<author>Mark Fergusona</author>
<author>Karen Katz</author>
<author>Robert MacIntyre</author>
</authors>
<title>Bracketing Guidelines for Treebank II Style. Penn Treebank Project,</title>
<date>1995</date>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="5433" citStr="Bies et al., 1995" startWordPosition="889" endWordPosition="892">e or a/an. They do not consider the possibility that no article should be generated. On the basis of a corpus of 400K NP instances derived from the Wall Street Journal, they construct decision trees for the 1,600 most frequent nouns by considering over 30,000 lexical, syntactic and semantic features. They achieve an accuracy of 81% with respect to these nouns. By guessing the for the remainder of the nouns, they achieve an overall accuracy of 78%. 3 Features Determining Automated Article Generation We have extracted 300K base noun phrases (NPs) from the Penn Treebank Wall Street Journal data (Bies et al., 1995) using the tgrep tool. The distribution of these NP instances with respect to articles is as follows: the 20.6%, a/an 9.4% and 70.0% with no article. We experimented with a range of features: 1. Head of the NP: We consider as the head of the NP the rightmost noun in the NP. If an NP does not contain a noun, we take the last word in the NP as its head. 2. Part-of-speech (PoS) tag of the head of the NP: PoS labels were taken from the Penn Treebank. We list the tags that occurred with (PP-DIR to/TO (NP the/DT problem/NN)) Figure 1: An example of a prepositional phrase annotated with a functional </context>
<context position="8077" citStr="Bies et al., 1995" startWordPosition="1381" endWordPosition="1384"> headlines and datelines LST list markers TTL titles Grammatical functions CLF true clefts NOM non NPs that function as NPs ADV clausal and NP adverbials LGS logical subjects in passives PRD nonVP predicates SUBJ surface subject TPC topicalized/fronted constituents CLR closely related BNF beneficiary of action DTV dative object Semantic roles VOC vocatives DIR direction and trajectory LOC location MNR manner PRP purpose and reason TMP temporal phrases PUT locative complement of put EXT spatial extent of activity Table 2: Functional tags and their meaning (Santorini, 1990) have one determiner (Bies et al., 1995), so we expect it to be a good predictor of situations where we should not generate an article. 7. Head countability preferences of the head of the NP: In case the head of an NP is a noun we also use its countability as a feature. We anticipate that this is a useful feature because singular indefinite countable nouns normally take the article a/n, whereas singular indefinite uncountable nouns normally take no article: a dog vs water. We looked up the countability from the transfer lexicon used in the Japanese-toEnglish machine translation system ALT-J/E (Ikehara et al., 1991). We used six valu</context>
</contexts>
<marker>Bies, Fergusona, Katz, MacIntyre, 1995</marker>
<rawString>Ann Bies, Mark Fergusona, Karen Katz, and Robert MacIntyre, 1995. Bracketing Guidelines for Treebank II Style. Penn Treebank Project, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francis Bond</author>
<author>Kentaro Ogura</author>
</authors>
<title>Reference in Japanese-to-English machine translation.</title>
<date>1998</date>
<journal>Machine Translation,</journal>
<pages>13--2</pages>
<contexts>
<context position="1554" citStr="Bond and Ogura, 1998" startWordPosition="237" endWordPosition="240"> whether to generate an article with respect to an English base noun phrase. We discuss competitive results obtained using a variety of lexical, syntactic and semantic features that play an important role in automated article generation. 1 Introduction Article choice can pose difficult problems in natural language applications. Machine translation (MT) is an example of such an application. When translating from a source language that lacks articles, such as Japanese or Russian, to one that requires them, such as English or German, the system must somehow generate the source language articles (Bond and Ogura, 1998). Similarly in automated summarization: when sentences or fragments are combined or reduced, it is possible that the form of a noun phrase (NP) is changed such that a change of the article associated with the NP&apos;s head becomes necessary. For example, consider the sentences A talk will be given on Friday about NLP; The talk will last for one hour which might get summarized as Friday&apos;s NLP talk will last one hour. However, given the input sentences, it is not clear how to decide not to generate an article for the subject NP in the output sentence. Another important application is in the field kn</context>
<context position="4446" citStr="Bond and Ogura, 1998" startWordPosition="725" endWordPosition="728">scribed in Minnen et al. (2000). 43 play an important role in automated article generation, and compare our results with other researchers&apos;. The paper is structured as follows. Section 2 relates our work to that of others. Section 3 introduces the features we use. Section 4 introduces the learning method we use. We discuss our results in Section 5 and suggest some directions for future research, then conclude with some final remarks in Section 6. 2 Related Work There has been considerable research on generating articles in machine translation systems (Gawroriska, 1990; Murata and Nagao, 1993; Bond and Ogura, 1998; Heine, 1998). These systems use hand-written rules and lexical information to generate articles. The best cited results, 88% accuracy, are quoted by Heine (1998) which were obtained with respect to a very small corpus of 1,000 sentences in a restricted domain. Knight and Chander (1994) present an approach that uses decision trees to determine whether to generate the or a/an. They do not consider the possibility that no article should be generated. On the basis of a corpus of 400K NP instances derived from the Wall Street Journal, they construct decision trees for the 1,600 most frequent noun</context>
</contexts>
<marker>Bond, Ogura, 1998</marker>
<rawString>Francis Bond and Kentaro Ogura. 1998. Reference in Japanese-to-English machine translation. Machine Translation, 13(2-3):107-134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francis Bond</author>
<author>Kentaro Ogura</author>
<author>Satoru Ikehara</author>
</authors>
<title>Countability and number in Japanese-toEnglish machine translation.</title>
<date>1994</date>
<booktitle>In 15th International Conference on Computational Linguistics: COLING-94,</booktitle>
<pages>32--38</pages>
<note>Kyoto. (http: //xxx. lanl.gov/abs/cmp-lg/9511001).</note>
<contexts>
<context position="9207" citStr="Bond et al., 1994" startWordPosition="1577" endWordPosition="1580">se-toEnglish machine translation system ALT-J/E (Ikehara et al., 1991). We used six values for the countability feature: FC (fully countable) for nouns that have both singular and plural forms and can be directly modified by numerals and modifiers such as many; UC (uncountable) for nouns that have no plural form and can be modified by much; Sc (strongly countable) for nouns that are more often countable than uncountable; WC (weakly countable) for nouns that are more often uncountable than countable; and PT (pluralia tantum) for nouns that only have plural forms, such as for example, scissors (Bond et al., 1994). Finally, we used the value UNKNOWN if the lexicon did not provide countability information for a noun or if the head of the NP was not a noun. 41.4% of the NP instances received the value UNKNOWN for this feature. 8. Semantic classes of the head of the NP: If the head of the NP is a noun we also take into account its semantic classification in a large semantic hierarchy. The underlying idea is that the semantic class of the noun can be used as a way to back off in case of unknown head nouns. The 2,710 node semantic hierarchy we used was also developed in the context of the ALT-J/E system (Ik</context>
</contexts>
<marker>Bond, Ogura, Ikehara, 1994</marker>
<rawString>Francis Bond, Kentaro Ogura, and Satoru Ikehara. 1994. Countability and number in Japanese-toEnglish machine translation. In 15th International Conference on Computational Linguistics: COLING-94, pages 32-38, Kyoto. (http: //xxx. lanl.gov/abs/cmp-lg/9511001).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lou Burnard</author>
</authors>
<title>User reference guide for the British National Corpus.</title>
<date>1995</date>
<tech>Technical report,</tech>
<institution>Oxford University Computing Services.</institution>
<contexts>
<context position="18485" citStr="Burnard, 1995" startWordPosition="3155" endWordPosition="3156">the applications we consider. For example, in machine translation, the input noun phrase may be all dogs, whereas the output could be either all dogs or all the dogs. At present, words such as all, both, half in our input are tagged as pre-determiners if there is a following determiner (it can only be the or a possessive), and determiners if there is no article. To train for a realistic application we need to collapse the determiner and pre-determiner inputs together in our training data. Furthermore, we are interested in training on corpora with less markup, like the British National Corpus (Burnard, 1995) or even no markup at all. By running a PoS tagger and then an NP chunker, we should be able to get a lot more training data, and thus significantly improve our coverage. If we can use plain text 47 to train on, then it will be easier to adapt our tool quickly to new domains, for which there are unlikely to be fully marked up corpora. 6 Concluding remarks We described a memory-based approach to automated article generation that uses a variety of lexical, syntactic and semantic features as provided by the Penn Treebank Wall Street Journal data and a large hand-encoded MT dictionary. With this a</context>
</contexts>
<marker>Burnard, 1995</marker>
<rawString>Lou Burnard. 1995. User reference guide for the British National Corpus. Technical report, Oxford University Computing Services.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
<author>Victor Poznanski</author>
</authors>
<title>An efficient chart generator for (semi-)lexicalist grammars.</title>
<date>1999</date>
<booktitle>In Proceedings of the 7th European Workshop on Natural Language Generation (EWNLG&apos;99),</booktitle>
<pages>86--95</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="3241" citStr="Carroll et al., 1999" startWordPosition="522" endWordPosition="525"> able to use a keyboard. Many have to rely on a slower physical interface (headstick, head-pointer, eye-tracker etc). We are attempting to use a range of NLP technology to improve text input speed for such users. Article choice is particularly important for this application: many AAC users drop articles and resort to a sort of telegraphese, but this causes degradation in comprehension of synthetic speech and contributes to its perception as unnatural and robot-like. Our particular goal is to be able to use an article generator in conjunction with a symbolic generator for AAC (Copestake, 1997; Carroll et al., 1999). In this paper we investigate the use of corpus data to collect statistical generalizations about article use in English so as to be able to generate them automatically. We use data from the Penn Treebank as input to a memory-based learner (TiMBL 3.0; Daelemans et al., 2000) that is used to predict whether to generate the or a/an or no article.&apos; We discuss a variety of lexical, syntactic and semantic features that Visiting CSLI, Stanford University (2000). 1We assume a postprocessor to determine whether to t Visiting CSLI, Stanford University (1999-2000). generate a or an as described in Minn</context>
</contexts>
<marker>Carroll, Copestake, Flickinger, Poznanski, 1999</marker>
<rawString>John Carroll, Ann Copestake, Dan Flickinger, and Victor Poznanski. 1999. An efficient chart generator for (semi-)lexicalist grammars. In Proceedings of the 7th European Workshop on Natural Language Generation (EWNLG&apos;99), pages 86-95, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
</authors>
<title>Augmented and alternative NLP techniques for augmentative and alternative communication.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL workshop on Natural Language Processing for Communication Aids,</booktitle>
<pages>37--42</pages>
<location>Madrid.</location>
<contexts>
<context position="3218" citStr="Copestake, 1997" startWordPosition="520" endWordPosition="521"> even if they are able to use a keyboard. Many have to rely on a slower physical interface (headstick, head-pointer, eye-tracker etc). We are attempting to use a range of NLP technology to improve text input speed for such users. Article choice is particularly important for this application: many AAC users drop articles and resort to a sort of telegraphese, but this causes degradation in comprehension of synthetic speech and contributes to its perception as unnatural and robot-like. Our particular goal is to be able to use an article generator in conjunction with a symbolic generator for AAC (Copestake, 1997; Carroll et al., 1999). In this paper we investigate the use of corpus data to collect statistical generalizations about article use in English so as to be able to generate them automatically. We use data from the Penn Treebank as input to a memory-based learner (TiMBL 3.0; Daelemans et al., 2000) that is used to predict whether to generate the or a/an or no article.&apos; We discuss a variety of lexical, syntactic and semantic features that Visiting CSLI, Stanford University (2000). 1We assume a postprocessor to determine whether to t Visiting CSLI, Stanford University (1999-2000). generate a or </context>
</contexts>
<marker>Copestake, 1997</marker>
<rawString>Ann Copestake. 1997. Augmented and alternative NLP techniques for augmentative and alternative communication. In Proceedings of the ACL workshop on Natural Language Processing for Communication Aids, pages 37-42, Madrid.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Antal van den Bosch</author>
<author>Jakub Zavrel</author>
</authors>
<title>Forgetting exceptions is harmful in language learning.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>34</volume>
<marker>Daelemans, van den Bosch, Zavrel, 1999</marker>
<rawString>Walter Daelemans, Antal van den Bosch, and Jakub Zavrel. 1999. Forgetting exceptions is harmful in language learning. Machine Learning, 34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Jakub Zavrel</author>
<author>Ko van der Sloot</author>
<author>Antal van den Bosch</author>
</authors>
<title>TiMBL: Tilburg memory based learner, version 3.0, reference guide. ILK</title>
<date>2000</date>
<tech>Technical Report 00-01,</tech>
<location>ILK, Tilburg, The</location>
<note>http: //ilk .kub. nl).</note>
<marker>Daelemans, Zavrel, van der Sloot, van den Bosch, 2000</marker>
<rawString>Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and Antal van den Bosch. 2000. TiMBL: Tilburg memory based learner, version 3.0, reference guide. ILK Technical Report 00-01, ILK, Tilburg, The Netherlands. (ILK-0001; http: //ilk .kub. nl).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Gawroriska</author>
</authors>
<title>Translation Great Problem&amp;quot; on the problem of inserting articles when translating from Russian into Swedish.</title>
<date>1990</date>
<booktitle>In 13th International Conference on Computational Linguistics: COLING-90,</booktitle>
<location>Helsinki.</location>
<contexts>
<context position="4400" citStr="Gawroriska, 1990" startWordPosition="719" endWordPosition="720">ersity (1999-2000). generate a or an as described in Minnen et al. (2000). 43 play an important role in automated article generation, and compare our results with other researchers&apos;. The paper is structured as follows. Section 2 relates our work to that of others. Section 3 introduces the features we use. Section 4 introduces the learning method we use. We discuss our results in Section 5 and suggest some directions for future research, then conclude with some final remarks in Section 6. 2 Related Work There has been considerable research on generating articles in machine translation systems (Gawroriska, 1990; Murata and Nagao, 1993; Bond and Ogura, 1998; Heine, 1998). These systems use hand-written rules and lexical information to generate articles. The best cited results, 88% accuracy, are quoted by Heine (1998) which were obtained with respect to a very small corpus of 1,000 sentences in a restricted domain. Knight and Chander (1994) present an approach that uses decision trees to determine whether to generate the or a/an. They do not consider the possibility that no article should be generated. On the basis of a corpus of 400K NP instances derived from the Wall Street Journal, they construct d</context>
</contexts>
<marker>Gawroriska, 1990</marker>
<rawString>Barbara Gawroriska. 1990. &amp;quot;Translation Great Problem&amp;quot; on the problem of inserting articles when translating from Russian into Swedish. In 13th International Conference on Computational Linguistics: COLING-90, Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia E Heine</author>
</authors>
<title>Definiteness predictions for Japanese noun phrases.</title>
<date>1998</date>
<booktitle>In 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics: COLING/ACL-98,</booktitle>
<pages>519--525</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="4460" citStr="Heine, 1998" startWordPosition="729" endWordPosition="730">l. (2000). 43 play an important role in automated article generation, and compare our results with other researchers&apos;. The paper is structured as follows. Section 2 relates our work to that of others. Section 3 introduces the features we use. Section 4 introduces the learning method we use. We discuss our results in Section 5 and suggest some directions for future research, then conclude with some final remarks in Section 6. 2 Related Work There has been considerable research on generating articles in machine translation systems (Gawroriska, 1990; Murata and Nagao, 1993; Bond and Ogura, 1998; Heine, 1998). These systems use hand-written rules and lexical information to generate articles. The best cited results, 88% accuracy, are quoted by Heine (1998) which were obtained with respect to a very small corpus of 1,000 sentences in a restricted domain. Knight and Chander (1994) present an approach that uses decision trees to determine whether to generate the or a/an. They do not consider the possibility that no article should be generated. On the basis of a corpus of 400K NP instances derived from the Wall Street Journal, they construct decision trees for the 1,600 most frequent nouns by consideri</context>
</contexts>
<marker>Heine, 1998</marker>
<rawString>Julia E. Heine. 1998. Definiteness predictions for Japanese noun phrases. In 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics: COLING/ACL-98, pages 519-525, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoru Ikehara</author>
<author>Satoshi Shirai</author>
<author>Akio Yokoo</author>
<author>Hiromi Nakaiwa</author>
</authors>
<title>Toward an MT system without pre-editing - effects of new methods in ALTJ/E-.</title>
<date>1991</date>
<booktitle>In Third Machine Translation Summit: MT Summit III,</booktitle>
<pages>101--106</pages>
<location>Washington</location>
<contexts>
<context position="8659" citStr="Ikehara et al., 1991" startWordPosition="1483" endWordPosition="1486">have one determiner (Bies et al., 1995), so we expect it to be a good predictor of situations where we should not generate an article. 7. Head countability preferences of the head of the NP: In case the head of an NP is a noun we also use its countability as a feature. We anticipate that this is a useful feature because singular indefinite countable nouns normally take the article a/n, whereas singular indefinite uncountable nouns normally take no article: a dog vs water. We looked up the countability from the transfer lexicon used in the Japanese-toEnglish machine translation system ALT-J/E (Ikehara et al., 1991). We used six values for the countability feature: FC (fully countable) for nouns that have both singular and plural forms and can be directly modified by numerals and modifiers such as many; UC (uncountable) for nouns that have no plural form and can be modified by much; Sc (strongly countable) for nouns that are more often countable than uncountable; WC (weakly countable) for nouns that are more often uncountable than countable; and PT (pluralia tantum) for nouns that only have plural forms, such as for example, scissors (Bond et al., 1994). Finally, we used the value UNKNOWN if the lexicon </context>
</contexts>
<marker>Ikehara, Shirai, Yokoo, Nakaiwa, 1991</marker>
<rawString>Satoru Ikehara, Satoshi Shirai, Akio Yokoo, and Hiromi Nakaiwa. 1991. Toward an MT system without pre-editing - effects of new methods in ALTJ/E-. In Third Machine Translation Summit: MT Summit III, pages 101-106, Washington DC. (http: //xxx . lanl .gov/abs/cmp-1g/9510008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Ishwar Chander</author>
</authors>
<title>Automated postediting of documents.</title>
<date>1994</date>
<booktitle>In Proceedings of the 12th National Conference on Artificial Intelligence: AAAI-94,</booktitle>
<pages>779--784</pages>
<location>Seattle. (http:</location>
<contexts>
<context position="4734" citStr="Knight and Chander (1994)" startWordPosition="772" endWordPosition="775">es the learning method we use. We discuss our results in Section 5 and suggest some directions for future research, then conclude with some final remarks in Section 6. 2 Related Work There has been considerable research on generating articles in machine translation systems (Gawroriska, 1990; Murata and Nagao, 1993; Bond and Ogura, 1998; Heine, 1998). These systems use hand-written rules and lexical information to generate articles. The best cited results, 88% accuracy, are quoted by Heine (1998) which were obtained with respect to a very small corpus of 1,000 sentences in a restricted domain. Knight and Chander (1994) present an approach that uses decision trees to determine whether to generate the or a/an. They do not consider the possibility that no article should be generated. On the basis of a corpus of 400K NP instances derived from the Wall Street Journal, they construct decision trees for the 1,600 most frequent nouns by considering over 30,000 lexical, syntactic and semantic features. They achieve an accuracy of 81% with respect to these nouns. By guessing the for the remainder of the nouns, they achieve an overall accuracy of 78%. 3 Features Determining Automated Article Generation We have extract</context>
<context position="15813" citStr="Knight and Chander (1994)" startWordPosition="2696" endWordPosition="2699"> the baseline of generating no articles (70.0%) or using only the head of the NP for training (79.4%). We 46 Algorithm None Gain ratio Feature Weighting Shared variance k Information gain x2 IB 1 1 83.5% 83.5% 83.3% 83.2% 83.3% (MVDM) 4 83.5% 83.6% 83.3% 83.3% 83.3% 16 83.6% 83.5% 83.2% 83.2% 83.2% ml 1 83.1% 83.5% 83.3% 83.2% 83.3% (overlap) 4 82.9% 83.1% 83.1% 83.1% 83.1% 16 82.9% 83.0% 82.9% 82.9% 82.9% IGTREE 82.9% 82.5% 82.4% 82.6% Table 3: Accuracy results broken down with respect to memory-based learning methods used also improve significantly upon earlier results of 78% as reported by Knight and Chander (1994), which in any case is a simpler task since it only involved choice between the and a/an. Further, our results are competitive with state of the art rule-based systems. Because different corpora are used to obtain the various results reported in the literature and the problem is often defined differently, detailed comparison is difficult. However, the accuracy achieved appears to approach the accuracy results achieved with handwritten rules. In order to test the effect of the size of the training data, we tested used the best performing algorithm from our earlier experiment (ml with MVDM, gain</context>
</contexts>
<marker>Knight, Chander, 1994</marker>
<rawString>Kevin Knight and Ishwar Chander. 1994. Automated postediting of documents. In Proceedings of the 12th National Conference on Artificial Intelligence: AAAI-94, pages 779-784, Seattle. (http: //xxx.lanl.gov/abs/cmp-lg/9407028).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guido Minnen</author>
<author>John Carroll</author>
<author>Darren Pearce</author>
</authors>
<title>Robust, applied morphological generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the first International Natural Language Genration Conference,</booktitle>
<location>Mitzpe Ramon,</location>
<contexts>
<context position="3857" citStr="Minnen et al. (2000)" startWordPosition="625" endWordPosition="628">999). In this paper we investigate the use of corpus data to collect statistical generalizations about article use in English so as to be able to generate them automatically. We use data from the Penn Treebank as input to a memory-based learner (TiMBL 3.0; Daelemans et al., 2000) that is used to predict whether to generate the or a/an or no article.&apos; We discuss a variety of lexical, syntactic and semantic features that Visiting CSLI, Stanford University (2000). 1We assume a postprocessor to determine whether to t Visiting CSLI, Stanford University (1999-2000). generate a or an as described in Minnen et al. (2000). 43 play an important role in automated article generation, and compare our results with other researchers&apos;. The paper is structured as follows. Section 2 relates our work to that of others. Section 3 introduces the features we use. Section 4 introduces the learning method we use. We discuss our results in Section 5 and suggest some directions for future research, then conclude with some final remarks in Section 6. 2 Related Work There has been considerable research on generating articles in machine translation systems (Gawroriska, 1990; Murata and Nagao, 1993; Bond and Ogura, 1998; Heine, 19</context>
</contexts>
<marker>Minnen, Carroll, Pearce, 2000</marker>
<rawString>Guido Minnen, John Carroll, and Darren Pearce. 2000. Robust, applied morphological generation. In Proceedings of the first International Natural Language Genration Conference, Mitzpe Ramon, Israel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaki Murata</author>
<author>Makoto Nagao</author>
</authors>
<title>Determination of referential property and number of nouns in Japanese sentences for machine translation into English.</title>
<date>1993</date>
<booktitle>In Fifth International Conference on Theoretical and Methodological Issues in Machine Translation: TMI-93,</booktitle>
<pages>218--25</pages>
<location>Kyoto,</location>
<contexts>
<context position="4424" citStr="Murata and Nagao, 1993" startWordPosition="721" endWordPosition="724">. generate a or an as described in Minnen et al. (2000). 43 play an important role in automated article generation, and compare our results with other researchers&apos;. The paper is structured as follows. Section 2 relates our work to that of others. Section 3 introduces the features we use. Section 4 introduces the learning method we use. We discuss our results in Section 5 and suggest some directions for future research, then conclude with some final remarks in Section 6. 2 Related Work There has been considerable research on generating articles in machine translation systems (Gawroriska, 1990; Murata and Nagao, 1993; Bond and Ogura, 1998; Heine, 1998). These systems use hand-written rules and lexical information to generate articles. The best cited results, 88% accuracy, are quoted by Heine (1998) which were obtained with respect to a very small corpus of 1,000 sentences in a restricted domain. Knight and Chander (1994) present an approach that uses decision trees to determine whether to generate the or a/an. They do not consider the possibility that no article should be generated. On the basis of a corpus of 400K NP instances derived from the Wall Street Journal, they construct decision trees for the 1,</context>
</contexts>
<marker>Murata, Nagao, 1993</marker>
<rawString>Masaki Murata and Makoto Nagao. 1993. Determination of referential property and number of nouns in Japanese sentences for machine translation into English. In Fifth International Conference on Theoretical and Methodological Issues in Machine Translation: TMI-93, pages 218-25, Kyoto, July. (http://xxx.lanl.gov/abs/cmp-lg/ 9405019).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beatrice Santorini</author>
</authors>
<title>Part-of-speech tagging guidelines for the Penn Treebank Project.</title>
<date>1990</date>
<tech>Technical Report MS-CIS-90-47,</tech>
<institution>Department of Computer and Information Science, University of Pennsylvania.</institution>
<contexts>
<context position="8037" citStr="Santorini, 1990" startWordPosition="1376" endWordPosition="1377">al Marks: Tag (ft) Text categories HLN headlines and datelines LST list markers TTL titles Grammatical functions CLF true clefts NOM non NPs that function as NPs ADV clausal and NP adverbials LGS logical subjects in passives PRD nonVP predicates SUBJ surface subject TPC topicalized/fronted constituents CLR closely related BNF beneficiary of action DTV dative object Semantic roles VOC vocatives DIR direction and trajectory LOC location MNR manner PRP purpose and reason TMP temporal phrases PUT locative complement of put EXT spatial extent of activity Table 2: Functional tags and their meaning (Santorini, 1990) have one determiner (Bies et al., 1995), so we expect it to be a good predictor of situations where we should not generate an article. 7. Head countability preferences of the head of the NP: In case the head of an NP is a noun we also use its countability as a feature. We anticipate that this is a useful feature because singular indefinite countable nouns normally take the article a/n, whereas singular indefinite uncountable nouns normally take no article: a dog vs water. We looked up the countability from the transfer lexicon used in the Japanese-toEnglish machine translation system ALT-J/E </context>
</contexts>
<marker>Santorini, 1990</marker>
<rawString>Beatrice Santorini. 1990. Part-of-speech tagging guidelines for the Penn Treebank Project. Technical Report MS-CIS-90-47, Department of Computer and Information Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakub Zavrel</author>
<author>Walter Daelemans</author>
</authors>
<title>Memory-based learning: Using similarity for smoothing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Madrid,</location>
<contexts>
<context position="11567" citStr="Zavrel and Daelemans, 1997" startWordPosition="1991" endWordPosition="1994">hat for typical natural language tasks, this approach has the advantage that it also extrapolates from exceptional and low-frequency instances. In addition, as a result of automatically weighing features in the similarity function used to determine the class of a test instance, it allows the user to incorporate large 2If a noun has multiple senses, we collapse them by taking the semantic classes of a noun to be the union of the semantic classes of all its senses. 45 numbers of features from heterogeneous sources: When data is sparse, feature weighing embodies a smoothing-by-similarity effect (Zavrel and Daelemans, 1997). 5 Evaluation and Discussion We tested the features discussed in section 3 with respect to a number of different memorybased learning methods as implemented in the TiMBL system (Daelemans et al., 2000). We considered two different learning algorithms. The first, ml is a k-nearest neighbour algorithm.3. This can be used with two different metrics to judge the distance between the examples: overlap and modified value difference metric (MVDM). TiMBL automatically learns weights for the features, using one of five different weighting methods: no weighting, gain ratio, information gain, chi-square</context>
</contexts>
<marker>Zavrel, Daelemans, 1997</marker>
<rawString>Jakub Zavrel and Walter Daelemans. 1997. Memory-based learning: Using similarity for smoothing. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, Madrid, Spain.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>