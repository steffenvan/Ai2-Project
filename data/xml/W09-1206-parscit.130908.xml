<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.093600">
<title confidence="0.885791">
Multilingual Semantic Role Labeling
</title>
<author confidence="0.9932">
Anders Bj¨orkelund Love Hafdell Pierre Nugues
</author>
<affiliation confidence="0.998191">
Department of Computer Science, Lund University
</affiliation>
<address confidence="0.906488">
S-221 00 Lund, Sweden
</address>
<email confidence="0.944990333333333">
fte04abj@student.lth.se
love hafdell@hotmail.com
Pierre.Nugues@cs.lth.se
</email>
<sectionHeader confidence="0.996154" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999937476190476">
This paper describes our contribution to the
semantic role labeling task (SRL-only) of the
CoNLL-2009 shared task in the closed chal-
lenge (Haji6 et al., 2009). Our system con-
sists of a pipeline of independent, local clas-
sifiers that identify the predicate sense, the ar-
guments of the predicates, and the argument
labels. Using these local models, we carried
out a beam search to generate a pool of candi-
dates. We then reranked the candidates using
a joint learning approach that combines the lo-
cal models and proposition features.
To address the multilingual nature of the data,
we implemented a feature selection procedure
that systematically explored the feature space,
yielding significant gains over a standard set
of features. Our system achieved the second
best semantic score overall with an average la-
beled semantic F1 of 80.31. It obtained the
best F1 score on the Chinese and German data
and the second best one on English.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999926">
In this paper, we describe a three-stage analysis ap-
proach that uses the output of a dependency parser
and identifies the arguments of the predicates in a
sentence. The first stage consists of a pipeline of
independent classifiers. We carried out the pred-
icate disambiguation with a set of greedy classi-
fiers, where we applied one classifier per predicate
lemma. We then used a beam search to identify
the arguments of each predicate and to label them,
yielding a pool of candidate propositions. The sec-
ond stage consists of a reranker that we applied to
</bodyText>
<page confidence="0.996651">
43
</page>
<bodyText confidence="0.998310833333333">
the candidates using the local models and proposi-
tion features. We combined the score of the greedy
classifiers and the reranker in a third stage to select
the best candidate proposition. Figure 1 shows the
system architecture.
We evaluated our semantic parser on a set of seven
languages provided by the organizers of the CoNLL-
2009 shared task: Catalan and Spanish (Taul´e et
al., 2008), Chinese (Palmer and Xue, 2009), Czech
(Haji6 et al., 2006), English (Surdeanu et al., 2008),
German (Burchardt et al., 2006), and Japanese
(Kawahara et al., 2002). Our system achieved an
average labeled semantic F1 of 80.31, which cor-
responded to the second best semantic score over-
all. After the official evaluation was completed, we
discovered a fault in the training procedure of the
reranker for Spanish. The revised average labeled
semantic F1 after correction was 80.80.
</bodyText>
<sectionHeader confidence="0.987144" genericHeader="method">
2 SRL Pipeline
</sectionHeader>
<bodyText confidence="0.992272214285714">
The pipeline of classifiers consists of a predicate
disambiguation (PD) module, an argument identi-
fication module (AI), and an argument classifica-
tion (AC) module. Aside from the lack of a pred-
icate identification module, which was not needed,
as predicates were given, this architecture is identi-
cal to the one adopted by recent systems (Surdeanu
et al., 2008), as well as the general approach within
the field (Gildea and Jurafsky, 2002; Toutanova et
al., 2005).
We build all the classifiers using the L2-
regularized linear logistic regression from the LIB-
LINEAR package (Fan et al., 2008). The package
implementation makes models very fast to train and
</bodyText>
<note confidence="0.8180605">
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 43–48,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<figure confidence="0.999133133333333">
Local classiÞer pipeline
Global model
Sense disambiguation
greedy search
Argument identiÞcation
beam search
Argument labeling
beam search
N candidates
Reranker
Local features + proposition features
Reranked
candidates
Linear combination of models
N candidates
</figure>
<figureCaption confidence="0.999994">
Figure 1: System architecture.
</figureCaption>
<bodyText confidence="0.997366333333333">
use for classification. Since models are logistic, they
produce an output in the form of probabilities that
we use later in the reranker (see Sect. 3).
</bodyText>
<subsectionHeader confidence="0.97804">
2.1 Predicate Disambiguation
</subsectionHeader>
<bodyText confidence="0.999812846153846">
We carried out a disambiguation for all the lem-
mas that had multiple senses in the corpora and we
trained one classifier per lemma. We did not use the
predicate lexicons and we considered lemmas with a
unique observed sense as unambiguous.
English required a special processing as the sense
nomenclature overlapped between certain nominal
and verbal predicates. For instance, the nominal
predicate plan.01 and the verbal predicate plan.01
do not correspond to the same semantic frame.
Hence, we trained two classifiers for each lemma
plan that could be both a nominal and verbal predi-
cate.
</bodyText>
<tableCaption confidence="0.986995">
Table 1: Feature sets for predicate disambiguation.
</tableCaption>
<equation confidence="0.991964">
ca ch cz en ge sp
PredWord • • • • • •
PredPOS • • • •
PredDeprel •
PredFeats
PredParentWord • • • • •
PredParentPOS • • •
PredParentFeats • •
DepSubCat • • • • •
ChildDepSet • • • • • •
ChildWordSet • • • • • •
ChildPOSSet • • • • •
</equation>
<subsectionHeader confidence="0.999748">
2.2 Argument Identification and Classification
</subsectionHeader>
<bodyText confidence="0.999447214285714">
We implemented the argument identification and
classification as two separate stages, because it en-
abled us to apply and optimize different feature sets
in each step. Arguments were identified by means
of a binary classifier. No pruning was done, each
word in the sentence was considered as a potential
argument to all predicates of the same sentence.
Arguments were then labeled using a multiclass
classifier; each class corresponding to a certain la-
bel. We did not apply any special processing with
multiple dependencies in Czech and Japanese. In-
stead, we concatenated the composite labels (i.e.
double edge) to form unique labels (i.e. single edge)
having their own class.
</bodyText>
<subsectionHeader confidence="0.99723">
2.3 Identification and Classification Features
</subsectionHeader>
<bodyText confidence="0.999980958333333">
For the English corpus, we used two sets of features
for the nominal and the verbal predicates both in the
AI and AC steps. This allowed us to create different
classifiers for different kinds of predicates. We ex-
tended this approach with a default classifier catch-
ing predicates that were wrongly tagged by the POS
tagger. For both steps, we used the union of the two
feature sets for this catch-all class.
We wanted to employ this procedure with the two
other languages, Czech and Japanese, where predi-
cates had more than one POS type. As feature selec-
tion (See Sect. 2.4) took longer than expected, par-
ticularly in Czech due to the size of the corpus and
the annotation, we had to abandon this idea and we
trained a single classifier for all POS tags in the AI
and AC steps.
For each data set, we extracted sets of features
similar to the ones described by Johansson and
Nugues (2008). We used a total of 32 features that
we denote with the prefixes: Pred-, PredParent-,
Arg-, Left-, Right-, LeftSibling-, and RightSibling-
for, respectively, the predicate, the parent of the
predicate, the argument, the leftmost and rightmost
dependents of the argument, and the left and right
</bodyText>
<page confidence="0.999484">
44
</page>
<tableCaption confidence="0.987766">
Table 2: Feature sets for argument identification and classification.
</tableCaption>
<figure confidence="0.931008353846154">
Argument classification
ca ch cz en ge ja sp
• N •
• V •
• • • N,V • •
• • • N,V • • •
• •
• V •
V •
• • V • • •
• •
• N •
• • • N,V • • •
• • • N,V •
• • • •
• • • V • •
• • V •
• V • •
• • • N,V • •
• • N • •
• V
•
• • N,V •
• • N,V •
• •
• • N •
• • N,V •
• •
• • •
• •
•
Argument identification
ca ch cz en ge ja sp
PredWord
PredPOS N •
PredLemma N •
PredDeprel
Sense • • V •
PredFeats •
PredParentWord V
PredParentPOS V
PredParentFeats •
DepSubCat • •
ChildDepSet • •
ChildWordSet N
ChildPOSSet •
ArgWord • • N,V • • •
ArgPOS • • N,V • • •
ArgFeats •
ArgDeprel • • • V • •
DeprelPath • • • N,V • • •
POSPath • • • N,V • • •
Position • N,V •
LeftWord • •
LeftPOS •
LeftFeats • •
RightWord • N
RightPOS N
RightFeats
LeftSiblingWord • •
LeftSiblingPOS • •
LeftSiblingFeats •
RightSiblingWord • • V • • •
RightSiblingPOS
RightSiblingFeats
</figure>
<bodyText confidence="0.992913">
sibling of the argument. The suffix of these names
corresponds to the column name of the CoNLL for-
mat, except Word which corresponds to the Form
column. Additional features are:
</bodyText>
<listItem confidence="0.985503944444444">
• Sense: the value of the Pred column, e.g.
plan.01.
• Position: the position of the argument with re-
spect to the predicate, i.e. before, on, or after.
• DepSubCat: the subcategorization frame of the
predicate, e.g. OBJ+OPRD+SUB.
• DeprelPath: the path from predicate to argu-
ment concatenating dependency labels with the
direction of the edge, e.g. OBJTOPRDISUBI.
• POSPath: same as DeprelPath, but depen-
dency labels are exchanged for POS tags, e.g.
NNTNNSINNPI.
• ChildDepSet: the set of dependency labels of
the children of the predicate, e.g. {OBJ, SUB}.
• ChildPOSSet: the set of POS tags of the chil-
dren of the predicate, e.g. {NN, NNS}.
• ChildWordSet: the set of words (Form) of the
children of the predicate, e.g. {fish, me}.
</listItem>
<page confidence="0.994951">
45
</page>
<subsectionHeader confidence="0.964188">
2.4 Feature Selection
</subsectionHeader>
<bodyText confidence="0.99998992">
We selected the feature sets using a greedy forward
procedure. We first built a set of single features and,
to improve the separability of our linear classifiers,
we paired features to build bigrams. We searched
the space of feature bigrams using the same proce-
dure. See Johansson (2008, page 83), for a com-
plete description. We intended to carry out a cross-
validation search. Due to the lack of time, we re-
sorted to using 80% of the training set for training
and 20% for evaluating the features. Table 2 con-
tains the complete list of single features we used.
We omitted the feature bigrams.
Feature selection turned out to be a massive task.
It took us three to four weeks searching the feature
spaces, yet in most cases we were forced to interrupt
the selection process after a few bigram features in
order to have our system ready in time. This means
that our feature sets can probably be further opti-
mized.
When the training data was initially released,
we used the exact feature set from Johansson and
Nugues (2008) to compute baseline results on the
development set for all the languages. After feature
selection, we observed an increase in labeled seman-
tic F1 close to 10% in most languages.
</bodyText>
<subsectionHeader confidence="0.997774">
2.5 Applying Beam Search
</subsectionHeader>
<bodyText confidence="0.999942913043479">
The AI module proceeds left to right considering
each word as an argument of the current predicate.
The current partial propositions are scored by com-
puting the product of the probabilities of all the
words considered so far. After each word, the cur-
rent pool of partial candidates is reduced to the beam
size, k, and at the end of the sentence, the top k scor-
ing propositions are passed on to the AC module.
Given k unlabeled propositions, the AC module
applies a beam search on each of these propositions
independently. This is done in a similar manner,
proceeding from left to right among the identified
arguments, keeping the l best labelings in its beam,
and returning the top l propositions, when all iden-
tified arguments have been processed. This yields
n = k x l complete propositions, unless one of the
unlabeled propositions has zero arguments, in which
case we haven= (k − 1) x l + 1.
The probability of a labeled proposition according
to the local pipeline is given by PLocaj = PAI x
PAC, where PAI and PAC is the output probability
from the AI and AC modules, respectively. In the
case of empty propositions, PAC was set to 1.
</bodyText>
<sectionHeader confidence="0.971617" genericHeader="method">
3 Global Reranker
</sectionHeader>
<bodyText confidence="0.999983375">
We implemented a global reranker following
Toutanova et al. (2005). To generate training ex-
amples for the reranker, we trained m AI and AC
classifiers by partitioning the training set in m parts
and using m − 1 of these parts for each AI and AC
classifier, respectively.
We applied these AI and AC classifiers on the part
of the corpus they were not trained on and we then
generated the top n propositions for each predicate.
We ran the CoNLL evaluation script on the proposi-
tions and we marked the top scoring one(s) as pos-
itive. We marked the others negative. If the correct
proposition was not in the pool of candidates, we
added it as an extra positive example. We used these
positive and negative examples as training data for
the global reranker.
</bodyText>
<subsectionHeader confidence="0.998346">
3.1 Reranker Features
</subsectionHeader>
<bodyText confidence="0.999356913043479">
We used all the features from the local pipeline for
all the languages. We built a vector where the AI
features were prefixed with AI- and the AC features
prefixed with lab−, where lab was any of the argu-
ment labels.
We added one proposition feature to the concate-
nation of local features, namely the sequence of core
argument labels, e.g. A0+plan.01+A1. In Catalan
and Spanish, we considered all the labels prefixed by
arg0, arg1, arg2, or arg3 as core labels. In Chinese
and English, we considered only the labels A0, A1,
A2, A3, and A4. In Czech, German, and Japanese,
we considered all the labels as core labels.
Hence, the total size of the reranker vector space
is |AI |+ |L |x |AC |+ |G|, where |AI |and |AC|
denotes the size of the AI and AC vector spaces, re-
spectively, |L |corresponds to the number of labels,
and |G |is the size of additional global features.
We ran experiments with the grammatical
voice that we included in the string represent-
ing the sequence of core argument labels, e.g.
A1+plan.01/Tassive+A0. The voice was derived by
hand-crafted rules in Catalan, English, German, and
</bodyText>
<page confidence="0.998456">
46
</page>
<bodyText confidence="0.999972125">
Spanish, and given in the Feat column in Czech.
However, we did not notice any significant gain in
performance. The hand-crafted rules use lexical
forms and dependencies, which we believe classi-
fiers are able to derive themselves using the local
model features. This also applies to Czech, as Pred-
Feats was a feature used in the local pipeline, both
in the AI and AC steps.
</bodyText>
<subsectionHeader confidence="0.999914">
3.2 Weighting the Models
</subsectionHeader>
<bodyText confidence="0.999967636363636">
In Sect. 2.5, we described how the pipeline was used
to generate the top n propositions, each with its own
local probability PLocal. Similar to softmax, we nor-
malized these local probabilities by dividing each of
them by their total sum. We denote this normalized
probability by P�Local. The reranker gives a proba-
bility on the complete proposition, PReranker. We
weighted these probabilities and chose the proposi-
tion maximizing PFinal = (P�Local)α × PReranker.
This is equivalent to a linear combination of the log
probabilities.
</bodyText>
<subsectionHeader confidence="0.999787">
3.3 Parameters Used
</subsectionHeader>
<bodyText confidence="0.999863">
For the submission to the CoNLL 2009 Shared Task,
we set the beam widths to k = l = 4, yielding can-
didate pools of size n = 13 or n = 16 (See Sec-
tion 2.5). We used m = 5 for training the reranker
and α = 1 for combining the local model with the
reranker.
</bodyText>
<sectionHeader confidence="0.999854" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999903333333333">
Our system achieved the second best semantic score,
all tasks, with an average labeled semantic F1 of
80.31. It obtained the best F1 score on the Chinese
and German data and the second best on English.
Our system also reached the third rank in the out-of-
domain data, all tasks, with a labeled semantic F1 of
74.38. Post-evaluation, we discovered a bug in the
Spanish reranker model causing the poor results in
this language. After correcting this, we could reach
a labeled semantic F1 of 79.91 in Spanish. Table 3
shows our official results in the shared task as well
as the post-evaluation update.
We also compared the performance of a greedy
strategy with that of a global model. Table 4 shows
these figures with post-evaluation figures in Spanish.
Table 5 shows the training time, parsing time, and
the parsing speed in predicates per second. These
figures correspond to complete execution time of
parsing, including loading models into memory, i.e.
a constant overhead, that explains the low parsing
speed in German. We implemented our system to
be flexible for easy debugging and testing various
ideas. Optimizing the implementation would reduce
execution times significantly.
</bodyText>
<tableCaption confidence="0.981461">
Table 3: Summary of submitted results: closed challenge,
semantic F1. * denotes the post-evaluation results ob-
</tableCaption>
<table confidence="0.999470666666667">
tained for Spanish after a bug fix.
Unlabeled Labeled
Catalan 93.60 80.01
Chinese 84.76 78.60
Czech 92.63 85.41
English 91.17 85.63
German 92.13 79.71
Japanese 83.45 76.30
Spanish 92.69 76.52
Spanish* 93.76 79.91
Average 90.06 80.31
Average* 90.21 80.80
</table>
<tableCaption confidence="0.947763">
Table 4: Improvement of reranker. * denotes the post-
</tableCaption>
<table confidence="0.97057075">
evaluation results obtained for Spanish after a bug fix.
Greedy Reranker Gain
Catalan 79.54 80.01 0.47
Chinese 77.84 78.60 0.76
Czech 84.99 85.41 0.42
English 84.44 85.63 1.19
German 79.01 79.71 0.70
Japanese 75.61 76.30 0.69
Spanish 79.28 76.52 -2.76
Spanish* 79.28 79.91 0.63
Average 80.10 80.31 0.21
Average* 80.10 80.80 0.70
</table>
<sectionHeader confidence="0.998794" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999952666666667">
We have built and described a streamlined and ef-
fective semantic role labeler that did not use any
lexicons or complex linguistic features. We used a
generic feature selection procedure that keeps lan-
guage adaptation minimal and delivers a relatively
even performance across the data sets. The system is
</bodyText>
<page confidence="0.999647">
47
</page>
<tableCaption confidence="0.99933">
Table 5: Summary of training and parsing times on an Apple Mac Pro, 3.2 GHz.
</tableCaption>
<table confidence="0.998774222222222">
Training Parsing (Greedy) Speed (Greedy) Parsing (Reranker) Speed (Reranker)
(min) (min:sec) (pred/sec) (min:sec) (pred/sec)
Catalan 46 1:10 71 1:21 62
Chinese 139 2:35 79 3:45 55
Czech 299 18:47 40 33:49 22
English 421 6:25 27 8:51 20
German 15 0:21 26 0:22 25
Japanese 48 0:37 84 1:02 50
Spanish 51 1:15 69 1:47 48
</table>
<bodyText confidence="0.9995907">
robust and can handle incorrect syntactic parse trees
with a good level of immunity. While input parse
trees in Chinese and German had a labeled syntac-
tic accuracy of 78.46 (Hajic et al., 2009), we could
reach a labeled semantic F1 of 78.60 and 79.71 in
these languages. We also implemented an efficient
global reranker in all languages yielding a 0.7 av-
erage increase in labeled semantic F1. The reranker
step, however, comes at the expense of parsing times
increased by factors ranging from 1.04 to 1.82.
</bodyText>
<sectionHeader confidence="0.99875" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999910280701755">
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pad´o, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2006), Genoa, Italy.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871–1874.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245–288.
Jan Hajic, Jarmila Panevov´a, Eva Hajicov´a, Petr
Sgall, Petr Pajas, Jan Step´anek, Jiri Havelka, Marie
Mikulov´a, and Zden6k Zabokrtsk´y. 2006. Prague De-
pendency Treebank 2.0.
Jan Hajic, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart´�, Llu´�s
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad´o, Jan gt6p´anek, Pavel Straii´ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic depen-
dencies in multiple languages. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic–semantic analysis
with PropBank and NomBank. In Proceedings of the
Shared Task Session of CoNLL-2008.
Richard Johansson. 2008. Dependency-based Semantic
Analysis ofNatural-language Text. Ph.D. thesis, Lund
University, December 5.
Daisuke Kawahara, Sadao Kurohashi, and Kˆoiti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008–2013, Las Palmas, Canary
Islands.
Martha Palmer and Nianwen Xue. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143–172.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu´�s M`arquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the 12th Con-
ference on Computational Natural Language Learning
(CoNLL-2008).
Mariona Taul´e, Maria Ant`onia Mart´�, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC-2008), Marrakesh, Morroco.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2005. Joint learning improves semantic role
labeling. In Proceedings ofACL-2005.
</reference>
<page confidence="0.999354">
48
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.235112">
<title confidence="0.99995">Multilingual Semantic Role Labeling</title>
<author confidence="0.996084">Anders Bj¨orkelund Love Hafdell Pierre Nugues</author>
<affiliation confidence="0.999299">Department of Computer Science, Lund</affiliation>
<address confidence="0.632675">S-221 00 Lund,</address>
<email confidence="0.6182705">lovePierre.Nugues@cs.lth.se</email>
<abstract confidence="0.997005318181818">This paper describes our contribution to the semantic role labeling task (SRL-only) of the CoNLL-2009 shared task in the closed challenge (Haji6 et al., 2009). Our system consists of a pipeline of independent, local classifiers that identify the predicate sense, the arguments of the predicates, and the argument labels. Using these local models, we carried out a beam search to generate a pool of candidates. We then reranked the candidates using a joint learning approach that combines the local models and proposition features. To address the multilingual nature of the data, we implemented a feature selection procedure that systematically explored the feature space, yielding significant gains over a standard set of features. Our system achieved the second best semantic score overall with an average labeled semantic F1 of 80.31. It obtained the best F1 score on the Chinese and German data and the second best one on English.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Aljoscha Burchardt</author>
<author>Katrin Erk</author>
<author>Anette Frank</author>
<author>Andrea Kowalski</author>
<author>Sebastian Pad´o</author>
<author>Manfred Pinkal</author>
</authors>
<title>The SALSA corpus: a German corpus resource for lexical semantics.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC-2006),</booktitle>
<location>Genoa, Italy.</location>
<marker>Burchardt, Erk, Frank, Kowalski, Pad´o, Pinkal, 2006</marker>
<rawString>Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea Kowalski, Sebastian Pad´o, and Manfred Pinkal. 2006. The SALSA corpus: a German corpus resource for lexical semantics. In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC-2006), Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>Xiang-Rui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="3214" citStr="Fan et al., 2008" startWordPosition="514" endWordPosition="517">as 80.80. 2 SRL Pipeline The pipeline of classifiers consists of a predicate disambiguation (PD) module, an argument identification module (AI), and an argument classification (AC) module. Aside from the lack of a predicate identification module, which was not needed, as predicates were given, this architecture is identical to the one adopted by recent systems (Surdeanu et al., 2008), as well as the general approach within the field (Gildea and Jurafsky, 2002; Toutanova et al., 2005). We build all the classifiers using the L2- regularized linear logistic regression from the LIBLINEAR package (Fan et al., 2008). The package implementation makes models very fast to train and Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 43–48, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Local classiÞer pipeline Global model Sense disambiguation greedy search Argument identiÞcation beam search Argument labeling beam search N candidates Reranker Local features + proposition features Reranked candidates Linear combination of models N candidates Figure 1: System architecture. use for classification. Since models are logis</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="3060" citStr="Gildea and Jurafsky, 2002" startWordPosition="489" endWordPosition="492"> evaluation was completed, we discovered a fault in the training procedure of the reranker for Spanish. The revised average labeled semantic F1 after correction was 80.80. 2 SRL Pipeline The pipeline of classifiers consists of a predicate disambiguation (PD) module, an argument identification module (AI), and an argument classification (AC) module. Aside from the lack of a predicate identification module, which was not needed, as predicates were given, this architecture is identical to the one adopted by recent systems (Surdeanu et al., 2008), as well as the general approach within the field (Gildea and Jurafsky, 2002; Toutanova et al., 2005). We build all the classifiers using the L2- regularized linear logistic regression from the LIBLINEAR package (Fan et al., 2008). The package implementation makes models very fast to train and Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 43–48, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Local classiÞer pipeline Global model Sense disambiguation greedy search Argument identiÞcation beam search Argument labeling beam search N candidates Reranker Local features + propos</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajic</author>
<author>Jarmila Panevov´a</author>
<author>Eva Hajicov´a</author>
<author>Petr Sgall</author>
<author>Petr Pajas</author>
<author>Jan Step´anek</author>
<author>Jiri Havelka</author>
</authors>
<title>Marie Mikulov´a, and Zden6k Zabokrtsk´y.</title>
<date>2006</date>
<journal>Prague Dependency Treebank</journal>
<volume>2</volume>
<marker>Hajic, Panevov´a, Hajicov´a, Sgall, Pajas, Step´anek, Havelka, 2006</marker>
<rawString>Jan Hajic, Jarmila Panevov´a, Eva Hajicov´a, Petr Sgall, Petr Pajas, Jan Step´anek, Jiri Havelka, Marie Mikulov´a, and Zden6k Zabokrtsk´y. 2006. Prague Dependency Treebank 2.0.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jan Hajic</author>
<author>Massimiliano Ciaramita</author>
<author>Richard Johansson</author>
<author>Daisuke Kawahara</author>
<author>Maria Ant`onia Mart´�</author>
<author>Llu´�s M`arquez</author>
<author>Adam Meyers</author>
<author>Joakim Nivre</author>
<author>Sebastian Pad´o</author>
</authors>
<title>gt6p´anek, Pavel Straii´ak, Mihai Surdeanu, Nianwen Xue, and Yi Zhang.</title>
<date></date>
<booktitle>In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009),</booktitle>
<location>Boulder, Colorado, USA.</location>
<marker>Hajic, Ciaramita, Johansson, Kawahara, Mart´�, M`arquez, Meyers, Nivre, Pad´o, </marker>
<rawString>Jan Hajic, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Ant`onia Mart´�, Llu´�s M`arquez, Adam Meyers, Joakim Nivre, Sebastian Pad´o, Jan gt6p´anek, Pavel Straii´ak, Mihai Surdeanu, Nianwen Xue, and Yi Zhang. 2009. The CoNLL2009 shared task: Syntactic and semantic dependencies in multiple languages. In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009), June 4-5, Boulder, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Dependency-based syntactic–semantic analysis with PropBank and NomBank.</title>
<date>2008</date>
<booktitle>In Proceedings of the Shared Task Session of CoNLL-2008.</booktitle>
<contexts>
<context position="6481" citStr="Johansson and Nugues (2008)" startWordPosition="1056" endWordPosition="1059">catching predicates that were wrongly tagged by the POS tagger. For both steps, we used the union of the two feature sets for this catch-all class. We wanted to employ this procedure with the two other languages, Czech and Japanese, where predicates had more than one POS type. As feature selection (See Sect. 2.4) took longer than expected, particularly in Czech due to the size of the corpus and the annotation, we had to abandon this idea and we trained a single classifier for all POS tags in the AI and AC steps. For each data set, we extracted sets of features similar to the ones described by Johansson and Nugues (2008). We used a total of 32 features that we denote with the prefixes: Pred-, PredParent-, Arg-, Left-, Right-, LeftSibling-, and RightSiblingfor, respectively, the predicate, the parent of the predicate, the argument, the leftmost and rightmost dependents of the argument, and the left and right 44 Table 2: Feature sets for argument identification and classification. Argument classification ca ch cz en ge ja sp • N • • V • • • • N,V • • • • • N,V • • • • • • V • V • • • V • • • • • • N • • • • N,V • • • • • • N,V • • • • • • • • V • • • • V • • V • • • • • N,V • • • • N • • • V • • • N,V • • • N,V</context>
<context position="9644" citStr="Johansson and Nugues (2008)" startWordPosition="1679" endWordPosition="1682">of time, we resorted to using 80% of the training set for training and 20% for evaluating the features. Table 2 contains the complete list of single features we used. We omitted the feature bigrams. Feature selection turned out to be a massive task. It took us three to four weeks searching the feature spaces, yet in most cases we were forced to interrupt the selection process after a few bigram features in order to have our system ready in time. This means that our feature sets can probably be further optimized. When the training data was initially released, we used the exact feature set from Johansson and Nugues (2008) to compute baseline results on the development set for all the languages. After feature selection, we observed an increase in labeled semantic F1 close to 10% in most languages. 2.5 Applying Beam Search The AI module proceeds left to right considering each word as an argument of the current predicate. The current partial propositions are scored by computing the product of the probabilities of all the words considered so far. After each word, the current pool of partial candidates is reduced to the beam size, k, and at the end of the sentence, the top k scoring propositions are passed on to th</context>
</contexts>
<marker>Johansson, Nugues, 2008</marker>
<rawString>Richard Johansson and Pierre Nugues. 2008. Dependency-based syntactic–semantic analysis with PropBank and NomBank. In Proceedings of the Shared Task Session of CoNLL-2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
</authors>
<title>Dependency-based Semantic Analysis ofNatural-language Text.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>Lund University,</institution>
<contexts>
<context position="8910" citStr="Johansson (2008" startWordPosition="1548" endWordPosition="1549"> for POS tags, e.g. NNTNNSINNPI. • ChildDepSet: the set of dependency labels of the children of the predicate, e.g. {OBJ, SUB}. • ChildPOSSet: the set of POS tags of the children of the predicate, e.g. {NN, NNS}. • ChildWordSet: the set of words (Form) of the children of the predicate, e.g. {fish, me}. 45 2.4 Feature Selection We selected the feature sets using a greedy forward procedure. We first built a set of single features and, to improve the separability of our linear classifiers, we paired features to build bigrams. We searched the space of feature bigrams using the same procedure. See Johansson (2008, page 83), for a complete description. We intended to carry out a crossvalidation search. Due to the lack of time, we resorted to using 80% of the training set for training and 20% for evaluating the features. Table 2 contains the complete list of single features we used. We omitted the feature bigrams. Feature selection turned out to be a massive task. It took us three to four weeks searching the feature spaces, yet in most cases we were forced to interrupt the selection process after a few bigram features in order to have our system ready in time. This means that our feature sets can probab</context>
</contexts>
<marker>Johansson, 2008</marker>
<rawString>Richard Johansson. 2008. Dependency-based Semantic Analysis ofNatural-language Text. Ph.D. thesis, Lund University, December 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
<author>Kˆoiti Hasida</author>
</authors>
<title>Construction of a Japanese relevance-tagged corpus.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC-2002),</booktitle>
<pages>2008--2013</pages>
<location>Las Palmas, Canary Islands.</location>
<contexts>
<context position="2292" citStr="Kawahara et al., 2002" startWordPosition="365" endWordPosition="368">sitions. The second stage consists of a reranker that we applied to 43 the candidates using the local models and proposition features. We combined the score of the greedy classifiers and the reranker in a third stage to select the best candidate proposition. Figure 1 shows the system architecture. We evaluated our semantic parser on a set of seven languages provided by the organizers of the CoNLL2009 shared task: Catalan and Spanish (Taul´e et al., 2008), Chinese (Palmer and Xue, 2009), Czech (Haji6 et al., 2006), English (Surdeanu et al., 2008), German (Burchardt et al., 2006), and Japanese (Kawahara et al., 2002). Our system achieved an average labeled semantic F1 of 80.31, which corresponded to the second best semantic score overall. After the official evaluation was completed, we discovered a fault in the training procedure of the reranker for Spanish. The revised average labeled semantic F1 after correction was 80.80. 2 SRL Pipeline The pipeline of classifiers consists of a predicate disambiguation (PD) module, an argument identification module (AI), and an argument classification (AC) module. Aside from the lack of a predicate identification module, which was not needed, as predicates were given, </context>
</contexts>
<marker>Kawahara, Kurohashi, Hasida, 2002</marker>
<rawString>Daisuke Kawahara, Sadao Kurohashi, and Kˆoiti Hasida. 2002. Construction of a Japanese relevance-tagged corpus. In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC-2002), pages 2008–2013, Las Palmas, Canary Islands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Nianwen Xue</author>
</authors>
<title>Adding semantic roles to the Chinese Treebank.</title>
<date>2009</date>
<journal>Natural Language Engineering,</journal>
<volume>15</volume>
<issue>1</issue>
<contexts>
<context position="2160" citStr="Palmer and Xue, 2009" startWordPosition="344" endWordPosition="347">lemma. We then used a beam search to identify the arguments of each predicate and to label them, yielding a pool of candidate propositions. The second stage consists of a reranker that we applied to 43 the candidates using the local models and proposition features. We combined the score of the greedy classifiers and the reranker in a third stage to select the best candidate proposition. Figure 1 shows the system architecture. We evaluated our semantic parser on a set of seven languages provided by the organizers of the CoNLL2009 shared task: Catalan and Spanish (Taul´e et al., 2008), Chinese (Palmer and Xue, 2009), Czech (Haji6 et al., 2006), English (Surdeanu et al., 2008), German (Burchardt et al., 2006), and Japanese (Kawahara et al., 2002). Our system achieved an average labeled semantic F1 of 80.31, which corresponded to the second best semantic score overall. After the official evaluation was completed, we discovered a fault in the training procedure of the reranker for Spanish. The revised average labeled semantic F1 after correction was 80.80. 2 SRL Pipeline The pipeline of classifiers consists of a predicate disambiguation (PD) module, an argument identification module (AI), and an argument cl</context>
</contexts>
<marker>Palmer, Xue, 2009</marker>
<rawString>Martha Palmer and Nianwen Xue. 2009. Adding semantic roles to the Chinese Treebank. Natural Language Engineering, 15(1):143–172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Llu´�s M`arquez</author>
<author>Joakim Nivre</author>
</authors>
<title>The CoNLL2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL-2008).</booktitle>
<marker>Surdeanu, Johansson, Meyers, M`arquez, Nivre, 2008</marker>
<rawString>Mihai Surdeanu, Richard Johansson, Adam Meyers, Llu´�s M`arquez, and Joakim Nivre. 2008. The CoNLL2008 shared task on joint parsing of syntactic and semantic dependencies. In Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL-2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mariona Taul´e</author>
<author>Maria Ant`onia Mart´�</author>
<author>Marta Recasens</author>
</authors>
<title>AnCora: Multilevel Annotated Corpora for Catalan and Spanish.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC-2008),</booktitle>
<location>Marrakesh, Morroco.</location>
<marker>Taul´e, Mart´�, Recasens, 2008</marker>
<rawString>Mariona Taul´e, Maria Ant`onia Mart´�, and Marta Recasens. 2008. AnCora: Multilevel Annotated Corpora for Catalan and Spanish. In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC-2008), Marrakesh, Morroco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Aria Haghighi</author>
<author>Christopher D Manning</author>
</authors>
<title>Joint learning improves semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL-2005.</booktitle>
<contexts>
<context position="3085" citStr="Toutanova et al., 2005" startWordPosition="493" endWordPosition="496">we discovered a fault in the training procedure of the reranker for Spanish. The revised average labeled semantic F1 after correction was 80.80. 2 SRL Pipeline The pipeline of classifiers consists of a predicate disambiguation (PD) module, an argument identification module (AI), and an argument classification (AC) module. Aside from the lack of a predicate identification module, which was not needed, as predicates were given, this architecture is identical to the one adopted by recent systems (Surdeanu et al., 2008), as well as the general approach within the field (Gildea and Jurafsky, 2002; Toutanova et al., 2005). We build all the classifiers using the L2- regularized linear logistic regression from the LIBLINEAR package (Fan et al., 2008). The package implementation makes models very fast to train and Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 43–48, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Local classiÞer pipeline Global model Sense disambiguation greedy search Argument identiÞcation beam search Argument labeling beam search N candidates Reranker Local features + proposition features Reranked c</context>
<context position="11072" citStr="Toutanova et al. (2005)" startWordPosition="1931" endWordPosition="1934">ified arguments, keeping the l best labelings in its beam, and returning the top l propositions, when all identified arguments have been processed. This yields n = k x l complete propositions, unless one of the unlabeled propositions has zero arguments, in which case we haven= (k − 1) x l + 1. The probability of a labeled proposition according to the local pipeline is given by PLocaj = PAI x PAC, where PAI and PAC is the output probability from the AI and AC modules, respectively. In the case of empty propositions, PAC was set to 1. 3 Global Reranker We implemented a global reranker following Toutanova et al. (2005). To generate training examples for the reranker, we trained m AI and AC classifiers by partitioning the training set in m parts and using m − 1 of these parts for each AI and AC classifier, respectively. We applied these AI and AC classifiers on the part of the corpus they were not trained on and we then generated the top n propositions for each predicate. We ran the CoNLL evaluation script on the propositions and we marked the top scoring one(s) as positive. We marked the others negative. If the correct proposition was not in the pool of candidates, we added it as an extra positive example. </context>
</contexts>
<marker>Toutanova, Haghighi, Manning, 2005</marker>
<rawString>Kristina Toutanova, Aria Haghighi, and Christopher D. Manning. 2005. Joint learning improves semantic role labeling. In Proceedings ofACL-2005.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>