<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000807">
<title confidence="0.982156">
Regularization and Search for Minimum Error Rate Training
</title>
<author confidence="0.998449">
Daniel Cer, Daniel Jurafsky, and Christopher D. Manning
</author>
<affiliation confidence="0.993044">
Stanford University
</affiliation>
<address confidence="0.86647">
Stanford, CA 94305
</address>
<email confidence="0.998467">
cerd,jurafsky,manning@stanford.edu
</email>
<sectionHeader confidence="0.998585" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999058615384615">
Minimum error rate training (MERT) is a
widely used learning procedure for statistical
machine translation models. We contrast three
search strategies for MERT: Powell’s method,
the variant of coordinate descent found in the
Moses MERT utility, and a novel stochastic
method. It is shown that the stochastic method
obtains test set gains of +0.98 BLEU on MT03
and +0.61 BLEU on MT05. We also present
a method for regularizing the MERT objec-
tive that achieves statistically significant gains
when combined with both Powell’s method
and coordinate descent.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999911631578947">
Och (2003) introduced minimum error rate training
(MERT) as an alternative training regime to the con-
ditional likelihood objective previously used with
log-linear translation models (Och &amp; Ney, 2002).
This approach attempts to improve translation qual-
ity by optimizing an automatic translation evalua-
tion metric, such as the BLEU score (Papineni et al.,
2002). This is accomplished by either directly walk-
ing the error surface provided by an evaluation met-
ric w.r.t. the model weights or by using gradient-
based techniques on a continuous approximation of
such a surface. While the former is piecewise con-
stant and thus cannot be optimized using gradient
techniques, Och (2003) provides an approach that
performs such training efficiently.
In this paper we explore a number of variations on
MERT. First, it is shown that performance gains can
be had by making use of a stochastic search strategy
as compare to that obtained by Powell’s method and
</bodyText>
<page confidence="0.931956">
26
</page>
<bodyText confidence="0.999906625">
coordinate descent. Subsequently, results are pre-
sented for two regularization strategies1. Both allow
coordinate descent and Powell’s method to achieve
performance that is on par with stochastic search.
In what follows, we briefly review minimum er-
ror rate training, introduce our stochastic search and
regularization strategies, and then present experi-
mental results.
</bodyText>
<sectionHeader confidence="0.984871" genericHeader="method">
2 Minimum Error Rate Training
</sectionHeader>
<bodyText confidence="0.99996025">
Let F be a collection of foreign sentences to be
translated, with individual sentences f0, f1, ... ,
fry,,. For each fi, the surface form of an indi-
vidual candidate translation is given by ei with
hidden state hi associated with the derivation of
ei from fi. Each ei is drawn from £, which
represents all possible strings our translation sys-
tem can produce. The (ei, hi, fi) triples are con-
verted into vectors of m feature functions by
IF : £ x x x F —* R&apos; whose dot product with the
weight vector w assigns a score to each triple.
The idealized translation process then is to find the
highest scoring pair (ei, hi) for each fi, or rather
(ei, hi) = argmax(eEE,hEW) w · IF(e, h, f).
The aggregate argmax for the entire data set F is
given by equation (1)2. This gives E, which repre-
sents the set of translations selected by the model for
data set F when parameterized by the weight vec-
tor w. Let’s assume we have an automated mea-
sure of translation quality f that maps the collec-
</bodyText>
<footnote confidence="0.9553524">
1While we prefer the term regularization, the strategies pre-
sented here could also be referred to as smoothing methods.
2Here, the translation of the entire data set is treated as a
single structured prediction problem using the feature function
vector T(E, H, F) = Ea T(ei, hi, fi)
</footnote>
<note confidence="0.96763525">
Proceedings of the Third Workshop on Statistical Machine Translation, pages 26–34,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
id Translation log(PTM(f|e)) log(PLM(e)) BLEU-2
e1 This is it -1.2 -0.1 29.64
e2 This is small house -0.2 -1.2 63.59
e3 This is miniscule building -1.6 -0.9 31.79
e4 This is a small house -0.1 -0.9 100.00
ref This is a small house
</note>
<tableCaption confidence="0.905376333333333">
Table 1: Four hypothetical translations and their corresponding log model scores from a translation model PTM(f|e)
and a language model PLM(e), along with their BLEU-2 scores according to the given reference translation. The
MERT error surface for these translations is given in figure 1.
</tableCaption>
<bodyText confidence="0.9978946">
tion of translations Ew onto some real valued loss,
f : £n —* R. For instance, in the experiments that
follow, the loss corresponds to 1 minus the BLEU
score assigned to Ew for a given collection of refer-
ence translations.
</bodyText>
<equation confidence="0.9974325">
(Ew, Hw) = argmax w · IF(E, H, F) (1)
(EEgn,HEWn)
</equation>
<bodyText confidence="0.998847333333333">
Using n-best lists produced by a decoder to ap-
proximate £n and Hn, MERT searches for the
weight vector w* that minimizes the loss f. Let-
ting �Ew denote the result of the translation argmax
w.r.t. the approximate hypothesis space, the MERT
search is then expressed by equation (2). Notice the
objective function being optimized is equivalent to
the loss assigned by the automatic measure of trans-
lation quality, i.e. O(w) = f(�Ew).
</bodyText>
<equation confidence="0.9946575">
w* = argmin f(kw) (2)
w
</equation>
<bodyText confidence="0.999915083333333">
After performing the parameter search, the de-
coder is then re-run using the weights w* to produce
a new set of n-best lists, which are then concate-
nated with the prior n-best lists in order to obtain a
better approximation of £n and Hn. The parameter
search given in (2) can then be performed over the
improved approximation. This process repeats un-
til either no novel entries are produced for the com-
bined n-best lists or the weights change by less than
some 2 across iterations.
Unlike the objective functions associated with
other popular learning algorithms, the objective O
is piecewise constant over its entire domain. That
is, while small perturbations in the weights, w, will
change the score assigned by w · IF(e, h, f) to each
triple, (e, h, f), such perturbations will generally not
change the ranking between the pair selected by the
argmax, (e*, h*) = argmaxw · IF(e, h, f), and any
given competing pair (e&apos;, h&apos;). However, at certain
critical points, the score assigned to some compet-
ing pair (e&apos;, h&apos;) will exceed that assigned to the
prior winner (e*�old, h*�old). At this point, the pair
returned by argmax w · IF(e, h, f) will change and
loss f will be evaluated using the newly selected e&apos;.
</bodyText>
<figureCaption confidence="0.8336935">
Figure 1: MERT objective for the translations given
in table 1. Regions are labeled with the translation
that dominates within it, i.e. argmaxw · IF(e,f),
and with their corresponding objective values,
</figureCaption>
<equation confidence="0.317737">
1 − B(argmax w · IP(e, f)).
</equation>
<bodyText confidence="0.9940066">
This is illustrated in figure (1), which plots the
MERT objective function for a simple model with
two parameters, wtm &amp; wlm, and for which the
space of possible translations, £, consists of the four
sentences given in table 13. Here, the loss f is de-
</bodyText>
<footnote confidence="0.997335">
3For this example, we ignore the latent variables, h, associ-
</footnote>
<page confidence="0.999275">
27
</page>
<bodyText confidence="0.999978096774193">
fined as 1.0−BLEU-2(e). That is, f is the differ-
ence between a perfect BLEU score and the BLEU
score calculated for each translation using unigram
and bi-gram counts.
The surface can be visualized as a collection of
plateaus that all meet at the origin and then extend
off into infinity. The latter property illustrates that
the objective is scale invariant w.r.t. the weight vec-
tor w. That is, since any vector w&apos; = Aw b),&gt;0 will
still result in the same relative rankings of all pos-
sible translations according to w · IF(e, h, f), such
scaling will not change the translation selected by
the argmax. At the boundaries between regions, the
objective is undefined, as 2 or more candidates are
assigned identical scores by the model. Thus, it is
unclear what should be returned by the argmax for
subsequent scoring by f.
Since the objective is piecewise constant, it can-
not be minimized using gradient descent or even the
sub-gradient method. Two applicable methods in-
clude downhill simplex and Powell’s method (Press
et al., 2007). The former attempts to find a lo-
cal minimum in an n dimensional space by itera-
tively shrinking or growing an n + 1 vertex simplex4
based on the objective values of the current vertex
points and select nearby points. In contrast, Pow-
ell’s method operates by starting with a single point
in weight space, and then performing a series of line
minimizations until no more progress can be made.
In this paper, we focus on line minimization based
techniques, such as Powell’s method.
</bodyText>
<subsectionHeader confidence="0.990877">
2.1 Global minimum along a line
</subsectionHeader>
<bodyText confidence="0.999392733333333">
Even without gradient information, numerous meth-
ods can be used to find, or approximately find, local
minima along a line. However, by exploiting the fact
that the underlying scores assigned to competing hy-
potheses, w · IF(e, h, f), vary linearly w.r.t. changes
in the weight vector, w, Och (2003) proposed a strat-
egy for finding the global minimum along any given
search direction.
The insight behind the algorithm is as follows.
Let’s assume we are examining two competing
ated with the derivation of each e from the foreign sentence f.
If included, such variables would only change the graph in that
multiple different derivations would be possible for each e&apos;. If
present, the graph could then include disjoint regions that all
map to the same e&apos; and thus the same objective value.
</bodyText>
<footnote confidence="0.6875545">
4A simplex can be thought of as a generalization of a triangle
to arbitrary dimensional spaces.
</footnote>
<figureCaption confidence="0.839504666666667">
Figure 2: Illustration of how the model score assigned
to each candidate translation varies during a line search
along the coordinate direction wlm with a starting point
</figureCaption>
<bodyText confidence="0.93362875862069">
of (wtm, wlm) = (1.0, 0.5). Each plotted line corre-
sponds to the model score for one of the translation candi-
dates. The vertical bands are labeled with the hypothesis
that dominates in that region. The transitions between
bands result from the dotted intersections between 1-best
lines.
translation/derivation pairs, (e1, h1) &amp; (e2, h2).
Further, let’s say the score assigned by the
model to (e1, h1) is greater than (e2, h2), i.e.
w · IF(e1, h1, f) &gt; w · IF(e2, h2, f). Since the
scores of the two vary linearly along any search
direction, d, we can find the point at which the
model’s relative preference for the competing
pairs switches as p = w· (e2,ha,f)−w· (e2,hi ,f)
d· (e ,h ,f)−d· (e1,h ,f) .
At this particular point, we have the equality
(pd + w) · IF(e1, h1, f) = (pd + w) · IF(e2, h2, f),
or rather the point at which the scores assigned
by the model to the candidates intersect along
search direction d5. Such points correspond to
the boundaries between adjacent plateaus in the
objective, as prior to the boundary the loss function
f is computed using the translation, e1, and after the
boundary it is computed using e2.
To find the global minimum for a search direc-
tion d, we move along d and for each plateau we
5Notice that, this point only exists if the slopes of the
candidates’ model scores along d are not equivalent, i.e. if
d · T(e2, h2, f) # d · T(e1, h1, f).
</bodyText>
<page confidence="0.992871">
28
</page>
<table confidence="0.9996262">
Translation m b 1-best
e1 -0.1 -1.25 (0.86,+∞]
e2 -1.2 -0.8 (-0.83,0.88)
e3 -0.9 -2.05 n/a
e4 -0.9 -0.55 [−∞,-0.83]
</table>
<tableCaption confidence="0.996953">
Table 2: Slopes, m, intercepts, b, and 1-best ranges
</tableCaption>
<bodyText confidence="0.998666608695652">
for the 4 translations given in table 1 during a line
search along the coordinate wl,,,,,, with a starting point of
(wt,,,,,, wl,,,,) = (1.0, 0.5). This line search in illustrated
in figure(2).
identify all the points at which the score assigned
by the model to the current 1-best translation inter-
sects the score assigned to competing translations.
At the closest such intersection, we have a new 1-
best translation. Moving to the plateau associated
with this new 1-best, we then repeat the search for
the nearest subsequent intersection. This continues
until we know what the 1-best translations are for all
points along d. The global minimum can then be
found by examining ` once for each of these.
Let’s return briefly to our earlier example given in
table 1. Starting at position (wtm, wlm) = (1.0, 0.5)
and searching along the wlm coordinate, i.e.
(dtm, dlm) = (0.0, 1.0), table 2 gives the line
search slopes, m = d · IF(e, h, f), and intercepts,
b = w · IF(e, h, f), for each of the four candidate
translations. Using the procedure just described, we
can then find what range of values along d each
candidate translation is assigned the highest rela-
tive model score. Figure 2 illustrates how the score
assigned by the model to each of the translations
changes as we move along d. Each of the banded re-
gions corresponds to a plateau in the objective, and
each of the top most line intersections represents the
transition from one plateau to the next. Note that,
while the surface that is defined by the line segments
with the highest classifier score for each region is
convex, this is not a convex optimization problem as
we are optimizing over the loss ` rather than classi-
fier score.
Pseudocode for the line search is given in algo-
rithm 1. Letting n denote the number of foreign sen-
tences, f, in a dataset, and having m denote the size
of the individual n-best lists, |l|, the time complexity
of the algorithm is given by O(nm2). This is seen
in that each time we check for the nearest intersec-
tion to the current 1-best for some n-best list l, we
Algorithm 1 Och (2003)’s line search method to
find the global minimum in the loss, `, when start-
ing at the point w and searching along the direction
d using the candidate translations given in the col-
lection of n-best lists L.
</bodyText>
<equation confidence="0.8906065">
Input: L, w, d, `
I ⇐ {}
for l ∈ L do
fore ∈ l do
m{e} ⇐ e.features · d
b{e} ⇐ e.features · w
end for
bestn ⇐ argmaxe.l m{e} {b{e} breaks ties}
loop � �
0, b{bestn}−b{e}
bestn+1 = argmine.l max
m{e}−m{bestn}
intercept ⇐ max(0, b{bestn}−b{bestn+1} 1
\\ m{bestn+1}−m{bestn} J
if intercept &gt; 0 then
add(I, intercept)
else
break
end if
end loop
end for
add(I, max(I) + 22)
ibest = argmini.Z eval`(L, w + (i − 2) · d)
return w +
</equation>
<bodyText confidence="0.99974925">
must calculate its intersection with all other candi-
date translations that have yet to be selected as the
1-best. And, for each of the n n-best lists, this may
have to be done up to m − 1 times.
</bodyText>
<subsectionHeader confidence="0.999701">
2.2 Search Strategies
</subsectionHeader>
<bodyText confidence="0.999909555555555">
In this section, we review two search strategies that,
in conjunction with the line search just described,
can be used to drive MERT. The first, Powell’s
method, was advocated by Och (2003) when MERT
was first introduced for statistical machine transla-
tion. The second, which we call Koehn-coordinate
descent (KCD)6, is used by the MERT utility pack-
aged with the popular Moses statistical machine
translation system (Koehn et al., 2007).
</bodyText>
<footnote confidence="0.9677454">
6Moses uses David Chiang’s CMERT package. Within the
source file mert.c, the function that implements the overall
search strategy, optimize koehn(), is based on Philipp Koehn’s
Perl script for MERT optimization that was distributed with
Pharaoh.
</footnote>
<equation confidence="0.874251">
(ibest − 2) · d
</equation>
<page confidence="0.992108">
29
</page>
<subsubsectionHeader confidence="0.735999">
2.2.1 Powell’s Method
</subsubsectionHeader>
<bodyText confidence="0.999925652173913">
Powell’s method (Press et al., 2007) attempts to
efficiently search the objective by constructing a set
of mutually non-interfering search directions. The
basic procedure is as follows: (i) A collection of
search directions is initialized to be the coordinates
of the space being searched; (ii) The objective is
minimized by looping through the search directions
and performing a line minimization for each; (iii) A
new search direction is constructed that summarizes
the cumulative direction of the progress made dur-
ing step (ii) (i.e., dnew — wpreii − wpostii). After
a line minimization is performed along dnew, it is
used to replace one of the existing search directions.
(iv) The process repeats until no more progress can
be made. For a quadratic function of n variables,
this procedure comes with the guarantee that it will
reach the minimum within n iterations of the outer
loop. However, since Powell’s method is usually ap-
plied to non-quadratic optimization problems, a typ-
ical implementation will forego the quadratic con-
vergence guarantees in favor of a heuristic scheme
that allows for better navigation of complex sur-
faces.
</bodyText>
<subsubsectionHeader confidence="0.621067">
2.2.2 Koehn’s Coordinate Descent
</subsubsectionHeader>
<bodyText confidence="0.9999538">
KCD is a variant of coordinate descent that, at
each iteration, moves along the coordinate which al-
lows for the most progress in the objective. In or-
der to determine which coordinate this is, the rou-
tine performs a trial line minimization along each. It
then updates the weight vector with the one that it
found to be most successful. While much less so-
phisticated that Powell, our results indicate that this
method may be marginally more effective at opti-
mizing the MERT objective7.
</bodyText>
<sectionHeader confidence="0.999057" genericHeader="method">
3 Extensions
</sectionHeader>
<bodyText confidence="0.9996296">
In this section we present and motivate two novel
extensions to MERT. The first is a stochastic alterna-
tive to the Powell and KCD search strategies, while
the second is an efficient method for regularizing the
objective.
</bodyText>
<footnote confidence="0.943874166666667">
7While we are not aware of any previously published results
that demonstrate this, it is likely that we were not the first to
make this discovery as even though Moses’ MERT implemen-
tation includes a vestigial implementation of Powell’s method,
the code is hardwired to call optimize koehn rather than the rou-
tine for Powell.
</footnote>
<subsectionHeader confidence="0.983235">
3.1 Random Search Directions
</subsectionHeader>
<bodyText confidence="0.999947382978724">
One significant advantage of Powell’s algorithm
over coordinate descent is that it can optimize along
diagonal search directions in weight space. That is,
given a model with a dozen or so features, it can
explore gains that are to be had by simultaneously
varying two or more of the feature weights. In gen-
eral, the diagonals that Powell’s method constructs
allow it to walk objective functions more efficiently
than coordinate descent (Press et al., 2007). How-
ever, given that we have a line search algorithm
that will find the global minima along any given
search direction, diagonal search may be of even
more value. That is, similar to ridge phenomenon
that arise in traditional hill climbing search, it is pos-
sible that there are points in the objective that are the
global minimum along any given coordinate direc-
tion, but are not the global minimum along diagonal
directions.
However, one substantial disadvantage for Pow-
ell is that the assumptions it uses to build up the di-
agonal search directions do not hold in the present
context. Specifically, the search directions are built
up under the assumption that near a minimum the
surface looks approximately quadratic and that we
are performing local line minimizations within such
regions. However, since we are performing global
line minimizations, it is possible for the algorithm to
jump from the region around one minima to another.
If Powell’s method has already started to tune its
search directions for the prior minima, it will likely
be less effective in its efforts to search the new re-
gion. To this extent, coordinate descent will be more
robust than Powell as it has no assumptions that are
violated when such a jump occurs.
One way of salvaging Powell’s algorithm in this
context would be to incorporate additional heuris-
tics that detect when the algorithm has jumped from
the region around one minima to another. When
this occurs, the search directions could be reset to
the coordinates of the space. However, we opt for a
simpler solution, which like Powell’s algorithm per-
forms searches along diagonals in the space, but that
like coordinate descent is sufficiently simple that the
algorithm will not be confused by sudden jumps be-
tween regions.
Specifically, the search procedure chooses di-
rections at random such that each component
</bodyText>
<page confidence="0.99742">
30
</page>
<figureCaption confidence="0.99894">
Figure 3: Regularization during line search - using, from left to right: (i) the maximum loss of adjacent plateaus, (ii)
the average loss of adjacent plateaus, (iii) no regularization. Each set of bars represents adjacent plateaus along the line
being searched, with the height of the bars representing their associated loss. The vertical lines indicate the surrogate
loss values used for the center region under each of the schemes (i-iii).
</figureCaption>
<bodyText confidence="0.999922111111111">
is distributed according to a Gaussian8, d s.t.
di — N(0,1). This allows the procedure to mini-
mize along diagonal search directions, while making
essentially no assumptions regarding the character-
istics of the objective or the relationship between a
series of sequential line minimizations. In the results
that follow, we show that, perhaps surprisingly, this
simple procedure outperforms both KCD and Pow-
ell’s method.
</bodyText>
<subsectionHeader confidence="0.991404">
3.2 Regularization
</subsectionHeader>
<bodyText confidence="0.999606043478261">
One potential drawback of MERT, as it is typically
implemented, is that it attempts to find the best pos-
sible set of parameters for a training set without
making any explicit efforts to find a set of param-
eters that can be expected to generalize well. For
example, let’s say that for some objective there is
a very deep but narrow minima that is surrounded
on all sides by very bad objective values. That
is, the BLEU score at the minima might be 39.1
while all surrounding plateaus have a BLEU score
that is &lt; 10. Intuitively, such a minima would be a
very bad solution, as the resulting parameters would
likely exhibit very poor generalization to other data
sets. This could be avoided by regularizing the sur-
face in order to eliminate such spurious minima.
One candidate for performing such regularization
is the continuous approximation of the MERT objec-
tive, O _ Ep,,(f). Och (2003) claimed that this ap-
proximation achieved essentially equivalent perfor-
mance to that obtained when directly using the loss
as the objective, O _ f. However, Zens et al. (2007)
found that O _ Ep,,(f) achieved substantially better
test set performance than O _ f, even though it per-
forms slightly worse on the data used to train the
parameters. Similarly, Smith and Eisner (2006) re-
ported test set gains for the related technique of min-
imum risk annealing, which incorporates a temper-
8However, we speculate that similar results could be ob-
tained using a uniform distribution over (−1, 1)
ature parameter that trades off between the smooth-
ness of the objective and the degree it reflects the
underlying piecewise constant error surface. How-
ever, the most straightforward implementation of
such methods requires a loss that can be applied at
the sentence level. If the evaluation metric of inter-
est does not have this property (e.g. BLEU), the loss
must be approximated using some surrogate, with
successful learning then being tied to how well the
surrogate captures the critical properties of the un-
derlying loss.
The techniques of Zens et al. (2007) &amp; Smith
and Eisner (2006) regularize by implicitly smooth-
ing over nearby plateaus in the error surface. We
propose an alternative scheme that operates directly
on the piecewise constant objective and that miti-
gates the problem of spurious local minima by ex-
plicitly smoothing over adjacent plateaus during the
line search. That is, when assessing the desirabil-
ity of any given plateau, we examine a fixed win-
dow w of adjacent plateaus along the direction be-
ing searched and combine their evaluation scores.
We explore two combination methods, max and
average. The former, max, assigns each plateau an
objective value that is equal to the maximum objec-
tive value in its surrounding window, while average
assigns a plateau an objective value that is equal to
its window’s average. Figure 3 illustrates both meth-
ods for regularizing the plateaus and contrasts them
with the case where no regularization is used. No-
tice that, while both methods discount spurious pits
in the objective, average still does place some value
on isolated deep plateaus, and max discounts them
completely.
Note that one potential weakness of this scheme
is the value assigned by the regularized objective
to any given point differs depending on the direc-
tion being searched. As such, it has the potential to
wreak havoc on methods such as Powell’s, which ef-
fectively attempt to learn about the curvature of the
</bodyText>
<page confidence="0.999701">
31
</page>
<bodyText confidence="0.708824">
objective from a sequence of line minimizations.
</bodyText>
<sectionHeader confidence="0.99569" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.99994052">
Three sets of experiments were performed. For the
first set, we compare the performance of Powell’s
method, KCD, and our novel stochastic search strat-
egy. We then evaluate the performance of all three
methods when the objective is regularized using the
average of adjacent plateaus for window sizes vary-
ing from 3 to 7. Finally, we repeat the regularization
experiment, but using the maximum objective value
from the adjacent plateaus. These experiments were
performed using the Chinese English evaluation data
provided for NIST MT eval 2002, 2003, and 2005.
MT02 was used as a dev set for MERT learning,
while MT03 and MT05 were used as our test sets.
For all experiments, MERT training was per-
formed using n-best lists from the decoder of size
100. During each iteration, the MERT search was
performed once with a starting point of the weights
used to generate the most recent set of n-best lists
and then 5 more times using randomly selected start-
ing points9. Of these, we retain the weights from
the search that obtained the lowest objective value.
Training continued until either decoding produced
no novel entries for the combined n-best lists or none
of the parameter values changed by more than 1e-5
across subsequent iterations.
</bodyText>
<subsectionHeader confidence="0.991098">
4.1 System
</subsectionHeader>
<bodyText confidence="0.999879">
Experiments were run using a right-to-left beam
search decoder that achieves a matching BLEU
score to Moses (Koehn et al., 2007) over a variety
of data sets. Moreover, when using the same under-
lying model, the two decoders only produce transla-
tions that differ by one or more words 0.2% of the
time. We made use of a stack size of 50 as it al-
lowed for faster experiments while only performing
modestly worse than a stack of 200. The distortion
limit was set to 6. And, we retrieved 20 translation
options for each unique source phrase.
Our phrase table was built using 1,140,693 sen-
tence pairs sampled from the GALE Y2 training
</bodyText>
<footnote confidence="0.987757666666667">
9Only 5 random restarts were used due to time constraints.
Ideally, a sizable number of random restarts should be used in
order to minimize the degree to which the results are influenced
by some runs receiving starting points that are better in general
or perhaps better/worse w.r.t. their specific optimization strat-
egy.
</footnote>
<table confidence="0.9971574">
Method Dev Test Test
MT02 MT03 MT05
KCD 30.967 30.778 29.580
Powell 30.638 30.692 29.780
Random 31.681 31.754 30.191
</table>
<tableCaption confidence="0.966075">
Table 3: BLEU scores obtained by models trained using
the three different parameter search strategies: Powell’s
method, KCD, and stochastic search.
</tableCaption>
<bodyText confidence="0.999906714285714">
data. The Chinese data was word segmented us-
ing the GALE Y2 retest release of the Stanford
CRF segmenter (Tseng et al., 2005). Phrases were
extracted using the typical approach described in
Koehn et al. (2003) of running GIZA++ (Och &amp;
Ney, 2003) in both directions and then merging
the alignments using the grow-diag-final heuristic.
From the merged alignments we also extracted a bi-
directional lexical reordering model conditioned on
the source and the target phrases (Tillmann, 2004)
(Koehn et al., 2007). A 5-gram language model
was created using the SRI language modeling toolkit
(Stolcke, 2002) and trained using the Gigaword cor-
pus and English sentences from the parallel data.
</bodyText>
<sectionHeader confidence="0.999979" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999870736842105">
As illustrated in table 3, Powell’s method and KCD
achieve a very similar level of performance, with
KCD modestly outperforming Powell on the MT03
test set while Powell modestly outperforms coordi-
nate descent on the MT05 test set. Moreover, the
fact that Powell’s algorithm did not perform better
than KCD on the training data10, and in fact actually
performed modestly worse, suggests that Powell’s
additional search machinery does not provide much
benefit for MERT objectives.
Similarly, the fact that the stochastic search ob-
tains a much higher dev set score than either Pow-
ell or KCD indicates that it is doing a better job
of optimizing the objective than either of the two
alternatives. These gains suggest that stochastic
search does make better use of the global minimum
line search than the alternative methods. Or, alter-
natively, it strengthens the claim that the method
succeeds at combining one of the critical strengths
</bodyText>
<footnote confidence="0.978632">
10This indicates that Powell failed to find a deeper minima
in the objective, since recall that the unregularized objective is
equivalent to the model’s dev set performance.
</footnote>
<page confidence="0.99129">
32
</page>
<table confidence="0.996650571428571">
Method Window Dev Test Test Method Window Dev Test Test
Avg MT02 MT03 MT05 Max MT02 MT03 MT05
Coordinate none 30.967 30.778 29.580 Coordinate none 30.967 30.778 29.580
3 31.665 31.675 30.266 3 31.536 31.927 30.334
5 31.317 31.229 30.182 5 31.484 31.702 29.687
7 31.205 31.824 30.149 7 31.627 31.294 30.199
Powell none 30.638 30.692 29.780 Powell none 30.638 30.692 29.780
3 31.333 31.412 29.890 3 31.428 30.944 29.598
5 31.748 31.777 30.334 5 31.407 31.596 30.090
7 31.249 31.571 30.161 7 30.870 30.911 29.620
Random none 31.681 31.754 30.191 Random none 31.681 31.754 30.191
3 31.548 31.778 30.263 3 31.179 30.898 29.529
5 31.336 31.647 30.415 5 30.903 31.666 29.963
7 30.501 29.336 28.372 7 31.920 31.906 30.674
</table>
<tableCaption confidence="0.985574666666667">
Table 4: BLEU scores obtained when regularizing using the average loss of adjacent plateaus, left, and the maximum
loss of adjacent plateaus, right. The none entry for each search strategy represents the baseline where no regularization
is used. Statistically significant test set gains, p &lt; 0.01, over the respective baselines are in bold face.
</tableCaption>
<bodyText confidence="0.999222555555556">
of Powell’s method, diagonal search, with coordi-
nate descent’s robustness to the sudden jumps be-
tween regions that result from global line minimiza-
tion. Using an approximate randomization test for
statistical significance (Riezler &amp; Maxwell, 2005),
and with KCD as a baseline, the gains obtained
by stochastic search on MT03 are statistically sig-
nificant (p = 0.002), as are the gains on MT05
(p = 0.005).
Table 4 indicates that performing regularization
by either averaging or taking the maximum of adja-
cent plateaus during the line search leads to gains for
both Powell’s method and KCD. However, no reli-
able additional gains appear to be had when stochas-
tic search is combined with regularization.
It may seem surprising that the regularization
gains for Powell &amp; KCD are seen not only in the test
sets but on the dev set as well. That is, in typical ap-
plications, regularization slightly decreases perfor-
mance on the data used to train the model. However,
this trend can in part be accounted for by the fact that
during training, MERT is using n-best lists for objec-
tive evaluations rather than the more expensive pro-
cess of running the decoder for each point that needs
to be checked. As such, during each iteration of
training, the decoding performance of the model ac-
tually represents its generalization performance rel-
ative to what was learned from the n-best lists cre-
ated during prior iterations. Moreover, better gen-
eralization from the prior n-best lists can also help
drive subsequent learning as there will then be more
high quality translations on the n-best lists used for
future iterations of learning. Additionally, regular-
ization can reduce search errors by reducing the risk
of getting stuck in spurious low loss pits that are in
otherwise bad regions of the space.
</bodyText>
<sectionHeader confidence="0.999823" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999987666666667">
We have presented two methods for improving the
performance of MERT. The first is a novel stochas-
tic search strategy that appears to make better use of
Och (2003)’s algorithm for finding the global min-
imum along any given search direction than either
coordinate descent or Powell’s method. The sec-
ond is a simple regularization scheme that leads to
performance gains for both coordinate descent and
Powell’s method. However, no further gains are ob-
tained by combining the stochastic search with reg-
ularization of the objective.
One quirk of the regularization scheme presented
here is that the regularization applied to any given
point in the objective varies depending upon what
direction the point is approached from. We are
currently looking at other similar regularization
schemes that maintain consistent objective values
regardless of the search direction.
</bodyText>
<sectionHeader confidence="0.999554" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.98791">
We extend our thanks to our three anonymous reviewers,
</bodyText>
<page confidence="0.996596">
33
</page>
<bodyText confidence="0.986963833333333">
Zens, R., Hasan, S., &amp; Ney, H. (2007). A system-
atic comparison of training criteria for statistical
machine translation. In EMNLP.
particularly for the depth of analysis provided. This paper
is based on work funded in part by the Defense Advanced
Research Projects Agency through IBM.
</bodyText>
<sectionHeader confidence="0.998363" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999247891891892">
Koehn, P., Hoang, H., Birch, A., Callison-Burch, C.,
Federico, M., Bertoldi, N., Cowan, B., Shen, W.,
Moran, C., Zens, R., Dyer, C., Bojar, O., Con-
stantin, A., &amp; Herbst, E. (2007). Moses: Open
source toolkit for statistical machine translation.
In ACL.
Koehn, P., Och, F. J., &amp; Marcu, D. (2003). Statistical
phrase-based translation. In HLT-NAACL.
Och, F.-J. (2003). Minimum error rate training in
statistical machine translation. In ACL.
Och, F. J., &amp; Ney, H. (2002). Discriminative train-
ing and maximum entropy models for statistical
machine translation. In ACL.
Och, F. J., &amp; Ney, H. (2003). A systematic compari-
son of various statistical alignment models. Com-
putational Linguistics, 29, 19–51.
Papineni, K., Roukos, S., Ward, T., &amp; Zhu, W.-J.
(2002). Bleu: a method for automatic evaluation
of machine translation. In ACL.
Press, W. H., Teukolsky, S. A., Vetterling, W. T., &amp;
Flannery, B. P. (2007). Numerical recipes 3rd edi-
tion: The art of scientific computing. Cambridge
University Press.
Riezler, S., &amp; Maxwell, J. T. (2005). On some pit-
falls in automatic evaluation and significance test-
ing for mt. In ACL.
Smith, D. A., &amp; Eisner, J. (2006). Minimum risk
annealing for training log-linear models. In ACL.
Stolcke, A. (2002). Srilm – an extensible language
modeling toolkit. In ICSLP.
Tillmann, C. (2004). A unigram orientation model
for statistical machine translation. In ACL.
Tseng, H., Chang, P., Andrew, G., Jurafsky, D.,
&amp; Manning, C. (2005). A conditional random
field word segmenter for sighan bakeoff 2005. In
SIGHAN Workshop on Chinese Language Pro-
cessing.
</reference>
<page confidence="0.999308">
34
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.322010">
<title confidence="0.998315">Regularization and Search for Minimum Error Rate Training</title>
<author confidence="0.406262">D</author>
<affiliation confidence="0.571621">Stanford</affiliation>
<address confidence="0.693611">Stanford, CA</address>
<email confidence="0.988963">cerd,jurafsky,manning@stanford.edu</email>
<abstract confidence="0.997418285714286">Minimum error rate training (MERT) is a widely used learning procedure for statistical machine translation models. We contrast three search strategies for MERT: Powell’s method, the variant of coordinate descent found in the Moses MERT utility, and a novel stochastic method. It is shown that the stochastic method obtains test set gains of +0.98 BLEU on MT03 and +0.61 BLEU on MT05. We also present a method for regularizing the MERT objective that achieves statistically significant gains when combined with both Powell’s method and coordinate descent.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="14057" citStr="Koehn et al., 2007" startWordPosition="2438" endWordPosition="2441">th all other candidate translations that have yet to be selected as the 1-best. And, for each of the n n-best lists, this may have to be done up to m − 1 times. 2.2 Search Strategies In this section, we review two search strategies that, in conjunction with the line search just described, can be used to drive MERT. The first, Powell’s method, was advocated by Och (2003) when MERT was first introduced for statistical machine translation. The second, which we call Koehn-coordinate descent (KCD)6, is used by the MERT utility packaged with the popular Moses statistical machine translation system (Koehn et al., 2007). 6Moses uses David Chiang’s CMERT package. Within the source file mert.c, the function that implements the overall search strategy, optimize koehn(), is based on Philipp Koehn’s Perl script for MERT optimization that was distributed with Pharaoh. (ibest − 2) · d 29 2.2.1 Powell’s Method Powell’s method (Press et al., 2007) attempts to efficiently search the objective by constructing a set of mutually non-interfering search directions. The basic procedure is as follows: (i) A collection of search directions is initialized to be the coordinates of the space being searched; (ii) The objective is</context>
<context position="24645" citStr="Koehn et al., 2007" startWordPosition="4186" endWordPosition="4189">each iteration, the MERT search was performed once with a starting point of the weights used to generate the most recent set of n-best lists and then 5 more times using randomly selected starting points9. Of these, we retain the weights from the search that obtained the lowest objective value. Training continued until either decoding produced no novel entries for the combined n-best lists or none of the parameter values changed by more than 1e-5 across subsequent iterations. 4.1 System Experiments were run using a right-to-left beam search decoder that achieves a matching BLEU score to Moses (Koehn et al., 2007) over a variety of data sets. Moreover, when using the same underlying model, the two decoders only produce translations that differ by one or more words 0.2% of the time. We made use of a stack size of 50 as it allowed for faster experiments while only performing modestly worse than a stack of 200. The distortion limit was set to 6. And, we retrieved 20 translation options for each unique source phrase. Our phrase table was built using 1,140,693 sentence pairs sampled from the GALE Y2 training 9Only 5 random restarts were used due to time constraints. Ideally, a sizable number of random resta</context>
<context position="26238" citStr="Koehn et al., 2007" startWordPosition="4452" endWordPosition="4455">dels trained using the three different parameter search strategies: Powell’s method, KCD, and stochastic search. data. The Chinese data was word segmented using the GALE Y2 retest release of the Stanford CRF segmenter (Tseng et al., 2005). Phrases were extracted using the typical approach described in Koehn et al. (2003) of running GIZA++ (Och &amp; Ney, 2003) in both directions and then merging the alignments using the grow-diag-final heuristic. From the merged alignments we also extracted a bidirectional lexical reordering model conditioned on the source and the target phrases (Tillmann, 2004) (Koehn et al., 2007). A 5-gram language model was created using the SRI language modeling toolkit (Stolcke, 2002) and trained using the Gigaword corpus and English sentences from the parallel data. 5 Results As illustrated in table 3, Powell’s method and KCD achieve a very similar level of performance, with KCD modestly outperforming Powell on the MT03 test set while Powell modestly outperforms coordinate descent on the MT05 test set. Moreover, the fact that Powell’s algorithm did not perform better than KCD on the training data10, and in fact actually performed modestly worse, suggests that Powell’s additional s</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N., Cowan, B., Shen, W., Moran, C., Zens, R., Dyer, C., Bojar, O., Constantin, A., &amp; Herbst, E. (2007). Moses: Open source toolkit for statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="25941" citStr="Koehn et al. (2003)" startWordPosition="4406" endWordPosition="4409">nfluenced by some runs receiving starting points that are better in general or perhaps better/worse w.r.t. their specific optimization strategy. Method Dev Test Test MT02 MT03 MT05 KCD 30.967 30.778 29.580 Powell 30.638 30.692 29.780 Random 31.681 31.754 30.191 Table 3: BLEU scores obtained by models trained using the three different parameter search strategies: Powell’s method, KCD, and stochastic search. data. The Chinese data was word segmented using the GALE Y2 retest release of the Stanford CRF segmenter (Tseng et al., 2005). Phrases were extracted using the typical approach described in Koehn et al. (2003) of running GIZA++ (Och &amp; Ney, 2003) in both directions and then merging the alignments using the grow-diag-final heuristic. From the merged alignments we also extracted a bidirectional lexical reordering model conditioned on the source and the target phrases (Tillmann, 2004) (Koehn et al., 2007). A 5-gram language model was created using the SRI language modeling toolkit (Stolcke, 2002) and trained using the Gigaword corpus and English sentences from the parallel data. 5 Results As illustrated in table 3, Powell’s method and KCD achieve a very similar level of performance, with KCD modestly o</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Koehn, P., Och, F. J., &amp; Marcu, D. (2003). Statistical phrase-based translation. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F-J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="777" citStr="Och (2003)" startWordPosition="112" endWordPosition="113">manning@stanford.edu Abstract Minimum error rate training (MERT) is a widely used learning procedure for statistical machine translation models. We contrast three search strategies for MERT: Powell’s method, the variant of coordinate descent found in the Moses MERT utility, and a novel stochastic method. It is shown that the stochastic method obtains test set gains of +0.98 BLEU on MT03 and +0.61 BLEU on MT05. We also present a method for regularizing the MERT objective that achieves statistically significant gains when combined with both Powell’s method and coordinate descent. 1 Introduction Och (2003) introduced minimum error rate training (MERT) as an alternative training regime to the conditional likelihood objective previously used with log-linear translation models (Och &amp; Ney, 2002). This approach attempts to improve translation quality by optimizing an automatic translation evaluation metric, such as the BLEU score (Papineni et al., 2002). This is accomplished by either directly walking the error surface provided by an evaluation metric w.r.t. the model weights or by using gradientbased techniques on a continuous approximation of such a surface. While the former is piecewise constant </context>
<context position="8381" citStr="Och (2003)" startWordPosition="1417" endWordPosition="1418">ect nearby points. In contrast, Powell’s method operates by starting with a single point in weight space, and then performing a series of line minimizations until no more progress can be made. In this paper, we focus on line minimization based techniques, such as Powell’s method. 2.1 Global minimum along a line Even without gradient information, numerous methods can be used to find, or approximately find, local minima along a line. However, by exploiting the fact that the underlying scores assigned to competing hypotheses, w · IF(e, h, f), vary linearly w.r.t. changes in the weight vector, w, Och (2003) proposed a strategy for finding the global minimum along any given search direction. The insight behind the algorithm is as follows. Let’s assume we are examining two competing ated with the derivation of each e from the foreign sentence f. If included, such variables would only change the graph in that multiple different derivations would be possible for each e&apos;. If present, the graph could then include disjoint regions that all map to the same e&apos; and thus the same objective value. 4A simplex can be thought of as a generalization of a triangle to arbitrary dimensional spaces. Figure 2: Illus</context>
<context position="12774" citStr="Och (2003)" startWordPosition="2203" endWordPosition="2204">hile the surface that is defined by the line segments with the highest classifier score for each region is convex, this is not a convex optimization problem as we are optimizing over the loss ` rather than classifier score. Pseudocode for the line search is given in algorithm 1. Letting n denote the number of foreign sentences, f, in a dataset, and having m denote the size of the individual n-best lists, |l|, the time complexity of the algorithm is given by O(nm2). This is seen in that each time we check for the nearest intersection to the current 1-best for some n-best list l, we Algorithm 1 Och (2003)’s line search method to find the global minimum in the loss, `, when starting at the point w and searching along the direction d using the candidate translations given in the collection of n-best lists L. Input: L, w, d, ` I ⇐ {} for l ∈ L do fore ∈ l do m{e} ⇐ e.features · d b{e} ⇐ e.features · w end for bestn ⇐ argmaxe.l m{e} {b{e} breaks ties} loop � � 0, b{bestn}−b{e} bestn+1 = argmine.l max m{e}−m{bestn} intercept ⇐ max(0, b{bestn}−b{bestn+1} 1 \\ m{bestn+1}−m{bestn} J if intercept &gt; 0 then add(I, intercept) else break end if end loop end for add(I, max(I) + 22) ibest = argmini.Z eval`(L</context>
<context position="20668" citStr="Och (2003)" startWordPosition="3529" endWordPosition="3530">at for some objective there is a very deep but narrow minima that is surrounded on all sides by very bad objective values. That is, the BLEU score at the minima might be 39.1 while all surrounding plateaus have a BLEU score that is &lt; 10. Intuitively, such a minima would be a very bad solution, as the resulting parameters would likely exhibit very poor generalization to other data sets. This could be avoided by regularizing the surface in order to eliminate such spurious minima. One candidate for performing such regularization is the continuous approximation of the MERT objective, O _ Ep,,(f). Och (2003) claimed that this approximation achieved essentially equivalent performance to that obtained when directly using the loss as the objective, O _ f. However, Zens et al. (2007) found that O _ Ep,,(f) achieved substantially better test set performance than O _ f, even though it performs slightly worse on the data used to train the parameters. Similarly, Smith and Eisner (2006) reported test set gains for the related technique of minimum risk annealing, which incorporates a temper8However, we speculate that similar results could be obtained using a uniform distribution over (−1, 1) ature paramete</context>
<context position="30558" citStr="Och (2003)" startWordPosition="5168" endWordPosition="5169">from the n-best lists created during prior iterations. Moreover, better generalization from the prior n-best lists can also help drive subsequent learning as there will then be more high quality translations on the n-best lists used for future iterations of learning. Additionally, regularization can reduce search errors by reducing the risk of getting stuck in spurious low loss pits that are in otherwise bad regions of the space. 6 Conclusions We have presented two methods for improving the performance of MERT. The first is a novel stochastic search strategy that appears to make better use of Och (2003)’s algorithm for finding the global minimum along any given search direction than either coordinate descent or Powell’s method. The second is a simple regularization scheme that leads to performance gains for both coordinate descent and Powell’s method. However, no further gains are obtained by combining the stochastic search with regularization of the objective. One quirk of the regularization scheme presented here is that the regularization applied to any given point in the objective varies depending upon what direction the point is approached from. We are currently looking at other similar </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Och, F.-J. (2003). Minimum error rate training in statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="966" citStr="Och &amp; Ney, 2002" startWordPosition="137" endWordPosition="140">r MERT: Powell’s method, the variant of coordinate descent found in the Moses MERT utility, and a novel stochastic method. It is shown that the stochastic method obtains test set gains of +0.98 BLEU on MT03 and +0.61 BLEU on MT05. We also present a method for regularizing the MERT objective that achieves statistically significant gains when combined with both Powell’s method and coordinate descent. 1 Introduction Och (2003) introduced minimum error rate training (MERT) as an alternative training regime to the conditional likelihood objective previously used with log-linear translation models (Och &amp; Ney, 2002). This approach attempts to improve translation quality by optimizing an automatic translation evaluation metric, such as the BLEU score (Papineni et al., 2002). This is accomplished by either directly walking the error surface provided by an evaluation metric w.r.t. the model weights or by using gradientbased techniques on a continuous approximation of such a surface. While the former is piecewise constant and thus cannot be optimized using gradient techniques, Och (2003) provides an approach that performs such training efficiently. In this paper we explore a number of variations on MERT. Fir</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Och, F. J., &amp; Ney, H. (2002). Discriminative training and maximum entropy models for statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<contexts>
<context position="25977" citStr="Och &amp; Ney, 2003" startWordPosition="4413" endWordPosition="4416">ng points that are better in general or perhaps better/worse w.r.t. their specific optimization strategy. Method Dev Test Test MT02 MT03 MT05 KCD 30.967 30.778 29.580 Powell 30.638 30.692 29.780 Random 31.681 31.754 30.191 Table 3: BLEU scores obtained by models trained using the three different parameter search strategies: Powell’s method, KCD, and stochastic search. data. The Chinese data was word segmented using the GALE Y2 retest release of the Stanford CRF segmenter (Tseng et al., 2005). Phrases were extracted using the typical approach described in Koehn et al. (2003) of running GIZA++ (Och &amp; Ney, 2003) in both directions and then merging the alignments using the grow-diag-final heuristic. From the merged alignments we also extracted a bidirectional lexical reordering model conditioned on the source and the target phrases (Tillmann, 2004) (Koehn et al., 2007). A 5-gram language model was created using the SRI language modeling toolkit (Stolcke, 2002) and trained using the Gigaword corpus and English sentences from the parallel data. 5 Results As illustrated in table 3, Powell’s method and KCD achieve a very similar level of performance, with KCD modestly outperforming Powell on the MT03 test</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Och, F. J., &amp; Ney, H. (2003). A systematic comparison of various statistical alignment models. Computational Linguistics, 29, 19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1126" citStr="Papineni et al., 2002" startWordPosition="162" endWordPosition="165"> method obtains test set gains of +0.98 BLEU on MT03 and +0.61 BLEU on MT05. We also present a method for regularizing the MERT objective that achieves statistically significant gains when combined with both Powell’s method and coordinate descent. 1 Introduction Och (2003) introduced minimum error rate training (MERT) as an alternative training regime to the conditional likelihood objective previously used with log-linear translation models (Och &amp; Ney, 2002). This approach attempts to improve translation quality by optimizing an automatic translation evaluation metric, such as the BLEU score (Papineni et al., 2002). This is accomplished by either directly walking the error surface provided by an evaluation metric w.r.t. the model weights or by using gradientbased techniques on a continuous approximation of such a surface. While the former is piecewise constant and thus cannot be optimized using gradient techniques, Och (2003) provides an approach that performs such training efficiently. In this paper we explore a number of variations on MERT. First, it is shown that performance gains can be had by making use of a stochastic search strategy as compare to that obtained by Powell’s method and 26 coordinate</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Papineni, K., Roukos, S., Ward, T., &amp; Zhu, W.-J. (2002). Bleu: a method for automatic evaluation of machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W H Press</author>
<author>S A Teukolsky</author>
<author>W T Vetterling</author>
<author>B P Flannery</author>
</authors>
<title>Numerical recipes 3rd edition: The art of scientific computing.</title>
<date>2007</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="7572" citStr="Press et al., 2007" startWordPosition="1275" endWordPosition="1278"> w&apos; = Aw b),&gt;0 will still result in the same relative rankings of all possible translations according to w · IF(e, h, f), such scaling will not change the translation selected by the argmax. At the boundaries between regions, the objective is undefined, as 2 or more candidates are assigned identical scores by the model. Thus, it is unclear what should be returned by the argmax for subsequent scoring by f. Since the objective is piecewise constant, it cannot be minimized using gradient descent or even the sub-gradient method. Two applicable methods include downhill simplex and Powell’s method (Press et al., 2007). The former attempts to find a local minimum in an n dimensional space by iteratively shrinking or growing an n + 1 vertex simplex4 based on the objective values of the current vertex points and select nearby points. In contrast, Powell’s method operates by starting with a single point in weight space, and then performing a series of line minimizations until no more progress can be made. In this paper, we focus on line minimization based techniques, such as Powell’s method. 2.1 Global minimum along a line Even without gradient information, numerous methods can be used to find, or approximatel</context>
<context position="14382" citStr="Press et al., 2007" startWordPosition="2489" endWordPosition="2492">t, Powell’s method, was advocated by Och (2003) when MERT was first introduced for statistical machine translation. The second, which we call Koehn-coordinate descent (KCD)6, is used by the MERT utility packaged with the popular Moses statistical machine translation system (Koehn et al., 2007). 6Moses uses David Chiang’s CMERT package. Within the source file mert.c, the function that implements the overall search strategy, optimize koehn(), is based on Philipp Koehn’s Perl script for MERT optimization that was distributed with Pharaoh. (ibest − 2) · d 29 2.2.1 Powell’s Method Powell’s method (Press et al., 2007) attempts to efficiently search the objective by constructing a set of mutually non-interfering search directions. The basic procedure is as follows: (i) A collection of search directions is initialized to be the coordinates of the space being searched; (ii) The objective is minimized by looping through the search directions and performing a line minimization for each; (iii) A new search direction is constructed that summarizes the cumulative direction of the progress made during step (ii) (i.e., dnew — wpreii − wpostii). After a line minimization is performed along dnew, it is used to replace</context>
<context position="17045" citStr="Press et al., 2007" startWordPosition="2926" endWordPosition="2929">ludes a vestigial implementation of Powell’s method, the code is hardwired to call optimize koehn rather than the routine for Powell. 3.1 Random Search Directions One significant advantage of Powell’s algorithm over coordinate descent is that it can optimize along diagonal search directions in weight space. That is, given a model with a dozen or so features, it can explore gains that are to be had by simultaneously varying two or more of the feature weights. In general, the diagonals that Powell’s method constructs allow it to walk objective functions more efficiently than coordinate descent (Press et al., 2007). However, given that we have a line search algorithm that will find the global minima along any given search direction, diagonal search may be of even more value. That is, similar to ridge phenomenon that arise in traditional hill climbing search, it is possible that there are points in the objective that are the global minimum along any given coordinate direction, but are not the global minimum along diagonal directions. However, one substantial disadvantage for Powell is that the assumptions it uses to build up the diagonal search directions do not hold in the present context. Specifically,</context>
</contexts>
<marker>Press, Teukolsky, Vetterling, Flannery, 2007</marker>
<rawString>Press, W. H., Teukolsky, S. A., Vetterling, W. T., &amp; Flannery, B. P. (2007). Numerical recipes 3rd edition: The art of scientific computing. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riezler</author>
<author>J T Maxwell</author>
</authors>
<title>On some pitfalls in automatic evaluation and significance testing for mt.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="28843" citStr="Riezler &amp; Maxwell, 2005" startWordPosition="4872" endWordPosition="4875"> 28.372 7 31.920 31.906 30.674 Table 4: BLEU scores obtained when regularizing using the average loss of adjacent plateaus, left, and the maximum loss of adjacent plateaus, right. The none entry for each search strategy represents the baseline where no regularization is used. Statistically significant test set gains, p &lt; 0.01, over the respective baselines are in bold face. of Powell’s method, diagonal search, with coordinate descent’s robustness to the sudden jumps between regions that result from global line minimization. Using an approximate randomization test for statistical significance (Riezler &amp; Maxwell, 2005), and with KCD as a baseline, the gains obtained by stochastic search on MT03 are statistically significant (p = 0.002), as are the gains on MT05 (p = 0.005). Table 4 indicates that performing regularization by either averaging or taking the maximum of adjacent plateaus during the line search leads to gains for both Powell’s method and KCD. However, no reliable additional gains appear to be had when stochastic search is combined with regularization. It may seem surprising that the regularization gains for Powell &amp; KCD are seen not only in the test sets but on the dev set as well. That is, in t</context>
</contexts>
<marker>Riezler, Maxwell, 2005</marker>
<rawString>Riezler, S., &amp; Maxwell, J. T. (2005). On some pitfalls in automatic evaluation and significance testing for mt. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Smith</author>
<author>J Eisner</author>
</authors>
<title>Minimum risk annealing for training log-linear models.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="21045" citStr="Smith and Eisner (2006)" startWordPosition="3591" endWordPosition="3594">tion to other data sets. This could be avoided by regularizing the surface in order to eliminate such spurious minima. One candidate for performing such regularization is the continuous approximation of the MERT objective, O _ Ep,,(f). Och (2003) claimed that this approximation achieved essentially equivalent performance to that obtained when directly using the loss as the objective, O _ f. However, Zens et al. (2007) found that O _ Ep,,(f) achieved substantially better test set performance than O _ f, even though it performs slightly worse on the data used to train the parameters. Similarly, Smith and Eisner (2006) reported test set gains for the related technique of minimum risk annealing, which incorporates a temper8However, we speculate that similar results could be obtained using a uniform distribution over (−1, 1) ature parameter that trades off between the smoothness of the objective and the degree it reflects the underlying piecewise constant error surface. However, the most straightforward implementation of such methods requires a loss that can be applied at the sentence level. If the evaluation metric of interest does not have this property (e.g. BLEU), the loss must be approximated using some </context>
</contexts>
<marker>Smith, Eisner, 2006</marker>
<rawString>Smith, D. A., &amp; Eisner, J. (2006). Minimum risk annealing for training log-linear models. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>Srilm – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In ICSLP.</booktitle>
<contexts>
<context position="26331" citStr="Stolcke, 2002" startWordPosition="4468" endWordPosition="4469">hastic search. data. The Chinese data was word segmented using the GALE Y2 retest release of the Stanford CRF segmenter (Tseng et al., 2005). Phrases were extracted using the typical approach described in Koehn et al. (2003) of running GIZA++ (Och &amp; Ney, 2003) in both directions and then merging the alignments using the grow-diag-final heuristic. From the merged alignments we also extracted a bidirectional lexical reordering model conditioned on the source and the target phrases (Tillmann, 2004) (Koehn et al., 2007). A 5-gram language model was created using the SRI language modeling toolkit (Stolcke, 2002) and trained using the Gigaword corpus and English sentences from the parallel data. 5 Results As illustrated in table 3, Powell’s method and KCD achieve a very similar level of performance, with KCD modestly outperforming Powell on the MT03 test set while Powell modestly outperforms coordinate descent on the MT05 test set. Moreover, the fact that Powell’s algorithm did not perform better than KCD on the training data10, and in fact actually performed modestly worse, suggests that Powell’s additional search machinery does not provide much benefit for MERT objectives. Similarly, the fact that t</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Stolcke, A. (2002). Srilm – an extensible language modeling toolkit. In ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Tillmann</author>
</authors>
<title>A unigram orientation model for statistical machine translation.</title>
<date>2004</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="26217" citStr="Tillmann, 2004" startWordPosition="4450" endWordPosition="4451">es obtained by models trained using the three different parameter search strategies: Powell’s method, KCD, and stochastic search. data. The Chinese data was word segmented using the GALE Y2 retest release of the Stanford CRF segmenter (Tseng et al., 2005). Phrases were extracted using the typical approach described in Koehn et al. (2003) of running GIZA++ (Och &amp; Ney, 2003) in both directions and then merging the alignments using the grow-diag-final heuristic. From the merged alignments we also extracted a bidirectional lexical reordering model conditioned on the source and the target phrases (Tillmann, 2004) (Koehn et al., 2007). A 5-gram language model was created using the SRI language modeling toolkit (Stolcke, 2002) and trained using the Gigaword corpus and English sentences from the parallel data. 5 Results As illustrated in table 3, Powell’s method and KCD achieve a very similar level of performance, with KCD modestly outperforming Powell on the MT03 test set while Powell modestly outperforms coordinate descent on the MT05 test set. Moreover, the fact that Powell’s algorithm did not perform better than KCD on the training data10, and in fact actually performed modestly worse, suggests that </context>
</contexts>
<marker>Tillmann, 2004</marker>
<rawString>Tillmann, C. (2004). A unigram orientation model for statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Tseng</author>
<author>P Chang</author>
<author>G Andrew</author>
<author>D Jurafsky</author>
<author>C Manning</author>
</authors>
<title>A conditional random field word segmenter for sighan bakeoff</title>
<date>2005</date>
<booktitle>In SIGHAN Workshop on Chinese Language Processing.</booktitle>
<contexts>
<context position="25857" citStr="Tseng et al., 2005" startWordPosition="4393" endWordPosition="4396">m restarts should be used in order to minimize the degree to which the results are influenced by some runs receiving starting points that are better in general or perhaps better/worse w.r.t. their specific optimization strategy. Method Dev Test Test MT02 MT03 MT05 KCD 30.967 30.778 29.580 Powell 30.638 30.692 29.780 Random 31.681 31.754 30.191 Table 3: BLEU scores obtained by models trained using the three different parameter search strategies: Powell’s method, KCD, and stochastic search. data. The Chinese data was word segmented using the GALE Y2 retest release of the Stanford CRF segmenter (Tseng et al., 2005). Phrases were extracted using the typical approach described in Koehn et al. (2003) of running GIZA++ (Och &amp; Ney, 2003) in both directions and then merging the alignments using the grow-diag-final heuristic. From the merged alignments we also extracted a bidirectional lexical reordering model conditioned on the source and the target phrases (Tillmann, 2004) (Koehn et al., 2007). A 5-gram language model was created using the SRI language modeling toolkit (Stolcke, 2002) and trained using the Gigaword corpus and English sentences from the parallel data. 5 Results As illustrated in table 3, Powe</context>
</contexts>
<marker>Tseng, Chang, Andrew, Jurafsky, Manning, 2005</marker>
<rawString>Tseng, H., Chang, P., Andrew, G., Jurafsky, D., &amp; Manning, C. (2005). A conditional random field word segmenter for sighan bakeoff 2005. In SIGHAN Workshop on Chinese Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>