<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001682">
<title confidence="0.785188">
UP13: Knowledge-poor Methods (Sometimes) Perform Poorly
</title>
<author confidence="0.46844">
Thierry Poibeau
</author>
<note confidence="0.976798">
Laboratoire d&apos;Informatique de Paris-Nord
CNRS UMR 7030 et universit6 Paris 13
</note>
<address confidence="0.483156">
99, avenue J.-B. Cl6ment F-93430 Villetaneuse
</address>
<email confidence="0.976085">
thierry.poibeau@lipn.univ-paris13.fr
</email>
<sectionHeader confidence="0.998477" genericHeader="abstract">
Abstract
</sectionHeader>
<subsectionHeader confidence="0.601278">
This short paper presents a system developed at
</subsectionHeader>
<bodyText confidence="0.980093">
the Universit6 Paris 13 for the Semeval 2007
Metonymy Resolution Task (task #08, location
name track; see Markert and Nissim, 2007).
The system makes use of plain word forms
only. In this paper, we evaluate the accuracy of
this minimalist approach, compare it to a more
complex one which uses both syntactic and
semantic features, and discuss its usefulness for
metonymy resolution in general.
</bodyText>
<sectionHeader confidence="0.999414" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9881141">
This short paper presents the system developed at
the Universit6 Paris 13 for the Metonymy
resolution task, during Semeval 2007 (Markert and
Nissim, 2007). Two sub-tasks were proposed,
concerning 1) country names and 2) company
names. We only participated in the first task
(country names). We developed a simple approach
which we present and thoroughly evaluate in this
paper. We discuss the relevance of this approach
and compare it to more complex ones.
</bodyText>
<sectionHeader confidence="0.995491" genericHeader="introduction">
2 Motivation
</sectionHeader>
<bodyText confidence="0.99997735">
We participated in the metonymy task with a very
basic system. The idea was to investigate the
efficiency of a minimalist (though, not Chomskian)
system. This system tags entities on the basis of
discriminative (plain) word forms occurring in a
given window only. Our aim was to find out which
word forms are discriminative enough to be
considered as parameters.
In the past, we developed a system for
metonymy resolution for French, evaluated in the
framework of the ESTER evaluation (Gravier,
2004). This system, described in Poibeau (2006),
uses various kinds of information, among others:
plain word forms, part-of-speech tags, and
syntactic and semantic tags (conceptual word
classes).
The usefulness of complex linguistic features
(especially syntactic and semantic tags) is
questionable: they may be hard to compute, error-
prone and their contribution is not clear. We
therefore developed a new version of the system
mainly based on 1) a distributional analysis (on
surface word forms) along with 2) a filtering
process. The latter restricted metonymic readings
to country and capital names (as opposed to other
location names), since they include a vast majority
of the metonymic readings (this proved to be
efficient but is of course a harsh pragmatic over-
simplification without real linguistic basis). We
nevertheless obtained a highly versatile system,
performing reasonably well, compared to our
previous, much more complex implementation
(F-score was .58 instead of .63; we computed
F-score with β= 1).
In the framework of the Semeval evaluation, the
filtering process is irrelevant since only country
names are considered as entities. However, we
thought that it would be interesting to develop a
very basic system, to evaluate the performance one
can obtain using plain word forms only.
</bodyText>
<sectionHeader confidence="0.994887" genericHeader="method">
3 A (too) Lazy Approach
</sectionHeader>
<bodyText confidence="0.999973714285714">
We chose not to use any part-of-speech tagger or
syntactic or semantic analyzer; we did not use any
external knowledge or any other annotated corpus
than the one provided for the training phase. Since
no NLP tool was used, we had to duplicate most of
the words in order to get the singular and the plural
form. Our system is thus very simple compared to
</bodyText>
<page confidence="0.966173">
418
</page>
<bodyText confidence="0.991243805555555">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 418–421,
Prague, June 2007. c�2007 Association for Computational Linguistics
the state-of-art in this domain (e.g. Nissim and
Markert, 2003).
We used discriminative plain words only. These
are gathered as follows: all the words in a given
window (here we use a 7 word window, before and
after the target entity since it gave the best results
on the training data) are extracted and associated
with two classes (literal vs. non literal). We thus
consider the most discriminative words, i.e. words
that appear frequently in some contexts but not in
others (literal vs. non-literal readings).
Discriminative words are elements that are
abnormally frequent or rare in one corpus
compared to another one.
Characteristic features are selected based on
their probabilities. Probability levels measure the
significance of the differences between the relative
frequency of an expression or a feature within a
group (or a category) with its global relative
frequency calculated over the entire corpus (Lafon,
1980). They are calculated under the hypothesis of
a random distribution of the forms. The smaller the
probability levels, the more characteristic the
corresponding forms (Lebart and Salem, 1997).
We thus obtained 4 lists of discriminative words
(literal vs. non-literal X before vs. after the target
entity). As the result, some semantic families
emerged, especially for words appearing before
literal readings: lists of prepositions (in, at,
within...) and geographical items (east, west,
western...). Some lists were manually completed,
when a &amp;quot;natural&amp;quot; series appeared to be incomplete
(for example, if we got east, west, north, we
completed the word series with south).
</bodyText>
<subsectionHeader confidence="0.999877">
3.1 Reducing the Size of the Search Space
</subsectionHeader>
<bodyText confidence="0.999850785714286">
The approach described so far may seems a bit
simplistic (and, indeed, it is!), but nevertheless it
yielded highly discriminative features. For
example, if we only tag country names
immediately preceded by the preposition in as
`literal&apos;, we obtain the results presented in table 1
(in the following tables, precision is the most
relevant issue; coverage gives an idea of the
percentage of tagged entities by the considered
feature, compared to the total number of entities to
be tagged). Figure 1 shows that detecting the
preposition in in front of a location name
discriminates almost perfectly 23% of the literal
readings.
</bodyText>
<table confidence="0.997895333333333">
Training Test
Precision 1 .98
Coverage .23 .23
</table>
<tableCaption confidence="0.77177">
Table 1. Results for the pattern in + LOC
(result tag = literal)
</tableCaption>
<bodyText confidence="0.995288777777778">
A simple discriminative analysis of the training
corpus produces the following list of prepositions
and geographical discriminative features: &amp;quot;at&amp;quot;,
&amp;quot;within&amp;quot;, &amp;quot;in&amp;quot;, &amp;quot;into&amp;quot;, &amp;quot;from&amp;quot;, &amp;quot;coast&amp;quot;,
&amp;quot;land&amp;quot;, &amp;quot;area&amp;quot;, &amp;quot;southern&amp;quot;, &amp;quot;south&amp;quot;, &amp;quot;east&amp;quot;,
&amp;quot;north&amp;quot;, &amp;quot;west&amp;quot;, &amp;quot;western&amp;quot;, &amp;quot;eastern&amp;quot;, etc 1.
Table 2 presents the results obtained from this list
of words (occurring in a 7 word window, on the
left of the target word):
</bodyText>
<table confidence="0.999504333333333">
Training Test
Precision .9 1 .88
Coverage .60 .55
</table>
<tableCaption confidence="0.9977765">
Table 2. Results for the pattern &lt;at+within+...&gt;
+ LOC (note that table 1 is contained in table 2)
</tableCaption>
<bodyText confidence="0.772214">
Another typical feature was the use of the entity in
a genitive construction (e.g. in Iran&apos;s official
commitment, Iran is considered as a literal
reading). The presence of &apos;s on the right side of the
target entity is highly discriminative (table 3):
</bodyText>
<table confidence="0.999317">
Training Test
Precision .87 .89
Coverage . 15 . 17
</table>
<tableCaption confidence="0.9810835">
Table 3. Results for the pattern LOC’s
(result tag = literal)
</tableCaption>
<bodyText confidence="0.999755416666667">
This strategy may seem strange, since the task is to
find metonymic readings rather than literal ones
(the baseline is to tag all the target entities as
literal). However, it is useful in reducing the size
of the search space by approximately 50%. This
means that more than 70% of the entities with a
literal meaning can be tagged with a confidence
around 90% using this technique, thus reducing the
number of problematic cases. The resulting file is
relatively balanced: it contains about 50-60% of
literal meaning and 40-50% of metaphorical
meaning (instead of a classical ratio 80% vs. 20%).
</bodyText>
<footnote confidence="0.658546333333333">
1 The list also contains nouns and verbs like: &amp;quot;enter&amp;quot;,
&amp;quot;entered&amp;quot;, &amp;quot;fly&amp;quot;, &amp;quot;flown&amp;quot;, &amp;quot;went&amp;quot;, &amp;quot;go&amp;quot;, &amp;quot;come&amp;quot;,
&amp;quot;land&amp;quot;, &amp;quot;country&amp;quot;, &amp;quot;mountain&amp;quot;...
</footnote>
<page confidence="0.992839">
419
</page>
<subsectionHeader confidence="0.944606">
3.2 Loolung for Metonymy, Desperately ...
</subsectionHeader>
<bodyText confidence="0.996851111111111">
We used the same strategy for metonymic
readings. We have observed in the past that word
forms are much more efficient for literal readings
than for metonymic readings. However, the fact
that the location name is followed by a verb like
&amp;quot;has&amp;quot;, &amp;quot;should&amp;quot;, &amp;quot;was&amp;quot;, &amp;quot;would&amp;quot;, &amp;quot;will&amp;quot;
seemed to be discriminative on the training corpus.
Unfortunately, this feature did not work well on
the test corpus (table 4).
</bodyText>
<table confidence="0.998765333333333">
Training Test
Precision .6 .3
Coverage . 1 .04
</table>
<tableCaption confidence="0.7985335">
Table 4. Results for the pattern LOC + &lt;was,
should...&gt; (result tag = metonymic)
</tableCaption>
<bodyText confidence="0.999427705882353">
This simply means that a syntactic analysis would
be useful to discriminate between the sentences
where the target entity is the subject of the
following verb (in this context, the entity is most of
the time used with a metaphoric reading; to go
further, one needs to filter the verb according to
semantic classes).
Another point that was clear from the task
guidelines was that sport&apos;s teams correspond to
metonymic readings. The list of characteristic
words for this class, obtained from the training
corpus was the following: &amp;quot;player&amp;quot;, &amp;quot;team&amp;quot;,
&amp;quot;defender&amp;quot;, &amp;quot;plays&amp;quot;, &amp;quot;role&amp;quot;, &amp;quot;score&amp;quot;,
&amp;quot;scores&amp;quot;, &amp;quot;scored&amp;quot;, &amp;quot;win&amp;quot;, &amp;quot;won&amp;quot;, &amp;quot;cup&amp;quot;, &amp;quot;v&amp;quot;2,
&amp;quot;against&amp;quot;, &amp;quot;penalty&amp;quot;, &amp;quot;goal&amp;quot;, &amp;quot;goals&amp;quot;,
&amp;quot;champion&amp;quot;, &amp;quot;champions&amp;quot;, etc. But, bad luck!
This list did not work well on the test corpus either:
</bodyText>
<table confidence="0.998225333333333">
Training Test
Precision .64 .32
Coverage . 13 .05
</table>
<tableCaption confidence="0.8857685">
Table 5. Results for the pattern LOC +
&lt;player, team...&gt; (result tag= metonymic)
Table 5 shows that coverage as well as precision
are very low.
</tableCaption>
<bodyText confidence="0.968391133333333">
Yet another category included words related to
the political role of countries, which entails a
metonymic reading: &amp;quot;role&amp;quot;, &amp;quot;institution&amp;quot;,
&amp;quot;preoccupation&amp;quot;, &amp;quot;attitude&amp;quot;, &amp;quot;ally&amp;quot;,
&amp;quot;allies&amp;quot;, &amp;quot;institutions&amp;quot;, &amp;quot;initiative&amp;quot;,
2 v for versus, especially in sports: Arsenal-MU 3 v 2.
&amp;quot;according&amp;quot;, &amp;quot;authority&amp;quot;... All these categories
had low coverage on the test corpus. This is not so
surprising and is related to our knowledge-poor
strategy: the training corpus is relatively small and
it was foreseeable that we would miss most of the
relevant contexts. However, we wanted to maintain
precision above .5 (i.e. relevant contexts should
remain relevant), but failed in this, as one can see
from the overall results.
</bodyText>
<sectionHeader confidence="0.992709" genericHeader="method">
4 0verall Evaluation
</sectionHeader>
<bodyText confidence="0.995564791666667">
We mainly discuss here the results of the coarse
evaluation, where only literal vs non-literal
meanings were targeted. We did not develop any
specific strategy for the other tracks (medium and
fine) since there were too few examples in the
training data. We just transferred non-literal
readings to the most probable class according to
the training corpus (metonymic for medium,
place-for-people for fine). However, the
performance of our system (i.e. accuracy) is
relatively stable between these three tracks, since
the distribution of examples between the different
classes is very unequally distributed.
Before giving the results, recall that our purpose
was to investigate a knowledge-poor strategy, in
order to establish how far one can get using only
surface indicators. Thus, unsurprisingly, our results
for the training corpus were already lower than
those obtained using a more sophisticated system
(Nissim and Markert, 2003). They are however a
good indicator of performance when one uses only
surface features.
The accuracy on the training corpus was .8 15.
Precision and recall are presented in the table 6.
</bodyText>
<table confidence="0.99860025">
Literal Non-lit.
Precision .88 .54
Recall .88 .57
P&amp;R .88 .55
</table>
<tableCaption confidence="0.972943">
Table 6. Overall results on the training corpus
</tableCaption>
<bodyText confidence="0.9994012">
Accuracy on the test corpus is .754 only. Table 7
shows the results obtained for the different kinds of
location names. The result is obvious: there is a
significant drop in both recall and precision,
compared to the results on the training corpus.
</bodyText>
<page confidence="0.993257">
420
</page>
<table confidence="0.999574">
Literal Non lit.
Precision .83 .38
Recall .86 .3 1
P&amp;R .84 .34
</table>
<tableCaption confidence="0.997984">
Table 7. Overall results on the test corpus
</tableCaption>
<sectionHeader confidence="0.999052" genericHeader="evaluation">
5 Discussion
</sectionHeader>
<bodyText confidence="0.963779781818182">
Metonymy is a complex linguistic phenomenon
and it is thus no surprise that such a basic system
performed badly, even if the drop in precision
between training and test set was disappointing.
The main conclusion of this approach is that
surface forms can be used to reduce the size of the
search space with a relatively good accuracy. A
large part of the literal readings can be tagged
using surface forms only. For the remaining cases,
the use of more sophisticated linguistic information
(both syntactic and semantic) is necessary.
During this work, we discovered some
problematic target entities whose annotation is
challenging. For instance, we tagged the following
example as metonymic (because of the keywords
“role” and “above”), whereas it is tagged as
literal in the gold standard:
This two-track approach was seen (...) as
reflecting continued manoeuvring over
the role of the &lt;annot&gt; &lt;location
reading=&amp;quot;literal&amp;quot;&gt; United States
&lt;/location&gt; &lt;/annot&gt; in the alliance, ...
See also the following example (tagged by our
system as metonymic because of the keyword
“relations”, but assumed to be literal in the gold
standard):
Relations with China and &lt;annot&gt;
&lt;location reading=&amp;quot;literal&amp;quot;&gt; Singapore
&lt;/location&gt;&lt;/annot&gt; ...
On the other hand, the following example was
tagged as literal by our system (due to the
preposition in) instead of metonymic.
After their European Championship
victory (...), Holland will be expected
to do well in &lt;annot&gt; &lt;location
reading=&amp;quot;metonymic&amp;quot; metotype=&amp;quot;place-
for-event&amp;quot;&gt; Italy &lt;/location&gt;&lt;/annot&gt;.
If Italy is assumed to refer to the World Cup
occurring in Italy, we think that the literal reading
is not completely irrelevant (a paraphrase could be:
&amp;quot;...to do well during their stay in Italy&amp;quot; which is
clearly literal).
Metonymy is a form of figurative speech &amp;quot;in
which one expression is used to refer to the
referent of a related one&amp;quot; (Markert and Nissim,
2007). The phenomenon corresponds to a semantic
shift in interpretation (&amp;quot;a profile shift&amp;quot;) that
appears to be a function of salience (Cruse and
Croft, 2004). We assume that this semantic shift
does not completely erase the original referent: it
rather puts the focus on a specific feature of the
content (&amp;quot;the profile&amp;quot;) of the standard referent. If
we adopt this theory, we can explain why it may be
difficult to tag some examples, since both readings
may co-exist.
</bodyText>
<sectionHeader confidence="0.999817" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999850125">
In this paper, we presented a (minimalist) system
for metonymy resolution and evaluated its
usefulness for the task. The system worked well
for reducing the size of the search space but
performed badly for the recognition of metonymic
readings themselves. It should be used in
combination with more complex features,
especially syntactic and semantic information.
</bodyText>
<sectionHeader confidence="0.999439" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9996215">
A. Cruse and W. Croft. 2004. Meaning in language, an
introduction to semantics and pragmatics. Oxford
University Press, Oxford.
G. Gravier, J.-F. Bonastre, E. Geoffrois, S. Galliano, K.
Mc Tait and K. Choukri. 2004. The ESTER
evaluation campaign for the rich transcription of
French broadcast news&amp;quot;. Proceedings of LREC&apos;04.
Lisbon, Portugal. pp. 885-888.
P. Lafon. 1980. Sur la variabilit6 de la fr6quence des
formes dans un corpus. Mots. 1. pp. 127-165.
L. Lebart and A. Salem. 1997. Exploring Textual Data.
Springer. Berlin.
K. Markert and M. Nissim. 2007. Task08: Metonymy
Resolution at Semeval 2007. Proceedings of Semeval
2007. Prague, Czech Rep.
M. Nissim and K. Markert. 2003. Syntactic Features
and Word Similarity for supervised Metonymy
Resolution. Proceedings ofACL&apos;03. Sapporo, Japan.
pp. 56-63.
T. Poibeau. 2006. Dealing with Metonymic Readings of
Named Entities. Proceedings of C0GSCI&apos;06.
Vancouver, Canada. pp. 1962-1968.
</reference>
<page confidence="0.998844">
421
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.206427">
<title confidence="0.679555">UP13: Knowledge-poor Methods (Sometimes) Perform Poorly</title>
<author confidence="0.526484">Thierry Poibeau</author>
<note confidence="0.81740625">Laboratoire d&apos;Informatique de Paris-Nord CNRS UMR 7030 et universit6 Paris 13 99, avenue J.-B. Cl6ment F-93430 Villetaneuse thierry.poibeau@lipn.univ-paris13.fr</note>
<abstract confidence="0.992784181818182">This short paper presents a system developed at the Universit6 Paris 13 for the Semeval 2007 Metonymy Resolution Task (task #08, location name track; see Markert and Nissim, 2007). The system makes use of plain word forms only. In this paper, we evaluate the accuracy of this minimalist approach, compare it to a more complex one which uses both syntactic and semantic features, and discuss its usefulness for metonymy resolution in general.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Cruse</author>
<author>W Croft</author>
</authors>
<title>Meaning in language, an introduction to semantics and pragmatics.</title>
<date>2004</date>
<publisher>Oxford University Press,</publisher>
<location>Oxford.</location>
<contexts>
<context position="13471" citStr="Cruse and Croft, 2004" startWordPosition="2127" endWordPosition="2130">o do well in &lt;annot&gt; &lt;location reading=&amp;quot;metonymic&amp;quot; metotype=&amp;quot;placefor-event&amp;quot;&gt; Italy &lt;/location&gt;&lt;/annot&gt;. If Italy is assumed to refer to the World Cup occurring in Italy, we think that the literal reading is not completely irrelevant (a paraphrase could be: &amp;quot;...to do well during their stay in Italy&amp;quot; which is clearly literal). Metonymy is a form of figurative speech &amp;quot;in which one expression is used to refer to the referent of a related one&amp;quot; (Markert and Nissim, 2007). The phenomenon corresponds to a semantic shift in interpretation (&amp;quot;a profile shift&amp;quot;) that appears to be a function of salience (Cruse and Croft, 2004). We assume that this semantic shift does not completely erase the original referent: it rather puts the focus on a specific feature of the content (&amp;quot;the profile&amp;quot;) of the standard referent. If we adopt this theory, we can explain why it may be difficult to tag some examples, since both readings may co-exist. 6 Conclusion In this paper, we presented a (minimalist) system for metonymy resolution and evaluated its usefulness for the task. The system worked well for reducing the size of the search space but performed badly for the recognition of metonymic readings themselves. It should be used in </context>
</contexts>
<marker>Cruse, Croft, 2004</marker>
<rawString>A. Cruse and W. Croft. 2004. Meaning in language, an introduction to semantics and pragmatics. Oxford University Press, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gravier</author>
<author>J-F Bonastre</author>
<author>E Geoffrois</author>
<author>S Galliano</author>
<author>K Mc Tait</author>
<author>K Choukri</author>
</authors>
<title>The ESTER evaluation campaign for the rich transcription of French broadcast news&amp;quot;.</title>
<date>2004</date>
<booktitle>Proceedings of LREC&apos;04.</booktitle>
<pages>885--888</pages>
<location>Lisbon,</location>
<marker>Gravier, Bonastre, Geoffrois, Galliano, Tait, Choukri, 2004</marker>
<rawString>G. Gravier, J.-F. Bonastre, E. Geoffrois, S. Galliano, K. Mc Tait and K. Choukri. 2004. The ESTER evaluation campaign for the rich transcription of French broadcast news&amp;quot;. Proceedings of LREC&apos;04. Lisbon, Portugal. pp. 885-888.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Lafon</author>
</authors>
<title>Sur la variabilit6 de la fr6quence des formes dans un corpus.</title>
<date>1980</date>
<journal>Mots.</journal>
<volume>1</volume>
<pages>127--165</pages>
<contexts>
<context position="4448" citStr="Lafon, 1980" startWordPosition="692" endWordPosition="693"> with two classes (literal vs. non literal). We thus consider the most discriminative words, i.e. words that appear frequently in some contexts but not in others (literal vs. non-literal readings). Discriminative words are elements that are abnormally frequent or rare in one corpus compared to another one. Characteristic features are selected based on their probabilities. Probability levels measure the significance of the differences between the relative frequency of an expression or a feature within a group (or a category) with its global relative frequency calculated over the entire corpus (Lafon, 1980). They are calculated under the hypothesis of a random distribution of the forms. The smaller the probability levels, the more characteristic the corresponding forms (Lebart and Salem, 1997). We thus obtained 4 lists of discriminative words (literal vs. non-literal X before vs. after the target entity). As the result, some semantic families emerged, especially for words appearing before literal readings: lists of prepositions (in, at, within...) and geographical items (east, west, western...). Some lists were manually completed, when a &amp;quot;natural&amp;quot; series appeared to be incomplete (for example, i</context>
</contexts>
<marker>Lafon, 1980</marker>
<rawString>P. Lafon. 1980. Sur la variabilit6 de la fr6quence des formes dans un corpus. Mots. 1. pp. 127-165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lebart</author>
<author>A Salem</author>
</authors>
<title>Exploring Textual Data.</title>
<date>1997</date>
<publisher>Springer.</publisher>
<location>Berlin.</location>
<contexts>
<context position="4638" citStr="Lebart and Salem, 1997" startWordPosition="718" endWordPosition="721">literal readings). Discriminative words are elements that are abnormally frequent or rare in one corpus compared to another one. Characteristic features are selected based on their probabilities. Probability levels measure the significance of the differences between the relative frequency of an expression or a feature within a group (or a category) with its global relative frequency calculated over the entire corpus (Lafon, 1980). They are calculated under the hypothesis of a random distribution of the forms. The smaller the probability levels, the more characteristic the corresponding forms (Lebart and Salem, 1997). We thus obtained 4 lists of discriminative words (literal vs. non-literal X before vs. after the target entity). As the result, some semantic families emerged, especially for words appearing before literal readings: lists of prepositions (in, at, within...) and geographical items (east, west, western...). Some lists were manually completed, when a &amp;quot;natural&amp;quot; series appeared to be incomplete (for example, if we got east, west, north, we completed the word series with south). 3.1 Reducing the Size of the Search Space The approach described so far may seems a bit simplistic (and, indeed, it is!)</context>
</contexts>
<marker>Lebart, Salem, 1997</marker>
<rawString>L. Lebart and A. Salem. 1997. Exploring Textual Data. Springer. Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Markert</author>
<author>M Nissim</author>
</authors>
<title>Task08: Metonymy Resolution at Semeval</title>
<date>2007</date>
<booktitle>Proceedings of Semeval</booktitle>
<location>Prague, Czech Rep.</location>
<contexts>
<context position="853" citStr="Markert and Nissim, 2007" startWordPosition="121" endWordPosition="124">3.fr Abstract This short paper presents a system developed at the Universit6 Paris 13 for the Semeval 2007 Metonymy Resolution Task (task #08, location name track; see Markert and Nissim, 2007). The system makes use of plain word forms only. In this paper, we evaluate the accuracy of this minimalist approach, compare it to a more complex one which uses both syntactic and semantic features, and discuss its usefulness for metonymy resolution in general. 1 Introduction This short paper presents the system developed at the Universit6 Paris 13 for the Metonymy resolution task, during Semeval 2007 (Markert and Nissim, 2007). Two sub-tasks were proposed, concerning 1) country names and 2) company names. We only participated in the first task (country names). We developed a simple approach which we present and thoroughly evaluate in this paper. We discuss the relevance of this approach and compare it to more complex ones. 2 Motivation We participated in the metonymy task with a very basic system. The idea was to investigate the efficiency of a minimalist (though, not Chomskian) system. This system tags entities on the basis of discriminative (plain) word forms occurring in a given window only. Our aim was to find </context>
<context position="13319" citStr="Markert and Nissim, 2007" startWordPosition="2103" endWordPosition="2106">gged as literal by our system (due to the preposition in) instead of metonymic. After their European Championship victory (...), Holland will be expected to do well in &lt;annot&gt; &lt;location reading=&amp;quot;metonymic&amp;quot; metotype=&amp;quot;placefor-event&amp;quot;&gt; Italy &lt;/location&gt;&lt;/annot&gt;. If Italy is assumed to refer to the World Cup occurring in Italy, we think that the literal reading is not completely irrelevant (a paraphrase could be: &amp;quot;...to do well during their stay in Italy&amp;quot; which is clearly literal). Metonymy is a form of figurative speech &amp;quot;in which one expression is used to refer to the referent of a related one&amp;quot; (Markert and Nissim, 2007). The phenomenon corresponds to a semantic shift in interpretation (&amp;quot;a profile shift&amp;quot;) that appears to be a function of salience (Cruse and Croft, 2004). We assume that this semantic shift does not completely erase the original referent: it rather puts the focus on a specific feature of the content (&amp;quot;the profile&amp;quot;) of the standard referent. If we adopt this theory, we can explain why it may be difficult to tag some examples, since both readings may co-exist. 6 Conclusion In this paper, we presented a (minimalist) system for metonymy resolution and evaluated its usefulness for the task. The syst</context>
</contexts>
<marker>Markert, Nissim, 2007</marker>
<rawString>K. Markert and M. Nissim. 2007. Task08: Metonymy Resolution at Semeval 2007. Proceedings of Semeval 2007. Prague, Czech Rep.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Nissim</author>
<author>K Markert</author>
</authors>
<title>Syntactic Features and Word Similarity for supervised Metonymy Resolution.</title>
<date>2003</date>
<booktitle>Proceedings ofACL&apos;03.</booktitle>
<pages>56--63</pages>
<location>Sapporo,</location>
<contexts>
<context position="3584" citStr="Nissim and Markert, 2003" startWordPosition="554" endWordPosition="557">orms only. 3 A (too) Lazy Approach We chose not to use any part-of-speech tagger or syntactic or semantic analyzer; we did not use any external knowledge or any other annotated corpus than the one provided for the training phase. Since no NLP tool was used, we had to duplicate most of the words in order to get the singular and the plural form. Our system is thus very simple compared to 418 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 418–421, Prague, June 2007. c�2007 Association for Computational Linguistics the state-of-art in this domain (e.g. Nissim and Markert, 2003). We used discriminative plain words only. These are gathered as follows: all the words in a given window (here we use a 7 word window, before and after the target entity since it gave the best results on the training data) are extracted and associated with two classes (literal vs. non literal). We thus consider the most discriminative words, i.e. words that appear frequently in some contexts but not in others (literal vs. non-literal readings). Discriminative words are elements that are abnormally frequent or rare in one corpus compared to another one. Characteristic features are selected bas</context>
<context position="10751" citStr="Nissim and Markert, 2003" startWordPosition="1688" endWordPosition="1691">robable class according to the training corpus (metonymic for medium, place-for-people for fine). However, the performance of our system (i.e. accuracy) is relatively stable between these three tracks, since the distribution of examples between the different classes is very unequally distributed. Before giving the results, recall that our purpose was to investigate a knowledge-poor strategy, in order to establish how far one can get using only surface indicators. Thus, unsurprisingly, our results for the training corpus were already lower than those obtained using a more sophisticated system (Nissim and Markert, 2003). They are however a good indicator of performance when one uses only surface features. The accuracy on the training corpus was .8 15. Precision and recall are presented in the table 6. Literal Non-lit. Precision .88 .54 Recall .88 .57 P&amp;R .88 .55 Table 6. Overall results on the training corpus Accuracy on the test corpus is .754 only. Table 7 shows the results obtained for the different kinds of location names. The result is obvious: there is a significant drop in both recall and precision, compared to the results on the training corpus. 420 Literal Non lit. Precision .83 .38 Recall .86 .3 1 </context>
</contexts>
<marker>Nissim, Markert, 2003</marker>
<rawString>M. Nissim and K. Markert. 2003. Syntactic Features and Word Similarity for supervised Metonymy Resolution. Proceedings ofACL&apos;03. Sapporo, Japan. pp. 56-63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Poibeau</author>
</authors>
<title>Dealing with Metonymic Readings of Named Entities.</title>
<date>2006</date>
<booktitle>Proceedings of C0GSCI&apos;06.</booktitle>
<pages>1962--1968</pages>
<location>Vancouver,</location>
<contexts>
<context position="1711" citStr="Poibeau (2006)" startWordPosition="262" endWordPosition="263">ance of this approach and compare it to more complex ones. 2 Motivation We participated in the metonymy task with a very basic system. The idea was to investigate the efficiency of a minimalist (though, not Chomskian) system. This system tags entities on the basis of discriminative (plain) word forms occurring in a given window only. Our aim was to find out which word forms are discriminative enough to be considered as parameters. In the past, we developed a system for metonymy resolution for French, evaluated in the framework of the ESTER evaluation (Gravier, 2004). This system, described in Poibeau (2006), uses various kinds of information, among others: plain word forms, part-of-speech tags, and syntactic and semantic tags (conceptual word classes). The usefulness of complex linguistic features (especially syntactic and semantic tags) is questionable: they may be hard to compute, errorprone and their contribution is not clear. We therefore developed a new version of the system mainly based on 1) a distributional analysis (on surface word forms) along with 2) a filtering process. The latter restricted metonymic readings to country and capital names (as opposed to other location names), since t</context>
</contexts>
<marker>Poibeau, 2006</marker>
<rawString>T. Poibeau. 2006. Dealing with Metonymic Readings of Named Entities. Proceedings of C0GSCI&apos;06. Vancouver, Canada. pp. 1962-1968.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>