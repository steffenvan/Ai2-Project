<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000297">
<title confidence="0.988512">
Calibrating Features for Semantic Role Labeling
</title>
<author confidence="0.934223">
Nianwen Xue
</author>
<affiliation confidence="0.899283">
CIS, University of Pennsylvania
</affiliation>
<address confidence="0.329191">
Levine Hall, 3330 Walnut Street
Philadelphia, PA 19104-6389
U.S.A.
</address>
<email confidence="0.442421">
xueniwenAlinc.cis.upenn.edu
</email>
<note confidence="0.5592102">
Martha Palmer
CIS, University of Pennsylvania
Levine Hall, 3330 Walnut Street
Philadelphia, PA 19104-6389
U.S.A.
</note>
<email confidence="0.79989">
mpalmerAlinc.cis.upenn.edu
</email>
<sectionHeader confidence="0.936513" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999980705882353">
This paper takes a critical look at the features
used in the semantic role tagging literature and
show that the information in the input, gener-
ally a syntactic parse tree, has yet to be fully
exploited. We propose an additional set of fea-
tures and our experiments show that these fea-
tures lead to fairly significant improvements in
the tasks we performed. We further show that
different features are needed for different sub-
tasks. Finally, we show that by using a Maxi-
mum Entropy classifier and fewer features, we
achieved results comparable with the best previ-
ously reported results obtained with SVM mod-
els. We believe this is a clear indication that
developing features that capture the right kind
of information is crucial to advancing the state-
of-the-art in semantic analysis.
</bodyText>
<sectionHeader confidence="0.995598" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.989136262626263">
There has been growing interest in domain-
independent semantic analysis, fed off recent
efforts in semantic annotation. The availabil-
ity of semantically annotated corpora such as
the Proposition Banks (Kingsbury and Palmer,
2002; Xue and Palmer, 2003) and FrameNet
(Baker et al., 1998) have enabled the devel-
opment of a rapidly growing list of statistical
semantic analyzers (Giidea and Jurafsky, 2002;
Giidea and Palmer, 2002; Chen and Rambow,
2003; Pradhan et al., 2003; Pradhan et al., 2004;
Sun and Jurafsky, 2004; Palmer et al., submit-
ted). The shared task of the CoNLL-2004 is
devoted to semantic role labeling (Carreras and
Marquez, 2004). Most of these systems gen-
erally take as input a syntactic parse tree and
use the syntactic information as features to tag
the syntactic constituents with semantic role la-
bels. Although these systems have shown great
promise, we demonstrate that the features used
in previous work have not fully exploited the
information that a parse tree provides. In this
paper we prepose an additional set of features
and show that these features lead to fairly signif-
icant improvements in the tasks we performed.
This paper is organized as follows. In the next
section, we briefly describe the annotation of the
Proposition Bank, the data for our automatic
semantic role labeling experiments. Section 3
describes the architecture of our system. We
take a critical look at the previously used fea-
tures against each subtask and propose a new
set of features in Section 4. Section 5 presents
experimental results that show the effectiveness
of these new features and a comparison with
previous results. We conclude in Section 6.
2 The PropBank and Semantic Role
Labeling
The PropBank adds a layer of semantic anno-
tation to the Treebank II (Marcus et al., 1993;
Marcus et al., 1994) to capture generalizations
that are not adequately represented in the tree-
bank parse trees. For example, in both John
broke the window into a million pieces yester-
day and The window broke into a million pieces
yesterday, the window plays the same role with
regard to the verb break in both sentences even
though they occur in different syntactic posi-
tions. The PropBank annotation captures this
regularity by assigning a semantic role label to
each argument of the verb independently of its
syntactic position. This means a fixed set of
roles are specified for each verb and a differ-
ent label is assigned to each role. In PropBank
annotations, these roles are labeled with a se-
quence of integers, starting with 01 and prefixed
with ARG. For example, the verb break, has four
such numbered arguments: ARGO: the breaker,
AR Cl: thing broken, ARC?: instrument and
ARCS: pieces. It is worth pointing out that
even though the same numbers (0-5) are used
to label the semantic roles of all verbs, these
roles can only be interpreted in a verb-specific
&apos;There are some exceptions.
manner. That is, an argument marked with the
same number, e.g. ARG, may not share any
semantic similarities for different verbs.
In addition to the numbered arguments,
which are considered to be core to a verb,
there are also elements that are less closely re-
lated to the verb. This roughly parallels the
argument/adjunct dichotomy but the distinc-
tion may not be drawn along the same lines
as in the theoretic linguistics literature. These
adjunct-like elements are labeled AR GM, fol-
lowed by a secondary tag indicating the type of
adjunct. For example, yesterday in those above-
mentioned sentences is not specific to the verb
break and instead it applies to a wide variety of
verbs. Therefore it will be marked as AR GM,
followed by a secondary tag - TMP, indicating
the temporal nature of this constituent. The
secondary tags are effectively a global classifi-
cation of adjunct-like elements. There are 12
secondary tags for ARCMs in the Proposition
Bank: DIR, LOC, MNR, TMP, EXT, REC,
PRD, PRP, DIS, ADV, MOD, NE.
Some verbs require different sets of arguments
for different senses, and accurately characteriz-
ing the semantic roles of their arguments neces-
sitates first distinguishing these senses. For ex-
ample, the verb &amp;quot;pass&amp;quot; takes three arguments,
legislative body, bill and law when it means &amp;quot;vote
and pass&amp;quot;, while it takes only two arguments
entity moving ahead and entity falling behind
when it means &amp;quot;overtake&amp;quot;. Each sense of this
verb is likely to be realized in a set of distinct
subcategorization frames and is therefore called
a frameset.
</bodyText>
<listItem confidence="0.97125225">
(1) a. The congress passed the bill into
law.
b. A couple of my law clerks were going
to pass me in three or four years.
</listItem>
<bodyText confidence="0.908321142857143">
Semantic role tagging There are different
ways to formulate the semantic role tagging task
based on the annotation of the PropBank, de-
pending on what type information one wants to
learn automatically. For comparison purposes
we ignore the frameset information for now,
following the practice of Gildea and Palmer
(Gildea and Jurafsky, 2002; Pradhan et al.,
2003) and others. For each verb, we will predict
the core arguments ARG/O-5], as well as the sec-
ondary tags for ARCMs. The total tagset will
2Modals (MOD) and negation markers (NEG) are
clearly not adjuncts. They are included because they
are critical to the interpretation of the events
be ARG/O-5], ARGa3 ARGM x secondary tags.
There are also constituents that are not seman-
tic arguments (by semantic arguments we mean
both numbered arguments and ARCMs) to a
given verb and we will label such constituents
NULL. Semantic role tagging is thus an one of
N classification task.
</bodyText>
<sectionHeader confidence="0.962107" genericHeader="introduction">
3 System architecture
</sectionHeader>
<bodyText confidence="0.997415392857143">
Although it is conceivable that one can sim-
ply treat this as a multi-category classification
problem, there are at least two reasons why
such a simple approach will not work effectively.
One is that for a given verb, the majority of
the constituents in a syntactic tree are not its
semantic arguments. When negative samples
(constituents marked NULL) overwhelm posi-
tive samples, the current machine-learning algo-
rithms will not be effective. The second reason,
which is more subtle, is that information that
is effective in separating arguments from NULL
elements may not be as effective in distinguish-
ing different types of arguments and vice versa,
as we will show in our experiments. Based on
these considerations, we will adopt a three-stage
architecture:
Stage 1: To save training time, we use a sim-
ple algorithm to filter out constituents that
are clearly not semantic arguments to the
predicate in question.
Stage 2: We then classify the candidates de-
rived from the first stage as either semantic
arguments or non-arguments.
Stage 3: Finally we run a multi-category classi-
fier to classify the constituents that are la-
beled as arguments into one of the classes
plus NULL.
</bodyText>
<figureCaption confidence="0.916722">
Pruning Algorithm The first stage is done
with a simple algorithm and it is illustrated with
Figure 1:
</figureCaption>
<bodyText confidence="0.553433166666667">
Step 1: Designate the predicate as the current
node and collect its sisters (constituents at-
tached at the same level as the predicate)
unless its sisters are coordinated with the
predicate. If a sister is a PP, also collect its
immediate children.
</bodyText>
<footnote confidence="0.835125">
3A limited number of arguments in the PropBank
are labeled AR GA, meaning causative agent for induced
action.
</footnote>
<figure confidence="0.989324116666667">
cited
VP
VBD
Premier
Ryzhkov
warned
VBD
mismanagement
were
S
S
CC S
of tough measures
VBD PP
VP
and
NP
VP
NP
Strikes
and
S
VP
NP
VP
VBD NP/arg0
We
PP VBG
NP
NP/arg2 SBAR/arg1
got
calls from big
block houses asking us
S
IN
if we want to
make bids on
anything
S
VP
NP/argo
VBD
NP/arg2 NP/arg1
The Supreme
Court
NP SBAR
gave
states
more leeway to restrict abortion
S
NP/arg0
VP
VBD
PP/arg2
NP/arg1 ADVP
Some of them
put the shares back
NP
IN
on the shelf
</figure>
<bodyText confidence="0.999689">
each argument of the verb. There are again two
experiment conditions. In the first experiment,
the constituents that are arguments to a verb
is already known, and the task is only to as-
sign the correct semantic role label to the con-
stituents. In the second experiment, this same
task is performed on the output of the argument
identification task presented in Table 1. The
same experiments are repeated using automatic
parses produced by the Collins parser. The re-
sults are presented in Table 2. Row 1 presents
results of all arguments when functional tags of
the ArgMs are predicted, while Row 2 presents
results of all arguments when functional tags are
ignored. Finally Row 3 presents results when
only the core arguments (numbered arguments)
are calculated.
</bodyText>
<table confidence="0.97463375">
data set CS Acc. CS (f%) CP (f%)
all (ArgM+) 92.95 88.51 76.21
all (ArgM) 95.42 90.55 77.76
Core 94.96 (f) 90.58 78.16
</table>
<tableCaption confidence="0.9797">
Table 2: Classification results: CS Acc. = Gold
</tableCaption>
<bodyText confidence="0.979863931034483">
Standard accuracy, CS (f) = Cold Standard f-
score, CP = Collins Parser
Feature performance Table 3 shows the
performance of the new features. The base-
line system uses the original features proposed
in (Giidea and Palmer, 2002) and each row
shows the improvement over the baseline when
that feature is added to the baseline features.
The results are on known (when constituents
that are semantic arguments are given) and un-
known (when constituents that are arguments
have to be identified first before being classified)
constituents respectively using Cold Standard
Treebank parses. It is clear that the syntac-
tic frame feature results in the most improve-
ment (more than 1.7%) over the baseline, with
the head of the PP parent feature being a close
second. It is also worth noting that although
the feature combining position and voice re-
sults in an improvement when the constituents
are known, it actually results in a small loss
when the constituents are unknown. This indi-
cates that the slight change in the classification
task (for classification of unknown constituents,
an additional category NULL is added) could
change the feature performance. The last three
features are from (Pradhan et al., 2004), and
they also result in an improvement in perfor-
mance.
</bodyText>
<table confidence="0.998812636363636">
Features accu. Gold(f)
baseline 88.09 82.89
syn frame 89.82 84.64
prd-hw 88.69 83.77
prd-pt 89.12 83.81
v-p 88.44 82.57
PP parent 89.53 84.34
First word 88.60 83.01
Last word 88.64 83.51
Left sister 89.20 83.74
all 92.95 88.51
</table>
<tableCaption confidence="0.99996">
Table 3: Results with different feature sets
</tableCaption>
<subsectionHeader confidence="0.92298">
5.3 Comparison with other systems
</subsectionHeader>
<bodyText confidence="0.999978756097561">
Rapid progress has been made in semantic role
labeling since the Propbank annotation became
first available in 2002. The progress can be at-
tributed to better modeling techniques, more
relevant features and in a small measure, cleaner
annotation. The first system trained on the
Propbank is by Giidea and Palmer (2002), who
reported 82.8% in accuracy on Cold Standard
parses when the constituents that are seman-
tic arguments are given, 67.6% and 53.6% (f-
measure) using Cold Standard and automatic
parses respectively when the constituents for
the arguments have to be first identified. Since
then, various degrees of improvement have been
reported (Giidea and Hockenmaier, 2003; Prad-
han et al., 2003; Chen and Rambow, 2003).
As far as we know the best results so far are
reported by (Pradhan et al., 2004), where a
wide range of features, including features ex-
tracted from named entities, verb clusters and
verb senses, temporal cue words, dynamic con-
text, are tested with an SVM classifier. Their
system achieved an accuracy of 93.0% on known
constituents and 89.4% (f-measure) on unknown
constituents using Cold Standard parses. They
did not report results that use automatic parses
with this version of the data, but using a pre-
vious version of the data, they reported an f-
score of 79.4% using automatic parses (Char-
niak, 2001). By carefully designing features
that can all be directly extracted from the tree-
bank parse trees, our system achieved very com-
parable results using a Maxent classifier and a
much smaller feature set: 92.95% on known con-
stituents, 88.51% on unknown constituents and
76.21% when the Collins parser is used. The
results on known constituents are almost iden-
tical and the larger difference when automatic
parses are used could be attributed to the dif-
ferent parsers, as we used output from an earlier
version of the Collins parser.
</bodyText>
<sectionHeader confidence="0.988964" genericHeader="conclusions">
6 Conclusions and Future work
</sectionHeader>
<bodyText confidence="0.999971818181818">
This paper takes a critical look at the features
used in the semantic role tagging literature and
show that the information in the input, gener-
ally a syntactic parse tree, has yet to be fully
exploited. We propose an additional set of fea-
tures and our experiments show that these fea-
tures lead to fairly significant improvements in
the tasks we performed. We further show that
different features are needed for different sub-
tasks. Finally, we show that using a maxi-
mum entropy classifier and fewer features, we
achieved results that are comparable to the best
previously reported results obtained with SVM
models. We believe this is a clear indication
that developing features that capture the right
kind of information is crucial to advancing the
state-of-the-art in semantic analysis. We also
believe that the features we proposed here are
to a large extent complementary to those pro-
posed in a recent work by Pradhan et al (2004)
and we intend to incorporate them in our sys-
tem.
</bodyText>
<sectionHeader confidence="0.997118" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999923714285714">
We would like to thank Scott Cotton for pro-
viding the PropBank API 6 which greatly sim-
plifies the implementation of our system. This
work is funded in part by the DOD via grant
MDA904-02-C-0412, and in part by the NSF
ITR via grant 130-1303-4-541984-XXXX-2000-
1070. .
</bodyText>
<sectionHeader confidence="0.997183" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.985512133333333">
C. Baker, C. Fillmore, and J. Lowe. 1998. The
berkeley framenet project. In Proceedings of
COLING-ACL, Singapore.
Xavier Carreras and Lillis Marquez. 2004. In-
troduction to the CoNLL-2004 Shared Task:
Semantic Role Labeling. In Proceedings of
CoNLL 2004.
E. Charniak. 2001. Immediate-head Parsing for
Language Models. In A CL- 01.
John Chen and Owen Rambow. 2003. Use of
Deep Linguistic Features for the Recognition
and Labeling of Semantic Arguments. In Pro-
ceedings of the 2003 Conference on Empiri-
cal Methods in Natural Language Processing,
Sapporo, Japan.
</reference>
<footnote confidence="0.5508">
6Available at http://chronis.philli.net/ scott/code
</footnote>
<reference confidence="0.996861769230769">
Michael Collins. 1999. Head-driven Statistical
Models for Natural Language Parsing. Ph.D.
thesis, University of Pennsylvania.
Dan Gildea and Julia Hockenmaier. 2003. Iden-
tifying Semantic Roles Using Combinatory
Categorial Grammar. In EMNLP- 03, Sap-
poro, Japan.
D. Gildea and D. Jurafsky. 2002. Automatic la-
beling for semantic roles. Computational Lin-
guistics, 28(3) :245-288.
Dan Gildea and Martha Palmer. 2002. The
Necessity of Parsing for Predicate Argument
Recognition. In Proceedings of the 40th Meet-
ing of the Association for Computational Lin-
guistics, Philadelphia, PA.
Paul Kingsbury and Martha Palmer. 2002.
From Treebank to PropBank. In Pro-
ceedings of the 3rd International Confer-
ence on Language Resources and Evaluation
(LREC2002), Las Palmas, Spain.
M. Marcus, B. Santorini, and M. A.
Marcinkiewicz. 1993. Building a Large
Annotated Corpus of English: the Penn
Treebank. Computational Linguistics.
Mitchell Marcus, Grace Kim, Mary Ann
Marcinkiewicz, et al. 1994. The Penn
Treebank: Annotating Predicate Argument
Structure. In Proc of ARPA speech and Nat-
ural language workshop.
Martha Palmer, Dan Gildea, and Paul Kings-
bury. submitted. The proposition bank: An
annotated corpus of semantic roles. Compu-
tational Linguistics.
Sameer Pradhan, Kadri Hacioglu, Wayne Ward,
James H. Martin, and Daniel Jurafsky. 2003.
Semantic Role Parsing: Adding Semantic
Structure to Unstructured Text. In Proceed-
ings of the International Conference on Data
Mining (ICDM-2003).
Sameer Pradhan, Wayne Ward, Kadri Hacioglu,
James H. Martin, and Daniel Jurafsky. 2004.
Shallow Semantic Parsing Using Support Vec-
tor Machines. In Proceedings of NAA CL-HLT
2004, Boston, Mass.
Honglin Sun and Daniel Jurafsky. 2004. Shal-
low semantic parsing of chinese. In Proceed-
ings of NAACL 2004, Boston, USA.
Nianwen Xue and Martha Palmer. 2003. An-
notating the Propositions in the Penn Chi-
nese Treebank. In The Proceedings of the
2nd SIGHAN Workshop on Chinese Lan-
guage Processing, Sapporo, Japan.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.694235">
<title confidence="0.999856">Calibrating Features for Semantic Role Labeling</title>
<author confidence="0.929419">Nianwen</author>
<affiliation confidence="0.998451">CIS, University of</affiliation>
<address confidence="0.9626885">Levine Hall, 3330 Walnut Philadelphia, PA</address>
<email confidence="0.99082">xueniwenAlinc.cis.upenn.edu</email>
<author confidence="0.987836">Martha</author>
<affiliation confidence="0.9995">CIS, University of</affiliation>
<address confidence="0.921762">Levine Hall, 3330 Walnut Philadelphia, PA</address>
<email confidence="0.999515">mpalmerAlinc.cis.upenn.edu</email>
<abstract confidence="0.998425">This paper takes a critical look at the features used in the semantic role tagging literature and show that the information in the input, generally a syntactic parse tree, has yet to be fully exploited. We propose an additional set of features and our experiments show that these features lead to fairly significant improvements in the tasks we performed. We further show that different features are needed for different subtasks. Finally, we show that by using a Maximum Entropy classifier and fewer features, we achieved results comparable with the best previously reported results obtained with SVM models. We believe this is a clear indication that developing features that capture the right kind of information is crucial to advancing the stateof-the-art in semantic analysis.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Baker</author>
<author>C Fillmore</author>
<author>J Lowe</author>
</authors>
<title>The berkeley framenet project.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<contexts>
<context position="1417" citStr="Baker et al., 1998" startWordPosition="211" endWordPosition="214">a Maximum Entropy classifier and fewer features, we achieved results comparable with the best previously reported results obtained with SVM models. We believe this is a clear indication that developing features that capture the right kind of information is crucial to advancing the stateof-the-art in semantic analysis. 1 Introduction There has been growing interest in domainindependent semantic analysis, fed off recent efforts in semantic annotation. The availability of semantically annotated corpora such as the Proposition Banks (Kingsbury and Palmer, 2002; Xue and Palmer, 2003) and FrameNet (Baker et al., 1998) have enabled the development of a rapidly growing list of statistical semantic analyzers (Giidea and Jurafsky, 2002; Giidea and Palmer, 2002; Chen and Rambow, 2003; Pradhan et al., 2003; Pradhan et al., 2004; Sun and Jurafsky, 2004; Palmer et al., submitted). The shared task of the CoNLL-2004 is devoted to semantic role labeling (Carreras and Marquez, 2004). Most of these systems generally take as input a syntactic parse tree and use the syntactic information as features to tag the syntactic constituents with semantic role labels. Although these systems have shown great promise, we demonstrat</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>C. Baker, C. Fillmore, and J. Lowe. 1998. The berkeley framenet project. In Proceedings of COLING-ACL, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Lillis Marquez</author>
</authors>
<title>Introduction to the CoNLL-2004 Shared Task: Semantic Role Labeling.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL</booktitle>
<contexts>
<context position="1777" citStr="Carreras and Marquez, 2004" startWordPosition="270" endWordPosition="273">owing interest in domainindependent semantic analysis, fed off recent efforts in semantic annotation. The availability of semantically annotated corpora such as the Proposition Banks (Kingsbury and Palmer, 2002; Xue and Palmer, 2003) and FrameNet (Baker et al., 1998) have enabled the development of a rapidly growing list of statistical semantic analyzers (Giidea and Jurafsky, 2002; Giidea and Palmer, 2002; Chen and Rambow, 2003; Pradhan et al., 2003; Pradhan et al., 2004; Sun and Jurafsky, 2004; Palmer et al., submitted). The shared task of the CoNLL-2004 is devoted to semantic role labeling (Carreras and Marquez, 2004). Most of these systems generally take as input a syntactic parse tree and use the syntactic information as features to tag the syntactic constituents with semantic role labels. Although these systems have shown great promise, we demonstrate that the features used in previous work have not fully exploited the information that a parse tree provides. In this paper we prepose an additional set of features and show that these features lead to fairly significant improvements in the tasks we performed. This paper is organized as follows. In the next section, we briefly describe the annotation of the</context>
</contexts>
<marker>Carreras, Marquez, 2004</marker>
<rawString>Xavier Carreras and Lillis Marquez. 2004. Introduction to the CoNLL-2004 Shared Task: Semantic Role Labeling. In Proceedings of CoNLL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Immediate-head Parsing for Language Models.</title>
<date>2001</date>
<journal>In A CL-</journal>
<volume>01</volume>
<contexts>
<context position="12564" citStr="Charniak, 2001" startWordPosition="2089" endWordPosition="2091">w, 2003). As far as we know the best results so far are reported by (Pradhan et al., 2004), where a wide range of features, including features extracted from named entities, verb clusters and verb senses, temporal cue words, dynamic context, are tested with an SVM classifier. Their system achieved an accuracy of 93.0% on known constituents and 89.4% (f-measure) on unknown constituents using Cold Standard parses. They did not report results that use automatic parses with this version of the data, but using a previous version of the data, they reported an fscore of 79.4% using automatic parses (Charniak, 2001). By carefully designing features that can all be directly extracted from the treebank parse trees, our system achieved very comparable results using a Maxent classifier and a much smaller feature set: 92.95% on known constituents, 88.51% on unknown constituents and 76.21% when the Collins parser is used. The results on known constituents are almost identical and the larger difference when automatic parses are used could be attributed to the different parsers, as we used output from an earlier version of the Collins parser. 6 Conclusions and Future work This paper takes a critical look at the </context>
</contexts>
<marker>Charniak, 2001</marker>
<rawString>E. Charniak. 2001. Immediate-head Parsing for Language Models. In A CL- 01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Chen</author>
<author>Owen Rambow</author>
</authors>
<title>Use of Deep Linguistic Features for the Recognition and Labeling of Semantic Arguments.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="1581" citStr="Chen and Rambow, 2003" startWordPosition="237" endWordPosition="240">s is a clear indication that developing features that capture the right kind of information is crucial to advancing the stateof-the-art in semantic analysis. 1 Introduction There has been growing interest in domainindependent semantic analysis, fed off recent efforts in semantic annotation. The availability of semantically annotated corpora such as the Proposition Banks (Kingsbury and Palmer, 2002; Xue and Palmer, 2003) and FrameNet (Baker et al., 1998) have enabled the development of a rapidly growing list of statistical semantic analyzers (Giidea and Jurafsky, 2002; Giidea and Palmer, 2002; Chen and Rambow, 2003; Pradhan et al., 2003; Pradhan et al., 2004; Sun and Jurafsky, 2004; Palmer et al., submitted). The shared task of the CoNLL-2004 is devoted to semantic role labeling (Carreras and Marquez, 2004). Most of these systems generally take as input a syntactic parse tree and use the syntactic information as features to tag the syntactic constituents with semantic role labels. Although these systems have shown great promise, we demonstrate that the features used in previous work have not fully exploited the information that a parse tree provides. In this paper we prepose an additional set of feature</context>
<context position="11957" citStr="Chen and Rambow, 2003" startWordPosition="1984" endWordPosition="1987">ailable in 2002. The progress can be attributed to better modeling techniques, more relevant features and in a small measure, cleaner annotation. The first system trained on the Propbank is by Giidea and Palmer (2002), who reported 82.8% in accuracy on Cold Standard parses when the constituents that are semantic arguments are given, 67.6% and 53.6% (fmeasure) using Cold Standard and automatic parses respectively when the constituents for the arguments have to be first identified. Since then, various degrees of improvement have been reported (Giidea and Hockenmaier, 2003; Pradhan et al., 2003; Chen and Rambow, 2003). As far as we know the best results so far are reported by (Pradhan et al., 2004), where a wide range of features, including features extracted from named entities, verb clusters and verb senses, temporal cue words, dynamic context, are tested with an SVM classifier. Their system achieved an accuracy of 93.0% on known constituents and 89.4% (f-measure) on unknown constituents using Cold Standard parses. They did not report results that use automatic parses with this version of the data, but using a previous version of the data, they reported an fscore of 79.4% using automatic parses (Charniak</context>
</contexts>
<marker>Chen, Rambow, 2003</marker>
<rawString>John Chen and Owen Rambow. 2003. Use of Deep Linguistic Features for the Recognition and Labeling of Semantic Arguments. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gildea</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Identifying Semantic Roles Using Combinatory Categorial Grammar.</title>
<date>2003</date>
<booktitle>In EMNLP- 03,</booktitle>
<location>Sapporo, Japan.</location>
<marker>Gildea, Hockenmaier, 2003</marker>
<rawString>Dan Gildea and Julia Hockenmaier. 2003. Identifying Semantic Roles Using Combinatory Categorial Grammar. In EMNLP- 03, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic labeling for semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<pages>245--288</pages>
<contexts>
<context position="6030" citStr="Gildea and Jurafsky, 2002" startWordPosition="986" endWordPosition="989"> behind when it means &amp;quot;overtake&amp;quot;. Each sense of this verb is likely to be realized in a set of distinct subcategorization frames and is therefore called a frameset. (1) a. The congress passed the bill into law. b. A couple of my law clerks were going to pass me in three or four years. Semantic role tagging There are different ways to formulate the semantic role tagging task based on the annotation of the PropBank, depending on what type information one wants to learn automatically. For comparison purposes we ignore the frameset information for now, following the practice of Gildea and Palmer (Gildea and Jurafsky, 2002; Pradhan et al., 2003) and others. For each verb, we will predict the core arguments ARG/O-5], as well as the secondary tags for ARCMs. The total tagset will 2Modals (MOD) and negation markers (NEG) are clearly not adjuncts. They are included because they are critical to the interpretation of the events be ARG/O-5], ARGa3 ARGM x secondary tags. There are also constituents that are not semantic arguments (by semantic arguments we mean both numbered arguments and ARCMs) to a given verb and we will label such constituents NULL. Semantic role tagging is thus an one of N classification task. 3 Sys</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>D. Gildea and D. Jurafsky. 2002. Automatic labeling for semantic roles. Computational Linguistics, 28(3) :245-288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gildea</author>
<author>Martha Palmer</author>
</authors>
<title>The Necessity of Parsing for Predicate Argument Recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Meeting of the Association for Computational Linguistics,</booktitle>
<location>Philadelphia, PA.</location>
<marker>Gildea, Palmer, 2002</marker>
<rawString>Dan Gildea and Martha Palmer. 2002. The Necessity of Parsing for Predicate Argument Recognition. In Proceedings of the 40th Meeting of the Association for Computational Linguistics, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Kingsbury</author>
<author>Martha Palmer</author>
</authors>
<title>From Treebank to PropBank.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC2002),</booktitle>
<location>Las Palmas,</location>
<contexts>
<context position="1360" citStr="Kingsbury and Palmer, 2002" startWordPosition="201" endWordPosition="204">e needed for different subtasks. Finally, we show that by using a Maximum Entropy classifier and fewer features, we achieved results comparable with the best previously reported results obtained with SVM models. We believe this is a clear indication that developing features that capture the right kind of information is crucial to advancing the stateof-the-art in semantic analysis. 1 Introduction There has been growing interest in domainindependent semantic analysis, fed off recent efforts in semantic annotation. The availability of semantically annotated corpora such as the Proposition Banks (Kingsbury and Palmer, 2002; Xue and Palmer, 2003) and FrameNet (Baker et al., 1998) have enabled the development of a rapidly growing list of statistical semantic analyzers (Giidea and Jurafsky, 2002; Giidea and Palmer, 2002; Chen and Rambow, 2003; Pradhan et al., 2003; Pradhan et al., 2004; Sun and Jurafsky, 2004; Palmer et al., submitted). The shared task of the CoNLL-2004 is devoted to semantic role labeling (Carreras and Marquez, 2004). Most of these systems generally take as input a syntactic parse tree and use the syntactic information as features to tag the syntactic constituents with semantic role labels. Altho</context>
</contexts>
<marker>Kingsbury, Palmer, 2002</marker>
<rawString>Paul Kingsbury and Martha Palmer. 2002. From Treebank to PropBank. In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC2002), Las Palmas, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: the Penn Treebank. Computational Linguistics.</title>
<date>1993</date>
<contexts>
<context position="2922" citStr="Marcus et al., 1993" startWordPosition="461" endWordPosition="464">as follows. In the next section, we briefly describe the annotation of the Proposition Bank, the data for our automatic semantic role labeling experiments. Section 3 describes the architecture of our system. We take a critical look at the previously used features against each subtask and propose a new set of features in Section 4. Section 5 presents experimental results that show the effectiveness of these new features and a comparison with previous results. We conclude in Section 6. 2 The PropBank and Semantic Role Labeling The PropBank adds a layer of semantic annotation to the Treebank II (Marcus et al., 1993; Marcus et al., 1994) to capture generalizations that are not adequately represented in the treebank parse trees. For example, in both John broke the window into a million pieces yesterday and The window broke into a million pieces yesterday, the window plays the same role with regard to the verb break in both sentences even though they occur in different syntactic positions. The PropBank annotation captures this regularity by assigning a semantic role label to each argument of the verb independently of its syntactic position. This means a fixed set of roles are specified for each verb and a </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a Large Annotated Corpus of English: the Penn Treebank. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Grace Kim</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>The Penn Treebank: Annotating Predicate Argument Structure.</title>
<date>1994</date>
<booktitle>In Proc of ARPA speech and Natural language workshop.</booktitle>
<contexts>
<context position="2944" citStr="Marcus et al., 1994" startWordPosition="465" endWordPosition="468">xt section, we briefly describe the annotation of the Proposition Bank, the data for our automatic semantic role labeling experiments. Section 3 describes the architecture of our system. We take a critical look at the previously used features against each subtask and propose a new set of features in Section 4. Section 5 presents experimental results that show the effectiveness of these new features and a comparison with previous results. We conclude in Section 6. 2 The PropBank and Semantic Role Labeling The PropBank adds a layer of semantic annotation to the Treebank II (Marcus et al., 1993; Marcus et al., 1994) to capture generalizations that are not adequately represented in the treebank parse trees. For example, in both John broke the window into a million pieces yesterday and The window broke into a million pieces yesterday, the window plays the same role with regard to the verb break in both sentences even though they occur in different syntactic positions. The PropBank annotation captures this regularity by assigning a semantic role label to each argument of the verb independently of its syntactic position. This means a fixed set of roles are specified for each verb and a different label is ass</context>
</contexts>
<marker>Marcus, Kim, Marcinkiewicz, 1994</marker>
<rawString>Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, et al. 1994. The Penn Treebank: Annotating Predicate Argument Structure. In Proc of ARPA speech and Natural language workshop.</rawString>
</citation>
<citation valid="false">
<authors>
<author>submitted</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<journal>Computational Linguistics.</journal>
<marker>submitted, </marker>
<rawString>Martha Palmer, Dan Gildea, and Paul Kingsbury. submitted. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Kadri Hacioglu</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Semantic Role Parsing: Adding Semantic Structure to Unstructured Text.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Conference on Data Mining (ICDM-2003).</booktitle>
<contexts>
<context position="1603" citStr="Pradhan et al., 2003" startWordPosition="241" endWordPosition="244"> that developing features that capture the right kind of information is crucial to advancing the stateof-the-art in semantic analysis. 1 Introduction There has been growing interest in domainindependent semantic analysis, fed off recent efforts in semantic annotation. The availability of semantically annotated corpora such as the Proposition Banks (Kingsbury and Palmer, 2002; Xue and Palmer, 2003) and FrameNet (Baker et al., 1998) have enabled the development of a rapidly growing list of statistical semantic analyzers (Giidea and Jurafsky, 2002; Giidea and Palmer, 2002; Chen and Rambow, 2003; Pradhan et al., 2003; Pradhan et al., 2004; Sun and Jurafsky, 2004; Palmer et al., submitted). The shared task of the CoNLL-2004 is devoted to semantic role labeling (Carreras and Marquez, 2004). Most of these systems generally take as input a syntactic parse tree and use the syntactic information as features to tag the syntactic constituents with semantic role labels. Although these systems have shown great promise, we demonstrate that the features used in previous work have not fully exploited the information that a parse tree provides. In this paper we prepose an additional set of features and show that these </context>
<context position="6053" citStr="Pradhan et al., 2003" startWordPosition="990" endWordPosition="993">take&amp;quot;. Each sense of this verb is likely to be realized in a set of distinct subcategorization frames and is therefore called a frameset. (1) a. The congress passed the bill into law. b. A couple of my law clerks were going to pass me in three or four years. Semantic role tagging There are different ways to formulate the semantic role tagging task based on the annotation of the PropBank, depending on what type information one wants to learn automatically. For comparison purposes we ignore the frameset information for now, following the practice of Gildea and Palmer (Gildea and Jurafsky, 2002; Pradhan et al., 2003) and others. For each verb, we will predict the core arguments ARG/O-5], as well as the secondary tags for ARCMs. The total tagset will 2Modals (MOD) and negation markers (NEG) are clearly not adjuncts. They are included because they are critical to the interpretation of the events be ARG/O-5], ARGa3 ARGM x secondary tags. There are also constituents that are not semantic arguments (by semantic arguments we mean both numbered arguments and ARCMs) to a given verb and we will label such constituents NULL. Semantic role tagging is thus an one of N classification task. 3 System architecture Althou</context>
<context position="11933" citStr="Pradhan et al., 2003" startWordPosition="1979" endWordPosition="1983">tation became first available in 2002. The progress can be attributed to better modeling techniques, more relevant features and in a small measure, cleaner annotation. The first system trained on the Propbank is by Giidea and Palmer (2002), who reported 82.8% in accuracy on Cold Standard parses when the constituents that are semantic arguments are given, 67.6% and 53.6% (fmeasure) using Cold Standard and automatic parses respectively when the constituents for the arguments have to be first identified. Since then, various degrees of improvement have been reported (Giidea and Hockenmaier, 2003; Pradhan et al., 2003; Chen and Rambow, 2003). As far as we know the best results so far are reported by (Pradhan et al., 2004), where a wide range of features, including features extracted from named entities, verb clusters and verb senses, temporal cue words, dynamic context, are tested with an SVM classifier. Their system achieved an accuracy of 93.0% on known constituents and 89.4% (f-measure) on unknown constituents using Cold Standard parses. They did not report results that use automatic parses with this version of the data, but using a previous version of the data, they reported an fscore of 79.4% using au</context>
</contexts>
<marker>Pradhan, Hacioglu, Ward, Martin, Jurafsky, 2003</marker>
<rawString>Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James H. Martin, and Daniel Jurafsky. 2003. Semantic Role Parsing: Adding Semantic Structure to Unstructured Text. In Proceedings of the International Conference on Data Mining (ICDM-2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Wayne Ward</author>
<author>Kadri Hacioglu</author>
<author>James H Martin</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Shallow Semantic Parsing Using Support Vector Machines.</title>
<date>2004</date>
<booktitle>In Proceedings of NAA CL-HLT 2004,</booktitle>
<location>Boston, Mass.</location>
<contexts>
<context position="1625" citStr="Pradhan et al., 2004" startWordPosition="245" endWordPosition="248">res that capture the right kind of information is crucial to advancing the stateof-the-art in semantic analysis. 1 Introduction There has been growing interest in domainindependent semantic analysis, fed off recent efforts in semantic annotation. The availability of semantically annotated corpora such as the Proposition Banks (Kingsbury and Palmer, 2002; Xue and Palmer, 2003) and FrameNet (Baker et al., 1998) have enabled the development of a rapidly growing list of statistical semantic analyzers (Giidea and Jurafsky, 2002; Giidea and Palmer, 2002; Chen and Rambow, 2003; Pradhan et al., 2003; Pradhan et al., 2004; Sun and Jurafsky, 2004; Palmer et al., submitted). The shared task of the CoNLL-2004 is devoted to semantic role labeling (Carreras and Marquez, 2004). Most of these systems generally take as input a syntactic parse tree and use the syntactic information as features to tag the syntactic constituents with semantic role labels. Although these systems have shown great promise, we demonstrate that the features used in previous work have not fully exploited the information that a parse tree provides. In this paper we prepose an additional set of features and show that these features lead to fairl</context>
<context position="10872" citStr="Pradhan et al., 2004" startWordPosition="1809" endWordPosition="1812">is clear that the syntactic frame feature results in the most improvement (more than 1.7%) over the baseline, with the head of the PP parent feature being a close second. It is also worth noting that although the feature combining position and voice results in an improvement when the constituents are known, it actually results in a small loss when the constituents are unknown. This indicates that the slight change in the classification task (for classification of unknown constituents, an additional category NULL is added) could change the feature performance. The last three features are from (Pradhan et al., 2004), and they also result in an improvement in performance. Features accu. Gold(f) baseline 88.09 82.89 syn frame 89.82 84.64 prd-hw 88.69 83.77 prd-pt 89.12 83.81 v-p 88.44 82.57 PP parent 89.53 84.34 First word 88.60 83.01 Last word 88.64 83.51 Left sister 89.20 83.74 all 92.95 88.51 Table 3: Results with different feature sets 5.3 Comparison with other systems Rapid progress has been made in semantic role labeling since the Propbank annotation became first available in 2002. The progress can be attributed to better modeling techniques, more relevant features and in a small measure, cleaner ann</context>
</contexts>
<marker>Pradhan, Ward, Hacioglu, Martin, Jurafsky, 2004</marker>
<rawString>Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James H. Martin, and Daniel Jurafsky. 2004. Shallow Semantic Parsing Using Support Vector Machines. In Proceedings of NAA CL-HLT 2004, Boston, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Honglin Sun</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Shallow semantic parsing of chinese.</title>
<date>2004</date>
<booktitle>In Proceedings of NAACL 2004,</booktitle>
<location>Boston, USA.</location>
<contexts>
<context position="1649" citStr="Sun and Jurafsky, 2004" startWordPosition="249" endWordPosition="252">ight kind of information is crucial to advancing the stateof-the-art in semantic analysis. 1 Introduction There has been growing interest in domainindependent semantic analysis, fed off recent efforts in semantic annotation. The availability of semantically annotated corpora such as the Proposition Banks (Kingsbury and Palmer, 2002; Xue and Palmer, 2003) and FrameNet (Baker et al., 1998) have enabled the development of a rapidly growing list of statistical semantic analyzers (Giidea and Jurafsky, 2002; Giidea and Palmer, 2002; Chen and Rambow, 2003; Pradhan et al., 2003; Pradhan et al., 2004; Sun and Jurafsky, 2004; Palmer et al., submitted). The shared task of the CoNLL-2004 is devoted to semantic role labeling (Carreras and Marquez, 2004). Most of these systems generally take as input a syntactic parse tree and use the syntactic information as features to tag the syntactic constituents with semantic role labels. Although these systems have shown great promise, we demonstrate that the features used in previous work have not fully exploited the information that a parse tree provides. In this paper we prepose an additional set of features and show that these features lead to fairly significant improvemen</context>
</contexts>
<marker>Sun, Jurafsky, 2004</marker>
<rawString>Honglin Sun and Daniel Jurafsky. 2004. Shallow semantic parsing of chinese. In Proceedings of NAACL 2004, Boston, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Martha Palmer</author>
</authors>
<title>Annotating the Propositions in the Penn Chinese Treebank.</title>
<date>2003</date>
<booktitle>In The Proceedings of the 2nd SIGHAN Workshop on Chinese Language Processing,</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="1383" citStr="Xue and Palmer, 2003" startWordPosition="205" endWordPosition="208">sks. Finally, we show that by using a Maximum Entropy classifier and fewer features, we achieved results comparable with the best previously reported results obtained with SVM models. We believe this is a clear indication that developing features that capture the right kind of information is crucial to advancing the stateof-the-art in semantic analysis. 1 Introduction There has been growing interest in domainindependent semantic analysis, fed off recent efforts in semantic annotation. The availability of semantically annotated corpora such as the Proposition Banks (Kingsbury and Palmer, 2002; Xue and Palmer, 2003) and FrameNet (Baker et al., 1998) have enabled the development of a rapidly growing list of statistical semantic analyzers (Giidea and Jurafsky, 2002; Giidea and Palmer, 2002; Chen and Rambow, 2003; Pradhan et al., 2003; Pradhan et al., 2004; Sun and Jurafsky, 2004; Palmer et al., submitted). The shared task of the CoNLL-2004 is devoted to semantic role labeling (Carreras and Marquez, 2004). Most of these systems generally take as input a syntactic parse tree and use the syntactic information as features to tag the syntactic constituents with semantic role labels. Although these systems have </context>
</contexts>
<marker>Xue, Palmer, 2003</marker>
<rawString>Nianwen Xue and Martha Palmer. 2003. Annotating the Propositions in the Penn Chinese Treebank. In The Proceedings of the 2nd SIGHAN Workshop on Chinese Language Processing, Sapporo, Japan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>