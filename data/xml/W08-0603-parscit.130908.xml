<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.022841">
<title confidence="0.998783">
Using Automated Feature Optimisation to Create an Adaptable
Relation Extraction System
</title>
<author confidence="0.998983">
Barry Haddow
</author>
<affiliation confidence="0.850201">
School of Informatics, University of Edinburgh,
2 Buccleuch Place, Edinburgh, Scotland, EH8 9LW
</affiliation>
<email confidence="0.997722">
bhaddow@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.993829" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.994272777777778">
An adaptable relation extraction system for the
biomedical domain is presented. The system
makes use of a large set of contextual and shal-
low syntactic features, which can be automati-
cally optimised for each relation type. The sys-
tem is tested on three different relation types;
protein-protein interactions, tissue expression
relations and fragment to parent protein rela-
tions.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999927714285714">
In biomedical information extraction, research in
named entity recognition (NER) and relation extrac-
tion (RE) has tended to focus on the extracting pro-
teins and their interactions, with less thought given
to how to adapt such systems to other entities and
relations of biomedical interest. This is especially
true for RE, where there is very little work on rela-
tions other than protein-protein interactions. Nev-
ertheless, in order to create applications of use to
biologists such as curation assistants and improved
information extraction and retrieval systems it will
be necessary to treat a broader range of semantic re-
lations. The recent release of the Genia event corpus
(Kim et al., 2008) will help to drive this research.
The aim of this paper is to address the problem of
how to create an RE system, which can be adapted to
different biomedical RE problems with a minimum of
manual intervention. Since this paper focuses on re-
lation extraction, it will be assumed that the named
entities are given, in other words the human anno-
tated entities are used in all experiments. The ap-
proach taken to RE is to treat it as a supervised
classification problem on relation candidates, using
a large collection of shallow syntactic and contextual
features. Relation candidates are pairs of entities,
picked out using an appropriate candidate generation
strategy. The use of shallow (as opposed to deep)
syntactic features means that the system can rely
</bodyText>
<page confidence="0.992452">
19
</page>
<bodyText confidence="0.999937657894737">
on relatively robust linguistic tools such as part-of-
speech taggers and chunkers, rather than more brit-
tle and less widely available tools such as parsers.
The difficulty with feature-based methods is, how-
ever, how to select the best performing feature set,
as simply adding all possible features does not nec-
essarily give the best results (Guyon and Elisseeff,
2003). The approach taken here is to implement a
large feature set and then use a greedy search to
explore the feature set and select the best subset
of features. This method of feature set optimisa-
tion is not new (for example, it was applied by one
team (Ganchev et al., 2007) on the BioCreative II
Gene Mention task ), but in this work a comparison
of search starting points and feature groupings will
be presented.
All RE systems require a human-annotated corpus
for testing, and since a supervised machine learning
approach is employed, a corpus is also required for
training the system. The experiments described in
this paper make use of the ITI TXM corpora (Alex
et al., 2008), which include the PPI corpus address-
ing protein-protein interactions, and the TE corpus
addressing tissue expression. Both corpora consist of
approximately 200 full-text biomedical research pa-
pers annotated with entities, normalisations of enti-
ties to standard databases, relations, and with en-
riched information added to the relations. Only the
entities and relations will be considered here.
This paper is organised as follows: after reviewing
related work in the following section, the RE system
is described in Section 3, including a description of
the corpora, the relation candidate extraction strate-
gies, the features employed, the feature optimisation
methods and the evaluation method. In Section 4
the results of the optimisation experiments are pre-
sented and discussed, with some concluding remarks
in Section 5.
</bodyText>
<note confidence="0.784948">
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 19–27,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.997805" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999843711538462">
Recent interest in the extraction of protein-protein
interactions has been given added impetus by shared
tasks such as the Language Learning in Logic
(Cussens and Nedellec, 2005), and the BioCreative
II Interaction Pairs Subtask (Krallinger et al., 2008).
It should be noted that the latter task, rather than
being concerned with the extraction of specific inter-
action relation mentions, required systems to list the
(curatable) interactions at a document level. Many
teams, however, extracted the interaction mentions
as a first step and then processed these to give the
document level list of curatable interactions.
The extraction of protein-protein interactions has
also been helped by the availability of annotated cor-
pora, such as AIMed (Bunescu et al., 2005), which
consists of around 1000 Medline abstracts annotated
with proteins and their interactions. In common with
the LLL corpus, the AIMed corpus only contains
intra-sentential relations, and is somewhat smaller
than the corpus used in the current work. In addi-
tion to the work by the corpus creators (Bunescu and
Mooney, 2007), other authors have achieved good re-
sults on AIMed by making use of dependency parses
in different ways (Erkan et al., 2007; Katrenko and
Adriaans, 2006). It is not clear, however, how well
these techniques would transfer to other, similar, RE
problems, and how much work would be involved in
tuning the systems for a new problem.
Supervised learning based on shallow syntactic fea-
tures has also been applied to the biomedical do-
main, again focusing on protein-protein interactions
(Nielsen, 2006; Giuliano et al., 2006). A system-
atic exploration of a set of such features for protein-
protein interaction extraction was recently provided
by Jiang and Zhai (2007), who also used features de-
rived from the Collins parser. They did not, however,
experiment with the automated optimisation of the
feature sets. In the news domain, the best reported
results on the ACE dataset&apos; have been achieved by
a composite kernel which depends partially on a full
parse, and partially on a collection of shallow syn-
tactic features (Zhou et al., 2007).
Aside from protein-protein interactions, there has
been little work directed at other types of relations
in the biomedical domain. Recent corpus annota-
tion projects such as Genia (Kim et al., 2008) and
BioInfer (Pyysalo et al., 2007) include multiple types
of relations, however many of the relation types are
represented in fairly small quantities. In earlier work
(Skounakis et al., 2003), the extraction of cell local-
isation relations was studied using an automatically
created corpus.
</bodyText>
<footnote confidence="0.964041">
1http://www.nist.gov/speech/tests/ace/
</footnote>
<sectionHeader confidence="0.990006" genericHeader="method">
3 Methods
</sectionHeader>
<subsectionHeader confidence="0.996792">
3.1 Corpora
</subsectionHeader>
<bodyText confidence="0.998613148148148">
The ITI TXM corpora contain annotations related
to protein-protein interactions (in the PPI corpus),
and annotations related to tissue expression exper-
iments (in the TE corpus). Each corpus consists of
biomedical research articles, selected from PubMed
and PubMedCentral either because they contain ex-
perimentally proven protein-protein interactions (for
the PPI corpus), or because they contain tissue ex-
pression experiments (for the TE corpus).
The articles were annotated by a team of quali-
fied biologists. The annotations consisted of entities
(Table 1), normalisations of selected entities to stan-
dard databases, relations (Table 2) and enrichment
of relations with additional information of interest
to curators. For each corpus, the entities marked
were those involved in the relation which formed the
principal focus of that corpus (either PPI or TE), and
those which could affect this relation. In the TE cor-
pus, TE relations were marked when the text stated
that a particular gene or gene product was present or
absent in a particular tissue, whilst PPI relations were
marked whenever a statement (positive or negative)
was made about the interaction of a pair of Proteins,
Mutants, Fragments, Complexes or Fusions. In ad-
dition, both corpora were annotated with FRAG re-
lations which connect Fragments and Mutants with
their parent Proteins.
</bodyText>
<table confidence="0.996742">
Corpus Entities
PPI CellLine, Complex, DrugCompound,
ExperimentalMethod, Fragment, Fusion,
Modification, Mutant, Protein
TE Complex, DevelopmentalStage, Disease,
DrugCompound, ExperimentalMethod,
Fragment, Fusion, GOMOP, Gene,
Mutant, Protein, Tissue, mRNAcDNA
</table>
<tableCaption confidence="0.90024675">
Table 1: The entity types in the TE and PPI corpora.
Note that GOMOP stands for “Gene or mRNAcDNA
or Protein” and was used when the annotators felt the
author was using the term in an ambiguous way.
</tableCaption>
<bodyText confidence="0.999692363636364">
In order to monitor annotation quality, and to
measure of the difficulty of the task, some documents
were multiply annotated. The counts of the numbers
of unique documents in each section, together with
the numbers of annotated documents are shown in
Table 3. Note that the multiply annotated docu-
ments were not reconciled, but the multiple copies
were included in the corpus. Each corpus was split
into three sections – TRAIN, DEVTEST and TEST –
with the first two sections being used for system de-
velopment, and the last reserved for final testing.
</bodyText>
<page confidence="0.988821">
20
</page>
<table confidence="0.997808125">
Corpus Relation Entity 1 Types Entity 2 Types Count
type
PPI PPI Protein, Fusion, Mutant, Fragment or Protein, Fusion, Mutant, Fragment or 11,523
Complex Complex
FRAG Protein Mutant or Fragment 16,002
TE TE Gene, Protein, mRNAcDNA, GOMOP, Tissue 12,426
Fusion, Mutant, Complex or Fragment
FRAG Protein Mutant or Fragment 4,735
</table>
<tableCaption confidence="0.984217">
Table 2: Relation types in each corpus.
</tableCaption>
<table confidence="0.99919475">
Corpus Segment Unique Doc- Annotated
uments Documents
PPI TRAIN 133 221
DEVTEST 39 58
TRAIN 45 57
TE TRAIN 151 221
DEVTEST 41 48
TEST 46 59
</table>
<tableCaption confidence="0.995832">
Table 3: Counts of documents and annotations in each
corpus.
</tableCaption>
<table confidence="0.9995768">
Corpus Relation Intra Inter
PPI PPI 10,607(92.1%) 916(7.9%)
FRAG 10,176(63.6%) 5,826(36.4%)
TE TE 10,356(83.3%) 2,070(16.7%)
FRAG 3,335(70.4%) 1,400(29.6%)
</table>
<tableCaption confidence="0.999522">
Table 4: Counts of inter and intra-sentential relations.
</tableCaption>
<bodyText confidence="0.999877">
Annotators were permitted to mark relations
between entities in the same sentence (intra-
sentential), or between entities in different sentences
(inter-sentential). The majority of relations were
intra-sentential, with FRAG relations showing the
highest proportion of inter-sententials. Table 4 shows
the counts of inter/intra-sentential relations of each
type.
Some examples of each type of relation will now
be presented. The first example is from PubMed
16436664, and is a TE relation:
Our recent observations that hαv05i1 is up-
regulated in hscleroderma fibroblastsi1 and
that the transient overexpression of αv05
increases the human hα2(I) collageni2 gene
expression in normal hfibroblastsi2 ...
There are two different TE relations in this sentence
fragment, indicated by the numerical subscripts; the
first connects a Tissue and a Complex, and the sec-
ond connects a Tissue with a Gene. Another example
from the same paper shows a FRAG relation.
Because h05i1 has a hcytoplasmic domaini1
highly homologous to that of 06-subunit, 42
we made a hypothesis that αv05 activates
SLC by the nonproteolytic pathway.
The annotators could also mark negative TE and PPI
relations, as shown in the following example of a PPI
relation taken from PubMedCentral 1075921.
It was also previously reported that two
truncated versions of hp53i1,2, consisting
of residues h2-45i1,3 and h46-71i2,4, do not
bind hhRPA70i3,4 (47)
Here the PPI relations connect the two Fragments
(“2-45&amp;quot; and “46-71&amp;quot;) to the Protein “hRPA70&amp;quot;,
whilst FRAG relations connect the Fragments with
their parent Protein “p53&amp;quot;.
In contrast with the straightforward intra-
sentential relations shown above, the following (from
PubMed 16399077) is an example of an inter-
sentential TE relation (only the related entities are
shown).
To test whether SPE can activate Toll sig-
naling, we expressed activated SPE in hS2
cellsi1 and in flies, and we then assayed
the expression of the gene for Drosomycin
(Drs), an antifungal peptide known to be
induced by Toll signaling in response to mi-
crobial infection (Lemaitre et al., 1996). In
both cases, hDrsi2 expression was signifi-
cantly induced in the absence of infection,
In this example, the annotator has connected a Tis-
sue on the first sentence, with an mRNAcDNA in
the second.
The multiply annotated documents in the corpus
were used to calculate the inter-annotator agreement
(IAA), by scoring different versions of the annota-
tion of the same document against each other. For
each corresponding pair of annotations, one anno-
tator was selected as the “gold”, and the other an-
notator scored against the first using precision, re-
call and F1 on relations. Only relations where both
annotators agreed on the participating entities were
considered. The scores for each annotated document
pair were then micro-averaged (where each example
</bodyText>
<page confidence="0.996677">
21
</page>
<table confidence="0.999287428571428">
Corpus Type Intra Inter All
PPI PPI 69.7 41.1 67.0
FRAG 90.5 73.9 84.6
All 78.7 67.3 76.1
TE TE 72.8 59.4 70.1
FRAG 89.7 69.0 84.0
All 77.4 62.7 74.1
</table>
<tableCaption confidence="0.9527595">
Table 5: IAA for relation annotation, split by inter- and
intra-sentential
</tableCaption>
<bodyText confidence="0.99996796">
is given equal weight) to produce overall IAA scores
for the corpus, shown in Table 5.
The main observations from Table 5 are that TE
and PPI relations are harder to annotate than FRAG
relations, and that inter-sentential are harder than
intra-sentential. In particular, the IAA for intra-
sentential FRAG relations is very high, probably be-
cause many of these are very straightforward con-
structions such as “Fragment of Protein”. Inter-
sentential relations are often less clear as they in-
volve linking information between several sentences,
for example using coreferences.
Both corpora were pre-processed before RE was
applied. The pre-processing involved tokenisation,
sentence boundary detection, lemmatising. part-of-
speech tagging, head word detection and chunking.
The part-of-speech tagging uses the Curran &amp; Clark
maximum entropy Markov model tagger (Curran and
Clark, 2003) trained on MedPost data (Smith et
al., 2004), whilst the other preprocessing stages are
all rule-based. The tokenisation, sentence bound-
ary detection, head word identification and chunk-
ing components were implemented with the LT-XML2
tools (Grover and Tobin, 2006), and the lemmatisa-
tion used morpha (Minnen et al., 2000).
</bodyText>
<subsectionHeader confidence="0.999639">
3.2 The Relation Extraction System
</subsectionHeader>
<bodyText confidence="0.98036001754386">
Relation extraction is treated a classification prob-
lem, by generating candidate relations, and classify-
ing them as either true or false. In the optimisa-
tion experiments described in this paper, Zhang Le&apos;s
maximum entropy (MAXENT) classifier2 was used,
since its performance was very competitive and its
fast training time permitted extensive feature exper-
imentation. The Gaussian prior was set to 0.1, and
the maximum training iterations to 100. In order to
assess the performance of the final system, MAXENT
was compared with support vector machines (SVM)
using the SVMl&apos;ght toolkit (Joachims, 1999). Since
both the classifiers assign a confidence to each pre-
diction, a varying threshold can be applied to the
output of the classifier to provide a precision-recall
2http://homepages.inf.ed.ac.uk/s0450736/maxent-
toolkit.html
tradeoff.
Candidate relations were generated by consider-
ing entity pairs of the appropriate type, taking into
account the distance between the entities. It was
thought that inter-sentential and intra-sentential re-
lations would require different feature sets and differ-
ent models, so inter- and intra-sentential candidates
were generated separately. For intra-sentential rela-
tions, all entity pairs of the appropriate type (as in
Table 2) in the same sentence were permitted as can-
didates, with the sole exclusion being that any enti-
ties contained in a Fusion entity were not allowed to
participate in candidate TE relations. This restric-
tion was in place in the annotation guidelines, so no
such relations were annotated. For intra-sentential
relations in the training data, around 25-30% of the
candidate relations are actual relations.
Generating inter-sentential candidates is more
problematic, as measures must be taken to limit the
number of candidates. Inter-sentential FRAG candi-
dates are restricted to a distance of no more than 5
sentences, whilst inter-sentential PPI and TE candi-
dates are restricted to participants in adjacent sen-
tences. Inter-sentential RE is performed after intra-
sentential RE, so the candidate generation strategy
has access to the annotated intra-sentential relations
(in training) and the predicted intra-sentential rela-
tions (in testing). For TE and PPI, candidates are
only created for those entities not already in a rela-
tion, and for FRAG candidates are only created if the
Mutant or Fragment is not already in a relation. Fur-
thermore, for FRAG relations, if there is more than
one Protein instance with the same lexical form in
the 5 sentence window, then a candidate relation is
only created between a given Fragment/Mutant and
the nearest occurrence of this Protein. For inter-
sentential FRAG relations, around 20% of the candi-
dates are actual relations, however for TE and PPI,
only about 1% of the candidate relations are actual
relations.
</bodyText>
<subsectionHeader confidence="0.967294">
3.3 Features
</subsectionHeader>
<bodyText confidence="0.99864">
Each candidate relation is mapped to a feature rep-
resentation, where the features are binary or real-
valued functions of relations. The majority of the
features are binary, although these are actually spe-
cial cases of real-valued functions, taking values 0 or
1. A feature representation of a relation is normally
written as a sequence of strings, each corresponding
to a different feature, and the presence or absence of
a binary feature indicating whether it is on or off. In
order that the relation extractor could be applied to
different problems and optimised, a large number of
features were implemented, with the intention that
the feature space could be automatically searched to
find the best subset.
</bodyText>
<page confidence="0.982772">
22
</page>
<bodyText confidence="0.998835910714286">
Features are normally grouped into feature tem-
plates and, as is common in the literature, the feature
templates may also be referred to as features. For in-
stance, a feature template may be “the token to the
right of the second entity in the relation”, which then
gives rise to a set of boolean features with the pre-
fix ctxt-w-rf1-. One such feature in this set is the
feature which indicates that the token to the right
of the second entity is “the&amp;quot;, i.e. ctxt-w-rf1-the.
The feature templates are then collected into fea-
ture groups, such as “context features”, which are
really just a convenient way of conceptualising, im-
plementing and managing the features, and do not
necessarily reflect any common behaviour amongst
the features in a group.
The following is a comprehensive list of the feature
implemented in the RE system with features listed
by group, and the possible options for each feature
group given. The options are used to turn on or off
feature templates in the group, or change templates,
and may be boolean or numerical. The nature of
the options will be important in the feature explo-
ration experiments since they influence the type of
search operations which may be used to explore the
feature space. The features are virtually all domain
independent, except for perhaps the SignSlashSign
feature which is specific to TE. The RelationKey-
wordFeature can easily be ported to a new domain
by generating a list of keywords appropriate for the
given relation.
In the feature group descriptions which follow, the
term “participants” refers to the entities within the
candidate relation. Some of the features make use of
the “vlw backoff&amp;quot;, which for a given token is defined
as the verb stem, backing off to the lemma if that is
not available, backing off to the token itself.
Chunk This group has three optional templates; one
which adds the concatenated sequence of chunk types
between the participants, and two templates which
add the count of chunks between the participants as
binary and numeric features, respectively. So if the
chunk count is, for example 4, the binary feature
would be chunk-bwcount-4 and the numeric feature
would have name chunk-bwcount and value 4.
EntitiesBetween This has templates to indicate the
type and relative position of the entities between the
two participants. For TE relations, only Tissue en-
tities are considered, whilst for other relations only
Proteins are considered.
Entity Features derived from the participating enti-
ties are added by the templates in this group, which
has options to turn on the entity’s text, class and
bigrams of these. There is also a feature template
which adds all words in the entities as separate fea-
tures, and one that adds all words in the second en-
tity only, plus options to add features which indicate
when the two entities have the same textual form, or
when one is a substring of the other.
EntityContext The entity context can include to-
kens, part-of-speech tags, chunk tags and vlw back-
offs, each within window sizes determined by numer-
ical options. A further option can switch on a tem-
plate which adds the concatenation of all vlw back-
offs in the context, on either side of each entity, and
there is also an option to convert all tokens and vlw
backoffs to lower case before creating the features.
EntityDistance Options on this group allow the ad-
dition of the token distance and sentence distance
between the entities, as numeric or binary features.
There is also an option to add a coarse three-way
classification of the token distance.
EntityFrequency Counts are made of the number of
occurrences of each entity surface form in the docu-
ment, limited to Tissue entities for TE relations, and
Proteins for FRAC and PPI relations. The only option
for this feature group adds a template which gener-
ates a binary feature indicating the frequency rank
of the participants’ surface forms in the document.
EntityPattern The entity pattern for a given intra-
sentential candidate relation shows how its partic-
ipants lie with respect to the other entities in the
sentence. The pattern is a concatenated sequence
of the entity types in the sentence, with the par-
ticipants in upper case and other entities in lower
case. Only entity types which are valid participants
in the relation in question are included. For example
protein-PROTEIN-TISSUE would indicate a relation
between a Protein and a Tissue, with another Pro-
tein occurring first in the sentence. Options in this
group add the patterm, the total number of entities
in the sentence, and the numbers of entities for each
type.
Frame The frame is the concatenation of the tokens
between the two participants. Two boolean options
on this group specify whether or not to include the
token concatenation, and whether or not to include
the part-of-speech concatenation. A further numeric
option is used to limit the maximum frame length;
when this is set to a non-zero value longer frames are
discarded.
HeadWord All the headwords of the chunks in the
sentences containing and between the participants
are listed and used to construct the features in this
group. Options specify whether to include head
nouns and/or head verbs, and whether to convert
the headwords to lower case or replace them by their
vlw backoffs. A further option allows an additional
marker to be added to each headword feature to in-
dicate whether it is before, between or after the par-
ticipants.
NestedEntity This feature indicates whether the
participants are contained in other entities, or in each
</bodyText>
<page confidence="0.995894">
23
</page>
<bodyText confidence="0.999883822222222">
other. The first option adds a feature template which
indicates which type of entity containing the two par-
ticipants, if they are both contained. The second op-
tion adds a feature to indicate whether one of the
entities is contained within the other, and the third
adds a feature to indicate whether or not there is any
whitespace between the two entities.
Ngram Three options specify what type of ngrams
to add; whether to add unigrams of the tokens in
the sentences containing the participants, whether to
add bigrams of the same tokens, and whether to add
cross-bigrams, which are bigrams of tokens before
and between the participants, and of tokens between
and after the participants. Additional options spec-
ify whether to convert tokens to vlw backoff or lower
case and whether to replace all sequences of digits
by “0&amp;quot;. Further options can be used to indicate that
only ngrams in between the participants should be
added, that each ngram feature should be marked as
before, between or after, or that all entities should
replaced in the text by their type.
RelationKeyword Relation keywords are terms an-
notated as relation indicators for PPI and TE, and
linked to relations. For PPI they are interaction
words, and for TE they are expression level words.
Keywords are matched from a list generated during
training and there are feature options to match these
keywords before, between and after the participants,
and to add templates for the existence of a keyword,
the text of the keyword, and whether or not it is a
head word.
RelativeEntityPosition The only option on this
group specifies whether or not to sort the partici-
pant entities, alphabetically by entity type. Binary
features are added indicating whether the first entity
in the candidate relation is the first in the document,
whether it is the second, whether the participants
overlap or whether they coincide.
SignSlashSign This group is only used for TE re-
lations and is designed to detected the presence of
indicators like +/+ and −/+ in the sentence(s) con-
taining the relation. Options allow the existence and
type of the one of these expressions to be indicated,
and also its position relative to the participants, and
whether it is adjacent to one of the participants.
</bodyText>
<subsectionHeader confidence="0.951408">
3.4 Optimisation
</subsectionHeader>
<bodyText confidence="0.99993556">
Feature selection methods include wrapper methods
where feature sets are assessed according to their ef-
fectiveness for a given learner, and filter methods
where features are removed using some criterion be-
fore being passed to the learner (Guyon and Elisseeff,
2003). In building the RE system, it was found that
filter methods did not work well, probably due to the
large number of interactions between the features, so
a wrapper optimisation method was employed, con-
sisting of greedy search through the space of possible
feature sets.
In the greedy search method, an initial feature set
is selected and a model trained on the TRAIN set
and tested on the DEVTEST set. A series of search
operators (see below) are applied to the feature set
to produce a list of proposed new feature sets, one
corresponding to each operator, and the new feature
sets are tested in the same way. If any of these new
feature sets produces better results than the origi-
nal initial set, then the best set replaces the initial
feature set and the process is iterated. The greedy
search terminates when none of the search operators
leads to an improvement. Three types of search oper-
ators are used in the greedy search, defined in terms
of the feature set structure described in Section 3.3:
</bodyText>
<listItem confidence="0.840119666666667">
1. The deletion of a feature group.
2. The increase or decrease of a numerical option
on a feature group (e.g. context size), where the
size of the change is not greater than 2.
3. The flipping of a boolean option on a feature
group.
</listItem>
<bodyText confidence="0.999972428571429">
In theory search operators which add or remove in-
dividual features could be used, but due to the large
number of features the use of such operators is not
practical. In addition, it may have been possible
to achieve more robust results using cross-validation
rather than heldout testing, but that would also re-
sult in a large increase in search time.
</bodyText>
<subsectionHeader confidence="0.938898">
3.5 Evaluation
</subsectionHeader>
<bodyText confidence="0.999992909090909">
In all RE experiments, the annotated entities were
assumed as given so that only RE performance was
being assessed. The performance was measured us-
ing precision-recall break-even point (BEP), which is
found by adjusting the decision boundary (thresh-
old) of the classifier until the precision and recall are
equal then taking the value of the Fl at this thresh-
old. The BEP has the advantage over Fl that its
definition is independent of the choice of threshold,
but it can still be compared easily to the IAA and is
based on the familiar concepts of precision and recall.
</bodyText>
<sectionHeader confidence="0.999929" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.9995378">
Performance of the RE system on each of the four re-
lation types was optimised using the greedy feature
exploration method described in Section 3.4. Inter
and intra-sentential relations were treated separately,
with intra-sentential relation performance optimised
first. The inter-sentential performance was then as-
sessed using a “pipeline” consisting of the best intra-
sentential relation extractor, and the inter-sentential
system being optimised.
The greedy search experiments for intra-sentential
</bodyText>
<page confidence="0.997002">
24
</page>
<bodyText confidence="0.9998703">
relations used two different starting feature sets, an
all set in which all features groups and options were
switched on, and the context sizes in EntityContext
were set to 3, and a base set which used just Ngram
and RelativeEntityPosition features. The models
were trained on TRAIN and scored on DEVTEST us-
ing BEP. In the calculation of BEP, all relations
of the appropriate type were considered, including
inter-sententials. The results of the greedy search on
intra-sentential relations are shown in Table 6.
</bodyText>
<table confidence="0.9990128">
Corpus Relation Initial Initial Final
Type Features BEP BEP
PPI PPI base 36.8 52.2
all 51.6 53.4
FRAG base 49.2 56.0
all 55.9 57.4
TE TE base 45.9 51.9
all 50.6 53.8
FRAG base 53.7 62.7
all 60.1 61.2
</table>
<tableCaption confidence="0.95497">
Table 6: Greedy search feature exploration for intra-
sentential relations. Performance is measured on all re-
lations, testing on DEVTEST.
</tableCaption>
<bodyText confidence="0.9999364">
For all relation types, the greedy search improves
the performance over the base and all feature sets,
usually reaching the highest performance when start-
ing from all. Comparing the results in Table 6
with the IAA figures provided in Table 5 shows that
the system performance is around 75-80% of IAA,
with the lowest relative performances observed for
FRAG relations. These relations include a higher pro-
portion of inter-sententials, so systems which ignore
inter-sententials suffer a larger loss in performance.
After choosing the best system for intra-sentential
relations, the same greedy optimisation was per-
formed on the inter-sentential relations using virtu-
ally the same initial feature sets. The only differ-
ence in the feature sets is that additional options are
added to the EntityDistance feature to indicate the
sentential distance between the entities. The result
of the greedy search on the inter-sentential relations
is shown in Table 7.
The inter-sentential relation optimisation is only
really successful for the FRAG relations in the PPI
corpus. For TE and PPI inter-sentential relations, the
number of negative examples dwarfs the few posi-
tive examples making it very difficult for the ma-
chine learner. For FRAG relations in both corpora,
some progress is made on the performance on inter-
sentential relations (detailed breakdown not shown)
but in the TE corpus this does not translate to an
overall improvement in BEP. This is because the
inter- and intra-sentential probabilities have quite
</bodyText>
<table confidence="0.999072">
Corpus Relation Initial Initial Final
Type Features BEP BEP
PPI PPI base 53.4 53.4
all 53.4 53.4
FRAG base 59.6 62.2
all 61.7 62.5
TE TE base 53.9 54.0
all 53.9 54.0
FRAG base 60.4 62.8
all 62.6 62.7
</table>
<tableCaption confidence="0.979895666666667">
Table 7: Greedy search feature exploration for inter-
sentential relations. Performance is measured on all re-
lations, testing on DEVTEST.
</tableCaption>
<bodyText confidence="0.999365214285714">
different ranges for FRAG relations meaning that the
threshold probabilities would have to be chosen sep-
arately to give the best Fl score.
The greedy search results just presented were
based on a partitioning of the feature sets into groups
which correspond to the way in which the features
were implemented. Since the search operators apply
at group granularity, and are not able to select fea-
tures from within a group, the way in which the fea-
tures are grouped is likely to have a bearing on the
performance of the best system found by the algo-
rithm. The next set of experiments investigates the
effective the feature grouping by conducting greedy
search with groups chosen randomly.
</bodyText>
<table confidence="0.9982654">
Corpus Relation Initial Final BEP Ensemble
Type BEP BEP
PPI PPI 51.1 52.9, 52.4, 52.7, 52.5
52.8, 52.6
FRAG 55.7 56.3, 56.1, 56.1, 56.3
56.3, 56.4
TE TE 51.4 52.0 , 51.8, 52.5, 52.1
51.9, 52.9
FRAG 60.1 60.8, 60.5, 60.4, 60.4
60.7, 60.5
</table>
<tableCaption confidence="0.668759">
Table 8: Greedy search feature exploration with random
feature groupings for intra-sentential relations. The ini-
tial feature set is a slightly modified all in each case, and
the search was run 5 times, testing on DEVTEST. The
ensemble system combines the 5 optimised feature sets
using the geometric mean probability.
</tableCaption>
<bodyText confidence="0.999706">
Using a variant of the all feature set where the con-
text sizes in EntityContext were set to 5, a greedy
search for the best performing system was imple-
mented by first dividing the feature set randomly
into 50 groups, and at each iteration testing the
performance with each group added and removed
in turn. The search was iterated until no further
improvement in performance was obtained, where
</bodyText>
<page confidence="0.992842">
25
</page>
<bodyText confidence="0.997777685714286">
performance was measured using BEP. As for the
previous greedy feature optimisations, the relation
extractor was trained on TRAIN and tested on DE-
VTEST. The results for intra-sentential relations are
shown in Table 8, where the experiment was repeated
several times with different (randomly chosen) par-
titions. After performing the five random knockout
searches of the feature space, an ensemble system
was created for each relation type by training a sys-
tem with each feature set and combining the five by
taking the geometric mean of the probabilities. The
performance of the ensemble system is shown in the
final column of Table 8.
Comparing the results in Table 8 with the corre-
sponding results for intra-sentential relations in Ta-
ble 6, it can be seen that splitting the features into re-
lated groups works better than random groups. The
ensemble does not improve on the individual scores,
probably because the systems in the ensemble are
not diverse enough (Dietterich, 2000)
To see how well the best feature sets generalise to
unseen data, RE systems were trained on TRAIN and
DEVTEST combined, and tested on TEST using dif-
ferent feature sets; the baseline sets (base and all),
and the fully optimised set (best). In addition, to
ensure that the greedy feature optimisation was not
biasing the feature set towards the particular learner
employed (i.e. MAXENT), systems were also trained
and tested using SVM. The MAXENT system had its
Gaussian prior optimised on the DEVTEST set, whilst
SVM was found to work best with a linear kernel, and
its cost factor was optimised on DEVTEST. The value
of the decision function was used for thresholding the
SVM model in order to calculate the BEP. The com-
parison of all systems on TEST is shown in Table 9.
</bodyText>
<table confidence="0.997108909090909">
Corpus Relation Learner Feature Set
Type
base all best
PPI PPI MAXENT 39.7 48.3 49.1
SVM 39.6 49.2 49.9
PPI FRAG MAXENT 56.9 68.0 69.4
SVM 54.9 68.2 69.5
TE TE MAXENT 39.0 47.9 46.8
SVM 39.6 49.8 50.1
TE FRAG MAXENT 60.1 63.4 68.9
SVM 59.7 67.7 70.4
</table>
<tableCaption confidence="0.93722875">
Table 9: The performance of the system trained on TRAIN
and DEVTEST, and tested on TEST. Performance is com-
pared across the baseline feature sets (base and all) and
the optimised feature set (best) using each classifier.
</tableCaption>
<bodyText confidence="0.999775777777778">
The results in Table 9 show that, in general, both
classifiers perform better with the all feature set than
with the base feature set, and best of all with the
best feature set. The SVM classifier preserves this
ordering throughout, and actually performs better
than the MAXENT classifier overall, even though the
features were optimised for MAXENT. For MAXENT,
the best model outperforms all in three out of four
cases, with the exception being TE.
</bodyText>
<sectionHeader confidence="0.999193" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999986461538461">
It has been shown that a relation extraction system
based on a supervised classifier and a large collection
of shallow linguistic features can be applied to three
different types of relations in two different biomedical
corpora. Automated feature optimisation produced
small gains in performance which were still apparent
on a blind test set. Even though a wrapper method
was used using a specific classifier (MAXENT), the
feature set optimisations were still valid for an SVM
classifier.
Since the greedy search through feature space is
essentially a beam search with a beam size of one, it
could be extended by using a larger beam-size, run-
ning the feature set comparisons in parallel to reduce
total running time to a manageable size. Ad-hoc ex-
periments have suggested that better results could
be obtained by restarting the feature optimisation
in different positions, indicating that local optima
could be a problem, but a thorough investigation
of the search space nature has been left for future
work. Furthermore, the hyperparameter optimisa-
tion of the classifiers (for example the Gaussian prior
in MAXENT) could be incorporated into the search.
Whilst the relation extractor was successful on
intra-sentential relations, it is less successful on inter-
sentential relations, perhaps becuase of the lingusitic
complexity of these, and the sparsity of positive ex-
amples. The split into inter- and inter-sentential
examples in the current system seems justified as
they have quite different characteristic, but there
may also be a case for splitting the intra-sententials
further, into intra- and inter-clausals, as suggested
by Maslennikov and Chua (2007), and then treating
inter-clausals and inter-sententials together. Whilst
intra-clausals are more likely to use simple construc-
tions and be amenable to modelling with shallow lin-
guistic features, inter-sententials and inter-clausals
are more likely to use complex linguistic phenomena
such as corefereces.
</bodyText>
<sectionHeader confidence="0.997547" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.983829">
This work was supported by the Text Mining Pro-
gramme of ITI Life Sciences Scotland (http://www.
itilifesciences.com).
</bodyText>
<page confidence="0.993546">
26
</page>
<sectionHeader confidence="0.995802" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999855317647059">
Bea Alex, Claire Grover, Barry Haddow, Mijail Kabad-
jov, Ewan Klein, Michael Matthews, Stuart Roebuck,
Richard Tobin, and Xinglong Wang. 2008. The
ITI TXM Corpora: Tissue Expressions and Protein-
Protein Interactions. In Proceedings of LREC.
Razvan C. Bunescu and Raymond J. Mooney. 2007. Ex-
tracting relations from text: From word sequences to
dependency paths. In Anne Kao and Steve Poteet, ed-
itors, Text Mining and Natural Language Processing,
pages 29–44. Springer.
Razvan Bunescu, Ruifang Ge, Rohit J. Kate, Edward M.
Marcotte, Raymond J. Mooney, Arun K. Ramani,
and Yuk W. Wong. 2005. Comparative experiments
on learning information extractors for proteins and
their interactions. Artificial Intelligence in Medicine,
33(2):139–155.
James Curran and Stephen Clark. 2003. Language in-
dependent NER using a maximum entropy tagger. In
Proceedings of CoNLL.
James Cussens and Claire Nedellec, editors. 2005. Pro-
ceedings of Language Learning in Logic.
Thomas G. Dietterich. 2000. Ensemble methods in ma-
chine learning. Lecture Notes in Computer Science,
1857:1–15.
Gunes Erkan, Arzucan Ozgur, and Dragomir R. Radev.
2007. Semi-supervised classification for extracting pro-
tein interaction sentences using dependency parsing.
In Proceedings of EMNLP-CoNLL.
Kuzman Ganchev, Koby Crammer, Fernando Pereira,
Gideon Mann, Kedar Bellare, Andrew McCallum,
Steven Carroll, Yang Jin, and Peter White. 2007.
Penn/UMass/CHOP Biocreative II systems. In Pro-
ceedings of the Second BioCreative Challenge Evalua-
tion Workshop.
Claudio Giuliano, Alberto Lavelli, and Lorenza Romano.
2006. Exploiting shallow linguistic information for re-
lation extraction from biomedical literature. In Pro-
ceedings of EACL.
Claire Grover and Richard Tobin. 2006. Rule-based
chunking and reusability. In Proceedings of LREC.
Isabelle Guyon and Andre Elisseeff. 2003. An intro-
duction to variable and feature selection. Journal of
Machine Learning Research, 3(Mar):1157–1182.
Jing Jiang and Chengxiang Zhai. 2007. A systematic
exploration of the feature space for relation extraction.
In Proceedings of NAACL.
Thorsten Joachims. 1999. Making large-scale support
vector machine learning practical. In Advances in Ker-
nel Methods: Support Vector Machines. MIT Press,
Cambridge, MA.
S. Katrenko and P. W. Adriaans. 2006. Learning rela-
tions from biomedical corpora using dependency tree
levels. In Proceedings of Benelearn.
Jin D. Kim, Tomoko Ohta, and Jun&apos;ichi Tsujii. 2008.
Corpus annotation for mining biomedical events from
literature. BMC Bioinformatics, 9(1).
Martin Krallinger, Florian Leitner, Carlos Rodriguez-
Penagos, and Alfonso Valencia. 2008. Overview of
the protein-protein interaction annotation extraction
task of BioCreative II. Genome Biology (in press).
Mstislav Maslennikov and Tat S. Chua. 2007. A multi-
resolution framework for information extraction from
free text. In Proceedings of ACL.
Guido Minnen, John Carroll, and Darren Pearce. 2000.
Robust, applied morphological generation. In Proceed-
ings of INLG.
Leif Arda Nielsen. 2006. Extracting protein-protein in-
teractions using simple contextual features. In Pro-
ceedings of BioNLP.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjorne, Jorma Boberg, Jouni Jarvinen, and Tapio
Salakoski. 2007. Bioinfer: A corpus for information
extraction in the biomedical domain. BMC Bioinfor-
matics, 8(1).
Marios Skounakis, Mark Craven, and Soumya Ray. 2003.
Hierarchical hidden markov models for information ex-
traction. In Georg Gottlob, Toby Walsh, Georg Gott-
lob, and Toby Walsh, editors, Proceedings of IJCAI.
L. Smith, T. Rindflesch, and W. J. Wilbur. 2004. Med-
Post: a part-of-speech tagger for biomedical text.
Bioinformatics, 20(14):2320–2321.
Guodong Zhou, Min Zhang, Donghong Ji, and Qiaoming
Zhu. 2007. Tree kernel-based relation extraction with
context-sensitive structured parse tree information. In
Proceedings of EMNLP-CoNLL.
</reference>
<page confidence="0.9988">
27
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.658967">
<title confidence="0.999526">Using Automated Feature Optimisation to Create an Relation Extraction System</title>
<author confidence="0.997865">Barry</author>
<affiliation confidence="0.990352">School of Informatics, University of</affiliation>
<address confidence="0.725077">2 Buccleuch Place, Edinburgh, Scotland, EH8</address>
<email confidence="0.997928">bhaddow@inf.ed.ac.uk</email>
<abstract confidence="0.9912599">An adaptable relation extraction system for the biomedical domain is presented. The system makes use of a large set of contextual and shallow syntactic features, which can be automatically optimised for each relation type. The system is tested on three different relation types; protein-protein interactions, tissue expression relations and fragment to parent protein relations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Bea Alex</author>
<author>Claire Grover</author>
<author>Barry Haddow</author>
<author>Mijail Kabadjov</author>
<author>Ewan Klein</author>
<author>Michael Matthews</author>
<author>Stuart Roebuck</author>
<author>Richard Tobin</author>
<author>Xinglong Wang</author>
</authors>
<title>The ITI TXM Corpora: Tissue Expressions and ProteinProtein Interactions.</title>
<date>2008</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="3117" citStr="Alex et al., 2008" startWordPosition="501" endWordPosition="504">t and then use a greedy search to explore the feature set and select the best subset of features. This method of feature set optimisation is not new (for example, it was applied by one team (Ganchev et al., 2007) on the BioCreative II Gene Mention task ), but in this work a comparison of search starting points and feature groupings will be presented. All RE systems require a human-annotated corpus for testing, and since a supervised machine learning approach is employed, a corpus is also required for training the system. The experiments described in this paper make use of the ITI TXM corpora (Alex et al., 2008), which include the PPI corpus addressing protein-protein interactions, and the TE corpus addressing tissue expression. Both corpora consist of approximately 200 full-text biomedical research papers annotated with entities, normalisations of entities to standard databases, relations, and with enriched information added to the relations. Only the entities and relations will be considered here. This paper is organised as follows: after reviewing related work in the following section, the RE system is described in Section 3, including a description of the corpora, the relation candidate extractio</context>
</contexts>
<marker>Alex, Grover, Haddow, Kabadjov, Klein, Matthews, Roebuck, Tobin, Wang, 2008</marker>
<rawString>Bea Alex, Claire Grover, Barry Haddow, Mijail Kabadjov, Ewan Klein, Michael Matthews, Stuart Roebuck, Richard Tobin, and Xinglong Wang. 2008. The ITI TXM Corpora: Tissue Expressions and ProteinProtein Interactions. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>Extracting relations from text: From word sequences to dependency paths.</title>
<date>2007</date>
<booktitle>Text Mining and Natural Language Processing,</booktitle>
<pages>29--44</pages>
<editor>In Anne Kao and Steve Poteet, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="5215" citStr="Bunescu and Mooney, 2007" startWordPosition="816" endWordPosition="819"> teams, however, extracted the interaction mentions as a first step and then processed these to give the document level list of curatable interactions. The extraction of protein-protein interactions has also been helped by the availability of annotated corpora, such as AIMed (Bunescu et al., 2005), which consists of around 1000 Medline abstracts annotated with proteins and their interactions. In common with the LLL corpus, the AIMed corpus only contains intra-sentential relations, and is somewhat smaller than the corpus used in the current work. In addition to the work by the corpus creators (Bunescu and Mooney, 2007), other authors have achieved good results on AIMed by making use of dependency parses in different ways (Erkan et al., 2007; Katrenko and Adriaans, 2006). It is not clear, however, how well these techniques would transfer to other, similar, RE problems, and how much work would be involved in tuning the systems for a new problem. Supervised learning based on shallow syntactic features has also been applied to the biomedical domain, again focusing on protein-protein interactions (Nielsen, 2006; Giuliano et al., 2006). A systematic exploration of a set of such features for proteinprotein interac</context>
</contexts>
<marker>Bunescu, Mooney, 2007</marker>
<rawString>Razvan C. Bunescu and Raymond J. Mooney. 2007. Extracting relations from text: From word sequences to dependency paths. In Anne Kao and Steve Poteet, editors, Text Mining and Natural Language Processing, pages 29–44. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Ruifang Ge</author>
<author>Rohit J Kate</author>
<author>Edward M Marcotte</author>
<author>Raymond J Mooney</author>
<author>Arun K Ramani</author>
<author>Yuk W Wong</author>
</authors>
<title>Comparative experiments on learning information extractors for proteins and their interactions.</title>
<date>2005</date>
<journal>Artificial Intelligence in Medicine,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="4888" citStr="Bunescu et al., 2005" startWordPosition="764" endWordPosition="767">Logic (Cussens and Nedellec, 2005), and the BioCreative II Interaction Pairs Subtask (Krallinger et al., 2008). It should be noted that the latter task, rather than being concerned with the extraction of specific interaction relation mentions, required systems to list the (curatable) interactions at a document level. Many teams, however, extracted the interaction mentions as a first step and then processed these to give the document level list of curatable interactions. The extraction of protein-protein interactions has also been helped by the availability of annotated corpora, such as AIMed (Bunescu et al., 2005), which consists of around 1000 Medline abstracts annotated with proteins and their interactions. In common with the LLL corpus, the AIMed corpus only contains intra-sentential relations, and is somewhat smaller than the corpus used in the current work. In addition to the work by the corpus creators (Bunescu and Mooney, 2007), other authors have achieved good results on AIMed by making use of dependency parses in different ways (Erkan et al., 2007; Katrenko and Adriaans, 2006). It is not clear, however, how well these techniques would transfer to other, similar, RE problems, and how much work </context>
</contexts>
<marker>Bunescu, Ge, Kate, Marcotte, Mooney, Ramani, Wong, 2005</marker>
<rawString>Razvan Bunescu, Ruifang Ge, Rohit J. Kate, Edward M. Marcotte, Raymond J. Mooney, Arun K. Ramani, and Yuk W. Wong. 2005. Comparative experiments on learning information extractors for proteins and their interactions. Artificial Intelligence in Medicine, 33(2):139–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Curran</author>
<author>Stephen Clark</author>
</authors>
<title>Language independent NER using a maximum entropy tagger.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="13883" citStr="Curran and Clark, 2003" startWordPosition="2167" endWordPosition="2170"> In particular, the IAA for intrasentential FRAG relations is very high, probably because many of these are very straightforward constructions such as “Fragment of Protein”. Intersentential relations are often less clear as they involve linking information between several sentences, for example using coreferences. Both corpora were pre-processed before RE was applied. The pre-processing involved tokenisation, sentence boundary detection, lemmatising. part-ofspeech tagging, head word detection and chunking. The part-of-speech tagging uses the Curran &amp; Clark maximum entropy Markov model tagger (Curran and Clark, 2003) trained on MedPost data (Smith et al., 2004), whilst the other preprocessing stages are all rule-based. The tokenisation, sentence boundary detection, head word identification and chunking components were implemented with the LT-XML2 tools (Grover and Tobin, 2006), and the lemmatisation used morpha (Minnen et al., 2000). 3.2 The Relation Extraction System Relation extraction is treated a classification problem, by generating candidate relations, and classifying them as either true or false. In the optimisation experiments described in this paper, Zhang Le&apos;s maximum entropy (MAXENT) classifier</context>
</contexts>
<marker>Curran, Clark, 2003</marker>
<rawString>James Curran and Stephen Clark. 2003. Language independent NER using a maximum entropy tagger. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<date>2005</date>
<booktitle>Proceedings of Language Learning in Logic.</booktitle>
<editor>James Cussens and Claire Nedellec, editors.</editor>
<marker>2005</marker>
<rawString>James Cussens and Claire Nedellec, editors. 2005. Proceedings of Language Learning in Logic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas G Dietterich</author>
</authors>
<title>Ensemble methods in machine learning.</title>
<date>2000</date>
<journal>Lecture Notes in Computer Science,</journal>
<pages>1857--1</pages>
<contexts>
<context position="33810" citStr="Dietterich, 2000" startWordPosition="5447" endWordPosition="5448">f the feature space, an ensemble system was created for each relation type by training a system with each feature set and combining the five by taking the geometric mean of the probabilities. The performance of the ensemble system is shown in the final column of Table 8. Comparing the results in Table 8 with the corresponding results for intra-sentential relations in Table 6, it can be seen that splitting the features into related groups works better than random groups. The ensemble does not improve on the individual scores, probably because the systems in the ensemble are not diverse enough (Dietterich, 2000) To see how well the best feature sets generalise to unseen data, RE systems were trained on TRAIN and DEVTEST combined, and tested on TEST using different feature sets; the baseline sets (base and all), and the fully optimised set (best). In addition, to ensure that the greedy feature optimisation was not biasing the feature set towards the particular learner employed (i.e. MAXENT), systems were also trained and tested using SVM. The MAXENT system had its Gaussian prior optimised on the DEVTEST set, whilst SVM was found to work best with a linear kernel, and its cost factor was optimised on D</context>
</contexts>
<marker>Dietterich, 2000</marker>
<rawString>Thomas G. Dietterich. 2000. Ensemble methods in machine learning. Lecture Notes in Computer Science, 1857:1–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gunes Erkan</author>
<author>Arzucan Ozgur</author>
<author>Dragomir R Radev</author>
</authors>
<title>Semi-supervised classification for extracting protein interaction sentences using dependency parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="5339" citStr="Erkan et al., 2007" startWordPosition="838" endWordPosition="841">atable interactions. The extraction of protein-protein interactions has also been helped by the availability of annotated corpora, such as AIMed (Bunescu et al., 2005), which consists of around 1000 Medline abstracts annotated with proteins and their interactions. In common with the LLL corpus, the AIMed corpus only contains intra-sentential relations, and is somewhat smaller than the corpus used in the current work. In addition to the work by the corpus creators (Bunescu and Mooney, 2007), other authors have achieved good results on AIMed by making use of dependency parses in different ways (Erkan et al., 2007; Katrenko and Adriaans, 2006). It is not clear, however, how well these techniques would transfer to other, similar, RE problems, and how much work would be involved in tuning the systems for a new problem. Supervised learning based on shallow syntactic features has also been applied to the biomedical domain, again focusing on protein-protein interactions (Nielsen, 2006; Giuliano et al., 2006). A systematic exploration of a set of such features for proteinprotein interaction extraction was recently provided by Jiang and Zhai (2007), who also used features derived from the Collins parser. They</context>
</contexts>
<marker>Erkan, Ozgur, Radev, 2007</marker>
<rawString>Gunes Erkan, Arzucan Ozgur, and Dragomir R. Radev. 2007. Semi-supervised classification for extracting protein interaction sentences using dependency parsing. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
<author>Gideon Mann</author>
<author>Kedar Bellare</author>
<author>Andrew McCallum</author>
<author>Steven Carroll</author>
<author>Yang Jin</author>
<author>Peter White</author>
</authors>
<title>Penn/UMass/CHOP Biocreative II systems.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second BioCreative Challenge Evaluation Workshop.</booktitle>
<contexts>
<context position="2711" citStr="Ganchev et al., 2007" startWordPosition="433" endWordPosition="436">st linguistic tools such as part-ofspeech taggers and chunkers, rather than more brittle and less widely available tools such as parsers. The difficulty with feature-based methods is, however, how to select the best performing feature set, as simply adding all possible features does not necessarily give the best results (Guyon and Elisseeff, 2003). The approach taken here is to implement a large feature set and then use a greedy search to explore the feature set and select the best subset of features. This method of feature set optimisation is not new (for example, it was applied by one team (Ganchev et al., 2007) on the BioCreative II Gene Mention task ), but in this work a comparison of search starting points and feature groupings will be presented. All RE systems require a human-annotated corpus for testing, and since a supervised machine learning approach is employed, a corpus is also required for training the system. The experiments described in this paper make use of the ITI TXM corpora (Alex et al., 2008), which include the PPI corpus addressing protein-protein interactions, and the TE corpus addressing tissue expression. Both corpora consist of approximately 200 full-text biomedical research pa</context>
</contexts>
<marker>Ganchev, Crammer, Pereira, Mann, Bellare, McCallum, Carroll, Jin, White, 2007</marker>
<rawString>Kuzman Ganchev, Koby Crammer, Fernando Pereira, Gideon Mann, Kedar Bellare, Andrew McCallum, Steven Carroll, Yang Jin, and Peter White. 2007. Penn/UMass/CHOP Biocreative II systems. In Proceedings of the Second BioCreative Challenge Evaluation Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudio Giuliano</author>
<author>Alberto Lavelli</author>
<author>Lorenza Romano</author>
</authors>
<title>Exploiting shallow linguistic information for relation extraction from biomedical literature.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="5736" citStr="Giuliano et al., 2006" startWordPosition="901" endWordPosition="904">s used in the current work. In addition to the work by the corpus creators (Bunescu and Mooney, 2007), other authors have achieved good results on AIMed by making use of dependency parses in different ways (Erkan et al., 2007; Katrenko and Adriaans, 2006). It is not clear, however, how well these techniques would transfer to other, similar, RE problems, and how much work would be involved in tuning the systems for a new problem. Supervised learning based on shallow syntactic features has also been applied to the biomedical domain, again focusing on protein-protein interactions (Nielsen, 2006; Giuliano et al., 2006). A systematic exploration of a set of such features for proteinprotein interaction extraction was recently provided by Jiang and Zhai (2007), who also used features derived from the Collins parser. They did not, however, experiment with the automated optimisation of the feature sets. In the news domain, the best reported results on the ACE dataset&apos; have been achieved by a composite kernel which depends partially on a full parse, and partially on a collection of shallow syntactic features (Zhou et al., 2007). Aside from protein-protein interactions, there has been little work directed at other</context>
</contexts>
<marker>Giuliano, Lavelli, Romano, 2006</marker>
<rawString>Claudio Giuliano, Alberto Lavelli, and Lorenza Romano. 2006. Exploiting shallow linguistic information for relation extraction from biomedical literature. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Grover</author>
<author>Richard Tobin</author>
</authors>
<title>Rule-based chunking and reusability.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="14148" citStr="Grover and Tobin, 2006" startWordPosition="2206" endWordPosition="2209">several sentences, for example using coreferences. Both corpora were pre-processed before RE was applied. The pre-processing involved tokenisation, sentence boundary detection, lemmatising. part-ofspeech tagging, head word detection and chunking. The part-of-speech tagging uses the Curran &amp; Clark maximum entropy Markov model tagger (Curran and Clark, 2003) trained on MedPost data (Smith et al., 2004), whilst the other preprocessing stages are all rule-based. The tokenisation, sentence boundary detection, head word identification and chunking components were implemented with the LT-XML2 tools (Grover and Tobin, 2006), and the lemmatisation used morpha (Minnen et al., 2000). 3.2 The Relation Extraction System Relation extraction is treated a classification problem, by generating candidate relations, and classifying them as either true or false. In the optimisation experiments described in this paper, Zhang Le&apos;s maximum entropy (MAXENT) classifier2 was used, since its performance was very competitive and its fast training time permitted extensive feature experimentation. The Gaussian prior was set to 0.1, and the maximum training iterations to 100. In order to assess the performance of the final system, MAX</context>
</contexts>
<marker>Grover, Tobin, 2006</marker>
<rawString>Claire Grover and Richard Tobin. 2006. Rule-based chunking and reusability. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isabelle Guyon</author>
<author>Andre Elisseeff</author>
</authors>
<title>An introduction to variable and feature selection.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1157</pages>
<contexts>
<context position="2439" citStr="Guyon and Elisseeff, 2003" startWordPosition="382" endWordPosition="385">ge collection of shallow syntactic and contextual features. Relation candidates are pairs of entities, picked out using an appropriate candidate generation strategy. The use of shallow (as opposed to deep) syntactic features means that the system can rely 19 on relatively robust linguistic tools such as part-ofspeech taggers and chunkers, rather than more brittle and less widely available tools such as parsers. The difficulty with feature-based methods is, however, how to select the best performing feature set, as simply adding all possible features does not necessarily give the best results (Guyon and Elisseeff, 2003). The approach taken here is to implement a large feature set and then use a greedy search to explore the feature set and select the best subset of features. This method of feature set optimisation is not new (for example, it was applied by one team (Ganchev et al., 2007) on the BioCreative II Gene Mention task ), but in this work a comparison of search starting points and feature groupings will be presented. All RE systems require a human-annotated corpus for testing, and since a supervised machine learning approach is employed, a corpus is also required for training the system. The experimen</context>
<context position="25812" citStr="Guyon and Elisseeff, 2003" startWordPosition="4115" endWordPosition="4118">roup is only used for TE relations and is designed to detected the presence of indicators like +/+ and −/+ in the sentence(s) containing the relation. Options allow the existence and type of the one of these expressions to be indicated, and also its position relative to the participants, and whether it is adjacent to one of the participants. 3.4 Optimisation Feature selection methods include wrapper methods where feature sets are assessed according to their effectiveness for a given learner, and filter methods where features are removed using some criterion before being passed to the learner (Guyon and Elisseeff, 2003). In building the RE system, it was found that filter methods did not work well, probably due to the large number of interactions between the features, so a wrapper optimisation method was employed, consisting of greedy search through the space of possible feature sets. In the greedy search method, an initial feature set is selected and a model trained on the TRAIN set and tested on the DEVTEST set. A series of search operators (see below) are applied to the feature set to produce a list of proposed new feature sets, one corresponding to each operator, and the new feature sets are tested in th</context>
</contexts>
<marker>Guyon, Elisseeff, 2003</marker>
<rawString>Isabelle Guyon and Andre Elisseeff. 2003. An introduction to variable and feature selection. Journal of Machine Learning Research, 3(Mar):1157–1182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>Chengxiang Zhai</author>
</authors>
<title>A systematic exploration of the feature space for relation extraction.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="5877" citStr="Jiang and Zhai (2007)" startWordPosition="924" endWordPosition="927">s on AIMed by making use of dependency parses in different ways (Erkan et al., 2007; Katrenko and Adriaans, 2006). It is not clear, however, how well these techniques would transfer to other, similar, RE problems, and how much work would be involved in tuning the systems for a new problem. Supervised learning based on shallow syntactic features has also been applied to the biomedical domain, again focusing on protein-protein interactions (Nielsen, 2006; Giuliano et al., 2006). A systematic exploration of a set of such features for proteinprotein interaction extraction was recently provided by Jiang and Zhai (2007), who also used features derived from the Collins parser. They did not, however, experiment with the automated optimisation of the feature sets. In the news domain, the best reported results on the ACE dataset&apos; have been achieved by a composite kernel which depends partially on a full parse, and partially on a collection of shallow syntactic features (Zhou et al., 2007). Aside from protein-protein interactions, there has been little work directed at other types of relations in the biomedical domain. Recent corpus annotation projects such as Genia (Kim et al., 2008) and BioInfer (Pyysalo et al.</context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jing Jiang and Chengxiang Zhai. 2007. A systematic exploration of the feature space for relation extraction. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale support vector machine learning practical.</title>
<date>1999</date>
<booktitle>In Advances in Kernel Methods: Support Vector Machines.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="14843" citStr="Joachims, 1999" startWordPosition="2315" endWordPosition="2316">action System Relation extraction is treated a classification problem, by generating candidate relations, and classifying them as either true or false. In the optimisation experiments described in this paper, Zhang Le&apos;s maximum entropy (MAXENT) classifier2 was used, since its performance was very competitive and its fast training time permitted extensive feature experimentation. The Gaussian prior was set to 0.1, and the maximum training iterations to 100. In order to assess the performance of the final system, MAXENT was compared with support vector machines (SVM) using the SVMl&apos;ght toolkit (Joachims, 1999). Since both the classifiers assign a confidence to each prediction, a varying threshold can be applied to the output of the classifier to provide a precision-recall 2http://homepages.inf.ed.ac.uk/s0450736/maxenttoolkit.html tradeoff. Candidate relations were generated by considering entity pairs of the appropriate type, taking into account the distance between the entities. It was thought that inter-sentential and intra-sentential relations would require different feature sets and different models, so inter- and intra-sentential candidates were generated separately. For intra-sentential relat</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale support vector machine learning practical. In Advances in Kernel Methods: Support Vector Machines. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Katrenko</author>
<author>P W Adriaans</author>
</authors>
<title>Learning relations from biomedical corpora using dependency tree levels.</title>
<date>2006</date>
<booktitle>In Proceedings of Benelearn.</booktitle>
<contexts>
<context position="5369" citStr="Katrenko and Adriaans, 2006" startWordPosition="842" endWordPosition="845"> The extraction of protein-protein interactions has also been helped by the availability of annotated corpora, such as AIMed (Bunescu et al., 2005), which consists of around 1000 Medline abstracts annotated with proteins and their interactions. In common with the LLL corpus, the AIMed corpus only contains intra-sentential relations, and is somewhat smaller than the corpus used in the current work. In addition to the work by the corpus creators (Bunescu and Mooney, 2007), other authors have achieved good results on AIMed by making use of dependency parses in different ways (Erkan et al., 2007; Katrenko and Adriaans, 2006). It is not clear, however, how well these techniques would transfer to other, similar, RE problems, and how much work would be involved in tuning the systems for a new problem. Supervised learning based on shallow syntactic features has also been applied to the biomedical domain, again focusing on protein-protein interactions (Nielsen, 2006; Giuliano et al., 2006). A systematic exploration of a set of such features for proteinprotein interaction extraction was recently provided by Jiang and Zhai (2007), who also used features derived from the Collins parser. They did not, however, experiment </context>
</contexts>
<marker>Katrenko, Adriaans, 2006</marker>
<rawString>S. Katrenko and P. W. Adriaans. 2006. Learning relations from biomedical corpora using dependency tree levels. In Proceedings of Benelearn.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin D Kim</author>
<author>Tomoko Ohta</author>
<author>Jun&apos;ichi Tsujii</author>
</authors>
<title>Corpus annotation for mining biomedical events from literature.</title>
<date>2008</date>
<journal>BMC Bioinformatics,</journal>
<volume>9</volume>
<issue>1</issue>
<contexts>
<context position="1313" citStr="Kim et al., 2008" startWordPosition="195" endWordPosition="198">NER) and relation extraction (RE) has tended to focus on the extracting proteins and their interactions, with less thought given to how to adapt such systems to other entities and relations of biomedical interest. This is especially true for RE, where there is very little work on relations other than protein-protein interactions. Nevertheless, in order to create applications of use to biologists such as curation assistants and improved information extraction and retrieval systems it will be necessary to treat a broader range of semantic relations. The recent release of the Genia event corpus (Kim et al., 2008) will help to drive this research. The aim of this paper is to address the problem of how to create an RE system, which can be adapted to different biomedical RE problems with a minimum of manual intervention. Since this paper focuses on relation extraction, it will be assumed that the named entities are given, in other words the human annotated entities are used in all experiments. The approach taken to RE is to treat it as a supervised classification problem on relation candidates, using a large collection of shallow syntactic and contextual features. Relation candidates are pairs of entitie</context>
<context position="6448" citStr="Kim et al., 2008" startWordPosition="1018" endWordPosition="1021">was recently provided by Jiang and Zhai (2007), who also used features derived from the Collins parser. They did not, however, experiment with the automated optimisation of the feature sets. In the news domain, the best reported results on the ACE dataset&apos; have been achieved by a composite kernel which depends partially on a full parse, and partially on a collection of shallow syntactic features (Zhou et al., 2007). Aside from protein-protein interactions, there has been little work directed at other types of relations in the biomedical domain. Recent corpus annotation projects such as Genia (Kim et al., 2008) and BioInfer (Pyysalo et al., 2007) include multiple types of relations, however many of the relation types are represented in fairly small quantities. In earlier work (Skounakis et al., 2003), the extraction of cell localisation relations was studied using an automatically created corpus. 1http://www.nist.gov/speech/tests/ace/ 3 Methods 3.1 Corpora The ITI TXM corpora contain annotations related to protein-protein interactions (in the PPI corpus), and annotations related to tissue expression experiments (in the TE corpus). Each corpus consists of biomedical research articles, selected from P</context>
</contexts>
<marker>Kim, Ohta, Tsujii, 2008</marker>
<rawString>Jin D. Kim, Tomoko Ohta, and Jun&apos;ichi Tsujii. 2008. Corpus annotation for mining biomedical events from literature. BMC Bioinformatics, 9(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Krallinger</author>
<author>Florian Leitner</author>
<author>Carlos RodriguezPenagos</author>
<author>Alfonso Valencia</author>
</authors>
<title>Overview of the protein-protein interaction annotation extraction task of BioCreative II. Genome Biology</title>
<date>2008</date>
<note>(in press).</note>
<contexts>
<context position="4377" citStr="Krallinger et al., 2008" startWordPosition="685" endWordPosition="688"> the feature optimisation methods and the evaluation method. In Section 4 the results of the optimisation experiments are presented and discussed, with some concluding remarks in Section 5. BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 19–27, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics 2 Related Work Recent interest in the extraction of protein-protein interactions has been given added impetus by shared tasks such as the Language Learning in Logic (Cussens and Nedellec, 2005), and the BioCreative II Interaction Pairs Subtask (Krallinger et al., 2008). It should be noted that the latter task, rather than being concerned with the extraction of specific interaction relation mentions, required systems to list the (curatable) interactions at a document level. Many teams, however, extracted the interaction mentions as a first step and then processed these to give the document level list of curatable interactions. The extraction of protein-protein interactions has also been helped by the availability of annotated corpora, such as AIMed (Bunescu et al., 2005), which consists of around 1000 Medline abstracts annotated with proteins and their inter</context>
</contexts>
<marker>Krallinger, Leitner, RodriguezPenagos, Valencia, 2008</marker>
<rawString>Martin Krallinger, Florian Leitner, Carlos RodriguezPenagos, and Alfonso Valencia. 2008. Overview of the protein-protein interaction annotation extraction task of BioCreative II. Genome Biology (in press).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mstislav Maslennikov</author>
<author>Tat S Chua</author>
</authors>
<title>A multiresolution framework for information extraction from free text.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Maslennikov, Chua, 2007</marker>
<rawString>Mstislav Maslennikov and Tat S. Chua. 2007. A multiresolution framework for information extraction from free text. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guido Minnen</author>
<author>John Carroll</author>
<author>Darren Pearce</author>
</authors>
<title>Robust, applied morphological generation.</title>
<date>2000</date>
<booktitle>In Proceedings of INLG.</booktitle>
<contexts>
<context position="14205" citStr="Minnen et al., 2000" startWordPosition="2216" endWordPosition="2219">ora were pre-processed before RE was applied. The pre-processing involved tokenisation, sentence boundary detection, lemmatising. part-ofspeech tagging, head word detection and chunking. The part-of-speech tagging uses the Curran &amp; Clark maximum entropy Markov model tagger (Curran and Clark, 2003) trained on MedPost data (Smith et al., 2004), whilst the other preprocessing stages are all rule-based. The tokenisation, sentence boundary detection, head word identification and chunking components were implemented with the LT-XML2 tools (Grover and Tobin, 2006), and the lemmatisation used morpha (Minnen et al., 2000). 3.2 The Relation Extraction System Relation extraction is treated a classification problem, by generating candidate relations, and classifying them as either true or false. In the optimisation experiments described in this paper, Zhang Le&apos;s maximum entropy (MAXENT) classifier2 was used, since its performance was very competitive and its fast training time permitted extensive feature experimentation. The Gaussian prior was set to 0.1, and the maximum training iterations to 100. In order to assess the performance of the final system, MAXENT was compared with support vector machines (SVM) using</context>
</contexts>
<marker>Minnen, Carroll, Pearce, 2000</marker>
<rawString>Guido Minnen, John Carroll, and Darren Pearce. 2000. Robust, applied morphological generation. In Proceedings of INLG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leif Arda Nielsen</author>
</authors>
<title>Extracting protein-protein interactions using simple contextual features.</title>
<date>2006</date>
<booktitle>In Proceedings of BioNLP.</booktitle>
<contexts>
<context position="5712" citStr="Nielsen, 2006" startWordPosition="899" endWordPosition="900"> than the corpus used in the current work. In addition to the work by the corpus creators (Bunescu and Mooney, 2007), other authors have achieved good results on AIMed by making use of dependency parses in different ways (Erkan et al., 2007; Katrenko and Adriaans, 2006). It is not clear, however, how well these techniques would transfer to other, similar, RE problems, and how much work would be involved in tuning the systems for a new problem. Supervised learning based on shallow syntactic features has also been applied to the biomedical domain, again focusing on protein-protein interactions (Nielsen, 2006; Giuliano et al., 2006). A systematic exploration of a set of such features for proteinprotein interaction extraction was recently provided by Jiang and Zhai (2007), who also used features derived from the Collins parser. They did not, however, experiment with the automated optimisation of the feature sets. In the news domain, the best reported results on the ACE dataset&apos; have been achieved by a composite kernel which depends partially on a full parse, and partially on a collection of shallow syntactic features (Zhou et al., 2007). Aside from protein-protein interactions, there has been littl</context>
</contexts>
<marker>Nielsen, 2006</marker>
<rawString>Leif Arda Nielsen. 2006. Extracting protein-protein interactions using simple contextual features. In Proceedings of BioNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sampo Pyysalo</author>
<author>Filip Ginter</author>
<author>Juho Heimonen</author>
<author>Jari Bjorne</author>
<author>Jorma Boberg</author>
<author>Jouni Jarvinen</author>
<author>Tapio Salakoski</author>
</authors>
<title>Bioinfer: A corpus for information extraction in the biomedical domain.</title>
<date>2007</date>
<journal>BMC Bioinformatics,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="6484" citStr="Pyysalo et al., 2007" startWordPosition="1024" endWordPosition="1027">nd Zhai (2007), who also used features derived from the Collins parser. They did not, however, experiment with the automated optimisation of the feature sets. In the news domain, the best reported results on the ACE dataset&apos; have been achieved by a composite kernel which depends partially on a full parse, and partially on a collection of shallow syntactic features (Zhou et al., 2007). Aside from protein-protein interactions, there has been little work directed at other types of relations in the biomedical domain. Recent corpus annotation projects such as Genia (Kim et al., 2008) and BioInfer (Pyysalo et al., 2007) include multiple types of relations, however many of the relation types are represented in fairly small quantities. In earlier work (Skounakis et al., 2003), the extraction of cell localisation relations was studied using an automatically created corpus. 1http://www.nist.gov/speech/tests/ace/ 3 Methods 3.1 Corpora The ITI TXM corpora contain annotations related to protein-protein interactions (in the PPI corpus), and annotations related to tissue expression experiments (in the TE corpus). Each corpus consists of biomedical research articles, selected from PubMed and PubMedCentral either becau</context>
</contexts>
<marker>Pyysalo, Ginter, Heimonen, Bjorne, Boberg, Jarvinen, Salakoski, 2007</marker>
<rawString>Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari Bjorne, Jorma Boberg, Jouni Jarvinen, and Tapio Salakoski. 2007. Bioinfer: A corpus for information extraction in the biomedical domain. BMC Bioinformatics, 8(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marios Skounakis</author>
<author>Mark Craven</author>
<author>Soumya Ray</author>
</authors>
<title>Hierarchical hidden markov models for information extraction.</title>
<date>2003</date>
<booktitle>Proceedings of IJCAI.</booktitle>
<editor>In Georg Gottlob, Toby Walsh, Georg Gottlob, and Toby Walsh, editors,</editor>
<contexts>
<context position="6641" citStr="Skounakis et al., 2003" startWordPosition="1048" endWordPosition="1051">ts. In the news domain, the best reported results on the ACE dataset&apos; have been achieved by a composite kernel which depends partially on a full parse, and partially on a collection of shallow syntactic features (Zhou et al., 2007). Aside from protein-protein interactions, there has been little work directed at other types of relations in the biomedical domain. Recent corpus annotation projects such as Genia (Kim et al., 2008) and BioInfer (Pyysalo et al., 2007) include multiple types of relations, however many of the relation types are represented in fairly small quantities. In earlier work (Skounakis et al., 2003), the extraction of cell localisation relations was studied using an automatically created corpus. 1http://www.nist.gov/speech/tests/ace/ 3 Methods 3.1 Corpora The ITI TXM corpora contain annotations related to protein-protein interactions (in the PPI corpus), and annotations related to tissue expression experiments (in the TE corpus). Each corpus consists of biomedical research articles, selected from PubMed and PubMedCentral either because they contain experimentally proven protein-protein interactions (for the PPI corpus), or because they contain tissue expression experiments (for the TE co</context>
</contexts>
<marker>Skounakis, Craven, Ray, 2003</marker>
<rawString>Marios Skounakis, Mark Craven, and Soumya Ray. 2003. Hierarchical hidden markov models for information extraction. In Georg Gottlob, Toby Walsh, Georg Gottlob, and Toby Walsh, editors, Proceedings of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Smith</author>
<author>T Rindflesch</author>
<author>W J Wilbur</author>
</authors>
<title>MedPost: a part-of-speech tagger for biomedical text.</title>
<date>2004</date>
<journal>Bioinformatics,</journal>
<volume>20</volume>
<issue>14</issue>
<contexts>
<context position="13928" citStr="Smith et al., 2004" startWordPosition="2175" endWordPosition="2178">relations is very high, probably because many of these are very straightforward constructions such as “Fragment of Protein”. Intersentential relations are often less clear as they involve linking information between several sentences, for example using coreferences. Both corpora were pre-processed before RE was applied. The pre-processing involved tokenisation, sentence boundary detection, lemmatising. part-ofspeech tagging, head word detection and chunking. The part-of-speech tagging uses the Curran &amp; Clark maximum entropy Markov model tagger (Curran and Clark, 2003) trained on MedPost data (Smith et al., 2004), whilst the other preprocessing stages are all rule-based. The tokenisation, sentence boundary detection, head word identification and chunking components were implemented with the LT-XML2 tools (Grover and Tobin, 2006), and the lemmatisation used morpha (Minnen et al., 2000). 3.2 The Relation Extraction System Relation extraction is treated a classification problem, by generating candidate relations, and classifying them as either true or false. In the optimisation experiments described in this paper, Zhang Le&apos;s maximum entropy (MAXENT) classifier2 was used, since its performance was very co</context>
</contexts>
<marker>Smith, Rindflesch, Wilbur, 2004</marker>
<rawString>L. Smith, T. Rindflesch, and W. J. Wilbur. 2004. MedPost: a part-of-speech tagger for biomedical text. Bioinformatics, 20(14):2320–2321.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guodong Zhou</author>
<author>Min Zhang</author>
<author>Donghong Ji</author>
<author>Qiaoming Zhu</author>
</authors>
<title>Tree kernel-based relation extraction with context-sensitive structured parse tree information.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="6249" citStr="Zhou et al., 2007" startWordPosition="987" endWordPosition="990"> biomedical domain, again focusing on protein-protein interactions (Nielsen, 2006; Giuliano et al., 2006). A systematic exploration of a set of such features for proteinprotein interaction extraction was recently provided by Jiang and Zhai (2007), who also used features derived from the Collins parser. They did not, however, experiment with the automated optimisation of the feature sets. In the news domain, the best reported results on the ACE dataset&apos; have been achieved by a composite kernel which depends partially on a full parse, and partially on a collection of shallow syntactic features (Zhou et al., 2007). Aside from protein-protein interactions, there has been little work directed at other types of relations in the biomedical domain. Recent corpus annotation projects such as Genia (Kim et al., 2008) and BioInfer (Pyysalo et al., 2007) include multiple types of relations, however many of the relation types are represented in fairly small quantities. In earlier work (Skounakis et al., 2003), the extraction of cell localisation relations was studied using an automatically created corpus. 1http://www.nist.gov/speech/tests/ace/ 3 Methods 3.1 Corpora The ITI TXM corpora contain annotations related </context>
</contexts>
<marker>Zhou, Zhang, Ji, Zhu, 2007</marker>
<rawString>Guodong Zhou, Min Zhang, Donghong Ji, and Qiaoming Zhu. 2007. Tree kernel-based relation extraction with context-sensitive structured parse tree information. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>