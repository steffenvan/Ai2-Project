<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000190">
<title confidence="0.999461">
Blinov: Distributed Representations of Words for
Aspect-Based Sentiment Analysis at SemEval 2014
</title>
<author confidence="0.993766">
Pavel Blinov, Eugeny Kotelnikov
</author>
<affiliation confidence="0.990496">
Vyatka State Humanities University
</affiliation>
<email confidence="0.996767">
{blinoff.pavel, kotelnikov.ev}@gmail.com
</email>
<sectionHeader confidence="0.982978" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999639375">
The article describes our system
submitted to the SemEval-2014 task
on Aspect-Based Sentiment Analy-
sis. The methods based on distribut-
ed representations of words for the
aspect term extraction and aspect
term polarity detection tasks are pre-
sented. The methods for the aspect
category detection and category po-
larity detection tasks are presented
as well. Well-known skip-gram
model for constructing the distribut-
ed representations is briefly de-
scribed. The results of our methods
are shown in comparison with the
baseline and the best result.
</bodyText>
<sectionHeader confidence="0.995164" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.987401933333333">
The sentiment analysis became an important
Natural Language Processing (NLP) task in the
recent few years. As many NLP tasks it’s a chal-
lenging one. The sentiment analysis can be very
helpful for some practical applications. For ex-
ample, it allows to study the users’ opinions
about a product automatically.
Many research has been devoted to the general
sentiment analysis (Pang et al., 2002),
(Amine et al., 2013), (Blinov et al., 2013) or
analysis of individual sentences (Yu and Hatzi-
vassiloglou, 2003), (Kim and Hovy, 2004),
(Wiebe and Riloff, 2005). Soon it became clear
that the sentiment analysis on the level of a
whole text or even sentences is too coarse. Gen-
</bodyText>
<note confidence="0.8637215">
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
</note>
<bodyText confidence="0.999927851851852">
eral sentiment analysis by its design is not capa-
ble to perform the detailed analysis of an ex-
pressed opinion. For example, it cannot correctly
detect the opinion in the sentence “Great food
but the service was dreadful!”. The sentence car-
ries opposite opinions on two facets of a restau-
rant. Therefore the more detailed version of the
sentiment analysis is needed. Such a version is
called the aspect-based sentiment analysis and it
works on the level of the significant aspects of
the target entity (Liu, 2012).
The aspect-based sentiment analysis includes
two main subtasks: the aspect term extraction
and its polarity detection (Liu, 2012). In this arti-
cle we describe the methods which address both
subtasks. The methods are based on the distribut-
ed representations of words. Such word represen-
tations (or word embeddings) are useful in many
NLP task, e.g. (Turian et al., 2009), (Al-
Rfou’ et al., 2013), (Turney, 2013).
The remainder of the article is as follows: sec-
tion two gives the overview of the data; the third
section shortly describes the distributed represen-
tations of words. The methods of the aspect term
extraction and polarity detection are presented in
the fourth and the fifth sections respectively. The
conclusions are given in the sixth section.
</bodyText>
<sectionHeader confidence="0.921728" genericHeader="method">
2 The Data
</sectionHeader>
<bodyText confidence="0.99967175">
The organisers provided the train data for restau-
rant and laptop domains. But as it will be clear
further our methods are heavily dependent on
unlabelled text data. So we additionally collected
the user reviews about restaurants from tripad-
viser.com and about laptops from amazon.com.
General statistics of the data are shown in Ta-
ble 1.
</bodyText>
<page confidence="0.47106">
140
</page>
<note confidence="0.9731205">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 140–144,
Dublin, Ireland, August 23-24, 2014.
</note>
<tableCaption confidence="0.999883">
Table 1: The amount of reviews.
</tableCaption>
<table confidence="0.942045666666667">
Domain The amount of reviews
Restaurants 652 055
Laptops 109 550
</table>
<bodyText confidence="0.997938666666667">
For all the data we performed tokenization,
stemming and morphological analysis using the
FreeLing library (Padró and Stanilovsky, 2012).
</bodyText>
<sectionHeader confidence="0.99334" genericHeader="method">
3 Distributed Representations of Words
</sectionHeader>
<bodyText confidence="0.999664114285714">
In this section we’ll try to give the high level
idea of the distributed representations of words.
The more technical details can be found in
(Mikolov et al., 2013).
It is closely related with a new promising di-
rection in machine learning called the deep learn-
ing. The core idea of the unsupervised deep
learning algorithms is to find automatically the
“good” set of features to represent the target ob-
ject (text, image, audio signal, etc.). The object
represented by the vector of real numbers is
called the distributed representation (Ru-
melhart et al., 1986). We used the skip-gram
model (Mikolov et al., 2013) implemented in
Gensim toolkit (Řehůřek and Sojka, 2010).
In general the learning procedure is as follows.
All the texts of the corpus are stuck together in a
single sequence of sentences. On the basis of the
corpus the lexicon is constructed. Next, the di-
mensionality of the vectors is chosen (we used
300 in our experiments). The greater number of
dimensions allows to capture more language reg-
ularities but leads to more computational com-
plexity of the learning. Each word from the lexi-
con is associated with the real numbers vector of
the selected dimensionality. Originally all the
vectors are randomly initialized. During the
learning procedure the algorithm “slides” with
the fixed size window (it’s algorithm parameter
that was retained by default – 5 words) along the
words of the sequence and calculates the proba-
bility (1) of context words appearance within the
window based on its central word under review
(or more precisely, its vector representation)
(Mikolov et al., 2013).
</bodyText>
<equation confidence="0.98910025">
XwO |wI)  W
 exp( v 
w 1
vw
</equation>
<bodyText confidence="0.999577555555555">
where and are the input and output vector
representations of w; and are the current
and predicted words, – the number of words
in vocabulary.
The ultimate goal of the described process is
to get such “good” vectors for each word, which
allow to predict its probable context. All such
vectors together form the vector space where
semantically similar words are grouped.
</bodyText>
<sectionHeader confidence="0.947205" genericHeader="method">
4 Aspect Term Extraction Method
</sectionHeader>
<bodyText confidence="0.999992">
We apply the same method for the aspect term
extraction task (Pontiki et al., 2014) for both do-
mains. The method consists of two steps: the
candidate selection and the term extraction.
</bodyText>
<subsectionHeader confidence="0.997542">
4.1 Candidate Selection
</subsectionHeader>
<bodyText confidence="0.999408833333333">
First of all we collect some statistics about the
terms in the train collection. We analysed two
facets of the aspect terms: the number of words
and their morphological structure. The infor-
mation about the number of words in a term is
shown in Table 2.
</bodyText>
<tableCaption confidence="0.8760575">
Table 2: The statistics for the number of words
in a term.
</tableCaption>
<table confidence="0.9966406">
Aspect term Domain
Restaurant, % Laptop, %
One-word 72.13 55.66
Two-word 19.05 32.87
Greater 8.82 11.47
</table>
<bodyText confidence="0.9197545">
On the basis of that we’ve decided to process
only single and two-word aspect terms. From the
single terms we treat only singular (NN, e.g.
staff, rice, texture, processor, ram, insult) and
plural nouns (NNS, e.g. perks, bagels, times,
dvds, buttons, pictures) as possible candidates,
because they largely predominate among the
one-word terms. All conjunctions of the form
NN_NN (e.g. sea_bass, lotus_leaf, chicken_dish,
battery_life, virus_protection, custom-
er_disservice) and NN_NNS (e.g. sushi_places,
menu_choices, seafood_lovers, usb_devices, re-
covery_discs, software_works) were candidates
for the two-word terms also because they are
most common in two-word aspect terms.
a  (a1,...,an) b  ( b 1 ,..., bn )
</bodyText>
<subsectionHeader confidence="0.865246">
4.2 Term Extraction
</subsectionHeader>
<bodyText confidence="0.9353303">
The second step for the aspect term identification
is the term extraction. As has already been told
the space (see Section 3) specifies the word
groups. Therefore the measure of similarity be-
tween the words (vectors) can be defined. For
NLP tasks it is often the cosine similarity meas-
ure. The similarity between two vectors

and is given by
(Manning et al., 2008):
</bodyText>
<equation confidence="0.6641205">
exp(vwO TvwI
)
T
, (1)
w
vw)
141
(2)
</equation>
<bodyText confidence="0.9994295">
where – the angle between the vectors, n – the
dimensionality of the space.
In case of the restaurant domain the category
and aspect terms are specified. For each category
the seed of the aspect terms can be automatically
selected: if only one category is assigned for a
train sentence then all its terms belong to it.
Within each set the average similarity between
the terms (the threshold category) can be found.
For the new candidate the average similarities
with the category’s seeds are calculated. If it is
greater than the threshold of any category than
the candidate is marked as an aspect term.
Also we’ve additionally applied some rules:
</bodyText>
<listItem confidence="0.997949666666667">
• Join consecutive terms in a single term.
• Join neutral adjective ahead the term (see
Section 5.2 for clarification about the neu-
tral adjective).
• Join fragments matching the pattern: &lt;an
aspect term&gt; of &lt;an aspect term&gt;.
</listItem>
<bodyText confidence="0.99911525">
In case of the laptop domain there are no spec-
ified categories so we treated all terms as the
terms belonging to one general category. And the
same procedure with candidates was performed.
</bodyText>
<subsectionHeader confidence="0.995815">
4.3 Category Detection
</subsectionHeader>
<bodyText confidence="0.999981333333333">
For the restaurant domain there was also the as-
pect category detection task (Ponti-
ki et al., 2014).
Since each word is represented by a vector,
each sentence can be cast to a single point as the
average of its vectors. Further average point for
each category can be found by means of the sen-
tence points. Then for an unseen sentence the
average point of its word vectors is calculated.
The category is selected by calculating the dis-
tances between all category points and a new
point and by choosing the minimum distance.
</bodyText>
<subsectionHeader confidence="0.669745">
4.4 Results
</subsectionHeader>
<bodyText confidence="0.9743490625">
The aspect term extraction and the aspect catego-
ry detection tasks were evaluated with Precision,
Recall and F-measure (Pontiki et al., 2014). The
F-measure was a primary metric for these tasks
so we present only it.
The result of our method ranked 19 out of 28
submissions (constrained and unconstrained) for
the aspect term extraction task for the laptop do-
main and 17 out of 29 for the restaurant domain.
For the category detection task (restaurant do-
main) the method ranked 9 out of 21.
Table 3 shows the results of our method
(Bold) for aspect term extraction task in compar-
ison with the baseline (Pontiki et al., 2014) and
the best result. Analogically the results for the
aspect category detection task are presented in
</bodyText>
<tableCaption confidence="0.938225666666667">
Table 4.
Table 3: Aspect term extraction results
(F-measure).
</tableCaption>
<table confidence="0.999678">
Laptop Restaurant
Best 0.7455 0.8401
Blinov 0.5207 0.7121
Baseline 0.3564 0.4715
</table>
<tableCaption confidence="0.893036">
Table 4: Aspect category detection results
(F-measure).
</tableCaption>
<table confidence="0.943595">
Restaurant
Best 0.8858
Blinov 0.7527
Baseline 0.6389
</table>
<sectionHeader confidence="0.88848" genericHeader="method">
5 Polarity Detection Method
</sectionHeader>
<bodyText confidence="0.999753166666667">
Our polarity detection method also exploits the
vector space (from Section 3) because the emo-
tional similarity between words can be traced in
it. As with the aspect term extraction method we
follow two-stage approach: the candidate selec-
tion and the polarity detection.
</bodyText>
<subsectionHeader confidence="0.991545">
5.1 Candidate Selection
</subsectionHeader>
<bodyText confidence="0.999193125">
All adjectives and verbs are considered as the
polarity term candidates. The amplifiers and the
negations have an important role in the process
of result polarity forming. In our method we took
into account only negations because it strongly
affects the word polarity. We’ve joined into one
unit all text fragments that match the following
pattern: not + &lt;JJ  |VB&gt;.
</bodyText>
<subsectionHeader confidence="0.996609">
5.2 Term Polarity Detection
</subsectionHeader>
<bodyText confidence="0.990432153846154">
At first we manually collected the small etalon
sets of positive and negative words for each do-
main. Every set contained 15 words that clearly
identify the sentiment. For example, for the posi-
tive polarity there were words such as: great,
fast, attentive, yummy, etc. and for the negative
polarity there were words like: terrible, ugly,
not_work, offensive, etc.
By measuring the average similarity for a can-
didate to the positive and the negative seed
words we decided whether it is positive (+1) or
negative (–1). Also we set up a neutral threshold
and a candidate’s polarity was treated as neutral
</bodyText>
<figure confidence="0.514401">
(0) if it didn’t exceed the threshold.
,
142
</figure>
<bodyText confidence="0.9934566">
For each term (within the window of 6 words)
we were looking for its closest polarity term can-
didate and sum up their polarities. For the final
decision about the term’s polarity there were
some conditions:
</bodyText>
<listItem confidence="0.9831965">
• If sum &gt; 0 then positive.
• If sum &lt; 0 then negative.
• If sum == 0 and all polarity terms are neu-
tral then neutral else conflict.
</listItem>
<subsectionHeader confidence="0.993156">
5.3 Category Polarity Detection
</subsectionHeader>
<bodyText confidence="0.999985">
By analogy with the category detection method,
using the train collection, we calculate the aver-
age polarity points for each category, i.e. there
were 5×4 such points (5 categories and 4 values
of polarity). Then a sentence was cast to a point
as the average of all its word-vectors. And clos-
est polarity points for the specified categories
defined the polarity.
</bodyText>
<subsectionHeader confidence="0.67824">
5.4 Results
</subsectionHeader>
<bodyText confidence="0.989073">
The results of our method (Bold) for the polarity
detection tasks are around the baseline results for
the Accuracy measure (Tables 5, 6).
</bodyText>
<tableCaption confidence="0.99699">
Table 5: Aspect term polarity detection results
</tableCaption>
<table confidence="0.99976875">
Laptop Restaurant
Best 0.7049 0.8095
Blinov 0.5229 0.6358
Baseline 0.5107 0.6428
</table>
<tableCaption confidence="0.7828705">
Table 6: Category polarity detection results
(Accuracy).
</tableCaption>
<table confidence="0.99326125">
Restaurant
Best 0.8293
Blinov 0.6566
Baseline 0.6566
</table>
<bodyText confidence="0.5728232">
However the test data is skewed to the positive
class and for that case the Accuracy is a poor
indicator. Because of that we also show macro F-
measure results for our and baseline methods
(Tables 7, 8).
</bodyText>
<tableCaption confidence="0.998036">
Table 7: Aspect term polarity detection results
</tableCaption>
<table confidence="0.999474666666667">
Laptop Restaurant
Blinov 0.3738 0.4334
Baseline 0.2567 0.2989
</table>
<tableCaption confidence="0.868125">
Table 8: Category polarity detection results
(F-measure).
</tableCaption>
<table confidence="0.917270333333333">
Restaurant
Blinov 0.5051
Baseline 0.3597
</table>
<bodyText confidence="0.99972075">
From that we can conclude that our method of
the polarity detection more delicately deals with
the minor represented classes than the baseline
method.
</bodyText>
<sectionHeader confidence="0.998616" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999865">
In the article we presented the methods for two
main subtasks for aspect-based sentiment analy-
sis: the aspect term extraction and the polarity
detection. The methods are based on the distrib-
uted representation of words and the notion of
similarities between the words.
For the aspect term extraction and category
detection tasks we get satisfied results which are
consistent with our cross-validation metrics. Un-
fortunately for the polarity detection tasks the
result of our method by official metrics are low.
But we showed that the proposed method is not
so bad and is capable to deal with the skewed
data better than the baseline method.
</bodyText>
<sectionHeader confidence="0.995491" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9996695">
We would like to thank the organizers and the
reviewers for their efforts.
</bodyText>
<sectionHeader confidence="0.872041" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.994922380952381">
Abdelmalek Amine, Reda Mohamed Hamou and
Michel Simonet. 2013. Detecting Opinions in
Tweets. International Journal of Data Mining and
Emerging Technologies, 3(1):23–32.
Christopher Manning, Prabhakar Raghavan and Hin-
rich Schütze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, New York,
NY, USA.
Bo Pang, Lillian Lee and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 79–86.
Bing Liu. 2012. Sentiment Analysis and Opinion
Mining. Synthesis Lectures on Human Language
Technologies.
Pavel Blinov, Maria Klekovkina, Eugeny Kotelnikov
and Oleg Pestov. 2013. Research of lexical ap-
proach and machine learning methods for senti-
ment analysis. Computational Linguistics and In-
tellectual Technologies, 2(12):48–58.
</reference>
<figure confidence="0.896030333333333">
(A
(F-
143
</figure>
<reference confidence="0.999315037735849">
Janyce Wiebe and Ellen Riloff. 2005. Creating sub-
jective and objective sentence classifiers from un-
annotated texts. In Proceedings of the 6th Interna-
tional Conference on Computational Linguistics
and Intelligent Text Processing, pages 486–497.
Joseph Turian, Lev Ratinov, Yoshua Bengio and Dan
Roth. 2009. A preliminary evaluation of word rep-
resentations for named-entity recognition. In Pro-
ceedings of NIPS Workshop on Grammar Induc-
tion, Representation of Language and Language
Learning.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado and Jeffrey Dean. 2013. Distributed Represen-
tations of Words and Phrases and their Composi-
tionality. In Proceedings of NIPS, pages 3111–
3119.
Lluis Padró and Evgeny Stanilovsky. 2012. FreeLing
3.0: Towards Wider Multilinguality. In Proceed-
ings of the Language Resources and Evaluation
Conference, LREC 2012, pages 2473–2479.
Soo-Min Kim and Eduard Hovy. 2004. Determining
the sentiment of opinions. In Proceedings of the
20th International Conference on Computational
Linguistics, COLING-2004.
Maria Pontiki, Dimitrios Galanis, John Pavlopoulos,
Harris Papageorgiou, Ion Androutsopoulos and
Suresh Manandhar. 2014. SemEval-2014 Task 4:
Aspect Based Sentiment Analysis. In Proceedings
of the 8th International Workshop on Semantic
Evaluation, SemEval 2014, Dublin, Ireland.
Peter Turney. 2013. Distributional semantics beyond
words: Supervised learning of analogy and para-
phrase. Transactions of the Association for Compu-
tational Linguistics, 1:353-366.
Radim Řehůřek and Petr Sojka. 2010. Software
Framework for Topic Modelling with Large Cor-
pora. In Proceedings of the LREC 2010 Workshop
on New Challenges for NLP Frameworks, pages
46–50.
Rami Al-Rfou’, Bryan Perozzi, Steven Skiena. 2013.
Polyglot: Distributed Word Representations for
Multilingual NLP. In Proceedings of Conference
on Computational Natural Language Learning,
CoNLL’2013.
David Rumelhart, Geoffrey Hintont, Ronald Wil-
liams. 1986. Learning representations by back-
propagating errors. Nature.
Hong Yu and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: Separating
facts from opinions and identifying the polarity of
opinion sentences. In Proceedings of the 2003
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 129–136.
</reference>
<page confidence="0.94673">
144
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.775783">
<title confidence="0.891413">Blinov: Distributed Representations of Words Aspect-Based Sentiment Analysis at SemEval 2014</title>
<author confidence="0.986857">Pavel Blinov</author>
<author confidence="0.986857">Eugeny</author>
<affiliation confidence="0.997803">Vyatka State Humanities University</affiliation>
<email confidence="0.993132">blinoff.pavel@gmail.com</email>
<email confidence="0.993132">kotelnikov.ev@gmail.com</email>
<abstract confidence="0.998999823529412">The article describes our system submitted to the SemEval-2014 task on Aspect-Based Sentiment Analysis. The methods based on distributed representations of words for the aspect term extraction and aspect term polarity detection tasks are presented. The methods for the aspect category detection and category polarity detection tasks are presented as well. Well-known skip-gram model for constructing the distributed representations is briefly described. The results of our methods are shown in comparison with the baseline and the best result.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Abdelmalek Amine</author>
<author>Reda Mohamed Hamou</author>
<author>Michel Simonet</author>
</authors>
<title>Detecting Opinions in Tweets.</title>
<date>2013</date>
<journal>International Journal of Data Mining and Emerging Technologies,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="1187" citStr="Amine et al., 2013" startWordPosition="173" endWordPosition="176">. Well-known skip-gram model for constructing the distributed representations is briefly described. The results of our methods are shown in comparison with the baseline and the best result. 1 Introduction The sentiment analysis became an important Natural Language Processing (NLP) task in the recent few years. As many NLP tasks it’s a challenging one. The sentiment analysis can be very helpful for some practical applications. For example, it allows to study the users’ opinions about a product automatically. Many research has been devoted to the general sentiment analysis (Pang et al., 2002), (Amine et al., 2013), (Blinov et al., 2013) or analysis of individual sentences (Yu and Hatzivassiloglou, 2003), (Kim and Hovy, 2004), (Wiebe and Riloff, 2005). Soon it became clear that the sentiment analysis on the level of a whole text or even sentences is too coarse. GenThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ eral sentiment analysis by its design is not capable to perform the detailed analysis of an expressed opinion. For example, it cannot</context>
</contexts>
<marker>Amine, Hamou, Simonet, 2013</marker>
<rawString>Abdelmalek Amine, Reda Mohamed Hamou and Michel Simonet. 2013. Detecting Opinions in Tweets. International Journal of Data Mining and Emerging Technologies, 3(1):23–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Manning</author>
</authors>
<title>Prabhakar Raghavan and Hinrich Schütze.</title>
<date>2008</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA.</location>
<marker>Manning, 2008</marker>
<rawString>Christopher Manning, Prabhakar Raghavan and Hinrich Schütze. 2008. Introduction to Information Retrieval. Cambridge University Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="1165" citStr="Pang et al., 2002" startWordPosition="169" endWordPosition="172">are presented as well. Well-known skip-gram model for constructing the distributed representations is briefly described. The results of our methods are shown in comparison with the baseline and the best result. 1 Introduction The sentiment analysis became an important Natural Language Processing (NLP) task in the recent few years. As many NLP tasks it’s a challenging one. The sentiment analysis can be very helpful for some practical applications. For example, it allows to study the users’ opinions about a product automatically. Many research has been devoted to the general sentiment analysis (Pang et al., 2002), (Amine et al., 2013), (Blinov et al., 2013) or analysis of individual sentences (Yu and Hatzivassiloglou, 2003), (Kim and Hovy, 2004), (Wiebe and Riloff, 2005). Soon it became clear that the sentiment analysis on the level of a whole text or even sentences is too coarse. GenThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ eral sentiment analysis by its design is not capable to perform the detailed analysis of an expressed opinion. </context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language Technologies.</title>
<date>2012</date>
<contexts>
<context position="2166" citStr="Liu, 2012" startWordPosition="333" endWordPosition="334"> footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ eral sentiment analysis by its design is not capable to perform the detailed analysis of an expressed opinion. For example, it cannot correctly detect the opinion in the sentence “Great food but the service was dreadful!”. The sentence carries opposite opinions on two facets of a restaurant. Therefore the more detailed version of the sentiment analysis is needed. Such a version is called the aspect-based sentiment analysis and it works on the level of the significant aspects of the target entity (Liu, 2012). The aspect-based sentiment analysis includes two main subtasks: the aspect term extraction and its polarity detection (Liu, 2012). In this article we describe the methods which address both subtasks. The methods are based on the distributed representations of words. Such word representations (or word embeddings) are useful in many NLP task, e.g. (Turian et al., 2009), (AlRfou’ et al., 2013), (Turney, 2013). The remainder of the article is as follows: section two gives the overview of the data; the third section shortly describes the distributed representations of words. The methods of the as</context>
</contexts>
<marker>Liu, 2012</marker>
<rawString>Bing Liu. 2012. Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pavel Blinov</author>
<author>Maria Klekovkina</author>
</authors>
<title>Eugeny Kotelnikov and Oleg Pestov.</title>
<date>2013</date>
<booktitle>Computational Linguistics and Intellectual Technologies,</booktitle>
<pages>2--12</pages>
<marker>Blinov, Klekovkina, 2013</marker>
<rawString>Pavel Blinov, Maria Klekovkina, Eugeny Kotelnikov and Oleg Pestov. 2013. Research of lexical approach and machine learning methods for sentiment analysis. Computational Linguistics and Intellectual Technologies, 2(12):48–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Ellen Riloff</author>
</authors>
<title>Creating subjective and objective sentence classifiers from unannotated texts.</title>
<date>2005</date>
<booktitle>In Proceedings of the 6th International Conference on Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>486--497</pages>
<contexts>
<context position="1326" citStr="Wiebe and Riloff, 2005" startWordPosition="195" endWordPosition="198">n in comparison with the baseline and the best result. 1 Introduction The sentiment analysis became an important Natural Language Processing (NLP) task in the recent few years. As many NLP tasks it’s a challenging one. The sentiment analysis can be very helpful for some practical applications. For example, it allows to study the users’ opinions about a product automatically. Many research has been devoted to the general sentiment analysis (Pang et al., 2002), (Amine et al., 2013), (Blinov et al., 2013) or analysis of individual sentences (Yu and Hatzivassiloglou, 2003), (Kim and Hovy, 2004), (Wiebe and Riloff, 2005). Soon it became clear that the sentiment analysis on the level of a whole text or even sentences is too coarse. GenThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ eral sentiment analysis by its design is not capable to perform the detailed analysis of an expressed opinion. For example, it cannot correctly detect the opinion in the sentence “Great food but the service was dreadful!”. The sentence carries opposite opinions on two fac</context>
</contexts>
<marker>Wiebe, Riloff, 2005</marker>
<rawString>Janyce Wiebe and Ellen Riloff. 2005. Creating subjective and objective sentence classifiers from unannotated texts. In Proceedings of the 6th International Conference on Computational Linguistics and Intelligent Text Processing, pages 486–497.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
<author>Dan Roth</author>
</authors>
<title>A preliminary evaluation of word representations for named-entity recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of NIPS Workshop on Grammar Induction, Representation of Language and Language Learning.</booktitle>
<contexts>
<context position="2537" citStr="Turian et al., 2009" startWordPosition="390" endWordPosition="393">n two facets of a restaurant. Therefore the more detailed version of the sentiment analysis is needed. Such a version is called the aspect-based sentiment analysis and it works on the level of the significant aspects of the target entity (Liu, 2012). The aspect-based sentiment analysis includes two main subtasks: the aspect term extraction and its polarity detection (Liu, 2012). In this article we describe the methods which address both subtasks. The methods are based on the distributed representations of words. Such word representations (or word embeddings) are useful in many NLP task, e.g. (Turian et al., 2009), (AlRfou’ et al., 2013), (Turney, 2013). The remainder of the article is as follows: section two gives the overview of the data; the third section shortly describes the distributed representations of words. The methods of the aspect term extraction and polarity detection are presented in the fourth and the fifth sections respectively. The conclusions are given in the sixth section. 2 The Data The organisers provided the train data for restaurant and laptop domains. But as it will be clear further our methods are heavily dependent on unlabelled text data. So we additionally collected the user </context>
</contexts>
<marker>Turian, Ratinov, Bengio, Roth, 2009</marker>
<rawString>Joseph Turian, Lev Ratinov, Yoshua Bengio and Dan Roth. 2009. A preliminary evaluation of word representations for named-entity recognition. In Proceedings of NIPS Workshop on Grammar Induction, Representation of Language and Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<date>2013</date>
<booktitle>Distributed Representations of Words and Phrases and their Compositionality. In Proceedings of NIPS,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="3851" citStr="Mikolov et al., 2013" startWordPosition="601" endWordPosition="604">tistics of the data are shown in Table 1. 140 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 140–144, Dublin, Ireland, August 23-24, 2014. Table 1: The amount of reviews. Domain The amount of reviews Restaurants 652 055 Laptops 109 550 For all the data we performed tokenization, stemming and morphological analysis using the FreeLing library (Padró and Stanilovsky, 2012). 3 Distributed Representations of Words In this section we’ll try to give the high level idea of the distributed representations of words. The more technical details can be found in (Mikolov et al., 2013). It is closely related with a new promising direction in machine learning called the deep learning. The core idea of the unsupervised deep learning algorithms is to find automatically the “good” set of features to represent the target object (text, image, audio signal, etc.). The object represented by the vector of real numbers is called the distributed representation (Rumelhart et al., 1986). We used the skip-gram model (Mikolov et al., 2013) implemented in Gensim toolkit (Řehůřek and Sojka, 2010). In general the learning procedure is as follows. All the texts of the corpus are stuck togethe</context>
<context position="5287" citStr="Mikolov et al., 2013" startWordPosition="837" endWordPosition="840"> to capture more language regularities but leads to more computational complexity of the learning. Each word from the lexicon is associated with the real numbers vector of the selected dimensionality. Originally all the vectors are randomly initialized. During the learning procedure the algorithm “slides” with the fixed size window (it’s algorithm parameter that was retained by default – 5 words) along the words of the sequence and calculates the probability (1) of context words appearance within the window based on its central word under review (or more precisely, its vector representation) (Mikolov et al., 2013). XwO |wI)  W  exp( v  w 1 vw where and are the input and output vector representations of w; and are the current and predicted words, – the number of words in vocabulary. The ultimate goal of the described process is to get such “good” vectors for each word, which allow to predict its probable context. All such vectors together form the vector space where semantically similar words are grouped. 4 Aspect Term Extraction Method We apply the same method for the aspect term extraction task (Pontiki et al., 2014) for both domains. The method consists of two steps: the candidate selection and t</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado and Jeffrey Dean. 2013. Distributed Representations of Words and Phrases and their Compositionality. In Proceedings of NIPS, pages 3111– 3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lluis Padró</author>
<author>Evgeny Stanilovsky</author>
</authors>
<title>FreeLing 3.0: Towards Wider Multilinguality.</title>
<date>2012</date>
<booktitle>In Proceedings of the Language Resources and Evaluation Conference, LREC 2012,</booktitle>
<pages>2473--2479</pages>
<contexts>
<context position="3646" citStr="Padró and Stanilovsky, 2012" startWordPosition="567" endWordPosition="570">will be clear further our methods are heavily dependent on unlabelled text data. So we additionally collected the user reviews about restaurants from tripadviser.com and about laptops from amazon.com. General statistics of the data are shown in Table 1. 140 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 140–144, Dublin, Ireland, August 23-24, 2014. Table 1: The amount of reviews. Domain The amount of reviews Restaurants 652 055 Laptops 109 550 For all the data we performed tokenization, stemming and morphological analysis using the FreeLing library (Padró and Stanilovsky, 2012). 3 Distributed Representations of Words In this section we’ll try to give the high level idea of the distributed representations of words. The more technical details can be found in (Mikolov et al., 2013). It is closely related with a new promising direction in machine learning called the deep learning. The core idea of the unsupervised deep learning algorithms is to find automatically the “good” set of features to represent the target object (text, image, audio signal, etc.). The object represented by the vector of real numbers is called the distributed representation (Rumelhart et al., 1986</context>
</contexts>
<marker>Padró, Stanilovsky, 2012</marker>
<rawString>Lluis Padró and Evgeny Stanilovsky. 2012. FreeLing 3.0: Towards Wider Multilinguality. In Proceedings of the Language Resources and Evaluation Conference, LREC 2012, pages 2473–2479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Determining the sentiment of opinions.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics, COLING-2004.</booktitle>
<contexts>
<context position="1300" citStr="Kim and Hovy, 2004" startWordPosition="191" endWordPosition="194">f our methods are shown in comparison with the baseline and the best result. 1 Introduction The sentiment analysis became an important Natural Language Processing (NLP) task in the recent few years. As many NLP tasks it’s a challenging one. The sentiment analysis can be very helpful for some practical applications. For example, it allows to study the users’ opinions about a product automatically. Many research has been devoted to the general sentiment analysis (Pang et al., 2002), (Amine et al., 2013), (Blinov et al., 2013) or analysis of individual sentences (Yu and Hatzivassiloglou, 2003), (Kim and Hovy, 2004), (Wiebe and Riloff, 2005). Soon it became clear that the sentiment analysis on the level of a whole text or even sentences is too coarse. GenThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ eral sentiment analysis by its design is not capable to perform the detailed analysis of an expressed opinion. For example, it cannot correctly detect the opinion in the sentence “Great food but the service was dreadful!”. The sentence carries op</context>
</contexts>
<marker>Kim, Hovy, 2004</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2004. Determining the sentiment of opinions. In Proceedings of the 20th International Conference on Computational Linguistics, COLING-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Pontiki</author>
</authors>
<title>Dimitrios Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos and Suresh Manandhar.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation, SemEval 2014,</booktitle>
<location>Dublin, Ireland.</location>
<marker>Pontiki, 2014</marker>
<rawString>Maria Pontiki, Dimitrios Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos and Suresh Manandhar. 2014. SemEval-2014 Task 4: Aspect Based Sentiment Analysis. In Proceedings of the 8th International Workshop on Semantic Evaluation, SemEval 2014, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Distributional semantics beyond words: Supervised learning of analogy and paraphrase.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>1--353</pages>
<contexts>
<context position="2577" citStr="Turney, 2013" startWordPosition="399" endWordPosition="400">e detailed version of the sentiment analysis is needed. Such a version is called the aspect-based sentiment analysis and it works on the level of the significant aspects of the target entity (Liu, 2012). The aspect-based sentiment analysis includes two main subtasks: the aspect term extraction and its polarity detection (Liu, 2012). In this article we describe the methods which address both subtasks. The methods are based on the distributed representations of words. Such word representations (or word embeddings) are useful in many NLP task, e.g. (Turian et al., 2009), (AlRfou’ et al., 2013), (Turney, 2013). The remainder of the article is as follows: section two gives the overview of the data; the third section shortly describes the distributed representations of words. The methods of the aspect term extraction and polarity detection are presented in the fourth and the fifth sections respectively. The conclusions are given in the sixth section. 2 The Data The organisers provided the train data for restaurant and laptop domains. But as it will be clear further our methods are heavily dependent on unlabelled text data. So we additionally collected the user reviews about restaurants from tripadvis</context>
</contexts>
<marker>Turney, 2013</marker>
<rawString>Peter Turney. 2013. Distributional semantics beyond words: Supervised learning of analogy and paraphrase. Transactions of the Association for Computational Linguistics, 1:353-366.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radim Řehůřek</author>
<author>Petr Sojka</author>
</authors>
<title>Software Framework for Topic Modelling with Large Corpora.</title>
<date>2010</date>
<booktitle>In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks,</booktitle>
<pages>46--50</pages>
<contexts>
<context position="4355" citStr="Řehůřek and Sojka, 2010" startWordPosition="684" endWordPosition="687">evel idea of the distributed representations of words. The more technical details can be found in (Mikolov et al., 2013). It is closely related with a new promising direction in machine learning called the deep learning. The core idea of the unsupervised deep learning algorithms is to find automatically the “good” set of features to represent the target object (text, image, audio signal, etc.). The object represented by the vector of real numbers is called the distributed representation (Rumelhart et al., 1986). We used the skip-gram model (Mikolov et al., 2013) implemented in Gensim toolkit (Řehůřek and Sojka, 2010). In general the learning procedure is as follows. All the texts of the corpus are stuck together in a single sequence of sentences. On the basis of the corpus the lexicon is constructed. Next, the dimensionality of the vectors is chosen (we used 300 in our experiments). The greater number of dimensions allows to capture more language regularities but leads to more computational complexity of the learning. Each word from the lexicon is associated with the real numbers vector of the selected dimensionality. Originally all the vectors are randomly initialized. During the learning procedure the a</context>
</contexts>
<marker>Řehůřek, Sojka, 2010</marker>
<rawString>Radim Řehůřek and Petr Sojka. 2010. Software Framework for Topic Modelling with Large Corpora. In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 46–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rami Al-Rfou’</author>
<author>Bryan Perozzi</author>
<author>Steven Skiena</author>
</authors>
<title>Polyglot: Distributed Word Representations for Multilingual NLP.</title>
<date>2013</date>
<booktitle>In Proceedings of Conference on Computational Natural Language Learning, CoNLL’2013.</booktitle>
<marker>Al-Rfou’, Perozzi, Skiena, 2013</marker>
<rawString>Rami Al-Rfou’, Bryan Perozzi, Steven Skiena. 2013. Polyglot: Distributed Word Representations for Multilingual NLP. In Proceedings of Conference on Computational Natural Language Learning, CoNLL’2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Rumelhart</author>
<author>Geoffrey Hintont</author>
<author>Ronald Williams</author>
</authors>
<title>Learning representations by backpropagating errors.</title>
<date>1986</date>
<journal>Nature.</journal>
<contexts>
<context position="4247" citStr="Rumelhart et al., 1986" startWordPosition="666" endWordPosition="670">and Stanilovsky, 2012). 3 Distributed Representations of Words In this section we’ll try to give the high level idea of the distributed representations of words. The more technical details can be found in (Mikolov et al., 2013). It is closely related with a new promising direction in machine learning called the deep learning. The core idea of the unsupervised deep learning algorithms is to find automatically the “good” set of features to represent the target object (text, image, audio signal, etc.). The object represented by the vector of real numbers is called the distributed representation (Rumelhart et al., 1986). We used the skip-gram model (Mikolov et al., 2013) implemented in Gensim toolkit (Řehůřek and Sojka, 2010). In general the learning procedure is as follows. All the texts of the corpus are stuck together in a single sequence of sentences. On the basis of the corpus the lexicon is constructed. Next, the dimensionality of the vectors is chosen (we used 300 in our experiments). The greater number of dimensions allows to capture more language regularities but leads to more computational complexity of the learning. Each word from the lexicon is associated with the real numbers vector of the selec</context>
</contexts>
<marker>Rumelhart, Hintont, Williams, 1986</marker>
<rawString>David Rumelhart, Geoffrey Hintont, Ronald Williams. 1986. Learning representations by backpropagating errors. Nature.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong Yu</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>129--136</pages>
<contexts>
<context position="1278" citStr="Yu and Hatzivassiloglou, 2003" startWordPosition="186" endWordPosition="190"> briefly described. The results of our methods are shown in comparison with the baseline and the best result. 1 Introduction The sentiment analysis became an important Natural Language Processing (NLP) task in the recent few years. As many NLP tasks it’s a challenging one. The sentiment analysis can be very helpful for some practical applications. For example, it allows to study the users’ opinions about a product automatically. Many research has been devoted to the general sentiment analysis (Pang et al., 2002), (Amine et al., 2013), (Blinov et al., 2013) or analysis of individual sentences (Yu and Hatzivassiloglou, 2003), (Kim and Hovy, 2004), (Wiebe and Riloff, 2005). Soon it became clear that the sentiment analysis on the level of a whole text or even sentences is too coarse. GenThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ eral sentiment analysis by its design is not capable to perform the detailed analysis of an expressed opinion. For example, it cannot correctly detect the opinion in the sentence “Great food but the service was dreadful!”. T</context>
</contexts>
<marker>Yu, Hatzivassiloglou, 2003</marker>
<rawString>Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, pages 129–136.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>