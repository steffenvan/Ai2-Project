<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003141">
<title confidence="0.886828">
CMU System Combination for WMT’09
</title>
<author confidence="0.983772">
Almut Silja Hildebrand Stephan Vogel
</author>
<affiliation confidence="0.9974485">
Carnegie Mellon University Carnegie Mellon University
Pittsburgh, USA Pittsburgh, USA
</affiliation>
<email confidence="0.99659">
silja@cs.cmu.edu vogel@cs.cmu.edu
</email>
<sectionHeader confidence="0.994739" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996626">
This paper describes the CMU entry for
the system combination shared task at
WMT’09. Our combination method is hy-
pothesis selection, which uses information
from n-best lists from several MT systems.
The sentence level features are indepen-
dent from the MT systems involved. To
compensate for various n-best list sizes in
the workshop shared task including first-
best-only entries, we normalize one of our
high-impact features for varying sub-list
size. We combined restricted data track
entries in French - English, German - En-
glish and Hungarian - English using pro-
vided data only.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999452">
For the combination of machine translation sys-
tems there have been two main approaches de-
scribed in recent publications. One uses confusion
network decoding to combine translation systems
as described in (Rosti et al., 2008) and (Karakos et
al., 2008). The other approach selects whole hy-
potheses from a combined n-best list (Hildebrand
and Vogel, 2008).
Our setup follows the approach described in
(Hildebrand and Vogel, 2008). We combine the
output from the available translation systems into
one joint n-best list, then calculate a set of fea-
tures consistently for all hypotheses. We use MER
training on a development set to determine feature
weights and re-rank the joint n-best list.
</bodyText>
<sectionHeader confidence="0.999122" genericHeader="introduction">
2 Features
</sectionHeader>
<bodyText confidence="0.9991195">
For our entries to the WMT’09 we used the fol-
lowing feature groups:
</bodyText>
<listItem confidence="0.9999504">
• Language model score
• Word lexicon scores
• Sentence length features
• Rank feature
• Normalized n-gram agreement
</listItem>
<bodyText confidence="0.9999256">
The details on language model and word lexi-
con scores can be found in (Hildebrand and Vogel,
2008). We use two sentence length features, which
are the ratio of the hypothesis length to the length
of the source sentence and the difference between
the hypothesis length and the average length of
the hypotheses in the n-best list for the respec-
tive source sentence. We also use the rank of the
hypothesis in the original system’s n-best list as a
feature.
</bodyText>
<subsectionHeader confidence="0.960661">
2.1 Normalized N-gram Agreement
</subsectionHeader>
<bodyText confidence="0.9462394">
The participants of the WMT’09 shared transla-
tion task provided output from their translation
systems in various sizes. Most submission were
1st-best translation only, some submitted 10-best
up to 300-best lists.
In preliminary experiments we saw that adding
a high scoring 1st-best translation to a joint n-best
list composed of several larger n-best lists does not
yield the desired improvement. This might be due
to the fact, that hypotheses within an n-best list
originating from one single system (sub-list) tend
to be much more similar to each other than to hy-
potheses from another system. This leads to hy-
potheses from larger sub-lists scoring higher in the
n-best list based features, e.g. because they collect
more n-gram matches within their sub-list, which
”supports” them the more the larger it is.
Previous experiments on Chinese-English
showed, that the two feature groups with the
highest impact on the combination result are the
language model and the n-best list based n-gram
agreement. Therefore we decided to focus on the
n-best list n-gram agreement for exploring sub-list
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 47–50,
Athens, Greece, 30 March – 31 March 2009. c�2009 Association for Computational Linguistics
</bodyText>
<page confidence="0.999307">
47
</page>
<bodyText confidence="0.999534461538462">
size normalization to adapt to the data situation
with various n-best list sizes.
The n-gram agreement score of each n-gram in
the target sentence is the relative frequency of tar-
get sentences in the n-best list for one source sen-
tence that contain the n-gram e, independent of
the position of the n-gram in the sentence. This
feature represents the percentage of the transla-
tion hypotheses, which contain the respective n-
gram. If a hypothesis contains an n-gram more
than once, it is only counted once, hence the max-
imum for the agreement score a(e) is 1.0 (100%).
The agreement score a(e) for each n-gram e is:
</bodyText>
<equation confidence="0.996480666666667">
C
a(e) = (1)
L
</equation>
<bodyText confidence="0.999325538461539">
where C is the count of the hypotheses containing
the n-gram and L is the size of the n-best list for
this source sentence.
To compensate for the various n-best list sizes
provided to us we modified the n-best list n-gram
agreement by normalizing the count of hypotheses
that contain the n-gram by the size of the sub-list
it came from. It can be viewed as either collecting
fractional counts for each n-gram match, or as cal-
culating the n-gram agreement percentage for each
sub-list and then interpolating them. The normal-
ized n-gram agreement score anorm(e) for each n-
gram e is:
</bodyText>
<equation confidence="0.97462525">
XP
1
anorm(e) = P
j=1
</equation>
<bodyText confidence="0.9998284">
where P is the number of systems, Cj is the count
of the hypotheses containing the n-gram e in the
sublist pj and Lj is the size of the sublist pj.
For the extreme case of a sub-list size of one
the fact of finding an n-gram in that hypothesis
or not has a rather strong impact on the normal-
ized agreement score. Therefore we introduce a
smoothing factor A in a way that it has an increas-
ing influence the smaller the sub-list is:
used an initial value of A = 0.1 for our experi-
ments.
In all three cases the score for the whole hypoth-
esis is the sum over the word scores normalized
by the sentence length. We use n-gram lengths
n = 1..6 as six separate features.
</bodyText>
<sectionHeader confidence="0.8303365" genericHeader="method">
3 Preliminary Experiments
Arabic-English
</sectionHeader>
<bodyText confidence="0.999453111111111">
For the development of the modification on the n-
best list n-gram agreement feature we used n-best
lists from three large scale Arabic to English trans-
lation systems. We evaluate using the case insen-
sitive BLEU score for the MT08 test set with four
references, which was unseen data for the individ-
ual systems as well as the system combination. Ta-
ble 1 shows the initial scores of the three input sys-
tems.
</bodyText>
<table confidence="0.77828125">
system MT08
A 47.47
B 46.33
C 44.42
</table>
<tableCaption confidence="0.993897">
Table 1: Arabic-English Baselines: BLEU
</tableCaption>
<bodyText confidence="0.9999334">
To compare the behavior of the combination
result for different n-best list sizes we combined
the 100-best lists from systems A and C and then
added three n-best list sizes from the middle sys-
tem B into the combination: 1-best, 10-best and
full 100-best. For each of these four combination
options we ran the hypothesis selection using the
plain version of the n-gram agreement feature a as
well as the normalized version without anorm and
with smoothing asmooth .
</bodyText>
<table confidence="0.999113714285714">
combination a anorm asmooth
A &amp; C 48.04 48.09 48.13
A &amp; C &amp; B1 47.84 48.34 48.21
A &amp; C &amp; B10 48.29 48.33 48.47
A &amp; C &amp; B100 48.91 48.95 49.02
Cj (2)
Lj
</table>
<tableCaption confidence="0.999512">
Table 2: Combination results: BLEU on MT08
</tableCaption>
<equation confidence="0.962597833333333">
J
asmooth(e) = 1ILjCj A
P Lj
j=1
+rLj − Cj Al
L Lj Lj J
</equation>
<bodyText confidence="0.999757454545454">
where P is the number of systems, Cj is the count
of the hypotheses containing the n-gram in the
sublist pj and Lj is the size of the sublist pj. We
The modified feature has as expected no impact
on the combination of n-best lists of the same size
(see Table 2), however it shows an improvement
of BLEU +0.5 for the combination with the 1st-
best from system B. The smoothing seems to have
no significant impact for this dataset, but differ-
ent smoothing factors will be investigated in the
future.
</bodyText>
<figure confidence="0.463601">
(3)
48
4 Workshop Results BLEU on unseen data compared to the one with-
out normalization.
</figure>
<bodyText confidence="0.999917366666667">
To train our language models and word lexica
we only used provided data. Therefore we ex-
cluded systems from the combination, which were
to our knowledge using unrestricted training data
(google). We did not include any contrastive sys-
tems.
We trained the statistical word lexica on the par-
allel data provided for each language pair1. For
each combination we used two language models,
a 1.2 giga-word 3-gram language model, trained
on the provided monolingual English data and a 4-
gram language model trained on the English part
of the parallel training data of the respective lan-
guages. We used the SRILM toolkit (Stolcke,
2002) for training.
For each of the three language pairs we submit-
ted a combination that used the plain version of the
n-gram agreement feature as well as one using the
normalized smoothed version.
The provided system combination development
set, which we used for tuning our feature weights,
was the same for all language pairs, 502 sentences
with only one reference.
For combination we tokenized and lowercased
all data, because the n-best lists were submitted
in various formats. Therefore we report the case
insensitive scores here. The combination was op-
timized toward the BLEU metric, therefore results
for TER and METEOR are not very meaningful
here and only reported for completeness.
</bodyText>
<subsectionHeader confidence="0.99831">
4.1 French-English
</subsectionHeader>
<bodyText confidence="0.999979466666667">
14 systems were submitted to the restricted data
track for the French-English translation task. The
scores on the combination development set range
from BLEU 27.56 to 15.09 (case insensitive eval-
uation).
We received n-best lists from five systems, a
300-best, a 200-best two 100-best and one 10-best
list. We included up to 100 hypotheses per system
in our joint n-best list.
For our workshop submission we combined the
top nine systems with the last system scoring
24.23 as well as all 14 systems. Comparing the
results for the two combinations of all 14 systems
(see Table 3), the one with the sub-list normaliza-
tion for the n-gram agreement feature gains +0.8
</bodyText>
<footnote confidence="0.8271545">
1http://www.statmt.org/wmt09/translation-
task.html#training
</footnote>
<table confidence="0.997072">
system dev test TER Meteor
best single 27.56 26.88 56.32 52.68
top 9 awnooth 29.85 28.07 55.23 53.90
all 14 awnooth 30.39 28.46 55.12 54.35
all 14 29.49 27.65 55.41 53.74
</table>
<tableCaption confidence="0.99978">
Table 3: French-English Results: BLEU
</tableCaption>
<bodyText confidence="0.94086925">
Our system combination via hypothesis selec-
tion could improve the translation quality by +1.6
BLEU on the unseen test set compared to the best
single system.
</bodyText>
<figure confidence="0.998003">
100%
90%
80%
70%
I* 264
H 41
G 110
F* 423
E* 584
D* 562
C 104
B* 434
A 177
10%
0%
</figure>
<figureCaption confidence="0.885693333333333">
Figure 1: Contributions of the individual systems
to the final translation.
Figure 1 shows, how many hypotheses were
</figureCaption>
<bodyText confidence="0.916568428571429">
contributed by the individual systems to the fi-
nal translation (unseen data). The systems A to
N are ordered by their BLEU score on the devel-
opment set. The systems which provided n-best
lists, marked with a star in the diagram, clearly
dominate the selection. The low scoring systems
contribute very little as expected.
</bodyText>
<subsectionHeader confidence="0.998683">
4.2 German-English
</subsectionHeader>
<bodyText confidence="0.994066">
14 systems were submitted to the restricted data
track for the German-English translation task. The
scores on the combination development set range
</bodyText>
<figure confidence="0.9989171">
60%
50%
40%
30%
20%
43N 7
M 18
L 16
K 12
J 10
</figure>
<page confidence="0.998042">
49
</page>
<bodyText confidence="0.999984785714286">
from BLEU 27.56 to 7 (case insensitive evalua-
tion). The two lowest scoring systems at BLEU
11 and 7 were so far from the rest of the systems
that we decided to exclude them, assuming an er-
ror had occurred.
Within the remaining 12 submissions were four
n-best lists, three 100-best and one 10-best.
For our submissions we combined the top seven
systems between BLEU 22.91 and 20.24 as well as
the top 12 systems where the last one of those was
scoring BLEU 16.00 on the development set. For
this language pair the combination with the nor-
malized n-gram agreement also outperforms the
one without by +0.8 BLEU (see Table 4).
</bodyText>
<table confidence="0.9836452">
system dev test TER Meteor
best single 22.91 21.03 61.87 47.96
top 7 asmooth 25.13 22.86 60.73 49.71
top 12 asmooth 25.32 22.98 60.72 50.01
top 12 25.12 22.20 60.95 49.33
</table>
<tableCaption confidence="0.998833">
Table 4: German-English Results: BLEU
</tableCaption>
<bodyText confidence="0.998427">
Our system combination via hypothesis selec-
tion could improve translation quality by +1.95
BLEU on the unseen test set over the best single
system.
</bodyText>
<subsectionHeader confidence="0.999286">
4.3 Hungarian-English
</subsectionHeader>
<bodyText confidence="0.996318833333333">
Only three systems were submitted for the
Hungarian-English translation task. Scores on the
combination development set ranged from BLEU
13.63 to 10.04 (case insensitive evaluation). Only
the top system provided an n-best list. We used
100-best hypotheses.
</bodyText>
<table confidence="0.99381675">
system dev test TER Meteor
best single 13.63 12.73 68.75 36.76
3 sys asmooth 14.98 13.74 72.34 38.20
3 sys 14.14 13.18 74.29 37.52
</table>
<tableCaption confidence="0.99944">
Table 5: Hungarian-English Results: BLEU
</tableCaption>
<bodyText confidence="0.999956">
We submitted combinations of the three systems
by using the modified smoothed n-gram agree-
ment feature and the plain version of the n-gram
agreement feature. Here also the normalized ver-
sion of the feature gives an improvement of +0.56
BLEU with an overall improvement of +1.0 BLEU
over the best single system (see Table 5).
</bodyText>
<sectionHeader confidence="0.998024" genericHeader="conclusions">
5 Summary
</sectionHeader>
<bodyText confidence="0.999989666666667">
It is beneficial to include more systems, even if
they are more than 7 points BLEU behind the best
system, as the comparison to the combinations
with fewer systems shows.
In the mixed size data situation of the workshop
the modified feature shows a clear improvement
for all three language pairs. Different smoothing
factors should be investigated for these data sets
in the future.
</bodyText>
<sectionHeader confidence="0.998167" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999938333333333">
We would like to thank the participants in the
WMT’09 workshop shared translation task for
providing their data, especially n-best lists.
</bodyText>
<sectionHeader confidence="0.999199" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999679923076923">
Almut Silja Hildebrand and Stephan Vogel. 2008.
Combination of machine translation systems via hy-
pothesis selection from combined n-best lists. In
MT at work: Proceedings of the Eighth Confer-
ence of the Association for Machine Translation in
the Americas, pages 254–261, Waikiki, Hawaii, Oc-
tober. Association for Machine Translation in the
Americas.
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,
and Markus Dreyer. 2008. Machine translation
system combination using itg-based alignments. In
Proceedings of ACL-08: HLT, Short Papers, pages
81–84, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2008. Incremental hy-
pothesis alignment for building confusion networks
with application to machine translation system com-
bination. In Proceedings of the Third Workshop
on Statistical Machine Translation, pages 183–186,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Andreas Stolcke. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings Interna-
tional Conference for Spoken Language Processing,
Denver, Colorado, September.
</reference>
<page confidence="0.997689">
50
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.761549">
<title confidence="0.821385">CMU System Combination for WMT’09</title>
<author confidence="0.999322">Almut Silja Hildebrand Stephan Vogel</author>
<affiliation confidence="0.99999">Carnegie Mellon University Carnegie Mellon University</affiliation>
<address confidence="0.986238">Pittsburgh, USA Pittsburgh, USA</address>
<email confidence="0.999519">silja@cs.cmu.eduvogel@cs.cmu.edu</email>
<abstract confidence="0.9962265">This paper describes the CMU entry for the system combination shared task at WMT’09. Our combination method is hypothesis selection, which uses information from n-best lists from several MT systems. The sentence level features are independent from the MT systems involved. To compensate for various n-best list sizes in the workshop shared task including firstbest-only entries, we normalize one of our high-impact features for varying sub-list size. We combined restricted data track entries in French - English, German - English and Hungarian - English using provided data only.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Almut Silja Hildebrand</author>
<author>Stephan Vogel</author>
</authors>
<title>Combination of machine translation systems via hypothesis selection from combined n-best lists.</title>
<date>2008</date>
<booktitle>In MT at work: Proceedings of the Eighth Conference of the Association for Machine Translation in the Americas,</booktitle>
<pages>254--261</pages>
<location>Waikiki, Hawaii,</location>
<contexts>
<context position="1149" citStr="Hildebrand and Vogel, 2008" startWordPosition="170" endWordPosition="173">e workshop shared task including firstbest-only entries, we normalize one of our high-impact features for varying sub-list size. We combined restricted data track entries in French - English, German - English and Hungarian - English using provided data only. 1 Introduction For the combination of machine translation systems there have been two main approaches described in recent publications. One uses confusion network decoding to combine translation systems as described in (Rosti et al., 2008) and (Karakos et al., 2008). The other approach selects whole hypotheses from a combined n-best list (Hildebrand and Vogel, 2008). Our setup follows the approach described in (Hildebrand and Vogel, 2008). We combine the output from the available translation systems into one joint n-best list, then calculate a set of features consistently for all hypotheses. We use MER training on a development set to determine feature weights and re-rank the joint n-best list. 2 Features For our entries to the WMT’09 we used the following feature groups: • Language model score • Word lexicon scores • Sentence length features • Rank feature • Normalized n-gram agreement The details on language model and word lexicon scores can be found i</context>
</contexts>
<marker>Hildebrand, Vogel, 2008</marker>
<rawString>Almut Silja Hildebrand and Stephan Vogel. 2008. Combination of machine translation systems via hypothesis selection from combined n-best lists. In MT at work: Proceedings of the Eighth Conference of the Association for Machine Translation in the Americas, pages 254–261, Waikiki, Hawaii, October. Association for Machine Translation in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Damianos Karakos</author>
<author>Jason Eisner</author>
<author>Sanjeev Khudanpur</author>
<author>Markus Dreyer</author>
</authors>
<title>Machine translation system combination using itg-based alignments.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT, Short Papers,</booktitle>
<pages>81--84</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="1047" citStr="Karakos et al., 2008" startWordPosition="154" endWordPosition="157"> are independent from the MT systems involved. To compensate for various n-best list sizes in the workshop shared task including firstbest-only entries, we normalize one of our high-impact features for varying sub-list size. We combined restricted data track entries in French - English, German - English and Hungarian - English using provided data only. 1 Introduction For the combination of machine translation systems there have been two main approaches described in recent publications. One uses confusion network decoding to combine translation systems as described in (Rosti et al., 2008) and (Karakos et al., 2008). The other approach selects whole hypotheses from a combined n-best list (Hildebrand and Vogel, 2008). Our setup follows the approach described in (Hildebrand and Vogel, 2008). We combine the output from the available translation systems into one joint n-best list, then calculate a set of features consistently for all hypotheses. We use MER training on a development set to determine feature weights and re-rank the joint n-best list. 2 Features For our entries to the WMT’09 we used the following feature groups: • Language model score • Word lexicon scores • Sentence length features • Rank feat</context>
</contexts>
<marker>Karakos, Eisner, Khudanpur, Dreyer, 2008</marker>
<rawString>Damianos Karakos, Jason Eisner, Sanjeev Khudanpur, and Markus Dreyer. 2008. Machine translation system combination using itg-based alignments. In Proceedings of ACL-08: HLT, Short Papers, pages 81–84, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko Rosti</author>
<author>Bing Zhang</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
</authors>
<title>Incremental hypothesis alignment for building confusion networks with application to machine translation system combination.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>183--186</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="1020" citStr="Rosti et al., 2008" startWordPosition="149" endWordPosition="152">e sentence level features are independent from the MT systems involved. To compensate for various n-best list sizes in the workshop shared task including firstbest-only entries, we normalize one of our high-impact features for varying sub-list size. We combined restricted data track entries in French - English, German - English and Hungarian - English using provided data only. 1 Introduction For the combination of machine translation systems there have been two main approaches described in recent publications. One uses confusion network decoding to combine translation systems as described in (Rosti et al., 2008) and (Karakos et al., 2008). The other approach selects whole hypotheses from a combined n-best list (Hildebrand and Vogel, 2008). Our setup follows the approach described in (Hildebrand and Vogel, 2008). We combine the output from the available translation systems into one joint n-best list, then calculate a set of features consistently for all hypotheses. We use MER training on a development set to determine feature weights and re-rank the joint n-best list. 2 Features For our entries to the WMT’09 we used the following feature groups: • Language model score • Word lexicon scores • Sentence </context>
</contexts>
<marker>Rosti, Zhang, Matsoukas, Schwartz, 2008</marker>
<rawString>Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas, and Richard Schwartz. 2008. Incremental hypothesis alignment for building confusion networks with application to machine translation system combination. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 183–186, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings International Conference for Spoken Language Processing,</booktitle>
<location>Denver, Colorado,</location>
<contexts>
<context position="7785" citStr="Stolcke, 2002" startWordPosition="1335" endWordPosition="1336">nguage models and word lexica we only used provided data. Therefore we excluded systems from the combination, which were to our knowledge using unrestricted training data (google). We did not include any contrastive systems. We trained the statistical word lexica on the parallel data provided for each language pair1. For each combination we used two language models, a 1.2 giga-word 3-gram language model, trained on the provided monolingual English data and a 4- gram language model trained on the English part of the parallel training data of the respective languages. We used the SRILM toolkit (Stolcke, 2002) for training. For each of the three language pairs we submitted a combination that used the plain version of the n-gram agreement feature as well as one using the normalized smoothed version. The provided system combination development set, which we used for tuning our feature weights, was the same for all language pairs, 502 sentences with only one reference. For combination we tokenized and lowercased all data, because the n-best lists were submitted in various formats. Therefore we report the case insensitive scores here. The combination was optimized toward the BLEU metric, therefore resu</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm - an extensible language modeling toolkit. In Proceedings International Conference for Spoken Language Processing, Denver, Colorado, September.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>