<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.9099696">
A DP based Search Algorithm for Statistical Machine Translation
S. Nieflen, S. Vogel, H. Ney, and C. Tillmann
Lehrstuhl fiir Informatik VI
RINTH Aachen - University of Technology
D-52056 Aachen, Germany
</note>
<email confidence="0.786859">
Email: niessen@inf ormatik.rwth—aachen.de
</email>
<sectionHeader confidence="0.979542" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999987">
We introduce a novel search algorithm for statisti-
cal machine translation based on dynamic program-
ming (DP). During the search process two statis-
tical knowledge sources are combined: a translation
model and a bigram language model. This search al-
gorithm expands hypotheses along the positions of
the target string while guaranteeing progressive cov-
erage of the words in the source string. We present
experimental results on the Verbmobil task.
</bodyText>
<sectionHeader confidence="0.993789" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999960307692308">
In this paper, we address the problem of finding the
most probable target language representation of a
given source language string. In our approach, we
use a DP based search algorithm which sequentially
visits the target string positions while progressively
considering the source string words.
The organization of the paper is as follows. Af-
ter reviewing the statistical approach to machine
translation, we first describe the statistical know-
ledge sources used during the search process. We
then present our DP based search algorithm in de-
tail. Finally, experimental results for a bilingual cor-
pus are reported.
</bodyText>
<subsectionHeader confidence="0.989402">
1.1 Statistical Machine Translation
</subsectionHeader>
<bodyText confidence="0.998657833333333">
In statistical machine translation, the goal of the
search strategy can be formulated as follows: We
are given a source language (Trench&apos;) string =
which is to be translated into a target lan-
guage (`English&apos;) string ei = el el with the un-
known length I. Every English string is considered
as a possible translation for the input string. If we
assign a probability Pr(effg) to each pair of strings
(ef, , fi/), then we have to choose the length /opt and
the English string 6/1&amp;quot;` that maximize Pr(ef If!) for
a given French string AI. According to Bayes deci-
sion rule, /opt and 6/1&amp;quot;e can be found by
</bodyText>
<equation confidence="0.77022925">
(Lpt, = argmax {Pr(efIfil)}
1,e
argmax {Pr(ef)•Pr(fillef)}. (1)
1,e(
</equation>
<bodyText confidence="0.989890444444445">
Pr(ef) is the English language model, whereas
Pr(fil lef ) is the string translation model.
The overall architecture of the statistical transla-
tion approach is summarized in Fig. 1. In this figure,
we already anticipate the fact that we will transform
the source strings in a certain manner and that we
will countermand these transformations on the pro-
duced output strings. This aspect is explained in
more detail in Section 3.
</bodyText>
<subsectionHeader confidence="0.776544">
Source Language Text
</subsectionHeader>
<figure confidence="0.929998153846154">
Pr(r: lel
Transformation
Lexicon Model
Global Search:
maximize Pr( el ) • Pr(f lel )
over el
Pr(el)
Alignment Model
Language Model
4.
Transformation
0
Target Language Text
</figure>
<figureCaption confidence="0.874234">
Figure 1: Architecture of the translation approach
based on Bayes&apos; decision rule.
</figureCaption>
<bodyText confidence="0.989977888888889">
The task of statistical machine translation can be
subdivided into two fields:
I. the field of modelling, which introduces struc-
tures into the probabilistic dependencies and
provides methods for estimating the parameters
of the models from bilingual corpora;
2. the field of decoding, i.e. finding a search algo-
rithm, which performs the argmax operation in
Eq. (1) as efficient as possible.
</bodyText>
<page confidence="0.988738">
960
</page>
<subsectionHeader confidence="0.518207">
L2 Alignment with Mixture Distribution
</subsectionHeader>
<bodyText confidence="0.9998004">
Several papers have discussed the first issue, espe-
cially the problem of word alignments for bilingual
corpora (Brown et al., 1993), (Dagan et al., 1993),
(Kay and ROscheisen, 1993), (Fung and Church,
1994), (Vogel et al., 1996).
In our search procedure, we use a mixture-based
alignment model that slightly differs from the model
introduced as Model 2 in (Brown et al., 1993). It is
based on a decomposition of the joint probability for
fil into a product of the probabilities for each word
</bodyText>
<equation confidence="0.910369">
Pr(g lef) = P(JII) 11 P(f.; I ) , (2)
3=1
</equation>
<bodyText confidence="0.9997245">
where the lengths of the strings are regarded as
random variables and modelled by the distribution
p(JII). Now we assume a sort of pairwise interac-
tion between the French word fl and each English
word e, in ef. . These dependencies are captured in
the form of a mixture distribution:
</bodyText>
<equation confidence="0.9451715">
p(fi ief) = E /*Li, /) • p(fi lei) . (3)
i=1
Inserting this into (2), we get
J I
Pr( f = p(J11) flE p(iIj, J, I) p( filet) (4)
3=12=1
</equation>
<bodyText confidence="0.999889785714286">
with the following components: the sentence length
probability p(JII), the mixture alignment probabil-
ity p(iIj, J, I) and the translation probability p(f
So far, the model allows all English words in the
target string to contribute to the translation of a
French word. This is expressed by the sum over i
in Eq. (4). It is reasonable to assume that for each
source string position j one position i in the target
string dominates this sum. This conforms with the
experience, that in most cases a clear word-to-word
correspondence between a string and its translation
exists. As a consequence, we use the so-called max-
imum approximation: At each point, only the best
choice of i is considered for the alignment path:
</bodyText>
<equation confidence="0.84479">
Pr (fif lef)= P(.111&amp;quot;)11 max I)-P(filei)- (5)
jr.eizEtim
</equation>
<bodyText confidence="0.975591">
We can now formulate the criterion to be maximized
by a search algorithm:
</bodyText>
<equation confidence="0.950072333333333">
max [p(JII) max {Pr(ef)-
rj max [P(ili, J,i) • P(f; lei)] I] •
E. i ti,/1
</equation>
<page confidence="0.903061">
3=1
</page>
<bodyText confidence="0.999284">
Because of the problem of data sparseness, we use
a parametric model for the alignment probabilities.
It assumes that the distance of the positions relative
to the diagonal of the (j, i) plane is the dominating
factor:
</bodyText>
<equation confidence="0.999431">
r(i — j
P(ili, .1,1) =
(7)
ar=1 r(i&apos; i7)
</equation>
<bodyText confidence="0.999656333333333">
As described in (Brown et al., 1993), the EM al-
gorithm can be used to estimate the parameters of
the model.
</bodyText>
<subsectionHeader confidence="0.996559">
1.3 Search in Statistical Machine
Translation
</subsectionHeader>
<bodyText confidence="0.997842909090909">
In the last few years, there has been a number of
papers considering the problem of finding an effi-
cient search procedure (Wu, 1996), (Tillmann et al.,
1997a), (Tillmann et al., 1997b), (Wang and Waibel,
1997). All of these approaches use a bigram language
model, because they are quite simple and easy-to-
use and they have proven their prediction power
in stochastic language processing, especially speech
recognition. Assuming a bigram language model, we
would like to re-formulate Eq. (6) in the following
way:
</bodyText>
<equation confidence="0.991554">
max [p(JII) max { H max [p(ei lei--1)*
el j iE[1,1]
=1
P(il.i, i) • P(fi lei)] }]
</equation>
<bodyText confidence="0.9998325">
Any search algorithm tending to perform the max-
imum operations in Eq. (8) has to guarantee, that
the predecessor word ei_1 can be determined at the
time when a certain word ei at position i in the tar-
get string is under consideration. Different solutions
to this problem have been studied.
(Tillmann et al., 1997b) and (Tillmann et al.,
1997a) propose a search procedure based on dynamic
programming, that examines the source string se-
quentially. Although it is very efficient in terms
of translation speed, it suffers from the drawback
of being dependent on the so-called monotonicity
constraint: The alignment paths are assumed to
be monotone. Hence, the word at position i — 1
in the target sentence can be determined when the
algorithm produces ei. This approximation corre-
sponds to the assumption of the fundamental simi-
larity of the sentence structures in both languages.
In (Tillmann et al., 1997b) text transformations in
the source language are used to adapt the word or-
dering in the source strings to the target language
grammar.
(Wang and Waibel, 1997) describe an algorithm
based on A&amp;quot;-search. Here, hypotheses are extended
</bodyText>
<page confidence="0.8474365">
(6)
961
</page>
<bodyText confidence="0.99997875">
by adding a word to the end of the target string
while considering the source string words in any or-
der. The underlying translation model is Model 2
from (Brown et al., 1993).
(Wu, 1996) formulates a DP search for stochastic
bracketing transduction grammars. The bigram lan-
guage model is integrated into the algorithm at the
point, where two partial parse trees are combined.
</bodyText>
<sectionHeader confidence="0.998654" genericHeader="method">
2 DP Search
</sectionHeader>
<subsectionHeader confidence="0.998554">
2.1 The Inverted Alignment Model
</subsectionHeader>
<bodyText confidence="0.999947186046511">
For our search method, we chose an algorithm which
is based on dynamic programming. Compared to an
A*-based algorithm dynamic programming has the
fundamental advantage, that solutions of subprob-
lems are stored and can then be re-used in later
stages of the search process. However, for the op-
timization criterion considered here dynamic pro-
gramming is only suboptimal because the decompo-
sition into independent subproblems is only approx-
imately possible: to prevent the search time of a
search algorithm from increasing exponentially with
the string lengths and vocabulary sizes, local deci-
sions have to be made at an earlier stage of the opti-
mization process that might turn out to be subopti-
mal in a later stage but cannot be altered then. As
a consequence, the global optimum might be missed
in some cases.
The search algorithm we present here combines
the advantages of dynamic programming with the
search organization along the positions of the target
string, which allows the integration of the bigram in
a very natural way without restricting the alignment
paths to the class of monotone alignments.
The alignment model as described above is defined
as a function that assigns exactly one target word to
each source word. We introduce a new interpretation
of the alignment model: Each position i in ef is
assigned a position bi = j in fj1. Fig. 2 illustrates
the possible transitions in this inverted model.
At each position i of ef, , each word of the target
language vocabulary can be inserted. In addition,
the fertility I must be chosen: A position i and the
word e, at this position are considered to correspond
to a sequence of words in ff. In most cases,
the optimal fertility is 1. It is also possible, that a
word ei has fertility 0, which means that there is no
directly corresponding word in the source string. We
call this a skip, because the position i is skipped in
the alignment path.
Using a bigram language model, Eq. (9) specifies
the modified search criterion for our algorithm. Here
as above, we assume the maximum approximation to
be valid.
</bodyText>
<equation confidence="0.859129333333333">
%• • • •• •
. . .
. . • •
. . . .
•
cf&apos;• • • W`tf • • •
</equation>
<figureCaption confidence="0.9423755">
position in source string
Figure 2: Transitions in the inverted model.
</figureCaption>
<bodyText confidence="0.360025">
max [P(Jin ma-x [p(eilei_i).
</bodyText>
<equation confidence="0.98830125">
ei
1 1=1
max II /) • P(.61ei)} (9)
_
</equation>
<bodyText confidence="0.9998582">
For better legibility, we regard the second product
in Eq. (9) to be equal to 1, if 1 = 0. It should be
stressed that the pair (I, ei) optimizing Eq. (9) is
not guaranteed to be also optimal in terms of the
original criterion (6).
</bodyText>
<subsectionHeader confidence="0.999247">
2.2 Basic Problem: Position Coverage
</subsectionHeader>
<bodyText confidence="0.9999949375">
A closer look at Eq. (9) reveals the most important
problem of the search organization along the target
string positions: It is not guaranteed, that all the
words in the source string are considered. In other
words we have to force the algorithm to cover all
input string positions. Different strategies to solve
this problem are possible: For example, we can in-
troduce a reward for covering a position, which has
not yet been covered. Or a penalty can be imposed
for each position without correspondence in the tar-
get string.
In preliminary experiments, we found that the
most promising method to satisfy the position cov-
erage constraint is the introduction of an additional
parameter into the recursion formula for DP. In the
following, we will explain this method in detail.
</bodyText>
<subsectionHeader confidence="0.998066">
2.3 Recursion Formula for DP
</subsectionHeader>
<bodyText confidence="0.970485714285714">
In the DP formalism, the search process is described
recursively. Assuming a total length I of the target
string, Qi(c, i, j, e) is the probability of the best par-
tial path ending in the coordinates i in ef and j in
fj1, if the last word e, is e and if c positions in the
source string have been covered.
position in target string
</bodyText>
<page confidence="0.931002">
962
</page>
<bodyText confidence="0.970443">
This quantity is defined recursively. Leaving a
word ei without any assignment (skip) is the easiest
case:
</bodyText>
<equation confidence="0.957817">
(c, j, e) = max {p(ele&apos;)Q i(c, i — 1, j, e1)} .
e&apos;
</equation>
<bodyText confidence="0.999902545454546">
Note that it is not necessary to maximize over the
predecessor positions j&apos;: This maximization is sub-
sumed by the maximization over the positions on the
next level, as can easily be proved.
In the original criterion (6), each position j in the
source string is aligned to exactly one target string
position i. Hence, if i is assigned to 1 subsequent po-
sitions in f , we want to verify that none of these po-
sitions has already been covered: We define a control
function v which returns 1 if the above constraint is
satisfied and 0 otherwise. Then we can write:
</bodyText>
<equation confidence="0.9604065">
[
Qii(c7 i7 e) = max ll fp(ily,J,/)•p(hlei)}•
1&gt;0
j=j-1-1-1
max {p(ele&apos;) •
e&apos;
ma...x[Q r(c — 1,i — 1, , e&apos;) • v(c,l, , j,e&apos;)]}] ,
.7&apos;
</equation>
<bodyText confidence="0.928536444444444">
We now have to find the maximum:
Qi(c,i,i,e) = max {Q7(c, e), C21/(c, e)} •
The decisions made during the dynamic program-
ming process (choices of 1, f and e&apos;) are stored for
recovering the whole translation hypothesis.
The best translation hypothesis can be found by
optimizing the target string length I and requiring
the number of covered positions to be equal to the
source string length J:
</bodyText>
<equation confidence="0.4309825">
max {p(JII) max Q 1(J, I , j, e)} . (10)
2,e
</equation>
<subsectionHeader confidence="0.998483">
2.4 Acceleration Techniques
</subsectionHeader>
<bodyText confidence="0.989181">
The time complexity of the translation method as
described above is
</bodyText>
<equation confidence="0.877737">
ocir2nax -J3 • 1E12) ,
</equation>
<bodyText confidence="0.992536555555556">
where 1E1 is the size of the target language vocab-
ulary 6. Some refinements of this algorithm have
been implemented to increase the translation speed.
1. We can expect the progression of the source
string coverage to be roughly proportional to
the progression of the translation procedure
along the target string. So it is legitimate to
define a minimal and maximal coverage for each
level i:
</bodyText>
<equation confidence="0.964488">
. J . J
cmm(i) = it — r, cmax(i) = + r ,
</equation>
<bodyText confidence="0.872083538461539">
where r is a constant integer number. In prelim-
inary experiments we found that we could set r
to 3 without any loss in translation accuracy.
This reduces the time complexity by a factor J.
2. Optimizing the target string length as formu-
lated in Eq. (10) requires the dynamic program-
ming procedure to start all over again for each
I. If we assume the dependence of the align-
ment probabilities p(ilj , J, I) on I to be negligi-
ble, we can renormalize them by using an esti-
mated target string length I and use p(i1j, J, 1).
Now we can produce one translation el at each
level i = I without restarting the whole process:
</bodyText>
<equation confidence="0.87292">
max Q (J, I , j,e) . (11)
</equation>
<bodyText confidence="0.661514">
For 1 we choose: I= 1(J) = J•P--L. where pi
</bodyText>
<subsubsectionHeader confidence="0.378252">
FL.1
</subsubsectionHeader>
<bodyText confidence="0.999650111111111">
and pj denote the average lengths of the target
and source strings, respectively.
This approximation is partly undone by what
we call rescoring: For each translation hypoth-
esis ef with length I, we compute the &amp;quot;true&amp;quot;
score C2(/) by searching the best inverted align-
ment given ef and f and evaluating the prob-
abilities along this alignment. Hence, we finally
find the best translation via Eq. (12):
</bodyText>
<equation confidence="0.943169">
max {p(JII) • *(I)} . (12)
</equation>
<bodyText confidence="0.901882">
The time complexity for this additional step is
negligible, since there is no optimization over
the English words, which is the dominant factor
in the overall time complexity
O(Imax • J2 ler)
</bodyText>
<listItem confidence="0.70204">
3. We introduced two thresholds:
</listItem>
<bodyText confidence="0.974383">
If e&apos; is the predecessor word of e and e is
not aligned to the source string (&amp;quot;skip&amp;quot;),
then p(ele&apos;) must be higher than OL•
OT: A word e can only be associated with a
source language word f, if p(f le) is higher
than OT.
This restricts the optimization over the target
language vocabulary to a relatively small set of
candidate words. The resulting time complexity
is
O(/rna,,, • J2 161) .
4. When searching for the best partial path to a
gridpoint G = (c,i, j,e), we can sort the arcs
leading to G in a specific manner that allows us
to stop the computation whenever it becomes
clear that no better partial path to G exists.
The effect of this measure depends on the qual-
ity of the used models; in preliminary experi-
ments we observed a speed-up factor of about
3.5.
</bodyText>
<page confidence="0.999001">
963
</page>
<sectionHeader confidence="0.996237" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.9995761875">
The search algorithm suggested in this paper was
tested on the Verbmobil Corpus. The results of pre-
liminary tests on a small automatically generated
Corpus (Amengual et al., 1996) were quite promis-
ing and encouraged us to apply our search algorithm
to a more realistic task.
The Verbmobil Corpus consists of spontaneously
spoken dialogs in the domain of appointment sche-
duling (Wahlster, 1993). German source sentences
are translated into English. In Table 1 the character-
istics of the training and test sets are summarized.
The vocabularies include category labels for dates,
proper names, numbers, times, names of places and
spellings. The model parameters were trained on
16 296 sentence pairs, where names etc. had been
replaced by the appropriate labels.
</bodyText>
<tableCaption confidence="0.99146">
Table 1: Training and test conditions of the Verb-
mobil task.
</tableCaption>
<table confidence="0.954932333333333">
Words in Vocabulary
German 4 498
English 2 958
Number of Sentences 16 296
in Training Corpus
in Test Corpus 150
</table>
<bodyText confidence="0.999865875">
Given the vocabulary sizes, it becomes quite ob-
vious that the lexicon probabilities p(fle) can not
be trained sufficiently on only 16 296 sentence pairs.
The fact that about 40% of the words in the lexicon
are seen only once in training illustrates this. To im-
prove the lexicon probabilities, we interpolated them
with lexicon probabilities p m ( f le) manually created
from a German-English dictionary:
</bodyText>
<equation confidence="0.916904">
PM(f) = if (e, f) is in the dictionary
0 otherwise
</equation>
<bodyText confidence="0.9322818">
where Ne is the number of German words listed as
translations of the English word e. The two lexica
were combined by linear interpolation with the in-
terpolation parameter A. For our first experiments,
we set A to 0.5.
The test corpus consisted of 150 sentences, for
which sample translations exist. The labels were
translated separately: First, the test sentences were
preprocessed in order to replace words or groups
of words by the correct category label. Then, our
search algorithm translated the transformed sen-
tences. In the last step, a simple rule-based algo-
rithm replaced the category labels by the transla-
tions of the original words.
We used a bigram language model for the Eng-
lish language. Its perplexity on the corpus of trans-
formed sample translations (i.e. after labelling) was
13.8.
In preliminary evaluations, optimal values for the
thresholds th and OT had been determined and kept
fixed during the experiments.
As an automatic and easy-to-use measure of the
translation performance, the Levenshtein distance
between the produced translations and the sample
translations was calculated. The translation results
are summarized in Table 2.
Table 2: Word error rates on the Verbmobil Corpus:
insertions (INS), deletions (DEL) and total rate
of word errors (WER) before (BL) and after (AL)
rule-based translation of the labels.
</bodyText>
<table confidence="0.9973185">
before / after Error Rates (%)
INS DEL WER
BL 7.3 18.4 45.0
AL 7.6 17.3 39.6
</table>
<bodyText confidence="0.979449828571428">
(Tillmann et al., 1997a) report a word error rate
of 51.8% on similar data.
Although the Levenshtein distance has the great
advantage to be automatically computable, we have
to keep in mind, that it depends fundamentally on
the choice of the sample translation. For example,
each of the expressions &amp;quot;thanks&amp;quot;, &amp;quot;thank you&amp;quot; and
&amp;quot;thank you very much&amp;quot; is a legitimate translation
of the German &amp;quot;danke schon&amp;quot;, but when calculating
the Levenshtein distance to a sample translation, at
least two of them will produce word errors. The
more words the vocabulary contains, the more im-
portant will be the problem of synonyms.
This is why we also asked five experts to classify
independently the produced translations into three
categories, being the same as in (Wang and Waibel,
1997):
Correct translations are grammatical and convey
the same meaning as the input.
Acceptable translations convey the same meaning
but with small grammatical mistakes or they convey
most but not the entire meaning of the input.
Incorrect translations are ungrammatical or con-
vey little meaningful information or the information
is different from the input.
Examples for each category are given in Table
3. Table 4 shows the statistics of the translation
performance. When different judgements existed
for one sentence, the majority vote was accepted.
For the calculation of the subjective sentence error
rate (SSER), translations from the second category
counted as &amp;quot;half-correct&amp;quot;.
When evaluating the performance of a statistical
machine translator, we would like to distinguish er-
rors due to the weakness of the underlying models
</bodyText>
<page confidence="0.99846">
964
</page>
<tableCaption confidence="0.9626395">
Table 3: Examples of Correct (C), Acceptable (A), and Incorrect (I) translations on Verbmobil. The source
language is German and the target language is English.
</tableCaption>
<table confidence="0.998059933333333">
C Input: Ah neunter Marz bin ich in Köln.
Output: I am in Cologne on the ninth of March.
Input: Habe ich mir notiert.
Output: I have noted that.
A Input: Samstag und Februar sind gut , aber der siebzehnte ware besser.
Output: Saturday and February are quite but better the seventeenth.
Input: Ich konnte erst eigentlich jetzt wieder dann November vorschlagen. Ab zweiten
Output: November.
I could actually coming back November then. Suggest beginning the second of November.
I Input: Ja, also mit Dienstag und mittwochs und so hatte ich Zeit, aber Montag kommen wir
Output: hier nicht weg aus Kiel.
Yes, and including on Tuesday and Wednesday as well, I have time on Monday but we
will come to be away from Kiel.
Input: Dann fahren wir da los.
Output: We go out.
</table>
<tableCaption confidence="0.977414">
Table 4: Subjective evaluation of the translation
</tableCaption>
<bodyText confidence="0.926283285714286">
performance on Verbmobil: number of sentences
evaluated as Correct (C), Acceptable (A) or In-
correct (I). For the total percentage of non-correct
translations (SSER), the &amp;quot;acceptable&amp;quot; translations
are counted as half-errors.
Total Correct Acceptable Incorrect SSER
150 61 45 44 44.3%
from search errors, occuring whenever the search
algorithm misses a translation hypothesis with a
higher score. Unfortunately, we can never be sure
that a search error does not occur, because we do
not know whether or not there is another string with
an even higher score than the produced output.
Nevertheless, it is quite interesting to compare the
score of the algorithm&apos;s output and the score of the
sample translation in such cases in which the out-
put is not correct (it is classified as &amp;quot;acceptable&amp;quot; or
&amp;quot;incorrect&amp;quot;).
The original value to be maximized by the search
algorithm (see Eq. (6)) is the score as defined by the
underlying models and described by Eq. (13).
</bodyText>
<equation confidence="0.70306125">
Pr(ef).p(JII) nlax [p(iIj, J, I) • p(fi lei)] . (13)
i
Et1,11
J.1
</equation>
<bodyText confidence="0.999302347826087">
We calculated this score for the sample trans-
lations as well as for the automatically generated
translations. Table 5 shows the result of the com-
parison. In most cases, the incorrect outputs have
higher scores than the sample translations, which
leads to the conclusion that the improvement of the
models (stronger language model for the target lan-
guage, better translation model and especially more
training data) will have a strong impact on the qual-
ity of the produced translations. The other cases, i.
e. those in which the models prefer the sample trans-
lations to the produced output, might be due to the
difference of the original search criterion (6) and the
criterion (9), which is the basis of our search algo-
rithm. The approximation made by the introduction
of the parameters OT and OL, is an additional reason
for search errors.
Table 5: Comparison: Score of Reference Transla-
tion e and Translator Output e&apos; for &amp;quot;acceptable&amp;quot;
translations (A) and &amp;quot;incorrect&amp;quot; translations (I).
For the total number of non-correct translations
(T), the &amp;quot;acceptable&amp;quot; translations are counted as
half-errors.
</bodyText>
<table confidence="0.77668525">
A I T %
Total number 45 44 66.5 100.0
Score(e) &gt; Score(e1) 11 13 18.5 27.8
Score(e) &lt; Score(e1) 34 31 48.0 72.2
</table>
<bodyText confidence="0.999816222222222">
As far as we know, only two recent papers have
dealt with decoding problem for machine translation
systems that use translation models based on hid-
den alignments without a monotonicity constraint:
(Berger et al., 1994) and (Wang and Waibel, 1997).
The former uses data sets that differ significantly
from the Verbmobil task and hence, the reported
results cannot be compared to ours. The latter
presents experiments carried out on a corpus corn-
</bodyText>
<page confidence="0.994892">
965
</page>
<bodyText confidence="0.998956571428571">
parable to our test data in terms of vocabulary sizes,
domain and number of test sentences. The authors
report a subjective sentence error rate which is in
the same range as ours. An exact comparison is
only possible if exactly the same training and test-
ing data are used and if all the details of the search
algorithms are considered.
</bodyText>
<sectionHeader confidence="0.996096" genericHeader="method">
4 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.976979805555556">
In this paper, we have presented a new search al-
gorithm for statistical machine translation. First
experiments prove its applicability to realistic and
complex tasks such as spontaneously spoken dialogs.
Several improvements to our algorithm are plan-
ned, the most important one being the implementa-
tion of pruning methods (Ney et al., 1992). Pruning
methods have already been used successfully in ma-
chine translation (Tillmann et al., 1997a). The first
question to be answered in this context is how to
make two different hypotheses H1 and H2 compara-
ble: Even if they cover the same number of source
string words, they might cover different words, es-
pecially words that are not equally difficult to trans-
late, which corresponds to higher or lower transla-
tion probability estimates. To cope with this prob-
lem, we will introduce a heuristic for the estimation
of the cost of translating the remaining source words.
This is similar to the heuristics in ..-1-search.
(Vogel et al., 1996) report better perplexity re-
sults on the Verbmobil Corpus with their HMM-
based alignment model in comparison to Model 2
of (Brown et al., 1993). For such a model, however,
the new interpretation of the alignments becomes
essential: We cannot adopt the estimates for the
alignment probabilities p(ili&apos; , I). Instead, we have
to re-calculate them as inverted alignments. This
will provide estimates for the probabilities p(jlf , J).
The most important advantage of the HMM-based
alignment models for our approach is the fact, that
they do not depend on the unknown target string
length I.
Acknowledgement. This work was partly sup-
ported by the German Federal Ministry of Educa-
tion, Science, Research and Technology under the
Contract Number 011V 601 A (Verbmobil).
</bodyText>
<sectionHeader confidence="0.998172" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.999795306451613">
J. C. Amengual, J. M. Benedi, A. Castafio, A. Mar-
zal, F. Prat, E. Vidal, J. M. Vilar, C. Delogu,
A. di Carlo, H. Ney, and S. Vogel. 1996. Example-
Based Understanding and Translation Systems
(EuTrans): Final Report, Part I. Deliverable of
ESPRIT project No. 20268, October.
A.L. Berger, P.F. Brown, J. Cocke, S.A. Della
Pietra, V.J. Della Pietra, J.R. Gillett, J.D. Laf-
ferty, R.L. Mercer, H. Printz, and L. tires. 1994.
The Candide System for Machine Translation. In
Proc. ARPA Human Language Technology Work-
shop, Plainsboro, NJ, pages 152-157. Morgan
Kaufmann Publ., March.
P.F. Brown, S.A. Della Pietra, V.J. Della Pietra,
and R.L. Mercer. 1993. Mathematics of Statisti-
cal Machine Translation: Parameter Estimation.
Computational Linguistics, 19(2):263-311.
I. Dagan, K. W. Church, and W. A. Gale. 1993.
Robust Bilingual Word Alignment for Machine
Aided Translation. In Proceedings of the Work-
shop on Very Large Corpora, Columbus, Ohio,
pages 1-8.
P. Fung and K.W. Church. 1994. K-vec: A new Ap-
proach for Aligning Parallel Texts. In Proceedings
of the 15th International Conference on Compu-
tational Linguistics, Kyoto, Japan, pages 1096-
1102.
M. Kay and M. Rtischeisen. 1993. Text-Trans-
lation Alignment. Computational Linguistics,
19(1):121-142.
H. Ney, D. Mergel, A. Noll, and A. Paeseler. 1992.
Data Driven Search Organization for Continuous
Speech Recognition. IEEE Transactions on Sig-
nal Processing, 40(2):272-281, February.
C. Tillmann, S. Vogel, H. Ney, H. Sawaf, and A. Zu-
biaga. 1997a. Accelerated DP based Search for
Statistical Translation. In Proceedings of the 5th
European Conference on Speech Communication
and Technology, Rhodes, Greece, pages 2667-2670,
September.
C. Tillmann, S. Vogel, H. Ney, and A. Zubia-
ga. 1997b. A DP-Based Search using Monotone
Alignments in Statistical Translation. In Proceed-
ings of the ACL/EACL &apos;97, Madrid, Spain, pages
289-296, July.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-
Based Word Alignment in Statistical Translation.
In Proceedings of the 16th International Confer-
ence on Computational Linguistics, Copenhagen,
Denmark, pages 836-841, August.
W. Wahlster. 1993. Verbmobil: Translation of Face-
to-Face Dialogs. In Proceedings of the MT Sum-
mit IV, pages 127-135, Kobe, Japan.
Ye-Yi Wang and A. Waibel. 1997. Decoding Algo-
rithm in Statistical Translation. In Proceedings of
the ACL/EACL &apos;97, Madrid, Spain, pages 366-
372, July.
D. Wu. 1996. A Polynomial-Time Algorithm for
Statistical Machine Translation. In Proceedings
of the 34th Annual Conference of the Association
for Computational Linguistics, Santa Cruz, CA,
pages 152 — 158, June.
</reference>
<page confidence="0.998994">
966
</page>
<sectionHeader confidence="0.498556" genericHeader="method">
Zusammenfassung
</sectionHeader>
<bodyText confidence="0.999818727272727">
Wir stellen einen neuartigen.. Suchalgorithmus fiir
die statistische maschinelle Ubersetzung vor, der
auf der dynamischen Programmierung (DP) beruht.
Wahrend des Suchprozesses werden.zwei statistische
Wissensquellen kombiniert: Ein Ubersetzungsmo-
dell und em n Bigramm-Sprachmodell. Dieser Such-
algorithmus erweitert Hypothesen entlang den Posi-
tionen des Zielsatzes, wobei garantiert wird, dai3 alle
Worter im Quellsatz beriicksichtigt werden. Es wer-
den experimentelle Ergebnisse auf der Verbmobil-
Aufgabe angegeben.
</bodyText>
<subsectionHeader confidence="0.560703">
Résumé
</subsectionHeader>
<bodyText confidence="0.999932666666667">
Nous presentons un nouveau algorithme de recherche
pour la traduction automatique &amp;quot;statistique qui est
basee sur la programmation dynamique (DP). Pen-
dant la recherche deux sources d&apos;information statis-
tiques sont combinees: Un modele de traduction
et un bigram language model. Cet algorithme de
recherche construit des hypotheses le long des po-
sitions de la phrase en langue de cible tout en
garantissant la consideration progressive des mots
dans la phrase en langue de source. Des resultats
experimentaux sur la ta.che Verbmobil sont presen-
tes.
</bodyText>
<page confidence="0.995487">
967
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.823287">
<title confidence="0.999705">A DP based Search Algorithm for Statistical Machine Translation</title>
<author confidence="0.972751">S Vogel Nieflen</author>
<author confidence="0.972751">Tillmann</author>
<affiliation confidence="0.922478">fiir Informatik VI RINTH Aachen - University of Technology</affiliation>
<address confidence="0.999931">D-52056 Aachen, Germany</address>
<email confidence="0.999255">ormatik.rwth—aachen.de</email>
<abstract confidence="0.9982112">We introduce a novel search algorithm for statistical machine translation based on dynamic programming (DP). During the search process two statistical knowledge sources are combined: a translation model and a bigram language model. This search algorithm expands hypotheses along the positions of the target string while guaranteeing progressive coverage of the words in the source string. We present experimental results on the Verbmobil task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J C Amengual</author>
<author>J M Benedi</author>
<author>A Castafio</author>
<author>A Marzal</author>
<author>F Prat</author>
<author>E Vidal</author>
<author>J M Vilar</author>
<author>C Delogu</author>
<author>A di Carlo</author>
<author>H Ney</author>
<author>S Vogel</author>
</authors>
<title>ExampleBased Understanding and Translation Systems (EuTrans): Final Report, Part I. Deliverable of ESPRIT project</title>
<date>1996</date>
<tech>No. 20268,</tech>
<marker>Amengual, Benedi, Castafio, Marzal, Prat, Vidal, Vilar, Delogu, di Carlo, Ney, Vogel, 1996</marker>
<rawString>J. C. Amengual, J. M. Benedi, A. Castafio, A. Marzal, F. Prat, E. Vidal, J. M. Vilar, C. Delogu, A. di Carlo, H. Ney, and S. Vogel. 1996. ExampleBased Understanding and Translation Systems (EuTrans): Final Report, Part I. Deliverable of ESPRIT project No. 20268, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>P F Brown</author>
<author>J Cocke</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>J R Gillett</author>
<author>J D Lafferty</author>
<author>R L Mercer</author>
<author>H Printz</author>
<author>L tires</author>
</authors>
<title>The Candide System for Machine Translation. In</title>
<date>1994</date>
<booktitle>Proc. ARPA Human Language Technology Workshop, Plainsboro, NJ,</booktitle>
<pages>152--157</pages>
<publisher>Morgan Kaufmann Publ.,</publisher>
<contexts>
<context position="23095" citStr="Berger et al., 1994" startWordPosition="3948" endWordPosition="3951">onal reason for search errors. Table 5: Comparison: Score of Reference Translation e and Translator Output e&apos; for &amp;quot;acceptable&amp;quot; translations (A) and &amp;quot;incorrect&amp;quot; translations (I). For the total number of non-correct translations (T), the &amp;quot;acceptable&amp;quot; translations are counted as half-errors. A I T % Total number 45 44 66.5 100.0 Score(e) &gt; Score(e1) 11 13 18.5 27.8 Score(e) &lt; Score(e1) 34 31 48.0 72.2 As far as we know, only two recent papers have dealt with decoding problem for machine translation systems that use translation models based on hidden alignments without a monotonicity constraint: (Berger et al., 1994) and (Wang and Waibel, 1997). The former uses data sets that differ significantly from the Verbmobil task and hence, the reported results cannot be compared to ours. The latter presents experiments carried out on a corpus corn965 parable to our test data in terms of vocabulary sizes, domain and number of test sentences. The authors report a subjective sentence error rate which is in the same range as ours. An exact comparison is only possible if exactly the same training and testing data are used and if all the details of the search algorithms are considered. 4 Conclusion and Future Work In th</context>
</contexts>
<marker>Berger, Brown, Cocke, Pietra, Pietra, Gillett, Lafferty, Mercer, Printz, tires, 1994</marker>
<rawString>A.L. Berger, P.F. Brown, J. Cocke, S.A. Della Pietra, V.J. Della Pietra, J.R. Gillett, J.D. Lafferty, R.L. Mercer, H. Printz, and L. tires. 1994. The Candide System for Machine Translation. In Proc. ARPA Human Language Technology Workshop, Plainsboro, NJ, pages 152-157. Morgan Kaufmann Publ., March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<date>1993</date>
<journal>Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics,</journal>
<pages>19--2</pages>
<contexts>
<context position="3298" citStr="Brown et al., 1993" startWordPosition="522" endWordPosition="525">e translation approach based on Bayes&apos; decision rule. The task of statistical machine translation can be subdivided into two fields: I. the field of modelling, which introduces structures into the probabilistic dependencies and provides methods for estimating the parameters of the models from bilingual corpora; 2. the field of decoding, i.e. finding a search algorithm, which performs the argmax operation in Eq. (1) as efficient as possible. 960 L2 Alignment with Mixture Distribution Several papers have discussed the first issue, especially the problem of word alignments for bilingual corpora (Brown et al., 1993), (Dagan et al., 1993), (Kay and ROscheisen, 1993), (Fung and Church, 1994), (Vogel et al., 1996). In our search procedure, we use a mixture-based alignment model that slightly differs from the model introduced as Model 2 in (Brown et al., 1993). It is based on a decomposition of the joint probability for fil into a product of the probabilities for each word Pr(g lef) = P(JII) 11 P(f.; I ) , (2) 3=1 where the lengths of the strings are regarded as random variables and modelled by the distribution p(JII). Now we assume a sort of pairwise interaction between the French word fl and each English w</context>
<context position="5339" citStr="Brown et al., 1993" startWordPosition="890" endWordPosition="893">e so-called maximum approximation: At each point, only the best choice of i is considered for the alignment path: Pr (fif lef)= P(.111&amp;quot;)11 max I)-P(filei)- (5) jr.eizEtim We can now formulate the criterion to be maximized by a search algorithm: max [p(JII) max {Pr(ef)- rj max [P(ili, J,i) • P(f; lei)] I] • E. i ti,/1 3=1 Because of the problem of data sparseness, we use a parametric model for the alignment probabilities. It assumes that the distance of the positions relative to the diagonal of the (j, i) plane is the dominating factor: r(i — j P(ili, .1,1) = (7) ar=1 r(i&apos; i7) As described in (Brown et al., 1993), the EM algorithm can be used to estimate the parameters of the model. 1.3 Search in Statistical Machine Translation In the last few years, there has been a number of papers considering the problem of finding an efficient search procedure (Wu, 1996), (Tillmann et al., 1997a), (Tillmann et al., 1997b), (Wang and Waibel, 1997). All of these approaches use a bigram language model, because they are quite simple and easy-touse and they have proven their prediction power in stochastic language processing, especially speech recognition. Assuming a bigram language model, we would like to re-formulate</context>
<context position="7370" citStr="Brown et al., 1993" startWordPosition="1233" endWordPosition="1236">nce can be determined when the algorithm produces ei. This approximation corresponds to the assumption of the fundamental similarity of the sentence structures in both languages. In (Tillmann et al., 1997b) text transformations in the source language are used to adapt the word ordering in the source strings to the target language grammar. (Wang and Waibel, 1997) describe an algorithm based on A&amp;quot;-search. Here, hypotheses are extended (6) 961 by adding a word to the end of the target string while considering the source string words in any order. The underlying translation model is Model 2 from (Brown et al., 1993). (Wu, 1996) formulates a DP search for stochastic bracketing transduction grammars. The bigram language model is integrated into the algorithm at the point, where two partial parse trees are combined. 2 DP Search 2.1 The Inverted Alignment Model For our search method, we chose an algorithm which is based on dynamic programming. Compared to an A*-based algorithm dynamic programming has the fundamental advantage, that solutions of subproblems are stored and can then be re-used in later stages of the search process. However, for the optimization criterion considered here dynamic programming is o</context>
<context position="24815" citStr="Brown et al., 1993" startWordPosition="4237" endWordPosition="4240">e two different hypotheses H1 and H2 comparable: Even if they cover the same number of source string words, they might cover different words, especially words that are not equally difficult to translate, which corresponds to higher or lower translation probability estimates. To cope with this problem, we will introduce a heuristic for the estimation of the cost of translating the remaining source words. This is similar to the heuristics in ..-1-search. (Vogel et al., 1996) report better perplexity results on the Verbmobil Corpus with their HMMbased alignment model in comparison to Model 2 of (Brown et al., 1993). For such a model, however, the new interpretation of the alignments becomes essential: We cannot adopt the estimates for the alignment probabilities p(ili&apos; , I). Instead, we have to re-calculate them as inverted alignments. This will provide estimates for the probabilities p(jlf , J). The most important advantage of the HMM-based alignment models for our approach is the fact, that they do not depend on the unknown target string length I. Acknowledgement. This work was partly supported by the German Federal Ministry of Education, Science, Research and Technology under the Contract Number 011V</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P.F. Brown, S.A. Della Pietra, V.J. Della Pietra, and R.L. Mercer. 1993. Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19(2):263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>K W Church</author>
<author>W A Gale</author>
</authors>
<title>Robust Bilingual Word Alignment for Machine Aided Translation.</title>
<date>1993</date>
<booktitle>In Proceedings of the Workshop on Very Large Corpora,</booktitle>
<pages>1--8</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="3320" citStr="Dagan et al., 1993" startWordPosition="526" endWordPosition="529"> based on Bayes&apos; decision rule. The task of statistical machine translation can be subdivided into two fields: I. the field of modelling, which introduces structures into the probabilistic dependencies and provides methods for estimating the parameters of the models from bilingual corpora; 2. the field of decoding, i.e. finding a search algorithm, which performs the argmax operation in Eq. (1) as efficient as possible. 960 L2 Alignment with Mixture Distribution Several papers have discussed the first issue, especially the problem of word alignments for bilingual corpora (Brown et al., 1993), (Dagan et al., 1993), (Kay and ROscheisen, 1993), (Fung and Church, 1994), (Vogel et al., 1996). In our search procedure, we use a mixture-based alignment model that slightly differs from the model introduced as Model 2 in (Brown et al., 1993). It is based on a decomposition of the joint probability for fil into a product of the probabilities for each word Pr(g lef) = P(JII) 11 P(f.; I ) , (2) 3=1 where the lengths of the strings are regarded as random variables and modelled by the distribution p(JII). Now we assume a sort of pairwise interaction between the French word fl and each English word e, in ef. . These </context>
</contexts>
<marker>Dagan, Church, Gale, 1993</marker>
<rawString>I. Dagan, K. W. Church, and W. A. Gale. 1993. Robust Bilingual Word Alignment for Machine Aided Translation. In Proceedings of the Workshop on Very Large Corpora, Columbus, Ohio, pages 1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Fung</author>
<author>K W Church</author>
</authors>
<title>K-vec: A new Approach for Aligning Parallel Texts.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics, Kyoto, Japan,</booktitle>
<pages>1096--1102</pages>
<contexts>
<context position="3373" citStr="Fung and Church, 1994" startWordPosition="534" endWordPosition="537">tical machine translation can be subdivided into two fields: I. the field of modelling, which introduces structures into the probabilistic dependencies and provides methods for estimating the parameters of the models from bilingual corpora; 2. the field of decoding, i.e. finding a search algorithm, which performs the argmax operation in Eq. (1) as efficient as possible. 960 L2 Alignment with Mixture Distribution Several papers have discussed the first issue, especially the problem of word alignments for bilingual corpora (Brown et al., 1993), (Dagan et al., 1993), (Kay and ROscheisen, 1993), (Fung and Church, 1994), (Vogel et al., 1996). In our search procedure, we use a mixture-based alignment model that slightly differs from the model introduced as Model 2 in (Brown et al., 1993). It is based on a decomposition of the joint probability for fil into a product of the probabilities for each word Pr(g lef) = P(JII) 11 P(f.; I ) , (2) 3=1 where the lengths of the strings are regarded as random variables and modelled by the distribution p(JII). Now we assume a sort of pairwise interaction between the French word fl and each English word e, in ef. . These dependencies are captured in the form of a mixture di</context>
</contexts>
<marker>Fung, Church, 1994</marker>
<rawString>P. Fung and K.W. Church. 1994. K-vec: A new Approach for Aligning Parallel Texts. In Proceedings of the 15th International Conference on Computational Linguistics, Kyoto, Japan, pages 1096-1102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kay</author>
<author>M Rtischeisen</author>
</authors>
<date>1993</date>
<journal>Text-Translation Alignment. Computational Linguistics,</journal>
<pages>19--1</pages>
<marker>Kay, Rtischeisen, 1993</marker>
<rawString>M. Kay and M. Rtischeisen. 1993. Text-Translation Alignment. Computational Linguistics, 19(1):121-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ney</author>
<author>D Mergel</author>
<author>A Noll</author>
<author>A Paeseler</author>
</authors>
<title>Data Driven Search Organization for Continuous Speech Recognition.</title>
<date>1992</date>
<journal>IEEE Transactions on Signal Processing,</journal>
<pages>40--2</pages>
<contexts>
<context position="24030" citStr="Ney et al., 1992" startWordPosition="4104" endWordPosition="4107">es. The authors report a subjective sentence error rate which is in the same range as ours. An exact comparison is only possible if exactly the same training and testing data are used and if all the details of the search algorithms are considered. 4 Conclusion and Future Work In this paper, we have presented a new search algorithm for statistical machine translation. First experiments prove its applicability to realistic and complex tasks such as spontaneously spoken dialogs. Several improvements to our algorithm are planned, the most important one being the implementation of pruning methods (Ney et al., 1992). Pruning methods have already been used successfully in machine translation (Tillmann et al., 1997a). The first question to be answered in this context is how to make two different hypotheses H1 and H2 comparable: Even if they cover the same number of source string words, they might cover different words, especially words that are not equally difficult to translate, which corresponds to higher or lower translation probability estimates. To cope with this problem, we will introduce a heuristic for the estimation of the cost of translating the remaining source words. This is similar to the heur</context>
</contexts>
<marker>Ney, Mergel, Noll, Paeseler, 1992</marker>
<rawString>H. Ney, D. Mergel, A. Noll, and A. Paeseler. 1992. Data Driven Search Organization for Continuous Speech Recognition. IEEE Transactions on Signal Processing, 40(2):272-281, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Tillmann</author>
<author>S Vogel</author>
<author>H Ney</author>
<author>H Sawaf</author>
<author>A Zubiaga</author>
</authors>
<title>Accelerated DP based Search for Statistical Translation.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th European Conference on Speech Communication and Technology, Rhodes, Greece,</booktitle>
<pages>2667--2670</pages>
<contexts>
<context position="5613" citStr="Tillmann et al., 1997" startWordPosition="938" endWordPosition="941"> max [P(ili, J,i) • P(f; lei)] I] • E. i ti,/1 3=1 Because of the problem of data sparseness, we use a parametric model for the alignment probabilities. It assumes that the distance of the positions relative to the diagonal of the (j, i) plane is the dominating factor: r(i — j P(ili, .1,1) = (7) ar=1 r(i&apos; i7) As described in (Brown et al., 1993), the EM algorithm can be used to estimate the parameters of the model. 1.3 Search in Statistical Machine Translation In the last few years, there has been a number of papers considering the problem of finding an efficient search procedure (Wu, 1996), (Tillmann et al., 1997a), (Tillmann et al., 1997b), (Wang and Waibel, 1997). All of these approaches use a bigram language model, because they are quite simple and easy-touse and they have proven their prediction power in stochastic language processing, especially speech recognition. Assuming a bigram language model, we would like to re-formulate Eq. (6) in the following way: max [p(JII) max { H max [p(ei lei--1)* el j iE[1,1] =1 P(il.i, i) • P(fi lei)] }] Any search algorithm tending to perform the maximum operations in Eq. (8) has to guarantee, that the predecessor word ei_1 can be determined at the time when a c</context>
<context position="6955" citStr="Tillmann et al., 1997" startWordPosition="1161" endWordPosition="1164">tudied. (Tillmann et al., 1997b) and (Tillmann et al., 1997a) propose a search procedure based on dynamic programming, that examines the source string sequentially. Although it is very efficient in terms of translation speed, it suffers from the drawback of being dependent on the so-called monotonicity constraint: The alignment paths are assumed to be monotone. Hence, the word at position i — 1 in the target sentence can be determined when the algorithm produces ei. This approximation corresponds to the assumption of the fundamental similarity of the sentence structures in both languages. In (Tillmann et al., 1997b) text transformations in the source language are used to adapt the word ordering in the source strings to the target language grammar. (Wang and Waibel, 1997) describe an algorithm based on A&amp;quot;-search. Here, hypotheses are extended (6) 961 by adding a word to the end of the target string while considering the source string words in any order. The underlying translation model is Model 2 from (Brown et al., 1993). (Wu, 1996) formulates a DP search for stochastic bracketing transduction grammars. The bigram language model is integrated into the algorithm at the point, where two partial parse tre</context>
<context position="18087" citStr="Tillmann et al., 1997" startWordPosition="3126" endWordPosition="3129">tions, optimal values for the thresholds th and OT had been determined and kept fixed during the experiments. As an automatic and easy-to-use measure of the translation performance, the Levenshtein distance between the produced translations and the sample translations was calculated. The translation results are summarized in Table 2. Table 2: Word error rates on the Verbmobil Corpus: insertions (INS), deletions (DEL) and total rate of word errors (WER) before (BL) and after (AL) rule-based translation of the labels. before / after Error Rates (%) INS DEL WER BL 7.3 18.4 45.0 AL 7.6 17.3 39.6 (Tillmann et al., 1997a) report a word error rate of 51.8% on similar data. Although the Levenshtein distance has the great advantage to be automatically computable, we have to keep in mind, that it depends fundamentally on the choice of the sample translation. For example, each of the expressions &amp;quot;thanks&amp;quot;, &amp;quot;thank you&amp;quot; and &amp;quot;thank you very much&amp;quot; is a legitimate translation of the German &amp;quot;danke schon&amp;quot;, but when calculating the Levenshtein distance to a sample translation, at least two of them will produce word errors. The more words the vocabulary contains, the more important will be the problem of synonyms. This is </context>
<context position="24129" citStr="Tillmann et al., 1997" startWordPosition="4119" endWordPosition="4122">exact comparison is only possible if exactly the same training and testing data are used and if all the details of the search algorithms are considered. 4 Conclusion and Future Work In this paper, we have presented a new search algorithm for statistical machine translation. First experiments prove its applicability to realistic and complex tasks such as spontaneously spoken dialogs. Several improvements to our algorithm are planned, the most important one being the implementation of pruning methods (Ney et al., 1992). Pruning methods have already been used successfully in machine translation (Tillmann et al., 1997a). The first question to be answered in this context is how to make two different hypotheses H1 and H2 comparable: Even if they cover the same number of source string words, they might cover different words, especially words that are not equally difficult to translate, which corresponds to higher or lower translation probability estimates. To cope with this problem, we will introduce a heuristic for the estimation of the cost of translating the remaining source words. This is similar to the heuristics in ..-1-search. (Vogel et al., 1996) report better perplexity results on the Verbmobil Corpu</context>
</contexts>
<marker>Tillmann, Vogel, Ney, Sawaf, Zubiaga, 1997</marker>
<rawString>C. Tillmann, S. Vogel, H. Ney, H. Sawaf, and A. Zubiaga. 1997a. Accelerated DP based Search for Statistical Translation. In Proceedings of the 5th European Conference on Speech Communication and Technology, Rhodes, Greece, pages 2667-2670, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Tillmann</author>
<author>S Vogel</author>
<author>H Ney</author>
<author>A Zubiaga</author>
</authors>
<title>A DP-Based Search using Monotone Alignments in Statistical Translation.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL/EACL &apos;97,</booktitle>
<pages>289--296</pages>
<location>Madrid,</location>
<contexts>
<context position="5613" citStr="Tillmann et al., 1997" startWordPosition="938" endWordPosition="941"> max [P(ili, J,i) • P(f; lei)] I] • E. i ti,/1 3=1 Because of the problem of data sparseness, we use a parametric model for the alignment probabilities. It assumes that the distance of the positions relative to the diagonal of the (j, i) plane is the dominating factor: r(i — j P(ili, .1,1) = (7) ar=1 r(i&apos; i7) As described in (Brown et al., 1993), the EM algorithm can be used to estimate the parameters of the model. 1.3 Search in Statistical Machine Translation In the last few years, there has been a number of papers considering the problem of finding an efficient search procedure (Wu, 1996), (Tillmann et al., 1997a), (Tillmann et al., 1997b), (Wang and Waibel, 1997). All of these approaches use a bigram language model, because they are quite simple and easy-touse and they have proven their prediction power in stochastic language processing, especially speech recognition. Assuming a bigram language model, we would like to re-formulate Eq. (6) in the following way: max [p(JII) max { H max [p(ei lei--1)* el j iE[1,1] =1 P(il.i, i) • P(fi lei)] }] Any search algorithm tending to perform the maximum operations in Eq. (8) has to guarantee, that the predecessor word ei_1 can be determined at the time when a c</context>
<context position="6955" citStr="Tillmann et al., 1997" startWordPosition="1161" endWordPosition="1164">tudied. (Tillmann et al., 1997b) and (Tillmann et al., 1997a) propose a search procedure based on dynamic programming, that examines the source string sequentially. Although it is very efficient in terms of translation speed, it suffers from the drawback of being dependent on the so-called monotonicity constraint: The alignment paths are assumed to be monotone. Hence, the word at position i — 1 in the target sentence can be determined when the algorithm produces ei. This approximation corresponds to the assumption of the fundamental similarity of the sentence structures in both languages. In (Tillmann et al., 1997b) text transformations in the source language are used to adapt the word ordering in the source strings to the target language grammar. (Wang and Waibel, 1997) describe an algorithm based on A&amp;quot;-search. Here, hypotheses are extended (6) 961 by adding a word to the end of the target string while considering the source string words in any order. The underlying translation model is Model 2 from (Brown et al., 1993). (Wu, 1996) formulates a DP search for stochastic bracketing transduction grammars. The bigram language model is integrated into the algorithm at the point, where two partial parse tre</context>
<context position="18087" citStr="Tillmann et al., 1997" startWordPosition="3126" endWordPosition="3129">tions, optimal values for the thresholds th and OT had been determined and kept fixed during the experiments. As an automatic and easy-to-use measure of the translation performance, the Levenshtein distance between the produced translations and the sample translations was calculated. The translation results are summarized in Table 2. Table 2: Word error rates on the Verbmobil Corpus: insertions (INS), deletions (DEL) and total rate of word errors (WER) before (BL) and after (AL) rule-based translation of the labels. before / after Error Rates (%) INS DEL WER BL 7.3 18.4 45.0 AL 7.6 17.3 39.6 (Tillmann et al., 1997a) report a word error rate of 51.8% on similar data. Although the Levenshtein distance has the great advantage to be automatically computable, we have to keep in mind, that it depends fundamentally on the choice of the sample translation. For example, each of the expressions &amp;quot;thanks&amp;quot;, &amp;quot;thank you&amp;quot; and &amp;quot;thank you very much&amp;quot; is a legitimate translation of the German &amp;quot;danke schon&amp;quot;, but when calculating the Levenshtein distance to a sample translation, at least two of them will produce word errors. The more words the vocabulary contains, the more important will be the problem of synonyms. This is </context>
<context position="24129" citStr="Tillmann et al., 1997" startWordPosition="4119" endWordPosition="4122">exact comparison is only possible if exactly the same training and testing data are used and if all the details of the search algorithms are considered. 4 Conclusion and Future Work In this paper, we have presented a new search algorithm for statistical machine translation. First experiments prove its applicability to realistic and complex tasks such as spontaneously spoken dialogs. Several improvements to our algorithm are planned, the most important one being the implementation of pruning methods (Ney et al., 1992). Pruning methods have already been used successfully in machine translation (Tillmann et al., 1997a). The first question to be answered in this context is how to make two different hypotheses H1 and H2 comparable: Even if they cover the same number of source string words, they might cover different words, especially words that are not equally difficult to translate, which corresponds to higher or lower translation probability estimates. To cope with this problem, we will introduce a heuristic for the estimation of the cost of translating the remaining source words. This is similar to the heuristics in ..-1-search. (Vogel et al., 1996) report better perplexity results on the Verbmobil Corpu</context>
</contexts>
<marker>Tillmann, Vogel, Ney, Zubiaga, 1997</marker>
<rawString>C. Tillmann, S. Vogel, H. Ney, and A. Zubiaga. 1997b. A DP-Based Search using Monotone Alignments in Statistical Translation. In Proceedings of the ACL/EACL &apos;97, Madrid, Spain, pages 289-296, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Vogel</author>
<author>H Ney</author>
<author>C Tillmann</author>
</authors>
<title>HMMBased Word Alignment in Statistical Translation.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics,</booktitle>
<pages>836--841</pages>
<location>Copenhagen, Denmark,</location>
<contexts>
<context position="3395" citStr="Vogel et al., 1996" startWordPosition="538" endWordPosition="541"> can be subdivided into two fields: I. the field of modelling, which introduces structures into the probabilistic dependencies and provides methods for estimating the parameters of the models from bilingual corpora; 2. the field of decoding, i.e. finding a search algorithm, which performs the argmax operation in Eq. (1) as efficient as possible. 960 L2 Alignment with Mixture Distribution Several papers have discussed the first issue, especially the problem of word alignments for bilingual corpora (Brown et al., 1993), (Dagan et al., 1993), (Kay and ROscheisen, 1993), (Fung and Church, 1994), (Vogel et al., 1996). In our search procedure, we use a mixture-based alignment model that slightly differs from the model introduced as Model 2 in (Brown et al., 1993). It is based on a decomposition of the joint probability for fil into a product of the probabilities for each word Pr(g lef) = P(JII) 11 P(f.; I ) , (2) 3=1 where the lengths of the strings are regarded as random variables and modelled by the distribution p(JII). Now we assume a sort of pairwise interaction between the French word fl and each English word e, in ef. . These dependencies are captured in the form of a mixture distribution: p(fi ief) </context>
<context position="24673" citStr="Vogel et al., 1996" startWordPosition="4212" endWordPosition="4215">lready been used successfully in machine translation (Tillmann et al., 1997a). The first question to be answered in this context is how to make two different hypotheses H1 and H2 comparable: Even if they cover the same number of source string words, they might cover different words, especially words that are not equally difficult to translate, which corresponds to higher or lower translation probability estimates. To cope with this problem, we will introduce a heuristic for the estimation of the cost of translating the remaining source words. This is similar to the heuristics in ..-1-search. (Vogel et al., 1996) report better perplexity results on the Verbmobil Corpus with their HMMbased alignment model in comparison to Model 2 of (Brown et al., 1993). For such a model, however, the new interpretation of the alignments becomes essential: We cannot adopt the estimates for the alignment probabilities p(ili&apos; , I). Instead, we have to re-calculate them as inverted alignments. This will provide estimates for the probabilities p(jlf , J). The most important advantage of the HMM-based alignment models for our approach is the fact, that they do not depend on the unknown target string length I. Acknowledgemen</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>S. Vogel, H. Ney, and C. Tillmann. 1996. HMMBased Word Alignment in Statistical Translation. In Proceedings of the 16th International Conference on Computational Linguistics, Copenhagen, Denmark, pages 836-841, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wahlster</author>
</authors>
<title>Verbmobil: Translation of Faceto-Face Dialogs.</title>
<date>1993</date>
<booktitle>In Proceedings of the MT Summit IV,</booktitle>
<pages>127--135</pages>
<location>Kobe, Japan.</location>
<contexts>
<context position="15652" citStr="Wahlster, 1993" startWordPosition="2727" endWordPosition="2728">ever it becomes clear that no better partial path to G exists. The effect of this measure depends on the quality of the used models; in preliminary experiments we observed a speed-up factor of about 3.5. 963 3 Experiments The search algorithm suggested in this paper was tested on the Verbmobil Corpus. The results of preliminary tests on a small automatically generated Corpus (Amengual et al., 1996) were quite promising and encouraged us to apply our search algorithm to a more realistic task. The Verbmobil Corpus consists of spontaneously spoken dialogs in the domain of appointment scheduling (Wahlster, 1993). German source sentences are translated into English. In Table 1 the characteristics of the training and test sets are summarized. The vocabularies include category labels for dates, proper names, numbers, times, names of places and spellings. The model parameters were trained on 16 296 sentence pairs, where names etc. had been replaced by the appropriate labels. Table 1: Training and test conditions of the Verbmobil task. Words in Vocabulary German 4 498 English 2 958 Number of Sentences 16 296 in Training Corpus in Test Corpus 150 Given the vocabulary sizes, it becomes quite obvious that th</context>
</contexts>
<marker>Wahlster, 1993</marker>
<rawString>W. Wahlster. 1993. Verbmobil: Translation of Faceto-Face Dialogs. In Proceedings of the MT Summit IV, pages 127-135, Kobe, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ye-Yi Wang</author>
<author>A Waibel</author>
</authors>
<title>Decoding Algorithm in Statistical Translation.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL/EACL &apos;97,</booktitle>
<pages>366--372</pages>
<location>Madrid,</location>
<contexts>
<context position="5666" citStr="Wang and Waibel, 1997" startWordPosition="946" endWordPosition="949">ecause of the problem of data sparseness, we use a parametric model for the alignment probabilities. It assumes that the distance of the positions relative to the diagonal of the (j, i) plane is the dominating factor: r(i — j P(ili, .1,1) = (7) ar=1 r(i&apos; i7) As described in (Brown et al., 1993), the EM algorithm can be used to estimate the parameters of the model. 1.3 Search in Statistical Machine Translation In the last few years, there has been a number of papers considering the problem of finding an efficient search procedure (Wu, 1996), (Tillmann et al., 1997a), (Tillmann et al., 1997b), (Wang and Waibel, 1997). All of these approaches use a bigram language model, because they are quite simple and easy-touse and they have proven their prediction power in stochastic language processing, especially speech recognition. Assuming a bigram language model, we would like to re-formulate Eq. (6) in the following way: max [p(JII) max { H max [p(ei lei--1)* el j iE[1,1] =1 P(il.i, i) • P(fi lei)] }] Any search algorithm tending to perform the maximum operations in Eq. (8) has to guarantee, that the predecessor word ei_1 can be determined at the time when a certain word ei at position i in the target string is </context>
<context position="7115" citStr="Wang and Waibel, 1997" startWordPosition="1188" endWordPosition="1191">ally. Although it is very efficient in terms of translation speed, it suffers from the drawback of being dependent on the so-called monotonicity constraint: The alignment paths are assumed to be monotone. Hence, the word at position i — 1 in the target sentence can be determined when the algorithm produces ei. This approximation corresponds to the assumption of the fundamental similarity of the sentence structures in both languages. In (Tillmann et al., 1997b) text transformations in the source language are used to adapt the word ordering in the source strings to the target language grammar. (Wang and Waibel, 1997) describe an algorithm based on A&amp;quot;-search. Here, hypotheses are extended (6) 961 by adding a word to the end of the target string while considering the source string words in any order. The underlying translation model is Model 2 from (Brown et al., 1993). (Wu, 1996) formulates a DP search for stochastic bracketing transduction grammars. The bigram language model is integrated into the algorithm at the point, where two partial parse trees are combined. 2 DP Search 2.1 The Inverted Alignment Model For our search method, we chose an algorithm which is based on dynamic programming. Compared to an</context>
<context position="18837" citStr="Wang and Waibel, 1997" startWordPosition="3249" endWordPosition="3252">lly computable, we have to keep in mind, that it depends fundamentally on the choice of the sample translation. For example, each of the expressions &amp;quot;thanks&amp;quot;, &amp;quot;thank you&amp;quot; and &amp;quot;thank you very much&amp;quot; is a legitimate translation of the German &amp;quot;danke schon&amp;quot;, but when calculating the Levenshtein distance to a sample translation, at least two of them will produce word errors. The more words the vocabulary contains, the more important will be the problem of synonyms. This is why we also asked five experts to classify independently the produced translations into three categories, being the same as in (Wang and Waibel, 1997): Correct translations are grammatical and convey the same meaning as the input. Acceptable translations convey the same meaning but with small grammatical mistakes or they convey most but not the entire meaning of the input. Incorrect translations are ungrammatical or convey little meaningful information or the information is different from the input. Examples for each category are given in Table 3. Table 4 shows the statistics of the translation performance. When different judgements existed for one sentence, the majority vote was accepted. For the calculation of the subjective sentence erro</context>
<context position="23123" citStr="Wang and Waibel, 1997" startWordPosition="3953" endWordPosition="3956">ors. Table 5: Comparison: Score of Reference Translation e and Translator Output e&apos; for &amp;quot;acceptable&amp;quot; translations (A) and &amp;quot;incorrect&amp;quot; translations (I). For the total number of non-correct translations (T), the &amp;quot;acceptable&amp;quot; translations are counted as half-errors. A I T % Total number 45 44 66.5 100.0 Score(e) &gt; Score(e1) 11 13 18.5 27.8 Score(e) &lt; Score(e1) 34 31 48.0 72.2 As far as we know, only two recent papers have dealt with decoding problem for machine translation systems that use translation models based on hidden alignments without a monotonicity constraint: (Berger et al., 1994) and (Wang and Waibel, 1997). The former uses data sets that differ significantly from the Verbmobil task and hence, the reported results cannot be compared to ours. The latter presents experiments carried out on a corpus corn965 parable to our test data in terms of vocabulary sizes, domain and number of test sentences. The authors report a subjective sentence error rate which is in the same range as ours. An exact comparison is only possible if exactly the same training and testing data are used and if all the details of the search algorithms are considered. 4 Conclusion and Future Work In this paper, we have presented </context>
</contexts>
<marker>Wang, Waibel, 1997</marker>
<rawString>Ye-Yi Wang and A. Waibel. 1997. Decoding Algorithm in Statistical Translation. In Proceedings of the ACL/EACL &apos;97, Madrid, Spain, pages 366-372, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>A Polynomial-Time Algorithm for Statistical Machine Translation.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Conference of the Association for Computational Linguistics,</booktitle>
<pages>152--158</pages>
<location>Santa Cruz, CA,</location>
<contexts>
<context position="5589" citStr="Wu, 1996" startWordPosition="936" endWordPosition="937"> {Pr(ef)- rj max [P(ili, J,i) • P(f; lei)] I] • E. i ti,/1 3=1 Because of the problem of data sparseness, we use a parametric model for the alignment probabilities. It assumes that the distance of the positions relative to the diagonal of the (j, i) plane is the dominating factor: r(i — j P(ili, .1,1) = (7) ar=1 r(i&apos; i7) As described in (Brown et al., 1993), the EM algorithm can be used to estimate the parameters of the model. 1.3 Search in Statistical Machine Translation In the last few years, there has been a number of papers considering the problem of finding an efficient search procedure (Wu, 1996), (Tillmann et al., 1997a), (Tillmann et al., 1997b), (Wang and Waibel, 1997). All of these approaches use a bigram language model, because they are quite simple and easy-touse and they have proven their prediction power in stochastic language processing, especially speech recognition. Assuming a bigram language model, we would like to re-formulate Eq. (6) in the following way: max [p(JII) max { H max [p(ei lei--1)* el j iE[1,1] =1 P(il.i, i) • P(fi lei)] }] Any search algorithm tending to perform the maximum operations in Eq. (8) has to guarantee, that the predecessor word ei_1 can be determi</context>
<context position="7382" citStr="Wu, 1996" startWordPosition="1237" endWordPosition="1238">when the algorithm produces ei. This approximation corresponds to the assumption of the fundamental similarity of the sentence structures in both languages. In (Tillmann et al., 1997b) text transformations in the source language are used to adapt the word ordering in the source strings to the target language grammar. (Wang and Waibel, 1997) describe an algorithm based on A&amp;quot;-search. Here, hypotheses are extended (6) 961 by adding a word to the end of the target string while considering the source string words in any order. The underlying translation model is Model 2 from (Brown et al., 1993). (Wu, 1996) formulates a DP search for stochastic bracketing transduction grammars. The bigram language model is integrated into the algorithm at the point, where two partial parse trees are combined. 2 DP Search 2.1 The Inverted Alignment Model For our search method, we chose an algorithm which is based on dynamic programming. Compared to an A*-based algorithm dynamic programming has the fundamental advantage, that solutions of subproblems are stored and can then be re-used in later stages of the search process. However, for the optimization criterion considered here dynamic programming is only suboptim</context>
</contexts>
<marker>Wu, 1996</marker>
<rawString>D. Wu. 1996. A Polynomial-Time Algorithm for Statistical Machine Translation. In Proceedings of the 34th Annual Conference of the Association for Computational Linguistics, Santa Cruz, CA, pages 152 — 158, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>