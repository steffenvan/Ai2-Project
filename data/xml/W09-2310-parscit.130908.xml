<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000457">
<title confidence="0.9930225">
Coupling hierarchical word reordering and decoding in phrase-based
statistical machine translation
</title>
<author confidence="0.85397">
Maxim Khalilov and José A.R. Fonollosa
</author>
<affiliation confidence="0.57254">
Universitat Politècnica de Catalunya
</affiliation>
<address confidence="0.9828405">
Campus Nord UPC, 08034,
Barcelona, Spain
</address>
<email confidence="0.999157">
{khalilov,adrian}@gps.tsc.upc.edu
</email>
<author confidence="0.986036">
Mark Dras
</author>
<affiliation confidence="0.991682">
Macquarie University
</affiliation>
<address confidence="0.7644645">
North Ryde NSW 2109,
Sydney, Australia
</address>
<email confidence="0.998006">
madras@ics.mq.edu.au
</email>
<sectionHeader confidence="0.996653" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9986765">
In this paper, we start with the existing idea of
taking reordering rules automatically derived
from syntactic representations, and applying
them in a preprocessing step before translation
to make the source sentence structurally more
like the target; and we propose a new approach
to hierarchically extracting these rules. We
evaluate this, combined with a lattice-based
decoding, and show improvements over state-
of-the-art distortion models.
</bodyText>
<sectionHeader confidence="0.998791" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999612857142857">
One of the big challenges for the MT community is
the problem of placing translated words in a natural
order. This issue originates from the fact that dif-
ferent languages are characterized by different word
order requirements. The problem is especially im-
portant if the distance between words which should
be reordered is high (global reordering); in this case
the reordering decision is very difficult to take based
on statistical information due to dramatic expansion
of the search space with the increase in number of
words involved in the search process.
Classically, statistical machine translation (SMT)
systems do not incorporate any linguistic analysis
and work at the surface level of word forms. How-
ever, more recently MT systems are moving towards
including additional linguistic and syntactic infor-
mative sources (for example, source- and/or target-
side syntax) into word reordering process. In this pa-
per we propose using a syntactic reordering system
operating with fully, partially and non- lexicalized
reordering patterns, which are applied on the step
</bodyText>
<page confidence="0.972314">
78
</page>
<bodyText confidence="0.999942">
prior to translation; the novel idea in this paper is in
the derivation of these rules in a hierarchical manner,
inspired by Imamura et al (2005). Furthermore, we
propose generating a word lattice from the bilingual
corpus with the reordered source side, extending the
search space on the decoding step. A thorough study
of the combination of syntactical and word lattice re-
ordering approaches is another novelty of the paper.
</bodyText>
<sectionHeader confidence="0.999672" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999910833333333">
Many reordering algorithms have appeared over the
past few years. Word class-based reordering was a
part of Och’s Alignment Template system (Och et
al., 2004); the main criticism of this approach is that
it shows bad performance for the pair of languages
with very distinct word order. The state-of-the-art
SMT system Moses implements a distance-based re-
ordering model (Koehn et al., 2003) and a distor-
tion model, operating with rewrite patterns extracted
from a phrase alignment table (Tillman, 2004).
Many SMT models implement the brute force ap-
proach, introducing several constrains for the re-
ordering search as described in Kanthak et al. (2005)
and Crego et al. (2005). The main criticism of such
systems is that the constraints are not lexicalized.
Recently there has been interest in SMT exploiting
non-monotonic decoding which allow for extension
of the search space and linguistic information in-
volvement. The variety of such models includes a
constrained distance-based reordering (Costa-jussà
et al., 2006); and a constrained version of distortion
model where the reordering search problem is tack-
led through a set of linguistically motivated rules
used during decoding (Crego and Mariño, 2007).
</bodyText>
<note confidence="0.9650715">
Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 78–86,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999941677419355">
A quite popular class of reordering algorithms is
a monotonization of the source part of the parallel
corpus prior to translation. The first work on this
approach is described in Nießen and Ney (2004),
where morpho-syntactic information was used to ac-
count for the reorderings needed. A representative
set of similar systems includes: a set of hand-crafted
reordering patterns for German-to-English (Collins
et al., 2005) and Chinese-English (Wang et al.,
2007) translations, emphasizing the distinction be-
tween German/Chinese and English clause struc-
ture; and statistical machine reordering (SMR) tech-
nique where a monotonization of the source words
sequence is performed by translating them into the
reordered one using well established SMT mecha-
nism (Costa-jussà and Fonollosa, 2006). Coupling
of SMR algorithm and the search space extension
via generating a set of weighted reordering hypothe-
ses has demonstrated a significant improvement, as
shown in Costa-jussà and Fonollosa (2008).
The technique proposed in this study is most
similar to the one proposed for French-to-English
translation task in Xia and McCord (2004), where
the authors present a hybrid system for French-
English translation based on the principle of auto-
matic rewrite patterns extraction using a parse tree
and phrase alignments. We propose using a word
distortion model not only to monotonize the source
part of the corpus (using a different approach to
rewrite rule organization from Xia and McCord), but
also to extend the search space during decoding.
</bodyText>
<sectionHeader confidence="0.966741" genericHeader="method">
3 Baseline phrase-based SMT systems
</sectionHeader>
<bodyText confidence="0.999766307692308">
The reference system which was used as a transla-
tion mechanism is the state-of-the-art Moses-based
SMT (Koehn et al., 2007). The training and weights
tuning procedures can be found on the Moses web
page1.
Classical phrase-based translation is considered
as a three step algorithm: (1) the source sequence
of words is segmented into phrases, (2) each phrase
is translated into the target language using a transla-
tion table, (3) the target phrases are reordered to fit
the target language. The probabilities of the phrases
are estimated by relative frequencies of their appear-
ance in the training corpus.
</bodyText>
<footnote confidence="0.940076">
1http://www.statmt.org/moses/
</footnote>
<bodyText confidence="0.99958475">
In baseline experiments we used a phrase depen-
dent lexicalized reordering model, as proposed in
Tillmann (2004). According to this model, mono-
tonic or reordered local orientations enriched with
probabilities are learned from training data. During
decoding, translation is viewed as a monotone block
sequence generation process with the possibility to
swap a pair of neighbor blocks.
</bodyText>
<sectionHeader confidence="0.645772" genericHeader="method">
4 Syntax-based reordering coupled with
word graph
</sectionHeader>
<bodyText confidence="0.983358333333333">
Our syntax-based reordering system requires access
to source and target language parse trees and word
alignments intersections.
</bodyText>
<subsectionHeader confidence="0.973464">
4.1 Notation
</subsectionHeader>
<bodyText confidence="0.999736142857143">
Syntax-based reordering (SBR) operates with source
and target parse trees that represent the syntactic
structure of a string in source and target languages
according to a Context-Free Grammar (CFG).
We call this representation &amp;quot;CFG form&amp;quot;. We
formally define a CFG in the usual way as G =
(N, T, R, S), where N is a set of nonterminal sym-
bols (corresponding to source-side phrase and part-
of-speech tags); T is a set of source-side terminals
(the lexicon), R is a set of production rules of the
form q —* -y, with q E N and -y, which is a sequence
of terminal and nonterminal symbols; and S E N is
the distinguished symbol.
The reordering rules then have the form
</bodyText>
<equation confidence="0.999386">
q0@0 ... qk@k —*
qd�@d0 ...qdk@dk|Lexicon|p1 (1)
</equation>
<bodyText confidence="0.9998838">
where qi E N for all 0 &lt; i &lt; k; (do ... dk) is
a permutation of (0 ... k); Lexicon comes from the
source-side set of words for each qi; and pi is a prob-
ability associated with the rule. Figure 1 gives two
examples of the rule format.
</bodyText>
<subsectionHeader confidence="0.996983">
4.2 Rules extraction
</subsectionHeader>
<bodyText confidence="0.994501">
Concept. Inspired by the ideas presented in Imamura
et al. (2005), where monolingual correspondences of
syntactic nodes are used during decoding, we extract
a set of bilingual patterns allowing for reordering as
described below:
</bodyText>
<page confidence="0.992257">
79
</page>
<listItem confidence="0.9357843">
(1) align the monotone bilingual corpus with
GIZA++ (Och and Ney, 2003) and find
the intersection of direct and inverse word
alignments, resulting in the construction
of the projection matrix P (see below));
(2) parse the source and the target parts of the
parallel corpus;
(3) extract reordering patterns from the par-
allel non-isomorphic CFG-trees based on
the word alignment intersection.
</listItem>
<bodyText confidence="0.989878142857143">
Step 2 is straightforward; we explain aspects of
Steps 1 and 3 in more detail below. Figures 1 and 2
show an example of the extraction of two lexicalized
rules for a parallel Arabic-English sentence:
Arabic: h*A hW fndq +k
English: this is your hotel
We use this below in our explanations.
</bodyText>
<figureCaption confidence="0.998076">
Figure 2: Example of subtree transfer and reordering
rules extraction.
</figureCaption>
<bodyText confidence="0.999700846153846">
Projection matrix. Bilingual content can be rep-
resented in the form of words or sequences of words
depending on the syntactic role of the corresponding
grammatical element (constituent or POS).
Given two parse trees and a word alignment in-
tersection, a projection matrix P is defined as an
M x N matrix such that M is the number of words
in the target phrase; N is the number of words in
the source phrase; and a cell (i, j) has a value based
on the alignment intersection — this value is zero
if word i and word j do not align, and is a unique
non-zero link number if they do.
For the trees in Figure 2,
</bodyText>
<equation confidence="0.97421675">
� 1 0 0 0
P= � � � 0 2 0 0 �
0 0 0 3 � � �
0 0 4 0
</equation>
<bodyText confidence="0.975606666666667">
Unary chains. Given an unary chain of the form
X —* Y , rules are extracted for each level in this
chain. For example given a rule
</bodyText>
<equation confidence="0.9246">
NP@0 ADVP@1 —* ADVP@1 NP@0
</equation>
<bodyText confidence="0.9853535">
and a unary chain &amp;quot;ADVP —* AD&amp;quot;, a following
equivalent rule will be generated
</bodyText>
<equation confidence="0.958774">
NP@0 AD@1 —* AD@1 NP@0.
</equation>
<bodyText confidence="0.99938975">
The role of target-side parse tree. Although re-
ordering is performed on the source side only, the
target-side tree is of great importance: the reorder-
ing rules can be only extracted if the words covered
by the rule are entirely covered by both a node in
the source and in the target trees. It allows the more
accurate determination of the covering and limits of
the extracted rules.
</bodyText>
<subsectionHeader confidence="0.999368">
4.3 Rules organization
</subsectionHeader>
<bodyText confidence="0.999987142857143">
Once the list of fully lexicalized reordering patterns
is extracted, all the rules are progressively processed
reducing the amount of lexical information. These
initial rules are iteratively expanded such that each
element of the pattern is generalized until all the lex-
ical elements of the rule are represented in the form
of fully unlexicalized categories. Hence, from each
</bodyText>
<figure confidence="0.3981675">
NN@0 NP@1 —* NP@1 NN@0  |NN@0 &lt;&lt; fndq &gt;&gt; NP@1 &lt;&lt; +k &gt;&gt;  |p
NN@0 NNP@1 —* NNP@1 NN@0  |NN@0 &lt;&lt; fndq &gt;&gt; NNP@1 &lt;&lt; +k &gt;&gt;  |pi
</figure>
<figureCaption confidence="0.996312">
Figure 1: Directly extracted rules.
</figureCaption>
<page confidence="0.976034">
80
</page>
<bodyText confidence="0.9994334">
initial pattern with N lexical elements, 2N − 2 par-
tially lexicalized rules and 1 general rule are gener-
ated. An example of the process of delexicalization
can be found in Figure 3.
Thus, finally three types of rules are available: (1)
fully lexicalized (initial) rules, (2) partially lexical-
ized rules and (3) unlexicalized (general) rules.
On the next step, the sets are processed separately:
patterns are pruned and ambiguous rules are re-
moved. All the rules from the fully lexicalized, par-
tially lexicalized and general sets that appear fewer
than k times are directly discarded (k is a shorthand
for kful, kpart and kgener). The probability of a
pattern is estimated based on relative frequency of
their appearance in the training corpus. Only one
the most probable rule is stored. Fully lexicalized
rules are not pruned (kful = 0); partially lexicalized
rules that have been seen only once were discarded
(kpart = 1); the thresholds kgener was set to 3: it
limits the number of general patterns capturing rare
grammatical exceptions which can be easily found
in any language.
Only the one-best reordering is used in other
stages of the algorithm, so the rule output function-
ing as an input to the next rule can lead to situa-
tions reverting the change of word order that the
previously applied rule made. Therefore, the rules
that can be ambiguous when applied sequentially
during decoding are pruned according to the higher
probability principle. For example, for the pair of
patterns with the same lexicon (which is empty for
a general rule leading to a recurring contradiction
NP@0 VP@1 —* VP@1 NP@0 p1, VP@0 NP@1
—* NP@1 VP@0 p2 ), the less probable rule is re-
moved.
Finally, there are three resulting parameter tables
analogous to the &amp;quot;r-table&amp;quot; as stated in (Yamada and
Knight, 2001), consisting of POS- and constituent-
based patterns allowing for reordering and mono-
tone distortion (examples can be found in Table 5).
</bodyText>
<subsectionHeader confidence="0.983334">
4.4 Source-side monotonization
</subsectionHeader>
<bodyText confidence="0.999485">
Rule application is performed as a bottom-up parse
tree traversal following two principles:
</bodyText>
<listItem confidence="0.7258254">
(1) the longest possible rule is applied, i.e. among
a set of nested rules, the rule with a longest left-side
covering is selected. For example, in the case of the
appearance of an NN JJ RB sequence and presence
of the two reordering rules
</listItem>
<equation confidence="0.9975275">
NN@0 JJ@1 —* ... and
NN@0 JJ@1 RB@2 —* ...
</equation>
<bodyText confidence="0.720515">
the latter pattern will be applied.
</bodyText>
<listItem confidence="0.586129">
(2) the rule containing the maximum lexical infor-
mation is applied, i.e. in case there is more than one
alternative pattern from different groups, the lexical-
ized rules have preference over the partially lexical-
ized, and partially lexicalized over general ones.
</listItem>
<figureCaption confidence="0.999382">
Figure 4: Reordered source-side parse tree.
</figureCaption>
<bodyText confidence="0.974885222222222">
Once the reordering of the training corpus is
ready, it is realigned and new more monotonic align-
ment is passed to the SMT system. In theory, the
word links from the original alignment can be used,
however, due to our experience, running GIZA++
again results in a better word alignment since it is
easier to learn on the modified training example.
Example of correct local reordering done with the
SBR model can be found in Figure 4.
</bodyText>
<figure confidence="0.74389875">
Initial rule: NN@0 NP@1 —* NP@1 NN@0  |NN@0 &lt;&lt; fndq &gt;&gt; NP@1 &lt;&lt; +k &gt;&gt;  |p1
Part. lexic. rules: NN@0 NP@1 —* NP@1 NN@0  |NN@0 &lt;&lt; fndq &gt;&gt; NP@1 &lt;&lt; - &gt;&gt;  |p2
NN@0 NP@1 —* NP@1 NN@0  |NN@0 &lt;&lt; - &gt;&gt; NP@1 &lt;&lt; +k &gt;&gt;  |p3
General rule: NN@0 NP@1 —* NP@1 NN@0  |p4
</figure>
<figureCaption confidence="0.996936">
Figure 3: Example of a lexical rule expansion.
</figureCaption>
<page confidence="0.988861">
81
</page>
<subsectionHeader confidence="0.996078">
4.5 Coupling with decoding
</subsectionHeader>
<bodyText confidence="0.999919481481481">
In order to improve reordering power of the transla-
tion system, we implemented an additional reorder-
ing as described in Crego and Mariño (2006).
Multiple word segmentations is encoded in a lat-
tice, which is then passed to the input of the de-
coder, containing reordering alternatives consistent
with the previously extracted rules. The decoder
takes the n-best reordering of a source sentence
coded in the form of a word lattice. This approach
is in line with recent research tendencies in SMT, as
described for example in (Hildebrand et al., 2008;
Xu et al., 2005). Originally, word lattice algorithms
do not involve syntax into reordering process, there-
fore their reordering power is limited at representing
long-distance reordering. Our approach is designed
in the spirit of hybrid MT, integrating syntax trans-
fer approach and statistical word lattice methods to
achieve better MT performance on the basis of the
standard state-of-the-art models.
During training a set of word permutation patterns
is automatically learned following given word-to-
word alignment. Since the original and monotonized
(reordered) alignments may vary, different sets of
reordering patterns are generated. Note that no in-
formation about the syntax of the sentence is used:
the reordering permutations are motivated by the
crossed links found in the word alignment and, con-
</bodyText>
<figure confidence="0.999220238095238">
(a) Monotonic search, plain text: &gt;n +h mTEm *w tAryx Eryq
(b) Word lattice, plain text: &gt;n +h mTEm *w tAryx Eryq
S &gt; n 1 + h 2 mTEm 3 * w 4 tAryx 5 Eryq L
S 1 2 3 4 5 6 7 8 9
&gt; n
+ h
+ h
mTEm mTEm
&gt; n
* w
&gt; n
* w
mTEm
tAryx
tAryx
mTEm
E ryq
Eryq
1 0 L
tAryx
(c) Word lattice, reordered text: &gt;n +h mTEm *w Eryq tAryx
</figure>
<figureCaption confidence="0.986074">
Figure 5: Comparative example of a monotone search (a), word lattice for a plain (b) and reordered (c) source
sentences.
</figureCaption>
<figure confidence="0.995064166666667">
tAryx
tAryx
Eryq
* w
* w
* w
tAryx
+ h
mTEm mTEm
E ryq
&gt; n
mTEm
S 1 2 3 4 5 6 7 8 9 1 0 1 1 1 2 1 3 1 4 L
* w
&gt; n
&gt; n
+ h
mTEm
* w
* w
Eryq
E ryq
Eryq
Eryq
</figure>
<page confidence="0.993352">
82
</page>
<bodyText confidence="0.9956895">
sequently, the generalization power of this frame-
work is limited to local permutations.
On the step prior to decoding, the system gen-
erates word reordering graph for every source sen-
tence, expressed in the form of a word lattice. The
decoder processes word lattice instead of only one
input hypothesis, extending the monotonic search
graph with alternative paths.
Original sentence in Arabic, the English gloss and
reference translation are:
Ar.: &gt;n +h mTEm *w Eryq tAryx
Gl.: this restaurant has history illustrious
Ref: ’this restaurant has an illustrious history’
The monotonic search graph (a) is extended with
a word lattice for the monotonic train set (b) and re-
ordered train sets (c). Figure 5 shows an example
of the input word graph expressed in the form of a
word lattice. Lattice (c) differ from the graph (b) in
number of edges and provides more input options to
the decoder. The decision about final translation is
taken during decoding considering all the possible
paths, provided by the word lattice.
</bodyText>
<sectionHeader confidence="0.990447" genericHeader="evaluation">
5 Experiments and results
</sectionHeader>
<subsectionHeader confidence="0.958627">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.997471444444444">
The experiments were performed on two Arabic-
English corpora: the BTEC’08 corpus from the
tourist domain and the 50K first-lines extraction
from the corpus that was provided to the NIST’08
evaluation campaign and belongs to the news do-
main (NIST50K). The corpora differ mainly in the
average sentence length (ASL), which is the key cor-
pus characteristic in global reordering studies.
A training set statistics can be found in Table 1.
</bodyText>
<table confidence="0.993744">
BTEC NIST50K
Ar En Ar En
Sentences 24.9 K 24.9 K 50 K 50 K
Words 225 K 210 K 1.2 M 1.35 M
ASL 9.05 8.46 24.61 26.92
Voc 11.4 K 7.6 K 55.3 36.3
</table>
<tableCaption confidence="0.999947">
Table 1: Basic statistics of the BTEC training corpus.
</tableCaption>
<bodyText confidence="0.999443">
The BTEC development dataset consists of 489
sentences and 3.8 K running words, with 6 human-
made reference translations per sentence; the dataset
used to test the translation quality has 500 sentences,
4.1 K words and is also provided with 6 reference
translations.
The NIST50K development set consists of 1353
sentences and 43 K words; the test data contains
1056 sentences and 33 K running words. Both
datasets have 4 reference translations per sentence.
</bodyText>
<subsectionHeader confidence="0.999576">
5.2 Arabic data preprocessing
</subsectionHeader>
<bodyText confidence="0.999944666666667">
We took a similar approach to that shown in Habash
and Sadat (2006), using the MADA+TOKAN sys-
tem for disambiguation and tokenization. For dis-
ambiguation only diacritic unigram statistics were
employed. For tokenization we used the D3 scheme
with -TAGBIES option. The scheme splits the fol-
lowing set of clitics: w+, f+, b+, k+, l+, Al+ and
pronominal clitics. The -TAGBIES option produces
Bies POS tags on all taggable tokens.
</bodyText>
<subsectionHeader confidence="0.996799">
5.3 Experimental setup
</subsectionHeader>
<bodyText confidence="0.99999295">
We used the Stanford Parser (Klein and Man-
ning, 2003) for both languages, Penn English Tree-
bank (Marcus et al., 1993) and Penn Arabic Tree-
bank set (Kulick et al., 2006). The English Treebank
is provided with 48 POS and 14 syntactic tags, the
Arabic Treebank has 26 POS and 23 syntactic cate-
gories.
As mentioned above, specific rules are not pruned
away due to a limited amount of training material we
set the thresholds kp,,,t and kg,,,,,, to relatively low
values, 1 and 3, respectively.
Evaluation conditions were case-insensitive and
with punctuation marks considered. The target-
side 4-gram language model was estimated using
the SRILM toolkit (Stolcke, 2002) and modified
Kneser-Ney discounting with interpolation. The
highest BLEU score (Papineni et al., 2002) was cho-
sen as the optimization criterion. Apart from BLEU,
a standard automatic measure METEOR (Banerjee
and Lavie, 2005) was used for evaluation.
</bodyText>
<subsectionHeader confidence="0.730285">
5.4 Results
</subsectionHeader>
<bodyText confidence="0.9976695">
The scores considered are: BLEU scores obtained
for the development set as the final point of the
MERT procedure (Dev), and BLEU and METEOR
scores obtained on test dataset (Test).
We present BTEC results (Tables 2), character-
ized by relatively short sentence length, and the re-
</bodyText>
<page confidence="0.997373">
83
</page>
<bodyText confidence="0.997490333333333">
sults obtained on the NIST corpus (Tables 3) with
much longer sentences and much need of global re-
ordering.
</bodyText>
<table confidence="0.998901">
Dev Test
BLEU BLEU METEOR
Plain 48.31 45.02 65.98
BL 48.46 47.10 68.10
SBR 48.75 47.52 67.33
SBR+lattice 48.90 48.78 68.85
</table>
<tableCaption confidence="0.978962">
Table 2: Summary of BTEC experimental results.
</tableCaption>
<table confidence="0.999827">
Dev Test
BLEU BLEU METEOR
Plain 41.83 43.80 62.03
BL 42.68 43.52 62.17
SBR 42.71 44.01 63.29
SBR+lattice 43.05 44.89 63.30
</table>
<tableCaption confidence="0.999865">
Table 3: Summary of NIST50K experimental results.
</tableCaption>
<bodyText confidence="0.999658054054054">
Four SMT systems are contrasted: BL refers to
the Moses baseline system: the training data is not
reordered, lexicalized reordering model (Tillman,
2004) is applied; SBR refers to the monotonic sys-
tem configuration with reordered (SBR) source part;
SBR+lattice is the run with reordered source part, on
the translation step the input is represented as a word
lattice.
We also compare the proposed approach with a
monotonic system configuration (Plain). It shows
the effect of source-reordering and lattice input, also
decoded monotonically.
Automatic scores obtained on the test dataset
evolve similarly when the SBR and word lattice rep-
resentation applied to BTEC and NIST50K tasks.
The combined method coupling two reordering
techniques was more effective than the techniques
applied independently and shows an improvement
in terms of BLEU for both corpora. The METEOR
score is only slightly better for the SBR configura-
tions in case of BTEC task; in the case of NIST50K
the METEOR improvement is more evident. The
general trend is that automatic scores evaluated on
the test set increase with the reordering model com-
plexity.
Application of the SBR algorithm only (without
a word lattice decoding) does not allow achieving
statistical significance threshold for a 95% confi-
dence interval and 1000 resamples (Koehn, 2004)
for either of considered corpora. However, the
SBR+lattice system configuration outperforms the
BL by about 1.7 BLEU points (3.5%) for BTEC task
and about 1.4 BLEU point (3.1%) for NIST task.
These differences is statistically significant.
Figure 6 demonstrates how two reordering tech-
niques interact within a sentence with a need for
both global and local word permutations.
</bodyText>
<subsectionHeader confidence="0.998227">
5.5 Syntax-based rewrite rules
</subsectionHeader>
<bodyText confidence="0.999953666666667">
As mentioned above, the SBR operates with three
groups of reordering rules, which are the product
of complete or partial delexicalization of the origi-
nally extracted patterns. The groups are processed
and pruned independently. Basic rules statistics for
both translation tasks can be found in Table 4.
The major part of reordering rules consists of
two or three elements (for BTEC task there are
no patterns including more than three nodes). For
NIST50K there are a few rules with higher size in
words of the move (up to 8). In addition, there are
some long lexicalized rules (7-8), generating a high
number of partially lexicalized patterns.
Table 5 shows the most frequent reordering rules
with non-monotonic right part from each group.
</bodyText>
<reference confidence="0.63821575">
Ar. plain.: AElnt Ajhzp AlAElAm l bEvp AlAmm AlmtHdp fy syrAlywn An ...
En. gloss: announced press release by mission nations united in sierra leone that ...
En. ref.: ’a press release by the united nations mission to sierra leone announced that ...’
Ar. reord.: Ajhzp AlAElAm l bEvp AlmtHdp AlAmm fy syrAlywn AElnt An ...
</reference>
<figureCaption confidence="0.9926465">
Figure 6: Example of SBR application (highlited bold) and local reordering error corrected with word lattice reorder-
ing (underlined).
</figureCaption>
<page confidence="0.996728">
84
</page>
<sectionHeader confidence="0.999275" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999989125">
In this study we have shown how the translation
quality can be improved, coupling (1) SBR al-
gorithm and (2) word alignment-based reordering
framework applied during decoding. The system
automatically learns a set of syntactic reordering
patterns that exploit systematic differences between
word order of source and target languages.
Translation accuracy is clearly higher when al-
lowing for SBR coupled with word lattice input rep-
resentation than standard Moses SMT with existing
(lexicalized) reordering models within the decoder
and one input hypothesis condition. We have also
compared the reordering model a monotonic system.
The method was tested translating from Arabic to
English. Two corpora and tasks were considered:
the BTEC task with much need of local reordering
and the NIST50K task requiring long-distance per-
mutations caused by longer sentences.
The reordering approach can be expanded for any
other pair of languages with available parse tools.
We also expect that the method scale to a large train-
ing set, and that the improvement will still be kept,
however, we plan to confirm this assumption exper-
imentally in the near future.
</bodyText>
<sectionHeader confidence="0.996856" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.983913333333333">
This work has been funded by the Spanish Gov-
ernment under grant TEC2006-13964-C03 (AVI-
VAVOZ project) and under a FPU grant.
</bodyText>
<table confidence="0.951757444444444">
Group # of rules Voc 2-element 3-element 4-element [5-8]-element
BTEC experiments
Specific rules 703 413 406 7 0 0
Partially lexicalized rules 1,306 432 382 50 0 0
General rules 259 5 259 0 0 0
NIST50K experiments
Specific rules 517 399 193 109 72 25
Partially lexicalized rules 17,897 14,263 374 638 1,010 12,241
General rules 489 372 180 90 72 30
</table>
<tableCaption confidence="0.999214">
Table 4: Basic reordering rules statistics.
</tableCaption>
<figure confidence="0.938569666666667">
Specific rules
NN@0 NP@1 -&gt; NP@1 NN@0  |NN@0 « Asm » NP@1 « +y »  |0.0270
DTNN@0 DTJJ@1 -&gt; DTJJ@1 DTNN@0  |DTNN@0 « AlAmm »DTJJ@1 « AlmtHdp »  |0.0515
Partially lexicalized rules
DTNN@0 DTJJ@1 -&gt; DTJJ@1 DTNN@0  |DTNN@0 « NON »DTJJ@1 « AlmtHdp »  |0.0017
NN@0 NNP@1 -&gt; NNP@1 NN@0  |NN@0 « NON »NNP@1 « $rm »  |0.0017
General rules
PP@0 NP@1 -&gt; PP@0 NP@1  |0.0432
NN@0 DTNN@1 DTJJ@2 -&gt; NN@0 DTJJ@2 DTNN@1 |0.0259
</figure>
<tableCaption confidence="0.989596">
Table 5: Examples of Arabic-to-English reordering rules.
</tableCaption>
<page confidence="0.99956">
85
</page>
<sectionHeader confidence="0.98361" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999754203883496">
S. Banerjee and A. Lavie. 2005. METEOR: An auto-
matic metric for MT evaluation with improved corre-
lation with human judgments. In Proceedings of the
ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summariza-
tion, pages 65–72.
M. Collins, Ph. Koehn, and I. Kuˇcerová. 2005. Clause
restructuring for statistical machine translation. In
Proceedings of the 43rd Annual Meeting on ACL 2005,
pages 531–540.
M.R. Costa-jussà and J.A.R. Fonollosa. 2006. Sta-
tistical machine reordering. In Proceedings of the
HLT/EMNLP 2006.
M.R. Costa-jussà and J.A.R. Fonollosa. 2008. Comput-
ing multiple weighted reordering hypotheses for a sta-
tistical machine translation phrase-based system. In In
Proc. of the AMTA’08, Honolulu, USA, October.
M.R. Costa-jussà, J.M. Crego, A. de Gispert, P. Lambert,
M. Khalilov, J. A. Fonollosa, J.B. Mari no, and R.E.
Banchs. 2006. TALP phrase-based system and TALP
system combination for IWSLT 2006. In Proceedings
of the IWSLT 2006, pages 123–129.
J.M. Crego and J. B Mariño. 2006. Reordering experi-
ments for N-gram-based SMT. In SLT’06, pages 242–
245.
J.M. Crego and J.B. Mariño. 2007. Syntax-enhanced N-
gram-based smt. In Proceedings of MT SUMMIT XI.
J.M. Crego, J. B. Mariño, and A. de Gispert. 2005. Re-
ordered search and tuple unfolding for ngram-based
smt. In In Proc. of MT SUMMIT X, pages 283–289,
September.
S. Nießen and H. Ney. 2004. Statistical machine transla-
tion with scarce resources using morpho-syntactic in-
formation. volume 30, pages 181–204.
N. Habash and F. Sadat. 2006. Arabic preprocessing
schemes for statistical machine translation. In Pro-
ceedings of the Human Language Technology Confer-
ence of the NAACL, pages 49–52.
A.S. Hildebrand, K. Rottmann, M. Noamany, Q. Gao,
S. Hewavitharana, N. Bach, and S. Vogel. 2008. Re-
cent improvements in the cmu large scale chinese-
english smt system. In Proceedings of ACL-08: HLT
(Companion Volume), pages 77–80.
K. Imamura, H. Okuma, and E. Sumita. 2005. Practical
approach to syntax-based statistical machine transla-
tion. In Proceedings of MT Summit X, pages 267–274.
S. Kanthak, D. Vilar, E. Matusov, R. Zens, and H. Ney.
2005. Novel reordering approaches in phrase-based
statistical machine translation. In In Proc. of the ACL
Workshop on Building and Using Parallel Texts, pages
167–174, June.
D. Klein and C. Manning. 2003. Accurate unlexicalized
parsing. In Proceedings of the 41st Annual Meeting of
the ACL 2003, pages 423–430.
Ph. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based machine translation. In Proceedings of
the HLT-NAACL 2003, pages 48–54.
Ph. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: open-source toolkit for
statistical machine translation. In Proceedings of ACL
2007, pages 177–180.
Ph. Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388–395.
S. Kulick, R. Gabbard, and M. Marcus. 2006. Parsing the
Arabic Treebank: Analysis and improvements. Tree-
banks and Linguistic Theories.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313–330.
F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19–51.
F.J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng,
V. Jain, Z. Jin, and D. Radev. 2004. A Smorgasbord of
Features for Statistical Machine Translation. In Pro-
ceedings of HLT/NAACL04, pages 161–168.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of ACL 2002, pages 311–
318.
A. Stolcke. 2002. SRILM: an extensible language mod-
eling toolkit. In Proceedings of the Int. Conf. on Spo-
ken Language Processing, pages 901–904.
C. Tillman. 2004. A unigram orientation model for sta-
tistical machine translation. In Proceedings of HLT-
NAACL’04.
C. Wang, M. Collins, and P. Koehn. 2007. Chinese syn-
tactic reordering for statistical machine translation. In
Proceedings of the Joint Conference on EMNLP.
F. Xia and M. McCord. 2004. Improving a statistical mt
system with automatically learned rewrite patterns. In
Proceedings of the COLING 2004.
J. Xu, E. Matusov, R. Zens, and H. Ney. 2005. In-
tegrated chinese word segmentation in statistical ma-
chine translation. In Proc. of IWSLT 2005.
K. Yamada and K. Knight. 2001. A syntax-based statis-
tical translation model. In Proceedings of ACL 2001,
pages 523–530.
</reference>
<page confidence="0.998563">
86
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.058196">
<title confidence="0.987088">Coupling hierarchical word reordering and decoding in statistical machine translation</title>
<author confidence="0.98309">A R Khalilov</author>
<affiliation confidence="0.898087">Universitat Politècnica de Campus Nord UPC,</affiliation>
<address confidence="0.65705">Barcelona,</address>
<email confidence="0.999844">khalilov@gps.tsc.upc.edu</email>
<email confidence="0.999844">adrian@gps.tsc.upc.edu</email>
<author confidence="0.95261">Mark</author>
<title confidence="0.2393335">Macquarie North Ryde NSW</title>
<author confidence="0.520284">Sydney</author>
<email confidence="0.998525">madras@ics.mq.edu.au</email>
<abstract confidence="0.998020181818182">In this paper, we start with the existing idea of taking reordering rules automatically derived from syntactic representations, and applying them in a preprocessing step before translation to make the source sentence structurally more like the target; and we propose a new approach to hierarchically extracting these rules. We evaluate this, combined with a lattice-based decoding, and show improvements over stateof-the-art distortion models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>plain</author>
</authors>
<title>AElnt Ajhzp AlAElAm l bEvp AlAmm AlmtHdp fy syrAlywn An ... En. gloss: announced press release by mission nations united in sierra leone that ... En. ref.: ’a press release by the united nations mission to sierra leone announced that ...’ Ar. reord.: Ajhzp AlAElAm l bEvp AlmtHdp AlAmm fy syrAlywn AElnt An ...</title>
<marker>plain, </marker>
<rawString>Ar. plain.: AElnt Ajhzp AlAElAm l bEvp AlAmm AlmtHdp fy syrAlywn An ... En. gloss: announced press release by mission nations united in sierra leone that ... En. ref.: ’a press release by the united nations mission to sierra leone announced that ...’ Ar. reord.: Ajhzp AlAElAm l bEvp AlmtHdp AlAmm fy syrAlywn AElnt An ...</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Banerjee</author>
<author>A Lavie</author>
</authors>
<title>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>65--72</pages>
<contexts>
<context position="19178" citStr="Banerjee and Lavie, 2005" startWordPosition="3219" endWordPosition="3222">26 POS and 23 syntactic categories. As mentioned above, specific rules are not pruned away due to a limited amount of training material we set the thresholds kp,,,t and kg,,,,,, to relatively low values, 1 and 3, respectively. Evaluation conditions were case-insensitive and with punctuation marks considered. The targetside 4-gram language model was estimated using the SRILM toolkit (Stolcke, 2002) and modified Kneser-Ney discounting with interpolation. The highest BLEU score (Papineni et al., 2002) was chosen as the optimization criterion. Apart from BLEU, a standard automatic measure METEOR (Banerjee and Lavie, 2005) was used for evaluation. 5.4 Results The scores considered are: BLEU scores obtained for the development set as the final point of the MERT procedure (Dev), and BLEU and METEOR scores obtained on test dataset (Test). We present BTEC results (Tables 2), characterized by relatively short sentence length, and the re83 sults obtained on the NIST corpus (Tables 3) with much longer sentences and much need of global reordering. Dev Test BLEU BLEU METEOR Plain 48.31 45.02 65.98 BL 48.46 47.10 68.10 SBR 48.75 47.52 67.33 SBR+lattice 48.90 48.78 68.85 Table 2: Summary of BTEC experimental results. Dev </context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>S. Banerjee and A. Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koehn</author>
<author>I Kuˇcerová</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on ACL</booktitle>
<pages>531--540</pages>
<marker>Koehn, Kuˇcerová, 2005</marker>
<rawString>M. Collins, Ph. Koehn, and I. Kuˇcerová. 2005. Clause restructuring for statistical machine translation. In Proceedings of the 43rd Annual Meeting on ACL 2005, pages 531–540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Costa-jussà</author>
<author>J A R Fonollosa</author>
</authors>
<title>Statistical machine reordering.</title>
<date>2006</date>
<booktitle>In Proceedings of the HLT/EMNLP</booktitle>
<contexts>
<context position="4497" citStr="Costa-jussà and Fonollosa, 2006" startWordPosition="669" endWordPosition="672">approach is described in Nießen and Ney (2004), where morpho-syntactic information was used to account for the reorderings needed. A representative set of similar systems includes: a set of hand-crafted reordering patterns for German-to-English (Collins et al., 2005) and Chinese-English (Wang et al., 2007) translations, emphasizing the distinction between German/Chinese and English clause structure; and statistical machine reordering (SMR) technique where a monotonization of the source words sequence is performed by translating them into the reordered one using well established SMT mechanism (Costa-jussà and Fonollosa, 2006). Coupling of SMR algorithm and the search space extension via generating a set of weighted reordering hypotheses has demonstrated a significant improvement, as shown in Costa-jussà and Fonollosa (2008). The technique proposed in this study is most similar to the one proposed for French-to-English translation task in Xia and McCord (2004), where the authors present a hybrid system for FrenchEnglish translation based on the principle of automatic rewrite patterns extraction using a parse tree and phrase alignments. We propose using a word distortion model not only to monotonize the source part </context>
</contexts>
<marker>Costa-jussà, Fonollosa, 2006</marker>
<rawString>M.R. Costa-jussà and J.A.R. Fonollosa. 2006. Statistical machine reordering. In Proceedings of the HLT/EMNLP 2006.</rawString>
</citation>
<citation valid="false">
<authors>
<author>M R Costa-jussà</author>
<author>J A R Fonollosa</author>
</authors>
<title>Computing multiple weighted reordering hypotheses for a statistical machine translation phrase-based system. In</title>
<date>2008</date>
<booktitle>In Proc. of the AMTA’08,</booktitle>
<pages>123--129</pages>
<location>Honolulu, USA,</location>
<contexts>
<context position="4699" citStr="Costa-jussà and Fonollosa (2008)" startWordPosition="699" endWordPosition="702"> reordering patterns for German-to-English (Collins et al., 2005) and Chinese-English (Wang et al., 2007) translations, emphasizing the distinction between German/Chinese and English clause structure; and statistical machine reordering (SMR) technique where a monotonization of the source words sequence is performed by translating them into the reordered one using well established SMT mechanism (Costa-jussà and Fonollosa, 2006). Coupling of SMR algorithm and the search space extension via generating a set of weighted reordering hypotheses has demonstrated a significant improvement, as shown in Costa-jussà and Fonollosa (2008). The technique proposed in this study is most similar to the one proposed for French-to-English translation task in Xia and McCord (2004), where the authors present a hybrid system for FrenchEnglish translation based on the principle of automatic rewrite patterns extraction using a parse tree and phrase alignments. We propose using a word distortion model not only to monotonize the source part of the corpus (using a different approach to rewrite rule organization from Xia and McCord), but also to extend the search space during decoding. 3 Baseline phrase-based SMT systems The reference system</context>
</contexts>
<marker>Costa-jussà, Fonollosa, 2008</marker>
<rawString>M.R. Costa-jussà and J.A.R. Fonollosa. 2008. Computing multiple weighted reordering hypotheses for a statistical machine translation phrase-based system. In In Proc. of the AMTA’08, Honolulu, USA, October. M.R. Costa-jussà, J.M. Crego, A. de Gispert, P. Lambert, M. Khalilov, J. A. Fonollosa, J.B. Mari no, and R.E. Banchs. 2006. TALP phrase-based system and TALP system combination for IWSLT 2006. In Proceedings of the IWSLT 2006, pages 123–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Crego</author>
<author>J B Mariño</author>
</authors>
<title>Reordering experiments for N-gram-based SMT. In</title>
<date>2006</date>
<booktitle>SLT’06,</booktitle>
<pages>242--245</pages>
<contexts>
<context position="13866" citStr="Crego and Mariño (2006)" startWordPosition="2284" endWordPosition="2287">alignment since it is easier to learn on the modified training example. Example of correct local reordering done with the SBR model can be found in Figure 4. Initial rule: NN@0 NP@1 —* NP@1 NN@0 |NN@0 &lt;&lt; fndq &gt;&gt; NP@1 &lt;&lt; +k &gt;&gt; |p1 Part. lexic. rules: NN@0 NP@1 —* NP@1 NN@0 |NN@0 &lt;&lt; fndq &gt;&gt; NP@1 &lt;&lt; - &gt;&gt; |p2 NN@0 NP@1 —* NP@1 NN@0 |NN@0 &lt;&lt; - &gt;&gt; NP@1 &lt;&lt; +k &gt;&gt; |p3 General rule: NN@0 NP@1 —* NP@1 NN@0 |p4 Figure 3: Example of a lexical rule expansion. 81 4.5 Coupling with decoding In order to improve reordering power of the translation system, we implemented an additional reordering as described in Crego and Mariño (2006). Multiple word segmentations is encoded in a lattice, which is then passed to the input of the decoder, containing reordering alternatives consistent with the previously extracted rules. The decoder takes the n-best reordering of a source sentence coded in the form of a word lattice. This approach is in line with recent research tendencies in SMT, as described for example in (Hildebrand et al., 2008; Xu et al., 2005). Originally, word lattice algorithms do not involve syntax into reordering process, therefore their reordering power is limited at representing long-distance reordering. Our appr</context>
</contexts>
<marker>Crego, Mariño, 2006</marker>
<rawString>J.M. Crego and J. B Mariño. 2006. Reordering experiments for N-gram-based SMT. In SLT’06, pages 242– 245.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Crego</author>
<author>J B Mariño</author>
</authors>
<title>Syntax-enhanced Ngram-based smt.</title>
<date>2007</date>
<booktitle>In Proceedings of MT SUMMIT XI.</booktitle>
<contexts>
<context position="3527" citStr="Crego and Mariño, 2007" startWordPosition="529" endWordPosition="532">ins for the reordering search as described in Kanthak et al. (2005) and Crego et al. (2005). The main criticism of such systems is that the constraints are not lexicalized. Recently there has been interest in SMT exploiting non-monotonic decoding which allow for extension of the search space and linguistic information involvement. The variety of such models includes a constrained distance-based reordering (Costa-jussà et al., 2006); and a constrained version of distortion model where the reordering search problem is tackled through a set of linguistically motivated rules used during decoding (Crego and Mariño, 2007). Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 78–86, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics A quite popular class of reordering algorithms is a monotonization of the source part of the parallel corpus prior to translation. The first work on this approach is described in Nießen and Ney (2004), where morpho-syntactic information was used to account for the reorderings needed. A representative set of similar systems includes: a set of hand-crafted reordering patterns for German-to-English (Collins et al., </context>
</contexts>
<marker>Crego, Mariño, 2007</marker>
<rawString>J.M. Crego and J.B. Mariño. 2007. Syntax-enhanced Ngram-based smt. In Proceedings of MT SUMMIT XI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Crego</author>
<author>J B Mariño</author>
<author>A de Gispert</author>
</authors>
<title>Reordered search and tuple unfolding for ngram-based smt. In</title>
<date>2005</date>
<booktitle>In Proc. of MT SUMMIT X,</booktitle>
<pages>283--289</pages>
<marker>Crego, Mariño, de Gispert, 2005</marker>
<rawString>J.M. Crego, J. B. Mariño, and A. de Gispert. 2005. Reordered search and tuple unfolding for ngram-based smt. In In Proc. of MT SUMMIT X, pages 283–289, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Nießen</author>
<author>H Ney</author>
</authors>
<title>Statistical machine translation with scarce resources using morpho-syntactic information.</title>
<date>2004</date>
<volume>30</volume>
<pages>181--204</pages>
<contexts>
<context position="3911" citStr="Nießen and Ney (2004)" startWordPosition="586" endWordPosition="589">tance-based reordering (Costa-jussà et al., 2006); and a constrained version of distortion model where the reordering search problem is tackled through a set of linguistically motivated rules used during decoding (Crego and Mariño, 2007). Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 78–86, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics A quite popular class of reordering algorithms is a monotonization of the source part of the parallel corpus prior to translation. The first work on this approach is described in Nießen and Ney (2004), where morpho-syntactic information was used to account for the reorderings needed. A representative set of similar systems includes: a set of hand-crafted reordering patterns for German-to-English (Collins et al., 2005) and Chinese-English (Wang et al., 2007) translations, emphasizing the distinction between German/Chinese and English clause structure; and statistical machine reordering (SMR) technique where a monotonization of the source words sequence is performed by translating them into the reordered one using well established SMT mechanism (Costa-jussà and Fonollosa, 2006). Coupling of </context>
</contexts>
<marker>Nießen, Ney, 2004</marker>
<rawString>S. Nießen and H. Ney. 2004. Statistical machine translation with scarce resources using morpho-syntactic information. volume 30, pages 181–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Habash</author>
<author>F Sadat</author>
</authors>
<title>Arabic preprocessing schemes for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL,</booktitle>
<pages>49--52</pages>
<contexts>
<context position="17909" citStr="Habash and Sadat (2006)" startWordPosition="3017" endWordPosition="3020">6 K 55.3 36.3 Table 1: Basic statistics of the BTEC training corpus. The BTEC development dataset consists of 489 sentences and 3.8 K running words, with 6 humanmade reference translations per sentence; the dataset used to test the translation quality has 500 sentences, 4.1 K words and is also provided with 6 reference translations. The NIST50K development set consists of 1353 sentences and 43 K words; the test data contains 1056 sentences and 33 K running words. Both datasets have 4 reference translations per sentence. 5.2 Arabic data preprocessing We took a similar approach to that shown in Habash and Sadat (2006), using the MADA+TOKAN system for disambiguation and tokenization. For disambiguation only diacritic unigram statistics were employed. For tokenization we used the D3 scheme with -TAGBIES option. The scheme splits the following set of clitics: w+, f+, b+, k+, l+, Al+ and pronominal clitics. The -TAGBIES option produces Bies POS tags on all taggable tokens. 5.3 Experimental setup We used the Stanford Parser (Klein and Manning, 2003) for both languages, Penn English Treebank (Marcus et al., 1993) and Penn Arabic Treebank set (Kulick et al., 2006). The English Treebank is provided with 48 POS and</context>
</contexts>
<marker>Habash, Sadat, 2006</marker>
<rawString>N. Habash and F. Sadat. 2006. Arabic preprocessing schemes for statistical machine translation. In Proceedings of the Human Language Technology Conference of the NAACL, pages 49–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A S Hildebrand</author>
<author>K Rottmann</author>
<author>M Noamany</author>
<author>Q Gao</author>
<author>S Hewavitharana</author>
<author>N Bach</author>
<author>S Vogel</author>
</authors>
<title>Recent improvements in the cmu large scale chineseenglish smt system.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT (Companion Volume),</booktitle>
<pages>77--80</pages>
<contexts>
<context position="14269" citStr="Hildebrand et al., 2008" startWordPosition="2351" endWordPosition="2354">Figure 3: Example of a lexical rule expansion. 81 4.5 Coupling with decoding In order to improve reordering power of the translation system, we implemented an additional reordering as described in Crego and Mariño (2006). Multiple word segmentations is encoded in a lattice, which is then passed to the input of the decoder, containing reordering alternatives consistent with the previously extracted rules. The decoder takes the n-best reordering of a source sentence coded in the form of a word lattice. This approach is in line with recent research tendencies in SMT, as described for example in (Hildebrand et al., 2008; Xu et al., 2005). Originally, word lattice algorithms do not involve syntax into reordering process, therefore their reordering power is limited at representing long-distance reordering. Our approach is designed in the spirit of hybrid MT, integrating syntax transfer approach and statistical word lattice methods to achieve better MT performance on the basis of the standard state-of-the-art models. During training a set of word permutation patterns is automatically learned following given word-toword alignment. Since the original and monotonized (reordered) alignments may vary, different sets</context>
</contexts>
<marker>Hildebrand, Rottmann, Noamany, Gao, Hewavitharana, Bach, Vogel, 2008</marker>
<rawString>A.S. Hildebrand, K. Rottmann, M. Noamany, Q. Gao, S. Hewavitharana, N. Bach, and S. Vogel. 2008. Recent improvements in the cmu large scale chineseenglish smt system. In Proceedings of ACL-08: HLT (Companion Volume), pages 77–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Imamura</author>
<author>H Okuma</author>
<author>E Sumita</author>
</authors>
<title>Practical approach to syntax-based statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of MT Summit X,</booktitle>
<pages>267--274</pages>
<contexts>
<context position="2025" citStr="Imamura et al (2005)" startWordPosition="295" endWordPosition="298">slation (SMT) systems do not incorporate any linguistic analysis and work at the surface level of word forms. However, more recently MT systems are moving towards including additional linguistic and syntactic informative sources (for example, source- and/or targetside syntax) into word reordering process. In this paper we propose using a syntactic reordering system operating with fully, partially and non- lexicalized reordering patterns, which are applied on the step 78 prior to translation; the novel idea in this paper is in the derivation of these rules in a hierarchical manner, inspired by Imamura et al (2005). Furthermore, we propose generating a word lattice from the bilingual corpus with the reordered source side, extending the search space on the decoding step. A thorough study of the combination of syntactical and word lattice reordering approaches is another novelty of the paper. 2 Related work Many reordering algorithms have appeared over the past few years. Word class-based reordering was a part of Och’s Alignment Template system (Och et al., 2004); the main criticism of this approach is that it shows bad performance for the pair of languages with very distinct word order. The state-of-the-</context>
<context position="7517" citStr="Imamura et al. (2005)" startWordPosition="1165" endWordPosition="1168"> set of source-side terminals (the lexicon), R is a set of production rules of the form q —* -y, with q E N and -y, which is a sequence of terminal and nonterminal symbols; and S E N is the distinguished symbol. The reordering rules then have the form q0@0 ... qk@k —* qd�@d0 ...qdk@dk|Lexicon|p1 (1) where qi E N for all 0 &lt; i &lt; k; (do ... dk) is a permutation of (0 ... k); Lexicon comes from the source-side set of words for each qi; and pi is a probability associated with the rule. Figure 1 gives two examples of the rule format. 4.2 Rules extraction Concept. Inspired by the ideas presented in Imamura et al. (2005), where monolingual correspondences of syntactic nodes are used during decoding, we extract a set of bilingual patterns allowing for reordering as described below: 79 (1) align the monotone bilingual corpus with GIZA++ (Och and Ney, 2003) and find the intersection of direct and inverse word alignments, resulting in the construction of the projection matrix P (see below)); (2) parse the source and the target parts of the parallel corpus; (3) extract reordering patterns from the parallel non-isomorphic CFG-trees based on the word alignment intersection. Step 2 is straightforward; we explain aspe</context>
</contexts>
<marker>Imamura, Okuma, Sumita, 2005</marker>
<rawString>K. Imamura, H. Okuma, and E. Sumita. 2005. Practical approach to syntax-based statistical machine translation. In Proceedings of MT Summit X, pages 267–274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kanthak</author>
<author>D Vilar</author>
<author>E Matusov</author>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>Novel reordering approaches in phrase-based statistical machine translation. In</title>
<date>2005</date>
<booktitle>In Proc. of the ACL Workshop on Building and Using Parallel Texts,</booktitle>
<pages>167--174</pages>
<contexts>
<context position="2971" citStr="Kanthak et al. (2005)" startWordPosition="445" endWordPosition="448">ave appeared over the past few years. Word class-based reordering was a part of Och’s Alignment Template system (Och et al., 2004); the main criticism of this approach is that it shows bad performance for the pair of languages with very distinct word order. The state-of-the-art SMT system Moses implements a distance-based reordering model (Koehn et al., 2003) and a distortion model, operating with rewrite patterns extracted from a phrase alignment table (Tillman, 2004). Many SMT models implement the brute force approach, introducing several constrains for the reordering search as described in Kanthak et al. (2005) and Crego et al. (2005). The main criticism of such systems is that the constraints are not lexicalized. Recently there has been interest in SMT exploiting non-monotonic decoding which allow for extension of the search space and linguistic information involvement. The variety of such models includes a constrained distance-based reordering (Costa-jussà et al., 2006); and a constrained version of distortion model where the reordering search problem is tackled through a set of linguistically motivated rules used during decoding (Crego and Mariño, 2007). Proceedings of SSST-3, Third Workshop on S</context>
</contexts>
<marker>Kanthak, Vilar, Matusov, Zens, Ney, 2005</marker>
<rawString>S. Kanthak, D. Vilar, E. Matusov, R. Zens, and H. Ney. 2005. Novel reordering approaches in phrase-based statistical machine translation. In In Proc. of the ACL Workshop on Building and Using Parallel Texts, pages 167–174, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the ACL</booktitle>
<pages>423--430</pages>
<contexts>
<context position="18344" citStr="Klein and Manning, 2003" startWordPosition="3086" endWordPosition="3090">sentences and 33 K running words. Both datasets have 4 reference translations per sentence. 5.2 Arabic data preprocessing We took a similar approach to that shown in Habash and Sadat (2006), using the MADA+TOKAN system for disambiguation and tokenization. For disambiguation only diacritic unigram statistics were employed. For tokenization we used the D3 scheme with -TAGBIES option. The scheme splits the following set of clitics: w+, f+, b+, k+, l+, Al+ and pronominal clitics. The -TAGBIES option produces Bies POS tags on all taggable tokens. 5.3 Experimental setup We used the Stanford Parser (Klein and Manning, 2003) for both languages, Penn English Treebank (Marcus et al., 1993) and Penn Arabic Treebank set (Kulick et al., 2006). The English Treebank is provided with 48 POS and 14 syntactic tags, the Arabic Treebank has 26 POS and 23 syntactic categories. As mentioned above, specific rules are not pruned away due to a limited amount of training material we set the thresholds kp,,,t and kg,,,,,, to relatively low values, 1 and 3, respectively. Evaluation conditions were case-insensitive and with punctuation marks considered. The targetside 4-gram language model was estimated using the SRILM toolkit (Stolc</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting of the ACL 2003, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och Koehn</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT-NAACL</booktitle>
<pages>48--54</pages>
<marker>Koehn, Marcu, 2003</marker>
<rawString>Ph. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-based machine translation. In Proceedings of the HLT-NAACL 2003, pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hoang Koehn</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: open-source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>177--180</pages>
<contexts>
<context position="5402" citStr="Koehn et al., 2007" startWordPosition="812" endWordPosition="815">h-to-English translation task in Xia and McCord (2004), where the authors present a hybrid system for FrenchEnglish translation based on the principle of automatic rewrite patterns extraction using a parse tree and phrase alignments. We propose using a word distortion model not only to monotonize the source part of the corpus (using a different approach to rewrite rule organization from Xia and McCord), but also to extend the search space during decoding. 3 Baseline phrase-based SMT systems The reference system which was used as a translation mechanism is the state-of-the-art Moses-based SMT (Koehn et al., 2007). The training and weights tuning procedures can be found on the Moses web page1. Classical phrase-based translation is considered as a three step algorithm: (1) the source sequence of words is segmented into phrases, (2) each phrase is translated into the target language using a translation table, (3) the target phrases are reordered to fit the target language. The probabilities of the phrases are estimated by relative frequencies of their appearance in the training corpus. 1http://www.statmt.org/moses/ In baseline experiments we used a phrase dependent lexicalized reordering model, as propos</context>
</contexts>
<marker>Koehn, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Ph. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: open-source toolkit for statistical machine translation. In Proceedings of ACL 2007, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>388--395</pages>
<contexts>
<context position="21270" citStr="Koehn, 2004" startWordPosition="3556" endWordPosition="3557">g two reordering techniques was more effective than the techniques applied independently and shows an improvement in terms of BLEU for both corpora. The METEOR score is only slightly better for the SBR configurations in case of BTEC task; in the case of NIST50K the METEOR improvement is more evident. The general trend is that automatic scores evaluated on the test set increase with the reordering model complexity. Application of the SBR algorithm only (without a word lattice decoding) does not allow achieving statistical significance threshold for a 95% confidence interval and 1000 resamples (Koehn, 2004) for either of considered corpora. However, the SBR+lattice system configuration outperforms the BL by about 1.7 BLEU points (3.5%) for BTEC task and about 1.4 BLEU point (3.1%) for NIST task. These differences is statistically significant. Figure 6 demonstrates how two reordering techniques interact within a sentence with a need for both global and local word permutations. 5.5 Syntax-based rewrite rules As mentioned above, the SBR operates with three groups of reordering rules, which are the product of complete or partial delexicalization of the originally extracted patterns. The groups are p</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Ph. Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of EMNLP 2004, pages 388–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kulick</author>
<author>R Gabbard</author>
<author>M Marcus</author>
</authors>
<title>Parsing the Arabic Treebank: Analysis and improvements. Treebanks and Linguistic Theories.</title>
<date>2006</date>
<contexts>
<context position="18459" citStr="Kulick et al., 2006" startWordPosition="3108" endWordPosition="3111">ng We took a similar approach to that shown in Habash and Sadat (2006), using the MADA+TOKAN system for disambiguation and tokenization. For disambiguation only diacritic unigram statistics were employed. For tokenization we used the D3 scheme with -TAGBIES option. The scheme splits the following set of clitics: w+, f+, b+, k+, l+, Al+ and pronominal clitics. The -TAGBIES option produces Bies POS tags on all taggable tokens. 5.3 Experimental setup We used the Stanford Parser (Klein and Manning, 2003) for both languages, Penn English Treebank (Marcus et al., 1993) and Penn Arabic Treebank set (Kulick et al., 2006). The English Treebank is provided with 48 POS and 14 syntactic tags, the Arabic Treebank has 26 POS and 23 syntactic categories. As mentioned above, specific rules are not pruned away due to a limited amount of training material we set the thresholds kp,,,t and kg,,,,,, to relatively low values, 1 and 3, respectively. Evaluation conditions were case-insensitive and with punctuation marks considered. The targetside 4-gram language model was estimated using the SRILM toolkit (Stolcke, 2002) and modified Kneser-Ney discounting with interpolation. The highest BLEU score (Papineni et al., 2002) wa</context>
</contexts>
<marker>Kulick, Gabbard, Marcus, 2006</marker>
<rawString>S. Kulick, R. Gabbard, and M. Marcus. 2006. Parsing the Arabic Treebank: Analysis and improvements. Treebanks and Linguistic Theories.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="18408" citStr="Marcus et al., 1993" startWordPosition="3098" endWordPosition="3101">nslations per sentence. 5.2 Arabic data preprocessing We took a similar approach to that shown in Habash and Sadat (2006), using the MADA+TOKAN system for disambiguation and tokenization. For disambiguation only diacritic unigram statistics were employed. For tokenization we used the D3 scheme with -TAGBIES option. The scheme splits the following set of clitics: w+, f+, b+, k+, l+, Al+ and pronominal clitics. The -TAGBIES option produces Bies POS tags on all taggable tokens. 5.3 Experimental setup We used the Stanford Parser (Klein and Manning, 2003) for both languages, Penn English Treebank (Marcus et al., 1993) and Penn Arabic Treebank set (Kulick et al., 2006). The English Treebank is provided with 48 POS and 14 syntactic tags, the Arabic Treebank has 26 POS and 23 syntactic categories. As mentioned above, specific rules are not pruned away due to a limited amount of training material we set the thresholds kp,,,t and kg,,,,,, to relatively low values, 1 and 3, respectively. Evaluation conditions were case-insensitive and with punctuation marks considered. The targetside 4-gram language model was estimated using the SRILM toolkit (Stolcke, 2002) and modified Kneser-Ney discounting with interpolation</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="7755" citStr="Och and Ney, 2003" startWordPosition="1201" endWordPosition="1204">e the form q0@0 ... qk@k —* qd�@d0 ...qdk@dk|Lexicon|p1 (1) where qi E N for all 0 &lt; i &lt; k; (do ... dk) is a permutation of (0 ... k); Lexicon comes from the source-side set of words for each qi; and pi is a probability associated with the rule. Figure 1 gives two examples of the rule format. 4.2 Rules extraction Concept. Inspired by the ideas presented in Imamura et al. (2005), where monolingual correspondences of syntactic nodes are used during decoding, we extract a set of bilingual patterns allowing for reordering as described below: 79 (1) align the monotone bilingual corpus with GIZA++ (Och and Ney, 2003) and find the intersection of direct and inverse word alignments, resulting in the construction of the projection matrix P (see below)); (2) parse the source and the target parts of the parallel corpus; (3) extract reordering patterns from the parallel non-isomorphic CFG-trees based on the word alignment intersection. Step 2 is straightforward; we explain aspects of Steps 1 and 3 in more detail below. Figures 1 and 2 show an example of the extraction of two lexicalized rules for a parallel Arabic-English sentence: Arabic: h*A hW fndq +k English: this is your hotel We use this below in our expl</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>D Gildea</author>
<author>S Khudanpur</author>
<author>A Sarkar</author>
<author>K Yamada</author>
<author>A Fraser</author>
<author>S Kumar</author>
<author>L Shen</author>
<author>D Smith</author>
<author>K Eng</author>
<author>V Jain</author>
<author>Z Jin</author>
<author>D Radev</author>
</authors>
<title>A Smorgasbord of Features for Statistical Machine Translation.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT/NAACL04,</booktitle>
<pages>161--168</pages>
<contexts>
<context position="2480" citStr="Och et al., 2004" startWordPosition="367" endWordPosition="370">d on the step 78 prior to translation; the novel idea in this paper is in the derivation of these rules in a hierarchical manner, inspired by Imamura et al (2005). Furthermore, we propose generating a word lattice from the bilingual corpus with the reordered source side, extending the search space on the decoding step. A thorough study of the combination of syntactical and word lattice reordering approaches is another novelty of the paper. 2 Related work Many reordering algorithms have appeared over the past few years. Word class-based reordering was a part of Och’s Alignment Template system (Och et al., 2004); the main criticism of this approach is that it shows bad performance for the pair of languages with very distinct word order. The state-of-the-art SMT system Moses implements a distance-based reordering model (Koehn et al., 2003) and a distortion model, operating with rewrite patterns extracted from a phrase alignment table (Tillman, 2004). Many SMT models implement the brute force approach, introducing several constrains for the reordering search as described in Kanthak et al. (2005) and Crego et al. (2005). The main criticism of such systems is that the constraints are not lexicalized. Rec</context>
</contexts>
<marker>Och, Gildea, Khudanpur, Sarkar, Yamada, Fraser, Kumar, Shen, Smith, Eng, Jain, Jin, Radev, 2004</marker>
<rawString>F.J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng, V. Jain, Z. Jin, and D. Radev. 2004. A Smorgasbord of Features for Statistical Machine Translation. In Proceedings of HLT/NAACL04, pages 161–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL 2002,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="19056" citStr="Papineni et al., 2002" startWordPosition="3200" endWordPosition="3203">set (Kulick et al., 2006). The English Treebank is provided with 48 POS and 14 syntactic tags, the Arabic Treebank has 26 POS and 23 syntactic categories. As mentioned above, specific rules are not pruned away due to a limited amount of training material we set the thresholds kp,,,t and kg,,,,,, to relatively low values, 1 and 3, respectively. Evaluation conditions were case-insensitive and with punctuation marks considered. The targetside 4-gram language model was estimated using the SRILM toolkit (Stolcke, 2002) and modified Kneser-Ney discounting with interpolation. The highest BLEU score (Papineni et al., 2002) was chosen as the optimization criterion. Apart from BLEU, a standard automatic measure METEOR (Banerjee and Lavie, 2005) was used for evaluation. 5.4 Results The scores considered are: BLEU scores obtained for the development set as the final point of the MERT procedure (Dev), and BLEU and METEOR scores obtained on test dataset (Test). We present BTEC results (Tables 2), characterized by relatively short sentence length, and the re83 sults obtained on the NIST corpus (Tables 3) with much longer sentences and much need of global reordering. Dev Test BLEU BLEU METEOR Plain 48.31 45.02 65.98 BL</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of ACL 2002, pages 311– 318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM: an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the Int. Conf. on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<contexts>
<context position="18953" citStr="Stolcke, 2002" startWordPosition="3188" endWordPosition="3189">2003) for both languages, Penn English Treebank (Marcus et al., 1993) and Penn Arabic Treebank set (Kulick et al., 2006). The English Treebank is provided with 48 POS and 14 syntactic tags, the Arabic Treebank has 26 POS and 23 syntactic categories. As mentioned above, specific rules are not pruned away due to a limited amount of training material we set the thresholds kp,,,t and kg,,,,,, to relatively low values, 1 and 3, respectively. Evaluation conditions were case-insensitive and with punctuation marks considered. The targetside 4-gram language model was estimated using the SRILM toolkit (Stolcke, 2002) and modified Kneser-Ney discounting with interpolation. The highest BLEU score (Papineni et al., 2002) was chosen as the optimization criterion. Apart from BLEU, a standard automatic measure METEOR (Banerjee and Lavie, 2005) was used for evaluation. 5.4 Results The scores considered are: BLEU scores obtained for the development set as the final point of the MERT procedure (Dev), and BLEU and METEOR scores obtained on test dataset (Test). We present BTEC results (Tables 2), characterized by relatively short sentence length, and the re83 sults obtained on the NIST corpus (Tables 3) with much lo</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM: an extensible language modeling toolkit. In Proceedings of the Int. Conf. on Spoken Language Processing, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Tillman</author>
</authors>
<title>A unigram orientation model for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of HLTNAACL’04.</booktitle>
<contexts>
<context position="2823" citStr="Tillman, 2004" startWordPosition="423" endWordPosition="424">ombination of syntactical and word lattice reordering approaches is another novelty of the paper. 2 Related work Many reordering algorithms have appeared over the past few years. Word class-based reordering was a part of Och’s Alignment Template system (Och et al., 2004); the main criticism of this approach is that it shows bad performance for the pair of languages with very distinct word order. The state-of-the-art SMT system Moses implements a distance-based reordering model (Koehn et al., 2003) and a distortion model, operating with rewrite patterns extracted from a phrase alignment table (Tillman, 2004). Many SMT models implement the brute force approach, introducing several constrains for the reordering search as described in Kanthak et al. (2005) and Crego et al. (2005). The main criticism of such systems is that the constraints are not lexicalized. Recently there has been interest in SMT exploiting non-monotonic decoding which allow for extension of the search space and linguistic information involvement. The variety of such models includes a constrained distance-based reordering (Costa-jussà et al., 2006); and a constrained version of distortion model where the reordering search problem </context>
<context position="20100" citStr="Tillman, 2004" startWordPosition="3372" endWordPosition="3373"> the re83 sults obtained on the NIST corpus (Tables 3) with much longer sentences and much need of global reordering. Dev Test BLEU BLEU METEOR Plain 48.31 45.02 65.98 BL 48.46 47.10 68.10 SBR 48.75 47.52 67.33 SBR+lattice 48.90 48.78 68.85 Table 2: Summary of BTEC experimental results. Dev Test BLEU BLEU METEOR Plain 41.83 43.80 62.03 BL 42.68 43.52 62.17 SBR 42.71 44.01 63.29 SBR+lattice 43.05 44.89 63.30 Table 3: Summary of NIST50K experimental results. Four SMT systems are contrasted: BL refers to the Moses baseline system: the training data is not reordered, lexicalized reordering model (Tillman, 2004) is applied; SBR refers to the monotonic system configuration with reordered (SBR) source part; SBR+lattice is the run with reordered source part, on the translation step the input is represented as a word lattice. We also compare the proposed approach with a monotonic system configuration (Plain). It shows the effect of source-reordering and lattice input, also decoded monotonically. Automatic scores obtained on the test dataset evolve similarly when the SBR and word lattice representation applied to BTEC and NIST50K tasks. The combined method coupling two reordering techniques was more effec</context>
</contexts>
<marker>Tillman, 2004</marker>
<rawString>C. Tillman. 2004. A unigram orientation model for statistical machine translation. In Proceedings of HLTNAACL’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Wang</author>
<author>M Collins</author>
<author>P Koehn</author>
</authors>
<title>Chinese syntactic reordering for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Conference on EMNLP.</booktitle>
<contexts>
<context position="4172" citStr="Wang et al., 2007" startWordPosition="623" endWordPosition="626">ird Workshop on Syntax and Structure in Statistical Translation, pages 78–86, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics A quite popular class of reordering algorithms is a monotonization of the source part of the parallel corpus prior to translation. The first work on this approach is described in Nießen and Ney (2004), where morpho-syntactic information was used to account for the reorderings needed. A representative set of similar systems includes: a set of hand-crafted reordering patterns for German-to-English (Collins et al., 2005) and Chinese-English (Wang et al., 2007) translations, emphasizing the distinction between German/Chinese and English clause structure; and statistical machine reordering (SMR) technique where a monotonization of the source words sequence is performed by translating them into the reordered one using well established SMT mechanism (Costa-jussà and Fonollosa, 2006). Coupling of SMR algorithm and the search space extension via generating a set of weighted reordering hypotheses has demonstrated a significant improvement, as shown in Costa-jussà and Fonollosa (2008). The technique proposed in this study is most similar to the one propose</context>
</contexts>
<marker>Wang, Collins, Koehn, 2007</marker>
<rawString>C. Wang, M. Collins, and P. Koehn. 2007. Chinese syntactic reordering for statistical machine translation. In Proceedings of the Joint Conference on EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Xia</author>
<author>M McCord</author>
</authors>
<title>Improving a statistical mt system with automatically learned rewrite patterns.</title>
<date>2004</date>
<booktitle>In Proceedings of the COLING</booktitle>
<contexts>
<context position="4837" citStr="Xia and McCord (2004)" startWordPosition="721" endWordPosition="724">etween German/Chinese and English clause structure; and statistical machine reordering (SMR) technique where a monotonization of the source words sequence is performed by translating them into the reordered one using well established SMT mechanism (Costa-jussà and Fonollosa, 2006). Coupling of SMR algorithm and the search space extension via generating a set of weighted reordering hypotheses has demonstrated a significant improvement, as shown in Costa-jussà and Fonollosa (2008). The technique proposed in this study is most similar to the one proposed for French-to-English translation task in Xia and McCord (2004), where the authors present a hybrid system for FrenchEnglish translation based on the principle of automatic rewrite patterns extraction using a parse tree and phrase alignments. We propose using a word distortion model not only to monotonize the source part of the corpus (using a different approach to rewrite rule organization from Xia and McCord), but also to extend the search space during decoding. 3 Baseline phrase-based SMT systems The reference system which was used as a translation mechanism is the state-of-the-art Moses-based SMT (Koehn et al., 2007). The training and weights tuning p</context>
</contexts>
<marker>Xia, McCord, 2004</marker>
<rawString>F. Xia and M. McCord. 2004. Improving a statistical mt system with automatically learned rewrite patterns. In Proceedings of the COLING 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Xu</author>
<author>E Matusov</author>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>Integrated chinese word segmentation in statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of IWSLT</booktitle>
<contexts>
<context position="14287" citStr="Xu et al., 2005" startWordPosition="2355" endWordPosition="2358">xical rule expansion. 81 4.5 Coupling with decoding In order to improve reordering power of the translation system, we implemented an additional reordering as described in Crego and Mariño (2006). Multiple word segmentations is encoded in a lattice, which is then passed to the input of the decoder, containing reordering alternatives consistent with the previously extracted rules. The decoder takes the n-best reordering of a source sentence coded in the form of a word lattice. This approach is in line with recent research tendencies in SMT, as described for example in (Hildebrand et al., 2008; Xu et al., 2005). Originally, word lattice algorithms do not involve syntax into reordering process, therefore their reordering power is limited at representing long-distance reordering. Our approach is designed in the spirit of hybrid MT, integrating syntax transfer approach and statistical word lattice methods to achieve better MT performance on the basis of the standard state-of-the-art models. During training a set of word permutation patterns is automatically learned following given word-toword alignment. Since the original and monotonized (reordered) alignments may vary, different sets of reordering pat</context>
</contexts>
<marker>Xu, Matusov, Zens, Ney, 2005</marker>
<rawString>J. Xu, E. Matusov, R. Zens, and H. Ney. 2005. Integrated chinese word segmentation in statistical machine translation. In Proc. of IWSLT 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yamada</author>
<author>K Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>523--530</pages>
<contexts>
<context position="12084" citStr="Yamada and Knight, 2001" startWordPosition="1971" endWordPosition="1974">ut functioning as an input to the next rule can lead to situations reverting the change of word order that the previously applied rule made. Therefore, the rules that can be ambiguous when applied sequentially during decoding are pruned according to the higher probability principle. For example, for the pair of patterns with the same lexicon (which is empty for a general rule leading to a recurring contradiction NP@0 VP@1 —* VP@1 NP@0 p1, VP@0 NP@1 —* NP@1 VP@0 p2 ), the less probable rule is removed. Finally, there are three resulting parameter tables analogous to the &amp;quot;r-table&amp;quot; as stated in (Yamada and Knight, 2001), consisting of POS- and constituentbased patterns allowing for reordering and monotone distortion (examples can be found in Table 5). 4.4 Source-side monotonization Rule application is performed as a bottom-up parse tree traversal following two principles: (1) the longest possible rule is applied, i.e. among a set of nested rules, the rule with a longest left-side covering is selected. For example, in the case of the appearance of an NN JJ RB sequence and presence of the two reordering rules NN@0 JJ@1 —* ... and NN@0 JJ@1 RB@2 —* ... the latter pattern will be applied. (2) the rule containing</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>K. Yamada and K. Knight. 2001. A syntax-based statistical translation model. In Proceedings of ACL 2001, pages 523–530.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>