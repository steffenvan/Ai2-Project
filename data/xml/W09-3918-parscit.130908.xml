<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002824">
<title confidence="0.983614">
On NoMatchs, NoInputs and BargeIns:
Do Non-Acoustic Features Support Anger Detection?
</title>
<author confidence="0.629216">
Jackson Liscombe
</author>
<affiliation confidence="0.250494">
SpeechCycle, Inc.
</affiliation>
<address confidence="0.660248">
Broadway 26
New York City, USA
</address>
<email confidence="0.988372">
jackson@speechcycle.com
</email>
<author confidence="0.996187">
Alexander Schmitt, Tobias Heinroth
</author>
<affiliation confidence="0.8942025">
Dialogue Systems Research Group
Institute for Information Technology
Ulm University, Germany
alexander.schmitt@uni-ulm.de
</affiliation>
<email confidence="0.623878">
tobias.heinroth@uni-ulm.de
</email>
<sectionHeader confidence="0.984423" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999752333333333">
Most studies on speech-based emotion
recognition are based on prosodic and
acoustic features, only employing artifi-
cial acted corpora where the results cannot
be generalized to telephone-based speech
applications. In contrast, we present an
approach based on utterances from 1,911
calls from a deployed telephone-based
speech application, taking advantage of
additional dialogue features, NLU features
and ASR features that are incorporated
into the emotion recognition process. De-
pending on the task, non-acoustic features
add 2.3% in classification accuracy com-
pared to using only acoustic features.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999973181818182">
Certainly, the most relevant employment of
speech-based emotion recognition is that of a
telephone-based Interactive Voice Response Sys-
tem (IVR).
Emotion recognition for IVR differs insofar
to “traditional” emotion recognition, that it can
be reduced to a binary classification problem,
namely the distinction between angry and non-
angry whereas studies on speech-based emotion
recognition analyze complete and relatively long
sentences covering the full bandwidth of human
emotions. In a way, emotion recognition in the
telephone domain is less challenging since a dis-
tinction between two different emotion classes,
angry and non-angry, is sufficient. We don’t have
to expect callers talking to IVRs in a sad, anxious,
happy, disgusted or bored manner. I.e., even if a
caller is happy, the effect on the dialogue will be
the same as if he is neutral. However, there still
remain challenges for the system developer such
as varying speech quality caused by, e.g., vary-
ing distance to the receiver during the call lead-
ing to loudness variations (which emotion recog-
nizers might mistakenly interpret as anger). But
also bandwidth limitation introduced by the tele-
phone channel and a strongly unbalanced distribu-
tion of non-angry and angry utterances with more
than 80% non-angry utterances make a reliable
distinction of the caller emotion difficult. While
hot anger with studio quality conditions can be de-
termined with over 90% (Pittermann et al., 2009)
studies on IVR anger recognition report lower ac-
curacies due to these limitations. However, there
is one advantage of anger recognition in IVR sys-
tems that can be exploited: additional information
is available from the dialogue context, the speech
recognizer and the natural language parser.
This contribution is organized as follows: first,
we introduce related work and describe our cor-
pus. In Section 4 we outline our employed features
with emphasis on the non-acoustic ones. Experi-
ments are shown in Section 5 where we analyze
the impact of the newly developed features before
we summarize our work in Section 6.
</bodyText>
<sectionHeader confidence="0.999766" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999787909090909">
Speech-based emotion research regarding tele-
phone applications has been increasingly dis-
cussed in the speech community. While in early
studies acted corpora were used, such as in (Ya-
coub et al., 2003), training and testing data in later
studies has been more and more based on real-
life data, see (Burkhardt et al., 2008),(Burkhardt
et al., 2009). Most studies are limited to acous-
tic/prosodic features that have been extracted out
of the audio data. Linguistic information was ad-
ditionaly exploited in (Lee et al., 2002) resulting in
</bodyText>
<subsubsectionHeader confidence="0.642129">
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 128–131,
</subsubsectionHeader>
<affiliation confidence="0.938232">
Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics
</affiliation>
<page confidence="0.999448">
128
</page>
<bodyText confidence="0.9988872">
a 45.7% accuracy improvement compared to using
only acoustic features. In (Liscombe et al., 2005)
the lexical and prosodic features were additionaly
enriched with dialogue act features leading to an
increase in accuracy of 2.3%.
</bodyText>
<sectionHeader confidence="0.969512" genericHeader="method">
3 Corpus Description
</sectionHeader>
<bodyText confidence="0.999961181818182">
For our studies we employed a corpus of 1,911
calls from an automated agent helping to resolve
internet-related problems comprising 22,724 utter-
ances. Three labelers divided the corpus into an-
gry, annoyed and non-angry utterances (Cohen’s
n = 0.70 on whole corpus; L1 vs. L2 n = 0.8,
L1 vs. L3 n = 0.71, L2 vs. L3 n = 0.59). The
reason for choosing three emotion classes instead
of a binary classification lies in the hope to find
clearer patterns for strong anger. A distinction be-
tween non-angry and somewhat annoyed callers
is rather difficult even for humans. The final la-
bel was defined based on majority voting resulting
in 90.2% non-angry, 5.1% garbage, 3.4% annoyed
and 0.7% angry utterances. 0.6% of the samples in
the corpus were sorted out since all three raters had
different opinions. The raters were asked to label
“garbage” when the utterance is incomprehensible
or consists of non-speech events. While the num-
ber of angry and annoyed utterances seems very
low, 429 calls (i.e. 22.4%) contained annoyed or
angry utterances.
</bodyText>
<sectionHeader confidence="0.999375" genericHeader="method">
4 Features
</sectionHeader>
<bodyText confidence="0.987507714285714">
We created two different feature sets: one based
on typical acoustic/prosodic features and another
one to which we will refer as ’non-acoustic’ fea-
tures consisting of features from the Automatich
Speech Recognition (ASR), Natural Language
Understanding (NLU), Dialogue Manager (DM)
and Context features.
</bodyText>
<subsectionHeader confidence="0.998482">
4.1 Acoustic Features
</subsectionHeader>
<bodyText confidence="0.9999892">
The acoustic/prosodic features were extracted
with the aid of Praat (Boersma, 2001) and con-
sist of power, mean, rms, mean harmonicity, pitch
(mean, deviation, voiced frames, time step, mean
slope, minimum, maximum, range), voiced pitch
(mean, minimum mean, maximum mean, range),
intensity (mean, maximum, minimum, deviation,
range), jitter points, formants 1-5, MFCC 1-12.
The extraction was performed on the complete
short utterance.
</bodyText>
<subsectionHeader confidence="0.963993">
4.2 Non-Acoustic Features
</subsectionHeader>
<bodyText confidence="0.99992362">
The second, i.e. non-acoustic, feature set is based
on features logged with the aid of the speech plat-
form hosting the IVR application and is presented
here in more detail. They include:
ASR features: raw ASR transcription of
caller’s utterance (Utterance) (unigram bag-of-
words); ASR confidence of returned utterance
transcription, as floating point number between 0
(least confident) and 1 (most confident) (Confi-
dence); names of all grammars active (Grammar-
Name); name of the grammar that returned the
parse (TriggeredGrammarName); did the caller
begin speaking before the prompt completed?
(’yes’, ’no’) (BargedIn); did the caller communi-
cate with speech (’voice’) or keypad (’dtmf’) (In-
putModeName); was the speech recognizer suc-
cessful (’Complete’) or not and if it was not suc-
cessful, an error message is recorded such as
’NoInput’ or ’NoMatch’ (RecognitionStatus)
NLU-Features: the semantic parse of the caller
utterance as returned by the activated grammar in
the current dialog module (Interpretation); given
caller speech input, we need to try and recognize
the semantic meaning. The first time we try to do
this, this is indicated with a value of ’Initial’. If
we were not returned a parse then we have to re-
prompt (’Retry1’ or ’Timeout1’). Similar for if the
caller asks for help or a repetition of the prompt.
Etc. (LoopName)
DM-Features: the text of what the auto-
mated agent said prior to recording the user input
(PromptName); the number of tries to elicit a de-
sired response. Integer values range from 0 (first
try) to 7 (6th try) (RoleIndex); an activity may re-
quest substantive user input (’Collection’) or con-
firm previous substantive input (’Confirmation’)
(RoleName); within a call each event is sequen-
tially organized by these numbers (SequenceID);
the name of the activity (aka dialog module) that
is active (ActivityName); type of activity. Possible
values are: Question, PlatformValue, Announce-
ment, Wait, Escalate (ActivityType)
Context-Features: We further developed addi-
tional cumulative features based on the previous
ones in order to keep track of the NoMatch, NoIn-
puts and similar parameters serving as an indicator
for the call quality: number of non-empty NLU
parses (CumUserTurns); number of statements
and questions by the system (CumSysTurns); num-
ber of questions (CumSysQuestions); number of
</bodyText>
<page confidence="0.997408">
129
</page>
<bodyText confidence="0.9994094">
help requests by the user (CumHelpReq); num-
ber of operator requests (CumOperatorReq); num-
ber of NoInput events (CumNoInputs); number
of NoMatch events (CumNoMatchs) number of
BargeIns (CumBargeIns).
</bodyText>
<sectionHeader confidence="0.998713" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.947337872340426">
In order to prevent an adaption of the anger model
to specific callers we seperated the corpus ran-
domly into 75% training and 25% testing material
and ensured that no speaker contained in training
was used for testing. To exclude that we receive a
good classification result by chance, we performed
50 iterations in each test and calculated the per-
formance’s mean and standard deviation over all
iterations.
Note, that our aim in this study is less finding
an optimum classifer, than finding additional fea-
tures that support the distinction between angry
and non-angry callers. Support Vector Machines
and Artificial Neural Networks are thus not con-
sidered, although the best performances are re-
ported with those learning algorithms. A simi-
lar performance, i.e. only slightly poorer, can be
reached with Rule Learners. They enable a thor-
ough study of the features, leading to the decision
for one or the other class, since they produce a
human readable set of if-then-else rules. Our hy-
potheses on a perfect feature set can thus easily be
confirmed or rejected.
We performed experiments with two differ-
ent classes: ’angry’ vs. ’non-angry’ and ’an-
gry+annoyed’ vs. ’non-angry’. Merging angry
and annoyed utterances aims on finding all callers,
where the customer satisfaction is endangered. In
both tasks, we employ a) only acoustic features
b) only ASR/NLU/DM/Context features and c) a
combination of both feature sets. The number of
utterances used for training and testing is shown in
Table 1.
As result we expect acoustic features to per-
form better than non-acoustic features. Among
the relevant non-acoustic features we assume as
an indicator for angry utterances low ASR confi-
dences and high barge-in rates, which we consider
as signal for the caller’s impatience. All tests have
been performed with the machine learning frame-
work RapidMiner (Mierswa et al., 2006) featuring
all common supervised and unsupervised learning
schemes.
Results are listed in Table 2, including preci-
Test A
angry+
annoyed non-a.
</bodyText>
<table confidence="0.883279">
Training — 320 — 320 — 80 — 80
Testing — 140 — 140 — 40 — 40
</table>
<tableCaption confidence="0.981947">
Table 1: Number of utterances employed for both
</tableCaption>
<bodyText confidence="0.959298875">
tests per iteration. Since the samples are selected
randomly and the corpus was separated by speak-
ers before training and testing, the numbers may
vary in each iteration.
sion and recall values. As expected, Test B (an-
gry vs. non-angry) has the highest accuracy with
87.23% since the patterns are more clearly sep-
arable compared to Test A (annoyed vs. non-
angry, 72.57%). Obviously, adding non-acoustic
features increases classification accuracy signifi-
cantly, but only where the acoustic features are
not expressive enough. While the additional in-
formation increases the accuracy of the combined
angry+annoyed task by 2.3 % (Test A), it does
not advance the distinction between only angry vs.
non-angry (Test B).
</bodyText>
<subsectionHeader confidence="0.996606">
5.1 Emotional History
</subsectionHeader>
<bodyText confidence="0.999912909090909">
One could expect, that the probability of
an angry/annoyed turn following another an-
gry/annoyed turn is rather high and that this in-
formation could be exploited. Thus, we further
included two features PrevEmotion and PrevPre-
vEmotion, taking into account the two previous
hand-labeled emotions in the dialogue discourse.
If they would contribute to the recognition pro-
cess, we would replace them by automatically la-
belled ones. All test results, however, did not im-
prove.
</bodyText>
<subsectionHeader confidence="0.999701">
5.2 Ruleset Analysis
</subsectionHeader>
<bodyText confidence="0.966697071428571">
For a determination of the relevant features in the
non-acoustic feature set, we analyzed the ruleset
generated by the RuleLearner in Test A. Interest-
ingly, a dominant feature in the resulting ruleset is
’AudioDuration’. While shorter utterances were
assigned to non-angry (about &lt;2s), longer utter-
ances tended to be assigned to angry/annoyed. A
following analysis of the utterance length confirms
this rule: utterances labeled as angry averaged
2.07 (+/-0.73) seconds, annoyed utterances lasted
1.82 (+/-0.57) s and non-angry samples were 1.57
(+/- 0.66) s in average. The number of NoMatch
Test B
angry non-a.
</bodyText>
<page confidence="0.978315">
130
</page>
<table confidence="0.998853625">
Test A: Angry/Annoyed vs. Non-angry only Acoustic only Non-Acoustic both
Accuracy 70.29 (+-2.94) % 61.43 (+-2.75) % 72.57 (+-2.37) %
Precision/Recall Class ’Ang./Ann.’ 71.51% / 61.57% 68.35% / 42.57% 73.67% / 70.14%
Precision/Recall Class ’Non-angry’ 69.19% / 73.00% 58.30% / 80.29% 71.57% / 75.00%
Test B: Angry vs. Non-angry only Acoustic only Non-Acoustic both
Accuracy 87.06 (+-3.76) % 64.29 (+-1.32) % 87.23 (+-3.72) %
Precision/Recall Class ’Angry’ 87.13% / 86.55% 66.0% / 58.9% 86.88% / 87.11%
Precision/Recall Class ’Non-angry’ 86.97% / 87.53% 62.9% 69.9% 87.55% / 87.33%
</table>
<tableCaption confidence="0.999149">
Table 2: Classification results for angry+annoyed vs. non-angry and angry vs. non-angry utterances.
</tableCaption>
<bodyText confidence="0.999834363636364">
events (CumNoMatch) up to the angry turn played
a less dominant role than expected: only 8 samples
were assigned to angry/annoyed due to reoccur-
ring NoMatch events (&gt;5 NoMatchs). Utterances
that contained ’Operator’, ’Agent’ or ’Help’ were,
as expected, assigned to angry/annoyed, however,
in combination with high AudioDuration values
(&gt;2s). Non-angry utterances were typically better
recognized: average ASR confidence values are
0.82 (+/-0.288) (non-angry), 0.71 (+/- 0.36) (an-
noyed) and 0.56 (+/- 0.41) (angry).
</bodyText>
<sectionHeader confidence="0.995165" genericHeader="conclusions">
6 Conclusion and Discussion
</sectionHeader>
<bodyText confidence="0.999994176470589">
In IVR systems, we can take advantage of non-
acoustic information, that comes from the dia-
logue context. As demonstrated in this work,
ASR, NLU, DM and contextual features sup-
port the distinction between angry and non-angry
callers. However, where the samples can be sepa-
rated into clear patterns, such as in Test B, no ben-
efit from the additional feature set can be expected.
In what sense a late fusion of linguistic, dialogue
and context features would improve the classifier,
i.e. by building various subsystems whose opin-
ions are subject to a voting mechanism, will be
evaluated in future work. We will also analyze
why the linguistic features did not have any vis-
ible impact on the classifier. Presumably a combi-
nation of n-grams, bag-of-words and bag of emo-
tional salience will improve classification.
</bodyText>
<sectionHeader confidence="0.999294" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999742">
We would like to take the opportunity to thank the
following colleagues for contributing to the devel-
opment of our emotion recognizer: Ulrich Tschaf-
fon, Shu Ding and Alexey Indiryakov.
</bodyText>
<sectionHeader confidence="0.999185" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999955878787879">
Paul Boersma. 2001. Praat, a System for Do-
ing Phonetics by Computer. Glot International,
5(9/10):341–345.
Felix Burkhardt, Richard Huber, and Joachim
Stegmann. 2008. Advances in anger detection with
real life data.
Felix Burkhardt, Tim Polzehl, Joachim Stegmann, Flo-
rian Metze, and Richard Huber. 2009. Detecting
real life anger. In Proc. of ICASSP, April.
Chul Min Lee, Shrikanth Narayanan, and Roberto Pier-
accini. 2002. Combining Acoustic and Language
Information for Emotion Recognition. In Interna-
tional Conference on Speech and Language Process-
ing (ICSLP), Denver, USA, October.
Jackson Liscombe, Guiseppe Riccardi, and Dilek
Hakkani-T¨ur. 2005. Using Context to Improve
Emotion Detection in Spoken Dialog Systems. In
International Conference on Speech and Language
Processing (ICSLP), Lisbon, Portugal, September.
Ingo Mierswa, Michael Wurst, Ralf Klinkenberg, Mar-
tin Scholz, and Timm Euler. 2006. Yale: Rapid
prototyping for complex data mining tasks. In KDD
’06: Proceedings of the 12th ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining, New York, NY, USA, August.
Johannes Pittermann, A. Pittermann, and Wolfgang
Minker. 2009. Handling Emotions in Human-
Computer Dialogues. Text, Speech and Language
Technology. Springer, Dordrecht (The Netherlands).
Sherif Yacoub, Steven Simske, Xiaofan Lin, and John
Burns. 2003. Recognition of emotions in interac-
tive voice response systems. In Proc. Eurospeech,
Geneva, pages 1–4.
</reference>
<page confidence="0.998309">
131
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.146853">
<title confidence="0.975362">On NoMatchs, NoInputs and BargeIns: Do Non-Acoustic Features Support Anger Detection?</title>
<author confidence="0.738401">Jackson</author>
<affiliation confidence="0.40264">SpeechCycle,</affiliation>
<address confidence="0.361229">Broadway</address>
<author confidence="0.7015">New York City</author>
<email confidence="0.999691">jackson@speechcycle.com</email>
<author confidence="0.99703">Alexander Schmitt</author>
<author confidence="0.99703">Tobias</author>
<affiliation confidence="0.999734666666667">Dialogue Systems Research Institute for Information Ulm University,</affiliation>
<email confidence="0.995925">tobias.heinroth@uni-ulm.de</email>
<abstract confidence="0.98814725">Most studies on speech-based emotion recognition are based on prosodic and acoustic features, only employing artificial acted corpora where the results cannot be generalized to telephone-based speech applications. In contrast, we present an approach based on utterances from 1,911 calls from a deployed telephone-based speech application, taking advantage of additional dialogue features, NLU features and ASR features that are incorporated into the emotion recognition process. Depending on the task, non-acoustic features add 2.3% in classification accuracy compared to using only acoustic features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Paul Boersma</author>
</authors>
<title>Praat, a System for Doing Phonetics by Computer.</title>
<date>2001</date>
<pages>5--9</pages>
<publisher>Glot International,</publisher>
<contexts>
<context position="5541" citStr="Boersma, 2001" startWordPosition="846" endWordPosition="847">he utterance is incomprehensible or consists of non-speech events. While the number of angry and annoyed utterances seems very low, 429 calls (i.e. 22.4%) contained annoyed or angry utterances. 4 Features We created two different feature sets: one based on typical acoustic/prosodic features and another one to which we will refer as ’non-acoustic’ features consisting of features from the Automatich Speech Recognition (ASR), Natural Language Understanding (NLU), Dialogue Manager (DM) and Context features. 4.1 Acoustic Features The acoustic/prosodic features were extracted with the aid of Praat (Boersma, 2001) and consist of power, mean, rms, mean harmonicity, pitch (mean, deviation, voiced frames, time step, mean slope, minimum, maximum, range), voiced pitch (mean, minimum mean, maximum mean, range), intensity (mean, maximum, minimum, deviation, range), jitter points, formants 1-5, MFCC 1-12. The extraction was performed on the complete short utterance. 4.2 Non-Acoustic Features The second, i.e. non-acoustic, feature set is based on features logged with the aid of the speech platform hosting the IVR application and is presented here in more detail. They include: ASR features: raw ASR transcription</context>
</contexts>
<marker>Boersma, 2001</marker>
<rawString>Paul Boersma. 2001. Praat, a System for Doing Phonetics by Computer. Glot International, 5(9/10):341–345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix Burkhardt</author>
<author>Richard Huber</author>
<author>Joachim Stegmann</author>
</authors>
<title>Advances in anger detection with real life data.</title>
<date>2008</date>
<contexts>
<context position="3391" citStr="Burkhardt et al., 2008" startWordPosition="506" endWordPosition="509">lows: first, we introduce related work and describe our corpus. In Section 4 we outline our employed features with emphasis on the non-acoustic ones. Experiments are shown in Section 5 where we analyze the impact of the newly developed features before we summarize our work in Section 6. 2 Related Work Speech-based emotion research regarding telephone applications has been increasingly discussed in the speech community. While in early studies acted corpora were used, such as in (Yacoub et al., 2003), training and testing data in later studies has been more and more based on reallife data, see (Burkhardt et al., 2008),(Burkhardt et al., 2009). Most studies are limited to acoustic/prosodic features that have been extracted out of the audio data. Linguistic information was additionaly exploited in (Lee et al., 2002) resulting in Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 128–131, Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics 128 a 45.7% accuracy improvement compared to using only acoustic features. In (Liscombe et al., 2005) the lexical and prosodic features were additionaly enriched w</context>
</contexts>
<marker>Burkhardt, Huber, Stegmann, 2008</marker>
<rawString>Felix Burkhardt, Richard Huber, and Joachim Stegmann. 2008. Advances in anger detection with real life data.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix Burkhardt</author>
<author>Tim Polzehl</author>
<author>Joachim Stegmann</author>
<author>Florian Metze</author>
<author>Richard Huber</author>
</authors>
<title>Detecting real life anger.</title>
<date>2009</date>
<booktitle>In Proc. of ICASSP,</booktitle>
<contexts>
<context position="3416" citStr="Burkhardt et al., 2009" startWordPosition="509" endWordPosition="512"> related work and describe our corpus. In Section 4 we outline our employed features with emphasis on the non-acoustic ones. Experiments are shown in Section 5 where we analyze the impact of the newly developed features before we summarize our work in Section 6. 2 Related Work Speech-based emotion research regarding telephone applications has been increasingly discussed in the speech community. While in early studies acted corpora were used, such as in (Yacoub et al., 2003), training and testing data in later studies has been more and more based on reallife data, see (Burkhardt et al., 2008),(Burkhardt et al., 2009). Most studies are limited to acoustic/prosodic features that have been extracted out of the audio data. Linguistic information was additionaly exploited in (Lee et al., 2002) resulting in Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 128–131, Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics 128 a 45.7% accuracy improvement compared to using only acoustic features. In (Liscombe et al., 2005) the lexical and prosodic features were additionaly enriched with dialogue act features</context>
</contexts>
<marker>Burkhardt, Polzehl, Stegmann, Metze, Huber, 2009</marker>
<rawString>Felix Burkhardt, Tim Polzehl, Joachim Stegmann, Florian Metze, and Richard Huber. 2009. Detecting real life anger. In Proc. of ICASSP, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chul Min Lee</author>
<author>Shrikanth Narayanan</author>
<author>Roberto Pieraccini</author>
</authors>
<title>Combining Acoustic and Language Information for Emotion Recognition.</title>
<date>2002</date>
<booktitle>In International Conference on Speech and Language Processing (ICSLP),</booktitle>
<location>Denver, USA,</location>
<contexts>
<context position="3591" citStr="Lee et al., 2002" startWordPosition="537" endWordPosition="540">impact of the newly developed features before we summarize our work in Section 6. 2 Related Work Speech-based emotion research regarding telephone applications has been increasingly discussed in the speech community. While in early studies acted corpora were used, such as in (Yacoub et al., 2003), training and testing data in later studies has been more and more based on reallife data, see (Burkhardt et al., 2008),(Burkhardt et al., 2009). Most studies are limited to acoustic/prosodic features that have been extracted out of the audio data. Linguistic information was additionaly exploited in (Lee et al., 2002) resulting in Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 128–131, Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics 128 a 45.7% accuracy improvement compared to using only acoustic features. In (Liscombe et al., 2005) the lexical and prosodic features were additionaly enriched with dialogue act features leading to an increase in accuracy of 2.3%. 3 Corpus Description For our studies we employed a corpus of 1,911 calls from an automated agent helping to resolve internet-relat</context>
</contexts>
<marker>Lee, Narayanan, Pieraccini, 2002</marker>
<rawString>Chul Min Lee, Shrikanth Narayanan, and Roberto Pieraccini. 2002. Combining Acoustic and Language Information for Emotion Recognition. In International Conference on Speech and Language Processing (ICSLP), Denver, USA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jackson Liscombe</author>
<author>Guiseppe Riccardi</author>
<author>Dilek Hakkani-T¨ur</author>
</authors>
<title>Using Context to Improve Emotion Detection in Spoken Dialog Systems.</title>
<date>2005</date>
<booktitle>In International Conference on Speech and Language Processing (ICSLP),</booktitle>
<location>Lisbon, Portugal,</location>
<marker>Liscombe, Riccardi, Hakkani-T¨ur, 2005</marker>
<rawString>Jackson Liscombe, Guiseppe Riccardi, and Dilek Hakkani-T¨ur. 2005. Using Context to Improve Emotion Detection in Spoken Dialog Systems. In International Conference on Speech and Language Processing (ICSLP), Lisbon, Portugal, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ingo Mierswa</author>
<author>Michael Wurst</author>
<author>Ralf Klinkenberg</author>
<author>Martin Scholz</author>
<author>Timm Euler</author>
</authors>
<title>Yale: Rapid prototyping for complex data mining tasks.</title>
<date>2006</date>
<booktitle>In KDD ’06: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<location>New York, NY, USA,</location>
<contexts>
<context position="10333" citStr="Mierswa et al., 2006" startWordPosition="1592" endWordPosition="1595"> the customer satisfaction is endangered. In both tasks, we employ a) only acoustic features b) only ASR/NLU/DM/Context features and c) a combination of both feature sets. The number of utterances used for training and testing is shown in Table 1. As result we expect acoustic features to perform better than non-acoustic features. Among the relevant non-acoustic features we assume as an indicator for angry utterances low ASR confidences and high barge-in rates, which we consider as signal for the caller’s impatience. All tests have been performed with the machine learning framework RapidMiner (Mierswa et al., 2006) featuring all common supervised and unsupervised learning schemes. Results are listed in Table 2, including preciTest A angry+ annoyed non-a. Training — 320 — 320 — 80 — 80 Testing — 140 — 140 — 40 — 40 Table 1: Number of utterances employed for both tests per iteration. Since the samples are selected randomly and the corpus was separated by speakers before training and testing, the numbers may vary in each iteration. sion and recall values. As expected, Test B (angry vs. non-angry) has the highest accuracy with 87.23% since the patterns are more clearly separable compared to Test A (annoyed </context>
</contexts>
<marker>Mierswa, Wurst, Klinkenberg, Scholz, Euler, 2006</marker>
<rawString>Ingo Mierswa, Michael Wurst, Ralf Klinkenberg, Martin Scholz, and Timm Euler. 2006. Yale: Rapid prototyping for complex data mining tasks. In KDD ’06: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, New York, NY, USA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Pittermann</author>
<author>A Pittermann</author>
<author>Wolfgang Minker</author>
</authors>
<title>Handling Emotions in HumanComputer Dialogues. Text, Speech and Language Technology.</title>
<date>2009</date>
<publisher>Springer,</publisher>
<location>Dordrecht (The Netherlands).</location>
<contexts>
<context position="2438" citStr="Pittermann et al., 2009" startWordPosition="349" endWordPosition="352">e as if he is neutral. However, there still remain challenges for the system developer such as varying speech quality caused by, e.g., varying distance to the receiver during the call leading to loudness variations (which emotion recognizers might mistakenly interpret as anger). But also bandwidth limitation introduced by the telephone channel and a strongly unbalanced distribution of non-angry and angry utterances with more than 80% non-angry utterances make a reliable distinction of the caller emotion difficult. While hot anger with studio quality conditions can be determined with over 90% (Pittermann et al., 2009) studies on IVR anger recognition report lower accuracies due to these limitations. However, there is one advantage of anger recognition in IVR systems that can be exploited: additional information is available from the dialogue context, the speech recognizer and the natural language parser. This contribution is organized as follows: first, we introduce related work and describe our corpus. In Section 4 we outline our employed features with emphasis on the non-acoustic ones. Experiments are shown in Section 5 where we analyze the impact of the newly developed features before we summarize our w</context>
</contexts>
<marker>Pittermann, Pittermann, Minker, 2009</marker>
<rawString>Johannes Pittermann, A. Pittermann, and Wolfgang Minker. 2009. Handling Emotions in HumanComputer Dialogues. Text, Speech and Language Technology. Springer, Dordrecht (The Netherlands).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sherif Yacoub</author>
<author>Steven Simske</author>
<author>Xiaofan Lin</author>
<author>John Burns</author>
</authors>
<title>Recognition of emotions in interactive voice response systems.</title>
<date>2003</date>
<booktitle>In Proc. Eurospeech,</booktitle>
<pages>1--4</pages>
<location>Geneva,</location>
<contexts>
<context position="3271" citStr="Yacoub et al., 2003" startWordPosition="483" endWordPosition="487">om the dialogue context, the speech recognizer and the natural language parser. This contribution is organized as follows: first, we introduce related work and describe our corpus. In Section 4 we outline our employed features with emphasis on the non-acoustic ones. Experiments are shown in Section 5 where we analyze the impact of the newly developed features before we summarize our work in Section 6. 2 Related Work Speech-based emotion research regarding telephone applications has been increasingly discussed in the speech community. While in early studies acted corpora were used, such as in (Yacoub et al., 2003), training and testing data in later studies has been more and more based on reallife data, see (Burkhardt et al., 2008),(Burkhardt et al., 2009). Most studies are limited to acoustic/prosodic features that have been extracted out of the audio data. Linguistic information was additionaly exploited in (Lee et al., 2002) resulting in Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 128–131, Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics 128 a 45.7% accuracy improvement compared t</context>
</contexts>
<marker>Yacoub, Simske, Lin, Burns, 2003</marker>
<rawString>Sherif Yacoub, Steven Simske, Xiaofan Lin, and John Burns. 2003. Recognition of emotions in interactive voice response systems. In Proc. Eurospeech, Geneva, pages 1–4.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>