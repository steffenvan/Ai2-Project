<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.6168925">
Learning Optimal Dialogue Strategies:
A Case Study of a Spoken Dialogue Agent for Email
</title>
<note confidence="0.852805933333333">
Marilyn A. Walker
walker@research.att.com
ATT Labs Research
180 Park Ave.
Florham Park, NJ 07932
Jeanne C. Fromer
jeannie@ai.mit.edu
MIT Al Lab
545 Technology Square
Cambridge, MA, 02139
Shrikanth Narayanan
shri@research.att.com
ATT Labs Research
180 Park Ave.
Florham Park, NJ 07932
</note>
<sectionHeader confidence="0.972408" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999971818181818">
This paper describes a novel method by which a dia-
logue agent can learn to choose an optimal dialogue
strategy. While it is widely agreed that dialogue
strategies should be formulated in terms of com-
municative intentions, there has been little work on
automatically optimizing an agent&apos;s choices when
there are multiple ways to realize a communica-
tive intention. Our method is based on a combina-
tion of learning algorithms and empirical evaluation
techniques. The learning component of our method
is based on algorithms for reinforcement learning,
such as dynamic programming and Q-learning. The
empirical component uses the PARADISE evalua-
tion framework (Walker et al., 1997) to identify the
important performance factors and to provide the
performance function needed by the learning algo-
rithm. We illustrate our method with a dialogue
agent named ELVIS (EmaiL Voice Interactive Sys-
tem), that supports access to email over the phone.
We show how ELVIS can learn to choose among
alternate strategies for agent initiative, for reading
messages, and for summarizing email folders.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999479666666667">
This paper describes a novel method by which a dia-
logue agent can learn to choose an optimal dialogue
strategy. The main problem for dialogue agents
is deciding what information to communicate to a
hearer and how and when to communicate it. For
example, consider one of the strategy choices faced
by a spoken dialogue agent that accesses email by
phone. When multiple messages match the user&apos;s
query, e.g. Read my messages from Kim, an email
agent must choose among multiple response strate-
gies. The agent might choose the Read-First strat-
egy in Dl:
</bodyText>
<listItem confidence="0.764191">
(D1) A: In the messages from Kim, there&apos;s 1 message
about &amp;quot;Interviewing Antonio&amp;quot; and 1 message
about &amp;quot;Meeting Today.&amp;quot; The first message is
titled, &amp;quot;Interviewing Antonio.&amp;quot; It says, &amp;quot;I&apos;d like
to interview him. I could also go along to lunch.
Kim.&amp;quot;
</listItem>
<bodyText confidence="0.979198">
D1 involves summarizing all the messages from
Kim, and then taking the initiative to read the first
one. Alternate strategies are the Read-Summary-
Only strategy in D2, where the agent provides infor-
mation that allows users to refine their selection cri-
teria, and the Read-Choice-Prompt strategy in D3,
where the agent explicitly tells the user what to say
in order to refine the selection:
</bodyText>
<listItem confidence="0.991790428571429">
(D2) A: In the messages from Kim, there&apos;s 1 message
about &amp;quot;Interviewing Antonio&amp;quot; and 1 message about
&amp;quot;Meeting Today.&amp;quot;
(D3) A: In the messages from Kim, there&apos;s 1 message
about &amp;quot;Interviewing Antonio&amp;quot; and 1 message about
&amp;quot;Meeting Today.&amp;quot; To hear the messages, say, &amp;quot;In-
terviewing Antonio&amp;quot; or &amp;quot;Meeting.&amp;quot;
</listItem>
<bodyText confidence="0.77939025">
Decision theoretic planning can be applied to the
problem of choosing among strategies, by associ-
ating a utility U with each strategy (action) choice
and by positing that agents should adhere to the
</bodyText>
<keyword confidence="0.272332">
Maximum Expected Utility Principle (Keeney and
Raiffa, 1976; Russell and Norvig, 1995),
</keyword>
<sectionHeader confidence="0.547443" genericHeader="introduction">
Maximum Expected Utility Principle:
</sectionHeader>
<bodyText confidence="0.996706454545455">
An optimal action is one that maximizes
the expected utility of outcome states.
An agent acts optimally by choosing a strategy
a in state Si that maximizes U(S,). But how are
the utility values U(Si) for each dialogue state Si
derived?
Several reinforcement learning algorithms based
on dynamic programming specify a way to calcu-
late U(S2) in terms of the utility of a successor state
Si (Bellman, 1957; Watkins, 1989; Sutton, 1991;
Barto et al., 1995). Thus if we know the utility for
</bodyText>
<page confidence="0.984876">
1345
</page>
<bodyText confidence="0.999895333333333">
the final state of the dialogue, we can calculate the
utilities for all the earlier states. However, until re-
cently there as been no way of determining a per-
formance function for assigning a utility to the final
state of a dialogue.
This paper presents a method based on dynamic
programming by which dialogue agents can learn
to optimize their choice of dialogue strategies. We
draw on the recently proposed PARADISE evalua-
tion framework (Walker et al., 1997) to identify the
important performance factors and to provide a per-
formance function for calculating the utility of the
final state of a dialogue. We illustrate our method
with a dialogue agent named ELVIS (EmaiL Voice
Interactive System), that supports access to email
over the phone. We test alternate strategies for agent
initiative, for reading messages, and for summariz-
ing email folders. We report results from modeling
a corpus of 232 spoken dialogues in which ELVIS
conversed with human users to carry out a set of
email tasks.
</bodyText>
<sectionHeader confidence="0.895437" genericHeader="method">
2 Method for Learning to Optimize
Dialogue Strategy Selection
</sectionHeader>
<bodyText confidence="0.999875111111111">
Our method for learning to optimize dialogue strat-
egy selection combines the application of PAR-
ADISE to empirical data (Walker et al., 1997), with
algorithms for learning optimal strategy choices.
PARADISE provides an empirical method for de-
riving a performance function that calculates over-
all agent performance as a linear combination of a
number of simpler metrics. Our learning method
consists of the following sequence of steps:
</bodyText>
<listItem confidence="0.999666538461539">
• Implement a spoken dialogue agent for a particular
domain.
• Implement multiple dialogue strategies and design
the agent so that strategies are selected randomly or
under experimenter control.
• Define a set of dialogue tasks for the domain, and
their information exchange requirements. Repre-
sent these tasks as attribute-value matrices to facil-
itate calculating task success.
• Collect experimental dialogues in which a number
of human users converse with the agent to do the
tasks.
• For each experimental dialogue:
</listItem>
<bodyText confidence="0.7270835">
— Log the history of the state-strategy choices
for each dialogue. Use this to estimate a state
transition model.
— Log a range of quantitative and qualitative
cost measures for each dialogue, either auto-
matically or with hand-tagging.
</bodyText>
<listItem confidence="0.978348615384615">
— Collect user satisfaction reports for each dia-
logue.
• Use multivariate linear regression with user satis-
faction as the dependent variable and task success
and the cost measures as independent variables to
determine a performance equation.
• Apply the derived performance equation to each di-
alogue to determine the utility of the final state of
the dialogue.
• Use reinforcement learning to propagate the utility
of the final state back to states Si where strategy
choices were made to determine which action max-
imizes U(Si).
</listItem>
<bodyText confidence="0.99630725">
These steps consist of those for deriving a perfor-
mance function (Section 3), and for using the de-
rived performance function as feedback to the agent
with a learning algorithm (Section 4).
</bodyText>
<sectionHeader confidence="0.9354945" genericHeader="method">
3 Using PARADISE to Derive a
Performance Function
</sectionHeader>
<subsectionHeader confidence="0.980546">
3.1 ELVIS Spoken Dialogue System
</subsectionHeader>
<bodyText confidence="0.99992471875">
ELVIS is implemented using a general-purpose
platform for spoken dialogue agents (Kamm et al.,
1997). The platform consists of a speech recognizer
that supports barge-in so that the user can interrupt
the agent when it is speaking. It also provides an
audio server for both voice recordings and text-to-
speech (TTS), an interface between the computer
running ELVIS and the telephone network, a mod-
ule for application specific functions, and modules
for specifying the application grammars and the dia-
logue manager. Our experiments are based on mod-
ifications to the dialogue manager as described be-
low.
The dialogue manager is based on a state ma-
chine. Each state specifies transitions to other states
and the conditions that license these transitions,
as well as a grammar for what the user can say.
State definitions also include the specification of
agent prompts in terms of templates, with variables
that are instantiated each time the state is entered.
Prompts include: (1) an initial prompt, which the
agent says upon entering the state (this may include
a response to the user&apos;s current request); (2) a help
prompt which the agent says if the user says help;
(3) multiple rejection prompts which the agent says
if the speech recognizer confidence is too low to
continue without more user input; (4) multiple time-
out prompts which the agent produces if the user
doesn&apos;t say anything.
Each of these specifications is affected by the
agent&apos;s dialogue strategy. An agent&apos;s dialogue strat-
egy is implemented as a combination of the prompts
</bodyText>
<page confidence="0.957169">
1346
</page>
<bodyText confidence="0.997700875">
that are played to the user and the state transitions
that the agent makes in response to the user&apos;s utter-
ance. In particular, alternative prompts can be spec-
ified for all types of prompts (initial, help, rejection
and timeout) to provide alternate dialogue strategies
in each state. We implemented alternate strategies
for reading messages, for initiative and for summa-
rization.
The alternate strategies for reading messages
are Read-First, Read-Summarize-Only and Read-
Choice-Prompt as in dialogues D1, D2, and D3.
The alternate strategies for initiative are a system-
initiative (SI) strategy and a mixed-initiative (MI)
strategy. In the SI strategy, the system maintains
control of the dialogue and the user must specify
one query parameter at a time, as in D4.
</bodyText>
<listItem confidence="0.67881">
(D4) A: Hi, Elvis here. You have 5 new and 0 unread
</listItem>
<bodyText confidence="0.4778535">
messages in your inbox. Say Read, Summarize, or
say Help for more options.
</bodyText>
<figure confidence="0.898452428571429">
U: Read.
A: Select by Sender, Subject or in order of arrival?
U: Sender.
A: Which Sender?
U: Kim.
A: In the messages from Kim, there&apos;s 1 message
about &amp;quot;Interviewing Antonio&amp;quot;
</figure>
<bodyText confidence="0.9115065">
In the MI strategy, the system never tells the user
what to do, and the user can take control of the dia-
logue and specify exactly what they want to do in a
single utterance, as in D5.
</bodyText>
<listItem confidence="0.993921">
(D5) A: Hi, Elvis here. I&apos;ve got your mail.
U: Read me my messages from Kim.
</listItem>
<bodyText confidence="0.939785">
We also implemented three alternate summariza-
tion strategies as responses to a user request to Sum-
marize my messages. The basis for these alternates
is ELVIS&apos; s decision about which message attributes
to mention in the summary. The Summarize-Choice
strategy in D6 asks the user to specify which of the
relevant attributes to summarize by.
</bodyText>
<listItem confidence="0.970020571428572">
(D6) A: Summarize by subject, by sender, or both?
U: Subject.
A: In your top level inbox, there&apos;s 1 message about
&amp;quot;Lunch,&amp;quot; 1 message about &amp;quot;Interviewing Antonio,&amp;quot;
•1 message about &amp;quot;Call Me Tomorrow,&amp;quot; 1 message
about &amp;quot;Evaluation Group Meeting,&amp;quot; and 1 message
about &amp;quot;Agent Personality.&amp;quot;
</listItem>
<bodyText confidence="0.661591">
The Summarize-Both strategy in D7 uses all at-
tributes in the summary:
</bodyText>
<listItem confidence="0.9772935">
(D7) A: In your top level inbox, from Kim, there&apos;s 1
message about &amp;quot;Lunch.&amp;quot; From Michael, there&apos;s 1
</listItem>
<bodyText confidence="0.999409230769231">
message about &amp;quot;Evaluation group meeting.&amp;quot; From
Noah, there&apos;s 1 message about &amp;quot;Call Me Tomor-
row&amp;quot; and 1 message about &amp;quot;Interviewing Antonio.&amp;quot;
And from Owen, there&apos;s 1 message about &amp;quot;Agent
Personality.&amp;quot;
In the Summarize-System strategy in D8, ELVIS
summarizes by subject or by sender based on the
current context. For instance, if the user is in the top
level inbox, ELVIS will summarize by sender, but if
the user is situated in a folder containing messages
from Noah, ELVIS will summarize by subject, as a
summary by sender would provide no new informa-
tion.
</bodyText>
<listItem confidence="0.902332">
(D8) A: In your top level inbox, there&apos;s 1 message
from Kim, 2 messages from Noah, 1 message from
Michael, and 1 message from Owen.
</listItem>
<bodyText confidence="0.9967636">
Transitions between states are driven by the
user&apos;s conversational behavior, such as whether s/he
says anything and what s/he says, the semantic in-
terpretation of the user&apos;s utterances, and the settings
of the agent&apos;s dialogue strategy parameters.
</bodyText>
<subsectionHeader confidence="0.989514">
3.2 Experimental Design
</subsectionHeader>
<bodyText confidence="0.999888692307693">
Experimental dialogues were collected via two ex-
periments in which users (AT&amp;T summer interns
and MIT graduate students) interacted with ELVIS
to complete three representative application tasks
that required them to access email messages in three
different email inboxes. In the second experiment,
users participated in a tutorial dialogue before do-
ing the three tasks. The first experiment varied ini-
tiative strategies and the second experiment varied
the presentation strategies for reading messages and
summarizing folders. In order to have adequate data
for learning, the agent must explore the space of
strategy combinations and collect enough samples
of each combination. In the second experiment, we
parameterized the agent so that each user interacted
with three different versions of ELVIS, one for each
task. These experiments resulted in a corpus of 108
dialogues testing the initiative strategies, and a cor-
pus of 124 dialogues testing the presentation strate-
gies.
Each of the three tasks were performed in se-
quence, and each task consisted of two scenarios.
Following PARADISE, the agent and the user had
to exchange information about criteria for selecting
messages and information within the message body
in each scenario. Scenario 1.1 is typical.
</bodyText>
<listItem confidence="0.9669015">
• 1.1: You are working at home in the morning and
plan to go directly to a meeting when you go into
</listItem>
<page confidence="0.983351">
1347
</page>
<bodyText confidence="0.9560415">
work. Kim said she would send you a message
telling you where and when the meeting is. Find
out the Meeting Time and the Meeting Place.
Scenario 1.1 is represented in terms of the at-
tribute value matrix (AVM) in Table 1. Successful
completion of a scenario requires that all attribute-
values must be exchanged (Walker et al., 1997). The
AVM representation for all six scenarios is similar
to Table 1, and is independent of ELVIS&apos; s dialogue
strategy.
</bodyText>
<table confidence="0.9967655">
attribute actual value
Selection Criteria Kim V Meeting
Email.attl 10:30
Email.att2 2D516
</table>
<tableCaption confidence="0.9275965">
Table 1: Attribute value matrix instantiation, Key
for Email Scenario 1.1
</tableCaption>
<subsectionHeader confidence="0.993347">
3.3 Data Collection
</subsectionHeader>
<bodyText confidence="0.999902481481482">
Three different methods are used to collect the mea-
sures for applying the PARADISE framework and
the data for learning: (1) All of the dialogues are
recorded; (2) The dialogue manager logs the agent s
dialogue behavior and a number of other measures
discussed below; (3) Users fill out web page forms
after each task (task success and user satisfaction
measures). Measures are in boldface below.
The dialogue recordings are used to transcribe the
user&apos;s utterances to derive performance measures
for speech recognition, to check the timing of the in-
teraction, to check whether users barged in on agent
utterances (Barge In), and to calculate the elapsed
time of the interaction (ET).
For each state, the system logs which dialogue
strategy the agent selects. In addition, the num-
ber of timeout prompts (Timeout Prompts), Rec-
ognizer Rejections, and the times the user said
Help (Help Requests) are logged. The number of
System Turns and the number of User Turns are
calculated on the basis of this data. In addition,
the recognition result for the user&apos;s utterance is ex-
tracted from the recognizer and logged. The tran-
scriptions are used in combination with the logged
recognition result to calculate a concept accuracy
measure for each utterance.1 Mean concept accu-
racy is then calculated over the whole dialogue and
</bodyText>
<footnote confidence="0.9875655">
1For example, the utterance Read my messages from Kim
contains two concepts, the read function, and the sender:kim
selection criterion. If the system understood only that the user
said Read, concept accuracy would be .5.
</footnote>
<bodyText confidence="0.999267944444444">
used as a Mean Recognition Score MRS for the di-
alogue.
The web page forms are the basis for calculat-
ing Task Success and User Satisfaction measures.
Users reported their perceptions as to whether they
had completed the task (Comp),2 and filled in an
AVM with the information that they had acquired
from the agent, e.g. the values for Email.attl and
Email.att2 in Table 1. The AVM matrix supports
calculating Task Success objectively by using the
Kappa statistic to compare the information in the
AVM that the users filled in with an AVM key such
as that in Table 1 (Walker et al., 1997).
In order to calculate User Satisfaction, users were
asked to evaluate the agent&apos;s performance with a
user satisfaction survey. The data from the survey
resulted in user satisfaction values that range from 0
to 33. See (Walker et al., 1998) for more details.
</bodyText>
<subsectionHeader confidence="0.998866">
3.4 Deriving a Performance Function
</subsectionHeader>
<bodyText confidence="0.999954833333333">
Overall, the results showed that users could success-
fully complete the tasks with all versions of ELVIS.
Most users completed each task in about 5 minutes
and average K over all subjects and tasks was .82.
However, there were differences between strategies;
as an example see Table 2.
</bodyText>
<table confidence="0.9992135">
Measure SYSTEM (SI) MIXED (MI)
Kappa .81 .83
Comp .83 .78
User Turns 25.94 17.59
System Turns 28.18 21.74
Elapsed Time (ET) 328.59 s 289.43 s
MeanRecog (MRS) .88 .72
Time Outs 2.24 4.15
Help Requests .70 .94
Barge Ins 5.2 3.5
Recognizer Rejects .98 1.67
User Satisfaction 26.6 23.7
</table>
<tableCaption confidence="0.8145535">
Table 2: Performance measure means per dialogue
for Initiative Strategies
</tableCaption>
<bodyText confidence="0.999086333333333">
PARADISE provides a way to calculate dialogue
agent performance as a linear combination of a
number of simpler metrics that can be directly mea-
sured such as those in Table 2. Performance for any
(sub)dialogue D is defined by the following equa-
tion:
</bodyText>
<equation confidence="0.583552">
Performance = (a *Ar(K)) - E wi*Ar(ci)
2 Yes,No responses are converted to 1,0.
</equation>
<page confidence="0.961587">
1348
</page>
<bodyText confidence="0.987548285714286">
where a is a weight on is, ci are the cost functions,
which are weighted by wi, and Al is a Z score nor-
malization function (Walker et al., 1997; Cohen,
1995). The Z score normalization function ensures
that, when the weights a and wi are solved for, that
the magnitude of the weights reflect the magnitude
of the contribution of that factor to performance.
The performance function is derived through mul-
tivariate linear regression with User Satisfaction as
the dependent variable and all the other measures as
independent variables (Walker et al., 1997). See Ta-
ble 2. In the ELVIS data, an initial regression over
the measures in Table 2 suggests that Comp, MRS
and ET are the only significant contributors to User
Satisfaction. A second regression including only
these factors results in the following equation:
Performance = .21 *C omp+.47 * M RS — .15* ET
with Comp (t=2.58, p =.01), MRS (t =5.75, p
=.0001) and ET (t=-1.8, p=.07) significant predic-
tors, accounting for 38% of the variance in R-
Squared (F (3,104)=21.2, p &lt;.0001). The mag-
nitude of the coefficients in this equation demon-
strates the performance of the speech recognizer
(MRS) is the most important predictor, followed by
users&apos; perception of Task Success (Comp) and ef-
ficiency (ET). In the next section, we show how to
use this derived performance equation to compute
the utility of the final state of the dialogue.
</bodyText>
<sectionHeader confidence="0.935303" genericHeader="method">
4 Applying Q-learning to ELVIS
</sectionHeader>
<subsectionHeader confidence="0.803591">
Experimental Data
</subsectionHeader>
<bodyText confidence="0.999447333333333">
The basic idea is to apply the performance func-
tion to the measures logged for each dialogue Di,
thereby replacing a range of measures with a single
performance value Pi. Given the performance val-
ues Pi, any of a number of automatic learning algo-
rithms can be used to determine which sequence of
action choices (dialogue strategies) maximize util-
ity, by using Pi as the utility for the final state of the
dialogue D. Possible algorithms include Genetic
Algorithms, Q-learning, TD-Learning, and Adap-
tive Dynamic Programming (Russell and Norvig,
1995). Here we use Q-learning to illustrate the
method (Watkins, 1989). See (Fromer, 1998) for
experiments using alternative algorithms.
The utility of doing action a in state Si, U (a, St)
(its Q-value), can be calculated terms of the utility
of a successor state Si, by obeying the following
recursive equation:
</bodyText>
<equation confidence="0.945689">
U (a, Si) = R(Si) E 49:7 U (al , Si)
</equation>
<bodyText confidence="0.999925555555555">
where R(51) is a reward associated with being in
state Si, a is a strategy from a finite set of strate-
gies A that are admissable in state Si, and M,Pi is
the probability of reaching state Si if strategy a is
selected in state Si.
In the experiments reported here, the reward asso-
ciated with each state, R(51), is zero.3 In addition,
since reliable a priori prediction of a user action in a
particular state is not possible (for example the user
may say Help or the speech recognizer may fail to
understand the user), the state transition model Mfli
is estimated from the logged state-strategy history
for the dialogue.
The utility values can be estimated to within a de-
sired threshold using Value Iteration, which updates
the estimate of U (a, Si), based on updated utility
estimates for neighboring states, so that the equa-
tion above becomes:
</bodyText>
<equation confidence="0.92053">
Un+1 (a, Si) = R(Si) E /14-4 nwc un(d, Si)
</equation>
<bodyText confidence="0.999962466666667">
where Uri(a, Si) is the utility estimate for doing
a in state Si after n iterations. Value Iteration
stops when the difference between Un (a, Si) and
Un+1(a, Si) is below a threshold, and utility values
have been associated with states where strategy se-
lections were made. After experimenting with var-
ious threshholds, we used a threshold of 5% of the
performance range of the dialogues.
The result of applying Q-learning to ELVIS data
for the initiative strategies is illustrated in Figure 1.
The figure plots utility estimates for SI and MI over
time. It is clear that the SI strategy is better because
it has a higher utility: at the end of 108 training
sessions (dialogues), the utility of SI is estimated
at .249 and the utility of MI is estimated at -0.174.
</bodyText>
<table confidence="0.991502571428571">
TYPE STRATEGY UTILITY
Read Read-First .21
Read-Choice-Prompt .07
Read-Summarize-Only .08
Summarize Summarize-System .162
Summarize-Choice -0.03
Summarize-Both .09
</table>
<tableCaption confidence="0.65415825">
Table 3: Utilities for Presentation Strategy Choices
after 124 Training Sessions
The SI and MI strategies affect the whole dia-
logue; the presentation strategies apply locally and
</tableCaption>
<footnote confidence="0.8120855">
3 See (Fromer, 1998) for experiments in which local rewards
are nonzero.
</footnote>
<page confidence="0.988572">
1349
</page>
<figure confidence="0.937243727272727">
Utilities for SI and MI over Training Sessions
0.8
0.6
0.4
0.2
P
3
-0.2
-0.4
-0.6
-0.8
</figure>
<figureCaption confidence="0.944102">
Figure 1: Results of applying Q-learning to System-
Initiative (SI) and Mixed-Initiative (MI) Strategies
for 108 ELVIS Dialogues
</figureCaption>
<bodyText confidence="0.9982">
can be actived in different states of the dialogue. We
examined the variation in a strategy&apos; s utility at each
phase of the task, by representing the task as having
three phases: no scenarios completed, one scenario
completed and both scenarios completed. Table 3
reports utilities for the use of a strategy after one
scenario was completed. The policy implied by the
utilities at other phases of the task are the same. See
(Fromer, 1998) for more detail.
The Read-First strategy in D1 has the best per-
formance of the read strategies. This strategy takes
the initiative to read a message, which might re-
sult in messages being read that the user wasn t in-
terested in. However since the user can barge-in
on system utterances, perhaps little is lost by tak-
ing the initiative to start reading a message. After
124 training sessions, the best summarize strategy
is Summarize-System, which automatically selects
which attributes to summarize by, and so does not
incur the cost of asking the user to specify these at-
tributes. However, the utilities for the Summarize-
Choice strategy have not completely converged after
124 trials.
</bodyText>
<sectionHeader confidence="0.998388" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999981160714286">
This paper illustrates a novel technique by which
an agent can learn to choose an optimal dialogue
strategy. We illustrate our technique with ELVIS, an
agent that supports access to email by phone, with
strategies for initiative, and for reading and sum-
marizing messages. We show that ELVIS can learn
that the System-Initiative strategy has higher utility
than the Mixed-Initiative strategy, that Read-First is
the best read strategy, and that Summarize-System
is the best summary strategy.
Here, our method was illustrated by evaluating
strategies for managing initiative and for message
presentation. However there are numerous dia-
logue strategies that an agent might use, e.g. to
gather information, handle errors, or manage the di-
alogue interaction (Chu-Carroll and Carberry, 1995;
Danieli and Gerbino, 1995; Hovy, 1993; McK-
eown, 1985; Moore and Paris, 1989). Previous
work in natural language generation has proposed
heuristics to determine an agent&apos;s choice of dialogue
strategy, based on factors such as discourse focus,
medium, style, and the content of previous expla-
nations (McKeown, 1985; Moore and Paris, 1989;
Maybury, 1991; Hovy, 1993). It should be possi-
ble to test experimentally whether an agent can au-
tomatically learn these heuristics since the method-
ology we propose is general, and could be applied
to any dialogue strategy choice that an agent might
make.
Previous work has also proposed that an agent&apos;s
choice of dialogue strategy can be treated as a
stochastic optimization problem (Walker, 1993;
Biermann and Long, 1996; Levin and Pieraccini,
1997). However, to our knowledge, these meth-
ods have not previously been applied to interactions
with real users. The lack of an appropriate perfor-
mance function has been a critical methodological
limitation.
We use the PARADISE framework (Walker et
al., 1997) to derive an empirically motivated per-
formance function, that combines both subjective
user preferences and objective system performance
measures into a single function. It would have been
impossible to predict a priori which dialogue fac-
tors influence the usability of a dialogue agent, and
to what degree. Our performance equation shows
that both dialogue quality and efficiency measures
contribute to agent performance, but that dialogue
quality measures have a greater influence. Further-
more, in contrast to assuming an a priori model, we
use the dialogues from real user-system interactions
to provide realistic estimates of Wi , the state tran-
sition model used by the learning algorithm. It is
impossible to predict a priori the transition frequen-
cies, given the imperfect nature of spoken language
understanding, and the unpredictability of user be-
</bodyText>
<figure confidence="0.96296725">
+ SI
O MI
20 40 60 80 100 120
Training Instances (Dialogues)
</figure>
<page confidence="0.897178">
1350
</page>
<bodyText confidence="0.994626583333333">
havior.
The use of this method introduces several open
issues. First, the results of the learning algorithm
are dependent on the representation of the state
space. In many reinforcement learning problems
(e.g. backgammon), the state space is pre-defined.
In spoken dialogue systems, the system designers
construct the state space and decide what state vari-
ables need to be monitored. Our initial results sug-
gest that the state representation that the agent uses
to interact with the user may not be the optimal state
representation for learning. See (Fromer, 1998).
Second, in advance of actually running learning ex-
periments, it is not clear how much experience an
agent will need to determine which strategy is bet-
ter. Figure 1 shows that it took no more than 50
dialogue samples for the algorithm to show the dif-
ferences in convergence trends when learning about
initiative strategies. However, it appears that more
data is needed to learn to distinguish between the
summarization strategies. Third, our experimental
data is based on short-term interactions with novice
users, but we might expect that users of an email
agent would engage in many interactions with the
same agent, and that preferences for agent interac-
tion strategies could change over time with user ex-
pertise. This means that the performance function
might change over time. Finally, the learning algo-
rithm that we report here is an off-line algorithm,
i.e. the agent collects a set of dialogues and then de-
cides on an optimal strategy as a result. In contrast,
it should be possible for the agent to learn on-line,
during the course of a dialogue, if the performance
function could be automatically calculated (or ap-
proximated). We are exploring these issues in on-
going work.
</bodyText>
<sectionHeader confidence="0.999391" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.742803333333333">
G. Di Fabbrizio, D. Hindle, J. Hirschberg, C.
Kamm, and D. Litman provided assistance with this
research or paper.
</bodyText>
<sectionHeader confidence="0.985854" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99982495">
A.G. Barto, S. J. Bradtke, and S. P. Singh. 1995. Learn-
ing to act using real-time dynamic programming. Ar-
tificial Intelligence Journal, 72(1-2): 81-138.
R. E. Bellman. 1957. Dynamic Programming. Princeton
University Press, Princeton, N.J.
A. W. Biermann and Philip M. Long. 1996. The compo-
sition of messages in speech-graphics interactive sys-
tems. In Proc. of the 1996 International Symposium
on Spoken Dialogue, pp. 97-100.
J. Chu-Carroll and S. Carberry. 1995. Response genera-
tion in collaborative negotiation. In Proc. of the 33rd
Annual Meeting of the ACL, pp. 136-143.
P. R. Cohen. 1995. Empirical Methods for Artificial In-
telligence. MIT Press, Boston.
M. Danieli and E. Gerbino. 1995. Metrics for evaluating
dialogue strategies in a spoken language system. In
Proc. of the 1995 AAAI Spring Symposium on Empir-
ical Methods in Discourse, pages 34-39.
J. C. Fromer. 1998. Learning optimal discourse strate-
gies in a spoken dialogue system. Technical Report
Forthcoming, MIT Al Lab M.S. Thesis.
E. H. Hovy. 1993. Automated discourse generation
using discourse structure relations. Artificial Intelli-
gence Journal, 63:341-385.
C. Karrun, S. Narayanan, D. Dutton, and R. Ritenour.
1997. Evaluating spoken dialog systems for telecom-
munication services. In EUROSPEECH 97.
R. Keeney and H. Raiffa. 1976. Decisions with Multiple
Objectives: Preferences and Value Tradeoffs. John
Wiley and Sons.
E. Levin and R. Pieraccini. 1997. A stochastic model
of computer-human interaction for learning dialogue
strategies. In EUROSPEECH 97.
M.T. Maybury. 1991. Planning multi-media explana-
tions using communicative acts. In Proc. of the Ninth
National Conf on Artificial Intelligence, pages 61-66.
K. R. McKeown. 1985. Discourse strategies for gen-
erating natural language text. Artificial Intelligence,
27(1): 1-42, September.
J. D. Moore and C. L. Paris. 1989. Planning text for
advisory dialogues. In Proc. 27th Annual Meeting of
the ACL.
S. Russell and P. Norvig. 1995. Artificial Intelligence: A
Modern Approach. Prentice Hall, N.J.
R. S. Sutton. 1991. Planning by incremental dynamic
programming. In Proc. Ninth Conf on Machine
Learning, pages 353-357. Morgan-Kaufmann.
M. A. Walker, D. Litman, C. Kamm, and A. Abella.
1997. PARADISE: A general framework for evalu-
ating spoken dialogue agents. In Proc. of the 35th An-
nual Meeting of the ACL, pp. 271-280.
M. Walker, J. Fromer, G. Di Fabbrizio, C. Mestel, and D.
Hindle. 1998. What can I say: Evaluating a spoken
language interface to email. In Proc. of the Conf on
Computer Human Interaction (CHI 98).
M. A. Walker. 1993. Informational Redundancy and Re-
source Bounds in Dialogue. Ph.D. thesis, University
of Pennsylvania.
C. J. Watkins. 1989. Models of Delayed Reinforcement
Learning. Ph.D. thesis, Cambridge University.
</reference>
<page confidence="0.992973">
1351
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.573919">
<title confidence="0.999712">Learning Optimal Dialogue Strategies: A Case Study of a Spoken Dialogue Agent for Email</title>
<author confidence="0.999972">Marilyn A Walker</author>
<email confidence="0.999791">walker@research.att.com</email>
<affiliation confidence="0.811836">ATT Labs Research</affiliation>
<address confidence="0.9837365">180 Park Ave. Florham Park, NJ 07932</address>
<author confidence="0.9974">Jeanne C Fromer</author>
<email confidence="0.998712">jeannie@ai.mit.edu</email>
<affiliation confidence="0.949727">MIT Al Lab</affiliation>
<address confidence="0.9974175">545 Technology Square Cambridge, MA, 02139</address>
<author confidence="0.99277">Shrikanth Narayanan</author>
<email confidence="0.999129">shri@research.att.com</email>
<affiliation confidence="0.805307">ATT Labs Research</affiliation>
<address confidence="0.985962">180 Park Ave. Florham Park, NJ 07932</address>
<abstract confidence="0.997661782608695">This paper describes a novel method by which a dialogue agent can learn to choose an optimal dialogue strategy. While it is widely agreed that dialogue strategies should be formulated in terms of communicative intentions, there has been little work on automatically optimizing an agent&apos;s choices when there are multiple ways to realize a communicative intention. Our method is based on a combination of learning algorithms and empirical evaluation techniques. The learning component of our method is based on algorithms for reinforcement learning, such as dynamic programming and Q-learning. The empirical component uses the PARADISE evaluation framework (Walker et al., 1997) to identify the important performance factors and to provide the performance function needed by the learning algorithm. We illustrate our method with a dialogue agent named ELVIS (EmaiL Voice Interactive System), that supports access to email over the phone. We show how ELVIS can learn to choose among alternate strategies for agent initiative, for reading messages, and for summarizing email folders.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A G Barto</author>
<author>S J Bradtke</author>
<author>S P Singh</author>
</authors>
<title>Learning to act using real-time dynamic programming.</title>
<date>1995</date>
<journal>Artificial Intelligence Journal,</journal>
<volume>72</volume>
<issue>1</issue>
<pages>81--138</pages>
<contexts>
<context position="3736" citStr="Barto et al., 1995" startWordPosition="594" endWordPosition="597"> positing that agents should adhere to the Maximum Expected Utility Principle (Keeney and Raiffa, 1976; Russell and Norvig, 1995), Maximum Expected Utility Principle: An optimal action is one that maximizes the expected utility of outcome states. An agent acts optimally by choosing a strategy a in state Si that maximizes U(S,). But how are the utility values U(Si) for each dialogue state Si derived? Several reinforcement learning algorithms based on dynamic programming specify a way to calculate U(S2) in terms of the utility of a successor state Si (Bellman, 1957; Watkins, 1989; Sutton, 1991; Barto et al., 1995). Thus if we know the utility for 1345 the final state of the dialogue, we can calculate the utilities for all the earlier states. However, until recently there as been no way of determining a performance function for assigning a utility to the final state of a dialogue. This paper presents a method based on dynamic programming by which dialogue agents can learn to optimize their choice of dialogue strategies. We draw on the recently proposed PARADISE evaluation framework (Walker et al., 1997) to identify the important performance factors and to provide a performance function for calculating t</context>
</contexts>
<marker>Barto, Bradtke, Singh, 1995</marker>
<rawString>A.G. Barto, S. J. Bradtke, and S. P. Singh. 1995. Learning to act using real-time dynamic programming. Artificial Intelligence Journal, 72(1-2): 81-138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Bellman</author>
</authors>
<title>Dynamic Programming.</title>
<date>1957</date>
<publisher>Princeton University Press,</publisher>
<location>Princeton, N.J.</location>
<contexts>
<context position="3686" citStr="Bellman, 1957" startWordPosition="588" endWordPosition="589"> U with each strategy (action) choice and by positing that agents should adhere to the Maximum Expected Utility Principle (Keeney and Raiffa, 1976; Russell and Norvig, 1995), Maximum Expected Utility Principle: An optimal action is one that maximizes the expected utility of outcome states. An agent acts optimally by choosing a strategy a in state Si that maximizes U(S,). But how are the utility values U(Si) for each dialogue state Si derived? Several reinforcement learning algorithms based on dynamic programming specify a way to calculate U(S2) in terms of the utility of a successor state Si (Bellman, 1957; Watkins, 1989; Sutton, 1991; Barto et al., 1995). Thus if we know the utility for 1345 the final state of the dialogue, we can calculate the utilities for all the earlier states. However, until recently there as been no way of determining a performance function for assigning a utility to the final state of a dialogue. This paper presents a method based on dynamic programming by which dialogue agents can learn to optimize their choice of dialogue strategies. We draw on the recently proposed PARADISE evaluation framework (Walker et al., 1997) to identify the important performance factors and t</context>
</contexts>
<marker>Bellman, 1957</marker>
<rawString>R. E. Bellman. 1957. Dynamic Programming. Princeton University Press, Princeton, N.J.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A W Biermann</author>
<author>Philip M Long</author>
</authors>
<title>The composition of messages in speech-graphics interactive systems.</title>
<date>1996</date>
<booktitle>In Proc. of the 1996 International Symposium on Spoken Dialogue,</booktitle>
<pages>97--100</pages>
<contexts>
<context position="24156" citStr="Biermann and Long, 1996" startWordPosition="3973" endWordPosition="3976"> heuristics to determine an agent&apos;s choice of dialogue strategy, based on factors such as discourse focus, medium, style, and the content of previous explanations (McKeown, 1985; Moore and Paris, 1989; Maybury, 1991; Hovy, 1993). It should be possible to test experimentally whether an agent can automatically learn these heuristics since the methodology we propose is general, and could be applied to any dialogue strategy choice that an agent might make. Previous work has also proposed that an agent&apos;s choice of dialogue strategy can be treated as a stochastic optimization problem (Walker, 1993; Biermann and Long, 1996; Levin and Pieraccini, 1997). However, to our knowledge, these methods have not previously been applied to interactions with real users. The lack of an appropriate performance function has been a critical methodological limitation. We use the PARADISE framework (Walker et al., 1997) to derive an empirically motivated performance function, that combines both subjective user preferences and objective system performance measures into a single function. It would have been impossible to predict a priori which dialogue factors influence the usability of a dialogue agent, and to what degree. Our per</context>
</contexts>
<marker>Biermann, Long, 1996</marker>
<rawString>A. W. Biermann and Philip M. Long. 1996. The composition of messages in speech-graphics interactive systems. In Proc. of the 1996 International Symposium on Spoken Dialogue, pp. 97-100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chu-Carroll</author>
<author>S Carberry</author>
</authors>
<title>Response generation in collaborative negotiation.</title>
<date>1995</date>
<booktitle>In Proc. of the 33rd Annual Meeting of the ACL,</booktitle>
<pages>136--143</pages>
<contexts>
<context position="23396" citStr="Chu-Carroll and Carberry, 1995" startWordPosition="3852" endWordPosition="3855">ent that supports access to email by phone, with strategies for initiative, and for reading and summarizing messages. We show that ELVIS can learn that the System-Initiative strategy has higher utility than the Mixed-Initiative strategy, that Read-First is the best read strategy, and that Summarize-System is the best summary strategy. Here, our method was illustrated by evaluating strategies for managing initiative and for message presentation. However there are numerous dialogue strategies that an agent might use, e.g. to gather information, handle errors, or manage the dialogue interaction (Chu-Carroll and Carberry, 1995; Danieli and Gerbino, 1995; Hovy, 1993; McKeown, 1985; Moore and Paris, 1989). Previous work in natural language generation has proposed heuristics to determine an agent&apos;s choice of dialogue strategy, based on factors such as discourse focus, medium, style, and the content of previous explanations (McKeown, 1985; Moore and Paris, 1989; Maybury, 1991; Hovy, 1993). It should be possible to test experimentally whether an agent can automatically learn these heuristics since the methodology we propose is general, and could be applied to any dialogue strategy choice that an agent might make. Previo</context>
</contexts>
<marker>Chu-Carroll, Carberry, 1995</marker>
<rawString>J. Chu-Carroll and S. Carberry. 1995. Response generation in collaborative negotiation. In Proc. of the 33rd Annual Meeting of the ACL, pp. 136-143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P R Cohen</author>
</authors>
<title>Empirical Methods for Artificial Intelligence.</title>
<date>1995</date>
<publisher>MIT Press,</publisher>
<location>Boston.</location>
<contexts>
<context position="17001" citStr="Cohen, 1995" startWordPosition="2792" endWordPosition="2793">r Rejects .98 1.67 User Satisfaction 26.6 23.7 Table 2: Performance measure means per dialogue for Initiative Strategies PARADISE provides a way to calculate dialogue agent performance as a linear combination of a number of simpler metrics that can be directly measured such as those in Table 2. Performance for any (sub)dialogue D is defined by the following equation: Performance = (a *Ar(K)) - E wi*Ar(ci) 2 Yes,No responses are converted to 1,0. 1348 where a is a weight on is, ci are the cost functions, which are weighted by wi, and Al is a Z score normalization function (Walker et al., 1997; Cohen, 1995). The Z score normalization function ensures that, when the weights a and wi are solved for, that the magnitude of the weights reflect the magnitude of the contribution of that factor to performance. The performance function is derived through multivariate linear regression with User Satisfaction as the dependent variable and all the other measures as independent variables (Walker et al., 1997). See Table 2. In the ELVIS data, an initial regression over the measures in Table 2 suggests that Comp, MRS and ET are the only significant contributors to User Satisfaction. A second regression includi</context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>P. R. Cohen. 1995. Empirical Methods for Artificial Intelligence. MIT Press, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Danieli</author>
<author>E Gerbino</author>
</authors>
<title>Metrics for evaluating dialogue strategies in a spoken language system.</title>
<date>1995</date>
<booktitle>In Proc. of the 1995 AAAI Spring Symposium on Empirical Methods in Discourse,</booktitle>
<pages>34--39</pages>
<contexts>
<context position="23423" citStr="Danieli and Gerbino, 1995" startWordPosition="3856" endWordPosition="3859">l by phone, with strategies for initiative, and for reading and summarizing messages. We show that ELVIS can learn that the System-Initiative strategy has higher utility than the Mixed-Initiative strategy, that Read-First is the best read strategy, and that Summarize-System is the best summary strategy. Here, our method was illustrated by evaluating strategies for managing initiative and for message presentation. However there are numerous dialogue strategies that an agent might use, e.g. to gather information, handle errors, or manage the dialogue interaction (Chu-Carroll and Carberry, 1995; Danieli and Gerbino, 1995; Hovy, 1993; McKeown, 1985; Moore and Paris, 1989). Previous work in natural language generation has proposed heuristics to determine an agent&apos;s choice of dialogue strategy, based on factors such as discourse focus, medium, style, and the content of previous explanations (McKeown, 1985; Moore and Paris, 1989; Maybury, 1991; Hovy, 1993). It should be possible to test experimentally whether an agent can automatically learn these heuristics since the methodology we propose is general, and could be applied to any dialogue strategy choice that an agent might make. Previous work has also proposed t</context>
</contexts>
<marker>Danieli, Gerbino, 1995</marker>
<rawString>M. Danieli and E. Gerbino. 1995. Metrics for evaluating dialogue strategies in a spoken language system. In Proc. of the 1995 AAAI Spring Symposium on Empirical Methods in Discourse, pages 34-39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Fromer</author>
</authors>
<title>Learning optimal discourse strategies in a spoken dialogue system.</title>
<date>1998</date>
<tech>Technical Report Forthcoming, MIT Al Lab M.S. Thesis.</tech>
<contexts>
<context position="18912" citStr="Fromer, 1998" startWordPosition="3107" endWordPosition="3108">apply the performance function to the measures logged for each dialogue Di, thereby replacing a range of measures with a single performance value Pi. Given the performance values Pi, any of a number of automatic learning algorithms can be used to determine which sequence of action choices (dialogue strategies) maximize utility, by using Pi as the utility for the final state of the dialogue D. Possible algorithms include Genetic Algorithms, Q-learning, TD-Learning, and Adaptive Dynamic Programming (Russell and Norvig, 1995). Here we use Q-learning to illustrate the method (Watkins, 1989). See (Fromer, 1998) for experiments using alternative algorithms. The utility of doing action a in state Si, U (a, St) (its Q-value), can be calculated terms of the utility of a successor state Si, by obeying the following recursive equation: U (a, Si) = R(Si) E 49:7 U (al , Si) where R(51) is a reward associated with being in state Si, a is a strategy from a finite set of strategies A that are admissable in state Si, and M,Pi is the probability of reaching state Si if strategy a is selected in state Si. In the experiments reported here, the reward associated with each state, R(51), is zero.3 In addition, since </context>
<context position="21186" citStr="Fromer, 1998" startWordPosition="3496" endWordPosition="3497">ility estimates for SI and MI over time. It is clear that the SI strategy is better because it has a higher utility: at the end of 108 training sessions (dialogues), the utility of SI is estimated at .249 and the utility of MI is estimated at -0.174. TYPE STRATEGY UTILITY Read Read-First .21 Read-Choice-Prompt .07 Read-Summarize-Only .08 Summarize Summarize-System .162 Summarize-Choice -0.03 Summarize-Both .09 Table 3: Utilities for Presentation Strategy Choices after 124 Training Sessions The SI and MI strategies affect the whole dialogue; the presentation strategies apply locally and 3 See (Fromer, 1998) for experiments in which local rewards are nonzero. 1349 Utilities for SI and MI over Training Sessions 0.8 0.6 0.4 0.2 P 3 -0.2 -0.4 -0.6 -0.8 Figure 1: Results of applying Q-learning to SystemInitiative (SI) and Mixed-Initiative (MI) Strategies for 108 ELVIS Dialogues can be actived in different states of the dialogue. We examined the variation in a strategy&apos; s utility at each phase of the task, by representing the task as having three phases: no scenarios completed, one scenario completed and both scenarios completed. Table 3 reports utilities for the use of a strategy after one scenario w</context>
<context position="25926" citStr="Fromer, 1998" startWordPosition="4253" endWordPosition="4254">0 80 100 120 Training Instances (Dialogues) 1350 havior. The use of this method introduces several open issues. First, the results of the learning algorithm are dependent on the representation of the state space. In many reinforcement learning problems (e.g. backgammon), the state space is pre-defined. In spoken dialogue systems, the system designers construct the state space and decide what state variables need to be monitored. Our initial results suggest that the state representation that the agent uses to interact with the user may not be the optimal state representation for learning. See (Fromer, 1998). Second, in advance of actually running learning experiments, it is not clear how much experience an agent will need to determine which strategy is better. Figure 1 shows that it took no more than 50 dialogue samples for the algorithm to show the differences in convergence trends when learning about initiative strategies. However, it appears that more data is needed to learn to distinguish between the summarization strategies. Third, our experimental data is based on short-term interactions with novice users, but we might expect that users of an email agent would engage in many interactions w</context>
</contexts>
<marker>Fromer, 1998</marker>
<rawString>J. C. Fromer. 1998. Learning optimal discourse strategies in a spoken dialogue system. Technical Report Forthcoming, MIT Al Lab M.S. Thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E H Hovy</author>
</authors>
<title>Automated discourse generation using discourse structure relations.</title>
<date>1993</date>
<journal>Artificial Intelligence Journal,</journal>
<pages>63--341</pages>
<contexts>
<context position="23435" citStr="Hovy, 1993" startWordPosition="3860" endWordPosition="3861"> for initiative, and for reading and summarizing messages. We show that ELVIS can learn that the System-Initiative strategy has higher utility than the Mixed-Initiative strategy, that Read-First is the best read strategy, and that Summarize-System is the best summary strategy. Here, our method was illustrated by evaluating strategies for managing initiative and for message presentation. However there are numerous dialogue strategies that an agent might use, e.g. to gather information, handle errors, or manage the dialogue interaction (Chu-Carroll and Carberry, 1995; Danieli and Gerbino, 1995; Hovy, 1993; McKeown, 1985; Moore and Paris, 1989). Previous work in natural language generation has proposed heuristics to determine an agent&apos;s choice of dialogue strategy, based on factors such as discourse focus, medium, style, and the content of previous explanations (McKeown, 1985; Moore and Paris, 1989; Maybury, 1991; Hovy, 1993). It should be possible to test experimentally whether an agent can automatically learn these heuristics since the methodology we propose is general, and could be applied to any dialogue strategy choice that an agent might make. Previous work has also proposed that an agent</context>
</contexts>
<marker>Hovy, 1993</marker>
<rawString>E. H. Hovy. 1993. Automated discourse generation using discourse structure relations. Artificial Intelligence Journal, 63:341-385.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Karrun</author>
<author>S Narayanan</author>
<author>D Dutton</author>
<author>R Ritenour</author>
</authors>
<title>Evaluating spoken dialog systems for telecommunication services.</title>
<date>1997</date>
<booktitle>In EUROSPEECH 97.</booktitle>
<marker>Karrun, Narayanan, Dutton, Ritenour, 1997</marker>
<rawString>C. Karrun, S. Narayanan, D. Dutton, and R. Ritenour. 1997. Evaluating spoken dialog systems for telecommunication services. In EUROSPEECH 97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Keeney</author>
<author>H Raiffa</author>
</authors>
<title>Decisions with Multiple Objectives: Preferences and Value Tradeoffs.</title>
<date>1976</date>
<publisher>John Wiley and Sons.</publisher>
<contexts>
<context position="3219" citStr="Keeney and Raiffa, 1976" startWordPosition="510" endWordPosition="513">tly tells the user what to say in order to refine the selection: (D2) A: In the messages from Kim, there&apos;s 1 message about &amp;quot;Interviewing Antonio&amp;quot; and 1 message about &amp;quot;Meeting Today.&amp;quot; (D3) A: In the messages from Kim, there&apos;s 1 message about &amp;quot;Interviewing Antonio&amp;quot; and 1 message about &amp;quot;Meeting Today.&amp;quot; To hear the messages, say, &amp;quot;Interviewing Antonio&amp;quot; or &amp;quot;Meeting.&amp;quot; Decision theoretic planning can be applied to the problem of choosing among strategies, by associating a utility U with each strategy (action) choice and by positing that agents should adhere to the Maximum Expected Utility Principle (Keeney and Raiffa, 1976; Russell and Norvig, 1995), Maximum Expected Utility Principle: An optimal action is one that maximizes the expected utility of outcome states. An agent acts optimally by choosing a strategy a in state Si that maximizes U(S,). But how are the utility values U(Si) for each dialogue state Si derived? Several reinforcement learning algorithms based on dynamic programming specify a way to calculate U(S2) in terms of the utility of a successor state Si (Bellman, 1957; Watkins, 1989; Sutton, 1991; Barto et al., 1995). Thus if we know the utility for 1345 the final state of the dialogue, we can calc</context>
</contexts>
<marker>Keeney, Raiffa, 1976</marker>
<rawString>R. Keeney and H. Raiffa. 1976. Decisions with Multiple Objectives: Preferences and Value Tradeoffs. John Wiley and Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Levin</author>
<author>R Pieraccini</author>
</authors>
<title>A stochastic model of computer-human interaction for learning dialogue strategies.</title>
<date>1997</date>
<booktitle>In EUROSPEECH 97.</booktitle>
<contexts>
<context position="24185" citStr="Levin and Pieraccini, 1997" startWordPosition="3977" endWordPosition="3980">an agent&apos;s choice of dialogue strategy, based on factors such as discourse focus, medium, style, and the content of previous explanations (McKeown, 1985; Moore and Paris, 1989; Maybury, 1991; Hovy, 1993). It should be possible to test experimentally whether an agent can automatically learn these heuristics since the methodology we propose is general, and could be applied to any dialogue strategy choice that an agent might make. Previous work has also proposed that an agent&apos;s choice of dialogue strategy can be treated as a stochastic optimization problem (Walker, 1993; Biermann and Long, 1996; Levin and Pieraccini, 1997). However, to our knowledge, these methods have not previously been applied to interactions with real users. The lack of an appropriate performance function has been a critical methodological limitation. We use the PARADISE framework (Walker et al., 1997) to derive an empirically motivated performance function, that combines both subjective user preferences and objective system performance measures into a single function. It would have been impossible to predict a priori which dialogue factors influence the usability of a dialogue agent, and to what degree. Our performance equation shows that </context>
</contexts>
<marker>Levin, Pieraccini, 1997</marker>
<rawString>E. Levin and R. Pieraccini. 1997. A stochastic model of computer-human interaction for learning dialogue strategies. In EUROSPEECH 97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M T Maybury</author>
</authors>
<title>Planning multi-media explanations using communicative acts.</title>
<date>1991</date>
<booktitle>In Proc. of the Ninth National Conf on Artificial Intelligence,</booktitle>
<pages>61--66</pages>
<contexts>
<context position="23748" citStr="Maybury, 1991" startWordPosition="3908" endWordPosition="3909"> by evaluating strategies for managing initiative and for message presentation. However there are numerous dialogue strategies that an agent might use, e.g. to gather information, handle errors, or manage the dialogue interaction (Chu-Carroll and Carberry, 1995; Danieli and Gerbino, 1995; Hovy, 1993; McKeown, 1985; Moore and Paris, 1989). Previous work in natural language generation has proposed heuristics to determine an agent&apos;s choice of dialogue strategy, based on factors such as discourse focus, medium, style, and the content of previous explanations (McKeown, 1985; Moore and Paris, 1989; Maybury, 1991; Hovy, 1993). It should be possible to test experimentally whether an agent can automatically learn these heuristics since the methodology we propose is general, and could be applied to any dialogue strategy choice that an agent might make. Previous work has also proposed that an agent&apos;s choice of dialogue strategy can be treated as a stochastic optimization problem (Walker, 1993; Biermann and Long, 1996; Levin and Pieraccini, 1997). However, to our knowledge, these methods have not previously been applied to interactions with real users. The lack of an appropriate performance function has be</context>
</contexts>
<marker>Maybury, 1991</marker>
<rawString>M.T. Maybury. 1991. Planning multi-media explanations using communicative acts. In Proc. of the Ninth National Conf on Artificial Intelligence, pages 61-66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K R McKeown</author>
</authors>
<title>Discourse strategies for generating natural language text.</title>
<date>1985</date>
<journal>Artificial Intelligence,</journal>
<volume>27</volume>
<issue>1</issue>
<pages>1--42</pages>
<contexts>
<context position="23450" citStr="McKeown, 1985" startWordPosition="3862" endWordPosition="3864">ive, and for reading and summarizing messages. We show that ELVIS can learn that the System-Initiative strategy has higher utility than the Mixed-Initiative strategy, that Read-First is the best read strategy, and that Summarize-System is the best summary strategy. Here, our method was illustrated by evaluating strategies for managing initiative and for message presentation. However there are numerous dialogue strategies that an agent might use, e.g. to gather information, handle errors, or manage the dialogue interaction (Chu-Carroll and Carberry, 1995; Danieli and Gerbino, 1995; Hovy, 1993; McKeown, 1985; Moore and Paris, 1989). Previous work in natural language generation has proposed heuristics to determine an agent&apos;s choice of dialogue strategy, based on factors such as discourse focus, medium, style, and the content of previous explanations (McKeown, 1985; Moore and Paris, 1989; Maybury, 1991; Hovy, 1993). It should be possible to test experimentally whether an agent can automatically learn these heuristics since the methodology we propose is general, and could be applied to any dialogue strategy choice that an agent might make. Previous work has also proposed that an agent&apos;s choice of di</context>
</contexts>
<marker>McKeown, 1985</marker>
<rawString>K. R. McKeown. 1985. Discourse strategies for generating natural language text. Artificial Intelligence, 27(1): 1-42, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Moore</author>
<author>C L Paris</author>
</authors>
<title>Planning text for advisory dialogues.</title>
<date>1989</date>
<booktitle>In Proc. 27th Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="23474" citStr="Moore and Paris, 1989" startWordPosition="3865" endWordPosition="3868">ading and summarizing messages. We show that ELVIS can learn that the System-Initiative strategy has higher utility than the Mixed-Initiative strategy, that Read-First is the best read strategy, and that Summarize-System is the best summary strategy. Here, our method was illustrated by evaluating strategies for managing initiative and for message presentation. However there are numerous dialogue strategies that an agent might use, e.g. to gather information, handle errors, or manage the dialogue interaction (Chu-Carroll and Carberry, 1995; Danieli and Gerbino, 1995; Hovy, 1993; McKeown, 1985; Moore and Paris, 1989). Previous work in natural language generation has proposed heuristics to determine an agent&apos;s choice of dialogue strategy, based on factors such as discourse focus, medium, style, and the content of previous explanations (McKeown, 1985; Moore and Paris, 1989; Maybury, 1991; Hovy, 1993). It should be possible to test experimentally whether an agent can automatically learn these heuristics since the methodology we propose is general, and could be applied to any dialogue strategy choice that an agent might make. Previous work has also proposed that an agent&apos;s choice of dialogue strategy can be t</context>
</contexts>
<marker>Moore, Paris, 1989</marker>
<rawString>J. D. Moore and C. L. Paris. 1989. Planning text for advisory dialogues. In Proc. 27th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Russell</author>
<author>P Norvig</author>
</authors>
<title>Artificial Intelligence: A Modern Approach.</title>
<date>1995</date>
<publisher>Prentice Hall, N.J.</publisher>
<contexts>
<context position="3246" citStr="Russell and Norvig, 1995" startWordPosition="514" endWordPosition="517">o say in order to refine the selection: (D2) A: In the messages from Kim, there&apos;s 1 message about &amp;quot;Interviewing Antonio&amp;quot; and 1 message about &amp;quot;Meeting Today.&amp;quot; (D3) A: In the messages from Kim, there&apos;s 1 message about &amp;quot;Interviewing Antonio&amp;quot; and 1 message about &amp;quot;Meeting Today.&amp;quot; To hear the messages, say, &amp;quot;Interviewing Antonio&amp;quot; or &amp;quot;Meeting.&amp;quot; Decision theoretic planning can be applied to the problem of choosing among strategies, by associating a utility U with each strategy (action) choice and by positing that agents should adhere to the Maximum Expected Utility Principle (Keeney and Raiffa, 1976; Russell and Norvig, 1995), Maximum Expected Utility Principle: An optimal action is one that maximizes the expected utility of outcome states. An agent acts optimally by choosing a strategy a in state Si that maximizes U(S,). But how are the utility values U(Si) for each dialogue state Si derived? Several reinforcement learning algorithms based on dynamic programming specify a way to calculate U(S2) in terms of the utility of a successor state Si (Bellman, 1957; Watkins, 1989; Sutton, 1991; Barto et al., 1995). Thus if we know the utility for 1345 the final state of the dialogue, we can calculate the utilities for all</context>
<context position="18827" citStr="Russell and Norvig, 1995" startWordPosition="3092" endWordPosition="3095">nal state of the dialogue. 4 Applying Q-learning to ELVIS Experimental Data The basic idea is to apply the performance function to the measures logged for each dialogue Di, thereby replacing a range of measures with a single performance value Pi. Given the performance values Pi, any of a number of automatic learning algorithms can be used to determine which sequence of action choices (dialogue strategies) maximize utility, by using Pi as the utility for the final state of the dialogue D. Possible algorithms include Genetic Algorithms, Q-learning, TD-Learning, and Adaptive Dynamic Programming (Russell and Norvig, 1995). Here we use Q-learning to illustrate the method (Watkins, 1989). See (Fromer, 1998) for experiments using alternative algorithms. The utility of doing action a in state Si, U (a, St) (its Q-value), can be calculated terms of the utility of a successor state Si, by obeying the following recursive equation: U (a, Si) = R(Si) E 49:7 U (al , Si) where R(51) is a reward associated with being in state Si, a is a strategy from a finite set of strategies A that are admissable in state Si, and M,Pi is the probability of reaching state Si if strategy a is selected in state Si. In the experiments repor</context>
</contexts>
<marker>Russell, Norvig, 1995</marker>
<rawString>S. Russell and P. Norvig. 1995. Artificial Intelligence: A Modern Approach. Prentice Hall, N.J.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R S Sutton</author>
</authors>
<title>Planning by incremental dynamic programming.</title>
<date>1991</date>
<booktitle>In Proc. Ninth Conf on Machine Learning,</booktitle>
<pages>353--357</pages>
<publisher>Morgan-Kaufmann.</publisher>
<contexts>
<context position="3715" citStr="Sutton, 1991" startWordPosition="592" endWordPosition="593"> choice and by positing that agents should adhere to the Maximum Expected Utility Principle (Keeney and Raiffa, 1976; Russell and Norvig, 1995), Maximum Expected Utility Principle: An optimal action is one that maximizes the expected utility of outcome states. An agent acts optimally by choosing a strategy a in state Si that maximizes U(S,). But how are the utility values U(Si) for each dialogue state Si derived? Several reinforcement learning algorithms based on dynamic programming specify a way to calculate U(S2) in terms of the utility of a successor state Si (Bellman, 1957; Watkins, 1989; Sutton, 1991; Barto et al., 1995). Thus if we know the utility for 1345 the final state of the dialogue, we can calculate the utilities for all the earlier states. However, until recently there as been no way of determining a performance function for assigning a utility to the final state of a dialogue. This paper presents a method based on dynamic programming by which dialogue agents can learn to optimize their choice of dialogue strategies. We draw on the recently proposed PARADISE evaluation framework (Walker et al., 1997) to identify the important performance factors and to provide a performance funct</context>
</contexts>
<marker>Sutton, 1991</marker>
<rawString>R. S. Sutton. 1991. Planning by incremental dynamic programming. In Proc. Ninth Conf on Machine Learning, pages 353-357. Morgan-Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Walker</author>
<author>D Litman</author>
<author>C Kamm</author>
<author>A Abella</author>
</authors>
<title>PARADISE: A general framework for evaluating spoken dialogue agents.</title>
<date>1997</date>
<booktitle>In Proc. of the 35th Annual Meeting of the ACL,</booktitle>
<pages>271--280</pages>
<contexts>
<context position="1057" citStr="Walker et al., 1997" startWordPosition="157" endWordPosition="160">agent can learn to choose an optimal dialogue strategy. While it is widely agreed that dialogue strategies should be formulated in terms of communicative intentions, there has been little work on automatically optimizing an agent&apos;s choices when there are multiple ways to realize a communicative intention. Our method is based on a combination of learning algorithms and empirical evaluation techniques. The learning component of our method is based on algorithms for reinforcement learning, such as dynamic programming and Q-learning. The empirical component uses the PARADISE evaluation framework (Walker et al., 1997) to identify the important performance factors and to provide the performance function needed by the learning algorithm. We illustrate our method with a dialogue agent named ELVIS (EmaiL Voice Interactive System), that supports access to email over the phone. We show how ELVIS can learn to choose among alternate strategies for agent initiative, for reading messages, and for summarizing email folders. 1 Introduction This paper describes a novel method by which a dialogue agent can learn to choose an optimal dialogue strategy. The main problem for dialogue agents is deciding what information to </context>
<context position="4234" citStr="Walker et al., 1997" startWordPosition="680" endWordPosition="683">ate U(S2) in terms of the utility of a successor state Si (Bellman, 1957; Watkins, 1989; Sutton, 1991; Barto et al., 1995). Thus if we know the utility for 1345 the final state of the dialogue, we can calculate the utilities for all the earlier states. However, until recently there as been no way of determining a performance function for assigning a utility to the final state of a dialogue. This paper presents a method based on dynamic programming by which dialogue agents can learn to optimize their choice of dialogue strategies. We draw on the recently proposed PARADISE evaluation framework (Walker et al., 1997) to identify the important performance factors and to provide a performance function for calculating the utility of the final state of a dialogue. We illustrate our method with a dialogue agent named ELVIS (EmaiL Voice Interactive System), that supports access to email over the phone. We test alternate strategies for agent initiative, for reading messages, and for summarizing email folders. We report results from modeling a corpus of 232 spoken dialogues in which ELVIS conversed with human users to carry out a set of email tasks. 2 Method for Learning to Optimize Dialogue Strategy Selection Ou</context>
<context position="13145" citStr="Walker et al., 1997" startWordPosition="2142" endWordPosition="2145">ng PARADISE, the agent and the user had to exchange information about criteria for selecting messages and information within the message body in each scenario. Scenario 1.1 is typical. • 1.1: You are working at home in the morning and plan to go directly to a meeting when you go into 1347 work. Kim said she would send you a message telling you where and when the meeting is. Find out the Meeting Time and the Meeting Place. Scenario 1.1 is represented in terms of the attribute value matrix (AVM) in Table 1. Successful completion of a scenario requires that all attributevalues must be exchanged (Walker et al., 1997). The AVM representation for all six scenarios is similar to Table 1, and is independent of ELVIS&apos; s dialogue strategy. attribute actual value Selection Criteria Kim V Meeting Email.attl 10:30 Email.att2 2D516 Table 1: Attribute value matrix instantiation, Key for Email Scenario 1.1 3.3 Data Collection Three different methods are used to collect the measures for applying the PARADISE framework and the data for learning: (1) All of the dialogues are recorded; (2) The dialogue manager logs the agent s dialogue behavior and a number of other measures discussed below; (3) Users fill out web page f</context>
<context position="15573" citStr="Walker et al., 1997" startWordPosition="2544" endWordPosition="2547">ad, concept accuracy would be .5. used as a Mean Recognition Score MRS for the dialogue. The web page forms are the basis for calculating Task Success and User Satisfaction measures. Users reported their perceptions as to whether they had completed the task (Comp),2 and filled in an AVM with the information that they had acquired from the agent, e.g. the values for Email.attl and Email.att2 in Table 1. The AVM matrix supports calculating Task Success objectively by using the Kappa statistic to compare the information in the AVM that the users filled in with an AVM key such as that in Table 1 (Walker et al., 1997). In order to calculate User Satisfaction, users were asked to evaluate the agent&apos;s performance with a user satisfaction survey. The data from the survey resulted in user satisfaction values that range from 0 to 33. See (Walker et al., 1998) for more details. 3.4 Deriving a Performance Function Overall, the results showed that users could successfully complete the tasks with all versions of ELVIS. Most users completed each task in about 5 minutes and average K over all subjects and tasks was .82. However, there were differences between strategies; as an example see Table 2. Measure SYSTEM (SI)</context>
<context position="16987" citStr="Walker et al., 1997" startWordPosition="2788" endWordPosition="2791">Ins 5.2 3.5 Recognizer Rejects .98 1.67 User Satisfaction 26.6 23.7 Table 2: Performance measure means per dialogue for Initiative Strategies PARADISE provides a way to calculate dialogue agent performance as a linear combination of a number of simpler metrics that can be directly measured such as those in Table 2. Performance for any (sub)dialogue D is defined by the following equation: Performance = (a *Ar(K)) - E wi*Ar(ci) 2 Yes,No responses are converted to 1,0. 1348 where a is a weight on is, ci are the cost functions, which are weighted by wi, and Al is a Z score normalization function (Walker et al., 1997; Cohen, 1995). The Z score normalization function ensures that, when the weights a and wi are solved for, that the magnitude of the weights reflect the magnitude of the contribution of that factor to performance. The performance function is derived through multivariate linear regression with User Satisfaction as the dependent variable and all the other measures as independent variables (Walker et al., 1997). See Table 2. In the ELVIS data, an initial regression over the measures in Table 2 suggests that Comp, MRS and ET are the only significant contributors to User Satisfaction. A second regr</context>
<context position="24440" citStr="Walker et al., 1997" startWordPosition="4017" endWordPosition="4020">gent can automatically learn these heuristics since the methodology we propose is general, and could be applied to any dialogue strategy choice that an agent might make. Previous work has also proposed that an agent&apos;s choice of dialogue strategy can be treated as a stochastic optimization problem (Walker, 1993; Biermann and Long, 1996; Levin and Pieraccini, 1997). However, to our knowledge, these methods have not previously been applied to interactions with real users. The lack of an appropriate performance function has been a critical methodological limitation. We use the PARADISE framework (Walker et al., 1997) to derive an empirically motivated performance function, that combines both subjective user preferences and objective system performance measures into a single function. It would have been impossible to predict a priori which dialogue factors influence the usability of a dialogue agent, and to what degree. Our performance equation shows that both dialogue quality and efficiency measures contribute to agent performance, but that dialogue quality measures have a greater influence. Furthermore, in contrast to assuming an a priori model, we use the dialogues from real user-system interactions to </context>
</contexts>
<marker>Walker, Litman, Kamm, Abella, 1997</marker>
<rawString>M. A. Walker, D. Litman, C. Kamm, and A. Abella. 1997. PARADISE: A general framework for evaluating spoken dialogue agents. In Proc. of the 35th Annual Meeting of the ACL, pp. 271-280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Walker</author>
<author>J Fromer</author>
<author>G Di Fabbrizio</author>
<author>C Mestel</author>
<author>D Hindle</author>
</authors>
<title>What can I say: Evaluating a spoken language interface to email.</title>
<date>1998</date>
<booktitle>In Proc. of the Conf on Computer Human Interaction (CHI 98).</booktitle>
<marker>Walker, Fromer, Di Fabbrizio, Mestel, Hindle, 1998</marker>
<rawString>M. Walker, J. Fromer, G. Di Fabbrizio, C. Mestel, and D. Hindle. 1998. What can I say: Evaluating a spoken language interface to email. In Proc. of the Conf on Computer Human Interaction (CHI 98).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Walker</author>
</authors>
<title>Informational Redundancy and Resource Bounds in Dialogue.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="24131" citStr="Walker, 1993" startWordPosition="3971" endWordPosition="3972">n has proposed heuristics to determine an agent&apos;s choice of dialogue strategy, based on factors such as discourse focus, medium, style, and the content of previous explanations (McKeown, 1985; Moore and Paris, 1989; Maybury, 1991; Hovy, 1993). It should be possible to test experimentally whether an agent can automatically learn these heuristics since the methodology we propose is general, and could be applied to any dialogue strategy choice that an agent might make. Previous work has also proposed that an agent&apos;s choice of dialogue strategy can be treated as a stochastic optimization problem (Walker, 1993; Biermann and Long, 1996; Levin and Pieraccini, 1997). However, to our knowledge, these methods have not previously been applied to interactions with real users. The lack of an appropriate performance function has been a critical methodological limitation. We use the PARADISE framework (Walker et al., 1997) to derive an empirically motivated performance function, that combines both subjective user preferences and objective system performance measures into a single function. It would have been impossible to predict a priori which dialogue factors influence the usability of a dialogue agent, an</context>
</contexts>
<marker>Walker, 1993</marker>
<rawString>M. A. Walker. 1993. Informational Redundancy and Resource Bounds in Dialogue. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Watkins</author>
</authors>
<title>Models of Delayed Reinforcement Learning.</title>
<date>1989</date>
<tech>Ph.D. thesis,</tech>
<institution>Cambridge University.</institution>
<contexts>
<context position="3701" citStr="Watkins, 1989" startWordPosition="590" endWordPosition="591">rategy (action) choice and by positing that agents should adhere to the Maximum Expected Utility Principle (Keeney and Raiffa, 1976; Russell and Norvig, 1995), Maximum Expected Utility Principle: An optimal action is one that maximizes the expected utility of outcome states. An agent acts optimally by choosing a strategy a in state Si that maximizes U(S,). But how are the utility values U(Si) for each dialogue state Si derived? Several reinforcement learning algorithms based on dynamic programming specify a way to calculate U(S2) in terms of the utility of a successor state Si (Bellman, 1957; Watkins, 1989; Sutton, 1991; Barto et al., 1995). Thus if we know the utility for 1345 the final state of the dialogue, we can calculate the utilities for all the earlier states. However, until recently there as been no way of determining a performance function for assigning a utility to the final state of a dialogue. This paper presents a method based on dynamic programming by which dialogue agents can learn to optimize their choice of dialogue strategies. We draw on the recently proposed PARADISE evaluation framework (Walker et al., 1997) to identify the important performance factors and to provide a per</context>
<context position="18892" citStr="Watkins, 1989" startWordPosition="3104" endWordPosition="3105">The basic idea is to apply the performance function to the measures logged for each dialogue Di, thereby replacing a range of measures with a single performance value Pi. Given the performance values Pi, any of a number of automatic learning algorithms can be used to determine which sequence of action choices (dialogue strategies) maximize utility, by using Pi as the utility for the final state of the dialogue D. Possible algorithms include Genetic Algorithms, Q-learning, TD-Learning, and Adaptive Dynamic Programming (Russell and Norvig, 1995). Here we use Q-learning to illustrate the method (Watkins, 1989). See (Fromer, 1998) for experiments using alternative algorithms. The utility of doing action a in state Si, U (a, St) (its Q-value), can be calculated terms of the utility of a successor state Si, by obeying the following recursive equation: U (a, Si) = R(Si) E 49:7 U (al , Si) where R(51) is a reward associated with being in state Si, a is a strategy from a finite set of strategies A that are admissable in state Si, and M,Pi is the probability of reaching state Si if strategy a is selected in state Si. In the experiments reported here, the reward associated with each state, R(51), is zero.3</context>
</contexts>
<marker>Watkins, 1989</marker>
<rawString>C. J. Watkins. 1989. Models of Delayed Reinforcement Learning. Ph.D. thesis, Cambridge University.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>