<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.027585">
<title confidence="0.9975795">
BioinformaticsUA: Machine Learning and Rule-Based Recognition of
Disorders and Clinical Attributes from Patient Notes
</title>
<author confidence="0.996597">
S´ergio Matos
</author>
<affiliation confidence="0.9899015">
DETI/IEETA
University of Aveiro
</affiliation>
<address confidence="0.754257">
3810-193 Aveiro, Portugal
</address>
<email confidence="0.997204">
aleixomatos@ua.pt
</email>
<author confidence="0.986547">
Jos´e Sequeira
</author>
<affiliation confidence="0.9854215">
DETI/IEETA
University of Aveiro
</affiliation>
<address confidence="0.754441">
3810-193 Aveiro, Portugal
</address>
<email confidence="0.997367">
sequeira@ua.pt
</email>
<author confidence="0.888505">
Jos´e Luis Oliveira
</author>
<affiliation confidence="0.9418905">
DETI/IEETA
University of Aveiro
</affiliation>
<address confidence="0.755661">
3810-193 Aveiro, Portugal
</address>
<email confidence="0.998208">
jlo@ua.pt
</email>
<sectionHeader confidence="0.995642" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999427277777778">
Natural language processing and text analy-
sis methods offer the potential of uncovering
hidden associations from large amounts of un-
processed texts. The SemEval-2015 Analy-
sis of Clinical Text task aimed at fostering re-
search on the application of these methods in
the clinical domain. The proposed task con-
sisted of disorder identification with normal-
ization to SNOMED-CT concepts, and disor-
der attribute identification, or template filling.
We participated in both sub-tasks, using a
combination of machine-learning and rules
for recognizing and normalizing disease men-
tions, and rule-based methods for template
filling. We achieved an F-score of 71.2% in
the entity recognition and normalization task,
and a slot weighted accuracy of 69.5% in the
template filling task.
</bodyText>
<sectionHeader confidence="0.998995" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999798804347826">
Biomedical text mining offers the promise of lever-
aging the huge amounts of information available
on scientific documents to help raise new hypothe-
ses and uncover hidden knowledge. Biomedical
text mining (TM) has been an important focus of
research during the last years, sustained by the
high volumes of data, the diverse computational
and multi-disciplinary challenges posed, and by the
potential impact of new discoveries (Simpson and
Demner-Fushman, 2012). These benefits have been
demonstrated in recent studies in which text mining
methods were used to suggest biomarkers for diag-
nosis and for measuring disease progression, targets
for new drugs, or new uses for existing drugs (Fri-
jters et al., 2010). Likewise, clinical information
stored as natural language text in discharge notes
and reports could be exploited to identify important
associations, and this has led to an increased interest
in applying text mining techniques to such texts, in
order to extract information related to diseases, med-
ications, and adverse drug events, for example (Zhu
et al., 2013).
Research efforts in biomedical text mining have
led to the development of various methods and tools
for the recognition of diverse entities, including
species names, genes and proteins, chemicals and
drugs, anatomical concepts and diseases. These
methods are based on dictionaries, rules, and ma-
chine learning, or a combination of those depend-
ing on the specificities and requirements of each
concept type. After identifying entity mentions in
text, it becomes necessary to perform entity normal-
ization, which consists in assigning a specific con-
cept identifier to each entity. This is usually per-
formed by matching the identified entities against
a knowledge-base, possibly evaluating the textual
context in which the entity occurred to identify the
best matching concept.
Following up on the 2014 task, in which the ob-
jective was the identification and normalization of
disease concepts in clinical texts (Pradhan et al.,
2014), two subtasks were defined for the SemEval-
2015 Analysis of Clinical Text task. Task 1 con-
sisted of recognizing concepts belonging to the
‘disorders’ semantic group of the Unified Medical
Language System (UMLS) and normalizing to the
</bodyText>
<page confidence="0.995256">
422
</page>
<note confidence="0.5395495">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 422–426,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<figure confidence="0.999435631578947">
Documents Annotated
Documents
Reader
Sentence
Tagger
NLP
Processing pipeline
Dictionaries
Dictionary
Tagger
ML Tagger
Models
Abbreviation resolution
Post-processing
Disambiguator
Custom Module
Relation extractor
Indexer
Writer
</figure>
<figureCaption confidence="0.999969">
Figure 1: Neji’s processing pipeline used for annotating the documents. Dashed boxes indicate optional modules.
</figureCaption>
<bodyText confidence="0.999839933333334">
SNOMED CT1 terminology, and Task 2 consisted
of identifying and normalizing specific attributes for
each disorder mention, including negation, severity,
and body location, for example. The task made use
of the ShARe corpus (Pradhan et al., 2013), which
contains manually annotated clinical notes from the
MIMIC II database2 (Saeed et al., 2011). The task
corpus comprised 531 documents, divided into a
training portion with 298 documents, a development
portion with 133 documents, and a test portion with
100 documents.
In this paper, we present a combined machine-
learning and rule-based approach for these tasks,
supported by a modular text analysis and annotation
pipeline.
</bodyText>
<sectionHeader confidence="0.990084" genericHeader="introduction">
2 Methods
</sectionHeader>
<bodyText confidence="0.9999831">
Our approach consists of three sequential steps,
namely: entity recognition, rule-based span adjust-
ment and normalization, and rule-based template
filling. For entity recognition we used Gimli (Cam-
pos et al., 2013b), an open-source tool for training
machine learning (ML) models that includes simple
configuration of the feature extraction process, and
Neji, a framework for biomedical concept recogni-
tion, integrating modules for natural language pro-
cessing (NLP) and information extraction (IE), spe-
</bodyText>
<footnote confidence="0.99934">
1http://www.ihtsdo.org/snomed-ct/
2http://mimic.physionet.org/database.html
</footnote>
<bodyText confidence="0.972061">
cially tuned for the biomedical domain (Campos et
al., 2013a). Figure 1 shows the complete processing
pipeline.
</bodyText>
<subsectionHeader confidence="0.986059">
2.1 Entity Recognition
</subsectionHeader>
<bodyText confidence="0.999883727272727">
We applied a supervised machine-learning ap-
proach, based on Conditional Random Fields
(CRFs) (Lafferty et al., 2001; McCallum, 2002).
The BIO (Beginning, Inside, Outside) scheme was
used to encode the entity annotations. To select the
best combination of features, we performed back-
ward feature elimination using the supplied train-
ing and development data to create and evaluate the
models. We then used all the data to train a first-
order CRF model with the final feature set, which
consisted of the following features:
</bodyText>
<listItem confidence="0.9987377">
• NLP features:
– Token and lemma
• Orthographic features:
– Capitalization (e.g., “StartCap” and “All-
Caps”);
– Digits and capitalized characters counting
(e.g., “TwoDigit” and “TwoCap”);
– Symbols (e.g., “Dash”, “Dot” and
“Comma”);
• Morphological features:
</listItem>
<page confidence="0.939753">
423
</page>
<listItem confidence="0.743220571428571">
– Suffixes and char n-grams of 2, 3 and 4
characters;
• Local context:
– Conjunctions of lemma and POS features,
built from the windows 1-1, 01, 1-2, -11,
10, 11, 1-1, 11 and 1-3, -11 around the
current token.
</listItem>
<bodyText confidence="0.9998413">
Apart from the ML model, documents were also
annotated with dictionaries for the UMLS ‘Dis-
orders’ semantic group and a specially compiled
acronyms dictionary, as used in the 2014 edition of
the task (Matos et al., 2014). In total, these dictio-
naries contain almost 1.5 million terms, of which
525 thousand (36%) are distinct terms, for nearly
293 thousand distinct concept identifiers. Including
this dictionary-matching step produced a small im-
provement in terms of F-score.
</bodyText>
<subsectionHeader confidence="0.976558">
2.2 Normalization
</subsectionHeader>
<bodyText confidence="0.999988">
According to the task description, only those UMLS
concepts that could be mapped to a SNOMED-CT
identifier should be considered in the normalization
step, while all other entities should be added to the
results without a concept identifier. To achieve this
step, we indexed the terms of the UMLS concepts
that included a SNOMED-CT identifier in a Solr 3
instance. Additionally, we also indexed each term
that occurred in the training and development data,
together with the corresponding identifier.
To perform normalization of an identified entity
mention, we follow a series of steps. First we search
the index for the exact term and, if it is found as a
gold-standard annotation on the training data, we as-
sign the same identifier to the new mention. If multi-
ple identifiers were used on the training data for the
same term, we keep the most commonly assigned
one. If the exact mention is not found on the train-
ing data, we try to remove a set of 162 prefix (e.g.
‘chronic’, ‘acute’, ‘large’) and 48 suffix terms (e.g.
‘changes’, ‘episodes’) obtained from an error analy-
sis on the development data. We then look for this
adjusted term on the gold standard annotations and
on the UMLS concept synonyms, and use the cor-
responding identifier and the adjusted mention span.
</bodyText>
<footnote confidence="0.736366">
3http://lucene.apache.org/solr/
</footnote>
<bodyText confidence="0.999981166666667">
Finally, we try to expand the term to include anatom-
ical regions occurring before or after the identified
disorder mention, in order to identify more specific
concepts. If such a concept is found on the index,
the corrected span is used, together with the corre-
sponding identifier.
</bodyText>
<subsectionHeader confidence="0.999402">
2.3 Template Filling
</subsectionHeader>
<bodyText confidence="0.999977">
This subtask consists of identifying various at-
tributes of the disorders, such as negation or un-
certainty, and normalizing their values according to
the nomenclature specified by the task. To address
this task, we followed a rule-based approach. For
each type of attribute, or slot, we compiled the cue
words and the corresponding normalized value from
the training and development data. We then created
patterns, implemented through regular expressions,
to locate these possible cues in the vicinity of each
disorder term. To apply the regular expressions, we
replace each entity mention in the texts by a generic
placeholder, adjusting the cue word spans accord-
ingly when a match is found. For example, to fill the
‘Severity’ attribute we look for the occurrence of a
cue word, associated to this attribute in the training
data, that occurs up to n4 characters before or after
a disorder mention. This can be expressed by the
following regular expression, in which only two al-
ternative cue words are shown for brevity:
</bodyText>
<equation confidence="0.9631855">
(mild|sharp|...)\s.{0,15}?__DISO__ |
__DISO__\s.{0,15}?(mild|sharp|...)
</equation>
<sectionHeader confidence="0.999322" genericHeader="related work">
3 Results and Discussion
</sectionHeader>
<subsectionHeader confidence="0.988909">
3.1 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999899333333333">
Task 1 was evaluated by strict and relaxed F-scores.
In the first case, the identified text span has to be ex-
actly the same as the gold-standard annotation, and
the predicted concept identifier has to match the gold
annotation. In the second case, a prediction is con-
sidered a true-positive is there is any word overlap
between the predicted span and the gold-standard,
as long as the identified is correctly predicted.
Task 2 was evaluated in terms of weighted accu-
racy, which is calculated using a pre-assigned weight
for each slot based on its prevalence in the training
set.
</bodyText>
<footnote confidence="0.997957">
4n was empirically set as 5 for the body location attribute,
and 15 for all other attributes
</footnote>
<page confidence="0.993457">
424
</page>
<table confidence="0.99868">
Task 1 performance (P / R / F)
Development Test
Run Strict Relaxed Strict Relaxed
1 48.1 / 54.4 / 51.0 51.8 / 58.0 / 54.7 0.669 / 0.738 / 0.702 0.698 / 0.769 / 0.732
2 62.3 / 70.6 / 66.2 67.5 / 74.7 / 70.9 0.690 / 0.736 / 0.712 0.719 / 0.766 / 0.742
3 62.3 / 70.5 / 66.1 67.4 / 74.5 / 70.8 0.691 / 0.735 / 0.712 0.720 / 0.765 / 0.742
</table>
<tableCaption confidence="0.99928">
Table 1: Development results and official results on the test dataset, for Task 1. P: Precision; R: Recall; F: F-score.
</tableCaption>
<subsectionHeader confidence="0.999989">
3.2 Test Results
</subsectionHeader>
<bodyText confidence="0.999832">
We submitted three runs of annotations for the doc-
uments in the test set, as described below:
</bodyText>
<listItem confidence="0.9868355">
• Run 1: In this run, the identified disorder men-
tions were not first checked against the training
data annotations;
• Run 2: The identified disorder mentions were
first checked against the training data annota-
tions and the corresponding identifier was used;
• Run 3: Same as Run 2, but the machine learn-
ing model was trained only on discharge doc-
uments, that is, other document types were not
used in the training.
</listItem>
<bodyText confidence="0.9999138">
Table 1 shows the results obtained on the devel-
opment set, and the official results obtained on the
test set for each submitted run in Task 1.
As can be observed from the results, using the
identifiers assigned in the training data for disease
mentions that re-occur in the test data has a very
positive impact on the results, increasing precision
by 2%. Although this approach may be considered
to artificially improve the results, the rationale for
using it is that human annotators tend to re-use the
same identifier in the case of a ambiguous term. The
same might also be true for clinical coders when pro-
cessing the patient notes.
Comparing our results to the best submitted runs,
we verify that we obtain the best recall rates when
considering both strict and relaxed scores, but with a
significant drop in precision when compared to those
results.
Figure 2 illustrates the results obtained on the
template filling task. We achieved a slot weighted
accuracy of 69.5%. Comparing the results, we
achieved the best accuracy for the disease CUI slot.
On the other hand, we achieved considerable lower
accuracies on the body location and conditional
slots, when compared to the top performing runs.
</bodyText>
<sectionHeader confidence="0.999558" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999953266666667">
We present results for the recognition, normalization
and template filling of disorder concepts in clinical
texts, using a machine-learning and rule-based ap-
proach. We achieved a strict F-score of 71.2% and
a relaxed F-score of 74.2%, and obtained the best
recall under both evaluation modes. One of the rea-
sons for the lower precision is related to the normal-
ization method. As future work, we will continue
developing this step.
We applied a simple rule-based approach for the
template filling task, and achieved a weighted accu-
racy of 69.5%. We aim to continue improving this
information extraction step, by acquiring a larger set
of possible cue words and revising some of the ex-
traction rules.
</bodyText>
<sectionHeader confidence="0.994949" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999778">
This work was supported by National Funds
through FCT - Foundation for Science and Tech-
nology, in the context of the project PEst-
OE/EEI/UI0127/2014. S. Matos is funded by FCT
under the FCT Investigator programme.
</bodyText>
<sectionHeader confidence="0.998599" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.973244777777778">
David Campos, S´ergio Matos, and Jos´e L. Oliveira.
2013a. A modular framework for biomedical concept
recognition. BMC Bioinformatics, 14:281.
David Campos, S´ergio Matos, and Jos´e L. Oliveira.
2013b. Gimli: open source and high-performance
biomedical name recognition. BMC bioinformatics,
14:54.
Raoul Frijters, Marianne van Vugt, Ruben Smeets,
Ren C. van Schaik, Jacob de Vlieg, and Wynand
</reference>
<page confidence="0.998888">
425
</page>
<figureCaption confidence="0.969298">
Figure 2: Official results for disease template filling (Task 2).
</figureCaption>
<reference confidence="0.988624955555556">
Alkema. 2010. Literature mining for the discovery
of hidden connections between drugs, genes and dis-
eases. PLoS Computational Biology, 6(9):e1000943.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In Proceedings of the Eighteenth International Con-
ference on Machine Learning, ICML ’01, pages 282–
289, San Francisco, CA, USA.
S´ergio Matos, Tiago Nunes, and Jos´e L. Oliveira. 2014.
BioinformaticsUA: Concept recognition in clinical
narratives using a modular and highly efficient text
processing framework. In Proceedings of the 8th
International Workshop on Semantic Evaluation (Se-
mEval 2014), pages 135–139.
Andrew K. McCallum. 2002. MALLET: A Machine
Learning for Language Toolkit.
Sameer Pradhan, Noemie Elhadad, Brett South, David
Martinez, Lee Christensen, Amy Vogel, Hanna Suomi-
nen, Wendy Chapman, and Guergana Savova. 2013.
Task 1: ShARe/CLEF eHealth Evaluation Lab 2013.
Online Working Notes of the CLEF 2013 Evaluation
Labs and Workshop.
Sameer Pradhan, No´emie Elhadad, Wendy Chapman,
Suresh Manandhar, and Guergana Savova. 2014.
SemEval-2014 Task 7: Analysis of clinical text. In
Proceedings of the 8th International Workshop on
Semantic Evaluation (SemEval 2014), pages 54–62,
Dublin, Ireland, August.
Mohammed Saeed, Mauricio Villarroel, Andrew Reis-
ner, Gari Clifford, Li-Wei Lehman, George Moody,
Thomas Heldt, Tin Kyaw, Benjamin Moody, and
Roger Mark. 2011. Multiparameter Intelligent Moni-
toring in Intensive Care II (MIMIC-II): a public-access
intensive care unit database. Critical Care Medicine,
39(5):952.
Matthew S. Simpson and Dina Demner-Fushman. 2012.
Biomedical text mining: A survey of recent progress.
In Charu C. Aggarwal and ChengXiang Zhai, editors,
Mining Text Data, pages 465–517.
Fei Zhu, Preecha Patumcharoenpol, Cheng Zhang, Yang
Yang, Jonathan Chan, Asawin Meechai, Wanwipa
Vongsangnak, and Bairong Shen. 2013. Biomedi-
cal text mining and its applications in cancer research.
Journal of Biomedical Informatics, 46(2):200–211.
</reference>
<page confidence="0.999181">
426
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.843337">
<title confidence="0.9993525">BioinformaticsUA: Machine Learning and Rule-Based Recognition Disorders and Clinical Attributes from Patient Notes</title>
<author confidence="0.998506">S´ergio</author>
<affiliation confidence="0.999902">University of</affiliation>
<address confidence="0.989985">3810-193 Aveiro,</address>
<email confidence="0.995668">aleixomatos@ua.pt</email>
<author confidence="0.88911">Jos´e</author>
<affiliation confidence="0.999873">University of</affiliation>
<address confidence="0.98857">3810-193 Aveiro,</address>
<email confidence="0.997054">sequeira@ua.pt</email>
<author confidence="0.993593">Jos´e Luis</author>
<affiliation confidence="0.999899">University of</affiliation>
<address confidence="0.992371">3810-193 Aveiro,</address>
<email confidence="0.998294">jlo@ua.pt</email>
<abstract confidence="0.999694210526316">Natural language processing and text analysis methods offer the potential of uncovering hidden associations from large amounts of unprocessed texts. The SemEval-2015 Analysis of Clinical Text task aimed at fostering research on the application of these methods in the clinical domain. The proposed task consisted of disorder identification with normalization to SNOMED-CT concepts, and disorder attribute identification, or template filling. We participated in both sub-tasks, using a combination of machine-learning and rules for recognizing and normalizing disease mentions, and rule-based methods for template filling. We achieved an F-score of 71.2% in the entity recognition and normalization task, and a slot weighted accuracy of 69.5% in the template filling task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Campos</author>
<author>S´ergio Matos</author>
<author>Jos´e L Oliveira</author>
</authors>
<title>A modular framework for biomedical concept recognition.</title>
<date>2013</date>
<journal>BMC Bioinformatics,</journal>
<pages>14--281</pages>
<contexts>
<context position="4842" citStr="Campos et al., 2013" startWordPosition="706" endWordPosition="710"> clinical notes from the MIMIC II database2 (Saeed et al., 2011). The task corpus comprised 531 documents, divided into a training portion with 298 documents, a development portion with 133 documents, and a test portion with 100 documents. In this paper, we present a combined machinelearning and rule-based approach for these tasks, supported by a modular text analysis and annotation pipeline. 2 Methods Our approach consists of three sequential steps, namely: entity recognition, rule-based span adjustment and normalization, and rule-based template filling. For entity recognition we used Gimli (Campos et al., 2013b), an open-source tool for training machine learning (ML) models that includes simple configuration of the feature extraction process, and Neji, a framework for biomedical concept recognition, integrating modules for natural language processing (NLP) and information extraction (IE), spe1http://www.ihtsdo.org/snomed-ct/ 2http://mimic.physionet.org/database.html cially tuned for the biomedical domain (Campos et al., 2013a). Figure 1 shows the complete processing pipeline. 2.1 Entity Recognition We applied a supervised machine-learning approach, based on Conditional Random Fields (CRFs) (Laffert</context>
</contexts>
<marker>Campos, Matos, Oliveira, 2013</marker>
<rawString>David Campos, S´ergio Matos, and Jos´e L. Oliveira. 2013a. A modular framework for biomedical concept recognition. BMC Bioinformatics, 14:281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Campos</author>
<author>S´ergio Matos</author>
<author>Jos´e L Oliveira</author>
</authors>
<title>Gimli: open source and high-performance biomedical name recognition.</title>
<date>2013</date>
<journal>BMC bioinformatics,</journal>
<pages>14--54</pages>
<contexts>
<context position="4842" citStr="Campos et al., 2013" startWordPosition="706" endWordPosition="710"> clinical notes from the MIMIC II database2 (Saeed et al., 2011). The task corpus comprised 531 documents, divided into a training portion with 298 documents, a development portion with 133 documents, and a test portion with 100 documents. In this paper, we present a combined machinelearning and rule-based approach for these tasks, supported by a modular text analysis and annotation pipeline. 2 Methods Our approach consists of three sequential steps, namely: entity recognition, rule-based span adjustment and normalization, and rule-based template filling. For entity recognition we used Gimli (Campos et al., 2013b), an open-source tool for training machine learning (ML) models that includes simple configuration of the feature extraction process, and Neji, a framework for biomedical concept recognition, integrating modules for natural language processing (NLP) and information extraction (IE), spe1http://www.ihtsdo.org/snomed-ct/ 2http://mimic.physionet.org/database.html cially tuned for the biomedical domain (Campos et al., 2013a). Figure 1 shows the complete processing pipeline. 2.1 Entity Recognition We applied a supervised machine-learning approach, based on Conditional Random Fields (CRFs) (Laffert</context>
</contexts>
<marker>Campos, Matos, Oliveira, 2013</marker>
<rawString>David Campos, S´ergio Matos, and Jos´e L. Oliveira. 2013b. Gimli: open source and high-performance biomedical name recognition. BMC bioinformatics, 14:54.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Raoul Frijters</author>
<author>Marianne van Vugt</author>
<author>Ruben Smeets</author>
<author>Ren C van Schaik</author>
<author>Jacob de Vlieg</author>
</authors>
<note>and Wynand</note>
<marker>Frijters, van Vugt, Smeets, van Schaik, de Vlieg, </marker>
<rawString>Raoul Frijters, Marianne van Vugt, Ruben Smeets, Ren C. van Schaik, Jacob de Vlieg, and Wynand</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alkema</author>
</authors>
<title>Literature mining for the discovery of hidden connections between drugs, genes and diseases.</title>
<date>2010</date>
<journal>PLoS Computational Biology,</journal>
<volume>6</volume>
<issue>9</issue>
<marker>Alkema, 2010</marker>
<rawString>Alkema. 2010. Literature mining for the discovery of hidden connections between drugs, genes and diseases. PLoS Computational Biology, 6(9):e1000943.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01,</booktitle>
<pages>282--289</pages>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="5456" citStr="Lafferty et al., 2001" startWordPosition="786" endWordPosition="789">., 2013b), an open-source tool for training machine learning (ML) models that includes simple configuration of the feature extraction process, and Neji, a framework for biomedical concept recognition, integrating modules for natural language processing (NLP) and information extraction (IE), spe1http://www.ihtsdo.org/snomed-ct/ 2http://mimic.physionet.org/database.html cially tuned for the biomedical domain (Campos et al., 2013a). Figure 1 shows the complete processing pipeline. 2.1 Entity Recognition We applied a supervised machine-learning approach, based on Conditional Random Fields (CRFs) (Lafferty et al., 2001; McCallum, 2002). The BIO (Beginning, Inside, Outside) scheme was used to encode the entity annotations. To select the best combination of features, we performed backward feature elimination using the supplied training and development data to create and evaluate the models. We then used all the data to train a firstorder CRF model with the final feature set, which consisted of the following features: • NLP features: – Token and lemma • Orthographic features: – Capitalization (e.g., “StartCap” and “AllCaps”); – Digits and capitalized characters counting (e.g., “TwoDigit” and “TwoCap”); – Symbo</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01, pages 282– 289, San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S´ergio Matos</author>
<author>Tiago Nunes</author>
<author>Jos´e L Oliveira</author>
</authors>
<title>BioinformaticsUA: Concept recognition in clinical narratives using a modular and highly efficient text processing framework.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>135--139</pages>
<contexts>
<context position="6553" citStr="Matos et al., 2014" startWordPosition="967" endWordPosition="970">on (e.g., “StartCap” and “AllCaps”); – Digits and capitalized characters counting (e.g., “TwoDigit” and “TwoCap”); – Symbols (e.g., “Dash”, “Dot” and “Comma”); • Morphological features: 423 – Suffixes and char n-grams of 2, 3 and 4 characters; • Local context: – Conjunctions of lemma and POS features, built from the windows 1-1, 01, 1-2, -11, 10, 11, 1-1, 11 and 1-3, -11 around the current token. Apart from the ML model, documents were also annotated with dictionaries for the UMLS ‘Disorders’ semantic group and a specially compiled acronyms dictionary, as used in the 2014 edition of the task (Matos et al., 2014). In total, these dictionaries contain almost 1.5 million terms, of which 525 thousand (36%) are distinct terms, for nearly 293 thousand distinct concept identifiers. Including this dictionary-matching step produced a small improvement in terms of F-score. 2.2 Normalization According to the task description, only those UMLS concepts that could be mapped to a SNOMED-CT identifier should be considered in the normalization step, while all other entities should be added to the results without a concept identifier. To achieve this step, we indexed the terms of the UMLS concepts that included a SNOM</context>
</contexts>
<marker>Matos, Nunes, Oliveira, 2014</marker>
<rawString>S´ergio Matos, Tiago Nunes, and Jos´e L. Oliveira. 2014. BioinformaticsUA: Concept recognition in clinical narratives using a modular and highly efficient text processing framework. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 135–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew K McCallum</author>
</authors>
<title>MALLET: A Machine Learning for Language Toolkit.</title>
<date>2002</date>
<contexts>
<context position="5473" citStr="McCallum, 2002" startWordPosition="790" endWordPosition="791">ce tool for training machine learning (ML) models that includes simple configuration of the feature extraction process, and Neji, a framework for biomedical concept recognition, integrating modules for natural language processing (NLP) and information extraction (IE), spe1http://www.ihtsdo.org/snomed-ct/ 2http://mimic.physionet.org/database.html cially tuned for the biomedical domain (Campos et al., 2013a). Figure 1 shows the complete processing pipeline. 2.1 Entity Recognition We applied a supervised machine-learning approach, based on Conditional Random Fields (CRFs) (Lafferty et al., 2001; McCallum, 2002). The BIO (Beginning, Inside, Outside) scheme was used to encode the entity annotations. To select the best combination of features, we performed backward feature elimination using the supplied training and development data to create and evaluate the models. We then used all the data to train a firstorder CRF model with the final feature set, which consisted of the following features: • NLP features: – Token and lemma • Orthographic features: – Capitalization (e.g., “StartCap” and “AllCaps”); – Digits and capitalized characters counting (e.g., “TwoDigit” and “TwoCap”); – Symbols (e.g., “Dash”,</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew K. McCallum. 2002. MALLET: A Machine Learning for Language Toolkit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Noemie Elhadad</author>
<author>Brett South</author>
<author>David Martinez</author>
<author>Lee Christensen</author>
<author>Amy Vogel</author>
<author>Hanna Suominen</author>
<author>Wendy Chapman</author>
<author>Guergana Savova</author>
</authors>
<date>2013</date>
<booktitle>Task 1: ShARe/CLEF eHealth Evaluation Lab 2013. Online Working Notes of the CLEF 2013 Evaluation Labs and Workshop.</booktitle>
<contexts>
<context position="4188" citStr="Pradhan et al., 2013" startWordPosition="608" endWordPosition="611">iation for Computational Linguistics Documents Annotated Documents Reader Sentence Tagger NLP Processing pipeline Dictionaries Dictionary Tagger ML Tagger Models Abbreviation resolution Post-processing Disambiguator Custom Module Relation extractor Indexer Writer Figure 1: Neji’s processing pipeline used for annotating the documents. Dashed boxes indicate optional modules. SNOMED CT1 terminology, and Task 2 consisted of identifying and normalizing specific attributes for each disorder mention, including negation, severity, and body location, for example. The task made use of the ShARe corpus (Pradhan et al., 2013), which contains manually annotated clinical notes from the MIMIC II database2 (Saeed et al., 2011). The task corpus comprised 531 documents, divided into a training portion with 298 documents, a development portion with 133 documents, and a test portion with 100 documents. In this paper, we present a combined machinelearning and rule-based approach for these tasks, supported by a modular text analysis and annotation pipeline. 2 Methods Our approach consists of three sequential steps, namely: entity recognition, rule-based span adjustment and normalization, and rule-based template filling. For</context>
</contexts>
<marker>Pradhan, Elhadad, South, Martinez, Christensen, Vogel, Suominen, Chapman, Savova, 2013</marker>
<rawString>Sameer Pradhan, Noemie Elhadad, Brett South, David Martinez, Lee Christensen, Amy Vogel, Hanna Suominen, Wendy Chapman, and Guergana Savova. 2013. Task 1: ShARe/CLEF eHealth Evaluation Lab 2013. Online Working Notes of the CLEF 2013 Evaluation Labs and Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>No´emie Elhadad</author>
<author>Wendy Chapman</author>
<author>Suresh Manandhar</author>
<author>Guergana Savova</author>
</authors>
<title>SemEval-2014 Task 7: Analysis of clinical text.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>54--62</pages>
<location>Dublin, Ireland,</location>
<contexts>
<context position="3183" citStr="Pradhan et al., 2014" startWordPosition="470" endWordPosition="473">g, or a combination of those depending on the specificities and requirements of each concept type. After identifying entity mentions in text, it becomes necessary to perform entity normalization, which consists in assigning a specific concept identifier to each entity. This is usually performed by matching the identified entities against a knowledge-base, possibly evaluating the textual context in which the entity occurred to identify the best matching concept. Following up on the 2014 task, in which the objective was the identification and normalization of disease concepts in clinical texts (Pradhan et al., 2014), two subtasks were defined for the SemEval2015 Analysis of Clinical Text task. Task 1 consisted of recognizing concepts belonging to the ‘disorders’ semantic group of the Unified Medical Language System (UMLS) and normalizing to the 422 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 422–426, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics Documents Annotated Documents Reader Sentence Tagger NLP Processing pipeline Dictionaries Dictionary Tagger ML Tagger Models Abbreviation resolution Post-processing Disambiguator </context>
</contexts>
<marker>Pradhan, Elhadad, Chapman, Manandhar, Savova, 2014</marker>
<rawString>Sameer Pradhan, No´emie Elhadad, Wendy Chapman, Suresh Manandhar, and Guergana Savova. 2014. SemEval-2014 Task 7: Analysis of clinical text. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 54–62, Dublin, Ireland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammed Saeed</author>
<author>Mauricio Villarroel</author>
<author>Andrew Reisner</author>
<author>Gari Clifford</author>
<author>Li-Wei Lehman</author>
<author>George Moody</author>
<author>Thomas Heldt</author>
<author>Tin Kyaw</author>
<author>Benjamin Moody</author>
<author>Roger Mark</author>
</authors>
<title>Multiparameter Intelligent Monitoring in Intensive Care II (MIMIC-II): a public-access intensive care unit database.</title>
<date>2011</date>
<journal>Critical Care Medicine,</journal>
<volume>39</volume>
<issue>5</issue>
<contexts>
<context position="4287" citStr="Saeed et al., 2011" startWordPosition="623" endWordPosition="626">ing pipeline Dictionaries Dictionary Tagger ML Tagger Models Abbreviation resolution Post-processing Disambiguator Custom Module Relation extractor Indexer Writer Figure 1: Neji’s processing pipeline used for annotating the documents. Dashed boxes indicate optional modules. SNOMED CT1 terminology, and Task 2 consisted of identifying and normalizing specific attributes for each disorder mention, including negation, severity, and body location, for example. The task made use of the ShARe corpus (Pradhan et al., 2013), which contains manually annotated clinical notes from the MIMIC II database2 (Saeed et al., 2011). The task corpus comprised 531 documents, divided into a training portion with 298 documents, a development portion with 133 documents, and a test portion with 100 documents. In this paper, we present a combined machinelearning and rule-based approach for these tasks, supported by a modular text analysis and annotation pipeline. 2 Methods Our approach consists of three sequential steps, namely: entity recognition, rule-based span adjustment and normalization, and rule-based template filling. For entity recognition we used Gimli (Campos et al., 2013b), an open-source tool for training machine </context>
</contexts>
<marker>Saeed, Villarroel, Reisner, Clifford, Lehman, Moody, Heldt, Kyaw, Moody, Mark, 2011</marker>
<rawString>Mohammed Saeed, Mauricio Villarroel, Andrew Reisner, Gari Clifford, Li-Wei Lehman, George Moody, Thomas Heldt, Tin Kyaw, Benjamin Moody, and Roger Mark. 2011. Multiparameter Intelligent Monitoring in Intensive Care II (MIMIC-II): a public-access intensive care unit database. Critical Care Medicine, 39(5):952.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew S Simpson</author>
<author>Dina Demner-Fushman</author>
</authors>
<title>Biomedical text mining: A survey of recent progress.</title>
<date>2012</date>
<booktitle>In Charu C. Aggarwal and ChengXiang Zhai, editors, Mining Text Data,</booktitle>
<pages>465--517</pages>
<contexts>
<context position="1639" citStr="Simpson and Demner-Fushman, 2012" startWordPosition="230" endWordPosition="233">filling. We achieved an F-score of 71.2% in the entity recognition and normalization task, and a slot weighted accuracy of 69.5% in the template filling task. 1 Introduction Biomedical text mining offers the promise of leveraging the huge amounts of information available on scientific documents to help raise new hypotheses and uncover hidden knowledge. Biomedical text mining (TM) has been an important focus of research during the last years, sustained by the high volumes of data, the diverse computational and multi-disciplinary challenges posed, and by the potential impact of new discoveries (Simpson and Demner-Fushman, 2012). These benefits have been demonstrated in recent studies in which text mining methods were used to suggest biomarkers for diagnosis and for measuring disease progression, targets for new drugs, or new uses for existing drugs (Frijters et al., 2010). Likewise, clinical information stored as natural language text in discharge notes and reports could be exploited to identify important associations, and this has led to an increased interest in applying text mining techniques to such texts, in order to extract information related to diseases, medications, and adverse drug events, for example (Zhu </context>
</contexts>
<marker>Simpson, Demner-Fushman, 2012</marker>
<rawString>Matthew S. Simpson and Dina Demner-Fushman. 2012. Biomedical text mining: A survey of recent progress. In Charu C. Aggarwal and ChengXiang Zhai, editors, Mining Text Data, pages 465–517.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Zhu</author>
<author>Preecha Patumcharoenpol</author>
<author>Cheng Zhang</author>
<author>Yang Yang</author>
<author>Jonathan Chan</author>
<author>Asawin Meechai</author>
<author>Wanwipa Vongsangnak</author>
<author>Bairong Shen</author>
</authors>
<title>Biomedical text mining and its applications in cancer research.</title>
<date>2013</date>
<journal>Journal of Biomedical Informatics,</journal>
<volume>46</volume>
<issue>2</issue>
<contexts>
<context position="2252" citStr="Zhu et al., 2013" startWordPosition="327" endWordPosition="330">012). These benefits have been demonstrated in recent studies in which text mining methods were used to suggest biomarkers for diagnosis and for measuring disease progression, targets for new drugs, or new uses for existing drugs (Frijters et al., 2010). Likewise, clinical information stored as natural language text in discharge notes and reports could be exploited to identify important associations, and this has led to an increased interest in applying text mining techniques to such texts, in order to extract information related to diseases, medications, and adverse drug events, for example (Zhu et al., 2013). Research efforts in biomedical text mining have led to the development of various methods and tools for the recognition of diverse entities, including species names, genes and proteins, chemicals and drugs, anatomical concepts and diseases. These methods are based on dictionaries, rules, and machine learning, or a combination of those depending on the specificities and requirements of each concept type. After identifying entity mentions in text, it becomes necessary to perform entity normalization, which consists in assigning a specific concept identifier to each entity. This is usually perf</context>
</contexts>
<marker>Zhu, Patumcharoenpol, Zhang, Yang, Chan, Meechai, Vongsangnak, Shen, 2013</marker>
<rawString>Fei Zhu, Preecha Patumcharoenpol, Cheng Zhang, Yang Yang, Jonathan Chan, Asawin Meechai, Wanwipa Vongsangnak, and Bairong Shen. 2013. Biomedical text mining and its applications in cancer research. Journal of Biomedical Informatics, 46(2):200–211.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>