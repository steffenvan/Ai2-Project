<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000187">
<title confidence="0.98854">
Predicting Grammaticality on an Ordinal Scale
</title>
<author confidence="0.912685">
Michael Heilman Aoife Cahill Nitin Madnani Melissa Lopez Matthew Mulholland
</author>
<affiliation confidence="0.804288">
Educational Testing Service
</affiliation>
<address confidence="0.859093">
Princeton, NJ, USA
</address>
<email confidence="0.968225">
{mheilman,acahill,nmadnani,mlopez002,mmulholland}@ets.org
</email>
<author confidence="0.91001">
Joel Tetreault
</author>
<affiliation confidence="0.73556">
Yahoo! Research
</affiliation>
<address confidence="0.729149">
New York, NY, USA
</address>
<email confidence="0.893707">
tetreaul@yahoo-inc.com
</email>
<sectionHeader confidence="0.991047" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997855625">
Automated methods for identifying
whether sentences are grammatical
have various potential applications (e.g.,
machine translation, automated essay
scoring, computer-assisted language
learning). In this work, we construct a
statistical model of grammaticality using
various linguistic features (e.g., mis-
spelling counts, parser outputs, n-gram
language model scores). We also present
a new publicly available dataset of learner
sentences judged for grammaticality on
an ordinal scale. In evaluations, we
compare our system to the one from Post
(2011) and find that our approach yields
state-of-the-art performance.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.989614095238095">
In this paper, we develop a system for the task
of predicting the grammaticality of sentences, and
present a dataset of learner sentences rated for
grammaticality. Such a system could be used, for
example, to check or to rank outputs from systems
for text summarization, natural language genera-
tion, or machine translation. It could also be used
in educational applications such as essay scoring.
Much of the previous research on predicting
grammaticality has focused on identifying (and
possibly correcting) specific types of grammati-
cal errors that are typically made by English lan-
guage learners, such as prepositions (Tetreault and
Chodorow, 2008), articles (Han et al., 2006), and
collocations (Dahlmeier and Ng, 2011). While
some applications (e.g., grammar checking) rely
on such fine-grained predictions, others might be
better addressed by sentence-level grammaticality
judgments (e.g., machine translation evaluation).
Regarding sentence-level grammaticality, there
has been much work on rating the grammatical-
ity of machine translation outputs (Gamon et al.,
2005; Parton et al., 2011), such as the MT Quality
Estimation Shared Tasks (Bojar et al., 2013, §6),
but relatively little on evaluating the grammatical-
ity of naturally occurring text. Also, most other re-
search on evaluating grammaticality involves arti-
ficial tasks or datasets (Sun et al., 2007; Lee et al.,
2007; Wong and Dras, 2010; Post, 2011).
Here, we make the following contributions.
• We develop a state-of-the-art approach for
predicting the grammaticality of sentences on
an ordinal scale, adapting various techniques
from the previous work described above.
• We create a dataset of grammatical and un-
grammatical sentences written by English
language learners, labeled on an ordinal scale
for grammaticality. With this unique data set,
which we will release to the research com-
munity, it is now possible to conduct realis-
tic evaluations for predicting sentence-level
grammaticality.
</bodyText>
<sectionHeader confidence="0.994296" genericHeader="method">
2 Dataset Description
</sectionHeader>
<bodyText confidence="0.989156066666667">
We created a dataset consisting of 3,129 sentences
randomly selected from essays written by non-
native speakers of English as part of a test of
English language proficiency. We oversampled
lower-scoring essays to increase the chances of
finding ungrammatical sentences. Two of the au-
thors of this paper, both native speakers of English
with linguistic training, annotated the data. We
refer to these annotators as expert judges. When
making judgments of the sentences, they saw the
previous sentence from the same essay as context.
These two authors were not directly involved in
development of the system in §3.
Each sentence was annotated on a scale from
1 to 4 as described below, with 4 being the most
</bodyText>
<page confidence="0.975059">
174
</page>
<bodyText confidence="0.9742378125">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 174–180,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
grammatical. We use an ordinal rather than bi-
nary scale, following previous work such as that of
Clark et al. (2013) and Crocker and Keller (2005)
who argue that the distinction between grammati-
cal and ungrammatical is not simply binary. Also,
for practical applications, we believe that it is use-
ful to distinguish sentences with minor errors from
those with major errors that may disrupt communi-
cation. Our annotation scheme was influenced by
a translation rating scheme by Coughlin (2003).
Every sentence judged on the 1–4 scale must be
a clause. There is an extra category (“Other”) for
sentences that do not fit this criterion. We exclude
instances of “Other” in our experiments (see §4).
</bodyText>
<listItem confidence="0.999488714285714">
4. Perfect The sentence is native-sounding. It has
no grammatical errors, but may contain very mi-
nor typographical and/or collocation errors, as in
Example (1).
(1) For instance, i stayed in a dorm when i
went to collge.
3. Comprehensible The sentence may contain
</listItem>
<bodyText confidence="0.849715181818182">
one or more minor grammatical errors, includ-
ing subject-verb agreement, determiner, and mi-
nor preposition errors that do not make the mean-
ing unclear, as in Example (2).
(2) We know during Spring Festival, Chinese
family will have a abundand family banquet
with family memebers.
“Chinese family”, which could be corrected to
“Chinese families”, “each Chinese family”, etc.,
would be an example of a minor grammatical er-
ror involving determiners.
</bodyText>
<sectionHeader confidence="0.775213" genericHeader="method">
2. Somewhat Comprehensible The sentence
</sectionHeader>
<bodyText confidence="0.915965">
may contain one or more serious grammatical
errors, including missing subject, verb, object,
etc., verb tense errors, and serious preposition
errors. Due to these errors, the sentence may
have multiple plausible interpretations, as in
Example (3).
</bodyText>
<listItem confidence="0.994729142857143">
(3) I can gain the transportations such as buses
and trains.
1. Incomprehensible The sentence contains so
many errors that it would be difficult to correct,
as in Example (4).
(4) Or you want to say he is only a little boy do
not everything clearly?
</listItem>
<bodyText confidence="0.994819260869565">
The phrase “do not everything” makes the sen-
tence practically incomprehensible since the sub-
ject of “do” is not clear.
O.Other/Incomplete This sentence is incom-
plete. These sentences, such as Example (5), ap-
pear in our corpus due to the nature of timed tests.
(5) The police officer handed the
This sentence is cut off and does not at least in-
clude one clause.
We measured interannotator agreement on a
subset of 442 sentences that were independently
annotated by both expert annotators. Exact agree-
ment was 71.3%, unweighted κ = 0.574, and
Pearson’s r = 0.759.1 For our experiments, one
expert annotator was arbitrarily selected, and for
the doubly-annotated sentences, only the judg-
ments from that annotator were retained.
The labels from the expert annotators are dis-
tributed as follows: 72 sentences are labeled 1;
538 are 2; 1,431 are 3; 978 are 4; and 110 are “O”.
We also gathered 5 additional judgments using
Crowdflower.2 For this, we excluded the “Other”
category and any sentences that had been marked
as such by the expert annotators. We used 100
(3.2%) of the judged sentences as “gold” data in
Crowdflower to block contributors who were not
following the annotation guidelines. For those
sentences, only disagreements within 1 point of
the expert annotator judgment were accepted. In
preliminary experiments, averaging the six judg-
ments (1 expert, 5 crowdsourced) for each item
led to higher human-machine agreement. For all
experiments reported later, we used this average
of six judgments as our gold standard.
For our experiments (§4), we randomly split the
data into training (50%), development (25%), and
testing (25%) sets. We also excluded all instances
labeled “Other”. These are relatively uncommon
and less interesting to this study. Also, we believe
that simpler, heuristic approaches could be used to
identify such sentences.
We use “GUG” (“Grammatical” versus “Un-
Grammatical”) to refer to this dataset. The dataset
is available for research at https://github.
com/EducationalTestingService/
gug-data.
</bodyText>
<footnote confidence="0.98978725">
1The reported agreement values assume that “Other”
maps to 0. For the sentences where both labels were in the
1–4 range (n = 424), Pearson’s r = 0.767.
2http://www.crowdflower.com
</footnote>
<page confidence="0.994834">
175
</page>
<note confidence="0.401386">
3 System Description • max log(count(s) + 1)
</note>
<bodyText confidence="0.976857333333333">
This section describes the statistical model (§3.1) sESn log(count(s) + 1)
and features (§3.2) used by our system. • min
sESn
</bodyText>
<subsectionHeader confidence="0.995421">
3.1 Statistical Model
</subsectionHeader>
<bodyText confidence="0.9999085">
We use `2-regularized linear regression (i.e., ridge
regression) to learn a model of sentence grammat-
icality from a variety of linguistic features.34
To tune the `2-regularization hyperparameter α,
the system performs 5-fold cross-validation on the
data used for training. The system evaluates α E
101−4,...,41 and selects the one that achieves the
highest cross-validation correlation r.
</bodyText>
<subsectionHeader confidence="0.978772">
3.2 Features
</subsectionHeader>
<bodyText confidence="0.999801">
Next, we describe the four types of features.
</bodyText>
<subsectionHeader confidence="0.921452">
3.2.1 Spelling Features
</subsectionHeader>
<bodyText confidence="0.999950777777778">
Given a sentence with with n word tokens, the
model filters out tokens containing nonalpha-
betic characters and then computes the num-
ber of misspelled words nmiss (later referred to
as num misspelled), the proportion of mis-
spelled words nmiss
n , and log(nmiss + 1) as fea-
tures. To identify misspellings, we use a freely
available spelling dictionary for U.S. English.5
</bodyText>
<subsectionHeader confidence="0.988091">
3.2.2 n-gram Count and Language Model
Features
</subsectionHeader>
<bodyText confidence="0.999993666666667">
Given each sentence, the model obtains the counts
of n-grams (n = 1... 3) from English Gigaword
and computes the following features:6
</bodyText>
<equation confidence="0.9861575">
log(count(s) + 1)
IISn11
</equation>
<bodyText confidence="0.969595846153846">
3We use ridge regression from the scikit-learn
toolkit (Pedregosa et al., 2011) v0.23.1 and the
SciKit-Learn Laboratory (http://github.com/
EducationalTestingService/skll).
4Regression models typically produce conservative pre-
dictions with lower variance than the original training data.
So that predictions better match the distribution of labels in
the training data, the system rescales its predictions. It saves
the mean and standard deviation of the training data gold
standard (Mgold and SDgold, respectively) and of its own
predictions on the training data (Mpred and SDpred, respec-
tively). During cross-validation, this is done for each fold.
From an initial prediction ˆy, it produces the final prediction:
</bodyText>
<equation confidence="0.783949333333333">
ˆy−Mpred ˆy� =∗ SD M This transformation does
r
SDped gold + gold•
</equation>
<bodyText confidence="0.3818795">
not affect Pearson’s r correlations or rankings, but it would
affect binarized predictions.
</bodyText>
<footnote confidence="0.9812354">
5http://pythonhosted.org/pyenchant/
6We use the New York Times (nyt), the Los Ange-
les Times-Washington Post (ltw), and the Washington Post-
Bloomberg News (wpb) sections from the fifth edition of En-
glish Gigaword (LDC2011T07).
</footnote>
<bodyText confidence="0.9995188">
where Sn represents the n-grams of order n from
the given sentence. The model computes the fol-
lowing features from a 5-gram language model
trained on the same three sections of English Gi-
gaword using the SRILM toolkit (Stolcke, 2002):
</bodyText>
<listItem confidence="0.9962954">
• the average log-probability of the
given sentence (referred to as
gigaword avglogprob later)
• the number of out-of-vocabulary words in the
sentence
</listItem>
<bodyText confidence="0.9757562">
Finally, the system computes the average
log-probability and number of out-of-vocabulary
words from a language model trained on a col-
lection of essays written by non-native English
speakers7 (“non-native LM”).
</bodyText>
<subsectionHeader confidence="0.789443">
3.2.3 Precision Grammar Features
</subsectionHeader>
<bodyText confidence="0.999924894736842">
Following Wagner et al. (2007) and Wagner et
al. (2009), we use features extracted from preci-
sion grammar parsers. These grammars have been
hand-crafted and designed to only provide com-
plete syntactic analyses for grammatically cor-
rect sentences. This is in contrast to treebank-
trained grammars, which will generally provide
some analysis regardless of grammaticality. Here,
we use (1) the Link Grammar Parser8 and (2)
the HPSG English Resource Grammar (Copestake
and Flickinger, 2000) and PET parser.9
We use a binary feature, complete link,
from the Link grammar that indicates whether at
least one complete linkage can be found for a sen-
tence. We also extract several features from the
HPSG analyses.10 They mostly reflect information
about unification success or failure and the associ-
ated costs. In each instance, we use the logarithm
of one plus the frequency.
</bodyText>
<footnote confidence="0.98597525">
7This did not overlap with the data described in §2 and
was a subset of the data released by Blanchard et al. (2013).
8http://www.link.cs.cmu.edu/link/
9http://moin.delph-in.net/PetTop
</footnote>
<bodyText confidence="0.676734461538462">
10The complete list of relevant statistics used as features
is: trees, unify cost succ, unify cost fail,
unifications succ, unifications fail,
subsumptions succ, subsumptions fail,
words, words pruned, aedges, pedges,
upedges, raedges, rpedges, medges. During
development, we observed that some of these features vary
for some inputs, probably due to parsing search timeouts. On
10 preliminary runs with the development set, this variance
had minimal effects on correlations with human judgments
(less than 0.00001 in terms of r).
�•
sESn
</bodyText>
<page confidence="0.779696">
176
</page>
<table confidence="0.999834333333333">
r
our system 0.668
− non-native LM (§3.2.2) 0.665
− HPSG parse (§3.2.3) 0.664
− PCFG parse (§3.2.4) 0.662
− spelling (§3.2.1) 0.643
− gigaword LM (§3.2.2) 0.638
− link parse (§3.2.3) 0.632
− gigaword count (§3.2.2) 0.630
</table>
<tableCaption confidence="0.996904">
Table 1: Pearson’s r on the development set, for
</tableCaption>
<bodyText confidence="0.727280666666667">
our full system and variations excluding each fea-
ture type. “− X” indicates the full model without
the “X” features.
</bodyText>
<sectionHeader confidence="0.33783" genericHeader="method">
3.2.4 PCFG Parsing Features
</sectionHeader>
<bodyText confidence="0.99924625">
We find phrase structure trees and basic depen-
dencies with the Stanford Parser’s English PCFG
model (Klein and Manning, 2003; de Marneffe et
al., 2006).11 We then compute the following:
</bodyText>
<listItem confidence="0.9972235">
• the parse score as provided by the Stan-
ford PCFG Parser, normalized for sentence
length, later referred to as parse prob
• a binary feature that captures whether the top
node of the tree is sentential or not (i.e. the
assumption is that if the top node is non-
sentential, then the sentence is a fragment)
• features binning the number of dep rela-
</listItem>
<bodyText confidence="0.9271078">
tions returned by the dependency conversion.
These dep relations are underspecified for
function and indicate that the parser was un-
able to find a standard relation such as subj,
possibly indicating a grammatical error.
</bodyText>
<sectionHeader confidence="0.999575" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.997258">
Next, we present evaluations on the GUG dataset.
</bodyText>
<subsectionHeader confidence="0.98347">
4.1 Feature Ablation
</subsectionHeader>
<bodyText confidence="0.999960357142857">
We conducted a feature ablation study to iden-
tify the contributions of the different types of fea-
tures described in §3.2. We compared the perfor-
mance of the full model with all of the features
to models with all but one type of feature. For
this experiment, all models were estimated from
the training set and evaluated on the development
set. We report performance in terms of Pearson’s
r between the averaged 1–4 human labels and un-
rounded system predictions.
The results are shown in Table 1. From these
results, the most useful features appear to be the
n-gram frequencies from Gigaword and whether
the link parser can fully parse the sentence.
</bodyText>
<subsectionHeader confidence="0.999457">
4.2 Test Set Results
</subsectionHeader>
<bodyText confidence="0.999990717948718">
In this section, we present results on the held-out
test set for the full model and various baselines,
summarized in Table 2. For test set evaluations,
we trained on the combination of the training and
development sets (§2), to maximize the amount of
training data for the final experiments.
We also trained and evaluated on binarized ver-
sions of the ordinal GUG labels: a sentence was
labeled 1 if the average judgment was at least 3.5
(i.e., would round to 4), and 0 otherwise. Evaluat-
ing on a binary scale allows us to measure how
well the system distinguishes grammatical sen-
tences from ungrammatical ones. For some ap-
plications, this two-way distinction may be more
relevant than the more fine-grained 1–4 scale. To
train our system on binarized data, we replaced the
E2-regularized linear regression model with an E2-
regularized logistic regression and used Kendall’s
T rank correlation between the predicted probabil-
ities of the positive class and the binary gold stan-
dard labels as the grid search metric (§3.1) instead
of Pearson’s r.
For the ordinal task, we report Pearson’s r be-
tween the averaged human judgments and each
system. For the binary task, we report percentage
accuracy. Since the predictions from the binary
and ordinal systems are on different scales, we in-
clude the nonparametric statistic Kendall’s T as a
secondary evaluation metric for both tasks.
We also evaluated the binary system for the or-
dinal task by computing correlations between its
estimated probabilities and the averaged human
scores, and we evaluated the ordinal system for the
binary task by binarizing its predictions.12
We compare our work to a modified version of
the publicly available13 system from Post (2011),
which performed very well on an artificial dataset.
To our knowledge, it is the only publicly available
system for grammaticality prediction. It is very
</bodyText>
<footnote confidence="0.985159363636364">
11We use the Nov. 12, 2013 version of the Stanford Parser.
12We selected a threshold for binarization from a grid of
1001 points from 1 to 4 that maximized the accuracy of bina-
rized predictions from a model trained on the training set and
evaluated on the binarized development set. For evaluating
the three single-feature baselines discussed below, we used
the same approach except with grid ranging from the min-
imum development set feature value to the maximum plus
0.1% of the range.
13The Post (2011) system is available at https://
github.com/mjpost/post2011judging.
</footnote>
<page confidence="0.982116">
177
</page>
<table confidence="0.999796333333333">
Ordinal Task % Acc. Binary Task T
r Sig.r T Sig.%Acc.
our system 0.644 0.479 79.3 0.419
our systemlogistic 0.616 * 0.484 80.7 0.428
Post 0.321 * 0.225 75.5 * 0.195
Postlogistic 0.259 * 0.181 74.4 * 0.181
complete link 0.386 * 0.335 74.8 * 0.302
gigaword avglogprob 0.414 * 0.290 76.7 * 0.280
num misspelled -0.462 * -0.370 74.8 * -0.335
</table>
<tableCaption confidence="0.996498">
Table 2: Human-machine agreement statistics for our system, the system from Post (2011), and simple
</tableCaption>
<bodyText confidence="0.993710611111111">
baselines, computed from the averages of human ratings in the testing set (§2). “*” in a Sig. column
indicates a statistically significant difference from “our system” (p &lt; .05, see text for details). A majority
baseline for the binary task achieves 74.8% accuracy. The best results for each metric are in bold.
different from our system since it relies on par-
tial tree-substitution grammar derivations as fea-
tures. We use the feature computation components
of that system but replace its statistical model. The
system was designed for use with a dataset consist-
ing of 50% grammatical and 50% ungrammatical
sentences, rather than data with ordinal or continu-
ous labels. Additionally, its classifier implementa-
tion does not output scores or probabilities. There-
fore, we used the same learning algorithms as for
our system (i.e., ridge regression for the ordinal
task and logistic regression for the binary task).14
To create further baselines for comparison,
we selected the following features that represent
ways one might approximate grammaticality if a
comprehensive model was unavailable: whether
the link parser can fully parse the sentence
(complete link), the Gigaword language
model score (gigaword avglogprob),
and the number of misspelled tokens
(num misspelled). Note that we expect
the number of misspelled tokens to be negatively
correlated with grammaticality. We flipped the
sign of the misspelling feature when computing
accuracy for the binary task.
To identify whether the differences in perfor-
mance for the ordinal task between our system and
each of the baselines are statistically significant,
we used the BQa Bootstrap (Efron and Tibshirani,
1993) with 10,000 replications to compute 95%
confidence intervals for the absolute value of r for
our system minus the absolute value of r for each
of the alternative methods. For the binary task, we
</bodyText>
<footnote confidence="0.849699">
14In preliminary experiments, we observed little difference
in performance between logistic regression and the original
support vector classifier used by the system from Post (2011).
</footnote>
<bodyText confidence="0.9550435">
used the sign test to test for significant differences
in accuracy. The results are in Table 2.
</bodyText>
<sectionHeader confidence="0.988793" genericHeader="conclusions">
5 Discussion and Conclusions
</sectionHeader>
<bodyText confidence="0.9999804">
In this paper, we developed a system for predict-
ing grammaticality on an ordinal scale and cre-
ated a labeled dataset that we have released pub-
licly (§2) to enable more realistic evaluations in
future research. Our system outperformed an ex-
isting state-of-the-art system (Post, 2011) in eval-
uations on binary and ordinal scales. This is the
most realistic evaluation of methods for predicting
sentence-level grammaticality to date.
Surprisingly, the system from Post (2011) per-
formed quite poorly on the GUG dataset. We spec-
ulate that this is due to the fact that the Post sys-
tem relies heavily on features extracted from au-
tomatic syntactic parses. While Post found that
such a system can effectively distinguish gram-
matical news text sentences from sentences gen-
erated by a language model, measuring the gram-
maticality of real sentences from language learn-
ers seems to require a wider variety of features,
including n-gram counts, language model scores,
etc. Of course, our findings do not indicate that
syntactic features such as those from Post (2011)
are without value. In future work, it may be pos-
sible to improve grammaticality measurement by
integrating such features into a larger system.
</bodyText>
<sectionHeader confidence="0.996516" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9991654">
We thank Beata Beigman Klebanov, Yoko Futagi,
Su-Youn Yoon, and the anonymous reviewers for
their helpful comments. We also thank Jennifer
Foster for discussions about this work and Matt
Post for making his system publicly available.
</bodyText>
<page confidence="0.997533">
178
</page>
<sectionHeader confidence="0.984092" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999320271028038">
Daniel Blanchard, Joel Tetreault, Derrick Higgins,
Aoife Cahill, and Martin Chodorow. 2013.
TOEFL11: A Corpus of Non-Native English. Tech-
nical report, Educational Testing Service.
Ondˇrej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1–44, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Alexander Clark, Gianluca Giorgolo, and Shalom Lap-
pin. 2013. Towards a statistical model of grammat-
icality. In Proceedings of the 35th Annual Confer-
ence of the Cognitive Science Society, pages 2064–
2069.
Ann Copestake and Dan Flickinger. 2000. An
open-source grammar development environment
and broad-coverage English grammar using HPSG.
In Proceedings of the 2nd International Confer-
ence on Language Resources and Evaluation (LREC
2000), Athens, Greece.
Deborah Coughlin. 2003. Correlating automated and
human assessments of machine translation quality.
In Proceedings of MT Summit IX, pages 63–70.
Matthew W. Crocker and Frank Keller. 2005. Prob-
abilistic grammars as models of gradience in lan-
guage processing. In Gradience in Grammar: Gen-
erative Perspectives. University Press.
Daniel Dahlmeier and Hwee Tou Ng. 2011. Correcting
Semantic Collocation Errors with L1-induced Para-
phrases. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 107–117, Edinburgh, Scotland, UK., July.
Association for Computational Linguistics.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
LREC 2006, pages 449–454.
B. Efron and R. Tibshirani. 1993. An Introduction to
the Bootstrap. Chapman and Hall/CRC, Boca Ra-
ton, FL.
Michael Gamon, Anthony Aue, and Martine Smets.
2005. Sentence-level MT evaluation without refer-
ence translations: Beyond language modeling. In
Proceedings of EAMT, pages 103–111. Springer-
Verlag.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineer-
ing, 12(2):115–129.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics, pages 423–430, Sapporo,
Japan, July. Association for Computational Linguis-
tics.
John Lee, Ming Zhou, and Xiaohua Liu. 2007. De-
tection of Non-Native Sentences Using Machine-
Translated Training Data. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Compu-
tational Linguistics; Companion Volume, Short Pa-
pers, pages 93–96, Rochester, New York, April. As-
sociation for Computational Linguistics.
Kristen Parton, Joel Tetreault, Nitin Madnani, and Mar-
tin Chodorow. 2011. E-rating machine translation.
In Proceedings of the Sixth Workshop on Statistical
Machine Translation, pages 108–115, Edinburgh,
Scotland, July. Association for Computational Lin-
guistics.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine Learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825–2830.
Matt Post. 2011. Judging Grammaticality with Tree
Substitution Grammar Derivations. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 217–222, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In 7th International Con-
ference on Spoken Language Processing.
Guihua Sun, Xiaohua Liu, Gao Cong, Ming Zhou,
Zhongyang Xiong, John Lee, and Chin-Yew Lin.
2007. Detecting Erroneous Sentences using Auto-
matically Mined Sequential Patterns. In Proceed-
ings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 81–88, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
Joel R. Tetreault and Martin Chodorow. 2008. The
Ups and Downs of Preposition Error Detection in
ESL Writing. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics
(Coling 2008), pages 865–872, Manchester, UK,
August. Coling 2008 Organizing Committee.
Joachim Wagner, Jennifer Foster, and Josef van Gen-
abith. 2007. A Comparative Evaluation of Deep
and Shallow Approaches to the Automatic Detec-
tion of Common Grammatical Errors. In Proceed-
ings of the 2007 Joint Conference on Empirical
</reference>
<page confidence="0.986876">
179
</page>
<reference confidence="0.998791846153846">
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 112–121, Prague, Czech Republic,
June. Association for Computational Linguistics.
Joachim Wagner, Jennifer Foster, and Josef van Gen-
abith. 2009. Judging grammaticality: Experi-
ments in sentence classification. CALICO Journal,
26(3):474–490.
Sze-Meng Jojo Wong and Mark Dras. 2010. Parser
Features for Sentence Grammaticality Classifica-
tion. In Proceedings of the Australasian Language
Technology Association Workshop 2010, pages 67–
75, Melbourne, Australia, December.
</reference>
<page confidence="0.997763">
180
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.065151">
<title confidence="0.999747">Predicting Grammaticality on an Ordinal Scale</title>
<author confidence="0.998564">Michael Heilman Aoife Cahill Nitin Madnani Melissa Lopez Matthew</author>
<affiliation confidence="0.504329333333333">Educational Testing Princeton, NJ, Joel</affiliation>
<address confidence="0.4214915">Yahoo! New York, NY,</address>
<email confidence="0.999479">tetreaul@yahoo-inc.com</email>
<abstract confidence="0.998900882352941">Automated methods for identifying whether sentences are grammatical have various potential applications (e.g., machine translation, automated essay scoring, computer-assisted language learning). In this work, we construct a statistical model of grammaticality using various linguistic features (e.g., miscounts, parser outputs, language model scores). We also present a new publicly available dataset of learner sentences judged for grammaticality on an ordinal scale. In evaluations, we compare our system to the one from Post (2011) and find that our approach yields state-of-the-art performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Daniel Blanchard</author>
<author>Joel Tetreault</author>
</authors>
<location>Derrick Higgins,</location>
<marker>Blanchard, Tetreault, </marker>
<rawString>Daniel Blanchard, Joel Tetreault, Derrick Higgins,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
<author>Martin Chodorow</author>
</authors>
<title>TOEFL11: A Corpus of Non-Native English.</title>
<date>2013</date>
<tech>Technical report, Educational Testing Service.</tech>
<marker>Cahill, Chodorow, 2013</marker>
<rawString>Aoife Cahill, and Martin Chodorow. 2013. TOEFL11: A Corpus of Non-Native English. Technical report, Educational Testing Service.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondˇrej Bojar</author>
<author>Christian Buck</author>
<author>Chris Callison-Burch</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2013</date>
<booktitle>Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>1--44</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="2102" citStr="Bojar et al., 2013" startWordPosition="287" endWordPosition="290">that are typically made by English language learners, such as prepositions (Tetreault and Chodorow, 2008), articles (Han et al., 2006), and collocations (Dahlmeier and Ng, 2011). While some applications (e.g., grammar checking) rely on such fine-grained predictions, others might be better addressed by sentence-level grammaticality judgments (e.g., machine translation evaluation). Regarding sentence-level grammaticality, there has been much work on rating the grammaticality of machine translation outputs (Gamon et al., 2005; Parton et al., 2011), such as the MT Quality Estimation Shared Tasks (Bojar et al., 2013, §6), but relatively little on evaluating the grammaticality of naturally occurring text. Also, most other research on evaluating grammaticality involves artificial tasks or datasets (Sun et al., 2007; Lee et al., 2007; Wong and Dras, 2010; Post, 2011). Here, we make the following contributions. • We develop a state-of-the-art approach for predicting the grammaticality of sentences on an ordinal scale, adapting various techniques from the previous work described above. • We create a dataset of grammatical and ungrammatical sentences written by English language learners, labeled on an ordinal </context>
</contexts>
<marker>Bojar, Buck, Callison-Burch, Federmann, Haddow, Koehn, Monz, Post, Soricut, Specia, 2013</marker>
<rawString>Ondˇrej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 1–44, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
<author>Gianluca Giorgolo</author>
<author>Shalom Lappin</author>
</authors>
<title>Towards a statistical model of grammaticality.</title>
<date>2013</date>
<booktitle>In Proceedings of the 35th Annual Conference of the Cognitive Science Society,</booktitle>
<pages>pages</pages>
<contexts>
<context position="3960" citStr="Clark et al. (2013)" startWordPosition="578" endWordPosition="581">rt judges. When making judgments of the sentences, they saw the previous sentence from the same essay as context. These two authors were not directly involved in development of the system in §3. Each sentence was annotated on a scale from 1 to 4 as described below, with 4 being the most 174 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 174–180, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics grammatical. We use an ordinal rather than binary scale, following previous work such as that of Clark et al. (2013) and Crocker and Keller (2005) who argue that the distinction between grammatical and ungrammatical is not simply binary. Also, for practical applications, we believe that it is useful to distinguish sentences with minor errors from those with major errors that may disrupt communication. Our annotation scheme was influenced by a translation rating scheme by Coughlin (2003). Every sentence judged on the 1–4 scale must be a clause. There is an extra category (“Other”) for sentences that do not fit this criterion. We exclude instances of “Other” in our experiments (see §4). 4. Perfect The sentenc</context>
</contexts>
<marker>Clark, Giorgolo, Lappin, 2013</marker>
<rawString>Alexander Clark, Gianluca Giorgolo, and Shalom Lappin. 2013. Towards a statistical model of grammaticality. In Proceedings of the 35th Annual Conference of the Cognitive Science Society, pages 2064– 2069.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
</authors>
<title>An open-source grammar development environment and broad-coverage English grammar using HPSG.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2nd International Conference on Language Resources and Evaluation (LREC</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="11452" citStr="Copestake and Flickinger, 2000" startWordPosition="1750" endWordPosition="1753">om a language model trained on a collection of essays written by non-native English speakers7 (“non-native LM”). 3.2.3 Precision Grammar Features Following Wagner et al. (2007) and Wagner et al. (2009), we use features extracted from precision grammar parsers. These grammars have been hand-crafted and designed to only provide complete syntactic analyses for grammatically correct sentences. This is in contrast to treebanktrained grammars, which will generally provide some analysis regardless of grammaticality. Here, we use (1) the Link Grammar Parser8 and (2) the HPSG English Resource Grammar (Copestake and Flickinger, 2000) and PET parser.9 We use a binary feature, complete link, from the Link grammar that indicates whether at least one complete linkage can be found for a sentence. We also extract several features from the HPSG analyses.10 They mostly reflect information about unification success or failure and the associated costs. In each instance, we use the logarithm of one plus the frequency. 7This did not overlap with the data described in §2 and was a subset of the data released by Blanchard et al. (2013). 8http://www.link.cs.cmu.edu/link/ 9http://moin.delph-in.net/PetTop 10The complete list of relevant s</context>
</contexts>
<marker>Copestake, Flickinger, 2000</marker>
<rawString>Ann Copestake and Dan Flickinger. 2000. An open-source grammar development environment and broad-coverage English grammar using HPSG. In Proceedings of the 2nd International Conference on Language Resources and Evaluation (LREC 2000), Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deborah Coughlin</author>
</authors>
<title>Correlating automated and human assessments of machine translation quality.</title>
<date>2003</date>
<booktitle>In Proceedings of MT Summit IX,</booktitle>
<pages>63--70</pages>
<contexts>
<context position="4335" citStr="Coughlin (2003)" startWordPosition="639" endWordPosition="640"> (Short Papers), pages 174–180, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics grammatical. We use an ordinal rather than binary scale, following previous work such as that of Clark et al. (2013) and Crocker and Keller (2005) who argue that the distinction between grammatical and ungrammatical is not simply binary. Also, for practical applications, we believe that it is useful to distinguish sentences with minor errors from those with major errors that may disrupt communication. Our annotation scheme was influenced by a translation rating scheme by Coughlin (2003). Every sentence judged on the 1–4 scale must be a clause. There is an extra category (“Other”) for sentences that do not fit this criterion. We exclude instances of “Other” in our experiments (see §4). 4. Perfect The sentence is native-sounding. It has no grammatical errors, but may contain very minor typographical and/or collocation errors, as in Example (1). (1) For instance, i stayed in a dorm when i went to collge. 3. Comprehensible The sentence may contain one or more minor grammatical errors, including subject-verb agreement, determiner, and minor preposition errors that do not make the</context>
</contexts>
<marker>Coughlin, 2003</marker>
<rawString>Deborah Coughlin. 2003. Correlating automated and human assessments of machine translation quality. In Proceedings of MT Summit IX, pages 63–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew W Crocker</author>
<author>Frank Keller</author>
</authors>
<title>Probabilistic grammars as models of gradience in language processing. In Gradience in Grammar: Generative Perspectives.</title>
<date>2005</date>
<publisher>University Press.</publisher>
<contexts>
<context position="3990" citStr="Crocker and Keller (2005)" startWordPosition="583" endWordPosition="586">udgments of the sentences, they saw the previous sentence from the same essay as context. These two authors were not directly involved in development of the system in §3. Each sentence was annotated on a scale from 1 to 4 as described below, with 4 being the most 174 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 174–180, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics grammatical. We use an ordinal rather than binary scale, following previous work such as that of Clark et al. (2013) and Crocker and Keller (2005) who argue that the distinction between grammatical and ungrammatical is not simply binary. Also, for practical applications, we believe that it is useful to distinguish sentences with minor errors from those with major errors that may disrupt communication. Our annotation scheme was influenced by a translation rating scheme by Coughlin (2003). Every sentence judged on the 1–4 scale must be a clause. There is an extra category (“Other”) for sentences that do not fit this criterion. We exclude instances of “Other” in our experiments (see §4). 4. Perfect The sentence is native-sounding. It has n</context>
</contexts>
<marker>Crocker, Keller, 2005</marker>
<rawString>Matthew W. Crocker and Frank Keller. 2005. Probabilistic grammars as models of gradience in language processing. In Gradience in Grammar: Generative Perspectives. University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Correcting Semantic Collocation Errors with L1-induced Paraphrases.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>107--117</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="1661" citStr="Dahlmeier and Ng, 2011" startWordPosition="226" endWordPosition="229">resent a dataset of learner sentences rated for grammaticality. Such a system could be used, for example, to check or to rank outputs from systems for text summarization, natural language generation, or machine translation. It could also be used in educational applications such as essay scoring. Much of the previous research on predicting grammaticality has focused on identifying (and possibly correcting) specific types of grammatical errors that are typically made by English language learners, such as prepositions (Tetreault and Chodorow, 2008), articles (Han et al., 2006), and collocations (Dahlmeier and Ng, 2011). While some applications (e.g., grammar checking) rely on such fine-grained predictions, others might be better addressed by sentence-level grammaticality judgments (e.g., machine translation evaluation). Regarding sentence-level grammaticality, there has been much work on rating the grammaticality of machine translation outputs (Gamon et al., 2005; Parton et al., 2011), such as the MT Quality Estimation Shared Tasks (Bojar et al., 2013, §6), but relatively little on evaluating the grammaticality of naturally occurring text. Also, most other research on evaluating grammaticality involves arti</context>
</contexts>
<marker>Dahlmeier, Ng, 2011</marker>
<rawString>Daniel Dahlmeier and Hwee Tou Ng. 2011. Correcting Semantic Collocation Errors with L1-induced Paraphrases. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 107–117, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating Typed Dependency Parses from Phrase Structure Parses. In LREC</title>
<date>2006</date>
<pages>449--454</pages>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating Typed Dependency Parses from Phrase Structure Parses. In LREC 2006, pages 449–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Efron</author>
<author>R Tibshirani</author>
</authors>
<title>An Introduction to the Bootstrap. Chapman and Hall/CRC,</title>
<date>1993</date>
<location>Boca Raton, FL.</location>
<contexts>
<context position="19030" citStr="Efron and Tibshirani, 1993" startWordPosition="2982" endWordPosition="2985">ticality if a comprehensive model was unavailable: whether the link parser can fully parse the sentence (complete link), the Gigaword language model score (gigaword avglogprob), and the number of misspelled tokens (num misspelled). Note that we expect the number of misspelled tokens to be negatively correlated with grammaticality. We flipped the sign of the misspelling feature when computing accuracy for the binary task. To identify whether the differences in performance for the ordinal task between our system and each of the baselines are statistically significant, we used the BQa Bootstrap (Efron and Tibshirani, 1993) with 10,000 replications to compute 95% confidence intervals for the absolute value of r for our system minus the absolute value of r for each of the alternative methods. For the binary task, we 14In preliminary experiments, we observed little difference in performance between logistic regression and the original support vector classifier used by the system from Post (2011). used the sign test to test for significant differences in accuracy. The results are in Table 2. 5 Discussion and Conclusions In this paper, we developed a system for predicting grammaticality on an ordinal scale and creat</context>
</contexts>
<marker>Efron, Tibshirani, 1993</marker>
<rawString>B. Efron and R. Tibshirani. 1993. An Introduction to the Bootstrap. Chapman and Hall/CRC, Boca Raton, FL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
<author>Anthony Aue</author>
<author>Martine Smets</author>
</authors>
<title>Sentence-level MT evaluation without reference translations: Beyond language modeling.</title>
<date>2005</date>
<booktitle>In Proceedings of EAMT,</booktitle>
<pages>103--111</pages>
<publisher>SpringerVerlag.</publisher>
<contexts>
<context position="2012" citStr="Gamon et al., 2005" startWordPosition="271" endWordPosition="274">has focused on identifying (and possibly correcting) specific types of grammatical errors that are typically made by English language learners, such as prepositions (Tetreault and Chodorow, 2008), articles (Han et al., 2006), and collocations (Dahlmeier and Ng, 2011). While some applications (e.g., grammar checking) rely on such fine-grained predictions, others might be better addressed by sentence-level grammaticality judgments (e.g., machine translation evaluation). Regarding sentence-level grammaticality, there has been much work on rating the grammaticality of machine translation outputs (Gamon et al., 2005; Parton et al., 2011), such as the MT Quality Estimation Shared Tasks (Bojar et al., 2013, §6), but relatively little on evaluating the grammaticality of naturally occurring text. Also, most other research on evaluating grammaticality involves artificial tasks or datasets (Sun et al., 2007; Lee et al., 2007; Wong and Dras, 2010; Post, 2011). Here, we make the following contributions. • We develop a state-of-the-art approach for predicting the grammaticality of sentences on an ordinal scale, adapting various techniques from the previous work described above. • We create a dataset of grammatica</context>
</contexts>
<marker>Gamon, Aue, Smets, 2005</marker>
<rawString>Michael Gamon, Anthony Aue, and Martine Smets. 2005. Sentence-level MT evaluation without reference translations: Beyond language modeling. In Proceedings of EAMT, pages 103–111. SpringerVerlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Na-Rae Han</author>
<author>Martin Chodorow</author>
<author>Claudia Leacock</author>
</authors>
<title>Detecting errors in English article usage by non-native speakers.</title>
<date>2006</date>
<journal>Natural Language Engineering,</journal>
<volume>12</volume>
<issue>2</issue>
<contexts>
<context position="1618" citStr="Han et al., 2006" startWordPosition="220" endWordPosition="223">he grammaticality of sentences, and present a dataset of learner sentences rated for grammaticality. Such a system could be used, for example, to check or to rank outputs from systems for text summarization, natural language generation, or machine translation. It could also be used in educational applications such as essay scoring. Much of the previous research on predicting grammaticality has focused on identifying (and possibly correcting) specific types of grammatical errors that are typically made by English language learners, such as prepositions (Tetreault and Chodorow, 2008), articles (Han et al., 2006), and collocations (Dahlmeier and Ng, 2011). While some applications (e.g., grammar checking) rely on such fine-grained predictions, others might be better addressed by sentence-level grammaticality judgments (e.g., machine translation evaluation). Regarding sentence-level grammaticality, there has been much work on rating the grammaticality of machine translation outputs (Gamon et al., 2005; Parton et al., 2011), such as the MT Quality Estimation Shared Tasks (Bojar et al., 2013, §6), but relatively little on evaluating the grammaticality of naturally occurring text. Also, most other research</context>
</contexts>
<marker>Han, Chodorow, Leacock, 2006</marker>
<rawString>Na-Rae Han, Martin Chodorow, and Claudia Leacock. 2006. Detecting errors in English article usage by non-native speakers. Natural Language Engineering, 12(2):115–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sapporo, Japan,</location>
<contexts>
<context position="13101" citStr="Klein and Manning, 2003" startWordPosition="2011" endWordPosition="2014">on correlations with human judgments (less than 0.00001 in terms of r). �• sESn 176 r our system 0.668 − non-native LM (§3.2.2) 0.665 − HPSG parse (§3.2.3) 0.664 − PCFG parse (§3.2.4) 0.662 − spelling (§3.2.1) 0.643 − gigaword LM (§3.2.2) 0.638 − link parse (§3.2.3) 0.632 − gigaword count (§3.2.2) 0.630 Table 1: Pearson’s r on the development set, for our full system and variations excluding each feature type. “− X” indicates the full model without the “X” features. 3.2.4 PCFG Parsing Features We find phrase structure trees and basic dependencies with the Stanford Parser’s English PCFG model (Klein and Manning, 2003; de Marneffe et al., 2006).11 We then compute the following: • the parse score as provided by the Stanford PCFG Parser, normalized for sentence length, later referred to as parse prob • a binary feature that captures whether the top node of the tree is sentential or not (i.e. the assumption is that if the top node is nonsentential, then the sentence is a fragment) • features binning the number of dep relations returned by the dependency conversion. These dep relations are underspecified for function and indicate that the parser was unable to find a standard relation such as subj, possibly ind</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate Unlexicalized Parsing. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 423–430, Sapporo, Japan, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lee</author>
<author>Ming Zhou</author>
<author>Xiaohua Liu</author>
</authors>
<title>Detection of Non-Native Sentences Using MachineTranslated Training Data.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers,</booktitle>
<pages>93--96</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York,</location>
<contexts>
<context position="2321" citStr="Lee et al., 2007" startWordPosition="322" endWordPosition="325">cking) rely on such fine-grained predictions, others might be better addressed by sentence-level grammaticality judgments (e.g., machine translation evaluation). Regarding sentence-level grammaticality, there has been much work on rating the grammaticality of machine translation outputs (Gamon et al., 2005; Parton et al., 2011), such as the MT Quality Estimation Shared Tasks (Bojar et al., 2013, §6), but relatively little on evaluating the grammaticality of naturally occurring text. Also, most other research on evaluating grammaticality involves artificial tasks or datasets (Sun et al., 2007; Lee et al., 2007; Wong and Dras, 2010; Post, 2011). Here, we make the following contributions. • We develop a state-of-the-art approach for predicting the grammaticality of sentences on an ordinal scale, adapting various techniques from the previous work described above. • We create a dataset of grammatical and ungrammatical sentences written by English language learners, labeled on an ordinal scale for grammaticality. With this unique data set, which we will release to the research community, it is now possible to conduct realistic evaluations for predicting sentence-level grammaticality. 2 Dataset Descripti</context>
</contexts>
<marker>Lee, Zhou, Liu, 2007</marker>
<rawString>John Lee, Ming Zhou, and Xiaohua Liu. 2007. Detection of Non-Native Sentences Using MachineTranslated Training Data. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers, pages 93–96, Rochester, New York, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristen Parton</author>
<author>Joel Tetreault</author>
<author>Nitin Madnani</author>
<author>Martin Chodorow</author>
</authors>
<title>E-rating machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>108--115</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="2034" citStr="Parton et al., 2011" startWordPosition="275" endWordPosition="278">ifying (and possibly correcting) specific types of grammatical errors that are typically made by English language learners, such as prepositions (Tetreault and Chodorow, 2008), articles (Han et al., 2006), and collocations (Dahlmeier and Ng, 2011). While some applications (e.g., grammar checking) rely on such fine-grained predictions, others might be better addressed by sentence-level grammaticality judgments (e.g., machine translation evaluation). Regarding sentence-level grammaticality, there has been much work on rating the grammaticality of machine translation outputs (Gamon et al., 2005; Parton et al., 2011), such as the MT Quality Estimation Shared Tasks (Bojar et al., 2013, §6), but relatively little on evaluating the grammaticality of naturally occurring text. Also, most other research on evaluating grammaticality involves artificial tasks or datasets (Sun et al., 2007; Lee et al., 2007; Wong and Dras, 2010; Post, 2011). Here, we make the following contributions. • We develop a state-of-the-art approach for predicting the grammaticality of sentences on an ordinal scale, adapting various techniques from the previous work described above. • We create a dataset of grammatical and ungrammatical se</context>
</contexts>
<marker>Parton, Tetreault, Madnani, Chodorow, 2011</marker>
<rawString>Kristen Parton, Joel Tetreault, Nitin Madnani, and Martin Chodorow. 2011. E-rating machine translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 108–115, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pedregosa</author>
<author>G Varoquaux</author>
<author>A Gramfort</author>
<author>V Michel</author>
<author>B Thirion</author>
<author>O Grisel</author>
<author>M Blondel</author>
<author>P Prettenhofer</author>
<author>R Weiss</author>
<author>V Dubourg</author>
<author>J Vanderplas</author>
<author>A Passos</author>
<author>D Cournapeau</author>
<author>M Brucher</author>
<author>M Perrot</author>
<author>E Duchesnay</author>
</authors>
<title>Scikit-learn: Machine Learning in Python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<contexts>
<context position="9317" citStr="Pedregosa et al., 2011" startWordPosition="1432" endWordPosition="1435">rd tokens, the model filters out tokens containing nonalphabetic characters and then computes the number of misspelled words nmiss (later referred to as num misspelled), the proportion of misspelled words nmiss n , and log(nmiss + 1) as features. To identify misspellings, we use a freely available spelling dictionary for U.S. English.5 3.2.2 n-gram Count and Language Model Features Given each sentence, the model obtains the counts of n-grams (n = 1... 3) from English Gigaword and computes the following features:6 log(count(s) + 1) IISn11 3We use ridge regression from the scikit-learn toolkit (Pedregosa et al., 2011) v0.23.1 and the SciKit-Learn Laboratory (http://github.com/ EducationalTestingService/skll). 4Regression models typically produce conservative predictions with lower variance than the original training data. So that predictions better match the distribution of labels in the training data, the system rescales its predictions. It saves the mean and standard deviation of the training data gold standard (Mgold and SDgold, respectively) and of its own predictions on the training data (Mpred and SDpred, respectively). During cross-validation, this is done for each fold. From an initial prediction ˆ</context>
</contexts>
<marker>Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, Duchesnay, 2011</marker>
<rawString>F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Post</author>
</authors>
<title>Judging Grammaticality with Tree Substitution Grammar Derivations.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>217--222</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="858" citStr="Post (2011)" startWordPosition="106" endWordPosition="107">! Research New York, NY, USA tetreaul@yahoo-inc.com Abstract Automated methods for identifying whether sentences are grammatical have various potential applications (e.g., machine translation, automated essay scoring, computer-assisted language learning). In this work, we construct a statistical model of grammaticality using various linguistic features (e.g., misspelling counts, parser outputs, n-gram language model scores). We also present a new publicly available dataset of learner sentences judged for grammaticality on an ordinal scale. In evaluations, we compare our system to the one from Post (2011) and find that our approach yields state-of-the-art performance. 1 Introduction In this paper, we develop a system for the task of predicting the grammaticality of sentences, and present a dataset of learner sentences rated for grammaticality. Such a system could be used, for example, to check or to rank outputs from systems for text summarization, natural language generation, or machine translation. It could also be used in educational applications such as essay scoring. Much of the previous research on predicting grammaticality has focused on identifying (and possibly correcting) specific ty</context>
<context position="2355" citStr="Post, 2011" startWordPosition="330" endWordPosition="331">tions, others might be better addressed by sentence-level grammaticality judgments (e.g., machine translation evaluation). Regarding sentence-level grammaticality, there has been much work on rating the grammaticality of machine translation outputs (Gamon et al., 2005; Parton et al., 2011), such as the MT Quality Estimation Shared Tasks (Bojar et al., 2013, §6), but relatively little on evaluating the grammaticality of naturally occurring text. Also, most other research on evaluating grammaticality involves artificial tasks or datasets (Sun et al., 2007; Lee et al., 2007; Wong and Dras, 2010; Post, 2011). Here, we make the following contributions. • We develop a state-of-the-art approach for predicting the grammaticality of sentences on an ordinal scale, adapting various techniques from the previous work described above. • We create a dataset of grammatical and ungrammatical sentences written by English language learners, labeled on an ordinal scale for grammaticality. With this unique data set, which we will release to the research community, it is now possible to conduct realistic evaluations for predicting sentence-level grammaticality. 2 Dataset Description We created a dataset consisting</context>
<context position="16194" citStr="Post (2011)" startWordPosition="2532" endWordPosition="2533">e averaged human judgments and each system. For the binary task, we report percentage accuracy. Since the predictions from the binary and ordinal systems are on different scales, we include the nonparametric statistic Kendall’s T as a secondary evaluation metric for both tasks. We also evaluated the binary system for the ordinal task by computing correlations between its estimated probabilities and the averaged human scores, and we evaluated the ordinal system for the binary task by binarizing its predictions.12 We compare our work to a modified version of the publicly available13 system from Post (2011), which performed very well on an artificial dataset. To our knowledge, it is the only publicly available system for grammaticality prediction. It is very 11We use the Nov. 12, 2013 version of the Stanford Parser. 12We selected a threshold for binarization from a grid of 1001 points from 1 to 4 that maximized the accuracy of binarized predictions from a model trained on the training set and evaluated on the binarized development set. For evaluating the three single-feature baselines discussed below, we used the same approach except with grid ranging from the minimum development set feature val</context>
<context position="19407" citStr="Post (2011)" startWordPosition="3043" endWordPosition="3044">g accuracy for the binary task. To identify whether the differences in performance for the ordinal task between our system and each of the baselines are statistically significant, we used the BQa Bootstrap (Efron and Tibshirani, 1993) with 10,000 replications to compute 95% confidence intervals for the absolute value of r for our system minus the absolute value of r for each of the alternative methods. For the binary task, we 14In preliminary experiments, we observed little difference in performance between logistic regression and the original support vector classifier used by the system from Post (2011). used the sign test to test for significant differences in accuracy. The results are in Table 2. 5 Discussion and Conclusions In this paper, we developed a system for predicting grammaticality on an ordinal scale and created a labeled dataset that we have released publicly (§2) to enable more realistic evaluations in future research. Our system outperformed an existing state-of-the-art system (Post, 2011) in evaluations on binary and ordinal scales. This is the most realistic evaluation of methods for predicting sentence-level grammaticality to date. Surprisingly, the system from Post (2011) </context>
</contexts>
<marker>Post, 2011</marker>
<rawString>Matt Post. 2011. Judging Grammaticality with Tree Substitution Grammar Derivations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 217–222, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - An Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In 7th International Conference on Spoken Language Processing.</booktitle>
<contexts>
<context position="10571" citStr="Stolcke, 2002" startWordPosition="1621" endWordPosition="1622">Mpred ˆy� =∗ SD M This transformation does r SDped gold + gold• not affect Pearson’s r correlations or rankings, but it would affect binarized predictions. 5http://pythonhosted.org/pyenchant/ 6We use the New York Times (nyt), the Los Angeles Times-Washington Post (ltw), and the Washington PostBloomberg News (wpb) sections from the fifth edition of English Gigaword (LDC2011T07). where Sn represents the n-grams of order n from the given sentence. The model computes the following features from a 5-gram language model trained on the same three sections of English Gigaword using the SRILM toolkit (Stolcke, 2002): • the average log-probability of the given sentence (referred to as gigaword avglogprob later) • the number of out-of-vocabulary words in the sentence Finally, the system computes the average log-probability and number of out-of-vocabulary words from a language model trained on a collection of essays written by non-native English speakers7 (“non-native LM”). 3.2.3 Precision Grammar Features Following Wagner et al. (2007) and Wagner et al. (2009), we use features extracted from precision grammar parsers. These grammars have been hand-crafted and designed to only provide complete syntactic ana</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - An Extensible Language Modeling Toolkit. In 7th International Conference on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guihua Sun</author>
<author>Xiaohua Liu</author>
<author>Gao Cong</author>
<author>Ming Zhou</author>
<author>Zhongyang Xiong</author>
<author>John Lee</author>
<author>Chin-Yew Lin</author>
</authors>
<title>Detecting Erroneous Sentences using Automatically Mined Sequential Patterns.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>81--88</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2303" citStr="Sun et al., 2007" startWordPosition="318" endWordPosition="321">(e.g., grammar checking) rely on such fine-grained predictions, others might be better addressed by sentence-level grammaticality judgments (e.g., machine translation evaluation). Regarding sentence-level grammaticality, there has been much work on rating the grammaticality of machine translation outputs (Gamon et al., 2005; Parton et al., 2011), such as the MT Quality Estimation Shared Tasks (Bojar et al., 2013, §6), but relatively little on evaluating the grammaticality of naturally occurring text. Also, most other research on evaluating grammaticality involves artificial tasks or datasets (Sun et al., 2007; Lee et al., 2007; Wong and Dras, 2010; Post, 2011). Here, we make the following contributions. • We develop a state-of-the-art approach for predicting the grammaticality of sentences on an ordinal scale, adapting various techniques from the previous work described above. • We create a dataset of grammatical and ungrammatical sentences written by English language learners, labeled on an ordinal scale for grammaticality. With this unique data set, which we will release to the research community, it is now possible to conduct realistic evaluations for predicting sentence-level grammaticality. 2</context>
</contexts>
<marker>Sun, Liu, Cong, Zhou, Xiong, Lee, Lin, 2007</marker>
<rawString>Guihua Sun, Xiaohua Liu, Gao Cong, Ming Zhou, Zhongyang Xiong, John Lee, and Chin-Yew Lin. 2007. Detecting Erroneous Sentences using Automatically Mined Sequential Patterns. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 81–88, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel R Tetreault</author>
<author>Martin Chodorow</author>
</authors>
<title>The Ups and Downs of Preposition Error Detection in ESL Writing.</title>
<date>2008</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>865--872</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="1589" citStr="Tetreault and Chodorow, 2008" startWordPosition="215" endWordPosition="218">lop a system for the task of predicting the grammaticality of sentences, and present a dataset of learner sentences rated for grammaticality. Such a system could be used, for example, to check or to rank outputs from systems for text summarization, natural language generation, or machine translation. It could also be used in educational applications such as essay scoring. Much of the previous research on predicting grammaticality has focused on identifying (and possibly correcting) specific types of grammatical errors that are typically made by English language learners, such as prepositions (Tetreault and Chodorow, 2008), articles (Han et al., 2006), and collocations (Dahlmeier and Ng, 2011). While some applications (e.g., grammar checking) rely on such fine-grained predictions, others might be better addressed by sentence-level grammaticality judgments (e.g., machine translation evaluation). Regarding sentence-level grammaticality, there has been much work on rating the grammaticality of machine translation outputs (Gamon et al., 2005; Parton et al., 2011), such as the MT Quality Estimation Shared Tasks (Bojar et al., 2013, §6), but relatively little on evaluating the grammaticality of naturally occurring te</context>
</contexts>
<marker>Tetreault, Chodorow, 2008</marker>
<rawString>Joel R. Tetreault and Martin Chodorow. 2008. The Ups and Downs of Preposition Error Detection in ESL Writing. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 865–872, Manchester, UK, August. Coling 2008 Organizing Committee.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Joachim Wagner</author>
<author>Jennifer Foster</author>
<author>Josef van Genabith</author>
</authors>
<title>A Comparative Evaluation of Deep and Shallow Approaches to the Automatic Detection of Common Grammatical Errors.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>112--121</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<marker>Wagner, Foster, van Genabith, 2007</marker>
<rawString>Joachim Wagner, Jennifer Foster, and Josef van Genabith. 2007. A Comparative Evaluation of Deep and Shallow Approaches to the Automatic Detection of Common Grammatical Errors. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 112–121, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joachim Wagner</author>
<author>Jennifer Foster</author>
<author>Josef van Genabith</author>
</authors>
<title>Judging grammaticality: Experiments in sentence classification.</title>
<date>2009</date>
<journal>CALICO Journal,</journal>
<volume>26</volume>
<issue>3</issue>
<marker>Wagner, Foster, van Genabith, 2009</marker>
<rawString>Joachim Wagner, Jennifer Foster, and Josef van Genabith. 2009. Judging grammaticality: Experiments in sentence classification. CALICO Journal, 26(3):474–490.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sze-Meng Jojo Wong</author>
<author>Mark Dras</author>
</authors>
<title>Parser Features for Sentence Grammaticality Classification.</title>
<date>2010</date>
<booktitle>In Proceedings of the Australasian Language Technology Association Workshop</booktitle>
<pages>67--75</pages>
<location>Melbourne, Australia,</location>
<contexts>
<context position="2342" citStr="Wong and Dras, 2010" startWordPosition="326" endWordPosition="329">h fine-grained predictions, others might be better addressed by sentence-level grammaticality judgments (e.g., machine translation evaluation). Regarding sentence-level grammaticality, there has been much work on rating the grammaticality of machine translation outputs (Gamon et al., 2005; Parton et al., 2011), such as the MT Quality Estimation Shared Tasks (Bojar et al., 2013, §6), but relatively little on evaluating the grammaticality of naturally occurring text. Also, most other research on evaluating grammaticality involves artificial tasks or datasets (Sun et al., 2007; Lee et al., 2007; Wong and Dras, 2010; Post, 2011). Here, we make the following contributions. • We develop a state-of-the-art approach for predicting the grammaticality of sentences on an ordinal scale, adapting various techniques from the previous work described above. • We create a dataset of grammatical and ungrammatical sentences written by English language learners, labeled on an ordinal scale for grammaticality. With this unique data set, which we will release to the research community, it is now possible to conduct realistic evaluations for predicting sentence-level grammaticality. 2 Dataset Description We created a datas</context>
</contexts>
<marker>Wong, Dras, 2010</marker>
<rawString>Sze-Meng Jojo Wong and Mark Dras. 2010. Parser Features for Sentence Grammaticality Classification. In Proceedings of the Australasian Language Technology Association Workshop 2010, pages 67– 75, Melbourne, Australia, December.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>