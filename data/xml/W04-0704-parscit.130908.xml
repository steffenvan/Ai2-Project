<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.956979">
Coreference Resolution for Information Extraction
</title>
<author confidence="0.835958">
Dmitry Zelenko and Chinatsu Aone and Jason Tibbetts
</author>
<affiliation confidence="0.6021">
SRA International
</affiliation>
<address confidence="0.530138">
4300 Fair Lakes Ct.
Fairfax, VA 22033
</address>
<email confidence="0.980014">
{dmitry zelenko,chinatsu aone,jason tibbetts}@sra.com
</email>
<sectionHeader confidence="0.978472" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998428">
We compare several approaches to coreference
resolution in the context of information extrac-
tion. We present a loss-based decoding frame-
work for coreference resolution and a greedy al-
gorithm for approximate coreference decoding,
in conjunction with Perceptron and logistic re-
gression learning algorithms. We experimen-
tally evaluate the presented approaches using
the Automatic Content Extraction evaluation
methodology, with promising results.
</bodyText>
<sectionHeader confidence="0.995499" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99989214893617">
Coreference resolution is an important problem
of determining whether discourse references in
text correspond to the same real world entities
(Mitkov, 2002). In this paper, we address a
restricted version of the coreference resolution
problem in the context of information extrac-
tion. The information extraction perspective on
coreference resolution imposes a limited scope
on the set of entities to be resolved. We are
not interested in resolving all coreferences in
a document, but only those involving entities
to be extracted as part of a specific extraction
task. Thus, we can safely ignore coreference res-
olution of those names, noun phrases, and pro-
nouns that are deemed irrelevant to the extrac-
tion task at hand.
The extraction-oriented coreference resolu-
tion problem is motivated by the Entity De-
tection and Tracking (EDT) task of the Au-
tomatic Content Extraction evaluation (ACE,
2003). The EDT task requires detecting of
named, nominal, and pronominal mentions and
tracking mentions corresponding to the same
real world entities. We adopt the ACE con-
vention of using mentions for the names, noun
phrases, and pronouns, while reserving entities
to represent the equivalence classes of mentions,
i.e. , the sets of mentions corresponding to the
same real world entities.
In this paper, we will take an entity men-
tion extraction component as given (the men-
tion extraction component of (Aone and Ramos-
Santacruz, 2000)), and consider coreference res-
olution algorithms that work with already ex-
tracted entity mentions.
A brief outline of the paper follows. In Sec-
tion 2, we survey previous work on corefer-
ence resolution. In Section 3, we present our
coreference resolution framework that encom-
passes a number of standard coreference reso-
lution approaches. In Section 4, we introduce
a loss-based coreference decoding methodology
and present an approximate greedy coreference
decoding algorithm. In Section 5, we experi-
mentally evaluate several coreference resolution
architectures, in the context of information ex-
traction.
</bodyText>
<sectionHeader confidence="0.919398" genericHeader="method">
2 Coreference Resolution Overview
</sectionHeader>
<bodyText confidence="0.9999465">
The problem of anaphora resolution is often
studied (Mitkov, 2002), which is closely re-
lated to the coreference resolution problem.
Anaphora is a phenomenon of referring to a
preceding mention in a document. The refer-
ence is then called an anaphor and the referred
mention is termed an antecedent. Anaphora
resolution problem is often restricted to nomi-
nal and pronominal anaphors, thereby ignoring
the problem of name coreference, which is ex-
tremely important for information extraction.
Additionally, since anaphora addresses (liter-
ally) only backward references, the infrequent
phenomenon of forward references (termed cat-
aphora) is not covered by anaphora resolution.
In our presentation, the term &amp;quot;coreference res-
olution&amp;quot; implies resolution of named, nominal,
and pronominal entity mentions that subsumes
both backward and forward references.
Let us define the coreference relation coref
on a set of document entity mentions. We say
that the relation coref (x, y) holds if and only if
the mentions x and y are coreferent.
It is frequently helpful to compartmentalize
the relation coref (x, y) and, hence, the coref-
erence resolution task into three different sub-
tasks corresponding to different kinds of entities
involved. More precisely, if x or y is a pronom-
inal entity, then we obtain a pronoun resolu-
tion problem. Otherwise, if x or y is a nominal
entity, then we have a noun phrase resolution
problem. Finally, if both x and y are named
entities, then it is a name resolution problem.
An information extraction system needs to
address all three aspects of the coreference res-
olution problem. Yet different modeling and al-
gorithmic choices may be appropriate for name,
noun phrase, and pronoun resolution.
Most early work on coreference and anaphora
resolution dealt with pronoun coreference (Lap-
pin and Leass, 1994; Kennedy and Boguraev,
1996). The early approaches identified a set
of pronouns in a document, and, for each pro-
noun, sought to determine the best antecedent.
Different definitions of &amp;quot;best&amp;quot; led to different
carefully designed and complex rules that were
sometimes based on existing discourse theories
(Sidner, 1979).
The area of pronoun and noun phrase coref-
erence resolution was greatly revitalized since
mid-1990s by application of learning approaches
to the problem. We note, amongst many, the
work of (Aone and Bennett, 1996; McCarthy
and Lehnert, 1995; Ng, 2001; Ng and Cardie,
2002).
A coreference example is a feature-based rep-
resentation of a pair of mentions that is de-
signed to make manifest the properties of the
anaphor and its candidate antecedent that are
most helpful in making the decision whether the
anaphor indeed refers to the antecedent in ques-
tion. A coreference example has a binary label
reflecting whether the entities that constitute
the example are indeed coreferent or not. Most
learning-based systems for coreference resolu-
tion employed larger hand-crafted feature sets
(Ng, 2001).
A number of learning algorithms have been
experimentally evaluated on the coreference res-
olution problem. Many published studies em-
ployed a decision tree algorithm (Aone and Ben-
nett, 1996; Ng, 2001; Ng and Cardie, 2002). We
also note a few global probabilistic modeling
approaches to coreference resolution: the gen-
erative probabilistic model of (Charniak et al.,
1998) and the conditional random filed model
of (McCallum and Wellner, 2003).
The coreference classifiers tha are output by
learning algorithms need to be used in conjunc-
tion with coreference decoding algorithms in or-
der to induce the coref equivalence relation on
the set of mentions. A most popular corefer-
ence decoding algorithm links an anaphor to the
first preceding antecedent predicted as corefer-
ent with the anaphor (Ng, 2001). We will call it
the link-first decoding algorithm. An alterna-
tive decoding algorithm (termed link-best) links
the anaphor to the most probable preceding an-
tecedent, where the probability of antecedent is
taken to the confidence of the coreference clas-
sifier prediction (Ng and Cardie, 2002). We
will consider both link-first and link-best decod-
ing algorithms and compare them with the new
decoding framework that we introduce in Sec-
tion 4.
Our decoding framework most resembles the
work of (McCallum and Wellner, 2003), where a
coreference model represents a conditional ran-
dom field. The coreference decoding problem
for the conditional random field leads to a corre-
lation clustering problem (Bansal et al., 2002).
We also reduce the coreference decoding prob-
lem to a correlation clustering problem, but use
a different approximation algorithm for its so-
lution.
In the absence of training data, we note ap-
plication of clustering for coreference of noun
phrases (Cardie and Wagstaff, 1999). Namely,
the noun phrase attributes are used to define a
distance function that is used within a heuris-
tic clustering algorithm to produce a clustering
of noun phrases that aims to correspond to the
coreference partition of the corresponding noun
phrase entities.
In addition to the work on coreference res-
olution within documents, there is an emerg-
ing body on a more general identity uncertainty
problem, which is concerned with determining
whether two records describe the same entity
(Pasula et al., 2003).
</bodyText>
<sectionHeader confidence="0.9582945" genericHeader="method">
3 Coreference Resolution
Architecture
</sectionHeader>
<bodyText confidence="0.999814">
We consider the following components in adap-
tive approaches to coreference resolution.
</bodyText>
<listItem confidence="0.997888">
• Coreference examples and their feature rep-
resentation.
• Coreference examples&apos; generation process.
• Learning algorithms for coreference classi-
fiers.
• Decoding algorithm that combines predic-
tions of coreference classifiers into a coher-
ent discourse interpretation.
</listItem>
<bodyText confidence="0.946304">
Let us examine the components in more de-
tail.
</bodyText>
<subsectionHeader confidence="0.978779">
3.1 Coreference Examples and
Classifiers
</subsectionHeader>
<bodyText confidence="0.9998756">
We use entity mentions produced by the men-
tion extraction system (Aone and Ramos-
Santacruz, 2000) that augments mentions with
the following additional attributes (when appro-
priate): POS (a part of speech tag from a re-
stricted set of POS tags), head (the mention
head, mostly pertinent for named and nomi-
nal mentions), gender (the gender of the men-
tion, if available), number (plurality of the men-
tion), first name (the first name of the men-
tion, if available), last name (the last name
of the mention, if available), determiner (the
string value of the mention determiner, if avail-
able), personal pronoun type (for pronouns,
the first/second/third person pronoun indica-
tor), possessive pronoun type (for pronouns,
the possessive pronoun indicator).
For an anaphor/antecedent pair, we will use
the following set of features in coreference ex-
amples.
</bodyText>
<listItem confidence="0.978745043478261">
• Every conjunction A = X n B = Y , where
A is an anaphor attribute, B is an an-
tecedent attribute, and X and Y are the
corresponding values of the attributes in
the given anaphor and the antecedent.
• For every common attribute A of an
anaphor and an antecedent, the value of the
proposition, anaphor.A = antecedent.A
that reflects the same attributes have the
same value in both the given anaphor and
the antecedent.
• For nominal and pronominal anaphors,
we used distance features (number of
words/sentences/paragraphs between an
anaphor and an antecedent). The distance
features are discretized using the entropy-
based discretization algorithm with the
minimal description length stopping crite-
rion (Dougherty et al., 1995).
• For nominal anaphors, we used the &amp;quot;appos-
itive&amp;quot; feature, if the anaphor is in apposi-
tion with the antecedent.
• For named anaphors, we used the substring
</listItem>
<bodyText confidence="0.972805631578948">
feature (indicating whether one name is a
substring of the other) and the common
acronym/alias feature (indicating whether
one name is a common acronym/alias of the
other).
We note that the extracted mentions
are typed, and we will consider only an
anaphor/antecedent pair, where the anaphor
and the antecedent belong to the same type
(e.g., we will not try to corefer a person
with an organization). We also incorpo-
rate additional knowledge in the coreference
resolution process by imposing constraints on
the set of possible anaphor/antecedent pairs.
Note that, for the sake of brevity, we use the
words &amp;quot;anaphor&amp;quot; (ana) and &amp;quot;antecedent&amp;quot; (ante)
for both anaphora and cataphora phenomena.
The constraints are specified by the following
relation tsCandidateAntecedent:
</bodyText>
<figure confidence="0.5601745">
isCandidateAntecedent(anaphora, antecedent) _
1, if anaphora is pronominal,
and anaphora follows antecedent
1, if anaphora is pronominal,
and antecedent is not pronominal
1, if anaphora is nominal,
and antecedent is not pronominal,
and it precedes anaphora
1, if ana is nominal,
and ante is a name
1, if anaphora is a name,
and antecedent is a name
that precedes anaphora
0, otherwise
</figure>
<bodyText confidence="0.955944642857143">
The constraints restrict the list of possible an-
tecedents for different classes of anaphora by in-
corporating coreference knowledge. The knowl-
edge specifies that pronominal anaphora never
refer forward to other pronouns, that nominal
anaphora refer to preceding nominals or names,
and names refer only to preceding names.
We also relax the assumption of the single
classifier. Instead, we split the coreference res-
olution classifier into several distinct projected
classifiers depending on the type and level of
an anaphor. The classifiers are presented in Ta-
ble 1. The split is a result of the fact that differ-
ent features are appropriate for different types
and levels of an anaphor. For example, while the
distance between an anaphor and antecedent (in
terms of words, sentences, paragraphs) might be
extremely useful for pronominal anaphors, it is
not a valuable feature for name coreference res-
olution. Also, different coreference usage pat-
terns may be prevalent for different kinds of
anaphor. For instance, we may expect that the
_ ��������������������
�������������������
pronouns &amp;quot;I&amp;quot; and &amp;quot;it&amp;quot; behave differently with
respect to coreference phenomena. Hence, dif-
ferent models may be appropriate for different
kinds of pronominal anaphors.
</bodyText>
<table confidence="0.999800875">
Anaphor (Type = t) Classifier
Name Cname,t
Nominal Cnominal,t
1st person pronoun Cfirst
2nd person pronoun Csecond
It pronoun Cit
Plural 3rd person pronoun Cthey
Singular 3rd person pronoun Cthird
</table>
<tableCaption confidence="0.999612">
Table 1: Coreference classifiers
</tableCaption>
<bodyText confidence="0.991367666666667">
Thus, for the 5 entity types used in our eval-
uation in Section 5, we learn 15 distinct coref-
erence classifiers.
</bodyText>
<subsectionHeader confidence="0.996732">
3.2 Coreference Example Generation
</subsectionHeader>
<bodyText confidence="0.99999224">
We employ two strategies for coreference ex-
ample generation. For classifiers used in the
link-first decoding algorithm, we proceed from
a fixed anaphora backward (in text), and gen-
erate a negative example for each candidate an-
tecedent until an antecedent coreferent with the
anaphor is encountered. A positive example is
generated for the antecedent, and the process of
generating examples for the fixed anaphor stops.
We also impose an upper bound M on the num-
ber of preceding candidate antecedents that we
consider. If none of the M preceding candidate
antecedents is coreferent with the anaphor, then
we proceed from the anaphor forward in the
same fashion until we encounter an antecedent
coreferent anaphor or exhaust the M forward
candidate antecedents. This is termed a sequen-
tial example generation process.
For classifiers used in the link-best decoding
algorithm and the loss-based decoding frame-
work presented in Section 4, we generate an ex-
ample for every candidate antecedent residing
within the window of M candidate antecedents
around the anaphor. This is termed an exhaus-
tive example generation process.
</bodyText>
<subsectionHeader confidence="0.994429">
3.3 Coreference Decoding Algorithms
</subsectionHeader>
<bodyText confidence="0.999978794117647">
The coreference decoding procedure combines
the predictions of coreference classifiers into a
single coherent interpretation.
The most prevalent decoding approach is the
link-first coreference decoding algorithm (Ng,
2001). The algorithm processes mentions se-
quentially in order of their appearance in the
document and establishes a coreference relation
between a mention mi and the closest preced-
ing candidate antecedent classified as corefer-
ent with mi by the learned coreference classifier,
among the M preceding candidate antecedents.
If no such preceding mention is found, the algo-
rithm establishes a coreference relation between
the mention mi and the closest following candi-
date antecedent classified as coreferent with mi,
among the M following candidate antecedents.
After a single pass through the document, the
equivalence classes are constructed via the tran-
sitive closure of the established coreference re-
lations.
A popular alternative to link-first is the link-
best coreference decoding algorithm (Ng and
Cardie, 2002). The algorithm processes men-
tions sequentially in order of their appearance in
the document and establishes a coreference rela-
tion between a mention mi and the most prob-
able candidate antecedent classified as corefer-
ent with mi by the learned coreference classifier,
among the M preceding and M following can-
didate antecedents. After a single pass through
the document, the equivalence classes are again
constructed via the transitive closure of the es-
tablished coreference relations.
</bodyText>
<subsectionHeader confidence="0.872645">
3.4 Machine Learning Algorithms
</subsectionHeader>
<bodyText confidence="0.999625944444445">
We will use two machine learning algorithms
in our experiments: logistic regression (Berk-
son, 1944) and the (voted) Perceptron algo-
rithm (Freund and Schapire, 1999).
We introduce the following notation. Let
x E X denote a (coreference) example, where
X C_ RN is an example space. Let y E Y =
{—1,1} be an example label, where —1 (+1) cor-
responds to a negative (positive) coreference ex-
ample. We term a pair (x, y) a labeled example.
Let S = {(xl, y&apos;), ... , (xs, ys)} be a (train-
ing) sample of labeled examples. A learning al-
gorithm A uses the sample S to produce a clas-
sifier c : X —� Y . Both Perceptron and logistic
regression learn linear classifiers&apos; in RN, that
is, cu ,(x) = sgn(wTx). Perceptron and logistic
regression seek classifiers that minimize partic-
ular loss functions l (c, x, y) with respect to the
</bodyText>
<footnote confidence="0.9822464">
&apos;Without loss of generality, we assume that the in-
tercept value of the classifier linear function is equal to
0.
2A loss function l(c, x, y) quantifies the error (loss) of
the classifier c on the labeled example (x, y).
</footnote>
<equation confidence="0.985132666666667">
sample S:
cw,p, = arg min
cw:wERN
</equation>
<bodyText confidence="0.9606105">
The Perceptron learning algorithm approxi-
mately minimizes the 0-1 loss function 1 = 10 =
(sgn(—gwTx))+, where (a)+ = 1 if a &gt; 0
and (a)+ = 0, otherwise. Logistic regression
minimizes the logistic loss function 1 = 1loy =
1n (1 + e—YwT X).
</bodyText>
<sectionHeader confidence="0.964789" genericHeader="method">
4 Loss-based Coreference Decoding
</sectionHeader>
<bodyText confidence="0.999966">
Let M = m1, ... , mn be the set of document
mentions of the same type. Let A be a learning
algorithm for learning a coreference classifier c
mapping a pair of mentions (mi, mj) to {—1, 1}.
Let 1 be a loss function that is being minimized
by A.
Let M1, M2,..., Mk be an equivalence class
partition of M. Define the variable mij that
indicates whether two mentions belong to the
same equivalence class, as follows:
</bodyText>
<equation confidence="0.791340666666667">
mij = { 1, if 31 E {1, ... ,k}, so that
—1, otherwise
mi E Ml and mjEM1
</equation>
<bodyText confidence="0.9994754">
Let .M = {mij} be an equivalence class par-
tition of the entities M, and let xij denote the
coreference example computed for mentions mi
and mj. Then, the partition induces the follow-
ing loss with respect to the classifier c:
</bodyText>
<equation confidence="0.929775">
1 (c, .M) =E 1 (c, xij, mij) (2)
i&gt;j
</equation>
<bodyText confidence="0.996701333333333">
During decoding, we seek the partition .M*
that minimizes the partition loss (2), given the
classifier c.
</bodyText>
<equation confidence="0.993227">
.M* = argminm1(c, .M)
</equation>
<bodyText confidence="0.999693285714286">
We note that the classification decisions are
not independent of one another, since the equiv-
alence relation is transitive, and mij = 1nmjk =
1 implies that mik = 1.
Let us denote wij = wTxij. In order to
make (2) more manageable for our particular
loss functions, we observe that
</bodyText>
<equation confidence="0.425930666666667">
min 1: 1 (c, xij, mij) = max 1: g (c, xij, mij)
mzj j mzj
i i,j
</equation>
<bodyText confidence="0.875273">
where g (c, xij, mij) = 1 (c, xij, —mij) —
</bodyText>
<table confidence="0.746965625">
1 (c, xij, mij). In particular, for the Perceptron
0-1 loss function, we see that g(cw, x, y) =
10 (cw, x, —g) — 10 (cw, x, 2J) = sgn(ywT x), and
the objective function (2) becomes
� sgn(wijmij) —�T,,,zj max (3)
i,j
For logistic regression, g(cw, x, g)
1�og (cW, x, —2J) 1�og (cW, x, 2J)
</table>
<equation confidence="0.959917166666667">
T
1n 1+eyw T = 1n (eyWT X) = gwTx, and
1+e-yw x
the objective function (2) becomes
E wijmij —�mzj max (4)
i,j
</equation>
<bodyText confidence="0.999900428571429">
The optimization problems (3) and (4) are the
formulations of the unweighted and weighted
versions, respectively, of the correlation cluster-
ing problem (Bansal et al., 2002). Both versions
are NP-hard, so we have to resort to approxi-
mation algorithms for their solution. (Bansal
et al., 2002) presents two approximation algo-
rithms for the correlation clustering problem,
and (McCallum and Wellner, 2003) use a vari-
ant of the Minimizing Disagreements algo-
rithm in conjunction with their conditional ran-
dom field coreference model. We instead intro-
duce a simple greedy decoding algorithm pre-
sented in the following section.
</bodyText>
<subsectionHeader confidence="0.639332">
4.1 Greedy Coreference Decoding
Algorithm
</subsectionHeader>
<bodyText confidence="0.999958111111111">
The greedy decoding algorithm incrementally
optimizes the (gain) function Ei j gijmij (where
gij is either wij or sgn(wij)). The logistic re-
gression version of the algorithm is presented as
Algorithm 1. The algorithm initially puts each
mention into a separate entity and then itera-
tively merges the entities, while the merge im-
proves the gain function. During each iteration,
the pair of entities is selected in a greedy fash-
ion to optimize the gain improvement achieved
by the merge. Note that the algorithm itera-
tively updates the weights between the already
merged entities.
We note that no approximation results are
known for Algorithm 1. We experimentally
evaluate the greedy decoding algorithm in Sec-
tion 5 and compare it with link-first and link-
best decoding algorithms.
</bodyText>
<sectionHeader confidence="0.995922" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999672">
We evaluate several coreference resolution con-
figurations composed of the different compo-
</bodyText>
<equation confidence="0.994846666666667">
s
1(cw, xi, 2Ji) (1)
i=1
</equation>
<bodyText confidence="0.716389">
Algorithm 1 The Greedy Decoding Algorithm
(ml, m2, ... , mn) is the list of mentions or-
dered by their location in the document
cu, is the coreference classifier
</bodyText>
<equation confidence="0.894345166666667">
.M = {{1}, {2}, ... , {n}}
for all (m2, mj), {i} E .M, { j} E .M do
if isCandidateAntecedent(mi,mj) then
w{Zll{il = wT xzj
else
w{Zl&apos;{�l = 0
end if
end for
Sort {w{Zl&apos;{�l}
wmax = wi*~j* = maxiEM&gt;jEM\i wi&gt;j
while 1.M1 &gt; 1 and wn,ax &gt; 0 do
.M=.M\{i,j}
for all k E .M do
wk~i*Uj* = wk,i* + wk&gt;j*
end for
.M=.MU{i*Uj*}
wmax = wi*~j* = maxiEM&gt;jEM\i wi&gt;j
end while
</equation>
<bodyText confidence="0.998913433333333">
nents presented in Section 3. The six dif-
ferent system configurations are shown in Ta-
ble 2. We set the value of M = 10 (the maxi-
mum anaphora/antecedent distance) for exam-
ple generation and link-best decoding proce-
dures.
All systems use the same features and the
same classifier configuration presented in Sec-
tion 3.
The data for our experiments comprised a col-
lection of news articles from the last 3 months
of 2000 used as development data in the ACE
2003 evaluation (ACE, 2003). The 105 articles
were split randomly into the training/testing
sets. The training set contained 53 articles, and
the testing set contained 52 articles.
We use the evaluation methodology of the
Automatic Content Extraction (ACE) program
(ACE, 2003). The ACE program quantifies per-
formance of information extraction systems in
terms of the value measure that ranges between
100 (a perfect system) to 0 (a system that out-
puts nothing) to —oc. It is possible for the value
to be negative, if a system outputs too many in-
correct entities.
We note that our coreference resolution eval-
uation is somewhat indirect since we measure
not the coreference performance per se (e.g., the
MUC coreference measure (MUC, 1998)), but
the impact on coreference resolution on infor-
</bodyText>
<table confidence="0.999513571428571">
Generation Algorithm Decoding
sequential logistic regression link-first
exhaustive logistic regression link-best
exhaustive logistic regression greedy
sequential Voted Perceptron link-first
exhaustive Voted Perceptron link-best
exhaustive Voted Perceptron greedy
</table>
<tableCaption confidence="0.732776">
Table 2: Coreference Resolution Configurations
</tableCaption>
<table confidence="0.999627333333333">
link-first link-best greedy
LR 75.9 74.2 76.4
VP 75.8 75.4 75.8
</table>
<tableCaption confidence="0.967486">
Table 3: ACE Values for Different Configura-
tions
</tableCaption>
<bodyText confidence="0.973123125">
mation extraction performance.
The ACE evaluation measure is also intrinsi-
cally imbalanced, that is, it penalizes for under-
merging entities a lot more than it penalizes
for over-merging entities. To counter this, we
optimized the cost ratios3 of coreference classi-
fiers on the training sets using the 5-fold cross-
validation.
</bodyText>
<subsectionHeader confidence="0.950314">
5.1 Coreference Evaluation Results
</subsectionHeader>
<bodyText confidence="0.999983947368421">
The Table 3 presents the ACE values for the
coreference decoding algorithms combined with
the corresponding learning algorithms. It is
worth noting that human-level performance for
the task is circa 85 (LDC, 2003). Therefore, the
systems achieve more than 85% of the human-
level performance.
The results indicate that the greedy decod-
ing algorithm compares favorably with the more
traditional coreference decoding approaches.
Surprisingly, for our dataset, the link-best de-
coding algorithm did not perform as well as
the competing approaches. The slight boost
in the performance obtained by employing the
weighted version of the greedy decoding algo-
rithm derived from the logistic loss function in-
dicates that the greedy decoding algorithm is
able to take into account calibrated loss values
of the logistic function.
</bodyText>
<footnote confidence="0.887246666666667">
3The cost ratio of C assigns the value of C to an
error made on a positive example, and the value of 1 to
an error made on a negative example.
</footnote>
<sectionHeader confidence="0.778694" genericHeader="conclusions">
References
</sectionHeader>
<bodyText confidence="0.816807">
2003. Automatic Content Extraction.
http://www.nist.gov/speech/tests/ace/index.htm.
</bodyText>
<sectionHeader confidence="0.989572" genericHeader="references">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999947517241379">
Our work addresses coreference resolution from
the principled decoding perspective. While
there has been a lot of work on using machine
learning for coreference resolution, the decoding
algorithms for coreference resolution were rarely
studied, and usually considered separately from
the underlying learning problems (with the no-
table exception of (McCallum and Wellner,
2003)). Our coreference decoding methodol-
ogy couples the learning algorithm loss func-
tions with the decoding objectives and reformu-
lates the decoding problem as a correlation clus-
tering problem with a learned distance metric.
The correlation clustering problem is beginning
to be widely studied, and we plan to evaluate
experimentally many pertinent algorithms pro-
posed within the theoretical computer science
community (Charikar et al., 2003; Demaine and
Immorlica, 2003).
We presented coreference resolution in the
context of information extraction and evaluated
extrinsically the performance of several corefer-
ence resolution configurations. Perhaps due to
the extrinsic evaluation, the conclusions are not
clear-cut, and an additional intrinsic evaluation
will be necessary to ascertain our results. Nev-
ertheless, we believe that our framework is use-
ful for designing coreference resolution systems,
and our results call for further research.
</bodyText>
<reference confidence="0.986690440476191">
C. Aone and S. W. Bennett. 1996. Apply-
ing machine learning to anaphora resolution.
In S. Wermter, E. Riloff, and G. Scheler,
editors, Connectionist, Statistical, and Sym-
bolic Approaches to Learning for Natural Lan-
guage Processing, volume 1040 of Lecture
Nodes in Artificial Intelligence. Springer Ver-
lag, Berlin.
C. Aone and M. Ramos-Santacruz. 2000. Rees:
A large-scale relation and event extraction
system. In Proceedings of the 6th Applied
Natural Language Processing Conference.
N. Bansal, A. Blum, and S. Chawla. 2002.
Correlation clustering. In Proceedings of The
43rd Annual IEEE Symposium on Founda-
tions of Computer Science.
J. Berkson. 1944. Application of the logistic
function to bio-assay. Jour- nal of the Amer-
ican Statistical Association, (9).
C. Cardie and K. Wagstaff. 1999. Noun phrase
coreference as clustering. In Proceedings of
the Joint Conference on Empirical Methods
in Natural Language Processing and Very
Large Corpora.
M. Charikar, V. Guruswami, and A. Wirth.
2003. Clustering with qualitative informa-
tion. In Proceedings of the 44th Annual IEEE
Symposium on Foundations of Computer Sci-
ence, page 524. IEEE Computer Society.
E. Charniak, N. Ge, and J. Hale. 1998. A sta-
tistical approach to anaphora resolution. In
Proceedings of the Sixth Workshop on Very
Large Corpora.
E. Demaine and N. Immorlica. 2003. Correla-
tion clustering with partial information. In
Proceedings of the 6th International Work-
shop on Approximation Algorithms for Com-
binatorial Optimization Problems.
J. Dougherty, R. Kohavi, and M. Sahami. 1995.
Supervised and unsupervised discretization of
continuous features. In Proc. 12th Interna-
tional Conference on Machine Learning. Mor-
gan Kaufmann.
Y. Freund and R. Schapire. 1999. Large margin
classification using the perceptron algorithm.
Machine Learning, 37 (3).
C. Kennedy and B. Boguraev. 1996. Anaphora
for everyone: pronominal anaphora resolu-
tion without a parser. In Proceedings of
the 16International Conference on Computa-
tional Linguistics.
S. Lappin and H. Leass. 1994. An algorithm
for pronominal anaphora resolution. Compu-
tational Linguistics, 20 (4).
2003. Personal communication. Linguistic Data
Consortium.
A. McCallum and B. Wellner. 2003. Toward
conditional models of identity uncertainty
with application to proper noun coreference.
In Proceedings of the IJCAI Workshop on In-
formation Integration on the Web.
J. McCarthy and W. Lehnert. 1995. Using de-
cision trees for coreference resolution. In Pro-
ceedings of the 1995 International Joint Con-
ference on Artificial Intelligence.
R. Mitkov. 2002. Anaphora Resolution. Long-
man.
1998. Proceedings of the 6th Message Under-
tanding Conference (MUC-7).
V. Ng and C. Cardie. 2002. Improving machine
learning approaches to coreference resolution.
In Proceedings of the 40th Anniversary Meet-
ing of the Association for Computational Lin-
guistics.
W. Soon; D. Lim; H. Ng. 2001. A machine
learning approach to coreference resolution of
noun phrases. Computational Linguistics, 12.
H. Pasula, B. Marthi, B. Milch, S. Russell, and
I. Shpitser. 2003. Identity uncertainty and
citation matching. In Advances in Neural In-
formation Processing Systems /5. MIT Press.
C. Sidner. 1979. Toward a computational the-
ory of definite anaphora comprehension in
English. Technical report, MIT Press.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.658455">
<title confidence="0.999798">Coreference Resolution for Information Extraction</title>
<author confidence="0.950633">Zelenko Aone</author>
<affiliation confidence="0.8045">SRA</affiliation>
<address confidence="0.921981">4300 Fair Lakes Fairfax, VA</address>
<email confidence="0.969457">dmitryzelenko@sra.com</email>
<email confidence="0.969457">chinatsuaone@sra.com</email>
<email confidence="0.969457">jasontibbetts@sra.com</email>
<abstract confidence="0.998929181818182">We compare several approaches to coreference resolution in the context of information extraction. We present a loss-based decoding framework for coreference resolution and a greedy algorithm for approximate coreference decoding, in conjunction with Perceptron and logistic regression learning algorithms. We experimentally evaluate the presented approaches using the Automatic Content Extraction evaluation methodology, with promising results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Aone</author>
<author>S W Bennett</author>
</authors>
<title>Applying machine learning to anaphora resolution.</title>
<date>1996</date>
<booktitle>Connectionist, Statistical, and Symbolic Approaches to Learning for Natural Language Processing,</booktitle>
<volume>1040</volume>
<editor>In S. Wermter, E. Riloff, and G. Scheler, editors,</editor>
<publisher>Springer Verlag,</publisher>
<location>Berlin.</location>
<contexts>
<context position="5097" citStr="Aone and Bennett, 1996" startWordPosition="775" endWordPosition="778">n coreference and anaphora resolution dealt with pronoun coreference (Lappin and Leass, 1994; Kennedy and Boguraev, 1996). The early approaches identified a set of pronouns in a document, and, for each pronoun, sought to determine the best antecedent. Different definitions of &amp;quot;best&amp;quot; led to different carefully designed and complex rules that were sometimes based on existing discourse theories (Sidner, 1979). The area of pronoun and noun phrase coreference resolution was greatly revitalized since mid-1990s by application of learning approaches to the problem. We note, amongst many, the work of (Aone and Bennett, 1996; McCarthy and Lehnert, 1995; Ng, 2001; Ng and Cardie, 2002). A coreference example is a feature-based representation of a pair of mentions that is designed to make manifest the properties of the anaphor and its candidate antecedent that are most helpful in making the decision whether the anaphor indeed refers to the antecedent in question. A coreference example has a binary label reflecting whether the entities that constitute the example are indeed coreferent or not. Most learning-based systems for coreference resolution employed larger hand-crafted feature sets (Ng, 2001). A number of learn</context>
</contexts>
<marker>Aone, Bennett, 1996</marker>
<rawString>C. Aone and S. W. Bennett. 1996. Applying machine learning to anaphora resolution. In S. Wermter, E. Riloff, and G. Scheler, editors, Connectionist, Statistical, and Symbolic Approaches to Learning for Natural Language Processing, volume 1040 of Lecture Nodes in Artificial Intelligence. Springer Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Aone</author>
<author>M Ramos-Santacruz</author>
</authors>
<title>Rees: A large-scale relation and event extraction system.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th Applied Natural Language Processing Conference.</booktitle>
<marker>Aone, Ramos-Santacruz, 2000</marker>
<rawString>C. Aone and M. Ramos-Santacruz. 2000. Rees: A large-scale relation and event extraction system. In Proceedings of the 6th Applied Natural Language Processing Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Bansal</author>
<author>A Blum</author>
<author>S Chawla</author>
</authors>
<title>Correlation clustering.</title>
<date>2002</date>
<booktitle>In Proceedings of The 43rd Annual IEEE Symposium on Foundations of Computer Science.</booktitle>
<contexts>
<context position="7194" citStr="Bansal et al., 2002" startWordPosition="1105" endWordPosition="1108">ink-best) links the anaphor to the most probable preceding antecedent, where the probability of antecedent is taken to the confidence of the coreference classifier prediction (Ng and Cardie, 2002). We will consider both link-first and link-best decoding algorithms and compare them with the new decoding framework that we introduce in Section 4. Our decoding framework most resembles the work of (McCallum and Wellner, 2003), where a coreference model represents a conditional random field. The coreference decoding problem for the conditional random field leads to a correlation clustering problem (Bansal et al., 2002). We also reduce the coreference decoding problem to a correlation clustering problem, but use a different approximation algorithm for its solution. In the absence of training data, we note application of clustering for coreference of noun phrases (Cardie and Wagstaff, 1999). Namely, the noun phrase attributes are used to define a distance function that is used within a heuristic clustering algorithm to produce a clustering of noun phrases that aims to correspond to the coreference partition of the corresponding noun phrase entities. In addition to the work on coreference resolution within doc</context>
<context position="18847" citStr="Bansal et al., 2002" startWordPosition="3024" endWordPosition="3027">i i,j where g (c, xij, mij) = 1 (c, xij, —mij) — 1 (c, xij, mij). In particular, for the Perceptron 0-1 loss function, we see that g(cw, x, y) = 10 (cw, x, —g) — 10 (cw, x, 2J) = sgn(ywT x), and the objective function (2) becomes � sgn(wijmij) —�T,,,zj max (3) i,j For logistic regression, g(cw, x, g) 1�og (cW, x, —2J) 1�og (cW, x, 2J) T 1n 1+eyw T = 1n (eyWT X) = gwTx, and 1+e-yw x the objective function (2) becomes E wijmij —�mzj max (4) i,j The optimization problems (3) and (4) are the formulations of the unweighted and weighted versions, respectively, of the correlation clustering problem (Bansal et al., 2002). Both versions are NP-hard, so we have to resort to approximation algorithms for their solution. (Bansal et al., 2002) presents two approximation algorithms for the correlation clustering problem, and (McCallum and Wellner, 2003) use a variant of the Minimizing Disagreements algorithm in conjunction with their conditional random field coreference model. We instead introduce a simple greedy decoding algorithm presented in the following section. 4.1 Greedy Coreference Decoding Algorithm The greedy decoding algorithm incrementally optimizes the (gain) function Ei j gijmij (where gij is either wi</context>
</contexts>
<marker>Bansal, Blum, Chawla, 2002</marker>
<rawString>N. Bansal, A. Blum, and S. Chawla. 2002. Correlation clustering. In Proceedings of The 43rd Annual IEEE Symposium on Foundations of Computer Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Berkson</author>
</authors>
<title>Application of the logistic function to bio-assay.</title>
<date>1944</date>
<journal>Jour- nal of the American Statistical Association,</journal>
<volume>9</volume>
<contexts>
<context position="15769" citStr="Berkson, 1944" startWordPosition="2437" endWordPosition="2439">he algorithm processes mentions sequentially in order of their appearance in the document and establishes a coreference relation between a mention mi and the most probable candidate antecedent classified as coreferent with mi by the learned coreference classifier, among the M preceding and M following candidate antecedents. After a single pass through the document, the equivalence classes are again constructed via the transitive closure of the established coreference relations. 3.4 Machine Learning Algorithms We will use two machine learning algorithms in our experiments: logistic regression (Berkson, 1944) and the (voted) Perceptron algorithm (Freund and Schapire, 1999). We introduce the following notation. Let x E X denote a (coreference) example, where X C_ RN is an example space. Let y E Y = {—1,1} be an example label, where —1 (+1) corresponds to a negative (positive) coreference example. We term a pair (x, y) a labeled example. Let S = {(xl, y&apos;), ... , (xs, ys)} be a (training) sample of labeled examples. A learning algorithm A uses the sample S to produce a classifier c : X —� Y . Both Perceptron and logistic regression learn linear classifiers&apos; in RN, that is, cu ,(x) = sgn(wTx). Percept</context>
</contexts>
<marker>Berkson, 1944</marker>
<rawString>J. Berkson. 1944. Application of the logistic function to bio-assay. Jour- nal of the American Statistical Association, (9).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cardie</author>
<author>K Wagstaff</author>
</authors>
<title>Noun phrase coreference as clustering.</title>
<date>1999</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Very Large Corpora.</booktitle>
<contexts>
<context position="7469" citStr="Cardie and Wagstaff, 1999" startWordPosition="1149" endWordPosition="1152">compare them with the new decoding framework that we introduce in Section 4. Our decoding framework most resembles the work of (McCallum and Wellner, 2003), where a coreference model represents a conditional random field. The coreference decoding problem for the conditional random field leads to a correlation clustering problem (Bansal et al., 2002). We also reduce the coreference decoding problem to a correlation clustering problem, but use a different approximation algorithm for its solution. In the absence of training data, we note application of clustering for coreference of noun phrases (Cardie and Wagstaff, 1999). Namely, the noun phrase attributes are used to define a distance function that is used within a heuristic clustering algorithm to produce a clustering of noun phrases that aims to correspond to the coreference partition of the corresponding noun phrase entities. In addition to the work on coreference resolution within documents, there is an emerging body on a more general identity uncertainty problem, which is concerned with determining whether two records describe the same entity (Pasula et al., 2003). 3 Coreference Resolution Architecture We consider the following components in adaptive ap</context>
</contexts>
<marker>Cardie, Wagstaff, 1999</marker>
<rawString>C. Cardie and K. Wagstaff. 1999. Noun phrase coreference as clustering. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Charikar</author>
<author>V Guruswami</author>
<author>A Wirth</author>
</authors>
<title>Clustering with qualitative information.</title>
<date>2003</date>
<booktitle>In Proceedings of the 44th Annual IEEE Symposium on Foundations of Computer Science,</booktitle>
<pages>524</pages>
<publisher>IEEE Computer Society.</publisher>
<marker>Charikar, Guruswami, Wirth, 2003</marker>
<rawString>M. Charikar, V. Guruswami, and A. Wirth. 2003. Clustering with qualitative information. In Proceedings of the 44th Annual IEEE Symposium on Foundations of Computer Science, page 524. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>N Ge</author>
<author>J Hale</author>
</authors>
<title>A statistical approach to anaphora resolution.</title>
<date>1998</date>
<booktitle>In Proceedings of the Sixth Workshop on Very Large Corpora.</booktitle>
<contexts>
<context position="6049" citStr="Charniak et al., 1998" startWordPosition="924" endWordPosition="927">in question. A coreference example has a binary label reflecting whether the entities that constitute the example are indeed coreferent or not. Most learning-based systems for coreference resolution employed larger hand-crafted feature sets (Ng, 2001). A number of learning algorithms have been experimentally evaluated on the coreference resolution problem. Many published studies employed a decision tree algorithm (Aone and Bennett, 1996; Ng, 2001; Ng and Cardie, 2002). We also note a few global probabilistic modeling approaches to coreference resolution: the generative probabilistic model of (Charniak et al., 1998) and the conditional random filed model of (McCallum and Wellner, 2003). The coreference classifiers tha are output by learning algorithms need to be used in conjunction with coreference decoding algorithms in order to induce the coref equivalence relation on the set of mentions. A most popular coreference decoding algorithm links an anaphor to the first preceding antecedent predicted as coreferent with the anaphor (Ng, 2001). We will call it the link-first decoding algorithm. An alternative decoding algorithm (termed link-best) links the anaphor to the most probable preceding antecedent, wher</context>
</contexts>
<marker>Charniak, Ge, Hale, 1998</marker>
<rawString>E. Charniak, N. Ge, and J. Hale. 1998. A statistical approach to anaphora resolution. In Proceedings of the Sixth Workshop on Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Demaine</author>
<author>N Immorlica</author>
</authors>
<title>Correlation clustering with partial information.</title>
<date>2003</date>
<booktitle>In Proceedings of the 6th International Workshop on Approximation Algorithms for Combinatorial Optimization Problems.</booktitle>
<marker>Demaine, Immorlica, 2003</marker>
<rawString>E. Demaine and N. Immorlica. 2003. Correlation clustering with partial information. In Proceedings of the 6th International Workshop on Approximation Algorithms for Combinatorial Optimization Problems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Dougherty</author>
<author>R Kohavi</author>
<author>M Sahami</author>
</authors>
<title>Supervised and unsupervised discretization of continuous features.</title>
<date>1995</date>
<booktitle>In Proc. 12th International Conference on Machine Learning.</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="10032" citStr="Dougherty et al., 1995" startWordPosition="1553" endWordPosition="1556"> X and Y are the corresponding values of the attributes in the given anaphor and the antecedent. • For every common attribute A of an anaphor and an antecedent, the value of the proposition, anaphor.A = antecedent.A that reflects the same attributes have the same value in both the given anaphor and the antecedent. • For nominal and pronominal anaphors, we used distance features (number of words/sentences/paragraphs between an anaphor and an antecedent). The distance features are discretized using the entropybased discretization algorithm with the minimal description length stopping criterion (Dougherty et al., 1995). • For nominal anaphors, we used the &amp;quot;appositive&amp;quot; feature, if the anaphor is in apposition with the antecedent. • For named anaphors, we used the substring feature (indicating whether one name is a substring of the other) and the common acronym/alias feature (indicating whether one name is a common acronym/alias of the other). We note that the extracted mentions are typed, and we will consider only an anaphor/antecedent pair, where the anaphor and the antecedent belong to the same type (e.g., we will not try to corefer a person with an organization). We also incorporate additional knowledge i</context>
</contexts>
<marker>Dougherty, Kohavi, Sahami, 1995</marker>
<rawString>J. Dougherty, R. Kohavi, and M. Sahami. 1995. Supervised and unsupervised discretization of continuous features. In Proc. 12th International Conference on Machine Learning. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="15834" citStr="Freund and Schapire, 1999" startWordPosition="2446" endWordPosition="2449"> of their appearance in the document and establishes a coreference relation between a mention mi and the most probable candidate antecedent classified as coreferent with mi by the learned coreference classifier, among the M preceding and M following candidate antecedents. After a single pass through the document, the equivalence classes are again constructed via the transitive closure of the established coreference relations. 3.4 Machine Learning Algorithms We will use two machine learning algorithms in our experiments: logistic regression (Berkson, 1944) and the (voted) Perceptron algorithm (Freund and Schapire, 1999). We introduce the following notation. Let x E X denote a (coreference) example, where X C_ RN is an example space. Let y E Y = {—1,1} be an example label, where —1 (+1) corresponds to a negative (positive) coreference example. We term a pair (x, y) a labeled example. Let S = {(xl, y&apos;), ... , (xs, ys)} be a (training) sample of labeled examples. A learning algorithm A uses the sample S to produce a classifier c : X —� Y . Both Perceptron and logistic regression learn linear classifiers&apos; in RN, that is, cu ,(x) = sgn(wTx). Perceptron and logistic regression seek classifiers that minimize partic</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Y. Freund and R. Schapire. 1999. Large margin classification using the perceptron algorithm. Machine Learning, 37 (3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Kennedy</author>
<author>B Boguraev</author>
</authors>
<title>Anaphora for everyone: pronominal anaphora resolution without a parser.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="4596" citStr="Kennedy and Boguraev, 1996" startWordPosition="697" endWordPosition="700">. More precisely, if x or y is a pronominal entity, then we obtain a pronoun resolution problem. Otherwise, if x or y is a nominal entity, then we have a noun phrase resolution problem. Finally, if both x and y are named entities, then it is a name resolution problem. An information extraction system needs to address all three aspects of the coreference resolution problem. Yet different modeling and algorithmic choices may be appropriate for name, noun phrase, and pronoun resolution. Most early work on coreference and anaphora resolution dealt with pronoun coreference (Lappin and Leass, 1994; Kennedy and Boguraev, 1996). The early approaches identified a set of pronouns in a document, and, for each pronoun, sought to determine the best antecedent. Different definitions of &amp;quot;best&amp;quot; led to different carefully designed and complex rules that were sometimes based on existing discourse theories (Sidner, 1979). The area of pronoun and noun phrase coreference resolution was greatly revitalized since mid-1990s by application of learning approaches to the problem. We note, amongst many, the work of (Aone and Bennett, 1996; McCarthy and Lehnert, 1995; Ng, 2001; Ng and Cardie, 2002). A coreference example is a feature-ba</context>
</contexts>
<marker>Kennedy, Boguraev, 1996</marker>
<rawString>C. Kennedy and B. Boguraev. 1996. Anaphora for everyone: pronominal anaphora resolution without a parser. In Proceedings of the 16International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lappin</author>
<author>H Leass</author>
</authors>
<title>An algorithm for pronominal anaphora resolution.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>4</issue>
<contexts>
<context position="4567" citStr="Lappin and Leass, 1994" startWordPosition="692" endWordPosition="696">nds of entities involved. More precisely, if x or y is a pronominal entity, then we obtain a pronoun resolution problem. Otherwise, if x or y is a nominal entity, then we have a noun phrase resolution problem. Finally, if both x and y are named entities, then it is a name resolution problem. An information extraction system needs to address all three aspects of the coreference resolution problem. Yet different modeling and algorithmic choices may be appropriate for name, noun phrase, and pronoun resolution. Most early work on coreference and anaphora resolution dealt with pronoun coreference (Lappin and Leass, 1994; Kennedy and Boguraev, 1996). The early approaches identified a set of pronouns in a document, and, for each pronoun, sought to determine the best antecedent. Different definitions of &amp;quot;best&amp;quot; led to different carefully designed and complex rules that were sometimes based on existing discourse theories (Sidner, 1979). The area of pronoun and noun phrase coreference resolution was greatly revitalized since mid-1990s by application of learning approaches to the problem. We note, amongst many, the work of (Aone and Bennett, 1996; McCarthy and Lehnert, 1995; Ng, 2001; Ng and Cardie, 2002). A corefe</context>
</contexts>
<marker>Lappin, Leass, 1994</marker>
<rawString>S. Lappin and H. Leass. 1994. An algorithm for pronominal anaphora resolution. Computational Linguistics, 20 (4).</rawString>
</citation>
<citation valid="true">
<title>Personal communication. Linguistic Data Consortium.</title>
<date>2003</date>
<marker>2003</marker>
<rawString>2003. Personal communication. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>B Wellner</author>
</authors>
<title>Toward conditional models of identity uncertainty with application to proper noun coreference.</title>
<date>2003</date>
<booktitle>In Proceedings of the IJCAI Workshop on Information Integration on the Web.</booktitle>
<contexts>
<context position="6120" citStr="McCallum and Wellner, 2003" startWordPosition="935" endWordPosition="938">hether the entities that constitute the example are indeed coreferent or not. Most learning-based systems for coreference resolution employed larger hand-crafted feature sets (Ng, 2001). A number of learning algorithms have been experimentally evaluated on the coreference resolution problem. Many published studies employed a decision tree algorithm (Aone and Bennett, 1996; Ng, 2001; Ng and Cardie, 2002). We also note a few global probabilistic modeling approaches to coreference resolution: the generative probabilistic model of (Charniak et al., 1998) and the conditional random filed model of (McCallum and Wellner, 2003). The coreference classifiers tha are output by learning algorithms need to be used in conjunction with coreference decoding algorithms in order to induce the coref equivalence relation on the set of mentions. A most popular coreference decoding algorithm links an anaphor to the first preceding antecedent predicted as coreferent with the anaphor (Ng, 2001). We will call it the link-first decoding algorithm. An alternative decoding algorithm (termed link-best) links the anaphor to the most probable preceding antecedent, where the probability of antecedent is taken to the confidence of the coref</context>
<context position="19077" citStr="McCallum and Wellner, 2003" startWordPosition="3059" endWordPosition="3062">ecomes � sgn(wijmij) —�T,,,zj max (3) i,j For logistic regression, g(cw, x, g) 1�og (cW, x, —2J) 1�og (cW, x, 2J) T 1n 1+eyw T = 1n (eyWT X) = gwTx, and 1+e-yw x the objective function (2) becomes E wijmij —�mzj max (4) i,j The optimization problems (3) and (4) are the formulations of the unweighted and weighted versions, respectively, of the correlation clustering problem (Bansal et al., 2002). Both versions are NP-hard, so we have to resort to approximation algorithms for their solution. (Bansal et al., 2002) presents two approximation algorithms for the correlation clustering problem, and (McCallum and Wellner, 2003) use a variant of the Minimizing Disagreements algorithm in conjunction with their conditional random field coreference model. We instead introduce a simple greedy decoding algorithm presented in the following section. 4.1 Greedy Coreference Decoding Algorithm The greedy decoding algorithm incrementally optimizes the (gain) function Ei j gijmij (where gij is either wij or sgn(wij)). The logistic regression version of the algorithm is presented as Algorithm 1. The algorithm initially puts each mention into a separate entity and then iteratively merges the entities, while the merge improves the </context>
<context position="24233" citStr="McCallum and Wellner, 2003" startWordPosition="3878" endWordPosition="3881">3The cost ratio of C assigns the value of C to an error made on a positive example, and the value of 1 to an error made on a negative example. References 2003. Automatic Content Extraction. http://www.nist.gov/speech/tests/ace/index.htm. 6 Discussion Our work addresses coreference resolution from the principled decoding perspective. While there has been a lot of work on using machine learning for coreference resolution, the decoding algorithms for coreference resolution were rarely studied, and usually considered separately from the underlying learning problems (with the notable exception of (McCallum and Wellner, 2003)). Our coreference decoding methodology couples the learning algorithm loss functions with the decoding objectives and reformulates the decoding problem as a correlation clustering problem with a learned distance metric. The correlation clustering problem is beginning to be widely studied, and we plan to evaluate experimentally many pertinent algorithms proposed within the theoretical computer science community (Charikar et al., 2003; Demaine and Immorlica, 2003). We presented coreference resolution in the context of information extraction and evaluated extrinsically the performance of several</context>
</contexts>
<marker>McCallum, Wellner, 2003</marker>
<rawString>A. McCallum and B. Wellner. 2003. Toward conditional models of identity uncertainty with application to proper noun coreference. In Proceedings of the IJCAI Workshop on Information Integration on the Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J McCarthy</author>
<author>W Lehnert</author>
</authors>
<title>Using decision trees for coreference resolution.</title>
<date>1995</date>
<booktitle>In Proceedings of the 1995 International Joint Conference on Artificial Intelligence.</booktitle>
<publisher>Longman.</publisher>
<contexts>
<context position="5125" citStr="McCarthy and Lehnert, 1995" startWordPosition="779" endWordPosition="782">ra resolution dealt with pronoun coreference (Lappin and Leass, 1994; Kennedy and Boguraev, 1996). The early approaches identified a set of pronouns in a document, and, for each pronoun, sought to determine the best antecedent. Different definitions of &amp;quot;best&amp;quot; led to different carefully designed and complex rules that were sometimes based on existing discourse theories (Sidner, 1979). The area of pronoun and noun phrase coreference resolution was greatly revitalized since mid-1990s by application of learning approaches to the problem. We note, amongst many, the work of (Aone and Bennett, 1996; McCarthy and Lehnert, 1995; Ng, 2001; Ng and Cardie, 2002). A coreference example is a feature-based representation of a pair of mentions that is designed to make manifest the properties of the anaphor and its candidate antecedent that are most helpful in making the decision whether the anaphor indeed refers to the antecedent in question. A coreference example has a binary label reflecting whether the entities that constitute the example are indeed coreferent or not. Most learning-based systems for coreference resolution employed larger hand-crafted feature sets (Ng, 2001). A number of learning algorithms have been exp</context>
</contexts>
<marker>McCarthy, Lehnert, 1995</marker>
<rawString>J. McCarthy and W. Lehnert. 1995. Using decision trees for coreference resolution. In Proceedings of the 1995 International Joint Conference on Artificial Intelligence. R. Mitkov. 2002. Anaphora Resolution. Longman.</rawString>
</citation>
<citation valid="true">
<date>1998</date>
<booktitle>Proceedings of the 6th Message Undertanding Conference (MUC-7).</booktitle>
<marker>1998</marker>
<rawString>1998. Proceedings of the 6th Message Undertanding Conference (MUC-7).</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
<author>C Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Anniversary Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5157" citStr="Ng and Cardie, 2002" startWordPosition="785" endWordPosition="788">erence (Lappin and Leass, 1994; Kennedy and Boguraev, 1996). The early approaches identified a set of pronouns in a document, and, for each pronoun, sought to determine the best antecedent. Different definitions of &amp;quot;best&amp;quot; led to different carefully designed and complex rules that were sometimes based on existing discourse theories (Sidner, 1979). The area of pronoun and noun phrase coreference resolution was greatly revitalized since mid-1990s by application of learning approaches to the problem. We note, amongst many, the work of (Aone and Bennett, 1996; McCarthy and Lehnert, 1995; Ng, 2001; Ng and Cardie, 2002). A coreference example is a feature-based representation of a pair of mentions that is designed to make manifest the properties of the anaphor and its candidate antecedent that are most helpful in making the decision whether the anaphor indeed refers to the antecedent in question. A coreference example has a binary label reflecting whether the entities that constitute the example are indeed coreferent or not. Most learning-based systems for coreference resolution employed larger hand-crafted feature sets (Ng, 2001). A number of learning algorithms have been experimentally evaluated on the cor</context>
<context position="6770" citStr="Ng and Cardie, 2002" startWordPosition="1038" endWordPosition="1041">tha are output by learning algorithms need to be used in conjunction with coreference decoding algorithms in order to induce the coref equivalence relation on the set of mentions. A most popular coreference decoding algorithm links an anaphor to the first preceding antecedent predicted as coreferent with the anaphor (Ng, 2001). We will call it the link-first decoding algorithm. An alternative decoding algorithm (termed link-best) links the anaphor to the most probable preceding antecedent, where the probability of antecedent is taken to the confidence of the coreference classifier prediction (Ng and Cardie, 2002). We will consider both link-first and link-best decoding algorithms and compare them with the new decoding framework that we introduce in Section 4. Our decoding framework most resembles the work of (McCallum and Wellner, 2003), where a coreference model represents a conditional random field. The coreference decoding problem for the conditional random field leads to a correlation clustering problem (Bansal et al., 2002). We also reduce the coreference decoding problem to a correlation clustering problem, but use a different approximation algorithm for its solution. In the absence of training </context>
<context position="15152" citStr="Ng and Cardie, 2002" startWordPosition="2342" endWordPosition="2345">antecedent classified as coreferent with mi by the learned coreference classifier, among the M preceding candidate antecedents. If no such preceding mention is found, the algorithm establishes a coreference relation between the mention mi and the closest following candidate antecedent classified as coreferent with mi, among the M following candidate antecedents. After a single pass through the document, the equivalence classes are constructed via the transitive closure of the established coreference relations. A popular alternative to link-first is the linkbest coreference decoding algorithm (Ng and Cardie, 2002). The algorithm processes mentions sequentially in order of their appearance in the document and establishes a coreference relation between a mention mi and the most probable candidate antecedent classified as coreferent with mi by the learned coreference classifier, among the M preceding and M following candidate antecedents. After a single pass through the document, the equivalence classes are again constructed via the transitive closure of the established coreference relations. 3.4 Machine Learning Algorithms We will use two machine learning algorithms in our experiments: logistic regressio</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>V. Ng and C. Cardie. 2002. Improving machine learning approaches to coreference resolution. In Proceedings of the 40th Anniversary Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Soon</author>
<author>D Lim</author>
<author>H Ng</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>12</volume>
<marker>Soon, Lim, Ng, 2001</marker>
<rawString>W. Soon; D. Lim; H. Ng. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Pasula</author>
<author>B Marthi</author>
<author>B Milch</author>
<author>S Russell</author>
<author>I Shpitser</author>
</authors>
<title>Identity uncertainty and citation matching.</title>
<date>2003</date>
<booktitle>In Advances in Neural Information Processing Systems /5.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="7978" citStr="Pasula et al., 2003" startWordPosition="1231" endWordPosition="1234">of training data, we note application of clustering for coreference of noun phrases (Cardie and Wagstaff, 1999). Namely, the noun phrase attributes are used to define a distance function that is used within a heuristic clustering algorithm to produce a clustering of noun phrases that aims to correspond to the coreference partition of the corresponding noun phrase entities. In addition to the work on coreference resolution within documents, there is an emerging body on a more general identity uncertainty problem, which is concerned with determining whether two records describe the same entity (Pasula et al., 2003). 3 Coreference Resolution Architecture We consider the following components in adaptive approaches to coreference resolution. • Coreference examples and their feature representation. • Coreference examples&apos; generation process. • Learning algorithms for coreference classifiers. • Decoding algorithm that combines predictions of coreference classifiers into a coherent discourse interpretation. Let us examine the components in more detail. 3.1 Coreference Examples and Classifiers We use entity mentions produced by the mention extraction system (Aone and RamosSantacruz, 2000) that augments mention</context>
</contexts>
<marker>Pasula, Marthi, Milch, Russell, Shpitser, 2003</marker>
<rawString>H. Pasula, B. Marthi, B. Milch, S. Russell, and I. Shpitser. 2003. Identity uncertainty and citation matching. In Advances in Neural Information Processing Systems /5. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sidner</author>
</authors>
<title>Toward a computational theory of definite anaphora comprehension in English.</title>
<date>1979</date>
<tech>Technical report,</tech>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4884" citStr="Sidner, 1979" startWordPosition="743" endWordPosition="744">needs to address all three aspects of the coreference resolution problem. Yet different modeling and algorithmic choices may be appropriate for name, noun phrase, and pronoun resolution. Most early work on coreference and anaphora resolution dealt with pronoun coreference (Lappin and Leass, 1994; Kennedy and Boguraev, 1996). The early approaches identified a set of pronouns in a document, and, for each pronoun, sought to determine the best antecedent. Different definitions of &amp;quot;best&amp;quot; led to different carefully designed and complex rules that were sometimes based on existing discourse theories (Sidner, 1979). The area of pronoun and noun phrase coreference resolution was greatly revitalized since mid-1990s by application of learning approaches to the problem. We note, amongst many, the work of (Aone and Bennett, 1996; McCarthy and Lehnert, 1995; Ng, 2001; Ng and Cardie, 2002). A coreference example is a feature-based representation of a pair of mentions that is designed to make manifest the properties of the anaphor and its candidate antecedent that are most helpful in making the decision whether the anaphor indeed refers to the antecedent in question. A coreference example has a binary label ref</context>
</contexts>
<marker>Sidner, 1979</marker>
<rawString>C. Sidner. 1979. Toward a computational theory of definite anaphora comprehension in English. Technical report, MIT Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>