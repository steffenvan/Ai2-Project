<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000162">
<title confidence="0.997812">
A Syllable-based Name Transliteration System
</title>
<author confidence="0.982369">
Xue Jiang 1, 2
</author>
<affiliation confidence="0.9354705">
1Institute of Software, Chinese
Academy of Science.
</affiliation>
<address confidence="0.501942">
Beijing China, 100190
</address>
<email confidence="0.921181">
jiangxue1024@yahoo.com.cn
</email>
<author confidence="0.98935">
Le Sun 1, Dakun Zhang 1
</author>
<affiliation confidence="0.881944333333333">
2School of Software Engineering,
Huazhong University of Science and
Technology. Wuhan China, 430074
</affiliation>
<email confidence="0.93672">
sunle@iscas.ac.cn
dakun04@iscas.ac.cn
</email>
<sectionHeader confidence="0.976027" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.992967529411765">
This paper describes the name entity transli-
teration system which we conducted for the
“NEWS2009 Machine Transliteration Shared
Task” (Li et al 2009). We get the translitera-
tion in Chinese from an English name with
three steps. We syllabify the English name
into a sequence of syllables by some rules,
and generate the most probable Pinyin se-
quence with the mapping model of English
syllables to Pinyin (EP model), then we con-
vert the Pinyin sequence into a Chinese cha-
racter sequence with the mapping model of
Pinyin to characters (PC model). And we get
the final Chinese character sequence. Our
system achieves an ACC of 0.498 and a
Mean F-score of 0.786 in the official evalua-
tion result.
</bodyText>
<sectionHeader confidence="0.995142" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999987409090909">
The main subject of shared task is to translate
English names (source language) to Chinese
names (target language). Firstly, we fix some
rules and syllabify the English names into a se-
quence of syllables by these rules, in the mean-
while, we convert the Chinese names into Pinyin
sequence. Secondly, we construct an EP model
referring to the method of phrase-based machine
translation. In the next, we construct a 2-gram
language model on characters and a chart reflect-
ing the using frequency of each character with
the same pronunciation, both of which constitute
the PC model converting Pinyin sequence into
character sequence. When a Pinyin is mapped to
several different characters, we can use them to
make a choice. In our experiment, we adopt the
corpus provided by NEWS2009 (Li et al 2004)
and the LDC Name Entity Lists 1 respectively to
conduct two EP models, while the NEWS2009
corpus for the PC model. The experiment indi-
cates that the larger a training corpus is, the more
precise the transliteration is.
</bodyText>
<sectionHeader confidence="0.92588" genericHeader="method">
2 Transliteration System Description
</sectionHeader>
<bodyText confidence="0.9999361">
Knowing from the definition of transliteration,
we must make the translating result maintain the
original pronunciation in source language. We
found that most English letters and letter compo-
sitions&apos; pronunciation are relatively fixed, so we
can take a syllabification on an English name,
therefore the syllable sequence can represent its
pronunciation. In Chinese, Pinyin is used to
represent a character&apos;s pronunciation. Based on
these analyses, we transliterate the English sylla-
ble sequence into a Pinyin sequence, and then
translate the Pinyin sequence into characters.
We suppose that the probability of a translitera-
tion from an English name to a Chinese name is
denoted by P(Ch|En), the probability of a transla-
tion from an English syllable sequence to a Pi-
nyin sequence is denoted by P(Py|En), and the
probability of a translation from a Pinyin se-
quence to a characters is denoted by P(Ch|Py),
then we can get the formula:
</bodyText>
<equation confidence="0.999944">
P(Ch|En) = P(Ch|Py) * P(Py|En) (1)
</equation>
<bodyText confidence="0.999791">
The character sequence in candidates having
the max value of P(Ch|En) is the best translitera-
tion(Wan and Verspoor, 1998).
</bodyText>
<subsectionHeader confidence="0.999382">
2.1 Syllabification of English Names
</subsectionHeader>
<bodyText confidence="0.9843105">
English letters can be divided into vowel letters
(VL) and consonant letters (CL). Usually, in a
</bodyText>
<listItem confidence="0.832669">
1: Chinese &lt;-&gt; English Name Entity Lists v 1.0, LDC Cata-
log No.: LDC2005T34
</listItem>
<page confidence="0.379934">
96
</page>
<note confidence="0.994632">
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 96–99,
Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999629769230769">
word, a phonetic syllable can be constructed in a
structure of CL+VL, CL+VL+CL, CL+VL+NL.
To adapt for Chinese phonetic rule, we divide the
continuous CLs into independent CLs(IC) and
divide structure of CL+VL+CL into CL+VL and
an IC. Take “Ronald” as an example, it can be
syllabified into “Ro/na/l/d”, “Ro” is CL+VL,
“nal” is CL+VL+CL, and is divided into CL+VL
and IC. „d&apos; is an independent CL(KUO et al.
2007). Of course there are some English names
more complex to be syllabified, so we define
seven rules for syllabification (JIANG et al.
2006):
</bodyText>
<listItem confidence="0.92956228">
(1) Define English letter set as O, vowel set as
V={a, e, i, o, u}, consonant set as C=O-V.
(2) Replace all “x” in a name with “ks” before
syllabification because it&apos;s always pro-
nounced as “ks”.
(3) The continuous VLs should be regarded as
one VL.
(4) There are some special cases in rule (3),
the continuous VLs like “oi”, “io”, “eo”
are pronounced as two syllables, so they
should be cut into two parts, so “Wilhoit”
will be syllabifyd into “wi/l/ho/i/t”.
(5) The continuous CLs should be cut into
several independent CLs. If the last one is
followed by some VLs, they will make up
a syllable.
(6) Some continuous CLs are pronounced as a
syllable, such as “ck”, “th”, these CLs will
not be syllabifyd and be regarded as a sin-
gle CL, “Jack” is syllabifyd into “Ja/ck”.
(7) There are some other composition with the
structure of VL+CL, such as “ing”, “er”,
“an” and so on. If it&apos;s a consonant behind
these compositions in the name, we can
syllabify it at the end of the composition,
</listItem>
<bodyText confidence="0.951345928571429">
while if it&apos;s a vowel behind them, we
should double write the last letter and syl-
labify the word between the two same let-
ters.
After syllabicating English names, we convert
corresponding Chinese names into Pinyin. There
are a few characters with multiple pronunciations
in the training data, we find them out and ensure
its pronunciation in a name manually.
We record all of these syllables got from the
training data set, if we meet a syllable out of vo-
cabulary when transliterating an English name,
we will find a similar one with the shortest edit-
distance in the vocabulary to replace that.
</bodyText>
<subsectionHeader confidence="0.9212985">
2.2 Mapping Model of English Syllables to
Pinyins
</subsectionHeader>
<bodyText confidence="0.9966355">
The EP model consists of a phrase-based ma-
chine translation model with a trigram language
model.
Given an English name f, we want to find its
Chinese translation e, which maximize the condi-
tional probability , as shown below.
</bodyText>
<equation confidence="0.986093">
e* = arg max Pr(e I f) (2)
e
</equation>
<bodyText confidence="0.990106">
Using Bayes rule, (1) can be decomposed into
a Translation Model and a Language
Model (Brown et al. 1993), which can
both be trained separately. These models are
usually regarded as features and combined with
scaling factors to form a log-linear model (Och
and Ney 2002). It can then be written as:
</bodyText>
<equation confidence="0.790350846153846">
(  |)
e f
Pr(e |f)pq
exp[ ( , )]
M
 exp [m

1 m
( &apos; , )]
e f
hm
e
&apos;
</equation>
<bodyText confidence="0.732865">
In our model, we use the following features:
</bodyText>
<listItem confidence="0.997756285714286">
• phrase translation probabilityp(eIf)
• lexical weightinglex(eIf)
• inverse phrase translation probability
• inverse lexical weightinglex(fIe)
• phrase penalty (always exp(1) = 2.718)
• word penalty (target name length)
• target language model, trigram
</listItem>
<bodyText confidence="0.9991462">
The first five features can be seen as a whole
phrase translation cost and used as one during
decoding.
In general, the translation process can be de-
scribed as follows:
</bodyText>
<listItem confidence="0.7132308">
(1). Segmenting input English syllable se-
quence f into J syllablesf,
(2). Translating each English syllable
into several Pinyinse;k
(3). Selecting the N-best words ,
</listItem>
<bodyText confidence="0.96953">
combined with reordering and Language
Model and other features
</bodyText>
<figure confidence="0.5986875">
m1 mhm e f
(3)
</figure>
<page confidence="0.562182">
97
</page>
<bodyText confidence="0.9911412">
(4). Rescoring the translation word set with
additional features to find the best one.
We use SRI toolkit to train our trigram lan-
guage model with modified Kneser-Ney smooth-
ing (Chen and Goodman 1998). In the standard
experiment, we use training data set provided by
NEWS2009 (Li et al 2004) to train this language
model, in the nonstandard one, we use that and
the LDC Name Entity Lists to train this language
model.
</bodyText>
<subsectionHeader confidence="0.9408495">
2.3 Mapping Model of Pinyins to Chinese
Characters
</subsectionHeader>
<bodyText confidence="0.99946765">
Since the Chinese characters used in people
names are limited, most of the conversions from
Pinyin to character are fixed. But some Pinyins
still have several corresponding characters, and
we should make a choice among these characters.
To solve this problem, we conduct a PC model
consisting a frequency chart which reflects the
using frequency of each character at different
positions in the names and a 2-gram language
model with absolute discounting smoothing.
A Chinese name is represented as C1C2 ...
Cn,Ci (1≤i≤n) is a Chinese character. C1 is at
the first position, we call it FW; C2 ...Cn-1 are in
the middle, we call them MW; Cn is at the last
position, we call it LW. Usually, each character
has different frequencies at these three positions.
In the training data set of NEWS2009, Pinyin
“luo” can be mapped to three characters: “罗”,
“洛”, and “萝”, each of them has different fre-
quencies at different positions.
</bodyText>
<table confidence="0.96573575">
FW MW LW
罗 0.677 0.647 0.501
洛 0.323 0.352 0.499
萝 0 0.001 0
</table>
<tableCaption confidence="0.999083">
Table 1. Different frequencies at different positions
</tableCaption>
<bodyText confidence="0.999936230769231">
From this table, we can see that at FW and
MW position, “罗” is more probable to be cho-
sen than the others, but sometimes “洛” or “萝”
is the correct one. In order to ensure characters
with lower frequency like “洛” and “萝” can be
chosen firstly in a certain context, we conduct a
2-gram language model.
If a Pinyin can be mapped to several charac-
ters, the condition probability (P(Chi|py)) indicat-
ing that how possible a character should be cho-
sen is determined by the weighted average of its
position frequency (P(Chi|pos)) and its probabili-
ty in the 2-gram language model (P(Chi|Chi-1)).
</bodyText>
<equation confidence="0.972604">
P(Chi|py) = a*P(Chi|pos)+(1-a)*P(Chi|Chi-1) (4)
0 &lt; a &lt; 1. In our experiments, we set a = 0.1.
</equation>
<subsectionHeader confidence="0.97847">
2.4 Experiments and Results
</subsectionHeader>
<bodyText confidence="0.9945462">
We carried out two experiments. The difference
between them is the training data for EP model.
The standard experiment adopts corpus provided
by NEWS2009, while the nonstandard one
adopts LDC Name Entity Lists.
</bodyText>
<table confidence="0.796643">
Corpora Name Num
LDC2005T34 572213
NEWS09_train_ench_31961 31961
</table>
<tableCaption confidence="0.999882">
Table 2. Corpora used for training the EP model
</tableCaption>
<bodyText confidence="0.998837">
Considering that an English name may be
translated to different Chinese names in different
corpora, so we established a unique PC model
with the training data set provided by
NEWS2009 to avoid the model‟s deviation
caused by different corpora.
The experimenting data is the development
data set provided by NEWS2009 (Li et al 2004),
testing script is also provided by NEWS2009.
First, we take a syllabification on testing
names. Then we use the EP model to generate 5-
best Pinyin sequences and their probabilities.
For each Pinyin sequence, the PC model gives 3-
best character sequences and their probabilities.
In the end, we sort the results by probabilities of
character sequences and corresponding Pinyin
sequences.
The evaluation results are shown below.
</bodyText>
<table confidence="0.998743428571429">
Metrics Standard Nonstandard
ACC 0.490677 0.502417
Mean F-score 0.782039 0.784203
MRR 0.606424 0.611214
MAP_ref 0.490677 0.502417
MAP_10 0.189290 0.189782
MAP_sys 0.191476 0.192129
</table>
<tableCaption confidence="0.996897">
Table 3. Evaluation results of standard and
nonstandard experiments
</tableCaption>
<bodyText confidence="0.999249">
It‟s easy to see that nonstandard test is better
than standard one on each metric. A larger cor-
pus does make a contribution to a more accurate
model.
</bodyText>
<note confidence="0.560864">
98
</note>
<bodyText confidence="0.999746">
For the official evaluation, we make two tests
on the testing data set provided by NEWS2009
(Li et al 2004). The table 4 shows respectively
the evaluation results of standard and nonstan-
dard tests given by NEWS2009.
</bodyText>
<table confidence="0.998623857142857">
Metrics Standard Nonstandard
ACC 0.498 0.500
Mean F-score 0.786 0.786
MRR 0.603 0.607
MAP_ref 0.498 0.500
MAP_10 0.187 0.189
MAP_sys 0.189 0.191
</table>
<tableCaption confidence="0.999496">
Table 4. Official evaluation results of standard and
</tableCaption>
<bodyText confidence="0.436363">
nonstandard tests
</bodyText>
<sectionHeader confidence="0.995661" genericHeader="method">
3 Conclusion
</sectionHeader>
<bodyText confidence="0.999995583333333">
We construct a name entity transliteration system
based on syllable. This system syllabifies Eng-
lish names by rules, then translates the syllables
to Pinyin and Chinese characters by statistics
model. We found that a larger corpus may im-
prove the transliteration. Besides, we can do
something else to improve that. We need to fix
more complex rules for syllabification. If we can
get the name user&apos;s gender from some features of
the name itself, then translate the male and fe-
male names on different Chinese character sets,
the results may be more precise.
</bodyText>
<sectionHeader confidence="0.995565" genericHeader="method">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.978845142857143">
This work was supported by the National
Science Foundation of China (60736044,
60773027), as well as 863 Hi-Tech Research and
Development Program of China (2006AA010108
-5, 2008AA01Z145).
We also thank Haizhou Li, Min Zhang and
Jian Su for providing the English-Chinese data.
</bodyText>
<sectionHeader confidence="0.950849" genericHeader="method">
Reference
</sectionHeader>
<reference confidence="0.998502514285714">
Franz Josef Och and Hermann Ney. 2002. “Discri-
minative Training and Maximum Entropy Models
for Statistical Machine Translation”. In Proceed-
ings of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL).
Haizhou Li, A Kumaran, Min Zhang, Vladimir Per-
vouchine, &amp;quot;Whitepaper of NEWS 2009 Machine
Transliteration Shared Task&amp;quot;. In Proceedings of
ACL-IJCNLP 2009 Named Entities Workshop
(NEWS 2009), Singapore, 2009
Haizhou Li, Min Zhang, Jian Su. 2004. “A joint
source channel model for machine transliteration”,
In Proceedings of the 42nd ACL, 2004
Jiang Long, Zhou Ming, and Chien Lee-feng. 2006.
“Named Entity Translation with Web Mining and
Transliteration”. Journal of Chinese Information
Processing, 21(1):1629--1634.
Jin-Shea Kuo, Haizhou Li, and Ying-Kuei Yang.
2007. “A Phonetic Similarity Model for Automatic
Extraction of Transliteration Pairs”. ACM Trans.
Asian Language Information Processing, 6(2), Sep-
tember 2007.
Peter F. Brown, Stephen A. Della Pietra, et al. 1993.
“The Mathematics of Statistical Machine Transla-
tion: Parameter Estimation”. Computational Lin-
guistics 19(2): 263-311.
Stanley F. Chen and Joshua Goodman. 1998. “An
empirical study of smoothing techniques for lan-
guage modeling”. Technical Report TR-10-98, Har-
vard University.
Stephen Wan and Cornelia Maria Verspoor. 1998.
“Automatic English-Chinese name transliteration
for development of multilingual resources”. In Pro-
ceedings of the 17th international conference on
Computational linguistics, 2: 1352 – 1356.
</reference>
<page confidence="0.92126">
99
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.515477">
<title confidence="0.999867">A Syllable-based Name Transliteration System</title>
<author confidence="0.997498">Jiang</author>
<affiliation confidence="0.961865">of Software, Chinese Academy of Science.</affiliation>
<address confidence="0.991013">Beijing China, 100190</address>
<email confidence="0.729065">jiangxue1024@yahoo.com.cn</email>
<author confidence="0.995608">Sun Dakun Zhang</author>
<affiliation confidence="0.998227">of Software Engineering, Huazhong University of Science and</affiliation>
<address confidence="0.877411">Technology. Wuhan China, 430074</address>
<email confidence="0.808107">dakun04@iscas.ac.cn</email>
<abstract confidence="0.998251722222222">This paper describes the name entity transliteration system which we conducted for the Machine Transliteration Shared et al 2009). We get the transliteration in Chinese from an English name with three steps. We syllabify the English name into a sequence of syllables by some rules, and generate the most probable Pinyin sequence with the mapping model of English syllables to Pinyin (EP model), then we convert the Pinyin sequence into a Chinese character sequence with the mapping model of Pinyin to characters (PC model). And we get the final Chinese character sequence. Our system achieves an ACC of 0.498 and a Mean F-score of 0.786 in the official evaluation result.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative Training and Maximum Entropy Models for Statistical Machine Translation”.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="6231" citStr="Och and Ney 2002" startWordPosition="1042" endWordPosition="1045">stance in the vocabulary to replace that. 2.2 Mapping Model of English Syllables to Pinyins The EP model consists of a phrase-based machine translation model with a trigram language model. Given an English name f, we want to find its Chinese translation e, which maximize the conditional probability , as shown below. e* = arg max Pr(e I f) (2) e Using Bayes rule, (1) can be decomposed into a Translation Model and a Language Model (Brown et al. 1993), which can both be trained separately. These models are usually regarded as features and combined with scaling factors to form a log-linear model (Och and Ney 2002). It can then be written as: ( |) e f Pr(e |f)pq exp[ ( , )] M  exp [m  1 m ( &apos; , )] e f hm e &apos; In our model, we use the following features: • phrase translation probabilityp(eIf) • lexical weightinglex(eIf) • inverse phrase translation probability • inverse lexical weightinglex(fIe) • phrase penalty (always exp(1) = 2.718) • word penalty (target name length) • target language model, trigram The first five features can be seen as a whole phrase translation cost and used as one during decoding. In general, the translation process can be described as follows: (1). Segmenting input English </context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. “Discriminative Training and Maximum Entropy Models for Statistical Machine Translation”. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
</authors>
<title>A Kumaran, Min Zhang, Vladimir Pervouchine, &amp;quot;Whitepaper of NEWS</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP 2009 Named Entities Workshop (NEWS</booktitle>
<marker>Li, 2009</marker>
<rawString>Haizhou Li, A Kumaran, Min Zhang, Vladimir Pervouchine, &amp;quot;Whitepaper of NEWS 2009 Machine Transliteration Shared Task&amp;quot;. In Proceedings of ACL-IJCNLP 2009 Named Entities Workshop (NEWS 2009), Singapore, 2009</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>Min Zhang</author>
<author>Jian Su</author>
</authors>
<title>A joint source channel model for machine transliteration”,</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd ACL,</booktitle>
<contexts>
<context position="1833" citStr="Li et al 2004" startWordPosition="292" endWordPosition="295"> sequence of syllables by these rules, in the meanwhile, we convert the Chinese names into Pinyin sequence. Secondly, we construct an EP model referring to the method of phrase-based machine translation. In the next, we construct a 2-gram language model on characters and a chart reflecting the using frequency of each character with the same pronunciation, both of which constitute the PC model converting Pinyin sequence into character sequence. When a Pinyin is mapped to several different characters, we can use them to make a choice. In our experiment, we adopt the corpus provided by NEWS2009 (Li et al 2004) and the LDC Name Entity Lists 1 respectively to conduct two EP models, while the NEWS2009 corpus for the PC model. The experiment indicates that the larger a training corpus is, the more precise the transliteration is. 2 Transliteration System Description Knowing from the definition of transliteration, we must make the translating result maintain the original pronunciation in source language. We found that most English letters and letter compositions&apos; pronunciation are relatively fixed, so we can take a syllabification on an English name, therefore the syllable sequence can represent its pron</context>
<context position="7341" citStr="Li et al 2004" startWordPosition="1236" endWordPosition="1239">ecoding. In general, the translation process can be described as follows: (1). Segmenting input English syllable sequence f into J syllablesf, (2). Translating each English syllable into several Pinyinse;k (3). Selecting the N-best words , combined with reordering and Language Model and other features m1 mhm e f (3) 97 (4). Rescoring the translation word set with additional features to find the best one. We use SRI toolkit to train our trigram language model with modified Kneser-Ney smoothing (Chen and Goodman 1998). In the standard experiment, we use training data set provided by NEWS2009 (Li et al 2004) to train this language model, in the nonstandard one, we use that and the LDC Name Entity Lists to train this language model. 2.3 Mapping Model of Pinyins to Chinese Characters Since the Chinese characters used in people names are limited, most of the conversions from Pinyin to character are fixed. But some Pinyins still have several corresponding characters, and we should make a choice among these characters. To solve this problem, we conduct a PC model consisting a frequency chart which reflects the using frequency of each character at different positions in the names and a 2-gram language </context>
<context position="9922" citStr="Li et al 2004" startWordPosition="1673" endWordPosition="1676">rence between them is the training data for EP model. The standard experiment adopts corpus provided by NEWS2009, while the nonstandard one adopts LDC Name Entity Lists. Corpora Name Num LDC2005T34 572213 NEWS09_train_ench_31961 31961 Table 2. Corpora used for training the EP model Considering that an English name may be translated to different Chinese names in different corpora, so we established a unique PC model with the training data set provided by NEWS2009 to avoid the model‟s deviation caused by different corpora. The experimenting data is the development data set provided by NEWS2009 (Li et al 2004), testing script is also provided by NEWS2009. First, we take a syllabification on testing names. Then we use the EP model to generate 5- best Pinyin sequences and their probabilities. For each Pinyin sequence, the PC model gives 3- best character sequences and their probabilities. In the end, we sort the results by probabilities of character sequences and corresponding Pinyin sequences. The evaluation results are shown below. Metrics Standard Nonstandard ACC 0.490677 0.502417 Mean F-score 0.782039 0.784203 MRR 0.606424 0.611214 MAP_ref 0.490677 0.502417 MAP_10 0.189290 0.189782 MAP_sys 0.1914</context>
</contexts>
<marker>Li, Zhang, Su, 2004</marker>
<rawString>Haizhou Li, Min Zhang, Jian Su. 2004. “A joint source channel model for machine transliteration”, In Proceedings of the 42nd ACL, 2004</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiang Long</author>
<author>Zhou Ming</author>
<author>Chien Lee-feng</author>
</authors>
<title>Named Entity Translation with Web Mining and Transliteration”.</title>
<date>2006</date>
<journal>Journal of Chinese Information Processing,</journal>
<volume>21</volume>
<issue>1</issue>
<marker>Long, Ming, Lee-feng, 2006</marker>
<rawString>Jiang Long, Zhou Ming, and Chien Lee-feng. 2006. “Named Entity Translation with Web Mining and Transliteration”. Journal of Chinese Information Processing, 21(1):1629--1634.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Shea Kuo</author>
<author>Haizhou Li</author>
<author>Ying-Kuei Yang</author>
</authors>
<title>A Phonetic Similarity Model for Automatic Extraction of Transliteration Pairs”.</title>
<date>2007</date>
<journal>ACM Trans. Asian Language Information Processing,</journal>
<volume>6</volume>
<issue>2</issue>
<marker>Kuo, Li, Yang, 2007</marker>
<rawString>Jin-Shea Kuo, Haizhou Li, and Ying-Kuei Yang. 2007. “A Phonetic Similarity Model for Automatic Extraction of Transliteration Pairs”. ACM Trans. Asian Language Information Processing, 6(2), September 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
</authors>
<date>1993</date>
<journal>The Mathematics of Statistical Machine Translation: Parameter Estimation”. Computational Linguistics</journal>
<volume>19</volume>
<issue>2</issue>
<pages>263--311</pages>
<marker>Brown, Pietra, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, et al. 1993. “The Mathematics of Statistical Machine Translation: Parameter Estimation”. Computational Linguistics 19(2): 263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling”.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Harvard University.</institution>
<contexts>
<context position="7250" citStr="Chen and Goodman 1998" startWordPosition="1220" endWordPosition="1223">ram The first five features can be seen as a whole phrase translation cost and used as one during decoding. In general, the translation process can be described as follows: (1). Segmenting input English syllable sequence f into J syllablesf, (2). Translating each English syllable into several Pinyinse;k (3). Selecting the N-best words , combined with reordering and Language Model and other features m1 mhm e f (3) 97 (4). Rescoring the translation word set with additional features to find the best one. We use SRI toolkit to train our trigram language model with modified Kneser-Ney smoothing (Chen and Goodman 1998). In the standard experiment, we use training data set provided by NEWS2009 (Li et al 2004) to train this language model, in the nonstandard one, we use that and the LDC Name Entity Lists to train this language model. 2.3 Mapping Model of Pinyins to Chinese Characters Since the Chinese characters used in people names are limited, most of the conversions from Pinyin to character are fixed. But some Pinyins still have several corresponding characters, and we should make a choice among these characters. To solve this problem, we conduct a PC model consisting a frequency chart which reflects the u</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1998. “An empirical study of smoothing techniques for language modeling”. Technical Report TR-10-98, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Wan</author>
<author>Cornelia Maria Verspoor</author>
</authors>
<title>Automatic English-Chinese name transliteration for development of multilingual resources”.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th international conference on Computational linguistics,</booktitle>
<volume>2</volume>
<pages>1352--1356</pages>
<contexts>
<context position="3178" citStr="Wan and Verspoor, 1998" startWordPosition="507" endWordPosition="511">the English syllable sequence into a Pinyin sequence, and then translate the Pinyin sequence into characters. We suppose that the probability of a transliteration from an English name to a Chinese name is denoted by P(Ch|En), the probability of a translation from an English syllable sequence to a Pinyin sequence is denoted by P(Py|En), and the probability of a translation from a Pinyin sequence to a characters is denoted by P(Ch|Py), then we can get the formula: P(Ch|En) = P(Ch|Py) * P(Py|En) (1) The character sequence in candidates having the max value of P(Ch|En) is the best transliteration(Wan and Verspoor, 1998). 2.1 Syllabification of English Names English letters can be divided into vowel letters (VL) and consonant letters (CL). Usually, in a 1: Chinese &lt;-&gt; English Name Entity Lists v 1.0, LDC Catalog No.: LDC2005T34 96 Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 96–99, Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP word, a phonetic syllable can be constructed in a structure of CL+VL, CL+VL+CL, CL+VL+NL. To adapt for Chinese phonetic rule, we divide the continuous CLs into independent CLs(IC) and divide structure of CL+VL+CL into CL+VL and an IC. Take “Ronald” as</context>
</contexts>
<marker>Wan, Verspoor, 1998</marker>
<rawString>Stephen Wan and Cornelia Maria Verspoor. 1998. “Automatic English-Chinese name transliteration for development of multilingual resources”. In Proceedings of the 17th international conference on Computational linguistics, 2: 1352 – 1356.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>