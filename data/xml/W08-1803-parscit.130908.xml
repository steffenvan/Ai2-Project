<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000281">
<title confidence="0.989432">
Simple is Best: Experiments with Different Document Segmentation
Strategies for Passage Retrieval
</title>
<author confidence="0.997525">
J¨org Tiedemann Jori Mur
</author>
<affiliation confidence="0.9996635">
Information Science Information Science
University of Groningen University of Groningen
</affiliation>
<email confidence="0.997606">
j.tiedemann@rug.nl j.mur@rug.nl
</email>
<sectionHeader confidence="0.993852" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999949272727273">
Passage retrieval is used in QA to fil-
ter large document collections in order
to find text units relevant for answering
given questions. In our QA system we ap-
ply standard IR techniques and index-time
passaging in the retrieval component. In
this paper we investigate several ways of
dividing documents into passages. In par-
ticular we look at semantically motivated
approaches (using coreference chains and
discourse clues) compared with simple
window-based techniques. We evaluate
retrieval performance and the overall QA
performance in order to study the impact
of the different segmentation approaches.
From our experiments we can conclude
that the simple techniques using fixed-
sized windows clearly outperform the se-
mantically motivated approaches, which
indicates that uniformity in size seems to
be more important than semantic coher-
ence in our setup.
</bodyText>
<sectionHeader confidence="0.999127" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999669888888889">
Passage retrieval in question answering is differ-
ent from information retrieval in general. Extract-
ing relevant passages from large document col-
lections is only one step in answering a natural
language question. There are two main differ-
ences: i) Passage retrieval queries are generated
from complete sentences (questions) compared to
bag-of-keyword queries usually used in IR. ii) Re-
trieved passages have to be processed further in or-
</bodyText>
<note confidence="0.723187">
© 2008. Licensed under the Creative Commons
</note>
<footnote confidence="0.967225666666667">
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
</footnote>
<bodyText confidence="0.999683333333333">
der to extract concrete answers to the given ques-
tion. Hence, the size of the passages retrieved is
important and smaller units are preferred. Here,
the division of documents into passages is crucial.
The textual units have to be big enough to en-
sure IR works properly and they have to be small
enough to enable efficient and accurate QA. In this
study we investigate whether semantically moti-
vated passages in the retrieval component lead to
better QA performance compared to the use of
document retrieval and window-based segmenta-
tion approaches.
</bodyText>
<subsectionHeader confidence="0.98925">
1.1 Index-time versus Search-time Passaging
</subsectionHeader>
<bodyText confidence="0.999963666666667">
In this paper, we experiment with various possi-
bilities of dividing documents into passages before
indexing them. This is also called index-time pas-
saging and refers to a one-step process of retriev-
ing appropriate textual units for subsequent an-
swer extraction modules (Roberts and Gaizauskas,
2004; Greenwood, 2004). This is in contrast to
other strategies using a two-step procedure consist-
ing of document retrieval and search-time passag-
ing thereafter. Here, we can distinguish between
approaches that only return one passage per rel-
evant document (see, for example, (Robertson et
al., 1992)) and the ones that allow multiple pas-
sages per document (see, for example (Moldovan
et al., 2000)). In general, allowing multiple pas-
sages per document is preferable for QA as possi-
ble answers can be contained at various positions
in a document (Roberts and Gaizauskas, 2004).
For this, an index-time approach has the advan-
tage that the retrieval of multiple passages per doc-
uments is straightforward because all of them com-
pete which each other in the same index using the
same metric for ranking.
A comparison between index-time and search-
</bodyText>
<page confidence="0.991895">
17
</page>
<note confidence="0.9945365">
Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 17–25
Manchester, UK. August 2008
</note>
<bodyText confidence="0.999916444444444">
time passaging has been carried out in (Roberts
and Gaizauskas, 2004). In their experiments,
index-time passaging performs similarly to search-
time passaging in terms of coverage and redun-
dancy (measures which have been introduced in
the same paper; see section 4.2 for more informa-
tion). Significant differences between the various
approaches can only be observed in redundancy on
higher ranks (above 50). However, as we will see
later in our experiments (section 4.2), redundancy
is not as important as coverage for our QA system
. Furthermore, retrieving more than about 40 pas-
sages does not produce significant improvements
of the QA system anymore but slows down the pro-
cessing time substantially.
Another argument for our focus on a one-step
retrieval procedure can be taken from (Tellex et al.,
2003). In this paper, the authors do not actually use
any index-time passaging approach but compare
various search-time passage retrieval algorithms.
However, they obtain a huge performance differ-
ence when applying an oracle document retriever
(only returning relevant documents in the first re-
trieval step) instead of a standard IR engine. Com-
pared to this, the differences between the various
passage retrieval approaches tested is very small.
From this we can conclude that much improve-
ment can be gained by improving the initial re-
trieval step, which seems to be the bottleneck in the
entire process. Unfortunately, the authors do not
compare their results with index-time approaches.
However, looking at the potential gain in document
retrieval and keeping in mind that the performance
of index-time and search-time approaches is rather
similar (as we have discussed earlier) we believe
that the index-time approach is preferable.
</bodyText>
<subsectionHeader confidence="0.985841">
1.2 Passages in IR
</subsectionHeader>
<bodyText confidence="0.999944546875">
Certainly, IR performance is effected by chang-
ing the size of the units to be indexed. The task
in document segmentation for our index-time pas-
saging approach is to find the proper division of
documents into text passages which optimize the
retrieval in terms of overall QA performance.
The general advantages of passage retrieval over
full-text document retrieval has been investigated
in various studies, e.g., (Kaszkiel and Zobel, 2001;
Callan, 1994; Hearst and Plaunt, 1993; Kaszkiel
and Zobel, 1997). Besides the argument of de-
creasing the search space for subsequent answer
extraction modules in QA, passage retrieval also
improves standard IR techniques by “normaliz-
ing” textual units in terms of size which is espe-
cially important in cases where documents come
from very diverse sources. IR is based on similar-
ity measures between documents and queries and
standard approaches have shortcomings when ap-
plying them to documents of various sizes and text
types. Often there is a bias for certain types raising
problems of discrimination between documents of
different lengths and content densities. Passages
on the other hand provide convenient units to be
returned to the user avoiding such ranking difficul-
ties (Kaszkiel and Zobel, 2001). For IR, passage-
level evidence may be incorporated into document
retrieval (Callan, 1994; Hearst and Plaunt, 1993)
or passages may be used directly as retrieval unit
(Kaszkiel and Zobel, 2001; Kaszkiel and Zobel,
1997). For QA only the latter is interesting and
will be applied in our experiments.
Passages can be defined in various ways. The
most obvious way is to use existing markup (ex-
plicit discourse information) to divide documents
into smaller units. Unfortunately, such markup is
not always available or ambiguous with other types
of separators. For example, headers, list elements
or table cells might be separated in the same way
(for example using an empty line) as discourse
related paragraphs. Also, the division into para-
graphs may differ a lot depending on the source
of the document. For example, Wikipedia entries
are divided on various levels into rather small units
whereas newspaper articles often include very long
paragraphs.
There are several ways of automatically divid-
ing documents into passages without relying on
existing markup. One way is to search for linguis-
tic clues that indicate a separation of consecutive
text blocks. These clues may include lexical pat-
terns and relations. We refer to such approaches
as semantically motivated document segmentation.
Another approach is to cut documents into arbi-
trary pieces ignoring any other type of informa-
tion. For example, we can use fixed-sized win-
dows to divide documents into passages of simi-
lar size. Such windows can be defined in terms of
words and characters (Kaszkiel and Zobel, 2001;
Monz, 2003) or sentences and paragraphs (Zobel
et al., 1995; Llopis et al., 2002). It is also possi-
ble to allow varying window sizes and overlapping
sections to be indexed (Kaszkiel and Zobel, 2001;
Monz, 2003). In this case it is up to the IR engine
</bodyText>
<page confidence="0.998492">
18
</page>
<bodyText confidence="0.999764125">
to decide which of the competing window types is
preferred and it may even return overlapping sec-
tions multiple times.
In the following sections we will discuss
two techniques of semantically motivated docu-
ment segmentation and compare them to simple
window-based techniques in terms of passage re-
trieval and QA performance.
</bodyText>
<sectionHeader confidence="0.516503" genericHeader="method">
2 Passage Retrieval in our QA system
</sectionHeader>
<bodyText confidence="0.999498310344828">
Our QA system is an open-domain question an-
swering system for Dutch. It includes two
strategies: (1) A table-lookup strategy using fact
databases that have been created off-line, and, (2)
an “on-line” answer extraction strategy with pas-
sage retrieval and subsequent answer identification
and ranking modules. We will only look at the
second strategy as we are interested in the passage
retrieval component and its impact on QA perfor-
mance.
The passage retrieval component is imple-
mented as an interface to several open-source IR
engines. The query is generated from the given
natural language question after question analysis.
Keywords are sent to the IR engine(s) and results
(in form of sentence IDs) are returned to the QA
system.
In the experiments described here, we apply
Zettair (Lester et al., 2006), an open-source IR en-
gine developed by the search engine group at the
RMIT University in Melbourne, Australia. It im-
plements a very efficient standard IR engine with
high retrieval performance according to our exper-
iments with various alternative systems. Zettair is
optimized for speed and is very efficient in both
indexing and retrieval. The outstanding speed in
indexing is very fortunate for our experiments in
which we had to create various indexes with dif-
ferent document segmentation strategies.
</bodyText>
<sectionHeader confidence="0.981484" genericHeader="method">
3 Document Segmentation
</sectionHeader>
<bodyText confidence="0.9999895">
We now discuss the different methods for docu-
ment segmentation, starting with the semantically
motivated ones and then looking at the window-
based techniques.
</bodyText>
<subsectionHeader confidence="0.999776">
3.1 Using Coreference Chains
</subsectionHeader>
<bodyText confidence="0.998109866666667">
Coreference is the relation which holds between
two NPs both of which are interpreted as refer-
ring to the same unique referent in the context
in which they occur ((Van Deemter and Kibble,
2000)). Since the coreference relation is an equiv-
alence relation and consequently a transitive rela-
tion chains of coreferring entities can be detected
in arbitrary documents. We can use these coref-
erence chains to demarcate passages in the text.
The assumption in this approach is that corefer-
ence chains mark semantically coherent passages,
which are good candidates for splitting up docu-
ments.
Figure 1 illustrates chains detected by a resolu-
tion system in five successive sentences.
</bodyText>
<listItem confidence="0.9739718">
1. [Jim McClements en Susan Sandvig-Shobe]i hebben
een onrechtmatig argument gebruikt.
2. [De Nederlandse scheidsrechter]j [Jacques de Koning]j
bevestigt dit.
3. [Kuipers]k versloeg zondag in een rechtstreeks duel
[Shani Davis]..
4. Toch werd [hij]k in de rangschikking achter [de
Amerikaan]. geklasseerd.
5. [De twee hoofdarbiters]i verklaarden dat [Kuipers’]k
voorste schaats niet op de grond stond.
</listItem>
<figure confidence="0.700395833333333">
Cluster i (1,5): [Jim McClements en Susan Sandvig-Shobe]
[De twee hoofdarbiters]
Cluster j (2): [De Nederlandse scheidsrechter]
[Jacques de Koning]
Cluster k (3-5): [Kuipers] [hij] [Kuipers’]
Cluster m (3,4): [Shani Davis] [de Amerikaan]
</figure>
<figureCaption confidence="0.986181">
Figure 1: Example of coreference chains used for
document segmentation
</figureCaption>
<bodyText confidence="0.999776444444445">
The coreferential units can then be used to form
passages consisting of all sentences the corefer-
ence chain spans over, i.e. the boundaries of pas-
sages are sentences containing the first occurrence
of the referent and the last occurrence of a refer-
ent. Thus, in the example in figure 1 we obtain
four passages: 1) sentence one to sentence five, 2)
sentence two, 3) sentence three to five, and, 4) sen-
tence three and four. Note that such passages can
be included in others and may overlap with yet oth-
ers. Furthermore, there might be sentences which
are not included in any chain which have to be han-
dled by some other techniques.
For our purposes we used our own coreference
resolution system which is based on information
derived from Alpino, a wide-coverage dependency
parser for Dutch (van Noord, 2006). We ap-
proached the task of coreference resolution as a
</bodyText>
<page confidence="0.99541">
19
</page>
<bodyText confidence="0.999939529411765">
clustering-based ranking task. Some NP pairs are
more likely to be coreferent than others. The sys-
tem ranks possible antecedents for each anaphor
considering syntactic features, semantic features
and surface structure features from the anaphor
and the candidate itself, as well as features from
the cluster to which the candidate belongs. It picks
the most likely candidate as the coreferring an-
tecedent.
References relations are detected between pro-
nouns, common nouns and named entities. The
resolution system yields a precision of 67.9% and
a recall of 45.6% (F-score = 54.5%) using MUC
scores (Vilain et al., 1993) on the annotated test
corpus developed by (Hoste, 2005) which consist
of articles taken from KNACK, a Flemish weekly
news magazine.
</bodyText>
<subsectionHeader confidence="0.998549">
3.2 TextTiling
</subsectionHeader>
<bodyText confidence="0.999984517241379">
TextTiling is a well-known algorithm for segment-
ing texts into subtopic passages (Hearst, 1997).
It is based on the assumption that a significant
portion of a set of lexical items in use during
the course of a given subtopic discussion changes
when that subtopic in the text changes.
Topic shifts are found by searching for lexi-
cal co-occurrence patterns and comparing adja-
cent blocks. First the text is subdivided into
pseudo-sentences of a predefined size rather than
using syntactically-determined sentences. These
pseudo-sentences are called token-sequences by
Hearst.
The algorithm identifies discourse boundaries
by calculating a score for each token-sequence
gap. This score is based on two methods,
block comparison and vocabulary introduction.
The block comparison method compares adjacent
blocks of text to see how similar they are accord-
ing to how many words the adjacent blocks have
in common. The vocabulary introduction method
is based on how many new words were seen in
the interval in which the token-sequence gap is the
midpoint.
The boundaries are assumed to occur at the
largest valleys in the graph that results from plot-
ting the token-sequences against their scores. In
this way the algorithm produces a flat subtopic
structure from a given document.
</bodyText>
<subsectionHeader confidence="0.992873">
3.3 Window-based
</subsectionHeader>
<bodyText confidence="0.999973173913044">
The simplest way of dividing documents into pas-
sages is to use a fixed-sized window. Here we
do not take any discourse information nor seman-
tic clue into account but split documents at arbi-
trary positions. Windows can be defined in various
ways, in terms of characters, words or sentences.
In our case it is important to keep sentences to-
gether because of the answer extraction compo-
nent in our QA system that works on that level
and expects complete sentences. Window-based
segmentation techniques may be applied with var-
ious amounts of overlaps. The simplest method is
to split documents into passages in a greedy way,
starting a new passage immediately after the pre-
vious one (and starting the entire process at the be-
ginning of each document)1. Another method is to
allow some overlap between consecutive passages,
i.e. starting a new passage at some position within
the previous one. If we use the maximum possible
overlap such an approach is usually called a “slid-
ing window” in which the difference between two
consecutive passages is only two basic units (sen-
tences) - the first and the last one.
</bodyText>
<sectionHeader confidence="0.999595" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.935696">
4.1 Setup
</subsectionHeader>
<bodyText confidence="0.999983136363636">
For our experiments we applied the Dutch news-
paper corpus used at the QA track at CLEF, the
cross-language evaluation forum. It contains about
190,000 documents consisting of about 4,000,000
sentences (roughly 80 million words). As men-
tioned earlier, we applied the open-source IR en-
gine, Zettair, in our experiments and used a lan-
guage modeling metric with Dirichlet smoothing,
which is implemented in the system.
The evaluation is based on 778 Dutch CLEF
questions from the QA tracks in the years 2003 –
2005 which are annotated with their answers. We
use simple matching of possible answer strings to
determine if a passage is relevant for finding an
accepted answer or not. Similarly, answer string
matching is applied to evaluate the output of the
entire QA system; i.e. an answer by the system
is counted as correct if it is identical to one of the
accepted answer strings without looking at the sup-
porting sentence/passage. For evaluation we used
the standard measure of MRR which is defined as
follows:
</bodyText>
<footnote confidence="0.9436906">
1Note that in our approach we still keep the document
boundaries intact, i.e. the segmentation ends at the end of
each document and starts from scratch at the beginning of the
next document. In this way, the last passage in a document
may be smaller than the pre-defined fixed size.
</footnote>
<page confidence="0.897471">
20
</page>
<equation confidence="0.78098">
1
rank(first correct answer)
coverage/MRR (in %)
</equation>
<bodyText confidence="0.999840666666667">
Using the string matching strategy for evalu-
ation this corresponds to the lenient MRR mea-
sures frequently used in the literature. Strict MRR
scores (requiring a match with supporting docu-
ments) is less appropriate for our data coming from
the CLEF QA tracks. In CLEF there are usually
only a few participants and, therefore, only a small
fraction of relevant documents are known for the
given questions.
</bodyText>
<subsectionHeader confidence="0.996522">
4.2 Evaluation of Passage Retrieval
</subsectionHeader>
<bodyText confidence="0.999987028571428">
There are various metrics that can be employed for
evaluating passage retrieval. Commonly it is ar-
gued that passage retrieval for QA is merely a fil-
tering task and ranking (precision) is less impor-
tant than recall. Therefore, the measure of redun-
dancy has been introduced which is defined as the
average number of relevant passages retrieved per
question (independent of any ranking). Passage re-
trieval is, of course, a bottleneck in QA systems
that make use of such a component. The system
has no chance to find an answer if the retrieval en-
gine fails to return relevant passages. Therefore,
another measure, coverage is often used in combi-
nation with redundancy. It is defined as the pro-
portion of questions for which at least one relevant
passage is found. In order to validate the use of
these measures in our setup we experimented with
retrieving various amounts of paragraphs. Figure 2
illustrates the relation of coverage and redundancy
scores compared to the overall QA performance
measured in terms of MRR scores.
From the figure we can conclude that cover-
age is more important than redundancy in our sys-
tem. In other words, our QA system is quite good
in finding appropriate answers if there is at least
one relevant passage in the set of retrieved ones.
Redundancy on the other hand does not seem to
provide valuable insides for the end-to-end perfor-
mance of our QA system.
However, our system also uses the passage re-
trieval score (and, hence, the ranking) as a clue
for answer extraction. Therefore, other standard
IR measures might be interesting for our investi-
gations as well. The following three metrics are
common in the IR literature.
</bodyText>
<figure confidence="0.9950105">
0 20 40 60 80 100
number of paragraphs retrieved
</figure>
<figureCaption confidence="0.999413">
Figure 2: The correlation between coverage and
</figureCaption>
<bodyText confidence="0.9567508">
redundancy and MRRQA with varying numbers
of paragraphs retrieved. Note that redundancy and
coverage use different scales on the y-axis which
makes them not directly comparable. The inten-
tion of this plot is to illustrate the tendency of both
measures in comparison with QA performance.
Mean average precision (MAP): Average of
precision scores for top k documents; MAP
is the mean of these averages over all the N
queries.
</bodyText>
<equation confidence="0.9785155">
1
MAP = N
</equation>
<bodyText confidence="0.984624">
(Pn(1..k) is the precision of the top k docu-
ments retrieved for query qn)
Uninterpolated average precision (UAP):
Average of precision scores at each relevant
document retrieved; UAP is the mean of
these averages over the N queries.
</bodyText>
<equation confidence="0.998465">
1 X Pn(1..k)
|Dnr  |k:dk∈Dnr
</equation>
<bodyText confidence="0.9965556">
(Dnr is the set of relevant documents among
the ones retrieved for question n)
Mean reciprocal ranks: The mean of the recip-
rocal rank of the first relevant passage re-
trieved.
</bodyText>
<equation confidence="0.3870905">
1
rank(first relevant passage)
</equation>
<bodyText confidence="0.683465">
In figure 3 the correlation of these measures with
the overall QA performance is illustrated.
</bodyText>
<figure confidence="0.9934928">
100
40
20
80
60
0
IR coverage
QA MRR IR redundancy
2.5
7.5
redundancy
5
10
XN
1
</figure>
<equation confidence="0.665660333333333">
MRRQA = N
1
N
X
n=1
1 K
X
k=1
K
Pn(1..k)
1
UAP = N
N
X
n=1
MRRIR = N
1
XN
</equation>
<page confidence="0.801433">
1
21
</page>
<figure confidence="0.9973665">
0 20 40 60 80 100
number of paragraphs retrieved
</figure>
<figureCaption confidence="0.9102675">
Figure 3: The correlation between IR evaluation
measures (MAP, UAP and MRRIR) and QA
evaluation scores (MRRQA) with varying num-
bers of paragraphs retrieved.
</figureCaption>
<bodyText confidence="0.999105846153846">
From the picture we can clearly see that the
MRRIR scores correlate the most with the QA
evaluation scores when retrieving different num-
bers of paragraphs. This, again, confirms the im-
portance of coverage as the MRRIR score only
takes the first relevant passage into account and ig-
nores the fact that there might be more answers
to be found in lower ranked passages. Hence,
MRRIR seems to be a good measure that com-
bines coverage with an evaluation of the rank-
ing and, therefore, we will use it as our main IR
evaluation metric instead of coverage, redundancy,
MAP &amp; UAP.
</bodyText>
<subsectionHeader confidence="0.997272">
4.3 Baselines
</subsectionHeader>
<bodyText confidence="0.999978611111111">
The CLEF newspaper corpus comes with para-
graph markup which can easily be used as the seg-
mentation granularity for passage retrieval. Table
1 shows the scores obtained by different baseline
retrieval approaches using either sentences, para-
graphs or documents as base units.
We can see from the results that document re-
trieval (used for QA) is clearly outperformed by
both sentence and paragraph retrieval. Surpris-
ingly, sentence retrieval works even better than
paragraph retrieval when looking at the QA per-
formance even though all IR evaluation measures
(cov, red, MRRIR) suggest a lower score. Note
that MRRIR is almost as good as MRRQA for
sentence retrieval whereas the difference between
them is quite large for the other settings. This indi-
cates the importance of narrowing down the search
space for the answer extraction modules. The
</bodyText>
<table confidence="0.9264084">
MRR
#sent cov red IR QA CLEF
sent 16,737 0.784 2.95 0.490 0.487 0.430
par 80,046 0.842 4.17 0.565 0.483 0.416
doc 618,865 0.877 6.13 0.666 0.457 0.387
</table>
<tableCaption confidence="0.956313">
Table 1: Baselines with sentence (sent), paragraph
</tableCaption>
<bodyText confidence="0.998900783783784">
(par) and document (doc) retrieval (20 units).
MRRQA is measured on the top 5 answers re-
trieved. CLEF is the accuracy of the QA system
measured on the top answer provided by the sys-
tem. cov refers to coverage and red refers to redun-
dancy. #sent gives the total number of sentences
included in the retrieved text units to give an im-
pression about the amount of text to be processed
by subsequent answer extraction modules.
amount of data to be processed is much smaller
for sentence retrieval than for the other two while
coverage is still reasonably high. The CLEF scores
(accuracy measured on the top answer provided by
the system) follow the same pattern. Here, the dif-
ference between sentence retrieval and document
retrieval is even more apparent.
Certainly, the success of the retrieval compo-
nent depends on the metric used for ranking doc-
uments as implemented in the IR engine. In or-
der to verify the importance of document seg-
mentation in a QA setting we also ran experi-
ments with another standard metric implemented
in Zettair, the Okapi BM-25 metric (Robertson et
al., 1992). Similar to the previous setting using
the LM metric, QA with paragraph retrieval (now
yielding MRRQA = 0.460) outperforms QA with
document retrieval (MRRQA = 0.449). How-
ever, sentence retrieval does not perform as well
(MRRQA = 0.420) which suggests that the Okapi
metric is not suited for very small retrieval units.
Still, the success of paragraph retrieval supports
the advantage of passage retrieval compared to
document retrieval and suggests potential QA per-
formance gains with improved document segmen-
tation strategies. In the remaining we only report
results using the LM metric for retrieval due to its
superior performance.
</bodyText>
<subsectionHeader confidence="0.999483">
4.4 Semantically Motivated Passages
</subsectionHeader>
<bodyText confidence="0.9996768">
As described earlier, coreference chains can be
used to extract semantically coherent passages
from textual documents. In our experiments we
used several settings for the integration of such
passages in the retrieval engine. First of all, coref-
</bodyText>
<figure confidence="0.996059">
IR MRR
IR UAP
IR MAP QA MRR
MRR/UAP/MAP (in %) 60
55
50
45
40
35
30
25
20
15
</figure>
<page confidence="0.996523">
22
</page>
<bodyText confidence="0.999818725490196">
erence chains have been used as the only way
of forming passages. Sentences which are not
included in any passage are included as single-
sentence passages. This settings is referred to as
sent/coref.
In the second setting we restrict the passages in
length. Coreference chains can be arbitrary long
and, as we can see in the results in table 2, the
IR engine tends to prefer long passages which is
not desirable in the QA setting. Hence, we define
the constraint that passages have to be longer than
200 characters and shorter than 1000. This setup is
referred to as sent/coref (200-1000).
In the third setting we combine paragraphs (us-
ing existing markup) and coreference chain pas-
sages including the length restriction. This is
mainly to get rid of the single-sentence passages
included in the previous settings. Note that all
paragraphs are used even if all sentences within
them are included in coreferential passages. Note
also that in all settings passages may refer to
overlapping text units as coreference chains may
stretch over various overlapping passages of a doc-
ument.
We did not perform an exhaustive optimization
of the length restriction. However, we experi-
mented with various settings and 200-1000 was the
best performing one in our experiments. For illus-
tration we include one additional experiment using
a slightly different length constraint (200-400) in
table 2.
For the document segmentation strategy us-
ing TextTiling we used a freely available im-
plementation of that algorithm (the Perl Module
Lingua::EN::Segmenter::TextTiling available
at CPAN). Note that we do not include other pas-
sages in this approach (paragraphs using existing
markup nor single-sentence passages).
Table 2 summarizes the scores obtained by the
various settings when applied for passage retrieval
and when embedded into the QA system.
It is worth noting that including coreferential
chains without length restriction forced the re-
trieval engine to return a lot of very long passages
which resulted in a degraded QA performance
(also in terms of processing time which is not
shown here). The combination of paragraphs and
coreferential passages with length restrictions pro-
duced MRRQA scores above the baseline. How-
ever, these improvements are not statistically sig-
nificant according to the Wilcoxon matched-pair
</bodyText>
<table confidence="0.999433857142857">
#sent IR MRR CLEF
QA
sent/coref 490,968 0.604 0.469 0.405
sent/coref (200-1000) 76,865 0.535 0.462 0.395
par+coref (200-1000) 82,378 0.560 0.493 0.426
par+coref (200-400) 67,580 0.555 0.489 0.422
TextTiling 107,879 0.586 0 0.503 0.434
</table>
<tableCaption confidence="0.979447">
Table 2: Passage retrieval with document segmen-
</tableCaption>
<bodyText confidence="0.886186916666667">
tation using coreference chains and TextTiling (re-
trieving a maximum of 20 passages; △ means sig-
nificant with p &lt; 0.05 and Wilcoxon Matched-pair
Signed-Ranks Test compared to paragraph base-
line – only tested for MRRQA)
signed-ranks test and looking at the corresponding
CLEF scores we can even see a slight drop in per-
formance. Applying TextTiling yielded improved
scores in both passage retrieval and QA perfor-
mance (MRRQA and CLEF). The MRRQA im-
provement is statistically significant according to
the same test.
</bodyText>
<subsectionHeader confidence="0.786269">
4.5 Window-based Passages
</subsectionHeader>
<bodyText confidence="0.999644125">
In comparison to the semantically motivated pas-
sages discussed above we also looked at simple
window-based passages as described earlier. Here
we do not consider any linguistic clues for divid-
ing the documents besides the sentence and docu-
ment boundaries. Table 3 summarizes the results
obtained for various fixed-sized windows used for
document segmentation.
</bodyText>
<table confidence="0.995951545454545">
#sent IR MRR CLEF
QA
2 sentences 33468 0.545 0 0.506 0.443
3 sentences 50190 0.554 0.504 0.436
4 sentences 66800 0.581 0 0.512 0.447
5 sentences 83575 0.588 0.493 0.422
6 sentences 100110 0.583 0.489 0.423
7 sentences 116872 0.572 0.491 0.422
8 sentences 133504 0.577 0.481 0.409
9 sentences 150156 0.578 0.475 0.405
10 sentences 166810 0.596 0.470 0.396
</table>
<tableCaption confidence="0.993614">
Table 3: Passage retrieval with window-based doc-
</tableCaption>
<bodyText confidence="0.978320222222222">
ument segmentation (△ means significant with
p &lt; 0.05 and Wilcoxon Matched-pair Signed-
Ranks Test)
Surprisingly, we can see that window-based seg-
mentation approaches with small sizes between 2
and 7 sentences yield improved scores compared
to the baseline. Two of the improvements (using
2-sentence passages and 4-sentence passages) are
statistically significant. Three settings also out-
</bodyText>
<page confidence="0.992996">
23
</page>
<bodyText confidence="0.9999288">
perform the best semantically motivated segmen-
tation approach. This result was unexpected espe-
cially considering the naive way of splitting docu-
ments into parts disregarding any discourse struc-
ture (besides document boundaries) and other se-
mantic clues.
We did another experiment using window-based
segmentation and a sliding window approach.
Here, fixed-sized passages are included starting at
every point in the document and, hence, various
overlapping passages are included in the index. In
this way we split documents at various points and
leave it to the IR engine to select the most ap-
propriate ones for a given query. The results are
shown in table 4.
</bodyText>
<figure confidence="0.993112818181818">
#sent IR MRR CLEF
QA
2 sent (sliding) 29095 0.548 △ 0.516 0.456
3 sent (sliding) 36415 0.549 0.484 0.411
4 sent (sliding) 41565 0.546 0.476 0.409
5 sent (sliding) 45737 0.534 0.465 0.403
6 sent (sliding) 49091 0.528 0.454 0.390
7 sent (sliding) 51823 0.529 0.439 0.372
8 sent (sliding) 54600 0.535 0.428 0.360
9 sent (sliding) 57071 0.531 0.420 0.351
10 sent (sliding) 59352 0.542 0.420 0.354
</figure>
<tableCaption confidence="0.640688">
Table 4: Passage retrieval with window-based doc-
</tableCaption>
<bodyText confidence="0.988967727272728">
ument segmentation and a sliding window
Again, we see a significant improvement with
2-sentence passages (the overall best score so far)
but the performance degrades when increasing the
window size. Note that the number of sentences re-
trieved is growing very slowly for larger windows.
This is because more and more of the overlapping
regions are retrieved and, hence, the total number
of unique sentences does not grow with the size
of the window as we have seen in the non-sliding
approach.
</bodyText>
<sectionHeader confidence="0.997647" genericHeader="conclusions">
5 Discussion &amp; Conclusions
</sectionHeader>
<bodyText confidence="0.999970184210526">
Our experiments show that passage retrieval is in-
deed different to general document retrieval. Im-
proved retrieval scores do not necessarily lead to
better QA performance. Important for QA is to
reduce the search space for subsequent answer ex-
traction modules and, hence, passage retrieval has
to balance retrieval accuracy and retrieval size. In
our setup it seems to be preferable to return very
small units with a reasonable coverage. For this,
index-time passaging is very effective.
In this study we were especially interested in se-
mantically motivated approaches to document seg-
mentation. In particular, two techniques have been
investigated, one using the well-known TextTil-
ing algorithm and one using coreference chains for
passage boundary detection. We compared them
to simple window-based techniques using various
sizes. From our experiments we can conclude that
simple document segmentation techniques using
small fixed-sized windows work best among the
ones tested here. Semantically motivated passages
in the retrieval component helped to slightly im-
prove QA performance but do not justify the effort
spent in producing them. One of the main reasons
for the failure of using coreference chains for seg-
mentation might be the fact that this approach pro-
duces many overlapping passages which does not
seem to be favorable for passage retrieval. This can
also be seen in the sliding window approach which
did not perform as well as the one without over-
lapping units (except for two-sentence passages).
In conclusion, uniformity in terms of length and
uniqueness (in terms of non-overlapping contents)
seem to be more important than semantic coher-
ence for one-step passage retrieval in QA. A fu-
ture direction could be to test an approach that bal-
ances both a uniform document segmentation and
semantic coherence.
</bodyText>
<sectionHeader confidence="0.999258" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998585">
Callan, James P. 1994. Passage-level evidence in doc-
ument retrieval. In SIGIR ’94: Proceedings of the
17th annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 302–310, New York, NY, USA. Springer-
Verlag New York, Inc.
Greenwood, Mark A. 2004. Using pertainyms to im-
prove passage retrieval for questions requesting in-
formation about a location. In Proceedings of the
Workshop on Information Retrievalfor Question An-
swering (SIGIR 2004), Sheffield, UK.
Hearst, Marti A. and Christian Plaunt. 1993. Subtopic
structuring for full-length document access. In Re-
search and Development in Information Retrieval,
pages 59–68.
Hearst, Marti A. 1997. Texttiling: Segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33–64.
Hoste, V. 2005. Optimization Issues in Machine Learn-
ing of Coreference Resolution. Ph.D. thesis, Univer-
sity of Antwerp.
Kaszkiel, Marcin and Justin Zobel. 1997. Passage re-
trieval revisited. In SIGIR ’97: Proceedings of the
20th annual international ACM SIGIR conference on
</reference>
<page confidence="0.976151">
24
</page>
<reference confidence="0.998524846153846">
Research and development in information retrieval,
pages 178–185, New York, NY, USA. ACM Press.
Kaszkiel, Marcin and Justin Zobel. 2001. Ef-
fective ranking with arbitrary passages. Journal
of the American Society of Information Science,
52(4):344–364.
Lester, Nicholas, Hugh Williams, Justin Zobel, Falk
Scholer, Dirk Bahle, John Yiannis, Bodo von
Billerbeck, Steven Garcia, and William Web-
ber. 2006. The Zettair search engine.
http://www.seg.rmit.edu.au/zettair/.
Llopis, F., J. Vicedo, and A. Ferrindez. 2002. Pas-
sage selection to improve question answering. In
Proceedings of the COLING 2002 Workshop on Mul-
tilingual Summarization and Question Answering.
Moldovan, D., S. Harabagiu, M. Pasca, R. Mihalcea,
R. Girju, R. Goodrum, and V. Rus. 2000. The struc-
ture and performance of an open-domain question
answering system.
Monz, Christof. 2003. From Document Retrieval to
Question Answering. Ph.D. thesis, University of
Amsterdam.
Roberts, Ian and Robert Gaizauskas. 2004. Evaluating
passage retrieval approaches for question answering.
In Proceedings of 26th European Conference on In-
formation Retrieval.
Robertson, Stephen E., Steve Walker, Micheline
Hancock-Beaulieu, Aarron Gull, and Marianna Lau.
1992. Okapi at TREC-3. In Text REtrieval Confer-
ence, pages 21–30.
Tellex, S., B. Katz, J. Lin, A. Fernandes, and G. Marton.
2003. Quantitative evaluation ofpassage retrieval al-
gorithms for question answering. In Proceedings of
the SIGIR conference on Research and development
in informaion retrieval, pages 41–47. ACM Press.
Van Deemter, K. and R. Kibble. 2000. On coreferring:
Coreference in muc and related annotation schemes.
Computational Linguistics, 26(4):629–637.
van Noord, Gertjan. 2006. At Last Parsing Is Now
Operational. In TALN 2006 Verbum Ex Machina,
Actes De La 13e Conference sur Le Traitement
Automatique des Langues naturelles, pages 20–42,
Leuven.
Vilain, M., J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1993. A model-theoretic coreference
scoring scheme. In Proceedings of the 6th confer-
ence on Message understanding (MUC 6), pages 45–
52.
Zobel, Justin, Alistair Moffat, Ross Wilkinson, and Ron
Sacks-Davis. 1995. Efficient retrieval of partial doc-
uments. Information Processing and Management,
31(3):361–377.
</reference>
<page confidence="0.998734">
25
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.896424">
<title confidence="0.997384">Simple is Best: Experiments with Different Document Strategies for Passage Retrieval</title>
<author confidence="0.999312">J¨org Tiedemann Jori Mur</author>
<affiliation confidence="0.9997035">Information Science Information Science University of Groningen University of Groningen</affiliation>
<email confidence="0.956931">j.tiedemann@rug.nlj.mur@rug.nl</email>
<abstract confidence="0.997475739130435">Passage retrieval is used in QA to filter large document collections in order to find text units relevant for answering given questions. In our QA system we apply standard IR techniques and index-time passaging in the retrieval component. In this paper we investigate several ways of dividing documents into passages. In particular we look at semantically motivated approaches (using coreference chains and discourse clues) compared with simple window-based techniques. We evaluate retrieval performance and the overall QA performance in order to study the impact of the different segmentation approaches. From our experiments we can conclude that the simple techniques using fixedsized windows clearly outperform the semantically motivated approaches, which indicates that uniformity in size seems to be more important than semantic coherence in our setup.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James P Callan</author>
</authors>
<title>Passage-level evidence in document retrieval.</title>
<date>1994</date>
<booktitle>In SIGIR ’94: Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>302--310</pages>
<publisher>SpringerVerlag</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="5820" citStr="Callan, 1994" startWordPosition="890" endWordPosition="891">ndex-time and search-time approaches is rather similar (as we have discussed earlier) we believe that the index-time approach is preferable. 1.2 Passages in IR Certainly, IR performance is effected by changing the size of the units to be indexed. The task in document segmentation for our index-time passaging approach is to find the proper division of documents into text passages which optimize the retrieval in terms of overall QA performance. The general advantages of passage retrieval over full-text document retrieval has been investigated in various studies, e.g., (Kaszkiel and Zobel, 2001; Callan, 1994; Hearst and Plaunt, 1993; Kaszkiel and Zobel, 1997). Besides the argument of decreasing the search space for subsequent answer extraction modules in QA, passage retrieval also improves standard IR techniques by “normalizing” textual units in terms of size which is especially important in cases where documents come from very diverse sources. IR is based on similarity measures between documents and queries and standard approaches have shortcomings when applying them to documents of various sizes and text types. Often there is a bias for certain types raising problems of discrimination between d</context>
</contexts>
<marker>Callan, 1994</marker>
<rawString>Callan, James P. 1994. Passage-level evidence in document retrieval. In SIGIR ’94: Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval, pages 302–310, New York, NY, USA. SpringerVerlag New York, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark A Greenwood</author>
</authors>
<title>Using pertainyms to improve passage retrieval for questions requesting information about a location.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on Information Retrievalfor Question Answering (SIGIR 2004),</booktitle>
<location>Sheffield, UK.</location>
<contexts>
<context position="2647" citStr="Greenwood, 2004" startWordPosition="388" endWordPosition="389">l enough to enable efficient and accurate QA. In this study we investigate whether semantically motivated passages in the retrieval component lead to better QA performance compared to the use of document retrieval and window-based segmentation approaches. 1.1 Index-time versus Search-time Passaging In this paper, we experiment with various possibilities of dividing documents into passages before indexing them. This is also called index-time passaging and refers to a one-step process of retrieving appropriate textual units for subsequent answer extraction modules (Roberts and Gaizauskas, 2004; Greenwood, 2004). This is in contrast to other strategies using a two-step procedure consisting of document retrieval and search-time passaging thereafter. Here, we can distinguish between approaches that only return one passage per relevant document (see, for example, (Robertson et al., 1992)) and the ones that allow multiple passages per document (see, for example (Moldovan et al., 2000)). In general, allowing multiple passages per document is preferable for QA as possible answers can be contained at various positions in a document (Roberts and Gaizauskas, 2004). For this, an index-time approach has the adv</context>
</contexts>
<marker>Greenwood, 2004</marker>
<rawString>Greenwood, Mark A. 2004. Using pertainyms to improve passage retrieval for questions requesting information about a location. In Proceedings of the Workshop on Information Retrievalfor Question Answering (SIGIR 2004), Sheffield, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
<author>Christian Plaunt</author>
</authors>
<title>Subtopic structuring for full-length document access.</title>
<date>1993</date>
<booktitle>In Research and Development in Information Retrieval,</booktitle>
<pages>59--68</pages>
<contexts>
<context position="5845" citStr="Hearst and Plaunt, 1993" startWordPosition="892" endWordPosition="895">search-time approaches is rather similar (as we have discussed earlier) we believe that the index-time approach is preferable. 1.2 Passages in IR Certainly, IR performance is effected by changing the size of the units to be indexed. The task in document segmentation for our index-time passaging approach is to find the proper division of documents into text passages which optimize the retrieval in terms of overall QA performance. The general advantages of passage retrieval over full-text document retrieval has been investigated in various studies, e.g., (Kaszkiel and Zobel, 2001; Callan, 1994; Hearst and Plaunt, 1993; Kaszkiel and Zobel, 1997). Besides the argument of decreasing the search space for subsequent answer extraction modules in QA, passage retrieval also improves standard IR techniques by “normalizing” textual units in terms of size which is especially important in cases where documents come from very diverse sources. IR is based on similarity measures between documents and queries and standard approaches have shortcomings when applying them to documents of various sizes and text types. Often there is a bias for certain types raising problems of discrimination between documents of different len</context>
</contexts>
<marker>Hearst, Plaunt, 1993</marker>
<rawString>Hearst, Marti A. and Christian Plaunt. 1993. Subtopic structuring for full-length document access. In Research and Development in Information Retrieval, pages 59–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Texttiling: Segmenting text into multi-paragraph subtopic passages.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="13434" citStr="Hearst, 1997" startWordPosition="2104" endWordPosition="2105">the candidate itself, as well as features from the cluster to which the candidate belongs. It picks the most likely candidate as the coreferring antecedent. References relations are detected between pronouns, common nouns and named entities. The resolution system yields a precision of 67.9% and a recall of 45.6% (F-score = 54.5%) using MUC scores (Vilain et al., 1993) on the annotated test corpus developed by (Hoste, 2005) which consist of articles taken from KNACK, a Flemish weekly news magazine. 3.2 TextTiling TextTiling is a well-known algorithm for segmenting texts into subtopic passages (Hearst, 1997). It is based on the assumption that a significant portion of a set of lexical items in use during the course of a given subtopic discussion changes when that subtopic in the text changes. Topic shifts are found by searching for lexical co-occurrence patterns and comparing adjacent blocks. First the text is subdivided into pseudo-sentences of a predefined size rather than using syntactically-determined sentences. These pseudo-sentences are called token-sequences by Hearst. The algorithm identifies discourse boundaries by calculating a score for each token-sequence gap. This score is based on t</context>
</contexts>
<marker>Hearst, 1997</marker>
<rawString>Hearst, Marti A. 1997. Texttiling: Segmenting text into multi-paragraph subtopic passages. Computational Linguistics, 23(1):33–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Hoste</author>
</authors>
<title>Optimization Issues in Machine Learning of Coreference Resolution.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Antwerp.</institution>
<contexts>
<context position="13247" citStr="Hoste, 2005" startWordPosition="2076" endWordPosition="2077">o be coreferent than others. The system ranks possible antecedents for each anaphor considering syntactic features, semantic features and surface structure features from the anaphor and the candidate itself, as well as features from the cluster to which the candidate belongs. It picks the most likely candidate as the coreferring antecedent. References relations are detected between pronouns, common nouns and named entities. The resolution system yields a precision of 67.9% and a recall of 45.6% (F-score = 54.5%) using MUC scores (Vilain et al., 1993) on the annotated test corpus developed by (Hoste, 2005) which consist of articles taken from KNACK, a Flemish weekly news magazine. 3.2 TextTiling TextTiling is a well-known algorithm for segmenting texts into subtopic passages (Hearst, 1997). It is based on the assumption that a significant portion of a set of lexical items in use during the course of a given subtopic discussion changes when that subtopic in the text changes. Topic shifts are found by searching for lexical co-occurrence patterns and comparing adjacent blocks. First the text is subdivided into pseudo-sentences of a predefined size rather than using syntactically-determined sentenc</context>
</contexts>
<marker>Hoste, 2005</marker>
<rawString>Hoste, V. 2005. Optimization Issues in Machine Learning of Coreference Resolution. Ph.D. thesis, University of Antwerp.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcin Kaszkiel</author>
<author>Justin Zobel</author>
</authors>
<title>Passage retrieval revisited.</title>
<date>1997</date>
<booktitle>In SIGIR ’97: Proceedings of the 20th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>178--185</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="5872" citStr="Kaszkiel and Zobel, 1997" startWordPosition="896" endWordPosition="899"> rather similar (as we have discussed earlier) we believe that the index-time approach is preferable. 1.2 Passages in IR Certainly, IR performance is effected by changing the size of the units to be indexed. The task in document segmentation for our index-time passaging approach is to find the proper division of documents into text passages which optimize the retrieval in terms of overall QA performance. The general advantages of passage retrieval over full-text document retrieval has been investigated in various studies, e.g., (Kaszkiel and Zobel, 2001; Callan, 1994; Hearst and Plaunt, 1993; Kaszkiel and Zobel, 1997). Besides the argument of decreasing the search space for subsequent answer extraction modules in QA, passage retrieval also improves standard IR techniques by “normalizing” textual units in terms of size which is especially important in cases where documents come from very diverse sources. IR is based on similarity measures between documents and queries and standard approaches have shortcomings when applying them to documents of various sizes and text types. Often there is a bias for certain types raising problems of discrimination between documents of different lengths and content densities.</context>
</contexts>
<marker>Kaszkiel, Zobel, 1997</marker>
<rawString>Kaszkiel, Marcin and Justin Zobel. 1997. Passage retrieval revisited. In SIGIR ’97: Proceedings of the 20th annual international ACM SIGIR conference on Research and development in information retrieval, pages 178–185, New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcin Kaszkiel</author>
<author>Justin Zobel</author>
</authors>
<title>Effective ranking with arbitrary passages.</title>
<date>2001</date>
<journal>Journal of the American Society of Information Science,</journal>
<volume>52</volume>
<issue>4</issue>
<contexts>
<context position="5806" citStr="Kaszkiel and Zobel, 2001" startWordPosition="886" endWordPosition="889"> that the performance of index-time and search-time approaches is rather similar (as we have discussed earlier) we believe that the index-time approach is preferable. 1.2 Passages in IR Certainly, IR performance is effected by changing the size of the units to be indexed. The task in document segmentation for our index-time passaging approach is to find the proper division of documents into text passages which optimize the retrieval in terms of overall QA performance. The general advantages of passage retrieval over full-text document retrieval has been investigated in various studies, e.g., (Kaszkiel and Zobel, 2001; Callan, 1994; Hearst and Plaunt, 1993; Kaszkiel and Zobel, 1997). Besides the argument of decreasing the search space for subsequent answer extraction modules in QA, passage retrieval also improves standard IR techniques by “normalizing” textual units in terms of size which is especially important in cases where documents come from very diverse sources. IR is based on similarity measures between documents and queries and standard approaches have shortcomings when applying them to documents of various sizes and text types. Often there is a bias for certain types raising problems of discrimina</context>
<context position="8182" citStr="Kaszkiel and Zobel, 2001" startWordPosition="1265" endWordPosition="1268">phs. There are several ways of automatically dividing documents into passages without relying on existing markup. One way is to search for linguistic clues that indicate a separation of consecutive text blocks. These clues may include lexical patterns and relations. We refer to such approaches as semantically motivated document segmentation. Another approach is to cut documents into arbitrary pieces ignoring any other type of information. For example, we can use fixed-sized windows to divide documents into passages of similar size. Such windows can be defined in terms of words and characters (Kaszkiel and Zobel, 2001; Monz, 2003) or sentences and paragraphs (Zobel et al., 1995; Llopis et al., 2002). It is also possible to allow varying window sizes and overlapping sections to be indexed (Kaszkiel and Zobel, 2001; Monz, 2003). In this case it is up to the IR engine 18 to decide which of the competing window types is preferred and it may even return overlapping sections multiple times. In the following sections we will discuss two techniques of semantically motivated document segmentation and compare them to simple window-based techniques in terms of passage retrieval and QA performance. 2 Passage Retrieval</context>
</contexts>
<marker>Kaszkiel, Zobel, 2001</marker>
<rawString>Kaszkiel, Marcin and Justin Zobel. 2001. Effective ranking with arbitrary passages. Journal of the American Society of Information Science, 52(4):344–364.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas Lester</author>
<author>Hugh Williams</author>
<author>Justin Zobel</author>
<author>Falk Scholer</author>
<author>Dirk Bahle</author>
<author>John Yiannis</author>
<author>Bodo von Billerbeck</author>
<author>Steven Garcia</author>
<author>William Webber</author>
</authors>
<title>The Zettair search engine.</title>
<date>2006</date>
<note>http://www.seg.rmit.edu.au/zettair/.</note>
<marker>Lester, Williams, Zobel, Scholer, Bahle, Yiannis, von Billerbeck, Garcia, Webber, 2006</marker>
<rawString>Lester, Nicholas, Hugh Williams, Justin Zobel, Falk Scholer, Dirk Bahle, John Yiannis, Bodo von Billerbeck, Steven Garcia, and William Webber. 2006. The Zettair search engine. http://www.seg.rmit.edu.au/zettair/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Llopis</author>
<author>J Vicedo</author>
<author>A Ferrindez</author>
</authors>
<title>Passage selection to improve question answering.</title>
<date>2002</date>
<booktitle>In Proceedings of the COLING 2002 Workshop on Multilingual Summarization and Question Answering.</booktitle>
<contexts>
<context position="8265" citStr="Llopis et al., 2002" startWordPosition="1279" endWordPosition="1282">elying on existing markup. One way is to search for linguistic clues that indicate a separation of consecutive text blocks. These clues may include lexical patterns and relations. We refer to such approaches as semantically motivated document segmentation. Another approach is to cut documents into arbitrary pieces ignoring any other type of information. For example, we can use fixed-sized windows to divide documents into passages of similar size. Such windows can be defined in terms of words and characters (Kaszkiel and Zobel, 2001; Monz, 2003) or sentences and paragraphs (Zobel et al., 1995; Llopis et al., 2002). It is also possible to allow varying window sizes and overlapping sections to be indexed (Kaszkiel and Zobel, 2001; Monz, 2003). In this case it is up to the IR engine 18 to decide which of the competing window types is preferred and it may even return overlapping sections multiple times. In the following sections we will discuss two techniques of semantically motivated document segmentation and compare them to simple window-based techniques in terms of passage retrieval and QA performance. 2 Passage Retrieval in our QA system Our QA system is an open-domain question answering system for Dut</context>
</contexts>
<marker>Llopis, Vicedo, Ferrindez, 2002</marker>
<rawString>Llopis, F., J. Vicedo, and A. Ferrindez. 2002. Passage selection to improve question answering. In Proceedings of the COLING 2002 Workshop on Multilingual Summarization and Question Answering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moldovan</author>
<author>S Harabagiu</author>
<author>M Pasca</author>
<author>R Mihalcea</author>
<author>R Girju</author>
<author>R Goodrum</author>
<author>V Rus</author>
</authors>
<title>The structure and performance of an open-domain question answering system.</title>
<date>2000</date>
<contexts>
<context position="3023" citStr="Moldovan et al., 2000" startWordPosition="446" endWordPosition="449">cuments into passages before indexing them. This is also called index-time passaging and refers to a one-step process of retrieving appropriate textual units for subsequent answer extraction modules (Roberts and Gaizauskas, 2004; Greenwood, 2004). This is in contrast to other strategies using a two-step procedure consisting of document retrieval and search-time passaging thereafter. Here, we can distinguish between approaches that only return one passage per relevant document (see, for example, (Robertson et al., 1992)) and the ones that allow multiple passages per document (see, for example (Moldovan et al., 2000)). In general, allowing multiple passages per document is preferable for QA as possible answers can be contained at various positions in a document (Roberts and Gaizauskas, 2004). For this, an index-time approach has the advantage that the retrieval of multiple passages per documents is straightforward because all of them compete which each other in the same index using the same metric for ranking. A comparison between index-time and search17 Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 17–25 Manchester, UK. August 2008 time passag</context>
</contexts>
<marker>Moldovan, Harabagiu, Pasca, Mihalcea, Girju, Goodrum, Rus, 2000</marker>
<rawString>Moldovan, D., S. Harabagiu, M. Pasca, R. Mihalcea, R. Girju, R. Goodrum, and V. Rus. 2000. The structure and performance of an open-domain question answering system.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christof Monz</author>
</authors>
<title>From Document Retrieval to Question Answering.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Amsterdam.</institution>
<contexts>
<context position="8195" citStr="Monz, 2003" startWordPosition="1269" endWordPosition="1270">s of automatically dividing documents into passages without relying on existing markup. One way is to search for linguistic clues that indicate a separation of consecutive text blocks. These clues may include lexical patterns and relations. We refer to such approaches as semantically motivated document segmentation. Another approach is to cut documents into arbitrary pieces ignoring any other type of information. For example, we can use fixed-sized windows to divide documents into passages of similar size. Such windows can be defined in terms of words and characters (Kaszkiel and Zobel, 2001; Monz, 2003) or sentences and paragraphs (Zobel et al., 1995; Llopis et al., 2002). It is also possible to allow varying window sizes and overlapping sections to be indexed (Kaszkiel and Zobel, 2001; Monz, 2003). In this case it is up to the IR engine 18 to decide which of the competing window types is preferred and it may even return overlapping sections multiple times. In the following sections we will discuss two techniques of semantically motivated document segmentation and compare them to simple window-based techniques in terms of passage retrieval and QA performance. 2 Passage Retrieval in our QA sy</context>
</contexts>
<marker>Monz, 2003</marker>
<rawString>Monz, Christof. 2003. From Document Retrieval to Question Answering. Ph.D. thesis, University of Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Roberts</author>
<author>Robert Gaizauskas</author>
</authors>
<title>Evaluating passage retrieval approaches for question answering.</title>
<date>2004</date>
<booktitle>In Proceedings of 26th European Conference on Information Retrieval.</booktitle>
<contexts>
<context position="2629" citStr="Roberts and Gaizauskas, 2004" startWordPosition="384" endWordPosition="387">perly and they have to be small enough to enable efficient and accurate QA. In this study we investigate whether semantically motivated passages in the retrieval component lead to better QA performance compared to the use of document retrieval and window-based segmentation approaches. 1.1 Index-time versus Search-time Passaging In this paper, we experiment with various possibilities of dividing documents into passages before indexing them. This is also called index-time passaging and refers to a one-step process of retrieving appropriate textual units for subsequent answer extraction modules (Roberts and Gaizauskas, 2004; Greenwood, 2004). This is in contrast to other strategies using a two-step procedure consisting of document retrieval and search-time passaging thereafter. Here, we can distinguish between approaches that only return one passage per relevant document (see, for example, (Robertson et al., 1992)) and the ones that allow multiple passages per document (see, for example (Moldovan et al., 2000)). In general, allowing multiple passages per document is preferable for QA as possible answers can be contained at various positions in a document (Roberts and Gaizauskas, 2004). For this, an index-time ap</context>
</contexts>
<marker>Roberts, Gaizauskas, 2004</marker>
<rawString>Roberts, Ian and Robert Gaizauskas. 2004. Evaluating passage retrieval approaches for question answering. In Proceedings of 26th European Conference on Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen E Robertson</author>
<author>Steve Walker</author>
<author>Micheline Hancock-Beaulieu</author>
<author>Aarron Gull</author>
<author>Marianna Lau</author>
</authors>
<title>Okapi at TREC-3.</title>
<date>1992</date>
<booktitle>In Text REtrieval Conference,</booktitle>
<pages>21--30</pages>
<contexts>
<context position="2925" citStr="Robertson et al., 1992" startWordPosition="429" endWordPosition="432">versus Search-time Passaging In this paper, we experiment with various possibilities of dividing documents into passages before indexing them. This is also called index-time passaging and refers to a one-step process of retrieving appropriate textual units for subsequent answer extraction modules (Roberts and Gaizauskas, 2004; Greenwood, 2004). This is in contrast to other strategies using a two-step procedure consisting of document retrieval and search-time passaging thereafter. Here, we can distinguish between approaches that only return one passage per relevant document (see, for example, (Robertson et al., 1992)) and the ones that allow multiple passages per document (see, for example (Moldovan et al., 2000)). In general, allowing multiple passages per document is preferable for QA as possible answers can be contained at various positions in a document (Roberts and Gaizauskas, 2004). For this, an index-time approach has the advantage that the retrieval of multiple passages per documents is straightforward because all of them compete which each other in the same index using the same metric for ranking. A comparison between index-time and search17 Coling 2008: Proceedings of the 2nd workshop on Informa</context>
<context position="23366" citStr="Robertson et al., 1992" startWordPosition="3791" endWordPosition="3794"> smaller for sentence retrieval than for the other two while coverage is still reasonably high. The CLEF scores (accuracy measured on the top answer provided by the system) follow the same pattern. Here, the difference between sentence retrieval and document retrieval is even more apparent. Certainly, the success of the retrieval component depends on the metric used for ranking documents as implemented in the IR engine. In order to verify the importance of document segmentation in a QA setting we also ran experiments with another standard metric implemented in Zettair, the Okapi BM-25 metric (Robertson et al., 1992). Similar to the previous setting using the LM metric, QA with paragraph retrieval (now yielding MRRQA = 0.460) outperforms QA with document retrieval (MRRQA = 0.449). However, sentence retrieval does not perform as well (MRRQA = 0.420) which suggests that the Okapi metric is not suited for very small retrieval units. Still, the success of paragraph retrieval supports the advantage of passage retrieval compared to document retrieval and suggests potential QA performance gains with improved document segmentation strategies. In the remaining we only report results using the LM metric for retriev</context>
</contexts>
<marker>Robertson, Walker, Hancock-Beaulieu, Gull, Lau, 1992</marker>
<rawString>Robertson, Stephen E., Steve Walker, Micheline Hancock-Beaulieu, Aarron Gull, and Marianna Lau. 1992. Okapi at TREC-3. In Text REtrieval Conference, pages 21–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tellex</author>
<author>B Katz</author>
<author>J Lin</author>
<author>A Fernandes</author>
<author>G Marton</author>
</authors>
<title>Quantitative evaluation ofpassage retrieval algorithms for question answering.</title>
<date>2003</date>
<booktitle>In Proceedings of the SIGIR conference on Research and development in informaion retrieval,</booktitle>
<pages>41--47</pages>
<publisher>ACM Press.</publisher>
<contexts>
<context position="4418" citStr="Tellex et al., 2003" startWordPosition="670" endWordPosition="673">ndancy (measures which have been introduced in the same paper; see section 4.2 for more information). Significant differences between the various approaches can only be observed in redundancy on higher ranks (above 50). However, as we will see later in our experiments (section 4.2), redundancy is not as important as coverage for our QA system . Furthermore, retrieving more than about 40 passages does not produce significant improvements of the QA system anymore but slows down the processing time substantially. Another argument for our focus on a one-step retrieval procedure can be taken from (Tellex et al., 2003). In this paper, the authors do not actually use any index-time passaging approach but compare various search-time passage retrieval algorithms. However, they obtain a huge performance difference when applying an oracle document retriever (only returning relevant documents in the first retrieval step) instead of a standard IR engine. Compared to this, the differences between the various passage retrieval approaches tested is very small. From this we can conclude that much improvement can be gained by improving the initial retrieval step, which seems to be the bottleneck in the entire process. </context>
</contexts>
<marker>Tellex, Katz, Lin, Fernandes, Marton, 2003</marker>
<rawString>Tellex, S., B. Katz, J. Lin, A. Fernandes, and G. Marton. 2003. Quantitative evaluation ofpassage retrieval algorithms for question answering. In Proceedings of the SIGIR conference on Research and development in informaion retrieval, pages 41–47. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Van Deemter</author>
<author>R Kibble</author>
</authors>
<title>On coreferring: Coreference in muc and related annotation schemes.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>4</issue>
<marker>Van Deemter, Kibble, 2000</marker>
<rawString>Van Deemter, K. and R. Kibble. 2000. On coreferring: Coreference in muc and related annotation schemes. Computational Linguistics, 26(4):629–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
</authors>
<title>At Last Parsing Is Now Operational.</title>
<date>2006</date>
<booktitle>In TALN 2006 Verbum Ex Machina, Actes De La 13e Conference sur Le Traitement Automatique des Langues naturelles,</booktitle>
<pages>20--42</pages>
<location>Leuven.</location>
<marker>van Noord, 2006</marker>
<rawString>van Noord, Gertjan. 2006. At Last Parsing Is Now Operational. In TALN 2006 Verbum Ex Machina, Actes De La 13e Conference sur Le Traitement Automatique des Langues naturelles, pages 20–42, Leuven.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Vilain</author>
<author>J Burger</author>
<author>J Aberdeen</author>
<author>D Connolly</author>
<author>L Hirschman</author>
</authors>
<title>A model-theoretic coreference scoring scheme.</title>
<date>1993</date>
<booktitle>In Proceedings of the 6th conference on Message understanding (MUC 6),</booktitle>
<pages>45--52</pages>
<contexts>
<context position="13191" citStr="Vilain et al., 1993" startWordPosition="2065" endWordPosition="2068">9 clustering-based ranking task. Some NP pairs are more likely to be coreferent than others. The system ranks possible antecedents for each anaphor considering syntactic features, semantic features and surface structure features from the anaphor and the candidate itself, as well as features from the cluster to which the candidate belongs. It picks the most likely candidate as the coreferring antecedent. References relations are detected between pronouns, common nouns and named entities. The resolution system yields a precision of 67.9% and a recall of 45.6% (F-score = 54.5%) using MUC scores (Vilain et al., 1993) on the annotated test corpus developed by (Hoste, 2005) which consist of articles taken from KNACK, a Flemish weekly news magazine. 3.2 TextTiling TextTiling is a well-known algorithm for segmenting texts into subtopic passages (Hearst, 1997). It is based on the assumption that a significant portion of a set of lexical items in use during the course of a given subtopic discussion changes when that subtopic in the text changes. Topic shifts are found by searching for lexical co-occurrence patterns and comparing adjacent blocks. First the text is subdivided into pseudo-sentences of a predefined</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1993</marker>
<rawString>Vilain, M., J. Burger, J. Aberdeen, D. Connolly, and L. Hirschman. 1993. A model-theoretic coreference scoring scheme. In Proceedings of the 6th conference on Message understanding (MUC 6), pages 45– 52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justin Zobel</author>
<author>Alistair Moffat</author>
<author>Ross Wilkinson</author>
<author>Ron Sacks-Davis</author>
</authors>
<title>Efficient retrieval of partial documents.</title>
<date>1995</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>31--3</pages>
<contexts>
<context position="8243" citStr="Zobel et al., 1995" startWordPosition="1275" endWordPosition="1278">o passages without relying on existing markup. One way is to search for linguistic clues that indicate a separation of consecutive text blocks. These clues may include lexical patterns and relations. We refer to such approaches as semantically motivated document segmentation. Another approach is to cut documents into arbitrary pieces ignoring any other type of information. For example, we can use fixed-sized windows to divide documents into passages of similar size. Such windows can be defined in terms of words and characters (Kaszkiel and Zobel, 2001; Monz, 2003) or sentences and paragraphs (Zobel et al., 1995; Llopis et al., 2002). It is also possible to allow varying window sizes and overlapping sections to be indexed (Kaszkiel and Zobel, 2001; Monz, 2003). In this case it is up to the IR engine 18 to decide which of the competing window types is preferred and it may even return overlapping sections multiple times. In the following sections we will discuss two techniques of semantically motivated document segmentation and compare them to simple window-based techniques in terms of passage retrieval and QA performance. 2 Passage Retrieval in our QA system Our QA system is an open-domain question an</context>
</contexts>
<marker>Zobel, Moffat, Wilkinson, Sacks-Davis, 1995</marker>
<rawString>Zobel, Justin, Alistair Moffat, Ross Wilkinson, and Ron Sacks-Davis. 1995. Efficient retrieval of partial documents. Information Processing and Management, 31(3):361–377.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>