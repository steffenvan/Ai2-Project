<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000086">
<title confidence="0.994303">
Investigating the Characteristics of Causal Relations in Japanese Text
</title>
<author confidence="0.901515">
Takashi Inui and Manabu Okumura
</author>
<affiliation confidence="0.933119">
Precision and Intelligence Laboratory
Tokyo Institute of Technology
</affiliation>
<address confidence="0.713255">
4259, Nagatsuta, Midori-ku, Yokohama, 226-8503, Japan
</address>
<email confidence="0.998953">
tinui@lr.pi.titech.ac.jp, oku@pi.titech.ac.jp
</email>
<sectionHeader confidence="0.995646" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999442">
We investigated of the characteristics of
in-text causal relations. We designed
causal relation tags. With our designed
tag set, three annotators annotated 750
Japanese newspaper articles. Then, using
the annotated corpus, we investigated the
causal relation instances from some view-
points. Our quantitative study shows that
what amount of causal relation instances
are present, where these relation instances
are present, and which types of linguistic
expressions are used for expressing these
relation instances in text.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99990025">
For many applications of natural language tech-
niques such as question-answering systems and di-
alogue systems, acquiring knowledge about causal
relations is one central issue. In recent researches,
some automatic acquisition methods for causal
knowledge have been proposed (Girju, 2003; Sato et
al., 1999; Inui, 2004). They have used as knowledge
resources a large amount of electric text documents:
newspaper articles and Web documents.
To realize their knowledge acquisition methods
accurately and efficiently, it is important to know-
ing the characteristics of presence of in-text causal
relations. However, while the acquisition methods
have been improved by some researches, the char-
acteristics of presence of in-text causal relations are
still unclear: we have no empirical study about what
amount of causal relation instances exist in text and
where in text causal relation instances tend to ap-
pear.
In this work, aiming to resolve the above issues,
we create a corpus annotated with causal relation
information which is useful for investigating what
amount of causal relation instances are present and
where these instances are present in text. Given
some Japanese newspaper articles, we add our de-
signed causal relation tags to the text segments. Af-
ter creating the annotated corpus, we investigate the
causal relation instances from three viewpoints: (i)
cue phrase markers, (ii) part-of-speech information,
and (iii) positions in sentences.
There are some pieces of previous work on anal-
ysis of in-text causal relations. However, although
causal relation instances appear in several different
ways, just a few forms have been treated in the pre-
vious studies: the verb phrase form with cue phrase
markers such as in (1a) has been mainly treated. In
contrast, we add our causal relation tags to several
types of linguistic expressions with wide coverage to
realize further analyses from above three points. Ac-
tually, we treat not only linguistic expressions with
explicit cues such as in (1a) , but also those with-
out explicit cues, i.e. implicit, as in (1b) , those
formed by noun phrases as in (1c), and those formed
between sentences as in (1d) .
</bodyText>
<figure confidence="0.631453555555555">
(1) a. - - - -
heavy rain-NOM fall-PAST because river-NOM rise-PAST
(explicit)
b. - - - -
heavy rain-NOM fall-PUNC river-NOM rise-PAST
(implicit)
c. - - -
heavy rain-because of river-NOM rise-PAST
(noun phrase)
</figure>
<page confidence="0.986744">
37
</page>
<note confidence="0.7664336">
Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 37–44,
Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
d. - - - - -
heavy rain-NOM fall-PAST-PUNC river-NOM rise-PAST
(between sentences)
</note>
<bodyText confidence="0.999975294117647">
We apply new criteria for judging whether a lin-
guistic expression includes a causal relation or not.
Generally, it is hard to define rigorously the notion
of causal relation. Therefore, in previous studies,
there have been no standard common criteria for
judging causal relations. Researchers have resorted
to annotators’ subjective judgements. Our criteria
are represented in the form of linguistic templates
which the annotators apply in making their judge-
ments (see Section 3.2).
In Section 2, we will outline several previous
research efforts on in-text causal relations. In
Section 3 to Section 6, we will describe the details
of the design of our causal relation tags and the an-
notation workflow. In Section 7, using the annotated
corpus, we will then discuss the results for the inves-
tigation of characteristics of in-text causal relations.
</bodyText>
<sectionHeader confidence="0.999424" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999954266666667">
Liu (2004) analyzed the differences of usages of
some Japanese connectives marking causal rela-
tions. The results are useful for accounting for an
appropriate connective for each context within the
documents. However Liu conducted no quantitative
studies.
Marcu (1997) investigated the frequency distri-
bution of English connectives including “because”
and “since” for implementation of rhetorical pars-
ing. However, although Marcu’s study was quanti-
tative one, Marcu treated only explicit linguistic ex-
pressions with connectives. In the Timebank corpus
(Pustejovsky et al., 2003), the causal relation infor-
mation is included. However, the information is op-
tional for implicit linguistic expressions.
Although both explicit expressions and implicit
expressions are treated in the Penn Discourse Tree-
bank (PDTB) corpus (Miltsakaki et al., 2004), no
information on causal relations is contained in this
corpus.
Altenberg (1984) investigated the frequency dis-
tribution of causal relation instances from some
viewpoints such as document style and the syntac-
tic form in English dialog data. Nishizawa (1997)
also conducted a similar work using Japanese dialog
data. Some parts of their viewpoints are overlapping
with ours. However, while their studies focused on
dialog data, our target is text documents. In fact, Al-
tenberg treated also English text documents. How-
ever, our focus in this work is Japanese.
</bodyText>
<sectionHeader confidence="0.997451" genericHeader="method">
3 Annotated information
</sectionHeader>
<subsectionHeader confidence="0.999247">
3.1 Causal relation tags
</subsectionHeader>
<bodyText confidence="0.9995705">
We use three tags head, mod, and causal rel to rep-
resent the basic causal relation information. Our an-
notation scheme for events is similar to that of the
PropBank (Palmer et al., 2005). An event is re-
garded as consisting of a head element and some
modifiers. The tags head and mod are used to repre-
sent an event which forms one part of the two events
held in a causal relation. The tag causal rel is used
to represent a causal relation between two annotated
events.
Figure 1 shows an example of attaching the causal
relation information to the sentence (2a), in which a
causal relation is held between two events indicated
(2b) and (2c) . Hereafter, we denote the former
(cause) part of event as e1 and the latter (effect) part
of event as e2.
</bodyText>
<figure confidence="0.742785">
(2) a.
(As the Golden Week holidays come, the number of
sightseers from all over begins to increase.)
b. e1 =
(The Golden Week holidays come)
c. e2 =
(The number of sightseers from all over begins
to increase)
</figure>
<figureCaption confidence="0.912532">
Figure 1: An example of attaching the causal rela-
tion information
</figureCaption>
<bodyText confidence="0.9721677">
The annotation process is executed as follows.
First, each sentence in the text is split to some bun-
setsu-phrase chunksl, as shown in Figure 1 (“/” in-
dicates a bunsetsu-phrase chunk boundary). Second,
for each bunsetsu-phrase, an annotator finds the seg-
ment which represents a head element of an event,
&apos;The bunsetsu-phrase is one of the fundamental units in
Japanese, which consists of a content word (noun, verb, ad-
jective, etc.) accompanied by some function words (particles,
auxiliaries, etc.).
</bodyText>
<page confidence="0.998562">
38
</page>
<bodyText confidence="0.999641944444444">
and he/she adds the head tag to the segment (see
also head1 and head2 in Figure 1). If the event has
any other elements in addition to head element, the
annotator also adds the mod tags to the segments
representing modifiers to the head element (mod1
and mod2 in Figure 1). The elements marked with
any tags which have a common suffix number are
constituents of the same event: that is, the elements
marked with head1 and mod1 tags are constituents of
e1 and the elements marked with head2 and mod2
are constituents of e2. Finally, the annotator adds
the causal rel tag between head1 and head2 as link
information which indicates that the corresponding
two events are held in a causal relation.
When there are any cue phrase markers helpful in
recognizing causal relations such as (because)
in (1a) , the annotator also adds the marker tag to
their segments.
</bodyText>
<subsectionHeader confidence="0.999762">
3.2 Annotation criteria
</subsectionHeader>
<bodyText confidence="0.999853615384615">
To judge whether two events represented in text are
held in a causal relation or not, we apply new criteria
based on linguistic test.
The linguistic test is a method for judging whether
target linguistic expressions conforms to a given set
of rules. In our cases, the target expressions are two
sets of bunsetsu-phrase chunks. Each set represents
as a whole an event which can be an argument in
a causal relation, such as in (2b) and (2c) . The
rules are realized as linguistic templates which are
linguistic expressions including several slots.
In practice, a linguistic test is usually applied us-
ing the following steps:
</bodyText>
<listItem confidence="0.9892756">
1. Preparing a template.
2. Embedding the target expressions in the slots
of the template to form a candidate sentence.
3. If the candidate sentence is syntactically and
semantically correct, the target expressions are
</listItem>
<bodyText confidence="0.967888375">
judged to conform to the rules. If the candi-
date sentence is incorrect, the targets are judged
non-conforming.
In this work, we prepared eighteen linguistic tem-
plates such as in Figure 2. The square brackets indi-
cate the slots. The symbol (adv) is replaced by one
of three adverbs (often), (usually), or
(always).
</bodyText>
<figureCaption confidence="0.968474">
Figure 2: An example of linguistic templates
</figureCaption>
<bodyText confidence="0.989490390243903">
We embed two target expressions representing
events in the slots of the template to form a candi-
date sentence. Then, if an annotator can recognize
that the candidate sentence is syntactically and se-
mantically correct, the causal relation is supposed to
hold between two events. In contrast, if recognized
that the candidate sentence is incorrect, this template
is rejected, and the other template is tried. If all
eighteen templates are rejected by the annotator, it
is supposed that there is no causal relations between
these two events. Note that the annotator’s recogni-
tion of whether the candidate sentence is correct or
incorrect, in other words, whether a causal relation
is held between the two events embedded in the can-
didate sentence or not, is not really relevant to the
author’s intention.
The fundamental idea of our criteria based on lin-
guistic test is similar to that of the criteria for anno-
tation of implicit connectives adopted in PDTB cor-
pus2. In the annotation process of the PDTB corpus,
an annotator judges whether or not the explicit con-
nective, for example, “because”, relates two linguis-
tic expressions representing events. This process is
essentially the same as ours.
Three adverbs in the linguistic templates,
(often), (usually) and (always), in-
dicate a pragmatic constraint on the necessity of the
relationship between any two events; the relations
indicated by these words usually have a high degree
of necessity. With this pragmatic constraint, we in-
troduce an attribute to the causal rel tags about the
degree of necessity. For each of eighteen templates,
if one judges the two target expressions as holding
a causal relation by using the template with one of
three adverbs, the necessity attribute value is added
to the relation instance. If one judges the two target
expressions as holding a causal relation by using the
template deleting (adv), three adverbs, the chance
2For detail instructions of the annotation criteria in PDTB
corpus, see http://www.cis.upenn.edu/˜pdtb/
manual/pdtb-tutorial.pdf.
</bodyText>
<figure confidence="0.9205115">
[e1]
(adv) [e2]
[e2] (adv) happened as a result of
the fact that [e1] happened.
</figure>
<page confidence="0.99167">
39
</page>
<bodyText confidence="0.985938">
attribute value is added.
We assume that a target expression embedded in
the slot is represented by a single sentence. If an
event is represented by noun phrase (NP), the fol-
lowing rewriting rules are applied before embedded
to the slot to transform the NP into a single sentence.
If a head element of a target expression represent-
ing an event is conjugated, the head element is re-
placed by its base form before embedded to the slot.
</bodyText>
<subsectionHeader confidence="0.999696">
3.3 Annotation ranges
</subsectionHeader>
<bodyText confidence="0.9999922">
Ideally, we should try to judge for tagging of the
causal relation tags over all any event pairs in text.
However, it seems that the more the distance be-
tween two events represented in text, the smaller
the probability of holding a causal relation between
them. Thus, we set a constraint on the ranges of
judgements. If both two events are represented
in the same sentence or two sentences adjacent to
each other, we try judgements, if not, skip judge-
ments. This constraint is applied only when tag-
ging the head tag. A modifier and its head ele-
ment are sometimes located in different sentences
overtly in Japanese text when anaphora or ellipsis
phenomenon occurs. In such cases, we add mod
tags to the text segments anywhere in the text.
</bodyText>
<sectionHeader confidence="0.99898" genericHeader="method">
4 Data
</sectionHeader>
<bodyText confidence="0.999978142857143">
We selected as text for annotation Mainichi Shimbun
newspaper articles (Mainichi, 1995). In particular,
we used only articles included on the social aspect
domain. When adding the causal relation tags to the
text, it is preferable that each annotator can under-
stand the whole contents of the articles. The con-
tents of social aspect domain articles seems to be fa-
miliar to everybody and are easier to understand than
the contents of articles included on politics, econ-
omy domain, etc.
Furthermore, in our previous examination, it is
found that as the length of articles gets longer, it is
getting hard to judge which bunsetsu-phrase chunks
represent as a whole an event. This is because as de-
scribed in Section 3.3, annotators sometimes need to
search several sentences for modifiers of the head el-
ement in order to add mod tags precisely. Therefore,
we focus on social aspect domain articles which
consists of less than or equal to 10 sentences. Af-
ter all, we extracted 750 articles (3912 sentences)
for our annotation work with above conditions.
</bodyText>
<sectionHeader confidence="0.992345" genericHeader="method">
5 Annotation workflow
</sectionHeader>
<bodyText confidence="0.99900815625">
Three annotators have been employed. Each anno-
tator has added tags to the same 750 document ar-
ticles independently. Two annotators of the three
are linguists, and the last one is the author of this
paper. We denote each annotator under anonymity,
A, B and C. After training phase for annotators, we
spent approximately one month to create a corpus
annotated with causal relation information. The an-
notation workflow is executed efficiently using an
annotation interface. Using the interface, all of an-
notators can add tags through only simple keyboard
and mouse operations. The annotation workflow is
as follows.
I. Annotation phase: A document article is dis-
played to each annotator. The sentences in
the document are automatically split to bun-
setsu-phrases by preprocessing. Some kinds of
words such as connectives and verbs are high-
lighted to draw annotators’ attention to the text
segments which could represent elements in
causal relation instances. The annotator finds
text segments which represent causal relation
instances, and then he/she adds the causal re-
lation tags to their segments as described in
Section 3.
II. Modification phase: After each annotator fin-
ished the annotation phase for a fixed number
of document articles (in this work, 30 docu-
ment articles), he/she moves to a modification
phase. In this phase, first, only the segments
with causal relation tags are extracted from the
documents such as instances in Table 1. Then,
</bodyText>
<figure confidence="0.576291833333333">
• NP NP +
ex.
ex. earthquake an earthquake happens
• NP NP +
ex.
ex. blackout a blackout happens
• NP NP +
ex.
ex. heavy rain it rains heavily
• nominalized verb verb
ex.
ex. tiredness someone gets tired
</figure>
<page confidence="0.985579">
40
</page>
<tableCaption confidence="0.999767">
Table 1: Examples of tagged instances
</tableCaption>
<table confidence="0.937570538461538">
mods heads mod2 head2
-
sixth floor-from tumble lie unconscious
-
river-to tumble help out
- - -
roof-from tumble head-ACC hit
➴ - -
handgun-with shoot heavy injury-ACC suffer
- -
head-DAT burn-ACC suffer heavy injury
-
heavy injury-ACC suffer take a sabbatical leave
</table>
<bodyText confidence="0.999328727272727">
the same annotator who adds tags to the ex-
tracted segments, checks their extracted causal
relation instances with attention. Since the
extraction is done automatically, each annota-
tor can check all the segments to be checked.
When wrong tagged instances are found, they
are corrected on the moment. After checking
and correcting for all the extracted instances,
the annotator moves back to the annotation
phase in order to annotate a new 30 document
articles set.
</bodyText>
<sectionHeader confidence="0.999976" genericHeader="method">
6 Results
</sectionHeader>
<subsectionHeader confidence="0.999929">
6.1 Total number of tagged instances
</subsectionHeader>
<bodyText confidence="0.999972727272727">
2014 instances were tagged by the annotator A, 1587
instances by B, 1048 instances by C. Some examples
of tagged instances are shown in Table 1.
The total numbers of tagged instances of the three
annotators are quite different. Although all annota-
tors tagged under the same annotation criteria, the
annotator A tagged to twice as many segments as
the annotator C did. Though this difference may be
caused by some factors, we assume that the differ-
ence is mainly caused by missing judgements, since
the annotators added tags to a variety of linguis-
tic expressions, especially expressions without cue
phrases.
To verify the above assumption, we again asked
each annotator to judge whether or not a pair of lin-
guistic expressions representing events is holding a
causal relation. In this additional work, in order
to prevent the annotators from skipping judgement
itself, we present beforehand to the annotators the
pairs of linguistic expressions to be judged. We pre-
sented a set of 600 pairs of linguistic expressions to
each of the three annotators. All of these pairs are
</bodyText>
<tableCaption confidence="0.995744">
Table 2: Inter-annotator agreement
</tableCaption>
<table confidence="0.999764125">
A B C Srrtixed S. Sc
1 0 0 921 632 535
0 1 0 487 487 255
0 0 1 187 134 207
1 1 0 372 230 90
1 0 1 133 92 77
0 1 1 140 107 83
1 1 1 588 270 64
</table>
<bodyText confidence="0.989075090909091">
the causal relation instances already tagged by one
or more annotators in the main work described in
the previous sections.
From the comparison between the results of the
additional work and those of the main work, we
found that if causal relation instances are expressed
without explicit cues in text, they tend to be more
frequently missed than those with explicit cues. The
missing judgements on expressions without explicit
cues are an important issue in the realization of more
sophisticated analyses.
</bodyText>
<subsectionHeader confidence="0.973131">
6.2 Inter-annotator agreement
</subsectionHeader>
<bodyText confidence="0.999982538461539">
We examined inter-annotator agreement. First,
we define an agreement measure between two rela-
tion instances. Let x and y be causal relation in-
stances tagged by two different annotators. The in-
stance x consists of e1x and e2x, and y consists of
e1y and e2y. The event e1x has head1x as its head el-
ement. Similarly, head2x, head1y and head2y are the
head elements corresponding respectively to events
e2x, e1y and e2y. Then, we regard two instances x
and y as the same instance, when head1x and head1y
are located in the same bunsetsu-phrase and head2x
and head2y are also located in the same bunsetsu-
phrase. Using the above defined agreement measure,
</bodyText>
<page confidence="0.998727">
41
</page>
<bodyText confidence="0.999928666666667">
we counted the number of instances tagged by the
different annotators.
Table 2 shows the results. The symbol “1” in
the left-hand side of Table 2 indicates that the cor-
responding annotator tagged to instances, and the
“0” indicates not tagged. For example, the fourth
row (“110”) indicates that both A and B tagged to
instances but C did not.
Let Smixed denote a set of all tagged instances, Sn
denote a set of all tagged instances with the neces-
sity attribute value, and S, denote a set of all tagged
instances with the chance attribute value.
First, we focus on the relation instances in the set
Smixed. The 1233 (= 372 + 133 + 140 + 588) in-
stances are tagged by more than one annotator, and
the 588 instances are tagged by all three annotators.
Next, we focus on the two different contrastive sets
of instances, Sn and S, The ratio of the instances
tagged by more than one annotator is small in S,
This becomes clear when we look at the bottom row
(“111”). While the 270 instances are tagged by all
three annotators in Sn, only the 64 instances are
tagged by all three annotators in S,
To statistically confirm this difference, we applied
the hypothesis test of the differences in population
rates. The null hypothesis is that the difference of
population rate is d %. As a result, the null hypoth-
esis was rejected at 0.01 significance level when d
was equal or less than 7 (p-value was equal or less
than 0.00805). In general, it can be assumed that if
a causal relation instance is recognized by many an-
notators, the instance is much reliable. Based on this
assumption and the results in Table 2, reliable in-
stances are more concentrated on the set of instances
with the necessity attribute value than those with the
chance attribute value.
</bodyText>
<sectionHeader confidence="0.997166" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.9933416">
In this section, we discuss some characteristics of
in-text causal relations and suggest some points for
developing the knowledge acquisition methods for
causal relations. Here, to guarantee the reliability
of the data used for the discussion, we focus on the
699 (= 230 + 92 + 107 + 270) instances marked by
more than one annotator with the necessity attribute
value. We examined the following three parts: (i)
cue phrase markers, (ii) the parts-of-speech of head
elements, and (iii) the positions of head elements.
</bodyText>
<tableCaption confidence="0.99163">
Table 3: The number of instances with/without cue
phrase markers
with marker 219
without marker 480
Table 4: Cue phrase markers marked by annotators
</tableCaption>
<figure confidence="0.9929619">
frequency
(because) 120
(by) 35
(result of) 5
(because) 5
(when) 5
(when) 4
(if) 4
(from) 4
(from) 3
</figure>
<subsectionHeader confidence="0.938575">
7.1 Cue phrase markers
</subsectionHeader>
<bodyText confidence="0.999899238095238">
While annotating the document articles with our
causal relation tags, head, mod, and causal rel, the
annotators also marked the cue phrase markers for
causal relations with the marker tag at the same
time. We investigated a proportion of instances at-
tached with the marker tag.
The result is shown in Table 3. Table 4 shows the
cue phrase markers actually marked by at least one
annotator 3.
It has been supposed that causal relation in-
stances are sometimes represented with no explicit
cue phrase marker. We empirically confirmed the
supposition. In our case, only 30% of our 699 in-
stances have one of cue phrase markers shown in
Table 4, though this value can be dependent of the
data.
This result suggests that in order to develop
knowledge acquisition methods for causal relations
with high coverage, we must deal with linguistic ex-
pressions with no explicit cue phrase markers as well
as those with cue phrase markers.
</bodyText>
<subsectionHeader confidence="0.998423">
7.2 The parts-of-speech of head elements
</subsectionHeader>
<bodyText confidence="0.999983166666667">
Next, we classified the events included in the 699
instances into two syntactic categories: the verb
phrase (VP) and the noun phrase (NP). To do this,
we used morphological information of their head el-
ements. If the part-of-speech of a head is verb or
adjective, the event is classified as a verb phrase. If
</bodyText>
<footnote confidence="0.832960333333333">
3The cue phrase markers whose frequencies are less than
three are not listed due to space limitation in Table 4.
marker
</footnote>
<page confidence="0.917201">
42
</page>
<figure confidence="0.98736736">
verb
VP
adjective
verbal noun
NP
general noun
others
e1 e2
365 412
322 269
12 18
# of bunsetsu phrases
400
350
300
250
200
150
100
50
0
e1 vp f
e1 np f
e1 vp c
e1 np c
</figure>
<tableCaption confidence="0.939243">
Table 5: The syntactic types
</tableCaption>
<bodyText confidence="0.999970875">
the part-of-speech of a head is noun (including gen-
eral noun and verbal noun), the event is classified
as a noun phrase. We used ChaSen 4 to get part-of-
speech information.
The result is shown in Table 5. More than half
events are classified as the VP. This matches our in-
tuition. However, the number of events classified as
the NP is comparable to the number of events clas-
sified as the VP; 322 events of e1 are represented as
noun phrases, and 269 events of e2 are also repre-
sented as noun phrases.
This result is quite suggestive. To promote the
current methods for knowledge acquisition to further
stage, we should develop a knowledge acquisition
framework applicable both to the verb phrases and
to the noun phrases.
</bodyText>
<subsectionHeader confidence="0.975311">
7.3 The positions of head elements
</subsectionHeader>
<bodyText confidence="0.999933705882353">
For each e1 and e2 included in the 699 instances,
we examined the positions of their head elements in
the sentences.
We consider dependency structures between bun-
setsu-phrases in the original sentences from which
causal relation instances are extracted. The depen-
dency structures form tree structures. The bunsetsu-
phrase located in the end of the sentence is the root
node of the tree. We focus on the depth of the head
element from the root node. We used CaboCha5 to
get dependency structure information between bun-
setsu-phrases.
The results are shown in Figure 3 and Figure 4.
Figure 3 is the result for the head elements of e1,
and Figure 4 is the result for the head elements of
e2. The letter “f” in Figure 3 and Figure 4 indicates
frequency at each position. Similarly, the letter “c”
</bodyText>
<footnote confidence="0.939037">
4Available from http://chasen.aist-nara.ac.
jp/hiki/ChaSen/.
5Available from http://chasen.org/˜taku/
software/cabocha/.
</footnote>
<figure confidence="0.987657">
0 2 4 6 8 10 12
depth
</figure>
<figureCaption confidence="0.999966">
Figure 4: The positions of head elements (e2)
</figureCaption>
<bodyText confidence="0.993761642857143">
indicates cumulative frequency.
In Figure 4, the 198 head elements of the events
represented as a verb phrase are located in the end
of the sentences, namely depth = 0. The 190 of
the 269 events represented as a noun phrase are lo-
cated in depth = 1. For events represented as either
a verb phrase or a noun phrase, over 80% of head
elements of the events are located within depth &lt; 3.
In Figure 3, similarly, over 80% of head elements of
the events are located within depth &lt; 4.
These findings suggest that the most of the events
are able to be found simply by searching the bun-
setsu-phrases located in the shallow position at the
phase of causal knowledge acquisition.
</bodyText>
<subsectionHeader confidence="0.967223">
7.4 Relative positions of two head elements
</subsectionHeader>
<bodyText confidence="0.999794">
Finally, we examined relative positions between
head elements of e1 and e2 where these two events
are held in a causal relation. In Section 7.3, we
discussed each absolute position for e1 and e2 by
means of the notion of depth in sentences. Here, we
focus on the difference (D) of the depth values be-
tween e1 and e2.
The result is shown in Table 6. The symbol “e1�
e2” in Table 6 indicates the case where the head ele-
ment of e1 is located nearer to the beginning of the
</bodyText>
<figureCaption confidence="0.998781">
Figure 3: The positions of head elements (e1)
</figureCaption>
<figure confidence="0.996721705882353">
450
400
350
300
250
200
150
100
50
0
0 2 4 6 8 10 12
depth
# of bunsetsu phrases
e2 vp f
e2 np f
e2 vp c
e2 np c
</figure>
<page confidence="0.999621">
43
</page>
<tableCaption confidence="0.996774">
Table 6: Relative positions of two head elements
</tableCaption>
<figure confidence="0.6031184">
e1==&gt;&apos; e2 e2==&gt;&apos; e1
259 15
intra-sentential
72
inter-sentential 141
</figure>
<bodyText confidence="0.938661347826087">
sentence than that of e2. The “e2==&gt;&apos; e1” indicates the
opposite case. The symbol “no dep” indicates the
case where neither the condition a nor b is satisfied:
a. the head element of e2 is an ancestor of the
head element of e1.
b. the head element of e2 is a descendant of the
head element of e1.
The symbol “inter-sentential” indicates the case
where two head elements appear in different sen-
tences.
The most instances 259 instances are catego-
rized into D = 1 on e1==&gt;- e2, that is, the head ele-
ment of e1 directly depends on the head element of
e2. This result matches our intuition. However, there
are several other cases. For example, 152 instances
are categorized into D = 2 on e1==&gt;. e2, 72 instances
are categorized into “no dep”. Most of the instances
extracted from sentences including any parallel re-
lations are categorized into “no dep”. In this study,
we consider causal relation instances as binary re-
lation. To deal with instances categorized into “no
dep” adequately, we should extend our framework
to the more complex structure.
</bodyText>
<sectionHeader confidence="0.998288" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999990375">
We reported our causal relation tags and the anno-
tation workflow. Using the annotated corpus, we
examined the causal relation instances in Japanese
text. From our investigation, it became clear that
what amount of causal relation instances are present,
where these relation instances are present, and
which types of linguistic expressions are used for
expressing these relation instances in text.
</bodyText>
<sectionHeader confidence="0.989184" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9969324">
This research is supported by the 21COE Program
“Framework for Systematization and Application of
Large-Scale Knowledge Resources” and the Grant-
in-Aid for Creative Basic Research (13NP0301)
“Language Understanding and Action Control”. We
would like to express our special thanks to Junji
Etoh, Yoshiko Ueda, Noriko Sogoh, and Tetsuro
Takahashi for helping us to create our corpus. We
are grateful to the reviewers for their suggestive
comments.
</bodyText>
<sectionHeader confidence="0.99916" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9997494">
B. Altenberg. 1984. Causal linking in spoken and written
English. Studia Linguistica, 38:1.
R. Girju. 2003. Automatic detection of causal relations
for question answering. In Proc. of the 41st ACL,
Workshop on Multilingual Summarization and Ques-
tion Answering.
T. Inui. 2004. Acquiring causal knowledge from text us-
ing connective markers. Ph.D. thesis, Graduate School
of Information Science, Nara Institute of Science and
Technology.
Y. Liu. 2004. Semantics and usages of connec-
tives for causal relations in modern Japanese - cases
of ’dakara’, ’sitagatte’, ’soreyue(ni)’, ’sonokekka’,
’sonotame(ni)’ -. Ph.D. thesis, The Graduate School
of Languages and Cultures, Nagoya University.
Mainichi. 1995. Mainichi Shimbun CD-ROM version.
D. Marcu. 1997. The rhetorical parsing, summarization,
and generation of natural language texts. Ph.D. the-
sis, Department of Computer Science, University of
Toronto.
E. Miltsakaki, R. Prasad, A. Joshi, and B. Webber. 2004.
Annotating discourse connectives and their arguments.
In Proc. of the HLT/NAACL Workshop on Frontiers in
Corpus Annotation.
S. Nishizawa and Y. Nakagawa. 1997. A method of dis-
course structure understanding in Japanese task-free
conversation for causal conjuction. Natural Language
Processing, 4(4):61–72. in Japanese .
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: A corpus annotated with semantic
roles. Computational Linguistics Journal, 31(1).
J. Pustejovsky, J. M. Casta˜no, R. Ingria, R. Sauri, R. J.
Gaizauskas, A.Setzer, G. Katz, and D. R. Radev. 2003.
TimeML: Robust specification of event and temporal
expressions in text. In New Directions in Question An-
swering, pages 28–34.
H. Sato, K. Kasahara, and K. Matsuzawa. 1999.
Rertrieval [sic] of simplified causal knowledge in text
and its application. In Technical report of IEICE,
Thought and Language. in Japanese .
</reference>
<figure confidence="0.9907795">
D = 1
= 2
&gt; 2
no dep
152 23
33 4
</figure>
<page confidence="0.982547">
44
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.588194">
<title confidence="0.838413333333333">Investigating the Characteristics of Causal Relations in Japanese Text Inui Precision and Intelligence</title>
<affiliation confidence="0.916161">Tokyo Institute of</affiliation>
<address confidence="0.97445">4259, Nagatsuta, Midori-ku, Yokohama, 226-8503,</address>
<abstract confidence="0.999618428571429">We investigated of the characteristics of in-text causal relations. We designed causal relation tags. With our designed tag set, three annotators annotated 750 Japanese newspaper articles. Then, using the annotated corpus, we investigated the causal relation instances from some viewpoints. Our quantitative study shows that what amount of causal relation instances are present, where these relation instances are present, and which types of linguistic expressions are used for expressing these relation instances in text.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B Altenberg</author>
</authors>
<title>Causal linking in spoken and written English. Studia Linguistica,</title>
<date>1984</date>
<pages>38--1</pages>
<contexts>
<context position="5223" citStr="Altenberg (1984)" startWordPosition="791" endWordPosition="792">n of English connectives including “because” and “since” for implementation of rhetorical parsing. However, although Marcu’s study was quantitative one, Marcu treated only explicit linguistic expressions with connectives. In the Timebank corpus (Pustejovsky et al., 2003), the causal relation information is included. However, the information is optional for implicit linguistic expressions. Although both explicit expressions and implicit expressions are treated in the Penn Discourse Treebank (PDTB) corpus (Miltsakaki et al., 2004), no information on causal relations is contained in this corpus. Altenberg (1984) investigated the frequency distribution of causal relation instances from some viewpoints such as document style and the syntactic form in English dialog data. Nishizawa (1997) also conducted a similar work using Japanese dialog data. Some parts of their viewpoints are overlapping with ours. However, while their studies focused on dialog data, our target is text documents. In fact, Altenberg treated also English text documents. However, our focus in this work is Japanese. 3 Annotated information 3.1 Causal relation tags We use three tags head, mod, and causal rel to represent the basic causal</context>
</contexts>
<marker>Altenberg, 1984</marker>
<rawString>B. Altenberg. 1984. Causal linking in spoken and written English. Studia Linguistica, 38:1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Girju</author>
</authors>
<title>Automatic detection of causal relations for question answering.</title>
<date>2003</date>
<booktitle>In Proc. of the 41st ACL, Workshop on Multilingual Summarization and Question Answering.</booktitle>
<contexts>
<context position="1102" citStr="Girju, 2003" startWordPosition="146" endWordPosition="147">ted corpus, we investigated the causal relation instances from some viewpoints. Our quantitative study shows that what amount of causal relation instances are present, where these relation instances are present, and which types of linguistic expressions are used for expressing these relation instances in text. 1 Introduction For many applications of natural language techniques such as question-answering systems and dialogue systems, acquiring knowledge about causal relations is one central issue. In recent researches, some automatic acquisition methods for causal knowledge have been proposed (Girju, 2003; Sato et al., 1999; Inui, 2004). They have used as knowledge resources a large amount of electric text documents: newspaper articles and Web documents. To realize their knowledge acquisition methods accurately and efficiently, it is important to knowing the characteristics of presence of in-text causal relations. However, while the acquisition methods have been improved by some researches, the characteristics of presence of in-text causal relations are still unclear: we have no empirical study about what amount of causal relation instances exist in text and where in text causal relation insta</context>
</contexts>
<marker>Girju, 2003</marker>
<rawString>R. Girju. 2003. Automatic detection of causal relations for question answering. In Proc. of the 41st ACL, Workshop on Multilingual Summarization and Question Answering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Inui</author>
</authors>
<title>Acquiring causal knowledge from text using connective markers.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>Graduate School of Information Science, Nara Institute of Science and Technology.</institution>
<contexts>
<context position="1134" citStr="Inui, 2004" startWordPosition="152" endWordPosition="153">causal relation instances from some viewpoints. Our quantitative study shows that what amount of causal relation instances are present, where these relation instances are present, and which types of linguistic expressions are used for expressing these relation instances in text. 1 Introduction For many applications of natural language techniques such as question-answering systems and dialogue systems, acquiring knowledge about causal relations is one central issue. In recent researches, some automatic acquisition methods for causal knowledge have been proposed (Girju, 2003; Sato et al., 1999; Inui, 2004). They have used as knowledge resources a large amount of electric text documents: newspaper articles and Web documents. To realize their knowledge acquisition methods accurately and efficiently, it is important to knowing the characteristics of presence of in-text causal relations. However, while the acquisition methods have been improved by some researches, the characteristics of presence of in-text causal relations are still unclear: we have no empirical study about what amount of causal relation instances exist in text and where in text causal relation instances tend to appear. In this wor</context>
</contexts>
<marker>Inui, 2004</marker>
<rawString>T. Inui. 2004. Acquiring causal knowledge from text using connective markers. Ph.D. thesis, Graduate School of Information Science, Nara Institute of Science and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Liu</author>
</authors>
<title>Semantics and usages of connectives for causal relations in modern Japanese - cases of ’dakara’, ’sitagatte’, ’soreyue(ni)’, ’sonokekka’, ’sonotame(ni)’ -.</title>
<date>2004</date>
<booktitle>Ph.D. thesis, The Graduate School of Languages and Cultures,</booktitle>
<location>Nagoya University. Mainichi.</location>
<contexts>
<context position="4311" citStr="Liu (2004)" startWordPosition="661" endWordPosition="662">causal relations. Researchers have resorted to annotators’ subjective judgements. Our criteria are represented in the form of linguistic templates which the annotators apply in making their judgements (see Section 3.2). In Section 2, we will outline several previous research efforts on in-text causal relations. In Section 3 to Section 6, we will describe the details of the design of our causal relation tags and the annotation workflow. In Section 7, using the annotated corpus, we will then discuss the results for the investigation of characteristics of in-text causal relations. 2 Related work Liu (2004) analyzed the differences of usages of some Japanese connectives marking causal relations. The results are useful for accounting for an appropriate connective for each context within the documents. However Liu conducted no quantitative studies. Marcu (1997) investigated the frequency distribution of English connectives including “because” and “since” for implementation of rhetorical parsing. However, although Marcu’s study was quantitative one, Marcu treated only explicit linguistic expressions with connectives. In the Timebank corpus (Pustejovsky et al., 2003), the causal relation information</context>
</contexts>
<marker>Liu, 2004</marker>
<rawString>Y. Liu. 2004. Semantics and usages of connectives for causal relations in modern Japanese - cases of ’dakara’, ’sitagatte’, ’soreyue(ni)’, ’sonokekka’, ’sonotame(ni)’ -. Ph.D. thesis, The Graduate School of Languages and Cultures, Nagoya University. Mainichi. 1995. Mainichi Shimbun CD-ROM version.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
</authors>
<title>The rhetorical parsing, summarization, and generation of natural language texts.</title>
<date>1997</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science, University of Toronto.</institution>
<contexts>
<context position="4568" citStr="Marcu (1997)" startWordPosition="698" endWordPosition="699">al previous research efforts on in-text causal relations. In Section 3 to Section 6, we will describe the details of the design of our causal relation tags and the annotation workflow. In Section 7, using the annotated corpus, we will then discuss the results for the investigation of characteristics of in-text causal relations. 2 Related work Liu (2004) analyzed the differences of usages of some Japanese connectives marking causal relations. The results are useful for accounting for an appropriate connective for each context within the documents. However Liu conducted no quantitative studies. Marcu (1997) investigated the frequency distribution of English connectives including “because” and “since” for implementation of rhetorical parsing. However, although Marcu’s study was quantitative one, Marcu treated only explicit linguistic expressions with connectives. In the Timebank corpus (Pustejovsky et al., 2003), the causal relation information is included. However, the information is optional for implicit linguistic expressions. Although both explicit expressions and implicit expressions are treated in the Penn Discourse Treebank (PDTB) corpus (Miltsakaki et al., 2004), no information on causal </context>
</contexts>
<marker>Marcu, 1997</marker>
<rawString>D. Marcu. 1997. The rhetorical parsing, summarization, and generation of natural language texts. Ph.D. thesis, Department of Computer Science, University of Toronto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Miltsakaki</author>
<author>R Prasad</author>
<author>A Joshi</author>
<author>B Webber</author>
</authors>
<title>Annotating discourse connectives and their arguments.</title>
<date>2004</date>
<booktitle>In Proc. of the HLT/NAACL Workshop on Frontiers in Corpus Annotation.</booktitle>
<contexts>
<context position="5141" citStr="Miltsakaki et al., 2004" startWordPosition="777" endWordPosition="780">Liu conducted no quantitative studies. Marcu (1997) investigated the frequency distribution of English connectives including “because” and “since” for implementation of rhetorical parsing. However, although Marcu’s study was quantitative one, Marcu treated only explicit linguistic expressions with connectives. In the Timebank corpus (Pustejovsky et al., 2003), the causal relation information is included. However, the information is optional for implicit linguistic expressions. Although both explicit expressions and implicit expressions are treated in the Penn Discourse Treebank (PDTB) corpus (Miltsakaki et al., 2004), no information on causal relations is contained in this corpus. Altenberg (1984) investigated the frequency distribution of causal relation instances from some viewpoints such as document style and the syntactic form in English dialog data. Nishizawa (1997) also conducted a similar work using Japanese dialog data. Some parts of their viewpoints are overlapping with ours. However, while their studies focused on dialog data, our target is text documents. In fact, Altenberg treated also English text documents. However, our focus in this work is Japanese. 3 Annotated information 3.1 Causal relat</context>
</contexts>
<marker>Miltsakaki, Prasad, Joshi, Webber, 2004</marker>
<rawString>E. Miltsakaki, R. Prasad, A. Joshi, and B. Webber. 2004. Annotating discourse connectives and their arguments. In Proc. of the HLT/NAACL Workshop on Frontiers in Corpus Annotation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Nishizawa</author>
<author>Y Nakagawa</author>
</authors>
<title>A method of discourse structure understanding in Japanese task-free conversation for causal conjuction.</title>
<date>1997</date>
<journal>Natural Language Processing,</journal>
<volume>4</volume>
<issue>4</issue>
<note>in Japanese .</note>
<marker>Nishizawa, Nakagawa, 1997</marker>
<rawString>S. Nishizawa and Y. Nakagawa. 1997. A method of discourse structure understanding in Japanese task-free conversation for causal conjuction. Natural Language Processing, 4(4):61–72. in Japanese .</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>D Gildea</author>
<author>P Kingsbury</author>
</authors>
<title>The proposition bank: A corpus annotated with semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics Journal,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="5935" citStr="Palmer et al., 2005" startWordPosition="906" endWordPosition="909">uch as document style and the syntactic form in English dialog data. Nishizawa (1997) also conducted a similar work using Japanese dialog data. Some parts of their viewpoints are overlapping with ours. However, while their studies focused on dialog data, our target is text documents. In fact, Altenberg treated also English text documents. However, our focus in this work is Japanese. 3 Annotated information 3.1 Causal relation tags We use three tags head, mod, and causal rel to represent the basic causal relation information. Our annotation scheme for events is similar to that of the PropBank (Palmer et al., 2005). An event is regarded as consisting of a head element and some modifiers. The tags head and mod are used to represent an event which forms one part of the two events held in a causal relation. The tag causal rel is used to represent a causal relation between two annotated events. Figure 1 shows an example of attaching the causal relation information to the sentence (2a), in which a causal relation is held between two events indicated (2b) and (2c) . Hereafter, we denote the former (cause) part of event as e1 and the latter (effect) part of event as e2. (2) a. (As the Golden Week holidays come</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>M. Palmer, D. Gildea, and P. Kingsbury. 2005. The proposition bank: A corpus annotated with semantic roles. Computational Linguistics Journal, 31(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
<author>J M Casta˜no</author>
<author>R Ingria</author>
<author>R Sauri</author>
<author>R J Gaizauskas</author>
<author>G Katz A Setzer</author>
<author>D R Radev</author>
</authors>
<title>TimeML: Robust specification of event and temporal expressions in text.</title>
<date>2003</date>
<booktitle>In New Directions in Question Answering,</booktitle>
<pages>28--34</pages>
<marker>Pustejovsky, Casta˜no, Ingria, Sauri, Gaizauskas, Setzer, Radev, 2003</marker>
<rawString>J. Pustejovsky, J. M. Casta˜no, R. Ingria, R. Sauri, R. J. Gaizauskas, A.Setzer, G. Katz, and D. R. Radev. 2003. TimeML: Robust specification of event and temporal expressions in text. In New Directions in Question Answering, pages 28–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sato</author>
<author>K Kasahara</author>
<author>K Matsuzawa</author>
</authors>
<title>Rertrieval [sic] of simplified causal knowledge in text and its application.</title>
<date>1999</date>
<booktitle>In Technical report of IEICE, Thought and Language. in Japanese .</booktitle>
<contexts>
<context position="1121" citStr="Sato et al., 1999" startWordPosition="148" endWordPosition="151">e investigated the causal relation instances from some viewpoints. Our quantitative study shows that what amount of causal relation instances are present, where these relation instances are present, and which types of linguistic expressions are used for expressing these relation instances in text. 1 Introduction For many applications of natural language techniques such as question-answering systems and dialogue systems, acquiring knowledge about causal relations is one central issue. In recent researches, some automatic acquisition methods for causal knowledge have been proposed (Girju, 2003; Sato et al., 1999; Inui, 2004). They have used as knowledge resources a large amount of electric text documents: newspaper articles and Web documents. To realize their knowledge acquisition methods accurately and efficiently, it is important to knowing the characteristics of presence of in-text causal relations. However, while the acquisition methods have been improved by some researches, the characteristics of presence of in-text causal relations are still unclear: we have no empirical study about what amount of causal relation instances exist in text and where in text causal relation instances tend to appear</context>
</contexts>
<marker>Sato, Kasahara, Matsuzawa, 1999</marker>
<rawString>H. Sato, K. Kasahara, and K. Matsuzawa. 1999. Rertrieval [sic] of simplified causal knowledge in text and its application. In Technical report of IEICE, Thought and Language. in Japanese .</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>