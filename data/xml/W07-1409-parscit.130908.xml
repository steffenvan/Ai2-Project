<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.988657">
On the Role of Lexical and World Knowledge in RTE3
</title>
<author confidence="0.9936735">
Peter Clark1, William R. Murray1, John Thompson1, Phil Harrison1,
Jerry Hobbs2, Christiane Fellbaum3
</author>
<affiliation confidence="0.939606">
1Boeing Phantom Works, The Boeing Company, Seattle, WA 98124
</affiliation>
<address confidence="0.8746875">
2USC/ISI, 4676 Admiralty Way, Marina del Rey, CA 90292
3Princeton University, NJ 08544
</address>
<email confidence="0.9944225">
{peter.e.clark,william.r.murray,john.a.thompson,philip.harrison}@boeing.com,
hobbs@isi.edu, fellbaum@clarity.princeton.edu
</email>
<sectionHeader confidence="0.995625" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999957111111111">
To score well in RTE3, and even more so
to create good justifications for entailments,
substantial lexical and world knowledge is
needed. With this in mind, we present an
analysis of a sample of the RTE3 positive
entailment pairs, to identify where and
what kinds of world knowledge are needed
to fully identify and justify the entailment,
and discuss several existing resources and
their capacity for supplying that knowledge.
We also briefly sketch the path we are fol-
lowing to build an RTE system (Our im-
plementation is very preliminary, scoring
50.9% at the time of RTE). The contribu-
tion of this paper is thus a framework for
discussing the knowledge requirements
posed by RTE and some exploration of
how these requirements can be met.
</bodyText>
<sectionHeader confidence="0.999129" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.992883315789473">
The Pascal RTE site defines entailment between
two texts T and H as holding &amp;quot;if, typically, a hu-
man reading T would infer that H is most likely
true&amp;quot; assuming &amp;quot;common human understanding of
language as well as common background knowl-
edge.&amp;quot; While a few RTE3 entailments can be rec-
ognized using simple syntactic matching, the ma-
jority rely on significant amounts of this &amp;quot;common
human understanding&amp;quot; of lexical and world knowl-
edge. Our goal in this paper is to analyze what that
knowledge is, create a preliminary framework for
it, and explore a few available sources for it. In the
short term, such knowledge can be (and has been)
used to drive semantic matching of the T and H
dependency/parse trees and their semantic repre-
54
sentations, as many prior RTE systems perform,
e.g., (Hickl et al., 2006). In the long term, com-
puters should be able to perform deep language
understanding to build a computational model of
the scenario being described in T, to reason about
the entailment, answer further questions, and create
meaningful justifications. With this longer term
goal in mind, it is useful to explore the types of
knowledge required. It also gives a snapshot of the
kinds of challenges that RTE3 poses.
The scope of this paper is to examine the underly-
ing lexical/world knowledge requirements of RTE,
rather than the more syntactic/grammatical issues
of parsing, coreference resolution, named entity
recognition, punctuation, coordination, typographi-
cal errors, etc. Although there is a somewhat blurry
line between the two, this separation is useful for
bounding the analysis. It should be noted that the
more syntactic issues are themselves vast in RTE,
but here we will not delve into them. Instead, we
will perform a thought experiment in which they
have been handled correctly.
</bodyText>
<sectionHeader confidence="0.957905" genericHeader="introduction">
2 Analysis
</sectionHeader>
<bodyText confidence="0.999888875">
Based on an analysis of 100 (25%) of the positive
entailments in the RTE3 test set, we have divided
the knowledge requirements into several rough
categories, which we now present. We then sum-
marize the frequency with which examples in this
sample fell into these categories. The examples
below are fragments of the original test questions,
abbreviated and occasionally simplified.
</bodyText>
<subsectionHeader confidence="0.98486">
2.1 Syntactic Matching
</subsectionHeader>
<bodyText confidence="0.78431125">
In a few cases, entailment can be identified by syn-
tactic matching of T and H, for example:
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 54–59,
Prague, June 2007. @2007 Association for Computational Linguistics
</bodyText>
<listItem confidence="0.7477685">
489.T &amp;quot;The Gurkhas come from Nepal and...”
489.H &amp;quot;The Gurkhas come from Nepal.&amp;quot;
</listItem>
<bodyText confidence="0.998846">
Other examples include 299, 489, and 456. In
some cases, the syntactic matching can be very
complex, e.g., examples 152, 724.
</bodyText>
<subsectionHeader confidence="0.989372">
2.2 Synonyms
</subsectionHeader>
<footnote confidence="0.77118">
Synonymy is often needed to recognize entailment,
648.T &amp;quot;...go through ... licencing procedures...&amp;quot;
648.H &amp;quot;...go through the licencing processes.&amp;quot;
</footnote>
<bodyText confidence="0.888683333333333">
Other examples include 286 (&amp;quot;dismiss&amp;quot;/&amp;quot;throw
out&amp;quot;), 37 (&amp;quot;begin”/&amp;quot;start&amp;quot;), 236 (&amp;quot;wildfire&amp;quot;/&amp;quot;bush
fire&amp;quot;), and, arguably, 462 (&amp;quot;revenue&amp;quot;/&amp;quot;proceeds&amp;quot;).
</bodyText>
<subsectionHeader confidence="0.996034">
2.3 Generalizations (Hypernyms)
</subsectionHeader>
<bodyText confidence="0.999907666666667">
Similarly, subsumption (generalization) relation-
ships between word senses need to be recognized
(whether or not a fixed set of senses are used), eg.
</bodyText>
<footnote confidence="0.842214">
148.T &amp;quot;Beverly served...at WEDCOR&amp;quot;
148.H &amp;quot;Beverly worked for WEDCOR.&amp;quot;
</footnote>
<bodyText confidence="0.987044">
Others include 178 (&amp;quot;succumbed&amp;quot; as a kind of
&amp;quot;killed&amp;quot;), and 453 (&amp;quot;take over&amp;quot; as a kind of &amp;quot;buy&amp;quot;).
</bodyText>
<subsectionHeader confidence="0.850798">
2.4 Noun Redundancy
Sometimes a noun in a compound can be dropped:
</subsectionHeader>
<bodyText confidence="0.924692666666667">
607.T &amp;quot;single-run production process...&amp;quot;
607.H &amp;quot;Single-run production...&amp;quot;
Other examples include 269 (&amp;quot;increasing preva-
lence of&amp;quot; → &amp;quot;increasing&amp;quot;), 604 (&amp;quot;mini-mill proc-
ess&amp;quot; → &amp;quot;mini-mill&amp;quot;), and (at the phrase level) 668
(&amp;quot;all segments of the public&amp;quot; → &amp;quot;the public&amp;quot;).
</bodyText>
<subsectionHeader confidence="0.948876">
2.5 Noun-Verb Relations
</subsectionHeader>
<bodyText confidence="0.939698555555556">
Often derivationally related nouns and verbs occur
in the pairs. To identify and justify the entailment,
the relationship and its nature is needed, as in:
480 &amp;quot;Marquez is a winner...&amp;quot; →&amp;quot;Marquez won...&amp;quot;
Other examples include 286 (&amp;quot;pirated&amp;quot;, &amp;quot;piracy&amp;quot;),
and 75 (&amp;quot;invent&amp;quot;, &amp;quot;invention&amp;quot;). In some cases, the
deverbal noun denotes the verb&apos;s event, in other
cases it denotes one of the verb’s arguments (e.g.,
&amp;quot;winner&amp;quot; as the subject/agent of a &amp;quot;win&amp;quot; event).
</bodyText>
<subsectionHeader confidence="0.980666">
2.6 Compound Nouns
</subsectionHeader>
<bodyText confidence="0.92861225">
Some examples require inferring the semantic rela-
tion between nouns in a compound, e.g.,
168 &amp;quot;Sirius CEO Karmazin&amp;quot; → &amp;quot;Karmazin is an
executive of Sirius&amp;quot;
583 &amp;quot;physicist Hawking&amp;quot; → &amp;quot;Hawking is a physi-
cist&amp;quot;
In some cases this is straightforward, others require
more detailed knowledge of the entities involved.
</bodyText>
<subsectionHeader confidence="0.980694">
2.7 Definitions
</subsectionHeader>
<bodyText confidence="0.983883214285714">
Although there is somewhat of a fuzzy boundary
between word and world knowledge, we draw this
distinction here. Some examples of RTE pairs
which require knowing word meanings are:
667 &amp;quot;... found guilty...&amp;quot; → &amp;quot;...convicted...&amp;quot;
328 &amp;quot;sufferers of coeliac disease...&amp;quot; → &amp;quot;coeliacs...&amp;quot;
The second example is particularly interesting as
many readers (and computers) will not have en-
countered the word &amp;quot;coeliacs&amp;quot; before, yet a person
can reasonably infer its meaning on the fly from
context and morphology - something challenging
for a machine to do. Definitions of compound
nouns are also sometimes needed, e.g., “family
planning” (612) and “cough syrup” (80).
</bodyText>
<subsectionHeader confidence="0.992512">
2.8 World Knowledge: General
</subsectionHeader>
<bodyText confidence="0.938631">
A large number of RTE pairs require non-
definitional knowledge about the way the world
(usually) is, e.g.,:
273 &amp;quot;bears kill people&amp;quot; → &amp;quot;bears attack people&amp;quot;
People recognize this entailment as they know
(have heard about) how people might be killed by
a bear, and hence can justify why the entailment is
valid. (They know that the first step in the bear
killing a person is for the bear to attack that person.)
Some other examples are:
499 &amp;quot;shot dead&amp;quot; → &amp;quot;murder&amp;quot;
705 &amp;quot;under a contract with&amp;quot; → &amp;quot;cooperates with&amp;quot;
721 &amp;quot;worked on the law&amp;quot; → &amp;quot;discussed the law&amp;quot;
731 &amp;quot;cut tracts of forest&amp;quot; → &amp;quot;cut trees in the forest&amp;quot;
732 &amp;quot;establishing tree farms&amp;quot;
→ &amp;quot;trees were planted&amp;quot;
</bodyText>
<listItem confidence="0.8210466">
639 &amp;quot;X&apos;s plans for reorganizing&amp;quot;
→ &amp;quot;X intends to reorganize&amp;quot;
328 &amp;quot;the diets must be followed by &lt;person&gt;&amp;quot;
→ &amp;quot;the diets are for &lt;person&gt;&amp;quot;
722 X and Y vote for Z → X and Y agree to Z.
</listItem>
<bodyText confidence="0.9533535">
All these cases appeal to a person&apos;s world knowl-
edge concerning the situation being described.
</bodyText>
<page confidence="0.991836">
55
</page>
<bodyText confidence="0.99467475">
though these verbs are all strongly associated with
words in T. For a computer to fully explain the
entailment, it would need similar knowledge.
As a second example, consider:
</bodyText>
<subsectionHeader confidence="0.707703">
2.9 World Knowledge: Core Theories
</subsectionHeader>
<bodyText confidence="0.99993275">
In addition to this more specific knowledge of the
world, some RTE examples appeal to more general,
fundamental knowledge (e.g., space, time, plans,
goals). For example
</bodyText>
<equation confidence="0.665072">
6.T &amp;quot;Yunupingu is one of the clan of...&amp;quot;
6.H &amp;quot;Yunupingu is a member of...&amp;quot;
</equation>
<bodyText confidence="0.74155375">
appeals to a basic rule of set inclusion, and 10 (a
negative entailment: &amp;quot;unsuccessfully sought elec-
tion&amp;quot; → not elected) appeals to core notions of
goals and achievement. Several examples appeal to
core spatial knowledge, e.g.:
491.T &amp;quot;...come from the high mountains of Nepal.&amp;quot;
491.H &amp;quot;...come from Nepal.&amp;quot;
178.T &amp;quot;...3 people in Saskatchewan succumbed to
the storm.&amp;quot;
178.H &amp;quot;...a storm in Saskatchewan.&amp;quot;
491 appeals to regional inclusion (if X location Y,
and Y is in Z, then X location Z), and 178 appeals
to colocation (if X is at Y, and X physically inter-
acts with Z, then Z is at Y). Other spatial examples
include 236 (&amp;quot;around Sydney&amp;quot; → &amp;quot;near Sydney&amp;quot;),
and 129 (&amp;quot;invasion of&amp;quot; → &amp;quot;arrived in&amp;quot;).
</bodyText>
<subsectionHeader confidence="0.431155">
2.10 World Knowledge: Frames and Scripts
</subsectionHeader>
<bodyText confidence="0.98801">
Although loosely delineated, another category of
world knowledge concerns stereotypical places,
situations and the events which occur in them, with
various representational schemes proposed in the
AI literature, e.g., Frames (Minsky 1985), Scripts
(Schank 1983). Some RTE examples require rec-
ognizing the implicit scenario (&amp;quot;frame&amp;quot;, &amp;quot;script&amp;quot;,
etc.) which T describes to confirm the new facts or
relationships introduced in H are valid. A first ex-
ample is:
</bodyText>
<listItem confidence="0.735071">
358.T &amp;quot;Kiesbauer was target of a letter bomb...&amp;quot;
358.H &amp;quot;A letter bomb was sent to Kiesbauer.&amp;quot;
</listItem>
<bodyText confidence="0.971004594594595">
A person recognizes H as entailed by T because
he/she knows the &amp;quot;script&amp;quot; for letter bombing,
which includes sending the bomb in the mail. Thus
a person could also recognize alternative verbs in
358.H as valid (e.g., &amp;quot;mailed&amp;quot;, &amp;quot;delivered&amp;quot;) or in-
valid (e.g., &amp;quot;thrown at&amp;quot;, &amp;quot;dropped on&amp;quot;), even
538.T &amp;quot;...the O. J. Simpson murder trial...&amp;quot;
538.H &amp;quot;O. J. Simpson was accused of murder.&amp;quot;
Again, this requires knowing about trials: That
they involve charges, a defendant, an accusation,
etc., in order to validate H as an entailed expansion
of T. In this example, there is also a second twist to
it as the noun phrase in 538.T equally supports the
hypothesis &amp;quot;O. J. Simpson was murdered.&amp;quot; (e.g.,
consider replacing &amp;quot;O. J.&amp;quot; with &amp;quot;Nicole&amp;quot;). It is only
a reference elsewhere in the T sentence to &amp;quot;Simp-
son&apos;s attorneys&amp;quot; that suggests Simpson is still alive,
and hence couldn&apos;t have been the victim, and hence
must be the accused, that clarifies 538.H as being
correct, a highly complex chain of reasoning.
As a third example, consider:
736.T &amp;quot;In a security fraud case, Milken was sen-
tenced to 10 years in prison.&amp;quot;
736.H &amp;quot;Milken was imprisoned for security fraud.&amp;quot;
This example is particularly interesting, as one
needs to recognize security fraud as Milken&apos;s crime,
a connection which not stated in T. A human
reader will recognize the notion of sentencing, and
thus expect to see a convict and his/her crime, and
hence can align these expectations with T, validat-
ing H. Thus again, deep knowledge of sentencing
is needed to understand and justify the entailment.
Some other examples requiring world knowledge
to validate their expansions, include 623 (&amp;quot;narcot-
ics-sniffing dogs&amp;quot; → &amp;quot;dogs are used to sniff out
narcotics&amp;quot;), and 11 (&amp;quot;the Nintendo release of the
game&amp;quot; → &amp;quot;the game is produced by Nintendo&amp;quot;).
</bodyText>
<sectionHeader confidence="0.76885" genericHeader="method">
2.11 Implicative Verbs
</sectionHeader>
<bodyText confidence="0.998933666666667">
Some RTE3 examples contain complement-taking
verbs that make an implication (either positive or
negative) about the complement. For example:
</bodyText>
<footnote confidence="0.97231725">
668 &amp;quot;A survey shows that X...&amp;quot; → &amp;quot;X...&amp;quot;
657 &amp;quot;...X was seen...&amp;quot; → &amp;quot;...X...&amp;quot;
725 “...decided to X...&amp;quot; → &amp;quot;...X...&amp;quot;
716 &amp;quot;...have been unable to X...&amp;quot; → &amp;quot;...do not X&amp;quot;
</footnote>
<page confidence="0.996623">
56
</page>
<bodyText confidence="0.931059461538462">
In the first 3 the implication is positive, but in the
last the implication is negative. (Nairn et al, 2006)
provide a detailed analysis of this type of behavior.
In fact, this notion of implicature (one part of a
sentence making an implication about another part)
extends beyond single verbs, and there are some
more complex examples in RTE3, e.g.:
453 &amp;quot;...won the battle to X...&amp;quot; --+ &amp;quot;...X...&amp;quot;
784.T &amp;quot;X reassures Russia it has nothing to fear...&amp;quot;
784.11 &amp;quot;Russia fears...&amp;quot;
In this last example the implication behavior is
quite complex: (loosely) If X reassures Y of Z,
then Y is concerned about not-Z.
</bodyText>
<subsectionHeader confidence="0.548739">
2.12 Metonymy/Transfer
</subsectionHeader>
<bodyText confidence="0.98883025">
In some cases, language allows us to replace a
word (sense) with a closely related word (sense):
708.T &amp;quot;Revenue from stores funded...&amp;quot;
708.11 &amp;quot;stores fund...&amp;quot;
Rules for metonymic transfer, e.g., (Fass 2000),
can be used to define which transfers are allowed.
Another example is 723 “...pursue its drive to-
wards X” --+ ”...pursue X”.
</bodyText>
<subsectionHeader confidence="0.507963">
2.13 Idioms/Protocol/Slang
</subsectionHeader>
<bodyText confidence="0.999188333333333">
Finally, some RTE pairs rely on understanding
idioms, slang, or special protocols used in lan-
guage, for example:
</bodyText>
<listItem confidence="0.8785126">
12 &amp;quot;Drew served as Justice. Kennon returned to
claim Drew&apos;s seat&amp;quot; --+ &amp;quot;Kennon served as Justice.&amp;quot;
486 &amp;quot;name, 1890-1970&amp;quot; --+ &amp;quot;name died in 1970&amp;quot;
408 &amp;quot;takes the title of&amp;quot; --+ &amp;quot;is&amp;quot;
688 &amp;quot;art finds its way back&amp;quot; --+ &amp;quot;art gets returned&amp;quot;
</listItem>
<bodyText confidence="0.99995025">
The phrases in these examples all have special
meaning which cannot be easily derived composi-
tionally from their words, and thus require special
handling within an entailment system.
</bodyText>
<subsectionHeader confidence="0.554811">
2.14 Frequency Statistics
</subsectionHeader>
<bodyText confidence="0.995948">
Table 1 shows the number of positive entailments
in our sample of 100 that fell into the different
categories (some fell into several). While there is a
certain subjectivity in the boundaries of the catego-
</bodyText>
<tableCaption confidence="0.9453695">
Table 1: Frequency of different entailment
phenomena from a sample of 100 RTE3 pairs.
</tableCaption>
<bodyText confidence="0.999954083333333">
ries, the most significant observation is that very
few entailments depend purely on syntactic ma-
nipulation and simple lexical knowledge (syno-
nyms, hypernyms), and that the vast majority of
entailments require significant world knowledge,
highlighting one of the biggest challenges of RTE.
In addition, the category of general world knowl-
edge -- small, non-definitional facts about the way
the world (usually) is -- is the largest, suggesting
that harvesting and using this kind of knowledge
should continue to be a priority for improving per-
formance on RTE-style tasks.
</bodyText>
<sectionHeader confidence="0.93502" genericHeader="method">
3 Sources of World Knowledge
</sectionHeader>
<bodyText confidence="0.99965625">
While there are many potential sources of the
knowledge that we have identified, we describe
three in a bit more detail and how they relate to the
earlier analysis.
</bodyText>
<subsectionHeader confidence="0.992229">
3.1 WordNet
</subsectionHeader>
<bodyText confidence="0.999949571428571">
WordNet (Fellbaum, 1998) is one of the most ex-
tensively used resources in RTE already and in
computational linguistics in general. Despite some
well-known problems, it provides broad coverage
of several key relationships between word senses
(and words), in particular for synonyms, hy-
pernyms (generalizations), meronyms (parts), and
semantically (“morphosemantically”) related
forms. From the preceding analysis, WordNet does
contain the synonyms {&amp;quot;procedure&amp;quot;,&amp;quot;process&amp;quot;},
{&amp;quot;dismiss&amp;quot;,&amp;quot;throw out&amp;quot;}, {&amp;quot;begin&amp;quot;,&amp;quot;start&amp;quot;}, but
does not contain the compound &amp;quot;wild fire&amp;quot; and
(strictly correctly) does not contain {&amp;quot;reve-
nue&amp;quot;,&amp;quot;proceeds&amp;quot;} as synonyms. In addition, the
</bodyText>
<page confidence="0.996466">
57
</page>
<bodyText confidence="0.999878607142857">
three hypernyms mentioned in the earlier analysis
are in WordNet. WordNet also links (via the “mor-
phosemantic” link) the 3 noun-verb pairs men-
tioned earlier (win/winner, pirated/piracy, in-
vent/invention) – however it does not currently
distinguish the nature of that link (e.g., agent, re-
sult, event). WordNet is currently being expanded
to include this information, as part of the
AQUAINT program.
Two independently developed versions of the
WordNet glosses expressed in logic are also avail-
able: Extended WordNet (Moldovan and Rus,
2001) and a version about to be released with
WordNet3.0 (again developed under AQUAINT).
These in principle can help with definitional
knowledge. From our earlier analysis, it turns out
&amp;quot;convicted&amp;quot; is conveniently defined in WordNet as
&amp;quot;pronounced or proved guilty&amp;quot; potentially bridging
the gap for pair 667, although there are problems
with the logical interpretation of this particular
gloss in both resources mentioned. WordNet does
have &amp;quot;coeliac&amp;quot;, but not in the sense of a person
with coeliac disease1.
In addition, several existing “core theories” (e.g.,
TimeML, IKRIS) that can supply some of the fun-
damental knowledge mentioned earlier (e.g., space,
time, goals) are being connected to WordNet under
the AQUAINT program.
</bodyText>
<subsectionHeader confidence="0.998633">
3.2 The DIRT Paraphrase Database
</subsectionHeader>
<bodyText confidence="0.99904025">
Paraphrases have been used successfully by several
RTE systems (e.g., Hickl et al., 2005). With re-
spect to our earlier analysis, we examined Lin and
Pantel&apos;s (2001) paraphrase database built with their
system DIRT, containing 12 million entries. While
there is of course noise in the database, it contains
a remarkable amount of sensible world knowledge
as well as syntactic rewrites, albeit encoded as
shallow rules lacking word senses.
Looking at the examples from our earlier analysis
of general world knowledge, we find that three are
supported by paraphrase rules in the database:
</bodyText>
<footnote confidence="0.9511554">
273: X kills Y → X attacks Y
499: X shoots Y → X murders Y
1 This seems to be an accidental gap; WordNet contains
many interlinked disease-patient noun pairs, incl. &amp;quot;dia-
betes-diabetic,&amp;quot; &amp;quot;epilepsy-eplileptic,&amp;quot; etc.
</footnote>
<note confidence="0.510186">
721: X works on Y → X discusses Y
And one that could be is not, namely:
</note>
<equation confidence="0.5349305">
705: X is under a contract with Y → X coop-
erates with Y (not in the database)
</equation>
<bodyText confidence="0.999023333333333">
Other examples are outside the scope of DIRT&apos;s
approach (i.e., “X pattern1 Y” → “X pattern2 Y”),
but nonetheless the coverage is encouraging.
</bodyText>
<subsectionHeader confidence="0.998004">
3.3 FrameNet
</subsectionHeader>
<bodyText confidence="0.999975407407408">
In our earlier analysis, we identified knowledge
about stereotypical situations and their events as
important for RTE. FrameNet (Baker et al, 1998)
attempts to encode this knowledge. FrameNet was
used with some success in RTE2 by Burchardt and
Frank (2005). FrameNet&apos;s basic unit - a Frame - is
a script-like conceptual schema that refers to a
situation, object, or event along with its partici-
pants (Frame Elements), identified independent of
their syntactic configuration.
We earlier discussed how 538.T &amp;quot;...the O. J. Simp-
son murder trial...&amp;quot; might entail 538.H &amp;quot;O. J. Simp-
son was accused of murder.&amp;quot; This case applies to
FrameNet’s Trial frame, which includes the Frame
Elements Defendant and Charges, with Charges
being defined as &amp;quot;The legal label for the crime that
the Defendant is accused of.&amp;quot;, thus stating the rela-
tionship between the defendant and the charges,
unstated in T but made explicit in H, validating the
entailment. However, the lexical units instantiat-
ing the Frame Elements are not yet disambiguated
against a lexical database, limiting full semantic
understanding. Moreover, FrameNet&apos;s world
knowledge is stated informally in English descrip-
tions, though it may be possible to convert these to
a more machine-processable form either manually
or automatically.
</bodyText>
<subsectionHeader confidence="0.908766">
3.4 Other Resources
</subsectionHeader>
<bodyText confidence="0.998302">
We have drawn attention to these three resources
as they provide some answers to the requirements
identified earlier. Several other publicly available
resources could also be of use, including VerbNet
(Univ Colorado at Boulder), the Component Li-
brary (Univ Texas at Austin), OpenCyc (Cycorp),
SUMO, Stanford&apos;s additions to WordNet, and the
Tuple Database (Boeing, following Schubert&apos;s
2002 approach), to name but a few.
</bodyText>
<page confidence="0.99686">
58
</page>
<sectionHeader confidence="0.706252" genericHeader="method">
4 Sketch of our RTE System
</sectionHeader>
<bodyText confidence="0.965457625">
Although not the primary purpose of this paper, we
briefly sketch the path we are following to build an
RTE system able to exploit the above resources.
Our implementation is very preliminary, scoring
50.9% at the time of RTE and 52.6% now (55.0%
on the 525 examples it is able to analyze, guessing
&amp;quot;no entailment&amp;quot; for the remainder). Nevertheless,
the following shows the direction we are moving in
Like several other groups, our basic approach is to
generate logic for the T and H sentences, and then
explore the application of inference rules to elabo-
rate T, or transform H, until H matches T. Parsing
is done using a broad coverage chart parser. Sub-
sequently, an abstracted form of the parse tree is
converted into a logical form, for example:
299.H &amp;quot;Tropical storms cause severe damage.&amp;quot;
</bodyText>
<equation confidence="0.888924833333333">
subject(_Cause1, _Storm1)
sobject(_Cause1, _Damage1)
mod(_Storm1, _Tropical1)
mod(_Damage1, _Severe1)
input-word(_Storm1, &amp;quot;storm&amp;quot;, noun)
[same for other words]
</equation>
<bodyText confidence="0.960289961538461">
Entailment is determined if every clause in the se-
mantic representation of H semantically matches
(subsumes) some clause in T. Two variables in a
clause match if their input words are the same, or
some WordNet sense of one is the same as or a
hypernym of the other. In addition, the system
searches for DIRT paraphrase rules that can trans-
form the sentences into a form which can then
match. The explicit use of WordNet and DIRT re-
sults in comprehensible, machine-generated justifi-
cations when entailments are found, , e.g.,:
T: &amp;quot;The Salvation Army operates the shelter under
a contract with the county.&amp;quot;
H: &amp;quot;The Salvation Army collaborates with the
county.&amp;quot;
Yes! Justification: I have general knowledge that:
IF X works with Y THEN X collaborates with Y
Here: X = the Salvation Army, Y = the county
Thus, here:
I can see from T: the Salvation Army works
with the county (because &amp;quot;operate&amp;quot; and &amp;quot;work&amp;quot;
mean roughly the same thing)
Thus it follows that:
The Salvation Army collaborates with the county.
We are continuing to develop our system and ex-
pand the number of knowledge sources it uses.
</bodyText>
<sectionHeader confidence="0.997901" genericHeader="conclusions">
5 Summary
</sectionHeader>
<bodyText confidence="0.999970071428571">
To recognize and justify textual entailments, and
ultimately understand language, considerable lexi-
cal and world knowledge is needed. We have pre-
sented an analysis of some of the knowledge re-
quirements of RTE3, and commented on some
available sources of that knowledge. The analysis
serves to highlight the depth and variety of knowl-
edge demanded by RTE3, and contributes a rough
framework for organizing these requirements. Ul-
timately, to fully understand language, extensive
knowledge of the world (either manually or auto-
matically acquired) is needed. From this analysis,
RTE3 is clearly providing a powerful driving force
for research in this direction.
</bodyText>
<sectionHeader confidence="0.997605" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9970805">
This work was performed under the DTO
AQUAINT program, contract N61339-06-C-0160.
</bodyText>
<sectionHeader confidence="0.999066" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999926884615385">
Collin Baker, Charles Fillmore, and John Lowe. 1998.
&amp;quot;The Berkeley FrameNet Project&amp;quot;. Proc 36th ACL
Aljoscha Burchardt and Anette Frank. 2005. Approach-
ing Textual Entailment with LFG and FrameNet
Frames. in 2nd PASCAL RTE Workshop, pp 92-97.
Dan Fass. 1991. &amp;quot;Met*: A Method for Discriminating
Metonymy and Metaphor by Computer&amp;quot;. In Compu-
tational Linguistics 17 (1), pp 49-90.
Christiane Fellbaum. 1998. “WordNet: An Electronic
Lexical Database.” The MIT Press.
Andrew Hickl, Jeremy Bensley, John Williams, Kirk
Roberts, Bryan Rink, and Ying Shi. “Recognizing
Textual Entailment with LCC’s Groundhog System”,
in Proc 2nd PASCAL RTE Workshop, pp 80-85.
Dekang Lin and Patrick Pantel. 2001. &amp;quot;Discovery of
Inference Rules for Question Answering&amp;quot;. Natural
Language Engineering 7 (4) pp 343-360.
Marvin Minsky. 1985. &amp;quot;A Framework for Representing
Knowledge&amp;quot;. In Readings in Knowledge Repn.
Dan Moldovan and Vasile Rus, 2001. Explaining An-
swers with Extended WordNet, ACL 2001.
Rowan Nairn, Cleo Condoravdi and Lauri Karttunen.
2006. Computing Relative Polarity for Textual Infer-
ence. In the Proceedings of ICoS-5.
Len Schubert. 2002. &amp;quot;Can we Derive General World
Knowledge from Texts?&amp;quot;, Proc. HLT&apos;02, pp84-87.
</reference>
<page confidence="0.999263">
59
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.324266">
<title confidence="0.987249">On the Role of Lexical and World Knowledge in RTE3</title>
<author confidence="0.99932">William R John Phil</author>
<affiliation confidence="0.690016">Christiane</affiliation>
<address confidence="0.8976095">Phantom Works, The Boeing Company, Seattle, WA 4676 Admiralty Way, Marina del Rey, CA</address>
<affiliation confidence="0.542479">University, NJ</affiliation>
<email confidence="0.999547">hobbs@isi.edu,fellbaum@clarity.princeton.edu</email>
<abstract confidence="0.999603684210527">To score well in RTE3, and even more so to create good justifications for entailments, substantial lexical and world knowledge is needed. With this in mind, we present an analysis of a sample of the RTE3 positive entailment pairs, to identify where and what kinds of world knowledge are needed to fully identify and justify the entailment, and discuss several existing resources and their capacity for supplying that knowledge. We also briefly sketch the path we are following to build an RTE system (Our implementation is very preliminary, scoring 50.9% at the time of RTE). The contribution of this paper is thus a framework for discussing the knowledge requirements posed by RTE and some exploration of how these requirements can be met.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Collin Baker</author>
<author>Charles Fillmore</author>
<author>John Lowe</author>
</authors>
<title>The Berkeley FrameNet Project&amp;quot;.</title>
<date>1998</date>
<booktitle>Proc 36th ACL</booktitle>
<contexts>
<context position="16952" citStr="Baker et al, 1998" startWordPosition="2699" endWordPosition="2702"> murders Y 1 This seems to be an accidental gap; WordNet contains many interlinked disease-patient noun pairs, incl. &amp;quot;diabetes-diabetic,&amp;quot; &amp;quot;epilepsy-eplileptic,&amp;quot; etc. 721: X works on Y → X discusses Y And one that could be is not, namely: 705: X is under a contract with Y → X cooperates with Y (not in the database) Other examples are outside the scope of DIRT&apos;s approach (i.e., “X pattern1 Y” → “X pattern2 Y”), but nonetheless the coverage is encouraging. 3.3 FrameNet In our earlier analysis, we identified knowledge about stereotypical situations and their events as important for RTE. FrameNet (Baker et al, 1998) attempts to encode this knowledge. FrameNet was used with some success in RTE2 by Burchardt and Frank (2005). FrameNet&apos;s basic unit - a Frame - is a script-like conceptual schema that refers to a situation, object, or event along with its participants (Frame Elements), identified independent of their syntactic configuration. We earlier discussed how 538.T &amp;quot;...the O. J. Simpson murder trial...&amp;quot; might entail 538.H &amp;quot;O. J. Simpson was accused of murder.&amp;quot; This case applies to FrameNet’s Trial frame, which includes the Frame Elements Defendant and Charges, with Charges being defined as &amp;quot;The legal l</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin Baker, Charles Fillmore, and John Lowe. 1998. &amp;quot;The Berkeley FrameNet Project&amp;quot;. Proc 36th ACL</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aljoscha Burchardt</author>
<author>Anette Frank</author>
</authors>
<title>Approaching Textual Entailment with LFG and FrameNet Frames.</title>
<date>2005</date>
<booktitle>in 2nd PASCAL RTE Workshop,</booktitle>
<pages>92--97</pages>
<contexts>
<context position="17061" citStr="Burchardt and Frank (2005)" startWordPosition="2717" endWordPosition="2720">un pairs, incl. &amp;quot;diabetes-diabetic,&amp;quot; &amp;quot;epilepsy-eplileptic,&amp;quot; etc. 721: X works on Y → X discusses Y And one that could be is not, namely: 705: X is under a contract with Y → X cooperates with Y (not in the database) Other examples are outside the scope of DIRT&apos;s approach (i.e., “X pattern1 Y” → “X pattern2 Y”), but nonetheless the coverage is encouraging. 3.3 FrameNet In our earlier analysis, we identified knowledge about stereotypical situations and their events as important for RTE. FrameNet (Baker et al, 1998) attempts to encode this knowledge. FrameNet was used with some success in RTE2 by Burchardt and Frank (2005). FrameNet&apos;s basic unit - a Frame - is a script-like conceptual schema that refers to a situation, object, or event along with its participants (Frame Elements), identified independent of their syntactic configuration. We earlier discussed how 538.T &amp;quot;...the O. J. Simpson murder trial...&amp;quot; might entail 538.H &amp;quot;O. J. Simpson was accused of murder.&amp;quot; This case applies to FrameNet’s Trial frame, which includes the Frame Elements Defendant and Charges, with Charges being defined as &amp;quot;The legal label for the crime that the Defendant is accused of.&amp;quot;, thus stating the relationship between the defendant an</context>
</contexts>
<marker>Burchardt, Frank, 2005</marker>
<rawString>Aljoscha Burchardt and Anette Frank. 2005. Approaching Textual Entailment with LFG and FrameNet Frames. in 2nd PASCAL RTE Workshop, pp 92-97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Fass</author>
</authors>
<title>Met*: A Method for Discriminating Metonymy and Metaphor by Computer&amp;quot;.</title>
<date>1991</date>
<journal>In Computational Linguistics</journal>
<volume>17</volume>
<issue>1</issue>
<pages>49--90</pages>
<marker>Fass, 1991</marker>
<rawString>Dan Fass. 1991. &amp;quot;Met*: A Method for Discriminating Metonymy and Metaphor by Computer&amp;quot;. In Computational Linguistics 17 (1), pp 49-90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.”</title>
<date>1998</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="13767" citStr="Fellbaum, 1998" startWordPosition="2209" endWordPosition="2210"> of entailments require significant world knowledge, highlighting one of the biggest challenges of RTE. In addition, the category of general world knowledge -- small, non-definitional facts about the way the world (usually) is -- is the largest, suggesting that harvesting and using this kind of knowledge should continue to be a priority for improving performance on RTE-style tasks. 3 Sources of World Knowledge While there are many potential sources of the knowledge that we have identified, we describe three in a bit more detail and how they relate to the earlier analysis. 3.1 WordNet WordNet (Fellbaum, 1998) is one of the most extensively used resources in RTE already and in computational linguistics in general. Despite some well-known problems, it provides broad coverage of several key relationships between word senses (and words), in particular for synonyms, hypernyms (generalizations), meronyms (parts), and semantically (“morphosemantically”) related forms. From the preceding analysis, WordNet does contain the synonyms {&amp;quot;procedure&amp;quot;,&amp;quot;process&amp;quot;}, {&amp;quot;dismiss&amp;quot;,&amp;quot;throw out&amp;quot;}, {&amp;quot;begin&amp;quot;,&amp;quot;start&amp;quot;}, but does not contain the compound &amp;quot;wild fire&amp;quot; and (strictly correctly) does not contain {&amp;quot;revenue&amp;quot;,&amp;quot;proceeds</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. “WordNet: An Electronic Lexical Database.” The MIT Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Andrew Hickl</author>
<author>Jeremy Bensley</author>
<author>John Williams</author>
<author>Kirk Roberts</author>
<author>Bryan Rink</author>
<author>Ying Shi</author>
</authors>
<title>Recognizing Textual Entailment with LCC’s Groundhog System”,</title>
<booktitle>in Proc 2nd PASCAL RTE Workshop,</booktitle>
<pages>80--85</pages>
<marker>Hickl, Bensley, Williams, Roberts, Rink, Shi, </marker>
<rawString>Andrew Hickl, Jeremy Bensley, John Williams, Kirk Roberts, Bryan Rink, and Ying Shi. “Recognizing Textual Entailment with LCC’s Groundhog System”, in Proc 2nd PASCAL RTE Workshop, pp 80-85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Discovery of Inference Rules for Question Answering&amp;quot;.</title>
<date>2001</date>
<journal>Natural Language Engineering</journal>
<volume>7</volume>
<issue>4</issue>
<pages>343--360</pages>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. &amp;quot;Discovery of Inference Rules for Question Answering&amp;quot;. Natural Language Engineering 7 (4) pp 343-360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marvin Minsky</author>
</authors>
<title>A Framework for Representing Knowledge&amp;quot;.</title>
<date>1985</date>
<booktitle>In Readings in Knowledge Repn.</booktitle>
<contexts>
<context position="8704" citStr="Minsky 1985" startWordPosition="1380" endWordPosition="1381">.&amp;quot; 178.H &amp;quot;...a storm in Saskatchewan.&amp;quot; 491 appeals to regional inclusion (if X location Y, and Y is in Z, then X location Z), and 178 appeals to colocation (if X is at Y, and X physically interacts with Z, then Z is at Y). Other spatial examples include 236 (&amp;quot;around Sydney&amp;quot; → &amp;quot;near Sydney&amp;quot;), and 129 (&amp;quot;invasion of&amp;quot; → &amp;quot;arrived in&amp;quot;). 2.10 World Knowledge: Frames and Scripts Although loosely delineated, another category of world knowledge concerns stereotypical places, situations and the events which occur in them, with various representational schemes proposed in the AI literature, e.g., Frames (Minsky 1985), Scripts (Schank 1983). Some RTE examples require recognizing the implicit scenario (&amp;quot;frame&amp;quot;, &amp;quot;script&amp;quot;, etc.) which T describes to confirm the new facts or relationships introduced in H are valid. A first example is: 358.T &amp;quot;Kiesbauer was target of a letter bomb...&amp;quot; 358.H &amp;quot;A letter bomb was sent to Kiesbauer.&amp;quot; A person recognizes H as entailed by T because he/she knows the &amp;quot;script&amp;quot; for letter bombing, which includes sending the bomb in the mail. Thus a person could also recognize alternative verbs in 358.H as valid (e.g., &amp;quot;mailed&amp;quot;, &amp;quot;delivered&amp;quot;) or invalid (e.g., &amp;quot;thrown at&amp;quot;, &amp;quot;dropped on&amp;quot;), eve</context>
</contexts>
<marker>Minsky, 1985</marker>
<rawString>Marvin Minsky. 1985. &amp;quot;A Framework for Representing Knowledge&amp;quot;. In Readings in Knowledge Repn.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Moldovan</author>
<author>Vasile Rus</author>
</authors>
<title>Explaining Answers with Extended WordNet,</title>
<date>2001</date>
<location>ACL</location>
<contexts>
<context position="14942" citStr="Moldovan and Rus, 2001" startWordPosition="2373" endWordPosition="2376">correctly) does not contain {&amp;quot;revenue&amp;quot;,&amp;quot;proceeds&amp;quot;} as synonyms. In addition, the 57 three hypernyms mentioned in the earlier analysis are in WordNet. WordNet also links (via the “morphosemantic” link) the 3 noun-verb pairs mentioned earlier (win/winner, pirated/piracy, invent/invention) – however it does not currently distinguish the nature of that link (e.g., agent, result, event). WordNet is currently being expanded to include this information, as part of the AQUAINT program. Two independently developed versions of the WordNet glosses expressed in logic are also available: Extended WordNet (Moldovan and Rus, 2001) and a version about to be released with WordNet3.0 (again developed under AQUAINT). These in principle can help with definitional knowledge. From our earlier analysis, it turns out &amp;quot;convicted&amp;quot; is conveniently defined in WordNet as &amp;quot;pronounced or proved guilty&amp;quot; potentially bridging the gap for pair 667, although there are problems with the logical interpretation of this particular gloss in both resources mentioned. WordNet does have &amp;quot;coeliac&amp;quot;, but not in the sense of a person with coeliac disease1. In addition, several existing “core theories” (e.g., TimeML, IKRIS) that can supply some of the </context>
</contexts>
<marker>Moldovan, Rus, 2001</marker>
<rawString>Dan Moldovan and Vasile Rus, 2001. Explaining Answers with Extended WordNet, ACL 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rowan Nairn</author>
<author>Cleo Condoravdi</author>
<author>Lauri Karttunen</author>
</authors>
<title>Computing Relative Polarity for Textual Inference.</title>
<date>2006</date>
<booktitle>In the Proceedings of ICoS-5.</booktitle>
<contexts>
<context position="11243" citStr="Nairn et al, 2006" startWordPosition="1800" endWordPosition="1803">heir expansions, include 623 (&amp;quot;narcotics-sniffing dogs&amp;quot; → &amp;quot;dogs are used to sniff out narcotics&amp;quot;), and 11 (&amp;quot;the Nintendo release of the game&amp;quot; → &amp;quot;the game is produced by Nintendo&amp;quot;). 2.11 Implicative Verbs Some RTE3 examples contain complement-taking verbs that make an implication (either positive or negative) about the complement. For example: 668 &amp;quot;A survey shows that X...&amp;quot; → &amp;quot;X...&amp;quot; 657 &amp;quot;...X was seen...&amp;quot; → &amp;quot;...X...&amp;quot; 725 “...decided to X...&amp;quot; → &amp;quot;...X...&amp;quot; 716 &amp;quot;...have been unable to X...&amp;quot; → &amp;quot;...do not X&amp;quot; 56 In the first 3 the implication is positive, but in the last the implication is negative. (Nairn et al, 2006) provide a detailed analysis of this type of behavior. In fact, this notion of implicature (one part of a sentence making an implication about another part) extends beyond single verbs, and there are some more complex examples in RTE3, e.g.: 453 &amp;quot;...won the battle to X...&amp;quot; --+ &amp;quot;...X...&amp;quot; 784.T &amp;quot;X reassures Russia it has nothing to fear...&amp;quot; 784.11 &amp;quot;Russia fears...&amp;quot; In this last example the implication behavior is quite complex: (loosely) If X reassures Y of Z, then Y is concerned about not-Z. 2.12 Metonymy/Transfer In some cases, language allows us to replace a word (sense) with a closely relate</context>
</contexts>
<marker>Nairn, Condoravdi, Karttunen, 2006</marker>
<rawString>Rowan Nairn, Cleo Condoravdi and Lauri Karttunen. 2006. Computing Relative Polarity for Textual Inference. In the Proceedings of ICoS-5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Len Schubert</author>
</authors>
<title>Can we Derive General World Knowledge from Texts?&amp;quot;,</title>
<date>2002</date>
<booktitle>Proc. HLT&apos;02,</booktitle>
<pages>84--87</pages>
<marker>Schubert, 2002</marker>
<rawString>Len Schubert. 2002. &amp;quot;Can we Derive General World Knowledge from Texts?&amp;quot;, Proc. HLT&apos;02, pp84-87.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>