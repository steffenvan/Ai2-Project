<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.028212">
<title confidence="0.923716">
ETS: Domain Adaptation and Stacking for Short Answer Scoring*
</title>
<author confidence="0.850028">
Michael Heilman and Nitin Madnani
</author>
<affiliation confidence="0.766234">
Educational Testing Service
</affiliation>
<address confidence="0.9025055">
660 Rosedale Road
Princeton, NJ 08541, USA
</address>
<email confidence="0.998745">
{mheilman,nmadnani}@ets.org
</email>
<sectionHeader confidence="0.99569" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.994127809523809">
Automatic scoring of short text responses to
educational assessment items is a challeng-
ing task, particularly because large amounts
of labeled data (i.e., human-scored responses)
may or may not be available due to the va-
riety of possible questions and topics. As
such, it seems desirable to integrate various
approaches, making use of model answers
from experts (e.g., to give higher scores to
responses that are similar), prescored student
responses (e.g., to learn direct associations
between particular phrases and scores), etc.
Here, we describe a system that uses stack-
ing (Wolpert, 1992) and domain adaptation
(Daume III, 2007) to achieve this aim, allow-
ing us to integrate item-specific n-gram fea-
tures and more general text similarity mea-
sures (Heilman and Madnani, 2012). We re-
port encouraging results from the Joint Stu-
dent Response Analysis and 8th Recognizing
Textual Entailment Challenge.
</bodyText>
<sectionHeader confidence="0.999083" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999523888888889">
In this paper, we address the problem of automati-
cally scoring short text responses to educational as-
sessment items for measuring content knowledge.
Many approaches can be and have been taken to
this problem—e.g., Leacock and Chodorow (2003),
Nielsen et al. (2008), inter alia. The effectiveness
of any particular approach likely depends on the the
availability of data (among other factors). For exam-
ple, if thousands of prescored responses are avail-
*System description papers for SemEval 2013 are required
to have a team ID (e.g., “ETS”) as a prefix.
able, then a simple classifier using n-gram features
may suffice. However, if only model answers (i.e.,
reference answers) or rubrics are available, more
general semantic similarity measures (or even rule-
based approaches) would be more effective.
It seems likely that, in many cases, there will
be model answers as well as a modest number of
prescored responses available, as was the case for
the Joint Student Response Analysis and 8th Rec-
ognizing Textual Entailment Challenge (�2). There-
fore, we desire to incorporate both task-specific fea-
tures, such as n-grams, as well as more general fea-
tures such as the semantic similarity of the response
to model answers.
We also observe that some features may them-
selves require machine learning or tuning on data
from the domain, in addition to any machine learn-
ing required for the overall system.
In this paper, we describe a machine learning ap-
proach to short answer scoring that allows us to in-
corporate both item-specific and general features by
using the domain adaptation technique of Daume III
(2007). In addition, the approach employs stacking
(Wolpert, 1992) to support the integration of com-
ponents that require tuning or machine learning.
</bodyText>
<sectionHeader confidence="0.960925" genericHeader="method">
2 Task Overview
</sectionHeader>
<bodyText confidence="0.999875285714286">
In this section, we describe the task to which we ap-
plied our system: the Joint Student Response Anal-
ysis and 8th Recognizing Textual Entailment Chal-
lenge (Dzikovska et al., 2013), which was task 7 at
SemEval 2013.
The aim of the task is to classify student responses
to assessment items from two datasets represent-
</bodyText>
<page confidence="0.965062">
275
</page>
<bodyText confidence="0.99508235483871">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 275–279, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
ing different science domains: the Beetle dataset,
which pertains to basic electricity and electronics
(Dzikovska et al., 2010), and the Science Entail-
ments corpus (SciEntsBank) (Nielsen et al., 2008),
which covers a wider range of scientific topics.
Responses were organized into five categories:
correct, partially correct, contradictory, irrelevant,
and non-domain. The SciEntsBank responses were
converted to this format as described by Dzikovska
et al. (2012).
The Beetle training data had about 4,000 student
answers to 47 questions. The SciEntsBank training
data had about 5,000 prescored student answers to
135 questions from 12 domains (different learning
modules). For each item, one or more model re-
sponses were provided by the task organizers.
There were three different evaluation scenarios:
“unseen answers”, for scoring new answers to items
represented in the training data; “unseen questions”,
for scoring answers to new items from domains rep-
resented in the training data; and “unseen domains”,
for scoring answers to items from new domains
(only for SciEntsBank since Beetle focused on a sin-
gle domain).
Performance was evaluated using accuracy,
macro-average F1 scores, and weighted average F1
scores.
For additional details, see the task description pa-
per (Dzikovska et al., 2013).
</bodyText>
<sectionHeader confidence="0.989733" genericHeader="method">
3 System Details
</sectionHeader>
<bodyText confidence="0.999963846153846">
In this section, we describe the short answer scoring
system we developed, and the variations of it that
comprise our submissions to task 7. We begin by
describing our statistical modeling approach. There-
after, we describe the features used by the model
(§3.1), including the PERP feature that relies on
stacking (Wolpert, 1992), and then the domain adap-
tation technique we used (§3.2).
Our system is a logistic regression model with
f2 regularization. It uses the implementation of lo-
gistic regression from the scikit-learn toolkit (Pe-
dregosa et al., 2011).1 To tune the C hyperparame-
ter, it uses a 5-fold cross-validation grid search (with
</bodyText>
<footnote confidence="0.990349666666667">
1The scikit-learn toolkit uses a one-versus-all scheme, us-
ing multiple binary logistic regression classifiers, rather than a
single multiclass logistic regression classifier.
</footnote>
<equation confidence="0.733015">
C ∈ 101−3,−2,...,31).
</equation>
<bodyText confidence="0.99998">
During development, we evaluated performance
using 10-fold cross-validation, with the 5-fold cross-
validation grid search still used for tuning within
each training partition (i.e., each set of 9 folds used
for training during cross-validation).
</bodyText>
<subsectionHeader confidence="0.970325">
3.1 Features
</subsectionHeader>
<bodyText confidence="0.957713">
Our full system includes the following features.
</bodyText>
<subsectionHeader confidence="0.767924">
3.1.1 Baseline Features
</subsectionHeader>
<bodyText confidence="0.999983">
It includes all of the baseline features generated
with the code provided by the task organizers.2
There are four types of lexically-driven text similar-
ity measures, and each is computed by comparing
the learner response to both the expected answer(s)
and the question, resulting in eight features in total.
They are described more fully by Dzikovska et al.
(2012).
</bodyText>
<subsectionHeader confidence="0.773811">
3.1.2 Intercept Feature
</subsectionHeader>
<bodyText confidence="0.999945625">
The system includes an intercept feature that is al-
ways equal to one, which, in combination with the
domain adaptation technique described in §3.2, al-
lows the system to model the a priori distribution
over classes for each domain and item. Having these
explicit intercept features effectively saves the learn-
ing algorithm from having to use other features to
encode the distribution over classes.
</bodyText>
<subsectionHeader confidence="0.802891">
3.1.3 Word and Character n-gram Features
</subsectionHeader>
<bodyText confidence="0.9978545">
The system includes binary indicator features for
the following types of n-grams:
</bodyText>
<listItem confidence="0.976964555555555">
• lowercased word n-grams in the response text
for n ∈ {1, 2, 3}.
• lowercased word n-grams in the response text
for n ∈ {4, 5, ... ,11}, grouped into 10,000
bins by hashing and using a modulo operation
(i.e., the “hashing trick”) (Weinberger et al.,
2009).
• lowercased character n-grams in the response
text for n ∈ {5, 6, 7, 8}
</listItem>
<footnote confidence="0.997136666666667">
2At the time of writing, the baseline code could
be downloaded at http://www.cs.york.ac.uk/
semeval-2013/task7/.
</footnote>
<page confidence="0.989269">
276
</page>
<subsectionHeader confidence="0.55006">
3.1.4 Text Similarity Features
</subsectionHeader>
<bodyText confidence="0.99976">
The system includes the following text similarity
features that compare the student response either to
</bodyText>
<listItem confidence="0.934678772727273">
a) the reference answers for the appropriate item, or
b) the student answers in the training set that are la-
beled “correct”.
• the maximum of the smoothed, uncased BLEU
(Papineni et al., 2002) scores obtained by com-
paring the student response to each correct
reference answer. We also include the word
n-gram precision and recall values for n E
11, 2, 3, 41 for the maximally similar reference
answer.
• the maximum of the smoothed, uncased BLEU
scores obtained by comparing the student re-
sponse to each correct training set student an-
swer. We also include the word n-gram preci-
sion and recall values for n E 11, 2, 3, 41 for
the maximally similar student answer.
• the maximum PERP (Heilman and Madnani,
2012) score obtained by comparing the student
response to the correct reference answers.
• the maximum PERP score obtained by compar-
ing the student response to the correct student
answers.
</listItem>
<bodyText confidence="0.999864282051282">
PERP is an edit-based approach to text similar-
ity. It computes the similarity of sentence pairs by
finding sequences of edit operations (e.g., insertions,
deletions, substitutions, and shifts) that convert one
sentence in a pair to the other. Then, using various
features of the edits and weights for those features
learned from labeled sentence pairs, it assigns a sim-
ilarity score. Heilman and Madnani (2012) provide
a detailed description of the original PERP system.
In addition, Heilman and Madnani (To Appear) de-
scribe some minor modifications to PERP used in
this work.
To estimate weights for PERP’s edit features, we
need labeled sentence pairs. First, we describe how
these labeled sentence pairs are generated from the
task data, and then we describe the stacking ap-
proach used to avoid training PERP on the same data
it will compute features for.
For the reference answer PERP feature, we use
the Cartesian product of the set of correct reference
answers (“good” or “best” for Beetle) and the set
of student answers, using 1 as the similarity score
(i.e., the label for training PERP) for pairs where the
student answer is labeled “correct” and 0 for all oth-
ers. For the student answer PERP feature, we use
the Cartesian product of the set of correct student
answers and the set of all student answers, using 1
as the similarity score for pairs where both student
answers are labeled “correct” and 0 for all others.3
We use 10 iterations for training PERP.
In order to avoid training PERP on the same re-
sponses it will compute features for, we use 10-fold
stacking (Wolpert, 1992). In this process, the train-
ing data are split up into ten folds. To compute the
PERP features for the instances in each fold, PERP
is trained on the other nine folds. After all 10 itera-
tions, there are PERP features for every example in
the training set. This process is similar to 10-fold
cross-validation.
</bodyText>
<subsectionHeader confidence="0.998508">
3.2 Domain Adaptation
</subsectionHeader>
<bodyText confidence="0.9999811">
The system uses the domain adaptation technique
from Daume III (2007) to support generalization
across items and domains.
Instead of having a single weight for each feature,
following Daume III (2007), the system has multiple
copies with potentially different weights: a generic
copy, a domain-specific copy, and an item-specific
copy. For an answer to an unseen item (i.e., ques-
tion) from a new domain in the test set, only the
generic feature will be active. In contrast, for an an-
swer to an item represented in the training data, the
generic, domain-specific, and item-specific copies
of the feature would be active and contribute to the
score.
For our submissions, this feature copying ap-
proach was not used for the baseline features
(�3.1.1) or the BLEU and PERP text similarity fea-
tures (�3.1.4), which are less item-specific. Those
features had only general copies. We did not test
whether doing so would affect performance.
</bodyText>
<footnote confidence="0.9733772">
3The Cartesian product of the sets of correct student answers
and of all student answers will contain some pairs of identi-
cal correct answers. We decided to simply include these when
training PERP, since we felt it would be desirable for PERP to
learn that identical sentences should be considered similar.
</footnote>
<page confidence="0.986219">
277
</page>
<table confidence="0.999748571428571">
Submission Beetle Q A SciEntsBank D
A Q
Run 1 .5520 .5470 .5350 .4870 .4470
Run 2 .7050 .6140 .6250 .3560 .4340
Run 3 .7000 .5860 .6400 .4110 .4140
maximum .7050 .6140 .6400 .4920 .4710
mean .5143 .3978 .4568 .3769 .3736
</table>
<tableCaption confidence="0.911243333333334">
Table 1: Weighted average Fl scores for 5-way classification for our SemEval 2013 task 7 submissions, along with
the maximum and mean performance, for comparison. “A” = unseen answers, “Q” = unseen questions, “D” = unseen
domains (see §2 for details). Results that were the maximum score among submissions for part of the task are in bold.
</tableCaption>
<subsectionHeader confidence="0.998715">
3.3 Submissions
</subsectionHeader>
<bodyText confidence="0.997594666666667">
We submitted three variations of the system. For
each variation, a separate model was trained for Bee-
tle and for SciEntsBank.
</bodyText>
<listItem confidence="0.999369625">
• Run 1: This run included the baseline (§3.1.1),
intercept (§3.1.2), and the text-similarity fea-
tures (§3.1.4) that compare student responses to
reference answers (but not those that compare
to scored student responses in the training set).
• Run 2: This run included the baseline (§3.1.1),
intercept (§3.1.2), and n-gram features (§3.1.3).
• Run 3: This run included all features.
</listItem>
<sectionHeader confidence="0.982431" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999971697674419">
Table 1 presents the weighted averages of Fl scores
across the five categories for the 5-way subtask, for
each dataset and scenario. The maximum and mean
scores of all the submissions are included for com-
parison. These results were provided to us by the
task organizers.
For conciseness, we do not include accuracy or
macro-average Fl scores here. We observed that, in
general, the results from different evaluation metrics
were very similar to each other. We refer the reader
to the task description paper (Dzikovska et al., 2013)
for a full report of the task results.
Interestingly, the differences in performance be-
tween the unseen answers task and the other tasks
was somewhat larger for the SciEntsBank dataset
than for the Beetle dataset. We speculate that this re-
sult is because the SciEntsBank data covered a more
diverse set of topics.
Note that Runs 1 and 2 use subsets of the features
from the full system (Run 3). While Runs 1 and 2
are not directly comparable to each other, Runs 1
and 3 can be compared to measure the effect of the
features based on other previously scored student re-
sponses (i.e., n-grams, and the PERP and BLEU fea-
tures based on student responses). Similarly, Runs 2
and 3 can be compared to measure the combined ef-
fect of all BLEU and PERP features.
It appears that features of the other student re-
sponses improve performance for the unseen an-
swers task. For example, the full system (Run 3)
performed better than Run 1, which did not include
features of other student responses, on the unseen
answers task for both Beetle and SciEntsBank.
However, it is not clear whether the PERP and
BLEU features improve performance. The full sys-
tem (Run 3) did not always outperform Run 2, which
did not include these features.
We leave to future work various additional ques-
tions, such as whether student response features or
reference answer similarity features are more use-
ful in general, and whether there are any systematic
differences between human-machine and human-
human disagreements.
</bodyText>
<sectionHeader confidence="0.998953" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999951777777778">
We have presented an approach for short answer
scoring that uses stacking (Wolpert, 1992) and do-
main adaptation (Daume III, 2007) to support the
integration of various types of task-specific and gen-
eral features. Evaluation results from task 7 at Se-
mEval 2013 indicate that the system achieves rela-
tively high levels of agreement with human scores,
as compared to other systems submitted to the
shared task.
</bodyText>
<page confidence="0.996204">
278
</page>
<sectionHeader confidence="0.998324" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999959">
We would like to thank the task organizers for facil-
itating this research and Dan Blanchard for helping
with scikit-learn.
</bodyText>
<sectionHeader confidence="0.990087" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999815932432432">
Hal Daume III. 2007. Frustratingly easy domain adapta-
tion. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256–263, Prague, Czech Republic, June. Association
for Computational Linguistics.
Myroslava O. Dzikovska, Diana Bental, Johanna D.
Moore, Natalie Steinhauser, Gwendolyn Campbell,
Elaine Farrow, and Charles B. Callaway. 2010. In-
telligent tutoring with natural language support in the
BEETLE II system. In Proceedings of Fifth European
Conference on Technology Enhanced Learning (EC-
TEL 2010).
Myroslava O. Dzikovska, Rodney D. Nielsen, and Chris
Brew. 2012. Towards effective tutorial feedback
for explanation questions: A dataset and baselines.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 200–210, Montr´eal, Canada, June. Association
for Computational Linguistics.
Myroslava O. Dzikovska, Rodney Nielsen, Chris Brew,
Claudia Leacock, Danilo Giampiccolo, Luisa Ben-
tivogli, Peter Clark, Ido Dagan, and Hoa Trang Dang.
2013. Semeval-2013 task 7: The joint student re-
sponse analysis and 8th recognizing textual entailment
challenge. In *SEM 2013: The First Joint Conference
on Lexical and Computational Semantics, Atlanta,
Georgia, USA, 13-14 June. Association for Compu-
tational Linguistics.
Michael Heilman and Nitin Madnani. 2012. ETS: Dis-
criminative edit models for paraphrase scoring. In
*SEM 2012: The First Joint Conference on Lexi-
cal and Computational Semantics – Volume 1: Pro-
ceedings of the main conference and the shared task,
and Volume 2: Proceedings of the Sixth International
Workshop on Semantic Evaluation (SemEval 2012),
pages 529–535, Montr´eal, Canada, 7-8 June. Associa-
tion for Computational Linguistics.
Michael Heilman and Nitin Madnani. To Appear. Henry:
Domain adapation and stacking for text similarity. In
*SEM 2013: The Second Joint Conference on Lexical
and Computational Semantics. Association for Com-
putational Linguistics.
C. Leacock and M. Chodorow. 2003. c-rater: Scoring of
short-answer questions. Computers and the Humani-
ties, 37.
Rodney D. Nielsen, Wayne Ward, and James H. Martin.
2008. Classification errors in a domain-independent
assessment system. In Proceedings of the Third Work-
shop on Innovative Use of NLP for Building Educa-
tional Applications, pages 10–18, Columbus, Ohio,
June. Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311–318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and E. Duches-
nay. 2011. Scikit-learn: Machine learning in Python.
Journal of Machine Learning Research, 12:2825–
2830.
Kilian Weinberger, Anirban Dasgupta, John Langford,
Alex Smola, and Josh Attenberg. 2009. Feature hash-
ing for large scale multitask learning. In Proceedings
of the 26th Annual International Conference on Ma-
chine Learning, ICML ’09, pages 1113–1120, New
York, NY, USA. ACM.
David H. Wolpert. 1992. Stacked generalization. Neural
Networks, 5:241–259.
</reference>
<page confidence="0.998561">
279
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.407389">
<title confidence="0.999387">Domain Adaptation and Stacking for Short Answer</title>
<author confidence="0.953634">Heilman</author>
<affiliation confidence="0.939916">Educational Testing</affiliation>
<address confidence="0.998186">660 Rosedale Princeton, NJ 08541,</address>
<abstract confidence="0.944101227272727">Automatic scoring of short text responses to educational assessment items is a challenging task, particularly because large amounts of labeled data (i.e., human-scored responses) may or may not be available due to the variety of possible questions and topics. As such, it seems desirable to integrate various approaches, making use of model answers from experts (e.g., to give higher scores to responses that are similar), prescored student responses (e.g., to learn direct associations between particular phrases and scores), etc. Here, we describe a system that uses stacking (Wolpert, 1992) and domain adaptation (Daume III, 2007) to achieve this aim, allowus to integrate item-specific features and more general text similarity measures (Heilman and Madnani, 2012). We report encouraging results from the Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Hal Daume</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>256--263</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<marker>Daume, 2007</marker>
<rawString>Hal Daume III. 2007. Frustratingly easy domain adaptation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 256–263, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Myroslava O Dzikovska</author>
<author>Diana Bental</author>
<author>Johanna D Moore</author>
<author>Natalie Steinhauser</author>
<author>Gwendolyn Campbell</author>
<author>Elaine Farrow</author>
<author>Charles B Callaway</author>
</authors>
<title>Intelligent tutoring with natural language support in the BEETLE II system.</title>
<date>2010</date>
<booktitle>In Proceedings of Fifth European Conference on Technology Enhanced Learning (ECTEL</booktitle>
<contexts>
<context position="3583" citStr="Dzikovska et al., 2010" startWordPosition="555" endWordPosition="558">Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge (Dzikovska et al., 2013), which was task 7 at SemEval 2013. The aim of the task is to classify student responses to assessment items from two datasets represent275 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 275–279, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics ing different science domains: the Beetle dataset, which pertains to basic electricity and electronics (Dzikovska et al., 2010), and the Science Entailments corpus (SciEntsBank) (Nielsen et al., 2008), which covers a wider range of scientific topics. Responses were organized into five categories: correct, partially correct, contradictory, irrelevant, and non-domain. The SciEntsBank responses were converted to this format as described by Dzikovska et al. (2012). The Beetle training data had about 4,000 student answers to 47 questions. The SciEntsBank training data had about 5,000 prescored student answers to 135 questions from 12 domains (different learning modules). For each item, one or more model responses were prov</context>
</contexts>
<marker>Dzikovska, Bental, Moore, Steinhauser, Campbell, Farrow, Callaway, 2010</marker>
<rawString>Myroslava O. Dzikovska, Diana Bental, Johanna D. Moore, Natalie Steinhauser, Gwendolyn Campbell, Elaine Farrow, and Charles B. Callaway. 2010. Intelligent tutoring with natural language support in the BEETLE II system. In Proceedings of Fifth European Conference on Technology Enhanced Learning (ECTEL 2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Myroslava O Dzikovska</author>
<author>Rodney D Nielsen</author>
<author>Chris Brew</author>
</authors>
<title>Towards effective tutorial feedback for explanation questions: A dataset and baselines.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>200--210</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="3920" citStr="Dzikovska et al. (2012)" startWordPosition="602" endWordPosition="605">ernational Workshop on Semantic Evaluation (SemEval 2013), pages 275–279, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics ing different science domains: the Beetle dataset, which pertains to basic electricity and electronics (Dzikovska et al., 2010), and the Science Entailments corpus (SciEntsBank) (Nielsen et al., 2008), which covers a wider range of scientific topics. Responses were organized into five categories: correct, partially correct, contradictory, irrelevant, and non-domain. The SciEntsBank responses were converted to this format as described by Dzikovska et al. (2012). The Beetle training data had about 4,000 student answers to 47 questions. The SciEntsBank training data had about 5,000 prescored student answers to 135 questions from 12 domains (different learning modules). For each item, one or more model responses were provided by the task organizers. There were three different evaluation scenarios: “unseen answers”, for scoring new answers to items represented in the training data; “unseen questions”, for scoring answers to new items from domains represented in the training data; and “unseen domains”, for scoring answers to items from new domains (only </context>
<context position="6309" citStr="Dzikovska et al. (2012)" startWordPosition="968" endWordPosition="971">ion, with the 5-fold crossvalidation grid search still used for tuning within each training partition (i.e., each set of 9 folds used for training during cross-validation). 3.1 Features Our full system includes the following features. 3.1.1 Baseline Features It includes all of the baseline features generated with the code provided by the task organizers.2 There are four types of lexically-driven text similarity measures, and each is computed by comparing the learner response to both the expected answer(s) and the question, resulting in eight features in total. They are described more fully by Dzikovska et al. (2012). 3.1.2 Intercept Feature The system includes an intercept feature that is always equal to one, which, in combination with the domain adaptation technique described in §3.2, allows the system to model the a priori distribution over classes for each domain and item. Having these explicit intercept features effectively saves the learning algorithm from having to use other features to encode the distribution over classes. 3.1.3 Word and Character n-gram Features The system includes binary indicator features for the following types of n-grams: • lowercased word n-grams in the response text for n ∈</context>
</contexts>
<marker>Dzikovska, Nielsen, Brew, 2012</marker>
<rawString>Myroslava O. Dzikovska, Rodney D. Nielsen, and Chris Brew. 2012. Towards effective tutorial feedback for explanation questions: A dataset and baselines. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 200–210, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Myroslava O Dzikovska</author>
<author>Rodney Nielsen</author>
<author>Chris Brew</author>
<author>Claudia Leacock</author>
<author>Danilo Giampiccolo</author>
<author>Luisa Bentivogli</author>
<author>Peter Clark</author>
<author>Ido Dagan</author>
<author>Hoa Trang Dang</author>
</authors>
<title>Semeval-2013 task 7: The joint student response analysis and 8th recognizing textual entailment challenge.</title>
<date>2013</date>
<booktitle>In *SEM 2013: The First Joint Conference on Lexical and Computational Semantics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="3065" citStr="Dzikovska et al., 2013" startWordPosition="481" endWordPosition="484">omain, in addition to any machine learning required for the overall system. In this paper, we describe a machine learning approach to short answer scoring that allows us to incorporate both item-specific and general features by using the domain adaptation technique of Daume III (2007). In addition, the approach employs stacking (Wolpert, 1992) to support the integration of components that require tuning or machine learning. 2 Task Overview In this section, we describe the task to which we applied our system: the Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge (Dzikovska et al., 2013), which was task 7 at SemEval 2013. The aim of the task is to classify student responses to assessment items from two datasets represent275 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 275–279, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics ing different science domains: the Beetle dataset, which pertains to basic electricity and electronics (Dzikovska et al., 2010), and the Science Entailments corpus (SciEntsBank) (Nielsen et al., 2008), which c</context>
<context position="4756" citStr="Dzikovska et al., 2013" startWordPosition="731" endWordPosition="734">r each item, one or more model responses were provided by the task organizers. There were three different evaluation scenarios: “unseen answers”, for scoring new answers to items represented in the training data; “unseen questions”, for scoring answers to new items from domains represented in the training data; and “unseen domains”, for scoring answers to items from new domains (only for SciEntsBank since Beetle focused on a single domain). Performance was evaluated using accuracy, macro-average F1 scores, and weighted average F1 scores. For additional details, see the task description paper (Dzikovska et al., 2013). 3 System Details In this section, we describe the short answer scoring system we developed, and the variations of it that comprise our submissions to task 7. We begin by describing our statistical modeling approach. Thereafter, we describe the features used by the model (§3.1), including the PERP feature that relies on stacking (Wolpert, 1992), and then the domain adaptation technique we used (§3.2). Our system is a logistic regression model with f2 regularization. It uses the implementation of logistic regression from the scikit-learn toolkit (Pedregosa et al., 2011).1 To tune the C hyperpa</context>
<context position="13124" citStr="Dzikovska et al., 2013" startWordPosition="2108" endWordPosition="2111"> (§3.1.2), and n-gram features (§3.1.3). • Run 3: This run included all features. 4 Results Table 1 presents the weighted averages of Fl scores across the five categories for the 5-way subtask, for each dataset and scenario. The maximum and mean scores of all the submissions are included for comparison. These results were provided to us by the task organizers. For conciseness, we do not include accuracy or macro-average Fl scores here. We observed that, in general, the results from different evaluation metrics were very similar to each other. We refer the reader to the task description paper (Dzikovska et al., 2013) for a full report of the task results. Interestingly, the differences in performance between the unseen answers task and the other tasks was somewhat larger for the SciEntsBank dataset than for the Beetle dataset. We speculate that this result is because the SciEntsBank data covered a more diverse set of topics. Note that Runs 1 and 2 use subsets of the features from the full system (Run 3). While Runs 1 and 2 are not directly comparable to each other, Runs 1 and 3 can be compared to measure the effect of the features based on other previously scored student responses (i.e., n-grams, and the </context>
</contexts>
<marker>Dzikovska, Nielsen, Brew, Leacock, Giampiccolo, Bentivogli, Clark, Dagan, Dang, 2013</marker>
<rawString>Myroslava O. Dzikovska, Rodney Nielsen, Chris Brew, Claudia Leacock, Danilo Giampiccolo, Luisa Bentivogli, Peter Clark, Ido Dagan, and Hoa Trang Dang. 2013. Semeval-2013 task 7: The joint student response analysis and 8th recognizing textual entailment challenge. In *SEM 2013: The First Joint Conference on Lexical and Computational Semantics, Atlanta, Georgia, USA, 13-14 June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Michael Heilman</author>
<author>Nitin Madnani</author>
</authors>
<title>ETS: Discriminative edit models for paraphrase scoring.</title>
<date>2012</date>
<booktitle>In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>529--535</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal,</location>
<contexts>
<context position="983" citStr="Heilman and Madnani, 2012" startWordPosition="144" endWordPosition="147">, human-scored responses) may or may not be available due to the variety of possible questions and topics. As such, it seems desirable to integrate various approaches, making use of model answers from experts (e.g., to give higher scores to responses that are similar), prescored student responses (e.g., to learn direct associations between particular phrases and scores), etc. Here, we describe a system that uses stacking (Wolpert, 1992) and domain adaptation (Daume III, 2007) to achieve this aim, allowing us to integrate item-specific n-gram features and more general text similarity measures (Heilman and Madnani, 2012). We report encouraging results from the Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge. 1 Introduction In this paper, we address the problem of automatically scoring short text responses to educational assessment items for measuring content knowledge. Many approaches can be and have been taken to this problem—e.g., Leacock and Chodorow (2003), Nielsen et al. (2008), inter alia. The effectiveness of any particular approach likely depends on the the availability of data (among other factors). For example, if thousands of prescored responses are avail*System des</context>
<context position="8147" citStr="Heilman and Madnani, 2012" startWordPosition="1276" endWordPosition="1279">training set that are labeled “correct”. • the maximum of the smoothed, uncased BLEU (Papineni et al., 2002) scores obtained by comparing the student response to each correct reference answer. We also include the word n-gram precision and recall values for n E 11, 2, 3, 41 for the maximally similar reference answer. • the maximum of the smoothed, uncased BLEU scores obtained by comparing the student response to each correct training set student answer. We also include the word n-gram precision and recall values for n E 11, 2, 3, 41 for the maximally similar student answer. • the maximum PERP (Heilman and Madnani, 2012) score obtained by comparing the student response to the correct reference answers. • the maximum PERP score obtained by comparing the student response to the correct student answers. PERP is an edit-based approach to text similarity. It computes the similarity of sentence pairs by finding sequences of edit operations (e.g., insertions, deletions, substitutions, and shifts) that convert one sentence in a pair to the other. Then, using various features of the edits and weights for those features learned from labeled sentence pairs, it assigns a similarity score. Heilman and Madnani (2012) provi</context>
</contexts>
<marker>Heilman, Madnani, 2012</marker>
<rawString>Michael Heilman and Nitin Madnani. 2012. ETS: Discriminative edit models for paraphrase scoring. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 529–535, Montr´eal, Canada, 7-8 June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Michael Heilman</author>
<author>Nitin Madnani</author>
</authors>
<title>To Appear. Henry: Domain adapation and stacking for text similarity.</title>
<booktitle>In *SEM 2013: The Second Joint Conference on Lexical and Computational Semantics. Association for Computational Linguistics.</booktitle>
<marker>Heilman, Madnani, </marker>
<rawString>Michael Heilman and Nitin Madnani. To Appear. Henry: Domain adapation and stacking for text similarity. In *SEM 2013: The Second Joint Conference on Lexical and Computational Semantics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>M Chodorow</author>
</authors>
<title>c-rater: Scoring of short-answer questions.</title>
<date>2003</date>
<journal>Computers and the Humanities,</journal>
<volume>37</volume>
<contexts>
<context position="1362" citStr="Leacock and Chodorow (2003)" startWordPosition="202" endWordPosition="205"> Here, we describe a system that uses stacking (Wolpert, 1992) and domain adaptation (Daume III, 2007) to achieve this aim, allowing us to integrate item-specific n-gram features and more general text similarity measures (Heilman and Madnani, 2012). We report encouraging results from the Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge. 1 Introduction In this paper, we address the problem of automatically scoring short text responses to educational assessment items for measuring content knowledge. Many approaches can be and have been taken to this problem—e.g., Leacock and Chodorow (2003), Nielsen et al. (2008), inter alia. The effectiveness of any particular approach likely depends on the the availability of data (among other factors). For example, if thousands of prescored responses are avail*System description papers for SemEval 2013 are required to have a team ID (e.g., “ETS”) as a prefix. able, then a simple classifier using n-gram features may suffice. However, if only model answers (i.e., reference answers) or rubrics are available, more general semantic similarity measures (or even rulebased approaches) would be more effective. It seems likely that, in many cases, ther</context>
</contexts>
<marker>Leacock, Chodorow, 2003</marker>
<rawString>C. Leacock and M. Chodorow. 2003. c-rater: Scoring of short-answer questions. Computers and the Humanities, 37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rodney D Nielsen</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
</authors>
<title>Classification errors in a domain-independent assessment system.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>10--18</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="1385" citStr="Nielsen et al. (2008)" startWordPosition="206" endWordPosition="209">hat uses stacking (Wolpert, 1992) and domain adaptation (Daume III, 2007) to achieve this aim, allowing us to integrate item-specific n-gram features and more general text similarity measures (Heilman and Madnani, 2012). We report encouraging results from the Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge. 1 Introduction In this paper, we address the problem of automatically scoring short text responses to educational assessment items for measuring content knowledge. Many approaches can be and have been taken to this problem—e.g., Leacock and Chodorow (2003), Nielsen et al. (2008), inter alia. The effectiveness of any particular approach likely depends on the the availability of data (among other factors). For example, if thousands of prescored responses are avail*System description papers for SemEval 2013 are required to have a team ID (e.g., “ETS”) as a prefix. able, then a simple classifier using n-gram features may suffice. However, if only model answers (i.e., reference answers) or rubrics are available, more general semantic similarity measures (or even rulebased approaches) would be more effective. It seems likely that, in many cases, there will be model answers</context>
<context position="3656" citStr="Nielsen et al., 2008" startWordPosition="566" endWordPosition="569">lenge (Dzikovska et al., 2013), which was task 7 at SemEval 2013. The aim of the task is to classify student responses to assessment items from two datasets represent275 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 275–279, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics ing different science domains: the Beetle dataset, which pertains to basic electricity and electronics (Dzikovska et al., 2010), and the Science Entailments corpus (SciEntsBank) (Nielsen et al., 2008), which covers a wider range of scientific topics. Responses were organized into five categories: correct, partially correct, contradictory, irrelevant, and non-domain. The SciEntsBank responses were converted to this format as described by Dzikovska et al. (2012). The Beetle training data had about 4,000 student answers to 47 questions. The SciEntsBank training data had about 5,000 prescored student answers to 135 questions from 12 domains (different learning modules). For each item, one or more model responses were provided by the task organizers. There were three different evaluation scenar</context>
</contexts>
<marker>Nielsen, Ward, Martin, 2008</marker>
<rawString>Rodney D. Nielsen, Wayne Ward, and James H. Martin. 2008. Classification errors in a domain-independent assessment system. In Proceedings of the Third Workshop on Innovative Use of NLP for Building Educational Applications, pages 10–18, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="7629" citStr="Papineni et al., 2002" startWordPosition="1184" endWordPosition="1187">0,000 bins by hashing and using a modulo operation (i.e., the “hashing trick”) (Weinberger et al., 2009). • lowercased character n-grams in the response text for n ∈ {5, 6, 7, 8} 2At the time of writing, the baseline code could be downloaded at http://www.cs.york.ac.uk/ semeval-2013/task7/. 276 3.1.4 Text Similarity Features The system includes the following text similarity features that compare the student response either to a) the reference answers for the appropriate item, or b) the student answers in the training set that are labeled “correct”. • the maximum of the smoothed, uncased BLEU (Papineni et al., 2002) scores obtained by comparing the student response to each correct reference answer. We also include the word n-gram precision and recall values for n E 11, 2, 3, 41 for the maximally similar reference answer. • the maximum of the smoothed, uncased BLEU scores obtained by comparing the student response to each correct training set student answer. We also include the word n-gram precision and recall values for n E 11, 2, 3, 41 for the maximally similar student answer. • the maximum PERP (Heilman and Madnani, 2012) score obtained by comparing the student response to the correct reference answers</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pedregosa</author>
<author>G Varoquaux</author>
<author>A Gramfort</author>
<author>V Michel</author>
<author>B Thirion</author>
<author>O Grisel</author>
<author>M Blondel</author>
<author>P Prettenhofer</author>
<author>R Weiss</author>
<author>V Dubourg</author>
<author>J Vanderplas</author>
<author>A Passos</author>
<author>D Cournapeau</author>
<author>M Brucher</author>
<author>M Perrot</author>
<author>E Duchesnay</author>
</authors>
<title>Scikit-learn: Machine learning in Python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>12</volume>
<pages>2830</pages>
<contexts>
<context position="5332" citStr="Pedregosa et al., 2011" startWordPosition="823" endWordPosition="827">task description paper (Dzikovska et al., 2013). 3 System Details In this section, we describe the short answer scoring system we developed, and the variations of it that comprise our submissions to task 7. We begin by describing our statistical modeling approach. Thereafter, we describe the features used by the model (§3.1), including the PERP feature that relies on stacking (Wolpert, 1992), and then the domain adaptation technique we used (§3.2). Our system is a logistic regression model with f2 regularization. It uses the implementation of logistic regression from the scikit-learn toolkit (Pedregosa et al., 2011).1 To tune the C hyperparameter, it uses a 5-fold cross-validation grid search (with 1The scikit-learn toolkit uses a one-versus-all scheme, using multiple binary logistic regression classifiers, rather than a single multiclass logistic regression classifier. C ∈ 101−3,−2,...,31). During development, we evaluated performance using 10-fold cross-validation, with the 5-fold crossvalidation grid search still used for tuning within each training partition (i.e., each set of 9 folds used for training during cross-validation). 3.1 Features Our full system includes the following features. 3.1.1 Basel</context>
</contexts>
<marker>Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, Duchesnay, 2011</marker>
<rawString>F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825– 2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kilian Weinberger</author>
<author>Anirban Dasgupta</author>
<author>John Langford</author>
<author>Alex Smola</author>
<author>Josh Attenberg</author>
</authors>
<title>Feature hashing for large scale multitask learning.</title>
<date>2009</date>
<booktitle>In Proceedings of the 26th Annual International Conference on Machine Learning, ICML ’09,</booktitle>
<pages>1113--1120</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="7111" citStr="Weinberger et al., 2009" startWordPosition="1101" endWordPosition="1104">ws the system to model the a priori distribution over classes for each domain and item. Having these explicit intercept features effectively saves the learning algorithm from having to use other features to encode the distribution over classes. 3.1.3 Word and Character n-gram Features The system includes binary indicator features for the following types of n-grams: • lowercased word n-grams in the response text for n ∈ {1, 2, 3}. • lowercased word n-grams in the response text for n ∈ {4, 5, ... ,11}, grouped into 10,000 bins by hashing and using a modulo operation (i.e., the “hashing trick”) (Weinberger et al., 2009). • lowercased character n-grams in the response text for n ∈ {5, 6, 7, 8} 2At the time of writing, the baseline code could be downloaded at http://www.cs.york.ac.uk/ semeval-2013/task7/. 276 3.1.4 Text Similarity Features The system includes the following text similarity features that compare the student response either to a) the reference answers for the appropriate item, or b) the student answers in the training set that are labeled “correct”. • the maximum of the smoothed, uncased BLEU (Papineni et al., 2002) scores obtained by comparing the student response to each correct reference answe</context>
</contexts>
<marker>Weinberger, Dasgupta, Langford, Smola, Attenberg, 2009</marker>
<rawString>Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola, and Josh Attenberg. 2009. Feature hashing for large scale multitask learning. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML ’09, pages 1113–1120, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David H Wolpert</author>
</authors>
<title>Stacked generalization.</title>
<date>1992</date>
<journal>Neural Networks,</journal>
<pages>5--241</pages>
<contexts>
<context position="797" citStr="Wolpert, 1992" startWordPosition="116" endWordPosition="117">ni}@ets.org Abstract Automatic scoring of short text responses to educational assessment items is a challenging task, particularly because large amounts of labeled data (i.e., human-scored responses) may or may not be available due to the variety of possible questions and topics. As such, it seems desirable to integrate various approaches, making use of model answers from experts (e.g., to give higher scores to responses that are similar), prescored student responses (e.g., to learn direct associations between particular phrases and scores), etc. Here, we describe a system that uses stacking (Wolpert, 1992) and domain adaptation (Daume III, 2007) to achieve this aim, allowing us to integrate item-specific n-gram features and more general text similarity measures (Heilman and Madnani, 2012). We report encouraging results from the Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge. 1 Introduction In this paper, we address the problem of automatically scoring short text responses to educational assessment items for measuring content knowledge. Many approaches can be and have been taken to this problem—e.g., Leacock and Chodorow (2003), Nielsen et al. (2008), inter alia</context>
<context position="2787" citStr="Wolpert, 1992" startWordPosition="436" endWordPosition="437">esire to incorporate both task-specific features, such as n-grams, as well as more general features such as the semantic similarity of the response to model answers. We also observe that some features may themselves require machine learning or tuning on data from the domain, in addition to any machine learning required for the overall system. In this paper, we describe a machine learning approach to short answer scoring that allows us to incorporate both item-specific and general features by using the domain adaptation technique of Daume III (2007). In addition, the approach employs stacking (Wolpert, 1992) to support the integration of components that require tuning or machine learning. 2 Task Overview In this section, we describe the task to which we applied our system: the Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge (Dzikovska et al., 2013), which was task 7 at SemEval 2013. The aim of the task is to classify student responses to assessment items from two datasets represent275 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 275–279, Atlanta, Georgia</context>
<context position="5103" citStr="Wolpert, 1992" startWordPosition="789" endWordPosition="790">o items from new domains (only for SciEntsBank since Beetle focused on a single domain). Performance was evaluated using accuracy, macro-average F1 scores, and weighted average F1 scores. For additional details, see the task description paper (Dzikovska et al., 2013). 3 System Details In this section, we describe the short answer scoring system we developed, and the variations of it that comprise our submissions to task 7. We begin by describing our statistical modeling approach. Thereafter, we describe the features used by the model (§3.1), including the PERP feature that relies on stacking (Wolpert, 1992), and then the domain adaptation technique we used (§3.2). Our system is a logistic regression model with f2 regularization. It uses the implementation of logistic regression from the scikit-learn toolkit (Pedregosa et al., 2011).1 To tune the C hyperparameter, it uses a 5-fold cross-validation grid search (with 1The scikit-learn toolkit uses a one-versus-all scheme, using multiple binary logistic regression classifiers, rather than a single multiclass logistic regression classifier. C ∈ 101−3,−2,...,31). During development, we evaluated performance using 10-fold cross-validation, with the 5-f</context>
<context position="9923" citStr="Wolpert, 1992" startWordPosition="1577" endWordPosition="1578">“best” for Beetle) and the set of student answers, using 1 as the similarity score (i.e., the label for training PERP) for pairs where the student answer is labeled “correct” and 0 for all others. For the student answer PERP feature, we use the Cartesian product of the set of correct student answers and the set of all student answers, using 1 as the similarity score for pairs where both student answers are labeled “correct” and 0 for all others.3 We use 10 iterations for training PERP. In order to avoid training PERP on the same responses it will compute features for, we use 10-fold stacking (Wolpert, 1992). In this process, the training data are split up into ten folds. To compute the PERP features for the instances in each fold, PERP is trained on the other nine folds. After all 10 iterations, there are PERP features for every example in the training set. This process is similar to 10-fold cross-validation. 3.2 Domain Adaptation The system uses the domain adaptation technique from Daume III (2007) to support generalization across items and domains. Instead of having a single weight for each feature, following Daume III (2007), the system has multiple copies with potentially different weights: </context>
</contexts>
<marker>Wolpert, 1992</marker>
<rawString>David H. Wolpert. 1992. Stacked generalization. Neural Networks, 5:241–259.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>