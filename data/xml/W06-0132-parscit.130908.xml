<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000940">
<title confidence="0.9982845">
Chinese Word Segmentation and Named Entity Recognition Based on
Conditional Random Fields Models
</title>
<author confidence="0.999296">
Yuanyong Feng Le Sun Yuanhua Lv
</author>
<affiliation confidence="0.998929">
Institute of Software, Chinese Academy of Sciences, Beijing, 100080, China
</affiliation>
<email confidence="0.996069">
{yuanyong02, sunle, yuanhua04}@ios.cn
</email>
<sectionHeader confidence="0.993841" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996774">
This paper mainly describes a Chinese
named entity recognition (NER) system
NER@ISCAS, which integrates text,
part-of-speech and a small-vocabulary-
character-lists feature for MSRA NER
open track under the framework of Con-
ditional Random Fields (CRFs) model.
The techniques used for the close NER
and word segmentation tracks are also
presented.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999842571428571">
The system NER@ISCAS is designed under the
Conditional Random Fields (CRFs. Lafferty et
al., 2001) framework. It integrates multiple fea-
tures based on single Chinese character or space
separated ASCII words. The early designed sys-
tem (Feng et al., 2005) is used for the MSRA
NER open track this year. The output of an ex-
ternal part-of-speech tagging tool and some care-
fully collected small-scale-character-lists are
used as outer knowledge.
The close word segmentation and named en-
tity recognition tracks are also based on this sys-
tem by some adjustments.
The remaining of this paper is organized as
follows. Section 2 introduces Conditional Ran-
dom Fields model. Section 3 presents the details
of our system on Chinese NER integrating mul-
tiple features. Section 4 describes the features
extraction for close track. Section 5 gives the
evaluation results. We end our paper with some
conclusions and future works.
</bodyText>
<sectionHeader confidence="0.98954" genericHeader="introduction">
2 Conditional Random Fields Model
</sectionHeader>
<bodyText confidence="0.999397583333333">
Conditional random fields are undirected graphi-
cal models for calculating the conditional prob-
ability for output vertices based on input ones.
While sharing the same exponential form with
maximum entropy models, they have more effi-
cient procedures for complete, non-greedy finite-
state inference and training.
Given an observation sequence o=&lt;o1, o2, ...,
oT&gt;, linear-chain CRFs model based on the as-
sumption of first order Markov chains defines
the corresponding state sequence s′ probability as
follows (Lafferty et al., 2001):
</bodyText>
<equation confidence="0.996152428571428">
1
(s  |o) =exp(∑ ∑ λk k t
f s s t
( , , , ))
o
−1 t
Zo
</equation>
<bodyText confidence="0.997783235294118">
Where Λ is the model parameter set, Zo is the
normalization factor over all state sequences, fk is
an arbitrary feature function, and λk is the learned
feature weight. A feature function defines its
value to be 0 in most cases, and to be 1 in some
designated cases. For example, the value of a
feature named “MAYBE-SURNAME” is 1 if
and only if st-1 is OTHER, st is PER, and the t-th
character in o is a common-surname.
The inference and training procedures of
CRFs can be derived directly from those equiva-
lences in HMM. For instance, the forward vari-
able αt(si) defines the probability that state at
time t being si at time t given the observation
sequence o. Assumed that we know the proba-
bilities of each possible value si for the beginning
state α0(si), then we have
</bodyText>
<equation confidence="0.991616166666666">
α + s
t 1( i)=∑ ′ ∑ ′ o
α s
t ( )exp( k k ( , i, , )
λ f s s t
s′ k
</equation>
<bodyText confidence="0.9943795">
In similar ways, we can obtain the backward
variables and Baum-Welch algorithm.
</bodyText>
<sectionHeader confidence="0.853610666666667" genericHeader="method">
3 Chinese NER Using CRFs Model Inte-
grating Multiple Features for Open
Track
</sectionHeader>
<bodyText confidence="0.9992255">
In our system the text feature, part-of-speech
(POS) feature, and small-vocabulary-character-
lists (SVCL) feature are combined under a uni-
fied CRFs framework.
</bodyText>
<equation confidence="0.970965">
T
pΛ
t k
=1
</equation>
<page confidence="0.814601">
181
</page>
<bodyText confidence="0.984271941176471">
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 181–184,
Sydney, July 2006. c�2006 Association for Computational Linguistics
The text feature includes single Chinese char-
acter, some continuous digits or letters.
POS feature is an important feature which car-
ries some syntactic information. Our POS tag set
follows the criterion of modern Chinese corpora
construction (Yu, 1999), which contains 39 tags.
The last feature is based on lists. We first list
all digits and English letters in Chinese. Then
most frequently used character feature in Chinese
NER are collected, including 100 single charac-
ter surnames, 100 location tail characters, and 40
organization tail characters. The total number of
these items in our lists is less than 600. The lists
altogether make up a list feature (SVCL). Some
examples of this list are given in Table 1.
</bodyText>
<table confidence="0.564860133333333">
Value Description Examples
digit Arabic digit(s) 1,2,3
letter Letter(s) A,B,C,...,a, b, c
Continuous digits and/or letters (The sequence is
regarded as a single token)
chseq Chinese order 1 ㈠, ⑴, ①, I
chdigit Chinese digit 1 , 壹, 一
tianseq Chinese order 2 甲, Z, NJ, 丁
chsurn Surname 李, 吴, S, 王
notname Not name 将 , 对 , 那 , 的 , 是 , 说
loctch LOC tail char- C, 国, 岛, 海 , a ,
acter 庄 , 冲
orgtch ORG tail char- 府, 团 , 校 , 协 , 局 ,
acter 办, 军
other Other case r , mn, 息 , !, 。
</table>
<tableCaption confidence="0.978578">
Table 1. Some Examples of SVCL Feature
</tableCaption>
<bodyText confidence="0.999729857142857">
Each token is presented by its feature vector,
which is combined by these features we just dis-
cussed. Once all token feature (Maybe including
context features) values are determined, an ob-
servation sequence is feed into the model.
Each token state is a combination of the type
of the named entity it belongs to and the bound-
ary type it locates within. The entity types are
person name (PER), location name (LOC), or-
ganization name (ORG), date expression (DAT),
time expression (TIM), numeric expression
(NUM), and not named entity (OTH). The
boundary types are simply Beginning, Inside,
and Outside (BIO).
</bodyText>
<sectionHeader confidence="0.989118" genericHeader="method">
4 Feature Extraction for Close Tracks
</sectionHeader>
<bodyText confidence="0.955667238095238">
In close tracks, only character and word list fea-
tures which are extracted from training data are
applied for word segmentation. In NER track we
also include a named entity list extracted from
the training data.
To extract the list feature, we simply search
each text string among the list items in maximum
length forward way.
Taking the word segmentation task for in-
stance, when a text string c1c2...cn is given, we
tag each character into a BIO-WL style. If
cici+1...cj matches an item I of length j-i+1 and no
other item I’ of length k (k&gt;j-i+1) in the list
matches cici+1...cj...ck+i-1, then the characters are
tagged as follows:
ci ci+1 ... cj
B-WL I-WL ... I-WL
If no item in the list matches head subpart of
the string, then ci is tagged as 0.
The tagging operation iterates on the
remaining part until all characters are tagged.
</bodyText>
<sectionHeader confidence="0.931158" genericHeader="evaluation">
5 Evaluation
5.1 Results
</sectionHeader>
<bodyText confidence="0.9980638">
The system for our MSRA NER open track
submission has some bugs and was trained on a
much smaller training data set than the full set
the organizer provided. The results are very low,
see Table 2:
</bodyText>
<table confidence="0.99062075">
Accuracy 96.28%
Precision 83.20%
Recall 67.03%
FB1 74.24%
</table>
<tableCaption confidence="0.999426">
Table 2. MSRA NER Open
</tableCaption>
<bodyText confidence="0.998525">
When we fixed the bug and retrained on the
full training corpus, the result comes out to be as
follows:
</bodyText>
<table confidence="0.9913825">
Accuracy 98.24%
Precision 89.38%
Recall 83.07%
FB1 86.11%
</table>
<tableCaption confidence="0.999502">
Table 3. MSRA NER Open (retrained)
</tableCaption>
<bodyText confidence="0.9987735">
All the submissions on close tracks are trained
on 80% of the training corpora, the remaining
20% parts are used for development. The results
are shown in Table 4 and Table 5:
</bodyText>
<page confidence="0.994476">
182
</page>
<table confidence="0.99958175">
Measure Corpus
UPUC CityU CKIP MSRA
Recall 0.922 0.952 0.939 0.933
Precision 0.912 0.954 0.929 0.942
FB1 0.917 0.953 0.934 0.937
OOV 0.680 0.747 0.606 0.640
Recall
IV Recall 0.945 0.960 0.954 0.943
</table>
<tableCaption confidence="0.993171">
Table 4. WS Close
</tableCaption>
<table confidence="0.9999204">
Measure MSRA CityU LDC
Accuracy 92.44 97.80 93.82
Precision 81.64 92.76 81.43
Recall 31.24 81.81 59.53
FB1 45.19 86.94 68.78
</table>
<tableCaption confidence="0.997714">
Table 5. NER Close
</tableCaption>
<bodyText confidence="0.9974375">
The reason for low measure on MSRA NER
track exists in that we chose a much smaller
training data file encoded in CP936 (about 7% of
the full data set). This file may be an incomplete
output when the organizer transfers from another
encoding scheme.
</bodyText>
<subsectionHeader confidence="0.998048">
5.2 Errors from NER Track
</subsectionHeader>
<bodyText confidence="0.997831">
The NER errors in our system are mainly as fol-
lows:
</bodyText>
<listItem confidence="0.966516">
• Abbreviations
</listItem>
<bodyText confidence="0.999612333333333">
Abbreviations are very common among the er-
rors. Among them, a significant part of abbrevia-
tions are mentioned before their corresponding
full names. Some common abbreviations has no
corresponding full names appeared in document.
Here are some examples:
</bodyText>
<equation confidence="0.967897083333333">
R1:针对大陆人民申请进入 金 妈 地
区,[内政部警政署入出境管理局 ORG]
[金门 GPE]、[妈祖 GPE]服务站定于
明天
K:针对大陆人民申请进入 [金 GPE]
[妈 GPE]地区,[内政部警政署入出境
管理局 ORG][金门 GPE]、[妈祖
GPE]服务站定于明天......
R: 总后[嫩江基地 LOC]的先进事迹
K: [总后嫩江基地 LOC]的先进事迹
R: [中 丹 LOC]兩國
K: [中 LOC][丹 LOC]兩國
</equation>
<bodyText confidence="0.9885045">
In current system, the recognition is fully de-
pended on the linear-chain CRFs model, which is
heavily based on local window observation fea-
tures; no abbreviation list or special abbreviation
</bodyText>
<footnote confidence="0.354219">
1 R stands for system response, K for key.
</footnote>
<bodyText confidence="0.999753">
recognition involved. Because lack of constraint
checking on distant entity mentions, the system
fails to catch the interaction among similar text
fragments cross sentences.
</bodyText>
<listItem confidence="0.962287">
• Concatenated Names
</listItem>
<bodyText confidence="0.9919142">
For many reasons, Chinese names in titles and
some sentences, especially in news, are not sepa-
rated. The system often fails to judge the right
boundaries and the reasonable type classification.
For example:
</bodyText>
<figure confidence="0.7972955">
R:身边还有[张龙 赵虎 PER]王朝[马
汉 PER] 四个卫士
K:身边还有[张龙 PER][赵虎
PER][王朝 PER][马汉 PER] 四个卫
士
R:将[瓦西里斯 LOC]与[奥纳西斯
PER]比较
K:将[瓦西里斯 PER]与[奥纳西斯
PER]比较
• Hints
</figure>
<bodyText confidence="0.907743272727273">
Though it helps to recognize an entity at most
cases, the small-vocabulary-list hint feature may
recommend a wrong decision sometimes. For
instance, common surname character “王” in the
following sentence is wrongly labeled when no
word segmentation information given:
R:[希腊 LOC]船[王 康斯坦塔科普洛
斯 PER]
K:[希腊 LOC]船 王[康斯坦塔科普洛
斯 PER]
Other errors of this type may result from fail-
ing to identify verbs and prepositions, such as:
R:[中共中央 致 中国致公党十一大
ORG]的贺词 向[致公党 ORG]的同志们
K:[中共中央 ORG]致[中国致公党十
一大 ORG]的贺词......向[致公党 ORG]
的同志们
R:全国保护明天行动组委会 举行表彰会
K:[全国保护明天行动组委会 ORG]举行表
彰会
R:包公 赶驴
K:[包公 PER] 赶驴
</bodyText>
<listItem confidence="0.757312">
• Other Types:
</listItem>
<bodyText confidence="0.53577275">
R:特别助理 由喜贵 等也同机抵达。
K:特别助理[由喜贵 PER]等也同机抵
达。
R:脸谱上还有 日 月 的图案
</bodyText>
<page confidence="0.955917">
183
</page>
<bodyText confidence="0.4861145">
K: LOC][ LOC]k�1
N
</bodyText>
<sectionHeader confidence="0.979223" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999932727272727">
We mainly described a Chinese named entity
recognition system NER@ISCAS, which inte-
grates text, part-of-speech and a small-
vocabulary-character-lists feature for MSRA
NER open track under the framework of Condi-
tional Random Fields (CRFs) model. Although it
provides a unified framework to integrate multi-
ple flexible features, and to achieve global opti-
mization on input text sequence, the popular lin-
ear chained Conditional Random Fields model
often fails to catch semantic relations among re-
occurred mentions and adjoining entities in a
catenation structure.
The situations containing exact reoccurrence
and shortened occurrence enlighten us to take
more effort on feature engineering or post proc-
essing on abbreviations / recurrence recognition.
Another effort may be poured on the common
patterns, such as paraphrase, counting, and con-
straints on Chinese person name lengths.
From current point of view, enriching the hint
lists is also desirable.
</bodyText>
<sectionHeader confidence="0.967974" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.9973655">
This work is supported by the National Science
Fund of China under contract 60203007.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999462736842105">
Chinese 863 program. 2005. Results on Named
Entity Recognition. The 2004HTRDP Chinese
Information Processing and Intelligent Hu-
man-Machine Interface Technology Evalua-
tion.
Yuanyong Feng, Le Sun and Junlin Zhang. 2005.
Early Results for Chinese Named Entity Rec-
ognition Using Conditional Random Fields
Model, HMM and Maximum Entropy. IEEE
Natural Language Processing &amp; Knowledge
Engineering. Beijing: Publishing House,
BUPT. pp. 549~552.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields:
Probabilistic Models for Segmenting and La-
beling Sequence Data. ICML.
Shiwen Yu. 1999. Manual on Modern Chinese
Corpora Construction. Institute of Computa-
tional Language, Peking Unversity. Beijing.
</reference>
<page confidence="0.998705">
184
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.519219">
<title confidence="0.983822">Chinese Word Segmentation and Named Entity Recognition Based on Conditional Random Fields Models</title>
<author confidence="0.990852">Yuanyong Feng Le_Sun Yuanhua Lv</author>
<affiliation confidence="0.927198">Institute of Software, Chinese Academy of Sciences, Beijing, 100080,</affiliation>
<email confidence="0.599926">yuanyong02@ios.cn</email>
<email confidence="0.599926">sunle@ios.cn</email>
<email confidence="0.599926">yuanhua04@ios.cn</email>
<abstract confidence="0.992921454545455">This paper mainly describes a Chinese named entity recognition (NER) system NER@ISCAS, which integrates text, part-of-speech and a small-vocabularycharacter-lists feature for MSRA NER open track under the framework of Conditional Random Fields (CRFs) model. The techniques used for the close NER and word segmentation tracks are also presented.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chinese</author>
</authors>
<title>Results on Named Entity Recognition.</title>
<date>2005</date>
<booktitle>The 2004HTRDP Chinese Information Processing and Intelligent Human-Machine Interface Technology Evaluation.</booktitle>
<marker>Chinese, 2005</marker>
<rawString>Chinese 863 program. 2005. Results on Named Entity Recognition. The 2004HTRDP Chinese Information Processing and Intelligent Human-Machine Interface Technology Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuanyong Feng</author>
<author>Le Sun</author>
<author>Junlin Zhang</author>
</authors>
<title>Early Results for Chinese Named Entity Recognition Using Conditional Random Fields Model,</title>
<date>2005</date>
<booktitle>HMM and Maximum Entropy. IEEE Natural Language Processing &amp; Knowledge Engineering.</booktitle>
<pages>549--552</pages>
<publisher>Publishing House, BUPT.</publisher>
<location>Beijing:</location>
<marker>Feng, Le Sun, Zhang, 2005</marker>
<rawString>Yuanyong Feng, Le Sun and Junlin Zhang. 2005. Early Results for Chinese Named Entity Recognition Using Conditional Random Fields Model, HMM and Maximum Entropy. IEEE Natural Language Processing &amp; Knowledge Engineering. Beijing: Publishing House, BUPT. pp. 549~552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<publisher>ICML.</publisher>
<contexts>
<context position="709" citStr="Lafferty et al., 2001" startWordPosition="96" endWordPosition="99">ds Models Yuanyong Feng Le Sun Yuanhua Lv Institute of Software, Chinese Academy of Sciences, Beijing, 100080, China {yuanyong02, sunle, yuanhua04}@ios.cn Abstract This paper mainly describes a Chinese named entity recognition (NER) system NER@ISCAS, which integrates text, part-of-speech and a small-vocabularycharacter-lists feature for MSRA NER open track under the framework of Conditional Random Fields (CRFs) model. The techniques used for the close NER and word segmentation tracks are also presented. 1 Introduction The system NER@ISCAS is designed under the Conditional Random Fields (CRFs. Lafferty et al., 2001) framework. It integrates multiple features based on single Chinese character or space separated ASCII words. The early designed system (Feng et al., 2005) is used for the MSRA NER open track this year. The output of an external part-of-speech tagging tool and some carefully collected small-scale-character-lists are used as outer knowledge. The close word segmentation and named entity recognition tracks are also based on this system by some adjustments. The remaining of this paper is organized as follows. Section 2 introduces Conditional Random Fields model. Section 3 presents the details of o</context>
<context position="2084" citStr="Lafferty et al., 2001" startWordPosition="312" endWordPosition="315">ts. We end our paper with some conclusions and future works. 2 Conditional Random Fields Model Conditional random fields are undirected graphical models for calculating the conditional probability for output vertices based on input ones. While sharing the same exponential form with maximum entropy models, they have more efficient procedures for complete, non-greedy finitestate inference and training. Given an observation sequence o=&lt;o1, o2, ..., oT&gt;, linear-chain CRFs model based on the assumption of first order Markov chains defines the corresponding state sequence s′ probability as follows (Lafferty et al., 2001): 1 (s |o) =exp(∑ ∑ λk k t f s s t ( , , , )) o −1 t Zo Where Λ is the model parameter set, Zo is the normalization factor over all state sequences, fk is an arbitrary feature function, and λk is the learned feature weight. A feature function defines its value to be 0 in most cases, and to be 1 in some designated cases. For example, the value of a feature named “MAYBE-SURNAME” is 1 if and only if st-1 is OTHER, st is PER, and the t-th character in o is a common-surname. The inference and training procedures of CRFs can be derived directly from those equivalences in HMM. For instance, the forwa</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiwen Yu</author>
</authors>
<date>1999</date>
<booktitle>Manual on Modern Chinese Corpora Construction. Institute of Computational Language,</booktitle>
<location>Peking Unversity. Beijing.</location>
<contexts>
<context position="3717" citStr="Yu, 1999" startWordPosition="617" endWordPosition="618">le Features for Open Track In our system the text feature, part-of-speech (POS) feature, and small-vocabulary-characterlists (SVCL) feature are combined under a unified CRFs framework. T pΛ t k =1 181 Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 181–184, Sydney, July 2006. c�2006 Association for Computational Linguistics The text feature includes single Chinese character, some continuous digits or letters. POS feature is an important feature which carries some syntactic information. Our POS tag set follows the criterion of modern Chinese corpora construction (Yu, 1999), which contains 39 tags. The last feature is based on lists. We first list all digits and English letters in Chinese. Then most frequently used character feature in Chinese NER are collected, including 100 single character surnames, 100 location tail characters, and 40 organization tail characters. The total number of these items in our lists is less than 600. The lists altogether make up a list feature (SVCL). Some examples of this list are given in Table 1. Value Description Examples digit Arabic digit(s) 1,2,3 letter Letter(s) A,B,C,...,a, b, c Continuous digits and/or letters (The sequenc</context>
</contexts>
<marker>Yu, 1999</marker>
<rawString>Shiwen Yu. 1999. Manual on Modern Chinese Corpora Construction. Institute of Computational Language, Peking Unversity. Beijing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>