<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000105">
<title confidence="0.9987145">
An Double Hidden HMM and an CRF for Segmentation Tasks with
Pinyin’s Finals
</title>
<author confidence="0.999208">
Huixing Jiang Zhe Dong
</author>
<affiliation confidence="0.859532666666667">
Center for Intelligence Science and Technology
Beijing University of Posts and Telecommunications
Beijing, China
</affiliation>
<email confidence="0.996295">
jhx0129@163.com jimmybupt@gmail.com
</email>
<sectionHeader confidence="0.9938" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999928941176471">
We have participated in the open tracks
and closed tracks on four corpora of Chi-
nese word segmentation tasks in CIPS-
SIGHAN-2010 Bake-offs. In our experi-
ments, we used the Chinese inner phonol-
ogy information in all tracks. For open
tracks, we proposed a double hidden lay-
ers’ HMM (DHHMM) in which Chinese
inner phonology information was used as
one hidden layer and the BIO tags as an-
other hidden layer. N-best results were
firstly generated by using DHHMM, then
the best one was selected by using a new
lexical statistic measure. For close tracks,
we used CRF model in which the Chinese
inner phonology information was used as
features.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999949028571429">
Chinese language has many characteristics not
possessed by other languages. One obvious is
that the written Chinese text does not have explicit
word boundaries like western languages. So word
segmentation became very significative for Chi-
nese information processing, and is usually con-
sidered as the first step of any further processing.
Identifying words has been a basic task for many
researchers who have devoted themselves on Chi-
nese text processing.
The biggest characteristic of Chinese language
is its trinity of sound, form and meaning (Pan,
2002). Hanyu Pinyin is the form of sound for
Chinese text and the Chinese phonology informa-
tion is explicit expressed by Pinyin which is the
inner features of Chinese Characters. And it nat-
urally contributes to the identification of Out-Of-
Vacabulary words (OOV).
In our work, Chinese phonology information is
used as basic features of Chinese characters in all
models. For open tracks, we propose a new dou-
ble hidden layers HMM in which a new phonol-
ogy information is built in as a hidden layer, a
new lexical association is proposed to deal with
the OOV questions and domains’ adaptation ques-
tions. And for closed tracks, CRF model has been
used, combined with Chinese inner phonology in-
formation. We used the CRF++ package Version
0.43 by Taku Kudol.
In the rest sections of this paper, we firstly in-
troduce the Chinese phonology in Section 2. Then
in the Section 3, the models used in our tasks are
presented. And the experiments and results are
described in Section 4. Finally, we give the con-
clusions and make prospect on future work.
</bodyText>
<sectionHeader confidence="0.935975" genericHeader="method">
2 Chinese Phonology
</sectionHeader>
<bodyText confidence="0.999729307692308">
Hanyu Pinyin is the form of sound for Chi-
nese text and the Chinese phonology informa-
tion is explicit expressed by Pinyin. It is cur-
rently the most commonly used romanization sys-
tem for Standard Mandarin. Hanyu means the
Chinese language, and Pinyin means “phonetics”,
or more literally, “spelling sound” or “spelled
sound” (wikipedia, 2010). The system has been
employed to teach Mandarin as home language
or as second language by China, Malaysia, Sin-
gapore et.al. Pinyin has been the most Chinese
character’s input method for computers and other
devices.
</bodyText>
<footnote confidence="0.94901">
1http://crfpp.sourceforge.net/
</footnote>
<bodyText confidence="0.999954904761905">
The romanization system was developed by a
government committee in the People’s Repub-
lic of China, and approved by the Chinese gov-
ernment on February 11, 1958. The Interna-
tional Organization for Standardization adopted
pinyin as the international standard in 1982, and
since then it has been adopted by many other
organizations(wikipedia, 2010). In this system,
pinyin is composed by initials(pinyin: shengmu),
finals(pinyin: yunmu) and tones(pinyin: sheng-
diao) instead of consonants and vowels used in
European language. For example, the Pinyin of
”中” is ”zhong1” composed by ”zh”, ”ong” and
”1”. In which ”zh” is initial, ”ong” is final and
”1” is the tone.
Every language has its rhythm and rhyme, so
Chinese is no exception. The rhythm system are
the driving force from the unconscious habit of
language(Edward, 1921). And the Pinyin’s finals
contribute the Chinese rhythm system, Which is
the basic assumption our research based on.
</bodyText>
<sectionHeader confidence="0.99579" genericHeader="method">
3 Algorithms
</sectionHeader>
<bodyText confidence="0.999879666666667">
Generally the task of segmentation can be viewed
as a sequence labeling problem. We first define a
tag set as TS = {B, I, E, S}, shown in Table 1.
</bodyText>
<tableCaption confidence="0.994208">
Table 1: The tag set used in this paper.
</tableCaption>
<bodyText confidence="0.869081857142857">
Label Explanation
B beginning character of a word
I inner character of a word
E end character of a word
S a single character as a word
For the piece ”是 英 国 前 王 妃 戴 安 娜” of
the example described in the experiments section,
firstly, the TS tags are labeled to it. And its re-
sult is ”是/S 英/B 国/E 前/S 王/B 妃/E 戴/B 安/I
娜/E”. Then the tags are combined sequentially to
get the finally result ”是 英国 前 王妃 戴安娜”.
In this section, A novel HMM solution is pre-
sented firstly for open tracks. Then the CRF solu-
tion for closed tracks is introduced.
</bodyText>
<subsectionHeader confidence="0.997067">
3.1 Double hidden layers’ HMM
</subsectionHeader>
<bodyText confidence="0.999859875">
For a given piece of Chinese sentence, X =
x1x2 ... xT, where xi, i = 1, ... , T is a Chinese
character. Suppose that we can give each Chinese
character xi a Pinyin’s final yi. And suppose the
label sequence of X is S = s1s2 ... sT, where
si E TS is the tag of xi. Then what we want to
find is an optimal tag sequence S* which is de-
fined in (1).
</bodyText>
<equation confidence="0.9986465">
S* = arg maxP(S,Y |X)
S
= arg max
S P(X|S,Y )P(S,Y ) (1)
</equation>
<bodyText confidence="0.9992988">
The model is described in Fig. 1. For a given
piece of Chinese character strings, One hidden
layer is label sequence S. Another hidden layer is
Pinyin’s finals sequence Y . The observation layer
is the given piece of Chinese characters X.
</bodyText>
<figureCaption confidence="0.998952">
Figure 1: Double Hidden Markov Model
</figureCaption>
<bodyText confidence="0.996740333333333">
For transition probability, second-order Markov
model is used to estimate probability of the double
hidden sequences as described in (2).
</bodyText>
<equation confidence="0.998738666666667">
∏ p(st, yt|st−1, yt−1) (2)
P (S, Y ) =
t
</equation>
<bodyText confidence="0.974057">
For emission probability, we keep the first-
order Markov assumption as shown in (5).
</bodyText>
<equation confidence="0.9907825">
P (X|S, Y ) = ∏ p(xt|st, yt) (3)
t
</equation>
<subsectionHeader confidence="0.529046">
3.1.1 Nbest results
</subsectionHeader>
<bodyText confidence="0.725254642857143">
Based on the work of (Jiang, 2010), a word lat-
tice is also built firstly, then in the second step, the
backward A* algorithm is used to find the top N
results instead of using the backward viterbi al-
gorithm to find the top one. The backward A*
search algorithm is described as follow (Wang,
2002; Och, 2001).
3.1.2 Reranking with a new lexical statistic
measure
Given two random Chinese characters X and Y
and assume that they appears in an aligned region
of the corpus. The distribution of the two random
Chinese characters could be depicted by a 2 by 2
contingency table shown in Fig. 2(Chang, 2002).
</bodyText>
<figureCaption confidence="0.992231">
Figure 2: A 2 by 2 contingency table
</figureCaption>
<bodyText confidence="0.969531">
In Fig. 2, a is the counts of X and Y co-occur; b
is the counts of the cases that X occurs but Y does
not; c is the counts of the cases that X does not
occur but Y does; d is the counts of the cases that
both X and Y do not occur. The Log-likelihood
rate is calculated by (4).
</bodyText>
<figure confidence="0.818827916666667">
a · N
LLR(x, y) = 2(a · log (a + b) · (a + c)
b · N
+ b · log (a + b) · (b + d)
c · N
+ c · log (c + d) · (a + c)
d · N
+ d · log (c + d) · (b + d)) (4)
For the N-best result described in sec. 3.1.1,
they can be re-ranked by (5).
LLR(xk, yk))
(5)
</figure>
<bodyText confidence="0.986828888888889">
where scoreh is the negative log value of
P(5,Y |X). K is the number of breaks in X and
xk is the left Chinese character of the k break
and yk is the right Chinese character of the k
break. A is the regulatory factor(in our experi-
ments A = 0.45).
Bigger value of LLR(xk, yk) means stronger
ability in combining of the two characters xk and
yk, then they should not be segmented.
</bodyText>
<subsectionHeader confidence="0.964905">
3.2 CRF model for closed tracks
</subsectionHeader>
<bodyText confidence="0.9973815">
Conditional random field, as statistical sequence
labeling model, has been used widely in segmen-
tation(Lafferty, 2001; Zhao, 2006). In the closed
tracks of the paper, we also use it.
</bodyText>
<subsectionHeader confidence="0.816959">
3.2.1 Feature templates
</subsectionHeader>
<bodyText confidence="0.999799">
We adopted two main kinds of features: n-gram
features and Pinyin’s finals features. The n-gram
feature set is quite orthodox, they are, namely, C-
2, C-1, C0, C1, C2, C-2C-1, C-1C0, C0C1, C1C2.
The Pinyin’s finals feature set is the same as n-
gram feature set. They are described in Table. 2.
</bodyText>
<tableCaption confidence="0.644248">
Table 2: Feature templates
Templates Category
</tableCaption>
<equation confidence="0.9383715">
C-2, C-1, C0, C1, C2 N-gram: Unigram
C-2C-1, C-1C0, C0C1, C1C2 N-gram: Bigram
P-2, P-1, P0, P1, P2 Phonetic: Unigram
P-2P-1, P-1P0, P0P1, P1P2 Phonetic: Bigram
</equation>
<sectionHeader confidence="0.99647" genericHeader="evaluation">
4 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.857151">
4.1 Dataset
</subsectionHeader>
<bodyText confidence="0.999905285714286">
We build a basic words dictionary for DHHMM
and a Pinyin’s finals dictionary for both DHHMM
and CRF from The Grammatical Knowledge-base
of Contemporary Chinese(Yu, 2001). For the fi-
nals dictionary, we give each Chinese character a
final extracted from its Pinyin. When it comes to
a polyphone, we just combine its all finals simply
to one. For example, ”中{ong}”, ”差{a&amp;ai&amp;i}”.
The training corpus (5,769 KB) we used is the
Labeled Corpus provided by the organizer. We
firstly add the Pinyin’s finals to each Chinese
character of it, then we train the parameters of
DHHMM and CRF model on it.
And the test corpus contains four domains: Lit-
erature (A), Computer (B), Medicine (C) and Fi-
nance(D).
The LLR function’s parameters{a, b, c, d} are
counted from the current test corpus A, B, C, or
D. It’s means that for segmenting A, the LLR pa-
rameters are counted from A, so the same for seg-
menting B, C and D.
</bodyText>
<equation confidence="0.96666">
5* = arg min(scoreh(5) + A
x
S K k=1
</equation>
<subsectionHeader confidence="0.960822">
4.2 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999877777777778">
The date, time, numbers and symbols information
are easily identified by rules. We propose four
regular expressions’ processes, in which the reg-
ular expressions’ processes are handled one after
another in order of date, time, numbers and sym-
bols. By now, a rough segmentation can be done.
For a character stream, the date, time, numbers
and symbols are firstly identified, then the whole
stream can be divided by these units to some
pieces of character strings which will be segment
by the models described in sec. 3. For example,
a character stream ”2009年的8月31日,是英国
前王妃戴安娜12周年忌日。” will be divided
to ”2009年 的 8月 31日 , 是英国前王妃戴安
娜 12 周年忌日 。”. Then the pieces ”的”, ”是
英国前王妃戴安娜”, ”周年忌日” will be seg-
mented sequentially by the models described in
Section 3.
</bodyText>
<subsectionHeader confidence="0.984153">
4.3 Results on DHHMM
</subsectionHeader>
<bodyText confidence="0.889952">
We evaluate our system by Precision Rate(6), Re-
call Rate(7), F1 measure(8) and OOV(Out-Of-
Vocabulary) Recall rate(9).
</bodyText>
<equation confidence="0.898333">
C(correct words in segmented result)
P=
C(words in segmented result)
C(correct words in segmented result)
R =
C(words in standard result)
2 * P * R
F1 = (8)
P + R
C(correct OOV in segmented result)
OR =
C(OOV in standard result)
(9)
In (6-9), C(· · ·) is the count of (· · ·).
</equation>
<bodyText confidence="0.9978765">
Table 3 are the results of the DHHMM on open
tracks.
In Table 3, OOVRR is the recall rate of OOV,
IVRR is the recall rate of IV(In Vocabulary).
</bodyText>
<subsectionHeader confidence="0.992926">
4.4 Postprocessing for CRF and Results on It
</subsectionHeader>
<bodyText confidence="0.974459833333333">
Since the CRF segmenter will not always return
a valid tag sequence that can be translated into
segmentation result, some corrections should be
made if such error occurs. We devised a dynamic
programming routine to tackle this problem: first
we compute the valid tag sequence that closest to
</bodyText>
<tableCaption confidence="0.82175">
Table 3: Results of open tracks using DHHMM:
</tableCaption>
<table confidence="0.858263625">
Literature (A), Computer (B), Medicine (C) and
Finance(D)
A B C D
R 0.893 0.918 0.917 0.928
P 0.918 0.896 0.907 0.934
F1 0.905 0.907 0.912 0.931
OOV RR 0.803 0.771 0.704 0.808
IV RR 0.899 0.945 0.943 0.939
</table>
<bodyText confidence="0.9556246">
the output of CRF segmenter (by term closest, we
mean least hamming distance), if there is a tie, we
choose the one has the least ’S’ tags, if the tie still
exists, we choose the one that comes lexicograph-
ically earlier (B &lt; I &lt; E &lt; S, described in
</bodyText>
<tableCaption confidence="0.8962636">
Table. 1). Table4 are the results of the CRF on
closed tracks.
Table 4: Results of closed tracks using CRF: Lit-
erature (A), Computer (B), Medicine (C) and Fi-
nance(D)
</tableCaption>
<table confidence="0.999405333333333">
A B C D
R 0.945 0.946 0.94 0.956
P 0.946 0.914 0.928 0.952
F1 0.946 0.93 0.934 0.954
OOV RR 0.816 0.808 0.761 0.849
IV RR 0.954 0.971 0.962 0.966
</table>
<bodyText confidence="0.991405714285714">
From the results of Table 3 and Table 4, we
can observe that the CRF model outperforms the
DHHMM by average 2.72% in F1 measure. In the
other hand, from Table 5, we can see that the com-
putation cost in DHHMM is less than half of the
time cost and lower one-fifth memory cost than
CRF model.
</bodyText>
<tableCaption confidence="0.8210445">
Table 5: The computation cost in DHHMM and
CRF
</tableCaption>
<table confidence="0.485926666666667">
Time cost(ms) Memory cost(MB)
DHHMM 34398 16.3
CRF 43415 35
</table>
<sectionHeader confidence="0.985739" genericHeader="conclusions">
5 Conclusions and Future works
</sectionHeader>
<bodyText confidence="0.9999788">
This paper has presented a double hidden lawyers
HMM for Chinese word segmentation task in
SIGHAN bakeoff 2010. It firstly created N top
results and then select the best one from it by a
new lexical association.
Chinese phonology (specially by Pinyin’s final
in text) is very useful inner information of Chinese
language, which is the first time used in our mod-
els. We have used it in both DHHMM and CRF
model.
In future work, there are lots of improvements
can be done. Firstly, which polyphone’s finals
should be used in a given context is a visible ques-
tion. And the strategy to train the parameter A de-
scribed in 3.1.2 can also be improved.
</bodyText>
<sectionHeader confidence="0.99856" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999449666666667">
This research has been partially supported by
the National Science Foundation of China (NO.
NSFC90920006). We also thank Caixia Yuan for
leading our discuss, Li Sun, Peng Zhang, Yaojing
Chen, Zhixu Lin, Gan Lin, Guannan Fang for their
useful helps in this work.
</bodyText>
<sectionHeader confidence="0.999194" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999722285714286">
Wenguo Pan. 2002. zibenwei yu hanyu yanjiu:120–
141. East China Normal University Press.
Sapir Edward 1921. Language: An introduction to the
study of speech:230. New York: Harcourt, Brace
and company.
wikipedia. 2010. Pinyin. http://en.wikipedia.org/wiki
/Pinyin#cite note-6.
Baobao Chang, Pernilla Danielsson, and Wolfgang
Teubert. 2002. Extraction of translation unit from
chinese-english parallel corpora, Proceedings of
the first SIGHAN workshop on Chinese language
processing:1–5.
Huixing Jiang, Xiaojie Wang, Jilei Tian. 2010.
Second-order HMMfor Event Extraction from Short
Message, 15th International Conference on Ap-
plications of Natural Language to Information Sys-
tems, Cardiff, Wales, UK.
Franz Josef Och, Nicola Ueffing, Hermann Ney. 2001.
An Efficient A* Search Algorithm for Statistical Ma-
chine Translation, Proceedings of the ACL Work-
shop on Data-Driven methods in Machine Transla-
tion 14(Toulouse, France): 1-8.
Ye-Yi Wang, Alex Waibel. 2002. Decoding Algo-
rithm in Statistical Machine Translation, Proceed-
ings of the 35th Annual Meeting of the Association
for Computational Linguistics and Eighth Confer-
ence of the European Chapter of the Association for
Computational Linguistics: 366-372.
Yu Shiwen, Zhu Xuefeng, Wang Hui. 2001. New
Progress of the Grammatical Knowledge-base of
Contemporary Chinese, ZHONGWEN XINXI
XUEBAO, 2001 Vol. 01.
John Lafferty, A.Mccallum, F.Pereira. 2001. Condi-
tional Random Field: Probabilitic Models for Seg-
menting and Labeling Sequence Data., Proceedings
of the Eighteenth International Conference on Ma-
chine Learning: 282–289.
Hai Zhao, Changning Huang, Mu Li. 2006. An
Improved Chinese Word Segmentation System with
Conditional Random Field, Proceedings of the Fifth
SIGHAN Workshop on Chinese Language Process-
ing (SIGHAN-5)(Sydney, Australia):162-165.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.771001">
<title confidence="0.9933375">An Double Hidden HMM and an CRF for Segmentation Tasks Pinyin’s Finals</title>
<author confidence="0.988185">Huixing Jiang Zhe Dong</author>
<affiliation confidence="0.967492333333333">Center for Intelligence Science and Beijing University of Posts and Beijing,</affiliation>
<email confidence="0.993615">jhx0129@163.comjimmybupt@gmail.com</email>
<abstract confidence="0.993335555555555">We have participated in the open tracks and closed tracks on four corpora of Chinese word segmentation tasks in CIPS- SIGHAN-2010 Bake-offs. In our experiments, we used the Chinese inner phonology information in all tracks. For open tracks, we proposed a double hidden layers’ HMM (DHHMM) in which Chinese inner phonology information was used as one hidden layer and the BIO tags as another hidden layer. N-best results were firstly generated by using DHHMM, then the best one was selected by using a new lexical statistic measure. For close tracks, we used CRF model in which the Chinese inner phonology information was used as features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Wenguo Pan</author>
</authors>
<title>zibenwei yu hanyu yanjiu:120– 141. East China</title>
<date>2002</date>
<publisher>Normal University Press.</publisher>
<contexts>
<context position="1464" citStr="Pan, 2002" startWordPosition="230" endWordPosition="231">ology information was used as features. 1 Introduction Chinese language has many characteristics not possessed by other languages. One obvious is that the written Chinese text does not have explicit word boundaries like western languages. So word segmentation became very significative for Chinese information processing, and is usually considered as the first step of any further processing. Identifying words has been a basic task for many researchers who have devoted themselves on Chinese text processing. The biggest characteristic of Chinese language is its trinity of sound, form and meaning (Pan, 2002). Hanyu Pinyin is the form of sound for Chinese text and the Chinese phonology information is explicit expressed by Pinyin which is the inner features of Chinese Characters. And it naturally contributes to the identification of Out-OfVacabulary words (OOV). In our work, Chinese phonology information is used as basic features of Chinese characters in all models. For open tracks, we propose a new double hidden layers HMM in which a new phonology information is built in as a hidden layer, a new lexical association is proposed to deal with the OOV questions and domains’ adaptation questions. And f</context>
</contexts>
<marker>Pan, 2002</marker>
<rawString>Wenguo Pan. 2002. zibenwei yu hanyu yanjiu:120– 141. East China Normal University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sapir Edward</author>
</authors>
<title>Language: An introduction to the study of speech:230.</title>
<date>1921</date>
<publisher>Harcourt, Brace and company.</publisher>
<location>New York:</location>
<contexts>
<context position="3927" citStr="Edward, 1921" startWordPosition="634" endWordPosition="635">on adopted pinyin as the international standard in 1982, and since then it has been adopted by many other organizations(wikipedia, 2010). In this system, pinyin is composed by initials(pinyin: shengmu), finals(pinyin: yunmu) and tones(pinyin: shengdiao) instead of consonants and vowels used in European language. For example, the Pinyin of ”中” is ”zhong1” composed by ”zh”, ”ong” and ”1”. In which ”zh” is initial, ”ong” is final and ”1” is the tone. Every language has its rhythm and rhyme, so Chinese is no exception. The rhythm system are the driving force from the unconscious habit of language(Edward, 1921). And the Pinyin’s finals contribute the Chinese rhythm system, Which is the basic assumption our research based on. 3 Algorithms Generally the task of segmentation can be viewed as a sequence labeling problem. We first define a tag set as TS = {B, I, E, S}, shown in Table 1. Table 1: The tag set used in this paper. Label Explanation B beginning character of a word I inner character of a word E end character of a word S a single character as a word For the piece ”是 英 国 前 王 妃 戴 安 娜” of the example described in the experiments section, firstly, the TS tags are labeled to it. And its result is ”是</context>
</contexts>
<marker>Edward, 1921</marker>
<rawString>Sapir Edward 1921. Language: An introduction to the study of speech:230. New York: Harcourt, Brace and company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>wikipedia</author>
</authors>
<date>2010</date>
<note>Pinyin. http://en.wikipedia.org/wiki /Pinyin#cite note-6.</note>
<contexts>
<context position="2859" citStr="wikipedia, 2010" startWordPosition="468" endWordPosition="469">per, we firstly introduce the Chinese phonology in Section 2. Then in the Section 3, the models used in our tasks are presented. And the experiments and results are described in Section 4. Finally, we give the conclusions and make prospect on future work. 2 Chinese Phonology Hanyu Pinyin is the form of sound for Chinese text and the Chinese phonology information is explicit expressed by Pinyin. It is currently the most commonly used romanization system for Standard Mandarin. Hanyu means the Chinese language, and Pinyin means “phonetics”, or more literally, “spelling sound” or “spelled sound” (wikipedia, 2010). The system has been employed to teach Mandarin as home language or as second language by China, Malaysia, Singapore et.al. Pinyin has been the most Chinese character’s input method for computers and other devices. 1http://crfpp.sourceforge.net/ The romanization system was developed by a government committee in the People’s Republic of China, and approved by the Chinese government on February 11, 1958. The International Organization for Standardization adopted pinyin as the international standard in 1982, and since then it has been adopted by many other organizations(wikipedia, 2010). In this</context>
</contexts>
<marker>wikipedia, 2010</marker>
<rawString>wikipedia. 2010. Pinyin. http://en.wikipedia.org/wiki /Pinyin#cite note-6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Baobao Chang</author>
<author>Pernilla Danielsson</author>
<author>Wolfgang Teubert</author>
</authors>
<title>Extraction of translation unit from chinese-english parallel corpora,</title>
<date>2002</date>
<booktitle>Proceedings of the first SIGHAN workshop on Chinese language processing:1–5.</booktitle>
<marker>Chang, Danielsson, Teubert, 2002</marker>
<rawString>Baobao Chang, Pernilla Danielsson, and Wolfgang Teubert. 2002. Extraction of translation unit from chinese-english parallel corpora, Proceedings of the first SIGHAN workshop on Chinese language processing:1–5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huixing Jiang</author>
<author>Xiaojie Wang</author>
<author>Jilei Tian</author>
</authors>
<title>Second-order HMMfor Event Extraction from Short Message,</title>
<date>2010</date>
<booktitle>15th International Conference on Applications of Natural Language to Information Systems,</booktitle>
<location>Cardiff, Wales, UK.</location>
<marker>Jiang, Wang, Tian, 2010</marker>
<rawString>Huixing Jiang, Xiaojie Wang, Jilei Tian. 2010. Second-order HMMfor Event Extraction from Short Message, 15th International Conference on Applications of Natural Language to Information Systems, Cardiff, Wales, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>An Efficient A* Search Algorithm for Statistical Machine Translation,</title>
<date>2001</date>
<booktitle>Proceedings of the ACL Workshop on Data-Driven methods in Machine Translation 14(Toulouse,</booktitle>
<pages>1--8</pages>
<location>France):</location>
<marker>Och, Ueffing, Ney, 2001</marker>
<rawString>Franz Josef Och, Nicola Ueffing, Hermann Ney. 2001. An Efficient A* Search Algorithm for Statistical Machine Translation, Proceedings of the ACL Workshop on Data-Driven methods in Machine Translation 14(Toulouse, France): 1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ye-Yi Wang</author>
<author>Alex Waibel</author>
</authors>
<title>Decoding Algorithm in Statistical Machine Translation,</title>
<date>2002</date>
<booktitle>Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics:</booktitle>
<pages>366--372</pages>
<marker>Wang, Waibel, 2002</marker>
<rawString>Ye-Yi Wang, Alex Waibel. 2002. Decoding Algorithm in Statistical Machine Translation, Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics: 366-372.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu Shiwen</author>
<author>Zhu Xuefeng</author>
<author>Wang Hui</author>
</authors>
<date>2001</date>
<booktitle>New Progress of the Grammatical Knowledge-base of Contemporary Chinese, ZHONGWEN XINXI XUEBAO,</booktitle>
<volume>01</volume>
<marker>Shiwen, Xuefeng, Hui, 2001</marker>
<rawString>Yu Shiwen, Zhu Xuefeng, Wang Hui. 2001. New Progress of the Grammatical Knowledge-base of Contemporary Chinese, ZHONGWEN XINXI XUEBAO, 2001 Vol. 01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>F Pereira A Mccallum</author>
</authors>
<title>Conditional Random Field: Probabilitic Models for Segmenting and Labeling Sequence Data.,</title>
<date>2001</date>
<booktitle>Proceedings of the Eighteenth International Conference on Machine Learning:</booktitle>
<pages>282--289</pages>
<marker>Lafferty, Mccallum, 2001</marker>
<rawString>John Lafferty, A.Mccallum, F.Pereira. 2001. Conditional Random Field: Probabilitic Models for Segmenting and Labeling Sequence Data., Proceedings of the Eighteenth International Conference on Machine Learning: 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Changning Huang</author>
<author>Mu Li</author>
</authors>
<title>An Improved Chinese Word Segmentation System with Conditional Random Field,</title>
<date>2006</date>
<booktitle>Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing (SIGHAN-5)(Sydney,</booktitle>
<pages>162--165</pages>
<marker>Zhao, Huang, Li, 2006</marker>
<rawString>Hai Zhao, Changning Huang, Mu Li. 2006. An Improved Chinese Word Segmentation System with Conditional Random Field, Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing (SIGHAN-5)(Sydney, Australia):162-165.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>