<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014950">
<title confidence="0.998091">
Adapting naturally occurring test suites
for evaluation of clinical question answering
</title>
<author confidence="0.987203">
Dina Demner-Fushman
</author>
<affiliation confidence="0.9747675">
Lister Hill National Center for Biomedical Communications,
National Library of Medicine, NIH, Bethesda, MD 20894, USA
</affiliation>
<email confidence="0.997636">
ddemner@mail.nih.gov
</email>
<sectionHeader confidence="0.998591" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999337285714286">
This paper describes the structure of a test
suite for evaluation of clinical question an-
swering systems; presents several manually
compiled resources found useful for test suite
generation; and describes the adaptation of
these resources for evaluation of a clinical
question answering system.
</bodyText>
<sectionHeader confidence="0.999395" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.989679047619048">
The community-wide interest in rapid development
in many areas of natural language processing and in-
formation retrieval resulted in creation of reusable
test collections in large-scale evaluations such as the
Text REtrieval Conference (TREC)1. Researchers in
more specific areas, for which no TREC or other col-
lections are available, have to create or find suitable
test collections to evaluate their systems.
For example, Cramer et al. (2006) recruited vol-
unteers and quickly gathered a sizeable corpus of
question-answer pairs for evaluation of German
open-domain question answering systems. This was
achieved through a Web-based tool that allowed
marking up “interesting” passages in Wikipedia ar-
ticles and then asking questions about the content
of those passages. This appealing approach can not
easily be applied in the domain of clinical ques-
tion answering because the quality of the questions
and answers as well as the answer completeness
are paramount. A test suite for evaluation of clini-
cal question answering systems should contain a set
</bodyText>
<footnote confidence="0.941127">
1http://trec.nist.gov/
</footnote>
<bodyText confidence="0.999828741935484">
of real-life questions asked by clinicians and high-
quality answers compiled by experts. The answers
should be presented in the form deemed useful by
clinicians.
One of the benefits of focusing on a specific do-
main, such as clinical question answering, is that the
user-needs and desirable results are well-studied and
their descriptions are readily-available. In the case
of clinical question answering, clinicians’ desider-
ata are: to see a “bottom-line advice” first, have
on-demand access to the context that was used in
generation of the advice, and finally have access
to the original sources of information (Ely et al.,
2005). A fair number of high-quality manually cre-
ated collections present answers to clinical questions
in this form and could be obtained online. Three par-
tially freely-available sources: Family Practitioner
Inquiry Network (FPIN)2, Parkhurst Exchange Fo-
rum (PE)3, and BMJ Clinical Evidence (BMJ-CE)4
were used to design and develop the presented test
suites and evaluation methods.
Although there seems to be a distinction between
test collections and test suites (Co-
hen et al., 2004) (the former defined as “pieces
of text” and associated with corpora, the latter, as
lists of specially constructed sentences, or sentence
sequences, or sentence fragments (Balkan et al.,
1994)), evaluation of answers to clinical questions
crosses this boundary and requires the availability
of carefully generated sentence fragments as well as
suitable document collections.
</bodyText>
<footnote confidence="0.999977666666667">
2http://www.primeanswers.org/primeanswers/
3http://www.parkhurstexchange.com/qa/index.php
4http://www.clinicalevidence.com/ceweb/conditions/index.jsp
</footnote>
<page confidence="0.988441">
21
</page>
<subsubsectionHeader confidence="0.827658">
Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 21–22,
</subsubsectionHeader>
<page confidence="0.364171">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<sectionHeader confidence="0.894" genericHeader="method">
2 Test suite structure
</sectionHeader>
<bodyText confidence="0.967016769230769">
The multi-tiered answer model of the FPIN and
BMJ-CE resources is adapted in this work. The top
tier contains the “bottom-line advice”. FPIN pro-
vides the key-points of the advice in the form of a
short sentence sequence, whereas BMJ-CE provides
a list of sentence fragments (see Figure 1). Both
sources employ experts in question areas to care-
fully construct the answers. The second tier elab-
orates each of the key-points in 2-3 paragraph-long
summaries generated by the same experts. The third
tier provides references to the original sources used
in answer compilation.
Likely to be beneficial:
</bodyText>
<listItem confidence="0.958926666666667">
• Angiotensin converting enzyme inhibitors
• Aspirin
• β Blockers...
</listItem>
<subsectionHeader confidence="0.558272">
Trade-off between benefits and harms:
</subsectionHeader>
<listItem confidence="0.618504">
• Nitrates (in the absence of thrombolysis)
Likely to be ineffective or harmful:.. .
</listItem>
<figureCaption confidence="0.96232825">
Figure 1: The top tier of a multi-tiered answer to the clin-
ical question How to improve outcomes in acute myocar-
dial infarction? contains key-points generated by a panel
of cardiologists.
</figureCaption>
<sectionHeader confidence="0.926632" genericHeader="method">
3 Using the test suite in an evaluation
</sectionHeader>
<bodyText confidence="0.999985454545455">
The answer presented in Figure 1 can be used to
evaluate a system’s answer to this question by ex-
tracting the reference list from the FPIN or BMJ-CE
answer. Similarly, the second-tier summaries can be
used to evaluate the context for the key-points gener-
ated by a system. The references can be used to eval-
uate the quality of the original sources retrieved by a
system if the documents in both lists are represented
using their unique identifiers: DOI or a PubMed5
identifier. Availability of these test suites provides
for the following evaluation forms:
</bodyText>
<listItem confidence="0.998817">
• diagnostic, in which developers could evaluate
how a tier is affected by changes in its own
module(s) or in the underlying tiers;
</listItem>
<footnote confidence="0.719615">
5http://www.ncbi.nlm.nih.gov/sites/entrez
</footnote>
<listItem confidence="0.943849333333333">
• task-oriented, in which the system is evaluated
as a whole on its ability to answer clinical ques-
tions.
</listItem>
<bodyText confidence="0.999554833333333">
It is conceivable to evaluate a system as a whole
by evaluating its performance in each tier and then
combining the results. In a task-oriented evalua-
tion, it seems reasonable to evaluate the quality of
the first-tier answer and verify the adequacy of the
second-tier context.
</bodyText>
<subsectionHeader confidence="0.983653">
3.1 Caveats
</subsectionHeader>
<bodyText confidence="0.99997625">
Even the simplest case of the top-tier evaluation,
checking the list of fragments generated by a sys-
tem against the reference list, ideally should be con-
ducted manually by a person with biomedical back-
ground. For example, Acetylsalicylic acid in a sys-
tem’s answer needs to be matched to Aspirin in the
reference list. Automation of this step is possible
through mapping of both lists to an ontology, e.g.,
UMLS6, but such evaluation will be significantly
less accurate and potentially biased (if a system uses
the same mapping algorithm to find the answer).
A manual evaluation based on 30 of 54 BMJ-CE
question-answer pairs in the presented test suite is
described in (Demner-Fushman and Lin, 2006). An-
other 50 question-answer pairs originated in FPIN
and PE.
</bodyText>
<sectionHeader confidence="0.999141" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999407105263158">
Cramer I., Leidner J.L. and Klakow D. 2006. Building
an Evaluation Corpus for German Question Answering
by Harvesting Wikipedia. LREC-2006, Genoa, Italy.
Cohen K.B., Tanabe L., Kinoshita S., and Hunter L.
2004. A resource for constructing customized test
suites for molecular biology entity identification sys-
tems. HLT-NAACL 2004 Workshop: Biolink 2004,
Boston, Massachusetts
Balkan L., Netter K., Arnold D. and Meijer S. 1994.
TSNLP. Test Suites for Natural Language Processing.
Language Engineering Convention, Paris, France.
Ely J.W., Osheroff J.A., Chambliss M.L., Ebell M.H.
and Rosenbaum M.E. 2005. Answering Physicians‘
Clinical Questions: Obstacles and Potential Solutions.
JAMIA, 12(2):217–224.
Demner-Fushman D. and Lin J. 2006. Answer Extrac-
tion, Semantic Clustering, and Extractive Summariza-
tion for Clinical Question Answering. ACL 2006, Syd-
ney, Australia
</reference>
<footnote confidence="0.857255">
6http://www.nlm.nih.gov/research/umls/
</footnote>
<page confidence="0.998622">
22
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.271175">
<title confidence="0.991986">Adapting naturally occurring test for evaluation of clinical question answering</title>
<author confidence="0.807302">Dina</author>
<affiliation confidence="0.436055">Lister Hill National Center for Biomedical</affiliation>
<address confidence="0.45214">National Library of Medicine, NIH, Bethesda, MD 20894,</address>
<email confidence="0.995197">ddemner@mail.nih.gov</email>
<abstract confidence="0.99946925">This paper describes the structure of a test suite for evaluation of clinical question answering systems; presents several manually compiled resources found useful for test suite generation; and describes the adaptation of these resources for evaluation of a clinical question answering system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>I Cramer</author>
<author>J L Leidner</author>
<author>D Klakow</author>
</authors>
<title>Building an Evaluation Corpus for German Question Answering by Harvesting Wikipedia.</title>
<date>2006</date>
<location>LREC-2006, Genoa, Italy.</location>
<contexts>
<context position="1008" citStr="Cramer et al. (2006)" startWordPosition="139" endWordPosition="142">ompiled resources found useful for test suite generation; and describes the adaptation of these resources for evaluation of a clinical question answering system. 1 Introduction The community-wide interest in rapid development in many areas of natural language processing and information retrieval resulted in creation of reusable test collections in large-scale evaluations such as the Text REtrieval Conference (TREC)1. Researchers in more specific areas, for which no TREC or other collections are available, have to create or find suitable test collections to evaluate their systems. For example, Cramer et al. (2006) recruited volunteers and quickly gathered a sizeable corpus of question-answer pairs for evaluation of German open-domain question answering systems. This was achieved through a Web-based tool that allowed marking up “interesting” passages in Wikipedia articles and then asking questions about the content of those passages. This appealing approach can not easily be applied in the domain of clinical question answering because the quality of the questions and answers as well as the answer completeness are paramount. A test suite for evaluation of clinical question answering systems should contai</context>
</contexts>
<marker>Cramer, Leidner, Klakow, 2006</marker>
<rawString>Cramer I., Leidner J.L. and Klakow D. 2006. Building an Evaluation Corpus for German Question Answering by Harvesting Wikipedia. LREC-2006, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K B Cohen</author>
<author>L Tanabe</author>
<author>S Kinoshita</author>
<author>L Hunter</author>
</authors>
<title>A resource for constructing customized test suites for molecular biology entity identification systems. HLT-NAACL</title>
<date>2004</date>
<location>Boston, Massachusetts</location>
<contexts>
<context position="2750" citStr="Cohen et al., 2004" startWordPosition="406" endWordPosition="410">o the context that was used in generation of the advice, and finally have access to the original sources of information (Ely et al., 2005). A fair number of high-quality manually created collections present answers to clinical questions in this form and could be obtained online. Three partially freely-available sources: Family Practitioner Inquiry Network (FPIN)2, Parkhurst Exchange Forum (PE)3, and BMJ Clinical Evidence (BMJ-CE)4 were used to design and develop the presented test suites and evaluation methods. Although there seems to be a distinction between test collections and test suites (Cohen et al., 2004) (the former defined as “pieces of text” and associated with corpora, the latter, as lists of specially constructed sentences, or sentence sequences, or sentence fragments (Balkan et al., 1994)), evaluation of answers to clinical questions crosses this boundary and requires the availability of carefully generated sentence fragments as well as suitable document collections. 2http://www.primeanswers.org/primeanswers/ 3http://www.parkhurstexchange.com/qa/index.php 4http://www.clinicalevidence.com/ceweb/conditions/index.jsp 21 Software Engineering, Testing, and Quality Assurance for Natural Langua</context>
</contexts>
<marker>Cohen, Tanabe, Kinoshita, Hunter, 2004</marker>
<rawString>Cohen K.B., Tanabe L., Kinoshita S., and Hunter L. 2004. A resource for constructing customized test suites for molecular biology entity identification systems. HLT-NAACL 2004 Workshop: Biolink 2004, Boston, Massachusetts</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Balkan</author>
<author>K Netter</author>
<author>D Arnold</author>
<author>S Meijer</author>
</authors>
<title>TSNLP. Test Suites for Natural Language Processing. Language Engineering Convention,</title>
<date>1994</date>
<location>Paris, France.</location>
<contexts>
<context position="2943" citStr="Balkan et al., 1994" startWordPosition="436" endWordPosition="439">ctions present answers to clinical questions in this form and could be obtained online. Three partially freely-available sources: Family Practitioner Inquiry Network (FPIN)2, Parkhurst Exchange Forum (PE)3, and BMJ Clinical Evidence (BMJ-CE)4 were used to design and develop the presented test suites and evaluation methods. Although there seems to be a distinction between test collections and test suites (Cohen et al., 2004) (the former defined as “pieces of text” and associated with corpora, the latter, as lists of specially constructed sentences, or sentence sequences, or sentence fragments (Balkan et al., 1994)), evaluation of answers to clinical questions crosses this boundary and requires the availability of carefully generated sentence fragments as well as suitable document collections. 2http://www.primeanswers.org/primeanswers/ 3http://www.parkhurstexchange.com/qa/index.php 4http://www.clinicalevidence.com/ceweb/conditions/index.jsp 21 Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 21–22, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics 2 Test suite structure The multi-tiered answer model of the FPIN and BMJ-CE resource</context>
</contexts>
<marker>Balkan, Netter, Arnold, Meijer, 1994</marker>
<rawString>Balkan L., Netter K., Arnold D. and Meijer S. 1994. TSNLP. Test Suites for Natural Language Processing. Language Engineering Convention, Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J W Ely</author>
<author>J A Osheroff</author>
<author>M L Chambliss</author>
<author>M H Ebell</author>
<author>M E Rosenbaum</author>
</authors>
<title>Answering Physicians‘ Clinical Questions: Obstacles and Potential Solutions.</title>
<date>2005</date>
<journal>JAMIA,</journal>
<volume>12</volume>
<issue>2</issue>
<contexts>
<context position="2269" citStr="Ely et al., 2005" startWordPosition="334" endWordPosition="337">questions asked by clinicians and highquality answers compiled by experts. The answers should be presented in the form deemed useful by clinicians. One of the benefits of focusing on a specific domain, such as clinical question answering, is that the user-needs and desirable results are well-studied and their descriptions are readily-available. In the case of clinical question answering, clinicians’ desiderata are: to see a “bottom-line advice” first, have on-demand access to the context that was used in generation of the advice, and finally have access to the original sources of information (Ely et al., 2005). A fair number of high-quality manually created collections present answers to clinical questions in this form and could be obtained online. Three partially freely-available sources: Family Practitioner Inquiry Network (FPIN)2, Parkhurst Exchange Forum (PE)3, and BMJ Clinical Evidence (BMJ-CE)4 were used to design and develop the presented test suites and evaluation methods. Although there seems to be a distinction between test collections and test suites (Cohen et al., 2004) (the former defined as “pieces of text” and associated with corpora, the latter, as lists of specially constructed sen</context>
</contexts>
<marker>Ely, Osheroff, Chambliss, Ebell, Rosenbaum, 2005</marker>
<rawString>Ely J.W., Osheroff J.A., Chambliss M.L., Ebell M.H. and Rosenbaum M.E. 2005. Answering Physicians‘ Clinical Questions: Obstacles and Potential Solutions. JAMIA, 12(2):217–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Demner-Fushman</author>
<author>J Lin</author>
</authors>
<title>Answer Extraction, Semantic Clustering, and Extractive Summarization for Clinical Question Answering. ACL</title>
<date>2006</date>
<location>Sydney, Australia</location>
<marker>Demner-Fushman, Lin, 2006</marker>
<rawString>Demner-Fushman D. and Lin J. 2006. Answer Extraction, Semantic Clustering, and Extractive Summarization for Clinical Question Answering. ACL 2006, Sydney, Australia</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>