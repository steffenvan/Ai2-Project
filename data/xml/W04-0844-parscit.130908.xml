<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<note confidence="0.539048">
SENSEVAL-3: Third International Workshop on the Evaluation of Systems
for the Semantic Analysis of Text, Barcelona, Spain, July 2004
Association for Computational Linguistics
</note>
<title confidence="0.99774">
Structural Semantic Interconnection: a knowledge-based approach to Word
Sense Disambiguation
</title>
<author confidence="0.980498">
Roberto NAVIGLI
</author>
<affiliation confidence="0.927541">
Dipartimento di Informatica,
</affiliation>
<address confidence="0.777307">
Università di Roma “La Sapienza”
Via Salaria, 113 - 00198 Roma, Italy
</address>
<email confidence="0.980872">
navigli@di.uniroma1.it
</email>
<sectionHeader confidence="0.993258" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998473">
In this paper we describe the SSI algorithm, a
structural pattern matching algorithm for
WSD. The algorithm has been applied to the
gloss disambiguation task of Senseval-3.
</bodyText>
<sectionHeader confidence="0.998489" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.964788233333333">
Our approach to WSD lies in the structural
pattern recognition framework. Structural or
syntactic pattern recognition (Bunke and Sanfeliu,
1990) has proven to be effective when the objects
to be classified contain an inherent, identifiable
organization, such as image data and time-series
data. For these objects, a representation based on a
“flat” vector of features causes a loss of
information that negatively impacts on
classification performances. Word senses clearly
fall under the category of objects that are better
described through a set of structured features.
The classification task in a structural pattern
recognition system is implemented through the
use of grammars that embody precise criteria to
discriminate among different classes. Learning a
structure for the objects to be classified is often a
major problem in many application areas of
structural pattern recognition. In the field of
computational linguistics, however, several efforts
have been made in the past years to produce large
lexical knowledge bases and annotated resources,
offering an ideal starting point for constructing
structured representations of word senses.
2 Building structural representations of
word senses
We build a structural representation of word
senses using a variety of knowledge sources, i.e.
WordNet, Domain Labels (Magnini and Cavaglia,
2000), annotated corpora like SemCor and LDC-
</bodyText>
<footnote confidence="0.853447">
DSO1. We use this information to automatically
1 LDC http://www.ldc.upenn.edu/
</footnote>
<bodyText confidence="0.850581823529412">
Paola VELARDI
Dipartimento di Informatica,
Università di Roma “La Sapienza”
Via Salaria, 113 - 00198 Roma, Italy
velardi@di.uniroma1.it
generate labeled directed graphs (digraphs)
representations of word senses. We call these
semantic graphs, since they represent alternative
conceptualizations for a lexical item.
Figure 1 shows an example of the semantic
graph generated for senses #1 of market, where
nodes represent concepts (WordNet synsets), and
edges are semantic relations. In each graph, we
include only nodes with a maximum distance of 3
from the central node, as suggested by the dashed
oval in Figure 1. This distance has been
experimentally established.
</bodyText>
<equation confidence="0.988570363636364">
artifact#1
trading#1
commerce#1
food#1
goods#1
commercial
enterprise#2
monopoly#1
business
activity#1
load#3
</equation>
<figureCaption confidence="0.984418">
Figure 1. Graph representations for sense #1 of market.
</figureCaption>
<bodyText confidence="0.9942172">
All the used semantic relations are explicitly
encoded in WordNet, except for three relations
named topic, gloss and domain, extracted
respectively from annotated corpora, sense
definitions and domain labels.
</bodyText>
<sectionHeader confidence="0.947054" genericHeader="method">
3 Summary description of the SSI algorithm
</sectionHeader>
<bodyText confidence="0.984100833333333">
The SSI algorithm consists of an initialization step
and an iterative step.
In a generic iteration of the algorithm the input
is a list of co-occurring terms T = [ t1, ..., tn ] and
a list of associated senses I = [St1 ,..., Stn ], i.e. the
semantic interpretation of T, where ti
</bodyText>
<footnote confidence="0.4295684">
S 2 is either
the chosen sense for ti (i.e., the result of a previous
2 Note that with ti
S we refer interchangeably to the semantic
graph associated with a sense or to the sense name.
</footnote>
<equation confidence="0.930082625">
transportation#5
enterprise#1
express#1
production#1
industry#2
activity#1
market#1
service#1
merchandise
#1
export#1
consumer
goods#1
grocery#2
clothing#1
consumption#1
</equation>
<bodyText confidence="0.947679666666667">
disambiguation step) or the empty set (i.e., the
term is not yet disambiguated).
A set of pending terms is also maintained, P =
</bodyText>
<equation confidence="0.9881555">
{  |ti = � }
ti S . I is named the semantic context of T
</equation>
<bodyText confidence="0.99824312">
and is used, at each step, to disambiguate new
terms in P.
The algorithm works in an iterative way, so that
at each stage either at least one term is removed
from P (i.e., at least a pending term is
disambiguated) or the procedure stops because no
more terms can be disambiguated. The output is
the updated list I of senses associated with the
input terms T.
Initially, the list I includes the senses of
monosemous terms in T. If no monosemous terms
are found, the algorithm makes an initial guess
based on the most probable sense of the less
ambiguous term. The initialisation policy is
adjusted depending upon the specific WSD task
considered. Section 5 describes the policy adopted
for the task of gloss disambiguation in WordNet.
During a generic iteration, the algorithm selects
those terms t in P showing an interconnection
between at least one sense S of t and one or more
senses in I. The likelihood for a sense S of being
the correct interpretation of t, given the semantic
context I, is estimated by the function
fI : CxT —&gt; 91, where C is the set of all the
concepts in the ontology O, defined as follows:
</bodyText>
<equation confidence="0.8814365">
fI
0 otherwise
</equation>
<bodyText confidence="0.9925075">
where Senses(t) is the subset of concepts C in O
associated with the term t, and
</bodyText>
<equation confidence="0.954239555555556">
e e
1 2 en � en
1
�( , &apos;) &apos;( { ( ... )|
S S =� w e e
1 2 n 1 n ~1
� � e S S
� � � �
... S S&apos;} ),
</equation>
<bodyText confidence="0.968029333333333">
i.e. a function (p’) of the weights (w) of each path
connecting S with S’, where S and S’ are
represented by semantic graphs. A semantic path
</bodyText>
<equation confidence="0.978914">
ee e en
�
1 2 n 1
between two senses S and S’, S � � � � � S ,
n
S 1 ... S 1 &apos;
</equation>
<bodyText confidence="0.983780333333333">
is represented by a sequence of edge labels
A proper choice for both p and p’ may
e1•e2•...•en.
be the sum function (or the average sum function).
A context-free grammar G = (E, N, SG, PG)
encodes all the meaningful semantic patterns. The
terminal symbols (E) are edge labels, while the
non-terminal symbols (N) encode (sub)paths
between concepts; SG is the start symbol of G and
PG the set of its productions.
We associate a weight with each production
A —&gt; a in PG, where A e N and a e (N u E) * , i.e.
a is a sequence of terminal and non-terminal
symbols. If the sequence of edge labels e1 • e2 ... en
belongs to L(G), the language generated by the
grammar, and provided that G is not ambiguous,
then w(e1.e2. ... en) is given by the sum of the
weights of the productions applied in the
derivation SG =&gt;+ e1 • e2 ... en . The grammar G is
described in the next section.
Finally, the algorithm selects arg max fI (S, t) as
</bodyText>
<subsectionHeader confidence="0.748423">
Se C
</subsectionHeader>
<bodyText confidence="0.998231888888889">
the most likely interpretation of t and updates the
list I with the chosen concept. A threshold can be
applied to f(S,t) to improve the robustness of
system’s choices.
At the end of a generic iteration, a number of
terms is disambiguated and each of them is
removed from the set of pending terms P. The
algorithm stops with output I when no sense S can
be found for the remaining terms in P such that
</bodyText>
<equation confidence="0.625956">
fI (S, t) &gt; 0, that is, P cannot be further reduced.
</equation>
<bodyText confidence="0.998970636363636">
In each iteration, interconnections can only be
found between the sense of a pending term t and
the senses disambiguated during the previous
iteration.
A special case of input for the SSI algorithm is
given by I = [0, 0, ..., O] , that is when no initial
semantic context is available (there are no
monosemous words in T). In this case, an
initialization policy selects a term t e T and the
execution is forked into as many processes as the
number of senses of t.
</bodyText>
<sectionHeader confidence="0.979229" genericHeader="method">
4 The grammar
</sectionHeader>
<bodyText confidence="0.9989267">
The grammar G has the purpose of describing
meaningful interconnecting patterns among
semantic graphs representing conceptualisations
in O. We define a pattern as a sequence of
consecutive semantic relations e1. e2 •...• en where
ei a E, the set of terminal symbols, i.e. the
vocabulary of conceptual relations in O. Two
relations ei ei +1 are consecutive if the edges
labelled with ei and ei+1 are incoming and/or
outgoing from the same concept node, that is
</bodyText>
<equation confidence="0.978436916666667">
e i e i e i e i e i e i e i e i
+ 1 + 1 + 1 1
� �
( )
S , � �
( )
S , � �
( )
S , S . A meaningful
+
� �
( )
</equation>
<bodyText confidence="0.970728615384615">
pattern between two senses S and S’ is a sequence
e1 • e2 • ... • en that belongs to L(G).
In its current version, the grammar G has been
defined manually, inspecting the intersecting
patterns automatically extracted from pairs of
manually disambiguated word senses co-occurring
in different domains. Some of the rules in G are
inspired by previous work on the eXtended
WordNet project described in (Milhalcea and
Moldovan, 2001). The terminal symbols ei are the
conceptual relations extracted from WordNet and
other on-line lexical-semantic resources, as
described in Section 2.
</bodyText>
<equation confidence="0.88902">
=
( , )
S t
� ��
IL
p( {O(S,S&apos; )  |S&apos;e I}) if Se Senses(t)c Synsets
</equation>
<bodyText confidence="0.97342975">
G is defined as a quadruple (E, N, SG, PG),
where E = { ekind-of, ehas-kind, epart-of, ehas-part, egloss, eis-
in-gloss, etopic, ... }, N = { SG, Ss, Sg, S1, S2, S3, S4, S5,
S6, E1, E2, ... }, and PG includes about 50
productions.
As stated in previous section, the weight
w(e1 • e2 • ... • en) of a semantic path e1, e2,..., en is given
by the sum of the weights of the productions
applied in the derivation SG =&gt;&apos; e1 • e2 • ... • en. These
weights have been learned using a perceptron
model, trained with standard word sense
disambiguation data, such as the SemCor corpus.
Examples of the rules in G are provided in the
subsequent Section 5.
5 Application of the SSI algorithm to the
disambiguation of WordNet glosses
For the gloss disambiguation task, the SSI
algorithm is initialized as follows: In step 1, the
list I includes the synset S whose gloss we wish to
disambiguate, and the list P includes all the terms
in the gloss and in the gloss of the hyperonym of
S. Words in the hyperonym’s gloss are useful to
augment the context available for disambiguation.
In the following, we present a sample execution of
the SSI algorithm for the gloss disambiguation
task applied to sense #1 of retrospective: “an
exhibition of a representative selection of an
artist’s life work”. For this task the algorithm uses
a context enriched with the definition of the synset
hyperonym, i.e. art exhibition#1: “an exhibition of
art objects (paintings or statues)”.
Initially we have:
</bodyText>
<equation confidence="0.997810333333333">
I = { retrospective#1 }3
P = { work, object, exhibition, life, statue, artist,
selection, representative, painting, art }
</equation>
<bodyText confidence="0.524329666666667">
At first, I is enriched with the senses of
monosemous words in the definition of
retrospective#1 and its hyperonym:
</bodyText>
<equation confidence="0.993815666666667">
I = { retrospective#1, statue#1, artist#1 }
P = { work, object, exhibition, life, selection,
representative, painting, art }
</equation>
<bodyText confidence="0.984044666666667">
since statue and artist are monosemous terms in
WordNet. During the first iteration, the algorithm
finds three matching paths4:
</bodyText>
<equation confidence="0.578885">
retrospective#1 � kind � ~of � � 2 exhibition#2, statue#1
art#1 and statue#1
</equation>
<footnote confidence="0.9884742">
3 For convenience here we denote I as a set rather
than a list.
4 With S~ R �� iS’ we denote a path of i consecutive
edges labeled with the relation R interconnecting S
with S’.
</footnote>
<equation confidence="0.796750333333333">
� kind � ~of � � 6 object#1
This leads to:
I = { retrospective#1, statue#1, artist#1,
exhibition#2, object#1, art#1 }
P = { work, life, selection, representative, painting
}
</equation>
<bodyText confidence="0.9419795">
During the second iteration, a
hyponymy/holonymy path (rule S2) is found:
art#1 ~ has-kind �� 4 2 painting#1 (painting is a kind
of art)which leads to:
</bodyText>
<equation confidence="0.999587333333333">
I = { retrospective#1, statue#1, artist#1,
exhibition#2, object#1, art#1, painting#1 }
P = { work, life, selection, representative }
</equation>
<bodyText confidence="0.997196">
The third iteration finds a co-occurrence (topic
rule) path between artist#1 and sense 12 of life
(biography, life history):
</bodyText>
<equation confidence="0.9708785">
artist#1 �topic�� life#12
then, we get:
I = { retrospective#1, statue#1, artist#1,
exhibition#2, object#1, art#1, painting#1, life#12
}
P = { work, selection, representative }
</equation>
<bodyText confidence="0.999885">
The algorithm stops because no additional
matches are found. The chosen senses concerning
terms contained in the hyperonym’s gloss were of
help during disambiguation, but are now
discarded. Thus we have:
</bodyText>
<equation confidence="0.9341965">
GlossSynsets(retrospective#1) = { artist#1,
exhibition#2, life#12, work#2 }
</equation>
<sectionHeader confidence="0.986437" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.995471">
The SSI algorithm is currently tailored for noun
disambiguation. Additional semantic knowledge
and ad-hoc rules would be needed to detect
semantic patterns centered on concepts associated
to verbs. Current research is directed towards
integrating in semantic graphs information from
FrameNet and VerbNet, but the main problem is
harmonizing these knowledge bases with
WordNet’s senses and relations inventory. A
second problem of SSI, when applied to
unrestricted WSD tasks, is that it is designed to
disambiguate with high precision, possibly low
recall. In many interesting applications of WSD,
especially in information retrieval, improved
document access may be obtained even when only
few words in a query are disambiguated, but the
disambiguation precision needs to be well over
the 70% threshold. Supporting experiments are
described in (Navigli and Velardi, 2003).
The results obtained by our system in Senseval-
3 reflect these limitations (see Figure 2).
The main run, named OntoLearn, uses a
threshold to select only those senses with a weight
� kind � ~of � � 3
over a given threshold. OntoLearnEx uses a non-
greedy version of the SSI algorithm. Again, a
threshold is used to accepts or reject sense
choices. Finally, OntoLearnB uses the “first
sense” heuristics to select a sense, every since a
sense choice is below the threshold (or no patterns
are found for a given word).
</bodyText>
<figureCaption confidence="0.999362">
Figure 2. Results of three runs submitted to Senseval-3.
</figureCaption>
<bodyText confidence="0.993581">
Table 1 shows the precision and recall of
OntoLearn main run by syntactic category. It
shows that, as expected, the SSI algorithm is
currently tuned for noun disambiguation.
</bodyText>
<table confidence="0.99864475">
Nouns Verbs Adj.
Precision 86.0% 69.4% 78.6%
Recall 44.7% 13.5% 26.2%
Attempted 52.0% 19.5% 33.3%
</table>
<tableCaption confidence="0.999879">
Table 1. Precision and Recall by syntactic category.
</tableCaption>
<bodyText confidence="0.997624210526316">
The official Senseval-3 evaluation has been
performed against a set of so called “golden
glosses” produced by Dan Moldovan and its
group5. This test set however had several
problems, that we partly detected and submitted to
the organisers.
Besides some technical errors in the data set
(presence of WordNet 1.7 and 2.0 senses, missing
glosses, etc.) there are sense-tagging
inconsistencies that are very evident.
For example, one of our highest performing
sense tagging rules in SSI is the direct
hyperonymy path. This rule reads as follows: “if
the word wj appears in the gloss of a synset Si, and
if one of the synsets of wj, Sj, is the direct
hyperonym of Si, then, select Sj as the correct
sense for wj”.
An example is custom#4 defined as “habitual
patronage”. We have that:
</bodyText>
<footnote confidence="0.880473333333333">
{custom-n#4}kind _ of
� {trade,patronage-n#5}
5 http://xwn.hlt.utdallas.edu/wsd.html
</footnote>
<bodyText confidence="0.999736363636363">
therefore we select sense #5 of patronage, while
Moldovan’s “golden” sense is #1.
We do not intend to dispute whether the
“questionable” sense assignment is the one
provided in the golden gloss or rather the
hyperonym selected by the WordNet
lexicographers. In any case, the detected patterns
show a clear inconsistency in the data.
These patterns (313) have been submitted to the
organisers, who then decided to remove them
from the data set.
</bodyText>
<sectionHeader confidence="0.998819" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.99997852631579">
The interesting feature of the SSI algorithm,
unlike many co-occurrence based and statistical
approaches to WSD, is a justification (i.e. a set of
semantic patterns) to support a sense choice.
Furthermore, each sense choice has a weight
representing the confidence of the system in its
output. Therefore SSI can be tuned for high
precision (possibly low recall), an asset that we
consider more realistic for practical WSD
applications.
Currently, the system is tuned for noun
disambiguation, since we build structural
representations of word senses using lexical
knowledge bases that are considerably richer for
nouns. Extending semantic graphs associated to
verbs and adding appropriate interconnection
rules implies harmonizing WordNet and available
lexical resources for verbs, e.g. FrameNet and
VerbNet. This extension is in progress.
</bodyText>
<sectionHeader confidence="0.999079" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998280809523809">
H. Bunke and A. Sanfeliu (editors) (1990)
Syntactic and Structural pattern Recognition:
Theory and Applications World Scientific, Series
in Computer Science vol. 7, 1990.
A. Gangemi, R. Navigli and P. Velardi (2003)
“The OntoWordNet Project: extension and
axiomatization of conceptual relations in
WordNet”, 2nd Int. Conf. ODBASE, ed. Springer
Verlag, 3-7 November 2003, Catania, Italy.
B. Magnini and G. Cavaglia (2000)
“Integrating Subject Field Codes into WordNet”,
Proceedings of LREC2000, Atenas 2000.
Milhalcea R., Moldovan D. I. (2001)
“eXtended WordNet: progress report”. NAACL
2001 Workshop on WordNet and other lexical
resources, Pittsburg, June 2001.
Navigli R. and Velardi P. (2003) “An Analysis
of Ontology-based Query Expansion Strategies”,
Workshop on Adaptive Text Extraction and
Mining September 22nd, 2003 Cavtat-Dubrovnik
(Croatia), held in conjunction with ECML 2003.
</reference>
<figure confidence="0.994167117647059">
99.90%
OntoLearn OntoLearnB OntoLearnEx
75.30%
49.70%
37.50%
82.60%
39.10%
32.30%
68.50%
68.40%
100%
80%
60%
40%
20%
0%
Precision Recall Attempted
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.232063">
<note confidence="0.7532195">SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, Barcelona, Spain, July 2004</note>
<title confidence="0.82613">Association for Computational Linguistics Structural Semantic Interconnection: a knowledge-based approach to Word Sense Disambiguation</title>
<author confidence="0.999424">Roberto NAVIGLI</author>
<affiliation confidence="0.9603495">Dipartimento di Informatica, Università di Roma “La Sapienza”</affiliation>
<address confidence="0.97641">Via Salaria, 113 - 00198 Roma, Italy</address>
<email confidence="0.989275">navigli@di.uniroma1.it</email>
<abstract confidence="0.9391946">In this paper we describe the SSI algorithm, a structural pattern matching algorithm for WSD. The algorithm has been applied to the gloss disambiguation task of Senseval-3.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Bunke</author>
<author>A Sanfeliu</author>
</authors>
<date>1990</date>
<booktitle>Syntactic and Structural pattern Recognition: Theory and Applications World Scientific, Series in Computer Science</booktitle>
<volume>7</volume>
<contexts>
<context position="747" citStr="Bunke and Sanfeliu, 1990" startWordPosition="97" endWordPosition="100"> July 2004 Association for Computational Linguistics Structural Semantic Interconnection: a knowledge-based approach to Word Sense Disambiguation Roberto NAVIGLI Dipartimento di Informatica, Università di Roma “La Sapienza” Via Salaria, 113 - 00198 Roma, Italy navigli@di.uniroma1.it Abstract In this paper we describe the SSI algorithm, a structural pattern matching algorithm for WSD. The algorithm has been applied to the gloss disambiguation task of Senseval-3. 1 Introduction Our approach to WSD lies in the structural pattern recognition framework. Structural or syntactic pattern recognition (Bunke and Sanfeliu, 1990) has proven to be effective when the objects to be classified contain an inherent, identifiable organization, such as image data and time-series data. For these objects, a representation based on a “flat” vector of features causes a loss of information that negatively impacts on classification performances. Word senses clearly fall under the category of objects that are better described through a set of structured features. The classification task in a structural pattern recognition system is implemented through the use of grammars that embody precise criteria to discriminate among different c</context>
</contexts>
<marker>Bunke, Sanfeliu, 1990</marker>
<rawString>H. Bunke and A. Sanfeliu (editors) (1990) Syntactic and Structural pattern Recognition: Theory and Applications World Scientific, Series in Computer Science vol. 7, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gangemi</author>
<author>R Navigli</author>
<author>P Velardi</author>
</authors>
<title>The OntoWordNet Project: extension and axiomatization of conceptual relations</title>
<date>2003</date>
<booktitle>in WordNet”, 2nd Int. Conf. ODBASE,</booktitle>
<pages>3--7</pages>
<editor>ed.</editor>
<publisher>Springer Verlag,</publisher>
<location>Catania, Italy.</location>
<marker>Gangemi, Navigli, Velardi, 2003</marker>
<rawString>A. Gangemi, R. Navigli and P. Velardi (2003) “The OntoWordNet Project: extension and axiomatization of conceptual relations in WordNet”, 2nd Int. Conf. ODBASE, ed. Springer Verlag, 3-7 November 2003, Catania, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Magnini</author>
<author>G Cavaglia</author>
</authors>
<title>Integrating Subject Field Codes into WordNet”,</title>
<date>2000</date>
<booktitle>Proceedings of LREC2000,</booktitle>
<location>Atenas</location>
<contexts>
<context position="1954" citStr="Magnini and Cavaglia, 2000" startWordPosition="275" endWordPosition="278">te among different classes. Learning a structure for the objects to be classified is often a major problem in many application areas of structural pattern recognition. In the field of computational linguistics, however, several efforts have been made in the past years to produce large lexical knowledge bases and annotated resources, offering an ideal starting point for constructing structured representations of word senses. 2 Building structural representations of word senses We build a structural representation of word senses using a variety of knowledge sources, i.e. WordNet, Domain Labels (Magnini and Cavaglia, 2000), annotated corpora like SemCor and LDCDSO1. We use this information to automatically 1 LDC http://www.ldc.upenn.edu/ Paola VELARDI Dipartimento di Informatica, Università di Roma “La Sapienza” Via Salaria, 113 - 00198 Roma, Italy velardi@di.uniroma1.it generate labeled directed graphs (digraphs) representations of word senses. We call these semantic graphs, since they represent alternative conceptualizations for a lexical item. Figure 1 shows an example of the semantic graph generated for senses #1 of market, where nodes represent concepts (WordNet synsets), and edges are semantic relations. </context>
</contexts>
<marker>Magnini, Cavaglia, 2000</marker>
<rawString>B. Magnini and G. Cavaglia (2000) “Integrating Subject Field Codes into WordNet”, Proceedings of LREC2000, Atenas 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Milhalcea</author>
<author>D I Moldovan</author>
</authors>
<title>eXtended WordNet: progress report”. NAACL</title>
<date>2001</date>
<booktitle>Workshop on WordNet and other lexical resources,</booktitle>
<location>Pittsburg,</location>
<contexts>
<context position="8366" citStr="Milhalcea and Moldovan, 2001" startWordPosition="1466" endWordPosition="1469">labelled with ei and ei+1 are incoming and/or outgoing from the same concept node, that is e i e i e i e i e i e i e i e i + 1 + 1 + 1 1 � � ( ) S , � � ( ) S , � � ( ) S , S . A meaningful + � � ( ) pattern between two senses S and S’ is a sequence e1 • e2 • ... • en that belongs to L(G). In its current version, the grammar G has been defined manually, inspecting the intersecting patterns automatically extracted from pairs of manually disambiguated word senses co-occurring in different domains. Some of the rules in G are inspired by previous work on the eXtended WordNet project described in (Milhalcea and Moldovan, 2001). The terminal symbols ei are the conceptual relations extracted from WordNet and other on-line lexical-semantic resources, as described in Section 2. = ( , ) S t � �� IL p( {O(S,S&apos; ) |S&apos;e I}) if Se Senses(t)c Synsets G is defined as a quadruple (E, N, SG, PG), where E = { ekind-of, ehas-kind, epart-of, ehas-part, egloss, eisin-gloss, etopic, ... }, N = { SG, Ss, Sg, S1, S2, S3, S4, S5, S6, E1, E2, ... }, and PG includes about 50 productions. As stated in previous section, the weight w(e1 • e2 • ... • en) of a semantic path e1, e2,..., en is given by the sum of the weights of the productions a</context>
</contexts>
<marker>Milhalcea, Moldovan, 2001</marker>
<rawString>Milhalcea R., Moldovan D. I. (2001) “eXtended WordNet: progress report”. NAACL 2001 Workshop on WordNet and other lexical resources, Pittsburg, June 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Navigli</author>
<author>P Velardi</author>
</authors>
<title>An Analysis of Ontology-based Query Expansion Strategies”,</title>
<date>2003</date>
<booktitle>Workshop on Adaptive Text Extraction and Mining</booktitle>
<contexts>
<context position="12726" citStr="Navigli and Velardi, 2003" startWordPosition="2182" endWordPosition="2185">g in semantic graphs information from FrameNet and VerbNet, but the main problem is harmonizing these knowledge bases with WordNet’s senses and relations inventory. A second problem of SSI, when applied to unrestricted WSD tasks, is that it is designed to disambiguate with high precision, possibly low recall. In many interesting applications of WSD, especially in information retrieval, improved document access may be obtained even when only few words in a query are disambiguated, but the disambiguation precision needs to be well over the 70% threshold. Supporting experiments are described in (Navigli and Velardi, 2003). The results obtained by our system in Senseval3 reflect these limitations (see Figure 2). The main run, named OntoLearn, uses a threshold to select only those senses with a weight � kind � ~of � � 3 over a given threshold. OntoLearnEx uses a nongreedy version of the SSI algorithm. Again, a threshold is used to accepts or reject sense choices. Finally, OntoLearnB uses the “first sense” heuristics to select a sense, every since a sense choice is below the threshold (or no patterns are found for a given word). Figure 2. Results of three runs submitted to Senseval-3. Table 1 shows the precision </context>
</contexts>
<marker>Navigli, Velardi, 2003</marker>
<rawString>Navigli R. and Velardi P. (2003) “An Analysis of Ontology-based Query Expansion Strategies”, Workshop on Adaptive Text Extraction and Mining September 22nd, 2003 Cavtat-Dubrovnik (Croatia), held in conjunction with ECML 2003.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>