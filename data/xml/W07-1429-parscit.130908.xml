<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.995675">
Biology Based Alignments of Paraphrases for Sentence Compression
</title>
<author confidence="0.93883">
Jo˜ao Cordeiro
</author>
<affiliation confidence="0.9539975">
CLT and Bioinformatics
University of Beira Interior
</affiliation>
<address confidence="0.54273">
Covilh˜a, Portugal
</address>
<email confidence="0.979753">
jpaulo@di.ubi.pt
</email>
<author confidence="0.952551">
G¨ael Dias
</author>
<affiliation confidence="0.9588575">
CLT and Bioinformatics
University of Beira Interior
</affiliation>
<address confidence="0.539181">
Covilh˜a, Portugal
</address>
<email confidence="0.97245">
ddg@di.ubi.pt
</email>
<author confidence="0.859455">
Guillaume Cleuziou
</author>
<affiliation confidence="0.820202">
LIFO - University of Orl´eans
</affiliation>
<address confidence="0.33796">
Orl´eans, France
guillaume.cleuziou@
</address>
<email confidence="0.36242">
univ-orleans.fr
</email>
<sectionHeader confidence="0.93216" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998218066666667">
1 In this paper, we present a study for ex-
tracting and aligning paraphrases in the con-
text of Sentence Compression. First, we jus-
tify the application of a new measure for the
automatic extraction of paraphrase corpora.
Second, we discuss the work done by (Barzi-
lay &amp; Lee, 2003) who use clustering of para-
phrases to induce rewriting rules. We will
see, through classical visualization method-
ologies (Kruskal &amp; Wish, 1977) and exhaus-
tive experiments, that clustering may not be
the best approach for automatic pattern iden-
tification. Finally, we will provide some re-
sults of different biology based methodolo-
gies for pairwise paraphrase alignment.
</bodyText>
<sectionHeader confidence="0.995066" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.940739444444444">
Sentence Compression can be seen as the removal
of redundant words or phrases from an input sen-
tence by creating a new sentence in which the gist
of the original meaning of the sentence remains un-
changed. Sentence Compression takes an impor-
tant place for Natural Language Processing (NLP)
tasks where specific constraints must be satisfied,
such as length in summarization (Barzilay &amp; Lee,
2002; Knight &amp; Marcu, 2002; Shinyama et al., 2002;
</bodyText>
<note confidence="0.805004333333333">
Barzilay &amp; Lee, 2003; Le Nguyen &amp; Ho, 2004;
Unno et al., 2006), style in text simplification (Marsi
&amp; Krahmer, 2005) or sentence simplification for
subtitling (Daelemans et al., 2004).
1Project partially funded by Portuguese FCT (Reference:
POSC/PLP/57438/2004)
</note>
<bodyText confidence="0.9992381">
Generally, Sentence Compression involves per-
forming the following three steps: (1) Extraction
of paraphrases from comparable corpora, (2) Align-
ment of paraphrases and (3) Induction of rewriting
rules. Obviously, each of these steps can be per-
formed in many different ways going from totally
unsupervised to totally supervised.
In this paper, we will focus on the first two steps.
In particular, we will first justify the application of
a new measure for the automatic extraction of para-
phrase corpora. Second, we will discuss the work
done by (Barzilay &amp; Lee, 2003) who use cluster-
ing of paraphrases to induce rewriting rules. We
will see, through classical visualization methodolo-
gies (Kruskal &amp; Wish, 1977) and exhaustive ex-
periments, that clustering may not be the best ap-
proach for automatic pattern identification. Finally,
we will provide some results of different biology
based methodologies for pairwise paraphrase align-
ment.
</bodyText>
<sectionHeader confidence="0.991553" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.560858666666667">
Two different approaches have been proposed for
Sentence Compression: purely statistical method-
ologies (Barzilay &amp; Lee, 2003; Le Nguyen &amp; Ho,
2004) and hybrid linguistic/statistic methodologies
(Knight &amp; Marcu, 2002; Shinyama et al., 2002;
Daelemans et al., 2004; Marsi &amp; Krahmer, 2005;
Unno et al., 2006).
As our work is based on the first paradigm, we
will focus on the works proposed by (Barzilay &amp;
Lee, 2003) and (Le Nguyen &amp; Ho, 2004).
(Barzilay &amp; Lee, 2003) present a knowledge-lean
algorithm that uses multiple-sequence alignment to
</bodyText>
<note confidence="0.792841666666667">
177
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 177–184,
Prague, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999704387755102">
learn generate sentence-level paraphrases essentially
from unannotated corpus data alone. In contrast to
(Barzilay &amp; Lee, 2002), they need neither paral-
lel data nor explicit information about sentence se-
mantics. Rather, they use two comparable corpora.
Their approach has three main steps. First, work-
ing on each of the comparable corpora separately,
they compute lattices compact graph-based repre-
sentations to find commonalities within groups of
structurally similar sentences. Next, they identify
pairs of lattices from the two different corpora that
are paraphrases of each other. Finally, given an input
sentence to be paraphrased, they match it to a lattice
and use a paraphrase from the matched lattices mate
to generate an output sentence.
(Le Nguyen &amp; Ho, 2004) propose a new sentence-
reduction algorithm that do not use syntactic pars-
ing for the input sentence. The algorithm is an ex-
tension of the template-translation algorithm (one of
example-based machine-translation methods) via in-
novative employment of the Hidden Markov model,
which uses the set of template rules learned from ex-
amples.
In particular, (Le Nguyen &amp; Ho, 2004) do not
propose any methodology to automatically extract
paraphrases. Instead, they collect a corpus by per-
forming the decomposition program using news and
their summaries. After correcting them manually,
they obtain more than 1,500 pairs of long and re-
duced sentences. Comparatively, (Barzilay &amp; Lee,
2003) propose to use the N-gram Overlap metric
to capture similarities between sentences and auto-
matically create paraphrase corpora. However, this
choice is arbitrary and mainly leads to the extraction
of quasi-exact or exact matching pairs. For that pur-
pose, we introduce a new metric, the Sumo-Metric.
Unlike (Le Nguyen &amp; Ho, 2004), one interesting
idea proposed by (Barzilay &amp; Lee, 2003) is to clus-
ter similar pairs of paraphrases to apply multiple-
sequence alignment. However, once again, this
choice is not justified and we will see by classi-
cal visualization methodologies (Kruskal &amp; Wish,
1977) and exhaustive experiments by applying dif-
ferent clustering algorithms, that clustering may not
be the best approach for automatic pattern identifi-
cation. As a consequence, we will study global and
local biology based sequence alignments compared
to multi-sequence alignment that may lead to better
results for the induction of rewriting rules.
</bodyText>
<sectionHeader confidence="0.993787" genericHeader="method">
3 Paraphrase Corpus Construction
</sectionHeader>
<bodyText confidence="0.999972117647059">
Paraphrase corpora are golden resources for learning
monolingual text-to-text rewritten patterns. How-
ever, such corpora are expensive to construct manu-
ally and will always be an imperfect and biased rep-
resentation of the language paraphrase phenomena.
Therefore, reliable automatic methodologies able to
extract paraphrases from text and subsequently cor-
pus construction are crucial, enabling better pattern
identification. In fact, text-to-text generation is a
particularly promising research direction given that
there are naturally occurring examples of compara-
ble texts that convey the same information but are
written in different styles. Web news stories are an
obvious example. Thus, presented with such texts,
one can pair sentences that convey the same infor-
mation, thereby building a training set of rewriting
examples i.e. a paraphrase corpus.
</bodyText>
<subsectionHeader confidence="0.999199">
3.1 Paraphrase Identification
</subsectionHeader>
<bodyText confidence="0.939035181818182">
A few unsupervised metrics have been applied to
automatic paraphrase identification and extraction
(Barzilay &amp; Lee, 2003; Dolan &amp; Brockett, 2004).
However, these unsupervised methodologies show a
major drawback by extracting quasi-exact2 or even
exact match pairs of sentences as they rely on clas-
sical string similarity measures such as the Edit Dis-
tance in the case of (Dolan &amp; Brockett, 2004) and
word N-gram overlap for (Barzilay &amp; Lee, 2003).
Such pairs are clearly useless.
More recently, (Anonymous, 2007) proposed a
new metric, the Sumo-Metric specially designed
for asymmetrical entailed pairs identification, and
proved better performance over previous established
metrics, even in the specific case when tested with
the Microsoft Paraphrase Research Corpus (Dolan
&amp; Brockett, 2004). For a given sentence pair, hav-
ing each sentence x and y words, and with A exclu-
sive links between the sentences, the Sumo-Metric is
defined in Equation 1 and 2.
2Almost equal strings, for example: Bush said America is
addicted to oil. and Mr. Bush said America is addicted to oil.
</bodyText>
<equation confidence="0.90043325">
178
S(x, y, A) if S(x, y, A) &lt; 1.0
0 if A = 0 (1)
e−k*S(x,r,a) otherwise
</equation>
<bodyText confidence="0.755807">
where
</bodyText>
<equation confidence="0.9995935">
S(x, y, A) = α �o92( x (2)
A) + � �o92(y A)
</equation>
<bodyText confidence="0.985649583333333">
with a, Q E [0, 1] and a + Q = 1.
(Anonymous, 2007) show that the Sumo-Metric
outperforms all state-of-the-art metrics over all
tested corpora. In particular, it shows systematically
better F-Measure and Accuracy measures over all
other metrics showing an improvement of (1) at least
2.86% in terms of F-Measure and 3.96% in terms
of Accuracy and (2) at most 6.61% in terms of F-
Measure and 6.74% in terms of Accuracy compared
to the second best metric which is also systemati-
cally the word N-gram overlap similarity measure
used by (Barzilay &amp; Lee, 2003).
</bodyText>
<subsectionHeader confidence="0.999728">
3.2 Clustering
</subsectionHeader>
<bodyText confidence="0.999906708333333">
Literature shows that there are two main reasons to
apply clustering for paraphrase extraction. On one
hand, as (Barzilay &amp; Lee, 2003) evidence, clusters
of paraphrases can lead to better learning of text-to-
text rewriting rules compared to just pairs of para-
phrases. On the other hand, clustering algorithms
may lead to better performance than stand-alone
similarity measures as they may take advantage of
the different structures of sentences in the cluster to
detect a new similar sentence.
However, as (Barzilay &amp; Lee, 2003) do not pro-
pose any evaluation of which clustering algorithm
should be used, we experiment a set of clustering al-
gorithms and present the comparative results. Con-
trarily to what expected, we will see that clustering
is not a worthy effort.
Instead of extracting only sentence pairs from cor-
pora3, one may consider the extraction of paraphrase
sentence clusters. There are many well-known clus-
tering algorithms, which may be applied to a cor-
pus sentence set S = {si, ..., sn}. Clustering im-
plies the definition of a similarity or (distance) ma-
trix Anun, where each each element aij is the simi-
larity (distance) between sentences si and sj.
</bodyText>
<note confidence="0.460669">
3A pair may be seen as a cluster with only two elements.
</note>
<subsectionHeader confidence="0.912111">
3.2.1 Experimental Results
</subsectionHeader>
<bodyText confidence="0.998374142857143">
We experimented four clustering algorithms on a
corpus of web news stories and then three human
judges manually cross-classified a random sample
of the generated clusters. They were asked to clas-
sify a cluster as a ”wrong cluster” if it contained at
least two sentences without any entailment relation
between them. Results are shown in the next table 1.
</bodyText>
<tableCaption confidence="0.999295">
Table 1: Precision of clustering algorithms
</tableCaption>
<table confidence="0.9172715">
BASE S-HAC C-HAC QT EM
0.618 0.577 0.569 0.640 0.489
</table>
<bodyText confidence="0.999912">
The ”BASE” column is the baseline, where the
Sumo-Metric was applied rather than clustering.
Columns ”S-HAC” and ”C-HAC” express the re-
sults for Single-link and Complete-link Hierarchi-
cal Agglomerative Clustering (Jain et al., 1999).
The ”QT” column shows the Quality Threshold al-
gorithm (Heyer et al., 1999) and the last column
”EM” is the Expectation Maximization clustering al-
gorithm (Hogg et al., 2005).
One main conclusion, from table 1 is that cluster-
ing tends to achieve worst results than simple para-
phrase pair extraction. Only the QT achieves better
results, but if we take the average of the four cluster-
ing algorithms it is equal to 0.568, smaller than the
0.618 baseline. Moreover, these results with the QT
algorithm were applied with a very restrictive value
for cluster attribution as it is shown in table 2 with
an average of almost two sentences per cluster.
</bodyText>
<tableCaption confidence="0.990053">
Table 2: Figures about clustering algorithms
</tableCaption>
<table confidence="0.9771672">
Algorithm # Sentences/# Clusters
S-HAC 6,23
C-HAC 2,17
QT 2,32
EM 4,16
</table>
<bodyText confidence="0.998935777777778">
In fact, table 2 shows that most of the clusters
have less than 6 sentences which leads to question
the results presented by (Barzilay &amp; Lee, 2003) who
only keep the clusters that contain more than 10 sen-
tences. In fact, the first conclusion is that the num-
ber of experimented clusters is very low, and more
important, all clusters with more than 10 sentences
showed to be of very bad quality.
The next subsection will reinforce the sight that
</bodyText>
<equation confidence="0.804079">
8
��� �
����
S(Sa,Sb) =
179
</equation>
<bodyText confidence="0.9756015">
clustering is a worthless effort for automatic para-
phrase corpora construction.
</bodyText>
<subsectionHeader confidence="0.670522">
3.2.2 Visualization
</subsectionHeader>
<bodyText confidence="0.9999654375">
In this subsection, we propose a visual analy-
sis of the different similarity measures tested pre-
viously: the Edit Distance (Levenshtein, 1966), the
BLEU metric (Papineni et al., 2001), the word N-
gram overlap and the Sumo-Metric. The goal of this
study is mainly to give the reader a visual interpre-
tation about the organization each measure induces
on the data.
To perform this study, we use a Multidimensional
Scaling (MDS) process which is a traditional data
analysis technique. MDS (Kruskal &amp; Wish, 1977)
allows to display the structure of distance-like data
into an Euclidean space.
Since the only available information is a similar-
ity in our case, we transform similarity values into
distance values as in Equation 3.
</bodyText>
<equation confidence="0.92421">
��� �1/2 (3)
</equation>
<bodyText confidence="0.989217514285714">
This transformation enables to obtain a (pseudo)
distance measure satisfying properties like minimal-
ity, identity and symmetry. On a theoretical point
of view, the measure we obtain is a pseudo-distance
only, since triangular inequality is not necessary sat-
isfied. In practice, the projection space we build with
the MDS from such a pseudo-distance is sufficient to
have an idea about whether data are organized into
classes.
We perform the MDS process on 500 sentences4
randomly selected from the Microsoft Research
Paraphrase Corpus. In particular, the projection over
the three first eigenvectors (or proper vectors) pro-
vides the best visualization where data are clearly
organized into several classes (at least two classes).
The obtained visualizations (Figure 1) show dis-
tinctly that no particular data organization can be
drawn from the used similarity measures. Indeed,
we observe only one central class with some ”satel-
lite” data randomly placed around the class.
The last observation allows us to anticipate on the
results we could obtain with a clustering step. First,
clustering seems not to be a natural way to manage
4The limitation to 500 data is due to computation costs since
MDS requires the diagonalization of the square similarity or
distance matrix.
such data. Then, according to the clustering method
used, several types of clusters can be expected: very
small clusters which contain ”satellite” data (pretty
relevant) or large clusters with part of the main cen-
tral class (pretty irrelevant). These results confirm
the observed figures in the previous subsection and
reinforce the sight that clustering is a worthless ef-
fort for automatic paraphrase corpora construction,
contrarily to what (Barzilay &amp; Lee, 2003) suggest.
</bodyText>
<sectionHeader confidence="0.979768" genericHeader="method">
4 Biology Based Alignments
</sectionHeader>
<bodyText confidence="0.9999433">
Sequence alignments have been extensively ex-
plored in bioinformatics since the beginning of the
Human Genome Project. In general, one wants to
align two sequences of symbols (genes in Biology)
to find structural similarities, differences or transfor-
mations between them.
In NLP, alignment is relevant in sub-domains
like Text Generation (Barzilay &amp; Lee, 2002). In
our work, we employ alignment methods for align-
ing words between two sentences, which are para-
phrases. The words are the base blocks of our se-
quences (sentences).
There are two main classes of pairwise align-
ments: the global and local classes. In the first
one, the algorithms try to fully align both sequences,
admitting gap insertions at a certain cost, while in
the local methods the goal is to find pairwise sub-
alignments. How suitable each algorithm may be
applied to a certain problem is discussed in the next
two subsections.
</bodyText>
<subsectionHeader confidence="0.979485">
4.1 Global Alignment
</subsectionHeader>
<bodyText confidence="0.999896571428571">
The well established and widely used Needleman-
Wunsch algorithm for pairwise global sequence
alignment, uses dynamic programming to find the
best possible alignment between two sequences. It is
an optimal algorithm. However, it reveals space and
time inefficiency as sequence length increases, since
an m * n matrix must be maintained and processed
during computations. This is the case with DNA se-
quence alignments, composed by many thousands of
nucleotides. Therefore, a huge optimization effort
were engaged and new algorithms appeared like k-
tuple, not guaranteeing to find optimal alignments
but able to tackle the complexity problem.
In our alignment tasks, we do not have these com-
</bodyText>
<page confidence="0.704884">
180
</page>
<bodyText confidence="0.999916125">
plexity obstacles, because in our corpora the mean
length of a sentence is equal to 20.9 words, which
is considerably smaller than in a DNA sequence.
Therefore an implementation of the Needleman-
Wunsch algorithm has been used to generate optimal
global alignments.
The figure 2 exemplifies a global word alignment
on a paraphrase pair.
</bodyText>
<subsectionHeader confidence="0.988607">
4.2 Local Alignment
</subsectionHeader>
<bodyText confidence="0.999897545454545">
The Smith-Waterman (SW) algorithm is similar to
the Needleman Wunsch (NW) one, since dynamic
programming is also followed hence denoting the
similar complexity issues, to which our alignment
task is immune. The main difference is that SW
seeks optimal sub-alignments instead of a global
alignment and, as described in the literature, it
is well tailored for pairs with considerable differ-
ences5, in length and type. In table 3 we exemplify
this by showing two character sequences6 where one
may clearly see that SW is preferable:
</bodyText>
<table confidence="0.997914">
N Char. Sequences Alignments
1 ABBAXYTRVRVTTRVTR XYTRV
FWHWWHGWGFXYTVWGF XYT-V
2 ABCDXYDRQR AB-CD
DQZZSTABZCD ABZCD
</table>
<tableCaption confidence="0.999898">
Table 3: Preferable local alignment cases.
</tableCaption>
<bodyText confidence="0.9996731">
Remark that in the second pair, only the maximal
local sub-alignment is shown. However, there ex-
ists another sub-alignment: (DRQ, D-Q). This means
that local alignment may be tuned to generate not
only the maximum sub-alignment but a set of sub-
alignments that satisfy some criterium, like having
alignment value greater than some minimum thresh-
old. In fact, this is useful in our word alignment
problem and were experimented by adapting the
Smith Waterman algorithm.
</bodyText>
<subsectionHeader confidence="0.994635">
4.3 Dynamic Alignment
</subsectionHeader>
<bodyText confidence="0.9999396">
According to the previous two subsections, where
two alignment strategies were presented, a natu-
ral question rises: which alignment algorithm to
use for our problem of inter-sentence word align-
ment? Initially, we thought to use only the global
</bodyText>
<footnote confidence="0.814032333333333">
5With sufficient similar sequences there is no difference be-
tween NW and SW.
6As in DNA subsequences and is same for word sequences.
</footnote>
<bodyText confidence="0.999173">
alignment Needleman Wunsch algorithm, since a
complete inter-sentence word alignment is obtained.
However, we noticed that this strategy is unappro-
priate for certain pairs, specially when there are syn-
tactical alternations, like in the next example:
</bodyText>
<figure confidence="0.924194">
During his magnificent speech, ���
the��������
president
remarkably praised IBM research.
The��������
president�������
praised IBM research, during his
speech.
</figure>
<bodyText confidence="0.9969496">
If a global alignment is applied for such a pair, then
weird alignments will be generated, like the one that
is shown in the next representation (we use character
sequences for space convenience and try to preserve
the word first letter, from the previous example):
</bodyText>
<equation confidence="0.9436285">
D H M S T P R Q I S _ _ _
_ _ _ _ T P _ Q I S D H S
</equation>
<bodyText confidence="0.999898666666667">
Here it would be more adequate to apply local align-
ment and extract all relevant sub-alignments. In this
case, two sub-alignments would be generated:
</bodyText>
<equation confidence="0.996834">
|D H M S ||T P R P I R|
|D H _ S ||T P _ P I R|
</equation>
<bodyText confidence="0.976317739130435">
Therefore, for inter-paraphrase word alignments,
we propose a dynamic algorithm which chooses the
best alignment to perform: global or local. To com-
pute this pre-scan, we regard the notion of link-
crossing between sequences as illustrated in the fig-
ure 3, where the 4 crossings are signalized with the
small squares.
It is easily verifiable that the maximum number
of crossings, among two sequences with n exclusive
links in between is equal to 0 = 2 ∗ n ∗ (n − 1).
We suggest that if a fraction of these crossings holds,
for example 0.4 ∗ 0 or 0.5 ∗ 0, then a local align-
ment should be used. Remark that the more this frac-
tion tends to 1.0 the more unlikely it is to use global
alignment.
Crossings may be calculated by taking index pairs
hxi, yii to represent links between sequences, where
xi and yi are respectively the first and second se-
quence indexes, for instance in figure 3 the ”U”
link has pair h5, 1i. It is easily verifiable that two
links hxi, yii and hxj, yji have a crossing point if-
(Xi — Xj) * (Yj — Yj) &lt; 0.
f:(xi—xj)*(yi—yj)&lt;0.
</bodyText>
<subsectionHeader confidence="0.998768">
4.4 Alignment with Similarity Matrix
</subsectionHeader>
<bodyText confidence="0.999613666666667">
In bioinformatics, DNA sequence alignment algo-
rithms are usually guided by a scoring function, re-
lated to the field of expertise, that defines what is
</bodyText>
<page confidence="0.817238">
181
</page>
<bodyText confidence="0.999866795454545">
the mutation probability between nucleotides. These
scoring functions are defined by PAM7 or BLO-
SUM8 matrices and encode evolutionary approx-
imations regarding the rates and probabilities of
amino acid mutations. Different matrices might pro-
duce different alignments.
Subsequently, this motivated the idea of model-
ing word mutation. It seems intuitive to allow such
a word mutation, considering the possible relation-
ships that exit between words: lexical, syntactical
or semantic. For example, it seems evident that be-
tween spirit and spiritual there exists a stronger rela-
tion (higher mutation probability) than between spir-
itual and hamburger.
A natural possibility to choose a word muta-
tion representation function is the Edit-distance
(Levenshtein, 1966) (edist(.,.)) as a negative re-
ward for word alignment. For a given word pair
(wi, wj), the greater the Edit-distance value, the
more unlikely the word wi will be aligned with
word wj. However, after some early experiments
with this function, it revealed to lead to some prob-
lems by enabling alignments between very differ-
ent words, like (total, israel), (fire, made) or
(troops, members), despite many good alignments
also achieved. This happens because the Edit-
distance returns relatively small values, unable to
sufficiently penalize different words, like the ones
listed before, to inhibit the alignment. In bioinfor-
matics language, it means that even for such pairs
the mutation probability is still high. Another prob-
lem of the Edit-distance is that it does not distin-
guish between long and small words, for instance
the pairs (in, by) and (governor, governed) have
both the Edit-distance equals to 2.
As a consequence, we propose a new func-
tion (Equation 4) for word mutation penaliza-
tion, able to give better answers for the men-
tioned problems. The idea is to divide the Edit-
distance value by the length of the normalized9
maximum common subsequence maxseq(., .) be-
tween both words. For example, the longest
common subsequence for the pair (w1, w2) =
(reinterpretation, interpreted) is ”interpret”,
</bodyText>
<footnote confidence="0.636449">
7Point Access Mutation.
8Blocks Substitution Matrices.
9The length of the longest common subsequence divided by
the word with maximum length value.
</footnote>
<bodyText confidence="0.905804">
with length equal to 9 and maxseq(w1, w2) =
</bodyText>
<equation confidence="0.99953825">
9 — 0.5625
max{16,111 —
edist(wi, wj )
costAlign(wi, wj) = − a + maxseq(wi, wj) (4)
</equation>
<bodyText confidence="0.975686333333333">
where a is a small value10 that acts like a
”safety hook” against divisions by zero, when
maxseq(wi,wj) = 0.
</bodyText>
<table confidence="0.994734">
word 1 word 2 -edist costAlign
rule ruler -1 -1.235
governor governed -2 -2.632
pay paying -3 -5.882
reinterpretation interpreted -7 -12.227
hamburger spiritual -9 -74.312
in by -2 -200.000
</table>
<tableCaption confidence="0.999363">
Table 4: Word mutation functions comparision.
</tableCaption>
<bodyText confidence="0.99987025">
Remark that with the costAlign(., .) scoring
function the problems with pairs like (in, by) simply
vanish. The smaller the words, the more constrained
the mutation will be.
</bodyText>
<sectionHeader confidence="0.978421" genericHeader="evaluation">
5 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.999336">
5.1 Corpus of Paraphrases
</subsectionHeader>
<bodyText confidence="0.999955333333333">
To test our alignment method, we used two types
of corpora. The first is the ”DUC 2002” corpus
(DUC2002) and the second is automatically ex-
tracted from related web news stories (WNS) auto-
matically extracted. For both original corpora, para-
phrase extraction has been performed by using the
Sumo-Metric and two corpora of paraphrases were
obtained. Afterwards the alignment algorithm was
applied over both corpora.
</bodyText>
<subsectionHeader confidence="0.999355">
5.2 Quality of Dynamic Alignment
</subsectionHeader>
<bodyText confidence="0.996773571428571">
We tested the proposed alignment methods by giving
a sample of 201 aligned paraphrase sentence pairs
to a human judge and ask to classify each pair as
correct, acorrect11, error 12, and merror13. We also
asked to classify the local alignment choice14 as ad-
equate or inadequate. The results are shown in the
next table:
</bodyText>
<table confidence="0.6944544">
10We take a = 0.01.
11Almost correct - minor errors exist
12With some errors.
13With many errors
14Global or local alignment.
182
Global Local
not para correct acorrect error merror adequate
31 108 28 12 8 12/14
15.5% 63.5% 16.5% 7.1% 4.7% 85.7%
</table>
<tableCaption confidence="0.999092">
Table 5: Precision of alignments.
</tableCaption>
<bodyText confidence="0.9995435">
For global alignments15 we have 11.8% pairs with
relevant errors and 85.7% (12 from 14) of all lo-
cal alignment decisions were classified as adequate.
The not para column shows the number of false
paraphrases identified, revealing a precision value of
84.5% for the Sumo-Metric.
</bodyText>
<sectionHeader confidence="0.988884" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9999774">
A set of important steps toward automatic construc-
tion of aligned paraphrase corpora are presented and
inherent relevant issues discussed, like clustering
and alignment. Experiments, by using 4 algorithms
and through visualization techniques, revealed that
clustering is a worthless effort for paraphrase cor-
pora construction, contrary to the literature claims
(Barzilay &amp; Lee, 2003). Therefore simple para-
phrase pair extraction is suggested and by using
a recent and more reliable metric (Sumo-Metric)
(Anonymous, 2007) designed for asymmetrical en-
tailed pairs. We also propose a dynamic choosing of
the alignment algorithm and a word scoring function
for the alignment algorithms.
In the future we intend to clean the automatic
constructed corpus by introducing syntactical con-
straints to filter the wrong alignments. Our next step
will be to employ Machine Learning techniques for
rewriting rule induction, by using this automatically
constructed aligned paraphrase corpus.
</bodyText>
<sectionHeader confidence="0.987767" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994194767857143">
Barzilay R. and Lee L. 2002. Bootstrapping Lexical
Choice via Multiple-Sequence Alignment. Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, (EMNLP), 164-171.
Barzilay, R., and Lee, L. 2003. Learning to para-
phrase: An unsupervised approach using multiple-
sequence alignment. Proceedings of HLT-NAACL.
15Percentage are calculated by dividing by 170 (201 − 31)
the number of true paraphrases that exists.
Dolan W.B. and Brockett C. 2004. Unsupervised con-
struction of large paraphrase corpora: Exploiting
massively parallel news sources. Proceedings of 20th
International Conference on Computational Linguis-
tics (COLING 2004).
Anonymous 2007. Learning Paraphrases from WNS
Corpora. Proceedings of 20th International FLAIRS
Conference. AAAI Press. Key West, Florida.
Daelemans W., Hothker A., and Tjong E. 2004. Auto-
matic Sentence Simplification for Subtitling in Dutch
and English. In Proceedings of LREC 2004, Lisbon,
Portugal.
Heyer L.J., Kruglyak S. and Yooseph S. 1999. Exploring
Expression Data: Identification and Analysis of Coex-
pressed Genes. Genome Research, 9:1106-1115.
Hogg R., McKean J., and Craig A. 2005 Introduction
to Mathematical Statistics. Upper Saddle River, NJ:
Pearson Prentice Hall, 359-364.
Jain A., Murty M. and Flynn P. Data clustering: a review.
ACM Computing Surveys, 31:264-323
Knight K. and Marcu D. 2002. Summarization beyond
sentence extraction: A probabilistic approach to sen-
tence compression. Artificial Intelligence, 139(1):91-
107.
Kruskal J. B. and Wish M. 1977. Multidimensional Scal-
ing. Sage Publications. Beverly Hills. CA.
Le Nguyen M., Horiguchi S., A. S., and Ho B. T. 2004.
Example-based sentence reduction using the hidden
markov model. ACM Transactions on Asian Language
Information Processing (TALIP), 3(2):146-158.
Levenshtein V. 1966. Binary Codes Capable of Cor-
recting Deletions, Insertions, and Reversals. Soviet
Physice-Doklady, 10:707-710.
Marsi E. and Krahmer E. 2005. Explorations in sentence
fusion. In Proceedings of the 10th European Work-
shop on Natural Language Generation.
Papineni K., Roukos S., Ward T., Zhu W.-J. 2001.
BLEU: a Method for Automatic Evaluation of Ma-
chine Translation. IBM Research Report RC22176.
Shinyama Y., Sekine S., and Sudo K. 2002. Auto-
matic Paraphrase Acquisition from News Articles. Sao
Diego, USA.
Unno Y., Ninomiya T., Miyao Y. and Tsujii J. 2006.
Trimming CFG Parse Trees for Sentence Compres-
sion Using Machine Learning Approaches. In the Pro-
ceedings of the COLING/ACL 2006 Main Conference
Poster Sessions.
</reference>
<page confidence="0.530837">
183
</page>
<figure confidence="0.996587553571429">
Edit Distance BLEU Metric
-0.2-0.15-0.1-0.05 0 0.05 0.1 0.15 0.2-0.25 -0.1
-0.0
-0.15
-0.2
0.150.2
00.050.1
5
0.3
0.2
0.1
0
-0.1
-0.2
-0.3
Word N-Gram Family
0.2
0.15
0.1
-0.05 00.05
-0.25-0.2-0.15-0.1-0.05 0 0.05 0.1 0.15 0.2 0.25-0.2 -0.1
-0.15
Sumo-Metric
0.2
0.15
0.1
0.05
0
-0.05
-0.1
-0.15
-0.2
0.15
0.1
0 0.05
0.02
-0.04 -0.02 -0.02 0
0 0.02 0.04 -0.04
-0.05
-0.35-0.3-0.25-0.2-0.15-0.1-0.05 0 0.050.10.150.2-0.2 -0.1
-0.15
0.04
0.04
0.02
0
-0.02
-0.04
0.2
0.15
0.1
0.05
0
-0.05
-0.1
-0.15
-0.2
</figure>
<figureCaption confidence="0.9879755">
Figure 1: MDS on 500 sentences with the Edit Distance (top left), the BLEU Metric (top right), the Word
N-Gram Family (bottom left) and the Sumo-Metric (bottom right).
</figureCaption>
<bodyText confidence="0.784421">
To the horror of their television fans , Miss Ball and Arnaz were divorced in 1960.
__ ___ ______ __ _____ ____ _ ____Ball and Arnaz ____divorced in 1960.
</bodyText>
<figureCaption confidence="0.999652">
Figure 2: Global aligned words in a paraphrase pair.
Figure 3: Crossings between a sequence pair.
</figureCaption>
<page confidence="0.594794">
184
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.291264">
<title confidence="0.947934">Biology Based Alignments of Paraphrases for Sentence Compression</title>
<affiliation confidence="0.9314935">CLT and University of Beira</affiliation>
<email confidence="0.97314">jpaulo@di.ubi.pt</email>
<author confidence="0.949087">G¨ael</author>
<affiliation confidence="0.9945745">CLT and University of Beira</affiliation>
<email confidence="0.976486">ddg@di.ubi.pt</email>
<author confidence="0.946343">Guillaume</author>
<affiliation confidence="0.9779">LIFO - University of</affiliation>
<address confidence="0.528754">Orl´eans,</address>
<email confidence="0.998055">univ-orleans.fr</email>
<abstract confidence="0.98771275">this paper, we present a study for extracting and aligning paraphrases in the context of Sentence Compression. First, we justify the application of a new measure for the automatic extraction of paraphrase corpora. Second, we discuss the work done by (Barzilay &amp; Lee, 2003) who use clustering of paraphrases to induce rewriting rules. We will see, through classical visualization methodologies (Kruskal &amp; Wish, 1977) and exhaustive experiments, that clustering may not be the best approach for automatic pattern identification. Finally, we will provide some results of different biology based methodologies for pairwise paraphrase alignment.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>L Lee</author>
</authors>
<title>Bootstrapping Lexical Choice via Multiple-Sequence Alignment.</title>
<date>2002</date>
<booktitle>Proceedings of the Conference on Empirical Methods in Natural Language Processing, (EMNLP),</booktitle>
<pages>164--171</pages>
<contexts>
<context position="1430" citStr="Barzilay &amp; Lee, 2002" startWordPosition="213" endWordPosition="216"> experiments, that clustering may not be the best approach for automatic pattern identification. Finally, we will provide some results of different biology based methodologies for pairwise paraphrase alignment. 1 Introduction Sentence Compression can be seen as the removal of redundant words or phrases from an input sentence by creating a new sentence in which the gist of the original meaning of the sentence remains unchanged. Sentence Compression takes an important place for Natural Language Processing (NLP) tasks where specific constraints must be satisfied, such as length in summarization (Barzilay &amp; Lee, 2002; Knight &amp; Marcu, 2002; Shinyama et al., 2002; Barzilay &amp; Lee, 2003; Le Nguyen &amp; Ho, 2004; Unno et al., 2006), style in text simplification (Marsi &amp; Krahmer, 2005) or sentence simplification for subtitling (Daelemans et al., 2004). 1Project partially funded by Portuguese FCT (Reference: POSC/PLP/57438/2004) Generally, Sentence Compression involves performing the following three steps: (1) Extraction of paraphrases from comparable corpora, (2) Alignment of paraphrases and (3) Induction of rewriting rules. Obviously, each of these steps can be performed in many different ways going from totally </context>
<context position="3510" citStr="Barzilay &amp; Lee, 2002" startWordPosition="531" endWordPosition="534"> &amp; Marcu, 2002; Shinyama et al., 2002; Daelemans et al., 2004; Marsi &amp; Krahmer, 2005; Unno et al., 2006). As our work is based on the first paradigm, we will focus on the works proposed by (Barzilay &amp; Lee, 2003) and (Le Nguyen &amp; Ho, 2004). (Barzilay &amp; Lee, 2003) present a knowledge-lean algorithm that uses multiple-sequence alignment to 177 Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 177–184, Prague, June 2007. c�2007 Association for Computational Linguistics learn generate sentence-level paraphrases essentially from unannotated corpus data alone. In contrast to (Barzilay &amp; Lee, 2002), they need neither parallel data nor explicit information about sentence semantics. Rather, they use two comparable corpora. Their approach has three main steps. First, working on each of the comparable corpora separately, they compute lattices compact graph-based representations to find commonalities within groups of structurally similar sentences. Next, they identify pairs of lattices from the two different corpora that are paraphrases of each other. Finally, given an input sentence to be paraphrased, they match it to a lattice and use a paraphrase from the matched lattices mate to generate</context>
<context position="14595" citStr="Barzilay &amp; Lee, 2002" startWordPosition="2311" endWordPosition="2314">nt). These results confirm the observed figures in the previous subsection and reinforce the sight that clustering is a worthless effort for automatic paraphrase corpora construction, contrarily to what (Barzilay &amp; Lee, 2003) suggest. 4 Biology Based Alignments Sequence alignments have been extensively explored in bioinformatics since the beginning of the Human Genome Project. In general, one wants to align two sequences of symbols (genes in Biology) to find structural similarities, differences or transformations between them. In NLP, alignment is relevant in sub-domains like Text Generation (Barzilay &amp; Lee, 2002). In our work, we employ alignment methods for aligning words between two sentences, which are paraphrases. The words are the base blocks of our sequences (sentences). There are two main classes of pairwise alignments: the global and local classes. In the first one, the algorithms try to fully align both sequences, admitting gap insertions at a certain cost, while in the local methods the goal is to find pairwise subalignments. How suitable each algorithm may be applied to a certain problem is discussed in the next two subsections. 4.1 Global Alignment The well established and widely used Need</context>
</contexts>
<marker>Barzilay, Lee, 2002</marker>
<rawString>Barzilay R. and Lee L. 2002. Bootstrapping Lexical Choice via Multiple-Sequence Alignment. Proceedings of the Conference on Empirical Methods in Natural Language Processing, (EMNLP), 164-171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>L Lee</author>
</authors>
<title>Learning to paraphrase: An unsupervised approach using multiplesequence alignment.</title>
<date>2003</date>
<booktitle>Proceedings of HLT-NAACL. 15Percentage are calculated by dividing by 170 (201 − 31)</booktitle>
<contexts>
<context position="652" citStr="Barzilay &amp; Lee, 2003" startWordPosition="89" endWordPosition="93">aphrases for Sentence Compression Jo˜ao Cordeiro CLT and Bioinformatics University of Beira Interior Covilh˜a, Portugal jpaulo@di.ubi.pt G¨ael Dias CLT and Bioinformatics University of Beira Interior Covilh˜a, Portugal ddg@di.ubi.pt Guillaume Cleuziou LIFO - University of Orl´eans Orl´eans, France guillaume.cleuziou@ univ-orleans.fr Abstract 1 In this paper, we present a study for extracting and aligning paraphrases in the context of Sentence Compression. First, we justify the application of a new measure for the automatic extraction of paraphrase corpora. Second, we discuss the work done by (Barzilay &amp; Lee, 2003) who use clustering of paraphrases to induce rewriting rules. We will see, through classical visualization methodologies (Kruskal &amp; Wish, 1977) and exhaustive experiments, that clustering may not be the best approach for automatic pattern identification. Finally, we will provide some results of different biology based methodologies for pairwise paraphrase alignment. 1 Introduction Sentence Compression can be seen as the removal of redundant words or phrases from an input sentence by creating a new sentence in which the gist of the original meaning of the sentence remains unchanged. Sentence Co</context>
<context position="2304" citStr="Barzilay &amp; Lee, 2003" startWordPosition="350" endWordPosition="353">nded by Portuguese FCT (Reference: POSC/PLP/57438/2004) Generally, Sentence Compression involves performing the following three steps: (1) Extraction of paraphrases from comparable corpora, (2) Alignment of paraphrases and (3) Induction of rewriting rules. Obviously, each of these steps can be performed in many different ways going from totally unsupervised to totally supervised. In this paper, we will focus on the first two steps. In particular, we will first justify the application of a new measure for the automatic extraction of paraphrase corpora. Second, we will discuss the work done by (Barzilay &amp; Lee, 2003) who use clustering of paraphrases to induce rewriting rules. We will see, through classical visualization methodologies (Kruskal &amp; Wish, 1977) and exhaustive experiments, that clustering may not be the best approach for automatic pattern identification. Finally, we will provide some results of different biology based methodologies for pairwise paraphrase alignment. 2 Related Work Two different approaches have been proposed for Sentence Compression: purely statistical methodologies (Barzilay &amp; Lee, 2003; Le Nguyen &amp; Ho, 2004) and hybrid linguistic/statistic methodologies (Knight &amp; Marcu, 2002;</context>
<context position="4831" citStr="Barzilay &amp; Lee, 2003" startWordPosition="737" endWordPosition="740">e syntactic parsing for the input sentence. The algorithm is an extension of the template-translation algorithm (one of example-based machine-translation methods) via innovative employment of the Hidden Markov model, which uses the set of template rules learned from examples. In particular, (Le Nguyen &amp; Ho, 2004) do not propose any methodology to automatically extract paraphrases. Instead, they collect a corpus by performing the decomposition program using news and their summaries. After correcting them manually, they obtain more than 1,500 pairs of long and reduced sentences. Comparatively, (Barzilay &amp; Lee, 2003) propose to use the N-gram Overlap metric to capture similarities between sentences and automatically create paraphrase corpora. However, this choice is arbitrary and mainly leads to the extraction of quasi-exact or exact matching pairs. For that purpose, we introduce a new metric, the Sumo-Metric. Unlike (Le Nguyen &amp; Ho, 2004), one interesting idea proposed by (Barzilay &amp; Lee, 2003) is to cluster similar pairs of paraphrases to apply multiplesequence alignment. However, once again, this choice is not justified and we will see by classical visualization methodologies (Kruskal &amp; Wish, 1977) and</context>
<context position="6811" citStr="Barzilay &amp; Lee, 2003" startWordPosition="1030" endWordPosition="1033">nabling better pattern identification. In fact, text-to-text generation is a particularly promising research direction given that there are naturally occurring examples of comparable texts that convey the same information but are written in different styles. Web news stories are an obvious example. Thus, presented with such texts, one can pair sentences that convey the same information, thereby building a training set of rewriting examples i.e. a paraphrase corpus. 3.1 Paraphrase Identification A few unsupervised metrics have been applied to automatic paraphrase identification and extraction (Barzilay &amp; Lee, 2003; Dolan &amp; Brockett, 2004). However, these unsupervised methodologies show a major drawback by extracting quasi-exact2 or even exact match pairs of sentences as they rely on classical string similarity measures such as the Edit Distance in the case of (Dolan &amp; Brockett, 2004) and word N-gram overlap for (Barzilay &amp; Lee, 2003). Such pairs are clearly useless. More recently, (Anonymous, 2007) proposed a new metric, the Sumo-Metric specially designed for asymmetrical entailed pairs identification, and proved better performance over previous established metrics, even in the specific case when teste</context>
<context position="8443" citStr="Barzilay &amp; Lee, 2003" startWordPosition="1312" endWordPosition="1315">x,r,a) otherwise where S(x, y, A) = α �o92( x (2) A) + � �o92(y A) with a, Q E [0, 1] and a + Q = 1. (Anonymous, 2007) show that the Sumo-Metric outperforms all state-of-the-art metrics over all tested corpora. In particular, it shows systematically better F-Measure and Accuracy measures over all other metrics showing an improvement of (1) at least 2.86% in terms of F-Measure and 3.96% in terms of Accuracy and (2) at most 6.61% in terms of FMeasure and 6.74% in terms of Accuracy compared to the second best metric which is also systematically the word N-gram overlap similarity measure used by (Barzilay &amp; Lee, 2003). 3.2 Clustering Literature shows that there are two main reasons to apply clustering for paraphrase extraction. On one hand, as (Barzilay &amp; Lee, 2003) evidence, clusters of paraphrases can lead to better learning of text-totext rewriting rules compared to just pairs of paraphrases. On the other hand, clustering algorithms may lead to better performance than stand-alone similarity measures as they may take advantage of the different structures of sentences in the cluster to detect a new similar sentence. However, as (Barzilay &amp; Lee, 2003) do not propose any evaluation of which clustering algor</context>
<context position="11302" citStr="Barzilay &amp; Lee, 2003" startWordPosition="1787" endWordPosition="1790"> extraction. Only the QT achieves better results, but if we take the average of the four clustering algorithms it is equal to 0.568, smaller than the 0.618 baseline. Moreover, these results with the QT algorithm were applied with a very restrictive value for cluster attribution as it is shown in table 2 with an average of almost two sentences per cluster. Table 2: Figures about clustering algorithms Algorithm # Sentences/# Clusters S-HAC 6,23 C-HAC 2,17 QT 2,32 EM 4,16 In fact, table 2 shows that most of the clusters have less than 6 sentences which leads to question the results presented by (Barzilay &amp; Lee, 2003) who only keep the clusters that contain more than 10 sentences. In fact, the first conclusion is that the number of experimented clusters is very low, and more important, all clusters with more than 10 sentences showed to be of very bad quality. The next subsection will reinforce the sight that 8 ��� � ���� S(Sa,Sb) = 179 clustering is a worthless effort for automatic paraphrase corpora construction. 3.2.2 Visualization In this subsection, we propose a visual analysis of the different similarity measures tested previously: the Edit Distance (Levenshtein, 1966), the BLEU metric (Papineni et al</context>
<context position="14199" citStr="Barzilay &amp; Lee, 2003" startWordPosition="2252" endWordPosition="2255">l way to manage 4The limitation to 500 data is due to computation costs since MDS requires the diagonalization of the square similarity or distance matrix. such data. Then, according to the clustering method used, several types of clusters can be expected: very small clusters which contain ”satellite” data (pretty relevant) or large clusters with part of the main central class (pretty irrelevant). These results confirm the observed figures in the previous subsection and reinforce the sight that clustering is a worthless effort for automatic paraphrase corpora construction, contrarily to what (Barzilay &amp; Lee, 2003) suggest. 4 Biology Based Alignments Sequence alignments have been extensively explored in bioinformatics since the beginning of the Human Genome Project. In general, one wants to align two sequences of symbols (genes in Biology) to find structural similarities, differences or transformations between them. In NLP, alignment is relevant in sub-domains like Text Generation (Barzilay &amp; Lee, 2002). In our work, we employ alignment methods for aligning words between two sentences, which are paraphrases. The words are the base blocks of our sequences (sentences). There are two main classes of pairwi</context>
<context position="24562" citStr="Barzilay &amp; Lee, 2003" startWordPosition="3952" endWordPosition="3955">85.7% (12 from 14) of all local alignment decisions were classified as adequate. The not para column shows the number of false paraphrases identified, revealing a precision value of 84.5% for the Sumo-Metric. 6 Conclusion and Future Work A set of important steps toward automatic construction of aligned paraphrase corpora are presented and inherent relevant issues discussed, like clustering and alignment. Experiments, by using 4 algorithms and through visualization techniques, revealed that clustering is a worthless effort for paraphrase corpora construction, contrary to the literature claims (Barzilay &amp; Lee, 2003). Therefore simple paraphrase pair extraction is suggested and by using a recent and more reliable metric (Sumo-Metric) (Anonymous, 2007) designed for asymmetrical entailed pairs. We also propose a dynamic choosing of the alignment algorithm and a word scoring function for the alignment algorithms. In the future we intend to clean the automatic constructed corpus by introducing syntactical constraints to filter the wrong alignments. Our next step will be to employ Machine Learning techniques for rewriting rule induction, by using this automatically constructed aligned paraphrase corpus. Refere</context>
</contexts>
<marker>Barzilay, Lee, 2003</marker>
<rawString>Barzilay, R., and Lee, L. 2003. Learning to paraphrase: An unsupervised approach using multiplesequence alignment. Proceedings of HLT-NAACL. 15Percentage are calculated by dividing by 170 (201 − 31) the number of true paraphrases that exists.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W B Dolan</author>
<author>C Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>Proceedings of 20th International Conference on Computational Linguistics (COLING</booktitle>
<contexts>
<context position="6836" citStr="Dolan &amp; Brockett, 2004" startWordPosition="1034" endWordPosition="1037"> identification. In fact, text-to-text generation is a particularly promising research direction given that there are naturally occurring examples of comparable texts that convey the same information but are written in different styles. Web news stories are an obvious example. Thus, presented with such texts, one can pair sentences that convey the same information, thereby building a training set of rewriting examples i.e. a paraphrase corpus. 3.1 Paraphrase Identification A few unsupervised metrics have been applied to automatic paraphrase identification and extraction (Barzilay &amp; Lee, 2003; Dolan &amp; Brockett, 2004). However, these unsupervised methodologies show a major drawback by extracting quasi-exact2 or even exact match pairs of sentences as they rely on classical string similarity measures such as the Edit Distance in the case of (Dolan &amp; Brockett, 2004) and word N-gram overlap for (Barzilay &amp; Lee, 2003). Such pairs are clearly useless. More recently, (Anonymous, 2007) proposed a new metric, the Sumo-Metric specially designed for asymmetrical entailed pairs identification, and proved better performance over previous established metrics, even in the specific case when tested with the Microsoft Para</context>
</contexts>
<marker>Dolan, Brockett, 2004</marker>
<rawString>Dolan W.B. and Brockett C. 2004. Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. Proceedings of 20th International Conference on Computational Linguistics (COLING 2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anonymous</author>
</authors>
<title>Learning Paraphrases from WNS Corpora.</title>
<date>2007</date>
<booktitle>Proceedings of 20th International FLAIRS Conference.</booktitle>
<publisher>AAAI Press. Key West,</publisher>
<location>Florida.</location>
<contexts>
<context position="7203" citStr="Anonymous, 2007" startWordPosition="1095" endWordPosition="1096">y building a training set of rewriting examples i.e. a paraphrase corpus. 3.1 Paraphrase Identification A few unsupervised metrics have been applied to automatic paraphrase identification and extraction (Barzilay &amp; Lee, 2003; Dolan &amp; Brockett, 2004). However, these unsupervised methodologies show a major drawback by extracting quasi-exact2 or even exact match pairs of sentences as they rely on classical string similarity measures such as the Edit Distance in the case of (Dolan &amp; Brockett, 2004) and word N-gram overlap for (Barzilay &amp; Lee, 2003). Such pairs are clearly useless. More recently, (Anonymous, 2007) proposed a new metric, the Sumo-Metric specially designed for asymmetrical entailed pairs identification, and proved better performance over previous established metrics, even in the specific case when tested with the Microsoft Paraphrase Research Corpus (Dolan &amp; Brockett, 2004). For a given sentence pair, having each sentence x and y words, and with A exclusive links between the sentences, the Sumo-Metric is defined in Equation 1 and 2. 2Almost equal strings, for example: Bush said America is addicted to oil. and Mr. Bush said America is addicted to oil. 178 S(x, y, A) if S(x, y, A) &lt; 1.0 0 </context>
</contexts>
<marker>Anonymous, 2007</marker>
<rawString>Anonymous 2007. Learning Paraphrases from WNS Corpora. Proceedings of 20th International FLAIRS Conference. AAAI Press. Key West, Florida.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>A Hothker</author>
<author>E Tjong</author>
</authors>
<title>Automatic Sentence Simplification for Subtitling in Dutch and English.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC 2004,</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="1660" citStr="Daelemans et al., 2004" startWordPosition="251" endWordPosition="254">entence Compression can be seen as the removal of redundant words or phrases from an input sentence by creating a new sentence in which the gist of the original meaning of the sentence remains unchanged. Sentence Compression takes an important place for Natural Language Processing (NLP) tasks where specific constraints must be satisfied, such as length in summarization (Barzilay &amp; Lee, 2002; Knight &amp; Marcu, 2002; Shinyama et al., 2002; Barzilay &amp; Lee, 2003; Le Nguyen &amp; Ho, 2004; Unno et al., 2006), style in text simplification (Marsi &amp; Krahmer, 2005) or sentence simplification for subtitling (Daelemans et al., 2004). 1Project partially funded by Portuguese FCT (Reference: POSC/PLP/57438/2004) Generally, Sentence Compression involves performing the following three steps: (1) Extraction of paraphrases from comparable corpora, (2) Alignment of paraphrases and (3) Induction of rewriting rules. Obviously, each of these steps can be performed in many different ways going from totally unsupervised to totally supervised. In this paper, we will focus on the first two steps. In particular, we will first justify the application of a new measure for the automatic extraction of paraphrase corpora. Second, we will dis</context>
<context position="2950" citStr="Daelemans et al., 2004" startWordPosition="446" endWordPosition="449">araphrases to induce rewriting rules. We will see, through classical visualization methodologies (Kruskal &amp; Wish, 1977) and exhaustive experiments, that clustering may not be the best approach for automatic pattern identification. Finally, we will provide some results of different biology based methodologies for pairwise paraphrase alignment. 2 Related Work Two different approaches have been proposed for Sentence Compression: purely statistical methodologies (Barzilay &amp; Lee, 2003; Le Nguyen &amp; Ho, 2004) and hybrid linguistic/statistic methodologies (Knight &amp; Marcu, 2002; Shinyama et al., 2002; Daelemans et al., 2004; Marsi &amp; Krahmer, 2005; Unno et al., 2006). As our work is based on the first paradigm, we will focus on the works proposed by (Barzilay &amp; Lee, 2003) and (Le Nguyen &amp; Ho, 2004). (Barzilay &amp; Lee, 2003) present a knowledge-lean algorithm that uses multiple-sequence alignment to 177 Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 177–184, Prague, June 2007. c�2007 Association for Computational Linguistics learn generate sentence-level paraphrases essentially from unannotated corpus data alone. In contrast to (Barzilay &amp; Lee, 2002), they need neither parallel data nor ex</context>
</contexts>
<marker>Daelemans, Hothker, Tjong, 2004</marker>
<rawString>Daelemans W., Hothker A., and Tjong E. 2004. Automatic Sentence Simplification for Subtitling in Dutch and English. In Proceedings of LREC 2004, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L J Heyer</author>
<author>S Kruglyak</author>
<author>S Yooseph</author>
</authors>
<title>Exploring Expression Data: Identification and Analysis of Coexpressed Genes.</title>
<date>1999</date>
<journal>Genome Research,</journal>
<pages>9--1106</pages>
<contexts>
<context position="10470" citStr="Heyer et al., 1999" startWordPosition="1644" endWordPosition="1647">ple of the generated clusters. They were asked to classify a cluster as a ”wrong cluster” if it contained at least two sentences without any entailment relation between them. Results are shown in the next table 1. Table 1: Precision of clustering algorithms BASE S-HAC C-HAC QT EM 0.618 0.577 0.569 0.640 0.489 The ”BASE” column is the baseline, where the Sumo-Metric was applied rather than clustering. Columns ”S-HAC” and ”C-HAC” express the results for Single-link and Complete-link Hierarchical Agglomerative Clustering (Jain et al., 1999). The ”QT” column shows the Quality Threshold algorithm (Heyer et al., 1999) and the last column ”EM” is the Expectation Maximization clustering algorithm (Hogg et al., 2005). One main conclusion, from table 1 is that clustering tends to achieve worst results than simple paraphrase pair extraction. Only the QT achieves better results, but if we take the average of the four clustering algorithms it is equal to 0.568, smaller than the 0.618 baseline. Moreover, these results with the QT algorithm were applied with a very restrictive value for cluster attribution as it is shown in table 2 with an average of almost two sentences per cluster. Table 2: Figures about clusteri</context>
</contexts>
<marker>Heyer, Kruglyak, Yooseph, 1999</marker>
<rawString>Heyer L.J., Kruglyak S. and Yooseph S. 1999. Exploring Expression Data: Identification and Analysis of Coexpressed Genes. Genome Research, 9:1106-1115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hogg</author>
<author>J McKean</author>
<author>A Craig</author>
</authors>
<title>Introduction to Mathematical Statistics. Upper Saddle River,</title>
<date>2005</date>
<pages>359--364</pages>
<publisher>Pearson Prentice Hall,</publisher>
<location>NJ:</location>
<contexts>
<context position="10568" citStr="Hogg et al., 2005" startWordPosition="1660" endWordPosition="1663">tained at least two sentences without any entailment relation between them. Results are shown in the next table 1. Table 1: Precision of clustering algorithms BASE S-HAC C-HAC QT EM 0.618 0.577 0.569 0.640 0.489 The ”BASE” column is the baseline, where the Sumo-Metric was applied rather than clustering. Columns ”S-HAC” and ”C-HAC” express the results for Single-link and Complete-link Hierarchical Agglomerative Clustering (Jain et al., 1999). The ”QT” column shows the Quality Threshold algorithm (Heyer et al., 1999) and the last column ”EM” is the Expectation Maximization clustering algorithm (Hogg et al., 2005). One main conclusion, from table 1 is that clustering tends to achieve worst results than simple paraphrase pair extraction. Only the QT achieves better results, but if we take the average of the four clustering algorithms it is equal to 0.568, smaller than the 0.618 baseline. Moreover, these results with the QT algorithm were applied with a very restrictive value for cluster attribution as it is shown in table 2 with an average of almost two sentences per cluster. Table 2: Figures about clustering algorithms Algorithm # Sentences/# Clusters S-HAC 6,23 C-HAC 2,17 QT 2,32 EM 4,16 In fact, tabl</context>
</contexts>
<marker>Hogg, McKean, Craig, 2005</marker>
<rawString>Hogg R., McKean J., and Craig A. 2005 Introduction to Mathematical Statistics. Upper Saddle River, NJ: Pearson Prentice Hall, 359-364.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A Jain</author>
<author>M Murty</author>
<author>P Flynn</author>
</authors>
<title>Data clustering: a review.</title>
<journal>ACM Computing Surveys,</journal>
<pages>31--264</pages>
<marker>Jain, Murty, Flynn, </marker>
<rawString>Jain A., Murty M. and Flynn P. Data clustering: a review. ACM Computing Surveys, 31:264-323</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>D Marcu</author>
</authors>
<title>Summarization beyond sentence extraction: A probabilistic approach to sentence compression.</title>
<date>2002</date>
<journal>Artificial Intelligence,</journal>
<pages>139--1</pages>
<contexts>
<context position="1452" citStr="Knight &amp; Marcu, 2002" startWordPosition="217" endWordPosition="220">stering may not be the best approach for automatic pattern identification. Finally, we will provide some results of different biology based methodologies for pairwise paraphrase alignment. 1 Introduction Sentence Compression can be seen as the removal of redundant words or phrases from an input sentence by creating a new sentence in which the gist of the original meaning of the sentence remains unchanged. Sentence Compression takes an important place for Natural Language Processing (NLP) tasks where specific constraints must be satisfied, such as length in summarization (Barzilay &amp; Lee, 2002; Knight &amp; Marcu, 2002; Shinyama et al., 2002; Barzilay &amp; Lee, 2003; Le Nguyen &amp; Ho, 2004; Unno et al., 2006), style in text simplification (Marsi &amp; Krahmer, 2005) or sentence simplification for subtitling (Daelemans et al., 2004). 1Project partially funded by Portuguese FCT (Reference: POSC/PLP/57438/2004) Generally, Sentence Compression involves performing the following three steps: (1) Extraction of paraphrases from comparable corpora, (2) Alignment of paraphrases and (3) Induction of rewriting rules. Obviously, each of these steps can be performed in many different ways going from totally unsupervised to totall</context>
<context position="2903" citStr="Knight &amp; Marcu, 2002" startWordPosition="438" endWordPosition="441">Barzilay &amp; Lee, 2003) who use clustering of paraphrases to induce rewriting rules. We will see, through classical visualization methodologies (Kruskal &amp; Wish, 1977) and exhaustive experiments, that clustering may not be the best approach for automatic pattern identification. Finally, we will provide some results of different biology based methodologies for pairwise paraphrase alignment. 2 Related Work Two different approaches have been proposed for Sentence Compression: purely statistical methodologies (Barzilay &amp; Lee, 2003; Le Nguyen &amp; Ho, 2004) and hybrid linguistic/statistic methodologies (Knight &amp; Marcu, 2002; Shinyama et al., 2002; Daelemans et al., 2004; Marsi &amp; Krahmer, 2005; Unno et al., 2006). As our work is based on the first paradigm, we will focus on the works proposed by (Barzilay &amp; Lee, 2003) and (Le Nguyen &amp; Ho, 2004). (Barzilay &amp; Lee, 2003) present a knowledge-lean algorithm that uses multiple-sequence alignment to 177 Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 177–184, Prague, June 2007. c�2007 Association for Computational Linguistics learn generate sentence-level paraphrases essentially from unannotated corpus data alone. In contrast to (Barzilay &amp; Lee</context>
</contexts>
<marker>Knight, Marcu, 2002</marker>
<rawString>Knight K. and Marcu D. 2002. Summarization beyond sentence extraction: A probabilistic approach to sentence compression. Artificial Intelligence, 139(1):91-107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J B Kruskal</author>
<author>M Wish</author>
</authors>
<title>Multidimensional Scaling. Sage Publications.</title>
<date>1977</date>
<location>Beverly Hills. CA.</location>
<contexts>
<context position="795" citStr="Kruskal &amp; Wish, 1977" startWordPosition="112" endWordPosition="115">Dias CLT and Bioinformatics University of Beira Interior Covilh˜a, Portugal ddg@di.ubi.pt Guillaume Cleuziou LIFO - University of Orl´eans Orl´eans, France guillaume.cleuziou@ univ-orleans.fr Abstract 1 In this paper, we present a study for extracting and aligning paraphrases in the context of Sentence Compression. First, we justify the application of a new measure for the automatic extraction of paraphrase corpora. Second, we discuss the work done by (Barzilay &amp; Lee, 2003) who use clustering of paraphrases to induce rewriting rules. We will see, through classical visualization methodologies (Kruskal &amp; Wish, 1977) and exhaustive experiments, that clustering may not be the best approach for automatic pattern identification. Finally, we will provide some results of different biology based methodologies for pairwise paraphrase alignment. 1 Introduction Sentence Compression can be seen as the removal of redundant words or phrases from an input sentence by creating a new sentence in which the gist of the original meaning of the sentence remains unchanged. Sentence Compression takes an important place for Natural Language Processing (NLP) tasks where specific constraints must be satisfied, such as length in </context>
<context position="2447" citStr="Kruskal &amp; Wish, 1977" startWordPosition="372" endWordPosition="375">action of paraphrases from comparable corpora, (2) Alignment of paraphrases and (3) Induction of rewriting rules. Obviously, each of these steps can be performed in many different ways going from totally unsupervised to totally supervised. In this paper, we will focus on the first two steps. In particular, we will first justify the application of a new measure for the automatic extraction of paraphrase corpora. Second, we will discuss the work done by (Barzilay &amp; Lee, 2003) who use clustering of paraphrases to induce rewriting rules. We will see, through classical visualization methodologies (Kruskal &amp; Wish, 1977) and exhaustive experiments, that clustering may not be the best approach for automatic pattern identification. Finally, we will provide some results of different biology based methodologies for pairwise paraphrase alignment. 2 Related Work Two different approaches have been proposed for Sentence Compression: purely statistical methodologies (Barzilay &amp; Lee, 2003; Le Nguyen &amp; Ho, 2004) and hybrid linguistic/statistic methodologies (Knight &amp; Marcu, 2002; Shinyama et al., 2002; Daelemans et al., 2004; Marsi &amp; Krahmer, 2005; Unno et al., 2006). As our work is based on the first paradigm, we will </context>
<context position="5427" citStr="Kruskal &amp; Wish, 1977" startWordPosition="832" endWordPosition="835">y, (Barzilay &amp; Lee, 2003) propose to use the N-gram Overlap metric to capture similarities between sentences and automatically create paraphrase corpora. However, this choice is arbitrary and mainly leads to the extraction of quasi-exact or exact matching pairs. For that purpose, we introduce a new metric, the Sumo-Metric. Unlike (Le Nguyen &amp; Ho, 2004), one interesting idea proposed by (Barzilay &amp; Lee, 2003) is to cluster similar pairs of paraphrases to apply multiplesequence alignment. However, once again, this choice is not justified and we will see by classical visualization methodologies (Kruskal &amp; Wish, 1977) and exhaustive experiments by applying different clustering algorithms, that clustering may not be the best approach for automatic pattern identification. As a consequence, we will study global and local biology based sequence alignments compared to multi-sequence alignment that may lead to better results for the induction of rewriting rules. 3 Paraphrase Corpus Construction Paraphrase corpora are golden resources for learning monolingual text-to-text rewritten patterns. However, such corpora are expensive to construct manually and will always be an imperfect and biased representation of the </context>
<context position="12234" citStr="Kruskal &amp; Wish, 1977" startWordPosition="1944" endWordPosition="1947">(Sa,Sb) = 179 clustering is a worthless effort for automatic paraphrase corpora construction. 3.2.2 Visualization In this subsection, we propose a visual analysis of the different similarity measures tested previously: the Edit Distance (Levenshtein, 1966), the BLEU metric (Papineni et al., 2001), the word Ngram overlap and the Sumo-Metric. The goal of this study is mainly to give the reader a visual interpretation about the organization each measure induces on the data. To perform this study, we use a Multidimensional Scaling (MDS) process which is a traditional data analysis technique. MDS (Kruskal &amp; Wish, 1977) allows to display the structure of distance-like data into an Euclidean space. Since the only available information is a similarity in our case, we transform similarity values into distance values as in Equation 3. ��� �1/2 (3) This transformation enables to obtain a (pseudo) distance measure satisfying properties like minimality, identity and symmetry. On a theoretical point of view, the measure we obtain is a pseudo-distance only, since triangular inequality is not necessary satisfied. In practice, the projection space we build with the MDS from such a pseudo-distance is sufficient to have </context>
</contexts>
<marker>Kruskal, Wish, 1977</marker>
<rawString>Kruskal J. B. and Wish M. 1977. Multidimensional Scaling. Sage Publications. Beverly Hills. CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Le Nguyen</author>
<author>S Horiguchi</author>
<author>A S</author>
<author>B T Ho</author>
</authors>
<title>Example-based sentence reduction using the hidden markov model.</title>
<date>2004</date>
<journal>ACM Transactions on Asian Language Information Processing (TALIP),</journal>
<pages>3--2</pages>
<marker>Le Nguyen, Horiguchi, S, Ho, 2004</marker>
<rawString>Le Nguyen M., Horiguchi S., A. S., and Ho B. T. 2004. Example-based sentence reduction using the hidden markov model. ACM Transactions on Asian Language Information Processing (TALIP), 3(2):146-158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Levenshtein</author>
</authors>
<title>Binary Codes Capable of Correcting Deletions, Insertions, and Reversals. Soviet Physice-Doklady,</title>
<date>1966</date>
<pages>10--707</pages>
<contexts>
<context position="11869" citStr="Levenshtein, 1966" startWordPosition="1884" endWordPosition="1885">ion the results presented by (Barzilay &amp; Lee, 2003) who only keep the clusters that contain more than 10 sentences. In fact, the first conclusion is that the number of experimented clusters is very low, and more important, all clusters with more than 10 sentences showed to be of very bad quality. The next subsection will reinforce the sight that 8 ��� � ���� S(Sa,Sb) = 179 clustering is a worthless effort for automatic paraphrase corpora construction. 3.2.2 Visualization In this subsection, we propose a visual analysis of the different similarity measures tested previously: the Edit Distance (Levenshtein, 1966), the BLEU metric (Papineni et al., 2001), the word Ngram overlap and the Sumo-Metric. The goal of this study is mainly to give the reader a visual interpretation about the organization each measure induces on the data. To perform this study, we use a Multidimensional Scaling (MDS) process which is a traditional data analysis technique. MDS (Kruskal &amp; Wish, 1977) allows to display the structure of distance-like data into an Euclidean space. Since the only available information is a similarity in our case, we transform similarity values into distance values as in Equation 3. ��� �1/2 (3) This t</context>
<context position="20675" citStr="Levenshtein, 1966" startWordPosition="3325" endWordPosition="3326">ximations regarding the rates and probabilities of amino acid mutations. Different matrices might produce different alignments. Subsequently, this motivated the idea of modeling word mutation. It seems intuitive to allow such a word mutation, considering the possible relationships that exit between words: lexical, syntactical or semantic. For example, it seems evident that between spirit and spiritual there exists a stronger relation (higher mutation probability) than between spiritual and hamburger. A natural possibility to choose a word mutation representation function is the Edit-distance (Levenshtein, 1966) (edist(.,.)) as a negative reward for word alignment. For a given word pair (wi, wj), the greater the Edit-distance value, the more unlikely the word wi will be aligned with word wj. However, after some early experiments with this function, it revealed to lead to some problems by enabling alignments between very different words, like (total, israel), (fire, made) or (troops, members), despite many good alignments also achieved. This happens because the Editdistance returns relatively small values, unable to sufficiently penalize different words, like the ones listed before, to inhibit the ali</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>Levenshtein V. 1966. Binary Codes Capable of Correcting Deletions, Insertions, and Reversals. Soviet Physice-Doklady, 10:707-710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Marsi</author>
<author>E Krahmer</author>
</authors>
<title>Explorations in sentence fusion.</title>
<date>2005</date>
<booktitle>In Proceedings of the 10th European Workshop on Natural Language Generation.</booktitle>
<contexts>
<context position="1593" citStr="Marsi &amp; Krahmer, 2005" startWordPosition="242" endWordPosition="245"> methodologies for pairwise paraphrase alignment. 1 Introduction Sentence Compression can be seen as the removal of redundant words or phrases from an input sentence by creating a new sentence in which the gist of the original meaning of the sentence remains unchanged. Sentence Compression takes an important place for Natural Language Processing (NLP) tasks where specific constraints must be satisfied, such as length in summarization (Barzilay &amp; Lee, 2002; Knight &amp; Marcu, 2002; Shinyama et al., 2002; Barzilay &amp; Lee, 2003; Le Nguyen &amp; Ho, 2004; Unno et al., 2006), style in text simplification (Marsi &amp; Krahmer, 2005) or sentence simplification for subtitling (Daelemans et al., 2004). 1Project partially funded by Portuguese FCT (Reference: POSC/PLP/57438/2004) Generally, Sentence Compression involves performing the following three steps: (1) Extraction of paraphrases from comparable corpora, (2) Alignment of paraphrases and (3) Induction of rewriting rules. Obviously, each of these steps can be performed in many different ways going from totally unsupervised to totally supervised. In this paper, we will focus on the first two steps. In particular, we will first justify the application of a new measure for </context>
<context position="2973" citStr="Marsi &amp; Krahmer, 2005" startWordPosition="450" endWordPosition="453">riting rules. We will see, through classical visualization methodologies (Kruskal &amp; Wish, 1977) and exhaustive experiments, that clustering may not be the best approach for automatic pattern identification. Finally, we will provide some results of different biology based methodologies for pairwise paraphrase alignment. 2 Related Work Two different approaches have been proposed for Sentence Compression: purely statistical methodologies (Barzilay &amp; Lee, 2003; Le Nguyen &amp; Ho, 2004) and hybrid linguistic/statistic methodologies (Knight &amp; Marcu, 2002; Shinyama et al., 2002; Daelemans et al., 2004; Marsi &amp; Krahmer, 2005; Unno et al., 2006). As our work is based on the first paradigm, we will focus on the works proposed by (Barzilay &amp; Lee, 2003) and (Le Nguyen &amp; Ho, 2004). (Barzilay &amp; Lee, 2003) present a knowledge-lean algorithm that uses multiple-sequence alignment to 177 Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 177–184, Prague, June 2007. c�2007 Association for Computational Linguistics learn generate sentence-level paraphrases essentially from unannotated corpus data alone. In contrast to (Barzilay &amp; Lee, 2002), they need neither parallel data nor explicit information abou</context>
</contexts>
<marker>Marsi, Krahmer, 2005</marker>
<rawString>Marsi E. and Krahmer E. 2005. Explorations in sentence fusion. In Proceedings of the 10th European Workshop on Natural Language Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2001</date>
<note>IBM Research Report RC22176.</note>
<contexts>
<context position="11910" citStr="Papineni et al., 2001" startWordPosition="1889" endWordPosition="1892">y &amp; Lee, 2003) who only keep the clusters that contain more than 10 sentences. In fact, the first conclusion is that the number of experimented clusters is very low, and more important, all clusters with more than 10 sentences showed to be of very bad quality. The next subsection will reinforce the sight that 8 ��� � ���� S(Sa,Sb) = 179 clustering is a worthless effort for automatic paraphrase corpora construction. 3.2.2 Visualization In this subsection, we propose a visual analysis of the different similarity measures tested previously: the Edit Distance (Levenshtein, 1966), the BLEU metric (Papineni et al., 2001), the word Ngram overlap and the Sumo-Metric. The goal of this study is mainly to give the reader a visual interpretation about the organization each measure induces on the data. To perform this study, we use a Multidimensional Scaling (MDS) process which is a traditional data analysis technique. MDS (Kruskal &amp; Wish, 1977) allows to display the structure of distance-like data into an Euclidean space. Since the only available information is a similarity in our case, we transform similarity values into distance values as in Equation 3. ��� �1/2 (3) This transformation enables to obtain a (pseudo</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Papineni K., Roukos S., Ward T., Zhu W.-J. 2001. BLEU: a Method for Automatic Evaluation of Machine Translation. IBM Research Report RC22176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Shinyama</author>
<author>S Sekine</author>
<author>K Sudo</author>
</authors>
<title>Automatic Paraphrase Acquisition from News Articles. Sao Diego,</title>
<date>2002</date>
<location>USA.</location>
<contexts>
<context position="1475" citStr="Shinyama et al., 2002" startWordPosition="221" endWordPosition="224"> best approach for automatic pattern identification. Finally, we will provide some results of different biology based methodologies for pairwise paraphrase alignment. 1 Introduction Sentence Compression can be seen as the removal of redundant words or phrases from an input sentence by creating a new sentence in which the gist of the original meaning of the sentence remains unchanged. Sentence Compression takes an important place for Natural Language Processing (NLP) tasks where specific constraints must be satisfied, such as length in summarization (Barzilay &amp; Lee, 2002; Knight &amp; Marcu, 2002; Shinyama et al., 2002; Barzilay &amp; Lee, 2003; Le Nguyen &amp; Ho, 2004; Unno et al., 2006), style in text simplification (Marsi &amp; Krahmer, 2005) or sentence simplification for subtitling (Daelemans et al., 2004). 1Project partially funded by Portuguese FCT (Reference: POSC/PLP/57438/2004) Generally, Sentence Compression involves performing the following three steps: (1) Extraction of paraphrases from comparable corpora, (2) Alignment of paraphrases and (3) Induction of rewriting rules. Obviously, each of these steps can be performed in many different ways going from totally unsupervised to totally supervised. In this p</context>
<context position="2926" citStr="Shinyama et al., 2002" startWordPosition="442" endWordPosition="445">who use clustering of paraphrases to induce rewriting rules. We will see, through classical visualization methodologies (Kruskal &amp; Wish, 1977) and exhaustive experiments, that clustering may not be the best approach for automatic pattern identification. Finally, we will provide some results of different biology based methodologies for pairwise paraphrase alignment. 2 Related Work Two different approaches have been proposed for Sentence Compression: purely statistical methodologies (Barzilay &amp; Lee, 2003; Le Nguyen &amp; Ho, 2004) and hybrid linguistic/statistic methodologies (Knight &amp; Marcu, 2002; Shinyama et al., 2002; Daelemans et al., 2004; Marsi &amp; Krahmer, 2005; Unno et al., 2006). As our work is based on the first paradigm, we will focus on the works proposed by (Barzilay &amp; Lee, 2003) and (Le Nguyen &amp; Ho, 2004). (Barzilay &amp; Lee, 2003) present a knowledge-lean algorithm that uses multiple-sequence alignment to 177 Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 177–184, Prague, June 2007. c�2007 Association for Computational Linguistics learn generate sentence-level paraphrases essentially from unannotated corpus data alone. In contrast to (Barzilay &amp; Lee, 2002), they need neit</context>
</contexts>
<marker>Shinyama, Sekine, Sudo, 2002</marker>
<rawString>Shinyama Y., Sekine S., and Sudo K. 2002. Automatic Paraphrase Acquisition from News Articles. Sao Diego, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Unno</author>
<author>T Ninomiya</author>
<author>Y Miyao</author>
<author>J Tsujii</author>
</authors>
<title>Trimming CFG Parse Trees for Sentence Compression Using Machine Learning Approaches.</title>
<date>2006</date>
<booktitle>In the Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions.</booktitle>
<contexts>
<context position="1539" citStr="Unno et al., 2006" startWordPosition="234" endWordPosition="237">ll provide some results of different biology based methodologies for pairwise paraphrase alignment. 1 Introduction Sentence Compression can be seen as the removal of redundant words or phrases from an input sentence by creating a new sentence in which the gist of the original meaning of the sentence remains unchanged. Sentence Compression takes an important place for Natural Language Processing (NLP) tasks where specific constraints must be satisfied, such as length in summarization (Barzilay &amp; Lee, 2002; Knight &amp; Marcu, 2002; Shinyama et al., 2002; Barzilay &amp; Lee, 2003; Le Nguyen &amp; Ho, 2004; Unno et al., 2006), style in text simplification (Marsi &amp; Krahmer, 2005) or sentence simplification for subtitling (Daelemans et al., 2004). 1Project partially funded by Portuguese FCT (Reference: POSC/PLP/57438/2004) Generally, Sentence Compression involves performing the following three steps: (1) Extraction of paraphrases from comparable corpora, (2) Alignment of paraphrases and (3) Induction of rewriting rules. Obviously, each of these steps can be performed in many different ways going from totally unsupervised to totally supervised. In this paper, we will focus on the first two steps. In particular, we wi</context>
<context position="2993" citStr="Unno et al., 2006" startWordPosition="454" endWordPosition="457">ee, through classical visualization methodologies (Kruskal &amp; Wish, 1977) and exhaustive experiments, that clustering may not be the best approach for automatic pattern identification. Finally, we will provide some results of different biology based methodologies for pairwise paraphrase alignment. 2 Related Work Two different approaches have been proposed for Sentence Compression: purely statistical methodologies (Barzilay &amp; Lee, 2003; Le Nguyen &amp; Ho, 2004) and hybrid linguistic/statistic methodologies (Knight &amp; Marcu, 2002; Shinyama et al., 2002; Daelemans et al., 2004; Marsi &amp; Krahmer, 2005; Unno et al., 2006). As our work is based on the first paradigm, we will focus on the works proposed by (Barzilay &amp; Lee, 2003) and (Le Nguyen &amp; Ho, 2004). (Barzilay &amp; Lee, 2003) present a knowledge-lean algorithm that uses multiple-sequence alignment to 177 Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 177–184, Prague, June 2007. c�2007 Association for Computational Linguistics learn generate sentence-level paraphrases essentially from unannotated corpus data alone. In contrast to (Barzilay &amp; Lee, 2002), they need neither parallel data nor explicit information about sentence semantics</context>
</contexts>
<marker>Unno, Ninomiya, Miyao, Tsujii, 2006</marker>
<rawString>Unno Y., Ninomiya T., Miyao Y. and Tsujii J. 2006. Trimming CFG Parse Trees for Sentence Compression Using Machine Learning Approaches. In the Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>