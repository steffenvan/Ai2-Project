<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011255">
<title confidence="0.997255">
Predicting Salient Updates for Disaster Summarization
</title>
<author confidence="0.99063">
Chris Kedzie and Kathleen McKeown Fernando Diaz
</author>
<affiliation confidence="0.9907405">
Columbia University Microsoft Research
Department of Computer Science fdiaz@microsoft.com
</affiliation>
<email confidence="0.993424">
{kedzie, kathy}@cs.columbia.edu
</email>
<sectionHeader confidence="0.99729" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999941">
During crises such as natural disasters or
other human tragedies, information needs
of both civilians and responders often re-
quire urgent, specialized treatment. Moni-
toring and summarizing a text stream dur-
ing such an event remains a difficult prob-
lem. We present a system for update sum-
marization which predicts the salience of
sentences with respect to an event and
then uses these predictions to directly bias
a clustering algorithm for sentence se-
lection, increasing the quality of the up-
dates. We use novel, disaster-specific
features for salience prediction, including
geo-locations and language models repre-
senting the language of disaster. Our eval-
uation on a standard set of retrospective
events using ROUGE shows that salience
prediction provides a significant improve-
ment over other approaches.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999944779661017">
During crises, information is critical for first re-
sponders, crisis management organizations, and
those caught in the event. When the event is sig-
nificant, as in the case of Hurricane Sandy, the
amount of content produced by traditional news
outlets, government agencies, relief organizations,
and social media can vastly overwhelm those try-
ing to monitor the situation. Crisis informatics
(Palen et al., 2010) is dedicated to finding methods
for sharing the right information in a timely fash-
ion during such an event. Research in this field has
focused on human-in-the-loop approaches rang-
ing from on the ground information gathering to
crowdsourced reporting and disaster management
(Starbird and Palen, 2013).
Multi-document summarization has the poten-
tial to assist the crisis informatics community. Au-
tomatic summarization could deliver relevant and
salient information at regular intervals, even when
human volunteers are unable to. Perhaps more im-
portantly it could help filter out unnecessary and
irrelevant detail when the volume of incoming in-
formation is large. While methods for identifying,
tracking, and summarizing events from text based
input have been explored extensively (Allan et al.,
1998; Filatova and Hatzivassiloglou, 2004; Wang
et al., 2011), these experiments were not devel-
oped to handle streaming data from a heteroge-
neous environment at web scale. These methods
also rely heavily on redundancy which is subop-
timal for time sensitive domains where there is a
high cost in delaying information.
In this paper, we present an update summariza-
tion system to track events across time. Our sys-
tem predicts sentence salience in the context of a
large-scale event, such as a disaster, and integrates
these predictions into a clustering based multi-
document summarization system. We demonstrate
that combining salience with clustering produces
more relevant summaries compared to baselines
using clustering or relevance alone. Our experi-
ments suggest that this is because our system is
better able to adapt to dynamic changes in input
volume that adversely affect methods that use re-
dundancy as a proxy for salience.
In addition to the tight integration between clus-
tering and salience prediction, our approach also
exploits knowledge about the event to determine
salience. Thus, salience represents both how typi-
cal a sentence is of the event type (e.g., industrial
accident, hurricane, riot) and whether it specifies
information about this particular event. Our fea-
ture representation includes a set of language mod-
els, one for each event type, to measure the typi-
cality of the sentence with regard to the current
event, the distance of mentioned locations from
the center of the event, and the change in word
frequencies over the time of the event. While we
evaluate these features in the domain of disasters,
</bodyText>
<page confidence="0.936983">
1608
</page>
<note confidence="0.976613666666667">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1608–1617,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999567">
this approach is generally applicable to many up-
date summarization tasks.
Our approach achieves a statistically significant
improvement in ROUGE scores compared to mul-
tiple baselines. Additionally, we introduce novel
methods for estimating the average information
gain each update provides and how completely the
update summary covers the event it is tracking; our
system’s updates contain more relevant informa-
tion on average than the competing baselines.
The remainder of the paper is organized as fol-
lows. We begin with a review of related work
in the information retrieval and multi-document
summarization literature. Section 3 outlines the
details of our salience and summarization models.
Next we describe our data (Section 4) and experi-
ments (Section 5). Finally, we discuss our results
(Section 6) and conclude the paper.
</bodyText>
<sectionHeader confidence="0.999943" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999984954545455">
A principal concern in extractive multi-document
summarization is the selection of salient sentences
for inclusion in summary output (Nenkova and
McKeown, 2012). Existing approaches generally
fall into one of three categories, each with specific
trade-offs with respect to update summarization.
First, centrality-focused approaches (including
graph (Erkan and Radev, 2004), cluster (Hatzivas-
siloglou et al., 2001), and centroid (Radev et al.,
2004) methods) are very natural for retrospective
analysis in the sense that they let the data “speak
for itself.” These methods equate salience with
centrality, either to the input or some other ag-
gregate object (i.e. a cluster center or input cen-
troid). However, they rely chiefly on redundancy.
When applied to an unfolding event, there may not
exist enough redundant content at the event onset
for these methods to exploit. Once the event onset
has passed, however, the redundancy reduction of
these methods is quite beneficial.
The second category, predictive approaches, in-
cludes ranking and classification based methods.
Sentences have been ranked by the average word
probability, average TF*IDF score, and the num-
ber of topically related words (topic-signatures in
the summarization literature) (Nenkova and Van-
derwende, 2005; Hovy and Lin, 1998; Lin and
Hovy, 2000). The first two statistics are easily
computable from the input sentences, while the
third only requires an additional, generic back-
ground corpus. In classification based methods,
model features are usually derived from human
generated summaries, and are non-lexical in na-
ture (e.g., sentence starting position, number of
topic-signatures, number of unique words). Sem-
inal work in this area has employed naive Bayes
and logistic regression classifiers to identify sen-
tences for summary inclusion (Kupiec et al., 1995;
Conroy et al., 2001). While these methods are
less dependent on redundancy, the expressiveness
of their features is limited. Our model expands
on these basic features to account for geographic,
temporal, and language model features.
The last category includes probabilistic
(Haghighi and Vanderwende, 2009), information
theoretic, and set cover (Lin and Bilmes, 2011)
approaches. While these methods are focused on
producing diverse summaries, they are difficult to
adapt to the streaming setting, where we do not
necessarily have a fixed summary length and the
corpus to be summarized contains many irrelevant
sentences, i.e. there are large portions of the
corpora that we specifically want to avoid.
Several researchers have recognized the impor-
tance of summarization during natural disasters.
(Guo et al., 2013) developed a system for detect-
ing novel, relevant, and comprehensive sentences
immediately after a natural disaster. (Wang and
Li, 2010) present a clustering-based approach to
efficiently detect important updates during natural
disasters. The algorithm works by hierarchically
clustering sentences online, allowing the system to
output a more expressive narrative structure than
(Guo et al., 2013). Our system attempts to unify
these system’s approaches (predictive ranking and
clustering respectively).
</bodyText>
<sectionHeader confidence="0.980761" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.999818428571428">
Our update summarization system takes as input
a) a short query defining the event to be tracked
(e.g. ‘Hurricane Sandy’), b) an event category
defining the type of event to be tracked (e.g. ‘hur-
ricane’), c) a stream of time-stamped documents
presented in temporal order, and d) an evaluation
time period of interest. While processing docu-
ments throughout the time period of interest, the
system outputs sentences from these documents
likely to be useful to the query issuer. We refer
to these selected sentences as updates.
In order to measure the usefulness of a sys-
tem’s updates, we consider the degree to which
the system output reflects the different aspects of
</bodyText>
<page confidence="0.992485">
1609
</page>
<bodyText confidence="0.970018333333333">
– hurricane force wind warnings are in effect
from Rhode Island Sound to Chincoteague
Bay
– over 5000 commercial airline flights sched-
uled for October 28 and October 29 were
cancelled
</bodyText>
<figureCaption confidence="0.985148">
Figure 1: Example nuggets from Hurricane Sandy.
</figureCaption>
<bodyText confidence="0.99884980952381">
an event. Events are often composed of a vari-
ety of sub-events. For example, the Hurricane
Sandy event includes sub-events related to the
storm making landfall, the ensuing flooding, the
many transportation issues, among many others.
An ideal system would update the user about each
of these sub-events as they occur. We refer to
these sub-events as the nuggets associated with an
event. A nugget is defined as a fine-grained atomic
sub-event associated with an event. We present
2 example nuggets associated with the Hurricane
Sandy event in Figure 1. Each event has anywhere
from 50 to several hundred nuggets in total in our
gold dataset. We describe how these nuggets are
found in Section 4.
Throughout our treatment of our algorithm, the
salience of an update captures the degree to which
it reflects an event’s unobserved nuggets. Assum-
ing that we have a text representation for each of
our nuggets, the salience of an update u with re-
spect to a set of nuggets N is defined as,
</bodyText>
<equation confidence="0.994581">
salience(u) = maxn∈N sim(u, n) (1)
</equation>
<bodyText confidence="0.999892666666667">
where sim(·) is the semantic similarity such as the
cosine similarity of latent vectors associated with
the update and nugget text (Guo and Diab, 2012).
</bodyText>
<subsectionHeader confidence="0.999259">
3.1 Update Summarization
</subsectionHeader>
<bodyText confidence="0.998697857142857">
Our system architecture follows a simple pipeline
design where each stage provides an additional
level of processing or filtering of the input sen-
tences. We begin with an empty update summary
U. At each hour we receive a new batch of sen-
tences bt from the stream of event relevant docu-
ments and perform the following actions:
</bodyText>
<listItem confidence="0.999434">
1. predict the salience of sentences in bt (Sec-
tion 3.2),
2. select a set of exemplar sentences in bt by
combining clustering with salience predic-
tions (Section 3.3),
3. add the most novel and salient exemplars to
U (Section 3.4).
</listItem>
<bodyText confidence="0.9975595">
The resultant list of updates U is our summary of
the event.
</bodyText>
<subsectionHeader confidence="0.990487">
3.2 Salience Prediction
3.2.1 Features
</subsectionHeader>
<bodyText confidence="0.999942022727273">
We want our model to be predictive of sen-
tence salience across different event instances
so we avoid event-specific lexical features. In-
stead, we extract features such as language model
scores, geographic relevance, and temporal rele-
vance from each sentence.
Basic Features We employ several basic fea-
tures that have been used previously in supervised
models to rank sentence salience (Kupiec et al.,
1995; Conroy et al., 2001). These include sen-
tence length, the number of capitalized words nor-
malized by sentence length, document position,
and number of named entities. The data stream
comprises text extracted from raw html docu-
ments; these features help to downweight sen-
tences that are not content (e.g. web page titles,
links to other content) or more heavily weight im-
portant sentences (e.g., that appear in prominent
positions such as paragraph initial or article ini-
tial).
Query Features Query features measure the
relationship between the sentence and the event
query and type. These include the number of
query words present in the sentence in addition to
the number of event type synonyms, hypernyms,
and hyponyms using WordNet (Miller, 1995). For
example, for event type earthquake, we match sen-
tence terms “quake”, “temblor”, “seism”, and “af-
tershock”.
Language Model Features Language models
allow us to measure the likelihood of producing
a sentence from a particular source. We consider
two types of language model features. The first
model is estimated from a corpus of generic news
articles (we used the 1995-2010 Associated Press
section of the Gigaword corpus (Graff and Cieri,
2003)). This model is intended to assess the gen-
eral writing quality (grammaticality, word usage)
of an input sentence and helps our model to select
sentences written in the newswire style.
The second model is estimated from text spe-
cific to our event types. For each event type
we create a corpus of related documents using
pages and subcategories listed under a related
</bodyText>
<page confidence="0.922241">
1610
</page>
<figure confidence="0.97273">
Storm Earthquake Meteor Impact
Accident Riot Protest
Hostages Shooting Bombing
</figure>
<figureCaption confidence="0.999313">
Figure 2: TREC TS event types.
</figureCaption>
<bodyText confidence="0.99975086746988">
Wikipedia category. For example, the language
model for event type ‘earthquake’ is estimated
from Wikipedia pages under the category Cate-
gory:Earthquakes. Fig. 2 lists the event types
found in our dataset. These models are intended
to detect sentences similar to those appearing in
summaries of other events in the same category
(e.g. most earthquake summaries are likely to in-
clude higher probability for ngrams including the
token ‘magnitude’). While we focus our system on
the language of news and disaster, we emphasize
that the use of language modeling can be an effec-
tive feature for multi-document summarization for
other domains that have related text corpora.
We use the SRILM toolkit (Stolcke and others,
2002) to implement a 5-gram Kneser-Ney model
for both the background language model and the
event specific language models. For each sentence
we use the average token log probability under
each model as a feature.
Geographic Relevance Features The disasters
in our corpus are all phenomena that affect some
part of the world. Where possible, we would like
to capture a sentence’s proximity to the event, i.e.
when a sentence references a location, it should be
close to the area of the disaster.
There are two challenges to using geographic
features. First, we do not know where the event is,
and second, most sentences do not contain refer-
ences to a location. We address the first issue by
extracting all locations from documents relevant to
the event at the current hour and looking up their
latitude and longitude using a publicly available
geo-location service. Since the documents that are
at least somewhat relevant to the event, we assume
in aggregate the locations should give us a rough
area of interest. The locations are clustered and
we treat the resulting cluster centers as the event
locations for the current time.
The second issue arises from the fact that the
majority of sentences in our data do not contain
explicit references to locations, i.e. a sequence of
tokens tagged as location named entities. Our in-
tuition is that geographic relevance is important in
the disaster domain, and we would like to take ad-
vantage of the sentences that do have location in-
formation present. To make up for this imbalance,
we instead compute an overall location for the
document and derive geographic features based on
the document’s proximity to the event in question.
These features are assigned to all sentences in the
document.
Our method of computing document-level ge-
ographic relevance features is as follows. Using
the locations in each document, we compute the
median distance to the nearest event location. Be-
cause document position is a good indicator of im-
portance we also compute the distance of the first
mentioned location to the nearest event location.
All sentences in the document take as features
these two distance calculations. Because some
events can move, we also compute these distances
to event locations from the previous hour.
Temporal Relevance Features As we track
events over time, it is likely that the coverage of
the event may die down, only to spike back up
when there is a breaking development. Identify-
ing terms that are “bursty,” i.e. suddenly peaking
in usage, can help to locate novel sentences that
are part of the most recent reportage and have yet
to fall into the background.
We compute the IDF for each hour in our data
stream. For each sentence, the average TF*IDF
for the current hour t is taken as a feature. Addi-
tionally, we use the difference in average TF*IDF
from time t to t − i for i = {1, ... , 24} to mea-
sure how the TF*IDF scores for the sentence have
changed over the last 24 hours, i.e. we keep the
sentence term frequencies fixed and compute the
difference in IDF. Large changes in IDF value in-
dicate the sentence contains bursty terms. We also
use the time (in hours) since the event started as a
feature.
</bodyText>
<subsectionHeader confidence="0.834084">
3.2.2 Model
</subsectionHeader>
<bodyText confidence="0.99998275">
Given our feature representation of the input sen-
tences, we need only target salience values for
model learning. For each event in our training
data, we sample a set of sentences and each sen-
tence’s salience is computed according to Equa-
tion 1. This results in a training set of sen-
tences, their feature representations, and their tar-
get salience values to predict.
We opt to use a Gaussian process (GP) re-
gression model (Rasmussen and Williams, 2006)
with a Radial Basis Function (RBF) kernel for the
salience prediction task. Our features fall naturally
</bodyText>
<page confidence="0.958056">
1611
</page>
<bodyText confidence="0.999951833333333">
into five groups and we use a separate RBF kernel
for each, using the sum of each feature group RBF
kernel as the final input to the GP model.
average but the volume of input is low. The adap-
tive nature of our model differentiates our method
from most other update summarization systems.
</bodyText>
<subsectionHeader confidence="0.995419">
3.3 Exemplar Selection
</subsectionHeader>
<bodyText confidence="0.999775583333333">
Once we have predicted the salience for a batch
of sentences, we must now select a set of update
candidates, i.e. sentences that are both salient and
representative of the current batch. To accomplish
this, we combine the output of our salience pre-
diction model with the affinity propagation algo-
rithm. Affinity propagation (AP) is a clustering
algorithm that identifies a subset of data points as
exemplars and forms clusters by assigning the re-
maining points to one of the exemplars (Frey and
Dueck, 2007). AP attempts to maximize the net
similarity objective
</bodyText>
<equation confidence="0.983719">
n n
S = E sim(i, ei) + E salience(ei)
i:i7�ei i:i=ei
</equation>
<bodyText confidence="0.999969580645161">
where ei is the exemplar of the i-th data point, and
functions sim and salience express the pairwise
similarity of data points and our predicted apri-
ori preference of a data point to be an exemplar
respectively. AP differs from other k-centers algo-
rithms in that it simultaneously considers all data
points as exemplars, making it less prone to find-
ing local optima as a result of poor initialization.
Furthermore, the second term in S incorporates
the individual importance of data points as candi-
date exemplars; most other clustering algorithms
only make use of the first term, i.e. the pairwise
similarities between data points.
AP has several useful properties and interpre-
tations. Chiefly, the number of clusters k is not
a model hyper-parameter. Given that our task re-
quires clustering many batches of streaming data,
searching for an optimal k would be computation-
ally prohibitive. With AP, k is determined by the
similarities and preferences of the data. Generally
lower preferences will result in fewer clusters.
Recall that salience(s) is a prediction of the
semantic similarity of s to information about the
event be summarized, i.e. the set of event nuggets.
Intuitively, when maximizing objective function
S, AP must balance between best representing the
input data and representing the most salient in-
put. Additionally, when the level of input is high
but the salience predictions are low, the preference
term will guide AP toward a solution with fewer
clusters; vice-versa when input is very salient on
</bodyText>
<subsectionHeader confidence="0.990631">
3.4 Update Selection
</subsectionHeader>
<bodyText confidence="0.999987333333333">
The exemplar sentences from the exemplar selec-
tion stage are the most salient and representative of
the input for the current hour. However, we need
to reconcile these sentences with updates from the
previous hour to ensure that the most salient and
least redundant updates are selected. To ensure
that only the most salient updates are selected we
apply a minimum salience threshold; after exem-
plar sentences have been identified, any exemplars
whose salience is less than Asal are removed from
consideration.
Next, to prevent adding updates that are redun-
dant with previous output updates, we filter out ex-
emplars that are too similar to previous updates.
The exemplars are examined sequentially in order
of decreasing salience and a similarity threshold is
applied, where the exemplar is ignored if its max-
imum semantic similarity to any previous updates
in the summary is greater than Asim.
Exemplars that pass these thresholds are se-
lected as updates and added to the summary.
</bodyText>
<sectionHeader confidence="0.99893" genericHeader="method">
4 Data
</sectionHeader>
<bodyText confidence="0.999733739130435">
For the document stream, we use the news portion
of the 2014 TREC KBA Stream Corpus (Frank et
al., 2012). The documents from this corpus come
from hourly crawls of the web covering October
2011 through February 2013.
Our experiments also make use of the TREC
Temporal Summarization (TS) Track data from
2013 and 2014 (Aslam et al., 2013). This data in-
cludes 25 events and event metadata (e.g., a user
search query for the event, the event type, and
event evaluation time frame). All events occurred
during the time span of the TREC KBA Stream
Corpus. For each event we create a stream of rel-
evant documents by selecting only documents that
contain the complete set of query words.
Along with the metadata, NIST assessors con-
structed a set of ground truth nuggets for each
event. Nuggets are brief and important text snip-
pets that represent sub-events that should be con-
veyed by an ideal update summary. In order to ac-
complish this, for each event, assessors were pro-
vided with the revision history of the Wikipedia
page associated with the event. For example, the
</bodyText>
<page confidence="0.988599">
1612
</page>
<bodyText confidence="0.999969625">
revision history for the Wikipedia page for ‘Hurri-
cane Sandy’ will contain text additions including
those related to individual nuggets. The assess-
ment task involves reviewing the Wikipedia revi-
sions in the evaluation time frame and marking
the text additions capturing a new, unique nugget.
More detail on this process can be found in the
track description (Aslam et al., 2013).
</bodyText>
<sectionHeader confidence="0.999609" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9992608">
We evaluate our system on two metrics: ROUGE
(Lin, 2004), an automatic summarization method
and an evaluation of system expected gain and
comprehensiveness—metrics adapted from the
TREC TS track (Aslam et al., 2013).
</bodyText>
<subsectionHeader confidence="0.968091">
5.1 Training and Testing
</subsectionHeader>
<bodyText confidence="0.999399">
Of the 25 events in the TREC TS data, 24 are
covered by the news portion of the TREC KBA
Stream Corpus. From these 24, we set aside
three events to use as a development set. All
system salience and similarity threshold parame-
ters are tuned on the development set to maximize
ROUGE-2 F1 scores.
We train a salience model for each event us-
ing 1000 sentences randomly sampled from the
event’s document stream.
We perform a leave-one-out evaluation of each
event. At test time, we predict a sentence’s
salience using the average predictions of the 23
other models.
</bodyText>
<subsectionHeader confidence="0.992716">
5.2 ROUGE Evaluation
</subsectionHeader>
<bodyText confidence="0.991633315789474">
ROUGE measures the ngram overlap between
a model summary and an automatically gener-
ated system summary. Model summaries for
each event were constructed by concatenating the
event’s nuggets. Generally, ROUGE evaluation
assumes both model and system summaries are of
a bounded length. Since our systems are summa-
rizing events over a span of two weeks time, the
total length of our system output is much longer
than the model. To address this, for each sys-
tem/event pair, we sample with replacement 1000
random summaries of length less than or equal to
the model summary (truncating the last sentence
when neccessary). The final ROUGE scores for
the system are the average scores from these 1000
samples.
Because we are interested in system perfor-
mance over time, we also evaluate systems at 12
hour intervals using the same regime as above.
The model summaries in this case are retrospec-
tive, and this evaluation reveals how quickly sys-
tems can cover information in the model.
|S|
where S is the set of system updates, S,,, is the set
of nuggets contained in S, and  |·  |is the number
of elements in the set. To compute the set S,,, we
match each system update to 0 or more nuggets,
where an update matches a nugget if their seman-
tic similarity is above a threshold. S,,, results from
the unique set of nuggets matched. Because an
update can map to more than one nugget, it is pos-
sible to receive an expected gain greater than 1.
An expected gain of 1 would indicate that every
sentence was both relevant and contained a unique
piece of information.
Comprehensiveness Additionally, we can use
the nuggets to measure the completeness of an up-
date summary. We define
</bodyText>
<equation confidence="0.63697">
Com Sn
prehensiveness =
</equation>
<bodyText confidence="0.999992071428571">
where N is the set of event nuggets. A compre-
hensiveness of 1 indicates that the summary has
covered all nugget information for the event; the
maximum attainable comprehensiveness is 1.
Update-nugget matches are computed automat-
ically; a match exists if the semantic similarity of
the update/nugget pair is above a threshold. De-
termining an optimal threshold to count matches
is difficult so we evaluate at threshold values rang-
ing from .5 to 1, where values closer to 1 are
more conservative estimates of performance. A
manual inspection of matches suggests that se-
mantic similarity values around .7 produce reason-
able matches. The average semantic similarity of
</bodyText>
<subsectionHeader confidence="0.914504">
5.3 Expected Gain and Comprehensiveness
</subsectionHeader>
<bodyText confidence="0.999659090909091">
NIST developed metrics for evaluating update
summarization systems as part of the TREC TS
track. We present results on two of these metrics,
the expected gain and comprehensiveness.
Expected Gain We treat the event’s nuggets as
unique units of information. When a system adds
an update to its summary, it is potentially adding
some of this nugget information. It would be in-
structive to know how much unique and novel in-
formation each update is adding on average to the
summary. To that end, we define
</bodyText>
<equation confidence="0.9695085">
E[Gain] = |S,,,|
|N|
</equation>
<page confidence="0.892831">
1613
</page>
<bodyText confidence="0.99986275">
manual matches performed by NIST assessors was
much lower at approximately .25, increasing our
confidence in the automatic matches in the .5–1
range.
</bodyText>
<subsectionHeader confidence="0.995395">
5.4 Model Comparisons
</subsectionHeader>
<bodyText confidence="0.999892857142857">
We refer to our complete model as
AP+SALIENCE. We compare this model
against several variants and baselines intended to
measure the contribution of different components.
All thresholds for all runs are tuned on the
development set.
Affinity Propagation only (AP) The purpose
of this model is to directly measure the effect of
integrating salience and clustering by providing a
baseline that uses the identical clustering compo-
nent, but without the salience information. In this
model, input sentences are apriori equally likely
to be exemplars; the salience values are uniformly
set as the median value of the input similarity
scores, as is commonly used in the AP literature
(Frey and Dueck, 2007). After clustering a sen-
tence batch, the exemplars are examined in order
of increasing time since event start and selected
as updates if their maximum similarity to the pre-
vious updates is less than Asim, as in the novelty
filtering stage of AP+SALIENCE.
</bodyText>
<subsectionHeader confidence="0.601151">
Hierarchichal Agglomerative Clustering
</subsectionHeader>
<bodyText confidence="0.999885333333333">
(HAC) We provide another clustering baseline,
single-linkage hierarchichal agglomerative clus-
tering. We include this baseline to show that
AP+SALIENCE is not just an improvement over
AP but centrality driven methods in general.
HAC was chosen over other clustering ap-
proaches because the number of clusters is not an
explicit hyper-parameter. To produce flat clusters
from the hierarchical clustering, we flatten the
HAC dendrogram using the cophenetic distance
criteria, i.e. observations in each flat cluster have
no greater a cophenetic distance than a threshold.
Cluster centers are determined to be the sentence
with highest cosine similariy to the flat cluster
mean. Cluster centers are examined in time order
and are added to the summary if their similarity to
previous updates is below a similarity threshold
Asim as is done in the AP model.
Rank by Salience (RS) We also isolate the im-
pact of our salience model in order to demonstrate
that the fusion of clustering and salience predic-
tion improves over predicting salience alone. In
this model we predict the salience of sentences as
in step 1 for AP+SALIENCE. We omit the cluster-
ing phase (step 2). Updates are selected identically
to step 3 of AP+SALIENCE, proceeding in order
of decreasing salience, selecting updates that are
above a salience threshold Asal and below a simi-
larity threshold Asim with respect to the previously
selected updates.
</bodyText>
<sectionHeader confidence="0.99966" genericHeader="evaluation">
6 Results
</sectionHeader>
<subsectionHeader confidence="0.843979">
6.1 ROUGE
</subsectionHeader>
<table confidence="0.999314083333333">
ROUGE-1
System Recall Prec. F1
AP+SALIENCE 0.282 0.344 0.306
AP 0.245 0.285 0.263
RS 0.230 0.271 0.247
HAC 0.169 0.230 0.186
ROUGE-2
System Recall Prec. F1
AP+SALIENCE 0.045 0.056 0.049
AP 0.033 0.038 0.035
RS 0.031 0.037 0.034
HAC 0.017 0.024 0.019
</table>
<tableCaption confidence="0.999732">
Table 1: System ROUGE performance.
</tableCaption>
<bodyText confidence="0.9975795">
Table 1 shows our results for system output
samples against the full summary of nuggets us-
ing ROUGE. This improvement is statistically sig-
nificant for all ngram precision, recall, and F-
measures at the α = .01 level using the Wilcoxon
signed-rank test.
AP+SALIENCE maintains its performance
above the baselines over time as well. Fig-
ure 3 shows the ROUGE-1 scores over time.
We show the difference in unigram precision
(bigram precision is not shown but it follows
similar curve). Within the initial days of the
event, AP+SALIENCE is able to take the lead
over the over systems in ngram precision. The
AP+SALIENCE model is better able to find salient
updates earlier on; for the disaster domain, this is
an especially important quality of the model.
Moreover, the AP+SALIENCE’s recall is not di-
minished by the high precision and remains com-
petitive with AP. Over time AP+SALIENCE’s re-
call also begins to pull away, while the other mod-
els start to suffer from topic drift.
</bodyText>
<subsectionHeader confidence="0.999078">
6.2 Expected Gain and Comprehensiveness
</subsectionHeader>
<bodyText confidence="0.9944535">
Figure 4 shows the expected gain across a range
of similarity thresholds, where thresholds closer
</bodyText>
<page confidence="0.989122">
1614
</page>
<figure confidence="0.97745275">
2 4 6 8 10
0.4
0.3
Recall 0.2
0.1
0.0
2 4 6 8 10
days since event start
</figure>
<figureCaption confidence="0.9786025">
Figure 3: System ROUGE-1 performance over
time.
</figureCaption>
<bodyText confidence="0.998771647058823">
to 1 are more conservative estimates. The ranking
of the systems remains constant across the sweep
with AP+SALIENCE beating all baseline systems.
Predicting salience in general is helpful for keep-
ing a summary on topic as the RS approach out
performs the clustering only approaches on ex-
pected gain.
When looking at the comprehensiveness of the
summaries AP outperforms AP+SALIENCE. The
compromise encoded in the AP+SALIENCE ob-
jective function, between being representative and
being salient, is seen clearly here where the per-
formance of the AP+SALIENCE methods is lower
bounded by the salience focused RS system and
upper bounded by the clustering only AP system.
Overall, AP+SALIENCE achieves the best balance
of these two metrics.
</bodyText>
<subsectionHeader confidence="0.999453">
6.3 Feature Ablation
</subsectionHeader>
<bodyText confidence="0.9997665">
Table 2 shows the results of our feature ablation
tests. Removing the language models yields a
statistically significant drop in both ngram recall
and F-measure. Interestingly, removing the ba-
sic features leads to an increase in both unigram
and bigram precision; in the bigram case this is
enough to cause a statistically significant increase
in F-measure over the full model. In other words,
the generic features actually lead to an inferior
model when we can incorporate more appropri-
ate domain specific features. The result mirrors
Sparck Jones’ claim that generic approaches to
</bodyText>
<figure confidence="0.9947061">
0.12
0.10
0.08
0.06
0.04
0.02
0.00
0.5 0.6 0.7 0.8 0.9 1.0
0.5 0.6 0.7 0.8 0.9 1.0
Similarity Threshold
</figure>
<figureCaption confidence="0.981562">
Figure 4: Expected Gain and Comprehensiveness
performance.
</figureCaption>
<bodyText confidence="0.9980907">
summarization cannot produce a useful summary
(Sparck-Jones, 1998).
Removing the language model and geographic
relevance features leads to a statistically signifi-
cant drop in ROUGE-1 F1 scores. Unfortunately,
this is not the case for the temporal relevance
features. We surmise that these features are too
strongly correlated with each other, i.e. the differ-
ences in TF*IDF between hours are definitely not
i.i.d. variables.
</bodyText>
<sectionHeader confidence="0.998918" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999546636363636">
In this paper, we have presented an update sum-
marization system for the disaster domain, and
demonstrated improved system performance by
integrating sentence salience with clustering.
We also have shown that features specifically
targeted to the domain of disaster yield better sum-
maries. We developed novel features that capture
the language typical of different event types and
that identify sentences specific to the particular
disaster based on location.
In the future we would like to explore the appli-
</bodyText>
<figure confidence="0.974907239130434">
AP+Salience
AP
HAC
RS
Expected Gain 0.12
0.10
0.08
0.06
0.04
0.02
0.00
0.5 0.6 0.7 0.8 0.9 1.0
Comp.
0.6
0.5
0.4
0.3
0.2
0.1
0.0
AP+Salience
AP
HAC
RS
Prec. 0.4
0.3
0.2
0.1
0.0
F-measure
1615
2012 Pakistan Garment Factory Fires
• The fire broke out when people in the building were trying to start their generator after the electricity went out.
Pakistani television showed pictures of what appeared to be a three-story building with flames leaping from the top-floor
• windows and smoke billowing into the night sky.
The people went to the back side of the building but there was no access, so we had to made forceful entries and rescue
• the people, said Numan Noor, a firefighter on the scene.
“We have recovered 63 bodies, including three found when we reached the basement of the building,” Karachi fire chief
• Ehtesham Salim told AFP on Wednesday.
2012 Romanian Protests
Clashes between riot police and demonstrators have also erupted in the Romanian capital Bucharest for a third day in a
• row.
• BOC urged Romanians to understand that tough austerity measures are needed to avoid a default.
• More than 1,000 protesters rallied in Bucharest’s main university square, blocking traffic.
Bucharest: a Romanian medical official says 59 people suffered injuries as days of protests against the government and
• austerity measures turned violent.
</figure>
<figureCaption confidence="0.994681">
Figure 5: AP+SALIENCE summary excerpts.
</figureCaption>
<table confidence="0.9801155">
ROUGE-1
System Recall Prec. F1
Full System 0.282 0.344 0.306
No Basic 0.263 0.380† 0.294
No LM 0.223† 0.361 0.254†
No Time 0.297† 0.367†† 0.322†
No Geo 0.232†† 0.381 0.265†
No Query 0.251 0.377 0.280
ROUGE-2
System Recall Prec. F1
Full System 0.045 0.056 0.049
No Basic 0.046 0.068†† 0.051†
No LM 0.033† 0.056 0.038†
No Time 0.052†† 0.064†† 0.056††
No Geo 0.037† 0.065 0.042
No Query 0.043 0.068† 0.048
</table>
<tableCaption confidence="0.8348045">
Table 2: Feature ablation ROUGE performance.
† indicates statistically significant difference from
</tableCaption>
<bodyText confidence="0.9790258">
full model at the α = .05 level. †† indicates sta-
tistically significant difference from full model at
the α = .01 level.
cation of the AP+SALIENCE model and features
to a wider class of events.
</bodyText>
<sectionHeader confidence="0.999131" genericHeader="acknowledgments">
8 Acknowledgments
</sectionHeader>
<bodyText confidence="0.992168666666667">
The research described here was supported in part
by the National Science Foundation (NSF) under
IIS-1422863. Any opinions, findings and conclu-
sions or recommendations expressed in this paper
are those of the authors and do not necessarily re-
flect the views of the NSF.
</bodyText>
<sectionHeader confidence="0.999343" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999692">
James Allan, Jaime G Carbonell, George Doddington,
Jonathan Yamron, and Yiming Yang. 1998. Topic
detection and tracking pilot study final report.
Javed Aslam, Matthew Ekstrand-Abueg, Virgil Pavlu,
Fernado Diaz, and Tetsuya Sakai. 2013. Trec
2013 temporal summarization. In Proceedings of
the 22nd Text Retrieval Conference (TREC), Novem-
ber.
James M Conroy, Judith D Schlesinger, PO Dianne,
Mary E Okurowski, et al. 2001. Using hmm and
logistic regression to generate extract summaries for
duc.
G¨unes Erkan and Dragomir R Radev. 2004.
Lexrank: Graph-based lexical centrality as salience
in text summarization. J. Artif. Intell. Res.(JAIR),
22(1):457–479.
Elena Filatova and Vasileios Hatzivassiloglou. 2004.
Event-based extractive summarization. In ACL
Workshop on Summarization, Barcelona, Spain.
John R Frank, Max Kleiman-Weiner, Daniel A
Roberts, Feng Niu, Ce Zhang, Christopher R´e, and
Ian Soboroff. 2012. Building an entity-centric
stream filtering test collection for trec 2012. Tech-
nical report, DTIC Document.
Brendan J Frey and Delbert Dueck. 2007. Clustering
by passing messages between data points. science,
315(5814):972–976.
David Graff and C Cieri. 2003. English gigaword cor-
pus. Linguistic Data Consortium.
</reference>
<page confidence="0.809876">
1616
</page>
<reference confidence="0.999719969387756">
Weiwei Guo and Mona Diab. 2012. A simple unsuper-
vised latent semantics based approach for sentence
similarity. In Proceedings of the First Joint Con-
ference on Lexical and Computational Semantics-
Volume 1: Proceedings of the main conference and
the shared task, and Volume 2: Proceedings of the
Sixth International Workshop on Semantic Evalua-
tion, pages 586–590. Association for Computational
Linguistics.
Qi Guo, Fernando Diaz, and Elad Yom-Tov. 2013.
Updating users about time critical events. In Pavel
Serdyukov, Pavel Braslavski, SergeiO. Kuznetsov,
Jaap Kamps, Stefan R¨uger, Eugene Agichtein, Ilya
Segalovich, and Emine Yilmaz, editors, Advances
in Information Retrieval, volume 7814 of Lec-
ture Notes in Computer Science, pages 483–494.
Springer Berlin Heidelberg.
Aria Haghighi and Lucy Vanderwende. 2009. Ex-
ploring content models for multi-document summa-
rization. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 362–370. Association for
Computational Linguistics.
Vasileios Hatzivassiloglou, Judith L Klavans,
Melissa L Holcombe, Regina Barzilay, Min-Yen
Kan, and Kathleen McKeown. 2001. Simfinder:
A flexible clustering tool for summarization. Pro-
ceedings of the NAACL Workshop on Automatic
Summarizatio.
Eduard Hovy and Chin-Yew Lin. 1998. Automated
text summarization and the summarist system. In
Proceedings of a workshop on held at Baltimore,
Maryland: October 13-15, 1998, pages 197–214.
Association for Computational Linguistics.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In Proceedings
of the 18th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, pages 68–73. ACM.
Hui Lin and Jeff Bilmes. 2011. A class of submodu-
lar functions for document summarization. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 510–520. As-
sociation for Computational Linguistics.
Chin-Yew Lin and Eduard Hovy. 2000. The automated
acquisition of topic signatures for text summariza-
tion. In Proceedings of the 18th conference on Com-
putational linguistics-Volume 1, pages 495–501. As-
sociation for Computational Linguistics.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74–81.
George A Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39–41.
Ani Nenkova and Kathleen McKeown. 2012. A survey
of text summarization techniques. In Mining Text
Data, pages 43–76. Springer.
Ani Nenkova and Lucy Vanderwende. 2005. The im-
pact of frequency on summarization. Microsoft Re-
search, Redmond, Washington, Tech. Rep. MSR-TR-
2005-101.
Leysia Palen, Kenneth M Anderson, Gloria Mark,
James Martin, Douglas Sicker, Martha Palmer, and
Dirk Grunwald. 2010. A vision for technology-
mediated support for public participation &amp; assis-
tance in mass emergencies &amp; disasters. In Proceed-
ings of the 2010 ACM-BCS visions of computer sci-
ence conference, page 8. British Computer Society.
Dragomir R Radev, Hongyan Jing, Małgorzata Sty´s,
and Daniel Tam. 2004. Centroid-based summariza-
tion of multiple documents. Information Processing
&amp; Management, 40(6):919–938.
Carl Edward Rasmussen and Christopher K. I.
Williams. 2006. Gaussian Processes for Machine
Learning. The MIT Press.
Karen Sparck-Jones. 1998. Automatic summarizing:
factors and directions. In Advances in automatic text
summarization, eds. Mani and Maybury.
Kate Starbird and Leysia Palen. 2013. Working and
sustaining the virtual disaster desk. In Proceedings
of the 2013 conference on Computer supported co-
operative work, pages 491–502. ACM.
Andreas Stolcke et al. 2002. Srilm-an extensible lan-
guage modeling toolkit. In INTERSPEECH.
Dingding Wang and Tao Li. 2010. Document up-
date summarization using incremental hierarchical
clustering. In Proceedings of the 19th ACM inter-
national conference on Information and knowledge
management, CIKM ’10, pages 279–288, New York,
NY, USA. ACM.
William Yang Wang, Kapil Thadani, and Kathleen
McKeown. 2011. Identifying event descriptions us-
ing co-training with online news summaries. In pro-
ceedings of IJCNLP, Chiang-Mai, Thailand, Nov.
</reference>
<page confidence="0.993559">
1617
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.922609">
<title confidence="0.999822">Predicting Salient Updates for Disaster Summarization</title>
<author confidence="0.995399">Kedzie McKeown Fernando Diaz</author>
<affiliation confidence="0.982375">Columbia University Microsoft Research of Computer Science</affiliation>
<abstract confidence="0.998120523809524">During crises such as natural disasters or other human tragedies, information needs of both civilians and responders often require urgent, specialized treatment. Monitoring and summarizing a text stream during such an event remains a difficult problem. We present a system for update summarization which predicts the salience of sentences with respect to an event and then uses these predictions to directly bias a clustering algorithm for sentence selection, increasing the quality of the updates. We use novel, disaster-specific features for salience prediction, including geo-locations and language models representing the language of disaster. Our evaluation on a standard set of retrospective events using ROUGE shows that salience prediction provides a significant improvement over other approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Allan</author>
<author>Jaime G Carbonell</author>
<author>George Doddington</author>
<author>Jonathan Yamron</author>
<author>Yiming Yang</author>
</authors>
<title>Topic detection and tracking pilot study final report.</title>
<date>1998</date>
<contexts>
<context position="2263" citStr="Allan et al., 1998" startWordPosition="329" endWordPosition="332">ng from on the ground information gathering to crowdsourced reporting and disaster management (Starbird and Palen, 2013). Multi-document summarization has the potential to assist the crisis informatics community. Automatic summarization could deliver relevant and salient information at regular intervals, even when human volunteers are unable to. Perhaps more importantly it could help filter out unnecessary and irrelevant detail when the volume of incoming information is large. While methods for identifying, tracking, and summarizing events from text based input have been explored extensively (Allan et al., 1998; Filatova and Hatzivassiloglou, 2004; Wang et al., 2011), these experiments were not developed to handle streaming data from a heterogeneous environment at web scale. These methods also rely heavily on redundancy which is suboptimal for time sensitive domains where there is a high cost in delaying information. In this paper, we present an update summarization system to track events across time. Our system predicts sentence salience in the context of a large-scale event, such as a disaster, and integrates these predictions into a clustering based multidocument summarization system. We demonstr</context>
</contexts>
<marker>Allan, Carbonell, Doddington, Yamron, Yang, 1998</marker>
<rawString>James Allan, Jaime G Carbonell, George Doddington, Jonathan Yamron, and Yiming Yang. 1998. Topic detection and tracking pilot study final report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Javed Aslam</author>
<author>Matthew Ekstrand-Abueg</author>
<author>Virgil Pavlu</author>
<author>Fernado Diaz</author>
<author>Tetsuya Sakai</author>
</authors>
<title>Trec</title>
<date>2013</date>
<booktitle>In Proceedings of the 22nd Text Retrieval Conference (TREC),</booktitle>
<contexts>
<context position="21254" citStr="Aslam et al., 2013" startWordPosition="3409" endWordPosition="3412">of decreasing salience and a similarity threshold is applied, where the exemplar is ignored if its maximum semantic similarity to any previous updates in the summary is greater than Asim. Exemplars that pass these thresholds are selected as updates and added to the summary. 4 Data For the document stream, we use the news portion of the 2014 TREC KBA Stream Corpus (Frank et al., 2012). The documents from this corpus come from hourly crawls of the web covering October 2011 through February 2013. Our experiments also make use of the TREC Temporal Summarization (TS) Track data from 2013 and 2014 (Aslam et al., 2013). This data includes 25 events and event metadata (e.g., a user search query for the event, the event type, and event evaluation time frame). All events occurred during the time span of the TREC KBA Stream Corpus. For each event we create a stream of relevant documents by selecting only documents that contain the complete set of query words. Along with the metadata, NIST assessors constructed a set of ground truth nuggets for each event. Nuggets are brief and important text snippets that represent sub-events that should be conveyed by an ideal update summary. In order to accomplish this, for e</context>
<context position="22595" citStr="Aslam et al., 2013" startWordPosition="3633" endWordPosition="3636"> the 1612 revision history for the Wikipedia page for ‘Hurricane Sandy’ will contain text additions including those related to individual nuggets. The assessment task involves reviewing the Wikipedia revisions in the evaluation time frame and marking the text additions capturing a new, unique nugget. More detail on this process can be found in the track description (Aslam et al., 2013). 5 Experiments We evaluate our system on two metrics: ROUGE (Lin, 2004), an automatic summarization method and an evaluation of system expected gain and comprehensiveness—metrics adapted from the TREC TS track (Aslam et al., 2013). 5.1 Training and Testing Of the 25 events in the TREC TS data, 24 are covered by the news portion of the TREC KBA Stream Corpus. From these 24, we set aside three events to use as a development set. All system salience and similarity threshold parameters are tuned on the development set to maximize ROUGE-2 F1 scores. We train a salience model for each event using 1000 sentences randomly sampled from the event’s document stream. We perform a leave-one-out evaluation of each event. At test time, we predict a sentence’s salience using the average predictions of the 23 other models. 5.2 ROUGE Ev</context>
</contexts>
<marker>Aslam, Ekstrand-Abueg, Pavlu, Diaz, Sakai, 2013</marker>
<rawString>Javed Aslam, Matthew Ekstrand-Abueg, Virgil Pavlu, Fernado Diaz, and Tetsuya Sakai. 2013. Trec 2013 temporal summarization. In Proceedings of the 22nd Text Retrieval Conference (TREC), November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James M Conroy</author>
<author>Judith D Schlesinger</author>
<author>PO Dianne</author>
<author>Mary E Okurowski</author>
</authors>
<title>Using hmm and logistic regression to generate extract summaries for duc.</title>
<date>2001</date>
<contexts>
<context position="6853" citStr="Conroy et al., 2001" startWordPosition="1032" endWordPosition="1035">iterature) (Nenkova and Vanderwende, 2005; Hovy and Lin, 1998; Lin and Hovy, 2000). The first two statistics are easily computable from the input sentences, while the third only requires an additional, generic background corpus. In classification based methods, model features are usually derived from human generated summaries, and are non-lexical in nature (e.g., sentence starting position, number of topic-signatures, number of unique words). Seminal work in this area has employed naive Bayes and logistic regression classifiers to identify sentences for summary inclusion (Kupiec et al., 1995; Conroy et al., 2001). While these methods are less dependent on redundancy, the expressiveness of their features is limited. Our model expands on these basic features to account for geographic, temporal, and language model features. The last category includes probabilistic (Haghighi and Vanderwende, 2009), information theoretic, and set cover (Lin and Bilmes, 2011) approaches. While these methods are focused on producing diverse summaries, they are difficult to adapt to the streaming setting, where we do not necessarily have a fixed summary length and the corpus to be summarized contains many irrelevant sentences</context>
<context position="11346" citStr="Conroy et al., 2001" startWordPosition="1756" endWordPosition="1759">e predictions (Section 3.3), 3. add the most novel and salient exemplars to U (Section 3.4). The resultant list of updates U is our summary of the event. 3.2 Salience Prediction 3.2.1 Features We want our model to be predictive of sentence salience across different event instances so we avoid event-specific lexical features. Instead, we extract features such as language model scores, geographic relevance, and temporal relevance from each sentence. Basic Features We employ several basic features that have been used previously in supervised models to rank sentence salience (Kupiec et al., 1995; Conroy et al., 2001). These include sentence length, the number of capitalized words normalized by sentence length, document position, and number of named entities. The data stream comprises text extracted from raw html documents; these features help to downweight sentences that are not content (e.g. web page titles, links to other content) or more heavily weight important sentences (e.g., that appear in prominent positions such as paragraph initial or article initial). Query Features Query features measure the relationship between the sentence and the event query and type. These include the number of query words</context>
</contexts>
<marker>Conroy, Schlesinger, Dianne, Okurowski, 2001</marker>
<rawString>James M Conroy, Judith D Schlesinger, PO Dianne, Mary E Okurowski, et al. 2001. Using hmm and logistic regression to generate extract summaries for duc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨unes Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>Lexrank: Graph-based lexical centrality as salience in text summarization.</title>
<date>2004</date>
<journal>J. Artif. Intell. Res.(JAIR),</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="5370" citStr="Erkan and Radev, 2004" startWordPosition="803" endWordPosition="806"> summarization literature. Section 3 outlines the details of our salience and summarization models. Next we describe our data (Section 4) and experiments (Section 5). Finally, we discuss our results (Section 6) and conclude the paper. 2 Related Work A principal concern in extractive multi-document summarization is the selection of salient sentences for inclusion in summary output (Nenkova and McKeown, 2012). Existing approaches generally fall into one of three categories, each with specific trade-offs with respect to update summarization. First, centrality-focused approaches (including graph (Erkan and Radev, 2004), cluster (Hatzivassiloglou et al., 2001), and centroid (Radev et al., 2004) methods) are very natural for retrospective analysis in the sense that they let the data “speak for itself.” These methods equate salience with centrality, either to the input or some other aggregate object (i.e. a cluster center or input centroid). However, they rely chiefly on redundancy. When applied to an unfolding event, there may not exist enough redundant content at the event onset for these methods to exploit. Once the event onset has passed, however, the redundancy reduction of these methods is quite benefici</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>G¨unes Erkan and Dragomir R Radev. 2004. Lexrank: Graph-based lexical centrality as salience in text summarization. J. Artif. Intell. Res.(JAIR), 22(1):457–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Filatova</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Event-based extractive summarization.</title>
<date>2004</date>
<booktitle>In ACL Workshop on Summarization,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="2300" citStr="Filatova and Hatzivassiloglou, 2004" startWordPosition="333" endWordPosition="336">d information gathering to crowdsourced reporting and disaster management (Starbird and Palen, 2013). Multi-document summarization has the potential to assist the crisis informatics community. Automatic summarization could deliver relevant and salient information at regular intervals, even when human volunteers are unable to. Perhaps more importantly it could help filter out unnecessary and irrelevant detail when the volume of incoming information is large. While methods for identifying, tracking, and summarizing events from text based input have been explored extensively (Allan et al., 1998; Filatova and Hatzivassiloglou, 2004; Wang et al., 2011), these experiments were not developed to handle streaming data from a heterogeneous environment at web scale. These methods also rely heavily on redundancy which is suboptimal for time sensitive domains where there is a high cost in delaying information. In this paper, we present an update summarization system to track events across time. Our system predicts sentence salience in the context of a large-scale event, such as a disaster, and integrates these predictions into a clustering based multidocument summarization system. We demonstrate that combining salience with clus</context>
</contexts>
<marker>Filatova, Hatzivassiloglou, 2004</marker>
<rawString>Elena Filatova and Vasileios Hatzivassiloglou. 2004. Event-based extractive summarization. In ACL Workshop on Summarization, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John R Frank</author>
<author>Max Kleiman-Weiner</author>
<author>Daniel A Roberts</author>
<author>Feng Niu</author>
<author>Ce Zhang</author>
<author>Christopher R´e</author>
<author>Ian Soboroff</author>
</authors>
<title>Building an entity-centric stream filtering test collection for trec</title>
<date>2012</date>
<tech>Technical report, DTIC Document.</tech>
<marker>Frank, Kleiman-Weiner, Roberts, Niu, Zhang, R´e, Soboroff, 2012</marker>
<rawString>John R Frank, Max Kleiman-Weiner, Daniel A Roberts, Feng Niu, Ce Zhang, Christopher R´e, and Ian Soboroff. 2012. Building an entity-centric stream filtering test collection for trec 2012. Technical report, DTIC Document.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan J Frey</author>
<author>Delbert Dueck</author>
</authors>
<title>Clustering by passing messages between data points.</title>
<date>2007</date>
<journal>science,</journal>
<pages>315--5814</pages>
<contexts>
<context position="18281" citStr="Frey and Dueck, 2007" startWordPosition="2919" endWordPosition="2922">ature of our model differentiates our method from most other update summarization systems. 3.3 Exemplar Selection Once we have predicted the salience for a batch of sentences, we must now select a set of update candidates, i.e. sentences that are both salient and representative of the current batch. To accomplish this, we combine the output of our salience prediction model with the affinity propagation algorithm. Affinity propagation (AP) is a clustering algorithm that identifies a subset of data points as exemplars and forms clusters by assigning the remaining points to one of the exemplars (Frey and Dueck, 2007). AP attempts to maximize the net similarity objective n n S = E sim(i, ei) + E salience(ei) i:i7�ei i:i=ei where ei is the exemplar of the i-th data point, and functions sim and salience express the pairwise similarity of data points and our predicted apriori preference of a data point to be an exemplar respectively. AP differs from other k-centers algorithms in that it simultaneously considers all data points as exemplars, making it less prone to finding local optima as a result of poor initialization. Furthermore, the second term in S incorporates the individual importance of data points as</context>
<context position="26992" citStr="Frey and Dueck, 2007" startWordPosition="4372" endWordPosition="4375">several variants and baselines intended to measure the contribution of different components. All thresholds for all runs are tuned on the development set. Affinity Propagation only (AP) The purpose of this model is to directly measure the effect of integrating salience and clustering by providing a baseline that uses the identical clustering component, but without the salience information. In this model, input sentences are apriori equally likely to be exemplars; the salience values are uniformly set as the median value of the input similarity scores, as is commonly used in the AP literature (Frey and Dueck, 2007). After clustering a sentence batch, the exemplars are examined in order of increasing time since event start and selected as updates if their maximum similarity to the previous updates is less than Asim, as in the novelty filtering stage of AP+SALIENCE. Hierarchichal Agglomerative Clustering (HAC) We provide another clustering baseline, single-linkage hierarchichal agglomerative clustering. We include this baseline to show that AP+SALIENCE is not just an improvement over AP but centrality driven methods in general. HAC was chosen over other clustering approaches because the number of clusters</context>
</contexts>
<marker>Frey, Dueck, 2007</marker>
<rawString>Brendan J Frey and Delbert Dueck. 2007. Clustering by passing messages between data points. science, 315(5814):972–976.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
<author>C Cieri</author>
</authors>
<title>English gigaword corpus. Linguistic Data Consortium.</title>
<date>2003</date>
<contexts>
<context position="12523" citStr="Graff and Cieri, 2003" startWordPosition="1943" endWordPosition="1946">type. These include the number of query words present in the sentence in addition to the number of event type synonyms, hypernyms, and hyponyms using WordNet (Miller, 1995). For example, for event type earthquake, we match sentence terms “quake”, “temblor”, “seism”, and “aftershock”. Language Model Features Language models allow us to measure the likelihood of producing a sentence from a particular source. We consider two types of language model features. The first model is estimated from a corpus of generic news articles (we used the 1995-2010 Associated Press section of the Gigaword corpus (Graff and Cieri, 2003)). This model is intended to assess the general writing quality (grammaticality, word usage) of an input sentence and helps our model to select sentences written in the newswire style. The second model is estimated from text specific to our event types. For each event type we create a corpus of related documents using pages and subcategories listed under a related 1610 Storm Earthquake Meteor Impact Accident Riot Protest Hostages Shooting Bombing Figure 2: TREC TS event types. Wikipedia category. For example, the language model for event type ‘earthquake’ is estimated from Wikipedia pages unde</context>
</contexts>
<marker>Graff, Cieri, 2003</marker>
<rawString>David Graff and C Cieri. 2003. English gigaword corpus. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Weiwei Guo</author>
<author>Mona Diab</author>
</authors>
<title>A simple unsupervised latent semantics based approach for sentence similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational SemanticsVolume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation,</booktitle>
<pages>586--590</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="10235" citStr="Guo and Diab, 2012" startWordPosition="1570" endWordPosition="1573">. Each event has anywhere from 50 to several hundred nuggets in total in our gold dataset. We describe how these nuggets are found in Section 4. Throughout our treatment of our algorithm, the salience of an update captures the degree to which it reflects an event’s unobserved nuggets. Assuming that we have a text representation for each of our nuggets, the salience of an update u with respect to a set of nuggets N is defined as, salience(u) = maxn∈N sim(u, n) (1) where sim(·) is the semantic similarity such as the cosine similarity of latent vectors associated with the update and nugget text (Guo and Diab, 2012). 3.1 Update Summarization Our system architecture follows a simple pipeline design where each stage provides an additional level of processing or filtering of the input sentences. We begin with an empty update summary U. At each hour we receive a new batch of sentences bt from the stream of event relevant documents and perform the following actions: 1. predict the salience of sentences in bt (Section 3.2), 2. select a set of exemplar sentences in bt by combining clustering with salience predictions (Section 3.3), 3. add the most novel and salient exemplars to U (Section 3.4). The resultant li</context>
</contexts>
<marker>Guo, Diab, 2012</marker>
<rawString>Weiwei Guo and Mona Diab. 2012. A simple unsupervised latent semantics based approach for sentence similarity. In Proceedings of the First Joint Conference on Lexical and Computational SemanticsVolume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, pages 586–590. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Guo</author>
<author>Fernando Diaz</author>
<author>Elad Yom-Tov</author>
</authors>
<title>Updating users about time critical events.</title>
<date>2013</date>
<booktitle>Advances in Information Retrieval,</booktitle>
<volume>7814</volume>
<pages>483--494</pages>
<editor>In Pavel Serdyukov, Pavel Braslavski, SergeiO. Kuznetsov, Jaap Kamps, Stefan R¨uger, Eugene Agichtein, Ilya Segalovich, and Emine Yilmaz, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="7648" citStr="Guo et al., 2013" startWordPosition="1150" endWordPosition="1153">oral, and language model features. The last category includes probabilistic (Haghighi and Vanderwende, 2009), information theoretic, and set cover (Lin and Bilmes, 2011) approaches. While these methods are focused on producing diverse summaries, they are difficult to adapt to the streaming setting, where we do not necessarily have a fixed summary length and the corpus to be summarized contains many irrelevant sentences, i.e. there are large portions of the corpora that we specifically want to avoid. Several researchers have recognized the importance of summarization during natural disasters. (Guo et al., 2013) developed a system for detecting novel, relevant, and comprehensive sentences immediately after a natural disaster. (Wang and Li, 2010) present a clustering-based approach to efficiently detect important updates during natural disasters. The algorithm works by hierarchically clustering sentences online, allowing the system to output a more expressive narrative structure than (Guo et al., 2013). Our system attempts to unify these system’s approaches (predictive ranking and clustering respectively). 3 Method Our update summarization system takes as input a) a short query defining the event to b</context>
</contexts>
<marker>Guo, Diaz, Yom-Tov, 2013</marker>
<rawString>Qi Guo, Fernando Diaz, and Elad Yom-Tov. 2013. Updating users about time critical events. In Pavel Serdyukov, Pavel Braslavski, SergeiO. Kuznetsov, Jaap Kamps, Stefan R¨uger, Eugene Agichtein, Ilya Segalovich, and Emine Yilmaz, editors, Advances in Information Retrieval, volume 7814 of Lecture Notes in Computer Science, pages 483–494. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Exploring content models for multi-document summarization.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>362--370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7139" citStr="Haghighi and Vanderwende, 2009" startWordPosition="1072" endWordPosition="1075">s are usually derived from human generated summaries, and are non-lexical in nature (e.g., sentence starting position, number of topic-signatures, number of unique words). Seminal work in this area has employed naive Bayes and logistic regression classifiers to identify sentences for summary inclusion (Kupiec et al., 1995; Conroy et al., 2001). While these methods are less dependent on redundancy, the expressiveness of their features is limited. Our model expands on these basic features to account for geographic, temporal, and language model features. The last category includes probabilistic (Haghighi and Vanderwende, 2009), information theoretic, and set cover (Lin and Bilmes, 2011) approaches. While these methods are focused on producing diverse summaries, they are difficult to adapt to the streaming setting, where we do not necessarily have a fixed summary length and the corpus to be summarized contains many irrelevant sentences, i.e. there are large portions of the corpora that we specifically want to avoid. Several researchers have recognized the importance of summarization during natural disasters. (Guo et al., 2013) developed a system for detecting novel, relevant, and comprehensive sentences immediately </context>
</contexts>
<marker>Haghighi, Vanderwende, 2009</marker>
<rawString>Aria Haghighi and Lucy Vanderwende. 2009. Exploring content models for multi-document summarization. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 362–370. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Judith L Klavans</author>
<author>Melissa L Holcombe</author>
<author>Regina Barzilay</author>
<author>Min-Yen Kan</author>
<author>Kathleen McKeown</author>
</authors>
<title>Simfinder: A flexible clustering tool for summarization.</title>
<date>2001</date>
<booktitle>Proceedings of the NAACL Workshop on Automatic Summarizatio.</booktitle>
<contexts>
<context position="5411" citStr="Hatzivassiloglou et al., 2001" startWordPosition="808" endWordPosition="812">n 3 outlines the details of our salience and summarization models. Next we describe our data (Section 4) and experiments (Section 5). Finally, we discuss our results (Section 6) and conclude the paper. 2 Related Work A principal concern in extractive multi-document summarization is the selection of salient sentences for inclusion in summary output (Nenkova and McKeown, 2012). Existing approaches generally fall into one of three categories, each with specific trade-offs with respect to update summarization. First, centrality-focused approaches (including graph (Erkan and Radev, 2004), cluster (Hatzivassiloglou et al., 2001), and centroid (Radev et al., 2004) methods) are very natural for retrospective analysis in the sense that they let the data “speak for itself.” These methods equate salience with centrality, either to the input or some other aggregate object (i.e. a cluster center or input centroid). However, they rely chiefly on redundancy. When applied to an unfolding event, there may not exist enough redundant content at the event onset for these methods to exploit. Once the event onset has passed, however, the redundancy reduction of these methods is quite beneficial. The second category, predictive appro</context>
</contexts>
<marker>Hatzivassiloglou, Klavans, Holcombe, Barzilay, Kan, McKeown, 2001</marker>
<rawString>Vasileios Hatzivassiloglou, Judith L Klavans, Melissa L Holcombe, Regina Barzilay, Min-Yen Kan, and Kathleen McKeown. 2001. Simfinder: A flexible clustering tool for summarization. Proceedings of the NAACL Workshop on Automatic Summarizatio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Chin-Yew Lin</author>
</authors>
<title>Automated text summarization and the summarist system.</title>
<date>1998</date>
<booktitle>In Proceedings of a workshop on held at</booktitle>
<pages>197--214</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland:</location>
<contexts>
<context position="6294" citStr="Hovy and Lin, 1998" startWordPosition="947" endWordPosition="950"> or input centroid). However, they rely chiefly on redundancy. When applied to an unfolding event, there may not exist enough redundant content at the event onset for these methods to exploit. Once the event onset has passed, however, the redundancy reduction of these methods is quite beneficial. The second category, predictive approaches, includes ranking and classification based methods. Sentences have been ranked by the average word probability, average TF*IDF score, and the number of topically related words (topic-signatures in the summarization literature) (Nenkova and Vanderwende, 2005; Hovy and Lin, 1998; Lin and Hovy, 2000). The first two statistics are easily computable from the input sentences, while the third only requires an additional, generic background corpus. In classification based methods, model features are usually derived from human generated summaries, and are non-lexical in nature (e.g., sentence starting position, number of topic-signatures, number of unique words). Seminal work in this area has employed naive Bayes and logistic regression classifiers to identify sentences for summary inclusion (Kupiec et al., 1995; Conroy et al., 2001). While these methods are less dependent </context>
</contexts>
<marker>Hovy, Lin, 1998</marker>
<rawString>Eduard Hovy and Chin-Yew Lin. 1998. Automated text summarization and the summarist system. In Proceedings of a workshop on held at Baltimore, Maryland: October 13-15, 1998, pages 197–214. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Kupiec</author>
<author>Jan Pedersen</author>
<author>Francine Chen</author>
</authors>
<title>A trainable document summarizer.</title>
<date>1995</date>
<booktitle>In Proceedings of the 18th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>68--73</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6831" citStr="Kupiec et al., 1995" startWordPosition="1028" endWordPosition="1031">n the summarization literature) (Nenkova and Vanderwende, 2005; Hovy and Lin, 1998; Lin and Hovy, 2000). The first two statistics are easily computable from the input sentences, while the third only requires an additional, generic background corpus. In classification based methods, model features are usually derived from human generated summaries, and are non-lexical in nature (e.g., sentence starting position, number of topic-signatures, number of unique words). Seminal work in this area has employed naive Bayes and logistic regression classifiers to identify sentences for summary inclusion (Kupiec et al., 1995; Conroy et al., 2001). While these methods are less dependent on redundancy, the expressiveness of their features is limited. Our model expands on these basic features to account for geographic, temporal, and language model features. The last category includes probabilistic (Haghighi and Vanderwende, 2009), information theoretic, and set cover (Lin and Bilmes, 2011) approaches. While these methods are focused on producing diverse summaries, they are difficult to adapt to the streaming setting, where we do not necessarily have a fixed summary length and the corpus to be summarized contains man</context>
<context position="11324" citStr="Kupiec et al., 1995" startWordPosition="1752" endWordPosition="1755">ustering with salience predictions (Section 3.3), 3. add the most novel and salient exemplars to U (Section 3.4). The resultant list of updates U is our summary of the event. 3.2 Salience Prediction 3.2.1 Features We want our model to be predictive of sentence salience across different event instances so we avoid event-specific lexical features. Instead, we extract features such as language model scores, geographic relevance, and temporal relevance from each sentence. Basic Features We employ several basic features that have been used previously in supervised models to rank sentence salience (Kupiec et al., 1995; Conroy et al., 2001). These include sentence length, the number of capitalized words normalized by sentence length, document position, and number of named entities. The data stream comprises text extracted from raw html documents; these features help to downweight sentences that are not content (e.g. web page titles, links to other content) or more heavily weight important sentences (e.g., that appear in prominent positions such as paragraph initial or article initial). Query Features Query features measure the relationship between the sentence and the event query and type. These include the</context>
</contexts>
<marker>Kupiec, Pedersen, Chen, 1995</marker>
<rawString>Julian Kupiec, Jan Pedersen, and Francine Chen. 1995. A trainable document summarizer. In Proceedings of the 18th annual international ACM SIGIR conference on Research and development in information retrieval, pages 68–73. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Lin</author>
<author>Jeff Bilmes</author>
</authors>
<title>A class of submodular functions for document summarization.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,</booktitle>
<pages>510--520</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7200" citStr="Lin and Bilmes, 2011" startWordPosition="1081" endWordPosition="1084">al in nature (e.g., sentence starting position, number of topic-signatures, number of unique words). Seminal work in this area has employed naive Bayes and logistic regression classifiers to identify sentences for summary inclusion (Kupiec et al., 1995; Conroy et al., 2001). While these methods are less dependent on redundancy, the expressiveness of their features is limited. Our model expands on these basic features to account for geographic, temporal, and language model features. The last category includes probabilistic (Haghighi and Vanderwende, 2009), information theoretic, and set cover (Lin and Bilmes, 2011) approaches. While these methods are focused on producing diverse summaries, they are difficult to adapt to the streaming setting, where we do not necessarily have a fixed summary length and the corpus to be summarized contains many irrelevant sentences, i.e. there are large portions of the corpora that we specifically want to avoid. Several researchers have recognized the importance of summarization during natural disasters. (Guo et al., 2013) developed a system for detecting novel, relevant, and comprehensive sentences immediately after a natural disaster. (Wang and Li, 2010) present a clust</context>
</contexts>
<marker>Lin, Bilmes, 2011</marker>
<rawString>Hui Lin and Jeff Bilmes. 2011. A class of submodular functions for document summarization. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 510–520. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>The automated acquisition of topic signatures for text summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th conference on Computational linguistics-Volume 1,</booktitle>
<pages>495--501</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6315" citStr="Lin and Hovy, 2000" startWordPosition="951" endWordPosition="954"> However, they rely chiefly on redundancy. When applied to an unfolding event, there may not exist enough redundant content at the event onset for these methods to exploit. Once the event onset has passed, however, the redundancy reduction of these methods is quite beneficial. The second category, predictive approaches, includes ranking and classification based methods. Sentences have been ranked by the average word probability, average TF*IDF score, and the number of topically related words (topic-signatures in the summarization literature) (Nenkova and Vanderwende, 2005; Hovy and Lin, 1998; Lin and Hovy, 2000). The first two statistics are easily computable from the input sentences, while the third only requires an additional, generic background corpus. In classification based methods, model features are usually derived from human generated summaries, and are non-lexical in nature (e.g., sentence starting position, number of topic-signatures, number of unique words). Seminal work in this area has employed naive Bayes and logistic regression classifiers to identify sentences for summary inclusion (Kupiec et al., 1995; Conroy et al., 2001). While these methods are less dependent on redundancy, the ex</context>
</contexts>
<marker>Lin, Hovy, 2000</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2000. The automated acquisition of topic signatures for text summarization. In Proceedings of the 18th conference on Computational linguistics-Volume 1, pages 495–501. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop,</booktitle>
<pages>74--81</pages>
<contexts>
<context position="22436" citStr="Lin, 2004" startWordPosition="3612" endWordPosition="3613">er to accomplish this, for each event, assessors were provided with the revision history of the Wikipedia page associated with the event. For example, the 1612 revision history for the Wikipedia page for ‘Hurricane Sandy’ will contain text additions including those related to individual nuggets. The assessment task involves reviewing the Wikipedia revisions in the evaluation time frame and marking the text additions capturing a new, unique nugget. More detail on this process can be found in the track description (Aslam et al., 2013). 5 Experiments We evaluate our system on two metrics: ROUGE (Lin, 2004), an automatic summarization method and an evaluation of system expected gain and comprehensiveness—metrics adapted from the TREC TS track (Aslam et al., 2013). 5.1 Training and Testing Of the 25 events in the TREC TS data, 24 are covered by the news portion of the TREC KBA Stream Corpus. From these 24, we set aside three events to use as a development set. All system salience and similarity threshold parameters are tuned on the development set to maximize ROUGE-2 F1 scores. We train a salience model for each event using 1000 sentences randomly sampled from the event’s document stream. We perf</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: a lexical database for english.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="12073" citStr="Miller, 1995" startWordPosition="1874" endWordPosition="1875"> and number of named entities. The data stream comprises text extracted from raw html documents; these features help to downweight sentences that are not content (e.g. web page titles, links to other content) or more heavily weight important sentences (e.g., that appear in prominent positions such as paragraph initial or article initial). Query Features Query features measure the relationship between the sentence and the event query and type. These include the number of query words present in the sentence in addition to the number of event type synonyms, hypernyms, and hyponyms using WordNet (Miller, 1995). For example, for event type earthquake, we match sentence terms “quake”, “temblor”, “seism”, and “aftershock”. Language Model Features Language models allow us to measure the likelihood of producing a sentence from a particular source. We consider two types of language model features. The first model is estimated from a corpus of generic news articles (we used the 1995-2010 Associated Press section of the Gigaword corpus (Graff and Cieri, 2003)). This model is intended to assess the general writing quality (grammaticality, word usage) of an input sentence and helps our model to select senten</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Kathleen McKeown</author>
</authors>
<title>A survey of text summarization techniques.</title>
<date>2012</date>
<booktitle>In Mining Text Data,</booktitle>
<pages>43--76</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="5158" citStr="Nenkova and McKeown, 2012" startWordPosition="776" endWordPosition="779">es contain more relevant information on average than the competing baselines. The remainder of the paper is organized as follows. We begin with a review of related work in the information retrieval and multi-document summarization literature. Section 3 outlines the details of our salience and summarization models. Next we describe our data (Section 4) and experiments (Section 5). Finally, we discuss our results (Section 6) and conclude the paper. 2 Related Work A principal concern in extractive multi-document summarization is the selection of salient sentences for inclusion in summary output (Nenkova and McKeown, 2012). Existing approaches generally fall into one of three categories, each with specific trade-offs with respect to update summarization. First, centrality-focused approaches (including graph (Erkan and Radev, 2004), cluster (Hatzivassiloglou et al., 2001), and centroid (Radev et al., 2004) methods) are very natural for retrospective analysis in the sense that they let the data “speak for itself.” These methods equate salience with centrality, either to the input or some other aggregate object (i.e. a cluster center or input centroid). However, they rely chiefly on redundancy. When applied to an </context>
</contexts>
<marker>Nenkova, McKeown, 2012</marker>
<rawString>Ani Nenkova and Kathleen McKeown. 2012. A survey of text summarization techniques. In Mining Text Data, pages 43–76. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Lucy Vanderwende</author>
</authors>
<title>The impact of frequency on summarization. Microsoft Research,</title>
<date>2005</date>
<pages>2005--101</pages>
<location>Redmond, Washington, Tech. Rep.</location>
<contexts>
<context position="6274" citStr="Nenkova and Vanderwende, 2005" startWordPosition="942" endWordPosition="946">e object (i.e. a cluster center or input centroid). However, they rely chiefly on redundancy. When applied to an unfolding event, there may not exist enough redundant content at the event onset for these methods to exploit. Once the event onset has passed, however, the redundancy reduction of these methods is quite beneficial. The second category, predictive approaches, includes ranking and classification based methods. Sentences have been ranked by the average word probability, average TF*IDF score, and the number of topically related words (topic-signatures in the summarization literature) (Nenkova and Vanderwende, 2005; Hovy and Lin, 1998; Lin and Hovy, 2000). The first two statistics are easily computable from the input sentences, while the third only requires an additional, generic background corpus. In classification based methods, model features are usually derived from human generated summaries, and are non-lexical in nature (e.g., sentence starting position, number of topic-signatures, number of unique words). Seminal work in this area has employed naive Bayes and logistic regression classifiers to identify sentences for summary inclusion (Kupiec et al., 1995; Conroy et al., 2001). While these methods</context>
</contexts>
<marker>Nenkova, Vanderwende, 2005</marker>
<rawString>Ani Nenkova and Lucy Vanderwende. 2005. The impact of frequency on summarization. Microsoft Research, Redmond, Washington, Tech. Rep. MSR-TR2005-101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leysia Palen</author>
<author>Kenneth M Anderson</author>
<author>Gloria Mark</author>
<author>James Martin</author>
<author>Douglas Sicker</author>
<author>Martha Palmer</author>
<author>Dirk Grunwald</author>
</authors>
<title>A vision for technologymediated support for public participation &amp; assistance in mass emergencies &amp; disasters.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 ACM-BCS visions of computer science conference,</booktitle>
<pages>8</pages>
<publisher>British Computer Society.</publisher>
<contexts>
<context position="1464" citStr="Palen et al., 2010" startWordPosition="210" endWordPosition="213">resenting the language of disaster. Our evaluation on a standard set of retrospective events using ROUGE shows that salience prediction provides a significant improvement over other approaches. 1 Introduction During crises, information is critical for first responders, crisis management organizations, and those caught in the event. When the event is significant, as in the case of Hurricane Sandy, the amount of content produced by traditional news outlets, government agencies, relief organizations, and social media can vastly overwhelm those trying to monitor the situation. Crisis informatics (Palen et al., 2010) is dedicated to finding methods for sharing the right information in a timely fashion during such an event. Research in this field has focused on human-in-the-loop approaches ranging from on the ground information gathering to crowdsourced reporting and disaster management (Starbird and Palen, 2013). Multi-document summarization has the potential to assist the crisis informatics community. Automatic summarization could deliver relevant and salient information at regular intervals, even when human volunteers are unable to. Perhaps more importantly it could help filter out unnecessary and irrel</context>
</contexts>
<marker>Palen, Anderson, Mark, Martin, Sicker, Palmer, Grunwald, 2010</marker>
<rawString>Leysia Palen, Kenneth M Anderson, Gloria Mark, James Martin, Douglas Sicker, Martha Palmer, and Dirk Grunwald. 2010. A vision for technologymediated support for public participation &amp; assistance in mass emergencies &amp; disasters. In Proceedings of the 2010 ACM-BCS visions of computer science conference, page 8. British Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Hongyan Jing</author>
<author>Małgorzata Sty´s</author>
<author>Daniel Tam</author>
</authors>
<title>Centroid-based summarization of multiple documents.</title>
<date>2004</date>
<journal>Information Processing &amp; Management,</journal>
<volume>40</volume>
<issue>6</issue>
<marker>Radev, Jing, Sty´s, Tam, 2004</marker>
<rawString>Dragomir R Radev, Hongyan Jing, Małgorzata Sty´s, and Daniel Tam. 2004. Centroid-based summarization of multiple documents. Information Processing &amp; Management, 40(6):919–938.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Edward Rasmussen</author>
<author>Christopher K I Williams</author>
</authors>
<date>2006</date>
<booktitle>Gaussian Processes for Machine Learning.</booktitle>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="17353" citStr="Rasmussen and Williams, 2006" startWordPosition="2761" endWordPosition="2764"> the difference in IDF. Large changes in IDF value indicate the sentence contains bursty terms. We also use the time (in hours) since the event started as a feature. 3.2.2 Model Given our feature representation of the input sentences, we need only target salience values for model learning. For each event in our training data, we sample a set of sentences and each sentence’s salience is computed according to Equation 1. This results in a training set of sentences, their feature representations, and their target salience values to predict. We opt to use a Gaussian process (GP) regression model (Rasmussen and Williams, 2006) with a Radial Basis Function (RBF) kernel for the salience prediction task. Our features fall naturally 1611 into five groups and we use a separate RBF kernel for each, using the sum of each feature group RBF kernel as the final input to the GP model. average but the volume of input is low. The adaptive nature of our model differentiates our method from most other update summarization systems. 3.3 Exemplar Selection Once we have predicted the salience for a batch of sentences, we must now select a set of update candidates, i.e. sentences that are both salient and representative of the current</context>
</contexts>
<marker>Rasmussen, Williams, 2006</marker>
<rawString>Carl Edward Rasmussen and Christopher K. I. Williams. 2006. Gaussian Processes for Machine Learning. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Sparck-Jones</author>
</authors>
<title>Automatic summarizing: factors and directions.</title>
<date>1998</date>
<booktitle>In Advances in automatic text summarization, eds. Mani and Maybury.</booktitle>
<contexts>
<context position="31802" citStr="Sparck-Jones, 1998" startWordPosition="5155" endWordPosition="5156">ic features leads to an increase in both unigram and bigram precision; in the bigram case this is enough to cause a statistically significant increase in F-measure over the full model. In other words, the generic features actually lead to an inferior model when we can incorporate more appropriate domain specific features. The result mirrors Sparck Jones’ claim that generic approaches to 0.12 0.10 0.08 0.06 0.04 0.02 0.00 0.5 0.6 0.7 0.8 0.9 1.0 0.5 0.6 0.7 0.8 0.9 1.0 Similarity Threshold Figure 4: Expected Gain and Comprehensiveness performance. summarization cannot produce a useful summary (Sparck-Jones, 1998). Removing the language model and geographic relevance features leads to a statistically significant drop in ROUGE-1 F1 scores. Unfortunately, this is not the case for the temporal relevance features. We surmise that these features are too strongly correlated with each other, i.e. the differences in TF*IDF between hours are definitely not i.i.d. variables. 7 Conclusion In this paper, we have presented an update summarization system for the disaster domain, and demonstrated improved system performance by integrating sentence salience with clustering. We also have shown that features specificall</context>
</contexts>
<marker>Sparck-Jones, 1998</marker>
<rawString>Karen Sparck-Jones. 1998. Automatic summarizing: factors and directions. In Advances in automatic text summarization, eds. Mani and Maybury.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kate Starbird</author>
<author>Leysia Palen</author>
</authors>
<title>Working and sustaining the virtual disaster desk.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 conference on Computer supported cooperative work,</booktitle>
<pages>491--502</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1765" citStr="Starbird and Palen, 2013" startWordPosition="256" endWordPosition="259">ations, and those caught in the event. When the event is significant, as in the case of Hurricane Sandy, the amount of content produced by traditional news outlets, government agencies, relief organizations, and social media can vastly overwhelm those trying to monitor the situation. Crisis informatics (Palen et al., 2010) is dedicated to finding methods for sharing the right information in a timely fashion during such an event. Research in this field has focused on human-in-the-loop approaches ranging from on the ground information gathering to crowdsourced reporting and disaster management (Starbird and Palen, 2013). Multi-document summarization has the potential to assist the crisis informatics community. Automatic summarization could deliver relevant and salient information at regular intervals, even when human volunteers are unable to. Perhaps more importantly it could help filter out unnecessary and irrelevant detail when the volume of incoming information is large. While methods for identifying, tracking, and summarizing events from text based input have been explored extensively (Allan et al., 1998; Filatova and Hatzivassiloglou, 2004; Wang et al., 2011), these experiments were not developed to han</context>
</contexts>
<marker>Starbird, Palen, 2013</marker>
<rawString>Kate Starbird and Leysia Palen. 2013. Working and sustaining the virtual disaster desk. In Proceedings of the 2013 conference on Computer supported cooperative work, pages 491–502. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm-an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In INTERSPEECH.</booktitle>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke et al. 2002. Srilm-an extensible language modeling toolkit. In INTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dingding Wang</author>
<author>Tao Li</author>
</authors>
<title>Document update summarization using incremental hierarchical clustering.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th ACM international conference on Information and knowledge management, CIKM ’10,</booktitle>
<pages>279--288</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="7784" citStr="Wang and Li, 2010" startWordPosition="1170" endWordPosition="1173">d set cover (Lin and Bilmes, 2011) approaches. While these methods are focused on producing diverse summaries, they are difficult to adapt to the streaming setting, where we do not necessarily have a fixed summary length and the corpus to be summarized contains many irrelevant sentences, i.e. there are large portions of the corpora that we specifically want to avoid. Several researchers have recognized the importance of summarization during natural disasters. (Guo et al., 2013) developed a system for detecting novel, relevant, and comprehensive sentences immediately after a natural disaster. (Wang and Li, 2010) present a clustering-based approach to efficiently detect important updates during natural disasters. The algorithm works by hierarchically clustering sentences online, allowing the system to output a more expressive narrative structure than (Guo et al., 2013). Our system attempts to unify these system’s approaches (predictive ranking and clustering respectively). 3 Method Our update summarization system takes as input a) a short query defining the event to be tracked (e.g. ‘Hurricane Sandy’), b) an event category defining the type of event to be tracked (e.g. ‘hurricane’), c) a stream of tim</context>
</contexts>
<marker>Wang, Li, 2010</marker>
<rawString>Dingding Wang and Tao Li. 2010. Document update summarization using incremental hierarchical clustering. In Proceedings of the 19th ACM international conference on Information and knowledge management, CIKM ’10, pages 279–288, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Yang Wang</author>
<author>Kapil Thadani</author>
<author>Kathleen McKeown</author>
</authors>
<title>Identifying event descriptions using co-training with online news summaries.</title>
<date>2011</date>
<booktitle>In proceedings of IJCNLP,</booktitle>
<location>Chiang-Mai, Thailand,</location>
<contexts>
<context position="2320" citStr="Wang et al., 2011" startWordPosition="337" endWordPosition="340">ed reporting and disaster management (Starbird and Palen, 2013). Multi-document summarization has the potential to assist the crisis informatics community. Automatic summarization could deliver relevant and salient information at regular intervals, even when human volunteers are unable to. Perhaps more importantly it could help filter out unnecessary and irrelevant detail when the volume of incoming information is large. While methods for identifying, tracking, and summarizing events from text based input have been explored extensively (Allan et al., 1998; Filatova and Hatzivassiloglou, 2004; Wang et al., 2011), these experiments were not developed to handle streaming data from a heterogeneous environment at web scale. These methods also rely heavily on redundancy which is suboptimal for time sensitive domains where there is a high cost in delaying information. In this paper, we present an update summarization system to track events across time. Our system predicts sentence salience in the context of a large-scale event, such as a disaster, and integrates these predictions into a clustering based multidocument summarization system. We demonstrate that combining salience with clustering produces more</context>
</contexts>
<marker>Wang, Thadani, McKeown, 2011</marker>
<rawString>William Yang Wang, Kapil Thadani, and Kathleen McKeown. 2011. Identifying event descriptions using co-training with online news summaries. In proceedings of IJCNLP, Chiang-Mai, Thailand, Nov.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>