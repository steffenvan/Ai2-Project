<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000936">
<title confidence="0.9344945">
Domain Adaptation in Statistical Machine Translation with Mixture
Modelling *
</title>
<author confidence="0.697705">
Jorge Civera and Alfons Juan
</author>
<affiliation confidence="0.547358">
Universidad Polit´ecnica de Valencia
</affiliation>
<address confidence="0.809271">
Camino de Vera s/n
46022 Valencia, Spain
</address>
<email confidence="0.999534">
{jorcisai,ajuan}@iti.upv.es
</email>
<sectionHeader confidence="0.998604" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999893444444445">
Mixture modelling is a standard technique
for density estimation, but its use in sta-
tistical machine translation (SMT) has just
started to be explored. One of the main
advantages of this technique is its capabil-
ity to learn specific probability distributions
that better fit subsets of the training dataset.
This feature is even more important in SMT
given the difficulties to translate polysemic
terms whose semantic depends on the con-
text in which that term appears. In this pa-
per, we describe a mixture extension of the
HMM alignment model and the derivation of
Viterbi alignments to feed a state-of-the-art
phrase-based system. Experiments carried
out on the Europarl and News Commentary
corpora show the potential interest and limi-
tations of mixture modelling.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.962904170212766">
Mixture modelling is a popular approach for density
estimation in many scientific areas (G. J. McLach-
lan and D. Peel, 2000). One of the most interest-
ing properties of mixture modelling is its capability
to model multimodal datasets by defining soft parti-
tions on these datasets, and learning specific proba-
bility distributions for each partition, that better ex-
plains the general data generation process.
Work supported by the EC (FEDER) and the Spanish
MEC under grant TIN2006-15694-CO2-01, the Conselleria
d’Empresa, Universitat i Ci`encia - Generalitat Valenciana un-
der contract GV06/252, the Universidad Polit´ecnica de Valen-
cia with ILETA project and Ministerio de Educaci´on y Ciencia.
In Machine Translation (MT), it is common to
encounter large parallel corpora devoted to hetero-
geneous topics. These topics usually define sets
of topic-specific lexicons that need to be translated
taking into the semantic context in which they are
found. This semantic dependency problem could
be overcome by learning topic-dependent translation
models that capture together the semantic context
and the translation process.
However, there have not been until very recently
that the application of mixture modelling in SMT
has received increasing attention. In (Zhao and
Xing, 2006), three fairly sophisticated bayesian top-
ical translation models, taking IBM Model 1 as a
baseline model, were presented under the bilingual
topic admixture model formalism. These models
capture latent topics at the document level in order to
reduce semantic ambiguity and improve translation
coherence. The models proposed provide in some
cases better word alignment and translation quality
than HMM and IBM models on an English-Chinese
task. In (Civera and Juan, 2006), a mixture exten-
sion of IBM model 2 along with a specific dynamic-
programming decoding algorithm were proposed.
This IBM-2 mixture model offers a significant gain
in translation quality over the conventional IBM
model 2 on a semi-synthetic task.
In this work, we present a mixture extension of the
well-known HMM alignment model first proposed
in (Vogel and others, 1996) and refined in (Och and
Ney, 2003). This model possesses appealing proper-
ties among which are worth mentioning, the simplic-
ity of the first-order word alignment distribution that
can be made independent of absolute positions while
</bodyText>
<page confidence="0.967263">
177
</page>
<bodyText confidence="0.9162453">
Proceedings of the Second Workshop on Statistical Machine Translation, pages 177–180,
Prague, June 2007. c�2007 Association for Computational Linguistics
taking advantage of the localization phenomenon
of word alignment in European languages, and the
efficient and exact computation of the E-step and
Viterbi alignment by using a dynamic-programming
approach. These properties have made this model
suitable for extensions (Toutanova et al., 2002)
and integration in a phrase-based model (Deng and
Byrne, 2005) in the past.
</bodyText>
<sectionHeader confidence="0.490725" genericHeader="method">
2 HMM alignment model
</sectionHeader>
<bodyText confidence="0.9982346">
Given a bilingual pair (x, y), where x and y are mu-
tual translation, we incorporate the hidden variable
a = a1a2 · · · a|x |to reveal, for each source word po-
sition j, the target word position aj ∈ {0, 1, ... , |y|}
to which it is connected. Thus,
</bodyText>
<equation confidence="0.998423">
p(x  |y) = X p(x, a  |y) (1)
a∈A(x,y)
</equation>
<bodyText confidence="0.99956775">
where A(x, y) denotes the set of all possible align-
ments between x and y. The alignment-completed
probability P(x, a  |y) can be decomposed in terms
of source position-dependent probabilities as:
</bodyText>
<equation confidence="0.917864833333333">
|x|
p(x, a  |y)= p(aj  |aj−1
1 , xj−1
1 , y) p(xj  |aj1, xj−1
1 , y)
j=1
</equation>
<bodyText confidence="0.987588222222222">
(2)
The original formulation of the HMM alignment
model assumes that each source word is connected
to exactly one target word. This connection depends
on the target position to which was aligned the pre-
vious source word and the length of the target sen-
tence. Here, we drop both dependencies in order to
simplify to a jump width alignment probability dis-
tribution:
</bodyText>
<equation confidence="0.997395666666667">
� p(aj) j = 1
p(aj  |aj−1
1 , xj−1
1 ,y) ≈ p(aj−aj−1) j &gt; 1 (3)
p(xj  |aj1, xj−1
1 , y) ≈ p(xj  |yaj) (4)
</equation>
<bodyText confidence="0.908135666666667">
Furthermore, the treatment of the NULL word is
the same as that presented in (Och and Ney, 2003).
Finally, the HMM alignment model is defined as:
</bodyText>
<equation confidence="0.999264">
X p(a1) |x |p(aj−aj−1) |x |p(xj|yaj)
p(x|y) = Y Y (5)
a∈A(x,y) j=2 j=1
</equation>
<sectionHeader confidence="0.723123" genericHeader="method">
3 Mixture of HMM alignment models
</sectionHeader>
<bodyText confidence="0.9942205">
Let us suppose that p(x  |y) has been generated using
a T-component mixture of HMM alignment models:
</bodyText>
<equation confidence="0.99959675">
p(x  |y) = XT p(t  |y) p(x  |y, t)
t=1
XT p(t  |y) X p(x, a  |y, t) (6)
t=1 a∈A(x,y)
</equation>
<bodyText confidence="0.999917272727273">
In Eq. 6, we introduce mixture coefficients p(t  |y)
to weight the contribution of each HMM alignment
model in the mixture. While the term p(x, a  |y, t) is
decomposed as in the original HMM model.
The assumptions of the constituent HMM mod-
els are the same than those of the previous section,
but we obtain topic-dependent statistical dictionaries
and word alignments. Apropos of the mixture coef-
ficients, we simplify these terms dropping its depen-
dency on y, leaving as future work its inclusion in
the model. Formally, the assumptions are:
</bodyText>
<equation confidence="0.999952625">
p(t  |y) ≈ p(t) (7)
�p(aj |t) j=1
p(aj  |aj−1
1 , xj−1
1 , y, t)≈ (8)
p(aj −aj−1  |t) j &gt; 1
p(xj  |aj 1, xj−1
1 , y, t) ≈ p(xj  |yaj, t) (9)
</equation>
<bodyText confidence="0.8804075">
Replacing the assumptions in Eq. 6, we obtain the
(incomplete) HMM mixture model as follows:
</bodyText>
<equation confidence="0.999386">
p(x  |y) = XT p(t) X p(a1  |t)×
t=1 a∈A(x,y)
|x ||x|
× Y p(aj −aj−1  |t) Y p(xj|yaj,t) (10)
j=2 j=1
</equation>
<bodyText confidence="0.944122">
and the set of unknown parameters comprises:
</bodyText>
<equation confidence="0.999164">
p(t) t = 1... T
p(i |t) j = 1
p(i − i0  |t) j &gt; 1
p(u|v,t) ∀u ∈ X andv ∈ Y (11)
</equation>
<bodyText confidence="0.99701525">
X and Y, being the source and target vocabular-
ies.
The estimation of the unknown parameters in
Eq. 10 is troublesome, since topic and alignment
</bodyText>
<figure confidence="0.9459315">
⎧
⎨⎪⎪
⎪⎪⎩
0=
</figure>
<page confidence="0.982681">
178
</page>
<bodyText confidence="0.999898066666667">
data are missing. Here, we revert to the EM opti-
misation algoritm to compute these parameters.
In order to do that, we define the complete version
of Eq. 10 incorporating the indicator variables zt and
za, uncovering, the until now hidden variables. The
variable zt is a T-dimensional bit vector with 1 in
the position corresponding to the component gener-
ating (x, y) and zeros elsewhere, while the variable
za = za1 ... za|. |where zaj is a |y|-dimensional bit
vector with 1 in the position corresponding to the tar-
get position to which position j is aligned and zeros
elsewhere. Then, the complete model is:
The M step finds a new estimate of ~�, by max-
imising Eq. 12, using the expected value of the miss-
ing data from Eqs. 13,14 and 15 over all sample n:
</bodyText>
<equation confidence="0.99891125">
XN
1
p(t) =
N
n=1
znt
p(i  |t) ∝ XN zna1it
n=1
p(i − i0 |t) ∝ XN X |xn |(znaj−1i0znaji)t
n=1 j=1
p(x, zt, za  |y) ≈ T p(t)zt Y |y |p(i  |t)za1izt× p(u  |v, t) ∝ XN X |xn |X |yn |znajit δ(xnj, u)δ(yni, v)
Y i=1 n=1 j=1 i=1
t=1
|y|
p(xj  |yi, t)zajizt Y
i0=1
×
Y |y|
i=1
|x|
Y
j=1
p(i − i0  |t)zaj−1i0 zajizt
(12)
</equation>
<bodyText confidence="0.990093083333333">
Given the complete model, the EM algorithm
works in two basic steps in each iteration: the
E(xpectation) step and the M(aximisation) step. At
iteration k, the E step computes the expected value
of the hidden variables given the observed data
(x, y) and the estimate of the parameters ~�(k).
The E step reduces to the computation of the ex-
pected value of zt, zajizt and zaj−1i0 zajizt for each
sample n:
where
(zaj−1i0zaji)t ∝αj−1it p(i − i0  |t) p(xj|yi, t) βjit
and the recursive functions α and β defined as:
</bodyText>
<listItem confidence="0.47290375">
p(i |t) p(xj  |yi, t) j = 1
αj−1kt p(i − k  |t) p(xj  |yi, t) j &gt; 1
1 j = |x|
p(k − i  |t) p(xj+1  |yk, t)βj+1kt j &lt; |x|
</listItem>
<subsectionHeader confidence="0.994679">
3.1 Word alignment extraction
</subsectionHeader>
<bodyText confidence="0.9999904">
The HMM mixture model described in the previous
section was used to generate Viterbi alignments on
the training dataset. These optimal alignments are
the basis for phrase-based systems.
In the original HMM model, the Viterbi align-
ment can be efficiently computed by a dynamic-
programming algorithm with a complexity O(|x |·
|y|2). In the mixture HMM model, we approximate
the Viterbi alignment by maximising over the com-
ponents of the mixture:
</bodyText>
<equation confidence="0.814182666666667">
a� ≈ arg max
a max p(t) p(x, a  |y, t)
t
</equation>
<bodyText confidence="0.999627">
So we have that the complexity of the compu-
tation of the Viterbi alignment in a T-component
HMM mixture model is O(T · |x |· |y|2).
</bodyText>
<sectionHeader confidence="0.991768" genericHeader="evaluation">
4 Experimental results
</sectionHeader>
<bodyText confidence="0.999901769230769">
The data that was employed in the experiments to
train the HMM mixture model corresponds to the
concatenation of the Spanish-English partitions of
the Europarl and the News Commentary corpora.
The idea behind this decision was to let the mixture
model distinguish which bilingual pairs should con-
tribute to learn a given HMM component in the mix-
ture. Both corpora were preprocessed as suggested
for the baseline system by tokenizing, filtering sen-
tences longer than 40 words and lowercasing.
Regarding the components of the translation sys-
tem, 5-gram language models were trained on the
monolingual version of the corpora for English(En)
</bodyText>
<equation confidence="0.9936446">
zt ∝ p(t) X |y |α|x|it (13)
i=1
zajizt = zajit zt (14)
zaj−1i0zajizt = (zaj−1i0zaji)t zt (15)
zajit ∝ |y |αjktβjkt
X
k=1
⎧
⎨
⎩
αjit =
 |y |
P
k=1
⎧
⎨
⎩
βjit =
 |y |
P
</equation>
<page confidence="0.769721">
k=1
179
</page>
<bodyText confidence="0.999739">
and Spanish(Es), while phrase-based models with
lexicalized reordering model were trained using the
Moses toolkit (P. Koehn and others, 2007), but re-
placing the Viterbi alignments, usually provided by
GIZA++ (Och and Ney, 2003), by those of the HMM
mixture model with training scheme mix 15H5.
This configuration was used to translate both test de-
velopment sets, Europarl and News Commentary.
Concerning the weights of the different models,
we tuned those weights by minimum error rate train-
ing and we employed the same weighting scheme
for all the experiments in the same language pair.
Therefore, the same weighting scheme was used
over different number of components.
BLEU scores are reported in Tables 1 and 2 as a
function of the number of components in the HMM
mixture model on the preprocessed development test
sets of the Europarl and News Commentary corpora.
</bodyText>
<tableCaption confidence="0.961206">
Table 1: BLEU scores on the Europarl development
test data
</tableCaption>
<table confidence="0.998350333333333">
T 1 2 3 4
En-Es 31.27 31.08 31.12 31.11
Es-En 31.74 31.70 31.80 31.71
</table>
<tableCaption confidence="0.873672">
Table 2: BLEU scores on the News-Commentary
development test data
</tableCaption>
<table confidence="0.920019666666667">
T 1 2 3 4
En-Es 29.62 30.01 30.17 29.95
Es-En 29.15 29.22 29.11 29.02
</table>
<bodyText confidence="0.999506666666667">
As observed in Table 1, if we compare the BLEU
scores of the conventional single-component HMM
model to those of the HMM mixture model, it seems
that there is little or no gain from incorporating
more topics into the mixture for the Europarl cor-
pus. However, in Table 2, the BLEU scores on
the English-Spanish pair significantly increase as the
number of components is incremented. We believe
that this is due to the fact that the News Commen-
tary corpus seems to have greater influence on the
mixture model than on the single-component model,
specializing Viterbi alignments to favour this corpus.
</bodyText>
<sectionHeader confidence="0.997817" genericHeader="conclusions">
5 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999744333333333">
In this work, a novel mixture version of the HMM
alignment model was introduced. This model was
employed to generate topic-dependent Viterbi align-
ments that were input into a state-of-the-art phrase-
based system. The preliminary results reported on
the English-Spanish partitions of the Europarl and
News-Commentary corpora may raise some doubts
about the applicability of mixture modelling to SMT,
nonetheless in the advent of larger open-domain cor-
pora, the idea behind topic-specific translation mod-
els seem to be more than appropriate, necessary. On
the other hand, we are fully aware that indirectly
assessing the quality of a model through a phrase-
based system is a difficult task because of the differ-
ent factors involved (Ayan and Dorr, 2006).
Finally, the main problem in mixture modelling is
the linear growth of the set of parameters as the num-
ber of components increases. In the HMM, and also
in IBM models, this problem is aggravated because
of the use of statistical dictionary entailing a large
number of parameters. A possible solution is the im-
plementation of interpolation techniques to smooth
sharp distributions estimated on few events (Och and
Ney, 2003; Zhao and Xing, 2006).
</bodyText>
<sectionHeader confidence="0.999433" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99842125">
N. F. Ayan and B. J. Dorr. 2006. Going beyond AER: an
extensive analysis of word alignments and their impact
on MT. In Proc. ofACL’06, pages 9–16.
J. Civera and A. Juan. 2006. Mixtures of IBM Model 2.
In Proc. of EAMT’06, pages 159–167.
Y. Deng and W. Byrne. 2005. HMM word and phrase
alignment for statistical machine translation. In Proc.
ofHLT-EMNLP’05, pages 169–176.
G. J. McLachlan and D. Peel. 2000. Finite Mixture Mod-
els. Wiley.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19–51.
P. Koehn and others. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Proc.
ofACL’07 Demo Session, page To be published.
K. Toutanova, H. T. Ilhan, and C. D. Manning. 2002.
Extensions to HMM-based statistical word alignment
models. In Proc. of EMNLP ’02, pages 87–94.
S. Vogel et al. 1996. HMM-based word alignment in
statistical translation. In Proc. of CL, pages 836–841.
B. Zhao and E. P. Xing. 2006. BiTAM: Bilingual Topic
AdMixture Models for Word Alignment. In Proc. of
COLING/ACL’06.
</reference>
<page confidence="0.997656">
180
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.395766">
<title confidence="0.996787">Domain Adaptation in Statistical Machine Translation with Mixture</title>
<author confidence="0.519476">Civera</author>
<affiliation confidence="0.700717">Universidad Polit´ecnica de Camino de Vera</affiliation>
<address confidence="0.980543">46022 Valencia,</address>
<abstract confidence="0.998926210526316">Mixture modelling is a standard technique for density estimation, but its use in statistical machine translation (SMT) has just started to be explored. One of the main advantages of this technique is its capability to learn specific probability distributions that better fit subsets of the training dataset. This feature is even more important in SMT given the difficulties to translate polysemic terms whose semantic depends on the context in which that term appears. In this paper, we describe a mixture extension of the HMM alignment model and the derivation of Viterbi alignments to feed a state-of-the-art phrase-based system. Experiments carried out on the Europarl and News Commentary corpora show the potential interest and limitations of mixture modelling.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N F Ayan</author>
<author>B J Dorr</author>
</authors>
<title>Going beyond AER: an extensive analysis of word alignments and their impact on MT.</title>
<date>2006</date>
<booktitle>In Proc. ofACL’06,</booktitle>
<pages>9--16</pages>
<marker>Ayan, Dorr, 2006</marker>
<rawString>N. F. Ayan and B. J. Dorr. 2006. Going beyond AER: an extensive analysis of word alignments and their impact on MT. In Proc. ofACL’06, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Civera</author>
<author>A Juan</author>
</authors>
<date>2006</date>
<journal>Mixtures of IBM Model</journal>
<booktitle>In Proc. of EAMT’06,</booktitle>
<volume>2</volume>
<pages>159--167</pages>
<contexts>
<context position="2748" citStr="Civera and Juan, 2006" startWordPosition="411" endWordPosition="414">er, there have not been until very recently that the application of mixture modelling in SMT has received increasing attention. In (Zhao and Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 as a baseline model, were presented under the bilingual topic admixture model formalism. These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence. The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an English-Chinese task. In (Civera and Juan, 2006), a mixture extension of IBM model 2 along with a specific dynamicprogramming decoding algorithm were proposed. This IBM-2 mixture model offers a significant gain in translation quality over the conventional IBM model 2 on a semi-synthetic task. In this work, we present a mixture extension of the well-known HMM alignment model first proposed in (Vogel and others, 1996) and refined in (Och and Ney, 2003). This model possesses appealing properties among which are worth mentioning, the simplicity of the first-order word alignment distribution that can be made independent of absolute positions whi</context>
</contexts>
<marker>Civera, Juan, 2006</marker>
<rawString>J. Civera and A. Juan. 2006. Mixtures of IBM Model 2. In Proc. of EAMT’06, pages 159–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Deng</author>
<author>W Byrne</author>
</authors>
<title>HMM word and phrase alignment for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. ofHLT-EMNLP’05,</booktitle>
<pages>169--176</pages>
<contexts>
<context position="3864" citStr="Deng and Byrne, 2005" startWordPosition="579" endWordPosition="582">ity of the first-order word alignment distribution that can be made independent of absolute positions while 177 Proceedings of the Second Workshop on Statistical Machine Translation, pages 177–180, Prague, June 2007. c�2007 Association for Computational Linguistics taking advantage of the localization phenomenon of word alignment in European languages, and the efficient and exact computation of the E-step and Viterbi alignment by using a dynamic-programming approach. These properties have made this model suitable for extensions (Toutanova et al., 2002) and integration in a phrase-based model (Deng and Byrne, 2005) in the past. 2 HMM alignment model Given a bilingual pair (x, y), where x and y are mutual translation, we incorporate the hidden variable a = a1a2 · · · a|x |to reveal, for each source word position j, the target word position aj ∈ {0, 1, ... , |y|} to which it is connected. Thus, p(x |y) = X p(x, a |y) (1) a∈A(x,y) where A(x, y) denotes the set of all possible alignments between x and y. The alignment-completed probability P(x, a |y) can be decomposed in terms of source position-dependent probabilities as: |x| p(x, a |y)= p(aj |aj−1 1 , xj−1 1 , y) p(xj |aj1, xj−1 1 , y) j=1 (2) The origina</context>
</contexts>
<marker>Deng, Byrne, 2005</marker>
<rawString>Y. Deng and W. Byrne. 2005. HMM word and phrase alignment for statistical machine translation. In Proc. ofHLT-EMNLP’05, pages 169–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G J McLachlan</author>
<author>D Peel</author>
</authors>
<title>Finite Mixture Models.</title>
<date>2000</date>
<publisher>Wiley.</publisher>
<marker>McLachlan, Peel, 2000</marker>
<rawString>G. J. McLachlan and D. Peel. 2000. Finite Mixture Models. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="3154" citStr="Och and Ney, 2003" startWordPosition="478" endWordPosition="481"> ambiguity and improve translation coherence. The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an English-Chinese task. In (Civera and Juan, 2006), a mixture extension of IBM model 2 along with a specific dynamicprogramming decoding algorithm were proposed. This IBM-2 mixture model offers a significant gain in translation quality over the conventional IBM model 2 on a semi-synthetic task. In this work, we present a mixture extension of the well-known HMM alignment model first proposed in (Vogel and others, 1996) and refined in (Och and Ney, 2003). This model possesses appealing properties among which are worth mentioning, the simplicity of the first-order word alignment distribution that can be made independent of absolute positions while 177 Proceedings of the Second Workshop on Statistical Machine Translation, pages 177–180, Prague, June 2007. c�2007 Association for Computational Linguistics taking advantage of the localization phenomenon of word alignment in European languages, and the efficient and exact computation of the E-step and Viterbi alignment by using a dynamic-programming approach. These properties have made this model s</context>
<context position="5012" citStr="Och and Ney, 2003" startWordPosition="805" endWordPosition="808"> p(aj |aj−1 1 , xj−1 1 , y) p(xj |aj1, xj−1 1 , y) j=1 (2) The original formulation of the HMM alignment model assumes that each source word is connected to exactly one target word. This connection depends on the target position to which was aligned the previous source word and the length of the target sentence. Here, we drop both dependencies in order to simplify to a jump width alignment probability distribution: � p(aj) j = 1 p(aj |aj−1 1 , xj−1 1 ,y) ≈ p(aj−aj−1) j &gt; 1 (3) p(xj |aj1, xj−1 1 , y) ≈ p(xj |yaj) (4) Furthermore, the treatment of the NULL word is the same as that presented in (Och and Ney, 2003). Finally, the HMM alignment model is defined as: X p(a1) |x |p(aj−aj−1) |x |p(xj|yaj) p(x|y) = Y Y (5) a∈A(x,y) j=2 j=1 3 Mixture of HMM alignment models Let us suppose that p(x |y) has been generated using a T-component mixture of HMM alignment models: p(x |y) = XT p(t |y) p(x |y, t) t=1 XT p(t |y) X p(x, a |y, t) (6) t=1 a∈A(x,y) In Eq. 6, we introduce mixture coefficients p(t |y) to weight the contribution of each HMM alignment model in the mixture. While the term p(x, a |y, t) is decomposed as in the original HMM model. The assumptions of the constituent HMM models are the same than those</context>
<context position="9894" citStr="Och and Ney, 2003" startWordPosition="1726" endWordPosition="1729">y tokenizing, filtering sentences longer than 40 words and lowercasing. Regarding the components of the translation system, 5-gram language models were trained on the monolingual version of the corpora for English(En) zt ∝ p(t) X |y |α|x|it (13) i=1 zajizt = zajit zt (14) zaj−1i0zajizt = (zaj−1i0zaji)t zt (15) zajit ∝ |y |αjktβjkt X k=1 ⎧ ⎨ ⎩ αjit = |y | P k=1 ⎧ ⎨ ⎩ βjit = |y | P k=1 179 and Spanish(Es), while phrase-based models with lexicalized reordering model were trained using the Moses toolkit (P. Koehn and others, 2007), but replacing the Viterbi alignments, usually provided by GIZA++ (Och and Ney, 2003), by those of the HMM mixture model with training scheme mix 15H5. This configuration was used to translate both test development sets, Europarl and News Commentary. Concerning the weights of the different models, we tuned those weights by minimum error rate training and we employed the same weighting scheme for all the experiments in the same language pair. Therefore, the same weighting scheme was used over different number of components. BLEU scores are reported in Tables 1 and 2 as a function of the number of components in the HMM mixture model on the preprocessed development test sets of t</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>others</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proc. ofACL’07 Demo Session,</booktitle>
<pages>page</pages>
<note>To be published.</note>
<contexts>
<context position="9808" citStr="Koehn and others, 2007" startWordPosition="1712" endWordPosition="1715">onent in the mixture. Both corpora were preprocessed as suggested for the baseline system by tokenizing, filtering sentences longer than 40 words and lowercasing. Regarding the components of the translation system, 5-gram language models were trained on the monolingual version of the corpora for English(En) zt ∝ p(t) X |y |α|x|it (13) i=1 zajizt = zajit zt (14) zaj−1i0zajizt = (zaj−1i0zaji)t zt (15) zajit ∝ |y |αjktβjkt X k=1 ⎧ ⎨ ⎩ αjit = |y | P k=1 ⎧ ⎨ ⎩ βjit = |y | P k=1 179 and Spanish(Es), while phrase-based models with lexicalized reordering model were trained using the Moses toolkit (P. Koehn and others, 2007), but replacing the Viterbi alignments, usually provided by GIZA++ (Och and Ney, 2003), by those of the HMM mixture model with training scheme mix 15H5. This configuration was used to translate both test development sets, Europarl and News Commentary. Concerning the weights of the different models, we tuned those weights by minimum error rate training and we employed the same weighting scheme for all the experiments in the same language pair. Therefore, the same weighting scheme was used over different number of components. BLEU scores are reported in Tables 1 and 2 as a function of the number</context>
</contexts>
<marker>Koehn, others, 2007</marker>
<rawString>P. Koehn and others. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proc. ofACL’07 Demo Session, page To be published.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>H T Ilhan</author>
<author>C D Manning</author>
</authors>
<title>Extensions to HMM-based statistical word alignment models.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP ’02,</booktitle>
<pages>87--94</pages>
<contexts>
<context position="3801" citStr="Toutanova et al., 2002" startWordPosition="569" endWordPosition="572">ppealing properties among which are worth mentioning, the simplicity of the first-order word alignment distribution that can be made independent of absolute positions while 177 Proceedings of the Second Workshop on Statistical Machine Translation, pages 177–180, Prague, June 2007. c�2007 Association for Computational Linguistics taking advantage of the localization phenomenon of word alignment in European languages, and the efficient and exact computation of the E-step and Viterbi alignment by using a dynamic-programming approach. These properties have made this model suitable for extensions (Toutanova et al., 2002) and integration in a phrase-based model (Deng and Byrne, 2005) in the past. 2 HMM alignment model Given a bilingual pair (x, y), where x and y are mutual translation, we incorporate the hidden variable a = a1a2 · · · a|x |to reveal, for each source word position j, the target word position aj ∈ {0, 1, ... , |y|} to which it is connected. Thus, p(x |y) = X p(x, a |y) (1) a∈A(x,y) where A(x, y) denotes the set of all possible alignments between x and y. The alignment-completed probability P(x, a |y) can be decomposed in terms of source position-dependent probabilities as: |x| p(x, a |y)= p(aj |</context>
</contexts>
<marker>Toutanova, Ilhan, Manning, 2002</marker>
<rawString>K. Toutanova, H. T. Ilhan, and C. D. Manning. 2002. Extensions to HMM-based statistical word alignment models. In Proc. of EMNLP ’02, pages 87–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Vogel</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proc. of CL,</booktitle>
<pages>836--841</pages>
<marker>Vogel, 1996</marker>
<rawString>S. Vogel et al. 1996. HMM-based word alignment in statistical translation. In Proc. of CL, pages 836–841.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Zhao</author>
<author>E P Xing</author>
</authors>
<title>BiTAM: Bilingual Topic AdMixture Models for Word Alignment.</title>
<date>2006</date>
<booktitle>In Proc. of COLING/ACL’06.</booktitle>
<contexts>
<context position="2278" citStr="Zhao and Xing, 2006" startWordPosition="340" endWordPosition="343"> Ministerio de Educaci´on y Ciencia. In Machine Translation (MT), it is common to encounter large parallel corpora devoted to heterogeneous topics. These topics usually define sets of topic-specific lexicons that need to be translated taking into the semantic context in which they are found. This semantic dependency problem could be overcome by learning topic-dependent translation models that capture together the semantic context and the translation process. However, there have not been until very recently that the application of mixture modelling in SMT has received increasing attention. In (Zhao and Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 as a baseline model, were presented under the bilingual topic admixture model formalism. These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence. The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an English-Chinese task. In (Civera and Juan, 2006), a mixture extension of IBM model 2 along with a specific dynamicprogramming decoding algorithm were proposed. This IBM-2 mixture</context>
</contexts>
<marker>Zhao, Xing, 2006</marker>
<rawString>B. Zhao and E. P. Xing. 2006. BiTAM: Bilingual Topic AdMixture Models for Word Alignment. In Proc. of COLING/ACL’06.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>