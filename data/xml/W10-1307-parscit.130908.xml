<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.046175">
<title confidence="0.9996025">
Using Reinforcement Learning to Create Communication Channel
Management Strategies for Diverse Users
</title>
<author confidence="0.983951">
Rebecca Lunsford
</author>
<affiliation confidence="0.781398">
Center for Spoken Lang. Understanding
Oregon Health &amp; Science University
</affiliation>
<address confidence="0.813376">
Beaverton, OR, USA
</address>
<email confidence="0.998948">
lunsforr@ohsu.edu
</email>
<author confidence="0.980195">
Peter Heeman
</author>
<affiliation confidence="0.779045">
Center for Spoken Lang. Understanding
Oregon Health &amp; Science University
</affiliation>
<address confidence="0.815607">
Beaverton, OR, USA
</address>
<email confidence="0.999244">
heemanp@ohsu.edu
</email>
<sectionHeader confidence="0.995649" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.959364636363637">
Spoken dialogue systems typically do not
manage the communication channel, instead
using fixed values for such features as the
amplitude and speaking rate. Yet, the qual-
ity of a dialogue can be compromised if the
user has difficulty understanding the system.
In this proof-of-concept research, we explore
using reinforcement learning (RL) to create
policies that manage the communication chan-
nel to meet the needs of diverse users. To-
wards this end, we first formalize a prelimi-
nary communication channel model, in which
users provide explicit feedback regarding is-
sues with the communication channel, and the
system implicitly alters its amplitude to ac-
commodate the user’s optimal volume. Sec-
ond, we explore whether RL is an appropri-
ate tool for creating communication channel
management strategies, comparing two differ-
ent hand-crafted policies to policies trained
using both a dialogue-length and a novel an-
noyance cost. The learned policies performed
better than hand-crafted policies, with those
trained using the annoyance cost learning an
equitable tradeoff between users with differ-
ing needs and also learning to balance finding
a user’s optimal amplitude against dialogue-
length. These results suggest that RL can be
used to create effective communication chan-
nel management policies for diverse users.
Index Terms: communication channel, spoken di-
alogue systems, reinforcement learning, amplitude,
diverse users
</bodyText>
<page confidence="0.998003">
53
</page>
<sectionHeader confidence="0.99907" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9987610625">
Both Spoken Dialog Systems (SDS) and Assistive
Technology (AT) tend to have a narrow focus, sup-
porting only a subset of the population. SDS typ-
ically aim to support the “average man”, ignoring
wide variations in potential users’ ability to hear and
understand the system. AT aims to support peo-
ple with a recognized disability, but doesn’t sup-
port those whose impairment is not severe enough
to warrant the available devices or services, or those
who are unaware or have not acknowledged that they
need assistance. However, SDS should be able to
meet the needs of users whose abilities fall within,
and between, the extremes of severly impaired and
perfectly abled.
When aiming to support users with widely differ-
ing abilities, the cause of a user’s difficulty is less
important than adapting the communication channel
in a manner that aids understanding. For example,
speech that is presented more loudly and slowly can
help a hearing-impaired elderly person understand
the system, and can also help a person with no hear-
ing loss who is driving in a noisy car. Although one
user’s difficulty is due to impairment and the other
due to an adverse environment, a similar adaptation
may be appropriate to both.
During human-human communication, speakers
manage the communication channel; implicitly al-
tering their manner of speech to increase the likeli-
hood of being understood while concurrently econo-
mizing effort (Lindblom, 1990). In addition to these
implicit actions, speakers also make statements re-
ferring to breakdowns in the communication chan-
</bodyText>
<note confidence="0.984552">
Proceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 53–61,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999703125">
nel, explicitly pointing out potential problems or
corrections, (e.g. ”Could you please speak up?”)
(Jurafsky et al., 1997).
As for human-computer dialogue, SDS are prone
to misrecognition of users’ spoken utterances. Much
research has focused on developing techniques for
overcoming or avoiding system misunderstandings.
Yet, as the quality of automatic speech recognition
improves and SDS are deployed to diverse popula-
tions and in varied environments, systems will need
to better attend to possible human misunderstand-
ings. Future SDS will need to manage the commu-
nication channel, in addition to managing the task,
to aid in avoiding these misunderstandings.
Researchers have explored the use of reinforce-
ment learning (RL) to create dialogue policies that
balance and optimize measures of task success (e.g.,
see (Scheffler and Young, 2002; Levin et al., 2000;
Henderson et al., 2008; Walker, 2000)). Along these
lines, RL is potentially well suited to creating poli-
cies for the subtask of managing the communica-
tion channel, as it can learn to adapt to the user
while continuing the dialogue. In doing so, RL may
choose actions that appear costly at the time, but lead
to better overall dialogues.
Our long term goal is to learn how to manage the
communication channel along with the task, moving
away from just “what” to say and also focusing on
“how” to say it. For this proof-of-concept, our goals
are twofold: 1) to formalize a communication chan-
nel model that encompasses diverse users, initially
focusing just on explicit user actions and implicit
system actions, and 2) to determine whether RL is
an appropriate tool for learning an effective commu-
nication channel management strategy for diverse
users. To explore the above issues, we use a simple
communication channel model in which the system
needs to determine and maintain an amplitude level
that is pleasant and effective for users with differ-
ing amplitude preferences and needs. As our goal
includes decreasing the amount of potentially an-
noying utterances (i.e., those in which the system’s
amplitude setting is in discord with the user’s op-
timal amplitude), we introduce a user-centric cost
metric, which we have termed annoyance cost. We
then compare hand-crafted policies against policies
trained using both annoyance and more traditional
dialogue-length cost components.
</bodyText>
<sectionHeader confidence="0.999788" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.99911">
2.1 How People Manage the Channel
</subsectionHeader>
<bodyText confidence="0.999925355555555">
When conversing, speakers implicitly adjust fea-
tures of their speech (e.g., speaking rate, loudness)
to maintain the communication channel. For ex-
ample, speakers produce Lombard speech when in
noisy conditions, produce clear speech to better ac-
commodate a hard of hearing listener, and alter their
speech to more closely resemble the interlocutor’s
(Junqua, 1993; Lindblom, 1990). These changes in-
crease the intelligibility of the speech, thus helping
to maintain the communication channel (Payton et
al., 1994). Research has also shown that speakers
adjust their speaking style when addressing a com-
puter; exhibiting the same speech adaptations seen
during human-human communication (Bell et al.,
2003; Lunsford et al., 2006).
In addition to altering their speech implicitly,
speakers also explicitly point out communication
channel problems (Jurafsky et al., 1997). Exam-
ples include; requesting a change in speaking rate or
amplitude (“Could you please speak up?”), explain-
ing sources of communication channel interference
(“Oh, that noise is the coffee grinder.”), or asking
their interlocutor to repeat an utterance (“What was
that?”). These explicit utterances identify some is-
sue with the communication channel that must be
remedied before continuing the dialogue. In re-
sponse, interlocutors will rewind to a previous point
in the dialogue and alter their speech to ensure they
are understood. This approach, of adapting ones
speech in response to a communication problem, oc-
curs even when conversing with a computer (Stent et
al., 2008).
Both implicit speech alterations and explicit ut-
terances regarding the communication channel of-
ten address issues of amplitude. This is to be
expected, as speaking at an appropriate amplitude
is critical to maintaining an effective communica-
tion channel, with sub-optimal amplitude affecting
listeners’ understanding and performance (Baldwin
and Struckman-Johnson, 2002). In addition, Bald-
win (2001) found that audible, but lowered, ampli-
tude can negatively affect both younger and older
subjects’ reaction time and ability to respond cor-
rectly while multitasking, and that elderly listeners
are likely to need higher amplitudes than younger
</bodyText>
<page confidence="0.996953">
54
</page>
<bodyText confidence="0.98704375">
listeners to maintain similar performance. Just as
low amplitude can be difficult to understand, high
amplitude can be annoying, and, in the extreme,
cause pain.
</bodyText>
<subsectionHeader confidence="0.99936">
2.2 How Systems Manage the Channel
</subsectionHeader>
<bodyText confidence="0.999977146341463">
Towards improving listener understanding in a po-
tentially noisy environment, Martinson and Brock
(2007) take advantage of the mobility and sensory
capabilities of a robot. To determine the best course
of action, the robot maintains a noise map of the en-
vironment, measuring the environmental noise prior
to each TTS utterance. The robot then rotates to-
ward the listener, changes location, alters its am-
plitude, or pauses until the noise abates. A similar
technique, useful for remote listeners who may be
in a noisy environment or using a noisy communica-
tion medium, could analyze the signal-to-noise ratio
to ascertain the noise level in the listener’s environ-
ment. Although these techniques may be useful for
adjusting amplitude to compensate for noise in the
listener’s environment, they do not address speech
alterations needed to accommodate users with dif-
ferent hearing abilities or preferences.
Given the need to adapt to individual users, it
seems reasonable that users themselves would sim-
ply adjust volume on their local device. However,
there are issues with this approach. First, man-
ual adjustment of the volume would prove problem-
atic when the user’s hands and eyes are busy, such
as when driving a car. Second, during an ongo-
ing dialogue speakers tend to minimize pauses, re-
sponding quickly when given the turn (Sacks et al.,
1974). Stopping to alter the amplitude could re-
sult in longer than natural pauses, which systems
often respond to with increasingly lengthy ‘time-
out’ responses (Kotelly, 2003), or repeating the same
prompt endlessly (Villing et al., 2008). Third, al-
though we focus on amplitude adaptations in this
paper, amplitude is only one aspect of the commu-
nication channel. A fully functional communication
channel management solution would also incorpo-
rate adaptations of features such as speaking rate,
pausing, pitch range, emphasis, etc. This extended
set of features, because of their number and interac-
tion between them, do not readily lend themselves
to listener manipulation.
</bodyText>
<sectionHeader confidence="0.977558" genericHeader="method">
3 Reinforcement Learning
</sectionHeader>
<bodyText confidence="0.999670282608696">
RL has been used to create dialogue strategies that
specify what action to perform in each possible
system state so that a minimum dialogue cost is
achieved (Walker, 2000; Levin et al., 2000). To ac-
complish this, RL starts with a policy, namely what
action to perform in each state. It then uses this pol-
icy, with some exploration, to estimate the cost of
getting from each state with each possible action to
the final state. As more simulations are run, RL re-
fines its estimates and its current policy. RL will
converge to an optimal solution as long as assump-
tions about costs and state transitions are met. RL is
particularly well suited for learning dialogue strate-
gies as it will balance opposing goals (e.g., minimiz-
ing excessive confirmations vs. ensuring accurate
information).
RL has been applied to a number of dialogue
scenarios. For form-filling dialogues, in which the
user provides parameters for a database query, re-
searchers have used RL to decide what order to use
when prompting for the parameters and to decrease
resource costs such as database access (Levin et al.,
2000; Scheffler and Young, 2002). System misun-
derstanding caused by speech recognition errors has
also been modeled to determine whether, and how,
the system should confirm information (Scheffler
and Young, 2002). However, there is no known work
on using RL to manage the communication channel
so as to avoid user misunderstanding.
User Simulation: To train a dialogue strategy us-
ing RL, some method must be chosen to emulate
realistic user responses to system actions. Training
with actual users is generally considered untenable
since RL can require millions of runs. As such, re-
searchers create simulated users that mimic the re-
sponses of real users. The approach employed to
create these users varies between researchers; rang-
ing from simulations that employ only real user data
(Henderson et al., 2008), to those that model users
with probabilistic simulations based on known re-
alistic user behaviors (Levin et al., 2000). Ai et
al. suggest that less realistic user simulations that al-
low RL to explore more of the dialogue state space
may perform as well or better than simulations that
statistically recreate realistic user behavior (Ai et al.,
2007). For this proof-of-concept work, we employ a
</bodyText>
<page confidence="0.99171">
55
</page>
<bodyText confidence="0.99970225">
hand-crafted user simulation that allows full explo-
ration of the state space.
Costs: Although it is agreed that RL is a viable
approach to creating optimal dialogue policies, there
remains much debate as to what cost functions result
in the most useful policies. Typically, these costs in-
clude a measure of efficiency (e.g., number of turns)
and a measure of solution quality (e.g., the user suc-
cessfully completed the transaction) (Scheffler and
Young, 2002; Levin et al., 2000). For manag-
ing the communication channel, it is unclear how
the cost function should be structured. In this work
we compare two cost components, a more traditional
dialogue-length cost versus a novel annoyance cost,
to determine which best supports the creation of use-
ful policies.
</bodyText>
<sectionHeader confidence="0.998331" genericHeader="method">
4 Communication Channel Model
</sectionHeader>
<bodyText confidence="0.999737811320755">
Based on the literature reviewed in Section 2.1, we
devised a preliminary model that captures essential
elements of how users manage the communication
channel. For now, we only include explicit user ac-
tions, in which users directly address issues with
the communication channel, as noted by Jurafsky
et al. (1997). In addition, the users modeled are
both consistent and amenable; they provide feed-
back every time the system’s utterances are too loud
or too soft, and abandon the interaction only when
the system persists in presenting utterances outside
the user’s tolerance (either ten utterances that are too
loud or ten that are too soft).
For this work, we wish to create policies that treat
all users equitably. That is, we do not want to train
polices that give preferential treatment to a subset of
users simply because they are more common. To ac-
complish this, we use a flat rather than normal distri-
bution of users within the simulation, with both the
optimal amplitude and the tolerance range randomly
generated for each user. To represent users with dif-
fering amplitude needs, simulated users are modeled
to have an optimal amplitude between 2 and 8, and
a tolerance range of 1, 3 or 5. For example, a user
may have a optimal amplitude of 4, but be able to
tolerate an amplitude between 2 and 6.
When interacting with the computer, the user re-
sponds with: (a) the answer to the system’s query if
the amplitude is within their tolerance range; (b) too
soft (TS) if below their range; or (c) too loud (TL)
if the amplitude is above their tolerance range. As
a simplifying assumption, TS and TL represent any
user responses that address communication channel
issues related to amplitude. For example, the user
response “Pardon me?” would be represented by TS
and “There’s no need to shout!” by TL. With this
user model, the user only responds to the domain
task when the system employs an amplitude setting
within the user’s tolerance range.
For the system, we need to ensure that the sys-
tem’s amplitude range can accommodate any user-
tolerable amplitude. For this reason, the system’s
amplitude can vary between 0 and 10, and is ini-
tially set to 5 prior to each dialogue. In addition to
performing domain actions, the system specifies the
amount the amplitude should change: -2, -1, +0, +1,
+2. Each system communication to the user consists
of both a domain action and the system’s amplitude
change. Thus, the system manages the communica-
tion channel using only implicit actions. If the user
responds with TS or TL, the system will then restate
what it just said, perhaps altering the amplitude prior
to re-addressing the user.
</bodyText>
<sectionHeader confidence="0.996316" genericHeader="method">
5 Hand-crafted Policies
</sectionHeader>
<bodyText confidence="0.999955714285714">
To help in determining whether RL is an appropriate
tool for learning communication channel manage-
ment strategies, we designed two hand-crafted poli-
cies for comparison. The first handcrafted policy,
termed no-complaints, finds a tolerable amplitude
as quickly as possible, then holds that amplitude for
the remainder of the dialogue. As such, this policy
only changes the amplitude in response to explicit
complaints from the user. Specifically, the policy in-
creases the amplitude by 2 after a TS response, and
drops it by 2 after a TL. If altering the amplitude by
2 would cause the system to return to a setting al-
ready identified as too soft or too loud, the system
uses an amplitude change of 1.
The second policy, termed find-optimal, searches
for the user’s optimal amplitude, then maintains that
amplitude for the remainder of the dialogue. For
this policy, the system first increases the amplitude
by 1 until the user responds with TL (potentially in
response to the system’s first utterance), then de-
creases the amplitude by 1 until the user either re-
</bodyText>
<page confidence="0.984953">
56
</page>
<bodyText confidence="0.999738166666667">
sponds with TS or the optimal amplitude is clearly
identified based on the previous feedback. An am-
plitude change of 2 is used only when both the op-
timal amplitude is obvious and a change of 2 will
bring the amplitude setting to the optimal ampli-
tude.
</bodyText>
<sectionHeader confidence="0.991291" genericHeader="method">
6 RL and System Encoding
</sectionHeader>
<bodyText confidence="0.999665559322034">
To learn communication channel management poli-
cies we use RL with system and user actions spec-
ified using Information State Update rules (Hender-
son et al., 2008). Following Heeman (2007), we en-
code commonsense preconditions rather than trying
to learn them, and only use a subset of the informa-
tion state for RL.
Domain Task: We use a domain task that requires
the user to supply 9 pieces of information, excluding
user feedback relating to the communication chan-
nel. The system has a deterministic way of selecting
its actions, thus no learning is needed for the domain
task.
State Variables: For RL, each state is represented
by two variables; AmpHistory and Progress. Am-
pHistory models the user by tracking all previ-
ous user feedback. In addition, it tracks the cur-
rent amplitude setting. The string contains one
slot for each potential amplitude setting (0 through
10), with the current setting contained within “[]”.
Thus, at the beginning of each interaction, the string
is “ [-] ”, where “-” represents no
known data. Each time the user responds, the string
is updated to reflect which amplitude settings are too
soft (“&lt;”), too loud (“&gt;”), or within the user’s toler-
ance (“O”). When the user responds with TL/TS,
the system also updates all settings above/below the
current setting. The Progress variable is required
to satisfy the Markov property needed for RL. This
variable counts the number of successful informa-
tion exchanges (i.e., the user did not respond with
TS or TL). As the domain task requires 9 pieces of
information, the Progress variable ranged from 1 to
9.
Costs: Our user model only allows up to 10 utter-
ances that are too soft or too loud. If the cutoff is
reached, the domain task has not been completed, so
a solution quality cost of 100 is incurred. Cutting
off dialogues in this way has the additional benefit
of preventing a policy from looping forever during
testing. During training, to allow the system to bet-
ter model the cost of choosing the same action re-
peatedly, we use a longer cutoff of 1000 utterances
rather than 10.
In addition to solution quality, two different cost
components are utilized. The first, a dialogue-length
cost (DC), assigns a cost of 1 for each user utterance.
The second, an annoyance cost (AC), assigns a cost
calculated as the difference between the system’s
amplitude setting and the user’s optimal amplitude.
This difference is multiplied by 3 when the sys-
tem’s amplitude setting is below the user’s optimal.
This multiplier was chosen based on research that
demonstrated increased response times and errors
during cognitively challenging tasks when speech
was presented below, rather than above, typical con-
versational levels (Baldwin and Struckman-Johnson,
2002). Thus, only utterances at the optimal ampli-
tude have no cost.
</bodyText>
<sectionHeader confidence="0.999885" genericHeader="evaluation">
7 Results
</sectionHeader>
<bodyText confidence="0.9999155625">
With the above system and user models, we trained
policies using the two cost functions discussed
above, eight with the DC component and eight us-
ing the AC component. All used Q-Learning and
the ǫ-greedy method to explore the state space with
ǫ set at 20% (Sutton and Barto, 1998). Dialogue runs
were grouped into epochs of 100; after each epoch,
the current dialogue policy was updated. We trained
each policy for 60,000 epochs. After certain epochs,
we tested the policy on 5000 user tasks.
For our simple domain, the solution quality cost
remained 0 after about the 100th epoch, as all poli-
cies learned to avoid user abandonment. Because of
this, only the dialogue-length cost(DC) and annoy-
ance cost(AC) components are reflected in the fol-
lowing analyses.
</bodyText>
<subsectionHeader confidence="0.94403">
7.1 DC-Trained Policies
</subsectionHeader>
<bodyText confidence="0.999936714285714">
By 40,000 epochs, all eight DC policies converged
to one common optimal policy. Dialogues resulting
from the DC policies average 9.76 user utterances
long. DC policies start each dialogue using the de-
fault amplitude setting of 5. After receiving the ini-
tial user response, they aggressively explore the am-
plitude range. If the initial user response is TL (or
</bodyText>
<page confidence="0.996819">
57
</page>
<table confidence="0.9995075">
DC
AmpHistory System Amp User
----- [-] Query, +0 5 TS
&lt;&lt;&lt;&lt;&lt;[&lt;] Query, +2 7 Answer
&lt;&lt;&lt;&lt;&lt;&lt;-[0] Query2 +0 7 Answer
&lt;&lt;&lt;&lt;&lt;&lt;-[0] Query3 +0 7 Answer
&lt;&lt;&lt;&lt;&lt;&lt;-[0] Query4 +0 7 Answer
&lt;&lt;&lt;&lt;&lt;&lt;-[0] Query5 +0 7 Answer
&lt;&lt;&lt;&lt;&lt;&lt;-[0] Querys +0 7 Answer
. . . . . . . . . . . .
dialogue length cost = 10
AC
AmpHistory System Amp User
----- [-] Query, +1 6 TS
&lt;&lt;&lt;&lt;&lt;&lt;[&lt;] Query, +1 7 Answer
&lt;&lt;&lt;&lt;&lt;&lt;&lt;[0] Query2 +1 8 Answer
&lt;&lt;&lt;&lt;&lt;&lt;&lt;0[0] Query3 +1 9 Answer
&lt;&lt;&lt;&lt;&lt;&lt;00[0]- Query4 +1 10 TL
&lt;&lt;&lt;&lt;&lt;&lt;&lt;000[&gt;] Query4 -2 8 Answer
&lt;&lt;&lt;&lt;&lt;&lt;&lt;0[0]0&gt; Query5 +0 8 Answer
. . . . . . . . . . . .
annoyance cost = 12
</table>
<tableCaption confidence="0.992633">
Table 1: Comparison of DC (left) and AC (right) interactions with a user who has an optimal amplitude of 8 and a
tolerance range of 3. The policies continue as shown, without changing the amplitude level, until all 9 queries are
answered.
</tableCaption>
<bodyText confidence="0.999934933333333">
TS), they continue by decreasing (or increasing) the
amplitude by -2 (or +2) until they find a tolerable
volume, in which case they stop. Table 1 illustrates
the above noted aspects of the policy. Additionally,
if the policy receives user feedback that is contrary
to the last feedback (i.e., TS after TL, or TL after
TS), the policy backtracks one amplitude setting. In
addition, if the current amplitude is near the bound-
ary (3 or 7), the policy will change the volume by
-1 or +1 as changing it by -2 or +2 would cause it
to move outside users’ amplitude range of 2-8. In
essence, the DC policies are quite straightforward;
aggressively changing the amplitude if the user com-
plains, and assuming the amplitude is correct if the
user does not complain.
</bodyText>
<subsectionHeader confidence="0.981785">
7.2 AC-Trained Policies
</subsectionHeader>
<bodyText confidence="0.9999691">
By 55,000 epochs, AC policies converged to one of
two optimal solutions, with an average annoyance
cost of 7.49. As illustrated in Table 1, the behav-
ior of the AC policies is substantially more complex
than the DC policies. First, the AC policies start
by increasing the amplitude, delivering the first ut-
terance at a setting of 6 or 7. Second, the policies
do not stop exploring after they find a tolerable set-
ting, instead attempting to bracket the user’s toler-
ance range, thus identifying the user’s optimal am-
plitude. Third, AC policies sometimes avoid lower-
ing the amplitude, even when doing so would con-
cretely identify the user’s optimal amplitude. By do-
ing so, the policies potentially incur a cost of 1 for
all following turns, but avoid incurring a one time
cost of 3 or 6. In essence, the AC policies attempt to
find the user’s optimal amplitude but may stop short
as they approach the end of the dialogue, favoring a
slightly too high amplitude over one that might be
too low.
</bodyText>
<subsectionHeader confidence="0.999755">
7.3 Comparing AC- and DC- Trained Policies
</subsectionHeader>
<bodyText confidence="0.999690074074074">
The costs for the AC and DC trained policy sets can-
not be directly compared as each set used a different
cost function. However, we can compare them using
each others’ cost function.
First, we compare the two sets of policies in terms
of average dialogue-length. For example, in Table 1,
following a DC policy results in a dialogue-length
of 10. However, for the same user, following the AC
policy results in a dialogue-length of 11, one utter-
ance longer due to the TL response to Query4.
The average dialogue-length of the DC and AC
policies, averaged across users, is shown in the right-
most two columns of Figure 1. As expected, the DC
policies perform better in terms of dialogue-length,
averaging 9.76 utterances long. However, the AC
policies average 10.32 utterances long, only 0.52 ut-
terances longer. This similarity in length is to be ex-
pected, as system communication outside the user’s
tolerance range impedes progress and is costly using
either cost component.
We also compared the AC and DC policies’ aver-
age dialogue-length for users with the same optimal
amplitude (i.e., each column shows the average cost
across users with tolerance ranges of 1, 3 and 5), as
shown in Figure 1. From this figure it is clear that
there is little difference in dialogue-length between
AC and DC policies for users with the same optimal
</bodyText>
<page confidence="0.992335">
58
</page>
<figure confidence="0.97116762962963">
amplitude. In addition, for both policies, the lengths
are similar between users with differing optimal am-
plitudes.
e
d
AC Policies
DC Policies
12
10
8
6
4
2
0
35
14
AC Policies
DC Policies
30
25
20
15
10
5
0
2 3 4 5 6 7 8 Ave
User’s Optimal Amplitude
</figure>
<figureCaption confidence="0.973788666666667">
Figure 2: Comparison of the annoyance cost between AC
and DC policies for users with differing optimal ampli-
tudes.
</figureCaption>
<figure confidence="0.997907571428572">
Average Dialogue Length b
OD
nn
oPo
55
OD OD
12
71 01
8
6
4
Average Annoyance Cost
2 3 4 5 6 7 8 Ave
User’s Optimal Amplitude
</figure>
<figureCaption confidence="0.990666666666667">
Figure 1: Comparison of the dialogue-length between AC
and DC policies for users with differing optimal ampli-
tudes.
</figureCaption>
<bodyText confidence="0.999931387096774">
Second, we compare the two sets of polices in
terms of annoyance costs. For example, in Table 1,
following the AC policy results in an annoyance cost
of 12. For the same user, following the DC policy re-
sults in an annoyance cost of 36; 9 for Queryi as it is
three below the user’s optimal amplitude, and 3 for
each of the following nine utterances as they are all
one below optimal.
As shown in the rightmost columns of Figure 2,
DC policies average annoyance cost was 13.35, a
substantial 78% increase over the average cost of
7.49 for AC policies. Figure 2 also illustrates that
the AC and DC policies perform quite differently for
users with differing optimal amplitudes. For exam-
ple, users of the DC policies whose optimal is at (5),
or slightly below (4), the system’s default setting (5)
average lower annoyance costs than those using the
AC policies. However, these lowered costs for users
in the mid-range is gained at the expense of users
whose optimal amplitude is farther afield, especially
those users requiring higher amplitude settings. This
substantial difference between users with different
optimal amplitudes is because, for DC policies, the
interaction is often conducted at the very edge of the
users’ tolerance. In contrast, the AC policies risk
more intolerable utterances, but use this information
to decrease overall costs by better meeting users’
amplitude needs. As such, users of the AC policies
can expect the majority of the task to be conducted
at, or only one setting above, their optimal ampli-
tude.
</bodyText>
<subsectionHeader confidence="0.989636">
7.4 Comparing Hand-crafted and Learned
Policies
</subsectionHeader>
<bodyText confidence="0.99997453125">
Each of the two hand-crafted policies were run with
each user simulation (i.e., optimal amplitude from
2-8 and tolerance ranges of 1, 3, or 5). In addition,
we varied the domain task size, requiring between 4
and 10 pieces of information. DC and AC policies
were also trained for these domain task sizes.
As shown in Figure 3, The no-complain policy’s
annoyance costs ranged from 7.81 for dialogues re-
quiring four pieces of information to 14.67 for those
requiring ten pieces. The cost increases linearly with
the amount of information required, because the no-
complain policy maintains the first amplitude setting
found that does not result in a user response of TS
or TL. This ensures the amplitude setting is toler-
able to the user, but may not be the user’s optimal
amplitude.
In contrast, the find-optimal policy’s annoyance
costs initially increase from 9.67 for four pieces of
information to 12.24 for seven through ten pieces.
The cost does not continue to increase when the
amount of information required is greater than seven
because, for dialogues long enough to allow the sys-
tem to concretely identify the user’s optimal ampli-
tude, the cost is zero for all subsequent utterances.
Figure 3 also includes the mean annoyance cost
for the DC and AC policies. Although one might
expect the DC trained policies to resemble the
no-complain policy, the learned policy performs
slightly better. This difference is because the DC
policies learn the range of users’ optimal amplitude
settings (2-8), and do not move the amplitude below
2 or above 8. In contrast, the no-complain policies
</bodyText>
<page confidence="0.998333">
59
</page>
<figureCaption confidence="0.988785">
Figure 3: Average user annoyance costs for hand-crafted,
DC and AC policies across dialogues requiring differing
amounts of information.
</figureCaption>
<bodyText confidence="0.9999768">
behave consistently regardless of the current setting,
and thus will incur costs for exploring settings out-
side the range of users’ optimal amplitudes. Simi-
larly, AC policies could be anticipated to closely re-
semble the find-optimal policy. However, the AC
policies average cost is lower than the costs for ei-
ther hand-crafted policy, regardless of the amount
of information required. This difference is, in part,
due to differences in behavior at the ends of the
users’ optimal amplitude range, like the DC poli-
cies. However, additional factors include the AC
policies’ more varied use of amplitude changes and
their balancing of the remaining duration of the di-
alogue against the cost to perform additional explo-
ration, as discussed in subsection 7.2.
</bodyText>
<sectionHeader confidence="0.991578" genericHeader="discussions">
8 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999991378787879">
The first objective of this work was to create a model
of the communication channel that takes into ac-
count the abilities and preferences of diverse users.
In this model, each user has an optimal amplitude,
but will answer a system query delivered within a
range around that amplitude, although they find non-
preferred, especially too soft, amplitudes annoying.
When outside the user’s tolerance, the user pro-
vides explicit feedback regarding the communica-
tion channel breakdown. For the system, the model
specifies a composite system action, pairing a do-
main action with a possible communication chan-
nel management action to change the amplitude. By
modeling explicit user actions, and implicit system
actions, this model captures some essential elements
of how people manage the communication channel.
The second objective was to determine whether
RL is appropriate for learning communication chan-
nel management. As expected, the learned policies
found and maintained a tolerable amplitude setting
and eliminated user abandonment. We also com-
pared the learned policies with handcrafted solu-
tions, and found that the learned policies performed
better. This is primarily due to RL’s ability to auto-
matically balance the opposing goals of finding the
user’s optimal amplitude and minimizing dialogue-
length.
An added benefit of RL is that it optimizes the sys-
tem’s behavior for the users on which it is trained.
In this work, we purposely used a flat distribution of
users, which caused RL to find a policy (especially
when using annoyance costs) that does not penal-
ize the outliers, which are usually those with special
needs. In fact, we could modify the user distribution,
or the simulated users’ behavior, and RL would op-
timize the system’s behavior automatically.
In this work, we contrasted dialogue length (DC)
against annoyance cost (AC) components. We found
that the AC and DC policies share the objective of
finding an amplitude setting within the user’s tol-
erance range because both incur stepwise costs for
intolerable utterances. But, AC policies further re-
fine this objective by incurring costs for tolerable,
but non-optimal, amplitudes as well. AC policies
are using information that is not explicitly commu-
nicated to the system, but which none-the-less RL
can use while learning a policy.
As this was exploratory work, the user model does
not yet fully reflect expected user behavior. For ex-
ample, as the system’s amplitude decreases, users
may misunderstand the system’s query or fail to re-
spond at all. In future work we will use an enhanced
user model that includes more natural user behavior.
In addition, because we wanted the system to focus
on learning a communication channel management
strategy, the domain task was fixed. In future work,
we will use RL to learn policies that both accom-
plish a more complex domain task, and model con-
nections between domain tasks and communication
channel management. Ultimately, we need to con-
duct user-testing to measure the efficacy of the com-
munication channel management policies. We feel
confident that learned policies trained using a com-
munication channel model which reflects the range
of users’ abilities and preferences will prove effec-
tive for supporting all users.
</bodyText>
<page confidence="0.997838">
60
</page>
<sectionHeader confidence="0.995876" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999829728395062">
Hua Ai, Joel R. Tetreault, and Diane J. Litman. 2007.
Comparing user simulation models for dialog strategy
learning. In NAACL-HLT, April.
Carryl L. Baldwin and David Struckman-Johnson. 2002.
Impact of speech presentation level on cognitive task
performance: implications for auditory display design.
Ergonomics, 45(1):62–74.
Carryl L. Baldwin. 2001. Impact of age-related hearing
impairment on cognitive task performance: evidence
for improving existing methodologies. In Human Fac-
tors and Ergonomics Society Annual Meeting; Aging,
pages 245–249.
Linda Bell, Joakim Gustafson, and Mattias Heldner.
2003. Prosodic adaptation in humancomputer inter-
action. In Proceedings of ICPhS 03, volume 1, pages
833–836.
Peter Heeman. 2007. Combining reinforcement learning
with information-state update rules. In Proceedings
of the Conference of the North American Chapter of
the Association for Computational Linguistics, pages
268–275, Rochester, NY, April.
James Henderson, Oliver Lemon, and Kallirroi Georgila.
2008. Hybrid reinforcement/supervised learning of
dialogue policies from fixed data sets. Comput. Lin-
guist., 34(4):487–511.
J. C. Junqua. 1993. The lombard reflex and its role
on human listeners and automatic speech recogniz-
ers. The Journal of the Acoustical Society of America,
93(1):510–524, January.
Dan Jurafsky, Liz Shriberg, and Debra Biasca. 1997.
Switchboard: SWBD-DAMSL Coders Manual.
Blade Kotelly. 2003. The Art and Business of Speech
Recognition. Addison-Wesley, January.
E. Levin, R. Pieraccini, and W. Eckert. 2000. A stochas-
tic model of human-machine interaction for learning
dialog strategies. IEEE Transactions on Speech and
Audio Processing, 8(1):11–23.
Bjorn Lindblom, 1990. Explaining phonetic variation: A
sketch of the H &amp; H theory, pages 403–439. Kluwer
Academic Publishers.
Rebecca Lunsford, Sharon Oviatt, and Alexander M.
Arthur. 2006. Toward open-microphone engagement
for multiparty interactions. In Proceedings of the 8th
International Conference on Multimodal Interfaces,
pages 273–280, New York, NY, USA. ACM.
Eric Martinson and Derek Brock. 2007. Improv-
ing human-robot interaction through adaptation to the
auditory scene. In HRI ’07: Proceedings of the
ACM/IEEE international conference on Human-robot
interaction, pages 113–120, New York, NY, USA.
ACM.
K. L. Payton, R. M. Uchanski, and L. D. Braida. 1994.
Intelligibility of conversational and clear speech in
noise and reverberation for listeners with normal and
impaired hearing. The Journal of the Acoustical Soci-
ety ofAmerica, 95(3):1581–1592, March.
Harvey Sacks, Emanuel A. Schlegoff, and Gail Jefferson.
1974. A simplest sytsematic for the organization of
turn-taking for conversation. Language, 50(4):696–
735, December.
K. Scheffler and S. J. Young. 2002. Automatic learning
of dialogue strategy using dialogue simulation and re-
inforcement learning. In Proceedings of Human Lan-
guage Technology, pages 12–18, San Diego CA.
A. Stent, M. Huffman, and S. Brennan. 2008. Adapt-
ing speaking after evidence of misrecognition: Local
and global hyperarticulation. Speech Communication,
50(3):163–178, March.
Richard S. Sutton and Andrew G. Barto. 1998. Rein-
forcement Learning: An Introduction.
Jessica Villing, Cecilia Holtelius, Staffan Larsson, An-
ders Lindstr¨om, Alexander Seward, and Nina Aaberg.
2008. Interruption, resumption and domain switching
in in-vehicle dialogue. In GoTAL ’08: Proceedings of
the 6th international conference on Advances in Natu-
ral Language Processing, pages 488–499, Berlin, Hei-
delberg. Springer-Verlag.
Marilyn A. Walker. 2000. An application of reinforce-
ment learning to dialogue strategy selection in a spo-
ken dialogue system for email. Journal of Aritificial
Intelligence Research, 12:387–416.
</reference>
<page confidence="0.999272">
61
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.188953">
<title confidence="0.9980385">Using Reinforcement Learning to Create Communication Management Strategies for Diverse Users</title>
<author confidence="0.949982">Rebecca</author>
<affiliation confidence="0.999063">Center for Spoken Lang. Oregon Health &amp; Science</affiliation>
<address confidence="0.556721">Beaverton, OR,</address>
<email confidence="0.999636">lunsforr@ohsu.edu</email>
<author confidence="0.948681">Peter</author>
<affiliation confidence="0.999368">Center for Spoken Lang. Oregon Health &amp; Science</affiliation>
<address confidence="0.567747">Beaverton, OR,</address>
<email confidence="0.999671">heemanp@ohsu.edu</email>
<abstract confidence="0.999526264705882">Spoken dialogue systems typically do not manage the communication channel, instead using fixed values for such features as the amplitude and speaking rate. Yet, the quality of a dialogue can be compromised if the user has difficulty understanding the system. In this proof-of-concept research, we explore using reinforcement learning (RL) to create policies that manage the communication channel to meet the needs of diverse users. Towards this end, we first formalize a preliminary communication channel model, in which users provide explicit feedback regarding issues with the communication channel, and the system implicitly alters its amplitude to accommodate the user’s optimal volume. Second, we explore whether RL is an appropriate tool for creating communication channel management strategies, comparing two different hand-crafted policies to policies trained both a dialogue-length and a novel an- The learned policies performed better than hand-crafted policies, with those trained using the annoyance cost learning an equitable tradeoff between users with differing needs and also learning to balance finding a user’s optimal amplitude against dialoguelength. These results suggest that RL can be used to create effective communication channel management policies for diverse users. communication channel, spoken dialogue systems, reinforcement learning, amplitude, diverse users</abstract>
<intro confidence="0.672824">53</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Hua Ai</author>
<author>Joel R Tetreault</author>
<author>Diane J Litman</author>
</authors>
<title>Comparing user simulation models for dialog strategy learning.</title>
<date>2007</date>
<booktitle>In NAACL-HLT,</booktitle>
<contexts>
<context position="12616" citStr="Ai et al., 2007" startWordPosition="1963" endWordPosition="1966">an require millions of runs. As such, researchers create simulated users that mimic the responses of real users. The approach employed to create these users varies between researchers; ranging from simulations that employ only real user data (Henderson et al., 2008), to those that model users with probabilistic simulations based on known realistic user behaviors (Levin et al., 2000). Ai et al. suggest that less realistic user simulations that allow RL to explore more of the dialogue state space may perform as well or better than simulations that statistically recreate realistic user behavior (Ai et al., 2007). For this proof-of-concept work, we employ a 55 hand-crafted user simulation that allows full exploration of the state space. Costs: Although it is agreed that RL is a viable approach to creating optimal dialogue policies, there remains much debate as to what cost functions result in the most useful policies. Typically, these costs include a measure of efficiency (e.g., number of turns) and a measure of solution quality (e.g., the user successfully completed the transaction) (Scheffler and Young, 2002; Levin et al., 2000). For managing the communication channel, it is unclear how the cost fun</context>
</contexts>
<marker>Ai, Tetreault, Litman, 2007</marker>
<rawString>Hua Ai, Joel R. Tetreault, and Diane J. Litman. 2007. Comparing user simulation models for dialog strategy learning. In NAACL-HLT, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carryl L Baldwin</author>
<author>David Struckman-Johnson</author>
</authors>
<title>Impact of speech presentation level on cognitive task performance: implications for auditory display design.</title>
<date>2002</date>
<journal>Ergonomics,</journal>
<volume>45</volume>
<issue>1</issue>
<contexts>
<context position="7862" citStr="Baldwin and Struckman-Johnson, 2002" startWordPosition="1195" endWordPosition="1198">nse, interlocutors will rewind to a previous point in the dialogue and alter their speech to ensure they are understood. This approach, of adapting ones speech in response to a communication problem, occurs even when conversing with a computer (Stent et al., 2008). Both implicit speech alterations and explicit utterances regarding the communication channel often address issues of amplitude. This is to be expected, as speaking at an appropriate amplitude is critical to maintaining an effective communication channel, with sub-optimal amplitude affecting listeners’ understanding and performance (Baldwin and Struckman-Johnson, 2002). In addition, Baldwin (2001) found that audible, but lowered, amplitude can negatively affect both younger and older subjects’ reaction time and ability to respond correctly while multitasking, and that elderly listeners are likely to need higher amplitudes than younger 54 listeners to maintain similar performance. Just as low amplitude can be difficult to understand, high amplitude can be annoying, and, in the extreme, cause pain. 2.2 How Systems Manage the Channel Towards improving listener understanding in a potentially noisy environment, Martinson and Brock (2007) take advantage of the mo</context>
<context position="20188" citStr="Baldwin and Struckman-Johnson, 2002" startWordPosition="3235" endWordPosition="3238">wo different cost components are utilized. The first, a dialogue-length cost (DC), assigns a cost of 1 for each user utterance. The second, an annoyance cost (AC), assigns a cost calculated as the difference between the system’s amplitude setting and the user’s optimal amplitude. This difference is multiplied by 3 when the system’s amplitude setting is below the user’s optimal. This multiplier was chosen based on research that demonstrated increased response times and errors during cognitively challenging tasks when speech was presented below, rather than above, typical conversational levels (Baldwin and Struckman-Johnson, 2002). Thus, only utterances at the optimal amplitude have no cost. 7 Results With the above system and user models, we trained policies using the two cost functions discussed above, eight with the DC component and eight using the AC component. All used Q-Learning and the ǫ-greedy method to explore the state space with ǫ set at 20% (Sutton and Barto, 1998). Dialogue runs were grouped into epochs of 100; after each epoch, the current dialogue policy was updated. We trained each policy for 60,000 epochs. After certain epochs, we tested the policy on 5000 user tasks. For our simple domain, the solutio</context>
</contexts>
<marker>Baldwin, Struckman-Johnson, 2002</marker>
<rawString>Carryl L. Baldwin and David Struckman-Johnson. 2002. Impact of speech presentation level on cognitive task performance: implications for auditory display design. Ergonomics, 45(1):62–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carryl L Baldwin</author>
</authors>
<title>Impact of age-related hearing impairment on cognitive task performance: evidence for improving existing methodologies.</title>
<date>2001</date>
<booktitle>In Human Factors and Ergonomics Society Annual Meeting; Aging,</booktitle>
<pages>245--249</pages>
<contexts>
<context position="7891" citStr="Baldwin (2001)" startWordPosition="1201" endWordPosition="1203">in the dialogue and alter their speech to ensure they are understood. This approach, of adapting ones speech in response to a communication problem, occurs even when conversing with a computer (Stent et al., 2008). Both implicit speech alterations and explicit utterances regarding the communication channel often address issues of amplitude. This is to be expected, as speaking at an appropriate amplitude is critical to maintaining an effective communication channel, with sub-optimal amplitude affecting listeners’ understanding and performance (Baldwin and Struckman-Johnson, 2002). In addition, Baldwin (2001) found that audible, but lowered, amplitude can negatively affect both younger and older subjects’ reaction time and ability to respond correctly while multitasking, and that elderly listeners are likely to need higher amplitudes than younger 54 listeners to maintain similar performance. Just as low amplitude can be difficult to understand, high amplitude can be annoying, and, in the extreme, cause pain. 2.2 How Systems Manage the Channel Towards improving listener understanding in a potentially noisy environment, Martinson and Brock (2007) take advantage of the mobility and sensory capabiliti</context>
</contexts>
<marker>Baldwin, 2001</marker>
<rawString>Carryl L. Baldwin. 2001. Impact of age-related hearing impairment on cognitive task performance: evidence for improving existing methodologies. In Human Factors and Ergonomics Society Annual Meeting; Aging, pages 245–249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linda Bell</author>
<author>Joakim Gustafson</author>
<author>Mattias Heldner</author>
</authors>
<title>Prosodic adaptation in humancomputer interaction.</title>
<date>2003</date>
<booktitle>In Proceedings of ICPhS 03,</booktitle>
<volume>1</volume>
<pages>833--836</pages>
<contexts>
<context position="6649" citStr="Bell et al., 2003" startWordPosition="1016" endWordPosition="1019">rate, loudness) to maintain the communication channel. For example, speakers produce Lombard speech when in noisy conditions, produce clear speech to better accommodate a hard of hearing listener, and alter their speech to more closely resemble the interlocutor’s (Junqua, 1993; Lindblom, 1990). These changes increase the intelligibility of the speech, thus helping to maintain the communication channel (Payton et al., 1994). Research has also shown that speakers adjust their speaking style when addressing a computer; exhibiting the same speech adaptations seen during human-human communication (Bell et al., 2003; Lunsford et al., 2006). In addition to altering their speech implicitly, speakers also explicitly point out communication channel problems (Jurafsky et al., 1997). Examples include; requesting a change in speaking rate or amplitude (“Could you please speak up?”), explaining sources of communication channel interference (“Oh, that noise is the coffee grinder.”), or asking their interlocutor to repeat an utterance (“What was that?”). These explicit utterances identify some issue with the communication channel that must be remedied before continuing the dialogue. In response, interlocutors will</context>
</contexts>
<marker>Bell, Gustafson, Heldner, 2003</marker>
<rawString>Linda Bell, Joakim Gustafson, and Mattias Heldner. 2003. Prosodic adaptation in humancomputer interaction. In Proceedings of ICPhS 03, volume 1, pages 833–836.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Heeman</author>
</authors>
<title>Combining reinforcement learning with information-state update rules.</title>
<date>2007</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>268--275</pages>
<location>Rochester, NY,</location>
<contexts>
<context position="17643" citStr="Heeman (2007)" startWordPosition="2813" endWordPosition="2814">til the user responds with TL (potentially in response to the system’s first utterance), then decreases the amplitude by 1 until the user either re56 sponds with TS or the optimal amplitude is clearly identified based on the previous feedback. An amplitude change of 2 is used only when both the optimal amplitude is obvious and a change of 2 will bring the amplitude setting to the optimal amplitude. 6 RL and System Encoding To learn communication channel management policies we use RL with system and user actions specified using Information State Update rules (Henderson et al., 2008). Following Heeman (2007), we encode commonsense preconditions rather than trying to learn them, and only use a subset of the information state for RL. Domain Task: We use a domain task that requires the user to supply 9 pieces of information, excluding user feedback relating to the communication channel. The system has a deterministic way of selecting its actions, thus no learning is needed for the domain task. State Variables: For RL, each state is represented by two variables; AmpHistory and Progress. AmpHistory models the user by tracking all previous user feedback. In addition, it tracks the current amplitude set</context>
</contexts>
<marker>Heeman, 2007</marker>
<rawString>Peter Heeman. 2007. Combining reinforcement learning with information-state update rules. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics, pages 268–275, Rochester, NY, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
<author>Oliver Lemon</author>
<author>Kallirroi Georgila</author>
</authors>
<title>Hybrid reinforcement/supervised learning of dialogue policies from fixed data sets.</title>
<date>2008</date>
<journal>Comput. Linguist.,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="4439" citStr="Henderson et al., 2008" startWordPosition="668" endWordPosition="671">es for overcoming or avoiding system misunderstandings. Yet, as the quality of automatic speech recognition improves and SDS are deployed to diverse populations and in varied environments, systems will need to better attend to possible human misunderstandings. Future SDS will need to manage the communication channel, in addition to managing the task, to aid in avoiding these misunderstandings. Researchers have explored the use of reinforcement learning (RL) to create dialogue policies that balance and optimize measures of task success (e.g., see (Scheffler and Young, 2002; Levin et al., 2000; Henderson et al., 2008; Walker, 2000)). Along these lines, RL is potentially well suited to creating policies for the subtask of managing the communication channel, as it can learn to adapt to the user while continuing the dialogue. In doing so, RL may choose actions that appear costly at the time, but lead to better overall dialogues. Our long term goal is to learn how to manage the communication channel along with the task, moving away from just “what” to say and also focusing on “how” to say it. For this proof-of-concept, our goals are twofold: 1) to formalize a communication channel model that encompasses diver</context>
<context position="12266" citStr="Henderson et al., 2008" startWordPosition="1905" endWordPosition="1908"> (Scheffler and Young, 2002). However, there is no known work on using RL to manage the communication channel so as to avoid user misunderstanding. User Simulation: To train a dialogue strategy using RL, some method must be chosen to emulate realistic user responses to system actions. Training with actual users is generally considered untenable since RL can require millions of runs. As such, researchers create simulated users that mimic the responses of real users. The approach employed to create these users varies between researchers; ranging from simulations that employ only real user data (Henderson et al., 2008), to those that model users with probabilistic simulations based on known realistic user behaviors (Levin et al., 2000). Ai et al. suggest that less realistic user simulations that allow RL to explore more of the dialogue state space may perform as well or better than simulations that statistically recreate realistic user behavior (Ai et al., 2007). For this proof-of-concept work, we employ a 55 hand-crafted user simulation that allows full exploration of the state space. Costs: Although it is agreed that RL is a viable approach to creating optimal dialogue policies, there remains much debate </context>
<context position="17618" citStr="Henderson et al., 2008" startWordPosition="2807" endWordPosition="2811">rst increases the amplitude by 1 until the user responds with TL (potentially in response to the system’s first utterance), then decreases the amplitude by 1 until the user either re56 sponds with TS or the optimal amplitude is clearly identified based on the previous feedback. An amplitude change of 2 is used only when both the optimal amplitude is obvious and a change of 2 will bring the amplitude setting to the optimal amplitude. 6 RL and System Encoding To learn communication channel management policies we use RL with system and user actions specified using Information State Update rules (Henderson et al., 2008). Following Heeman (2007), we encode commonsense preconditions rather than trying to learn them, and only use a subset of the information state for RL. Domain Task: We use a domain task that requires the user to supply 9 pieces of information, excluding user feedback relating to the communication channel. The system has a deterministic way of selecting its actions, thus no learning is needed for the domain task. State Variables: For RL, each state is represented by two variables; AmpHistory and Progress. AmpHistory models the user by tracking all previous user feedback. In addition, it tracks </context>
</contexts>
<marker>Henderson, Lemon, Georgila, 2008</marker>
<rawString>James Henderson, Oliver Lemon, and Kallirroi Georgila. 2008. Hybrid reinforcement/supervised learning of dialogue policies from fixed data sets. Comput. Linguist., 34(4):487–511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Junqua</author>
</authors>
<title>The lombard reflex and its role on human listeners and automatic speech recognizers.</title>
<date>1993</date>
<journal>The Journal of the Acoustical Society of America,</journal>
<volume>93</volume>
<issue>1</issue>
<contexts>
<context position="6309" citStr="Junqua, 1993" startWordPosition="968" endWordPosition="969">user-centric cost metric, which we have termed annoyance cost. We then compare hand-crafted policies against policies trained using both annoyance and more traditional dialogue-length cost components. 2 Related Work 2.1 How People Manage the Channel When conversing, speakers implicitly adjust features of their speech (e.g., speaking rate, loudness) to maintain the communication channel. For example, speakers produce Lombard speech when in noisy conditions, produce clear speech to better accommodate a hard of hearing listener, and alter their speech to more closely resemble the interlocutor’s (Junqua, 1993; Lindblom, 1990). These changes increase the intelligibility of the speech, thus helping to maintain the communication channel (Payton et al., 1994). Research has also shown that speakers adjust their speaking style when addressing a computer; exhibiting the same speech adaptations seen during human-human communication (Bell et al., 2003; Lunsford et al., 2006). In addition to altering their speech implicitly, speakers also explicitly point out communication channel problems (Jurafsky et al., 1997). Examples include; requesting a change in speaking rate or amplitude (“Could you please speak u</context>
</contexts>
<marker>Junqua, 1993</marker>
<rawString>J. C. Junqua. 1993. The lombard reflex and its role on human listeners and automatic speech recognizers. The Journal of the Acoustical Society of America, 93(1):510–524, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Jurafsky</author>
<author>Liz Shriberg</author>
<author>Debra Biasca</author>
</authors>
<date>1997</date>
<journal>Switchboard: SWBD-DAMSL Coders Manual.</journal>
<contexts>
<context position="3674" citStr="Jurafsky et al., 1997" startWordPosition="552" endWordPosition="555">nage the communication channel; implicitly altering their manner of speech to increase the likelihood of being understood while concurrently economizing effort (Lindblom, 1990). In addition to these implicit actions, speakers also make statements referring to breakdowns in the communication chanProceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 53–61, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics nel, explicitly pointing out potential problems or corrections, (e.g. ”Could you please speak up?”) (Jurafsky et al., 1997). As for human-computer dialogue, SDS are prone to misrecognition of users’ spoken utterances. Much research has focused on developing techniques for overcoming or avoiding system misunderstandings. Yet, as the quality of automatic speech recognition improves and SDS are deployed to diverse populations and in varied environments, systems will need to better attend to possible human misunderstandings. Future SDS will need to manage the communication channel, in addition to managing the task, to aid in avoiding these misunderstandings. Researchers have explored the use of reinforcement learning </context>
<context position="6813" citStr="Jurafsky et al., 1997" startWordPosition="1039" endWordPosition="1042">commodate a hard of hearing listener, and alter their speech to more closely resemble the interlocutor’s (Junqua, 1993; Lindblom, 1990). These changes increase the intelligibility of the speech, thus helping to maintain the communication channel (Payton et al., 1994). Research has also shown that speakers adjust their speaking style when addressing a computer; exhibiting the same speech adaptations seen during human-human communication (Bell et al., 2003; Lunsford et al., 2006). In addition to altering their speech implicitly, speakers also explicitly point out communication channel problems (Jurafsky et al., 1997). Examples include; requesting a change in speaking rate or amplitude (“Could you please speak up?”), explaining sources of communication channel interference (“Oh, that noise is the coffee grinder.”), or asking their interlocutor to repeat an utterance (“What was that?”). These explicit utterances identify some issue with the communication channel that must be remedied before continuing the dialogue. In response, interlocutors will rewind to a previous point in the dialogue and alter their speech to ensure they are understood. This approach, of adapting ones speech in response to a communicat</context>
<context position="13769" citStr="Jurafsky et al. (1997)" startWordPosition="2148" endWordPosition="2151">or managing the communication channel, it is unclear how the cost function should be structured. In this work we compare two cost components, a more traditional dialogue-length cost versus a novel annoyance cost, to determine which best supports the creation of useful policies. 4 Communication Channel Model Based on the literature reviewed in Section 2.1, we devised a preliminary model that captures essential elements of how users manage the communication channel. For now, we only include explicit user actions, in which users directly address issues with the communication channel, as noted by Jurafsky et al. (1997). In addition, the users modeled are both consistent and amenable; they provide feedback every time the system’s utterances are too loud or too soft, and abandon the interaction only when the system persists in presenting utterances outside the user’s tolerance (either ten utterances that are too loud or ten that are too soft). For this work, we wish to create policies that treat all users equitably. That is, we do not want to train polices that give preferential treatment to a subset of users simply because they are more common. To accomplish this, we use a flat rather than normal distributio</context>
</contexts>
<marker>Jurafsky, Shriberg, Biasca, 1997</marker>
<rawString>Dan Jurafsky, Liz Shriberg, and Debra Biasca. 1997. Switchboard: SWBD-DAMSL Coders Manual.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Blade Kotelly</author>
</authors>
<title>The Art and Business of Speech Recognition.</title>
<date>2003</date>
<publisher>Addison-Wesley,</publisher>
<contexts>
<context position="9849" citStr="Kotelly, 2003" startWordPosition="1515" endWordPosition="1516"> Given the need to adapt to individual users, it seems reasonable that users themselves would simply adjust volume on their local device. However, there are issues with this approach. First, manual adjustment of the volume would prove problematic when the user’s hands and eyes are busy, such as when driving a car. Second, during an ongoing dialogue speakers tend to minimize pauses, responding quickly when given the turn (Sacks et al., 1974). Stopping to alter the amplitude could result in longer than natural pauses, which systems often respond to with increasingly lengthy ‘timeout’ responses (Kotelly, 2003), or repeating the same prompt endlessly (Villing et al., 2008). Third, although we focus on amplitude adaptations in this paper, amplitude is only one aspect of the communication channel. A fully functional communication channel management solution would also incorporate adaptations of features such as speaking rate, pausing, pitch range, emphasis, etc. This extended set of features, because of their number and interaction between them, do not readily lend themselves to listener manipulation. 3 Reinforcement Learning RL has been used to create dialogue strategies that specify what action to p</context>
</contexts>
<marker>Kotelly, 2003</marker>
<rawString>Blade Kotelly. 2003. The Art and Business of Speech Recognition. Addison-Wesley, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Levin</author>
<author>R Pieraccini</author>
<author>W Eckert</author>
</authors>
<title>A stochastic model of human-machine interaction for learning dialog strategies.</title>
<date>2000</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="4415" citStr="Levin et al., 2000" startWordPosition="664" endWordPosition="667"> developing techniques for overcoming or avoiding system misunderstandings. Yet, as the quality of automatic speech recognition improves and SDS are deployed to diverse populations and in varied environments, systems will need to better attend to possible human misunderstandings. Future SDS will need to manage the communication channel, in addition to managing the task, to aid in avoiding these misunderstandings. Researchers have explored the use of reinforcement learning (RL) to create dialogue policies that balance and optimize measures of task success (e.g., see (Scheffler and Young, 2002; Levin et al., 2000; Henderson et al., 2008; Walker, 2000)). Along these lines, RL is potentially well suited to creating policies for the subtask of managing the communication channel, as it can learn to adapt to the user while continuing the dialogue. In doing so, RL may choose actions that appear costly at the time, but lead to better overall dialogues. Our long term goal is to learn how to manage the communication channel along with the task, moving away from just “what” to say and also focusing on “how” to say it. For this proof-of-concept, our goals are twofold: 1) to formalize a communication channel mode</context>
<context position="10564" citStr="Levin et al., 2000" startWordPosition="1625" endWordPosition="1628">litude adaptations in this paper, amplitude is only one aspect of the communication channel. A fully functional communication channel management solution would also incorporate adaptations of features such as speaking rate, pausing, pitch range, emphasis, etc. This extended set of features, because of their number and interaction between them, do not readily lend themselves to listener manipulation. 3 Reinforcement Learning RL has been used to create dialogue strategies that specify what action to perform in each possible system state so that a minimum dialogue cost is achieved (Walker, 2000; Levin et al., 2000). To accomplish this, RL starts with a policy, namely what action to perform in each state. It then uses this policy, with some exploration, to estimate the cost of getting from each state with each possible action to the final state. As more simulations are run, RL refines its estimates and its current policy. RL will converge to an optimal solution as long as assumptions about costs and state transitions are met. RL is particularly well suited for learning dialogue strategies as it will balance opposing goals (e.g., minimizing excessive confirmations vs. ensuring accurate information). RL ha</context>
<context position="12385" citStr="Levin et al., 2000" startWordPosition="1924" endWordPosition="1927">d user misunderstanding. User Simulation: To train a dialogue strategy using RL, some method must be chosen to emulate realistic user responses to system actions. Training with actual users is generally considered untenable since RL can require millions of runs. As such, researchers create simulated users that mimic the responses of real users. The approach employed to create these users varies between researchers; ranging from simulations that employ only real user data (Henderson et al., 2008), to those that model users with probabilistic simulations based on known realistic user behaviors (Levin et al., 2000). Ai et al. suggest that less realistic user simulations that allow RL to explore more of the dialogue state space may perform as well or better than simulations that statistically recreate realistic user behavior (Ai et al., 2007). For this proof-of-concept work, we employ a 55 hand-crafted user simulation that allows full exploration of the state space. Costs: Although it is agreed that RL is a viable approach to creating optimal dialogue policies, there remains much debate as to what cost functions result in the most useful policies. Typically, these costs include a measure of efficiency (e</context>
</contexts>
<marker>Levin, Pieraccini, Eckert, 2000</marker>
<rawString>E. Levin, R. Pieraccini, and W. Eckert. 2000. A stochastic model of human-machine interaction for learning dialog strategies. IEEE Transactions on Speech and Audio Processing, 8(1):11–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bjorn Lindblom</author>
</authors>
<title>Explaining phonetic variation:</title>
<date>1990</date>
<journal>A sketch of the H &amp; H theory,</journal>
<pages>403--439</pages>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="3228" citStr="Lindblom, 1990" startWordPosition="491" endWordPosition="492">nnel in a manner that aids understanding. For example, speech that is presented more loudly and slowly can help a hearing-impaired elderly person understand the system, and can also help a person with no hearing loss who is driving in a noisy car. Although one user’s difficulty is due to impairment and the other due to an adverse environment, a similar adaptation may be appropriate to both. During human-human communication, speakers manage the communication channel; implicitly altering their manner of speech to increase the likelihood of being understood while concurrently economizing effort (Lindblom, 1990). In addition to these implicit actions, speakers also make statements referring to breakdowns in the communication chanProceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 53–61, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics nel, explicitly pointing out potential problems or corrections, (e.g. ”Could you please speak up?”) (Jurafsky et al., 1997). As for human-computer dialogue, SDS are prone to misrecognition of users’ spoken utterances. Much research has focused on developing techniques for over</context>
<context position="6326" citStr="Lindblom, 1990" startWordPosition="970" endWordPosition="971">ost metric, which we have termed annoyance cost. We then compare hand-crafted policies against policies trained using both annoyance and more traditional dialogue-length cost components. 2 Related Work 2.1 How People Manage the Channel When conversing, speakers implicitly adjust features of their speech (e.g., speaking rate, loudness) to maintain the communication channel. For example, speakers produce Lombard speech when in noisy conditions, produce clear speech to better accommodate a hard of hearing listener, and alter their speech to more closely resemble the interlocutor’s (Junqua, 1993; Lindblom, 1990). These changes increase the intelligibility of the speech, thus helping to maintain the communication channel (Payton et al., 1994). Research has also shown that speakers adjust their speaking style when addressing a computer; exhibiting the same speech adaptations seen during human-human communication (Bell et al., 2003; Lunsford et al., 2006). In addition to altering their speech implicitly, speakers also explicitly point out communication channel problems (Jurafsky et al., 1997). Examples include; requesting a change in speaking rate or amplitude (“Could you please speak up?”), explaining </context>
</contexts>
<marker>Lindblom, 1990</marker>
<rawString>Bjorn Lindblom, 1990. Explaining phonetic variation: A sketch of the H &amp; H theory, pages 403–439. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Lunsford</author>
<author>Sharon Oviatt</author>
<author>Alexander M Arthur</author>
</authors>
<title>Toward open-microphone engagement for multiparty interactions.</title>
<date>2006</date>
<booktitle>In Proceedings of the 8th International Conference on Multimodal Interfaces,</booktitle>
<pages>273--280</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="6673" citStr="Lunsford et al., 2006" startWordPosition="1020" endWordPosition="1023">maintain the communication channel. For example, speakers produce Lombard speech when in noisy conditions, produce clear speech to better accommodate a hard of hearing listener, and alter their speech to more closely resemble the interlocutor’s (Junqua, 1993; Lindblom, 1990). These changes increase the intelligibility of the speech, thus helping to maintain the communication channel (Payton et al., 1994). Research has also shown that speakers adjust their speaking style when addressing a computer; exhibiting the same speech adaptations seen during human-human communication (Bell et al., 2003; Lunsford et al., 2006). In addition to altering their speech implicitly, speakers also explicitly point out communication channel problems (Jurafsky et al., 1997). Examples include; requesting a change in speaking rate or amplitude (“Could you please speak up?”), explaining sources of communication channel interference (“Oh, that noise is the coffee grinder.”), or asking their interlocutor to repeat an utterance (“What was that?”). These explicit utterances identify some issue with the communication channel that must be remedied before continuing the dialogue. In response, interlocutors will rewind to a previous po</context>
</contexts>
<marker>Lunsford, Oviatt, Arthur, 2006</marker>
<rawString>Rebecca Lunsford, Sharon Oviatt, and Alexander M. Arthur. 2006. Toward open-microphone engagement for multiparty interactions. In Proceedings of the 8th International Conference on Multimodal Interfaces, pages 273–280, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Martinson</author>
<author>Derek Brock</author>
</authors>
<title>Improving human-robot interaction through adaptation to the auditory scene.</title>
<date>2007</date>
<booktitle>In HRI ’07: Proceedings of the ACM/IEEE international conference on Human-robot interaction,</booktitle>
<pages>113--120</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="8437" citStr="Martinson and Brock (2007)" startWordPosition="1283" endWordPosition="1286">d performance (Baldwin and Struckman-Johnson, 2002). In addition, Baldwin (2001) found that audible, but lowered, amplitude can negatively affect both younger and older subjects’ reaction time and ability to respond correctly while multitasking, and that elderly listeners are likely to need higher amplitudes than younger 54 listeners to maintain similar performance. Just as low amplitude can be difficult to understand, high amplitude can be annoying, and, in the extreme, cause pain. 2.2 How Systems Manage the Channel Towards improving listener understanding in a potentially noisy environment, Martinson and Brock (2007) take advantage of the mobility and sensory capabilities of a robot. To determine the best course of action, the robot maintains a noise map of the environment, measuring the environmental noise prior to each TTS utterance. The robot then rotates toward the listener, changes location, alters its amplitude, or pauses until the noise abates. A similar technique, useful for remote listeners who may be in a noisy environment or using a noisy communication medium, could analyze the signal-to-noise ratio to ascertain the noise level in the listener’s environment. Although these techniques may be use</context>
</contexts>
<marker>Martinson, Brock, 2007</marker>
<rawString>Eric Martinson and Derek Brock. 2007. Improving human-robot interaction through adaptation to the auditory scene. In HRI ’07: Proceedings of the ACM/IEEE international conference on Human-robot interaction, pages 113–120, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K L Payton</author>
<author>R M Uchanski</author>
<author>L D Braida</author>
</authors>
<title>Intelligibility of conversational and clear speech in noise and reverberation for listeners with normal and impaired hearing.</title>
<date>1994</date>
<journal>The Journal of the Acoustical Society ofAmerica,</journal>
<volume>95</volume>
<issue>3</issue>
<contexts>
<context position="6458" citStr="Payton et al., 1994" startWordPosition="988" endWordPosition="991">yance and more traditional dialogue-length cost components. 2 Related Work 2.1 How People Manage the Channel When conversing, speakers implicitly adjust features of their speech (e.g., speaking rate, loudness) to maintain the communication channel. For example, speakers produce Lombard speech when in noisy conditions, produce clear speech to better accommodate a hard of hearing listener, and alter their speech to more closely resemble the interlocutor’s (Junqua, 1993; Lindblom, 1990). These changes increase the intelligibility of the speech, thus helping to maintain the communication channel (Payton et al., 1994). Research has also shown that speakers adjust their speaking style when addressing a computer; exhibiting the same speech adaptations seen during human-human communication (Bell et al., 2003; Lunsford et al., 2006). In addition to altering their speech implicitly, speakers also explicitly point out communication channel problems (Jurafsky et al., 1997). Examples include; requesting a change in speaking rate or amplitude (“Could you please speak up?”), explaining sources of communication channel interference (“Oh, that noise is the coffee grinder.”), or asking their interlocutor to repeat an u</context>
</contexts>
<marker>Payton, Uchanski, Braida, 1994</marker>
<rawString>K. L. Payton, R. M. Uchanski, and L. D. Braida. 1994. Intelligibility of conversational and clear speech in noise and reverberation for listeners with normal and impaired hearing. The Journal of the Acoustical Society ofAmerica, 95(3):1581–1592, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harvey Sacks</author>
<author>Emanuel A Schlegoff</author>
<author>Gail Jefferson</author>
</authors>
<title>A simplest sytsematic for the organization of turn-taking for conversation.</title>
<date>1974</date>
<journal>Language,</journal>
<volume>50</volume>
<issue>4</issue>
<pages>735</pages>
<contexts>
<context position="9679" citStr="Sacks et al., 1974" startWordPosition="1487" endWordPosition="1490">tude to compensate for noise in the listener’s environment, they do not address speech alterations needed to accommodate users with different hearing abilities or preferences. Given the need to adapt to individual users, it seems reasonable that users themselves would simply adjust volume on their local device. However, there are issues with this approach. First, manual adjustment of the volume would prove problematic when the user’s hands and eyes are busy, such as when driving a car. Second, during an ongoing dialogue speakers tend to minimize pauses, responding quickly when given the turn (Sacks et al., 1974). Stopping to alter the amplitude could result in longer than natural pauses, which systems often respond to with increasingly lengthy ‘timeout’ responses (Kotelly, 2003), or repeating the same prompt endlessly (Villing et al., 2008). Third, although we focus on amplitude adaptations in this paper, amplitude is only one aspect of the communication channel. A fully functional communication channel management solution would also incorporate adaptations of features such as speaking rate, pausing, pitch range, emphasis, etc. This extended set of features, because of their number and interaction be</context>
</contexts>
<marker>Sacks, Schlegoff, Jefferson, 1974</marker>
<rawString>Harvey Sacks, Emanuel A. Schlegoff, and Gail Jefferson. 1974. A simplest sytsematic for the organization of turn-taking for conversation. Language, 50(4):696– 735, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Scheffler</author>
<author>S J Young</author>
</authors>
<title>Automatic learning of dialogue strategy using dialogue simulation and reinforcement learning.</title>
<date>2002</date>
<booktitle>In Proceedings of Human Language Technology,</booktitle>
<pages>12--18</pages>
<location>San Diego CA.</location>
<contexts>
<context position="4395" citStr="Scheffler and Young, 2002" startWordPosition="660" endWordPosition="663">uch research has focused on developing techniques for overcoming or avoiding system misunderstandings. Yet, as the quality of automatic speech recognition improves and SDS are deployed to diverse populations and in varied environments, systems will need to better attend to possible human misunderstandings. Future SDS will need to manage the communication channel, in addition to managing the task, to aid in avoiding these misunderstandings. Researchers have explored the use of reinforcement learning (RL) to create dialogue policies that balance and optimize measures of task success (e.g., see (Scheffler and Young, 2002; Levin et al., 2000; Henderson et al., 2008; Walker, 2000)). Along these lines, RL is potentially well suited to creating policies for the subtask of managing the communication channel, as it can learn to adapt to the user while continuing the dialogue. In doing so, RL may choose actions that appear costly at the time, but lead to better overall dialogues. Our long term goal is to learn how to manage the communication channel along with the task, moving away from just “what” to say and also focusing on “how” to say it. For this proof-of-concept, our goals are twofold: 1) to formalize a commun</context>
<context position="11491" citStr="Scheffler and Young, 2002" startWordPosition="1782" endWordPosition="1785">ent policy. RL will converge to an optimal solution as long as assumptions about costs and state transitions are met. RL is particularly well suited for learning dialogue strategies as it will balance opposing goals (e.g., minimizing excessive confirmations vs. ensuring accurate information). RL has been applied to a number of dialogue scenarios. For form-filling dialogues, in which the user provides parameters for a database query, researchers have used RL to decide what order to use when prompting for the parameters and to decrease resource costs such as database access (Levin et al., 2000; Scheffler and Young, 2002). System misunderstanding caused by speech recognition errors has also been modeled to determine whether, and how, the system should confirm information (Scheffler and Young, 2002). However, there is no known work on using RL to manage the communication channel so as to avoid user misunderstanding. User Simulation: To train a dialogue strategy using RL, some method must be chosen to emulate realistic user responses to system actions. Training with actual users is generally considered untenable since RL can require millions of runs. As such, researchers create simulated users that mimic the res</context>
<context position="13123" citStr="Scheffler and Young, 2002" startWordPosition="2045" endWordPosition="2048">e may perform as well or better than simulations that statistically recreate realistic user behavior (Ai et al., 2007). For this proof-of-concept work, we employ a 55 hand-crafted user simulation that allows full exploration of the state space. Costs: Although it is agreed that RL is a viable approach to creating optimal dialogue policies, there remains much debate as to what cost functions result in the most useful policies. Typically, these costs include a measure of efficiency (e.g., number of turns) and a measure of solution quality (e.g., the user successfully completed the transaction) (Scheffler and Young, 2002; Levin et al., 2000). For managing the communication channel, it is unclear how the cost function should be structured. In this work we compare two cost components, a more traditional dialogue-length cost versus a novel annoyance cost, to determine which best supports the creation of useful policies. 4 Communication Channel Model Based on the literature reviewed in Section 2.1, we devised a preliminary model that captures essential elements of how users manage the communication channel. For now, we only include explicit user actions, in which users directly address issues with the communicati</context>
</contexts>
<marker>Scheffler, Young, 2002</marker>
<rawString>K. Scheffler and S. J. Young. 2002. Automatic learning of dialogue strategy using dialogue simulation and reinforcement learning. In Proceedings of Human Language Technology, pages 12–18, San Diego CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stent</author>
<author>M Huffman</author>
<author>S Brennan</author>
</authors>
<title>Adapting speaking after evidence of misrecognition: Local and global hyperarticulation.</title>
<date>2008</date>
<journal>Speech Communication,</journal>
<volume>50</volume>
<issue>3</issue>
<contexts>
<context position="7490" citStr="Stent et al., 2008" startWordPosition="1145" endWordPosition="1148">r amplitude (“Could you please speak up?”), explaining sources of communication channel interference (“Oh, that noise is the coffee grinder.”), or asking their interlocutor to repeat an utterance (“What was that?”). These explicit utterances identify some issue with the communication channel that must be remedied before continuing the dialogue. In response, interlocutors will rewind to a previous point in the dialogue and alter their speech to ensure they are understood. This approach, of adapting ones speech in response to a communication problem, occurs even when conversing with a computer (Stent et al., 2008). Both implicit speech alterations and explicit utterances regarding the communication channel often address issues of amplitude. This is to be expected, as speaking at an appropriate amplitude is critical to maintaining an effective communication channel, with sub-optimal amplitude affecting listeners’ understanding and performance (Baldwin and Struckman-Johnson, 2002). In addition, Baldwin (2001) found that audible, but lowered, amplitude can negatively affect both younger and older subjects’ reaction time and ability to respond correctly while multitasking, and that elderly listeners are li</context>
</contexts>
<marker>Stent, Huffman, Brennan, 2008</marker>
<rawString>A. Stent, M. Huffman, and S. Brennan. 2008. Adapting speaking after evidence of misrecognition: Local and global hyperarticulation. Speech Communication, 50(3):163–178, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard S Sutton</author>
<author>Andrew G Barto</author>
</authors>
<title>Reinforcement Learning: An Introduction.</title>
<date>1998</date>
<contexts>
<context position="20541" citStr="Sutton and Barto, 1998" startWordPosition="3298" endWordPosition="3301">ser’s optimal. This multiplier was chosen based on research that demonstrated increased response times and errors during cognitively challenging tasks when speech was presented below, rather than above, typical conversational levels (Baldwin and Struckman-Johnson, 2002). Thus, only utterances at the optimal amplitude have no cost. 7 Results With the above system and user models, we trained policies using the two cost functions discussed above, eight with the DC component and eight using the AC component. All used Q-Learning and the ǫ-greedy method to explore the state space with ǫ set at 20% (Sutton and Barto, 1998). Dialogue runs were grouped into epochs of 100; after each epoch, the current dialogue policy was updated. We trained each policy for 60,000 epochs. After certain epochs, we tested the policy on 5000 user tasks. For our simple domain, the solution quality cost remained 0 after about the 100th epoch, as all policies learned to avoid user abandonment. Because of this, only the dialogue-length cost(DC) and annoyance cost(AC) components are reflected in the following analyses. 7.1 DC-Trained Policies By 40,000 epochs, all eight DC policies converged to one common optimal policy. Dialogues resulti</context>
</contexts>
<marker>Sutton, Barto, 1998</marker>
<rawString>Richard S. Sutton and Andrew G. Barto. 1998. Reinforcement Learning: An Introduction.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jessica Villing</author>
<author>Cecilia Holtelius</author>
<author>Staffan Larsson</author>
<author>Anders Lindstr¨om</author>
<author>Alexander Seward</author>
<author>Nina Aaberg</author>
</authors>
<title>Interruption, resumption and domain switching in in-vehicle dialogue.</title>
<date>2008</date>
<booktitle>In GoTAL ’08: Proceedings of the 6th international conference on Advances in Natural Language Processing,</booktitle>
<pages>488--499</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<marker>Villing, Holtelius, Larsson, Lindstr¨om, Seward, Aaberg, 2008</marker>
<rawString>Jessica Villing, Cecilia Holtelius, Staffan Larsson, Anders Lindstr¨om, Alexander Seward, and Nina Aaberg. 2008. Interruption, resumption and domain switching in in-vehicle dialogue. In GoTAL ’08: Proceedings of the 6th international conference on Advances in Natural Language Processing, pages 488–499, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
</authors>
<title>An application of reinforcement learning to dialogue strategy selection in a spoken dialogue system for email.</title>
<date>2000</date>
<journal>Journal of Aritificial Intelligence Research,</journal>
<pages>12--387</pages>
<contexts>
<context position="4454" citStr="Walker, 2000" startWordPosition="672" endWordPosition="673">iding system misunderstandings. Yet, as the quality of automatic speech recognition improves and SDS are deployed to diverse populations and in varied environments, systems will need to better attend to possible human misunderstandings. Future SDS will need to manage the communication channel, in addition to managing the task, to aid in avoiding these misunderstandings. Researchers have explored the use of reinforcement learning (RL) to create dialogue policies that balance and optimize measures of task success (e.g., see (Scheffler and Young, 2002; Levin et al., 2000; Henderson et al., 2008; Walker, 2000)). Along these lines, RL is potentially well suited to creating policies for the subtask of managing the communication channel, as it can learn to adapt to the user while continuing the dialogue. In doing so, RL may choose actions that appear costly at the time, but lead to better overall dialogues. Our long term goal is to learn how to manage the communication channel along with the task, moving away from just “what” to say and also focusing on “how” to say it. For this proof-of-concept, our goals are twofold: 1) to formalize a communication channel model that encompasses diverse users, initi</context>
<context position="10543" citStr="Walker, 2000" startWordPosition="1623" endWordPosition="1624">e focus on amplitude adaptations in this paper, amplitude is only one aspect of the communication channel. A fully functional communication channel management solution would also incorporate adaptations of features such as speaking rate, pausing, pitch range, emphasis, etc. This extended set of features, because of their number and interaction between them, do not readily lend themselves to listener manipulation. 3 Reinforcement Learning RL has been used to create dialogue strategies that specify what action to perform in each possible system state so that a minimum dialogue cost is achieved (Walker, 2000; Levin et al., 2000). To accomplish this, RL starts with a policy, namely what action to perform in each state. It then uses this policy, with some exploration, to estimate the cost of getting from each state with each possible action to the final state. As more simulations are run, RL refines its estimates and its current policy. RL will converge to an optimal solution as long as assumptions about costs and state transitions are met. RL is particularly well suited for learning dialogue strategies as it will balance opposing goals (e.g., minimizing excessive confirmations vs. ensuring accurat</context>
</contexts>
<marker>Walker, 2000</marker>
<rawString>Marilyn A. Walker. 2000. An application of reinforcement learning to dialogue strategy selection in a spoken dialogue system for email. Journal of Aritificial Intelligence Research, 12:387–416.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>