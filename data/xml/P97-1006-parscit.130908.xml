<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.977856">
Document Classification Using a Finite Mixture Model
</title>
<author confidence="0.863778">
Hang Li Kenji Yamanishi
</author>
<affiliation confidence="0.470074">
C&amp;C Res. Labs., NEC
</affiliation>
<address confidence="0.686297">
4-1-1 Miyazaki Miyamae-ku Kawasaki, 216, Japan
</address>
<email confidence="0.917459">
Email: flihang,yamanisil©sbl.cl.nec.co.jp
</email>
<sectionHeader confidence="0.990039" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995400545454545">
We propose a new method of classifying
documents into categories. We define for
each category a finite mixture model based
on soft clustering of words. We treat the
problem of classifying documents as that
of conducting statistical hypothesis testing
over finite mixture models, and employ the
EM algorithm to efficiently estimate pa-
rameters in a finite mixture model. Exper-
imental results indicate that our method
outperforms existing methods.
</bodyText>
<sectionHeader confidence="0.998465" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999159644067797">
We are concerned here with the issue of classifying
documents into categories. More precisely, we begin
with a number of categories (e.g., &apos;tennis, soccer,
skiing&apos;), each already containing certain documents.
Our goal is to determine into which categories newly
given documents ought to be assigned, and to do so
on the basis of the distribution of each document&apos;s
words.&apos;
Many methods have been proposed to address
this issue, and a number of them have proved to
be quite effective (e.g.,(Apte, Damerau, and Weiss,
1994; Cohen and Singer, 1996; Lewis, 1992; Lewis
and Ringuette, 1994; Lewis et al., 1996; Schutze,
Hull, and Pedersen, 1995; Yang and Chute, 1994)).
The simple method of conducting hypothesis testing
over word-based distributions in categories (defined
in Section 2) is not efficient in storage and suffers
from the data sparseness problem, i.e., the number
of parameters in the distributions is large and the
data size is not sufficiently large for accurately es-
timating them. In order to address this difficulty,
(Guthrie, Walker, and Guthrie, 1994) have proposed
using distributions based on what we refer to as hard
&apos;A related issue is the retrieval, from a data base, of
documents which are relevant to a given query (pseudo-
document) (e.g.,(Deerwester et al., 1990; Fuhr, 1989;
Robertson and Jones, 1976; Salton and McGill, 1983;
Wong and Yao, 1989)).
clustering of words, i.e., in which a word is assigned
to a single cluster and words in the same cluster are
treated uniformly. The use of hard clustering might,
however, degrade classification results, since the dis-
tributions it employs are not always precise enough
for representing the differences between categories.
We propose here to employ soft clustering&apos;, i.e.,
a word can be assigned to several different clusters
and each cluster is characterized by a specific word
probability distribution. We define for each cate-
gory a finite mixture model, which is a linear com-
bination of the word probability distributions of the
clusters. We thereby treat the problem of classify-
ing documents as that of conducting statistical hy-
pothesis testing over finite mixture models. In or-
der to accomplish hypothesis testing, we employ the
EM algorithm to efficiently and approximately cal-
culate from training data the maximum likelihood
estimates of parameters in a finite mixture model.
Our method overcomes the major drawbacks of
the method using word-based distributions and the
method based on hard clustering, while retaining
their merits; it in fact includes those two methods
as special cases. Experimental results indicate that
our method outperforms them.
Although the finite mixture model has already
been used elsewhere in natural language processing
(e.g. (Jelinek and Mercer, 1980; Pereira, Tishby,
and Lee, 1993)), this is the first work, to the best of
knowledge, that uses it in the context of document
classification.
</bodyText>
<sectionHeader confidence="0.996987" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<subsectionHeader confidence="0.754859">
Word-based method
</subsectionHeader>
<bodyText confidence="0.9999666">
A simple approach to document classification is to
view this problem as that of conducting hypothesis
testing over word-based distributions. In this paper,
we refer to this approach as the word-based method
(hereafter, referred to as WBM).
</bodyText>
<footnote confidence="0.956052333333333">
2We borrow from (Pereira, Tishby, and Lee, 1993)
the terms hard clustering and soft clustering, which were
used there in a different task.
</footnote>
<page confidence="0.999073">
39
</page>
<bodyText confidence="0.999810875">
Letting W denote a vocabulary (a set of words),
and w denote a random variable representing any
word in it, for each category ci (i = 1, • • • ,n), we
define its word-based distribution P(wici) as a his-
togram type of distribution over W. (The num-
ber of free parameters of such a distribution is thus
1141-1). WBM then views a document as a sequence
of words:
</bodyText>
<equation confidence="0.764773">
d = wi,• • ,wN (1)
</equation>
<bodyText confidence="0.99993675">
and assumes that each word is generated indepen-
dently according to a probability distribution of a
category. It then calculates the probability of a doc-
ument with respect to a category as
</bodyText>
<equation confidence="0.999777">
P(diCi) = P(W1, • • • ,wNici) = P(wtici), (2)
</equation>
<bodyText confidence="0.9999875">
and classifies the document into that category for
which the calculated probability is the largest. We
should note here that a document&apos;s probability with
respect to each category is equivalent to the likeli-
hood of each category with respect to the document,
and to classify the document into the category for
which it has the largest probability is equivalent to
classifying it into the category having the largest
likelihood with respect to it. Hereafter, we will use
only the term likelihood and denote it as L(dici).
Notice that in practice the parameters in a dis-
tribution must be estimated from training data. In
the case of WBM, the number of parameters is large;
the training data size, however, is usually not suffi-
ciently large for accurately estimating them. This
is the data .sparseness problem that so often stands
in the way of reliable statistical language processing
(e.g.(Gale and Church, 1990)). Moreover, the num-
ber of parameters in word-based distributions is too
large to be efficiently stored.
</bodyText>
<subsectionHeader confidence="0.837217">
Method based on hard clustering
</subsectionHeader>
<bodyText confidence="0.993652166666667">
In order to address the above difficulty, Guthrie
et.al. have proposed a method based on hard cluster-
ing of words (Guthrie, Walker, and Guthrie, 1994)
(hereafter we will refer to this method as HCM). Let
be categories. HCM first conducts hard
clustering of words. Specifically, it (a) defines a vo-
cabulary as a set of words W and defines as clusters
its subsets kJ., • • , km satisfying Ur_iks = W and
ki n k, = 0 (i j) (i.e., each word is assigned only
to a single cluster); and (b) treats uniformly all the
words assigned to the same cluster. HCM then de-
fines for each category ci a distribution of the clus-
ters P(ki lci) (j = 1, • • • , m). It replaces each word
wt in the document with the cluster kt to which it
belongs (t = 1,- • ,N). It assumes that a cluster kt
is distributed according to P(ki Ici) and calculates
the likelihood of each category Ci with respect to
the document by
</bodyText>
<equation confidence="0.9979745">
L(dICi) = • • • ,kNICi) = H P(ktici)• (3)
i=1
</equation>
<tableCaption confidence="0.987676">
Table 1: Frequencies of words
</tableCaption>
<table confidence="0.667389">
racket stroke shot goal kick ball
C1 4 1 2 1 0 2
C2 0 0 0 3 2 2
</table>
<tableCaption confidence="0.7903772">
Table 2: Clusters and words (L = 5,M = 5)
racket, stroke, shot
kick
k3 goal, ball
Table 3: Frequencies of clusters
</tableCaption>
<equation confidence="0.954446666666667">
k1 k2 k3
c1 7 0 3
C2 0 2 5
</equation>
<bodyText confidence="0.999681476190476">
There are any number of ways to create clusters in
hard clustering, but the method employed is crucial
to the accuracy of document classification. Guthrie
et. al. have devised a way suitable to documentation
classification. Suppose that there are two categories
c1=`tennis&apos; and c2=`soccer,&apos; and we obtain from the
training data (previously classified documents) the
frequencies of words in each category, such as those
in Tab. 1. Letting L and M be given positive inte-
gers, HCM creates three clusters: kl, k2 and k3, in
which k1 contains those words which are among the
L most frequent words in c1, and not among the M
most frequent in c2; k2 contains those words which
are among the L most frequent words in c2, and
not among the M most frequent in c1; and k3 con-
tains all remaining words (see Tab. 2). HCM then
counts the frequencies of clusters in each category
(see Tab. 3) and estimates the probabilities of clus-
ters being in each category (see Tab. 4).3 Suppose
that a newly given document, like d in Fig. 1, is to
be classified. HCM calculates the likelihood values
</bodyText>
<footnote confidence="0.941535333333333">
3We calculate the probabilities here by using the so-
called expected likelihood estimator (Gale and Church,
1990):
</footnote>
<equation confidence="0.978365">
P(k = f I c, ) +0.5 4
,l,) (
c )
f (c,)± 0.5 x m
</equation>
<bodyText confidence="0.859907">
where f(kIlc,) is the frequency of the cluster k, in c1,
f (c,) is the total frequency of clusters in c„ and in is the
total number of clusters.
</bodyText>
<page confidence="0.999586">
40
</page>
<tableCaption confidence="0.999587">
Table 4: Probability distributions of clusters
</tableCaption>
<table confidence="0.993474333333333">
k1 k2 k3
c1 0.65 0.04 0.30
c2 0.06 0.29 0.65
</table>
<bodyText confidence="0.9439005">
L(clIci) and L(dIc2) according to Eq. (3). (Tab. 5
shows the logarithms of the resulting likelihood val-
ues.) It then classifies d into c2, as log2 L(dIc2) is
larger than log2 L(dici)•
</bodyText>
<figureCaption confidence="0.829205">
d = kick, goal, goal, ball
Figure 1: Example document
</figureCaption>
<bodyText confidence="0.997119166666667">
HCM can handle the data sparseness problem
quite well. By assigning words to clusters, it can
drastically reduce the number of parameters to be
estimated. It can also save space for storing knowl-
edge. We argue, however, that the use of hard clus-
tering still has the following two problems:
</bodyText>
<listItem confidence="0.817291333333333">
1. HCM cannot assign a word to more than one
cluster at a time. Suppose that there is another
category c3 = &apos;skiing&apos; in which the word &apos;ball&apos;
</listItem>
<bodyText confidence="0.83909952173913">
does not appear, i.e., &apos;ball&apos; will be indicative of
both c1 and c2, but not c3. If we could assign
&apos;ball&apos; to both k1 and k2, the likelihood value for
classifying a document containing that word to
ci or c2 would become larger, and that for clas-
sifying it into c3 would become smaller. HCM,
however, cannot do that.
2. HCM cannot make the best use of information
about the differences among the frequencies of
words assigned to an individual cluster. For ex-
ample, it treats &apos;racket&apos; and &apos;shot&apos; uniformly be-
cause they are assigned to the same cluster ki
(see Tab. 5). &apos;Racket&apos; may, however, be more
indicative of c1 than &apos;shot,&apos; because it appears
more frequently in Cl than &apos;shot.&apos; HCM fails
to utilize this information. This problem will
become more serious when the values L and M
in word clustering are large, which renders the
clustering itself relatively meaningless.
From the perspective of number of parameters,
HCM employs models having very few parameters,
and thus may not sometimes represent much useful
information for classification.
</bodyText>
<sectionHeader confidence="0.993572" genericHeader="method">
3 Finite Mixture Model
</sectionHeader>
<bodyText confidence="0.977098666666667">
We propose a method of document classification
based on soft clustering of words. Let cl,•• , c„
be categories. We first conduct the soft cluster-
ing. Specifically, we (a) define a vocabulary as a
set W of words and define as clusters a number of
its subsets /el, • ,km satisfying u71k1 = W; (no-
tice that ki n kj = 0 (i j) does not necessarily
hold here, i.e., a word can be assigned to several dif-
ferent clusters); and (b) define for each cluster kj
</bodyText>
<equation confidence="0.963381">
(j = 1, • • • , m) a distribution Q(wlki) over its words
(E.Ek3 Q(wlki) = 1) and a distribution P(wIke)
satisfying:
Q(wlkj); w E kj,
P(Wik.i) = w tit ki, (5)
</equation>
<bodyText confidence="0.9986408">
where w denotes a random variable representing any
word in the vocabulary. We then define for each cat-
egory ce (i = 1,• • • , n) a distribution of the clusters
P(ki Ice) ,and define for each category a linear com-
bination of P(wlki):
</bodyText>
<equation confidence="0.999844">
P(wIce) = E p(kjici) x p(wiko (6)
</equation>
<bodyText confidence="0.999970857142857">
as the distribution over its words, which is referred
to as a finite mixture model(e.g., (Everitt and Hand,
1981)).
We treat the problem of classifying a document
as that of conducting the likelihood ratio test over
finite mixture models. That is, we view a document
as a sequence of words,
</bodyText>
<equation confidence="0.514135">
d= wi,•&amp;quot;,wN (7)
</equation>
<bodyText confidence="0.99952425">
where wt(t = 1, • , N) represents a word. We
assume that each word is independently generated
according to an unknown probability distribution
and determine which of the finite mixture mod-
els P(wIci)(i = 1, • • , n) is more likely to be the
probability distribution by observing the sequence of
words. Specifically, we calculate the likelihood value
for each category with respect to the document by:
</bodyText>
<equation confidence="0.853341">
L(dIci) = L(wi, • • • , WNICi)
= IIt=l P(wtici)
= n 1 ( Er_ip(kici) x P(wt Ike)) .
</equation>
<bodyText confidence="0.97043125">
(8)
We then classify it into the category having the
largest likelihood value with respect to it. Hereafter,
we will refer to this method as FMM.
FMM includes WBM and HCM as its special
cases. If we consider the specific case (1) in which
a word is assigned to a single cluster and P(wIkj) is
given by
</bodyText>
<equation confidence="0.9882125">
1 •
P(dk.i)= (151&apos; w E kj, (9)
, w
Table 5: Calculating log likelihood values
log2 L(dlci)
= 1 x log2 .04 + 3 x log2 .30 = —9.85
log2 L(dIc2)
= 1 x log2 .29 + 3 x log2 .65 = —3.65
</equation>
<page confidence="0.976656">
41
</page>
<bodyText confidence="0.99987225">
where Iki I denotes the number of elements belonging
to ki, then we will get the same classification result
as in HCM. In such a case, the likelihood value for
each category ci becomes:
</bodyText>
<equation confidence="0.998212333333333">
L(dlq) = ri/\-fl(P(ktici) x ilwtikt))
= 11.1P(ktici) x n,=, P(wtIkt),
(10)
</equation>
<bodyText confidence="0.99561875">
where kt is the cluster corresponding to wt. Since
the probability P(wtIkt) does not depend on cate-
gories, we can ignore the second term ritN=1 P(wtIkt)
in hypothesis testing, and thus our method essen-
tially becomes equivalent to HCM (c.f. Eq. (3)).
Further, in the specific case (2) in which m = n,
for each j, P(wlki) has IWI parameters: P(wIki) =
P(wIci), and P(ki lei) is given by
</bodyText>
<equation confidence="0.655193">
P(ki lei) =
</equation>
<bodyText confidence="0.997969666666667">
the likelihood used in hypothesis testing becomes
the same as that in Eq.(2), and thus our method
becomes equivalent to WBM.
</bodyText>
<sectionHeader confidence="0.8830205" genericHeader="method">
4 Estimation and Hypothesis
Testing
</sectionHeader>
<bodyText confidence="0.9992165">
In this section, we describe how to implement our
method.
</bodyText>
<subsectionHeader confidence="0.965396">
Creating clusters
</subsectionHeader>
<bodyText confidence="0.99994075">
There are any number of ways to create clusters on a
given set of words. As in the case of hard clustering,
the way that clusters are created is crucial to the
reliability of document classification. Here we give
one example approach to cluster creation.
We let the number of clusters equal that of cat-
egories (i.e., m = n) 4 and relate each cluster ki
to one category ci (i = 1, ,n). We then assign
individual words to those clusters in whose related
categories they most frequently appear. Letting 7
(0 &lt; y &lt; 1) be a predetermined threshold value, if
the following inequality holds:
</bodyText>
<equation confidence="0.803134">
f(wIci)
1(w)
</equation>
<bodyText confidence="0.964180363636364">
then we assign w to ki, the cluster related to
where f(wIci) denotes the frequency of the word w
in category c, and f(w) denotes the total frequency
of w. Using the data in Tab.1, we create two clusters:
k1 and k2, and relate them to e1 and c2, respectively.
40ne can certainly assume that m &gt; n.
For example, when 7 = 0.4, we assign &apos;goal&apos; to k2
only, as the relative frequency of &apos;goal&apos; in c2 is 0.75
and that in ci is only 0.25. We ignore in document
classification those words which cannot be assigned
to any cluster using this method, because they are
not indicative of any specific category. (For example,
when 7 &gt; 0.5 &apos;ball&apos; will not be assigned into any
cluster.) This helps to make classification efficient
and accurate. Tab. 6 shows the results of creating
clusters.
Estimating P(wlki)
We then consider the frequency of a word in a clus-
ter. If a word is assigned only to one cluster, we view
its total frequency as its frequency within that clus-
ter. For example, because &apos;goal&apos; is assigned only to
k2, we use as its frequency within that cluster the to-
tal count of its occurrence in all categories. If a word
is assigned to several different clusters, we distribute
its total frequency among those clusters in propor-
tion to the frequency with which the word appears
in each of their respective related categories. For
example, because &apos;ball&apos; is assigned to both k1 and
k2, we distribute its total frequency among the two
clusters in proportion to the frequency with which
&apos;ball&apos; appears in CI and c2, respectively. After that,
we obtain the frequencies of words in each cluster as
shown in Tab. 7.
</bodyText>
<tableCaption confidence="0.912575">
Table 7: Distributed frequencies of words
</tableCaption>
<bodyText confidence="0.9895252">
racket stroke shot goal kick ball
k1 4 1 2 0 0 2
k2 0 0 0 4 2 2
We then estimate the probabilities of words in
each cluster, obtaining the results in Tab. 8.5
</bodyText>
<tableCaption confidence="0.995029">
Table 8: Probability distributions of words
</tableCaption>
<table confidence="0.8468425">
racket stroke shot goal kick ball
k1 0.44 0.11 0.22 0 0 0.22
k2 0 0 0 0.50 0.25 0.25
Estimating P(ki Ici)
</table>
<bodyText confidence="0.995980333333333">
Let us next consider the estimation of P(ki Ici).
There are two common methods for statistical esti-
mation, the maximum likelihood estimation method
</bodyText>
<footnote confidence="0.71963">
5We calculate the probabilities by employing the
maximum likelihood estimator:
</footnote>
<equation confidence="0.9914415">
f (ks,lc,)
P(kici) =
</equation>
<bodyText confidence="0.998307">
where f(k,lc,) is the frequency of the cluster ks, in ct,
and f (c,) is the total frequency of clusters in c.
</bodyText>
<tableCaption confidence="0.68884">
Table 6: Clusters and words
</tableCaption>
<figure confidence="0.649198">
k1 racket, stroke, shot, ball
k2 kick, goal, ball
(11)
(12)
</figure>
<page confidence="0.977245">
42
</page>
<tableCaption confidence="0.957473">
Table 10: Calculating log likelihood values
</tableCaption>
<equation confidence="0.8331095">
log2 L(d Cl) = log2(.14 x .25) + 2 x log2(.14 x .50) + log2(.86 x .22 + .14 x .25) = —14.67
log2 L(d1c2) = log2(.96 x .25) + 2 x log2(.96 x .50) + log2(.04 x .22 + .96 x .25) = —6.18
</equation>
<tableCaption confidence="0.987396">
Table 9: Probability distributions of clusters
</tableCaption>
<table confidence="0.996078333333333">
k1 kz
c1 0.86 0.14
C2 0.04 0.96
</table>
<bodyText confidence="0.9989419375">
and the Bayes estimation method. In their imple-
mentation for estimating P(ki Ici), however, both of
them suffer from computational intractability. The
EM algorithm (Dempster, Laird, and Rubin, 1977)
can be used to efficiently approximate the maximum
likelihood estimator of P(kj lci). We employ here an
extended version of the EM algorithm (Helmbold et
al., 1995). (We have also devised, on the basis of
the Markov chain Monte Carlo (MCMC) technique
(e.g. (Tanner and Wong, 1987; Yamanishi, 1996))6,
an algorithm to efficiently approximate the Bayes
estimator of P(ki
For the sake of notational simplicity, for a fixed i,
let us write P(ki lci) as 0, and P(wlki) as Pi(w).
Then letting 0 = (01, • • • ,0,,), the finite mixture
model in Eq. (6) may be written as
</bodyText>
<equation confidence="0.980996">
171
PoDio= E0i x PAO- (14)
j.1
</equation>
<bodyText confidence="0.99955175">
For a given training sequence w1 • • wN, the maxi-
mum likelihood estimator of 9 is defined as the value
0 which maximizes the following log likelihood func-
tion
</bodyText>
<equation confidence="0.991443333333333">
L(0) = --N- &gt;2 log (E Oi (wt)) . (15)
t.i
1
</equation>
<bodyText confidence="0.9993156">
The EM algorithm first arbitrarily sets the initial
value of 0, which we denote as OM, and then suc-
cessively calculates the values of 0 on the basis of its
most recent values. Let s be a predetermined num-
ber. At the /th iteration (1 = 1, • • ,$), we calculate
</bodyText>
<equation confidence="0.9961195">
0(1) = (9(,1) , ,e) by
0(1) = 0(.1-1) (vL(0(1-1))i — 1) + 1) , (16)
</equation>
<bodyText confidence="0.999122">
where &gt; 0 (when zi = 1, Hembold et al. &apos;s version
simply becomes the standard EM algorithm), and
</bodyText>
<footnote confidence="0.92933425">
6We have confirmed in our preliminary experiment
that MCMC performs slightly better than EM in docu-
ment classification, but we omit the details here due to
space limitations.
</footnote>
<equation confidence="0.836036666666667">
VL(0) denotes
VL(0)=(— L(0)= Gel aem) •
(17)
</equation>
<bodyText confidence="0.993461714285714">
After s numbers of calculations, the EM algorithm
outputs 0(3) = QS:), • • • , OW) as an approximate of
O. It is theoretically guaranteed that the EM al-
gorithm converges to a local minimum of the given
likelihood (Dempster, Laird, and Rubin, 1977).
For the example in Tab. 1, we obtain the results
as shown in Tab. 9.
Testing
For the example in Tab. 1, we can calculate ac-
cording to Eq. (8) the likelihood values of the two
categories with respect to the document in Fig. 1
(Tab. 10 shows the logarithms of the likelihood val-
ues). We then classify the document into category
c2, as log2 L(dic2) is larger than log2 L(dici).
</bodyText>
<sectionHeader confidence="0.877082" genericHeader="method">
5 Advantages of FMM
</sectionHeader>
<bodyText confidence="0.999973233333333">
For a probabilistic approach to document classifica-
tion, the most important thing is to determine what
kind of probability model (distribution) to employ
as a representation of a category. It must (1) ap-
propriately represent a category, as well as (2) have
a proper preciseness in terms of number of param-
eters. The goodness and badness of selection of a
model directly affects classification results.
The finite mixture model we propose is particu-
larly well-suited to the representation of a category.
Described in linguistic terms, a cluster corresponds
to a topic and the words assigned to it are related
to that topic. Though documents generally concen-
trate on a single topic, they may sometimes refer
for a time to others, and while a document is dis-
cussing any one topic, it will naturally tend to use
words strongly related to that topic. A document in
the category of &apos;tennis&apos; is more likely to discuss the
topic of &apos;tennis,&apos; i.e., to use words strongly related
to &apos;tennis,&apos; but it may sometimes briefly shift to the
topic of &apos;soccer,&apos; i.e., use words strongly related to
&apos;soccer.&apos; A human can follow the sequence of words
in such a document, associate them with related top-
ics, and use the distributions of topics to classify the
document. Thus the use of the finite mixture model
can be considered as a stochastic implementation of
this process.
The use of FMM is also appropriate from the
viewpoint of number of parameters. Tab. 11 shows
the numbers of parameters in our method (FMM),
</bodyText>
<equation confidence="0.474748">
OL 8L
</equation>
<page confidence="0.997411">
43
</page>
<bodyText confidence="0.999874447368421">
HCM, and WBM, where I WI is the size of a vocab-
ulary, lhi is the sum of the sizes of word clusters
= E7-1 I), n is the number of categories,
and m is the number of clusters. The number of
parameters in FMM is much smaller than that in
WBM, which depends on I WI, a very large num-
ber in practice (notice that lk I is always smaller
than I WI when we employ the clustering method
(with -y &gt; 0.5) described in Section 4. As a result,
FMM requires less data for parameter estimation
than WBM and thus can handle the data sparseness
problem quite well. Furthermore, it can economize
on the space necessary for storing knowledge. On
the other hand, the number of parameters in FMM
is larger than that in HCM. It is able to represent the
differences between categories more precisely than
HCM, and thus is able to resolve the two problems,
described in Section 2, which plague HCM.
Another advantage of our method may be seen in
contrast to the use of latent semantic analysis (Deer-
wester et al., 1990) in document classification and
document retrieval. They claim that their method
can solve the following problems:
synonymy problem how to group synonyms, like
&apos;stroke&apos; and &apos;shot,&apos; and make each relatively
strongly indicative of a category even though
some may individually appear in the category
only very rarely;
polysemy problem how to determine that a word
like &apos;ball&apos; in a document refers to a &apos;tennis ball&apos;
and not a &apos;soccer ball,&apos; so as to classify the doc-
ument more accurately;
dependence problem how to use de-
pendent words, like &apos;kick&apos; and &apos;goal,&apos; to make
their combined appearance in a document more
indicative of a category.
As seen in Tab.6, our method also helps resolve all
of these problems.
</bodyText>
<sectionHeader confidence="0.986429" genericHeader="method">
6 Preliminary Experimental Results
</sectionHeader>
<bodyText confidence="0.999578428571429">
In this section, we describe the results of the exper-
iments we have conducted to compare the perfor-
mance of our method with that of HCM and others.
As a first data set, we used a subset of the Reuters
newswire data prepared by Lewis, called Reuters-
21578 Distribution 1.0.7 We selected nine overlap-
ping categories, i.e. in which a document may be-
</bodyText>
<footnote confidence="0.464229">
7 Reuters-21578 is available at
http://www.research.att.comfiewis.
</footnote>
<bodyText confidence="0.999634529411765">
long to several different categories. We adopted the
Lewis Split in the corpus to obtain the training data
and the test data. Tabs. 12 and 13 give the de-
tails. We did not conduct stemming, or use stop
words8. We then applied FMM, HCM, WBM ,and
a method based on cosine-similarity, which we de-
note as COS8, to conduct binary classification. In
particular, we learn the distribution for each cate-
gory and that for its complement category from the
training data, and then determine whether or not to
classify into each category the documents in the test
data. When applying FMM, we used our proposed
method of creating clusters in Section 4 and set -y
to be 0, 0.4,0.5, 0.7, because these are representative
values. For HCM, we classified words in the same
way as in FMM and set -y to be 0.5, 0.7, 0.9, 0.95.
(Notice that in HCM, -y cannot be set less than 0.5.)
</bodyText>
<tableCaption confidence="0.997314">
Table 12: The first data set
</tableCaption>
<table confidence="0.829724">
Num. of doc. in training data 707
Num. of doc in test data 228
Num. of (type of) words 10902
Avg. num. of words per doc. 310.6
</table>
<tableCaption confidence="0.998267">
Table 13: Categories in the first data set
</tableCaption>
<bodyText confidence="0.7812235">
wheat,corn,oilseed,sugar,coffee
soybean,cocoa,rice,cotton
</bodyText>
<tableCaption confidence="0.992286">
Table 14: The second data set
</tableCaption>
<bodyText confidence="0.974944809523809">
Num. of doc. training data 13625
Num. of doc. in test data 6188
Num. of (type of) words 50301
Avg. num. of words per doc. 181.3
As a second data set, we used the entire Reuters-
21578 data with the Lewis Split. Tab. 14 gives the
details. Again, we did not conduct stemming, or use
stop words. We then applied FMM, HCM, WBM ,
and COS to conduct binary classification. When ap-
plying FMM, we used our proposed method of creat-
ing clusters and set -y to be 0, 0.4, 0.5, 0.7. For HCM,
we classified words in the same way as in FMM and
set 7 to be 0.5,0.7, 0.9, 0.95. We have not fully com-
pleted these experiments, however, and here we only
&apos;Stop words&apos; refers to a predetermined list of words
containing those which are considered not useful for doc-
ument classification, such as articles and prepositions.
91n this method, categories and documents to be clas-
sified are viewed as vectors of word frequencies, and the
cosine value between the two vectors reflects similarity
(Salton and McGill, 1983).
</bodyText>
<tableCaption confidence="0.869675">
Table 11: Num. of parameters
</tableCaption>
<table confidence="0.848736333333333">
WBM 0(n • IWI)
HCM 0(n • m)
FMM O(ikl+ n • m)
</table>
<page confidence="0.980661">
44
</page>
<tableCaption confidence="0.977185">
Table 15: Tested categories in the second data set
</tableCaption>
<bodyText confidence="0.988382857142857">
earn,acq,crude,money-fx,grain
interest ,trade,ship,wheat ,corn
give the results of classifying into the ten categories
having the greatest numbers of documents in the test
data (see Tab. 15).
For both data sets, we evaluated each method in
terms of precision and recall by means of the so-
called micro-averaging
When applying WBM, HCM, and FMM, rather
than use the standard likelihood ratio testing, we
used the following heuristics. For simplicity, suppose
that there are only two categories c1 and c2. Letting
c be a given number larger than or equal 0, we assign
a new document d in the following way:
</bodyText>
<equation confidence="0.9630675">
&amp;quot;(log L(c/Ici) — log L(c/Ic2)) &gt; c; d ci,
L(d1c2) — log L(c/Ici)) &gt; c; d c2,
</equation>
<bodyText confidence="0.9723365">
otherwise; unclassify d,
(18)
where N is the size of document d. (One can easily
extend the method to cases with a greater number of
categories.) 11 For COS, we conducted classification
in a similar way.
Figs. 2 and 3 show precision-recall curves for the
first data set and those for the second data set, re-
spectively. In these graphs, values given after FMM
and HCM represent 7 in our clustering method (e.g.
FMM0.5, HCM0.5,etc). We adopted the break-even
point as a single measure for comparison, which is
the one at which precision equals recall; a higher
score for the break-even point indicates better per-
formance. Tab. 16 shows the break-even point for
each method for the first data set and Tab. 17 shows
that for the second data set. For the first data set,
FMMO attains the highest score at break-even point;
for the second data set, FMM0.5 attains the highest.
We considered the following questions:
</bodyText>
<listItem confidence="0.9850449">
(1) The training data used in the experimen-
tation may be considered sparse. Will a word-
clustering-based method (FMM) outperform a word-
based method (WBM) here?
(2) Is it better to conduct soft clustering (FMM)
than to do hard clustering (HCM)?
(3) With our current method of creating clusters,
as the threshold -y approaches 0, FMM behaves much
like WBM and it does not enjoy the effects of clus-
tering at all (the number of parameters is as large
</listItem>
<bodyText confidence="0.837231714285714">
&amp;quot;In micro-averaging(Lewis and Ringuette, 1994), pre-
cision is defined as the percentage of classified documents
in all categories which are correctly classified. Recall is
defined as the percentage of the total documents in all
categories which are correctly classified.
11Notice that words which are discarded in the cluster-
ing process should not to be counted in document size.
</bodyText>
<figureCaption confidence="0.994493">
Figure 2: Precision-recall curve for the first data set
</figureCaption>
<figure confidence="0.999743">
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
</figure>
<figureCaption confidence="0.9693855">
Figure 3: Precision-recall curve for the second data
set
</figureCaption>
<bodyText confidence="0.9996377">
as in WBM). This is because in this case (a) a word
will be assigned into all of the clusters, (b) the dis-
tribution of words in each cluster will approach that
in the corresponding category in WBM, and (c) the
likelihood value for each category will approach that
in WBM (recall case (2) in Section 3). Since creating
clusters in an optimal way is difficult, when cluster-
ing does not improve performance we can at least
make FMM perform as well as WBM by choosing
= 0. The question now is &amp;quot;does FMM perform
better than WBM when -y is 0?&amp;quot;
In looking into these issues, we found the follow-
ing:
(1) When 7&gt;&gt; 0, i.e., when we conduct clustering,
FMM does not perform better than WBM for the
first data set, but it performs better than WBM for
the second data set.
Evaluating classification results on the basis of
each individual category, we have found that for
three of the nine categories in the first data set,
</bodyText>
<figure confidence="0.910722">
0.2
0.3
0.4
0.5 0.6
recall
0.7
0.8
0.9
45
cos 0.60
WBM 0.62
HCM0.5 0.32
HCM0.7 0.42
HCM0.9 0.54
HCM0.95 0.51
FMMO 0.66
FMM0.4 0.54
FMMO .5 0.52
FMM0.7 0.42
Table 16: Break-even point for the first data set
0.1
0.2 0.3 0.4 0.5 0.6
0.9
0.7 0.6
&apos;COW -4-
&apos;W13/6&apos;
14C840.9&apos; •0..
&apos;FMM0.5*
0.9
0.8
3.7
a .
0.6
0.5
0.4
0
</figure>
<tableCaption confidence="0.998628">
Table 17: Break-even point for the second data set
</tableCaption>
<table confidence="0.9978097">
cos 0.52
WBM 0.62
HCM0.5 0.47
HCM0.7 0.51
HCM0.9 0.55
HCM0.95 0.31
FMMO 0.62
FMM0.4 0.54
F MMO .5 0.67
FMM0.7 0.62
</table>
<bodyText confidence="0.990916966666667">
FMM0.5 performs best, and that in two of the ten
categories in the second data set FMM0.5 performs
best. These results indicate that clustering some-
times does improve classification results when we
use our current way of creating clusters. (Fig. 4
shows the best result for each method for the cate-
gory &apos;corn&apos; in the first data set and Fig. 5 that for
&apos;grain&apos; in the second data set.)
(2) When -y&gt;&gt; 0, i.e., when we conduct clustering,
the best of FMM almost always outperforms that of
HCM.
(3) When -y = 0, FMM performs better than
WBM for the first data set, and that it performs
as well as WBM for the second data set.
In summary, FMM always outperforms HCM; in
some cases it performs better than WBM; and in
general it performs at least as well as WBM.
For both data sets, the best FMM results are supe-
rior to those of COS throughout. This indicates that
the probabilistic approach is more suitable than the
cosine approach for document classification based on
word distributions.
Although we have not completed our experiments
on the entire Reuters data set, we found that the re-
sults with FMM on the second data set are almost as
good as those obtained by the other approaches re-
ported in (Lewis and Ringuette, 1994). (The results
are not directly comparable, because (a) the results
in (Lewis and Ringuette, 1994) were obtained from
an older version of the Reuters data; and (b) they
</bodyText>
<figureCaption confidence="0.997258">
Figure 4: Precision-recall curve for category &apos;corn&apos;
</figureCaption>
<figure confidence="0.78144">
0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
</figure>
<figureCaption confidence="0.995975">
Figure 5: Precision-recall curve for category &apos;grain&apos;
</figureCaption>
<bodyText confidence="0.9708208">
used stop words, but we did not.)
We have also conducted experiments on the Su-
sanne corpus data&apos; and confirmed the effectiveness
of our method. We omit an explanation of this work
here due to space limitations.
</bodyText>
<sectionHeader confidence="0.998846" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.7085956">
Let us conclude this paper with the following re-
marks:
1. The primary contribution of this research is
that we have proposed the use of the finite mix-
ture model in document classification.
2. Experimental results indicate that our method
of using the finite mixture model outperforms
the method based on hard clustering of words.
3. Experimental results also indicate that in some
cases our method outperforms the word-based
</bodyText>
<footnote confidence="0.468423">
&apos;The Susanne corpus, which has four non-overlapping
categories, is available at ftp://ota.ox.ac.uk
</footnote>
<page confidence="0.999301">
46
</page>
<bodyText confidence="0.985678333333333">
method when we use our current method of cre-
ating clusters.
Our future work is to include:
</bodyText>
<listItem confidence="0.906206">
1. comparing the various methods over the entire
Reuters corpus and over other data bases,
2. developing better ways of creating clusters.
</listItem>
<bodyText confidence="0.999973">
Our proposed method is not limited to document
classification; it can also be applied to other natu-
ral language processing tasks, like word sense dis-
ambiguation, in which we can view the context sur-
rounding a ambiguous target word as a document
and the word-senses to be resolved as categories.
</bodyText>
<sectionHeader confidence="0.996376" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999975285714286">
We are grateful to Tomoyuki Fujita of NEC for his
constant encouragement. We also thank Naoki Abe
of NEC for his important suggestions, and Mark Pe-
tersen of Meiji Univ. for his help with the English of
this text. We would like to express special apprecia-
tion to the six ACL anonymous reviewers who have
provided many valuable comments and criticisms.
</bodyText>
<sectionHeader confidence="0.998744" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999791619047619">
Apte, Chidanand, Fred Damerau, and Sholom M.
Weiss. 1994. Automated learning of decision rules
for text categorization. ACM Tran. on Informa-
tion Systems, 12(3):233-251.
Cohen, William W. and Yoram Singer. 1996.
Context-sensitive learning methods for text cat-
egorization. Proc. of SIGIR&apos;96.
Deerwester, Scott, Susan T. Dumais, George W.
Furnas, Thomas K. Landauer, and Richard Harsh-
man. 1990. Indexing by latent semantic analysis.
Journ. of the American Society for Information
Science, 41(6):391-407.
Dempster, A.P., N.M. Laird, and D.B. Rubin. 1977.
Maximum likelihood from incomplete data via the
em algorithm. Journ. of the Royal Statistical So-
ciety, Series B, 39(1):1-38.
Everitt, B. and D. Hand. 1981. Finite Mixture Dis-
tributions. London: Chapman and Hall.
Fuhr, Norbert. 1989. Models for retrieval with prob-
abilistic indexing. Information Processing and
Management, 25(1):55-72.
Gale, Williams A. and Kenth W. Church. 1990.
Poor estimates of context are worse than none.
Proc. of the DARPA Speech and Natural Language
Workshop, pages 283-287.
Guthrie, Louise, Elbert Walker, and Joe Guthrie.
1994. Document classification by machine: The-
ory and practice. Proc. of COLING&apos;94, pages
1059-1063.
Helmbold, D., R. Schapire, Y. Siuger, and M. War-
muth. 1995. A comparison of new and old algo-
rithm for a mixture estimation problem. Proc. of
COLT&apos;95, pages 61-68.
Jelinek, F. and R.I. Mercer. 1980. Interpolated esti-
mation of markov source parameters from sparse
data. Proc. of Workshop on Pattern Recognition
in Practice, pages 381-402.
Lewis, David D. 1992. An evaluation of phrasal and
clustered representations on a text categorization
task. Proc. of SIGIR&apos;92, pages 37-50.
Lewis, David D. and Marc Ringuette. 1994. A com-
parison of two learning algorithms for test catego-
rization. Proc. of 3rd Annual Symposium on Doc-
ument Analysis and Information Retrieval, pages
81-93.
Lewis, David D., Robert E. Schapire, James P.
Callan, and Ron Papka. 1996. Training algo-
rithms for linear text classifiers. Proc. of SI-
GIR&apos;96.
Pereira, Fernando, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of english words.
Proc. of ACL&apos;93, pages 183-190.
Robertson, S.E. and K. Sparck Jones. 1976. Rel-
evance weighting of search terms. Journ. of
the American Society for Information Science,
27:129-146.
Salton, G. and M.J. McGill. 1983. Introduction to
Modern Information Retrieval. New York: Mc-
Graw Hill.
Schutze, Hinrich, David A. Hull, and Jan 0. Peder-
sen. 1995. A comparison of classifiers and doc-
ument representations for the routing problem.
Proc. of SIGIR&apos;95.
</reference>
<bodyText confidence="0.532998266666667">
Tanner, Martin A. and Wing Hung Wong. 1987.
The calculation of posterior distributions by data
augmentation. Journ. of the American Statistical
Association, 82(398):528-540.
Wong, S.K.M. and Y.Y. Yao. 1989. A probability
distribution model for information retrieval. In-
formation Processing and Management, 25(1):39-
53.
Yamanishi, Kenji. 1996. A randomized approxima-
tion of the mdl for stochastic models with hidden
variables. Proc. of COLT&apos;96, pages 99-109.
Yang, Yiming and Christoper G. Chute. 1994. An
example-based mapping method for text catego-
rization and retrieval. ACM Tran. on Information
Systems, 12(3):252-277.
</bodyText>
<page confidence="0.99932">
47
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.671664">
<title confidence="0.996492">Document Classification Using a Finite Mixture Model</title>
<author confidence="0.983506">Hang Li Kenji Yamanishi</author>
<affiliation confidence="0.681785">C&amp;C Res. Labs., NEC</affiliation>
<address confidence="0.996134">4-1-1 Miyazaki Miyamae-ku Kawasaki, 216, Japan</address>
<email confidence="0.999391">flihang,yamanisil©sbl.cl.nec.co.jp</email>
<abstract confidence="0.999402083333333">We propose a new method of classifying documents into categories. We define for category a mixture model clustering words. We treat the problem of classifying documents as that of conducting statistical hypothesis testing over finite mixture models, and employ the EM algorithm to efficiently estimate parameters in a finite mixture model. Experimental results indicate that our method outperforms existing methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chidanand Apte</author>
<author>Fred Damerau</author>
<author>Sholom M Weiss</author>
</authors>
<title>Automated learning of decision rules for text categorization.</title>
<date>1994</date>
<journal>ACM Tran. on Information Systems,</journal>
<pages>12--3</pages>
<contexts>
<context position="1175" citStr="Apte, Damerau, and Weiss, 1994" startWordPosition="173" endWordPosition="177">ite mixture model. Experimental results indicate that our method outperforms existing methods. 1 Introduction We are concerned here with the issue of classifying documents into categories. More precisely, we begin with a number of categories (e.g., &apos;tennis, soccer, skiing&apos;), each already containing certain documents. Our goal is to determine into which categories newly given documents ought to be assigned, and to do so on the basis of the distribution of each document&apos;s words.&apos; Many methods have been proposed to address this issue, and a number of them have proved to be quite effective (e.g.,(Apte, Damerau, and Weiss, 1994; Cohen and Singer, 1996; Lewis, 1992; Lewis and Ringuette, 1994; Lewis et al., 1996; Schutze, Hull, and Pedersen, 1995; Yang and Chute, 1994)). The simple method of conducting hypothesis testing over word-based distributions in categories (defined in Section 2) is not efficient in storage and suffers from the data sparseness problem, i.e., the number of parameters in the distributions is large and the data size is not sufficiently large for accurately estimating them. In order to address this difficulty, (Guthrie, Walker, and Guthrie, 1994) have proposed using distributions based on what we r</context>
</contexts>
<marker>Apte, Damerau, Weiss, 1994</marker>
<rawString>Apte, Chidanand, Fred Damerau, and Sholom M. Weiss. 1994. Automated learning of decision rules for text categorization. ACM Tran. on Information Systems, 12(3):233-251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
<author>Yoram Singer</author>
</authors>
<title>Context-sensitive learning methods for text categorization.</title>
<date>1996</date>
<booktitle>Proc. of SIGIR&apos;96.</booktitle>
<contexts>
<context position="1199" citStr="Cohen and Singer, 1996" startWordPosition="178" endWordPosition="181">results indicate that our method outperforms existing methods. 1 Introduction We are concerned here with the issue of classifying documents into categories. More precisely, we begin with a number of categories (e.g., &apos;tennis, soccer, skiing&apos;), each already containing certain documents. Our goal is to determine into which categories newly given documents ought to be assigned, and to do so on the basis of the distribution of each document&apos;s words.&apos; Many methods have been proposed to address this issue, and a number of them have proved to be quite effective (e.g.,(Apte, Damerau, and Weiss, 1994; Cohen and Singer, 1996; Lewis, 1992; Lewis and Ringuette, 1994; Lewis et al., 1996; Schutze, Hull, and Pedersen, 1995; Yang and Chute, 1994)). The simple method of conducting hypothesis testing over word-based distributions in categories (defined in Section 2) is not efficient in storage and suffers from the data sparseness problem, i.e., the number of parameters in the distributions is large and the data size is not sufficiently large for accurately estimating them. In order to address this difficulty, (Guthrie, Walker, and Guthrie, 1994) have proposed using distributions based on what we refer to as hard &apos;A relat</context>
</contexts>
<marker>Cohen, Singer, 1996</marker>
<rawString>Cohen, William W. and Yoram Singer. 1996. Context-sensitive learning methods for text categorization. Proc. of SIGIR&apos;96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan T Dumais</author>
<author>George W Furnas</author>
<author>Thomas K Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journ. of the American Society for Information Science,</journal>
<pages>41--6</pages>
<contexts>
<context position="1940" citStr="Deerwester et al., 1990" startWordPosition="296" endWordPosition="299">). The simple method of conducting hypothesis testing over word-based distributions in categories (defined in Section 2) is not efficient in storage and suffers from the data sparseness problem, i.e., the number of parameters in the distributions is large and the data size is not sufficiently large for accurately estimating them. In order to address this difficulty, (Guthrie, Walker, and Guthrie, 1994) have proposed using distributions based on what we refer to as hard &apos;A related issue is the retrieval, from a data base, of documents which are relevant to a given query (pseudodocument) (e.g.,(Deerwester et al., 1990; Fuhr, 1989; Robertson and Jones, 1976; Salton and McGill, 1983; Wong and Yao, 1989)). clustering of words, i.e., in which a word is assigned to a single cluster and words in the same cluster are treated uniformly. The use of hard clustering might, however, degrade classification results, since the distributions it employs are not always precise enough for representing the differences between categories. We propose here to employ soft clustering&apos;, i.e., a word can be assigned to several different clusters and each cluster is characterized by a specific word probability distribution. We define</context>
<context position="21162" citStr="Deerwester et al., 1990" startWordPosition="3764" endWordPosition="3768">method (with -y &gt; 0.5) described in Section 4. As a result, FMM requires less data for parameter estimation than WBM and thus can handle the data sparseness problem quite well. Furthermore, it can economize on the space necessary for storing knowledge. On the other hand, the number of parameters in FMM is larger than that in HCM. It is able to represent the differences between categories more precisely than HCM, and thus is able to resolve the two problems, described in Section 2, which plague HCM. Another advantage of our method may be seen in contrast to the use of latent semantic analysis (Deerwester et al., 1990) in document classification and document retrieval. They claim that their method can solve the following problems: synonymy problem how to group synonyms, like &apos;stroke&apos; and &apos;shot,&apos; and make each relatively strongly indicative of a category even though some may individually appear in the category only very rarely; polysemy problem how to determine that a word like &apos;ball&apos; in a document refers to a &apos;tennis ball&apos; and not a &apos;soccer ball,&apos; so as to classify the document more accurately; dependence problem how to use dependent words, like &apos;kick&apos; and &apos;goal,&apos; to make their combined appearance in a docu</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Deerwester, Scott, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journ. of the American Society for Information Science, 41(6):391-407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the em algorithm.</title>
<date>1977</date>
<journal>Journ. of the Royal Statistical Society, Series B,</journal>
<pages>39--1</pages>
<contexts>
<context position="16574" citStr="Dempster, Laird, and Rubin, 1977" startWordPosition="2926" endWordPosition="2930">the total frequency of clusters in c. Table 6: Clusters and words k1 racket, stroke, shot, ball k2 kick, goal, ball (11) (12) 42 Table 10: Calculating log likelihood values log2 L(d Cl) = log2(.14 x .25) + 2 x log2(.14 x .50) + log2(.86 x .22 + .14 x .25) = —14.67 log2 L(d1c2) = log2(.96 x .25) + 2 x log2(.96 x .50) + log2(.04 x .22 + .96 x .25) = —6.18 Table 9: Probability distributions of clusters k1 kz c1 0.86 0.14 C2 0.04 0.96 and the Bayes estimation method. In their implementation for estimating P(ki Ici), however, both of them suffer from computational intractability. The EM algorithm (Dempster, Laird, and Rubin, 1977) can be used to efficiently approximate the maximum likelihood estimator of P(kj lci). We employ here an extended version of the EM algorithm (Helmbold et al., 1995). (We have also devised, on the basis of the Markov chain Monte Carlo (MCMC) technique (e.g. (Tanner and Wong, 1987; Yamanishi, 1996))6, an algorithm to efficiently approximate the Bayes estimator of P(ki For the sake of notational simplicity, for a fixed i, let us write P(ki lci) as 0, and P(wlki) as Pi(w). Then letting 0 = (01, • • • ,0,,), the finite mixture model in Eq. (6) may be written as 171 PoDio= E0i x PAO- (14) j.1 For </context>
<context position="18264" citStr="Dempster, Laird, and Rubin, 1977" startWordPosition="3243" endWordPosition="3247"> 0(1) = (9(,1) , ,e) by 0(1) = 0(.1-1) (vL(0(1-1))i — 1) + 1) , (16) where &gt; 0 (when zi = 1, Hembold et al. &apos;s version simply becomes the standard EM algorithm), and 6We have confirmed in our preliminary experiment that MCMC performs slightly better than EM in document classification, but we omit the details here due to space limitations. VL(0) denotes VL(0)=(— L(0)= Gel aem) • (17) After s numbers of calculations, the EM algorithm outputs 0(3) = QS:), • • • , OW) as an approximate of O. It is theoretically guaranteed that the EM algorithm converges to a local minimum of the given likelihood (Dempster, Laird, and Rubin, 1977). For the example in Tab. 1, we obtain the results as shown in Tab. 9. Testing For the example in Tab. 1, we can calculate according to Eq. (8) the likelihood values of the two categories with respect to the document in Fig. 1 (Tab. 10 shows the logarithms of the likelihood values). We then classify the document into category c2, as log2 L(dic2) is larger than log2 L(dici). 5 Advantages of FMM For a probabilistic approach to document classification, the most important thing is to determine what kind of probability model (distribution) to employ as a representation of a category. It must (1) a</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Dempster, A.P., N.M. Laird, and D.B. Rubin. 1977. Maximum likelihood from incomplete data via the em algorithm. Journ. of the Royal Statistical Society, Series B, 39(1):1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Everitt</author>
<author>D Hand</author>
</authors>
<date>1981</date>
<booktitle>Finite Mixture Distributions.</booktitle>
<publisher>Chapman and Hall.</publisher>
<location>London:</location>
<contexts>
<context position="11007" citStr="Everitt and Hand, 1981" startWordPosition="1907" endWordPosition="1910">d can be assigned to several different clusters); and (b) define for each cluster kj (j = 1, • • • , m) a distribution Q(wlki) over its words (E.Ek3 Q(wlki) = 1) and a distribution P(wIke) satisfying: Q(wlkj); w E kj, P(Wik.i) = w tit ki, (5) where w denotes a random variable representing any word in the vocabulary. We then define for each category ce (i = 1,• • • , n) a distribution of the clusters P(ki Ice) ,and define for each category a linear combination of P(wlki): P(wIce) = E p(kjici) x p(wiko (6) as the distribution over its words, which is referred to as a finite mixture model(e.g., (Everitt and Hand, 1981)). We treat the problem of classifying a document as that of conducting the likelihood ratio test over finite mixture models. That is, we view a document as a sequence of words, d= wi,•&amp;quot;,wN (7) where wt(t = 1, • , N) represents a word. We assume that each word is independently generated according to an unknown probability distribution and determine which of the finite mixture models P(wIci)(i = 1, • • , n) is more likely to be the probability distribution by observing the sequence of words. Specifically, we calculate the likelihood value for each category with respect to the document by: L(dIc</context>
</contexts>
<marker>Everitt, Hand, 1981</marker>
<rawString>Everitt, B. and D. Hand. 1981. Finite Mixture Distributions. London: Chapman and Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norbert Fuhr</author>
</authors>
<title>Models for retrieval with probabilistic indexing.</title>
<date>1989</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>25--1</pages>
<contexts>
<context position="1952" citStr="Fuhr, 1989" startWordPosition="300" endWordPosition="301">onducting hypothesis testing over word-based distributions in categories (defined in Section 2) is not efficient in storage and suffers from the data sparseness problem, i.e., the number of parameters in the distributions is large and the data size is not sufficiently large for accurately estimating them. In order to address this difficulty, (Guthrie, Walker, and Guthrie, 1994) have proposed using distributions based on what we refer to as hard &apos;A related issue is the retrieval, from a data base, of documents which are relevant to a given query (pseudodocument) (e.g.,(Deerwester et al., 1990; Fuhr, 1989; Robertson and Jones, 1976; Salton and McGill, 1983; Wong and Yao, 1989)). clustering of words, i.e., in which a word is assigned to a single cluster and words in the same cluster are treated uniformly. The use of hard clustering might, however, degrade classification results, since the distributions it employs are not always precise enough for representing the differences between categories. We propose here to employ soft clustering&apos;, i.e., a word can be assigned to several different clusters and each cluster is characterized by a specific word probability distribution. We define for each ca</context>
</contexts>
<marker>Fuhr, 1989</marker>
<rawString>Fuhr, Norbert. 1989. Models for retrieval with probabilistic indexing. Information Processing and Management, 25(1):55-72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Williams A Gale</author>
<author>Kenth W Church</author>
</authors>
<title>Poor estimates of context are worse than none.</title>
<date>1990</date>
<booktitle>Proc. of the DARPA Speech and Natural Language Workshop,</booktitle>
<pages>283--287</pages>
<contexts>
<context position="5495" citStr="Gale and Church, 1990" startWordPosition="882" endWordPosition="885">nto the category for which it has the largest probability is equivalent to classifying it into the category having the largest likelihood with respect to it. Hereafter, we will use only the term likelihood and denote it as L(dici). Notice that in practice the parameters in a distribution must be estimated from training data. In the case of WBM, the number of parameters is large; the training data size, however, is usually not sufficiently large for accurately estimating them. This is the data .sparseness problem that so often stands in the way of reliable statistical language processing (e.g.(Gale and Church, 1990)). Moreover, the number of parameters in word-based distributions is too large to be efficiently stored. Method based on hard clustering In order to address the above difficulty, Guthrie et.al. have proposed a method based on hard clustering of words (Guthrie, Walker, and Guthrie, 1994) (hereafter we will refer to this method as HCM). Let be categories. HCM first conducts hard clustering of words. Specifically, it (a) defines a vocabulary as a set of words W and defines as clusters its subsets kJ., • • , km satisfying Ur_iks = W and ki n k, = 0 (i j) (i.e., each word is assigned only to a sing</context>
<context position="7989" citStr="Gale and Church, 1990" startWordPosition="1351" endWordPosition="1354">ng the L most frequent words in c1, and not among the M most frequent in c2; k2 contains those words which are among the L most frequent words in c2, and not among the M most frequent in c1; and k3 contains all remaining words (see Tab. 2). HCM then counts the frequencies of clusters in each category (see Tab. 3) and estimates the probabilities of clusters being in each category (see Tab. 4).3 Suppose that a newly given document, like d in Fig. 1, is to be classified. HCM calculates the likelihood values 3We calculate the probabilities here by using the socalled expected likelihood estimator (Gale and Church, 1990): P(k = f I c, ) +0.5 4 ,l,) ( c ) f (c,)± 0.5 x m where f(kIlc,) is the frequency of the cluster k, in c1, f (c,) is the total frequency of clusters in c„ and in is the total number of clusters. 40 Table 4: Probability distributions of clusters k1 k2 k3 c1 0.65 0.04 0.30 c2 0.06 0.29 0.65 L(clIci) and L(dIc2) according to Eq. (3). (Tab. 5 shows the logarithms of the resulting likelihood values.) It then classifies d into c2, as log2 L(dIc2) is larger than log2 L(dici)• d = kick, goal, goal, ball Figure 1: Example document HCM can handle the data sparseness problem quite well. By assigning wor</context>
</contexts>
<marker>Gale, Church, 1990</marker>
<rawString>Gale, Williams A. and Kenth W. Church. 1990. Poor estimates of context are worse than none. Proc. of the DARPA Speech and Natural Language Workshop, pages 283-287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Louise Guthrie</author>
<author>Elbert Walker</author>
<author>Joe Guthrie</author>
</authors>
<title>Document classification by machine: Theory and practice.</title>
<date>1994</date>
<booktitle>Proc. of COLING&apos;94,</booktitle>
<pages>1059--1063</pages>
<contexts>
<context position="1721" citStr="Guthrie, Walker, and Guthrie, 1994" startWordPosition="258" endWordPosition="262">a number of them have proved to be quite effective (e.g.,(Apte, Damerau, and Weiss, 1994; Cohen and Singer, 1996; Lewis, 1992; Lewis and Ringuette, 1994; Lewis et al., 1996; Schutze, Hull, and Pedersen, 1995; Yang and Chute, 1994)). The simple method of conducting hypothesis testing over word-based distributions in categories (defined in Section 2) is not efficient in storage and suffers from the data sparseness problem, i.e., the number of parameters in the distributions is large and the data size is not sufficiently large for accurately estimating them. In order to address this difficulty, (Guthrie, Walker, and Guthrie, 1994) have proposed using distributions based on what we refer to as hard &apos;A related issue is the retrieval, from a data base, of documents which are relevant to a given query (pseudodocument) (e.g.,(Deerwester et al., 1990; Fuhr, 1989; Robertson and Jones, 1976; Salton and McGill, 1983; Wong and Yao, 1989)). clustering of words, i.e., in which a word is assigned to a single cluster and words in the same cluster are treated uniformly. The use of hard clustering might, however, degrade classification results, since the distributions it employs are not always precise enough for representing the diff</context>
<context position="5781" citStr="Guthrie, Walker, and Guthrie, 1994" startWordPosition="927" endWordPosition="931">a distribution must be estimated from training data. In the case of WBM, the number of parameters is large; the training data size, however, is usually not sufficiently large for accurately estimating them. This is the data .sparseness problem that so often stands in the way of reliable statistical language processing (e.g.(Gale and Church, 1990)). Moreover, the number of parameters in word-based distributions is too large to be efficiently stored. Method based on hard clustering In order to address the above difficulty, Guthrie et.al. have proposed a method based on hard clustering of words (Guthrie, Walker, and Guthrie, 1994) (hereafter we will refer to this method as HCM). Let be categories. HCM first conducts hard clustering of words. Specifically, it (a) defines a vocabulary as a set of words W and defines as clusters its subsets kJ., • • , km satisfying Ur_iks = W and ki n k, = 0 (i j) (i.e., each word is assigned only to a single cluster); and (b) treats uniformly all the words assigned to the same cluster. HCM then defines for each category ci a distribution of the clusters P(ki lci) (j = 1, • • • , m). It replaces each word wt in the document with the cluster kt to which it belongs (t = 1,- • ,N). It assum</context>
</contexts>
<marker>Guthrie, Walker, Guthrie, 1994</marker>
<rawString>Guthrie, Louise, Elbert Walker, and Joe Guthrie. 1994. Document classification by machine: Theory and practice. Proc. of COLING&apos;94, pages 1059-1063.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Helmbold</author>
<author>R Schapire</author>
<author>Y Siuger</author>
<author>M Warmuth</author>
</authors>
<title>A comparison of new and old algorithm for a mixture estimation problem.</title>
<date>1995</date>
<booktitle>Proc. of COLT&apos;95,</booktitle>
<pages>61--68</pages>
<contexts>
<context position="16740" citStr="Helmbold et al., 1995" startWordPosition="2954" endWordPosition="2957">2 L(d Cl) = log2(.14 x .25) + 2 x log2(.14 x .50) + log2(.86 x .22 + .14 x .25) = —14.67 log2 L(d1c2) = log2(.96 x .25) + 2 x log2(.96 x .50) + log2(.04 x .22 + .96 x .25) = —6.18 Table 9: Probability distributions of clusters k1 kz c1 0.86 0.14 C2 0.04 0.96 and the Bayes estimation method. In their implementation for estimating P(ki Ici), however, both of them suffer from computational intractability. The EM algorithm (Dempster, Laird, and Rubin, 1977) can be used to efficiently approximate the maximum likelihood estimator of P(kj lci). We employ here an extended version of the EM algorithm (Helmbold et al., 1995). (We have also devised, on the basis of the Markov chain Monte Carlo (MCMC) technique (e.g. (Tanner and Wong, 1987; Yamanishi, 1996))6, an algorithm to efficiently approximate the Bayes estimator of P(ki For the sake of notational simplicity, for a fixed i, let us write P(ki lci) as 0, and P(wlki) as Pi(w). Then letting 0 = (01, • • • ,0,,), the finite mixture model in Eq. (6) may be written as 171 PoDio= E0i x PAO- (14) j.1 For a given training sequence w1 • • wN, the maximum likelihood estimator of 9 is defined as the value 0 which maximizes the following log likelihood function L(0) = --N-</context>
</contexts>
<marker>Helmbold, Schapire, Siuger, Warmuth, 1995</marker>
<rawString>Helmbold, D., R. Schapire, Y. Siuger, and M. Warmuth. 1995. A comparison of new and old algorithm for a mixture estimation problem. Proc. of COLT&apos;95, pages 61-68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
<author>R I Mercer</author>
</authors>
<title>Interpolated estimation of markov source parameters from sparse data.</title>
<date>1980</date>
<booktitle>Proc. of Workshop on Pattern Recognition in Practice,</booktitle>
<pages>381--402</pages>
<contexts>
<context position="3414" citStr="Jelinek and Mercer, 1980" startWordPosition="525" endWordPosition="528">ture models. In order to accomplish hypothesis testing, we employ the EM algorithm to efficiently and approximately calculate from training data the maximum likelihood estimates of parameters in a finite mixture model. Our method overcomes the major drawbacks of the method using word-based distributions and the method based on hard clustering, while retaining their merits; it in fact includes those two methods as special cases. Experimental results indicate that our method outperforms them. Although the finite mixture model has already been used elsewhere in natural language processing (e.g. (Jelinek and Mercer, 1980; Pereira, Tishby, and Lee, 1993)), this is the first work, to the best of knowledge, that uses it in the context of document classification. 2 Previous Work Word-based method A simple approach to document classification is to view this problem as that of conducting hypothesis testing over word-based distributions. In this paper, we refer to this approach as the word-based method (hereafter, referred to as WBM). 2We borrow from (Pereira, Tishby, and Lee, 1993) the terms hard clustering and soft clustering, which were used there in a different task. 39 Letting W denote a vocabulary (a set of wo</context>
</contexts>
<marker>Jelinek, Mercer, 1980</marker>
<rawString>Jelinek, F. and R.I. Mercer. 1980. Interpolated estimation of markov source parameters from sparse data. Proc. of Workshop on Pattern Recognition in Practice, pages 381-402.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
</authors>
<title>An evaluation of phrasal and clustered representations on a text categorization task.</title>
<date>1992</date>
<booktitle>Proc. of SIGIR&apos;92,</booktitle>
<pages>37--50</pages>
<contexts>
<context position="1212" citStr="Lewis, 1992" startWordPosition="182" endWordPosition="183">r method outperforms existing methods. 1 Introduction We are concerned here with the issue of classifying documents into categories. More precisely, we begin with a number of categories (e.g., &apos;tennis, soccer, skiing&apos;), each already containing certain documents. Our goal is to determine into which categories newly given documents ought to be assigned, and to do so on the basis of the distribution of each document&apos;s words.&apos; Many methods have been proposed to address this issue, and a number of them have proved to be quite effective (e.g.,(Apte, Damerau, and Weiss, 1994; Cohen and Singer, 1996; Lewis, 1992; Lewis and Ringuette, 1994; Lewis et al., 1996; Schutze, Hull, and Pedersen, 1995; Yang and Chute, 1994)). The simple method of conducting hypothesis testing over word-based distributions in categories (defined in Section 2) is not efficient in storage and suffers from the data sparseness problem, i.e., the number of parameters in the distributions is large and the data size is not sufficiently large for accurately estimating them. In order to address this difficulty, (Guthrie, Walker, and Guthrie, 1994) have proposed using distributions based on what we refer to as hard &apos;A related issue is t</context>
</contexts>
<marker>Lewis, 1992</marker>
<rawString>Lewis, David D. 1992. An evaluation of phrasal and clustered representations on a text categorization task. Proc. of SIGIR&apos;92, pages 37-50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>Marc Ringuette</author>
</authors>
<title>A comparison of two learning algorithms for test categorization.</title>
<date>1994</date>
<booktitle>Proc. of 3rd Annual Symposium on Document Analysis and Information Retrieval,</booktitle>
<pages>81--93</pages>
<contexts>
<context position="1239" citStr="Lewis and Ringuette, 1994" startWordPosition="184" endWordPosition="187">erforms existing methods. 1 Introduction We are concerned here with the issue of classifying documents into categories. More precisely, we begin with a number of categories (e.g., &apos;tennis, soccer, skiing&apos;), each already containing certain documents. Our goal is to determine into which categories newly given documents ought to be assigned, and to do so on the basis of the distribution of each document&apos;s words.&apos; Many methods have been proposed to address this issue, and a number of them have proved to be quite effective (e.g.,(Apte, Damerau, and Weiss, 1994; Cohen and Singer, 1996; Lewis, 1992; Lewis and Ringuette, 1994; Lewis et al., 1996; Schutze, Hull, and Pedersen, 1995; Yang and Chute, 1994)). The simple method of conducting hypothesis testing over word-based distributions in categories (defined in Section 2) is not efficient in storage and suffers from the data sparseness problem, i.e., the number of parameters in the distributions is large and the data size is not sufficiently large for accurately estimating them. In order to address this difficulty, (Guthrie, Walker, and Guthrie, 1994) have proposed using distributions based on what we refer to as hard &apos;A related issue is the retrieval, from a data b</context>
<context position="26667" citStr="Lewis and Ringuette, 1994" startWordPosition="4727" endWordPosition="4730">ns the highest score at break-even point; for the second data set, FMM0.5 attains the highest. We considered the following questions: (1) The training data used in the experimentation may be considered sparse. Will a wordclustering-based method (FMM) outperform a wordbased method (WBM) here? (2) Is it better to conduct soft clustering (FMM) than to do hard clustering (HCM)? (3) With our current method of creating clusters, as the threshold -y approaches 0, FMM behaves much like WBM and it does not enjoy the effects of clustering at all (the number of parameters is as large &amp;quot;In micro-averaging(Lewis and Ringuette, 1994), precision is defined as the percentage of classified documents in all categories which are correctly classified. Recall is defined as the percentage of the total documents in all categories which are correctly classified. 11Notice that words which are discarded in the clustering process should not to be counted in document size. Figure 2: Precision-recall curve for the first data set 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 Figure 3: Precision-recall curve for the second data set as in WBM). This is because in this case (a) a word will be assigned into all of the clusters, (b) the distribution of</context>
<context position="29761" citStr="Lewis and Ringuette, 1994" startWordPosition="5292" endWordPosition="5295"> for the second data set. In summary, FMM always outperforms HCM; in some cases it performs better than WBM; and in general it performs at least as well as WBM. For both data sets, the best FMM results are superior to those of COS throughout. This indicates that the probabilistic approach is more suitable than the cosine approach for document classification based on word distributions. Although we have not completed our experiments on the entire Reuters data set, we found that the results with FMM on the second data set are almost as good as those obtained by the other approaches reported in (Lewis and Ringuette, 1994). (The results are not directly comparable, because (a) the results in (Lewis and Ringuette, 1994) were obtained from an older version of the Reuters data; and (b) they Figure 4: Precision-recall curve for category &apos;corn&apos; 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Figure 5: Precision-recall curve for category &apos;grain&apos; used stop words, but we did not.) We have also conducted experiments on the Susanne corpus data&apos; and confirmed the effectiveness of our method. We omit an explanation of this work here due to space limitations. 7 Conclusions Let us conclude this paper with the following remarks: 1. The prima</context>
</contexts>
<marker>Lewis, Ringuette, 1994</marker>
<rawString>Lewis, David D. and Marc Ringuette. 1994. A comparison of two learning algorithms for test categorization. Proc. of 3rd Annual Symposium on Document Analysis and Information Retrieval, pages 81-93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>Robert E Schapire</author>
<author>James P Callan</author>
<author>Ron Papka</author>
</authors>
<title>Training algorithms for linear text classifiers.</title>
<date>1996</date>
<booktitle>Proc. of SIGIR&apos;96.</booktitle>
<contexts>
<context position="1259" citStr="Lewis et al., 1996" startWordPosition="188" endWordPosition="191"> Introduction We are concerned here with the issue of classifying documents into categories. More precisely, we begin with a number of categories (e.g., &apos;tennis, soccer, skiing&apos;), each already containing certain documents. Our goal is to determine into which categories newly given documents ought to be assigned, and to do so on the basis of the distribution of each document&apos;s words.&apos; Many methods have been proposed to address this issue, and a number of them have proved to be quite effective (e.g.,(Apte, Damerau, and Weiss, 1994; Cohen and Singer, 1996; Lewis, 1992; Lewis and Ringuette, 1994; Lewis et al., 1996; Schutze, Hull, and Pedersen, 1995; Yang and Chute, 1994)). The simple method of conducting hypothesis testing over word-based distributions in categories (defined in Section 2) is not efficient in storage and suffers from the data sparseness problem, i.e., the number of parameters in the distributions is large and the data size is not sufficiently large for accurately estimating them. In order to address this difficulty, (Guthrie, Walker, and Guthrie, 1994) have proposed using distributions based on what we refer to as hard &apos;A related issue is the retrieval, from a data base, of documents wh</context>
</contexts>
<marker>Lewis, Schapire, Callan, Papka, 1996</marker>
<rawString>Lewis, David D., Robert E. Schapire, James P. Callan, and Ron Papka. 1996. Training algorithms for linear text classifiers. Proc. of SIGIR&apos;96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Naftali Tishby</author>
<author>Lillian Lee</author>
</authors>
<title>Distributional clustering of english words.</title>
<date>1993</date>
<booktitle>Proc. of ACL&apos;93,</booktitle>
<pages>183--190</pages>
<contexts>
<context position="3446" citStr="Pereira, Tishby, and Lee, 1993" startWordPosition="529" endWordPosition="533">ccomplish hypothesis testing, we employ the EM algorithm to efficiently and approximately calculate from training data the maximum likelihood estimates of parameters in a finite mixture model. Our method overcomes the major drawbacks of the method using word-based distributions and the method based on hard clustering, while retaining their merits; it in fact includes those two methods as special cases. Experimental results indicate that our method outperforms them. Although the finite mixture model has already been used elsewhere in natural language processing (e.g. (Jelinek and Mercer, 1980; Pereira, Tishby, and Lee, 1993)), this is the first work, to the best of knowledge, that uses it in the context of document classification. 2 Previous Work Word-based method A simple approach to document classification is to view this problem as that of conducting hypothesis testing over word-based distributions. In this paper, we refer to this approach as the word-based method (hereafter, referred to as WBM). 2We borrow from (Pereira, Tishby, and Lee, 1993) the terms hard clustering and soft clustering, which were used there in a different task. 39 Letting W denote a vocabulary (a set of words), and w denote a random vari</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>Pereira, Fernando, Naftali Tishby, and Lillian Lee. 1993. Distributional clustering of english words. Proc. of ACL&apos;93, pages 183-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Robertson</author>
<author>K Sparck Jones</author>
</authors>
<title>Relevance weighting of search terms.</title>
<date>1976</date>
<journal>Journ. of the American Society for Information Science,</journal>
<pages>27--129</pages>
<contexts>
<context position="1979" citStr="Robertson and Jones, 1976" startWordPosition="302" endWordPosition="305">pothesis testing over word-based distributions in categories (defined in Section 2) is not efficient in storage and suffers from the data sparseness problem, i.e., the number of parameters in the distributions is large and the data size is not sufficiently large for accurately estimating them. In order to address this difficulty, (Guthrie, Walker, and Guthrie, 1994) have proposed using distributions based on what we refer to as hard &apos;A related issue is the retrieval, from a data base, of documents which are relevant to a given query (pseudodocument) (e.g.,(Deerwester et al., 1990; Fuhr, 1989; Robertson and Jones, 1976; Salton and McGill, 1983; Wong and Yao, 1989)). clustering of words, i.e., in which a word is assigned to a single cluster and words in the same cluster are treated uniformly. The use of hard clustering might, however, degrade classification results, since the distributions it employs are not always precise enough for representing the differences between categories. We propose here to employ soft clustering&apos;, i.e., a word can be assigned to several different clusters and each cluster is characterized by a specific word probability distribution. We define for each category a finite mixture mod</context>
</contexts>
<marker>Robertson, Jones, 1976</marker>
<rawString>Robertson, S.E. and K. Sparck Jones. 1976. Relevance weighting of search terms. Journ. of the American Society for Information Science, 27:129-146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGraw Hill.</publisher>
<location>New York:</location>
<contexts>
<context position="2004" citStr="Salton and McGill, 1983" startWordPosition="306" endWordPosition="309">based distributions in categories (defined in Section 2) is not efficient in storage and suffers from the data sparseness problem, i.e., the number of parameters in the distributions is large and the data size is not sufficiently large for accurately estimating them. In order to address this difficulty, (Guthrie, Walker, and Guthrie, 1994) have proposed using distributions based on what we refer to as hard &apos;A related issue is the retrieval, from a data base, of documents which are relevant to a given query (pseudodocument) (e.g.,(Deerwester et al., 1990; Fuhr, 1989; Robertson and Jones, 1976; Salton and McGill, 1983; Wong and Yao, 1989)). clustering of words, i.e., in which a word is assigned to a single cluster and words in the same cluster are treated uniformly. The use of hard clustering might, however, degrade classification results, since the distributions it employs are not always precise enough for representing the differences between categories. We propose here to employ soft clustering&apos;, i.e., a word can be assigned to several different clusters and each cluster is characterized by a specific word probability distribution. We define for each category a finite mixture model, which is a linear com</context>
<context position="24453" citStr="Salton and McGill, 1983" startWordPosition="4342" endWordPosition="4345">hen applying FMM, we used our proposed method of creating clusters and set -y to be 0, 0.4, 0.5, 0.7. For HCM, we classified words in the same way as in FMM and set 7 to be 0.5,0.7, 0.9, 0.95. We have not fully completed these experiments, however, and here we only &apos;Stop words&apos; refers to a predetermined list of words containing those which are considered not useful for document classification, such as articles and prepositions. 91n this method, categories and documents to be classified are viewed as vectors of word frequencies, and the cosine value between the two vectors reflects similarity (Salton and McGill, 1983). Table 11: Num. of parameters WBM 0(n • IWI) HCM 0(n • m) FMM O(ikl+ n • m) 44 Table 15: Tested categories in the second data set earn,acq,crude,money-fx,grain interest ,trade,ship,wheat ,corn give the results of classifying into the ten categories having the greatest numbers of documents in the test data (see Tab. 15). For both data sets, we evaluated each method in terms of precision and recall by means of the socalled micro-averaging When applying WBM, HCM, and FMM, rather than use the standard likelihood ratio testing, we used the following heuristics. For simplicity, suppose that there a</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>Salton, G. and M.J. McGill. 1983. Introduction to Modern Information Retrieval. New York: McGraw Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schutze</author>
<author>David A Hull</author>
<author>Jan</author>
</authors>
<title>A comparison of classifiers and document representations for the routing problem.</title>
<date>1995</date>
<booktitle>Proc. of SIGIR&apos;95.</booktitle>
<marker>Schutze, Hull, Jan, 1995</marker>
<rawString>Schutze, Hinrich, David A. Hull, and Jan 0. Pedersen. 1995. A comparison of classifiers and document representations for the routing problem. Proc. of SIGIR&apos;95.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>