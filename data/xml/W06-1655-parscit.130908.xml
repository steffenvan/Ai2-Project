<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000306">
<title confidence="0.999716">
A Hybrid Markov/Semi-Markov Conditional Random Field
for Sequence Segmentation
</title>
<author confidence="0.988319">
Galen Andrew
</author>
<affiliation confidence="0.962439">
Microsoft Research
</affiliation>
<address confidence="0.945921">
One Microsoft Way
Redmond, WA 98052
</address>
<email confidence="0.998456">
galena@microsoft.com
</email>
<sectionHeader confidence="0.997371" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999733227272727">
Markov order-1 conditional random fields
(CRFs) and semi-Markov CRFs are two
popular models for sequence segmenta-
tion and labeling. Both models have ad-
vantages in terms of the type of features
they most naturally represent. We pro-
pose a hybrid model that is capable of rep-
resenting both types of features, and de-
scribe efficient algorithms for its training
and inference. We demonstrate that our
hybrid model achieves error reductions of
18% and 25% over a standard order-1 CRF
and a semi-Markov CRF (resp.) on the
task of Chinese word segmentation. We
also propose the use of a powerful fea-
ture for the semi-Markov CRF: the log
conditional odds that a given token se-
quence constitutes a chunk according to
a generative model, which reduces error
by an additional 13%. Our best system
achieves 96.8% F-measure, the highest re-
ported score on this test set.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999965717391305">
The problem of segmenting sequence data into
chunks arises in many natural language applica-
tions, such as named-entity recognition, shallow
parsing, and word segmentation in East Asian lan-
guages. Two popular discriminative models that
have been proposed for these tasks are the condi-
tional random field (CRFs) (Lafferty et al., 2001)
and the semi-Markov conditional random field
(semi-CRF) (Sarawagi and Cohen, 2004).
A CRF in its basic form is a model for label-
ing tokens in a sequence; however it can easily
be adapted to perform segmentation via labeling
each token as BEGIN or CONTINUATION, or accord-
ing to some similar scheme. CRFs using this tech-
nique have been shown to be very successful at the
task of Chinese word segmentation (CWS), start-
ing with the model of Peng et al. (2004). In the
Second International Chinese Word Segmentation
Bakeoff (Emerson, 2005), two of the highest scor-
ing systems in the closed track competition were
based on a CRF model. (Tseng et al., 2005; Asa-
hara et al., 2005)
While the CRF is quite effective compared with
other models designed for CWS, one wonders
whether it may be limited by its restrictive inde-
pendence assumptions on non-adjacent labels: an
order-M CRF satisfies the order-M Markov as-
sumption that, globally conditioned on the input
sequence, each label is independent of all other
labels given the M labels to its left and right.
Consequently, the model only “sees” word bound-
aries within a moving window of M + 1 charac-
ters, which prohibits it from explicitly modeling
the tendency of strings longer than that window
to form words, or from modeling the lengths of
the words. Although the window can in principle
be widened by increasing M, this is not a practi-
cal solution as the complexity of training and de-
coding a linear sequence CRF grows exponentially
with the Markov order.
The semi-CRF is a sequence model that is de-
signed to address this difficulty via careful relax-
ation of the Markov assumption. Rather than re-
casting the segmentation problem as a labeling
problem, the semi-CRF directly models the dis-
tribution of chunk boundaries.1 In terms of inde-
</bodyText>
<footnote confidence="0.9894125">
1As it was originally described, the semi-CRF also as-
signs labels to each chunk, effectively performing joint seg-
mentation and labeling, but in a pure segmentation problem
such as CWS, the use of labels is unnecessary.
</footnote>
<page confidence="0.980377">
465
</page>
<note confidence="0.8560305">
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 465–472,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999928816901408">
pendence, using an order-M semi-CRF entails the
assumption that, globally conditioned on the input
sequence, the position of each chunk boundary is
independent of all other boundaries given the po-
sitions of the M boundaries to its left and right
regardless of how far away they are. Even with an
order-1 model, this enables several classes of fea-
tures that one would expect to be of great utility
to the word segmentation task, in particular word
length and word identity.
Despite this, the only work of which we are
aware exploring the use of a semi-Markov CRF
for Chinese word segmentation did not find signif-
icant gains over the standard CRF (Liang, 2005).
This is surprising, not only because the additional
features a semi-CRF enables are intuitively very
useful, but because as we will show, an order-M
semi-CRF is strictly more powerful than an or-
der-M CRF, in the sense that any feature that can
be used in the latter can also be used in the for-
mer, or equivalently, the semi-CRF makes strictly
weaker independence assumptions. Given a judi-
cious choice of features (or simply enough training
data) the semi-CRF should be superior.
We propose that the reason for this discrepancy
may be that despite the greater representational
power of the semi-CRF, there are some valuable
features that are more naturally expressed in a
CRF segmentation model, and so they are not typ-
ically included in semi-CRFs (indeed, they have
not to date been used in any semi-CRF model for
any task, to our knowledge). In this paper, we
show that semi-CRFs are strictly more expressive,
and also demonstrate how CRF-type features can
be used in a semi-CRF model for Chinese word
segmentation. Our experiments show that a model
incorporating both types of features can outper-
form models using only one or the other type.
Orthogonally, we explore in this paper the use
of a very powerful feature for the semi-CRF de-
rived from a generative model.
It is common in statistical NLP to use as fea-
tures in a discriminative model the (logarithm of
the) estimated probability of some event accord-
ing to a generative model. For example, Collins
(2000) uses a discriminative classifier for choosing
among the top N parse trees output by a generative
baseline model, and uses the log-probability of a
parse according to the baseline model as a feature
in the reranker. Similarly, the machine translation
system of Och and Ney uses log-probabilities of
phrasal translations and other events as features in
a log-linear model (Och and Ney, 2002; Och and
Ney, 2004). There are many reasons for incorpo-
rating these types of features, including the desire
to combine the higher accuracy of a discriminative
model with the simple parameter estimation and
inference of a generative one, and also the fact that
generative models are more robust in data sparse
scenarios (Ng and Jordan, 2001).
For word segmentation, one might want to use
as a local feature the log-probability that a segment
is a word, given the character sequence it spans. A
curious property of this feature is that it induces
a counterintuitive asymmetry between the is-word
and is-not-word cases: the component generative
model can effectively dictate that a certain chunk
is not a word, by assigning it a very low probability
(driving the feature value to negative infinity), but
it cannot dictate that a chunk is a word, because
the log-probability is bounded above.2 If instead
</bodyText>
<equation confidence="0.7339985">
the log conditional odds log P����&apos;)
P�����&apos;) is used, the
</equation>
<bodyText confidence="0.9996875">
asymmetry disappears. We show that such a log-
odds feature provides much greater benefit than
the log-probability, and that it is useful to include
such a feature even when the model also includes
indicator function features for every word in the
training corpus.
</bodyText>
<sectionHeader confidence="0.957695" genericHeader="method">
2 Hybrid Markov/Semi-Markov CRF
</sectionHeader>
<bodyText confidence="0.9908995">
The model we describe is formally a type of semi-
Markov CRF, distinguished only in that it also in-
volves CRF-style features. So we first describe the
semi-Markov model in its general form.
</bodyText>
<subsectionHeader confidence="0.919843">
2.1 Semi-Markov CRF
</subsectionHeader>
<bodyText confidence="0.999294625">
An (unlabeled) semi-Markov conditional random
field is a log-linear model defining the conditional
probability of a segmentation given an observation
sequence. The general form of a log-linear model
is as follows: given an input x E X, an output
y E Y , a feature mapping 4b: X x Y H R&apos;, and
a weight vector w, the conditional probability of
y given x is estimated as:
</bodyText>
<equation confidence="0.952235333333333">
exp (w &apos; �(x, y))
P(y  |x) �
Z(x)
</equation>
<bodyText confidence="0.986795">
where Z : x H R is a normalizing factor. w
is typically chosen to maximize the conditional
likelihood of a labeled training set. In the word
</bodyText>
<footnote confidence="0.9887325">
2We assume the weight assigned to the log-probability
feature is positive.
</footnote>
<page confidence="0.995738">
466
</page>
<equation confidence="0.5238215">
segmentation task, x is an ordered sequence of
characters (x1, x2, ... , xn), and y is a set of in-
</equation>
<bodyText confidence="0.9849784">
dices corresponding to the start of each word:
{y1, y2,.. . , ym} such that y1 = 1, ym &lt; n, and
for all j, yj &lt; yj+1. A log-linear model in this
space is an order-1 semi-CRF if its feature map 4b
decomposes according to
</bodyText>
<equation confidence="0.9980645">
-b(x, y) = �m 0S(yj,yj+1,x) (1)
j=1
</equation>
<bodyText confidence="0.987158289473684">
where 0S is a local feature map that only considers
one chunk at a time (defining ym+1 = n+1). This
decomposition is responsible for the characteristic
independence assumptions of the semi-CRF.
Hand-in-hand with the feature decomposition
and independence assumptions comes the capac-
ity for exact decoding using the Viterbi algorithm,
and exact computation of the objective gradient
using the forward-backward algorithm, both in
time quadratic in the lengths of the sentences.
Furthermore, if the model is constrained to pro-
pose only chunkings with maximum word length
k, then the time for inference and training be-
comes linear in the sentence length (and in k). For
Chinese word segmentation, choosing a moderate
value of k does not pose any significant risk, since
the vast majority of Chinese words are only a few
characters long: in our training set, 91% of word
tokens were one or two characters, and 99% were
five characters or less.
Using a semi-CRF as opposed to a traditional
Markov CRF allows us to model some aspects
of word segmentation that one would expect to
be very informative. In particular, it makes pos-
sible the use of local indicator function features
of the type “the chunk consists of character se-
quence X1, ... , Xt,” or “the chunk is of length E.”
It also enables “pseudo-bigram language model”
features, firing when a given word occurs in the
context of a given character unigram or bigram.3
And crucially, although it is slightly less natural
to do so, any feature used in an order-1 Markov
CRF can also be represented in a semi-CRF. As
Markov CRFs are used in the most competitive
Chinese word segmentation models to date, one
might expect that incorporating both types of fea-
tures could yield a superior model.
3We did not experiment with this type of feature.
</bodyText>
<subsectionHeader confidence="0.987924">
2.2 CRF vs. Semi-CRF
</subsectionHeader>
<bodyText confidence="0.9998435">
In order to compare the two types of linear CRFs,
it is convenient to define a representation of the
segmentation problem in terms of character labels
as opposed to sets of whole words. Denote by
L(y) E {B, C}n (for BEGIN vs. CONTINUATION)
the sequence {L1, L2,... Ln} of labels such that
Li = B if and only if yi E y. It is clear that if we
constrain L1 = B, the two representations y and
L(y) are equivalent. An order-1 Markov CRF is a
log-linear model in which the global feature vector
-b decomposes into a sum over local feature vec-
tors that consider bigrams of the label sequence:
</bodyText>
<equation confidence="0.992795333333333">
n
-b(x, y) = 0M(Li, Li+1, i, x) (2)
i=1
</equation>
<bodyText confidence="0.999222857142857">
(where Ln+1 is defined as B). The local features
that are most naturally expressed in this context
are indicators of some joint event of the label bi-
gram (Li, Li+1) and nearby characters in x. For
example, one might use the feature “the current
character xi is X and Li = C”, or “the current and
next characters are identical and Li = Li+1 = B.”
Although we have heretofore disparaged the
CRF as being incapable of representing such pow-
erful features as word identity, the type of features
that it most naturally represents should be help-
ful in CWS for generalizing to unseen words. For
example, the first feature mentioned above could
be valuable to rule out certain word boundaries if
X were a character that typically occurs only as a
suffix but that combines freely with a variety of
root forms to create new words. This type of fea-
ture (specifically, a feature indicating the absence
as opposed to the presence of a chunk boundary)
is a bit less natural in a semi-CRF, since in that
case local features 0S(yj, yj+1, x) are defined on
pairs of adjacent boundaries. Information about
which tokens are not on boundaries is only im-
plicit, making it a bit more difficult to incorporate
that information into the features. Indeed, neither
Liang (2005) nor Sarawagi and Cohen (2004) nor
any other system using a semi-Markov CRF on
any task has included this type of feature to our
knowledge. We hypothesize (and our experiments
confirm) that the lack of this feature explains the
failure of the semi-CRF to outperform the CRF for
word segmentation in the past.
Before showing how CRF-type features can be
used in a semi-CRF, we first demonstrate that the
semi-CRF is indeed strictly more expressive than
</bodyText>
<page confidence="0.99635">
467
</page>
<bodyText confidence="0.9970615">
the CRF, meaning that any global feature map q
that decomposes according to (2) also decomposes
according to (1). It is sufficient to show that for
any feature map qM of a Markov CRF, there exists
a semi-Markov-type feature map qS such that for
any x, y,
</bodyText>
<equation confidence="0.9895082">
n
qM(x, y) = 0M(Li, Li+1, i, x) (3)
i=1
�m 0S(yj, yj+1, x) = qS(x, y)
j=1
</equation>
<bodyText confidence="0.9997979">
To this end, note that there are only four possible
label bigrams: BB, BC, CB, and CC. As a di-
rect result of the definition of L(y), we have that
(Li, Li+1) = (B, B) if and only if some word of
length one begins at i, or equivalently, there exists
a word j such that yj = i and yj+1 −yj = 1. Sim-
ilarly, (Li, Li+1) = (B, C) if and only if some
word of length &gt; 1 begins at i, etc. Using these
conditions, we can define 0S to satisfy equation 3
as follows:
</bodyText>
<equation confidence="0.9974215">
0S(yj, yj+1, x) = 0M(B, B, yj, x)
if yj+1 −(yj = 1, and
0S (yj, yj+1, x) = 0M(B, C, yj, x)
+ yj+1−2
� 0M(C, C, k, x) (4)
k=yj+1
</equation>
<bodyText confidence="0.8821085">
+ 0M(C, B, yj+1 − 1, x)
otherwise. Defined thus, Em j=1 0S will contain ex-
actly n 0M terms, corresponding to the n label bi-
grams.4
</bodyText>
<subsectionHeader confidence="0.994298">
2.3 Order-1 Markov Features in a Semi-CRF
</subsectionHeader>
<bodyText confidence="0.9051345">
While it is fairly intuitive that any feature used in a
1-CRF can also be used in a semi-CRF, the above
argument reveals an algorithmic difficulty that is
likely another reason that such features are not typ-
ically used. The problem is essentially an effect of
the sum for CC label bigrams in (4): quadratic
time training and decoding assumes that the fea-
tures of each chunk 0S(yj, yj+1, x) can be multi-
plied with the weight vector w in a number of op-
erations that is roughly constant over all chunks,
4We have discussed the case of Markov order-1, but the
argument can be generalized to show that an order-M CRF
has an equivalent representation as an order-M semi-CRF,
for any M.
</bodyText>
<equation confidence="0.945836214285714">
procedure Compute5cores(x, w)
for i = 2 ... (n − 1) do
CrCC i← 0M(C, C, i, x) · w
end for
fora = 1 ... n do
CCsum ← 0
for b = (a + 1) ... (n + 1) do
if b − a = 1 then
Crab ← 0M(B, B, a, x) · w
else
Crab ← 0M(B, C, a, x) · w + CCsum
+0M(C, B, b − 1, x) · w
CCsum ← CCsum + CrCC
b−1
</equation>
<listItem confidence="0.690242">
end if
end for
end for
</listItem>
<figureCaption confidence="0.996141">
Figure 1: Dynamic program for computing chunk
scores Crab with 1-CRF-type features.
</figureCaption>
<bodyText confidence="0.998135692307692">
but if one naively distributes the product over the
sum, longer chunks will take proportionally longer
to score, resulting in cubic time algorithms.5
In fact, it is possible to use these features
without any asymptotic decrease in efficiency by
means of a dynamic program. Both Viterbi and
forward-backward involve the scores Crab = w ·
0S(a, b, x). Suppose that before starting those al-
gorithms, we compute and cache the score Crab of
each chunk, so that remainder the algorithm runs
in quadratic time, as usual. This pre-computation
can be done quickly if we first compute the values
CrCC i= w · 0M(C, C, i, x), and use them to fill in
the values of Crab as shown in Figure 1.
In addition, computing the gradient of the semi-
CRF objective requires that we compute the ex-
pected value of each feature. For CRF-type fea-
tures, this is tantamount to being able to compute
the probability that each label bigram (Li, Li+1)
takes any value. Assume that we have already run
standard forward-backward inference so that we
have for any (a, b) the probability that the subse-
quence (xa, xa+1, ... , xb−1) segments as a chunk,
P(chunk(a, b)). Computing the probability that
(Li, Li+1) takes the values BB, BC or CB is
simple to compute:
</bodyText>
<equation confidence="0.471239">
P(Li, Li+1 = BB) = P(chunk(i, i + 1))
</equation>
<footnote confidence="0.9055195">
5Note that the problem would arise even if only zero-order
Markov (label unigram) features were used, only in that case
the troublesome features would be those that involved the la-
bel unigram C.
</footnote>
<page confidence="0.983996">
468
</page>
<equation confidence="0.836668666666667">
and, e.g.,
P(Li, Li+1 = BC) = � P(chunk(i, j)),
j&gt;i+1
</equation>
<bodyText confidence="0.999885857142857">
but the same method of summing over chunks can-
not be used for the value CC since for each label
bigram there are quadratically many chunks cor-
responding to that value. In this case, the solution
is deceptively simple: using the fact that for any
given label bigram, the sum of the probabilities of
the four labels must be one, we can deduce that
</bodyText>
<equation confidence="0.996883">
P(Li, Li+1 = CC) = 1.0 − P(Li, Li+1 = BB)
− P(Li, Li+1 = BC) − P(Li, Li+1 = CB).
</equation>
<bodyText confidence="0.999469142857143">
One might object that features of the C and CC
labels (the ones presenting algorithmic difficulty)
are unnecessary, since under certain conditions,
their removal would not in fact change the expres-
sivity of the model or the distribution that maxi-
mizes training likelihood. This will indeed be the
case when the following conditions are fulfilled:
</bodyText>
<listItem confidence="0.977378">
1. All label bigram features are of the form
</listItem>
<equation confidence="0.9990955">
0M(Li,Li+1, i, x) =
11(Li, Li+1) = α &amp; pred(i, x)}
</equation>
<bodyText confidence="0.999429666666667">
for some label bigram α and predicate pred,
and any such feature with a given predicate
has variants for all four label bigrams α.
</bodyText>
<listItem confidence="0.85882">
2. No regularization is used during training.
</listItem>
<bodyText confidence="0.99995565">
A proof of this claim would require too much
space for this paper, but the key is that, given a
model satisfying the above conditions, one can
obtain an equivalent model via adding, for each
feature type over pred, some constant to the four
weights corresponding to the four label bigrams,
such that the CC bigram features all have weight
zero.
In practice, however, one or both of these con-
ditions is always broken. It is common knowl-
edge that regularization of log-linear models with
a large number of features is necessary to achieve
high performance, and typically in NLP one de-
fines feature templates and chooses only those fea-
tures that occur in some positive example in the
training set. In fact, if both of these conditions are
fulfilled, it is very likely that the optimal model
will have some weights with infinite values. We
conclude that it is not a practical alternative to omit
the C and CC label features.
</bodyText>
<subsectionHeader confidence="0.8620135">
2.4 Generative Features in a Discriminative
Model
</subsectionHeader>
<bodyText confidence="0.9989678125">
When using the output of a generative model as
a feature in a discriminative model, Raina et al.
(2004) provide a justification for the use of log
conditional odds as opposed to log-probability:
they show that using log conditional odds as fea-
tures in a logistic regression model is equivalent
to discriminatively training weights for the fea-
tures of a Naive Bayes classifier to maximize
conditional likelihood.6 They demonstrate that
the resulting classifier, termed a “hybrid genera-
tive/discriminative classifier”, achieves lower test
error than either pure Naive Bayes or pure logistic
regression on a text classification task, regardless
of training set size.
The hybrid generative/discriminative classifier
also uses a unique method for using the same data
used to estimate the parameters of the compo-
nent generative models for training the discrimina-
tive model parameters w without introducing bias.
A “leave-one-out” strategy is used to choose w,
whereby the feature values of the i-th training ex-
ample are computed using probabilities estimated
with the i-th example held out. The beauty of this
approach is that since the probabilities are esti-
mated according to (smoothed) relative frequency,
it is only necessary during feature computation to
maintain sufficient statistics and adjust them as
necessary for each example.
In this paper, we experiment with the use of
a single “hybrid” local semi-CRF feature, the
smoothed log conditional odds that a given sub-
sequence xab = (xa, . . . , xb−1) forms a word:
</bodyText>
<equation confidence="0.999864">
wordcount(xab) + 1
log
nonwordcount(xab) + 1,
</equation>
<bodyText confidence="0.9999766">
where wordcount(xab) is the number of times
xab forms a word in the training set, and
nonwordcount(xab) is the number of times xab
occurs, not segmented into a single word. The
models we test are not strictly speaking hybrid
generative/discriminative models, since we also
use indicator features not derived from a genera-
tive model. We did however use the leave-one-out
approach for computing the log conditional odds
feature during training.
</bodyText>
<footnote confidence="0.885794666666667">
6In fact, one more step beyond what is shown in that paper
is required to reach the stated conclusion, since their features
are not actually log conditional odds, but log � &lt;����
</footnote>
<note confidence="0.710966">
� &lt;�����. It is
</note>
<bodyText confidence="0.626363">
simple to show that in the given context this feature is equiv-
alent to log conditional odds.
</bodyText>
<page confidence="0.999694">
469
</page>
<sectionHeader confidence="0.9996" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999990666666667">
To test the ideas discussed in this paper, we com-
pared the performance of semi-CRFs using vari-
ous feature sets on a Chinese word segmentation
task. The data used was the Microsoft Research
Beijing corpus from the Second International
Chinese Word Segmentation Bakeoff (Emerson,
2005), and we used the same train/test split used in
the competition. The training set consists of 87K
sentences of Beijing dialect Chinese, hand seg-
mented into 2.37M words. The test set contains
107K words comprising roughly 4K sentences.
We used a maximum word length k of 15 in our
experiments, which accounted for 99.99% of the
word tokens in our training set. The 249 train-
ing sentences that contained words longer than 15
characters were discarded. We did not discard any
test sentences.
In order to be directly comparable to the Bake-
off results, we also worked under the very strict
“closed test” conditions of the Bakeoff, which re-
quire that no information or data outside of the
training set be used, not even prior knowledge of
which characters represent Arabic numerals, Latin
characters or punctuation marks.
</bodyText>
<subsectionHeader confidence="0.997462">
3.1 Features Used
</subsectionHeader>
<bodyText confidence="0.9997416">
We divide our main features into two types accord-
ing to whether they are most naturally used in a
CRF or a semi-CRF.
The CRF-type features are indicator functions
that fire when the character label (or label bigram)
takes some value and some predicate of the input
at a certain position relative to the label is satis-
fied. For each character label unigram L at posi-
tion i, we use the same set of predicate templates
checking:
</bodyText>
<listItem confidence="0.987981909090909">
• The identity of xi−1 and xi
• The identity of the character bigram starting
at positions i − 2, i − 1 and i
• Whether xj and xj+1 are identical, for j =
(i − 2) ... i
• Whether xj and xj+2 are identical, for j =
(i − 3) ... i
• Whether the sequence xj ... xj+3 forms an
AABB sequence for j = (i − 4) ... i
• Whether the sequence xj ... xj+3 forms an
ABAB sequence for j = (i − 4) ... i
</listItem>
<bodyText confidence="0.999932297297297">
The latter four feature templates are designed to
detect character or word reduplication, a morpho-
logical phenomenon that can influence word seg-
mentation in Chinese. The first two of these were
also used by Tseng et al. (2005).
For label bigrams (Li, Li+1), we use the same
templates, but extending the range of positions
by one to the right.7 Each label uni- or bigram
also has a “prior” feature that always fires for
that label configuration. All configurations con-
tain the above features for the label unigram B,
since these are easily used in either a CRF or semi-
CRF model. To determine the influence of CRF-
type features on performance, we also test config-
urations in which both B and C label features are
used, and configurations using all label uni- and
bigrams.
In the semi-Markov conditions, we also use as
feature templates indicators of the length of a word
t, fort = 1... k, and indicators of the identity of
the corresponding character sequence.
All feature templates were instantiated with val-
ues that occur in positive training examples. We
found that excluding CRF-type features that occur
only once in the training set consistently improved
performance on the development set, so we use a
count threshold of two for the experiments. We do
not do any thresholding of the semi-CRF features,
however.
Finally, we use the single generative feature,
log conditional odds that the given string forms
a word. We also present results using the more
typical log conditional probability instead of the
odds, for comparison. In fact, these are both semi-
Markov-type features, but we single them out to
determine what they contribute over and above the
other semi-Markov features.
</bodyText>
<subsectionHeader confidence="0.958632">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.999981">
The results of test set runs are summarized in ta-
ble 3.2. The columns indicate which CRF-type
features were used: features of only the label B,
features of label unigrams B and C, or features
of all label unigrams and bigrams. The rows indi-
cate which semi-Markov-type features were used:
</bodyText>
<footnote confidence="0.984806428571429">
7For both label unigram and label bigram features, the in-
dices are chosen so that the feature set exhibits no asymmetry
with respect to direction: for each feature considering some
boundary and some property of the character(s) at a given
offset to the left, there is a corresponding feature considering
that boundary and the same property of the character(s) at the
same offset to the right, and vice-versa.
</footnote>
<page confidence="0.986554">
470
</page>
<table confidence="0.999391428571428">
Features B only uni uni+bi
none 92.33 94.71 95.69
semi 95.28 96.05 96.46
prob 93.86 95.40 96.04
semi+prob 95.51 96.24 96.55
odds 95.10 96.06 96.40
semi+odds 96.27 96.77 96.84
</table>
<tableCaption confidence="0.903289">
Table 1: Test F-measure for different model con-
figurations.
</tableCaption>
<bodyText confidence="0.99950217948718">
“semi” means length and word identity features
were used, “prob” means the log-probability fea-
ture was used, and “odds” means the log-odds fea-
ture was used.
To establish the impact of each type of feature
(C label unigrams, label bigrams, semi-CRF-type
features, and the log-odds feature), we look at the
reduction in error brought about by adding each
type of feature. First consider the effect of the
CRF-type features. Adding the C label features
reduces error by 31% if no semi-CRF features are
used, by 16% when semi-CRF indicator features
are turned on, and by 13% when all semi-CRF fea-
tures (including log-odds) are used. Using all label
bigrams reduces error by 44%, 25%, and 15% in
these three conditions, respectively.
Contrary to previous conclusions, our results
show a significant impact due to the use of semi-
CRF-type features, when CRF-type features are
held constant. Adding semi-CRF indicator fea-
tures results in a 38% error reduction without
CRF-type features, and 18% with them. Adding
semi-CRF indicator features plus the log-odds fea-
ture gives 52% and 27% in these two conditions,
respectively.
Finally, across configurations, the log condi-
tional odds does much better than log condi-
tional probability. When the log-odds feature is
added to the complete CRF model (uni+bi) as
the only semi-CRF-type feature, errors are re-
duced by 24%, compared to only 7.6% for the log-
probability. Even when the other semi-CRF-type
features are present as well, log-odds reduces error
by 13% compared to 2.5% for log-probability.
Our best model, combining all features, resulted
in an error reduction of 12% over the highest score
on this dataset from the 2005 Sighan closed test
competition (96.4%), achieved by the pure CRF
system of Tseng et al. (2005).
</bodyText>
<subsectionHeader confidence="0.952149">
3.3 Discussion
</subsectionHeader>
<bodyText confidence="0.9999916">
Our results indicate that both Markov-type and
semi-Markov-type features are useful for generali-
zation to unseen data. This may be because the
two types of features are in a sense complemen-
tary: semi-Markov-type features such as word-
identity are valuable for modeling the tendency
of known strings to segment as words, while la-
bel based features are valuable for modeling prop-
erties of sub-lexical components such as affixes,
helping to generalize to words that have not previ-
ously been encountered. We did not explicitly test
the utility of CRF-type features for improving re-
call on out-of-vocabulary items, but we note that
in the Bakeoff, the model of Tseng et al. (2005),
which was very similar to our CRF-only system
(only containing a few more feature templates),
was consistently among the best performing sys-
tems in terms of test OOV recall (Emerson, 2005).
We also found that for this sequence segmenta-
tion task, the use of log conditional odds as a fea-
ture results in much better performance than the
use of the more typical log conditional probabil-
ity. It would be interesting to see the log-odds
applied in more contexts where log-probabilities
are typically used as features. We have presented
the intuitive argument that the log-odds may be
advantageous because it does not exhibit the 0-1
asymmetry of the log-probability, but it would be
satisfying to justify the choice on more theoretical
grounds.
</bodyText>
<sectionHeader confidence="0.996174" genericHeader="method">
4 Relation to Previous Work
</sectionHeader>
<bodyText confidence="0.999943666666667">
There is a significant volume of work explor-
ing the use of CRFs for a variety of chunking
tasks, including named-entity recognition, gene
prediction, shallow parsing and others (Finkel et
al., 2005; Culotta et al., 2005; Sha and Pereira,
2003). The current work indicates that these sys-
tems might be improved by moving to a semi-CRF
model.
There have not been a large number of studies
using the semi-CRF, but the few that have been
done found only marginal improvements over pure
CRF systems (Sarawagi and Cohen, 2004; Liang,
2005; Daum´e III and Marcu, 2005). Notably,
none of those studies experimented with features
of chunk non-boundaries, as is achieved by the use
of CRF-type features involving the label C, and
we take this to be the reason for their not obtain-
ing higher results.
</bodyText>
<page confidence="0.996417">
471
</page>
<bodyText confidence="0.999855">
Although it has become fairly common in NLP
to use the log conditional probabilities of events
as features in a discriminative model, we are not
aware of any work using the log conditional odds.
</bodyText>
<sectionHeader confidence="0.99936" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999895625">
We have shown that order-1 semi-Markov condi-
tional random fields are strictly more expressive
than order-1 Markov CRFs, and that the added
expressivity enables the use of features that lead
to improvements on a segmentation task. On the
other hand, Markov CRFs can more naturally in-
corporate certain features that may be useful for
modeling sub-chunk phenomena and generaliza-
tion to unseen chunks. To achieve the best per-
formance for segmentation, we propose that both
types of features be used, and we show how this
can be done efficiently.
Additionally, we have shown that a log condi-
tional odds feature estimated from a generative
model can be superior to the more common log
conditional probability.
</bodyText>
<sectionHeader confidence="0.999566" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999839666666667">
Many thanks to Kristina Toutanova for her
thoughtful discussion and feedback, and also to
the anonymous reviewers for their suggestions.
</bodyText>
<sectionHeader confidence="0.999553" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999848763888889">
Masayuki Asahara, Kenta Fukuoka, Ai Azuma, Chooi-
Ling Goh, Yotaro Watanabe, Yuji Matsumoto, and
Takahashi Tsuzuki. 2005. Combination of machine
learning methods for optimum chinese word seg-
mentation. In Proc. Fourth SIGHAN Workshop on
Chinese Language Processing, pages 134–137.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In Proc. 14th Interna-
tional Conf. on Machine Learning.
Aron Culotta, David Kulp, and Andrew McCallum.
2005. Gene prediction with conditional random
fields. Technical report, University of Massa-
chusetts Dept. of Computer Science, April.
Hal Daum´e III and Daniel Marcu. 2005. Learning
as search optimization: Approximate large margin
methods for structured prediction. In Proc. 19th In-
ternational Conf. on Machine Learning.
Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. In Proc. Fourth
SIGHAN Workshop on Chinese Language Process-
ing, pages 123–133.
Jenny Finkel, Trond Grenager, and Christopher D.
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. Proc. 41th Annual Meeting of the Assi-
ciation of Computation Linguistics.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proc. 18th International Conf. on
Machine Learning, pages 282–289. Morgan Kauf-
mann, San Francisco, CA.
Percy Liang. 2005. Semi-supervised learning for nat-
ural language. Master’s thesis, Massachusetts Insti-
tute of Technology.
Andrew Y. Ng and Michael I. Jordan. 2001. On dis-
criminative vs. generative classifiers: A comparison
of logistic regression and Naive Bayes. In Proc. Ad-
vances in Neural Information Processing 14.
Franz Josef Och and Hermann Ney. 2002. Discrim-
inative training and maximum entropy models for
statistical machine translation. Proc. 38th Annual
Meeting of the Assiciation of Computation Linguis-
tics.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417–449,
December.
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detec-
tion using conditional random fields. In Proc. 20th
International Conf. on Computational Linguistics.
Rajat Raina, Yirong Shen, Andrew Y. Ng, and Andrew
McCallum. 2004. Classification with hybrid gen-
erative/discriminative models. In Proc. Advances in
Neural Information Processing 17.
Brian Roark and Seeger Fisher. 2005. OGI/OHSU
baseline multilingual multi-document sumarization
system. In Proc. Multilingual Summarization Eval-
uation in ACL Workshop: Intrinsic and Extrinsic
Evaluation Measures for MT and/or Summarization.
Sunita Sarawagi and William Cohen. 2004. Semi-
markov conditional random fields for information
extraction. In Proc. 18th International Conf. on Ma-
chine Learning.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. Proc. HLT-NAACL.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter for sighan bake-
off 2005. In Proc. Fourth SIGHAN Workshop on
Chinese Language Processing, pages 168–171.
</reference>
<page confidence="0.998496">
472
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.713522">
<title confidence="0.9995345">A Hybrid Markov/Semi-Markov Conditional Random for Sequence Segmentation</title>
<author confidence="0.996232">Galen</author>
<affiliation confidence="0.963773">Microsoft</affiliation>
<address confidence="0.889552">One Microsoft Redmond, WA</address>
<email confidence="0.999927">galena@microsoft.com</email>
<abstract confidence="0.997456565217392">Markov order-1 conditional random fields (CRFs) and semi-Markov CRFs are two popular models for sequence segmentation and labeling. Both models have advantages in terms of the type of features they most naturally represent. We propose a hybrid model that is capable of representing both types of features, and describe efficient algorithms for its training and inference. We demonstrate that our hybrid model achieves error reductions of 18% and 25% over a standard order-1 CRF and a semi-Markov CRF (resp.) on the task of Chinese word segmentation. We also propose the use of a powerful feature for the semi-Markov CRF: the log conditional odds that a given token sequence constitutes a chunk according to a generative model, which reduces error by an additional 13%. Our best system achieves 96.8% F-measure, the highest reported score on this test set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Masayuki Asahara</author>
<author>Kenta Fukuoka</author>
<author>Ai Azuma</author>
<author>ChooiLing Goh</author>
<author>Yotaro Watanabe</author>
<author>Yuji Matsumoto</author>
<author>Takahashi Tsuzuki</author>
</authors>
<title>Combination of machine learning methods for optimum chinese word segmentation. In</title>
<date>2005</date>
<booktitle>Proc. Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>134--137</pages>
<contexts>
<context position="2054" citStr="Asahara et al., 2005" startWordPosition="332" endWordPosition="336">CRF) (Sarawagi and Cohen, 2004). A CRF in its basic form is a model for labeling tokens in a sequence; however it can easily be adapted to perform segmentation via labeling each token as BEGIN or CONTINUATION, or according to some similar scheme. CRFs using this technique have been shown to be very successful at the task of Chinese word segmentation (CWS), starting with the model of Peng et al. (2004). In the Second International Chinese Word Segmentation Bakeoff (Emerson, 2005), two of the highest scoring systems in the closed track competition were based on a CRF model. (Tseng et al., 2005; Asahara et al., 2005) While the CRF is quite effective compared with other models designed for CWS, one wonders whether it may be limited by its restrictive independence assumptions on non-adjacent labels: an order-M CRF satisfies the order-M Markov assumption that, globally conditioned on the input sequence, each label is independent of all other labels given the M labels to its left and right. Consequently, the model only “sees” word boundaries within a moving window of M + 1 characters, which prohibits it from explicitly modeling the tendency of strings longer than that window to form words, or from modeling th</context>
</contexts>
<marker>Asahara, Fukuoka, Azuma, Goh, Watanabe, Matsumoto, Tsuzuki, 2005</marker>
<rawString>Masayuki Asahara, Kenta Fukuoka, Ai Azuma, ChooiLing Goh, Yotaro Watanabe, Yuji Matsumoto, and Takahashi Tsuzuki. 2005. Combination of machine learning methods for optimum chinese word segmentation. In Proc. Fourth SIGHAN Workshop on Chinese Language Processing, pages 134–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In Proc. 14th International Conf. on Machine Learning.</booktitle>
<contexts>
<context position="5695" citStr="Collins (2000)" startWordPosition="946" endWordPosition="947"> paper, we show that semi-CRFs are strictly more expressive, and also demonstrate how CRF-type features can be used in a semi-CRF model for Chinese word segmentation. Our experiments show that a model incorporating both types of features can outperform models using only one or the other type. Orthogonally, we explore in this paper the use of a very powerful feature for the semi-CRF derived from a generative model. It is common in statistical NLP to use as features in a discriminative model the (logarithm of the) estimated probability of some event according to a generative model. For example, Collins (2000) uses a discriminative classifier for choosing among the top N parse trees output by a generative baseline model, and uses the log-probability of a parse according to the baseline model as a feature in the reranker. Similarly, the machine translation system of Och and Ney uses log-probabilities of phrasal translations and other events as features in a log-linear model (Och and Ney, 2002; Och and Ney, 2004). There are many reasons for incorporating these types of features, including the desire to combine the higher accuracy of a discriminative model with the simple parameter estimation and infe</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Michael Collins. 2000. Discriminative reranking for natural language parsing. In Proc. 14th International Conf. on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>David Kulp</author>
<author>Andrew McCallum</author>
</authors>
<title>Gene prediction with conditional random fields.</title>
<date>2005</date>
<tech>Technical report,</tech>
<institution>University of Massachusetts Dept. of Computer Science,</institution>
<contexts>
<context position="28668" citStr="Culotta et al., 2005" startWordPosition="4975" endWordPosition="4978">itional probability. It would be interesting to see the log-odds applied in more contexts where log-probabilities are typically used as features. We have presented the intuitive argument that the log-odds may be advantageous because it does not exhibit the 0-1 asymmetry of the log-probability, but it would be satisfying to justify the choice on more theoretical grounds. 4 Relation to Previous Work There is a significant volume of work exploring the use of CRFs for a variety of chunking tasks, including named-entity recognition, gene prediction, shallow parsing and others (Finkel et al., 2005; Culotta et al., 2005; Sha and Pereira, 2003). The current work indicates that these systems might be improved by moving to a semi-CRF model. There have not been a large number of studies using the semi-CRF, but the few that have been done found only marginal improvements over pure CRF systems (Sarawagi and Cohen, 2004; Liang, 2005; Daum´e III and Marcu, 2005). Notably, none of those studies experimented with features of chunk non-boundaries, as is achieved by the use of CRF-type features involving the label C, and we take this to be the reason for their not obtaining higher results. 471 Although it has become fai</context>
</contexts>
<marker>Culotta, Kulp, McCallum, 2005</marker>
<rawString>Aron Culotta, David Kulp, and Andrew McCallum. 2005. Gene prediction with conditional random fields. Technical report, University of Massachusetts Dept. of Computer Science, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Learning as search optimization: Approximate large margin methods for structured prediction.</title>
<date>2005</date>
<booktitle>In Proc. 19th International Conf. on Machine Learning.</booktitle>
<marker>Daum´e, Marcu, 2005</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2005. Learning as search optimization: Approximate large margin methods for structured prediction. In Proc. 19th International Conf. on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Emerson</author>
</authors>
<title>The second international chinese word segmentation bakeoff. In</title>
<date>2005</date>
<booktitle>Proc. Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>123--133</pages>
<contexts>
<context position="1916" citStr="Emerson, 2005" startWordPosition="308" endWordPosition="309">for these tasks are the conditional random field (CRFs) (Lafferty et al., 2001) and the semi-Markov conditional random field (semi-CRF) (Sarawagi and Cohen, 2004). A CRF in its basic form is a model for labeling tokens in a sequence; however it can easily be adapted to perform segmentation via labeling each token as BEGIN or CONTINUATION, or according to some similar scheme. CRFs using this technique have been shown to be very successful at the task of Chinese word segmentation (CWS), starting with the model of Peng et al. (2004). In the Second International Chinese Word Segmentation Bakeoff (Emerson, 2005), two of the highest scoring systems in the closed track competition were based on a CRF model. (Tseng et al., 2005; Asahara et al., 2005) While the CRF is quite effective compared with other models designed for CWS, one wonders whether it may be limited by its restrictive independence assumptions on non-adjacent labels: an order-M CRF satisfies the order-M Markov assumption that, globally conditioned on the input sequence, each label is independent of all other labels given the M labels to its left and right. Consequently, the model only “sees” word boundaries within a moving window of M + 1 </context>
<context position="20949" citStr="Emerson, 2005" startWordPosition="3663" endWordPosition="3664">feature during training. 6In fact, one more step beyond what is shown in that paper is required to reach the stated conclusion, since their features are not actually log conditional odds, but log � &lt;���� � &lt;�����. It is simple to show that in the given context this feature is equivalent to log conditional odds. 469 3 Experiments To test the ideas discussed in this paper, we compared the performance of semi-CRFs using various feature sets on a Chinese word segmentation task. The data used was the Microsoft Research Beijing corpus from the Second International Chinese Word Segmentation Bakeoff (Emerson, 2005), and we used the same train/test split used in the competition. The training set consists of 87K sentences of Beijing dialect Chinese, hand segmented into 2.37M words. The test set contains 107K words comprising roughly 4K sentences. We used a maximum word length k of 15 in our experiments, which accounted for 99.99% of the word tokens in our training set. The 249 training sentences that contained words longer than 15 characters were discarded. We did not discard any test sentences. In order to be directly comparable to the Bakeoff results, we also worked under the very strict “closed test” c</context>
<context position="27869" citStr="Emerson, 2005" startWordPosition="4845" endWordPosition="4846">able for modeling the tendency of known strings to segment as words, while label based features are valuable for modeling properties of sub-lexical components such as affixes, helping to generalize to words that have not previously been encountered. We did not explicitly test the utility of CRF-type features for improving recall on out-of-vocabulary items, but we note that in the Bakeoff, the model of Tseng et al. (2005), which was very similar to our CRF-only system (only containing a few more feature templates), was consistently among the best performing systems in terms of test OOV recall (Emerson, 2005). We also found that for this sequence segmentation task, the use of log conditional odds as a feature results in much better performance than the use of the more typical log conditional probability. It would be interesting to see the log-odds applied in more contexts where log-probabilities are typically used as features. We have presented the intuitive argument that the log-odds may be advantageous because it does not exhibit the 0-1 asymmetry of the log-probability, but it would be satisfying to justify the choice on more theoretical grounds. 4 Relation to Previous Work There is a significa</context>
</contexts>
<marker>Emerson, 2005</marker>
<rawString>Thomas Emerson. 2005. The second international chinese word segmentation bakeoff. In Proc. Fourth SIGHAN Workshop on Chinese Language Processing, pages 123–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Finkel</author>
<author>Trond Grenager</author>
<author>Christopher D Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>Proc. 41th Annual Meeting of the Assiciation of Computation Linguistics.</booktitle>
<contexts>
<context position="28646" citStr="Finkel et al., 2005" startWordPosition="4971" endWordPosition="4974">more typical log conditional probability. It would be interesting to see the log-odds applied in more contexts where log-probabilities are typically used as features. We have presented the intuitive argument that the log-odds may be advantageous because it does not exhibit the 0-1 asymmetry of the log-probability, but it would be satisfying to justify the choice on more theoretical grounds. 4 Relation to Previous Work There is a significant volume of work exploring the use of CRFs for a variety of chunking tasks, including named-entity recognition, gene prediction, shallow parsing and others (Finkel et al., 2005; Culotta et al., 2005; Sha and Pereira, 2003). The current work indicates that these systems might be improved by moving to a semi-CRF model. There have not been a large number of studies using the semi-CRF, but the few that have been done found only marginal improvements over pure CRF systems (Sarawagi and Cohen, 2004; Liang, 2005; Daum´e III and Marcu, 2005). Notably, none of those studies experimented with features of chunk non-boundaries, as is achieved by the use of CRF-type features involving the label C, and we take this to be the reason for their not obtaining higher results. 471 Alth</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Finkel, Trond Grenager, and Christopher D. Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. Proc. 41th Annual Meeting of the Assiciation of Computation Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>Proc. 18th International Conf. on Machine Learning,</booktitle>
<pages>282--289</pages>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="1381" citStr="Lafferty et al., 2001" startWordPosition="215" endWordPosition="218">erful feature for the semi-Markov CRF: the log conditional odds that a given token sequence constitutes a chunk according to a generative model, which reduces error by an additional 13%. Our best system achieves 96.8% F-measure, the highest reported score on this test set. 1 Introduction The problem of segmenting sequence data into chunks arises in many natural language applications, such as named-entity recognition, shallow parsing, and word segmentation in East Asian languages. Two popular discriminative models that have been proposed for these tasks are the conditional random field (CRFs) (Lafferty et al., 2001) and the semi-Markov conditional random field (semi-CRF) (Sarawagi and Cohen, 2004). A CRF in its basic form is a model for labeling tokens in a sequence; however it can easily be adapted to perform segmentation via labeling each token as BEGIN or CONTINUATION, or according to some similar scheme. CRFs using this technique have been shown to be very successful at the task of Chinese word segmentation (CWS), starting with the model of Peng et al. (2004). In the Second International Chinese Word Segmentation Bakeoff (Emerson, 2005), two of the highest scoring systems in the closed track competit</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. 18th International Conf. on Machine Learning, pages 282–289. Morgan Kaufmann, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
</authors>
<title>Semi-supervised learning for natural language. Master’s thesis,</title>
<date>2005</date>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="4229" citStr="Liang, 2005" startWordPosition="697" endWordPosition="698">e assumption that, globally conditioned on the input sequence, the position of each chunk boundary is independent of all other boundaries given the positions of the M boundaries to its left and right regardless of how far away they are. Even with an order-1 model, this enables several classes of features that one would expect to be of great utility to the word segmentation task, in particular word length and word identity. Despite this, the only work of which we are aware exploring the use of a semi-Markov CRF for Chinese word segmentation did not find significant gains over the standard CRF (Liang, 2005). This is surprising, not only because the additional features a semi-CRF enables are intuitively very useful, but because as we will show, an order-M semi-CRF is strictly more powerful than an order-M CRF, in the sense that any feature that can be used in the latter can also be used in the former, or equivalently, the semi-CRF makes strictly weaker independence assumptions. Given a judicious choice of features (or simply enough training data) the semi-CRF should be superior. We propose that the reason for this discrepancy may be that despite the greater representational power of the semi-CRF,</context>
<context position="12211" citStr="Liang (2005)" startWordPosition="2086" endWordPosition="2087"> valuable to rule out certain word boundaries if X were a character that typically occurs only as a suffix but that combines freely with a variety of root forms to create new words. This type of feature (specifically, a feature indicating the absence as opposed to the presence of a chunk boundary) is a bit less natural in a semi-CRF, since in that case local features 0S(yj, yj+1, x) are defined on pairs of adjacent boundaries. Information about which tokens are not on boundaries is only implicit, making it a bit more difficult to incorporate that information into the features. Indeed, neither Liang (2005) nor Sarawagi and Cohen (2004) nor any other system using a semi-Markov CRF on any task has included this type of feature to our knowledge. We hypothesize (and our experiments confirm) that the lack of this feature explains the failure of the semi-CRF to outperform the CRF for word segmentation in the past. Before showing how CRF-type features can be used in a semi-CRF, we first demonstrate that the semi-CRF is indeed strictly more expressive than 467 the CRF, meaning that any global feature map q that decomposes according to (2) also decomposes according to (1). It is sufficient to show that </context>
<context position="28980" citStr="Liang, 2005" startWordPosition="5031" endWordPosition="5032"> to justify the choice on more theoretical grounds. 4 Relation to Previous Work There is a significant volume of work exploring the use of CRFs for a variety of chunking tasks, including named-entity recognition, gene prediction, shallow parsing and others (Finkel et al., 2005; Culotta et al., 2005; Sha and Pereira, 2003). The current work indicates that these systems might be improved by moving to a semi-CRF model. There have not been a large number of studies using the semi-CRF, but the few that have been done found only marginal improvements over pure CRF systems (Sarawagi and Cohen, 2004; Liang, 2005; Daum´e III and Marcu, 2005). Notably, none of those studies experimented with features of chunk non-boundaries, as is achieved by the use of CRF-type features involving the label C, and we take this to be the reason for their not obtaining higher results. 471 Although it has become fairly common in NLP to use the log conditional probabilities of events as features in a discriminative model, we are not aware of any work using the log conditional odds. 5 Conclusion We have shown that order-1 semi-Markov conditional random fields are strictly more expressive than order-1 Markov CRFs, and that t</context>
</contexts>
<marker>Liang, 2005</marker>
<rawString>Percy Liang. 2005. Semi-supervised learning for natural language. Master’s thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>On discriminative vs. generative classifiers: A comparison of logistic regression and Naive Bayes.</title>
<date>2001</date>
<booktitle>In Proc. Advances in Neural Information Processing 14.</booktitle>
<contexts>
<context position="6425" citStr="Ng and Jordan, 2001" startWordPosition="1063" endWordPosition="1066">el, and uses the log-probability of a parse according to the baseline model as a feature in the reranker. Similarly, the machine translation system of Och and Ney uses log-probabilities of phrasal translations and other events as features in a log-linear model (Och and Ney, 2002; Och and Ney, 2004). There are many reasons for incorporating these types of features, including the desire to combine the higher accuracy of a discriminative model with the simple parameter estimation and inference of a generative one, and also the fact that generative models are more robust in data sparse scenarios (Ng and Jordan, 2001). For word segmentation, one might want to use as a local feature the log-probability that a segment is a word, given the character sequence it spans. A curious property of this feature is that it induces a counterintuitive asymmetry between the is-word and is-not-word cases: the component generative model can effectively dictate that a certain chunk is not a word, by assigning it a very low probability (driving the feature value to negative infinity), but it cannot dictate that a chunk is a word, because the log-probability is bounded above.2 If instead the log conditional odds log P����&apos;) P�</context>
</contexts>
<marker>Ng, Jordan, 2001</marker>
<rawString>Andrew Y. Ng and Michael I. Jordan. 2001. On discriminative vs. generative classifiers: A comparison of logistic regression and Naive Bayes. In Proc. Advances in Neural Information Processing 14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>Proc. 38th Annual Meeting of the Assiciation of Computation Linguistics.</booktitle>
<contexts>
<context position="6084" citStr="Och and Ney, 2002" startWordPosition="1007" endWordPosition="1010">erived from a generative model. It is common in statistical NLP to use as features in a discriminative model the (logarithm of the) estimated probability of some event according to a generative model. For example, Collins (2000) uses a discriminative classifier for choosing among the top N parse trees output by a generative baseline model, and uses the log-probability of a parse according to the baseline model as a feature in the reranker. Similarly, the machine translation system of Och and Ney uses log-probabilities of phrasal translations and other events as features in a log-linear model (Och and Ney, 2002; Och and Ney, 2004). There are many reasons for incorporating these types of features, including the desire to combine the higher accuracy of a discriminative model with the simple parameter estimation and inference of a generative one, and also the fact that generative models are more robust in data sparse scenarios (Ng and Jordan, 2001). For word segmentation, one might want to use as a local feature the log-probability that a segment is a word, given the character sequence it spans. A curious property of this feature is that it induces a counterintuitive asymmetry between the is-word and i</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. Proc. 38th Annual Meeting of the Assiciation of Computation Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="6104" citStr="Och and Ney, 2004" startWordPosition="1011" endWordPosition="1014">ative model. It is common in statistical NLP to use as features in a discriminative model the (logarithm of the) estimated probability of some event according to a generative model. For example, Collins (2000) uses a discriminative classifier for choosing among the top N parse trees output by a generative baseline model, and uses the log-probability of a parse according to the baseline model as a feature in the reranker. Similarly, the machine translation system of Och and Ney uses log-probabilities of phrasal translations and other events as features in a log-linear model (Och and Ney, 2002; Och and Ney, 2004). There are many reasons for incorporating these types of features, including the desire to combine the higher accuracy of a discriminative model with the simple parameter estimation and inference of a generative one, and also the fact that generative models are more robust in data sparse scenarios (Ng and Jordan, 2001). For word segmentation, one might want to use as a local feature the log-probability that a segment is a word, given the character sequence it spans. A curious property of this feature is that it induces a counterintuitive asymmetry between the is-word and is-not-word cases: th</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Fangfang Feng</author>
<author>Andrew McCallum</author>
</authors>
<title>Chinese segmentation and new word detection using conditional random fields.</title>
<date>2004</date>
<booktitle>In Proc. 20th International Conf. on Computational Linguistics.</booktitle>
<contexts>
<context position="1837" citStr="Peng et al. (2004)" startWordPosition="296" endWordPosition="299">in East Asian languages. Two popular discriminative models that have been proposed for these tasks are the conditional random field (CRFs) (Lafferty et al., 2001) and the semi-Markov conditional random field (semi-CRF) (Sarawagi and Cohen, 2004). A CRF in its basic form is a model for labeling tokens in a sequence; however it can easily be adapted to perform segmentation via labeling each token as BEGIN or CONTINUATION, or according to some similar scheme. CRFs using this technique have been shown to be very successful at the task of Chinese word segmentation (CWS), starting with the model of Peng et al. (2004). In the Second International Chinese Word Segmentation Bakeoff (Emerson, 2005), two of the highest scoring systems in the closed track competition were based on a CRF model. (Tseng et al., 2005; Asahara et al., 2005) While the CRF is quite effective compared with other models designed for CWS, one wonders whether it may be limited by its restrictive independence assumptions on non-adjacent labels: an order-M CRF satisfies the order-M Markov assumption that, globally conditioned on the input sequence, each label is independent of all other labels given the M labels to its left and right. Conse</context>
</contexts>
<marker>Peng, Feng, McCallum, 2004</marker>
<rawString>Fuchun Peng, Fangfang Feng, and Andrew McCallum. 2004. Chinese segmentation and new word detection using conditional random fields. In Proc. 20th International Conf. on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajat Raina</author>
<author>Yirong Shen</author>
<author>Andrew Y Ng</author>
<author>Andrew McCallum</author>
</authors>
<title>Classification with hybrid generative/discriminative models.</title>
<date>2004</date>
<booktitle>In Proc. Advances in Neural Information Processing 17.</booktitle>
<contexts>
<context position="18457" citStr="Raina et al. (2004)" startWordPosition="3262" endWordPosition="3265">tion of log-linear models with a large number of features is necessary to achieve high performance, and typically in NLP one defines feature templates and chooses only those features that occur in some positive example in the training set. In fact, if both of these conditions are fulfilled, it is very likely that the optimal model will have some weights with infinite values. We conclude that it is not a practical alternative to omit the C and CC label features. 2.4 Generative Features in a Discriminative Model When using the output of a generative model as a feature in a discriminative model, Raina et al. (2004) provide a justification for the use of log conditional odds as opposed to log-probability: they show that using log conditional odds as features in a logistic regression model is equivalent to discriminatively training weights for the features of a Naive Bayes classifier to maximize conditional likelihood.6 They demonstrate that the resulting classifier, termed a “hybrid generative/discriminative classifier”, achieves lower test error than either pure Naive Bayes or pure logistic regression on a text classification task, regardless of training set size. The hybrid generative/discriminative cl</context>
</contexts>
<marker>Raina, Shen, Ng, McCallum, 2004</marker>
<rawString>Rajat Raina, Yirong Shen, Andrew Y. Ng, and Andrew McCallum. 2004. Classification with hybrid generative/discriminative models. In Proc. Advances in Neural Information Processing 17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Seeger Fisher</author>
</authors>
<title>OGI/OHSU baseline multilingual multi-document sumarization system.</title>
<date>2005</date>
<booktitle>In Proc. Multilingual Summarization Evaluation in ACL Workshop: Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization.</booktitle>
<marker>Roark, Fisher, 2005</marker>
<rawString>Brian Roark and Seeger Fisher. 2005. OGI/OHSU baseline multilingual multi-document sumarization system. In Proc. Multilingual Summarization Evaluation in ACL Workshop: Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sunita Sarawagi</author>
<author>William Cohen</author>
</authors>
<title>Semimarkov conditional random fields for information extraction.</title>
<date>2004</date>
<booktitle>In Proc. 18th International Conf. on Machine Learning.</booktitle>
<contexts>
<context position="1464" citStr="Sarawagi and Cohen, 2004" startWordPosition="226" endWordPosition="229">en sequence constitutes a chunk according to a generative model, which reduces error by an additional 13%. Our best system achieves 96.8% F-measure, the highest reported score on this test set. 1 Introduction The problem of segmenting sequence data into chunks arises in many natural language applications, such as named-entity recognition, shallow parsing, and word segmentation in East Asian languages. Two popular discriminative models that have been proposed for these tasks are the conditional random field (CRFs) (Lafferty et al., 2001) and the semi-Markov conditional random field (semi-CRF) (Sarawagi and Cohen, 2004). A CRF in its basic form is a model for labeling tokens in a sequence; however it can easily be adapted to perform segmentation via labeling each token as BEGIN or CONTINUATION, or according to some similar scheme. CRFs using this technique have been shown to be very successful at the task of Chinese word segmentation (CWS), starting with the model of Peng et al. (2004). In the Second International Chinese Word Segmentation Bakeoff (Emerson, 2005), two of the highest scoring systems in the closed track competition were based on a CRF model. (Tseng et al., 2005; Asahara et al., 2005) While the</context>
<context position="12241" citStr="Sarawagi and Cohen (2004)" startWordPosition="2089" endWordPosition="2092"> out certain word boundaries if X were a character that typically occurs only as a suffix but that combines freely with a variety of root forms to create new words. This type of feature (specifically, a feature indicating the absence as opposed to the presence of a chunk boundary) is a bit less natural in a semi-CRF, since in that case local features 0S(yj, yj+1, x) are defined on pairs of adjacent boundaries. Information about which tokens are not on boundaries is only implicit, making it a bit more difficult to incorporate that information into the features. Indeed, neither Liang (2005) nor Sarawagi and Cohen (2004) nor any other system using a semi-Markov CRF on any task has included this type of feature to our knowledge. We hypothesize (and our experiments confirm) that the lack of this feature explains the failure of the semi-CRF to outperform the CRF for word segmentation in the past. Before showing how CRF-type features can be used in a semi-CRF, we first demonstrate that the semi-CRF is indeed strictly more expressive than 467 the CRF, meaning that any global feature map q that decomposes according to (2) also decomposes according to (1). It is sufficient to show that for any feature map qM of a Ma</context>
<context position="28967" citStr="Sarawagi and Cohen, 2004" startWordPosition="5027" endWordPosition="5030">but it would be satisfying to justify the choice on more theoretical grounds. 4 Relation to Previous Work There is a significant volume of work exploring the use of CRFs for a variety of chunking tasks, including named-entity recognition, gene prediction, shallow parsing and others (Finkel et al., 2005; Culotta et al., 2005; Sha and Pereira, 2003). The current work indicates that these systems might be improved by moving to a semi-CRF model. There have not been a large number of studies using the semi-CRF, but the few that have been done found only marginal improvements over pure CRF systems (Sarawagi and Cohen, 2004; Liang, 2005; Daum´e III and Marcu, 2005). Notably, none of those studies experimented with features of chunk non-boundaries, as is achieved by the use of CRF-type features involving the label C, and we take this to be the reason for their not obtaining higher results. 471 Although it has become fairly common in NLP to use the log conditional probabilities of events as features in a discriminative model, we are not aware of any work using the log conditional odds. 5 Conclusion We have shown that order-1 semi-Markov conditional random fields are strictly more expressive than order-1 Markov CRF</context>
</contexts>
<marker>Sarawagi, Cohen, 2004</marker>
<rawString>Sunita Sarawagi and William Cohen. 2004. Semimarkov conditional random fields for information extraction. In Proc. 18th International Conf. on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Sha</author>
<author>Fernando Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>Proc. HLT-NAACL.</booktitle>
<contexts>
<context position="28692" citStr="Sha and Pereira, 2003" startWordPosition="4979" endWordPosition="4982">t would be interesting to see the log-odds applied in more contexts where log-probabilities are typically used as features. We have presented the intuitive argument that the log-odds may be advantageous because it does not exhibit the 0-1 asymmetry of the log-probability, but it would be satisfying to justify the choice on more theoretical grounds. 4 Relation to Previous Work There is a significant volume of work exploring the use of CRFs for a variety of chunking tasks, including named-entity recognition, gene prediction, shallow parsing and others (Finkel et al., 2005; Culotta et al., 2005; Sha and Pereira, 2003). The current work indicates that these systems might be improved by moving to a semi-CRF model. There have not been a large number of studies using the semi-CRF, but the few that have been done found only marginal improvements over pure CRF systems (Sarawagi and Cohen, 2004; Liang, 2005; Daum´e III and Marcu, 2005). Notably, none of those studies experimented with features of chunk non-boundaries, as is achieved by the use of CRF-type features involving the label C, and we take this to be the reason for their not obtaining higher results. 471 Although it has become fairly common in NLP to use</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>Fei Sha and Fernando Pereira. 2003. Shallow parsing with conditional random fields. Proc. HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huihsin Tseng</author>
<author>Pichuan Chang</author>
<author>Galen Andrew</author>
<author>Daniel Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>A conditional random field word segmenter for sighan bakeoff 2005. In</title>
<date>2005</date>
<booktitle>Proc. Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>168--171</pages>
<contexts>
<context position="2031" citStr="Tseng et al., 2005" startWordPosition="328" endWordPosition="331"> random field (semi-CRF) (Sarawagi and Cohen, 2004). A CRF in its basic form is a model for labeling tokens in a sequence; however it can easily be adapted to perform segmentation via labeling each token as BEGIN or CONTINUATION, or according to some similar scheme. CRFs using this technique have been shown to be very successful at the task of Chinese word segmentation (CWS), starting with the model of Peng et al. (2004). In the Second International Chinese Word Segmentation Bakeoff (Emerson, 2005), two of the highest scoring systems in the closed track competition were based on a CRF model. (Tseng et al., 2005; Asahara et al., 2005) While the CRF is quite effective compared with other models designed for CWS, one wonders whether it may be limited by its restrictive independence assumptions on non-adjacent labels: an order-M CRF satisfies the order-M Markov assumption that, globally conditioned on the input sequence, each label is independent of all other labels given the M labels to its left and right. Consequently, the model only “sees” word boundaries within a moving window of M + 1 characters, which prohibits it from explicitly modeling the tendency of strings longer than that window to form wor</context>
<context position="22823" citStr="Tseng et al. (2005)" startWordPosition="4009" endWordPosition="4012">ing: • The identity of xi−1 and xi • The identity of the character bigram starting at positions i − 2, i − 1 and i • Whether xj and xj+1 are identical, for j = (i − 2) ... i • Whether xj and xj+2 are identical, for j = (i − 3) ... i • Whether the sequence xj ... xj+3 forms an AABB sequence for j = (i − 4) ... i • Whether the sequence xj ... xj+3 forms an ABAB sequence for j = (i − 4) ... i The latter four feature templates are designed to detect character or word reduplication, a morphological phenomenon that can influence word segmentation in Chinese. The first two of these were also used by Tseng et al. (2005). For label bigrams (Li, Li+1), we use the same templates, but extending the range of positions by one to the right.7 Each label uni- or bigram also has a “prior” feature that always fires for that label configuration. All configurations contain the above features for the label unigram B, since these are easily used in either a CRF or semiCRF model. To determine the influence of CRFtype features on performance, we also test configurations in which both B and C label features are used, and configurations using all label uni- and bigrams. In the semi-Markov conditions, we also use as feature tem</context>
<context position="26988" citStr="Tseng et al. (2005)" startWordPosition="4699" endWordPosition="4702">ations, the log conditional odds does much better than log conditional probability. When the log-odds feature is added to the complete CRF model (uni+bi) as the only semi-CRF-type feature, errors are reduced by 24%, compared to only 7.6% for the logprobability. Even when the other semi-CRF-type features are present as well, log-odds reduces error by 13% compared to 2.5% for log-probability. Our best model, combining all features, resulted in an error reduction of 12% over the highest score on this dataset from the 2005 Sighan closed test competition (96.4%), achieved by the pure CRF system of Tseng et al. (2005). 3.3 Discussion Our results indicate that both Markov-type and semi-Markov-type features are useful for generalization to unseen data. This may be because the two types of features are in a sense complementary: semi-Markov-type features such as wordidentity are valuable for modeling the tendency of known strings to segment as words, while label based features are valuable for modeling properties of sub-lexical components such as affixes, helping to generalize to words that have not previously been encountered. We did not explicitly test the utility of CRF-type features for improving recall on</context>
</contexts>
<marker>Tseng, Chang, Andrew, Jurafsky, Manning, 2005</marker>
<rawString>Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky, and Christopher Manning. 2005. A conditional random field word segmenter for sighan bakeoff 2005. In Proc. Fourth SIGHAN Workshop on Chinese Language Processing, pages 168–171.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>