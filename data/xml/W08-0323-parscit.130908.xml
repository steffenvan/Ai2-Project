<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003584">
<title confidence="0.9965565">
Using syntactic coupling features for discriminating phrase-based
translations (WMT-08 Shared Translation Task)
</title>
<author confidence="0.954697">
Vassilina Nikoulina and Marc Dymetman
</author>
<affiliation confidence="0.753869">
Xerox Research Centre Europe
Grenoble, France
</affiliation>
<email confidence="0.999103">
{nikoulina,dymetman}@xrce.xerox.com
</email>
<sectionHeader confidence="0.998601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998593555555556">
Our participation in the shared translation task
at WMT-08 focusses on news translation from
English to French. Our main goal is to con-
trast a baseline version of the phrase-based
MATRAX system, with a version that incor-
porates syntactic “coupling” features in order
to discriminate translations produced by the
baseline system. We report results comparing
different feature combinations.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99966084">
Our goal is to try to improve the fluency and ad-
equacy of a baseline phrase-based SMT system by
using a variety of “syntactic coupling features”, ex-
tracted from parses for the source and target strings.
These features are used for reranking the n-best can-
didates of the baseline system.
The phrase-based SMT system MATRAX, devel-
oped at XRCE, is used as the baseline in the experi-
ments. MATRAX is based on a fairly standard log-
linear model, but one original aspect of the system
is the use of non-contiguous bi-phrases such as ne
... plus / not ... anymore, where words in the source
and target phrases may be separated by gaps, to be
filled at translation time by lexical material provided
by some other such pairs (Simard et al., 2005).
For parsing, we use the Xerox Incremental Parser
XIP (A¨ıt-Mokhtar et al., 2002), which is a robust
dependency parser developed at the Xerox Research
Centre Europe. XIP is fast (around 2000 words per
second for English) and is well adapted to a situ-
ation, like the one we have here, were we need to
parse on the order of a few hundred target candi-
dates on the fly. Also of interest to us is the fact that
XIP produces labelled dependencies, a feature that
we use in some of our experiments.
</bodyText>
<subsectionHeader confidence="0.998258">
1.1 Decoding and Training
</subsectionHeader>
<bodyText confidence="0.999938272727273">
We resort to a standard reranking approach in which
we produce an n-best list of MATRAX candidate
translations (with n = 100 in our experiments), and
then rerank this list with a linear combination of our
parse-dependent features. In order to train the fea-
ture weights, we use an averaged structured percep-
tron approach (Roark et al., 2004), where we try to
learn weights such that the first candidate to emerge
is equal to the “oracle” candidate, that is, the candi-
date that is closest to the reference in terms of NIST
score.
</bodyText>
<subsectionHeader confidence="0.997785">
1.2 Coupling Features
</subsectionHeader>
<bodyText confidence="0.9999778">
Our general approach to computing coupling fea-
tures between the dependency structure of the source
and that of a candidate translation produced by MA-
TRAX is the following: we start by aligning the
words between the source and the candidate trans-
lation, we parse both sides, and we count (possi-
bly according to a weighting scheme) the number of
configurations (“rectangles”) that are of the follow-
ing type: ((s1, s12, s2), (t1, t12, t2)), where s12 is an
edge between s1 and s2, t12 is an edge between t1
and t2, s1 is aligned with t1 and s2 is aligned with
t2. We implemented several variants of this basic
scheme.
We start by describing different “generic” cou-
pling functions derived from the basic scheme, as-
</bodyText>
<page confidence="0.983649">
159
</page>
<note confidence="0.401099">
Proceedings of the Third Workshop on Statistical Machine Translation, pages 159–162,
</note>
<page confidence="0.479698">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.9999776">
suming that word alignments have been already de-
termined, then we describe the option of taking into
account specific dependency labels when counting
rectangles, and finally we describe two options for
computing the word alignments.
</bodyText>
<subsectionHeader confidence="0.535848">
1.2.1 Generic features
</subsectionHeader>
<bodyText confidence="0.997193066666667">
The first measure of coupling is based on sim-
ple, non-weighted, word alignments. Here we sim-
ply consider that a word of the source and a word
of the target are aligned or not aligned, without any
intermediary degree, and consider that a rectangle
exists on the quadruple of words s1, s2, t1, t2 iff si
is aligned to ti, s1 and s2 have a dependency link
between them (in whatever direction) and similarly
for t1 and t2. The first feature that we introduce,
Coupling-Count, is simply the count of all such rect-
angles between the source and the target.
We note that the value of this feature tends to be
correlated with the size of the source and target de-
pendency trees. We therefore introduce some nor-
malized variants of the feature:
</bodyText>
<listItem confidence="0.956532583333333">
• Coupling-Recall. We compute the number of
source edges for which there exists a projec-
tion in the target. More formally, the number of
edges between two words s1, s2 such that there
exist two words t1, t2 with si aligned to ti and
such that t1, t2 have an edge between them. We
then divide this number by the total number of
edges in the source.
• Coupling-Precision. We do the same thing this
time starting from the target.
• Coupling-F-measure. This is defined as the
harmonic mean of the two previous features.
</listItem>
<subsubsectionHeader confidence="0.572588">
1.2.2 Label-specific features
</subsubsectionHeader>
<bodyText confidence="0.999902681818182">
The features previously defined do not take into
account the labels associated with edges in the de-
pendency trees. However, while rectangles of the
form ((s1, subj, s2), (t1, subj, t2)) maybe rather sys-
tematic between such languages as English and
French, other rectangles may be much less so, due
on the one hand to actual linguistic divergences be-
tween the two languages, but also, as importantly
in practice, to different representational conventions
used by different grammar developers for the two
languages.1
In order to control this problem, we introduce a
collection of Label-Specific-Coupling features, each
for a specific pair of source label and target label.
The values of a label-specific feature are the num-
ber of occurrences for this specific label pair. We
use only label pairs that have been observed to be
aligned in the training corpus (that is, that partici-
pate in observed rectangles). In one version of that
approach, we use all such pairs found in the corpus,
in another version only the pairs above a certain fre-
quency threshold in the corpus.
</bodyText>
<subsectionHeader confidence="0.63647">
1.2.3 Alignment
</subsectionHeader>
<bodyText confidence="0.999987555555556">
In order to compute the features described above,
a prerequisite is to be able to determine a word align-
ment between the source and a candidate translation.
Our first approach is to use GIZA++ (correspond-
ing roughly to IBM Model 4) to create these align-
ments, by producing for a given source and a given
candidate translation n-best alignment lists in both
directions and applying standard techniques of sym-
metrization to produce a bidirectional alignment.
Another way to find word alignments is to use the
information provided by the baseline system. Since
MATRAX is a phrase-based system, it has access to
the bi-phrases (aligned by definition) that are used in
order to generate a candidate translation. However
note that when we use a bi-phrase based alignment,
there will be differences from the word alignment
that we discussed before, and we need to adapt our
coupling functions.
</bodyText>
<sectionHeader confidence="0.661048" genericHeader="introduction">
1.2.4 Related approaches
</sectionHeader>
<bodyText confidence="0.9999785">
There is a growing body of work on the use of
syntax for improving the quality of SMT systems.
Our approach is closest to the line taken in (Och
et al., 2003), where syntactic features are also used
for discriminating between candidates produced by
a phrase-based system, but here we introduce and
compare results for a wider variety of coupling fea-
tures, taking into account different combinations in-
volving normalization of the counts, symmetrized
features between the source and target, labelled de-
</bodyText>
<footnote confidence="0.661056666666667">
1Although the XIP formalism is shared between grammar
developers of French and English, the grammars do sometimes
follow different conventions.
</footnote>
<page confidence="0.995655">
160
</page>
<bodyText confidence="0.998181333333333">
pendencies, and also consider several ways for com-
puting the word alignment on the basis of which
edge couplings are determined.
</bodyText>
<sectionHeader confidence="0.998455" genericHeader="method">
2 Experiments
</sectionHeader>
<subsectionHeader confidence="0.870004">
2.1 Description
</subsectionHeader>
<bodyText confidence="0.999912947368421">
Our participation concerns the English to French
News translation task. To train our baseline system
we used the News Commentary corpus, namely the
training (— 1M words) and development (1057 sen-
tences) sets proposed for the shared translation task.
The same development set was used for the MERT
training procedure of the baseline system, as well
as for learning the parameters of the reranking pro-
cedure. Note that the test data on which we report
our experimental results here is the one proposed as
development test set for the News translation task
(1064 sentences, nc-devtest2007).
Using MATRAX as the baseline system we gen-
erate 100-best lists of candidate translations for all
source sentences of the test set, we rerank these can-
didates using our features, and we output the top
candidate. We present our results in Table 1, distin-
guished according to the actual combination of fea-
tures used in each experiment.
</bodyText>
<listItem confidence="0.965892684210526">
• The Baseline entry in the table corresponds to
MATRAX results on the test set, without the
use of any of the coupling features.
• We distinguish two sub-tables, according to
whether Giza-based alignments or phrase-
based alignments were used.
• The Generic keyword corresponds to the cou-
pling features introduced in section 1.2.1, based
on rectangle counts, independent of the labels
of the edges.
• The Matrax keyword corresponds to using
MATRAX “internal” features as reranking fea-
tures, along with the coupling features. These
MATRAX features are pretty standard phrase-
based features, apart from some features deal-
ing explicitly with gapped phrases, and are de-
scribed in detail in (Simard et al., 2005).
• The Labels and Frequent Labels keywords cor-
responds to using label-specific features. In
</listItem>
<bodyText confidence="0.983319">
the first case (Labels) we extracted all of the
aligned label pairs (label pair associated with
a coupling rectangle) found in a training set,
while in the second case (Frequent Labels), we
only kept the most frequently observed among
these label pairs.
</bodyText>
<listItem confidence="0.909366714285714">
• When several keywords appear on a line, we
used the union of the corresponding features,
and in the last line of the table, we show a
combination involving at the same time some
features computed on the basis of Giza-based
alignments and of phrase-based alignments.
• Along with the NIST and BLEU scores of each
</listItem>
<bodyText confidence="0.888690363636364">
combination, we also conducted an informal
manual assessment of the quality of the re-
sults relative to the MATRAX baseline. We
took a random sample of 100 source sentences
from the test set and for each sentence, assessed
whether the first candidate produced by rerank-
ing was better, worse, or indistinguishable in
terms of quality relative to the baseline trans-
lation. We report the number of improvements
(+) and deteriorations (-) among these 100 sam-
ples as well as their difference.2
</bodyText>
<sectionHeader confidence="0.998982" genericHeader="conclusions">
3 Discussion
</sectionHeader>
<bodyText confidence="0.999987357142857">
While the overall results in terms of Bleu and Nist
do not show major improvements relative to the
baseline, there are several interesting observations
to make. First of all, if we focus on feature com-
binations in which MATRAX features are included
(shown in italics in the table), we see that there is a
general tendency for the results, both in terms of au-
tomatic and human evaluations, to be better than for
the same combination without the MATRAX fea-
tures; the explanation seems to be that if we do
not use the MATRAX features during reranking, but
consider the 100 candidates in the n-best list to be
equally valuable from the viewpoint of MATRAX
features, we lose essential information that cannot
</bodyText>
<footnote confidence="0.854944833333333">
2All the results reported here correspond to our own evalu-
ations, prior to the WMT evaluations. Given time constraints,
we focussed more on contrasting the baseline with the baseline
+ coupling features, than in tuning the baseline itself for the
task at hand. After the submission deadline, we were able to
improve the baseline for this task.
</footnote>
<page confidence="0.983951">
161
</page>
<table confidence="0.998235904761905">
NIST BLEU - + Diff
Baseline 6.4093 0.2034 0 0 0
Giza-based alignments
Generic 6.3383 0.2043 15 17 2
Generic, Matrax 6.3782 0.2083 4 18 14
Labels 6.3483 0.1963 12 18 6
Labels, Generic 6.3514 0.2010 3 18 15
Labels, Generic, Matrax 6.4016 0.2075 3 20 17
Frequent Labels 6.3815 0.2054 7 11 4
Frequent Labels, Generic 6.3826 0.2044 6 18 12
Frequent Labels, Generic, Matrax 6.4177 0.2100 2 16 14
Phrase-based alignments
Generic 6.2869 0.1964 12 14 2
Generic, Matrax 6.3972 0.2031 4 11 7
Labels 6.3677 0.1995 16 15 -1
Labels, Generic 6.3567 0.1977 8 15 7
Labels, Generic, Matrax 6.4269 0.2049 4 17 13
Frequent Labels 6.3701 0.1998 3 15 12
Frequent Labels, Generic 6.3846 0.2013 7 16 9
Frequent Labels, Generic, Matrax 6.4160 0.2049 4 16 12
Giza Generic, Phrase Generic, Giza Labels, Matrax 6.4351 0.2060 7 22 15
</table>
<tableCaption confidence="0.99991">
Table 1: Reranking results.
</tableCaption>
<bodyText confidence="0.99990875">
be recovered simply by appeal to the syntactic cou-
pling features.
If we now concentrate on the lines which do in-
clude MATRAX features and compare their results
with the baseline, we see a trend for these results to
be better than the baseline, both in terms of auto-
matic measures as (more strongly) in terms of hu-
man evaluation. Taken individually, perhaps the im-
provements are not very clear, but collectively, a
trend does seem to appear in favor of syntactic cou-
pling features generally, although we have not con-
ducted formal statistical tests to validate this impres-
sion. A more detailed comparison between individ-
ual lines, inside the class of combinations that in-
clude MATRAX features, appears however difficult
to make on the basis of the reported experiments.
</bodyText>
<sectionHeader confidence="0.999643" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999728047619048">
Salah A¨ıt-Mokhtar, Jean-Pierre Chanod, and Claude
Roux. 2002. Robustness beyond shallowness: incre-
mental deep parsing. Natural Language Engineering,
8(3):121–144.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2003. Syn-
tax for Statistical Machine Translation: Final report of
John Hopkins 2003 Summer Workshop. Technical re-
port, John Hopkins University.
B. Roark, M. Saraclar, M. Collins, and M. Johnson.
2004. Discriminative language modeling with condi-
tional random fields and the perceptron algorithm. In
Proceedings of the 42nd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL’04), July.
Michel Simard, Nicola Cancedda, Bruno Cavestro,
Marc Dymetman, ´Eric Gaussier, Cyril Goutte,
Kenji Yamada, Philippe Langlais, and Arne Mauser.
2005. Translating with non-contiguous phrases. In
HLT/EMNLP.
</reference>
<page confidence="0.997776">
162
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.767458">
<title confidence="0.965664">Using syntactic coupling features for discriminating translations (WMT-08 Shared Translation Task)</title>
<author confidence="0.944902">Vassilina Nikoulina</author>
<author confidence="0.944902">Marc</author>
<affiliation confidence="0.997633">Xerox Research Centre</affiliation>
<address confidence="0.853156">Grenoble,</address>
<abstract confidence="0.9994427">Our participation in the shared translation task at WMT-08 focusses on news translation from English to French. Our main goal is to contrast a baseline version of the phrase-based MATRAX system, with a version that incorporates syntactic “coupling” features in order to discriminate translations produced by the baseline system. We report results comparing different feature combinations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Salah A¨ıt-Mokhtar</author>
<author>Jean-Pierre Chanod</author>
<author>Claude Roux</author>
</authors>
<title>Robustness beyond shallowness: incremental deep parsing.</title>
<date>2002</date>
<journal>Natural Language Engineering,</journal>
<volume>8</volume>
<issue>3</issue>
<marker>A¨ıt-Mokhtar, Chanod, Roux, 2002</marker>
<rawString>Salah A¨ıt-Mokhtar, Jean-Pierre Chanod, and Claude Roux. 2002. Robustness beyond shallowness: incremental deep parsing. Natural Language Engineering, 8(3):121–144.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Franz Josef Och</author>
<author>Daniel Gildea</author>
</authors>
<title>Sanjeev Khudanpur, Anoop Sarkar,</title>
<location>Kenji Yamada, Alex Fraser, Shankar Kumar, Libin Shen, David Smith, Katherine Eng,</location>
<marker>Och, Gildea, </marker>
<rawString>Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar, Libin Shen, David Smith, Katherine Eng,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Viren Jain</author>
<author>Zhen Jin</author>
<author>Dragomir Radev</author>
</authors>
<title>Syntax for Statistical Machine Translation: Final report of John Hopkins</title>
<date>2003</date>
<tech>Technical report,</tech>
<institution>John Hopkins University.</institution>
<marker>Jain, Jin, Radev, 2003</marker>
<rawString>Viren Jain, Zhen Jin, and Dragomir Radev. 2003. Syntax for Statistical Machine Translation: Final report of John Hopkins 2003 Summer Workshop. Technical report, John Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
<author>M Saraclar</author>
<author>M Collins</author>
<author>M Johnson</author>
</authors>
<title>Discriminative language modeling with conditional random fields and the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL’04),</booktitle>
<contexts>
<context position="2240" citStr="Roark et al., 2004" startWordPosition="366" endWordPosition="369">d to a situation, like the one we have here, were we need to parse on the order of a few hundred target candidates on the fly. Also of interest to us is the fact that XIP produces labelled dependencies, a feature that we use in some of our experiments. 1.1 Decoding and Training We resort to a standard reranking approach in which we produce an n-best list of MATRAX candidate translations (with n = 100 in our experiments), and then rerank this list with a linear combination of our parse-dependent features. In order to train the feature weights, we use an averaged structured perceptron approach (Roark et al., 2004), where we try to learn weights such that the first candidate to emerge is equal to the “oracle” candidate, that is, the candidate that is closest to the reference in terms of NIST score. 1.2 Coupling Features Our general approach to computing coupling features between the dependency structure of the source and that of a candidate translation produced by MATRAX is the following: we start by aligning the words between the source and the candidate translation, we parse both sides, and we count (possibly according to a weighting scheme) the number of configurations (“rectangles”) that are of the </context>
</contexts>
<marker>Roark, Saraclar, Collins, Johnson, 2004</marker>
<rawString>B. Roark, M. Saraclar, M. Collins, and M. Johnson. 2004. Discriminative language modeling with conditional random fields and the perceptron algorithm. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL’04), July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Simard</author>
<author>Nicola Cancedda</author>
<author>Bruno Cavestro</author>
<author>Marc Dymetman</author>
</authors>
<title>Eric Gaussier, Cyril Goutte,</title>
<date>2005</date>
<booktitle>In HLT/EMNLP.</booktitle>
<location>Kenji Yamada, Philippe Langlais, and</location>
<contexts>
<context position="1381" citStr="Simard et al., 2005" startWordPosition="213" endWordPosition="216">syntactic coupling features”, extracted from parses for the source and target strings. These features are used for reranking the n-best candidates of the baseline system. The phrase-based SMT system MATRAX, developed at XRCE, is used as the baseline in the experiments. MATRAX is based on a fairly standard loglinear model, but one original aspect of the system is the use of non-contiguous bi-phrases such as ne ... plus / not ... anymore, where words in the source and target phrases may be separated by gaps, to be filled at translation time by lexical material provided by some other such pairs (Simard et al., 2005). For parsing, we use the Xerox Incremental Parser XIP (A¨ıt-Mokhtar et al., 2002), which is a robust dependency parser developed at the Xerox Research Centre Europe. XIP is fast (around 2000 words per second for English) and is well adapted to a situation, like the one we have here, were we need to parse on the order of a few hundred target candidates on the fly. Also of interest to us is the fact that XIP produces labelled dependencies, a feature that we use in some of our experiments. 1.1 Decoding and Training We resort to a standard reranking approach in which we produce an n-best list of </context>
<context position="9295" citStr="Simard et al., 2005" startWordPosition="1541" endWordPosition="1544">out the use of any of the coupling features. • We distinguish two sub-tables, according to whether Giza-based alignments or phrasebased alignments were used. • The Generic keyword corresponds to the coupling features introduced in section 1.2.1, based on rectangle counts, independent of the labels of the edges. • The Matrax keyword corresponds to using MATRAX “internal” features as reranking features, along with the coupling features. These MATRAX features are pretty standard phrasebased features, apart from some features dealing explicitly with gapped phrases, and are described in detail in (Simard et al., 2005). • The Labels and Frequent Labels keywords corresponds to using label-specific features. In the first case (Labels) we extracted all of the aligned label pairs (label pair associated with a coupling rectangle) found in a training set, while in the second case (Frequent Labels), we only kept the most frequently observed among these label pairs. • When several keywords appear on a line, we used the union of the corresponding features, and in the last line of the table, we show a combination involving at the same time some features computed on the basis of Giza-based alignments and of phrase-bas</context>
</contexts>
<marker>Simard, Cancedda, Cavestro, Dymetman, 2005</marker>
<rawString>Michel Simard, Nicola Cancedda, Bruno Cavestro, Marc Dymetman, ´Eric Gaussier, Cyril Goutte, Kenji Yamada, Philippe Langlais, and Arne Mauser. 2005. Translating with non-contiguous phrases. In HLT/EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>