<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.914122">
Applying Conditional Random Fields to Japanese Morphological Analysis
</title>
<author confidence="0.899992">
Taku Kudo †∗ Kaoru Yamamoto$ Yuji Matsumoto ††Nara Institute of Science and Technology
</author>
<affiliation confidence="0.622408666666667">
8916-5, Takayama-Cho Ikoma, Nara, 630-0192 Japan
$CREST JST, Tokyo Institute of Technology
4259, Nagatuta Midori-Ku Yokohama, 226-8503 Japan
</affiliation>
<email confidence="0.994472">
taku-ku@is.naist.jp, kaoru@lr.pi.titech.ac.jp, matsu@is.naist.jp
</email>
<sectionHeader confidence="0.997371" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99996385">
This paper presents Japanese morphological analy-
sis based on conditional random fields (CRFs). Pre-
vious work in CRFs assumed that observation se-
quence (word) boundaries were fixed. However,
word boundaries are not clear in Japanese, and
hence a straightforward application of CRFs is not
possible. We show how CRFs can be applied to
situations where word boundary ambiguity exists.
CRFs offer a solution to the long-standing prob-
lems in corpus-based or statistical Japanese mor-
phological analysis. First, flexible feature designs
for hierarchical tagsets become possible. Second,
influences of label and length bias are minimized.
We experiment CRFs on the standard testbed corpus
used for Japanese morphological analysis, and eval-
uate our results using the same experimental dataset
as the HMMs and MEMMs previously reported in
this task. Our results confirm that CRFs not only
solve the long-standing problems but also improve
the performance over HMMs and MEMMs.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9997383125">
Conditional random fields (CRFs) (Lafferty et al.,
2001) applied to sequential labeling problems are
conditional models, trained to discriminate the cor-
rect sequence from all other candidate sequences
without making independence assumption for fea-
tures. They are considered to be the state-of-the-art
framework to date. Empirical successes with CRFs
have been reported recently in part-of-speech tag-
ging (Lafferty et al., 2001), shallow parsing (Sha
and Pereira, 2003), named entity recognition (Mc-
Callum and Li, 2003), Chinese word segmenta-
tion (Peng et al., 2004), and Information Extraction
(Pinto et al., 2003; Peng and McCallum, 2004).
Previous applications with CRFs assumed that
observation sequence (e.g. word) boundaries are
fixed, and the main focus was to predict label
</bodyText>
<page confidence="0.444041333333333">
∗At present, NTT Communication Science Laboratories,
2-4, Hikaridai, Seika-cho, Soraku, Kyoto, 619-0237 Japan
taku@cslab.kecl.ntt.co.jp
</page>
<bodyText confidence="0.999868514285714">
sequence (e.g. part-of-speech). However, word
boundaries are not clear in non-segmented lan-
guages. One has to identify word segmentation as
well as to predict part-of-speech in morphological
analysis of non-segmented languages. In this pa-
per, we show how CRFs can be applied to situations
where word boundary ambiguity exists.
CRFs offer a solution to the problems in Japanese
morphological analysis with hidden Markov models
(HMMs) (e.g., (Asahara and Matsumoto, 2000)) or
with maximum entropy Markov models (MEMMs)
(e.g., (Uchimoto et al., 2001)). First, as HMMs are
generative, it is hard to employ overlapping fea-
tures stemmed from hierarchical tagsets and non-
independent features of the inputs such as surround-
ing words, word suffixes and character types. These
features have usually been ignored in HMMs, de-
spite their effectiveness in unknown word guessing.
Second, as mentioned in the literature, MEMMs
could evade neither from label bias (Lafferty et
al., 2001) nor from length bias (a bias occurring
because of word boundary ambiguity). Easy se-
quences with low entropy are likely to be selected
during decoding in MEMMs. The consequence is
serious especially in Japanese morphological anal-
ysis due to hierarchical tagsets as well as word
boundary ambiguity. The key advantage of CRFs is
their flexibility to include a variety of features while
avoiding these bias.
In what follows, we describe our motivations of
applying CRFs to Japanese morphological analysis
(Section 2). Then, CRFs and their parameter esti-
mation are provided (Section 3). Finally, we dis-
cuss experimental results (Section 4) and give con-
clusions with possible future directions (Section 5).
</bodyText>
<sectionHeader confidence="0.986591" genericHeader="method">
2 Japanese Morphological Analysis
</sectionHeader>
<subsectionHeader confidence="0.910778">
2.1 Word Boundary Ambiguity
</subsectionHeader>
<bodyText confidence="0.937612666666667">
Word boundary ambiguity cannot be ignored when
dealing with non-segmented languages. A simple
approach would be to let a character be a token
(i.e., character-based Begin/Inside tagging) so that
boundary ambiguity never occur (Peng et al., 2004).
Input: “ ” (I live in Metropolis of Tokyo .)
</bodyText>
<figureCaption confidence="0.999873">
Figure 1: Example of lattice for Japanese morphological analysis
</figureCaption>
<figure confidence="0.997620789473684">
BOS
Lattice:
(east)
[Noun]
(Tokyo)
[Noun]
(capital)
[Noun]
(Kyoto)
[Noun]
(Metro.)
[Suffix]
(in)
[Particle]
(resemble)
[Verb]
EOS
(live)
[Verb]
</figure>
<bodyText confidence="0.999640324324324">
However, B/I tagging is not a standard method in
20-year history of corpus-based Japanese morpho-
logical analysis. This is because B/I tagging cannot
directly reflect lexicons which contain prior knowl-
edge about word segmentation. We cannot ignore
a lexicon since over 90% accuracy can be achieved
even using the longest prefix matching with the lex-
icon. Moreover, B/I tagging produces a number
of redundant candidates which makes the decoding
speed slower.
Traditionally in Japanese morphological analysis,
we assume that a lexicon, which lists a pair of a
word and its corresponding part-of-speech, is avail-
able. The lexicon gives a tractable way to build a
lattice from an input sentence. A lattice represents
all candidate paths or all candidate sequences of to-
kens, where each token denotes a word with its part-
of-speech 1.
Figure 1 shows an example where a total of 6
candidate paths are encoded and the optimal path
is marked with bold type. As we see, the set of la-
bels to predict and the set of states in the lattice are
different, unlike English part-of-speech tagging that
word boundary ambiguity does not exist.
Formally, the task of Japanese morphological
analysis can be defined as follows. Let x be an
input, unsegmented sentence. Let y be a path, a
sequence of tokens where each token is a pair of
word wi and its part-of-speech ti. In other words,
y = ((w1, t1), ... , (w#y, t#y)) where #y is the
number of tokens in the path y. Let Y(x) be a set of
candidate paths in a lattice built from the input sen-
tence x and a lexicon. The goal is to select a correct
path yˆ from all candidate paths in the Y(x). The
distinct property of Japanese morphological analy-
sis is that the number of tokens y varies, since the
set of labels and the set of states are not the same.
</bodyText>
<footnote confidence="0.9147532">
1If one cannot build a lattice because no matching word can
be found in the lexicon, unknown word processing is invoked.
Here, candidate tokens are built using character types, such as
hiragana, katakana, Chinese characters, alphabets, and num-
bers.
</footnote>
<subsectionHeader confidence="0.996086">
2.2 Long-Standing Problems
2.2.1 Hierarchical Tagset
</subsectionHeader>
<bodyText confidence="0.996345833333333">
Japanese part-of-speech (POS) tagsets used in
the two major Japanese morphological analyzers
ChaSen2 and JUMAN3 take the form of a hierar-
chical structure. For example, IPA tagset4 used
in ChaSen consists of three categories: part-of-
speech, conjugation form (cform), and conjugate
type (ctype). The cform and ctype are assigned only
to words that conjugate, such as verbs and adjec-
tives. The part-of-speech has at most four levels of
subcategories. The top level has 15 different cate-
gories, such as Noun, Verb, etc. Noun is subdivided
into Common Noun, Proper Noun and so on. Proper
Noun is again subdivided into Person, Organization
or Place, etc. The bottom level can be thought as
the word level (base form) with which we can com-
pletely discriminate all words as different POS. If
we distinguish each branch of the hierarchical tree
as a different label (ignoring the word level), the to-
tal number amounts to about 500, which is much
larger than the typical English POS tagset such as
Penn Treebank.
The major effort has been devoted how to in-
terpolate each level of the hierarchical structure as
well as to exploit atomic features such as word suf-
fixes and character types. If we only use the bot-
tom level, we suffer from the data sparseness prob-
lem. On the other hand, if we use the top level,
we lack in granularity of POS to capture fine dif-
ferences. For instance, some suffixes (e.g., san or
kun) appear after names, and are helpful to detect
words with Name POS. In addition, the conjugation
form (cfrom) must be distinguished appearing only
in the succeeding position in a bi-gram, since it is
dominated by the word appearing in the next.
Asahara et al. extended HMMs so as to incorpo-
rate 1) position-wise grouping, 2) word-level statis-
</bodyText>
<footnote confidence="0.999831666666667">
2http://chasen.naist.jp/
3http://www.kc.t.u-tokyo.ac.jp/nl-resource/juman.html
4http://chasen.naist.jp/stable/ipadic/
</footnote>
<bodyText confidence="0.9996606">
tics, and 3) smoothing of word and POS level statis-
tics (Asahara and Matsumoto, 2000). However, the
proposed method failed to capture non-independent
features such as suffixes and character types and se-
lected smoothing parameters in an ad-hoc way.
</bodyText>
<subsectionHeader confidence="0.956109">
2.2.2 Label Bias and Length Bias
</subsectionHeader>
<bodyText confidence="0.9994175">
It is known that maximum entropy Markov mod-
els (MEMMs) (McCallum et al., 2000) or other dis-
criminative models with independently trained next-
state classifiers potentially suffer from the label bias
(Lafferty et al., 2001) and length bias. In Japanese
morphological analysis, they are extremely serious
problems. This is because, as shown in Figure 1,
the branching variance is considerably high, and
the number of tokens varies according to the output
path.
</bodyText>
<equation confidence="0.99987275">
P(A, D  |x) = 0.6 * 0.6 * 1.0 = 0.36
P(B, E  |x) = 0.4 * 1.0 * 1.0 = 0.4
P(A, D  |x) = 0.6 * 0.6 * 1.0 = 0.36
P(B  |x) = 0.4 * 1.0 = 0.4
</equation>
<figureCaption confidence="0.997317">
Figure 2: Label and length bias in a lattice
</figureCaption>
<bodyText confidence="0.9997325">
An example of the label bias is illus-
trated in Figure 2:(a) where the path is
searched by sequential combinations of
maximum entropy models (MEMMs), i.e.,
</bodyText>
<equation confidence="0.999117">
P(y|x) = II#y
i=1 p((wi, ti)|(wi−1, ti−1)). Even
</equation>
<bodyText confidence="0.999903548387097">
if MEMMs learn the correct path A-D with in-
dependently trained maximum entropy models,
the path B-E will have a higher probability and
then be selected in decoding. This is because the
token B has only the single outgoing token E, and
the transition probability for B-E is always 1.0.
Generally speaking, the complexities of transitions
vary according to the tokens, and the transition
probabilities with low-entropy will be estimated
high in decoding. This problem occurs because the
training is performed only using the correct path,
ignoring all other transitions.
Moreover, we cannot ignore the influence of the
length bias either. By the length bias, we mean that
short paths, consisting of a small number of tokens,
are preferred to long path. Even if the transition
probability of each token is small, the total proba-
bility of the path will be amplified when the path is
short 2:(b)). Length bias occurs in Japanese mor-
phological analysis because the number of output
tokens y varies by use of prior lexicons.
Uchimoto et al. attempted a variant of MEMMs
for Japanese morphological analysis with a number
of features including suffixes and character types
(Uchimoto et al., 2001; Uchimoto et al., 2002;
Uchimoto et al., 2003). Although the performance
of unknown words were improved, that of known
words degraded due to the label and length bias.
Wrong segmentation had been reported in sentences
which are analyzed correctly by naive rule-based or
HMMs-based analyzers.
</bodyText>
<sectionHeader confidence="0.986961" genericHeader="method">
3 Conditional Random Fields
</sectionHeader>
<bodyText confidence="0.999654454545454">
Conditional random fields (CRFs) (Lafferty et al.,
2001) overcome the problems described in Sec-
tion 2.2. CRFs are discriminative models and can
thus capture many correlated features of the inputs.
This allows flexible feature designs for hierarchical
tagsets. CRFs have a single exponential model for
the joint probability of the entire paths given the in-
put sentence, while MEMMs consist of a sequential
combination of exponential models, each of which
estimates a conditional probability of next tokens
given the current state. This minimizes the influ-
ences of the label and length bias.
As explained in Section 2.1, there is word bound-
ary ambiguity in Japanese, and we choose to use
a lattice instead of B/I tagging. This implies that
the set of labels and the set of states are differ-
ent, and the number of tokens #y varies accord-
ing to a path. In order to accomodate this, we de-
fine CRFs for Japanese morphological analysis as
the conditional probability of an output path y =
((w1, t1), ... , (w#y, t#y)) given an input sequence
x:
</bodyText>
<equation confidence="0.995413333333333">
1 #Y
P(y|x) = ZX exp(
i=1
</equation>
<bodyText confidence="0.986">
where Zx is a normalization factor over all candi-
date paths, i.e.,
</bodyText>
<equation confidence="0.34425">
)λkfk((w� i−1,t� i−1), (w� i,t� i)) ,
</equation>
<figure confidence="0.988643033333333">
(a) Label bias
0.4
1.0
C
0.6
1.0
D
0.6
A
0.4
1.0
E
B
1.0
BOS
EOS
(b) Length bias
BOS
0.4
0.6
A
0.4
0.6
B
D
C
1.0
1.0
1.0
EOS
</figure>
<equation confidence="0.993438181818182">
P(A,D|x) &lt; P(B,E|x)
P(A,D|x) &lt; P(B |x)
E )λkfk((wi−1,ti−1), (wi,ti)) ,
k
EZX =
Y&apos;EY(X)
E
k
exp(
#Y&apos; E
i=1
</equation>
<bodyText confidence="0.970694076923077">
fk(hwi−1, ti−1i, hwi, tii) is an arbitrary feature func-
tion over i-th token hwi, tii, and its previous token
hwi−1, ti−1i 5. λk(∈ A = {λ1, ... , λK} ∈ RK) is a
learned weight or parameter associated with feature
function fk.
Note that our formulation of CRFs is different
from the widely-used formulations (e.g., (Sha and
Pereira, 2003; McCallum and Li, 2003; Peng et
al., 2004; Pinto et al., 2003; Peng and McCallum,
2004)). The previous applications of CRFs assign
a conditional probability for a label sequence y =
y1, ... , yT given an input sequence x = x1, ... , xT
as:
</bodyText>
<equation confidence="0.998582">
1 T
P(y |x) = Z exp (� λkfk (yi−1, yi, x))
x i=1 k
</equation>
<bodyText confidence="0.986479833333333">
In our formulation, CRFs deal with word boundary
ambiguity. Thus, the the size of output sequence T
is not fixed through all candidates y ∈ Y(x). The
index i is not tied with the input x as in the original
CRFs, but unique to the output y ∈ Y(x).
Here, we introduce the global feature vec-
</bodyText>
<equation confidence="0.755979666666667">
tor F(y, x) = {F1(y, x), ... , FK(y, x)}, where
Fk(y, x) = �#y
i=1 fk(hwi−1, ti−1i, hwi, tii). Using
</equation>
<bodyText confidence="0.96003775">
the global feature vector, P(y|x) can also be rep-
resented as P(y|x) = Zx1 exp(A · F(y, x)). The
most probable path yˆ for the input sentence x is then
given by
</bodyText>
<equation confidence="0.9992935">
yˆ = argmax P(y|x) = argmax A · F(y, x),
yEY(x) yEY(x)
</equation>
<bodyText confidence="0.918769444444444">
To maximize LΛ, we have to maximize the dif-
ference between the inner product (or score) of the
correct path A · F(yj, xj) and those of all other
candidates A · F(y, xj), y ∈ Y(xj). CRFs is
thus trained to discriminate the correct path from
all other candidates, which reduces the influences
of the label and length bias in encoding.
At the optimal point, the first-derivative of the
log-likelihood becomes 0, thus,
</bodyText>
<equation confidence="0.8348465">
(Fk(yj,xj) − EP(y|xj) [Fk(y,xj)])
= Ok − Ek = 0,
</equation>
<bodyText confidence="0.999845333333333">
where Ok = Ej Fk(yj, xj) is the count of fea-
ture k observed in the training data T, and Ek =
Ej EP(y|xj)[Fk(y, xj)] is the expectation of fea-
ture k over the model distribution P(y|x) and T.
The expectation can efficiently be calculated using
a variant of the forward-backward algorithm.
</bodyText>
<equation confidence="0.998726333333333">
EP(y|x)[Fk(y,x)] =
E α(w0,t0) · fk · exp(Ek0 λk0fk0) · β(w,t)
{(w0,t0),(w,t)}EB(x)
</equation>
<bodyText confidence="0.9992868">
where fk is an abbreviation for fk(hw&apos;, t&apos;i, hw, ti),
B(x) is a set of all bi-gram sequences observed
in the lattice for x, and α(w,t) and β(w,t) are the
forward-backward costs given by the following re-
cursive definitions:
</bodyText>
<equation confidence="0.5011684">
�=
j
δLΛ
δλk
Zx ,
</equation>
<bodyText confidence="0.98871575">
which can be found with the Viterbi algorithm.
An interesting note is that the decoding process of
CRFs can be reduced into a simple linear combina-
tions over all global features.
</bodyText>
<equation confidence="0.99960575">
�α(w,t) = α(w0,t0) · exp(E λkfk(hw&apos;, t&apos;i, hw, ti))
(w0,t0)ELT((w,t)) k
�β(w,t) = β(w0,t0) · exp(E λkfk(hw,ti, hw&apos;, t&apos;i)),
(w0,t0)ERT((w,t)) k
</equation>
<bodyText confidence="0.980259909090909">
where LT (hw, ti) and RT (hw, ti) denote a set of
tokens each of which connects to the token hw, ti
from the left and the right respectively. Note that
initial costs of two virtual tokens, α(wbos,tbos) and
β(weos,teos), are set to be 1. A normalization constant
is then given by Zx = α(weos,teos)(= β(wbos,tbos)).
We attempt two types of regularizations in order
to avoid overfitting. They are a Gaussian prior (L2-
norm) (Chen and Rosenfeld, 1999) and a Laplacian
prior (L1-norm) (Goodman, 2004; Peng and Mc-
Callum, 2004)
</bodyText>
<table confidence="0.469731">
�LΛ = C )) — 1 { Ek |λk |(L1-norm)
j log(P(yj|xj 2 Ek |λk|2 (L2-norm)
</table>
<subsectionHeader confidence="0.962036">
3.1 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.885148">
CRFs are trained using the standard maximum
likelihood estimation, i.e., maximizing the log-
likelihood LΛ of a given training set T =
{hxj,yji}N j=1,
</bodyText>
<footnote confidence="0.959039">
5We could use trigram or more general n-gram feature func-
tions (e.g., fk((wi−n, ti−n), ... , (wi, ti))), however we restrict
ourselves to bi-gram features for clarity.
</footnote>
<equation confidence="0.986843357142857">
Aˆ = argmax LΛ, where
ΛERK
�LΛ = log(P(yj|xj))
j
� �
A · F(yj, xj) − log(Zxj) .
�=
j
L � �
log
exp (A · [F(yj, xj) − F(y, xj)]))]
�=
j
yEY(xj)
</equation>
<bodyText confidence="0.940464857142857">
Below, we refer to CRFs with L1-norm and L2-
norm regularization as L1-CRFs and L2-CRFs re-
spectively. The parameter C E R+ is a hyperpa-
rameter of CRFs determined by a cross validation.
L1-CRFs can be reformulated into the con-
strained optimization problem below by letting
Ak = A+k − A−k :
</bodyText>
<equation confidence="0.861117">
�max: C log(P (Yj|Xj)) − � (A+k + A−k )/2
j k
s.t., A+k &gt; 0, A−k &gt; 0.
</equation>
<bodyText confidence="0.813166939393939">
At the optimal point, the following Karush-Kuhun-
Tucker conditions satisfy: A+k · [C · (Ok − Ek) −
1/2] = 0, A−k · [C · (Ek − Ok) − 1/2] = 0, and
|C · (Ok − Ek) |&lt; 1/2. These conditions mean
that both A+k and A−k are set to be 0 (i.e., Ak = 0),
when |C · (Ok − Ek) |&lt; 1/2. A non-zero weight
is assigned to Ak, only when |C · (Ok − Ek) |=
1/2. L2-CRFs, in contrast, give the optimal solution
when δLΛ δλk = C · (Ok − Ek) − Ak = 0. Omitting the
proof, (Ok − Ek) =� 0 can be shown and L2-CRFs
thus give a non-sparse solution where all Ak have
non-zero weights.
The relationship between two reguralizations
have been studied in Machine Learning community.
(Perkins et al., 2003) reported that L1-regularizer
should be chosen for a problem where most of given
features are irrelevant. On the other hand, L2-
regularizer should be chosen when most of given
features are relevant. An advantage of L1-based
regularizer is that it often leads to sparse solutions
where most of Ak are exactly 0. The features as-
signed zero weight are thought as irrelevant fea-
tures to classifications. The L2-based regularizer,
also seen in SVMs, produces a non-sparse solution
where all of Ak have non-zero weights. All features
are used with L2-CRFs.
The optimal solutions of L2-CRFs can be ob-
tained by using traditional iterative scaling algo-
rithms (e.g., IIS or GIS (Pietra et al., 1997)) or more
efficient quasi-Newton methods (e.g., L-BFGS (Liu
and Nocedal, 1989)). For L1-CRFs, constrained op-
timizers (e.g., L-BFGS-B (Byrd et al., 1995)) can be
used.
</bodyText>
<sectionHeader confidence="0.998699" genericHeader="evaluation">
4 Experiments and Discussion
</sectionHeader>
<subsectionHeader confidence="0.955925">
4.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.99937056">
We use two widely-used Japanese annotated cor-
pora in the research community, Kyoto Univer-
sity Corpus ver 2.0 (KC) and RWCP Text Corpus
(RWCP), for our experiments on CRFs. Note that
each corpus has a different POS tagset and details
(e.g., size of training and test dataset) are summa-
rized in Table 1.
One of the advantages of CRFs is that they are
flexible enough to capture many correlated fea-
tures, including overlapping and non-independent
features. We thus use as many features as possi-
ble, which could not be used in HMMs. Table 2
summarizes the set of feature templates used in the
KC data. The templates for RWCP are essentially
the same as those of KC except for the maximum
level of POS subcatgeories. Word-level templates
are employed when the words are lexicalized, i.e.,
those that belong to particle, auxiliary verb, or suf-
fix6. For an unknown word, length of the word, up
to 2 suffixes/prefixes and character types are used
as the features. We use all features observed in the
lattice without any cut-off thresholds. Table 1 also
includes the number of features in both data sets.
We evaluate performance with the standard F-
score (Fβ=1) defined as follows:
</bodyText>
<equation confidence="0.6096705">
= 2 · Recall · Precision
Fβ=1 Recall + Precision ,
</equation>
<bodyText confidence="0.8778188">
where Recall = # of correct tokens
# of tokens in test corpus
# of correct tokens
P recision = .
# of tokens in system output
In the evaluations of F-scores, three criteria of cor-
rectness are used: seg: (only the word segmentation
is evaluated), top: (word segmentation and the top
level of POS are evaluated), and all: (all informa-
tion is used for evaluation).
The hyperparameters C for L1-CRFs and L2-
CRFs are selected by cross-validation. Experiments
are implemented in C++ and executed on Linux
with XEON 2.8 GHz dual processors and 4.0 Gbyte
of main memory.
</bodyText>
<subsectionHeader confidence="0.689736">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.996719">
Tables 3 and 4 show experimental results using
KC and RWCP respectively. The three F-scores
(seg/top/all) for our CRFs and a baseline bi-gram
HMMs are listed.
In Table 3 (KC data set), the results of a variant
of maximum entropy Markov models (MEMMs)
(Uchimoto et al., 2001) and a rule-based analyzer
(JUMAN7) are also shown. To make a fare compar-
ison, we use exactly the same data as (Uchimoto et
al., 2001).
In Table 4 (RWCP data set), the result of an ex-
tended Hidden Markov Models (E-HMMs) (Asa-
</bodyText>
<footnote confidence="0.9981758">
6These lexicalizations are usually employed in Japanese
morphological analysis.
7JUMAN assigns “unknown POS” to the words not seen in
the lexicon. We simply replace the POS of these words with
the default POS, Noun-SAHEN.
</footnote>
<tableCaption confidence="0.995478">
Table 1: Details of Data Set
</tableCaption>
<table confidence="0.999893888888889">
KC RWCP
source Mainich News Article (’95) Mainich News Article (’94)
lexicon (# of words) JUMAN ver. 3.61 (1,983,173) IPADIC ver. 2.7.0 (379,010)
POS structure 2-levels POS, cfrom, ctype, base form 4-levels POS, cfrom, ctype, base form
# of training sentences 7,958 (Articles on Jan. 1st - Jan. 8th) 10,000 (first 10,000 sentences)
# of training tokens 198,514 265,631
# of test sentences 1,246 (Articles on Jan. 9th) 25,743 (all remaining sentences)
# of test tokens 31,302 655,710
# of features 791,798 580,032
</table>
<tableCaption confidence="0.8836038">
Table 2: Feature templates: fk((w&apos;, t&apos;), (w, t))
t0 = (p10, p20, cf0, ct, bw0), t = (p1, p2, cf, ct, bw), where p10/p1
and p20/p2 are the top and sub categories of POS. cf0/cf and ct0/ct
are the cfrom and ctype respectively. bw0/bw are the base form of the
words w0/w.
</tableCaption>
<table confidence="0.886192411764706">
type template
Unigram (p1)
basic features (p1, p2)
w is known (bw)
(bw, p1)
(bw, p1, p2)
w is unknown length of the word w
up to 2 suffixes x {0, (p1), (p1, p2)}
up to 2 prefixes x {0, (p1), (p1, p2)}
character type x {0, (p1), (p1, p2)}
Bigram (p10, p1)
basic features (p10, p1, p2)
(p10, p20, p1)
(p10, p20, p1, p2)
(p10, p20, cf0, p1, p2)
(p10, p20, ct0, p1, p2)
(p10, p20, cf0, ct0, p1, p2)
</table>
<figureCaption confidence="0.863911">
(p10, p20, p1, p2, cf)
(p10, p20, p1, p2, ct)
(p10, p20, p1, p2, cf, ct)
(p10, p20, cf0, p1, p2, cf)
(p10, p20, ct, p1, p2, ct)
(p10, p20, cf0, p1, p2, ct)
(p10, p20, ct0, p1, p2, cf)
(p10, p20, cf0, ct0, p1, p2, cf, ct)
w0 is lexicalized (p10, p20, cf0, ct0, bw0, p1, p2)
(p10, p20, cf0, ct0, bw0, p1, p2, cf)
(p10, p20, cf0, ct0, bw0, p1, p2, ct)
(p10, p20, cf0, ct0, bw0, p1, p2, cf, ct)
w is lexicalized (p10, p20, p1, p2, cf, ct, bw)
(p10, p20, cf0, p1, p2, cf, ct, bw)
(p10, p20, ct0, p1, p2, cf, ct, bw)
(p10, p20, cf0, ct0, p1, p2, cf, ct, bw)
w0/w are lexicalized (p10, p20, cf0, ct0, bw0, p1, p2, cf, ct, bw)
</figureCaption>
<bodyText confidence="0.999925733333333">
hara and Matsumoto, 2000) trained and tested with
the same corpus is also shown. E-HMMs is applied
to the current implementation of ChaSen. Details of
E-HMMs are described in Section 4.3.2.
We directly evaluated the difference of these sys-
tems using McNemar’s test. Since there are no
standard methods to evaluate the significance of F
scores, we convert the outputs into the character-
based B/I labels and then employ a McNemar’s
paired test on the labeling disagreements. This eval-
uation was also used in (Sha and Pereira, 2003). The
results of McNemar’s test suggest that L2-CRFs is
significantly better than other systems including L1-
CRFs8. The overall results support our empirical
success of morphological analysis based on CRFs.
</bodyText>
<subsectionHeader confidence="0.9394705">
4.3 Discussion
4.3.1 CRFs and MEMMs
</subsectionHeader>
<bodyText confidence="0.999884612903226">
Uchimoto el al. proposed a variant of MEMMs
trained with a number of features (Uchimoto et al.,
2001). Although they improved the accuracy for un-
known words, they fail to segment some sentences
which are correctly segmented with HMMs or rule-
based analyzers.
Figure 3 illustrates the sentences which are incor-
rectly segmented by Uchimoto’s MEMMs. The cor-
rect paths are indicated by bold boxes. Uchimoto et
al. concluded that these errors were caused by non-
standard entries in the lexicon. In Figure 3, “ロマ
ンは” (romanticist) and “ない心” (one’s heart) are
unusual spellings and they are normally written as
“ロマン派” and “内心” respectively. However, we
conjecture that these errors are caused by the influ-
ence of the length bias. To support our claim, these
sentences are correctly segmented by CRFs, HMMs
and rule-based analyzers using the same lexicon as
(Uchimoto et al., 2001). By the length bias, short
paths are preferred to long paths. Thus, single to-
ken “ロマンは” or “ない心” is likely to be selected
compared to multiple tokens “ロマン / は” or “な
い / 心”. Moreover, “ロマン” and “ロマンは” have
exactly the same POS (Noun), and transition proba-
bilities of these tokens become almost equal. Con-
sequentially, there is no choice but to select a short
path (single token) in order to maximize the whole
sentence probability.
Table 5 summarizes the number of errors in
HMMs, CRFs and MEMMs, using the KC data set.
Two types of errors, l-error and s-error, are given in
</bodyText>
<footnote confidence="0.947839">
8In all cases, the p-values are less than 1.0 x 10−4.
</footnote>
<tableCaption confidence="0.995843">
Table 3: Results of KC, (Fβ=1 (precision/recall))
</tableCaption>
<table confidence="0.999459833333333">
system seg top all
L2-CRFs (C =1.2) 98.96 (99.04/98.88) 98.31 (98.39/98.22) 96.75 (96.83/96.67)
L1-CRFs (C =3.0) 98.80 (98.84/98.77) 98.14 (98.18/98.11) 96.55 (96.58/96.51)
MEMMs (Uchimoto 01) 96.44 (95.78/97.10) 95.81 (95.15/96.47) 94.27 (93.62/94.92)
JUMAN (rule-based) 98.70 (98.88/98.51) 98.09 (98.27/97.91) 93.73 (93.91/93.56)
HMMs-bigram (baseline) 96.22 (96.16/96.28) 94.96 (94.90/95.02) 91.85 (91.79/91.90)
</table>
<tableCaption confidence="0.991051">
Table 4: Results of RWCP, (Fβ=1 (precision/recall))
</tableCaption>
<table confidence="0.9518672">
system seg top all
L2-CRFs (C =2.4) 99.11 (99.03/99.20) 98.73 (98.65/98.81) 97.66 (97.58/97.75)
L1-CRFs (C =3.0) 99.00 (98.86/99.13) 98.58 (98.44/98.72) 97.30 (97.16/97.43)
E-HMMs (Asahara 00) 98.87 (98.77/98.97) 98.33 (98.23/98.43) 96.95 (96.85/97.04)
HMMs-bigram (baseline) 98.82 (98.69/98.94) 98.10 (97.97/98.22) 95.90 (95.78/96.03)
</table>
<figure confidence="0.569978333333333">
MEMMs select
The romance on the sea they bet is ...
MEMMs select
</figure>
<figureCaption confidence="0.961607">
Figure 3: Errors with MEMMs
</figureCaption>
<tableCaption confidence="0.7211175">
(Correct paths are marked with bold boxes.)
Table 5: Number of errors in KC dataset
</tableCaption>
<table confidence="0.99243175">
# of l-errors # of s-errors
CRFs 79 (40%) 120 (60%)
HMMs 306 (44%) 387 (56%)
MEMMs 416 (70%) 183 (30%)
</table>
<tableCaption confidence="0.4459605">
l-error: output longer token than correct one
s-error: output shorter token than correct one
</tableCaption>
<bodyText confidence="0.999878">
this table. l-error (or s-error) means that a system
incorrectly outputs a longer (or shorter) token than
the correct token respectively. By length bias, long
tokens are preferred to short tokens. Thus, larger
number of l-errors implies that the result is highly
influenced by the length bias.
While the relative rates of l-error and s-error are
almost the same in HMMs and CRFs, the number
of l-errors with MEMMs amounts to 416, which
is 70% of total errors, and is even larger than that
of naive HMMs (306). This result supports our
claim that MEMMs is not sufficient to be applied to
Japanese morphological analysis where the length
bias is inevitable.
</bodyText>
<subsubsectionHeader confidence="0.680096">
4.3.2 CRFs and Extended-HMMs
</subsubsectionHeader>
<bodyText confidence="0.999957619047619">
Asahara et al. extended the original HMMs by 1)
position-wise grouping of POS tags, 2) word-level
statistics, and 3) smoothing of word and POS level
statistics (Asahara and Matsumoto, 2000). All of
these techniques are designed to capture hierarchi-
cal structures of POS tagsets. For instance, in the
position-wise grouping, optimal levels of POS hier-
archies are changed according to the contexts. Best
hierarchies for each context are selected by hand-
crafted rules or automatic error-driven procedures.
CRFs can realize such extensions naturally and
straightforwardly. In CRFs, position-wise grouping
and word-POS smoothing are simply integrated into
a design of feature functions. Parameters λk for
each feature are automatically configured by gen-
eral maximum likelihood estimation. As shown in
Table 2, we can employ a number of templates to
capture POS hierarchies. Furthermore, some over-
lapping features (e.g., forms and types of conjuga-
tion) can be used, which was not possible in the ex-
tended HMMs.
</bodyText>
<subsubsectionHeader confidence="0.588528">
4.3.3 L1-CRFs and L2-CRFs
</subsubsectionHeader>
<bodyText confidence="0.999728916666667">
L2-CRFs perform slightly better than L1-CRFs,
which indicates that most of given features
(i.e., overlapping features, POS hierarchies, suf-
fixes/prefixes and character types) are relevant to
both of two datasets. The numbers of active (non-
zero) features used in L1-CRFs are much smaller
(about 1/8 - 1/6) than those in L2-CRFs: (L2-
CRFs: 791,798 (KC) / 580,032 (RWCP) v.s., L1-
CRFs: 90,163 (KC) / 101,757 (RWCP)). L1-CRFs
are worth being examined if there are some practi-
cal constraints (e.g., limits of memory, disk or CPU
resources).
</bodyText>
<figure confidence="0.996595153846154">
sea
particle
bet
romance
romanticist
particle
A heart which beats rough waves is ...
rough waves
particle
lose
not
one’s heart
heart
</figure>
<sectionHeader confidence="0.954713" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9999665">
In this paper, we present how conditional random
fields can be applied to Japanese morphological
analysis in which word boundary ambiguity exists.
By virtue of CRFs, 1) a number of correlated fea-
tures for hierarchical tagsets can be incorporated
which was not possible in HMMs, and 2) influences
of label and length bias are minimized which caused
errors in MEMMs. We compare results between
CRFs, MEMMs and HMMs in two Japanese anno-
tated corpora, and CRFs outperform the other ap-
proaches. Although we discuss Japanese morpho-
logical analysis, the proposed approach can be ap-
plicable to other non-segmented languages such as
Chinese or Thai.
There exist some phenomena which cannot be an-
alyzed only with bi-gram features in Japanese mor-
phological analysis. To improve accuracy, tri-gram
or more general n-gram features would be useful.
CRFs have capability of handling such features.
However, the numbers of features and nodes in the
lattice increase exponentially as longer contexts are
captured. To deal with longer contexts, we need a
practical feature selection which effectively trades
between accuracy and efficiency. For this challenge,
McCallum proposes an interesting research avenue
to explore (McCallum, 2003).
</bodyText>
<sectionHeader confidence="0.999196" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999822333333333">
We would like to thank Kiyotaka Uchimoto and
Masayuki Asahara, who explained the details of
their Japanese morphological analyzers.
</bodyText>
<sectionHeader confidence="0.999245" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999003027777778">
Masayuki Asahara and Yuji Matsumoto. 2000. Ex-
tended models and tools for high-performance
part-of-speech tagger. In Proc of COLING, pages
21–27.
Richard H. Byrd, Peihuang Lu, Jorge Nocedal, and
Ci You Zhu. 1995. A limited memory algorithm
for bound constrained optimization. SIAM Jour-
nal on Scientific Computing, 16(6):1190–1208.
Stanley F. Chen and Ronald. Rosenfeld. 1999. A
gaussian prior for smoothing maximum entropy
models. Technical report, Carnegie Mellon Uni-
versity.
Joshua Goodman. 2004. Exponential priors
for maximum entropy models. In Proc. of
HLT/NAACL.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proc. ofICML, pages 282–289.
Dong C. Liu and Jorge Nocedal. 1989. On the
limited memory BFGS method for large scale
optimization. Math. Programming, 45(3, (Ser.
B)):503–528.
Andrew McCallum and Wei Li. 2003. Early re-
sults for named entity recognition with condi-
tional random fields, feature induction and web-
enhanced lexicons. In In Proc. of CoNLL.
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum entropy markov mod-
els for information and segmentation. In Proc. of
ICML, pages 591–598.
Andrew McCallum. 2003. Efficiently inducing fea-
tures of conditional random fields. In Nineteenth
Conference on Uncertainty in Artificial Intelli-
gence (UAI03).
Fuchun Peng and Andrew McCallum. 2004. Accu-
rate information extraction from research papers.
In Proc. ofHLT/NAACL.
Fuchun Peng, Fangfang Feng, and Andrew McCal-
lum. 2004. Chinese segmentation and new word
detection using conditional random fields (to ap-
pear). In Proc. of COLING.
Simon Perkins, Kevin Lacker, and James Thiler.
2003. Grafting: Fast, incremental feature selec-
tion by gradient descent in function space. JMLR,
3:1333–1356.
Della Pietra, Stephen, Vincent J. Della Pietra, and
John D. Lafferty. 1997. Inducing features of ran-
dom fields. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 19(4):380–393.
David Pinto, Andrew McCallum, Xing Wei, and
W. Bruce Croft. 2003. Table extraction using
conditional random fields. In In Proc. of SIGIR,
pages 235–242.
Fei Sha and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. In Proc. of
HLT-NAACL, pages 213–220.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi
Isahara. 2001. The unknown word problem: a
morphological analysis of Japanese using maxi-
mum entropy aided by a dictionary. In Proc. of
EMNLP, pages 91–99.
Kiyotaka Uchimoto, Chikashi Nobata, Atsushi
Yamada, Satoshi Sekine, and Hitoshi Isahara.
2002. Morphological analysis of the spontaneous
speech corpus. In Proc of COLING, pages 1298–
1302.
Kiyotaka Uchimoto, Chikashi Nobata, Atsushi Ya-
mada, and Hitoshi Isahara Satoshi Sekine. 2003.
Morphological analysis of a large spontaneous
speech corpus in Japanese. In Proc. of ACL,
pages 479–488.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.151468">
<title confidence="0.984592">Applying Conditional Random Fields to Japanese Morphological Analysis</title>
<author confidence="0.474585">Kudo Matsumoto Institute of Science</author>
<phone confidence="0.274345">8916-5, Takayama-Cho Ikoma, Nara, 630-0192</phone>
<affiliation confidence="0.533228">JST, Tokyo Institute of</affiliation>
<address confidence="0.928079">4259, Nagatuta Midori-Ku Yokohama, 226-8503</address>
<email confidence="0.993983">taku-ku@is.naist.jp,kaoru@lr.pi.titech.ac.jp,matsu@is.naist.jp</email>
<abstract confidence="0.998894428571428">This paper presents Japanese morphological analysis based on conditional random fields (CRFs). Previous work in CRFs assumed that observation sequence (word) boundaries were fixed. However, word boundaries are not clear in Japanese, and hence a straightforward application of CRFs is not possible. We show how CRFs can be applied to situations where word boundary ambiguity exists. CRFs offer a solution to the long-standing problems in corpus-based or statistical Japanese morphological analysis. First, flexible feature designs for hierarchical tagsets become possible. Second, influences of label and length bias are minimized. We experiment CRFs on the standard testbed corpus used for Japanese morphological analysis, and evaluate our results using the same experimental dataset as the HMMs and MEMMs previously reported in this task. Our results confirm that CRFs not only solve the long-standing problems but also improve the performance over HMMs and MEMMs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Masayuki Asahara</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Extended models and tools for high-performance part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In Proc of COLING,</booktitle>
<pages>21--27</pages>
<contexts>
<context position="2739" citStr="Asahara and Matsumoto, 2000" startWordPosition="387" endWordPosition="390">s was to predict label ∗At present, NTT Communication Science Laboratories, 2-4, Hikaridai, Seika-cho, Soraku, Kyoto, 619-0237 Japan taku@cslab.kecl.ntt.co.jp sequence (e.g. part-of-speech). However, word boundaries are not clear in non-segmented languages. One has to identify word segmentation as well as to predict part-of-speech in morphological analysis of non-segmented languages. In this paper, we show how CRFs can be applied to situations where word boundary ambiguity exists. CRFs offer a solution to the problems in Japanese morphological analysis with hidden Markov models (HMMs) (e.g., (Asahara and Matsumoto, 2000)) or with maximum entropy Markov models (MEMMs) (e.g., (Uchimoto et al., 2001)). First, as HMMs are generative, it is hard to employ overlapping features stemmed from hierarchical tagsets and nonindependent features of the inputs such as surrounding words, word suffixes and character types. These features have usually been ignored in HMMs, despite their effectiveness in unknown word guessing. Second, as mentioned in the literature, MEMMs could evade neither from label bias (Lafferty et al., 2001) nor from length bias (a bias occurring because of word boundary ambiguity). Easy sequences with lo</context>
<context position="8536" citStr="Asahara and Matsumoto, 2000" startWordPosition="1339" endWordPosition="1342"> of POS to capture fine differences. For instance, some suffixes (e.g., san or kun) appear after names, and are helpful to detect words with Name POS. In addition, the conjugation form (cfrom) must be distinguished appearing only in the succeeding position in a bi-gram, since it is dominated by the word appearing in the next. Asahara et al. extended HMMs so as to incorporate 1) position-wise grouping, 2) word-level statis2http://chasen.naist.jp/ 3http://www.kc.t.u-tokyo.ac.jp/nl-resource/juman.html 4http://chasen.naist.jp/stable/ipadic/ tics, and 3) smoothing of word and POS level statistics (Asahara and Matsumoto, 2000). However, the proposed method failed to capture non-independent features such as suffixes and character types and selected smoothing parameters in an ad-hoc way. 2.2.2 Label Bias and Length Bias It is known that maximum entropy Markov models (MEMMs) (McCallum et al., 2000) or other discriminative models with independently trained nextstate classifiers potentially suffer from the label bias (Lafferty et al., 2001) and length bias. In Japanese morphological analysis, they are extremely serious problems. This is because, as shown in Figure 1, the branching variance is considerably high, and the </context>
<context position="26844" citStr="Asahara and Matsumoto, 2000" startWordPosition="4552" endWordPosition="4555">result is highly influenced by the length bias. While the relative rates of l-error and s-error are almost the same in HMMs and CRFs, the number of l-errors with MEMMs amounts to 416, which is 70% of total errors, and is even larger than that of naive HMMs (306). This result supports our claim that MEMMs is not sufficient to be applied to Japanese morphological analysis where the length bias is inevitable. 4.3.2 CRFs and Extended-HMMs Asahara et al. extended the original HMMs by 1) position-wise grouping of POS tags, 2) word-level statistics, and 3) smoothing of word and POS level statistics (Asahara and Matsumoto, 2000). All of these techniques are designed to capture hierarchical structures of POS tagsets. For instance, in the position-wise grouping, optimal levels of POS hierarchies are changed according to the contexts. Best hierarchies for each context are selected by handcrafted rules or automatic error-driven procedures. CRFs can realize such extensions naturally and straightforwardly. In CRFs, position-wise grouping and word-POS smoothing are simply integrated into a design of feature functions. Parameters λk for each feature are automatically configured by general maximum likelihood estimation. As sh</context>
</contexts>
<marker>Asahara, Matsumoto, 2000</marker>
<rawString>Masayuki Asahara and Yuji Matsumoto. 2000. Extended models and tools for high-performance part-of-speech tagger. In Proc of COLING, pages 21–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard H Byrd</author>
<author>Peihuang Lu</author>
<author>Jorge Nocedal</author>
<author>Ci You Zhu</author>
</authors>
<title>A limited memory algorithm for bound constrained optimization.</title>
<date>1995</date>
<journal>SIAM Journal on Scientific Computing,</journal>
<volume>16</volume>
<issue>6</issue>
<contexts>
<context position="18056" citStr="Byrd et al., 1995" startWordPosition="3057" endWordPosition="3060"> regularizer is that it often leads to sparse solutions where most of Ak are exactly 0. The features assigned zero weight are thought as irrelevant features to classifications. The L2-based regularizer, also seen in SVMs, produces a non-sparse solution where all of Ak have non-zero weights. All features are used with L2-CRFs. The optimal solutions of L2-CRFs can be obtained by using traditional iterative scaling algorithms (e.g., IIS or GIS (Pietra et al., 1997)) or more efficient quasi-Newton methods (e.g., L-BFGS (Liu and Nocedal, 1989)). For L1-CRFs, constrained optimizers (e.g., L-BFGS-B (Byrd et al., 1995)) can be used. 4 Experiments and Discussion 4.1 Experimental Settings We use two widely-used Japanese annotated corpora in the research community, Kyoto University Corpus ver 2.0 (KC) and RWCP Text Corpus (RWCP), for our experiments on CRFs. Note that each corpus has a different POS tagset and details (e.g., size of training and test dataset) are summarized in Table 1. One of the advantages of CRFs is that they are flexible enough to capture many correlated features, including overlapping and non-independent features. We thus use as many features as possible, which could not be used in HMMs. T</context>
</contexts>
<marker>Byrd, Lu, Nocedal, Zhu, 1995</marker>
<rawString>Richard H. Byrd, Peihuang Lu, Jorge Nocedal, and Ci You Zhu. 1995. A limited memory algorithm for bound constrained optimization. SIAM Journal on Scientific Computing, 16(6):1190–1208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rosenfeld</author>
</authors>
<title>A gaussian prior for smoothing maximum entropy models.</title>
<date>1999</date>
<tech>Technical report,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="15553" citStr="Rosenfeld, 1999" startWordPosition="2593" endWordPosition="2594">over all global features. �α(w,t) = α(w0,t0) · exp(E λkfk(hw&apos;, t&apos;i, hw, ti)) (w0,t0)ELT((w,t)) k �β(w,t) = β(w0,t0) · exp(E λkfk(hw,ti, hw&apos;, t&apos;i)), (w0,t0)ERT((w,t)) k where LT (hw, ti) and RT (hw, ti) denote a set of tokens each of which connects to the token hw, ti from the left and the right respectively. Note that initial costs of two virtual tokens, α(wbos,tbos) and β(weos,teos), are set to be 1. A normalization constant is then given by Zx = α(weos,teos)(= β(wbos,tbos)). We attempt two types of regularizations in order to avoid overfitting. They are a Gaussian prior (L2- norm) (Chen and Rosenfeld, 1999) and a Laplacian prior (L1-norm) (Goodman, 2004; Peng and McCallum, 2004) �LΛ = C )) — 1 { Ek |λk |(L1-norm) j log(P(yj|xj 2 Ek |λk|2 (L2-norm) 3.1 Parameter Estimation CRFs are trained using the standard maximum likelihood estimation, i.e., maximizing the loglikelihood LΛ of a given training set T = {hxj,yji}N j=1, 5We could use trigram or more general n-gram feature functions (e.g., fk((wi−n, ti−n), ... , (wi, ti))), however we restrict ourselves to bi-gram features for clarity. Aˆ = argmax LΛ, where ΛERK �LΛ = log(P(yj|xj)) j � � A · F(yj, xj) − log(Zxj) . �= j L � � log exp (A · [F(yj, xj)</context>
</contexts>
<marker>Rosenfeld, 1999</marker>
<rawString>Stanley F. Chen and Ronald. Rosenfeld. 1999. A gaussian prior for smoothing maximum entropy models. Technical report, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Exponential priors for maximum entropy models.</title>
<date>2004</date>
<booktitle>In Proc. of HLT/NAACL.</booktitle>
<contexts>
<context position="15600" citStr="Goodman, 2004" startWordPosition="2600" endWordPosition="2601">(E λkfk(hw&apos;, t&apos;i, hw, ti)) (w0,t0)ELT((w,t)) k �β(w,t) = β(w0,t0) · exp(E λkfk(hw,ti, hw&apos;, t&apos;i)), (w0,t0)ERT((w,t)) k where LT (hw, ti) and RT (hw, ti) denote a set of tokens each of which connects to the token hw, ti from the left and the right respectively. Note that initial costs of two virtual tokens, α(wbos,tbos) and β(weos,teos), are set to be 1. A normalization constant is then given by Zx = α(weos,teos)(= β(wbos,tbos)). We attempt two types of regularizations in order to avoid overfitting. They are a Gaussian prior (L2- norm) (Chen and Rosenfeld, 1999) and a Laplacian prior (L1-norm) (Goodman, 2004; Peng and McCallum, 2004) �LΛ = C )) — 1 { Ek |λk |(L1-norm) j log(P(yj|xj 2 Ek |λk|2 (L2-norm) 3.1 Parameter Estimation CRFs are trained using the standard maximum likelihood estimation, i.e., maximizing the loglikelihood LΛ of a given training set T = {hxj,yji}N j=1, 5We could use trigram or more general n-gram feature functions (e.g., fk((wi−n, ti−n), ... , (wi, ti))), however we restrict ourselves to bi-gram features for clarity. Aˆ = argmax LΛ, where ΛERK �LΛ = log(P(yj|xj)) j � � A · F(yj, xj) − log(Zxj) . �= j L � � log exp (A · [F(yj, xj) − F(y, xj)]))] �= j yEY(xj) Below, we refer to</context>
</contexts>
<marker>Goodman, 2004</marker>
<rawString>Joshua Goodman. 2004. Exponential priors for maximum entropy models. In Proc. of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>Proc. ofICML,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="1409" citStr="Lafferty et al., 2001" startWordPosition="196" endWordPosition="199">-standing problems in corpus-based or statistical Japanese morphological analysis. First, flexible feature designs for hierarchical tagsets become possible. Second, influences of label and length bias are minimized. We experiment CRFs on the standard testbed corpus used for Japanese morphological analysis, and evaluate our results using the same experimental dataset as the HMMs and MEMMs previously reported in this task. Our results confirm that CRFs not only solve the long-standing problems but also improve the performance over HMMs and MEMMs. 1 Introduction Conditional random fields (CRFs) (Lafferty et al., 2001) applied to sequential labeling problems are conditional models, trained to discriminate the correct sequence from all other candidate sequences without making independence assumption for features. They are considered to be the state-of-the-art framework to date. Empirical successes with CRFs have been reported recently in part-of-speech tagging (Lafferty et al., 2001), shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003), Chinese word segmentation (Peng et al., 2004), and Information Extraction (Pinto et al., 2003; Peng and McCallum, 2004). Previous applic</context>
<context position="3240" citStr="Lafferty et al., 2001" startWordPosition="466" endWordPosition="469"> to the problems in Japanese morphological analysis with hidden Markov models (HMMs) (e.g., (Asahara and Matsumoto, 2000)) or with maximum entropy Markov models (MEMMs) (e.g., (Uchimoto et al., 2001)). First, as HMMs are generative, it is hard to employ overlapping features stemmed from hierarchical tagsets and nonindependent features of the inputs such as surrounding words, word suffixes and character types. These features have usually been ignored in HMMs, despite their effectiveness in unknown word guessing. Second, as mentioned in the literature, MEMMs could evade neither from label bias (Lafferty et al., 2001) nor from length bias (a bias occurring because of word boundary ambiguity). Easy sequences with low entropy are likely to be selected during decoding in MEMMs. The consequence is serious especially in Japanese morphological analysis due to hierarchical tagsets as well as word boundary ambiguity. The key advantage of CRFs is their flexibility to include a variety of features while avoiding these bias. In what follows, we describe our motivations of applying CRFs to Japanese morphological analysis (Section 2). Then, CRFs and their parameter estimation are provided (Section 3). Finally, we discu</context>
<context position="8953" citStr="Lafferty et al., 2001" startWordPosition="1404" endWordPosition="1407">is2http://chasen.naist.jp/ 3http://www.kc.t.u-tokyo.ac.jp/nl-resource/juman.html 4http://chasen.naist.jp/stable/ipadic/ tics, and 3) smoothing of word and POS level statistics (Asahara and Matsumoto, 2000). However, the proposed method failed to capture non-independent features such as suffixes and character types and selected smoothing parameters in an ad-hoc way. 2.2.2 Label Bias and Length Bias It is known that maximum entropy Markov models (MEMMs) (McCallum et al., 2000) or other discriminative models with independently trained nextstate classifiers potentially suffer from the label bias (Lafferty et al., 2001) and length bias. In Japanese morphological analysis, they are extremely serious problems. This is because, as shown in Figure 1, the branching variance is considerably high, and the number of tokens varies according to the output path. P(A, D |x) = 0.6 * 0.6 * 1.0 = 0.36 P(B, E |x) = 0.4 * 1.0 * 1.0 = 0.4 P(A, D |x) = 0.6 * 0.6 * 1.0 = 0.36 P(B |x) = 0.4 * 1.0 = 0.4 Figure 2: Label and length bias in a lattice An example of the label bias is illustrated in Figure 2:(a) where the path is searched by sequential combinations of maximum entropy models (MEMMs), i.e., P(y|x) = II#y i=1 p((wi, ti)|(</context>
<context position="11134" citStr="Lafferty et al., 2001" startWordPosition="1777" endWordPosition="1780">lysis because the number of output tokens y varies by use of prior lexicons. Uchimoto et al. attempted a variant of MEMMs for Japanese morphological analysis with a number of features including suffixes and character types (Uchimoto et al., 2001; Uchimoto et al., 2002; Uchimoto et al., 2003). Although the performance of unknown words were improved, that of known words degraded due to the label and length bias. Wrong segmentation had been reported in sentences which are analyzed correctly by naive rule-based or HMMs-based analyzers. 3 Conditional Random Fields Conditional random fields (CRFs) (Lafferty et al., 2001) overcome the problems described in Section 2.2. CRFs are discriminative models and can thus capture many correlated features of the inputs. This allows flexible feature designs for hierarchical tagsets. CRFs have a single exponential model for the joint probability of the entire paths given the input sentence, while MEMMs consist of a sequential combination of exponential models, each of which estimates a conditional probability of next tokens given the current state. This minimizes the influences of the label and length bias. As explained in Section 2.1, there is word boundary ambiguity in J</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. ofICML, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong C Liu</author>
<author>Jorge Nocedal</author>
</authors>
<title>On the limited memory BFGS method for large scale optimization.</title>
<date>1989</date>
<journal>Math. Programming,</journal>
<volume>45</volume>
<issue>3</issue>
<contexts>
<context position="17982" citStr="Liu and Nocedal, 1989" startWordPosition="3046" endWordPosition="3049">d be chosen when most of given features are relevant. An advantage of L1-based regularizer is that it often leads to sparse solutions where most of Ak are exactly 0. The features assigned zero weight are thought as irrelevant features to classifications. The L2-based regularizer, also seen in SVMs, produces a non-sparse solution where all of Ak have non-zero weights. All features are used with L2-CRFs. The optimal solutions of L2-CRFs can be obtained by using traditional iterative scaling algorithms (e.g., IIS or GIS (Pietra et al., 1997)) or more efficient quasi-Newton methods (e.g., L-BFGS (Liu and Nocedal, 1989)). For L1-CRFs, constrained optimizers (e.g., L-BFGS-B (Byrd et al., 1995)) can be used. 4 Experiments and Discussion 4.1 Experimental Settings We use two widely-used Japanese annotated corpora in the research community, Kyoto University Corpus ver 2.0 (KC) and RWCP Text Corpus (RWCP), for our experiments on CRFs. Note that each corpus has a different POS tagset and details (e.g., size of training and test dataset) are summarized in Table 1. One of the advantages of CRFs is that they are flexible enough to capture many correlated features, including overlapping and non-independent features. We</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>Dong C. Liu and Jorge Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Math. Programming, 45(3, (Ser. B)):503–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Wei Li</author>
</authors>
<title>Early results for named entity recognition with conditional random fields, feature induction and webenhanced lexicons. In</title>
<date>2003</date>
<booktitle>In Proc. of CoNLL.</booktitle>
<contexts>
<context position="1871" citStr="McCallum and Li, 2003" startWordPosition="262" endWordPosition="266">y solve the long-standing problems but also improve the performance over HMMs and MEMMs. 1 Introduction Conditional random fields (CRFs) (Lafferty et al., 2001) applied to sequential labeling problems are conditional models, trained to discriminate the correct sequence from all other candidate sequences without making independence assumption for features. They are considered to be the state-of-the-art framework to date. Empirical successes with CRFs have been reported recently in part-of-speech tagging (Lafferty et al., 2001), shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003), Chinese word segmentation (Peng et al., 2004), and Information Extraction (Pinto et al., 2003; Peng and McCallum, 2004). Previous applications with CRFs assumed that observation sequence (e.g. word) boundaries are fixed, and the main focus was to predict label ∗At present, NTT Communication Science Laboratories, 2-4, Hikaridai, Seika-cho, Soraku, Kyoto, 619-0237 Japan taku@cslab.kecl.ntt.co.jp sequence (e.g. part-of-speech). However, word boundaries are not clear in non-segmented languages. One has to identify word segmentation as well as to predict part-of-speech in morphological analysis o</context>
<context position="12836" citStr="McCallum and Li, 2003" startWordPosition="2090" endWordPosition="2093">(w� i−1,t� i−1), (w� i,t� i)) , (a) Label bias 0.4 1.0 C 0.6 1.0 D 0.6 A 0.4 1.0 E B 1.0 BOS EOS (b) Length bias BOS 0.4 0.6 A 0.4 0.6 B D C 1.0 1.0 1.0 EOS P(A,D|x) &lt; P(B,E|x) P(A,D|x) &lt; P(B |x) E )λkfk((wi−1,ti−1), (wi,ti)) , k EZX = Y&apos;EY(X) E k exp( #Y&apos; E i=1 fk(hwi−1, ti−1i, hwi, tii) is an arbitrary feature function over i-th token hwi, tii, and its previous token hwi−1, ti−1i 5. λk(∈ A = {λ1, ... , λK} ∈ RK) is a learned weight or parameter associated with feature function fk. Note that our formulation of CRFs is different from the widely-used formulations (e.g., (Sha and Pereira, 2003; McCallum and Li, 2003; Peng et al., 2004; Pinto et al., 2003; Peng and McCallum, 2004)). The previous applications of CRFs assign a conditional probability for a label sequence y = y1, ... , yT given an input sequence x = x1, ... , xT as: 1 T P(y |x) = Z exp (� λkfk (yi−1, yi, x)) x i=1 k In our formulation, CRFs deal with word boundary ambiguity. Thus, the the size of output sequence T is not fixed through all candidates y ∈ Y(x). The index i is not tied with the input x as in the original CRFs, but unique to the output y ∈ Y(x). Here, we introduce the global feature vector F(y, x) = {F1(y, x), ... , FK(y, x)}, w</context>
</contexts>
<marker>McCallum, Li, 2003</marker>
<rawString>Andrew McCallum and Wei Li. 2003. Early results for named entity recognition with conditional random fields, feature induction and webenhanced lexicons. In In Proc. of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Dayne Freitag</author>
<author>Fernando Pereira</author>
</authors>
<title>Maximum entropy markov models for information and segmentation.</title>
<date>2000</date>
<booktitle>In Proc. of ICML,</booktitle>
<pages>591--598</pages>
<contexts>
<context position="8810" citStr="McCallum et al., 2000" startWordPosition="1383" endWordPosition="1386">is dominated by the word appearing in the next. Asahara et al. extended HMMs so as to incorporate 1) position-wise grouping, 2) word-level statis2http://chasen.naist.jp/ 3http://www.kc.t.u-tokyo.ac.jp/nl-resource/juman.html 4http://chasen.naist.jp/stable/ipadic/ tics, and 3) smoothing of word and POS level statistics (Asahara and Matsumoto, 2000). However, the proposed method failed to capture non-independent features such as suffixes and character types and selected smoothing parameters in an ad-hoc way. 2.2.2 Label Bias and Length Bias It is known that maximum entropy Markov models (MEMMs) (McCallum et al., 2000) or other discriminative models with independently trained nextstate classifiers potentially suffer from the label bias (Lafferty et al., 2001) and length bias. In Japanese morphological analysis, they are extremely serious problems. This is because, as shown in Figure 1, the branching variance is considerably high, and the number of tokens varies according to the output path. P(A, D |x) = 0.6 * 0.6 * 1.0 = 0.36 P(B, E |x) = 0.4 * 1.0 * 1.0 = 0.4 P(A, D |x) = 0.6 * 0.6 * 1.0 = 0.36 P(B |x) = 0.4 * 1.0 = 0.4 Figure 2: Label and length bias in a lattice An example of the label bias is illustrate</context>
</contexts>
<marker>McCallum, Freitag, Pereira, 2000</marker>
<rawString>Andrew McCallum, Dayne Freitag, and Fernando Pereira. 2000. Maximum entropy markov models for information and segmentation. In Proc. of ICML, pages 591–598.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
</authors>
<title>Efficiently inducing features of conditional random fields.</title>
<date>2003</date>
<booktitle>In Nineteenth Conference on Uncertainty in Artificial Intelligence (UAI03).</booktitle>
<marker>McCallum, 2003</marker>
<rawString>Andrew McCallum. 2003. Efficiently inducing features of conditional random fields. In Nineteenth Conference on Uncertainty in Artificial Intelligence (UAI03).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Andrew McCallum</author>
</authors>
<title>Accurate information extraction from research papers.</title>
<date>2004</date>
<booktitle>In Proc. ofHLT/NAACL.</booktitle>
<contexts>
<context position="1992" citStr="Peng and McCallum, 2004" startWordPosition="282" endWordPosition="285">dom fields (CRFs) (Lafferty et al., 2001) applied to sequential labeling problems are conditional models, trained to discriminate the correct sequence from all other candidate sequences without making independence assumption for features. They are considered to be the state-of-the-art framework to date. Empirical successes with CRFs have been reported recently in part-of-speech tagging (Lafferty et al., 2001), shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003), Chinese word segmentation (Peng et al., 2004), and Information Extraction (Pinto et al., 2003; Peng and McCallum, 2004). Previous applications with CRFs assumed that observation sequence (e.g. word) boundaries are fixed, and the main focus was to predict label ∗At present, NTT Communication Science Laboratories, 2-4, Hikaridai, Seika-cho, Soraku, Kyoto, 619-0237 Japan taku@cslab.kecl.ntt.co.jp sequence (e.g. part-of-speech). However, word boundaries are not clear in non-segmented languages. One has to identify word segmentation as well as to predict part-of-speech in morphological analysis of non-segmented languages. In this paper, we show how CRFs can be applied to situations where word boundary ambiguity exi</context>
<context position="12901" citStr="Peng and McCallum, 2004" startWordPosition="2102" endWordPosition="2105">.0 D 0.6 A 0.4 1.0 E B 1.0 BOS EOS (b) Length bias BOS 0.4 0.6 A 0.4 0.6 B D C 1.0 1.0 1.0 EOS P(A,D|x) &lt; P(B,E|x) P(A,D|x) &lt; P(B |x) E )λkfk((wi−1,ti−1), (wi,ti)) , k EZX = Y&apos;EY(X) E k exp( #Y&apos; E i=1 fk(hwi−1, ti−1i, hwi, tii) is an arbitrary feature function over i-th token hwi, tii, and its previous token hwi−1, ti−1i 5. λk(∈ A = {λ1, ... , λK} ∈ RK) is a learned weight or parameter associated with feature function fk. Note that our formulation of CRFs is different from the widely-used formulations (e.g., (Sha and Pereira, 2003; McCallum and Li, 2003; Peng et al., 2004; Pinto et al., 2003; Peng and McCallum, 2004)). The previous applications of CRFs assign a conditional probability for a label sequence y = y1, ... , yT given an input sequence x = x1, ... , xT as: 1 T P(y |x) = Z exp (� λkfk (yi−1, yi, x)) x i=1 k In our formulation, CRFs deal with word boundary ambiguity. Thus, the the size of output sequence T is not fixed through all candidates y ∈ Y(x). The index i is not tied with the input x as in the original CRFs, but unique to the output y ∈ Y(x). Here, we introduce the global feature vector F(y, x) = {F1(y, x), ... , FK(y, x)}, where Fk(y, x) = �#y i=1 fk(hwi−1, ti−1i, hwi, tii). Using the glo</context>
<context position="15626" citStr="Peng and McCallum, 2004" startWordPosition="2602" endWordPosition="2606">i, hw, ti)) (w0,t0)ELT((w,t)) k �β(w,t) = β(w0,t0) · exp(E λkfk(hw,ti, hw&apos;, t&apos;i)), (w0,t0)ERT((w,t)) k where LT (hw, ti) and RT (hw, ti) denote a set of tokens each of which connects to the token hw, ti from the left and the right respectively. Note that initial costs of two virtual tokens, α(wbos,tbos) and β(weos,teos), are set to be 1. A normalization constant is then given by Zx = α(weos,teos)(= β(wbos,tbos)). We attempt two types of regularizations in order to avoid overfitting. They are a Gaussian prior (L2- norm) (Chen and Rosenfeld, 1999) and a Laplacian prior (L1-norm) (Goodman, 2004; Peng and McCallum, 2004) �LΛ = C )) — 1 { Ek |λk |(L1-norm) j log(P(yj|xj 2 Ek |λk|2 (L2-norm) 3.1 Parameter Estimation CRFs are trained using the standard maximum likelihood estimation, i.e., maximizing the loglikelihood LΛ of a given training set T = {hxj,yji}N j=1, 5We could use trigram or more general n-gram feature functions (e.g., fk((wi−n, ti−n), ... , (wi, ti))), however we restrict ourselves to bi-gram features for clarity. Aˆ = argmax LΛ, where ΛERK �LΛ = log(P(yj|xj)) j � � A · F(yj, xj) − log(Zxj) . �= j L � � log exp (A · [F(yj, xj) − F(y, xj)]))] �= j yEY(xj) Below, we refer to CRFs with L1-norm and L2-</context>
</contexts>
<marker>Peng, McCallum, 2004</marker>
<rawString>Fuchun Peng and Andrew McCallum. 2004. Accurate information extraction from research papers. In Proc. ofHLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Fangfang Feng</author>
<author>Andrew McCallum</author>
</authors>
<title>Chinese segmentation and new word detection using conditional random fields (to appear).</title>
<date>2004</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="1918" citStr="Peng et al., 2004" startWordPosition="271" endWordPosition="274"> the performance over HMMs and MEMMs. 1 Introduction Conditional random fields (CRFs) (Lafferty et al., 2001) applied to sequential labeling problems are conditional models, trained to discriminate the correct sequence from all other candidate sequences without making independence assumption for features. They are considered to be the state-of-the-art framework to date. Empirical successes with CRFs have been reported recently in part-of-speech tagging (Lafferty et al., 2001), shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003), Chinese word segmentation (Peng et al., 2004), and Information Extraction (Pinto et al., 2003; Peng and McCallum, 2004). Previous applications with CRFs assumed that observation sequence (e.g. word) boundaries are fixed, and the main focus was to predict label ∗At present, NTT Communication Science Laboratories, 2-4, Hikaridai, Seika-cho, Soraku, Kyoto, 619-0237 Japan taku@cslab.kecl.ntt.co.jp sequence (e.g. part-of-speech). However, word boundaries are not clear in non-segmented languages. One has to identify word segmentation as well as to predict part-of-speech in morphological analysis of non-segmented languages. In this paper, we sh</context>
<context position="4249" citStr="Peng et al., 2004" startWordPosition="621" endWordPosition="624">g these bias. In what follows, we describe our motivations of applying CRFs to Japanese morphological analysis (Section 2). Then, CRFs and their parameter estimation are provided (Section 3). Finally, we discuss experimental results (Section 4) and give conclusions with possible future directions (Section 5). 2 Japanese Morphological Analysis 2.1 Word Boundary Ambiguity Word boundary ambiguity cannot be ignored when dealing with non-segmented languages. A simple approach would be to let a character be a token (i.e., character-based Begin/Inside tagging) so that boundary ambiguity never occur (Peng et al., 2004). Input: “ ” (I live in Metropolis of Tokyo .) Figure 1: Example of lattice for Japanese morphological analysis BOS Lattice: (east) [Noun] (Tokyo) [Noun] (capital) [Noun] (Kyoto) [Noun] (Metro.) [Suffix] (in) [Particle] (resemble) [Verb] EOS (live) [Verb] However, B/I tagging is not a standard method in 20-year history of corpus-based Japanese morphological analysis. This is because B/I tagging cannot directly reflect lexicons which contain prior knowledge about word segmentation. We cannot ignore a lexicon since over 90% accuracy can be achieved even using the longest prefix matching with the</context>
<context position="12855" citStr="Peng et al., 2004" startWordPosition="2094" endWordPosition="2097">t� i)) , (a) Label bias 0.4 1.0 C 0.6 1.0 D 0.6 A 0.4 1.0 E B 1.0 BOS EOS (b) Length bias BOS 0.4 0.6 A 0.4 0.6 B D C 1.0 1.0 1.0 EOS P(A,D|x) &lt; P(B,E|x) P(A,D|x) &lt; P(B |x) E )λkfk((wi−1,ti−1), (wi,ti)) , k EZX = Y&apos;EY(X) E k exp( #Y&apos; E i=1 fk(hwi−1, ti−1i, hwi, tii) is an arbitrary feature function over i-th token hwi, tii, and its previous token hwi−1, ti−1i 5. λk(∈ A = {λ1, ... , λK} ∈ RK) is a learned weight or parameter associated with feature function fk. Note that our formulation of CRFs is different from the widely-used formulations (e.g., (Sha and Pereira, 2003; McCallum and Li, 2003; Peng et al., 2004; Pinto et al., 2003; Peng and McCallum, 2004)). The previous applications of CRFs assign a conditional probability for a label sequence y = y1, ... , yT given an input sequence x = x1, ... , xT as: 1 T P(y |x) = Z exp (� λkfk (yi−1, yi, x)) x i=1 k In our formulation, CRFs deal with word boundary ambiguity. Thus, the the size of output sequence T is not fixed through all candidates y ∈ Y(x). The index i is not tied with the input x as in the original CRFs, but unique to the output y ∈ Y(x). Here, we introduce the global feature vector F(y, x) = {F1(y, x), ... , FK(y, x)}, where Fk(y, x) = �#y</context>
</contexts>
<marker>Peng, Feng, McCallum, 2004</marker>
<rawString>Fuchun Peng, Fangfang Feng, and Andrew McCallum. 2004. Chinese segmentation and new word detection using conditional random fields (to appear). In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Perkins</author>
<author>Kevin Lacker</author>
<author>James Thiler</author>
</authors>
<title>Grafting: Fast, incremental feature selection by gradient descent in function space.</title>
<date>2003</date>
<journal>JMLR,</journal>
<pages>3--1333</pages>
<contexts>
<context position="17214" citStr="Perkins et al., 2003" startWordPosition="2921" endWordPosition="2924">er conditions satisfy: A+k · [C · (Ok − Ek) − 1/2] = 0, A−k · [C · (Ek − Ok) − 1/2] = 0, and |C · (Ok − Ek) |&lt; 1/2. These conditions mean that both A+k and A−k are set to be 0 (i.e., Ak = 0), when |C · (Ok − Ek) |&lt; 1/2. A non-zero weight is assigned to Ak, only when |C · (Ok − Ek) |= 1/2. L2-CRFs, in contrast, give the optimal solution when δLΛ δλk = C · (Ok − Ek) − Ak = 0. Omitting the proof, (Ok − Ek) =� 0 can be shown and L2-CRFs thus give a non-sparse solution where all Ak have non-zero weights. The relationship between two reguralizations have been studied in Machine Learning community. (Perkins et al., 2003) reported that L1-regularizer should be chosen for a problem where most of given features are irrelevant. On the other hand, L2- regularizer should be chosen when most of given features are relevant. An advantage of L1-based regularizer is that it often leads to sparse solutions where most of Ak are exactly 0. The features assigned zero weight are thought as irrelevant features to classifications. The L2-based regularizer, also seen in SVMs, produces a non-sparse solution where all of Ak have non-zero weights. All features are used with L2-CRFs. The optimal solutions of L2-CRFs can be obtained</context>
</contexts>
<marker>Perkins, Lacker, Thiler, 2003</marker>
<rawString>Simon Perkins, Kevin Lacker, and James Thiler. 2003. Grafting: Fast, incremental feature selection by gradient descent in function space. JMLR, 3:1333–1356.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Della Pietra</author>
<author>Vincent J Della Pietra Stephen</author>
<author>John D Lafferty</author>
</authors>
<title>Inducing features of random fields.</title>
<date>1997</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="17904" citStr="Pietra et al., 1997" startWordPosition="3035" endWordPosition="3038">t of given features are irrelevant. On the other hand, L2- regularizer should be chosen when most of given features are relevant. An advantage of L1-based regularizer is that it often leads to sparse solutions where most of Ak are exactly 0. The features assigned zero weight are thought as irrelevant features to classifications. The L2-based regularizer, also seen in SVMs, produces a non-sparse solution where all of Ak have non-zero weights. All features are used with L2-CRFs. The optimal solutions of L2-CRFs can be obtained by using traditional iterative scaling algorithms (e.g., IIS or GIS (Pietra et al., 1997)) or more efficient quasi-Newton methods (e.g., L-BFGS (Liu and Nocedal, 1989)). For L1-CRFs, constrained optimizers (e.g., L-BFGS-B (Byrd et al., 1995)) can be used. 4 Experiments and Discussion 4.1 Experimental Settings We use two widely-used Japanese annotated corpora in the research community, Kyoto University Corpus ver 2.0 (KC) and RWCP Text Corpus (RWCP), for our experiments on CRFs. Note that each corpus has a different POS tagset and details (e.g., size of training and test dataset) are summarized in Table 1. One of the advantages of CRFs is that they are flexible enough to capture ma</context>
</contexts>
<marker>Pietra, Stephen, Lafferty, 1997</marker>
<rawString>Della Pietra, Stephen, Vincent J. Della Pietra, and John D. Lafferty. 1997. Inducing features of random fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(4):380–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Pinto</author>
<author>Andrew McCallum</author>
<author>Xing Wei</author>
<author>W Bruce Croft</author>
</authors>
<title>Table extraction using conditional random fields. In</title>
<date>2003</date>
<booktitle>In Proc. of SIGIR,</booktitle>
<pages>235--242</pages>
<contexts>
<context position="1966" citStr="Pinto et al., 2003" startWordPosition="278" endWordPosition="281">tion Conditional random fields (CRFs) (Lafferty et al., 2001) applied to sequential labeling problems are conditional models, trained to discriminate the correct sequence from all other candidate sequences without making independence assumption for features. They are considered to be the state-of-the-art framework to date. Empirical successes with CRFs have been reported recently in part-of-speech tagging (Lafferty et al., 2001), shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003), Chinese word segmentation (Peng et al., 2004), and Information Extraction (Pinto et al., 2003; Peng and McCallum, 2004). Previous applications with CRFs assumed that observation sequence (e.g. word) boundaries are fixed, and the main focus was to predict label ∗At present, NTT Communication Science Laboratories, 2-4, Hikaridai, Seika-cho, Soraku, Kyoto, 619-0237 Japan taku@cslab.kecl.ntt.co.jp sequence (e.g. part-of-speech). However, word boundaries are not clear in non-segmented languages. One has to identify word segmentation as well as to predict part-of-speech in morphological analysis of non-segmented languages. In this paper, we show how CRFs can be applied to situations where w</context>
<context position="12875" citStr="Pinto et al., 2003" startWordPosition="2098" endWordPosition="2101">bias 0.4 1.0 C 0.6 1.0 D 0.6 A 0.4 1.0 E B 1.0 BOS EOS (b) Length bias BOS 0.4 0.6 A 0.4 0.6 B D C 1.0 1.0 1.0 EOS P(A,D|x) &lt; P(B,E|x) P(A,D|x) &lt; P(B |x) E )λkfk((wi−1,ti−1), (wi,ti)) , k EZX = Y&apos;EY(X) E k exp( #Y&apos; E i=1 fk(hwi−1, ti−1i, hwi, tii) is an arbitrary feature function over i-th token hwi, tii, and its previous token hwi−1, ti−1i 5. λk(∈ A = {λ1, ... , λK} ∈ RK) is a learned weight or parameter associated with feature function fk. Note that our formulation of CRFs is different from the widely-used formulations (e.g., (Sha and Pereira, 2003; McCallum and Li, 2003; Peng et al., 2004; Pinto et al., 2003; Peng and McCallum, 2004)). The previous applications of CRFs assign a conditional probability for a label sequence y = y1, ... , yT given an input sequence x = x1, ... , xT as: 1 T P(y |x) = Z exp (� λkfk (yi−1, yi, x)) x i=1 k In our formulation, CRFs deal with word boundary ambiguity. Thus, the the size of output sequence T is not fixed through all candidates y ∈ Y(x). The index i is not tied with the input x as in the original CRFs, but unique to the output y ∈ Y(x). Here, we introduce the global feature vector F(y, x) = {F1(y, x), ... , FK(y, x)}, where Fk(y, x) = �#y i=1 fk(hwi−1, ti−1i</context>
</contexts>
<marker>Pinto, McCallum, Wei, Croft, 2003</marker>
<rawString>David Pinto, Andrew McCallum, Xing Wei, and W. Bruce Croft. 2003. Table extraction using conditional random fields. In In Proc. of SIGIR, pages 235–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Sha</author>
<author>Fernando Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proc. of HLT-NAACL,</booktitle>
<pages>213--220</pages>
<contexts>
<context position="1821" citStr="Sha and Pereira, 2003" startWordPosition="255" endWordPosition="258">n this task. Our results confirm that CRFs not only solve the long-standing problems but also improve the performance over HMMs and MEMMs. 1 Introduction Conditional random fields (CRFs) (Lafferty et al., 2001) applied to sequential labeling problems are conditional models, trained to discriminate the correct sequence from all other candidate sequences without making independence assumption for features. They are considered to be the state-of-the-art framework to date. Empirical successes with CRFs have been reported recently in part-of-speech tagging (Lafferty et al., 2001), shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003), Chinese word segmentation (Peng et al., 2004), and Information Extraction (Pinto et al., 2003; Peng and McCallum, 2004). Previous applications with CRFs assumed that observation sequence (e.g. word) boundaries are fixed, and the main focus was to predict label ∗At present, NTT Communication Science Laboratories, 2-4, Hikaridai, Seika-cho, Soraku, Kyoto, 619-0237 Japan taku@cslab.kecl.ntt.co.jp sequence (e.g. part-of-speech). However, word boundaries are not clear in non-segmented languages. One has to identify word segmentation as well as to </context>
<context position="12813" citStr="Sha and Pereira, 2003" startWordPosition="2086" endWordPosition="2089">ate paths, i.e., )λkfk((w� i−1,t� i−1), (w� i,t� i)) , (a) Label bias 0.4 1.0 C 0.6 1.0 D 0.6 A 0.4 1.0 E B 1.0 BOS EOS (b) Length bias BOS 0.4 0.6 A 0.4 0.6 B D C 1.0 1.0 1.0 EOS P(A,D|x) &lt; P(B,E|x) P(A,D|x) &lt; P(B |x) E )λkfk((wi−1,ti−1), (wi,ti)) , k EZX = Y&apos;EY(X) E k exp( #Y&apos; E i=1 fk(hwi−1, ti−1i, hwi, tii) is an arbitrary feature function over i-th token hwi, tii, and its previous token hwi−1, ti−1i 5. λk(∈ A = {λ1, ... , λK} ∈ RK) is a learned weight or parameter associated with feature function fk. Note that our formulation of CRFs is different from the widely-used formulations (e.g., (Sha and Pereira, 2003; McCallum and Li, 2003; Peng et al., 2004; Pinto et al., 2003; Peng and McCallum, 2004)). The previous applications of CRFs assign a conditional probability for a label sequence y = y1, ... , yT given an input sequence x = x1, ... , xT as: 1 T P(y |x) = Z exp (� λkfk (yi−1, yi, x)) x i=1 k In our formulation, CRFs deal with word boundary ambiguity. Thus, the the size of output sequence T is not fixed through all candidates y ∈ Y(x). The index i is not tied with the input x as in the original CRFs, but unique to the output y ∈ Y(x). Here, we introduce the global feature vector F(y, x) = {F1(y,</context>
<context position="22999" citStr="Sha and Pereira, 2003" startWordPosition="3944" endWordPosition="3947"> ct0, p1, p2, cf, ct, bw) w0/w are lexicalized (p10, p20, cf0, ct0, bw0, p1, p2, cf, ct, bw) hara and Matsumoto, 2000) trained and tested with the same corpus is also shown. E-HMMs is applied to the current implementation of ChaSen. Details of E-HMMs are described in Section 4.3.2. We directly evaluated the difference of these systems using McNemar’s test. Since there are no standard methods to evaluate the significance of F scores, we convert the outputs into the characterbased B/I labels and then employ a McNemar’s paired test on the labeling disagreements. This evaluation was also used in (Sha and Pereira, 2003). The results of McNemar’s test suggest that L2-CRFs is significantly better than other systems including L1- CRFs8. The overall results support our empirical success of morphological analysis based on CRFs. 4.3 Discussion 4.3.1 CRFs and MEMMs Uchimoto el al. proposed a variant of MEMMs trained with a number of features (Uchimoto et al., 2001). Although they improved the accuracy for unknown words, they fail to segment some sentences which are correctly segmented with HMMs or rulebased analyzers. Figure 3 illustrates the sentences which are incorrectly segmented by Uchimoto’s MEMMs. The correc</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>Fei Sha and Fernando Pereira. 2003. Shallow parsing with conditional random fields. In Proc. of HLT-NAACL, pages 213–220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyotaka Uchimoto</author>
<author>Satoshi Sekine</author>
<author>Hitoshi Isahara</author>
</authors>
<title>The unknown word problem: a morphological analysis of Japanese using maximum entropy aided by a dictionary.</title>
<date>2001</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>91--99</pages>
<contexts>
<context position="2817" citStr="Uchimoto et al., 2001" startWordPosition="399" endWordPosition="402">aridai, Seika-cho, Soraku, Kyoto, 619-0237 Japan taku@cslab.kecl.ntt.co.jp sequence (e.g. part-of-speech). However, word boundaries are not clear in non-segmented languages. One has to identify word segmentation as well as to predict part-of-speech in morphological analysis of non-segmented languages. In this paper, we show how CRFs can be applied to situations where word boundary ambiguity exists. CRFs offer a solution to the problems in Japanese morphological analysis with hidden Markov models (HMMs) (e.g., (Asahara and Matsumoto, 2000)) or with maximum entropy Markov models (MEMMs) (e.g., (Uchimoto et al., 2001)). First, as HMMs are generative, it is hard to employ overlapping features stemmed from hierarchical tagsets and nonindependent features of the inputs such as surrounding words, word suffixes and character types. These features have usually been ignored in HMMs, despite their effectiveness in unknown word guessing. Second, as mentioned in the literature, MEMMs could evade neither from label bias (Lafferty et al., 2001) nor from length bias (a bias occurring because of word boundary ambiguity). Easy sequences with low entropy are likely to be selected during decoding in MEMMs. The consequence </context>
<context position="10757" citStr="Uchimoto et al., 2001" startWordPosition="1720" endWordPosition="1723">ns. Moreover, we cannot ignore the influence of the length bias either. By the length bias, we mean that short paths, consisting of a small number of tokens, are preferred to long path. Even if the transition probability of each token is small, the total probability of the path will be amplified when the path is short 2:(b)). Length bias occurs in Japanese morphological analysis because the number of output tokens y varies by use of prior lexicons. Uchimoto et al. attempted a variant of MEMMs for Japanese morphological analysis with a number of features including suffixes and character types (Uchimoto et al., 2001; Uchimoto et al., 2002; Uchimoto et al., 2003). Although the performance of unknown words were improved, that of known words degraded due to the label and length bias. Wrong segmentation had been reported in sentences which are analyzed correctly by naive rule-based or HMMs-based analyzers. 3 Conditional Random Fields Conditional random fields (CRFs) (Lafferty et al., 2001) overcome the problems described in Section 2.2. CRFs are discriminative models and can thus capture many correlated features of the inputs. This allows flexible feature designs for hierarchical tagsets. CRFs have a single </context>
<context position="20199" citStr="Uchimoto et al., 2001" startWordPosition="3427" endWordPosition="3430">on is evaluated), top: (word segmentation and the top level of POS are evaluated), and all: (all information is used for evaluation). The hyperparameters C for L1-CRFs and L2- CRFs are selected by cross-validation. Experiments are implemented in C++ and executed on Linux with XEON 2.8 GHz dual processors and 4.0 Gbyte of main memory. 4.2 Results Tables 3 and 4 show experimental results using KC and RWCP respectively. The three F-scores (seg/top/all) for our CRFs and a baseline bi-gram HMMs are listed. In Table 3 (KC data set), the results of a variant of maximum entropy Markov models (MEMMs) (Uchimoto et al., 2001) and a rule-based analyzer (JUMAN7) are also shown. To make a fare comparison, we use exactly the same data as (Uchimoto et al., 2001). In Table 4 (RWCP data set), the result of an extended Hidden Markov Models (E-HMMs) (Asa6These lexicalizations are usually employed in Japanese morphological analysis. 7JUMAN assigns “unknown POS” to the words not seen in the lexicon. We simply replace the POS of these words with the default POS, Noun-SAHEN. Table 1: Details of Data Set KC RWCP source Mainich News Article (’95) Mainich News Article (’94) lexicon (# of words) JUMAN ver. 3.61 (1,983,173) IPADIC </context>
<context position="23344" citStr="Uchimoto et al., 2001" startWordPosition="3998" endWordPosition="4001">Nemar’s test. Since there are no standard methods to evaluate the significance of F scores, we convert the outputs into the characterbased B/I labels and then employ a McNemar’s paired test on the labeling disagreements. This evaluation was also used in (Sha and Pereira, 2003). The results of McNemar’s test suggest that L2-CRFs is significantly better than other systems including L1- CRFs8. The overall results support our empirical success of morphological analysis based on CRFs. 4.3 Discussion 4.3.1 CRFs and MEMMs Uchimoto el al. proposed a variant of MEMMs trained with a number of features (Uchimoto et al., 2001). Although they improved the accuracy for unknown words, they fail to segment some sentences which are correctly segmented with HMMs or rulebased analyzers. Figure 3 illustrates the sentences which are incorrectly segmented by Uchimoto’s MEMMs. The correct paths are indicated by bold boxes. Uchimoto et al. concluded that these errors were caused by nonstandard entries in the lexicon. In Figure 3, “ロマ ンは” (romanticist) and “ない心” (one’s heart) are unusual spellings and they are normally written as “ロマン派” and “内心” respectively. However, we conjecture that these errors are caused by the influence </context>
</contexts>
<marker>Uchimoto, Sekine, Isahara, 2001</marker>
<rawString>Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isahara. 2001. The unknown word problem: a morphological analysis of Japanese using maximum entropy aided by a dictionary. In Proc. of EMNLP, pages 91–99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyotaka Uchimoto</author>
<author>Chikashi Nobata</author>
<author>Atsushi Yamada</author>
<author>Satoshi Sekine</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Morphological analysis of the spontaneous speech corpus.</title>
<date>2002</date>
<booktitle>In Proc of COLING,</booktitle>
<pages>1298--1302</pages>
<contexts>
<context position="10780" citStr="Uchimoto et al., 2002" startWordPosition="1724" endWordPosition="1727"> ignore the influence of the length bias either. By the length bias, we mean that short paths, consisting of a small number of tokens, are preferred to long path. Even if the transition probability of each token is small, the total probability of the path will be amplified when the path is short 2:(b)). Length bias occurs in Japanese morphological analysis because the number of output tokens y varies by use of prior lexicons. Uchimoto et al. attempted a variant of MEMMs for Japanese morphological analysis with a number of features including suffixes and character types (Uchimoto et al., 2001; Uchimoto et al., 2002; Uchimoto et al., 2003). Although the performance of unknown words were improved, that of known words degraded due to the label and length bias. Wrong segmentation had been reported in sentences which are analyzed correctly by naive rule-based or HMMs-based analyzers. 3 Conditional Random Fields Conditional random fields (CRFs) (Lafferty et al., 2001) overcome the problems described in Section 2.2. CRFs are discriminative models and can thus capture many correlated features of the inputs. This allows flexible feature designs for hierarchical tagsets. CRFs have a single exponential model for t</context>
</contexts>
<marker>Uchimoto, Nobata, Yamada, Sekine, Isahara, 2002</marker>
<rawString>Kiyotaka Uchimoto, Chikashi Nobata, Atsushi Yamada, Satoshi Sekine, and Hitoshi Isahara. 2002. Morphological analysis of the spontaneous speech corpus. In Proc of COLING, pages 1298– 1302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyotaka Uchimoto</author>
</authors>
<title>Chikashi Nobata, Atsushi Yamada, and Hitoshi Isahara Satoshi Sekine.</title>
<date>2003</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>479--488</pages>
<marker>Uchimoto, 2003</marker>
<rawString>Kiyotaka Uchimoto, Chikashi Nobata, Atsushi Yamada, and Hitoshi Isahara Satoshi Sekine. 2003. Morphological analysis of a large spontaneous speech corpus in Japanese. In Proc. of ACL, pages 479–488.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>