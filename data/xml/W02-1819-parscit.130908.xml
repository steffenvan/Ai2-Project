<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015835">
<title confidence="0.99788">
Learning Rules for Chinese Prosodic Phrase Prediction
</title>
<author confidence="0.991452">
†Zhao Sheng ‡Tao Jianhua §Cai Lianhong
</author>
<affiliation confidence="0.9802145">
Department of Computer Science and Technology
Tsinghua University, Beijing, 100084, China
</affiliation>
<email confidence="0.927485">
†szhao00@mails.tsinghua.edu.cn {‡jhtao, §clh-dcs}@tsinghua.edu.cn
</email>
<sectionHeader confidence="0.993436" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999977333333333">
This paper describes a rule-learning approach
towards Chinese prosodic phrase prediction for
TTS systems. Firstly, we prepared a speech
corpus having about 3000 sentences and
manually labelled the sentences with two-level
prosodic structure. Secondly, candidate
features related to prosodic phrasing and the
corresponding prosodic boundary labels are
extracted from the corpus text to establish an
example database. A series of comparative
experiments is conducted to figure out the
most effective features from the candidates.
Lastly, two typical rule learning algorithms
(C4.5 and TBL) are applied on the example
database to induce prediction rules. The paper
also suggests general evaluation parameters for
prosodic phrase prediction. With these
parameters, our methods are compared with
RNN and bigram based statistical methods on
the same corpus. The experiments show that
the automatic rule-learning approach can
achieve better prediction accuracy than the
non-rule based methods and yet retain the
advantage of the simplicity and
understandability of rule systems. Thus it is
justified as an effective alternative to prosodic
phrase prediction.
</bodyText>
<sectionHeader confidence="0.998878" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998745">
Prosodic phrase prediction or prosodic
phrasing plays an important role in improving
the naturalness and intelligence of TTS
systems. Linguistic research shows that the
utterance produced by human is structured in a
hierarchy of prosodic units, including
phonological phrase, intonation phrase and
utterance. (Abney, 1995) But the output of text
analysis of TTS systems is often a structure of
syntactic units, such as words or phrases,
which are not equivalent to the prosodic ones.
Therefore the object of prosodic phrasing is to
map the syntactic structure into its prosodic
counterpart.
A lot of methods have been introduced to
predict prosodic phrase in English text such as
Classification and Regression Tree (Wang and
Hirschberg, 1992), Hidden Markov Model
(Paul and Alan, 1998). For Chinese prosodic
phrasing, the traditional method is based on
handcrafted rules. Recurrent Neural Network
(Ying and Shi, 2001) as well as POS bigram
and CART based methods (Yao and Min, 2001)
is also experimented recently. Due to the
difference in training corpus and evaluation
methods between researchers, these results are
generally less comparable.
In this paper, a rule-learning approach is
proposed to predict prosodic phrase in
unrestricted Chinese text. Rule-based systems
are simple and easy to understand. But
handcrafted rules are usually difficult to
construct, maintain and evaluate. Thus two
typical rule-learning algorithms (C4.5
induction and transformation-based learning)
are employed to automatically induce
prediction rules from examples instead of
human. Generally speaking, automatic
rule-learning has two obvious advantages over
the previous methods:
</bodyText>
<listItem confidence="0.982775888888889">
1) Statistical methods like bigram or HMM
usually need large training corpus to
avoid sparse data problem while
rule-learning doesn’t have the restriction.
In the case of prosodic phrase prediction,
the corpus with prosodic labelling is often
relatively small. Rule-learning is just
suitable for this task.
2) CART, RNN or other neural network
</listItem>
<bodyText confidence="0.990232475">
methods have good learning ability but
the learned knowledge is represented as
trees or network weights, which are not so
much understandable as rules.
Once rules are learned from examples, they
can be analyzed by human to check if they
agree with the common linguistic knowledge.
We can add prediction rules converted from
our linguistic knowledge to the rule set, which
is especially useful when the training corpus
doesn’t cover wide enough phenomena of
prosodic phrasing. Furthermore, we can try to
interpret and understand rules learned by
machine so as to enrich our linguistic
knowledge. Hence rule-learning also helps us
mine knowledge from examples.
Since features related to prosodic phrasing
come from various linguistic sources, several
comparative experiments are conducted to
select the most effective features from the
candidates. The paper also suggests general
evaluation parameters for prosodic phrase
prediction. With these parameters, our methods
are compared with RNN and bigram based
statistical methods on the same corpus. The
experiments show that the automatic
rule-learning approach can achieve better
prediction accuracy than the non-rule based
methods and yet retain the advantage of the
simplicity and understandability of rule
systems. The paper proceeds as follows.
Section 2 introduces the rule-learning
algorithms we used. Section 3 describes
prosodic phrase prediction and its evaluation
parameters. Section 4 discusses the feature
selection and rule-learning experiments in
detail. Section 5 reports the evaluation results
of rule based and none-rule based methods.
Section 6 presents the conclusion and the view
of future work.
</bodyText>
<sectionHeader confidence="0.952861" genericHeader="method">
2 Rule Learning Algorithms
</sectionHeader>
<bodyText confidence="0.999939064516129">
Research on machine learning has
concentrated in the main on inducing rules
from unordered set of examples. And
knowledge represented in a collection of rules
is understandable and effective way to realize
some kind of intelligence. C4.5 (Quinlan, 1986)
and transformation-based learning (Brill, 1995)
are typical rule-learning algorithms that have
been applied to various NLP tasks such as
part-of-speech tagging and named entity
extraction etc.
Both algorithms are supervised learning and
can be used to induce rules from examples.
But they also have difference from each other.
Firstly the C4.5 rule induction is a completely
automatic process. What we need to do is to
extract appropriate features for our problem.
As to transformation-based learning
(henceforth TBL), transformation rule
templates, which determine the effectiveness
of the acquired rules, have to be designed
manually before learning. Thus TBL can only
be viewed as a semi-automatic method.
Secondly the induction of C4.5 rules using a
divide-and-conquer strategy is much faster
than the greedy searching for TBL ones. In
view of the above facts, C4.5 rules are induced
from examples first in our experiments. And
then the rules are used to guide the design of
rule templates for TBL. See section 4.8 for
detail.
</bodyText>
<sectionHeader confidence="0.991734" genericHeader="method">
3 Prosodic Phrase Prediction
</sectionHeader>
<subsectionHeader confidence="0.999857">
3.1 The Methodology
</subsectionHeader>
<bodyText confidence="0.999577733333333">
Linguistic research has suggested that Chinese
utterance is also structured in a prosodic
hierarchy, in which there are mainly three
levels of prosodic units: prosodic word,
prosodic phrase and intonation phrase (Li and
Lin, 2000).. Figure 1 shows the prosodic
structure of a Chinese sentence. In the tree
structure, the non-leaf nodes are prosodic units
and the leaves are syntactic words. A prosodic
phrase is composed of several prosodic words,
each of which in turn consists of several
syntactic words. Since intonation phrase is
usually indicated by punctuation marks, we
only need to consider the prediction of
prosodic word and phrase.
</bodyText>
<equation confidence="0.780604">
U
PP PP
</equation>
<figureCaption confidence="0.92946">
Figure 1: Two-level prosodic structure tree (U
for intonation phrase, PP for prosodic phrase,
PW for prosodic word)
</figureCaption>
<bodyText confidence="0.999376684210527">
Suppose we have a string of syntactic words
i.e. w1 , w2 ,... wn , the boundary between two
neighbouring words is represented
asp wi −wi+1 &gt;. There are total three types of
boundaries labelled as B0 (wi, wi+1 are in the
same prosodic word), B1 (the words are in the
same prosodic phrase, but not the same
prosodic word), or B2 (the words are in
different prosodic phrases) respectively. Thus
prosodic phrase prediction is to predict such
boundary labels, which can be viewed as a
classification task. We believe these labels are
determined by the contextual linguistic
information around the boundary. If we have a
speech corpus with prosodic labelling, features
related to prosodic phrasing can be extracted at
each boundary and combined with the
corresponding boundary labels to establish an
example database. Then rule-learning
</bodyText>
<equation confidence="0.9895198">
iWA &apos;1 41 n, flifjk
# N� T )&amp;quot;` i 34&amp;quot;A �&apos;1 b4k
PW PW PW
PW PW PW
PW
</equation>
<bodyText confidence="0.9969875">
algorithms are executed on the database to
induce rules for predicting boundary labels.
</bodyText>
<subsectionHeader confidence="0.999177">
3.2 Evaluation Parameters
</subsectionHeader>
<bodyText confidence="0.9992275">
As a classification task, prosodic phrase
prediction should be evaluated with
consideration on all the classes. The rules
induced from examples are applied on a test
corpus to predict the label of each boundary.
The predicted labels are compared with labels
given by human, which are thought to be true,
to ge
</bodyText>
<tableCaption confidence="0.996484">
Table 1: Confusion matrix
</tableCaption>
<bodyText confidence="0.9992985">
Cijs are the counts of boundaries whose true
label are Bi but predicted as Bj. From these
counts, we can deduce the evaluation
parameters for prosodic phrasing.
</bodyText>
<equation confidence="0.99174544">
2
Re c C
i ij
= ∑
ii =
/ C ( 0,1,2
i )
=Cii/∑Cji (i
j=0
F = 2 * Re * Pr /(Re Pr )( 0,1,2)
c e c + e i = (3)
i i i i i
2 2 2
Acc = ∑Cii /∑∑
1
i = 0 ji
= 0 = 0
2 2 2 2
Acc = ∑ ∑Cij C
+ 00 ) / ∑ ∑
2 (
j i
= 1 = 1 j i
==
0 0
</equation>
<bodyText confidence="0.993399222222222">
Reci defines the recall rate of boundary
label Bi. Pr ei defines the precision rate of
Bi. Fi is a combination of recall and precision
rate, suggested by (Rijsbergen, 1979). A cc1 is
the overall accuracy of all the labels. If we
merge B1 and B2 into one label, which can be
viewed as the prediction of prosodic word
boundary, Acc2 defines the overall accuracy of
this case.
</bodyText>
<sectionHeader confidence="0.999835" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998676">
4.1 The Corpus
</subsectionHeader>
<bodyText confidence="0.999981875">
In our experiments, the speech corpus of our
TTS system is used for training and testing.
The corpus has 3167 sentences, which are
randomly selected from newspaper and read
by a radiobroadcaster. We manually labelled
the sentences with two-level prosodic structure
by listening to the record speech. For example,
the sentence in Figure 1 is labelled as “5�/ B1
</bodyText>
<equation confidence="0.9935935">
&amp;MUB0 J/B1 n/B0 �/B0 TM/B2 F/B1 NT/B0
T /B1 ) /B0 JT1 1A /B0 n /B1 t /B2”.
</equation>
<bodyText confidence="0.9998965">
Preliminary tests show that manually labelling
can achieve a high consistency rate among
human. Therefore it is reasonable to make the
manually labelled results as the target of
learning algorithms.
The sentences of the corpus are also
processed with a text analyzer, where Chinese
word segmentation and part-of-speech tagging
are accomplished in one step using a statistical
language model. The segmentation and
tagging yields a gross accuracy rate over 94%.
The output of the text analyzer is directly used
as the training data of learning algorithms
without correcting segmentation or tagging
errors because we want to train classifiers with
noisy data in the real situation.
Here are some statistical figures about the
corpus. There are 56446 Chinese characters in
the corpus, which constitute 37669 words. The
number of prosodic word boundaries is 16194
and that of prosodic phrase ones is only 7231.
The average length of syntactic word, prosodic
word, prosodic phrase and sentence are 1.5,
2.4, 7.8 and 17.0 in character, respectively.
</bodyText>
<subsectionHeader confidence="0.986299">
4.2 Candidate Features
</subsectionHeader>
<bodyText confidence="0.999969666666667">
Feature selection is crucial to the classification
of prosodic boundary labels. Linguistic
information around the word boundary is the
main source of features. The features may
come from different levels including syllable,
word, phrase, sentence level. And the type of
features may be phonetic, lexical, syntactic,
semantic or pragmatic. Which features have
most close relation with prosodic phrasing and
how to represent them are still open research
problems. In our approach, we decide to list all
the possible features first and figure out the
most effective ones by experiments. The
features we currently consider are presented in
the following.
</bodyText>
<subsubsectionHeader confidence="0.998062">
4.2.1 Phonetic information
</subsubsectionHeader>
<bodyText confidence="0.999954357142857">
Chinese is well known as a monosyllabic,
tonal language. And phonetic study shows
sound will change in continuous speech
because of context or prosodic structure.
Retroflex, neutral tone and tone sandhi are
important phonetic phenomena that cause
sound variation. (Li and Lin, 2000). Thus
phonetic information about phone and syllable
is related to prosodic phasing. There are too
many tonal syllables (about 1300) in Chinese
to consider. Instead, the initials and finals of
the syllables (total about 60) near a word
boundary are taken into accounts, which are
represented as SYIF in the following text.
</bodyText>
<figure confidence="0.9977803125">
0
j=
2
True Predicted labels
labels
B0 B1 B2
B0 C00 C01 C02
B1 C10 C11 C12
B2 C20 C21 C22
ei
Pr
=
0,1,2)
ij
C
C ij
</figure>
<bodyText confidence="0.9841695">
Similarly the tones of the syllables, denoted by
TONE, are also included as phonetic features.
</bodyText>
<subsubsectionHeader confidence="0.996048">
4.2.2 Lexical information
</subsubsectionHeader>
<bodyText confidence="0.999952">
Words in natural language have different
occurrence frequency. And words that have
high occurrence frequency may be especially
important to prosodic phrasing (e.g. some
functional words in Chinese, ann,aRn
etc). Therefore lexical word is treated as a
candidate feature, represented as WORD.
</bodyText>
<subsubsectionHeader confidence="0.995809">
4.2.3 Syntactic information
</subsubsectionHeader>
<bodyText confidence="0.999968333333333">
Syntactic information has close relation with
prosodic structure. POS, which denotes
part-of-speech of words, is a basic syntactic
feature much easier to obtain with automatic
POS taggers. And it has been widely adopted
in previous researches. Since POS tag sets
varies with taggers, we try to determine the
best one for predicting prosodic phrase by
experiments.
</bodyText>
<subsubsectionHeader confidence="0.997708">
4.2.4 Other information
</subsubsectionHeader>
<bodyText confidence="0.999971714285714">
From the statistical figures of the corpus, both
prosodic word and phrase have limitation in
length. The length of syntactic word (WLEN),
the length of the sentence in character (SLENC)
and word (SLENW) are considered as length
features. In HMM-based methods, the chain of
boundary labels in a sentence is supposed to
conform to Markov assumption. And
according to experience, it is less possible for
two boundaries with label B2 to locate very
close to each other. Thus the label of previous
boundaries (BTYPE) and the distances from
them to current position are also possible
features.
</bodyText>
<subsectionHeader confidence="0.998471">
4.3 Example Database
</subsectionHeader>
<bodyText confidence="0.999605941176471">
All of the possible features are extracted from
the corpus at each boundary to establish an
example database. Table 2 shows parts of the
example entries of two word boundaries in
Figure 1. Each row is a type of feature. The
row name has a format of feature name plus a
number. The number indicates which word the
feature comes from. And the range of the
number is limited by a window size. For
example, POS_0 denotes part-of-speech of the
word just before the word boundary, POS_-1
denotes that of the second word previous to the
boundary and POS_1 denotes that of the word
just after the boundary. The rest may be
deduced by analogy. BTYPE_0 is the label of
current boundary and also the target to be
predicted.
</bodyText>
<table confidence="0.997095214285714">
Boundaries &lt;��—��&gt; &lt;��—4&gt;
Features
SYIF_0 an eng
SYIF_1 z b
TONE 0 3 2
TONE 1 4 4
WORD_0 �� �hiz
WORD_1 �� }�
POS_0 vn v
POS 1 v c
POS_-1 w u
WLEN_0 2 2
WLEN 1 2 1
BTYPE_0 B1 B2
</table>
<tableCaption confidence="0.997748">
Table 2: Example database entries
</tableCaption>
<subsectionHeader confidence="0.986008">
4.4 Feature Selection Experiments
</subsectionHeader>
<bodyText confidence="0.999965">
Once the example database is established, we
can begin to induce rules from it with rule
learners. If all the features were used in one
experiment, the feature space would get too
large to learn rules quickly. Moreover we want
to eliminate less significant features from the
database. A series of comparative experiments
is carried out to figure out the effective
features. C4.5 learner is used to perform the
learning task in the following experiments.
</bodyText>
<subsubsectionHeader confidence="0.963035">
4.4.1 Baseline experiment (No.1)
</subsubsectionHeader>
<bodyText confidence="0.99973425">
Since POS features are widely used, a baseline
experiment is performed with only two POS
features that are POS_0 and POS_1. The POS
tag set has total 30 tags from the tagger.
</bodyText>
<subsubsectionHeader confidence="0.982281">
4.4.2 POS window-size (No.2-9)
</subsubsectionHeader>
<bodyText confidence="0.999943111111111">
The window size determines the number of
words whose features are considered. Suppose
the window size is L+R, which means the
features of L words left to the boundary and R
words right to it are used. We design
experiments with the combination of different
value of L and R to find the best window of
POS features. The features in the window are
denoted by POS{-L+1, R} in a range form.
</bodyText>
<subsubsectionHeader confidence="0.965306">
4.4.3 POS set (No.10-11)
</subsubsectionHeader>
<bodyText confidence="0.999887916666667">
Experiments are conducted on three POS sets,
which are BSET, LSET and CSET. BSET is the
basic POS set from the tagger. LSET is an
enlarged version of BSET, which includes the
most frequent 100 words as independent tags.
CSET is built with clustering technique. Each
POS in the BSET is represented as a
6-dimension vector, whose components are the
probabilities of the boundary labels after and
before that POS. Then these vectors are
clustered into 10 groups. The window size
used is 1+1.
</bodyText>
<subsubsectionHeader confidence="0.965269">
4.4.4 Other experiments (No.12-17)
</subsubsectionHeader>
<bodyText confidence="0.999151">
WORDLEN and SLEN are added into the
baseline system to investigate the importance
of length features in No.12 and 13. SYIF,
TONE features of syllables around the
boundary are considered in No.14. Previous
boundary labels (BTYPE_-1, BTYPE_-2) are
tested in the experiments No.15 and 16.
WORD features are used in No.17 to find if
there exist some words that have special
prosodic effects.
</bodyText>
<table confidence="0.999484555555556">
No. Features POS tag set F0 F1 F2 Acc1 Acc2
1 POS{0,1} BSET 0.69 0.72 0.76 0.72 0.79
2 POS{0,0} BSET 0.57 0.53 0.14 0.50 0.64
3 POS{-1,0} BSET 0.55 0.59 0.37 0.54 0.68
4 POS{0,2} BSET 0.70 0.72 0.76 0.72 0.79
5 POS{-1,1} BSET 0.71 0.71 0.76 0.72 0.79
6 POS{-1,2} BSET 0.71 0.70 0.75 0.71 0.79
7 POS{-2,1} BSET 0.71 0.70 0.75 0.71 0.79
8 POS{-2,2} BSET 0.70 0.70 0.75 0.71 0.79
9 POS{-3,3} BSET 0.71 0.70 0.74 0.71 0.79
10 POS{0,1} LSET 0.72 0.74 0.77 0.74 0.81
11 POS{0,1} CSET 0.67 0.67 0.73 0.68 0.75
12 POS{0,1},WLEN{0,1} BSET 0.81 0.77 0.76 0.79 0.86
13 POS{0,1},WLEN{0,1},SLEN BSET 0.82 0.76 0.74 0.78 0.87
14 POS{0,1},TONE,SYIF BSET 0.71 0.72 0.75 0.72 0.79
15 POS{0,1},BTYPE_-1 BSET 0.75 0.74 0.76 0.75 0.82
16 POS{0,1},BTYPE_{-1,-2} BSET 0.75 0.73 0.76 0.74 0.82
17 POS{0,1},WORD{0,1} BSET 0.64 0.72 0.72 0.70 0.78
</table>
<tableCaption confidence="0.999855">
Table 3: Results of feature selection (F0, F1, F2, Acc1, Acc2 are defined in section 3.2)
</tableCaption>
<subsectionHeader confidence="0.990485">
4.5 Feature selection results
</subsectionHeader>
<bodyText confidence="0.998847">
The results of these experiments are listed in
Table 3. From the evaluation figures in the
table, we can draw the following conclusions
on the effect of the features on prosodic phrase
prediction:
</bodyText>
<listItem confidence="0.972847473684211">
1) Part-of-speech is a basic and useful
feature. A window size of 2+1 is already
enough. Larger window size will greatly
lengthen the time of training but make no
significant improvement on the accuracy
rate.
2) The largest POS set LSET performs better
than smaller ones like BSET and CSET.
That’s because small POS sets lead to
small feature space, which may be not big
enough to distinguish the training
examples.
3) Length features are beneficial to prosodic
phrase prediction.
4) Phonetic features are less useful than what
we think before.
5) Former boundary information is also
useful. When training, the former and
latter boundary labels are both known, but
</listItem>
<bodyText confidence="0.88183325">
when testing, exact former boundary
labels do not exist. We can use the
boundary labels that are already predicted
to help make decision on current label.
Although the error prediction of former
labels may lead to error of current
prediction, the result shows the accuracy
rate is improved.
6) WORD feature is not appropriate to use,
since the using of it greatly enlarges the
feature space and needs more training
examples.
</bodyText>
<subsectionHeader confidence="0.98078">
4.6 C4.5 Experiments
</subsectionHeader>
<bodyText confidence="0.999968111111111">
According to the feature selection results, we
know some features are effective to prosodic
phrase prediction but some are not. And the
solely using of effective features doesn’t result
in a high enough accuracy rate. In order to
improve the prediction accuracy, we combine
the effective features such as WLEN{-1, 1},
BTYPE{-1}, SLEN and POS{-1,1} in LSET tag
set together to induce C4.5 rules.
</bodyText>
<subsectionHeader confidence="0.998084">
4.7 Examples of C4.5 Rules
</subsectionHeader>
<bodyText confidence="0.999759285714286">
As mentioned above, rule systems have the
advantage of simplicity and understandability.
We examine the rules learned by C4.5 and find
they certainly reflect the usage of prosodic
structure in some sense. Here are some rules
followed by example sentences with the
current boundary labels in bold:
</bodyText>
<equation confidence="0.997172875">
1) if POS_1 == T then BTYPE_0 = B0
R/B0 �/B1 SAHUB0 T/B1 Z)JUIA/B2
2) if POS_1 == n, then BTYPE_0 = B0
�&amp;/B0 n, /B1 9,IA/B1 +MR/B2
3) if POS_0 == � then BTYPE_0 = B0
Nt�/B1 TN/B0 WB2
4) if POS_0 == v &amp;&amp; POS_1 == � then
BTYPE_0 = B0
JA/B1 I/B0 T/B11998 `�/B2
5) if POS_1 == c &amp;&amp; WLEN_0 &gt; 2 then
BTYPE_0 = B2
*T�K/B2 #/B1 Wh/B1 V-C/B2
M/ B1 #/B0 &apos;+/B1 *0/B0 */B2
6) if POS_-1 == n &amp;&amp; POS_0 == )2E &amp;&amp;
BTYPE_-1 == B0 then BTYPE_0 = B2
FPQ/B0 Z/B2 itl)�/B0 n, /B0 IMB2
</equation>
<bodyText confidence="0.999433642857143">
Rule 1, 2 and 3 shows the special prosodic
effect of functional words such as “T”, “n, ”,
“+”, which tends to adhere to prosodic words
in the sentences. Rule 4 exemplifies that the
syntactic structure “Verb+T” usually acts as a
prosodic word. Rule 5 concerns the
conjunction word, the boundary before which
would be B2 (prosodic phrase boundary) if the
previous word had a length above 2. The B2
boundary is thought to accentuate the word
before the conjunction. Rule 6 deals with the
structure “Noun+)�E”. We can see that these
rules coincide with the experience of prosodic
phrasing by human.
</bodyText>
<subsectionHeader confidence="0.913166">
4.8 TBL Experiments
</subsectionHeader>
<bodyText confidence="0.9987815">
A general TBL toolkit (Grace and Radu, 2001)
is used in our TBL experiments. The analysis
on C4.5 rules casts lights on the design of the
transformation rule templates of TBL. Since
the same features as C4.5 learning are used in
the rule templates, linguistic knowledge, which
has been embodied by C4.5 rules, should also
be captured by transformation rule templates.
Suppose a C4.5 rule, “if (POS_0 == n &amp;&amp;
POS_1 == u) then BTYPE_0 = B0”, has a high
prediction accuracy, it is reasonable to make
this rule as an instantiation of TBL rule
templates. Table 4 lists some of the rule
templates used in TBL experiments.
</bodyText>
<equation confidence="0.63003675">
POS_0 POS_1 =&gt; BTYPE_0
POS_-1 POS_0 POS_1 =&gt; BTYPE_0
BTYPE _0 POS_0 POS_1 =&gt; BTYPE_0
BTYPE _0 POS_-1 POS_0 POS_1 =&gt; BTYPE_0
POS_0 POS_1 WLEN_0 WLEN_1=&gt; BTYPE_0
WORD_0 POS_0 POS_1 =&gt; BTYPE_0
WORD_0 POS_-1 POS_0 POS_1 =&gt; BTYPE_0
BTYPE_0 WORD_0 POS_0 POS_1=&gt;BTYPE_0
</equation>
<tableCaption confidence="0.984963">
Table 4: Rule templates for TBL
</tableCaption>
<bodyText confidence="0.999862304347826">
The left part of a rule template is a list of
features, and the right is the target, BTYPE_0.
For example, “POS_0 POS_1 =&gt; BTYPE_0”,
which is a short form of “if (POS_0 == X &amp;&amp;
POS_1 == Y) then BTYPE_0 = Z”, means if
current POS were X and the next POS were Y,
the boundary label would be Z. X, Y, Z are
template variables. Let X=n Y=u Z=B0, the
template is instantiated into the C4.5 rule
above.
Due to the mechanism of TBL rules, there
exist rule templates like “BTYPE_0 POS_0
POS_1 =&gt; BTYPE_0”, in which the former
BTYPE_0 is the label before applying the rule
and the latter is after applying it. That’s
actually what transformation means. When
training, the initial boundary labels are all set
to B1. At each step, the algorithm tries all the
possible values for template variables to find
an instantiated rule that can achieve the best
score. When testing, the initial boundary labels
are set the same way, and then transformation
rules are applied one by one.
</bodyText>
<sectionHeader confidence="0.994574" genericHeader="evaluation">
5 Evaluation Results
</sectionHeader>
<bodyText confidence="0.999776275862069">
To evaluate the generalization ability of the
acquired rules, 5-fold cross validation tests are
executed on the corpus for both C4.5 and TBL.
We reimplemented the RNN algorithm and
POS bigram statistical model to predict
prosodic word boundary on the same corpus
for comparison. Since our corpus is not large
enough for HMM training and the CART
method is also decision-tree based as C4.5, we
didn’t realize them in our experiments. The
evaluation results are shown in Table 5.
Both the C4.5 rules and the TBL rules
outperform the RNN algorithm and POS
bigram method because the overall accuracy
rates Acc2 of the rule based methods are higher.
TBL achieves comparable accuracy with C4.5
induction, which demonstrates that the design
of transformation rule templates is successful.
Comparing Acc1 and Acc2 in Table 5, we
discover that prosodic word boundaries can be
more accurately predicted than prosodic phrase
ones. It can be explained as follows. Prosodic
word is the smallest prosodic unit in the
prosodic hierarchy, which has more relation
with the word level features such as POS,
word length etc. Prosodic phrase is a larger
prosodic unit less related to word level features,
thus it cannot be predicted accurately using
these features.
</bodyText>
<table confidence="0.9965156">
Tests Reco Pre0 F0 Rec1 Pre1 F1 Rec2 Pre2 F2 Acc1 Acc2
C4.5 0.914 0.837 0.874 0.814 0.822 0.818 0.712 0.829 0.766 0.829 0.904
TBL 0.849 0.884 0.866 0.782 0.848 0.814 0.851 0.613 0.713 0.818 0.895
bigram 0.653 0.746 0.696 0.874 0.816 0.844 N/A N/A N/A N/A 0.793
RNN 0.764 0.803 0.783 0.883 0.857 0.870 N/A N/A N/A N/A 0.837
</table>
<tableCaption confidence="0.998298">
Table 5: Evaluation results
</tableCaption>
<sectionHeader confidence="0.994033" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999989678571429">
In this paper, we describe an effective
approach to generate rules for Chinese
prosodic phrase prediction. The main idea is to
extract appropriate features from the linguistic
information and to apply rule-learning
algorithms to automatically induce rules for
predicting prosodic boundary labels. C4.5 and
TBL algorithms are experimented in our
research. In order to find the most effective
features, a series of feature selection
experiments is conducted. The acquired rules
achieve a best accuracy rate above 90% on test
data and outperform the RNN and bigram
based methods, which justifies rule-learning as
an effective alternative to prosodic phrase
prediction.
But the problem of prosodic phrase
prediction is far from solved. The best
accuracy rate got by machine is still much
lower than that by human. In our future work,
the study on this problem will go more deep
and wide. Other machine learning methods
will be experimented and compared with C4.5
and TBL. Features from deep syntactic,
semantic or discourse information will be paid
more attention to (Julia and Owen, 2001). And
the speech corpus will be enlarged to cover
more types of text and speaking styles.
</bodyText>
<sectionHeader confidence="0.996523" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9969186">
Our work is sponsored by 863 Hi-Tech
Research and Development Program of China
(No: 2001AA114072). We also would like to
thank the anonymous reviewers of the First
SigHAN Workshop for their comments.
</bodyText>
<sectionHeader confidence="0.999202" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.984463205128205">
Abney Steven. (1995) Chunks and
dependencies: bringing processing evidence
to bear on syntax. Computational
Linguistics and Foundations of Linguistic
Theory, CSLI.
Eric Brill. (1995) Transformation-Based Error
–Driven Learning and Natural Language
Processing: A Case Study in Part-of-Speech
Tagging. Computational Linguistics
21(4):543- 565.
C.J. van Rijsbergen. (1979) Information
Retrieval. Butterworths, London.
Grace Ngai and Radu Florian. (2001)
Transformation-Based Learning in the Fast
Lane. Proceedings of the 39th ACL
Conference.
Julia Hirschberg, Owen Rambow. (2001)
Learning Prosodic Features using a Tree
Representation. Eruospeech2001.
Li Aijun, Lin Maocan. (2000) Speech corpus
of Chinese discourse and the phonetic
research. ICSLP2000.
Michelle Wang and Julia Hirschberg. (1992)
Automatic classication of intonational
phrase boundaries. Computer Speech and
Language 6:175–196.
Paul Taylor and Alan W Black. (1998)
Assigning phrase breaks from
part-of-speech sequences. Computer Speech
and Language v12.
Quinlan,J.R. (1986) Induction of decision trees.
Machine Learning, 1(1):81-106.
Yao Qian, Min Chu, Hu Peng. (2001)
Segmenting unrestricted chinese text into
prosodic words instead of lexical words.
ICASSP2001.
Zhiwei Ying and Xiaohua Shi. (2001) An
RNN-based algorithm to detect prosodic
phrase for Chinese TTS. ICASSP2001.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.459911">
<title confidence="0.999986">Learning Rules for Chinese Prosodic Phrase Prediction</title>
<author confidence="0.999059">Sheng Jianhua</author>
<affiliation confidence="0.998455">Department of Computer Science and</affiliation>
<address confidence="0.471077">Tsinghua University, Beijing, 100084, China</address>
<abstract confidence="0.999160464285714">This paper describes a rule-learning approach towards Chinese prosodic phrase prediction for TTS systems. Firstly, we prepared a speech corpus having about 3000 sentences and manually labelled the sentences with two-level prosodic structure. Secondly, candidate features related to prosodic phrasing and the corresponding prosodic boundary labels are extracted from the corpus text to establish an example database. A series of comparative experiments is conducted to figure out the most effective features from the candidates. Lastly, two typical rule learning algorithms (C4.5 and TBL) are applied on the example database to induce prediction rules. The paper also suggests general evaluation parameters for prosodic phrase prediction. With these parameters, our methods are compared with RNN and bigram based statistical methods on the same corpus. The experiments show that the automatic rule-learning approach can achieve better prediction accuracy than the non-rule based methods and yet retain the advantage of the simplicity and understandability of rule systems. Thus it is justified as an effective alternative to prosodic phrase prediction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Abney Steven</author>
</authors>
<title>Chunks and dependencies: bringing processing evidence to bear on syntax.</title>
<date>1995</date>
<booktitle>Computational Linguistics and Foundations of Linguistic Theory, CSLI.</booktitle>
<marker>Steven, 1995</marker>
<rawString>Abney Steven. (1995) Chunks and dependencies: bringing processing evidence to bear on syntax. Computational Linguistics and Foundations of Linguistic Theory, CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transformation-Based Error –Driven Learning and Natural Language Processing: A Case Study in Part-of-Speech Tagging.</title>
<date>1995</date>
<journal>Computational Linguistics</journal>
<volume>21</volume>
<issue>4</issue>
<pages>565</pages>
<contexts>
<context position="5426" citStr="Brill, 1995" startWordPosition="785" endWordPosition="786">odic phrase prediction and its evaluation parameters. Section 4 discusses the feature selection and rule-learning experiments in detail. Section 5 reports the evaluation results of rule based and none-rule based methods. Section 6 presents the conclusion and the view of future work. 2 Rule Learning Algorithms Research on machine learning has concentrated in the main on inducing rules from unordered set of examples. And knowledge represented in a collection of rules is understandable and effective way to realize some kind of intelligence. C4.5 (Quinlan, 1986) and transformation-based learning (Brill, 1995) are typical rule-learning algorithms that have been applied to various NLP tasks such as part-of-speech tagging and named entity extraction etc. Both algorithms are supervised learning and can be used to induce rules from examples. But they also have difference from each other. Firstly the C4.5 rule induction is a completely automatic process. What we need to do is to extract appropriate features for our problem. As to transformation-based learning (henceforth TBL), transformation rule templates, which determine the effectiveness of the acquired rules, have to be designed manually before lear</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Eric Brill. (1995) Transformation-Based Error –Driven Learning and Natural Language Processing: A Case Study in Part-of-Speech Tagging. Computational Linguistics 21(4):543- 565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J van Rijsbergen</author>
</authors>
<title>Information Retrieval.</title>
<date>1979</date>
<publisher>Butterworths,</publisher>
<location>London.</location>
<marker>van Rijsbergen, 1979</marker>
<rawString>C.J. van Rijsbergen. (1979) Information Retrieval. Butterworths, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grace Ngai</author>
<author>Radu Florian</author>
</authors>
<title>Transformation-Based Learning in the Fast Lane.</title>
<date>2001</date>
<booktitle>Proceedings of the 39th ACL Conference.</booktitle>
<marker>Ngai, Florian, 2001</marker>
<rawString>Grace Ngai and Radu Florian. (2001) Transformation-Based Learning in the Fast Lane. Proceedings of the 39th ACL Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hirschberg</author>
<author>Owen Rambow</author>
</authors>
<title>Learning Prosodic Features using a Tree Representation.</title>
<date>2001</date>
<tech>Eruospeech2001.</tech>
<marker>Hirschberg, Rambow, 2001</marker>
<rawString>Julia Hirschberg, Owen Rambow. (2001) Learning Prosodic Features using a Tree Representation. Eruospeech2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Li Aijun</author>
<author>Lin Maocan</author>
</authors>
<title>Speech corpus of Chinese discourse and the phonetic research.</title>
<date>2000</date>
<pages>2000</pages>
<marker>Aijun, Maocan, 2000</marker>
<rawString>Li Aijun, Lin Maocan. (2000) Speech corpus of Chinese discourse and the phonetic research. ICSLP2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michelle Wang</author>
<author>Julia Hirschberg</author>
</authors>
<title>Automatic classication of intonational phrase boundaries.</title>
<date>1992</date>
<journal>Computer Speech and Language</journal>
<pages>6--175</pages>
<contexts>
<context position="2167" citStr="Wang and Hirschberg, 1992" startWordPosition="305" endWordPosition="308">e of TTS systems. Linguistic research shows that the utterance produced by human is structured in a hierarchy of prosodic units, including phonological phrase, intonation phrase and utterance. (Abney, 1995) But the output of text analysis of TTS systems is often a structure of syntactic units, such as words or phrases, which are not equivalent to the prosodic ones. Therefore the object of prosodic phrasing is to map the syntactic structure into its prosodic counterpart. A lot of methods have been introduced to predict prosodic phrase in English text such as Classification and Regression Tree (Wang and Hirschberg, 1992), Hidden Markov Model (Paul and Alan, 1998). For Chinese prosodic phrasing, the traditional method is based on handcrafted rules. Recurrent Neural Network (Ying and Shi, 2001) as well as POS bigram and CART based methods (Yao and Min, 2001) is also experimented recently. Due to the difference in training corpus and evaluation methods between researchers, these results are generally less comparable. In this paper, a rule-learning approach is proposed to predict prosodic phrase in unrestricted Chinese text. Rule-based systems are simple and easy to understand. But handcrafted rules are usually d</context>
</contexts>
<marker>Wang, Hirschberg, 1992</marker>
<rawString>Michelle Wang and Julia Hirschberg. (1992) Automatic classication of intonational phrase boundaries. Computer Speech and Language 6:175–196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Taylor</author>
<author>Alan W Black</author>
</authors>
<title>Assigning phrase breaks from part-of-speech sequences. Computer Speech and Language v12.</title>
<date>1998</date>
<marker>Taylor, Black, 1998</marker>
<rawString>Paul Taylor and Alan W Black. (1998) Assigning phrase breaks from part-of-speech sequences. Computer Speech and Language v12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<title>Induction of decision trees.</title>
<date>1986</date>
<booktitle>Machine Learning,</booktitle>
<pages>1--1</pages>
<contexts>
<context position="5378" citStr="Quinlan, 1986" startWordPosition="780" endWordPosition="781">rning algorithms we used. Section 3 describes prosodic phrase prediction and its evaluation parameters. Section 4 discusses the feature selection and rule-learning experiments in detail. Section 5 reports the evaluation results of rule based and none-rule based methods. Section 6 presents the conclusion and the view of future work. 2 Rule Learning Algorithms Research on machine learning has concentrated in the main on inducing rules from unordered set of examples. And knowledge represented in a collection of rules is understandable and effective way to realize some kind of intelligence. C4.5 (Quinlan, 1986) and transformation-based learning (Brill, 1995) are typical rule-learning algorithms that have been applied to various NLP tasks such as part-of-speech tagging and named entity extraction etc. Both algorithms are supervised learning and can be used to induce rules from examples. But they also have difference from each other. Firstly the C4.5 rule induction is a completely automatic process. What we need to do is to extract appropriate features for our problem. As to transformation-based learning (henceforth TBL), transformation rule templates, which determine the effectiveness of the acquired</context>
</contexts>
<marker>Quinlan, 1986</marker>
<rawString>Quinlan,J.R. (1986) Induction of decision trees. Machine Learning, 1(1):81-106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yao Qian</author>
<author>Min Chu</author>
<author>Hu Peng</author>
</authors>
<title>Segmenting unrestricted chinese text into prosodic words instead of lexical words.</title>
<date>2001</date>
<pages>2001</pages>
<marker>Qian, Chu, Peng, 2001</marker>
<rawString>Yao Qian, Min Chu, Hu Peng. (2001) Segmenting unrestricted chinese text into prosodic words instead of lexical words. ICASSP2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiwei Ying</author>
<author>Xiaohua Shi</author>
</authors>
<title>An RNN-based algorithm to detect prosodic phrase for Chinese TTS.</title>
<date>2001</date>
<tech>ICASSP2001.</tech>
<contexts>
<context position="2342" citStr="Ying and Shi, 2001" startWordPosition="331" endWordPosition="334">utterance. (Abney, 1995) But the output of text analysis of TTS systems is often a structure of syntactic units, such as words or phrases, which are not equivalent to the prosodic ones. Therefore the object of prosodic phrasing is to map the syntactic structure into its prosodic counterpart. A lot of methods have been introduced to predict prosodic phrase in English text such as Classification and Regression Tree (Wang and Hirschberg, 1992), Hidden Markov Model (Paul and Alan, 1998). For Chinese prosodic phrasing, the traditional method is based on handcrafted rules. Recurrent Neural Network (Ying and Shi, 2001) as well as POS bigram and CART based methods (Yao and Min, 2001) is also experimented recently. Due to the difference in training corpus and evaluation methods between researchers, these results are generally less comparable. In this paper, a rule-learning approach is proposed to predict prosodic phrase in unrestricted Chinese text. Rule-based systems are simple and easy to understand. But handcrafted rules are usually difficult to construct, maintain and evaluate. Thus two typical rule-learning algorithms (C4.5 induction and transformation-based learning) are employed to automatically induce</context>
</contexts>
<marker>Ying, Shi, 2001</marker>
<rawString>Zhiwei Ying and Xiaohua Shi. (2001) An RNN-based algorithm to detect prosodic phrase for Chinese TTS. ICASSP2001.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>