<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.028085">
<title confidence="0.997072">
Substring-based Transliteration with Conditional Random Fields
</title>
<author confidence="0.991439">
Sravana Reddy and Sonjia Waxmonsky
</author>
<affiliation confidence="0.9957535">
Department of Computer Science
The University of Chicago
</affiliation>
<address confidence="0.61551">
Chicago, IL 60637
</address>
<email confidence="0.997733">
{sravana, wax}@cs.uchicago.edu
</email>
<sectionHeader confidence="0.993648" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.983117142857143">
Motivated by phrase-based translation research,
we present a transliteration system where char-
acters are grouped into substrings to be mapped
atomically into the target language. We show how
this substring representation can be incorporated
into a Conditional Random Field model that uses
local context and phonemic information.
</bodyText>
<sectionHeader confidence="0.998751" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999969">
We present a transliteration system that is moti-
vated by research in phrase-based machine trans-
lation. In particular, we borrow the concept of
phrases, which are groups of words that are trans-
lated as a unit. These phrases correspond to multi-
character substrings in our transliteration task.
That is, source and target language strings are
treated not as sequences of characters but as se-
quences of non-overlapping substrings.
We model transliteration as a sequential label-
ing task where substring tokens in the source lan-
guage are labeled with tokens in the target lan-
guage. This is done using Conditional Random
Fields (CRFs), which are undirected graphical
models that maximize the posterior probabilities
of the label sequence given the input sequence. We
use as features both local contexts and phonemic
information acquired from an English pronuncia-
tion dictionary.
</bodyText>
<sectionHeader confidence="0.979021" genericHeader="method">
2 The Transliteration Process
</sectionHeader>
<bodyText confidence="0.975666">
Our transliteration system has the following steps:
</bodyText>
<listItem confidence="0.995323333333333">
1. Pre-processing of the target language.
2. Substring alphabet generation for both the
source and target. This step also generates
training data for the CRFs in Step 3 and 4.
3. CRF training on aligned data from Step 2.
4. Substring segmentation and translitera-
</listItem>
<bodyText confidence="0.868487">
tion of source language input.
Our training and test data consists of three sets –
English to Hindi, English to Kannada, and English
to Tamil (Kumaran and Kellner, 2007) – from the
NEWS 2009 Machine Transliteration Shared Task
(Li et al., 2009).
</bodyText>
<subsectionHeader confidence="0.978167">
2.1 Step 1: Pre-Processing
</subsectionHeader>
<bodyText confidence="0.999982260869565">
The written words of Hindi, Tamil, and Kannada
correspond almost perfectly to their phonological
forms, with each character mapping to a phoneme.
The only exception to this arises from the implicit
vowel (which may be a schwa /a/ or a central
vowel /,e/) that is inserted after consonants that
are not followed by the halanta or ‘killer stroke’.
Hence, any mappings of an English vowel to a
target language schwa will not be reflected in the
alignment of the named entity pair.
To minimize misalignments of target language
strings with the English strings during training,
we convert the Indic abugida strings to an in-
ternal phonemic representation. The conversion
maps each unicode character to its correspond-
ing phonemic character and inserts a single sym-
bol (representing the schwa/central vowel) after all
consonants that are not followed by the halanta.
These phoneme sequences are used as the in-
ternal representation of Indic character strings for
all later steps in our system. Once transliteration
is complete, the phonemic symbols are converted
back to unicode by reversing the above process.
</bodyText>
<subsectionHeader confidence="0.998238">
2.2 Step 2: Substring alphabet generation
</subsectionHeader>
<bodyText confidence="0.999678">
Our decision to use substrings in the transliteration
task is motivated by the differences in orthography
and phonology between the target and source lan-
guages, which prevent trivial one-to-one character
level alignment. We first discuss the cause of the
poor character alignment between English and the
</bodyText>
<page confidence="0.970401">
92
</page>
<note confidence="0.9834345">
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 92–95,
Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999909810810811">
Indic languages, and then describe how we trans-
form the input into substring representation.
English uses several digraphs for phonemes
that are represented by single characters in Indic
scripts, which are either part of standard ortho-
graphic convention (oo, ch, etc.), or necessitated
by the lack of a single phoneme that approximates
an Indic one (as in the case of aspirated conso-
nants). Conversely, English sometimes uses a sin-
gle character for a biphone (such as x for /ks/, or
u for /ju/ as in museum), which is represented by
two characters in the target languages. In certain
cases, a digraph in English is transliterated to a di-
graph in the target, as a result of metathesis (le —*
/al/, in words like temple). Further, all three tar-
get languages often insert vowels between English
consonant clusters; for instance, Hindi inserts a
schwa between s and p in ‘transport’, transliter-
ated as transaport ().
To handle these cases, we borrow the concept of
phrases from machine translation (Och and Ney,
2004), where groups of words are translated as a
unit. In the case of transliteration, the ‘phrases’
are commonly occurring substrings – sequences
of characters – in one language that map to a
character or a substring in the other. We use the
term ‘substrings’ after a previous work (Sherif and
Kondrak, 2007) that employs it in a noisy channel
transliteration system. Zhao et al. (2007) also use
substrings (which they call ‘blocks’) in a bi-stream
HMM.
We bootstrap the induction of substrings by
aligning all named entity pairs in the training data,
using the GIZA++ toolkit (Och and Ney, 2003).
The toolkit performs unidirectional one-to-many
alignments, meaning that a single symbol in its
source string can be aligned to at most one sym-
bol in its target. In order to induce many-to-many
alignments, GIZA++ is run on the data in both di-
rections (source language to target language and
target language to source), and the bidirectional
alignment of a named entity pair is taken to be the
union of the alignments in each direction. Any
inserted characters (maps within the alignment
where the source or target character is null) are
combined with the preceding character within the
string. For example, the initial bidirectional align-
ment of shivlal —* fivalal (�ш����) contains
the maps [sh —* f, i —* i, v — *v, null — *a, l —* l,
a —* a, and l —* l]. The null —* a map is combined
with the preceding map to give v —* va, and hence
a one-to-one alignment.
Multicharacter units formed by bidirectional
alignments are added to source and target alpha-
bets. The above example would add the substrings
‘sh’ to the source alphabet, and va to the target.
Very low frequency substrings in both languages
are removed, giving the final substring alphabets
of single and multicharacter tokens. These alpha-
bets (summarized in Table 1) are used as the token
set for the CRF in Step 3.
We now transform our training data into a
substring-based representation. The original
named entity pairs are replaced by their bidirec-
tional one-to-one alignments described earlier. For
example, the (s h i v l a l) —* (f i v a l
CL l) training pair is replaced by (sh i v l a l) —*
(f i va l a l). A few (less than 3%) of the
pairs are not aligned one-to-one, since their bidi-
rectional alignments contain low-frequency sub-
strings that have not been included in the alpha-
bet.1 These pairs are removed from the training
data, since only one-to-one alignments can be han-
dled by the CRF.
</bodyText>
<subsectionHeader confidence="0.984534">
2.3 Step 3: CRF transliteration
</subsectionHeader>
<bodyText confidence="0.999946941176471">
With the transformed training data in hand, we can
now train a CRF sequential model that uses sub-
strings rather than characters as the basic token
unit. The CRF algorithm is chosen for its ability
to handle non-independent features of the source
language input sequence. We use the open-source
CRF++ software package (Kudo, 2005).
Ganesh et al. (2008) also apply a CRF to the
transliteration task (Hindi to English) but with
different alignment methods than those presented
here. In particular, multicharacter substrings are
only used as tokens on the target (English) side,
and a null token is used to account for deletion.
We train our CRF using unigram, bigram, and
trigram features over the source substrings, as well
as pronunciation information described in §2.3.1.
Table 2 describes these feature sets.
</bodyText>
<subsubsectionHeader confidence="0.690255">
2.3.1 Phonetic information
</subsubsectionHeader>
<bodyText confidence="0.9996934">
Since the CRF model allows us to incorporate non-
independent features, we add pronunciation data
as a token-level feature. Doing so allows the CRF
to use phonetic information for local decision-
making. Word pronunciations were obtained from
</bodyText>
<footnote confidence="0.9710065">
1Note that if we did not filter out any of the substrings,
every pair would be aligned one-to-one.
</footnote>
<page confidence="0.987654">
93
</page>
<table confidence="0.9998562">
Target Language Source Target
# of Tokens Longest Token # of Tokens Longest Token
Hindi 196 augh, ough 141 aja (31Tzr), ksa (cRT)
Kannada 197 aine 137 aja, mja
Tamil 179 cque 117 mij, aja
</table>
<tableCaption confidence="0.995737">
Table 1: Overview of the substring alphabets generated in Step 2.
</tableCaption>
<table confidence="0.999464428571428">
Feature Set Description
U Unigram: s_1, s0, and s1
B Bigram: s_1+s0
T Trigram: s_2+s_1+s0,
s_1+s0+s1 and s0+s1+s2
P Phoneme assigned to s0
from dictionary lookup
</table>
<tableCaption confidence="0.993934">
Table 2: Feature sets used for CRF in Step 3. si is
</tableCaption>
<bodyText confidence="0.9764062">
the substring relative to the current substring so.
the CMU Pronouncing Dictionary2. Just over a
third of the English named entities have pronun-
ciation information available for some or all the
constituent words.
The CMU dictionary provides a sequence of
phoneme symbols for an English word. We in-
clude these phonemes as CRF features if and
only if a one-to-one correspondence exists be-
tween phonemes and substring tokens. For exam-
ple, the English word simon has the segmentation
(s i m o n) and pronunciation (S AY M AH N),
both of length five. Additionally, a check is done
to ensure that vowel phonemes do not align with
consonant characters and vice-versa.
</bodyText>
<subsectionHeader confidence="0.985627">
2.4 Step 4: Substring segmentation
</subsectionHeader>
<bodyText confidence="0.999854333333333">
In order to apply our trained model to unseen
data, we must segment named entities into non-
overlapping substrings that correspond to tokens
in the source alphabet generated in Step 2. For in-
stance, we need to convert the four character desh
to the three token sequence (d e sh).
This is a non-trivial task. We must allow for
the fact that substrings are not inserted every time
the component character sequence appears. For
instance, in our English/Hindi training set, the bi-
gram ti always reduces to a single substring token
when it occurs in the -tion suffix, but does not re-
duce in any other contexts (like martini). There
are also cases where more than one non-trivial seg-
mentation is possible. For example, two possible
</bodyText>
<footnote confidence="0.987075">
2The CMU Pronouncing Dictionary (v0.7a). Available at
http://www.speech.cs.cmu.edu/cgi-bin/cmudict
</footnote>
<bodyText confidence="0.994207407407408">
segmentations of desh are (d es h) and (d e sh),
with the latter being the one that best corresponds
to the three-character Hindi d”eS (kш).
One solution is to greedily choose the most
likely multi-character substring – in the example
cited, we can choose (d e sh) because sh reduces
more frequently than es. However, this creates the
problem in cases where no reduction should occur,
as with the ti in martini. Since contextual informa-
tion is necessary to determine the correct substring
segmentation, we model segmentation with a CRF,
using a combination of character unigram, bigram,
and trigram features.
We use an approach motivated by the In-
side/Outside representation of NP-chunking
which treats segmentation as a tagging process
over words (Ramshaw and Marcus, 1995). As
in NP-chunking, our goal is to identify non-
overlapping, non-recursive segments in our input
sequence. Our tagset is {I, O, B} where I
indicates that a character is inside a substring, O
indicates a character is outside a substring, and B
marks a right boundary.
After the test data has been segmented into its
substring representation, it can be passed as input
to the CRF model trained in Step 3 to produce our
final transliteration output.
</bodyText>
<sectionHeader confidence="0.999858" genericHeader="evaluation">
3 Results
</sectionHeader>
<bodyText confidence="0.999968">
We first report our results on the development data
provided by the NEWS task, for different feature
sets and segmentation methods. We then present
the performance of our system on the test data.3
</bodyText>
<subsectionHeader confidence="0.999575">
3.1 Development Data
</subsectionHeader>
<bodyText confidence="0.940503222222222">
Table 3 shows the results across feature sets.
Noting that the trigram feature T provides a
sizable improvement, we compare results from
U+B+T+P and U+B+P feature sets. Of the im-
proved cases, 75-84% are a single vowel-to-vowel
3For the development runs, we use the training set for
training, and the development for testing. For the final test
runs, we use both the training and development sets for train-
ing, and the test set for evaluation.
</bodyText>
<page confidence="0.998326">
94
</page>
<table confidence="0.999822615384616">
Language Feature Set ACC F-Score
U+P 24.6 86.2
Hindi U+B+P 26.2 86.5
U+B+T+P 34.5 88.6
U+B+T 34.2 88.3
U+P 26.7 87.8
Tamil U+B+P 27.6 88.0
U+B+T+P 34.9 89.8
U+B+T 33.1 89.7
U+P 22.5 86.0
Kannada U+B+P 22.6 86.2
U+B+T+P 28.7 88.0
U+B+T 27.5 87.9
</table>
<tableCaption confidence="0.990819">
Table 3: Accuracy (ACC) and F-score results (in
%) for CRF model on the development data.
</tableCaption>
<table confidence="0.999902">
Language Feature Set ACC F-Score
Hindi U+B+T+P 34.4 90.2
U+B+T 33.6 89.5
Tamil U+B+T+P 29.1 91.1
U+B+T 25.5 90.6
Kannada U+B+T+P 27.2 89.8
U+B+T 23.4 89.2
</table>
<tableCaption confidence="0.994763">
Table 4: Results on development data, restricted to
</tableCaption>
<bodyText confidence="0.9764590625">
NEs where P is included as a feature.
change, with the majority of the changes involving
a schwa/central vowel.
We see small gains from using the phonetic fea-
ture in both accuracy and F-Score. We further ex-
amine only those named entities where dictionary
information is applied, and as expected, this subset
shows greater improvement (Table 4).
Table 5 compares our the Inside/Outside tag-
ging approach with a greedy approach described
earlier. The greedy approach only inserts a multi-
character substring when that substring reduces
more than 50% of the time in the overall train-
ing corpus. Since the Greedy method uses no
local contextual information, results are signifi-
cantly lower given the same feature set.
</bodyText>
<table confidence="0.999408857142857">
Language Segmentation ACC F-Score
Hindi I-O-B 34.5 88.6
Greedy 30.3 86.7
Tamil I-O-B 34.9 89.8
Greedy 28.2 87.5
Kannada I-O-B 28.7 88.0
Greedy 25.0 86.7
</table>
<tableCaption confidence="0.729684">
Table 5: Comparison of segmentation methods
on development data, using the U+B+T+P feature
set.
</tableCaption>
<subsectionHeader confidence="0.999646">
3.2 Test Data
</subsectionHeader>
<bodyText confidence="0.99969575">
Our model produces 10 candidates for each named
entity in the test data, ranked by the probability
that the model assigns the candidate. We filter out
candidates below the rank of 5 whose scores are
less than 0.5 lower than that of the highest rank-
ing candidate. Table 6 shows our results on the
test data, using a CRF trained on the training and
development data, with the feature set U+B+T+P.
</bodyText>
<table confidence="0.997400142857143">
Hindi Kannada Tamil
Accuracy 41.8 36.3 43.5
F-Score 87.9 87.0 90.2
MRR 54.6 48.2 57.2
MAPref 41.2 35.5 43.0
MAPIo 18.3 16.4 19.5
MAPsys 24.0 21.8 26.5
</table>
<tableCaption confidence="0.991746">
Table 6: Final results on the test data (in %).
</tableCaption>
<sectionHeader confidence="0.996657" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9993585">
Surya Ganesh, Sree Harsh, Prasad Pingali, and Va-
sudeva Varma. 2008. Statistical transliteration for
cross language information retrieval using HMM
alignment model and CRF. In Proceedings of the
2nd Workshop on Cross Lingual Information Access.
Taku Kudo. 2005. CRF++: Yet another CRF toolkit.
Available at http://chasen.org/ taku/software/crf++/.
A. Kumaran and Tobias Kellner. 2007. A generic
framework for machine transliteration. In Proceed-
ings of SIGIR-07.
Haizhou Li, A Kumaran, Min Zhang, and Vladimir
Pervouchine. 2009. Whitepaper of NEWS 2009
machine transliteration shared task. In Proceed-
ings of ACL-IJCNLP 2009 Named Entities Work-
shop (NEWS 2009).
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417–449.
Lance Ramshaw and Mitch Marcus. 1995. Text
chunking using transformation-based learning. In
Proceedings of WVLC-3.
Tarek Sherif and Grzegorz Kondrak. 2007. Substring-
based transliteration. In Proceedings ofACL-07.
Bing Zhao, Nguyen Bach, Ian Lane, and Stephan Vo-
gel. 2007. A log-linear block transliteration model
based on bi-stream HMMs. In Proceedings of
NAACL HLT 2007.
</reference>
<page confidence="0.999065">
95
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.768215">
<title confidence="0.994519">Substring-based Transliteration with Conditional Random Fields</title>
<author confidence="0.975339">Reddy</author>
<affiliation confidence="0.9972175">Department of Computer The University of</affiliation>
<address confidence="0.803262">Chicago, IL</address>
<abstract confidence="0.99785875">Motivated by phrase-based translation research, we present a transliteration system where characters are grouped into substrings to be mapped atomically into the target language. We show how this substring representation can be incorporated into a Conditional Random Field model that uses local context and phonemic information.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Surya Ganesh</author>
<author>Sree Harsh</author>
<author>Prasad Pingali</author>
<author>Vasudeva Varma</author>
</authors>
<title>Statistical transliteration for cross language information retrieval using HMM alignment model and CRF.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2nd Workshop on Cross Lingual Information Access.</booktitle>
<contexts>
<context position="7484" citStr="Ganesh et al. (2008)" startWordPosition="1227" endWordPosition="1230"> one-to-one, since their bidirectional alignments contain low-frequency substrings that have not been included in the alphabet.1 These pairs are removed from the training data, since only one-to-one alignments can be handled by the CRF. 2.3 Step 3: CRF transliteration With the transformed training data in hand, we can now train a CRF sequential model that uses substrings rather than characters as the basic token unit. The CRF algorithm is chosen for its ability to handle non-independent features of the source language input sequence. We use the open-source CRF++ software package (Kudo, 2005). Ganesh et al. (2008) also apply a CRF to the transliteration task (Hindi to English) but with different alignment methods than those presented here. In particular, multicharacter substrings are only used as tokens on the target (English) side, and a null token is used to account for deletion. We train our CRF using unigram, bigram, and trigram features over the source substrings, as well as pronunciation information described in §2.3.1. Table 2 describes these feature sets. 2.3.1 Phonetic information Since the CRF model allows us to incorporate nonindependent features, we add pronunciation data as a token-level f</context>
</contexts>
<marker>Ganesh, Harsh, Pingali, Varma, 2008</marker>
<rawString>Surya Ganesh, Sree Harsh, Prasad Pingali, and Vasudeva Varma. 2008. Statistical transliteration for cross language information retrieval using HMM alignment model and CRF. In Proceedings of the 2nd Workshop on Cross Lingual Information Access.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
</authors>
<title>CRF++: Yet another CRF toolkit. Available at http://chasen.org/ taku/software/crf++/.</title>
<date>2005</date>
<contexts>
<context position="7462" citStr="Kudo, 2005" startWordPosition="1225" endWordPosition="1226">e not aligned one-to-one, since their bidirectional alignments contain low-frequency substrings that have not been included in the alphabet.1 These pairs are removed from the training data, since only one-to-one alignments can be handled by the CRF. 2.3 Step 3: CRF transliteration With the transformed training data in hand, we can now train a CRF sequential model that uses substrings rather than characters as the basic token unit. The CRF algorithm is chosen for its ability to handle non-independent features of the source language input sequence. We use the open-source CRF++ software package (Kudo, 2005). Ganesh et al. (2008) also apply a CRF to the transliteration task (Hindi to English) but with different alignment methods than those presented here. In particular, multicharacter substrings are only used as tokens on the target (English) side, and a null token is used to account for deletion. We train our CRF using unigram, bigram, and trigram features over the source substrings, as well as pronunciation information described in §2.3.1. Table 2 describes these feature sets. 2.3.1 Phonetic information Since the CRF model allows us to incorporate nonindependent features, we add pronunciation d</context>
</contexts>
<marker>Kudo, 2005</marker>
<rawString>Taku Kudo. 2005. CRF++: Yet another CRF toolkit. Available at http://chasen.org/ taku/software/crf++/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kumaran</author>
<author>Tobias Kellner</author>
</authors>
<title>A generic framework for machine transliteration.</title>
<date>2007</date>
<booktitle>In Proceedings of SIGIR-07.</booktitle>
<contexts>
<context position="1942" citStr="Kumaran and Kellner, 2007" startWordPosition="291" endWordPosition="294"> use as features both local contexts and phonemic information acquired from an English pronunciation dictionary. 2 The Transliteration Process Our transliteration system has the following steps: 1. Pre-processing of the target language. 2. Substring alphabet generation for both the source and target. This step also generates training data for the CRFs in Step 3 and 4. 3. CRF training on aligned data from Step 2. 4. Substring segmentation and transliteration of source language input. Our training and test data consists of three sets – English to Hindi, English to Kannada, and English to Tamil (Kumaran and Kellner, 2007) – from the NEWS 2009 Machine Transliteration Shared Task (Li et al., 2009). 2.1 Step 1: Pre-Processing The written words of Hindi, Tamil, and Kannada correspond almost perfectly to their phonological forms, with each character mapping to a phoneme. The only exception to this arises from the implicit vowel (which may be a schwa /a/ or a central vowel /,e/) that is inserted after consonants that are not followed by the halanta or ‘killer stroke’. Hence, any mappings of an English vowel to a target language schwa will not be reflected in the alignment of the named entity pair. To minimize misali</context>
</contexts>
<marker>Kumaran, Kellner, 2007</marker>
<rawString>A. Kumaran and Tobias Kellner. 2007. A generic framework for machine transliteration. In Proceedings of SIGIR-07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>A Kumaran</author>
<author>Min Zhang</author>
<author>Vladimir Pervouchine</author>
</authors>
<title>machine transliteration shared task.</title>
<date>2009</date>
<journal>Whitepaper of NEWS</journal>
<booktitle>In Proceedings of ACL-IJCNLP 2009 Named Entities Workshop (NEWS</booktitle>
<contexts>
<context position="2017" citStr="Li et al., 2009" startWordPosition="304" endWordPosition="307">h pronunciation dictionary. 2 The Transliteration Process Our transliteration system has the following steps: 1. Pre-processing of the target language. 2. Substring alphabet generation for both the source and target. This step also generates training data for the CRFs in Step 3 and 4. 3. CRF training on aligned data from Step 2. 4. Substring segmentation and transliteration of source language input. Our training and test data consists of three sets – English to Hindi, English to Kannada, and English to Tamil (Kumaran and Kellner, 2007) – from the NEWS 2009 Machine Transliteration Shared Task (Li et al., 2009). 2.1 Step 1: Pre-Processing The written words of Hindi, Tamil, and Kannada correspond almost perfectly to their phonological forms, with each character mapping to a phoneme. The only exception to this arises from the implicit vowel (which may be a schwa /a/ or a central vowel /,e/) that is inserted after consonants that are not followed by the halanta or ‘killer stroke’. Hence, any mappings of an English vowel to a target language schwa will not be reflected in the alignment of the named entity pair. To minimize misalignments of target language strings with the English strings during training</context>
</contexts>
<marker>Li, Kumaran, Zhang, Pervouchine, 2009</marker>
<rawString>Haizhou Li, A Kumaran, Min Zhang, and Vladimir Pervouchine. 2009. Whitepaper of NEWS 2009 machine transliteration shared task. In Proceedings of ACL-IJCNLP 2009 Named Entities Workshop (NEWS 2009).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="5236" citStr="Och and Ney, 2003" startWordPosition="829" endWordPosition="832">lation (Och and Ney, 2004), where groups of words are translated as a unit. In the case of transliteration, the ‘phrases’ are commonly occurring substrings – sequences of characters – in one language that map to a character or a substring in the other. We use the term ‘substrings’ after a previous work (Sherif and Kondrak, 2007) that employs it in a noisy channel transliteration system. Zhao et al. (2007) also use substrings (which they call ‘blocks’) in a bi-stream HMM. We bootstrap the induction of substrings by aligning all named entity pairs in the training data, using the GIZA++ toolkit (Och and Ney, 2003). The toolkit performs unidirectional one-to-many alignments, meaning that a single symbol in its source string can be aligned to at most one symbol in its target. In order to induce many-to-many alignments, GIZA++ is run on the data in both directions (source language to target language and target language to source), and the bidirectional alignment of a named entity pair is taken to be the union of the alignments in each direction. Any inserted characters (maps within the alignment where the source or target character is null) are combined with the preceding character within the string. For </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="4644" citStr="Och and Ney, 2004" startWordPosition="729" endWordPosition="732">ersely, English sometimes uses a single character for a biphone (such as x for /ks/, or u for /ju/ as in museum), which is represented by two characters in the target languages. In certain cases, a digraph in English is transliterated to a digraph in the target, as a result of metathesis (le —* /al/, in words like temple). Further, all three target languages often insert vowels between English consonant clusters; for instance, Hindi inserts a schwa between s and p in ‘transport’, transliterated as transaport (). To handle these cases, we borrow the concept of phrases from machine translation (Och and Ney, 2004), where groups of words are translated as a unit. In the case of transliteration, the ‘phrases’ are commonly occurring substrings – sequences of characters – in one language that map to a character or a substring in the other. We use the term ‘substrings’ after a previous work (Sherif and Kondrak, 2007) that employs it in a noisy channel transliteration system. Zhao et al. (2007) also use substrings (which they call ‘blocks’) in a bi-stream HMM. We bootstrap the induction of substrings by aligning all named entity pairs in the training data, using the GIZA++ toolkit (Och and Ney, 2003). The to</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lance Ramshaw</author>
<author>Mitch Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1995</date>
<booktitle>In Proceedings of WVLC-3.</booktitle>
<contexts>
<context position="11073" citStr="Ramshaw and Marcus, 1995" startWordPosition="1819" endWordPosition="1822">tion is to greedily choose the most likely multi-character substring – in the example cited, we can choose (d e sh) because sh reduces more frequently than es. However, this creates the problem in cases where no reduction should occur, as with the ti in martini. Since contextual information is necessary to determine the correct substring segmentation, we model segmentation with a CRF, using a combination of character unigram, bigram, and trigram features. We use an approach motivated by the Inside/Outside representation of NP-chunking which treats segmentation as a tagging process over words (Ramshaw and Marcus, 1995). As in NP-chunking, our goal is to identify nonoverlapping, non-recursive segments in our input sequence. Our tagset is {I, O, B} where I indicates that a character is inside a substring, O indicates a character is outside a substring, and B marks a right boundary. After the test data has been segmented into its substring representation, it can be passed as input to the CRF model trained in Step 3 to produce our final transliteration output. 3 Results We first report our results on the development data provided by the NEWS task, for different feature sets and segmentation methods. We then pre</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>Lance Ramshaw and Mitch Marcus. 1995. Text chunking using transformation-based learning. In Proceedings of WVLC-3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tarek Sherif</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Substringbased transliteration.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL-07.</booktitle>
<contexts>
<context position="4948" citStr="Sherif and Kondrak, 2007" startWordPosition="781" endWordPosition="784">al/, in words like temple). Further, all three target languages often insert vowels between English consonant clusters; for instance, Hindi inserts a schwa between s and p in ‘transport’, transliterated as transaport (). To handle these cases, we borrow the concept of phrases from machine translation (Och and Ney, 2004), where groups of words are translated as a unit. In the case of transliteration, the ‘phrases’ are commonly occurring substrings – sequences of characters – in one language that map to a character or a substring in the other. We use the term ‘substrings’ after a previous work (Sherif and Kondrak, 2007) that employs it in a noisy channel transliteration system. Zhao et al. (2007) also use substrings (which they call ‘blocks’) in a bi-stream HMM. We bootstrap the induction of substrings by aligning all named entity pairs in the training data, using the GIZA++ toolkit (Och and Ney, 2003). The toolkit performs unidirectional one-to-many alignments, meaning that a single symbol in its source string can be aligned to at most one symbol in its target. In order to induce many-to-many alignments, GIZA++ is run on the data in both directions (source language to target language and target language to </context>
</contexts>
<marker>Sherif, Kondrak, 2007</marker>
<rawString>Tarek Sherif and Grzegorz Kondrak. 2007. Substringbased transliteration. In Proceedings ofACL-07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Zhao</author>
<author>Nguyen Bach</author>
<author>Ian Lane</author>
<author>Stephan Vogel</author>
</authors>
<title>A log-linear block transliteration model based on bi-stream HMMs.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL HLT</booktitle>
<contexts>
<context position="5026" citStr="Zhao et al. (2007)" startWordPosition="794" endWordPosition="797">etween English consonant clusters; for instance, Hindi inserts a schwa between s and p in ‘transport’, transliterated as transaport (). To handle these cases, we borrow the concept of phrases from machine translation (Och and Ney, 2004), where groups of words are translated as a unit. In the case of transliteration, the ‘phrases’ are commonly occurring substrings – sequences of characters – in one language that map to a character or a substring in the other. We use the term ‘substrings’ after a previous work (Sherif and Kondrak, 2007) that employs it in a noisy channel transliteration system. Zhao et al. (2007) also use substrings (which they call ‘blocks’) in a bi-stream HMM. We bootstrap the induction of substrings by aligning all named entity pairs in the training data, using the GIZA++ toolkit (Och and Ney, 2003). The toolkit performs unidirectional one-to-many alignments, meaning that a single symbol in its source string can be aligned to at most one symbol in its target. In order to induce many-to-many alignments, GIZA++ is run on the data in both directions (source language to target language and target language to source), and the bidirectional alignment of a named entity pair is taken to be</context>
</contexts>
<marker>Zhao, Bach, Lane, Vogel, 2007</marker>
<rawString>Bing Zhao, Nguyen Bach, Ian Lane, and Stephan Vogel. 2007. A log-linear block transliteration model based on bi-stream HMMs. In Proceedings of NAACL HLT 2007.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>