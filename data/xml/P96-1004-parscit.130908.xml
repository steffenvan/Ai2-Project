<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.056686">
<title confidence="0.995295">
Morphological Cues for Lexical Semantics
</title>
<author confidence="0.951419">
Marc Light
</author>
<affiliation confidence="0.6428525">
Seminar fiir Sprachwissenschaft
Universitat Tübingen
</affiliation>
<address confidence="0.925129666666667">
Wilhelmstr. 113
D-72074 Tiibingen
Germany
</address>
<email confidence="0.93991">
lightOsfs.nphil.uni-tuebingen.de
</email>
<sectionHeader confidence="0.993008" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999703538461539">
Most natural language processing tasks re-
quire lexical semantic information. Au-
tomated acquisition of this information
would thus increase the robustness and
portability of NLP systems. This pa-
per describes an acquisition method which
makes use of fixed correspondences be-
tween derivational affixes and lexical se-
mantic information. One advantage of this
method, and of other methods that rely
only on surface characteristics of language,
is that the necessary input is currently
available.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999963861111111">
Some natural language processing (NLP) tasks can
be performed with only coarse-grained semantic in-
formation about individual words. For example,
a system could utilize word frequency and a word
cooccurrence matrix in order to perform informa-
tion retrieval. However, many NLP tasks require at
least a partial understanding of every sentence or
utterance in the input and thus have a much greater
need for lexical semantics. Natural language gen-
eration, providing a natural language front end to
a database, information extraction, machine trans-
lation, and task-oriented dialogue understanding all
require lexical semantics. The lexical semantic in-
formation commonly utilized includes verbal argu-
ment structure and selectional restrictions, corre-
sponding nominal semantic class, verbal aspectual
class, synonym and antonym relationships between
words, and various verbal semantic features such as
causation and manner.
Machine readable dictionaries do not include
much of this information and it is difficult and time
consuming to encode it by hand. As a consequence,
current NLP systems have only small lexicons and
thus can only operate in restricted domains. Auto-
mated methods for acquiring lexical semantics could
increase both the robustness and the portability of
such systems. In addition, such methods might pro-
vide ingight into human language acquisition.
After considering different possible approaches to
acquiring lexical semantic information, this paper
concludes that a &amp;quot;surface cueing&amp;quot; approach is cur-
rently the most promising. It then introduces mor-
phological cueing, a type of surface cueing, and dis-
cusses an implementation. It concludes by evalu-
ating morphological cues with respect to a list of
desiderata for good surface cues.
</bodyText>
<sectionHeader confidence="0.957128" genericHeader="method">
2 Approaches to Acquiring Lexical
Semantics
</sectionHeader>
<bodyText confidence="0.999171366666667">
One intuitively appealing idea is that humans ac-
quire the meanings of words by relating them to
semantic representations resulting from perceptual
or cognitive processing. For example, in a situation
where the father says Kim is throwing the ball and
points at Kim who is throwing the ball, a child might
be able learn what throw and ball mean. In the
human language acquisition literature, Grimshaw
(1981) and Pinker (1989) advocate this approach;
others have described partial computer implementa-
tions: Pustejovsky (1988) and Siskind (1990). How-
ever, this approach cannot yet provide for the auto-
matic acquisition of lexical semantics for use in NLP
systems, because the input required must be hand
coded: no current artificial intelligence system has
the perceptual and cognitive capabilities required to
produce the needed semantic representations.
Another approach would be to use the semantics
of surrounding words in an utterance to constrain
the meaning of an unknown word. Borrowing an
example from Pinker (1994), upon hearing I glipped
the paper to shreds, one could guess that the mean-
ing of glib has something to do with tearing. Sim-
ilarly, one could guess that filp means something
like eat upon hearing I filped the delicious sandwich
and now I&apos;m full. These guesses are cued by the
meanings of paper, shreds, sandwich, delicious, full,
and the partial syntactic analysis of the utterances
that contain them. Granger (1977), Berwick (1983),
and Hastings (1994) describe computational systems
</bodyText>
<page confidence="0.996515">
25
</page>
<bodyText confidence="0.998667017857143">
that implement this approach. However, this ap-
proach is hindered by the need for a large amount
of initial lexical semantic information and the need
for a robust natural language understanding system
that produces semantic representations as output,
since producing this output requires precisely the
lexical semantic information the system is trying to
acquire.
A third approach does not require any semantic
information related to perceptual input or the in-
put utterance. Instead it makes use of fixed cor-
respondences between surface characteristics of lan-
guage input and lexical semantic information: sur-
face characteristics serve as cues for lexical seman-
tics of the words. For example, if a verb is seen
with a noun phrase subject and a sentential comple-
ment, it often has verbal semantics involving spa-
tial perception and cognition, e.g., believe, think,
worry, and see (Fisher, Gleitman, and Gleitman,
1991; Gleitman, 1990). Similarly, the occurrence
of a verb in the progressive tense can be used as
a cue for the non-stativeness of the verb (Dorr
and Lee, 1992); stative verbs cannot appear in the
progress tense (e.g.,*Mary is loving her new shoes).
Another example is the use of patterns such as
N P,N P * ,and otherN P to find lexical semantic
information such as hyponym (Hearst, 1992). Tem-
ples, treasuries, and other important civic buildings
is an example of this pattern and from it the infor-
mation that temples and treasuries are types of civic
buildings would be cued. Finally, inducing lexical
semantics from distributional data (e.g., (Brown et
al., 1992; Church et al., 1989)) is also a form of sur-
face cueing. It should be noted that the set of fixed
correspondences between surface characteristics and
lexical semantic information, at this point, have to
be acquired through the analysis of the researcher—
the issue of how the fixed correspondences can be
automatically acquired will not be addressed here.
The main advantage of the surface cueing ap-
proach is that the input required is currently avail-
able: there is an ever increasing supply of on-
line text, which can be automatically part-of-speech
tagged, assigned shallow syntactic structure by ro-
bust partial parsing systems, and morphologically
analyzed, all without any prior lexical semantics.
A possible disadvantage of surface cueing is that
surface cues for a particular piece of lexical semantics
might be difficult to uncover or they might not exist
at all. In addition, the cues might not be present
for the words of interest. Thus, it is an empirical
question whether easily identifiable abundant sur-
face cues exist for the needed lexical semantic infor-
mation. The next section explores the possibility of
using derivational affixes as surface cues for lexical
semantics.
</bodyText>
<sectionHeader confidence="0.7971315" genericHeader="method">
3 Morphological Cues for Lexical
Semantic Information
</sectionHeader>
<bodyText confidence="0.999350375">
Many derivational affixes only apply to bases with
certain semantic characteristics and only produce
derived forms with certain semantic characteristics.
For example, the verbal prefix un- applies to telic
verbs and produces telic derived forms. Thus, it is
possible to use un- as a cue for telicity. By search-
ing a sufficiently large corpus we should be able to
identify a number of telic verbs. Examples from the
Brown corpus include clasp, coil, fasten, lace, and
screw.
A more implementation-oriented description of
the process is the following: (i) analyze affixes by
hand to gain fixed correspondences between affix and
lexical semantic information (ii) collect a large cor-
pus of text, (iii) tag it with part-of-speech tags, (iv)
morphologically analyze its words, (v) assign word
senses to the base and the derived forms of these
analyses, and (vi) use this morphological structure
plus fixed correspondences to assign semantics to
both the base senses and the derived form senses.
Step (i) amounts to doing a semantic analysis of a
number of affixes the goal of which is to find se-
mantic generalizations for an affix that hold for a
large percentage of its instances. Finding the right
generalizations and stating them explicitly can be
time consuming but is only performed once. Tagging
the corpus is necessary to make word sense disam-
biguation and morphological analysis easier. Word
sense disambiguation is necessary because one needs
to know which sense of the base is involved in a
particular derived form, more specifically, to which
sense should one assign the feature cued by the affix.
For example, stress can be either a noun the stress
on the third syllable or a verb the advisor stressed
the importance of finishing quickly. Since the suffix
-ful applies to nominal bases, only a noun reading is
possible as the stem of stressful and thus one would
attach the lexical semantics cued by -Jul to the noun
sense. However, stress has multiple readings even
as a noun: it also has the reading exemplified by
the new parent was under a lot of stress. Only this
reading is possible for stressful.
In order to produce the results presented in the
next section, the above steps were performed as fol-
lows. A set of 18 affixes were analyzed by hand pro-
viding the fixed correspondences between cue and
semantics. The cued lexical semantic information
was axiomatized using Episodic Logic (Hwang and
Schubert, 1993), a situation-based extension of stan-
dard first order logic. The Penn Treebank ver-
sion of the Brown corpus (Marcus, Santorini, and
Marcinkiewicz, 1993) served as the corpus. Only
its words and part-of-speech tags were utilized. Al-
though these tags were corrected by hand, part-of-
speech tagging can be automatically performed with
an error rate of 3 to 4 percent (Merialdo, 1994; Brill,
</bodyText>
<page confidence="0.972535">
26
</page>
<bodyText confidence="0.970684314285714">
1994). The Alvey morphological analyzer (Ritchie et
al., 1992) was used to assign morphological struc-
ture. It uses a lexicon with just over 62,000 en-
tries. This lexicon was derived from a machine read-
able dictionary but contains no semantic informa-
tion. Word sense disambiguation for the bases and
derived forms that could not be resolved using part-
of-speech tags was not performed. However, there
exist systems for such word sense disambiguation
which do not require explicit lexical semantic infor-
mation (Yarowsky, 1993; Schiitze, 1992).
Let us consider an example. One sense of the suf-
fix -ize applies to adjectival bases (e.g., centralize).
This sense of the affix will be referred to as -Aize.
(A related but different sense applies to nouns, e.g.,
glamorize. The part-of-speech of the base is used
to disambiguate these two senses of -ize.) First,
the regular expressions &amp;quot;.*IZ(EIINGIESIED)$&amp;quot; and
&amp;quot;V .*&amp;quot; are used to collect tokens from the corpus
that were likely to have been derived using -ize. The
Alvey morphological analyzer is then applied to each
type. It strips off -Aize from a word if it can find
an entry with a reference form of the appropriate or-
thographic shape and has the features &amp;quot;uninflected,&amp;quot;
&amp;quot;latinate,&amp;quot; and &amp;quot;adjective.&amp;quot; It may also build an ap-
propriate base using other affixes, e.g.,[[tradition -al]
-Aize].1 Finally, all derived forms are assigned the
lexical semantic feature CHANGE-OF-STATE and all
the bases are assigned the lexical semantic feature
IZE-DEPENDENT. Only the CHANGE-OF-STATE fea-
ture will be discussed here. It is defined by the axiom
below.
For all predicates P with features
CHANGE-OF-STATE and DYADIC:
Vx,y,e [P(x,y)**e -&gt;
</bodyText>
<equation confidence="0.612127">
Del:Cat-end-of(e1,e) A
cause(e,e1)]
Crstate(P)(y)**e1] A
3e2:at-beginning-of(e2,e)
[-irstate(P)(y)**e2]]]
</equation>
<bodyText confidence="0.9992182">
The operator ** is analogous to =in situation
semantics; it indicates, among other things, that a
formula describes an event. P is a place holder for
the semantic predicate corresponding to the word
sense which has the feature. It is assumed that each
word sense corresponds to a single semantic predi-
cate. The axiom states that if a CHANGE-OF-STATE
predicate describes an event, then the result state of
this predicate holds at the end of this event and that
it did not hold at the beginning, e.g., if one wants to
</bodyText>
<footnote confidence="0.869572375">
&apos;In an alternative version of the method, the mor-
phological analyzer is also able to construct a base on
its own when it is unable to find an appropriate base
in its lexicon. However, these &amp;quot;new&amp;quot; bases seldom cor-
respond to actual words and thus the results presented
here were derived using a morphological analyzer config-
ured to only use bases that are directly in its lexicon or
can be constructed from words in its lexicon.
</footnote>
<bodyText confidence="0.999923037037037">
formalize something it must be non-formal to begin
with and will be formal after. The result state of an
-Aize predicate is the predicate corresponding to its
base; this is stated in another axiom.
Precision figures for the method were collected as
follows. The method returns a set of normalized
(i.e., uninflected) word/feature pairs. A human then
determines which pairs are &amp;quot;correct&amp;quot; where correct
means that the axiom defining the feature holds for
the instances (tokens) of the word (type). Because of
the lack of word senses, the semantics assigned to a
particular word is only considered correct, if it holds
for all senses occurring in the relevant derived word
tokens.&apos; For example, the axiom above must hold
for all senses of centralize occurring in the corpus
in order for the centralize/CHANGE-OF-STATE pair
to be correct. The axiom for IZE-DEPENDENT must
hold only for those senses of central that occur in the
tokens of centralize for the centra//izE-DEPENDENT
pair to be correct. This definition of correct was
constructed, in part, to make relatively quick hu-
man judgements possible. It should also be noted
that the semantic judgements require that the se-
mantics be expressed in a precise way. This discipline
is enforced in part by requiring that the features be
axiomatized in a denotational logic. Another argu-
ment for such an axiomatization is that many NLP
systems utilize a denotational logic for representing
semantic information and thus the axioms provide a
straightforward interface to the lexicon.
To return to our example, as shown in Table 1,
there were 63 -Aize derived words (types) of which
78 percent conform to the CHANGE-OF-STATE ax-
iom. Of the bases, 80 percent conform to the IZE-
DEPENDENT axiom which will be discussed in the
next section. Among the conforming words were
equalize, stabilize, and federalize. Two words that
seem to be derived using the -ize suffix but do not
conform to the CHANGE-OF-STATE axiom are penal-
ize and socialize (with the guests). A different sort
of non-conformity is produced when the morpholog-
ical analyzer finds a spurious parse. For example, it
analyzed subsidize as [sub- [side -ize]] and thus pro-
duced the sidize/cHANGE-OF-STATE pair which for
the relevant tokens was incorrect. In the first sort,
the non-conformity arises because the cue does not
always correspond to the relevant lexical semantic
information. In the second sort, the non-conformity
arises because a cue has been found where one does
not exist. A system that utilizes a lexicon so con-
structed is interested primarily in the overall preci-
sion of the information contained within and thus
the results presented in the next section conflate
these two types of false positives.
</bodyText>
<footnote confidence="0.975026666666667">
2Although this definition is required for many cases,
in the vast majority of the cases, the derived form and
its base have only one possible sense (e.g., stressful).
</footnote>
<page confidence="0.99827">
27
</page>
<sectionHeader confidence="0.999958" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999969086206897">
This section starts by discussing the semantics of 18
derivational affixes: re-, un-, de-, -ize, -en, -ify, -le,
-ate, -ee, -er, -ant, -age, -ment, mis-, -able, -ful, -
less, and -ness. Following this discussion, a table of
precision statistics for the performance of these sur-
face cues is presented. Due to space limitations, the
lexical semantics cued by these affixes can only be
loosely specified. However, they have been axiom-
atized in a fashion exemplified by the CHANGE-OF-
STATE axiom above (see (Light, 1996; Light, 1992)).
The verbal prefixes un-, de-, and re- cue aspec-
tual information for their base and derived forms.
Some examples from the Brown corpus are unfas-
ten, unwind, decompose, defocus, reactivate, and
readapt. Above it was noted that un- is a cue for
telicity. In fact, both un- and de- cue the CHANGE-
OF-STATE feature for their base and derived forms—
the CHANGE-OF-STATE feature entails the TELIC fea-
ture. In addition, for un- and de-, the result state of
the derived form is the negation of the result state of
the base (NEG-OF-BASE-IS-RSTATE), e.g., the result
of unfastening something is the opposite of the result
of fastening it. As shown by examples like reswim
the last lap, re- only cues the TELIC feature for its
base and derived forms: the lap might have been
swum previously and thus the negation of the result
state does not have to have held previously (Dowty,
1979). For re-, the result state of the derived form
is the same as that of the base (RSTATE-EQ-BASE-
RSTATE), e.g., the result of reactivating something is
the same as activating it. In fact, if one reactivates
something then it is also being activated: the derived
form entails the base (ENTAILS-BASE). Finally, for
re-, the derived form entails that its result state held
previously, e.g., if one recentralizes something then
it must have been central at some point previous to
the event of recentralization (PRESUPS-RSTATE).
The suffixes -Aize, -Nize, -en, -Aify, -Nify all
cue the CHANGE-OF-STATE feature for their derived
form as was discussed for -Aize above. Some ex-
emplars are centralize, formalize, categorize, colo-
nize, brighten, stiffen, falsify, intensify, mummify,
and glorify. For -Aize, -en and -Aify a bit more can
be said about the result state: it is the base predi-
cate (RSTATE-EQ-BASE), e.g., the result of formaliz-
ing something is that it is formal. Finally -Aize, -en,
and -Aify cue the following feature for their bases:
if a state holds of some individual then either an
event described by the derived form predicate oc-
curred previously or the predicate was always true
of the individual (IZE-DEPENDENT), e.g., if some-
thing is central then either it was centralized or it
was always central.
The &amp;quot;suffixes&amp;quot; -le and -ate should really be called
verbal endings since they are not suffixes in English,
i.e., if one strips them off one is seldom left with a
word. (Consequently, only regular expressions were
used to collect types; the morphological analyzer was
not used.) Nonetheless, they cue lexical semantics
and are easily identified. Some examples are chuckle,
dangle, alleviate, and assimilate. The ending -ate
cues a CHANGE-OF-STATE verb and -le an ACTIVITY
verb.
The derived forms produced by -ee, -er, and -ant
all refer to participants of an event described by their
base (PART-IN-E). Some examples are appointee, de-
portee, blower, campaigner, assailant, and claimant.
In addition, the derived form of -cc is also sentient
of this event and non-volitional with respect to it
(Barker, 1995).
The nominalizing suffixes -age and -ment both
produce derived forms that refer to something re-
sulting from an event of the verbal base predicate.
Some examples are blockage, seepage, marriage, pay-
ment, restatement, shipment, and treatment. The
derived forms of -age entail that an event occurred
and refer to something resulting from it (EVENT-
AND-RESULTANT)), e.g., seepage entails that seep-
ing took place and that the seepage resulted from
this seeping. Similarly, the derived forms of -ment
entail that an event took place and refer either to
this event, the proposition that the event occurred,
or something resulting from the event (REFERS-TO-
E-OR-PROP-OR-RESULT), e.g., a restatement entails
that a restating occurred and refers either to this
event, the proposition that the event occurred, or to
the actual utterance or written document resulting
from the restating event. (This analysis is based on
(Zucchi, 1989).)
The verbal prefix mis-, e.g., miscalculate and mis-
quote, cues the feature that an action is performed
in an incorrect manner (INCORRECT-MANNER). The
suffix -able cues a feature that it is possible to per-
form some action (ABLE-TO-BE-PERFORMED), e.g.,
something is enforceable if it is possible that some-
thing can enforce it (Dowty, 1979). The words de-
rived using -ness refer to a state of something having
the property of the base (STATE-OF-HAVING-PROP-
OF-BASE), e.g., in Kim&apos;s fierceness at the meeting
yesterday was unusual the word fierceness refers to
a state of Kim being fierce. The suffix -ful marks
its base as abstract (ABSTRACT): careful, peaceful,
powerful, etc. In addition, it marks its derived form
as the antonym of a form derived by -less if it exists
(LESS-ANTONYM). The suffix -less marks its derived
forms with the analogous feature (FUL-ANTONYM).
Some examples are colorful/less, fearful/less, harm-
ful/less, and tasteful/less.
The precision statistics for the individual lexical
semantic features discussed above are presented in
Table 1 and Table 2. Lexical semantic informa-
tion was collected for 2535 words (bases and derived
forms). One way to summarize these tables is to cal-
culate a single precision number for all the features
in a table, i. e. , average the number of correct types
for each affix, sum these averages, and then divide
</bodyText>
<page confidence="0.996428">
28
</page>
<bodyText confidence="0.999975869565217">
this sum by the total number of types. Using this
statistic it can be said that if a random word is de-
rived, its features have a 76 percent chance of being
true and if it is a stem of a derived form, its features
have a 82 percent chance of being true.
Computing recall requires finding all true tokens
of a cue. This is a labor intensive task. It was
performed for the verbal prefix re- and the recall
was found to be 85 percent. The majority of the
missed re- verbs were due to the fact that the system
only looked at verbs starting with RE and not other
parts-of-speech, e.g., many nominalizations such as
reaccommodation contain the re- morphological cue.
However, increasing recall by looking at all open
class categories would probably decrease precision.
Another cause of reduced recall is that some stems
were not in the Alvey lexicon or could not be prop-
erly extracted by the morphological analyzer. For
example, -Nize could not be stripped from hypoth-
esize because Alvey failed to reconstruct hypothesis
from hypothes. However, for the affixes discussed
here, 89 percent of the bases were present in the
Alvey lexicon.
</bodyText>
<sectionHeader confidence="0.996737" genericHeader="conclusions">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999862875">
Good surface cues are easy to identify, abundant,
and correspond to the needed lexical semantic in-
formation (Hearst (1992) identifies a similar set
of desiderata). With respect to these desiderata,
derivational morphology is both a good cue and a
bad cue.
Let us start with why it is a bad cue: there may
be no derivational cues for the lexical semantics of
a particular word. This is not the case for other
surface cues, e.g., distributional cues exist for every
word in a corpus. In addition, even if a derivational
cue does exist, the reliability (on average approxi-
mately 76 percent) of the lexical semantic informa-
tion is too low for many NLP tasks. This unrelia-
bility is due in part to the inherent exceptionality of
lexical generalization and thus can be improved only
partially.
However, derivational morphology is a good cue
in the following ways. It provides exactly the type
of lexical semantics needed for many NLP tasks: the
affixes discussed in the previous section cued nomi-
nal semantic class, verbal aspectual class, antonym
relationships between words, sentience, etc. In ad-
dition, working with the Brown corpus (11 million
words) and 18 affixes provided such information for
over 2500 words. Since corpora with over 40 million
words are common and English has over 40 com-
mon derivational affixes, one would expect to be able
to increase this number by an order of magnitude.
In addition, most English words are either derived
themselves or serve as bases of at least one deriva-
tional affix.3 Finally, for some NLP tasks, 76 per-
</bodyText>
<footnote confidence="0.947555">
3The following experiment supports this claim. Just
</footnote>
<table confidence="0.999978657894737">
Feature Affix Types Precision
TELIC re- 164 91%
RSTATE-EQ-BASE- re- 164 65%
RSTATE
ENTAILS-BASE re- 164 65%
PRESUPS-RSTATE re- 164 65%
CHANGE-OF-STATE un- 23 100%
NEG-OF-BASE-IS- un- 23 91%
RSTATE
CHANGE-OF-STATE de- 35 34%
NEG-OF-BASE-IS- de- 35 20%
RSTATE
CHANGE-OF-STATE -Aize 63 78%
RSTATE-EQ-BASE -Aize 63 75%
CHANGE-OF-STATE -Nize 86 56%
ACTIVITY -le 71 55%
CHANGE-OF-STATE -en 36 100%
RSTATE-EQ-BASE -en 36 97%
CHANGE-OF-STATE -Aify 17 94%
RSTATE-EQ-BASE -Aify 17 58%
CHANGE-OF-STATE -Nify 21 67%
CHANGE-OF-STATE -ate 365 48%
PART-IN-E -ee 22 91%
SENTIENT -ee 22 82%
NON-VOLITIONAL -ee 22 68%
PART-IN-E -er 471 85%
PART-IN-E -ant 21 81%
EVENT-AND- -age 43 58%
RESULTANT
REFERS-TO-E-OR- -ment 166 88%
PROP-OR-RESULTANT
INCORRECT-MANNER mis- 21 86%
ABLE-TO-BE- -able 148 84%
PERFORMED
STATE-OF-HAVING- -ness 307 97%
PROP-OF-BASE
FUL-ANTONYM -less 22 77%
LESS-ANTONYM -ful 22 77%
</table>
<tableCaption confidence="0.944964">
Table 1: Derived words
</tableCaption>
<table confidence="0.999968375">
Feature Affix Types Precision
TELIC re- 164 91%
CHANGE-OF-STATE Vun- 23 91%
CHANGE-OF-STATE Vde- 33 36%
IZE-DEPENDENT -Aize 64 80%
IZE-DEPENDENT -en 36 72%
IZE-DEPENDENT -Aify 15 40%
ABSTRACT -ful 76 93%
</table>
<tableCaption confidence="0.995816">
Table 2: Base words
</tableCaption>
<page confidence="0.998112">
29
</page>
<bodyText confidence="0.999988409090909">
cent reliability may be adequate. In addition, some
affixes are much more reliable cues than others and
thus if higher reliability is required then only the
affixes with high precision might be used.
The above discussion makes it clear that morpho-
logical cueing provides only a partial solution to the
problem of acquiring lexical semantic information.
However, as mentioned in section 2 there are many
types of surface cues which correspond to a vari-
ety of lexical semantic information. A combination
of cues should produce better precision where the
same information is indicated by multiple cues. For
example, the morphological cue re- indicates telic-
ity and as mentioned above, the syntactic cue the
progressive tense indicates non-stativity (Dorr and
Lee, 1992). Since telicity is a type of non-stativity,
the information is mutually supportive. In addition,
using many different types of cues should provide a
greater variety of information in general. Thus mor-
phological cueing is best seen as one type of surface
cueing that can be used in combination with others
to provide lexical semantic information.
</bodyText>
<sectionHeader confidence="0.999245" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999871">
A portion of this work was performed at the Uni-
versity of Rochester Computer Science Department
and supported by ONR/ARPA research grant num-
ber N00014-924-1512.
</bodyText>
<sectionHeader confidence="0.996818" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9880524125">
Barker, Chris. 1995. The semantics of -ee. In Pro-
ceedings of the SALT conference.
Berwick, Robert. 1983. Learning word meanings
from examples. In Proceedings of the 8th Interna-
tional Joint Conference on Artificial Intelligence
(IJCAI-83).
Brill, Eric. 1994. Some advances in transformation-
based part of speech tagging. In Proceedings of
the Twelfth National conference on Artificial In-
telligence: American Association for Artificial In-
telligence (AAAI).
Brown, Peter F., Vincent J. Della Pietra, Peter V.
deSouza, Jennifer C. Lai, and Robert L. Mercer.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18(4).
Church, Kenneth, William Gale, Patrick Hanks, and
Donald Hindle. 1989. Parsing, word associa-
tions and typical predicate-argument relations. In
International Workshop on Parsing Technologies,
pages 389-98.
over 400 open class words were picked randomly from
the Brown corpus and the derived forms were marked
by hand. Based on this data, a random open class word
in the Brown corpus has a 17 percent chance of being
derived, a 56 percent chance of being a stem of a derived
form, and an 8 percent chance of being both.
Dorr, Bonnie J. and Ki Lee. 1992. Building a lex-
icon for machine translation: Use of corpora for
aspectual classification of verbs. Technical Report
CS-TR-2876, University of Maryland.
Dowty, David. 1979. Word Meaning and Montague
Grammar. Reidel.
Fisher, Cynthia, Henry Gleitrnan, and Lila R. Gleit-
man. 1991. On the semantic content of subcatego-
rization frames. Cognitive Psychology, 23(3):331-
392.
Gleitman, Lila. 1990. The structural sources of verb
meanings. Language Acquisition, 1:3-55.
Granger, R. 1977. Foulup: a program that figures
out meanings of words from context. In Proceed-
ings of the 5th International Joint Conference on
Artificial Intelligence.
Grimshaw, Jane. 1981. Form, function, and the lan-
guage acquisition device. In C. L. Baker and J. J.
McCarthy, editors, the logical problem of language
acquisition. MIT Press.
Hastings, Peter. 1994. Automatic Acquistion of
Word Meaning from Context. Ph.D. thesis, Uni-
versity of Michigan.
Hearst, Marti. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings
of the fifteenth International Conference on Com-
putational Linguistics (COLING).
Hwang, Chung Hee and Lenhart Schubert. 1993.
Episodic logic: a comprehensive natural represen-
tation for language understanding. Mind and Ma-
chine, 3(4):381-419.
Light, Marc. 1992. Rehashing Re-. In Proceedings
of the Eastern States Conference on Linguistics.
Cornell University Linguistics Department Work-
ing Papers.
Light, Marc. 1996. Morphological Cues for Lexical
Semantics. Ph.D. thesis, University of Rochester,
Rochester, NY.
Marcus, Mitchell, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Compu-
tational Linguistics, 19(2):313-330.
Merialdo, Bernard. 1994. Tagging English text with
a probabilistic model. Computational Linguistics,
20(2):155-172.
Pinker, Steven. 1989. Learnability and Cognition:
The Acquisition of Argument Structure. MIT
Press.
Pinker, Steven. 1994. How could a child use verb
syntax to learn verb semantics? Lingua, 92:377-
410.
Pustejovsky, James. 1988. Constraints on the acqui-
sition of semantic knowledge. International jour-
nal of intelligent systems, 3:247-268.
</reference>
<page confidence="0.975367">
30
</page>
<reference confidence="0.998683545454545">
Ritchie, Graeme D., Graham J. Russell, Alan W.
Black, and Steve G. Pulman. 1992. Computa-
tional Morphology: Practical Mechanisms for the
English Lexicon. MIT press.
Schiitze, Hinrich. 1992. Word sense disambiguation
with sublexical representations. In Statistically-
Based NLP Techniques (American Association
for Artificial Intelligence Workshop, July 12-16,
1992, San Jose, CA.), pages 109-113.
Siskind, Jeffrey M. 1990. Acquiring core meanings
of words, represented as Jackendoff-style concep-
tual structures, from correlated streams of linguis-
tic and non-linguistic input. In Proceedings of
the 28th Meeting of the Association for Compu-
tational Linguistics.
Yarowsky, David. 1993. One sense per collocation.
In Proceedings of the ARPA Workshop on Human
Language Technology. Morgan Kaufmann.
Zucchi, Alessandro. 1989. The Language of Propo-
sitions and Events: Issues in the Syntax and the
Semantics of Nominalization. Ph.D. thesis, Uni-
versity of Massachusetts, Amherst, MA.
</reference>
<page confidence="0.999914">
31
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.637303">
<title confidence="0.998931">Morphological Cues for Lexical Semantics</title>
<author confidence="0.999735">Marc Light</author>
<affiliation confidence="0.888707">Seminar fiir Sprachwissenschaft Universitat Tübingen</affiliation>
<address confidence="0.954891333333333">Wilhelmstr. 113 D-72074 Tiibingen Germany</address>
<abstract confidence="0.994833285714286">Most natural language processing tasks require lexical semantic information. Automated acquisition of this information would thus increase the robustness and portability of NLP systems. This paper describes an acquisition method which makes use of fixed correspondences between derivational affixes and lexical semantic information. One advantage of this method, and of other methods that rely only on surface characteristics of language, is that the necessary input is currently available.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chris Barker</author>
</authors>
<title>The semantics of -ee.</title>
<date>1995</date>
<booktitle>In Proceedings of the SALT conference.</booktitle>
<contexts>
<context position="18753" citStr="Barker, 1995" startWordPosition="3009" endWordPosition="3010">lar expressions were used to collect types; the morphological analyzer was not used.) Nonetheless, they cue lexical semantics and are easily identified. Some examples are chuckle, dangle, alleviate, and assimilate. The ending -ate cues a CHANGE-OF-STATE verb and -le an ACTIVITY verb. The derived forms produced by -ee, -er, and -ant all refer to participants of an event described by their base (PART-IN-E). Some examples are appointee, deportee, blower, campaigner, assailant, and claimant. In addition, the derived form of -cc is also sentient of this event and non-volitional with respect to it (Barker, 1995). The nominalizing suffixes -age and -ment both produce derived forms that refer to something resulting from an event of the verbal base predicate. Some examples are blockage, seepage, marriage, payment, restatement, shipment, and treatment. The derived forms of -age entail that an event occurred and refer to something resulting from it (EVENTAND-RESULTANT)), e.g., seepage entails that seeping took place and that the seepage resulted from this seeping. Similarly, the derived forms of -ment entail that an event took place and refer either to this event, the proposition that the event occurred, </context>
</contexts>
<marker>Barker, 1995</marker>
<rawString>Barker, Chris. 1995. The semantics of -ee. In Proceedings of the SALT conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Berwick</author>
</authors>
<title>Learning word meanings from examples.</title>
<date>1983</date>
<booktitle>In Proceedings of the 8th International Joint Conference on Artificial Intelligence (IJCAI-83).</booktitle>
<contexts>
<context position="3939" citStr="Berwick (1983)" startWordPosition="589" endWordPosition="590">resentations. Another approach would be to use the semantics of surrounding words in an utterance to constrain the meaning of an unknown word. Borrowing an example from Pinker (1994), upon hearing I glipped the paper to shreds, one could guess that the meaning of glib has something to do with tearing. Similarly, one could guess that filp means something like eat upon hearing I filped the delicious sandwich and now I&apos;m full. These guesses are cued by the meanings of paper, shreds, sandwich, delicious, full, and the partial syntactic analysis of the utterances that contain them. Granger (1977), Berwick (1983), and Hastings (1994) describe computational systems 25 that implement this approach. However, this approach is hindered by the need for a large amount of initial lexical semantic information and the need for a robust natural language understanding system that produces semantic representations as output, since producing this output requires precisely the lexical semantic information the system is trying to acquire. A third approach does not require any semantic information related to perceptual input or the input utterance. Instead it makes use of fixed correspondences between surface characte</context>
</contexts>
<marker>Berwick, 1983</marker>
<rawString>Berwick, Robert. 1983. Learning word meanings from examples. In Proceedings of the 8th International Joint Conference on Artificial Intelligence (IJCAI-83).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Some advances in transformationbased part of speech tagging.</title>
<date>1994</date>
<booktitle>In Proceedings of the Twelfth National conference on Artificial Intelligence: American Association for Artificial Intelligence (AAAI).</booktitle>
<marker>Brill, 1994</marker>
<rawString>Brill, Eric. 1994. Some advances in transformationbased part of speech tagging. In Proceedings of the Twelfth National conference on Artificial Intelligence: American Association for Artificial Intelligence (AAAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Peter V deSouza</author>
<author>Jennifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="5569" citStr="Brown et al., 1992" startWordPosition="849" endWordPosition="852">e of a verb in the progressive tense can be used as a cue for the non-stativeness of the verb (Dorr and Lee, 1992); stative verbs cannot appear in the progress tense (e.g.,*Mary is loving her new shoes). Another example is the use of patterns such as N P,N P * ,and otherN P to find lexical semantic information such as hyponym (Hearst, 1992). Temples, treasuries, and other important civic buildings is an example of this pattern and from it the information that temples and treasuries are types of civic buildings would be cued. Finally, inducing lexical semantics from distributional data (e.g., (Brown et al., 1992; Church et al., 1989)) is also a form of surface cueing. It should be noted that the set of fixed correspondences between surface characteristics and lexical semantic information, at this point, have to be acquired through the analysis of the researcher— the issue of how the fixed correspondences can be automatically acquired will not be addressed here. The main advantage of the surface cueing approach is that the input required is currently available: there is an ever increasing supply of online text, which can be automatically part-of-speech tagged, assigned shallow syntactic structure by r</context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>Brown, Peter F., Vincent J. Della Pietra, Peter V. deSouza, Jennifer C. Lai, and Robert L. Mercer. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
<author>William Gale</author>
<author>Patrick Hanks</author>
<author>Donald Hindle</author>
</authors>
<title>Parsing, word associations and typical predicate-argument relations.</title>
<date>1989</date>
<booktitle>In International Workshop on Parsing Technologies,</booktitle>
<pages>389--98</pages>
<contexts>
<context position="5591" citStr="Church et al., 1989" startWordPosition="853" endWordPosition="856">rogressive tense can be used as a cue for the non-stativeness of the verb (Dorr and Lee, 1992); stative verbs cannot appear in the progress tense (e.g.,*Mary is loving her new shoes). Another example is the use of patterns such as N P,N P * ,and otherN P to find lexical semantic information such as hyponym (Hearst, 1992). Temples, treasuries, and other important civic buildings is an example of this pattern and from it the information that temples and treasuries are types of civic buildings would be cued. Finally, inducing lexical semantics from distributional data (e.g., (Brown et al., 1992; Church et al., 1989)) is also a form of surface cueing. It should be noted that the set of fixed correspondences between surface characteristics and lexical semantic information, at this point, have to be acquired through the analysis of the researcher— the issue of how the fixed correspondences can be automatically acquired will not be addressed here. The main advantage of the surface cueing approach is that the input required is currently available: there is an ever increasing supply of online text, which can be automatically part-of-speech tagged, assigned shallow syntactic structure by robust partial parsing </context>
</contexts>
<marker>Church, Gale, Hanks, Hindle, 1989</marker>
<rawString>Church, Kenneth, William Gale, Patrick Hanks, and Donald Hindle. 1989. Parsing, word associations and typical predicate-argument relations. In International Workshop on Parsing Technologies, pages 389-98.</rawString>
</citation>
<citation valid="false">
<title>over 400 open class words were picked randomly from the Brown corpus and the derived forms were marked by hand. Based on this data, a random open class word in the Brown corpus has a 17 percent chance of being derived, a 56 percent chance of being a stem of a derived form, and an 8 percent chance of being both.</title>
<marker></marker>
<rawString>over 400 open class words were picked randomly from the Brown corpus and the derived forms were marked by hand. Based on this data, a random open class word in the Brown corpus has a 17 percent chance of being derived, a 56 percent chance of being a stem of a derived form, and an 8 percent chance of being both.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie J Dorr</author>
<author>Ki Lee</author>
</authors>
<title>Building a lexicon for machine translation: Use of corpora for aspectual classification of verbs.</title>
<date>1992</date>
<tech>Technical Report CS-TR-2876,</tech>
<institution>University of Maryland.</institution>
<contexts>
<context position="5065" citStr="Dorr and Lee, 1992" startWordPosition="765" endWordPosition="768">r the input utterance. Instead it makes use of fixed correspondences between surface characteristics of language input and lexical semantic information: surface characteristics serve as cues for lexical semantics of the words. For example, if a verb is seen with a noun phrase subject and a sentential complement, it often has verbal semantics involving spatial perception and cognition, e.g., believe, think, worry, and see (Fisher, Gleitman, and Gleitman, 1991; Gleitman, 1990). Similarly, the occurrence of a verb in the progressive tense can be used as a cue for the non-stativeness of the verb (Dorr and Lee, 1992); stative verbs cannot appear in the progress tense (e.g.,*Mary is loving her new shoes). Another example is the use of patterns such as N P,N P * ,and otherN P to find lexical semantic information such as hyponym (Hearst, 1992). Temples, treasuries, and other important civic buildings is an example of this pattern and from it the information that temples and treasuries are types of civic buildings would be cued. Finally, inducing lexical semantics from distributional data (e.g., (Brown et al., 1992; Church et al., 1989)) is also a form of surface cueing. It should be noted that the set of fix</context>
</contexts>
<marker>Dorr, Lee, 1992</marker>
<rawString>Dorr, Bonnie J. and Ki Lee. 1992. Building a lexicon for machine translation: Use of corpora for aspectual classification of verbs. Technical Report CS-TR-2876, University of Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Dowty</author>
</authors>
<title>Word Meaning and Montague Grammar.</title>
<date>1979</date>
<location>Reidel.</location>
<contexts>
<context position="16639" citStr="Dowty, 1979" startWordPosition="2667" endWordPosition="2668">, both un- and de- cue the CHANGEOF-STATE feature for their base and derived forms— the CHANGE-OF-STATE feature entails the TELIC feature. In addition, for un- and de-, the result state of the derived form is the negation of the result state of the base (NEG-OF-BASE-IS-RSTATE), e.g., the result of unfastening something is the opposite of the result of fastening it. As shown by examples like reswim the last lap, re- only cues the TELIC feature for its base and derived forms: the lap might have been swum previously and thus the negation of the result state does not have to have held previously (Dowty, 1979). For re-, the result state of the derived form is the same as that of the base (RSTATE-EQ-BASERSTATE), e.g., the result of reactivating something is the same as activating it. In fact, if one reactivates something then it is also being activated: the derived form entails the base (ENTAILS-BASE). Finally, for re-, the derived form entails that its result state held previously, e.g., if one recentralizes something then it must have been central at some point previous to the event of recentralization (PRESUPS-RSTATE). The suffixes -Aize, -Nize, -en, -Aify, -Nify all cue the CHANGE-OF-STATE featu</context>
<context position="20014" citStr="Dowty, 1979" startWordPosition="3206" endWordPosition="3207">-OR-PROP-OR-RESULT), e.g., a restatement entails that a restating occurred and refers either to this event, the proposition that the event occurred, or to the actual utterance or written document resulting from the restating event. (This analysis is based on (Zucchi, 1989).) The verbal prefix mis-, e.g., miscalculate and misquote, cues the feature that an action is performed in an incorrect manner (INCORRECT-MANNER). The suffix -able cues a feature that it is possible to perform some action (ABLE-TO-BE-PERFORMED), e.g., something is enforceable if it is possible that something can enforce it (Dowty, 1979). The words derived using -ness refer to a state of something having the property of the base (STATE-OF-HAVING-PROPOF-BASE), e.g., in Kim&apos;s fierceness at the meeting yesterday was unusual the word fierceness refers to a state of Kim being fierce. The suffix -ful marks its base as abstract (ABSTRACT): careful, peaceful, powerful, etc. In addition, it marks its derived form as the antonym of a form derived by -less if it exists (LESS-ANTONYM). The suffix -less marks its derived forms with the analogous feature (FUL-ANTONYM). Some examples are colorful/less, fearful/less, harmful/less, and tastef</context>
</contexts>
<marker>Dowty, 1979</marker>
<rawString>Dowty, David. 1979. Word Meaning and Montague Grammar. Reidel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia Fisher</author>
<author>Henry Gleitrnan</author>
<author>Lila R Gleitman</author>
</authors>
<title>On the semantic content of subcategorization frames.</title>
<date>1991</date>
<pages>23--3</pages>
<publisher>Cognitive Psychology,</publisher>
<marker>Fisher, Gleitrnan, Gleitman, 1991</marker>
<rawString>Fisher, Cynthia, Henry Gleitrnan, and Lila R. Gleitman. 1991. On the semantic content of subcategorization frames. Cognitive Psychology, 23(3):331-392.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lila Gleitman</author>
</authors>
<title>The structural sources of verb meanings.</title>
<date>1990</date>
<journal>Language Acquisition,</journal>
<pages>1--3</pages>
<contexts>
<context position="4925" citStr="Gleitman, 1990" startWordPosition="741" endWordPosition="742">ic information the system is trying to acquire. A third approach does not require any semantic information related to perceptual input or the input utterance. Instead it makes use of fixed correspondences between surface characteristics of language input and lexical semantic information: surface characteristics serve as cues for lexical semantics of the words. For example, if a verb is seen with a noun phrase subject and a sentential complement, it often has verbal semantics involving spatial perception and cognition, e.g., believe, think, worry, and see (Fisher, Gleitman, and Gleitman, 1991; Gleitman, 1990). Similarly, the occurrence of a verb in the progressive tense can be used as a cue for the non-stativeness of the verb (Dorr and Lee, 1992); stative verbs cannot appear in the progress tense (e.g.,*Mary is loving her new shoes). Another example is the use of patterns such as N P,N P * ,and otherN P to find lexical semantic information such as hyponym (Hearst, 1992). Temples, treasuries, and other important civic buildings is an example of this pattern and from it the information that temples and treasuries are types of civic buildings would be cued. Finally, inducing lexical semantics from di</context>
</contexts>
<marker>Gleitman, 1990</marker>
<rawString>Gleitman, Lila. 1990. The structural sources of verb meanings. Language Acquisition, 1:3-55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Granger</author>
</authors>
<title>Foulup: a program that figures out meanings of words from context.</title>
<date>1977</date>
<booktitle>In Proceedings of the 5th International Joint Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="3923" citStr="Granger (1977)" startWordPosition="587" endWordPosition="588">ded semantic representations. Another approach would be to use the semantics of surrounding words in an utterance to constrain the meaning of an unknown word. Borrowing an example from Pinker (1994), upon hearing I glipped the paper to shreds, one could guess that the meaning of glib has something to do with tearing. Similarly, one could guess that filp means something like eat upon hearing I filped the delicious sandwich and now I&apos;m full. These guesses are cued by the meanings of paper, shreds, sandwich, delicious, full, and the partial syntactic analysis of the utterances that contain them. Granger (1977), Berwick (1983), and Hastings (1994) describe computational systems 25 that implement this approach. However, this approach is hindered by the need for a large amount of initial lexical semantic information and the need for a robust natural language understanding system that produces semantic representations as output, since producing this output requires precisely the lexical semantic information the system is trying to acquire. A third approach does not require any semantic information related to perceptual input or the input utterance. Instead it makes use of fixed correspondences between </context>
</contexts>
<marker>Granger, 1977</marker>
<rawString>Granger, R. 1977. Foulup: a program that figures out meanings of words from context. In Proceedings of the 5th International Joint Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Grimshaw</author>
</authors>
<title>Form, function, and the language acquisition device.</title>
<date>1981</date>
<editor>In C. L. Baker and J. J. McCarthy, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2892" citStr="Grimshaw (1981)" startWordPosition="424" endWordPosition="425">a type of surface cueing, and discusses an implementation. It concludes by evaluating morphological cues with respect to a list of desiderata for good surface cues. 2 Approaches to Acquiring Lexical Semantics One intuitively appealing idea is that humans acquire the meanings of words by relating them to semantic representations resulting from perceptual or cognitive processing. For example, in a situation where the father says Kim is throwing the ball and points at Kim who is throwing the ball, a child might be able learn what throw and ball mean. In the human language acquisition literature, Grimshaw (1981) and Pinker (1989) advocate this approach; others have described partial computer implementations: Pustejovsky (1988) and Siskind (1990). However, this approach cannot yet provide for the automatic acquisition of lexical semantics for use in NLP systems, because the input required must be hand coded: no current artificial intelligence system has the perceptual and cognitive capabilities required to produce the needed semantic representations. Another approach would be to use the semantics of surrounding words in an utterance to constrain the meaning of an unknown word. Borrowing an example fro</context>
</contexts>
<marker>Grimshaw, 1981</marker>
<rawString>Grimshaw, Jane. 1981. Form, function, and the language acquisition device. In C. L. Baker and J. J. McCarthy, editors, the logical problem of language acquisition. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Hastings</author>
</authors>
<title>Automatic Acquistion of Word Meaning from Context.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Michigan.</institution>
<contexts>
<context position="3960" citStr="Hastings (1994)" startWordPosition="592" endWordPosition="593">r approach would be to use the semantics of surrounding words in an utterance to constrain the meaning of an unknown word. Borrowing an example from Pinker (1994), upon hearing I glipped the paper to shreds, one could guess that the meaning of glib has something to do with tearing. Similarly, one could guess that filp means something like eat upon hearing I filped the delicious sandwich and now I&apos;m full. These guesses are cued by the meanings of paper, shreds, sandwich, delicious, full, and the partial syntactic analysis of the utterances that contain them. Granger (1977), Berwick (1983), and Hastings (1994) describe computational systems 25 that implement this approach. However, this approach is hindered by the need for a large amount of initial lexical semantic information and the need for a robust natural language understanding system that produces semantic representations as output, since producing this output requires precisely the lexical semantic information the system is trying to acquire. A third approach does not require any semantic information related to perceptual input or the input utterance. Instead it makes use of fixed correspondences between surface characteristics of language i</context>
</contexts>
<marker>Hastings, 1994</marker>
<rawString>Hastings, Peter. 1994. Automatic Acquistion of Word Meaning from Context. Ph.D. thesis, University of Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the fifteenth International Conference on Computational Linguistics (COLING).</booktitle>
<contexts>
<context position="5293" citStr="Hearst, 1992" startWordPosition="807" endWordPosition="808">mple, if a verb is seen with a noun phrase subject and a sentential complement, it often has verbal semantics involving spatial perception and cognition, e.g., believe, think, worry, and see (Fisher, Gleitman, and Gleitman, 1991; Gleitman, 1990). Similarly, the occurrence of a verb in the progressive tense can be used as a cue for the non-stativeness of the verb (Dorr and Lee, 1992); stative verbs cannot appear in the progress tense (e.g.,*Mary is loving her new shoes). Another example is the use of patterns such as N P,N P * ,and otherN P to find lexical semantic information such as hyponym (Hearst, 1992). Temples, treasuries, and other important civic buildings is an example of this pattern and from it the information that temples and treasuries are types of civic buildings would be cued. Finally, inducing lexical semantics from distributional data (e.g., (Brown et al., 1992; Church et al., 1989)) is also a form of surface cueing. It should be noted that the set of fixed correspondences between surface characteristics and lexical semantic information, at this point, have to be acquired through the analysis of the researcher— the issue of how the fixed correspondences can be automatically acqu</context>
<context position="22304" citStr="Hearst (1992)" startWordPosition="3592" endWordPosition="3593">cal cue. However, increasing recall by looking at all open class categories would probably decrease precision. Another cause of reduced recall is that some stems were not in the Alvey lexicon or could not be properly extracted by the morphological analyzer. For example, -Nize could not be stripped from hypothesize because Alvey failed to reconstruct hypothesis from hypothes. However, for the affixes discussed here, 89 percent of the bases were present in the Alvey lexicon. 5 Evaluation Good surface cues are easy to identify, abundant, and correspond to the needed lexical semantic information (Hearst (1992) identifies a similar set of desiderata). With respect to these desiderata, derivational morphology is both a good cue and a bad cue. Let us start with why it is a bad cue: there may be no derivational cues for the lexical semantics of a particular word. This is not the case for other surface cues, e.g., distributional cues exist for every word in a corpus. In addition, even if a derivational cue does exist, the reliability (on average approximately 76 percent) of the lexical semantic information is too low for many NLP tasks. This unreliability is due in part to the inherent exceptionality of</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Hearst, Marti. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the fifteenth International Conference on Computational Linguistics (COLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chung Hee Hwang</author>
<author>Lenhart Schubert</author>
</authors>
<title>Episodic logic: a comprehensive natural representation for language understanding.</title>
<date>1993</date>
<journal>Mind and Machine,</journal>
<pages>3--4</pages>
<contexts>
<context position="9232" citStr="Hwang and Schubert, 1993" startWordPosition="1448" endWordPosition="1451"> a noun reading is possible as the stem of stressful and thus one would attach the lexical semantics cued by -Jul to the noun sense. However, stress has multiple readings even as a noun: it also has the reading exemplified by the new parent was under a lot of stress. Only this reading is possible for stressful. In order to produce the results presented in the next section, the above steps were performed as follows. A set of 18 affixes were analyzed by hand providing the fixed correspondences between cue and semantics. The cued lexical semantic information was axiomatized using Episodic Logic (Hwang and Schubert, 1993), a situation-based extension of standard first order logic. The Penn Treebank version of the Brown corpus (Marcus, Santorini, and Marcinkiewicz, 1993) served as the corpus. Only its words and part-of-speech tags were utilized. Although these tags were corrected by hand, part-ofspeech tagging can be automatically performed with an error rate of 3 to 4 percent (Merialdo, 1994; Brill, 26 1994). The Alvey morphological analyzer (Ritchie et al., 1992) was used to assign morphological structure. It uses a lexicon with just over 62,000 entries. This lexicon was derived from a machine readable dictio</context>
</contexts>
<marker>Hwang, Schubert, 1993</marker>
<rawString>Hwang, Chung Hee and Lenhart Schubert. 1993. Episodic logic: a comprehensive natural representation for language understanding. Mind and Machine, 3(4):381-419.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Light</author>
</authors>
<title>Rehashing Re-.</title>
<date>1992</date>
<booktitle>In Proceedings of the Eastern States Conference on Linguistics.</booktitle>
<institution>Cornell University Linguistics Department Working Papers.</institution>
<contexts>
<context position="15765" citStr="Light, 1992" startWordPosition="2515" endWordPosition="2516">ived form and its base have only one possible sense (e.g., stressful). 27 4 Results This section starts by discussing the semantics of 18 derivational affixes: re-, un-, de-, -ize, -en, -ify, -le, -ate, -ee, -er, -ant, -age, -ment, mis-, -able, -ful, - less, and -ness. Following this discussion, a table of precision statistics for the performance of these surface cues is presented. Due to space limitations, the lexical semantics cued by these affixes can only be loosely specified. However, they have been axiomatized in a fashion exemplified by the CHANGE-OFSTATE axiom above (see (Light, 1996; Light, 1992)). The verbal prefixes un-, de-, and re- cue aspectual information for their base and derived forms. Some examples from the Brown corpus are unfasten, unwind, decompose, defocus, reactivate, and readapt. Above it was noted that un- is a cue for telicity. In fact, both un- and de- cue the CHANGEOF-STATE feature for their base and derived forms— the CHANGE-OF-STATE feature entails the TELIC feature. In addition, for un- and de-, the result state of the derived form is the negation of the result state of the base (NEG-OF-BASE-IS-RSTATE), e.g., the result of unfastening something is the opposite o</context>
</contexts>
<marker>Light, 1992</marker>
<rawString>Light, Marc. 1992. Rehashing Re-. In Proceedings of the Eastern States Conference on Linguistics. Cornell University Linguistics Department Working Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Light</author>
</authors>
<title>Morphological Cues for Lexical Semantics.</title>
<date>1996</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Rochester,</institution>
<location>Rochester, NY.</location>
<contexts>
<context position="15751" citStr="Light, 1996" startWordPosition="2513" endWordPosition="2514">ases, the derived form and its base have only one possible sense (e.g., stressful). 27 4 Results This section starts by discussing the semantics of 18 derivational affixes: re-, un-, de-, -ize, -en, -ify, -le, -ate, -ee, -er, -ant, -age, -ment, mis-, -able, -ful, - less, and -ness. Following this discussion, a table of precision statistics for the performance of these surface cues is presented. Due to space limitations, the lexical semantics cued by these affixes can only be loosely specified. However, they have been axiomatized in a fashion exemplified by the CHANGE-OFSTATE axiom above (see (Light, 1996; Light, 1992)). The verbal prefixes un-, de-, and re- cue aspectual information for their base and derived forms. Some examples from the Brown corpus are unfasten, unwind, decompose, defocus, reactivate, and readapt. Above it was noted that un- is a cue for telicity. In fact, both un- and de- cue the CHANGEOF-STATE feature for their base and derived forms— the CHANGE-OF-STATE feature entails the TELIC feature. In addition, for un- and de-, the result state of the derived form is the negation of the result state of the base (NEG-OF-BASE-IS-RSTATE), e.g., the result of unfastening something is </context>
</contexts>
<marker>Light, 1996</marker>
<rawString>Light, Marc. 1996. Morphological Cues for Lexical Semantics. Ph.D. thesis, University of Rochester, Rochester, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<pages>19--2</pages>
<contexts>
<context position="9382" citStr="Marcus, Santorini, and Marcinkiewicz, 1993" startWordPosition="1470" endWordPosition="1474">However, stress has multiple readings even as a noun: it also has the reading exemplified by the new parent was under a lot of stress. Only this reading is possible for stressful. In order to produce the results presented in the next section, the above steps were performed as follows. A set of 18 affixes were analyzed by hand providing the fixed correspondences between cue and semantics. The cued lexical semantic information was axiomatized using Episodic Logic (Hwang and Schubert, 1993), a situation-based extension of standard first order logic. The Penn Treebank version of the Brown corpus (Marcus, Santorini, and Marcinkiewicz, 1993) served as the corpus. Only its words and part-of-speech tags were utilized. Although these tags were corrected by hand, part-ofspeech tagging can be automatically performed with an error rate of 3 to 4 percent (Merialdo, 1994; Brill, 26 1994). The Alvey morphological analyzer (Ritchie et al., 1992) was used to assign morphological structure. It uses a lexicon with just over 62,000 entries. This lexicon was derived from a machine readable dictionary but contains no semantic information. Word sense disambiguation for the bases and derived forms that could not be resolved using partof-speech ta</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Marcus, Mitchell, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Merialdo</author>
</authors>
<title>Tagging English text with a probabilistic model.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<pages>20--2</pages>
<contexts>
<context position="9609" citStr="Merialdo, 1994" startWordPosition="1511" endWordPosition="1512">eps were performed as follows. A set of 18 affixes were analyzed by hand providing the fixed correspondences between cue and semantics. The cued lexical semantic information was axiomatized using Episodic Logic (Hwang and Schubert, 1993), a situation-based extension of standard first order logic. The Penn Treebank version of the Brown corpus (Marcus, Santorini, and Marcinkiewicz, 1993) served as the corpus. Only its words and part-of-speech tags were utilized. Although these tags were corrected by hand, part-ofspeech tagging can be automatically performed with an error rate of 3 to 4 percent (Merialdo, 1994; Brill, 26 1994). The Alvey morphological analyzer (Ritchie et al., 1992) was used to assign morphological structure. It uses a lexicon with just over 62,000 entries. This lexicon was derived from a machine readable dictionary but contains no semantic information. Word sense disambiguation for the bases and derived forms that could not be resolved using partof-speech tags was not performed. However, there exist systems for such word sense disambiguation which do not require explicit lexical semantic information (Yarowsky, 1993; Schiitze, 1992). Let us consider an example. One sense of the suf</context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>Merialdo, Bernard. 1994. Tagging English text with a probabilistic model. Computational Linguistics, 20(2):155-172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Pinker</author>
</authors>
<title>Learnability and Cognition: The Acquisition of Argument Structure.</title>
<date>1989</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2910" citStr="Pinker (1989)" startWordPosition="427" endWordPosition="428">eing, and discusses an implementation. It concludes by evaluating morphological cues with respect to a list of desiderata for good surface cues. 2 Approaches to Acquiring Lexical Semantics One intuitively appealing idea is that humans acquire the meanings of words by relating them to semantic representations resulting from perceptual or cognitive processing. For example, in a situation where the father says Kim is throwing the ball and points at Kim who is throwing the ball, a child might be able learn what throw and ball mean. In the human language acquisition literature, Grimshaw (1981) and Pinker (1989) advocate this approach; others have described partial computer implementations: Pustejovsky (1988) and Siskind (1990). However, this approach cannot yet provide for the automatic acquisition of lexical semantics for use in NLP systems, because the input required must be hand coded: no current artificial intelligence system has the perceptual and cognitive capabilities required to produce the needed semantic representations. Another approach would be to use the semantics of surrounding words in an utterance to constrain the meaning of an unknown word. Borrowing an example from Pinker (1994), u</context>
</contexts>
<marker>Pinker, 1989</marker>
<rawString>Pinker, Steven. 1989. Learnability and Cognition: The Acquisition of Argument Structure. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Pinker</author>
</authors>
<title>How could a child use verb syntax to learn verb semantics?</title>
<date>1994</date>
<journal>Lingua,</journal>
<pages>92--377</pages>
<contexts>
<context position="3507" citStr="Pinker (1994)" startWordPosition="516" endWordPosition="517">nd Pinker (1989) advocate this approach; others have described partial computer implementations: Pustejovsky (1988) and Siskind (1990). However, this approach cannot yet provide for the automatic acquisition of lexical semantics for use in NLP systems, because the input required must be hand coded: no current artificial intelligence system has the perceptual and cognitive capabilities required to produce the needed semantic representations. Another approach would be to use the semantics of surrounding words in an utterance to constrain the meaning of an unknown word. Borrowing an example from Pinker (1994), upon hearing I glipped the paper to shreds, one could guess that the meaning of glib has something to do with tearing. Similarly, one could guess that filp means something like eat upon hearing I filped the delicious sandwich and now I&apos;m full. These guesses are cued by the meanings of paper, shreds, sandwich, delicious, full, and the partial syntactic analysis of the utterances that contain them. Granger (1977), Berwick (1983), and Hastings (1994) describe computational systems 25 that implement this approach. However, this approach is hindered by the need for a large amount of initial lexic</context>
</contexts>
<marker>Pinker, 1994</marker>
<rawString>Pinker, Steven. 1994. How could a child use verb syntax to learn verb semantics? Lingua, 92:377-410.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
</authors>
<title>Constraints on the acquisition of semantic knowledge. International journal of intelligent systems,</title>
<date>1988</date>
<pages>3--247</pages>
<contexts>
<context position="3009" citStr="Pustejovsky (1988)" startWordPosition="439" endWordPosition="440">ect to a list of desiderata for good surface cues. 2 Approaches to Acquiring Lexical Semantics One intuitively appealing idea is that humans acquire the meanings of words by relating them to semantic representations resulting from perceptual or cognitive processing. For example, in a situation where the father says Kim is throwing the ball and points at Kim who is throwing the ball, a child might be able learn what throw and ball mean. In the human language acquisition literature, Grimshaw (1981) and Pinker (1989) advocate this approach; others have described partial computer implementations: Pustejovsky (1988) and Siskind (1990). However, this approach cannot yet provide for the automatic acquisition of lexical semantics for use in NLP systems, because the input required must be hand coded: no current artificial intelligence system has the perceptual and cognitive capabilities required to produce the needed semantic representations. Another approach would be to use the semantics of surrounding words in an utterance to constrain the meaning of an unknown word. Borrowing an example from Pinker (1994), upon hearing I glipped the paper to shreds, one could guess that the meaning of glib has something t</context>
</contexts>
<marker>Pustejovsky, 1988</marker>
<rawString>Pustejovsky, James. 1988. Constraints on the acquisition of semantic knowledge. International journal of intelligent systems, 3:247-268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme D Ritchie</author>
<author>Graham J Russell</author>
<author>Alan W Black</author>
<author>Steve G Pulman</author>
</authors>
<title>Computational Morphology: Practical Mechanisms for the English Lexicon.</title>
<date>1992</date>
<publisher>MIT press.</publisher>
<contexts>
<context position="9683" citStr="Ritchie et al., 1992" startWordPosition="1520" endWordPosition="1523"> hand providing the fixed correspondences between cue and semantics. The cued lexical semantic information was axiomatized using Episodic Logic (Hwang and Schubert, 1993), a situation-based extension of standard first order logic. The Penn Treebank version of the Brown corpus (Marcus, Santorini, and Marcinkiewicz, 1993) served as the corpus. Only its words and part-of-speech tags were utilized. Although these tags were corrected by hand, part-ofspeech tagging can be automatically performed with an error rate of 3 to 4 percent (Merialdo, 1994; Brill, 26 1994). The Alvey morphological analyzer (Ritchie et al., 1992) was used to assign morphological structure. It uses a lexicon with just over 62,000 entries. This lexicon was derived from a machine readable dictionary but contains no semantic information. Word sense disambiguation for the bases and derived forms that could not be resolved using partof-speech tags was not performed. However, there exist systems for such word sense disambiguation which do not require explicit lexical semantic information (Yarowsky, 1993; Schiitze, 1992). Let us consider an example. One sense of the suffix -ize applies to adjectival bases (e.g., centralize). This sense of the</context>
</contexts>
<marker>Ritchie, Russell, Black, Pulman, 1992</marker>
<rawString>Ritchie, Graeme D., Graham J. Russell, Alan W. Black, and Steve G. Pulman. 1992. Computational Morphology: Practical Mechanisms for the English Lexicon. MIT press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schiitze</author>
</authors>
<title>Word sense disambiguation with sublexical representations.</title>
<date>1992</date>
<booktitle>In StatisticallyBased NLP Techniques (American Association for Artificial Intelligence Workshop,</booktitle>
<pages>109--113</pages>
<location>San Jose, CA.),</location>
<contexts>
<context position="10159" citStr="Schiitze, 1992" startWordPosition="1598" endWordPosition="1599">ly performed with an error rate of 3 to 4 percent (Merialdo, 1994; Brill, 26 1994). The Alvey morphological analyzer (Ritchie et al., 1992) was used to assign morphological structure. It uses a lexicon with just over 62,000 entries. This lexicon was derived from a machine readable dictionary but contains no semantic information. Word sense disambiguation for the bases and derived forms that could not be resolved using partof-speech tags was not performed. However, there exist systems for such word sense disambiguation which do not require explicit lexical semantic information (Yarowsky, 1993; Schiitze, 1992). Let us consider an example. One sense of the suffix -ize applies to adjectival bases (e.g., centralize). This sense of the affix will be referred to as -Aize. (A related but different sense applies to nouns, e.g., glamorize. The part-of-speech of the base is used to disambiguate these two senses of -ize.) First, the regular expressions &amp;quot;.*IZ(EIINGIESIED)$&amp;quot; and &amp;quot;V .*&amp;quot; are used to collect tokens from the corpus that were likely to have been derived using -ize. The Alvey morphological analyzer is then applied to each type. It strips off -Aize from a word if it can find an entry with a reference</context>
</contexts>
<marker>Schiitze, 1992</marker>
<rawString>Schiitze, Hinrich. 1992. Word sense disambiguation with sublexical representations. In StatisticallyBased NLP Techniques (American Association for Artificial Intelligence Workshop, July 12-16, 1992, San Jose, CA.), pages 109-113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey M Siskind</author>
</authors>
<title>Acquiring core meanings of words, represented as Jackendoff-style conceptual structures, from correlated streams of linguistic and non-linguistic input.</title>
<date>1990</date>
<booktitle>In Proceedings of the 28th Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3028" citStr="Siskind (1990)" startWordPosition="442" endWordPosition="443">rata for good surface cues. 2 Approaches to Acquiring Lexical Semantics One intuitively appealing idea is that humans acquire the meanings of words by relating them to semantic representations resulting from perceptual or cognitive processing. For example, in a situation where the father says Kim is throwing the ball and points at Kim who is throwing the ball, a child might be able learn what throw and ball mean. In the human language acquisition literature, Grimshaw (1981) and Pinker (1989) advocate this approach; others have described partial computer implementations: Pustejovsky (1988) and Siskind (1990). However, this approach cannot yet provide for the automatic acquisition of lexical semantics for use in NLP systems, because the input required must be hand coded: no current artificial intelligence system has the perceptual and cognitive capabilities required to produce the needed semantic representations. Another approach would be to use the semantics of surrounding words in an utterance to constrain the meaning of an unknown word. Borrowing an example from Pinker (1994), upon hearing I glipped the paper to shreds, one could guess that the meaning of glib has something to do with tearing. </context>
</contexts>
<marker>Siskind, 1990</marker>
<rawString>Siskind, Jeffrey M. 1990. Acquiring core meanings of words, represented as Jackendoff-style conceptual structures, from correlated streams of linguistic and non-linguistic input. In Proceedings of the 28th Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>One sense per collocation.</title>
<date>1993</date>
<booktitle>In Proceedings of the ARPA Workshop on Human Language Technology.</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="10142" citStr="Yarowsky, 1993" startWordPosition="1596" endWordPosition="1597">n be automatically performed with an error rate of 3 to 4 percent (Merialdo, 1994; Brill, 26 1994). The Alvey morphological analyzer (Ritchie et al., 1992) was used to assign morphological structure. It uses a lexicon with just over 62,000 entries. This lexicon was derived from a machine readable dictionary but contains no semantic information. Word sense disambiguation for the bases and derived forms that could not be resolved using partof-speech tags was not performed. However, there exist systems for such word sense disambiguation which do not require explicit lexical semantic information (Yarowsky, 1993; Schiitze, 1992). Let us consider an example. One sense of the suffix -ize applies to adjectival bases (e.g., centralize). This sense of the affix will be referred to as -Aize. (A related but different sense applies to nouns, e.g., glamorize. The part-of-speech of the base is used to disambiguate these two senses of -ize.) First, the regular expressions &amp;quot;.*IZ(EIINGIESIED)$&amp;quot; and &amp;quot;V .*&amp;quot; are used to collect tokens from the corpus that were likely to have been derived using -ize. The Alvey morphological analyzer is then applied to each type. It strips off -Aize from a word if it can find an entry</context>
</contexts>
<marker>Yarowsky, 1993</marker>
<rawString>Yarowsky, David. 1993. One sense per collocation. In Proceedings of the ARPA Workshop on Human Language Technology. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Zucchi</author>
</authors>
<title>The Language of Propositions and Events: Issues in the Syntax and the Semantics of Nominalization.</title>
<date>1989</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Massachusetts,</institution>
<location>Amherst, MA.</location>
<contexts>
<context position="19675" citStr="Zucchi, 1989" startWordPosition="3152" endWordPosition="3153">o something resulting from it (EVENTAND-RESULTANT)), e.g., seepage entails that seeping took place and that the seepage resulted from this seeping. Similarly, the derived forms of -ment entail that an event took place and refer either to this event, the proposition that the event occurred, or something resulting from the event (REFERS-TOE-OR-PROP-OR-RESULT), e.g., a restatement entails that a restating occurred and refers either to this event, the proposition that the event occurred, or to the actual utterance or written document resulting from the restating event. (This analysis is based on (Zucchi, 1989).) The verbal prefix mis-, e.g., miscalculate and misquote, cues the feature that an action is performed in an incorrect manner (INCORRECT-MANNER). The suffix -able cues a feature that it is possible to perform some action (ABLE-TO-BE-PERFORMED), e.g., something is enforceable if it is possible that something can enforce it (Dowty, 1979). The words derived using -ness refer to a state of something having the property of the base (STATE-OF-HAVING-PROPOF-BASE), e.g., in Kim&apos;s fierceness at the meeting yesterday was unusual the word fierceness refers to a state of Kim being fierce. The suffix -fu</context>
</contexts>
<marker>Zucchi, 1989</marker>
<rawString>Zucchi, Alessandro. 1989. The Language of Propositions and Events: Issues in the Syntax and the Semantics of Nominalization. Ph.D. thesis, University of Massachusetts, Amherst, MA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>