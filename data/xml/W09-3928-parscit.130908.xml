<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002187">
<title confidence="0.9952075">
The Role of Interactivity in Human-Machine Conversation for Automatic
Word Acquisition
</title>
<author confidence="0.99898">
Shaolin Qu Joyce Y. Chai
</author>
<affiliation confidence="0.9968065">
Department of Computer Science and Engineering
Michigan State University
</affiliation>
<address confidence="0.983076">
East Lansing, MI 48824
</address>
<email confidence="0.99984">
{qushaoli,jchai}@cse.msu.edu
</email>
<sectionHeader confidence="0.997402" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99991475">
Motivated by the psycholinguistic finding
that human eye gaze is tightly linked to
speech production, previous work has ap-
plied naturally occurring eye gaze for au-
tomatic vocabulary acquisition. However,
unlike in the typical settings for psycholin-
guistic studies, eye gaze can serve differ-
ent functions in human-machine conver-
sation. Some gaze streams do not link
to the content of the spoken utterances
and thus can be potentially detrimental to
word acquisition. To address this prob-
lem, this paper investigates the incorpo-
ration of interactivity in identifying the
close coupling of speech and gaze streams
for word acquisition. Our empirical re-
sults indicate that automatic identification
of closely coupled gaze-speech streams
leads to significantly better word acquisi-
tion performance.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999831896551724">
Spoken conversational interfaces have become in-
creasingly important in many applications such
as remote interaction with robots (Lemon et al.,
2002), intelligent space station control (Aist et
al., 2003), and automated training and educa-
tion (Razzaq and Heffernan, 2004). As in any con-
versational system, one major bottleneck in con-
versational interfaces is robust language interpre-
tation. To address this problem, previous multi-
modal conversational systems have utilized pen-
based or deictic gestures (Bangalore and John-
ston, 2004; Qu and Chai, 2006) to improve in-
terpretation. Besides gestures, eye movements
that naturally occur during interaction provide an-
other important channel for language understand-
ing, for example, reference resolution (Byron et
al., 2005; Prasov and Chai, 2008). Recent work
has also shown that what users look at on the inter-
face (e.g., natural scenes or generated graphic dis-
plays) during speech production provides unique
opportunities for word acquisition, namely auto-
matically acquiring semantic meanings of spoken
words by grounding them to visual entities (Liu
et al., 2007) or domain concepts (Qu and Chai,
2008).
Psycholinguistic studies have shown that eye
gaze indicates a person’s attention (Just and Car-
penter, 1976), and eye movement can facilitate
spoken language comprehension (Tanenhaus et
al., 1995; Eberhard et al., 1995). It has been
found that users’ eyes move to the mentioned ob-
ject directly before speaking a word (Meyer et
al., 1998; Rayner, 1998; Griffin and Bock, 2000).
This parallel behavior of eye gaze and speech pro-
duction motivates our previous work on word ac-
quisition (Liu et al., 2007; Qu and Chai, 2008).
However, in interactive conversation, human gaze
behavior is much more complex than in the typ-
ical controlled settings used in psycholinguistic
studies. There are different types of eye move-
ments (Kahneman, 1973). The naturally occur-
ring eye gaze during speech production may serve
different functions, for example, to engage in the
conversation or to manage turn taking (Nakano et
al., 2003). Furthermore, while interacting with a
graphic display, a user could be talking about ob-
jects that were previously seen on the display or
something completely unrelated to any object the
user is looking at. Therefore using every speech-
gaze pair for word acquisition can be detrimental.
The type of gaze that is mostly useful for word
acquisition is the kind that reflects the underlying
attention and tightly links to the content of the co-
occurring speech. Thus, one important question
is how to identify the closely coupled speech and
gaze streams to improve word acquisition.
To address this question, we develop an ap-
proach that incorporates interactivity (e.g., speech,
</bodyText>
<note confidence="0.709984">
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 188–195,
</note>
<affiliation confidence="0.662637">
Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics
</affiliation>
<page confidence="0.998866">
188
</page>
<bodyText confidence="0.999573125">
user activity, conversation context) with eye gaze
to identify closely coupled speech and gaze
streams. We further use the identified speech
and gaze streams to acquire words with a trans-
lation model. Our empirical evaluation demon-
strates that automatic identification of closely cou-
pled gaze-speech streams can lead to significantly
better word acquisition performance.
</bodyText>
<sectionHeader confidence="0.999874" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999983242424242">
Previous work has explored word acquisition by
grounding words to visual entities. In (Roy and
Pentland, 2002), given speech paired with video
images of objects, mutual information between
auditory and visual signals was used to acquire
words by associating acoustic phone sequences
with the visual prototypes (e.g., color, size, shape)
of objects. Given parallel pictures and descrip-
tion texts, generative models were used to acquire
words by associating words with image regions in
(Barnard et al., 2003). Different from this previous
work, in our work, the visual attention foci accom-
panying speech are indicated by eye gaze. As an
implicit and subconscious input, eye gaze brings
additional challenges in word acquisition.
Eye gaze has been explored for word acqui-
sition in previous work. In (Yu and Ballard,
2004), given speech paired with eye gaze and
video images, a translation model was used to
acquire words by associating acoustic phone se-
quences with visual representations of objects and
actions. Word acquisition from transcribed speech
and eye gaze during human-machine conversa-
tion has been investigated recently. In (Liu et
al., 2007), a translation model was developed to
associate words with visual objects on a graphi-
cal display. In our previous work (Qu and Chai,
2008), enhanced translation models incorporat-
ing speech-gaze temporal information and domain
knowledge were developed to improve word ac-
quisition. However, none of these previous works
has investigated the role of interactivity in word
acquisition, which is the focus of this paper.
</bodyText>
<sectionHeader confidence="0.994149" genericHeader="method">
3 Data Collection
</sectionHeader>
<bodyText confidence="0.999851">
We collected speech and eye gaze data through
user studies. This data set is different from the data
set used in our previous work (Qu and Chai, 2008).
The difference lies in two aspects: 1) the data for
this investigation was collected during mixed ini-
tiative human-machine conversation whereas the
data in (Qu and Chai, 2008) was based only on
question and answering; 2) user studies were con-
ducted in a more complex domain for this investi-
gation, which resulted in a richer data set that con-
tains a larger vocabulary.
</bodyText>
<subsectionHeader confidence="0.954474">
3.1 Domain
</subsectionHeader>
<figureCaption confidence="0.997843">
Figure 1: Treasure hunting domain
</figureCaption>
<bodyText confidence="0.991086538461538">
Figure 1 shows the 3D treasure hunting domain
used in our work. In this application, the user
needs to consult with a remote “expert” (i.e., an ar-
tificial system) to find hidden treasures in a castle
with 115 3D objects. The expert has some knowl-
edge about the treasures but can not see the cas-
tle. The user has to talk to the expert for advice
regarding finding the treasures. The application is
developed based on a game engine and provides an
immersive environment for the user to navigate in
the 3D space. During the experiment, each user’s
speech was recorded, and the user’s eye gaze was
captured by a Tobii eye tracker.
</bodyText>
<subsectionHeader confidence="0.999544">
3.2 Data Preprocessing
</subsectionHeader>
<bodyText confidence="0.999981333333333">
From 20 users’ experiments, we collected 3709 ut-
terances with accompanying gaze fixations. We
transcribed the collected speech. The vocabulary
size of the speech transcript is 1082, among which
227 are either nouns or adjectives. The user’s
speech was also automatically recognized online
by the Microsoft speech recognizer with a word
error rate (WER) of 48.1% for the 1-best recog-
nition. The vocabulary size of the 1-best speech
recognition is 3041, among which 1643 are either
nouns or adjectives.
The collected speech and gaze streams were au-
tomatically paired together by the system. Each
time the system detected a sentence boundary (in-
dicated by a long pause of 500 milliseconds) of the
user’s speech, it paired the recognized speech with
the gaze fixations that the system had been ac-
cumulating since the previously detected sentence
</bodyText>
<page confidence="0.998913">
189
</page>
<figureCaption confidence="0.86968">
Figure 2: Accompanying gaze fixations and the 1-best recognition of a user’s utterance “There’s a purple
vase and an orange vase.” (There are two incorrectly recognized words “in” and “face” in the 1-best
recognition)
</figureCaption>
<figure confidence="0.983758625">
There’s a purple vase in an orange face
speech stream
gaze stream
[fixated entity]
[vase_purple] [vase_greek3] [vase_greek3] [vase_greek3] [vase_greek3]
ts te
[table_vase]
gaze fixation
</figure>
<bodyText confidence="0.997984444444445">
boundary. Figure 2 shows a pair of user speech
and accompanying stream of gaze fixations. In
the speech stream, each spoken word was times-
tamped by the speech recognizer. In the gaze
stream, each gaze fixation has a starting timestamp
ts and an ending timestamp te provided by the eye
tracker. Each gaze fixation results in a fixated en-
tity (3D object). When multiple entities are fixated
by one gaze fixation due to the overlapping of en-
tities, the one in the forefront is chosen.
Given the paired speech and gaze streams, we
build a set of parallel word sequence and gaze fix-
ated entity sequence {(w, e)} for the task of word
acquisition. In section 6, we will evaluate word
acquisition in two settings: 1) word sequence w
contains all of the nouns/adjectives in the speech
transcript, and 2) w contains all of the recognized
nouns/adjectives in the 1-best speech recognition.
</bodyText>
<sectionHeader confidence="0.968417" genericHeader="method">
4 Word Acquisition With Eye Gaze
</sectionHeader>
<bodyText confidence="0.986940730769231">
The task of word acquisition in our application is
to ground words to the visual entities. Specifi-
cally, given the parallel word and entity sequences
{(w, e)}, we want to find the best match between
the words and the entities. Following our previ-
ous work (Qu and Chai, 2008), we formulate word
acquisition as a translation problem and use trans-
lation models for word acquisition. For each en-
tity e, we first estimate the word-entity association
probability p(w|e) with a translation model, then
choose the words with the highest probabilities as
acquired words for e.
Inspired by the psycholinguistic findings that
users’ eyes move to the mentioned object before
speaking a word (Meyer et al., 1998; Rayner,
1998; Griffin and Bock, 2000), in our previous
work (Qu and Chai, 2008), we have incorpo-
rated the gaze-speech temporal information in the
translation model as follows (referred as Model-2t
through the rest of this paper):
pt(aj = i|j, e, w)p(wj|ei)
where l and m are the lengths of entity and word
sequences respectively. In this equation, pt(aj =
i|j, e, w) is the temporal alignment probability
representing the probability that wj is aligned with
ei, which is further defined by:
</bodyText>
<equation confidence="0.99418">
pt(aj = i|j, e, w) =
�
0 d(ei, wj) &gt; 0
exp[α�d(ei,wj)]
Ei exp[α-d(ei,wj)] d(ei, wj) ≤ 0
</equation>
<bodyText confidence="0.9999923125">
where α is a scaling factor, and d(ei, wj) is the
temporal distance between ei and wj. Based on
the psycholinguistic finding that eye gaze happens
before a spoken word, wj is not allowed to be
aligned with ei when wj happens earlier than ei
(i.e., d(ei, wj) &gt; 0). When wj happens no earlier
than ei (i.e., d(ei, wj) ≤ 0), the closer they are, the
more likely they are aligned. An EM algorithm is
used to estimate p(w|e) and α in the model.
Our evaluation in (Qu and Chai, 2008) has
shown that Model-2t that incorporates temporal
alignment between speech and eye gaze achieves
significantly better word acquisition performance
compared to the model where no temporal align-
ment is introduced. Therefore, this model is used
for the investigation in this paper.
</bodyText>
<sectionHeader confidence="0.9294615" genericHeader="method">
5 Identification of Closely Coupled
Gaze-Speech Pairs
</sectionHeader>
<bodyText confidence="0.9991586">
Successful word acquisition with the translation
models relies on the tight coupling between the
gaze fixations and the speech content. As men-
tioned earlier, not all gaze-speech pairs have this
tight coupling. In a gaze-speech pair, if the speech
</bodyText>
<equation confidence="0.994672857142857">
l
m
H
j=1
p(w|e) =
�
i=0
</equation>
<page confidence="0.962206">
190
</page>
<bodyText confidence="0.9999236">
does not have any word that relates to any of the
gaze fixated entities, this instance only adds noise
to word acquisition. Therefore, we should identify
the closely coupled gaze-speech pairs and only use
them for word acquisition.
In this section, we first describe the feature ex-
traction, then evaluate the application of a logis-
tic regression classifier to predict whether a gaze-
speech pair is a closely coupled gaze-speech in-
stance – an instance where at least one noun or
adjective in the speech stream describes some en-
tity fixated by the gaze stream. For the training of
the classifier, we manually labeled each instance
as either a coupled instance or not based on the
speech transcript and the gaze fixations.
</bodyText>
<subsectionHeader confidence="0.994516">
5.1 Feature Extraction
</subsectionHeader>
<bodyText confidence="0.999587">
For a gaze-speech instance, the following sets of
features are automatically extracted.
</bodyText>
<subsubsectionHeader confidence="0.961804">
5.1.1 Speech Features (S)
</subsubsectionHeader>
<bodyText confidence="0.9833035">
The following features are extracted from
speech:
</bodyText>
<listItem confidence="0.962428833333333">
• cw – count of nouns and adjectives.
More nouns and adjectives are expected in
the user’s utterance describing entities.
• cw/ls – normalized noun/adjective count.
The effect of speech length ls on cw is con-
sidered.
</listItem>
<subsubsectionHeader confidence="0.971124">
5.1.2 Gaze Features (G)
</subsubsectionHeader>
<bodyText confidence="0.998877">
For each fixated entity ei, let lie be its temporal
fixation length. Note that several gaze fixations
may have the same fixated entity, lie is the total
length of all the gaze fixations that fixate on entity
ei. We extract the following features from gaze
stream:
</bodyText>
<listItem confidence="0.9998768">
• ce – count of different gaze fixated entities.
Fewer fixated entities are expected when the
user is describing entities while looking at
them.
• ce/ls – normalized entity count.
</listItem>
<bodyText confidence="0.8661805">
The effect of temporal spoken utterance
length ls on ce is considered.
</bodyText>
<listItem confidence="0.982454">
• maxi(lie) – maximal fixation length.
</listItem>
<bodyText confidence="0.948464">
At least one fixated entity’s fixation is ex-
pected to be long enough when the user is
describing entities while looking at them.
• mean(lie) – average fixation length.
The average gaze fixation length is expected
to be longer when the user is describing enti-
ties while looking at them.
</bodyText>
<listItem confidence="0.970906">
• var(lie) – variance of fixation lengths.
</listItem>
<bodyText confidence="0.958802111111111">
The variance of the fixation lengths is ex-
pected to be smaller when the user is describ-
ing entities while looking at them.
The number of gaze fixated entities is not only
determined by the user’s eye gaze, but also af-
fected by the visual scene. Let cse be the count
of all the entities that have been visible during the
time period concurrent with the gaze stream. We
also extract the following scene related feature:
</bodyText>
<listItem confidence="0.983863">
• ce/cse – scene-normalized fixated entity
count.
</listItem>
<bodyText confidence="0.9673725">
The effect of the visual scene on ce is consid-
ered.
</bodyText>
<subsubsectionHeader confidence="0.981038">
5.1.3 User Activity Features (UA)
</subsubsectionHeader>
<bodyText confidence="0.9996372">
While interacting with the system, the user’s ac-
tivity can also be helpful in determining whether
the user’s eye gaze is tightly linked to the content
of the speech. The following features are extracted
from the user’s activities:
</bodyText>
<listItem confidence="0.999308666666667">
• maximal distance of the user’s movements –
the maximal change of user position (3D co-
ordinates) during speech.
</listItem>
<bodyText confidence="0.99343">
The user is expected to move within a smaller
range while looking at entities and describing
them.
</bodyText>
<listItem confidence="0.993446">
• variance of the user’s positions
</listItem>
<bodyText confidence="0.998673">
The user is expected to move less frequently
while looking at entities and describing them.
</bodyText>
<subsubsectionHeader confidence="0.948373">
5.1.4 Conversation Context Features (CC)
</subsubsectionHeader>
<bodyText confidence="0.999941888888889">
While talking to the system (i.e., the “expert”),
the user’s language and gaze behavior are influ-
enced by the state of the conversation. For each
gaze-speech instance, we use the previous sys-
tem response type as a nominal feature to predict
whether this is a closely coupled gaze-speech in-
stance.
In our treasure hunting domain, there are 8 types
of system responses in 2 categories:
</bodyText>
<sectionHeader confidence="0.44483" genericHeader="method">
System Initiative Responses:
</sectionHeader>
<listItem confidence="0.998876">
• speci�c-see – the system asks whether the
user sees a certain entity, e.g., “Do you see
another couch?”.
• nonspeci�c-see – the system asks whether the
user sees anything, e.g., “Do you see any-
thing else?”, “Tell me what you see”.
</listItem>
<page confidence="0.865744">
191
</page>
<listItem confidence="0.996920222222222">
• previous-see – the system asks whether the
user has previously seen something, e.g.,
“Have you previously seen a similar object?”.
• describe – the system asks the user to de-
scribe in detail what the user sees, e.g., “De-
scribe it”, “Tell me more about it”.
• compare – the system asks the user to com-
pare what the user sees, e.g., “Compare these
objects”.
• repair-request – the system asks the user to
make clarification, e.g., “I did not understand
that”, “Please repeat that”.
• action-request – the system asks the user to
take action, e.g., “Go back”, “Try moving it”.
User Initiative Responses:
• misc – the system hands the initiative back
to the user without specifying further require-
ments, e.g., “I don’t know”, “Yes”.
</listItem>
<subsectionHeader confidence="0.999114">
5.2 Evaluation of Gaze-Speech Identification
</subsectionHeader>
<bodyText confidence="0.992698333333333">
Given the extracted features and the “closely cou-
pled” label of each instance in the training set, we
train a logistic regression classifier (le Cessie and
van Houwelingen, 1992) to predict whether an in-
stance is a closely coupled gaze-speech instance.
Since the goal of identifying closely coupled
gaze-speech instances is to improve word acqui-
sition and we are only interested in acquiring
nouns and adjectives, only the instances with rec-
ognized nouns/adjectives are used for training the
logistic regression classifier. Among the 2969 in-
stances with recognized nouns/adjectives and gaze
fixations, 2002 (67.4%) instances are labeled as
“closely coupled”. The prediction is evaluated by
a 10-fold cross validation.
</bodyText>
<table confidence="0.9993036">
Feature sets Precision Recall
Null (baseline) 0.674 1
S 0.686 0.995
G 0.707 0.958
UA 0.704 0.942
CC 0.688 0.936
G + UA 0.719 0.948
G+UA+S 0.741 0.908
G+UA+CC 0.731 0.918
G+UA+CC+S 0.748 0.899
</table>
<tableCaption confidence="0.819742">
Table 1: Gaze-speech prediction performance for
the instances with 1-best speech recognition
</tableCaption>
<bodyText confidence="0.994728326086956">
Table 1 shows the prediction precision and re-
call when different sets of features are used. As
seen in the table, as more features are used, the
prediction precision goes up and the recall goes
down. It is important to note that prediction pre-
cision is more critical than recall for word acqui-
sition when sufficient amount data is available.
Noisy instances where the gaze is not coupled with
the speech content will only hurt word acquisi-
tion since they will guide the translation models
to ground words to the wrong entities. Although
higher recall can be helpful, its effect is expected
to be reduced when more data becomes available.
The results show that speech features (S) and
conversation context features (CC), when used
alone, do not improve prediction precision much
compared to the baseline of predicting all in-
stances as closely coupled (with a precision of
67.4%). When used alone, gaze features (G) and
user activity features (UA) are the two most use-
ful feature sets for increasing prediction precision.
When they are used together, the prediction pre-
cision is further increased. Adding either speech
features or conversation context features to gaze
and user activity features (G + UA + S/CC) further
increases the prediction precision. Using all fea-
tures (G + UA + CC + S) achieves the highest pre-
diction precision, which is significantly better than
the baseline: z = 5.93, p &lt; 0.001. Therefore, we
choose to use all feature sets to identify the closely
coupled gaze-speech instances for word acquisi-
tion.
To compare the effects of the automatic gaze-
speech identification on word acquisition from
various speech input (1-best speech recognition,
speech transcript), we also use the logistic re-
gression classifier with all feature sets to iden-
tify the closely coupled gaze-speech instances for
the instances with speech transcript. For the in-
stances with speech transcript, there are 2948 in-
stances with nouns/adjectives and gaze fixations,
2128 (72.2%) of them being labeled as “closely
coupled”. The prediction precision is 77.9% and
the recall is 93.8%. The prediction precision is
significantly better than the baseline of predicting
all instances as coupled: z = 4.92,p &lt; 0.001.
</bodyText>
<sectionHeader confidence="0.973628" genericHeader="evaluation">
6 Evaluation of Word Acquisition
</sectionHeader>
<bodyText confidence="0.999647333333333">
Every conversational system has an initial vocabu-
lary where words are associated with domain con-
cepts of entities. In our evaluation, we assume that
</bodyText>
<page confidence="0.996659">
192
</page>
<bodyText confidence="0.999994615384615">
the system’s vocabulary has one default word for
each entity that indicates the semantic type of the
entity. For example, the word “barrel” is the de-
fault word for the entity barrel. For each entity,
we only evaluate those new words that are not in
the system’s vocabulary.
The acquired words are evaluated against the
“gold standard” words that were manually com-
piled for each entity and its properties based on
all users’ speech transcripts. For the 115 entities
in our domain, each entity has 1 to 20 “gold stan-
dard” words. The average number of “gold stan-
dard” words for an entity is 6.7.
</bodyText>
<subsectionHeader confidence="0.990227">
6.1 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.99995175">
We evaluate the n-best acquired words (words
grounded to domain concepts of entities) using
precision, recall, and F-measure. When a differ-
ent n is chosen, we will have different precision,
recall, and F-measure.
We also evaluate the whole ranked candidate
word list on Mean Reciprocal Rank Rate (MRRR)
as in (Qu and Chai, 2008):
</bodyText>
<equation confidence="0.9390695">
Ee ENeENe17&apos;CL/Z(wie)
#e
</equation>
<bodyText confidence="0.9999544">
where Ne is the number of all “gold standard”
words {we} for entity e, index(we) is the index
of word we in the ranked list of candidate words
for entity e.
MRRR measures how close the ranks of the
“gold standard” words in the candidate word lists
are to the best-case scenario where the top Ne
words are the “gold standard” words for e. The
higher the MRRR, the better is the acquisition per-
formance.
</bodyText>
<subsectionHeader confidence="0.999885">
6.2 Evaluation Results
</subsectionHeader>
<bodyText confidence="0.999964615384615">
We evaluate the effect of the closely coupled gaze-
speech instances on word acquisition from the 1-
best speech recognition and speech transcript. The
predicted closely coupled gaze-speech instances
are generated by a 10-fold cross validation with
the logistic regression classifier.
Figure 3 shows the precision, recall, and F-
measure of the n-best words acquired from 1-best
speech recognition by Model-2t using all instances
(all), predicted coupled instances (predicted), and
true (manually labeled) coupled instances (true).
As shown in the figure, using predicted coupled
instances achieves consistently better performance
</bodyText>
<figure confidence="0.996061571428571">
1 2 3 4 5 6 7 8 9 10
n-best
(a) precision
n-best
(b) recall
n-best
(c) F-measure
</figure>
<figureCaption confidence="0.991786">
Figure 3: Performance of word acquisition on 1-
best speech recognition
</figureCaption>
<bodyText confidence="0.996997769230769">
than using all instances. These results show that
the identification of coupled gaze-speech predic-
tion helps word acquisition. When the true cou-
pled instances are used, the performance is further
improved. This means that reliable identification
of coupled gaze-speech instances can lead to bet-
ter word acquisition.
Figure 4 shows the precision, recall, and F-
measure of the n-best words acquired from speech
transcript by Model-2t using all instances, pre-
dicted coupled instances, and true coupled in-
stances. Consistent with the performance based
on the 1-best speech recognition, we can observe
</bodyText>
<figure confidence="0.999167870967742">
0.45
0.4
0.35
0.3
0.25
0.2
all
predicted
true
Recall 0.35
0.3
0.25
0.2
0.15
0.1
0.05
all
predicted
true
1 2 3 4 5 6 7 8 9 10
F-measure 0.3
0.25
0.2
0.15
all
predicted
true
1 2 3 4 5 6 7 8 9 10
0.1
MRRR =
Precision
</figure>
<page confidence="0.538977">
193
</page>
<figure confidence="0.997434">
n-best
(a) precision
n-best
(b) recall
n-best
(c) F-measure
</figure>
<figureCaption confidence="0.968735">
Figure 4: Performance of word acquisition on
speech transcript
</figureCaption>
<bodyText confidence="0.9984857">
that automatic identification of coupled instances
results in better word acquisition performance and
using the true coupled instances results in even
better performance.
Table 2 presents the MRRRs achieved by
Model-2t when words are acquired from differ-
ent speech input (speech transcript, 1-best recog-
nition) with different set of instances (all in-
stances, predicted coupled instances, true coupled
instances). These results also show the consis-
tent behavior. Using predicted coupled instances
achieves significantly better MRRR than using all
instances no matter the words are acquired from 1-
best speech recognition (t = 2.59,p &lt; 0.006) or
speech transcript(t = 3.15, p &lt; 0.002). When the
true coupled instances are used, the performances
are further improved for both 1-best recognition
(t = 2.29,p &lt; 0.013) and speech transcript
(t = 5.21, p &lt; 0.001) compared to using pre-
dicted coupled instances.
</bodyText>
<table confidence="0.997295">
Instances All Predicted True
Transcript 0.462 0.480 0.526
1-best reco 0.343 0.369 0.390
</table>
<tableCaption confidence="0.999923">
Table 2: MRRRs based on different data set
</tableCaption>
<bodyText confidence="0.999901862068965">
The quality of speech recognition is critical to
word acquisition performance. Comparing word
acquisition based on speech transcript and 1-best
speech recognition, as expected, word acquisition
performance on speech transcript is much better
than on recognized speech. However, the acqui-
sition performance based on speech transcript is
still comparably low. For example, the recall of
acquired words is still below 55% even when the
10 best word candidates are acquired for each en-
tity. This is mainly due to the scarcity of words.
Many words appear less than three times in the
data, which makes them unlikely to be associated
with any entity by the translation model. When
more data is available, we expect to see better ac-
quisition performance.
Note that our current evaluation is based on a
two-stage approach, i.e., first identifying closely-
coupled streams based on supervised classifica-
tion and then automatically establishing mappings
between words and entities in an unsupervised
manner. There could be other approaches to ad-
dress the word acquisition problem (e.g., super-
vised learning to directly identify whether a word
is mapped to an object). Our two-stage approach
has the advantage of requiring minimum super-
vision since the models learned from the first
stage is application-independent and is potentially
portable to different domains.
</bodyText>
<sectionHeader confidence="0.999506" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999984857142857">
Unlike in the typical settings for psycholinguistic
studies, human eye gaze can serve different func-
tions during human machine conversation. Some
gaze and speech streams may not be tightly cou-
pled and thus can be detrimental to word acqui-
sition. Therefore, this paper describes an ap-
proach that incorporates features from the interac-
</bodyText>
<figure confidence="0.999331128205128">
0.55
all
predicted
true
1 2 3 4 5 6 7 8 9 10
0.5
0.45
0.4
0.35
0.3
0.25
Recall 0.55
0.5
0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
all
predicted
true
1 2 3 4 5 6 7 8 9 10
F-measure 0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
all
predicted
true
1 2 3 4 5 6 7 8 9 10
Precision
</figure>
<page confidence="0.995507">
194
</page>
<bodyText confidence="0.9999871">
tion context to identify closely coupled gaze and
speech streams. Our empirical results indicate
that the word acquisition based on these automati-
cally identified gaze-speech streams achieves sig-
nificantly better performance than the word acqui-
sition based on all gaze-speech streams. Our fu-
ture work will combine gaze-based word acquisi-
tion with multiple speech recognition hypotheses
(e.g., word lattices) to further improve word acqui-
sition and language interpretation performance.
</bodyText>
<sectionHeader confidence="0.998935" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998225">
This work was supported by grants IIS-0347548
and IIS-0535112 from the National Science Foun-
dation. We thank anonymous reviewers for their
valuable comments and suggestions.
</bodyText>
<sectionHeader confidence="0.999678" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999869395348838">
G. Aist, J. Dowding, B. A. Hockey, M. Rayner,
J. Hieronymus, D. Bohus, B. Boven, N. Blaylock,
E. Campana, S. Early, G. Gorrell, and S. Phan.
2003. Talking through procedures: An intelligent
space station procedure assistant. In Proceedings of
the 10th Conference of the European Chapter of the
Association for Computational Linguistics (EACL).
S. Bangalore and M. Johnston. 2004. Robust multi-
modal understanding. In Proceedings of the Inter-
national Conference on Acoustics, Speech, and Sig-
nal Processing (ICASSP).
K. Barnard, P. Duygulu, N. de Freitas, D. Forsyth,
D. Blei, and M. Jordan. 2003. Matching words and
pictures. Journal of Machine Learning Research,
3:1107–1135.
D. Byron, T. Mampilly, V. Sharma, and T. Xu. 2005.
Utilizing visual attention for cross-modal corefer-
ence interpretation. In Proceedings of the Fifth
International and Interdisciplinary Conference on
Modeling and Using Context (CONTEXT-05), pages
83–96.
K. Eberhard, M. Spivey-Knowiton, J. Sedivy, and
M. Tanenhaus. 1995. Eye movements as a win-
dow into real-time spoken language comprehension
in natural contexts. Journal of Psycholinguistic Re-
search, 24:409–436.
Z. Griffin and K. Bock. 2000. What the eyes say about
speaking. Psychological Science, 11:274–279.
M. Just and P. Carpenter. 1976. Eye fixations and cog-
nitive processes. Cognitive Psychology, 8:441–480.
D. Kahneman. 1973. Attention and Effort. Prentice-
Hall, Inc., Englewood Cliffs.
S. le Cessie and J. van Houwelingen. 1992. Ridge
estimators in logistic regression. Applied Statistics,
41(1):191–201.
O. Lemon, A. Gruenstein, and S. Peters. 2002. Col-
laborative activities and multitasking in dialogue
systems. Traitement Automatique des Langues,
43(2):131–154.
Y. Liu, J. Chai, and R. Jin. 2007. Automated vocab-
ulary acquisition and interpretation in multimodal
conversational systems. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics (ACL).
A. Meyer, A. Sleiderink, and W. Levelt. 1998. View-
ing and naming objects: eye movements during
noun phrase production. Cognition, 66(22):25–33.
Y. Nakano, G. Reinstein, T. Stocky, and J. Cassell.
2003. Towards a model of face-to-face grounding.
In Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics (ACL).
Z. Prasov and J. Chai. 2008. What’s in a gaze? the role
of eye-gaze in reference resolution in multimodal
conversational interfaces. In Proceedings of ACM
12th International Conference on Intelligent User
interfaces (IUI).
S. Qu and J. Chai. 2006. Salience modeling based
on non-verbal modalities for spoken language un-
derstanding. In Proceedings of the International
Conference on Multimodal Interfaces (ICMI), pages
193–200.
S. Qu and J. Chai. 2008. Incorporating temporal and
semantic information with eye gaze for automatic
word acquisition in multimodal conversational sys-
tems. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 244–253.
K. Rayner. 1998. Eye movements in reading and in-
formation processing - 20 years of research. Psy-
chological Bulletin, 124(3):372–422.
L. Razzaq and N. Heffernan. 2004. Tutorial dialog in
an equation solving intelligent tutoring system. In
Proceedings of the Workshop on Dialog-based In-
telligent Tutoring Systems: State of the art and new
research directions.
D. Roy and A. Pentland. 2002. Learning words from
sights and sounds, a computational model. Cogni-
tive Science, 26(1):113–146.
M. Tanenhaus, M. Spivey-Knowiton, K. Eberhard, and
J. Sedivy. 1995. Integration of visual and linguis-
tic information in spoken language comprehension.
Science, 268:1632–1634.
C. Yu and D. Ballard. 2004. A multimodal learning
interface for grounding spoken language in sensory
perceptions. ACM Transactions on Applied Percep-
tions, 1(1):57–80.
</reference>
<page confidence="0.998932">
195
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.698491">
<title confidence="0.9995175">The Role of Interactivity in Human-Machine Conversation for Word Acquisition</title>
<author confidence="0.999962">Shaolin Qu Joyce Y Chai</author>
<affiliation confidence="0.933412">Department of Computer Science and Michigan State</affiliation>
<address confidence="0.75865">East Lansing, MI</address>
<abstract confidence="0.998473952380952">Motivated by the psycholinguistic finding that human eye gaze is tightly linked to speech production, previous work has applied naturally occurring eye gaze for automatic vocabulary acquisition. However, unlike in the typical settings for psycholinguistic studies, eye gaze can serve different functions in human-machine conversation. Some gaze streams do not link to the content of the spoken utterances and thus can be potentially detrimental to word acquisition. To address this problem, this paper investigates the incorporation of interactivity in identifying the close coupling of speech and gaze streams for word acquisition. Our empirical results indicate that automatic identification of closely coupled gaze-speech streams leads to significantly better word acquisition performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Aist</author>
<author>J Dowding</author>
<author>B A Hockey</author>
<author>M Rayner</author>
<author>J Hieronymus</author>
<author>D Bohus</author>
<author>B Boven</author>
<author>N Blaylock</author>
<author>E Campana</author>
<author>S Early</author>
<author>G Gorrell</author>
<author>S Phan</author>
</authors>
<title>Talking through procedures: An intelligent space station procedure assistant.</title>
<date>2003</date>
<booktitle>In Proceedings of the 10th Conference of the European Chapter of the Association for Computational Linguistics (EACL).</booktitle>
<contexts>
<context position="1257" citStr="Aist et al., 2003" startWordPosition="177" endWordPosition="180"> utterances and thus can be potentially detrimental to word acquisition. To address this problem, this paper investigates the incorporation of interactivity in identifying the close coupling of speech and gaze streams for word acquisition. Our empirical results indicate that automatic identification of closely coupled gaze-speech streams leads to significantly better word acquisition performance. 1 Introduction Spoken conversational interfaces have become increasingly important in many applications such as remote interaction with robots (Lemon et al., 2002), intelligent space station control (Aist et al., 2003), and automated training and education (Razzaq and Heffernan, 2004). As in any conversational system, one major bottleneck in conversational interfaces is robust language interpretation. To address this problem, previous multimodal conversational systems have utilized penbased or deictic gestures (Bangalore and Johnston, 2004; Qu and Chai, 2006) to improve interpretation. Besides gestures, eye movements that naturally occur during interaction provide another important channel for language understanding, for example, reference resolution (Byron et al., 2005; Prasov and Chai, 2008). Recent work </context>
</contexts>
<marker>Aist, Dowding, Hockey, Rayner, Hieronymus, Bohus, Boven, Blaylock, Campana, Early, Gorrell, Phan, 2003</marker>
<rawString>G. Aist, J. Dowding, B. A. Hockey, M. Rayner, J. Hieronymus, D. Bohus, B. Boven, N. Blaylock, E. Campana, S. Early, G. Gorrell, and S. Phan. 2003. Talking through procedures: An intelligent space station procedure assistant. In Proceedings of the 10th Conference of the European Chapter of the Association for Computational Linguistics (EACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>M Johnston</author>
</authors>
<title>Robust multimodal understanding.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP).</booktitle>
<contexts>
<context position="1584" citStr="Bangalore and Johnston, 2004" startWordPosition="225" endWordPosition="229">ed gaze-speech streams leads to significantly better word acquisition performance. 1 Introduction Spoken conversational interfaces have become increasingly important in many applications such as remote interaction with robots (Lemon et al., 2002), intelligent space station control (Aist et al., 2003), and automated training and education (Razzaq and Heffernan, 2004). As in any conversational system, one major bottleneck in conversational interfaces is robust language interpretation. To address this problem, previous multimodal conversational systems have utilized penbased or deictic gestures (Bangalore and Johnston, 2004; Qu and Chai, 2006) to improve interpretation. Besides gestures, eye movements that naturally occur during interaction provide another important channel for language understanding, for example, reference resolution (Byron et al., 2005; Prasov and Chai, 2008). Recent work has also shown that what users look at on the interface (e.g., natural scenes or generated graphic displays) during speech production provides unique opportunities for word acquisition, namely automatically acquiring semantic meanings of spoken words by grounding them to visual entities (Liu et al., 2007) or domain concepts (</context>
</contexts>
<marker>Bangalore, Johnston, 2004</marker>
<rawString>S. Bangalore and M. Johnston. 2004. Robust multimodal understanding. In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Barnard</author>
<author>P Duygulu</author>
<author>N de Freitas</author>
<author>D Forsyth</author>
<author>D Blei</author>
<author>M Jordan</author>
</authors>
<title>Matching words and pictures.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1107</pages>
<marker>Barnard, Duygulu, de Freitas, Forsyth, Blei, Jordan, 2003</marker>
<rawString>K. Barnard, P. Duygulu, N. de Freitas, D. Forsyth, D. Blei, and M. Jordan. 2003. Matching words and pictures. Journal of Machine Learning Research, 3:1107–1135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Byron</author>
<author>T Mampilly</author>
<author>V Sharma</author>
<author>T Xu</author>
</authors>
<title>Utilizing visual attention for cross-modal coreference interpretation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Fifth International and Interdisciplinary Conference on Modeling and Using Context (CONTEXT-05),</booktitle>
<pages>83--96</pages>
<contexts>
<context position="1819" citStr="Byron et al., 2005" startWordPosition="260" endWordPosition="263">), intelligent space station control (Aist et al., 2003), and automated training and education (Razzaq and Heffernan, 2004). As in any conversational system, one major bottleneck in conversational interfaces is robust language interpretation. To address this problem, previous multimodal conversational systems have utilized penbased or deictic gestures (Bangalore and Johnston, 2004; Qu and Chai, 2006) to improve interpretation. Besides gestures, eye movements that naturally occur during interaction provide another important channel for language understanding, for example, reference resolution (Byron et al., 2005; Prasov and Chai, 2008). Recent work has also shown that what users look at on the interface (e.g., natural scenes or generated graphic displays) during speech production provides unique opportunities for word acquisition, namely automatically acquiring semantic meanings of spoken words by grounding them to visual entities (Liu et al., 2007) or domain concepts (Qu and Chai, 2008). Psycholinguistic studies have shown that eye gaze indicates a person’s attention (Just and Carpenter, 1976), and eye movement can facilitate spoken language comprehension (Tanenhaus et al., 1995; Eberhard et al., 19</context>
</contexts>
<marker>Byron, Mampilly, Sharma, Xu, 2005</marker>
<rawString>D. Byron, T. Mampilly, V. Sharma, and T. Xu. 2005. Utilizing visual attention for cross-modal coreference interpretation. In Proceedings of the Fifth International and Interdisciplinary Conference on Modeling and Using Context (CONTEXT-05), pages 83–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Eberhard</author>
<author>M Spivey-Knowiton</author>
<author>J Sedivy</author>
<author>M Tanenhaus</author>
</authors>
<title>Eye movements as a window into real-time spoken language comprehension in natural contexts.</title>
<date>1995</date>
<journal>Journal of Psycholinguistic Research,</journal>
<pages>24--409</pages>
<contexts>
<context position="2422" citStr="Eberhard et al., 1995" startWordPosition="353" endWordPosition="356">(Byron et al., 2005; Prasov and Chai, 2008). Recent work has also shown that what users look at on the interface (e.g., natural scenes or generated graphic displays) during speech production provides unique opportunities for word acquisition, namely automatically acquiring semantic meanings of spoken words by grounding them to visual entities (Liu et al., 2007) or domain concepts (Qu and Chai, 2008). Psycholinguistic studies have shown that eye gaze indicates a person’s attention (Just and Carpenter, 1976), and eye movement can facilitate spoken language comprehension (Tanenhaus et al., 1995; Eberhard et al., 1995). It has been found that users’ eyes move to the mentioned object directly before speaking a word (Meyer et al., 1998; Rayner, 1998; Griffin and Bock, 2000). This parallel behavior of eye gaze and speech production motivates our previous work on word acquisition (Liu et al., 2007; Qu and Chai, 2008). However, in interactive conversation, human gaze behavior is much more complex than in the typical controlled settings used in psycholinguistic studies. There are different types of eye movements (Kahneman, 1973). The naturally occurring eye gaze during speech production may serve different functi</context>
</contexts>
<marker>Eberhard, Spivey-Knowiton, Sedivy, Tanenhaus, 1995</marker>
<rawString>K. Eberhard, M. Spivey-Knowiton, J. Sedivy, and M. Tanenhaus. 1995. Eye movements as a window into real-time spoken language comprehension in natural contexts. Journal of Psycholinguistic Research, 24:409–436.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Griffin</author>
<author>K Bock</author>
</authors>
<title>What the eyes say about speaking.</title>
<date>2000</date>
<journal>Psychological Science,</journal>
<pages>11--274</pages>
<contexts>
<context position="2578" citStr="Griffin and Bock, 2000" startWordPosition="381" endWordPosition="384"> displays) during speech production provides unique opportunities for word acquisition, namely automatically acquiring semantic meanings of spoken words by grounding them to visual entities (Liu et al., 2007) or domain concepts (Qu and Chai, 2008). Psycholinguistic studies have shown that eye gaze indicates a person’s attention (Just and Carpenter, 1976), and eye movement can facilitate spoken language comprehension (Tanenhaus et al., 1995; Eberhard et al., 1995). It has been found that users’ eyes move to the mentioned object directly before speaking a word (Meyer et al., 1998; Rayner, 1998; Griffin and Bock, 2000). This parallel behavior of eye gaze and speech production motivates our previous work on word acquisition (Liu et al., 2007; Qu and Chai, 2008). However, in interactive conversation, human gaze behavior is much more complex than in the typical controlled settings used in psycholinguistic studies. There are different types of eye movements (Kahneman, 1973). The naturally occurring eye gaze during speech production may serve different functions, for example, to engage in the conversation or to manage turn taking (Nakano et al., 2003). Furthermore, while interacting with a graphic display, a use</context>
<context position="10105" citStr="Griffin and Bock, 2000" startWordPosition="1600" endWordPosition="1603"> parallel word and entity sequences {(w, e)}, we want to find the best match between the words and the entities. Following our previous work (Qu and Chai, 2008), we formulate word acquisition as a translation problem and use translation models for word acquisition. For each entity e, we first estimate the word-entity association probability p(w|e) with a translation model, then choose the words with the highest probabilities as acquired words for e. Inspired by the psycholinguistic findings that users’ eyes move to the mentioned object before speaking a word (Meyer et al., 1998; Rayner, 1998; Griffin and Bock, 2000), in our previous work (Qu and Chai, 2008), we have incorporated the gaze-speech temporal information in the translation model as follows (referred as Model-2t through the rest of this paper): pt(aj = i|j, e, w)p(wj|ei) where l and m are the lengths of entity and word sequences respectively. In this equation, pt(aj = i|j, e, w) is the temporal alignment probability representing the probability that wj is aligned with ei, which is further defined by: pt(aj = i|j, e, w) = � 0 d(ei, wj) &gt; 0 exp[α�d(ei,wj)] Ei exp[α-d(ei,wj)] d(ei, wj) ≤ 0 where α is a scaling factor, and d(ei, wj) is the temporal</context>
</contexts>
<marker>Griffin, Bock, 2000</marker>
<rawString>Z. Griffin and K. Bock. 2000. What the eyes say about speaking. Psychological Science, 11:274–279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Just</author>
<author>P Carpenter</author>
</authors>
<title>Eye fixations and cognitive processes.</title>
<date>1976</date>
<journal>Cognitive Psychology,</journal>
<pages>8--441</pages>
<contexts>
<context position="2311" citStr="Just and Carpenter, 1976" startWordPosition="336" endWordPosition="340">uring interaction provide another important channel for language understanding, for example, reference resolution (Byron et al., 2005; Prasov and Chai, 2008). Recent work has also shown that what users look at on the interface (e.g., natural scenes or generated graphic displays) during speech production provides unique opportunities for word acquisition, namely automatically acquiring semantic meanings of spoken words by grounding them to visual entities (Liu et al., 2007) or domain concepts (Qu and Chai, 2008). Psycholinguistic studies have shown that eye gaze indicates a person’s attention (Just and Carpenter, 1976), and eye movement can facilitate spoken language comprehension (Tanenhaus et al., 1995; Eberhard et al., 1995). It has been found that users’ eyes move to the mentioned object directly before speaking a word (Meyer et al., 1998; Rayner, 1998; Griffin and Bock, 2000). This parallel behavior of eye gaze and speech production motivates our previous work on word acquisition (Liu et al., 2007; Qu and Chai, 2008). However, in interactive conversation, human gaze behavior is much more complex than in the typical controlled settings used in psycholinguistic studies. There are different types of eye m</context>
</contexts>
<marker>Just, Carpenter, 1976</marker>
<rawString>M. Just and P. Carpenter. 1976. Eye fixations and cognitive processes. Cognitive Psychology, 8:441–480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Kahneman</author>
</authors>
<title>Attention and Effort.</title>
<date>1973</date>
<publisher>PrenticeHall, Inc.,</publisher>
<location>Englewood Cliffs.</location>
<contexts>
<context position="2936" citStr="Kahneman, 1973" startWordPosition="441" endWordPosition="442">movement can facilitate spoken language comprehension (Tanenhaus et al., 1995; Eberhard et al., 1995). It has been found that users’ eyes move to the mentioned object directly before speaking a word (Meyer et al., 1998; Rayner, 1998; Griffin and Bock, 2000). This parallel behavior of eye gaze and speech production motivates our previous work on word acquisition (Liu et al., 2007; Qu and Chai, 2008). However, in interactive conversation, human gaze behavior is much more complex than in the typical controlled settings used in psycholinguistic studies. There are different types of eye movements (Kahneman, 1973). The naturally occurring eye gaze during speech production may serve different functions, for example, to engage in the conversation or to manage turn taking (Nakano et al., 2003). Furthermore, while interacting with a graphic display, a user could be talking about objects that were previously seen on the display or something completely unrelated to any object the user is looking at. Therefore using every speechgaze pair for word acquisition can be detrimental. The type of gaze that is mostly useful for word acquisition is the kind that reflects the underlying attention and tightly links to t</context>
</contexts>
<marker>Kahneman, 1973</marker>
<rawString>D. Kahneman. 1973. Attention and Effort. PrenticeHall, Inc., Englewood Cliffs.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S le Cessie</author>
<author>J van Houwelingen</author>
</authors>
<title>Ridge estimators in logistic regression.</title>
<date>1992</date>
<journal>Applied Statistics,</journal>
<volume>41</volume>
<issue>1</issue>
<marker>le Cessie, van Houwelingen, 1992</marker>
<rawString>S. le Cessie and J. van Houwelingen. 1992. Ridge estimators in logistic regression. Applied Statistics, 41(1):191–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Lemon</author>
<author>A Gruenstein</author>
<author>S Peters</author>
</authors>
<title>Collaborative activities and multitasking in dialogue systems.</title>
<date>2002</date>
<booktitle>Traitement Automatique des Langues,</booktitle>
<pages>43--2</pages>
<contexts>
<context position="1202" citStr="Lemon et al., 2002" startWordPosition="169" endWordPosition="172">me gaze streams do not link to the content of the spoken utterances and thus can be potentially detrimental to word acquisition. To address this problem, this paper investigates the incorporation of interactivity in identifying the close coupling of speech and gaze streams for word acquisition. Our empirical results indicate that automatic identification of closely coupled gaze-speech streams leads to significantly better word acquisition performance. 1 Introduction Spoken conversational interfaces have become increasingly important in many applications such as remote interaction with robots (Lemon et al., 2002), intelligent space station control (Aist et al., 2003), and automated training and education (Razzaq and Heffernan, 2004). As in any conversational system, one major bottleneck in conversational interfaces is robust language interpretation. To address this problem, previous multimodal conversational systems have utilized penbased or deictic gestures (Bangalore and Johnston, 2004; Qu and Chai, 2006) to improve interpretation. Besides gestures, eye movements that naturally occur during interaction provide another important channel for language understanding, for example, reference resolution (B</context>
</contexts>
<marker>Lemon, Gruenstein, Peters, 2002</marker>
<rawString>O. Lemon, A. Gruenstein, and S. Peters. 2002. Collaborative activities and multitasking in dialogue systems. Traitement Automatique des Langues, 43(2):131–154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Liu</author>
<author>J Chai</author>
<author>R Jin</author>
</authors>
<title>Automated vocabulary acquisition and interpretation in multimodal conversational systems.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="2163" citStr="Liu et al., 2007" startWordPosition="314" endWordPosition="317">gestures (Bangalore and Johnston, 2004; Qu and Chai, 2006) to improve interpretation. Besides gestures, eye movements that naturally occur during interaction provide another important channel for language understanding, for example, reference resolution (Byron et al., 2005; Prasov and Chai, 2008). Recent work has also shown that what users look at on the interface (e.g., natural scenes or generated graphic displays) during speech production provides unique opportunities for word acquisition, namely automatically acquiring semantic meanings of spoken words by grounding them to visual entities (Liu et al., 2007) or domain concepts (Qu and Chai, 2008). Psycholinguistic studies have shown that eye gaze indicates a person’s attention (Just and Carpenter, 1976), and eye movement can facilitate spoken language comprehension (Tanenhaus et al., 1995; Eberhard et al., 1995). It has been found that users’ eyes move to the mentioned object directly before speaking a word (Meyer et al., 1998; Rayner, 1998; Griffin and Bock, 2000). This parallel behavior of eye gaze and speech production motivates our previous work on word acquisition (Liu et al., 2007; Qu and Chai, 2008). However, in interactive conversation, h</context>
<context position="5555" citStr="Liu et al., 2007" startWordPosition="845" endWordPosition="848">us work, in our work, the visual attention foci accompanying speech are indicated by eye gaze. As an implicit and subconscious input, eye gaze brings additional challenges in word acquisition. Eye gaze has been explored for word acquisition in previous work. In (Yu and Ballard, 2004), given speech paired with eye gaze and video images, a translation model was used to acquire words by associating acoustic phone sequences with visual representations of objects and actions. Word acquisition from transcribed speech and eye gaze during human-machine conversation has been investigated recently. In (Liu et al., 2007), a translation model was developed to associate words with visual objects on a graphical display. In our previous work (Qu and Chai, 2008), enhanced translation models incorporating speech-gaze temporal information and domain knowledge were developed to improve word acquisition. However, none of these previous works has investigated the role of interactivity in word acquisition, which is the focus of this paper. 3 Data Collection We collected speech and eye gaze data through user studies. This data set is different from the data set used in our previous work (Qu and Chai, 2008). The differenc</context>
</contexts>
<marker>Liu, Chai, Jin, 2007</marker>
<rawString>Y. Liu, J. Chai, and R. Jin. 2007. Automated vocabulary acquisition and interpretation in multimodal conversational systems. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Meyer</author>
<author>A Sleiderink</author>
<author>W Levelt</author>
</authors>
<title>Viewing and naming objects: eye movements during noun phrase production.</title>
<date>1998</date>
<journal>Cognition,</journal>
<volume>66</volume>
<issue>22</issue>
<contexts>
<context position="2539" citStr="Meyer et al., 1998" startWordPosition="375" endWordPosition="378">atural scenes or generated graphic displays) during speech production provides unique opportunities for word acquisition, namely automatically acquiring semantic meanings of spoken words by grounding them to visual entities (Liu et al., 2007) or domain concepts (Qu and Chai, 2008). Psycholinguistic studies have shown that eye gaze indicates a person’s attention (Just and Carpenter, 1976), and eye movement can facilitate spoken language comprehension (Tanenhaus et al., 1995; Eberhard et al., 1995). It has been found that users’ eyes move to the mentioned object directly before speaking a word (Meyer et al., 1998; Rayner, 1998; Griffin and Bock, 2000). This parallel behavior of eye gaze and speech production motivates our previous work on word acquisition (Liu et al., 2007; Qu and Chai, 2008). However, in interactive conversation, human gaze behavior is much more complex than in the typical controlled settings used in psycholinguistic studies. There are different types of eye movements (Kahneman, 1973). The naturally occurring eye gaze during speech production may serve different functions, for example, to engage in the conversation or to manage turn taking (Nakano et al., 2003). Furthermore, while in</context>
<context position="10066" citStr="Meyer et al., 1998" startWordPosition="1594" endWordPosition="1597"> entities. Specifically, given the parallel word and entity sequences {(w, e)}, we want to find the best match between the words and the entities. Following our previous work (Qu and Chai, 2008), we formulate word acquisition as a translation problem and use translation models for word acquisition. For each entity e, we first estimate the word-entity association probability p(w|e) with a translation model, then choose the words with the highest probabilities as acquired words for e. Inspired by the psycholinguistic findings that users’ eyes move to the mentioned object before speaking a word (Meyer et al., 1998; Rayner, 1998; Griffin and Bock, 2000), in our previous work (Qu and Chai, 2008), we have incorporated the gaze-speech temporal information in the translation model as follows (referred as Model-2t through the rest of this paper): pt(aj = i|j, e, w)p(wj|ei) where l and m are the lengths of entity and word sequences respectively. In this equation, pt(aj = i|j, e, w) is the temporal alignment probability representing the probability that wj is aligned with ei, which is further defined by: pt(aj = i|j, e, w) = � 0 d(ei, wj) &gt; 0 exp[α�d(ei,wj)] Ei exp[α-d(ei,wj)] d(ei, wj) ≤ 0 where α is a scalin</context>
</contexts>
<marker>Meyer, Sleiderink, Levelt, 1998</marker>
<rawString>A. Meyer, A. Sleiderink, and W. Levelt. 1998. Viewing and naming objects: eye movements during noun phrase production. Cognition, 66(22):25–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Nakano</author>
<author>G Reinstein</author>
<author>T Stocky</author>
<author>J Cassell</author>
</authors>
<title>Towards a model of face-to-face grounding.</title>
<date>2003</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="3116" citStr="Nakano et al., 2003" startWordPosition="468" endWordPosition="471"> before speaking a word (Meyer et al., 1998; Rayner, 1998; Griffin and Bock, 2000). This parallel behavior of eye gaze and speech production motivates our previous work on word acquisition (Liu et al., 2007; Qu and Chai, 2008). However, in interactive conversation, human gaze behavior is much more complex than in the typical controlled settings used in psycholinguistic studies. There are different types of eye movements (Kahneman, 1973). The naturally occurring eye gaze during speech production may serve different functions, for example, to engage in the conversation or to manage turn taking (Nakano et al., 2003). Furthermore, while interacting with a graphic display, a user could be talking about objects that were previously seen on the display or something completely unrelated to any object the user is looking at. Therefore using every speechgaze pair for word acquisition can be detrimental. The type of gaze that is mostly useful for word acquisition is the kind that reflects the underlying attention and tightly links to the content of the cooccurring speech. Thus, one important question is how to identify the closely coupled speech and gaze streams to improve word acquisition. To address this quest</context>
</contexts>
<marker>Nakano, Reinstein, Stocky, Cassell, 2003</marker>
<rawString>Y. Nakano, G. Reinstein, T. Stocky, and J. Cassell. 2003. Towards a model of face-to-face grounding. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Prasov</author>
<author>J Chai</author>
</authors>
<title>What’s in a gaze? the role of eye-gaze in reference resolution in multimodal conversational interfaces.</title>
<date>2008</date>
<booktitle>In Proceedings of ACM 12th International Conference on Intelligent User interfaces (IUI).</booktitle>
<contexts>
<context position="1843" citStr="Prasov and Chai, 2008" startWordPosition="264" endWordPosition="267"> station control (Aist et al., 2003), and automated training and education (Razzaq and Heffernan, 2004). As in any conversational system, one major bottleneck in conversational interfaces is robust language interpretation. To address this problem, previous multimodal conversational systems have utilized penbased or deictic gestures (Bangalore and Johnston, 2004; Qu and Chai, 2006) to improve interpretation. Besides gestures, eye movements that naturally occur during interaction provide another important channel for language understanding, for example, reference resolution (Byron et al., 2005; Prasov and Chai, 2008). Recent work has also shown that what users look at on the interface (e.g., natural scenes or generated graphic displays) during speech production provides unique opportunities for word acquisition, namely automatically acquiring semantic meanings of spoken words by grounding them to visual entities (Liu et al., 2007) or domain concepts (Qu and Chai, 2008). Psycholinguistic studies have shown that eye gaze indicates a person’s attention (Just and Carpenter, 1976), and eye movement can facilitate spoken language comprehension (Tanenhaus et al., 1995; Eberhard et al., 1995). It has been found t</context>
</contexts>
<marker>Prasov, Chai, 2008</marker>
<rawString>Z. Prasov and J. Chai. 2008. What’s in a gaze? the role of eye-gaze in reference resolution in multimodal conversational interfaces. In Proceedings of ACM 12th International Conference on Intelligent User interfaces (IUI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Qu</author>
<author>J Chai</author>
</authors>
<title>Salience modeling based on non-verbal modalities for spoken language understanding.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Multimodal Interfaces (ICMI),</booktitle>
<pages>193--200</pages>
<contexts>
<context position="1604" citStr="Qu and Chai, 2006" startWordPosition="230" endWordPosition="233">o significantly better word acquisition performance. 1 Introduction Spoken conversational interfaces have become increasingly important in many applications such as remote interaction with robots (Lemon et al., 2002), intelligent space station control (Aist et al., 2003), and automated training and education (Razzaq and Heffernan, 2004). As in any conversational system, one major bottleneck in conversational interfaces is robust language interpretation. To address this problem, previous multimodal conversational systems have utilized penbased or deictic gestures (Bangalore and Johnston, 2004; Qu and Chai, 2006) to improve interpretation. Besides gestures, eye movements that naturally occur during interaction provide another important channel for language understanding, for example, reference resolution (Byron et al., 2005; Prasov and Chai, 2008). Recent work has also shown that what users look at on the interface (e.g., natural scenes or generated graphic displays) during speech production provides unique opportunities for word acquisition, namely automatically acquiring semantic meanings of spoken words by grounding them to visual entities (Liu et al., 2007) or domain concepts (Qu and Chai, 2008). </context>
</contexts>
<marker>Qu, Chai, 2006</marker>
<rawString>S. Qu and J. Chai. 2006. Salience modeling based on non-verbal modalities for spoken language understanding. In Proceedings of the International Conference on Multimodal Interfaces (ICMI), pages 193–200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Qu</author>
<author>J Chai</author>
</authors>
<title>Incorporating temporal and semantic information with eye gaze for automatic word acquisition in multimodal conversational systems.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>244--253</pages>
<contexts>
<context position="2202" citStr="Qu and Chai, 2008" startWordPosition="321" endWordPosition="324">; Qu and Chai, 2006) to improve interpretation. Besides gestures, eye movements that naturally occur during interaction provide another important channel for language understanding, for example, reference resolution (Byron et al., 2005; Prasov and Chai, 2008). Recent work has also shown that what users look at on the interface (e.g., natural scenes or generated graphic displays) during speech production provides unique opportunities for word acquisition, namely automatically acquiring semantic meanings of spoken words by grounding them to visual entities (Liu et al., 2007) or domain concepts (Qu and Chai, 2008). Psycholinguistic studies have shown that eye gaze indicates a person’s attention (Just and Carpenter, 1976), and eye movement can facilitate spoken language comprehension (Tanenhaus et al., 1995; Eberhard et al., 1995). It has been found that users’ eyes move to the mentioned object directly before speaking a word (Meyer et al., 1998; Rayner, 1998; Griffin and Bock, 2000). This parallel behavior of eye gaze and speech production motivates our previous work on word acquisition (Liu et al., 2007; Qu and Chai, 2008). However, in interactive conversation, human gaze behavior is much more complex</context>
<context position="5694" citStr="Qu and Chai, 2008" startWordPosition="869" endWordPosition="872">gaze brings additional challenges in word acquisition. Eye gaze has been explored for word acquisition in previous work. In (Yu and Ballard, 2004), given speech paired with eye gaze and video images, a translation model was used to acquire words by associating acoustic phone sequences with visual representations of objects and actions. Word acquisition from transcribed speech and eye gaze during human-machine conversation has been investigated recently. In (Liu et al., 2007), a translation model was developed to associate words with visual objects on a graphical display. In our previous work (Qu and Chai, 2008), enhanced translation models incorporating speech-gaze temporal information and domain knowledge were developed to improve word acquisition. However, none of these previous works has investigated the role of interactivity in word acquisition, which is the focus of this paper. 3 Data Collection We collected speech and eye gaze data through user studies. This data set is different from the data set used in our previous work (Qu and Chai, 2008). The difference lies in two aspects: 1) the data for this investigation was collected during mixed initiative human-machine conversation whereas the data</context>
<context position="9642" citStr="Qu and Chai, 2008" startWordPosition="1527" endWordPosition="1530"> gaze fixated entity sequence {(w, e)} for the task of word acquisition. In section 6, we will evaluate word acquisition in two settings: 1) word sequence w contains all of the nouns/adjectives in the speech transcript, and 2) w contains all of the recognized nouns/adjectives in the 1-best speech recognition. 4 Word Acquisition With Eye Gaze The task of word acquisition in our application is to ground words to the visual entities. Specifically, given the parallel word and entity sequences {(w, e)}, we want to find the best match between the words and the entities. Following our previous work (Qu and Chai, 2008), we formulate word acquisition as a translation problem and use translation models for word acquisition. For each entity e, we first estimate the word-entity association probability p(w|e) with a translation model, then choose the words with the highest probabilities as acquired words for e. Inspired by the psycholinguistic findings that users’ eyes move to the mentioned object before speaking a word (Meyer et al., 1998; Rayner, 1998; Griffin and Bock, 2000), in our previous work (Qu and Chai, 2008), we have incorporated the gaze-speech temporal information in the translation model as follows</context>
<context position="11124" citStr="Qu and Chai, 2008" startWordPosition="1787" endWordPosition="1790"> wj is aligned with ei, which is further defined by: pt(aj = i|j, e, w) = � 0 d(ei, wj) &gt; 0 exp[α�d(ei,wj)] Ei exp[α-d(ei,wj)] d(ei, wj) ≤ 0 where α is a scaling factor, and d(ei, wj) is the temporal distance between ei and wj. Based on the psycholinguistic finding that eye gaze happens before a spoken word, wj is not allowed to be aligned with ei when wj happens earlier than ei (i.e., d(ei, wj) &gt; 0). When wj happens no earlier than ei (i.e., d(ei, wj) ≤ 0), the closer they are, the more likely they are aligned. An EM algorithm is used to estimate p(w|e) and α in the model. Our evaluation in (Qu and Chai, 2008) has shown that Model-2t that incorporates temporal alignment between speech and eye gaze achieves significantly better word acquisition performance compared to the model where no temporal alignment is introduced. Therefore, this model is used for the investigation in this paper. 5 Identification of Closely Coupled Gaze-Speech Pairs Successful word acquisition with the translation models relies on the tight coupling between the gaze fixations and the speech content. As mentioned earlier, not all gaze-speech pairs have this tight coupling. In a gaze-speech pair, if the speech l m H j=1 p(w|e) =</context>
<context position="20691" citStr="Qu and Chai, 2008" startWordPosition="3373" endWordPosition="3376">tandard” words that were manually compiled for each entity and its properties based on all users’ speech transcripts. For the 115 entities in our domain, each entity has 1 to 20 “gold standard” words. The average number of “gold standard” words for an entity is 6.7. 6.1 Evaluation Metrics We evaluate the n-best acquired words (words grounded to domain concepts of entities) using precision, recall, and F-measure. When a different n is chosen, we will have different precision, recall, and F-measure. We also evaluate the whole ranked candidate word list on Mean Reciprocal Rank Rate (MRRR) as in (Qu and Chai, 2008): Ee ENeENe17&apos;CL/Z(wie) #e where Ne is the number of all “gold standard” words {we} for entity e, index(we) is the index of word we in the ranked list of candidate words for entity e. MRRR measures how close the ranks of the “gold standard” words in the candidate word lists are to the best-case scenario where the top Ne words are the “gold standard” words for e. The higher the MRRR, the better is the acquisition performance. 6.2 Evaluation Results We evaluate the effect of the closely coupled gazespeech instances on word acquisition from the 1- best speech recognition and speech transcript. Th</context>
</contexts>
<marker>Qu, Chai, 2008</marker>
<rawString>S. Qu and J. Chai. 2008. Incorporating temporal and semantic information with eye gaze for automatic word acquisition in multimodal conversational systems. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 244–253.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Rayner</author>
</authors>
<title>Eye movements in reading and information processing - 20 years of research.</title>
<date>1998</date>
<journal>Psychological Bulletin,</journal>
<volume>124</volume>
<issue>3</issue>
<contexts>
<context position="2553" citStr="Rayner, 1998" startWordPosition="379" endWordPosition="380">erated graphic displays) during speech production provides unique opportunities for word acquisition, namely automatically acquiring semantic meanings of spoken words by grounding them to visual entities (Liu et al., 2007) or domain concepts (Qu and Chai, 2008). Psycholinguistic studies have shown that eye gaze indicates a person’s attention (Just and Carpenter, 1976), and eye movement can facilitate spoken language comprehension (Tanenhaus et al., 1995; Eberhard et al., 1995). It has been found that users’ eyes move to the mentioned object directly before speaking a word (Meyer et al., 1998; Rayner, 1998; Griffin and Bock, 2000). This parallel behavior of eye gaze and speech production motivates our previous work on word acquisition (Liu et al., 2007; Qu and Chai, 2008). However, in interactive conversation, human gaze behavior is much more complex than in the typical controlled settings used in psycholinguistic studies. There are different types of eye movements (Kahneman, 1973). The naturally occurring eye gaze during speech production may serve different functions, for example, to engage in the conversation or to manage turn taking (Nakano et al., 2003). Furthermore, while interacting with</context>
<context position="10080" citStr="Rayner, 1998" startWordPosition="1598" endWordPosition="1599">lly, given the parallel word and entity sequences {(w, e)}, we want to find the best match between the words and the entities. Following our previous work (Qu and Chai, 2008), we formulate word acquisition as a translation problem and use translation models for word acquisition. For each entity e, we first estimate the word-entity association probability p(w|e) with a translation model, then choose the words with the highest probabilities as acquired words for e. Inspired by the psycholinguistic findings that users’ eyes move to the mentioned object before speaking a word (Meyer et al., 1998; Rayner, 1998; Griffin and Bock, 2000), in our previous work (Qu and Chai, 2008), we have incorporated the gaze-speech temporal information in the translation model as follows (referred as Model-2t through the rest of this paper): pt(aj = i|j, e, w)p(wj|ei) where l and m are the lengths of entity and word sequences respectively. In this equation, pt(aj = i|j, e, w) is the temporal alignment probability representing the probability that wj is aligned with ei, which is further defined by: pt(aj = i|j, e, w) = � 0 d(ei, wj) &gt; 0 exp[α�d(ei,wj)] Ei exp[α-d(ei,wj)] d(ei, wj) ≤ 0 where α is a scaling factor, and </context>
</contexts>
<marker>Rayner, 1998</marker>
<rawString>K. Rayner. 1998. Eye movements in reading and information processing - 20 years of research. Psychological Bulletin, 124(3):372–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Razzaq</author>
<author>N Heffernan</author>
</authors>
<title>Tutorial dialog in an equation solving intelligent tutoring system.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on Dialog-based Intelligent Tutoring Systems: State of</booktitle>
<contexts>
<context position="1324" citStr="Razzaq and Heffernan, 2004" startWordPosition="187" endWordPosition="190">d acquisition. To address this problem, this paper investigates the incorporation of interactivity in identifying the close coupling of speech and gaze streams for word acquisition. Our empirical results indicate that automatic identification of closely coupled gaze-speech streams leads to significantly better word acquisition performance. 1 Introduction Spoken conversational interfaces have become increasingly important in many applications such as remote interaction with robots (Lemon et al., 2002), intelligent space station control (Aist et al., 2003), and automated training and education (Razzaq and Heffernan, 2004). As in any conversational system, one major bottleneck in conversational interfaces is robust language interpretation. To address this problem, previous multimodal conversational systems have utilized penbased or deictic gestures (Bangalore and Johnston, 2004; Qu and Chai, 2006) to improve interpretation. Besides gestures, eye movements that naturally occur during interaction provide another important channel for language understanding, for example, reference resolution (Byron et al., 2005; Prasov and Chai, 2008). Recent work has also shown that what users look at on the interface (e.g., natu</context>
</contexts>
<marker>Razzaq, Heffernan, 2004</marker>
<rawString>L. Razzaq and N. Heffernan. 2004. Tutorial dialog in an equation solving intelligent tutoring system. In Proceedings of the Workshop on Dialog-based Intelligent Tutoring Systems: State of the art and new research directions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roy</author>
<author>A Pentland</author>
</authors>
<title>Learning words from sights and sounds, a computational model.</title>
<date>2002</date>
<journal>Cognitive Science,</journal>
<volume>26</volume>
<issue>1</issue>
<contexts>
<context position="4514" citStr="Roy and Pentland, 2002" startWordPosition="683" endWordPosition="686">Dialogue, pages 188–195, Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics 188 user activity, conversation context) with eye gaze to identify closely coupled speech and gaze streams. We further use the identified speech and gaze streams to acquire words with a translation model. Our empirical evaluation demonstrates that automatic identification of closely coupled gaze-speech streams can lead to significantly better word acquisition performance. 2 Related Work Previous work has explored word acquisition by grounding words to visual entities. In (Roy and Pentland, 2002), given speech paired with video images of objects, mutual information between auditory and visual signals was used to acquire words by associating acoustic phone sequences with the visual prototypes (e.g., color, size, shape) of objects. Given parallel pictures and description texts, generative models were used to acquire words by associating words with image regions in (Barnard et al., 2003). Different from this previous work, in our work, the visual attention foci accompanying speech are indicated by eye gaze. As an implicit and subconscious input, eye gaze brings additional challenges in w</context>
</contexts>
<marker>Roy, Pentland, 2002</marker>
<rawString>D. Roy and A. Pentland. 2002. Learning words from sights and sounds, a computational model. Cognitive Science, 26(1):113–146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tanenhaus</author>
<author>M Spivey-Knowiton</author>
<author>K Eberhard</author>
<author>J Sedivy</author>
</authors>
<title>Integration of visual and linguistic information in spoken language comprehension.</title>
<date>1995</date>
<journal>Science,</journal>
<pages>268--1632</pages>
<contexts>
<context position="2398" citStr="Tanenhaus et al., 1995" startWordPosition="349" endWordPosition="352">e, reference resolution (Byron et al., 2005; Prasov and Chai, 2008). Recent work has also shown that what users look at on the interface (e.g., natural scenes or generated graphic displays) during speech production provides unique opportunities for word acquisition, namely automatically acquiring semantic meanings of spoken words by grounding them to visual entities (Liu et al., 2007) or domain concepts (Qu and Chai, 2008). Psycholinguistic studies have shown that eye gaze indicates a person’s attention (Just and Carpenter, 1976), and eye movement can facilitate spoken language comprehension (Tanenhaus et al., 1995; Eberhard et al., 1995). It has been found that users’ eyes move to the mentioned object directly before speaking a word (Meyer et al., 1998; Rayner, 1998; Griffin and Bock, 2000). This parallel behavior of eye gaze and speech production motivates our previous work on word acquisition (Liu et al., 2007; Qu and Chai, 2008). However, in interactive conversation, human gaze behavior is much more complex than in the typical controlled settings used in psycholinguistic studies. There are different types of eye movements (Kahneman, 1973). The naturally occurring eye gaze during speech production ma</context>
</contexts>
<marker>Tanenhaus, Spivey-Knowiton, Eberhard, Sedivy, 1995</marker>
<rawString>M. Tanenhaus, M. Spivey-Knowiton, K. Eberhard, and J. Sedivy. 1995. Integration of visual and linguistic information in spoken language comprehension. Science, 268:1632–1634.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Yu</author>
<author>D Ballard</author>
</authors>
<title>A multimodal learning interface for grounding spoken language in sensory perceptions.</title>
<date>2004</date>
<journal>ACM Transactions on Applied Perceptions,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="5222" citStr="Yu and Ballard, 2004" startWordPosition="794" endWordPosition="797"> and visual signals was used to acquire words by associating acoustic phone sequences with the visual prototypes (e.g., color, size, shape) of objects. Given parallel pictures and description texts, generative models were used to acquire words by associating words with image regions in (Barnard et al., 2003). Different from this previous work, in our work, the visual attention foci accompanying speech are indicated by eye gaze. As an implicit and subconscious input, eye gaze brings additional challenges in word acquisition. Eye gaze has been explored for word acquisition in previous work. In (Yu and Ballard, 2004), given speech paired with eye gaze and video images, a translation model was used to acquire words by associating acoustic phone sequences with visual representations of objects and actions. Word acquisition from transcribed speech and eye gaze during human-machine conversation has been investigated recently. In (Liu et al., 2007), a translation model was developed to associate words with visual objects on a graphical display. In our previous work (Qu and Chai, 2008), enhanced translation models incorporating speech-gaze temporal information and domain knowledge were developed to improve word</context>
</contexts>
<marker>Yu, Ballard, 2004</marker>
<rawString>C. Yu and D. Ballard. 2004. A multimodal learning interface for grounding spoken language in sensory perceptions. ACM Transactions on Applied Perceptions, 1(1):57–80.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>