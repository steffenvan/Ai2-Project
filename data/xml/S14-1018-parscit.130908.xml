<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.996393">
Cognitive Compositional Semantics using Continuation Dependencies
</title>
<author confidence="0.999368">
William Schuler Adam Wheeler
</author>
<affiliation confidence="0.993878">
Department of Linguistics Department of Linguistics
The Ohio State University The Ohio State University
</affiliation>
<email confidence="0.999258">
schuler@ling.osu.edu wheeler@ling.osu.edu
</email>
<sectionHeader confidence="0.993905" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999992066666667">
This paper describes a graphical semantic
representation based on bottom-up ‘con-
tinuation’ dependencies which has the im-
portant property that its vertices define a
usable set of discourse referents in work-
ing memory even in contexts involving
conjunction in the scope of quantifiers. An
evaluation on an existing quantifier scope
disambiguation task shows that non-local
continuation dependencies can be as reli-
ably learned from annotated data as repre-
sentations used in a state-of-the-art quanti-
fier scope resolver, suggesting that contin-
uation dependencies may provide a natural
representation for scope information.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999982650793651">
It is now fairly well established that at least shal-
low semantic interpretation informs parsing deci-
sions in human sentence processing (Tanenhaus et
al., 1995; Brown-Schmidt et al., 2002), and re-
cent evidence points to incremental processing of
quantifier implicatures as well (Degen and Tanen-
haus, 2011). This may indicate that inferences
about the meaning of quantifiers are processed di-
rectly in working memory. Human working mem-
ory is widely assumed to store events (includ-
ing linguistic events) as re-usable activation-based
states, connected by a durable but rapidly mutable
weight-based memory of cued associations (Marr,
1971; Anderson et al., 1977; Murdock, 1982; Mc-
Clelland et al., 1995; Howard and Kahana, 2002).
Complex dependency structures can therefore be
stored in this associative memory as graphs, with
states as vertices and cued associations as directed
edges (e.g. Kintsch, 1988). This kind of represen-
tation is necessary to formulate and evaluate algo-
rithmic claims (Marr, 1982) about cued associa-
tions and working memory use in human sentence
processing (e.g. van Schijndel and Schuler, 2013).
But accounting for syntax and semantics in this
way must be done carefully in order to preserve
linguistically important distinctions. For example,
positing spurious local dependencies in filler-gap
constructions can lead to missed integrations of
dependency structure in incremental processing,
resulting in weaker model fitting (van Schijndel et
al., 2013). Similar care may be necessary in cases
of dependencies arising from anaphoric corefer-
ence or quantifier scope.
Unfortunately, most existing theories of compo-
sitional semantics (Montague, 1973; Barwise and
Cooper, 1981; Bos, 1996; Baldridge and Kruijff,
2002; Koller, 2004; Copestake et al., 2005) are
defined at the computational level (Marr, 1982),
employing beta reduction over complete or under-
specified lambda calculus expressions as a precise
description of the language processing task to be
modeled, not at the algorithmic level, as a model
of human language processing itself. The struc-
tured expressions these theories generate are not
intended to represent re-usable referential states
of the sort that could be modeled in current theo-
ries of associative memory. As such, it should not
be surprising that structural adaptations of lambda
calculus expressions as referential states exhibit a
number of apparent deficiencies:
First, representations based on lambda calculus
expressions lack topologically distinguishable ref-
erents for sets defined in the context of outscop-
ing quantifiers. For example, a structural adapta-
tion of a lambda calculus expression for the sen-
tence Every line contains two numbers, shown in
Figure 1a (adapted from Koller, 2004), contains
referents for the set of all document lines (sL) and
for the set of all numbers (sN) which can be iden-
tified by cued associations to predicate constants
like Number, but it is not clear how a referent for
the set of numbers in document lines can be dis-
tinguished from a referent for the set of numbers
</bodyText>
<page confidence="0.976794">
141
</page>
<note confidence="0.979002">
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 141–150,
Dublin, Ireland, August 23-24 2014.
</note>
<figure confidence="0.995436075630252">
a) pL
b) pL
Every
01
sL
2
s0
L
0
Every sL
1
2
s0
L
1
0
0
2
0
1
2
1
2
0
1
2
A
pN
1
A
2
s0
N
pS
2
0
1
2
eN
A d0N
eC
dL
0
Line
A d0L
01
Two
0
A
dL
1
Line
0
A sS
A d0L
1
2
s0
S
eL
1
sN
1
dN
eL
0
1
pA
1
0
2
And
pN
1
01
2
Two
sN
s0
N
1
0
20
0
1
2
0
1
2
0
1
2
0
1
2
A
dS
eS
eN
1
0
0
Number
Contain
A d0S
A dN
2 1
0
A d0N
20
eB
eC
Space
Space BeginsWith Number Contain
BeginsWith
Number
Contain
c) (Every pL sL s0L) ∧ (Set sL dL eL) ∧ (Line eL dL) ∧ (Set s0L d0L pN) ∧
(Two pN sN s0N) ∧ (Set sN dN eN) ∧ (Number eN dN) ∧ (Set s0N d0N eC) ∧ (Contain eC d0L d0 N)
</figure>
<figureCaption confidence="0.999467">
Figure 1: Semantic dependency graph in a ‘direct’ (top-down) style, adapted from a disambiguated rep-
</figureCaption>
<bodyText confidence="0.954123327868853">
resentation of Koller (2004), excluding quantifiers over eventualities. The semantic dependency structure
for the sentence Every line contains two numbers (a), with flat logical form (c), is not a subgraph of the
semantic dependency structure for Every line begins with a space and contains two numbers (b), because
the structure is interrupted by the explicit conjunction predicate ‘And’.
in each document line (s0N) using local topological
features of the dependency graph, as would be re-
quired to accurately recall assertions about total or
average quantities of numbers in document lines.1
Second, graphs based on traditional lambda
calculus representations do not model conjuncts
as subgraphs of conjunctions. For example, the
graphical representation of the sentence Every line
1This graph matching can be implemented in a vectorial
model of associative memory by comparing the (e.g. cosine)
similarity of superposed vectors resulting from cueing in-
coming and outgoing dependencies with all possible labels
in increasingly longer paths from one or more constant vec-
tor states (e.g. vectors for predicate constants). This graph
matching does not necessarily preclude the introduction of
monotonicity constraints from matched quantifiers. For ex-
ample, More than two perl scripts work, can entail More
than two scripts work, using a subgraph in the first argu-
ment, but Fewer than two scripts work, can entail Fewer than
two perl scripts work, using a supergraph in the first argu-
ment. This consideration is similar to those observed in rep-
resentations based on natural logic (MacCartney and Man-
ning, 2009) which also uses low-level matching to perform
some kinds of inference, but representations based on natural
logic typically exclude other forms of inference, whereas the
present model does not.
This matching also assumes properties of nuclear scope
variables are inherited from associated restrictor variables,
e.g. through a set of dependencies from nuclear scope sets
to restrictor sets not shown in the figure. This assumption
will be revisited in Section 3.
begins with a space and contains two numbers
shown in Figure 1b does not contain the graphical
representation of the sentence Every line contains
two numbers shown in Figure 1a as a connected
subgraph. Although one might expect a query
about a conjunct to be directly answerable from
a knowledge base containing the conjoined repre-
sentation, the pattern of dependencies that make
up the conjunct in a graphical representation of a
lambda calculus expression does not match those
in the larger conjunction.
Finally, representations based on lambda calcu-
lus expressions contain vertices that do not seem
to correspond to viable discourse referents. For
example, following the sentence Every line con-
tains two numbers, using the lambda expression
shown in Figure 1b, dL may serve as a referent of
it in but it has only one underscore, sN may serve
as a referent of they in but they are not negative,
eC may serve as a referent of that in but that was
before it was edited, and pL may serve as a ref-
erent of that in but the compiler doesn’t enforce
that, but it is not clear what if anything would nat-
urally refer to the internal conjunction pA. Predi-
cations over such conjunctions (e.g. Kim believes
that every line begins with a space and contains
</bodyText>
<page confidence="0.997056">
142
</page>
<bodyText confidence="0.99998034375">
two numbers) are usually predicated at the outer
proposition pL, and in any case do not have truth
values that are independent of the same predica-
tion at each conjunct. One of the goals of Minimal
Recursion Semantics (Copestake et al., 2005) was
to eliminate similar kinds of superfluous conjunc-
tion structure.
Fortunately, lambda calculus expressions like
those shown in Figure 1 are not the only way to
represent compositional semantics of sentences.
This paper defines a graphical semantic depen-
dency representation that can be translated into
lambda calculus, but has the important property
that its vertices define a usable set of discourse
referents in working memory even in contexts in-
volving conjunction in the scope of quantifiers.
It does this by reversing the direction of de-
pendencies from parent-to-child subsumption in
a lambda-calculus tree to a representation sim-
ilar to the inside-out structure of function def-
initions in a continuation-passing style (Barker,
2002; Shan and Barker, 2006)2 so that sets are de-
fined in terms of their context, and explicit ‘And’
predicates are no longer required, leaving noth-
ing to get in the way of an exact pattern match.3
The learnability of the non-local continuation de-
pendencies involved in this representation is then
evaluated on an existing quantifier scope disam-
biguation task using a dependency-based statisti-
cal scope resolver, with results comparable to a
state-of-the-art unrestricted graph-based quantifier
scope resolver (Manshadi et al., 2013).
</bodyText>
<sectionHeader confidence="0.952024" genericHeader="introduction">
2 Continuation Dependencies
</sectionHeader>
<bodyText confidence="0.881121873015873">
This paper explores the use of a bottom-up depen-
dency representation, inspired by the inside-out
structure of function definitions in a continuation-
passing style (Barker, 2002; Shan and Barker,
2006), which creates discourse referents for sets
that are associated with particular scoping con-
texts. This dependency representation preserves
the propositions, sets, eventualities, and ordinary
2This representation also has much in common with gen-
eralized Skolem terms of Steedman (2012), which also repre-
sent dependencies to outscoped terms, but here continuation
dependencies are applied to all quantifiers, including univer-
sals.
3This also holds for explicit disjunction predicates, which
can be cast as conjunction through application of de Morgan’s
law and manipulation of the polarity of adjacent quantifiers.
For example, Every line begins with at least one space or
contains at least two numbers, is equivalent to No line be-
gins with fewer than one space and contains fewer than two
numbers.
discourse referents of a ‘direct’ representation (the
p, s, e, and d nodes in Figure 1), but replaces the
downward dependencies departing set referents
with upward dependencies to context sets (high-
lighted in Figure 2).
Figures 1c and 2c also show flat logical forms
composed of elementary predications, adapted
from Kruijff (2001) and Copestake et al. (2005),
for the sentence Every line contains two numbers,
which are formed by identifying the function as-
sociated with the predicate constant (e.g. Contain)
that is connected to each proposition or eventual-
ity referent (e.g. eC) by a dependency labeled ‘0’,
then applying that function to this referent, fol-
lowed by the list of arguments connected to this
referent by functions numbered ‘1’ and up: e.g.
(Contain eC dL dN). These dependencies can also
be defined by numbered dependency functions fn
from source instance j to destination instance i,
notated (fn j) = i. This notation will be used
in Section 4 to define constraints in the form of
equations. For example, the subject (first argu-
ment) of a lexical item may be constrained to be
the subject (first argument) of that item’s senten-
tial complement (second argument), as in an in-
stance of subject control, using the dependency
equation (f1 i) = (f1 (f2 i)).
Since continuation dependencies all flow up the
tree, any number of conjuncts can impinge upon a
common outscoping continuation, so there is no
longer any need for explicit conjunction nodes.
The representation is also attractive in that it lo-
cally distinguishes queries about, say, the cardi-
nality of the set of numbers in each document line
(Set sN dN sL) from queries about the cardinal-
ity of the set of numbers in general (Set sN dN s&apos;)
which is crucial for successful inference by pattern
matching. Finally, connected sets of continuation
dependencies form natural ‘scope graphs’ for use
in graph-based disambiguation algorithms (Man-
shadi and Allen, 2011; Manshadi et al., 2013),
which will be used to evaluate this representation
in Section 6.
</bodyText>
<sectionHeader confidence="0.87147" genericHeader="method">
3 Mapping to Lambda Calculus
</sectionHeader>
<bodyText confidence="0.999950333333333">
It is important for this representation not only
to have attractive graphical subsumption proper-
ties, but also to be sufficiently expressive to de-
fine corresponding expressions in lambda calcu-
lus. When continuation dependencies are filled in,
the resulting dependency structure can be trans-
</bodyText>
<page confidence="0.992661">
143
</page>
<figure confidence="0.999725794701986">
a)
eL
Every
A
0
1
dL
sL
0
pL
1
s&apos;
L
0 1
A dL&apos;
2
eN
Two
A
2
0
dN
sN
0
pN
1
1
A
0
d&apos;
N
2
s&apos;
N
1
Some
2
A
0
eC
sC
0
pC
1
1
A
0
2
1
e&apos;
C
s&apos;
C
1 2
1
Line Number Contain
1
0
b)
eL
0
Every
A
1
0
1
dL
sL
1
pL
s&apos;
L
0 1
A dL&apos;
2
eS
A
0 1
A dS
2 2 2
1 1 2
sS
0
1
pS
s&apos;
S
0 1
A d&apos;S
2
0
Some 2
A
0
eB
sB
1
0
1
pB
s&apos;
B
0 1
A e&apos;B
2
eN
Two
A
1
0
1
dN
sN
0 pN
1
1
A
0
2
d&apos;
N
s&apos;
N
1
0
Some
2
A
0
eC
sC
1
0
1
pC
A
0
2
e&apos;
C
s&apos;
C
1
Line Space BeginWith Number Contain
Line
Space
BeginWith
Number
Contain
c) (Every pL sL s&apos;L) ∧ (Set sL dL sL) ∧ (Line eL dL) ∧ (Set s&apos;L d&apos;L sL) ∧
(Two pN sN s&apos;N) ∧ (Set sN dN sL) ∧ (Number eN dN) ∧ (Set s&apos;N d&apos;N s&apos;L) ∧ (Contain eC d&apos;L d&apos;N)
</figure>
<figureCaption confidence="0.999868">
Figure 2: Semantic dependency graph in a ‘continuation-passing’ (bottom-up) style, including quantifiers
</figureCaption>
<bodyText confidence="0.9806024375">
over eventualities for verbs (in gray). The semantic dependency structure for the sentence Every line
contains two numbers (a), with flat logical form (c), is now contained by the semantic dependency
structure for Every line begins with a space and contains two numbers (b).
Γ, (f i0 .. i .. iN) ; (Ai o), Δ
lated into a lambda calculus expression by a de-
terministic algorithm which traverses sequences of
continuation dependencies and constructs accord-
ingly nested terms in a manner similar to that de-
fined for DRT (Kamp, 1981). This graphical rep-
resentation can be translated into lambda calculus
by representing the source graph as a set Γ of ele-
mentary predications (f i0 .. iN) and the target as
a set Δ of translated lambda calculus expressions,
e.g. (Ai (hf i0 .. i .. iN)). The set Δ can then be de-
rived from Γ using the following natural deduction
rules:4
</bodyText>
<listItem confidence="0.9856092">
• Initialize Δ with lambda terms (sets) that have
no outscoped sets in Γ:
Γ,(Set s i ) ; Δ
Γ, (Set s i ) ; (Ai True), Δ (Set s ) 0 Γ
• Add constraints to appropriate sets in Δ:
</listItem>
<bodyText confidence="0.7155">
4Here, set predications are defined with an additional final
argument position, which is defined to refer in a nuclear scope
set to the restrictor set that is its sibling, and in a restrictor set
to refer to sL.
</bodyText>
<listItem confidence="0.913478666666667">
Γ ; (Ai o ∧ (hf i0 .. i .. iN)), Δ i0 E E
• Add constraints of supersets as constraints on
subsets in Δ:
</listItem>
<bodyText confidence="0.548154">
Γ, (Set s i ), (Set s&apos; i&apos; s&apos;&apos;s) ;
</bodyText>
<equation confidence="0.58071675">
(Ai o ∧ (hf i0 .. i .. iN)), (Ai&apos; o&apos;), Δ
Γ, (Set s i ), (Set s&apos; i&apos; s&apos;&apos;s) ;
(Ai o ∧ (hf i0 .. i .. iN)),
(Ai&apos; o&apos; ∧ (hf i0 .. i&apos; .. iN)), Δ
</equation>
<listItem confidence="0.989181">
• Add quantifiers over completely constrained
sets in Δ:
</listItem>
<bodyText confidence="0.611291">
Γ, (Set s i ), (f p s&apos; s&apos;&apos;),
</bodyText>
<equation confidence="0.6314635">
(Set s&apos; i&apos; s ), (Set s&apos;&apos;i&apos;&apos;s&apos; s&apos;) ;
(Ai o), (Ai&apos; o&apos;), (Ai&apos;&apos; o&apos;&apos;), Δ
Γ,(Set s i ) ;
(Ai o ∧ (hf (Ai&apos; o&apos;) (Ai&apos;&apos; o&apos;&apos;))), Δ
</equation>
<bodyText confidence="0.99989875">
For example, the graph in Figure 2 can be trans-
lated into the following lambda calculus expres-
sion (including quantifiers over eventualities in the
source graph, to eliminate unbound variables):
</bodyText>
<equation confidence="0.926971428571429">
p E P,
(f&apos;.. i&apos;..) 0 Γ,
(f &apos;&apos;.. i&apos;&apos;..) Γ.
144
(EVERY (λdLSomE (λeLLiNE eL dL))
(λd0 LTwo (λdNSomE (λeNNumBER eN dN))
(λd0NSomE (λeCCoNTMN eC d0 L d0N))))
</equation>
<sectionHeader confidence="0.906853" genericHeader="method">
4 Derivation of Syntactic and Semantic
Dependencies
</sectionHeader>
<bodyText confidence="0.999985833333334">
The semantic dependency representation defined
in this paper assumes semantic dependencies other
than those representing continuations are derived
compositionally by a categorial grammar. In par-
ticular, this definition assumes a Generalized Cat-
egorial Grammar (GCG) (Bach, 1981; Oehrle,
1994), because it can be used to distinguish argu-
ment and modifier compositions (from which re-
strictor and nuclear scope sets are derived in a tree-
structured continuation graph), and because large
GCG-annotated corpora defined with this distinc-
tion are readily available (Nguyen et al., 2012).
GCG category types c ∈ C each consist of a prim-
itive category type u ∈ U, typically labeled with
the part of speech of the head of a category (e.g.
V, N, A, etc., for phrases or clauses headed by
verbs, nouns, adjectives, etc.), followed by one or
more unsatisfied dependencies, each consisting of
an operator o ∈ O (-a and -b for adjacent argument
dependencies preceding and succeeding a head, -c
and -d for adjacent conjunct dependencies preced-
ing and succeeding a head, -g for filler-gap depen-
dencies, -r for relative pronoun dependencies, and
some others), each followed by a dependent cate-
gory type from C. For example, the category type
for a transitive verb would be V-aN-bN, since it is
headed by a verb, and has unsatisfied dependen-
cies to satisfied noun-headed categories preced-
ing and succeeding it (for the subject and direct
object noun phrase, respectively). This formula-
tion has the advantage for semantic dependency
calculation that it distinguishes modifier and ar-
gument attachment. Since the semantic represen-
tation described in this paper makes explicit dis-
tinctions between restrictor sets and scope sets
(which is necessary for coherent interpretation of
quantifiers) it is necessary to consistently apply
predicate-argument constraints to discourse refer-
ents in the nuclear scope set of a quantifier and
modifier-modificand constraints to discourse ref-
erents in the restrictor set of a quantifier. For ex-
ample, in Sentence 1:
</bodyText>
<equation confidence="0.533029">
(1) Everything is [A-aN open].
</equation>
<bodyText confidence="0.718323">
the predicate open constrains the nuclear scope set
of every, but in Sentence 2:
</bodyText>
<listItem confidence="0.803541">
(2) Everything [A-aN open] is finished.
</listItem>
<bodyText confidence="0.99991902">
the predicate open constrains the restrictor set.
These constraints can be consistently applied in
the argument and modifier attachment rules of a
GCG.
Like a Combinatory Categorial Grammar
(Steedman, 2000), a GCG defines syntactic depen-
dencies for compositions that are determined by
the number and kind of unsatisfied dependencies
of the composed category types. These are similar
to dependencies for subject, direct object, prepo-
sition complement, etc., of Stanford dependencies
(de Marneffe et al., 2006), but are reduced to num-
bers based on the order of the associated depen-
dencies in the category type of the lexical head.
These syntactic dependencies are then associ-
ated with semantic dependencies, with the refer-
ent of a subject associated with the first argument
of an eventuality, the referent of a direct object as-
sociated with the second argument, and so on, for
all verb forms other than passive verbs. In the case
of passive verbs, the referent of a subject is asso-
ciated with the second argument of an eventuality,
the referent of a direct object associated with the
third argument, and so on.
In order to have a consistent treatment of ar-
gument and modifier attachment across all cate-
gory types, and also in order to model referents
of verbs as eventualities which can be quantified
by adverbs like never, once, twice, etc. (Parsons,
1990), it is desirable for eventualities associated
with verbs to also be quantified. Outgoing seman-
tic dependencies to arguments of eventualities are
then applied as constraints to the discourse refer-
ent variable of the restrictor sets of these quanti-
fiers. Incoming dependencies to eventualities and
other discourse referents used as modificands of
modifiers are also applied as constraints to dis-
course referent variables of restrictor sets, but in-
coming dependencies to discourse referents used
as arguments of predicates are applied as con-
straints to discourse referent variables of nuclear
scope sets. This assignment to restrictor or nuclear
scope sets depends on the context of the relevant
(argument or modifier attachment) parser opera-
tion, so associations between syntactic and seman-
tic dependencies must be left partially undefined
in lexical entries. Lexical entries are therefore de-
fined with separate syntactic and semantic depen-
dencies, using even numbers for syntactic depen-
dencies from lexical items, and odd numbers for
</bodyText>
<page confidence="0.990244">
145
</page>
<figure confidence="0.99956325">
b)
2
i&apos;
i
1
3
p&apos;
2
s&apos;
c)
2
s&apos;
1
1
p&apos;
i&apos;
3
i
a) containing
Contain
0
iC
1
3 5
pC
1
s&apos;
sC
s&apos;&apos;
1
1
1
d&apos;
eC
d&apos;&apos;
1 0 2
</figure>
<figureCaption confidence="0.813571333333333">
Figure 3: Example lexical semantic dependencies for the verb containing (a), and dependency equations
for argument attachment (b) and modifier attachment (c) in GCG deduction rules. Lexical dependencies
are shown in gray. Even numbered edges departing lexical items denote lexical syntactic dependen-
cies, and odd numbered edges departing lexical items are lexical semantic dependencies. Argument
attachments constrain semantic arguments to the nuclear scope sets of syntactic arguments, and modifier
attachments constrain semantic arguments to the restrictor sets of syntactic arguments.
</figureCaption>
<bodyText confidence="0.998826333333333">
semantic dependencies from lexical items. For ex-
ample, a lexical mapping for the finite transitive
verb contains might be associated with the pred-
icate Contain, and have the discourse referent of
its first lexical semantic argument (f1 (f3 i)) as-
sociated with the first argument of the eventuality
discourse referent of the restrictor set of its propo-
sition (f1 (f1 (f1 (f1 i)))), and the discourse referent
of its second lexical semantic argument (f1 (f5 i))
associated with the second argument of the even-
tuality discourse referent of the restrictor set of its
proposition (f2 (f1 (f1 (f1 i)))):
</bodyText>
<equation confidence="0.97303175">
contains =&gt; V-aN-bN : λi (f0 i)=contains
∧ (f0 (f1 (f1 (f1 i))))=Contain
∧ (f1 (f1 (f1 (f1 i))))=(f1 (f3 i))
∧ (f2 (f1 (f1 (f1 i))))=(f1 (f5 i))
</equation>
<bodyText confidence="0.9998052">
A graphical representation of these dependencies
is shown in Figure 3a. These lexical semantic con-
straints are then associated with syntactic depen-
dencies by grammar rules for argument and modi-
fier attachment, as described below.
</bodyText>
<subsectionHeader confidence="0.969121">
4.1 Inference rules for argument attachment
</subsectionHeader>
<bodyText confidence="0.966513272727273">
In GCG, as in other categorial grammars, infer-
ence rules for argument attachment apply functors
of category c-ad or c-bd to preceding or succeed-
ing arguments of category d:
d : g c-ad : h =&gt; c: (fc-ad g h) (Aa)
c-bd : g d : h =&gt; c: (fc-bd g h) (Ab)
where fuϕ1...ϕn are composition functions for uEU
and ϕE{-a, -b, -c, -dlxC, which connect the lexi-
cal item (f2n i) of a preceding child function g as
the 2nth argument of lexical item i of a succeeding
child function h, or vice versa:
</bodyText>
<equation confidence="0.974764666666667">
def
fuϕ1..nr1-ad = λghi (g (f2n i)) ∧ (h i)
∧ (f2n+1 i)=(f2 (f1 (f2n i))) (1a)
def
fuϕ1..nr1-bd = λghi (g i) ∧ (h (f2n i))
∧ (f2n+1 i)=(f2 (f1 (f2n i))) (1b)
</equation>
<bodyText confidence="0.999588833333333">
as shown in Figure 3b. This associates the lex-
ical semantic argument of the predicate (f2n+1 i)
with the nuclear scope of the quantifier propo-
sition associated with the syntactic argument
(f2 (f1 (f2n i))). For example, the following infer-
ence attaches a subject to a verb:
</bodyText>
<equation confidence="0.9904115">
contains two numbers
V-aN : λi (f0 i)=contains .. Aa
V : λi (f0 (f2 i))=line .. ∧ (f0 i)=contains ..
∧ (f3 i)=(f2 (f1 (f2 i)))
</equation>
<subsectionHeader confidence="0.971084">
4.2 Inference rules for modifier attachment
</subsectionHeader>
<bodyText confidence="0.9979988">
This grammar also uses distinguished inference
rules for modifier attachment. Inference rules for
modifier attachment apply preceding or succeed-
ing modifiers of category u-ad to modificands of
category c, for u E U and c, d E C:
</bodyText>
<tableCaption confidence="0.4801475">
u-ad : g c: h =&gt; c: (fPM g h) (Ma)
c: g u-ad : h =&gt; c: (fSM g h) (Mb)
</tableCaption>
<table confidence="0.588042">
every line
N: λi (f0 i)=line ..
</table>
<page confidence="0.969099">
146
</page>
<figureCaption confidence="0.984364">
Figure 4: Compositional analysis of noun phrase lines containing numbers exemplifying both argument
attachment (to numbers) and modifier attachment (to lines). Lexical dependencies are shown in gray, and
continuation dependencies (which do not result from syntactic composition) are highlighted.
</figureCaption>
<figure confidence="0.998669323943662">
0
SOME
2
pC
pL
1
1
2
1
2
2
1
0
2
lines
containing
numbers
0
1
1
0
LINE
CONTAIN
NUMBER
0 4 0
0
SOME
0
2
iL
iC
iN
1
3
1
5
1
0
SOME pN
2
sL
s&apos;
L
sC
s&apos;
C
sN
s&apos;
N
eL
A
0
1
dL
0 1
A d&apos;L
A eC
0
1
A
0
e&apos;
C
1
eN
A dN
0
1
A d&apos;N
0
1
</figure>
<bodyText confidence="0.99859875">
where fPM and fSM are category-independent com-
position functions for preceding and succeeding
modifiers, which return the lexical item of the ar-
gument (j) rather than of the predicate (i):
</bodyText>
<equation confidence="0.973483">
def
fPM = Ag h j 3i (f2 i)=j ∧ (g i) ∧ (h j)
∧ (f3 i)=(f1 (f1 (f2 i))) (2a)
def
fSM = Ag h j 3i (f2 i)=j ∧ (g j) ∧ (h i)
∧ (f3 i)=(f1 (f1 (f2 i))) (2b)
</equation>
<bodyText confidence="0.998058777777778">
as shown in Figure 3c. This allows categories
for predicates to be re-used as modifiers. Unlike
argument attachment, modifier attachment asso-
ciates the lexical semantic argument of the mod-
ifier (f2n+1 i) with the restrictor of the quantifier
proposition of the modificand (f1 (f1 (f2n i))). For
example, the following inference attaches an ad-
jectival modifier to the quantifier proposition of a
noun phrase:
</bodyText>
<table confidence="0.8158254">
every line containing two numbers
A-aN:Ai (f0 i)=containing ..
N:Ai (f0 i)=line ..
N: Ai (f0 i)=line .. ∧ 3j (f0 j)=containing ..
∧ (f2 j)=i ∧ (f3 j)=(f1 (f1 (f2 j)))
</table>
<bodyText confidence="0.7254315">
An example of argument and modifier attachment
is shown in Figure 4.
</bodyText>
<sectionHeader confidence="0.843649" genericHeader="method">
5 Estimation of Scope Dependencies
</sectionHeader>
<bodyText confidence="0.8640872">
Semantic dependency graphs obtained from GCG
derivations as described in Section 4 are scopally
underspecified. Scope disambiguations must then
be obtained by specifying continuation dependen-
cies from every set referent to some other set ref-
erent (or to a null context, indicating a top-level
set). In a sentence processing model, these non-
local continuation dependencies would be incre-
mentally calculated in working memory in a man-
ner similar to coreference resolution.5 However, in
this paper, in order to obtain a reasonable estimate
of the learnability of such a system, continuation
dependencies are assigned post-hoc by a statistical
inference algorithm.
The disambiguation algorithm first defines a
partition of the set of reified set referents into
sets is, s&apos;, s&apos;&apos;} of reified set referents s whose dis-
course referent variables (f1 s) are connected by
semantic dependencies. For example, sL, sC and
s&apos; N in Figure 4 are part of the same partition, but s&apos;L
is not.
Scope dependencies are then constructed from
these partitions using a greedy algorithm which
starts with an arbitrary set from this partition in
5Like any other dependency, a continuation dependency
may be stored during incremental processing when both its
cue (source) and target (destination) referents have been hy-
pothesized. For example, upon processing the word numbers
in the sentence Every line contains two numbers, a continu-
ation dependency may be stored from the nuclear scope set
associated with this word to the nuclear scope set of the sub-
ject every line, forming an in-situ interpretation with some
amount of activation (see Figure 4), and with some (proba-
bly smaller) amount of activation, a continuation dependency
may be stored from the nuclear scope set of this subject to
the nuclear scope set of this word, forming an inverted inter-
pretation. See Schuler (2014) for a model of how sentence
processing in associative memory might incrementally store
dependencies like these as cued associations.
Mb
</bodyText>
<page confidence="0.993959">
147
</page>
<bodyText confidence="0.98539116">
the dependency graph, then begins connecting it,
selecting the highest-ranked referent of that par-
tition that is not yet attached and designating it
as the new highest-scoping referent in that parti-
tion, attaching it as the context of the previously
highest-scoping referent in that partition if one ex-
ists. This proceeds until:
1. the algorithm reaches a restrictor or nuclear
scope referent with a sibling (superset or sub-
set) nuclear scope or restrictor referent that
has not yet served as the highest-scoping ref-
erent in its partition, at which point the algo-
rithm switches to the partition of that sibling
referent and begins connecting that; or
2. the algorithm reaches a restrictor or nuclear
scope referent with a sibling nuclear scope or
restrictor referent that is the highest-scoping
referent in its partition, in which case it con-
nects it to its sibling with a continuation de-
pendency from the nuclear scope referent to
the restrictor referent and merges the two sib-
lings’ partitions.
In this manner, all set referents in the dependency
graph are eventually assembled into a single tree
of continuation dependencies.
</bodyText>
<sectionHeader confidence="0.999481" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.999860027777778">
This paper defines a graphical semantic represen-
tation with desirable properties for storing sen-
tence meanings as cued associations in associa-
tive memory. In order to determine whether this
representation of continuation dependencies is re-
liably learnable, the set of test sentences from the
QuanText corpus (Manshadi et al., 2011) was au-
tomatically annotated with these continuation de-
pendencies and evaluated against the associated
set of gold-standard quantifier scopes. The sen-
tences in this corpus were collected as descrip-
tions of text editing tasks using unix tools like sed
and awk, collected from online tutorials and from
graduate students asked to write and describe ex-
ample scripts. Gold-standard scoping relations in
this corpus are specified over bracketed sequences
of words in each sentence. For example, the sen-
tence Print every line that starts with a number
might be annotated:
Print [1 every line] that starts with [2 a number] .
scoping relations: 1 &gt; 2
meaning that the quantifier over lines, referenced
in constituent 1, outscopes the quantifier over
numbers, referenced in constituent 2. In order to
isolate the learnablility of the continuation depen-
dencies described in this paper, both training and
test sentences of this corpus were annotated with
hand-corrected GCG derivations which are then
used to obtain semantic dependencies as described
in Section 4. Continuation dependencies are then
inferred from these semantic dependencies us-
ing the algorithm described in Section 5. Gold-
standard scoping relations are considered success-
fully recalled if a restrictor (f1 (f1 i)) or nuclear
scope (f2 (f1 i)) referent of any lexical item i within
the outscoped span is connected by a sequence of
continuation dependencies (in the appropriate di-
rection) to any restrictor or nuclear scope referent
of any lexical item within the outscoping span.
First, the algorithm was run without any lexical-
ization on the 94 non-duplicate sentences of the
QuanText test set. Results of this evaluation are
shown in the third line of Table 1 using the per-
sentence complete recall accuracy (‘AR’) defined
by Manshadi et al. (2013).
The algorithm was then run using bilexical
weights based on the frequencies ˜F(h, h&apos;) with
which a word h&apos; occurs as a head of a category
outscoped by a category headed by word h in the
350-sentence training set of the QuanText corpus.
For example, since quantifiers over lines are often
outscoped by quantifiers over files in the training
data, the system learns to rank continuation de-
pendencies to referents associated with the word
lines ahead of continuation dependencies to ref-
erents associated with the word files in bottom-
up inference. These lexical features may be par-
ticularly helpful because continuation dependen-
cies are generated only between directly adjacent
sets. Results for scope disambiguation using these
rankings are shown in the fourth line of Table 1.
This increase is statistically significant (p = 0.001
by two-tailed McNemar’s test). This significance
for local head-word features on continuation de-
pendencies shows that these dependencies can be
reliably learned from training examples, and sug-
gests that continuation dependencies may be a nat-
ural representation for scope information.
Interestingly, effects of lexical features for
quantifiers (the word each, or definite/indefinite
distinctions) were not substantial or statistically
significant, despite the relatively high frequencies
</bodyText>
<page confidence="0.994279">
148
</page>
<table confidence="0.5162228">
System AR
Manshadi and Allen (2011) baseline 63%
Manshadi et al. (2013) 72%
This system, w/o lexicalized model 61%
This system, w. lexicalized model 72%
</table>
<tableCaption confidence="0.752155333333333">
Table 1: Per-sentence complete recall accuracy
(‘AR’) of tree-based algorithm as compared to
Manshadi and Allen (2011) and Manshadi et al.
</tableCaption>
<bodyText confidence="0.975150695652174">
(2013) on explicit NP chunks in the QuanText test
set, correcting for use of gold standard trees as de-
scribed in footnote 19 of Manshadi et al. (2013).
of the words each and the in the test corpus (oc-
curring in 16% and 68% of test sentences, respec-
tively), which suggests that these words may often
be redundant with syntactic and head-word con-
straints. Results using preferences that rank refer-
ents quantified by the word each after other refer-
ents achieve a numerical increase in accuracy over
a model with no preferences (up 5 points, to 66%),
but it is not statistically significant (p = .13). Re-
sults using preferences that rank referents quanti-
fied by the word the after other referents achieve a
numerical increase in accuracy over a model with
no preferences (up 1 point, to 62%), but this is
even less significant (p = 1). Results are even
weaker in combination with head-word features
(up 1 point, to 73%, for each; down two points,
to 70%, for the). This suggests that world knowl-
edge (in the form of head-word information) may
be more salient to quantifier scope disambiguation
than many intuitive linguistic preferences.
</bodyText>
<sectionHeader confidence="0.998509" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999984888888889">
This paper has presented a graphical semantic de-
pendency representation based on bottom-up con-
tinuation dependencies which can be translated
into lambda calculus, but has the important prop-
erty that its vertices define a usable set of discourse
referents in working memory even in contexts in-
volving conjunction in the scope of quantifiers.
An evaluation on an existing quantifier scope dis-
ambiguation task shows that non-local continua-
tion dependencies can be as reliably learned from
annotated data as representations used in a state-
of-the-art quantifier scope resolver. This suggests
that continuation dependencies may be a natural
representation for scope information.
Continuation dependencies as defined in this
paper provide a local representation for quantifi-
cational context. This ensures that graphical repre-
sentations match only when their quantificational
contexts match. When used to guide a statistical
or vectorial representation, it is possible that this
local context will allow certain types of inference
to be defined by simple pattern matching, which
could be implemented in existing working mem-
ory models. Future work will explore the use of
this graph-based semantic representation as a ba-
sis for vectorial semantics in a cognitive model of
inference during sentence processing.
</bodyText>
<sectionHeader confidence="0.9984" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999898142857143">
The authors would like to thank Mehdi Manshadi
for assistance in obtaining the QuanText corpus.
The authors would also like to thank Erhard Hin-
richs, Craige Roberts, the members of the OSU
LLIC Reading Group, and the three anonymous
*SEM reviewers for their helpful comments about
this work.
</bodyText>
<sectionHeader confidence="0.999106" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999790466666667">
James A. Anderson, Jack W. Silverstein, Stephen A.
Ritz, and Randall S. Jones. 1977. Distinctive fea-
tures, categorical perception and probability learn-
ing: Some applications of a neural model. Psycho-
logical Review, 84:413–451.
Emmon Bach. 1981. Discontinuous constituents in
generalized categorial grammars. Proceedings of
the Annual Meeting of the Northeast Linguistic So-
ciety (NELS), 11:1–12.
Jason Baldridge and Geert-Jan M. Kruijff. 2002. Cou-
pling CCG and hybrid logic dependency seman-
tics. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics (ACL
2002), Philadelphia, Pennsylvania.
Chris Barker. 2002. Continuations and the nature
of quantification. Natural Language Semantics,
10:211–242.
Jon Barwise and Robin Cooper. 1981. Generalized
quantifiers and natural language. Linguistics and
Philosophy, 4.
Johan Bos. 1996. Predicate logic unplugged. In Pro-
ceedings of the 10th Amsterdam Colloquium, pages
133–143.
Sarah Brown-Schmidt, Ellen Campana, and Michael K.
Tanenhaus. 2002. Reference resolution in the wild:
Online circumscription of referential domains in a
natural interactive problem-solving task. In Pro-
ceedings of the 24th Annual Meeting of the Cogni-
tive Science Society, pages 148–153, Fairfax, VA,
August.
</reference>
<page confidence="0.989559">
149
</page>
<reference confidence="0.999872339622641">
Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan
Sag. 2005. Minimal recursion semantics: An intro-
duction. Research on Language and Computation,
pages 281–332.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC 2006.
Judith Degen and Michael K. Tanenhaus. 2011. Mak-
ing inferences: The case of scalar implicature pro-
cessing. In Proceedings of the 33rd Annual Confer-
ence of the Cognitive Science Society, pages 3299–
3304.
Marc W. Howard and Michael J. Kahana. 2002. A dis-
tributed representation of temporal context. Journal
of Mathematical Psychology, 45:269–299.
Hans Kamp. 1981. A theory of truth and semantic
representation. In Jeroen A. G. Groenendijk, Theo
M. V. Janssen, and Martin B. J. Stokhof, editors,
Formal Methods in the Study of Language: Math-
ematical Centre Tracts 135, pages 277–322. Mathe-
matical Center, Amsterdam.
Walter Kintsch. 1988. The role of knowledge in dis-
course comprehension: A construction-integration
model. Psychological review, 95(2):163–182.
Alexander Koller. 2004. Constraint-based and graph-
based resolution of ambiguities in natural language.
Ph.D. thesis, Universit¨at des Saarlandes.
Geert-Jan M. Kruijff. 2001. A Categorial-Modal
Architecture of Informativity: Dependency Gram-
mar Logic and Information Structure. Ph.D. thesis,
Charles University.
Bill MacCartney and Christopher D. Manning. 2009.
An Extended Model of Natural Logic. In Proceed-
ings of the Eighth International Conference on Com-
putational Semantics, IWCS-8 ’09, pages 140–156.
Association for Computational Linguistics.
Mehdi Manshadi and James F. Allen. 2011. Unre-
stricted quantifier scope disambiguation. In Graph-
based Methods for Natural Language Processing,
pages 51–59.
Mehdi Manshadi, James F. Allen, and Mary Swift.
2011. A corpus of scope-disambiguated english
text. In Proceedings of ACL, pages 141–146.
Mehdi Manshadi, Daniel Gildea, and James F. Allen.
2013. Plurality, negation, and quantification: To-
wards comprehensive quantifier scope disambigua-
tion. In Proceedings of ACL, pages 64–72.
David Marr. 1971. Simple memory: A theory
for archicortex. Philosophical Transactions of the
Royal Society (London) B, 262:23–81.
David Marr. 1982. Vision. A Computational Investiga-
tion into the Human Representation and Processing
of Visual Information. W.H. Freeman and Company.
J. L. McClelland, B. L. McNaughton, and R. C.
O’Reilly. 1995. Why there are complementary
learning systems in the hippocampus and neocortex:
Insights from the successes and failures of connec-
tionist models of learning and memory. Psychologi-
cal Review, 102:419–457.
Richard Montague. 1973. The proper treatment
of quantification in ordinary English. In J. Hin-
tikka, J.M.E. Moravcsik, and P. Suppes, editors,
Approaches to Natural Langauge, pages 221–242.
D. Riedel, Dordrecht. Reprinted in R. H. Thoma-
son ed., Formal Philosophy, Yale University Press,
1994.
B.B. Murdock. 1982. A theory for the storage and
retrieval of item and associative information. Psy-
chological Review, 89:609–626.
Luan Nguyen, Marten van Schijndel, and William
Schuler. 2012. Accurate unbounded dependency re-
covery using generalized categorial grammars. In
Proceedings of the 24th International Conference
on Computational Linguistics (COLING ’12), pages
2125–2140, Mumbai, India.
Richard T. Oehrle. 1994. Term-labeled categorial type
systems. Linguistics and Philosophy, 17(6):633–
678.
Terence Parsons. 1990. Events in the Semantics of
English. MIT Press.
William Schuler. 2014. Sentence processing in a
vectorial model of working memory. In Fifth An-
nual Workshop on Cognitive Modeling and Compu-
tational Linguistics (CMCL 2014).
Chung-chieh Shan and Chris Barker. 2006. Explaining
crossover and superiority as left-to-right evaluation.
Linguistics and Philosophy, 29:91–134.
Mark Steedman. 2000. The syntactic process. MIT
Press/Bradford Books, Cambridge, MA.
Mark Steedman. 2012. Taking Scope - The Natural
Semantics of Quantifiers. MIT Press.
Michael K. Tanenhaus, Michael J. Spivey-Knowlton,
Kathy M. Eberhard, and Julie E. Sedivy. 1995. In-
tegration of visual and linguistic information in spo-
ken language comprehension. Science, 268:1632–
1634.
Marten van Schijndel and William Schuler. 2013. An
analysis of frequency- and recency-based processing
costs. In Proceedings of NAACL-HLT 2013. Associ-
ation for Computational Linguistics.
Marten van Schijndel, Luan Nguyen, and William
Schuler. 2013. An analysis of memory-based pro-
cessing costs using incremental deep syntactic de-
pendency parsing. In Proceedings of CMCL 2013.
Association for Computational Linguistics.
</reference>
<page confidence="0.998321">
150
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.988669">
<title confidence="0.999714">Cognitive Compositional Semantics using Continuation Dependencies</title>
<author confidence="0.999949">William Schuler Adam Wheeler</author>
<affiliation confidence="0.9996915">Department of Linguistics Department of Linguistics The Ohio State University The Ohio State University</affiliation>
<email confidence="0.999217">schuler@ling.osu.eduwheeler@ling.osu.edu</email>
<abstract confidence="0.99939225">This paper describes a graphical semantic representation based on bottom-up ‘continuation’ dependencies which has the important property that its vertices define a usable set of discourse referents in working memory even in contexts involving conjunction in the scope of quantifiers. An evaluation on an existing quantifier scope disambiguation task shows that non-local continuation dependencies can be as reliably learned from annotated data as representations used in a state-of-the-art quantifier scope resolver, suggesting that continuation dependencies may provide a natural representation for scope information.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James A Anderson</author>
<author>Jack W Silverstein</author>
<author>Stephen A Ritz</author>
<author>Randall S Jones</author>
</authors>
<title>Distinctive features, categorical perception and probability learning: Some applications of a neural model.</title>
<date>1977</date>
<tech>Psychological Review, 84:413–451.</tech>
<contexts>
<context position="1540" citStr="Anderson et al., 1977" startWordPosition="215" endWordPosition="218">hat at least shallow semantic interpretation informs parsing decisions in human sentence processing (Tanenhaus et al., 1995; Brown-Schmidt et al., 2002), and recent evidence points to incremental processing of quantifier implicatures as well (Degen and Tanenhaus, 2011). This may indicate that inferences about the meaning of quantifiers are processed directly in working memory. Human working memory is widely assumed to store events (including linguistic events) as re-usable activation-based states, connected by a durable but rapidly mutable weight-based memory of cued associations (Marr, 1971; Anderson et al., 1977; Murdock, 1982; McClelland et al., 1995; Howard and Kahana, 2002). Complex dependency structures can therefore be stored in this associative memory as graphs, with states as vertices and cued associations as directed edges (e.g. Kintsch, 1988). This kind of representation is necessary to formulate and evaluate algorithmic claims (Marr, 1982) about cued associations and working memory use in human sentence processing (e.g. van Schijndel and Schuler, 2013). But accounting for syntax and semantics in this way must be done carefully in order to preserve linguistically important distinctions. For </context>
</contexts>
<marker>Anderson, Silverstein, Ritz, Jones, 1977</marker>
<rawString>James A. Anderson, Jack W. Silverstein, Stephen A. Ritz, and Randall S. Jones. 1977. Distinctive features, categorical perception and probability learning: Some applications of a neural model. Psychological Review, 84:413–451.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmon Bach</author>
</authors>
<title>Discontinuous constituents in generalized categorial grammars.</title>
<date>1981</date>
<booktitle>Proceedings of the Annual Meeting of the Northeast Linguistic Society (NELS),</booktitle>
<pages>11--1</pages>
<contexts>
<context position="16110" citStr="Bach, 1981" startWordPosition="2791" endWordPosition="2792">ing lambda calculus expression (including quantifiers over eventualities in the source graph, to eliminate unbound variables): p E P, (f&apos;.. i&apos;..) 0 Γ, (f &apos;&apos;.. i&apos;&apos;..) Γ. 144 (EVERY (λdLSomE (λeLLiNE eL dL)) (λd0 LTwo (λdNSomE (λeNNumBER eN dN)) (λd0NSomE (λeCCoNTMN eC d0 L d0N)))) 4 Derivation of Syntactic and Semantic Dependencies The semantic dependency representation defined in this paper assumes semantic dependencies other than those representing continuations are derived compositionally by a categorial grammar. In particular, this definition assumes a Generalized Categorial Grammar (GCG) (Bach, 1981; Oehrle, 1994), because it can be used to distinguish argument and modifier compositions (from which restrictor and nuclear scope sets are derived in a treestructured continuation graph), and because large GCG-annotated corpora defined with this distinction are readily available (Nguyen et al., 2012). GCG category types c ∈ C each consist of a primitive category type u ∈ U, typically labeled with the part of speech of the head of a category (e.g. V, N, A, etc., for phrases or clauses headed by verbs, nouns, adjectives, etc.), followed by one or more unsatisfied dependencies, each consisting o</context>
</contexts>
<marker>Bach, 1981</marker>
<rawString>Emmon Bach. 1981. Discontinuous constituents in generalized categorial grammars. Proceedings of the Annual Meeting of the Northeast Linguistic Society (NELS), 11:1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Baldridge</author>
<author>Geert-Jan M Kruijff</author>
</authors>
<title>Coupling CCG and hybrid logic dependency semantics.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="2618" citStr="Baldridge and Kruijff, 2002" startWordPosition="373" endWordPosition="376">r, 2013). But accounting for syntax and semantics in this way must be done carefully in order to preserve linguistically important distinctions. For example, positing spurious local dependencies in filler-gap constructions can lead to missed integrations of dependency structure in incremental processing, resulting in weaker model fitting (van Schijndel et al., 2013). Similar care may be necessary in cases of dependencies arising from anaphoric coreference or quantifier scope. Unfortunately, most existing theories of compositional semantics (Montague, 1973; Barwise and Cooper, 1981; Bos, 1996; Baldridge and Kruijff, 2002; Koller, 2004; Copestake et al., 2005) are defined at the computational level (Marr, 1982), employing beta reduction over complete or underspecified lambda calculus expressions as a precise description of the language processing task to be modeled, not at the algorithmic level, as a model of human language processing itself. The structured expressions these theories generate are not intended to represent re-usable referential states of the sort that could be modeled in current theories of associative memory. As such, it should not be surprising that structural adaptations of lambda calculus e</context>
</contexts>
<marker>Baldridge, Kruijff, 2002</marker>
<rawString>Jason Baldridge and Geert-Jan M. Kruijff. 2002. Coupling CCG and hybrid logic dependency semantics. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL 2002), Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Barker</author>
</authors>
<title>Continuations and the nature of quantification.</title>
<date>2002</date>
<journal>Natural Language Semantics,</journal>
<pages>10--211</pages>
<contexts>
<context position="9042" citStr="Barker, 2002" startWordPosition="1486" endWordPosition="1487">n Figure 1 are not the only way to represent compositional semantics of sentences. This paper defines a graphical semantic dependency representation that can be translated into lambda calculus, but has the important property that its vertices define a usable set of discourse referents in working memory even in contexts involving conjunction in the scope of quantifiers. It does this by reversing the direction of dependencies from parent-to-child subsumption in a lambda-calculus tree to a representation similar to the inside-out structure of function definitions in a continuation-passing style (Barker, 2002; Shan and Barker, 2006)2 so that sets are defined in terms of their context, and explicit ‘And’ predicates are no longer required, leaving nothing to get in the way of an exact pattern match.3 The learnability of the non-local continuation dependencies involved in this representation is then evaluated on an existing quantifier scope disambiguation task using a dependency-based statistical scope resolver, with results comparable to a state-of-the-art unrestricted graph-based quantifier scope resolver (Manshadi et al., 2013). 2 Continuation Dependencies This paper explores the use of a bottom-u</context>
</contexts>
<marker>Barker, 2002</marker>
<rawString>Chris Barker. 2002. Continuations and the nature of quantification. Natural Language Semantics, 10:211–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon Barwise</author>
<author>Robin Cooper</author>
</authors>
<title>Generalized quantifiers and natural language.</title>
<date>1981</date>
<journal>Linguistics and Philosophy,</journal>
<volume>4</volume>
<contexts>
<context position="2578" citStr="Barwise and Cooper, 1981" startWordPosition="367" endWordPosition="370">essing (e.g. van Schijndel and Schuler, 2013). But accounting for syntax and semantics in this way must be done carefully in order to preserve linguistically important distinctions. For example, positing spurious local dependencies in filler-gap constructions can lead to missed integrations of dependency structure in incremental processing, resulting in weaker model fitting (van Schijndel et al., 2013). Similar care may be necessary in cases of dependencies arising from anaphoric coreference or quantifier scope. Unfortunately, most existing theories of compositional semantics (Montague, 1973; Barwise and Cooper, 1981; Bos, 1996; Baldridge and Kruijff, 2002; Koller, 2004; Copestake et al., 2005) are defined at the computational level (Marr, 1982), employing beta reduction over complete or underspecified lambda calculus expressions as a precise description of the language processing task to be modeled, not at the algorithmic level, as a model of human language processing itself. The structured expressions these theories generate are not intended to represent re-usable referential states of the sort that could be modeled in current theories of associative memory. As such, it should not be surprising that str</context>
</contexts>
<marker>Barwise, Cooper, 1981</marker>
<rawString>Jon Barwise and Robin Cooper. 1981. Generalized quantifiers and natural language. Linguistics and Philosophy, 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
</authors>
<title>Predicate logic unplugged.</title>
<date>1996</date>
<booktitle>In Proceedings of the 10th Amsterdam Colloquium,</booktitle>
<pages>133--143</pages>
<contexts>
<context position="2589" citStr="Bos, 1996" startWordPosition="371" endWordPosition="372"> and Schuler, 2013). But accounting for syntax and semantics in this way must be done carefully in order to preserve linguistically important distinctions. For example, positing spurious local dependencies in filler-gap constructions can lead to missed integrations of dependency structure in incremental processing, resulting in weaker model fitting (van Schijndel et al., 2013). Similar care may be necessary in cases of dependencies arising from anaphoric coreference or quantifier scope. Unfortunately, most existing theories of compositional semantics (Montague, 1973; Barwise and Cooper, 1981; Bos, 1996; Baldridge and Kruijff, 2002; Koller, 2004; Copestake et al., 2005) are defined at the computational level (Marr, 1982), employing beta reduction over complete or underspecified lambda calculus expressions as a precise description of the language processing task to be modeled, not at the algorithmic level, as a model of human language processing itself. The structured expressions these theories generate are not intended to represent re-usable referential states of the sort that could be modeled in current theories of associative memory. As such, it should not be surprising that structural ada</context>
</contexts>
<marker>Bos, 1996</marker>
<rawString>Johan Bos. 1996. Predicate logic unplugged. In Proceedings of the 10th Amsterdam Colloquium, pages 133–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarah Brown-Schmidt</author>
<author>Ellen Campana</author>
<author>Michael K Tanenhaus</author>
</authors>
<title>Reference resolution in the wild: Online circumscription of referential domains in a natural interactive problem-solving task.</title>
<date>2002</date>
<booktitle>In Proceedings of the 24th Annual Meeting of the Cognitive Science Society,</booktitle>
<pages>148--153</pages>
<location>Fairfax, VA,</location>
<contexts>
<context position="1071" citStr="Brown-Schmidt et al., 2002" startWordPosition="144" endWordPosition="147">nts in working memory even in contexts involving conjunction in the scope of quantifiers. An evaluation on an existing quantifier scope disambiguation task shows that non-local continuation dependencies can be as reliably learned from annotated data as representations used in a state-of-the-art quantifier scope resolver, suggesting that continuation dependencies may provide a natural representation for scope information. 1 Introduction It is now fairly well established that at least shallow semantic interpretation informs parsing decisions in human sentence processing (Tanenhaus et al., 1995; Brown-Schmidt et al., 2002), and recent evidence points to incremental processing of quantifier implicatures as well (Degen and Tanenhaus, 2011). This may indicate that inferences about the meaning of quantifiers are processed directly in working memory. Human working memory is widely assumed to store events (including linguistic events) as re-usable activation-based states, connected by a durable but rapidly mutable weight-based memory of cued associations (Marr, 1971; Anderson et al., 1977; Murdock, 1982; McClelland et al., 1995; Howard and Kahana, 2002). Complex dependency structures can therefore be stored in this a</context>
</contexts>
<marker>Brown-Schmidt, Campana, Tanenhaus, 2002</marker>
<rawString>Sarah Brown-Schmidt, Ellen Campana, and Michael K. Tanenhaus. 2002. Reference resolution in the wild: Online circumscription of referential domains in a natural interactive problem-solving task. In Proceedings of the 24th Annual Meeting of the Cognitive Science Society, pages 148–153, Fairfax, VA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
<author>Carl Pollard</author>
<author>Ivan Sag</author>
</authors>
<title>Minimal recursion semantics: An introduction.</title>
<date>2005</date>
<booktitle>Research on Language and Computation,</booktitle>
<pages>281--332</pages>
<contexts>
<context position="2657" citStr="Copestake et al., 2005" startWordPosition="379" endWordPosition="382">antics in this way must be done carefully in order to preserve linguistically important distinctions. For example, positing spurious local dependencies in filler-gap constructions can lead to missed integrations of dependency structure in incremental processing, resulting in weaker model fitting (van Schijndel et al., 2013). Similar care may be necessary in cases of dependencies arising from anaphoric coreference or quantifier scope. Unfortunately, most existing theories of compositional semantics (Montague, 1973; Barwise and Cooper, 1981; Bos, 1996; Baldridge and Kruijff, 2002; Koller, 2004; Copestake et al., 2005) are defined at the computational level (Marr, 1982), employing beta reduction over complete or underspecified lambda calculus expressions as a precise description of the language processing task to be modeled, not at the algorithmic level, as a model of human language processing itself. The structured expressions these theories generate are not intended to represent re-usable referential states of the sort that could be modeled in current theories of associative memory. As such, it should not be surprising that structural adaptations of lambda calculus expressions as referential states exhibi</context>
<context position="8301" citStr="Copestake et al., 2005" startWordPosition="1372" endWordPosition="1375">n but they are not negative, eC may serve as a referent of that in but that was before it was edited, and pL may serve as a referent of that in but the compiler doesn’t enforce that, but it is not clear what if anything would naturally refer to the internal conjunction pA. Predications over such conjunctions (e.g. Kim believes that every line begins with a space and contains 142 two numbers) are usually predicated at the outer proposition pL, and in any case do not have truth values that are independent of the same predication at each conjunct. One of the goals of Minimal Recursion Semantics (Copestake et al., 2005) was to eliminate similar kinds of superfluous conjunction structure. Fortunately, lambda calculus expressions like those shown in Figure 1 are not the only way to represent compositional semantics of sentences. This paper defines a graphical semantic dependency representation that can be translated into lambda calculus, but has the important property that its vertices define a usable set of discourse referents in working memory even in contexts involving conjunction in the scope of quantifiers. It does this by reversing the direction of dependencies from parent-to-child subsumption in a lambd</context>
<context position="10957" citStr="Copestake et al. (2005)" startWordPosition="1775" endWordPosition="1778">on of de Morgan’s law and manipulation of the polarity of adjacent quantifiers. For example, Every line begins with at least one space or contains at least two numbers, is equivalent to No line begins with fewer than one space and contains fewer than two numbers. discourse referents of a ‘direct’ representation (the p, s, e, and d nodes in Figure 1), but replaces the downward dependencies departing set referents with upward dependencies to context sets (highlighted in Figure 2). Figures 1c and 2c also show flat logical forms composed of elementary predications, adapted from Kruijff (2001) and Copestake et al. (2005), for the sentence Every line contains two numbers, which are formed by identifying the function associated with the predicate constant (e.g. Contain) that is connected to each proposition or eventuality referent (e.g. eC) by a dependency labeled ‘0’, then applying that function to this referent, followed by the list of arguments connected to this referent by functions numbered ‘1’ and up: e.g. (Contain eC dL dN). These dependencies can also be defined by numbered dependency functions fn from source instance j to destination instance i, notated (fn j) = i. This notation will be used in Section</context>
</contexts>
<marker>Copestake, Flickinger, Pollard, Sag, 2005</marker>
<rawString>Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan Sag. 2005. Minimal recursion semantics: An introduction. Research on Language and Computation, pages 281–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of LREC 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith Degen</author>
<author>Michael K Tanenhaus</author>
</authors>
<title>Making inferences: The case of scalar implicature processing.</title>
<date>2011</date>
<booktitle>In Proceedings of the 33rd Annual Conference of the Cognitive Science Society,</booktitle>
<pages>3299--3304</pages>
<contexts>
<context position="1188" citStr="Degen and Tanenhaus, 2011" startWordPosition="161" endWordPosition="165"> quantifier scope disambiguation task shows that non-local continuation dependencies can be as reliably learned from annotated data as representations used in a state-of-the-art quantifier scope resolver, suggesting that continuation dependencies may provide a natural representation for scope information. 1 Introduction It is now fairly well established that at least shallow semantic interpretation informs parsing decisions in human sentence processing (Tanenhaus et al., 1995; Brown-Schmidt et al., 2002), and recent evidence points to incremental processing of quantifier implicatures as well (Degen and Tanenhaus, 2011). This may indicate that inferences about the meaning of quantifiers are processed directly in working memory. Human working memory is widely assumed to store events (including linguistic events) as re-usable activation-based states, connected by a durable but rapidly mutable weight-based memory of cued associations (Marr, 1971; Anderson et al., 1977; Murdock, 1982; McClelland et al., 1995; Howard and Kahana, 2002). Complex dependency structures can therefore be stored in this associative memory as graphs, with states as vertices and cued associations as directed edges (e.g. Kintsch, 1988). Th</context>
</contexts>
<marker>Degen, Tanenhaus, 2011</marker>
<rawString>Judith Degen and Michael K. Tanenhaus. 2011. Making inferences: The case of scalar implicature processing. In Proceedings of the 33rd Annual Conference of the Cognitive Science Society, pages 3299– 3304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc W Howard</author>
<author>Michael J Kahana</author>
</authors>
<title>A distributed representation of temporal context.</title>
<date>2002</date>
<journal>Journal of Mathematical Psychology,</journal>
<pages>45--269</pages>
<contexts>
<context position="1606" citStr="Howard and Kahana, 2002" startWordPosition="226" endWordPosition="229">cisions in human sentence processing (Tanenhaus et al., 1995; Brown-Schmidt et al., 2002), and recent evidence points to incremental processing of quantifier implicatures as well (Degen and Tanenhaus, 2011). This may indicate that inferences about the meaning of quantifiers are processed directly in working memory. Human working memory is widely assumed to store events (including linguistic events) as re-usable activation-based states, connected by a durable but rapidly mutable weight-based memory of cued associations (Marr, 1971; Anderson et al., 1977; Murdock, 1982; McClelland et al., 1995; Howard and Kahana, 2002). Complex dependency structures can therefore be stored in this associative memory as graphs, with states as vertices and cued associations as directed edges (e.g. Kintsch, 1988). This kind of representation is necessary to formulate and evaluate algorithmic claims (Marr, 1982) about cued associations and working memory use in human sentence processing (e.g. van Schijndel and Schuler, 2013). But accounting for syntax and semantics in this way must be done carefully in order to preserve linguistically important distinctions. For example, positing spurious local dependencies in filler-gap constr</context>
</contexts>
<marker>Howard, Kahana, 2002</marker>
<rawString>Marc W. Howard and Michael J. Kahana. 2002. A distributed representation of temporal context. Journal of Mathematical Psychology, 45:269–299.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Kamp</author>
</authors>
<title>A theory of truth and semantic representation.</title>
<date>1981</date>
<booktitle>Formal Methods in the Study of Language: Mathematical Centre Tracts 135,</booktitle>
<pages>277--322</pages>
<editor>In Jeroen A. G. Groenendijk, Theo M. V. Janssen, and Martin B. J. Stokhof, editors,</editor>
<publisher>Mathematical</publisher>
<location>Center, Amsterdam.</location>
<contexts>
<context position="14223" citStr="Kamp, 1981" startWordPosition="2420" endWordPosition="2421">endency graph in a ‘continuation-passing’ (bottom-up) style, including quantifiers over eventualities for verbs (in gray). The semantic dependency structure for the sentence Every line contains two numbers (a), with flat logical form (c), is now contained by the semantic dependency structure for Every line begins with a space and contains two numbers (b). Γ, (f i0 .. i .. iN) ; (Ai o), Δ lated into a lambda calculus expression by a deterministic algorithm which traverses sequences of continuation dependencies and constructs accordingly nested terms in a manner similar to that defined for DRT (Kamp, 1981). This graphical representation can be translated into lambda calculus by representing the source graph as a set Γ of elementary predications (f i0 .. iN) and the target as a set Δ of translated lambda calculus expressions, e.g. (Ai (hf i0 .. i .. iN)). The set Δ can then be derived from Γ using the following natural deduction rules:4 • Initialize Δ with lambda terms (sets) that have no outscoped sets in Γ: Γ,(Set s i ) ; Δ Γ, (Set s i ) ; (Ai True), Δ (Set s ) 0 Γ • Add constraints to appropriate sets in Δ: 4Here, set predications are defined with an additional final argument position, which </context>
</contexts>
<marker>Kamp, 1981</marker>
<rawString>Hans Kamp. 1981. A theory of truth and semantic representation. In Jeroen A. G. Groenendijk, Theo M. V. Janssen, and Martin B. J. Stokhof, editors, Formal Methods in the Study of Language: Mathematical Centre Tracts 135, pages 277–322. Mathematical Center, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Kintsch</author>
</authors>
<title>The role of knowledge in discourse comprehension: A construction-integration model. Psychological review,</title>
<date>1988</date>
<pages>95--2</pages>
<contexts>
<context position="1784" citStr="Kintsch, 1988" startWordPosition="254" endWordPosition="255">d Tanenhaus, 2011). This may indicate that inferences about the meaning of quantifiers are processed directly in working memory. Human working memory is widely assumed to store events (including linguistic events) as re-usable activation-based states, connected by a durable but rapidly mutable weight-based memory of cued associations (Marr, 1971; Anderson et al., 1977; Murdock, 1982; McClelland et al., 1995; Howard and Kahana, 2002). Complex dependency structures can therefore be stored in this associative memory as graphs, with states as vertices and cued associations as directed edges (e.g. Kintsch, 1988). This kind of representation is necessary to formulate and evaluate algorithmic claims (Marr, 1982) about cued associations and working memory use in human sentence processing (e.g. van Schijndel and Schuler, 2013). But accounting for syntax and semantics in this way must be done carefully in order to preserve linguistically important distinctions. For example, positing spurious local dependencies in filler-gap constructions can lead to missed integrations of dependency structure in incremental processing, resulting in weaker model fitting (van Schijndel et al., 2013). Similar care may be nec</context>
</contexts>
<marker>Kintsch, 1988</marker>
<rawString>Walter Kintsch. 1988. The role of knowledge in discourse comprehension: A construction-integration model. Psychological review, 95(2):163–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Koller</author>
</authors>
<title>Constraint-based and graphbased resolution of ambiguities in natural language.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>Universit¨at des Saarlandes.</institution>
<contexts>
<context position="2632" citStr="Koller, 2004" startWordPosition="377" endWordPosition="378">syntax and semantics in this way must be done carefully in order to preserve linguistically important distinctions. For example, positing spurious local dependencies in filler-gap constructions can lead to missed integrations of dependency structure in incremental processing, resulting in weaker model fitting (van Schijndel et al., 2013). Similar care may be necessary in cases of dependencies arising from anaphoric coreference or quantifier scope. Unfortunately, most existing theories of compositional semantics (Montague, 1973; Barwise and Cooper, 1981; Bos, 1996; Baldridge and Kruijff, 2002; Koller, 2004; Copestake et al., 2005) are defined at the computational level (Marr, 1982), employing beta reduction over complete or underspecified lambda calculus expressions as a precise description of the language processing task to be modeled, not at the algorithmic level, as a model of human language processing itself. The structured expressions these theories generate are not intended to represent re-usable referential states of the sort that could be modeled in current theories of associative memory. As such, it should not be surprising that structural adaptations of lambda calculus expressions as </context>
<context position="4767" citStr="Koller (2004)" startWordPosition="801" endWordPosition="802">0 1 2 A pN 1 A 2 s0 N pS 2 0 1 2 eN A d0N eC dL 0 Line A d0L 01 Two 0 A dL 1 Line 0 A sS A d0L 1 2 s0 S eL 1 sN 1 dN eL 0 1 pA 1 0 2 And pN 1 01 2 Two sN s0 N 1 0 20 0 1 2 0 1 2 0 1 2 0 1 2 A dS eS eN 1 0 0 Number Contain A d0S A dN 2 1 0 A d0N 20 eB eC Space Space BeginsWith Number Contain BeginsWith Number Contain c) (Every pL sL s0L) ∧ (Set sL dL eL) ∧ (Line eL dL) ∧ (Set s0L d0L pN) ∧ (Two pN sN s0N) ∧ (Set sN dN eN) ∧ (Number eN dN) ∧ (Set s0N d0N eC) ∧ (Contain eC d0L d0 N) Figure 1: Semantic dependency graph in a ‘direct’ (top-down) style, adapted from a disambiguated representation of Koller (2004), excluding quantifiers over eventualities. The semantic dependency structure for the sentence Every line contains two numbers (a), with flat logical form (c), is not a subgraph of the semantic dependency structure for Every line begins with a space and contains two numbers (b), because the structure is interrupted by the explicit conjunction predicate ‘And’. in each document line (s0N) using local topological features of the dependency graph, as would be required to accurately recall assertions about total or average quantities of numbers in document lines.1 Second, graphs based on traditiona</context>
</contexts>
<marker>Koller, 2004</marker>
<rawString>Alexander Koller. 2004. Constraint-based and graphbased resolution of ambiguities in natural language. Ph.D. thesis, Universit¨at des Saarlandes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geert-Jan M Kruijff</author>
</authors>
<title>A Categorial-Modal Architecture of Informativity: Dependency Grammar Logic and Information Structure.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>Charles University.</institution>
<contexts>
<context position="10929" citStr="Kruijff (2001)" startWordPosition="1772" endWordPosition="1773">n through application of de Morgan’s law and manipulation of the polarity of adjacent quantifiers. For example, Every line begins with at least one space or contains at least two numbers, is equivalent to No line begins with fewer than one space and contains fewer than two numbers. discourse referents of a ‘direct’ representation (the p, s, e, and d nodes in Figure 1), but replaces the downward dependencies departing set referents with upward dependencies to context sets (highlighted in Figure 2). Figures 1c and 2c also show flat logical forms composed of elementary predications, adapted from Kruijff (2001) and Copestake et al. (2005), for the sentence Every line contains two numbers, which are formed by identifying the function associated with the predicate constant (e.g. Contain) that is connected to each proposition or eventuality referent (e.g. eC) by a dependency labeled ‘0’, then applying that function to this referent, followed by the list of arguments connected to this referent by functions numbered ‘1’ and up: e.g. (Contain eC dL dN). These dependencies can also be defined by numbered dependency functions fn from source instance j to destination instance i, notated (fn j) = i. This nota</context>
</contexts>
<marker>Kruijff, 2001</marker>
<rawString>Geert-Jan M. Kruijff. 2001. A Categorial-Modal Architecture of Informativity: Dependency Grammar Logic and Information Structure. Ph.D. thesis, Charles University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>An Extended Model of Natural Logic.</title>
<date>2009</date>
<booktitle>In Proceedings of the Eighth International Conference on Computational Semantics, IWCS-8 ’09,</booktitle>
<pages>140--156</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6347" citStr="MacCartney and Manning, 2009" startWordPosition="1041" endWordPosition="1045">ndencies with all possible labels in increasingly longer paths from one or more constant vector states (e.g. vectors for predicate constants). This graph matching does not necessarily preclude the introduction of monotonicity constraints from matched quantifiers. For example, More than two perl scripts work, can entail More than two scripts work, using a subgraph in the first argument, but Fewer than two scripts work, can entail Fewer than two perl scripts work, using a supergraph in the first argument. This consideration is similar to those observed in representations based on natural logic (MacCartney and Manning, 2009) which also uses low-level matching to perform some kinds of inference, but representations based on natural logic typically exclude other forms of inference, whereas the present model does not. This matching also assumes properties of nuclear scope variables are inherited from associated restrictor variables, e.g. through a set of dependencies from nuclear scope sets to restrictor sets not shown in the figure. This assumption will be revisited in Section 3. begins with a space and contains two numbers shown in Figure 1b does not contain the graphical representation of the sentence Every line </context>
</contexts>
<marker>MacCartney, Manning, 2009</marker>
<rawString>Bill MacCartney and Christopher D. Manning. 2009. An Extended Model of Natural Logic. In Proceedings of the Eighth International Conference on Computational Semantics, IWCS-8 ’09, pages 140–156. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehdi Manshadi</author>
<author>James F Allen</author>
</authors>
<title>Unrestricted quantifier scope disambiguation.</title>
<date>2011</date>
<booktitle>In Graphbased Methods for Natural Language Processing,</booktitle>
<pages>51--59</pages>
<contexts>
<context position="12527" citStr="Manshadi and Allen, 2011" startWordPosition="2036" endWordPosition="2040"> flow up the tree, any number of conjuncts can impinge upon a common outscoping continuation, so there is no longer any need for explicit conjunction nodes. The representation is also attractive in that it locally distinguishes queries about, say, the cardinality of the set of numbers in each document line (Set sN dN sL) from queries about the cardinality of the set of numbers in general (Set sN dN s&apos;) which is crucial for successful inference by pattern matching. Finally, connected sets of continuation dependencies form natural ‘scope graphs’ for use in graph-based disambiguation algorithms (Manshadi and Allen, 2011; Manshadi et al., 2013), which will be used to evaluate this representation in Section 6. 3 Mapping to Lambda Calculus It is important for this representation not only to have attractive graphical subsumption properties, but also to be sufficiently expressive to define corresponding expressions in lambda calculus. When continuation dependencies are filled in, the resulting dependency structure can be trans143 a) eL Every A 0 1 dL sL 0 pL 1 s&apos; L 0 1 A dL&apos; 2 eN Two A 2 0 dN sN 0 pN 1 1 A 0 d&apos; N 2 s&apos; N 1 Some 2 A 0 eC sC 0 pC 1 1 A 0 2 1 e&apos; C s&apos; C 1 2 1 Line Number Contain 1 0 b) eL 0 Every A 1 </context>
<context position="31717" citStr="Manshadi and Allen (2011)" startWordPosition="5392" endWordPosition="5395">ngs are shown in the fourth line of Table 1. This increase is statistically significant (p = 0.001 by two-tailed McNemar’s test). This significance for local head-word features on continuation dependencies shows that these dependencies can be reliably learned from training examples, and suggests that continuation dependencies may be a natural representation for scope information. Interestingly, effects of lexical features for quantifiers (the word each, or definite/indefinite distinctions) were not substantial or statistically significant, despite the relatively high frequencies 148 System AR Manshadi and Allen (2011) baseline 63% Manshadi et al. (2013) 72% This system, w/o lexicalized model 61% This system, w. lexicalized model 72% Table 1: Per-sentence complete recall accuracy (‘AR’) of tree-based algorithm as compared to Manshadi and Allen (2011) and Manshadi et al. (2013) on explicit NP chunks in the QuanText test set, correcting for use of gold standard trees as described in footnote 19 of Manshadi et al. (2013). of the words each and the in the test corpus (occurring in 16% and 68% of test sentences, respectively), which suggests that these words may often be redundant with syntactic and head-word co</context>
</contexts>
<marker>Manshadi, Allen, 2011</marker>
<rawString>Mehdi Manshadi and James F. Allen. 2011. Unrestricted quantifier scope disambiguation. In Graphbased Methods for Natural Language Processing, pages 51–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehdi Manshadi</author>
<author>James F Allen</author>
<author>Mary Swift</author>
</authors>
<title>A corpus of scope-disambiguated english text.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>141--146</pages>
<contexts>
<context position="28554" citStr="Manshadi et al., 2011" startWordPosition="4899" endWordPosition="4902">to its sibling with a continuation dependency from the nuclear scope referent to the restrictor referent and merges the two siblings’ partitions. In this manner, all set referents in the dependency graph are eventually assembled into a single tree of continuation dependencies. 6 Evaluation This paper defines a graphical semantic representation with desirable properties for storing sentence meanings as cued associations in associative memory. In order to determine whether this representation of continuation dependencies is reliably learnable, the set of test sentences from the QuanText corpus (Manshadi et al., 2011) was automatically annotated with these continuation dependencies and evaluated against the associated set of gold-standard quantifier scopes. The sentences in this corpus were collected as descriptions of text editing tasks using unix tools like sed and awk, collected from online tutorials and from graduate students asked to write and describe example scripts. Gold-standard scoping relations in this corpus are specified over bracketed sequences of words in each sentence. For example, the sentence Print every line that starts with a number might be annotated: Print [1 every line] that starts w</context>
</contexts>
<marker>Manshadi, Allen, Swift, 2011</marker>
<rawString>Mehdi Manshadi, James F. Allen, and Mary Swift. 2011. A corpus of scope-disambiguated english text. In Proceedings of ACL, pages 141–146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehdi Manshadi</author>
<author>Daniel Gildea</author>
<author>James F Allen</author>
</authors>
<title>Plurality, negation, and quantification: Towards comprehensive quantifier scope disambiguation.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>64--72</pages>
<contexts>
<context position="9571" citStr="Manshadi et al., 2013" startWordPosition="1565" endWordPosition="1568">he inside-out structure of function definitions in a continuation-passing style (Barker, 2002; Shan and Barker, 2006)2 so that sets are defined in terms of their context, and explicit ‘And’ predicates are no longer required, leaving nothing to get in the way of an exact pattern match.3 The learnability of the non-local continuation dependencies involved in this representation is then evaluated on an existing quantifier scope disambiguation task using a dependency-based statistical scope resolver, with results comparable to a state-of-the-art unrestricted graph-based quantifier scope resolver (Manshadi et al., 2013). 2 Continuation Dependencies This paper explores the use of a bottom-up dependency representation, inspired by the inside-out structure of function definitions in a continuationpassing style (Barker, 2002; Shan and Barker, 2006), which creates discourse referents for sets that are associated with particular scoping contexts. This dependency representation preserves the propositions, sets, eventualities, and ordinary 2This representation also has much in common with generalized Skolem terms of Steedman (2012), which also represent dependencies to outscoped terms, but here continuation dependen</context>
<context position="12551" citStr="Manshadi et al., 2013" startWordPosition="2041" endWordPosition="2044">ber of conjuncts can impinge upon a common outscoping continuation, so there is no longer any need for explicit conjunction nodes. The representation is also attractive in that it locally distinguishes queries about, say, the cardinality of the set of numbers in each document line (Set sN dN sL) from queries about the cardinality of the set of numbers in general (Set sN dN s&apos;) which is crucial for successful inference by pattern matching. Finally, connected sets of continuation dependencies form natural ‘scope graphs’ for use in graph-based disambiguation algorithms (Manshadi and Allen, 2011; Manshadi et al., 2013), which will be used to evaluate this representation in Section 6. 3 Mapping to Lambda Calculus It is important for this representation not only to have attractive graphical subsumption properties, but also to be sufficiently expressive to define corresponding expressions in lambda calculus. When continuation dependencies are filled in, the resulting dependency structure can be trans143 a) eL Every A 0 1 dL sL 0 pL 1 s&apos; L 0 1 A dL&apos; 2 eN Two A 2 0 dN sN 0 pN 1 1 A 0 d&apos; N 2 s&apos; N 1 Some 2 A 0 eC sC 0 pC 1 1 A 0 2 1 e&apos; C s&apos; C 1 2 1 Line Number Contain 1 0 b) eL 0 Every A 1 0 1 dL sL 1 pL s&apos; L 0 1 </context>
<context position="30360" citStr="Manshadi et al. (2013)" startWordPosition="5187" endWordPosition="5190">dard scoping relations are considered successfully recalled if a restrictor (f1 (f1 i)) or nuclear scope (f2 (f1 i)) referent of any lexical item i within the outscoped span is connected by a sequence of continuation dependencies (in the appropriate direction) to any restrictor or nuclear scope referent of any lexical item within the outscoping span. First, the algorithm was run without any lexicalization on the 94 non-duplicate sentences of the QuanText test set. Results of this evaluation are shown in the third line of Table 1 using the persentence complete recall accuracy (‘AR’) defined by Manshadi et al. (2013). The algorithm was then run using bilexical weights based on the frequencies ˜F(h, h&apos;) with which a word h&apos; occurs as a head of a category outscoped by a category headed by word h in the 350-sentence training set of the QuanText corpus. For example, since quantifiers over lines are often outscoped by quantifiers over files in the training data, the system learns to rank continuation dependencies to referents associated with the word lines ahead of continuation dependencies to referents associated with the word files in bottomup inference. These lexical features may be particularly helpful bec</context>
<context position="31753" citStr="Manshadi et al. (2013)" startWordPosition="5398" endWordPosition="5401">le 1. This increase is statistically significant (p = 0.001 by two-tailed McNemar’s test). This significance for local head-word features on continuation dependencies shows that these dependencies can be reliably learned from training examples, and suggests that continuation dependencies may be a natural representation for scope information. Interestingly, effects of lexical features for quantifiers (the word each, or definite/indefinite distinctions) were not substantial or statistically significant, despite the relatively high frequencies 148 System AR Manshadi and Allen (2011) baseline 63% Manshadi et al. (2013) 72% This system, w/o lexicalized model 61% This system, w. lexicalized model 72% Table 1: Per-sentence complete recall accuracy (‘AR’) of tree-based algorithm as compared to Manshadi and Allen (2011) and Manshadi et al. (2013) on explicit NP chunks in the QuanText test set, correcting for use of gold standard trees as described in footnote 19 of Manshadi et al. (2013). of the words each and the in the test corpus (occurring in 16% and 68% of test sentences, respectively), which suggests that these words may often be redundant with syntactic and head-word constraints. Results using preferences</context>
</contexts>
<marker>Manshadi, Gildea, Allen, 2013</marker>
<rawString>Mehdi Manshadi, Daniel Gildea, and James F. Allen. 2013. Plurality, negation, and quantification: Towards comprehensive quantifier scope disambiguation. In Proceedings of ACL, pages 64–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Marr</author>
</authors>
<title>Simple memory: A theory for archicortex.</title>
<date>1971</date>
<journal>Philosophical Transactions of the Royal Society (London) B,</journal>
<pages>262--23</pages>
<contexts>
<context position="1517" citStr="Marr, 1971" startWordPosition="213" endWordPosition="214">stablished that at least shallow semantic interpretation informs parsing decisions in human sentence processing (Tanenhaus et al., 1995; Brown-Schmidt et al., 2002), and recent evidence points to incremental processing of quantifier implicatures as well (Degen and Tanenhaus, 2011). This may indicate that inferences about the meaning of quantifiers are processed directly in working memory. Human working memory is widely assumed to store events (including linguistic events) as re-usable activation-based states, connected by a durable but rapidly mutable weight-based memory of cued associations (Marr, 1971; Anderson et al., 1977; Murdock, 1982; McClelland et al., 1995; Howard and Kahana, 2002). Complex dependency structures can therefore be stored in this associative memory as graphs, with states as vertices and cued associations as directed edges (e.g. Kintsch, 1988). This kind of representation is necessary to formulate and evaluate algorithmic claims (Marr, 1982) about cued associations and working memory use in human sentence processing (e.g. van Schijndel and Schuler, 2013). But accounting for syntax and semantics in this way must be done carefully in order to preserve linguistically impor</context>
</contexts>
<marker>Marr, 1971</marker>
<rawString>David Marr. 1971. Simple memory: A theory for archicortex. Philosophical Transactions of the Royal Society (London) B, 262:23–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Marr</author>
</authors>
<date>1982</date>
<booktitle>Vision. A Computational Investigation into the Human Representation and Processing of Visual Information. W.H.</booktitle>
<publisher>Freeman and Company.</publisher>
<contexts>
<context position="1884" citStr="Marr, 1982" startWordPosition="270" endWordPosition="271">rectly in working memory. Human working memory is widely assumed to store events (including linguistic events) as re-usable activation-based states, connected by a durable but rapidly mutable weight-based memory of cued associations (Marr, 1971; Anderson et al., 1977; Murdock, 1982; McClelland et al., 1995; Howard and Kahana, 2002). Complex dependency structures can therefore be stored in this associative memory as graphs, with states as vertices and cued associations as directed edges (e.g. Kintsch, 1988). This kind of representation is necessary to formulate and evaluate algorithmic claims (Marr, 1982) about cued associations and working memory use in human sentence processing (e.g. van Schijndel and Schuler, 2013). But accounting for syntax and semantics in this way must be done carefully in order to preserve linguistically important distinctions. For example, positing spurious local dependencies in filler-gap constructions can lead to missed integrations of dependency structure in incremental processing, resulting in weaker model fitting (van Schijndel et al., 2013). Similar care may be necessary in cases of dependencies arising from anaphoric coreference or quantifier scope. Unfortunatel</context>
</contexts>
<marker>Marr, 1982</marker>
<rawString>David Marr. 1982. Vision. A Computational Investigation into the Human Representation and Processing of Visual Information. W.H. Freeman and Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L McClelland</author>
<author>B L McNaughton</author>
<author>R C O’Reilly</author>
</authors>
<title>Why there are complementary learning systems in the hippocampus and neocortex: Insights from the successes and failures of connectionist models of learning and memory.</title>
<date>1995</date>
<journal>Psychological Review,</journal>
<pages>102--419</pages>
<marker>McClelland, McNaughton, O’Reilly, 1995</marker>
<rawString>J. L. McClelland, B. L. McNaughton, and R. C. O’Reilly. 1995. Why there are complementary learning systems in the hippocampus and neocortex: Insights from the successes and failures of connectionist models of learning and memory. Psychological Review, 102:419–457.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Montague</author>
</authors>
<title>The proper treatment of quantification in ordinary English.</title>
<date>1973</date>
<booktitle>Approaches to Natural Langauge,</booktitle>
<pages>221--242</pages>
<editor>In J. Hintikka, J.M.E. Moravcsik, and P. Suppes, editors,</editor>
<publisher>Yale University Press,</publisher>
<contexts>
<context position="2552" citStr="Montague, 1973" startWordPosition="365" endWordPosition="366">an sentence processing (e.g. van Schijndel and Schuler, 2013). But accounting for syntax and semantics in this way must be done carefully in order to preserve linguistically important distinctions. For example, positing spurious local dependencies in filler-gap constructions can lead to missed integrations of dependency structure in incremental processing, resulting in weaker model fitting (van Schijndel et al., 2013). Similar care may be necessary in cases of dependencies arising from anaphoric coreference or quantifier scope. Unfortunately, most existing theories of compositional semantics (Montague, 1973; Barwise and Cooper, 1981; Bos, 1996; Baldridge and Kruijff, 2002; Koller, 2004; Copestake et al., 2005) are defined at the computational level (Marr, 1982), employing beta reduction over complete or underspecified lambda calculus expressions as a precise description of the language processing task to be modeled, not at the algorithmic level, as a model of human language processing itself. The structured expressions these theories generate are not intended to represent re-usable referential states of the sort that could be modeled in current theories of associative memory. As such, it should </context>
</contexts>
<marker>Montague, 1973</marker>
<rawString>Richard Montague. 1973. The proper treatment of quantification in ordinary English. In J. Hintikka, J.M.E. Moravcsik, and P. Suppes, editors, Approaches to Natural Langauge, pages 221–242. D. Riedel, Dordrecht. Reprinted in R. H. Thomason ed., Formal Philosophy, Yale University Press, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B B Murdock</author>
</authors>
<title>A theory for the storage and retrieval of item and associative information.</title>
<date>1982</date>
<journal>Psychological Review,</journal>
<pages>89--609</pages>
<contexts>
<context position="1555" citStr="Murdock, 1982" startWordPosition="219" endWordPosition="220">mantic interpretation informs parsing decisions in human sentence processing (Tanenhaus et al., 1995; Brown-Schmidt et al., 2002), and recent evidence points to incremental processing of quantifier implicatures as well (Degen and Tanenhaus, 2011). This may indicate that inferences about the meaning of quantifiers are processed directly in working memory. Human working memory is widely assumed to store events (including linguistic events) as re-usable activation-based states, connected by a durable but rapidly mutable weight-based memory of cued associations (Marr, 1971; Anderson et al., 1977; Murdock, 1982; McClelland et al., 1995; Howard and Kahana, 2002). Complex dependency structures can therefore be stored in this associative memory as graphs, with states as vertices and cued associations as directed edges (e.g. Kintsch, 1988). This kind of representation is necessary to formulate and evaluate algorithmic claims (Marr, 1982) about cued associations and working memory use in human sentence processing (e.g. van Schijndel and Schuler, 2013). But accounting for syntax and semantics in this way must be done carefully in order to preserve linguistically important distinctions. For example, positi</context>
</contexts>
<marker>Murdock, 1982</marker>
<rawString>B.B. Murdock. 1982. A theory for the storage and retrieval of item and associative information. Psychological Review, 89:609–626.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luan Nguyen</author>
<author>Marten van Schijndel</author>
<author>William Schuler</author>
</authors>
<title>Accurate unbounded dependency recovery using generalized categorial grammars.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics (COLING ’12),</booktitle>
<pages>2125--2140</pages>
<location>Mumbai, India.</location>
<marker>Nguyen, van Schijndel, Schuler, 2012</marker>
<rawString>Luan Nguyen, Marten van Schijndel, and William Schuler. 2012. Accurate unbounded dependency recovery using generalized categorial grammars. In Proceedings of the 24th International Conference on Computational Linguistics (COLING ’12), pages 2125–2140, Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard T Oehrle</author>
</authors>
<title>Term-labeled categorial type systems.</title>
<date>1994</date>
<journal>Linguistics and Philosophy,</journal>
<volume>17</volume>
<issue>6</issue>
<pages>678</pages>
<contexts>
<context position="16125" citStr="Oehrle, 1994" startWordPosition="2793" endWordPosition="2794">alculus expression (including quantifiers over eventualities in the source graph, to eliminate unbound variables): p E P, (f&apos;.. i&apos;..) 0 Γ, (f &apos;&apos;.. i&apos;&apos;..) Γ. 144 (EVERY (λdLSomE (λeLLiNE eL dL)) (λd0 LTwo (λdNSomE (λeNNumBER eN dN)) (λd0NSomE (λeCCoNTMN eC d0 L d0N)))) 4 Derivation of Syntactic and Semantic Dependencies The semantic dependency representation defined in this paper assumes semantic dependencies other than those representing continuations are derived compositionally by a categorial grammar. In particular, this definition assumes a Generalized Categorial Grammar (GCG) (Bach, 1981; Oehrle, 1994), because it can be used to distinguish argument and modifier compositions (from which restrictor and nuclear scope sets are derived in a treestructured continuation graph), and because large GCG-annotated corpora defined with this distinction are readily available (Nguyen et al., 2012). GCG category types c ∈ C each consist of a primitive category type u ∈ U, typically labeled with the part of speech of the head of a category (e.g. V, N, A, etc., for phrases or clauses headed by verbs, nouns, adjectives, etc.), followed by one or more unsatisfied dependencies, each consisting of an operator o</context>
</contexts>
<marker>Oehrle, 1994</marker>
<rawString>Richard T. Oehrle. 1994. Term-labeled categorial type systems. Linguistics and Philosophy, 17(6):633– 678.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terence Parsons</author>
</authors>
<title>Events in the Semantics of English.</title>
<date>1990</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="19369" citStr="Parsons, 1990" startWordPosition="3321" endWordPosition="3322">ciated with the first argument of an eventuality, the referent of a direct object associated with the second argument, and so on, for all verb forms other than passive verbs. In the case of passive verbs, the referent of a subject is associated with the second argument of an eventuality, the referent of a direct object associated with the third argument, and so on. In order to have a consistent treatment of argument and modifier attachment across all category types, and also in order to model referents of verbs as eventualities which can be quantified by adverbs like never, once, twice, etc. (Parsons, 1990), it is desirable for eventualities associated with verbs to also be quantified. Outgoing semantic dependencies to arguments of eventualities are then applied as constraints to the discourse referent variable of the restrictor sets of these quantifiers. Incoming dependencies to eventualities and other discourse referents used as modificands of modifiers are also applied as constraints to discourse referent variables of restrictor sets, but incoming dependencies to discourse referents used as arguments of predicates are applied as constraints to discourse referent variables of nuclear scope set</context>
</contexts>
<marker>Parsons, 1990</marker>
<rawString>Terence Parsons. 1990. Events in the Semantics of English. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Schuler</author>
</authors>
<title>Sentence processing in a vectorial model of working memory.</title>
<date>2014</date>
<booktitle>In Fifth Annual Workshop on Cognitive Modeling and Computational Linguistics (CMCL</booktitle>
<contexts>
<context position="26940" citStr="Schuler (2014)" startWordPosition="4645" endWordPosition="4646">e) and target (destination) referents have been hypothesized. For example, upon processing the word numbers in the sentence Every line contains two numbers, a continuation dependency may be stored from the nuclear scope set associated with this word to the nuclear scope set of the subject every line, forming an in-situ interpretation with some amount of activation (see Figure 4), and with some (probably smaller) amount of activation, a continuation dependency may be stored from the nuclear scope set of this subject to the nuclear scope set of this word, forming an inverted interpretation. See Schuler (2014) for a model of how sentence processing in associative memory might incrementally store dependencies like these as cued associations. Mb 147 the dependency graph, then begins connecting it, selecting the highest-ranked referent of that partition that is not yet attached and designating it as the new highest-scoping referent in that partition, attaching it as the context of the previously highest-scoping referent in that partition if one exists. This proceeds until: 1. the algorithm reaches a restrictor or nuclear scope referent with a sibling (superset or subset) nuclear scope or restrictor re</context>
</contexts>
<marker>Schuler, 2014</marker>
<rawString>William Schuler. 2014. Sentence processing in a vectorial model of working memory. In Fifth Annual Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2014).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chung-chieh Shan</author>
<author>Chris Barker</author>
</authors>
<title>Explaining crossover and superiority as left-to-right evaluation. Linguistics and Philosophy,</title>
<date>2006</date>
<pages>29--91</pages>
<contexts>
<context position="9066" citStr="Shan and Barker, 2006" startWordPosition="1488" endWordPosition="1491"> not the only way to represent compositional semantics of sentences. This paper defines a graphical semantic dependency representation that can be translated into lambda calculus, but has the important property that its vertices define a usable set of discourse referents in working memory even in contexts involving conjunction in the scope of quantifiers. It does this by reversing the direction of dependencies from parent-to-child subsumption in a lambda-calculus tree to a representation similar to the inside-out structure of function definitions in a continuation-passing style (Barker, 2002; Shan and Barker, 2006)2 so that sets are defined in terms of their context, and explicit ‘And’ predicates are no longer required, leaving nothing to get in the way of an exact pattern match.3 The learnability of the non-local continuation dependencies involved in this representation is then evaluated on an existing quantifier scope disambiguation task using a dependency-based statistical scope resolver, with results comparable to a state-of-the-art unrestricted graph-based quantifier scope resolver (Manshadi et al., 2013). 2 Continuation Dependencies This paper explores the use of a bottom-up dependency representat</context>
</contexts>
<marker>Shan, Barker, 2006</marker>
<rawString>Chung-chieh Shan and Chris Barker. 2006. Explaining crossover and superiority as left-to-right evaluation. Linguistics and Philosophy, 29:91–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The syntactic process.</title>
<date>2000</date>
<publisher>MIT Press/Bradford Books,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="18220" citStr="Steedman, 2000" startWordPosition="3128" endWordPosition="3129"> is necessary to consistently apply predicate-argument constraints to discourse referents in the nuclear scope set of a quantifier and modifier-modificand constraints to discourse referents in the restrictor set of a quantifier. For example, in Sentence 1: (1) Everything is [A-aN open]. the predicate open constrains the nuclear scope set of every, but in Sentence 2: (2) Everything [A-aN open] is finished. the predicate open constrains the restrictor set. These constraints can be consistently applied in the argument and modifier attachment rules of a GCG. Like a Combinatory Categorial Grammar (Steedman, 2000), a GCG defines syntactic dependencies for compositions that are determined by the number and kind of unsatisfied dependencies of the composed category types. These are similar to dependencies for subject, direct object, preposition complement, etc., of Stanford dependencies (de Marneffe et al., 2006), but are reduced to numbers based on the order of the associated dependencies in the category type of the lexical head. These syntactic dependencies are then associated with semantic dependencies, with the referent of a subject associated with the first argument of an eventuality, the referent of</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The syntactic process. MIT Press/Bradford Books, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>Taking Scope - The Natural Semantics of Quantifiers.</title>
<date>2012</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="10085" citStr="Steedman (2012)" startWordPosition="1639" endWordPosition="1640">able to a state-of-the-art unrestricted graph-based quantifier scope resolver (Manshadi et al., 2013). 2 Continuation Dependencies This paper explores the use of a bottom-up dependency representation, inspired by the inside-out structure of function definitions in a continuationpassing style (Barker, 2002; Shan and Barker, 2006), which creates discourse referents for sets that are associated with particular scoping contexts. This dependency representation preserves the propositions, sets, eventualities, and ordinary 2This representation also has much in common with generalized Skolem terms of Steedman (2012), which also represent dependencies to outscoped terms, but here continuation dependencies are applied to all quantifiers, including universals. 3This also holds for explicit disjunction predicates, which can be cast as conjunction through application of de Morgan’s law and manipulation of the polarity of adjacent quantifiers. For example, Every line begins with at least one space or contains at least two numbers, is equivalent to No line begins with fewer than one space and contains fewer than two numbers. discourse referents of a ‘direct’ representation (the p, s, e, and d nodes in Figure 1)</context>
</contexts>
<marker>Steedman, 2012</marker>
<rawString>Mark Steedman. 2012. Taking Scope - The Natural Semantics of Quantifiers. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael K Tanenhaus</author>
<author>Michael J Spivey-Knowlton</author>
<author>Kathy M Eberhard</author>
<author>Julie E Sedivy</author>
</authors>
<title>Integration of visual and linguistic information in spoken language comprehension.</title>
<date>1995</date>
<journal>Science,</journal>
<volume>268</volume>
<pages>1634</pages>
<contexts>
<context position="1042" citStr="Tanenhaus et al., 1995" startWordPosition="140" endWordPosition="143"> set of discourse referents in working memory even in contexts involving conjunction in the scope of quantifiers. An evaluation on an existing quantifier scope disambiguation task shows that non-local continuation dependencies can be as reliably learned from annotated data as representations used in a state-of-the-art quantifier scope resolver, suggesting that continuation dependencies may provide a natural representation for scope information. 1 Introduction It is now fairly well established that at least shallow semantic interpretation informs parsing decisions in human sentence processing (Tanenhaus et al., 1995; Brown-Schmidt et al., 2002), and recent evidence points to incremental processing of quantifier implicatures as well (Degen and Tanenhaus, 2011). This may indicate that inferences about the meaning of quantifiers are processed directly in working memory. Human working memory is widely assumed to store events (including linguistic events) as re-usable activation-based states, connected by a durable but rapidly mutable weight-based memory of cued associations (Marr, 1971; Anderson et al., 1977; Murdock, 1982; McClelland et al., 1995; Howard and Kahana, 2002). Complex dependency structures can </context>
</contexts>
<marker>Tanenhaus, Spivey-Knowlton, Eberhard, Sedivy, 1995</marker>
<rawString>Michael K. Tanenhaus, Michael J. Spivey-Knowlton, Kathy M. Eberhard, and Julie E. Sedivy. 1995. Integration of visual and linguistic information in spoken language comprehension. Science, 268:1632– 1634.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marten van Schijndel</author>
<author>William Schuler</author>
</authors>
<title>An analysis of frequency- and recency-based processing costs.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT 2013. Association for Computational Linguistics.</booktitle>
<marker>van Schijndel, Schuler, 2013</marker>
<rawString>Marten van Schijndel and William Schuler. 2013. An analysis of frequency- and recency-based processing costs. In Proceedings of NAACL-HLT 2013. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marten van Schijndel</author>
<author>Luan Nguyen</author>
<author>William Schuler</author>
</authors>
<title>An analysis of memory-based processing costs using incremental deep syntactic dependency parsing.</title>
<date>2013</date>
<booktitle>In Proceedings of CMCL 2013. Association for Computational Linguistics.</booktitle>
<marker>van Schijndel, Nguyen, Schuler, 2013</marker>
<rawString>Marten van Schijndel, Luan Nguyen, and William Schuler. 2013. An analysis of memory-based processing costs using incremental deep syntactic dependency parsing. In Proceedings of CMCL 2013. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>