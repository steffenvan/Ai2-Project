<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012648">
<title confidence="0.992331">
Off-topic essay detection using short prompt texts
</title>
<author confidence="0.995084">
Annie Louis Derrick Higgins
</author>
<affiliation confidence="0.835009">
University of Pennsylvania Educational Testing Service
Philadelphia, PA 19104, USA Princeton, NJ 08541, USA
</affiliation>
<email confidence="0.998975">
lannie@seas.upenn.edu dhiggins@ets.org
</email>
<sectionHeader confidence="0.993894" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999840157894737">
Our work addresses the problem of predict-
ing whether an essay is off-topic to a given
prompt or question without any previously-
seen essays as training data. Prior work has
used similarity between essay vocabulary and
prompt words to estimate the degree of on-
topic content. In our corpus of opinion es-
says, prompts are very short, and using sim-
ilarity with such prompts to detect off-topic
essays yields error rates of about 10%. We
propose two methods to enable better compar-
ison of prompt and essay text. We automat-
ically expand short prompts before compari-
son, with words likely to appear in an essay
to that prompt. We also apply spelling correc-
tion to the essay texts. Both methods reduce
the error rates during off-topic essay detection
and turn out to be complementary, leading to
even better performance when used in unison.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999990428571429">
It is important to limit the opportunity to sub-
mit uncooperative responses to educational software
(Baker et al., 2009). We address the task of detect-
ing essays that are irrelevant to a given prompt (es-
say question) when training data is not available and
the prompt text is very short.
When example essays for a prompt are available,
they can be used to learn word patterns to distin-
guish on-topic from off-topic essays. Alternatively,
prior work (Higgins et al., 2006) has motivated us-
ing similarity between essay and prompt vocabular-
ies to detect off-topic essays. In Section 2, we exam-
ine the performance of prompt-essay comparison for
four different essay types. We show that in the case
</bodyText>
<page confidence="0.941616">
92
</page>
<bodyText confidence="0.989274692307692">
of prompts with 9 or 13 content words on average,
the error rates are higher compared to those with 60
or more content words. In addition, more errors are
observed when the method is used on essays written
by English language learners compared to more ad-
vanced test takers. An example short prompt from
our opinion essays’ corpus is shown below. Test-
takers provided arguments for/or against the opinion
expressed by the prompt.
[1] “In the past, people were more friendly than
they are today.”
To address this problem, we propose two en-
hancements. We use unsupervised methods to ex-
pand the prompt text with words likely to appear
in essays to that prompt. Our approach is based
on the intuition that regularities exist in the words
which appear in essays, beyond the prevalence of
actual prompt words. In a similar vein, misspellings
in the essays, particulary of the prompt words, are
also problematic for prompt-based methods. There-
fore we apply spelling correction to the essay text
before comparison. Our results show that both meth-
ods lower the error rates. The relative performance
of the two methods varies depending on the essay
type; however, their combination gives the overall
best results regardless of essay type.
</bodyText>
<sectionHeader confidence="0.519544" genericHeader="method">
2 Effect of prompt and essay properties
</sectionHeader>
<bodyText confidence="0.999959">
In this section, we analyze the off-topic essay pre-
diction accuracies resulting from direct comparison
of original prompt and essay texts. We use four dif-
ferent corpora of essays collected and scored during
high stakes tests with an English writing component.
They differ in task type and average prompt length,
as well as the skill level expected from the test taker.
</bodyText>
<note confidence="0.5363605">
Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 92–95,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999968933333333">
In one of the tasks, the test taker reads a passage
and listens to a lecture and then writes a summary
of the main points. For such essays, the prompt
text (reading passage plus lecture transcript) avail-
able for comparison is quite long (about 276 con-
tent words). In the other 3 tasks, the test taker has
to provide an argument for or against some opin-
ion expressed in the prompt. One of these has long
prompts (60 content words). The other two involve
only single sentence prompts as in example [1] and
have 13 and 9 content words on average. Two of
these tasks focused on English language learners and
the other two involved advanced users (applicants to
graduate study programs in the U.S.). See Table 1
for a summary of the essay types.1
</bodyText>
<subsectionHeader confidence="0.939295">
2.1 Data
</subsectionHeader>
<bodyText confidence="0.999835769230769">
For each of the task types described above, our cor-
pus contains essays written to 10 different prompts.
We used essays to 3 prompts as development data.
To build an evaluation test set, we randomly sam-
pled 350 essays for each of the 7 remaining prompts
to use as positive examples. It is difficult to as-
semble a sufficient number of naturally-occurring
off-topic essays for testing. However, an essay on-
topic to a particular prompt can be considered as
pseudo off-topic to a different prompt. Hence, to
complement the positive examples for each prompt,
an equal number of negative examples were chosen
at random from essays to the remaining 6 prompts.
</bodyText>
<subsectionHeader confidence="0.996347">
2.2 Experimental setup
</subsectionHeader>
<bodyText confidence="0.9999778">
We use the approach for off-topic essay detection
suggested in prior work by Higgins et al. (2006).
The method uses cosine overlap between tf*idf vec-
tors of prompt and essay content words to measure
the similarity between a prompt-essay pair.
</bodyText>
<equation confidence="0.911004333333333">
vessay�vprompt
sim(prompt, essay) = (1)
11 vessay 11 11 vprompt
</equation>
<bodyText confidence="0.999729333333333">
An essay is compared with the target prompt
(prompt with which topicality must be checked) to-
gether with a set of reference prompts, different from
the target. The reference prompts are also chosen
to be different from the actual prompts of the neg-
ative examples in our dataset. If the target prompt
</bodyText>
<footnote confidence="0.676254">
1Essay sources: Type 1-TOEFL integrated writing task,
Type 4-TOEFL independent writing task, Types 2 &amp; 3-
argument and issue tasks in Analytical Writing section of GRE
</footnote>
<table confidence="0.9991824">
Type Skill Prompt len. Avg FP Avg FN
1 Learners 276 0.73 11.79
2 Advanced 60 0.20 6.20
3 Advanced 13 2.94 8.90
4 Learners 9 9.73 11.07
</table>
<tableCaption confidence="0.943865">
Table 1: Effect of essay types: average prompt length,
false positive and false negative rates
</tableCaption>
<bodyText confidence="0.984896090909091">
is ranked as most similar2 in the list of compared
prompts, the essay is classified as on-topic. 9 refer-
ence prompts were used in our experiments.
We compute two error rates.
FALSE POSITIVE - percentage of on-topic essays in-
correctly flagged as off-topic.
FALSE NEGATIVE - percentage of off-topic essays
which the system failed to flag.
In this task, it is of utmost importance to maintain
very low false positive rates, as incorrect labeling of
an on-topic essay as off-topic is undesirable.
</bodyText>
<subsectionHeader confidence="0.98843">
2.3 Observations
</subsectionHeader>
<bodyText confidence="0.999991277777778">
In Table 1, we report the average false positive and
false negative rates for the 7 prompts in the test set
for each essay type. For long prompts, both Types 1
and 2, the false positive rates are very low. The clas-
sification of Type 2 essays which were also written
by advanced test takers is the most accurate.
However, for essays with shorter prompts (Types
3 and 4), the false positive rates are higher. In fact,
in the case of Type 4 essays written by English lan-
guage learners, the false positive rates are as high as
10%. Therefore we focus on improving the results
in these two cases which involve short prompts.
Both prompt length and the English proficiency
of the test taker seem to influence the prediction ac-
curacies for off-topic essay detection. In our work,
we address these two challenges by: a) automatic
expansion of short prompts (Section 3) and b) cor-
rection of spelling errors in essay texts (Section 4).
</bodyText>
<sectionHeader confidence="0.992936" genericHeader="method">
3 Prompt expansion
</sectionHeader>
<bodyText confidence="0.9997635">
We designed four automatic methods to add relevant
words to the prompt text.
</bodyText>
<footnote confidence="0.681751666666667">
2Less strict cutoffs may be used, for example, on-topic if
target prompt is within rank 3 or 5, etc. However even a cutoff
of 2 incorrectly classifies 25% of off-topic essays as on-topic.
</footnote>
<page confidence="0.996018">
93
</page>
<subsectionHeader confidence="0.502431">
3.1 Unsupervised methods
</subsectionHeader>
<bodyText confidence="0.999975411764706">
Inflected forms: Given a prompt word, “friendly”,
its morphological variants—“friend”, “friendlier”,
“friendliness”—are also likely to be used in essays
to that prompt. Inflected forms are the simplest and
most restrictive class in our set of expansions. They
were obtained by a rule-based approach (Leacock
and Chodorow, 2003) which adds/modifies prefixes
and suffixes of words to obtain inflected forms.
These rules were adapted from WordNet rules de-
signed to get the base forms of inflected words.
Synonyms: Words with the same meaning as
prompt words might also be mentioned over the
course of an essay. For example, “favorable”
and “well-disposed” are synonyms for the word
“friendly” and likely to be good expansions. We
used an in-house tool to obtain synonyms from
WordNet for each of the prompt words. The lookup
involves a word sense disambiguation step to choose
the most relevant sense for polysemous words. All
the synonyms for the chosen sense of the prompt
word are added as expansions.
Distributionally similar words: We also consider
as expansions words that appear in similar contexts
as the prompt words. For example, “cordial”, “po-
lite”, “cheerful”, “hostile”, “calm”, “lively” and
“affable” often appear in the same contexts as the
word “friendly”. Such related words form part of
a concept like ‘behavioral characteristics ofpeople’
and are likely to appear in a discussion of any one
aspect. These expansions could comprise antonyms
and other related words too. This idea of word simi-
larity was implemented in work by Lin (1998). Sim-
ilarity between two words is estimated by examin-
ing the degree of overlap of their contexts in a large
corpus. We access Lin’s similarity estimates using
a tool from Leacock and Chodorow (2003) that re-
turns words with similarity values above a cutoff.
Word association norms: Word associations have
been of great interest in psycholinguistic research.
Participants are given a target word and asked to
mention words that readily come to mind. The most
frequent among these are recorded as free associa-
tions for that target. They form another interesting
category of expansions for our purpose because they
are known to be frequently recalled by human sub-
jects for a particular stimulus word. We added the
associations for prompt words from a collection of
5000 target words with their associations produced
by about 6000 participants (Nelson et al., 1998).
Sample associations for the word “friendly” include
“smile”, “amiable”, “greet” and “mean”.
</bodyText>
<subsectionHeader confidence="0.99997">
3.2 Weighting of prompt words and expansions
</subsectionHeader>
<bodyText confidence="0.999988083333333">
After expansion, the prompt lengths vary between
87 (word associations) and 229 (distributionally
similar words) content words, considerably higher
than the original average length of 9 and 13 content
words. We use a simple weighting scheme3 to mit-
igate the influence of noisy expansions. We assign
a weight of 20 to original prompt words and 1 to all
the expansions. While computing similarity, we use
these weight values as the assumed frequency of the
word in the prompt. In this case, the term frequency
of original words is set as 20 and all expansion terms
are considered to appear once in the new prompt.
</bodyText>
<sectionHeader confidence="0.856883" genericHeader="method">
4 Spelling correction of essay text
</sectionHeader>
<bodyText confidence="0.999991722222222">
Essays written by learners of a language are prone to
spelling errors. When such errors occur in the use of
the prompt words, prompt-based techniques will fail
to identify the essay as on-topic even if it actually is.
The usefulness of expansion could also be limited
if there are several spelling errors in the essay text.
Hence we explored the correction of spelling errors
in the essay before off-topic detection.
We use a tool from Leacock and Chodorow
(2003) to perform directed spelling correction, ie.,
focusing on correcting the spellings of words most
likely to match a given target list. We use the prompt
words as the targets. We also explore the simultane-
ous use of spelling correction and expansion. We
first obtain expansion words from one of our unsu-
pervised methods. We then use these along with
the prompt words for spelling correction followed
by matching of the expanded prompt and essay text.
</bodyText>
<sectionHeader confidence="0.998768" genericHeader="evaluation">
5 Results and discussion
</sectionHeader>
<bodyText confidence="0.9989205">
We used our proposed methods on the two essay col-
lections with very short prompts, Type 3 written by
</bodyText>
<footnote confidence="0.862293666666667">
3Without any weighting there was an increase in error rates
during development tests. We also experimented with a graph-
based approach to term weighting which gave similar results.
</footnote>
<page confidence="0.999159">
94
</page>
<bodyText confidence="0.99991970212766">
advanced test takers and Type 4 written by learn-
ers of English. Table 2 compares the suggested en-
hancements with the previously proposed method by
Higgins et al. (2006). As discussed in Section 2.3,
using only the original prompt words, error rates are
around 10% for both essay types. For advanced test
takers, the false positive rates are lower, around 3%.
Usefulness of expanded prompts All the expansion
methods lower the false positive error rates on es-
says written by learners with almost no increase in
the rate of false negatives. On average, the false
positive errors are reduced by about 3%. Inflected
forms constitute the best individual expansion cat-
egory. The overall best performance on this type
of essays is obtained by combining inflected forms
with word associations.
In contrast, for essays written by advanced test
takers, inflected forms is the worst expansion cate-
gory. Here word associations give the best results
reducing both false positive and false negative er-
rors; the reduction in false positives is almost 50%.
These results suggest that advanced users of English
use more diverse vocabulary in their essays which
are best matched by word associations.
Effect of spelling correction For essays written by
learners, spell-correcting the essay text before com-
parison (Spell) leads to huge reductions in error
rates. Using only the original prompt, the false pos-
itive rate is 4% lower with spelling correction than
without. Note that this result is even better than the
best expansion technique–inflected forms. However,
for essays written by advanced users, spelling cor-
rection does not provide any benefits. This result
is expected since these test-takers are less likely to
produce many spelling errors.
Combination of methods The benefits of the two
methods appear to be population dependent. For
learners of English, a spelling correction module
is necessary while for advanced users, the benefits
are minimal. On the other hand, prompt expansion
works extremely well for essays written by advanced
users. The expansions are also useful for essays
written by learners but the benefits are lower com-
pared to spelling correction. However, for both es-
say types, the combination of spelling correction and
best prompt expansion method (Spell + best expn.)
is better compared to either of them individually.
</bodyText>
<table confidence="0.9987433">
Method Learners Advanced
FP FN FP FN
Prompt only 9.73 11.07 2.94 9.06
Synonyms 7.03 12.01 1.39 9.76
Dist. 6.45 11.77 1.63 8.98
WAN 6.33 11.97 1.59 8.74
Infl. forms 6.25 11.65 2.53 9.06
Infl. forms + WAN 6.04 11.48 - -
Spell 5.43 12.71 2.53 9.27
Spell + best expn. 4.66 11.97 1.47 9.02
</table>
<tableCaption confidence="0.987847">
Table 2: Average error rates after prompt expansion and
spelling correction
</tableCaption>
<bodyText confidence="0.9971655">
Therefore the best policy would be to use both en-
hancements together for prompt-based methods.
</bodyText>
<sectionHeader confidence="0.999702" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99994925">
We have described methods for improving the accu-
racy of off-topic essay detection for short prompts.
We showed that it is possible to predict words that
are likely to be used in an essay based on words that
appear in its prompt. By adding such words to the
prompt automatically, we built a better representa-
tion of prompt content to compare with the essay
text. The best combination included inflected forms
and word associations, reducing the false positives
by almost 4%. We also showed that spelling correc-
tion is a very useful preprocessing step before off-
topic essay detection.
</bodyText>
<sectionHeader confidence="0.999261" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9984705">
R.S.J.d. Baker, A.M.J.B. de Carvalho, J. Raspat,
V. Aleven, A.T. Corbett, and K.R. Koedinger. 2009.
Educational software features that encourage and dis-
courage “gaming the system”. In Proceedings of the
International Conference on Artificial Intelligence in
Education.
D. Higgins, J. Burstein, and Y. Attali. 2006. Identifying
off-topic student essays without topic-specific training
data. Natural Language Engineering, 12(2):145–159.
C. Leacock and M. Chodorow. 2003. C-rater: Auto-
mated scoring of short-answer questions. Computers
and the Humanities, 37(4):389–405.
D. Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In COLING-ACL, pages 768–774.
D. L Nelson, C. L. McEvoy, and T. A. Schreiber.
1998. The University of South Florida word
association, rhyme, and word fragment norms,
http://www.usf.edu/FreeAssociation/.
</reference>
<page confidence="0.999067">
95
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.964461">
<title confidence="0.993755">Off-topic essay detection using short prompt texts</title>
<author confidence="0.999463">Annie Louis Derrick Higgins</author>
<affiliation confidence="0.99982">University of Pennsylvania Educational Testing Service</affiliation>
<address confidence="0.992335">Philadelphia, PA 19104, USA Princeton, NJ 08541, USA</address>
<email confidence="0.995267">lannie@seas.upenn.edudhiggins@ets.org</email>
<abstract confidence="0.99915175">Our work addresses the problem of predicting whether an essay is off-topic to a given or question previouslyseen essays as training data. Prior work has used similarity between essay vocabulary and prompt words to estimate the degree of ontopic content. In our corpus of opinion essays, prompts are very short, and using similarity with such prompts to detect off-topic essays yields error rates of about 10%. We propose two methods to enable better comparison of prompt and essay text. We automatically expand short prompts before comparison, with words likely to appear in an essay to that prompt. We also apply spelling correction to the essay texts. Both methods reduce the error rates during off-topic essay detection and turn out to be complementary, leading to even better performance when used in unison.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R S J d Baker</author>
<author>A M J B de Carvalho</author>
<author>J Raspat</author>
<author>V Aleven</author>
<author>A T Corbett</author>
<author>K R Koedinger</author>
</authors>
<title>Educational software features that encourage and discourage “gaming the system”.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Conference on Artificial Intelligence in Education.</booktitle>
<marker>Baker, de Carvalho, Raspat, Aleven, Corbett, Koedinger, 2009</marker>
<rawString>R.S.J.d. Baker, A.M.J.B. de Carvalho, J. Raspat, V. Aleven, A.T. Corbett, and K.R. Koedinger. 2009. Educational software features that encourage and discourage “gaming the system”. In Proceedings of the International Conference on Artificial Intelligence in Education.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Higgins</author>
<author>J Burstein</author>
<author>Y Attali</author>
</authors>
<title>Identifying off-topic student essays without topic-specific training data.</title>
<date>2006</date>
<journal>Natural Language Engineering,</journal>
<volume>12</volume>
<issue>2</issue>
<contexts>
<context position="1552" citStr="Higgins et al., 2006" startWordPosition="249" endWordPosition="252"> error rates during off-topic essay detection and turn out to be complementary, leading to even better performance when used in unison. 1 Introduction It is important to limit the opportunity to submit uncooperative responses to educational software (Baker et al., 2009). We address the task of detecting essays that are irrelevant to a given prompt (essay question) when training data is not available and the prompt text is very short. When example essays for a prompt are available, they can be used to learn word patterns to distinguish on-topic from off-topic essays. Alternatively, prior work (Higgins et al., 2006) has motivated using similarity between essay and prompt vocabularies to detect off-topic essays. In Section 2, we examine the performance of prompt-essay comparison for four different essay types. We show that in the case 92 of prompts with 9 or 13 content words on average, the error rates are higher compared to those with 60 or more content words. In addition, more errors are observed when the method is used on essays written by English language learners compared to more advanced test takers. An example short prompt from our opinion essays’ corpus is shown below. Testtakers provided argument</context>
<context position="5149" citStr="Higgins et al. (2006)" startWordPosition="857" endWordPosition="860"> To build an evaluation test set, we randomly sampled 350 essays for each of the 7 remaining prompts to use as positive examples. It is difficult to assemble a sufficient number of naturally-occurring off-topic essays for testing. However, an essay ontopic to a particular prompt can be considered as pseudo off-topic to a different prompt. Hence, to complement the positive examples for each prompt, an equal number of negative examples were chosen at random from essays to the remaining 6 prompts. 2.2 Experimental setup We use the approach for off-topic essay detection suggested in prior work by Higgins et al. (2006). The method uses cosine overlap between tf*idf vectors of prompt and essay content words to measure the similarity between a prompt-essay pair. vessay�vprompt sim(prompt, essay) = (1) 11 vessay 11 11 vprompt An essay is compared with the target prompt (prompt with which topicality must be checked) together with a set of reference prompts, different from the target. The reference prompts are also chosen to be different from the actual prompts of the negative examples in our dataset. If the target prompt 1Essay sources: Type 1-TOEFL integrated writing task, Type 4-TOEFL independent writing task</context>
<context position="12392" citStr="Higgins et al. (2006)" startWordPosition="2062" endWordPosition="2065">ised methods. We then use these along with the prompt words for spelling correction followed by matching of the expanded prompt and essay text. 5 Results and discussion We used our proposed methods on the two essay collections with very short prompts, Type 3 written by 3Without any weighting there was an increase in error rates during development tests. We also experimented with a graphbased approach to term weighting which gave similar results. 94 advanced test takers and Type 4 written by learners of English. Table 2 compares the suggested enhancements with the previously proposed method by Higgins et al. (2006). As discussed in Section 2.3, using only the original prompt words, error rates are around 10% for both essay types. For advanced test takers, the false positive rates are lower, around 3%. Usefulness of expanded prompts All the expansion methods lower the false positive error rates on essays written by learners with almost no increase in the rate of false negatives. On average, the false positive errors are reduced by about 3%. Inflected forms constitute the best individual expansion category. The overall best performance on this type of essays is obtained by combining inflected forms with w</context>
</contexts>
<marker>Higgins, Burstein, Attali, 2006</marker>
<rawString>D. Higgins, J. Burstein, and Y. Attali. 2006. Identifying off-topic student essays without topic-specific training data. Natural Language Engineering, 12(2):145–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>M Chodorow</author>
</authors>
<title>C-rater: Automated scoring of short-answer questions.</title>
<date>2003</date>
<journal>Computers and the Humanities,</journal>
<volume>37</volume>
<issue>4</issue>
<contexts>
<context position="8131" citStr="Leacock and Chodorow, 2003" startWordPosition="1360" endWordPosition="1363">ansion We designed four automatic methods to add relevant words to the prompt text. 2Less strict cutoffs may be used, for example, on-topic if target prompt is within rank 3 or 5, etc. However even a cutoff of 2 incorrectly classifies 25% of off-topic essays as on-topic. 93 3.1 Unsupervised methods Inflected forms: Given a prompt word, “friendly”, its morphological variants—“friend”, “friendlier”, “friendliness”—are also likely to be used in essays to that prompt. Inflected forms are the simplest and most restrictive class in our set of expansions. They were obtained by a rule-based approach (Leacock and Chodorow, 2003) which adds/modifies prefixes and suffixes of words to obtain inflected forms. These rules were adapted from WordNet rules designed to get the base forms of inflected words. Synonyms: Words with the same meaning as prompt words might also be mentioned over the course of an essay. For example, “favorable” and “well-disposed” are synonyms for the word “friendly” and likely to be good expansions. We used an in-house tool to obtain synonyms from WordNet for each of the prompt words. The lookup involves a word sense disambiguation step to choose the most relevant sense for polysemous words. All the</context>
<context position="9553" citStr="Leacock and Chodorow (2003)" startWordPosition="1592" endWordPosition="1595">. For example, “cordial”, “polite”, “cheerful”, “hostile”, “calm”, “lively” and “affable” often appear in the same contexts as the word “friendly”. Such related words form part of a concept like ‘behavioral characteristics ofpeople’ and are likely to appear in a discussion of any one aspect. These expansions could comprise antonyms and other related words too. This idea of word similarity was implemented in work by Lin (1998). Similarity between two words is estimated by examining the degree of overlap of their contexts in a large corpus. We access Lin’s similarity estimates using a tool from Leacock and Chodorow (2003) that returns words with similarity values above a cutoff. Word association norms: Word associations have been of great interest in psycholinguistic research. Participants are given a target word and asked to mention words that readily come to mind. The most frequent among these are recorded as free associations for that target. They form another interesting category of expansions for our purpose because they are known to be frequently recalled by human subjects for a particular stimulus word. We added the associations for prompt words from a collection of 5000 target words with their associat</context>
<context position="11465" citStr="Leacock and Chodorow (2003)" startWordPosition="1906" endWordPosition="1909">ase, the term frequency of original words is set as 20 and all expansion terms are considered to appear once in the new prompt. 4 Spelling correction of essay text Essays written by learners of a language are prone to spelling errors. When such errors occur in the use of the prompt words, prompt-based techniques will fail to identify the essay as on-topic even if it actually is. The usefulness of expansion could also be limited if there are several spelling errors in the essay text. Hence we explored the correction of spelling errors in the essay before off-topic detection. We use a tool from Leacock and Chodorow (2003) to perform directed spelling correction, ie., focusing on correcting the spellings of words most likely to match a given target list. We use the prompt words as the targets. We also explore the simultaneous use of spelling correction and expansion. We first obtain expansion words from one of our unsupervised methods. We then use these along with the prompt words for spelling correction followed by matching of the expanded prompt and essay text. 5 Results and discussion We used our proposed methods on the two essay collections with very short prompts, Type 3 written by 3Without any weighting t</context>
</contexts>
<marker>Leacock, Chodorow, 2003</marker>
<rawString>C. Leacock and M. Chodorow. 2003. C-rater: Automated scoring of short-answer questions. Computers and the Humanities, 37(4):389–405.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In COLING-ACL,</booktitle>
<pages>768--774</pages>
<location>http://www.usf.edu/FreeAssociation/.</location>
<contexts>
<context position="9355" citStr="Lin (1998)" startWordPosition="1560" endWordPosition="1561">the chosen sense of the prompt word are added as expansions. Distributionally similar words: We also consider as expansions words that appear in similar contexts as the prompt words. For example, “cordial”, “polite”, “cheerful”, “hostile”, “calm”, “lively” and “affable” often appear in the same contexts as the word “friendly”. Such related words form part of a concept like ‘behavioral characteristics ofpeople’ and are likely to appear in a discussion of any one aspect. These expansions could comprise antonyms and other related words too. This idea of word similarity was implemented in work by Lin (1998). Similarity between two words is estimated by examining the degree of overlap of their contexts in a large corpus. We access Lin’s similarity estimates using a tool from Leacock and Chodorow (2003) that returns words with similarity values above a cutoff. Word association norms: Word associations have been of great interest in psycholinguistic research. Participants are given a target word and asked to mention words that readily come to mind. The most frequent among these are recorded as free associations for that target. They form another interesting category of expansions for our purpose be</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998. Automatic retrieval and clustering of similar words. In COLING-ACL, pages 768–774. D. L Nelson, C. L. McEvoy, and T. A. Schreiber. 1998. The University of South Florida word association, rhyme, and word fragment norms, http://www.usf.edu/FreeAssociation/.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>