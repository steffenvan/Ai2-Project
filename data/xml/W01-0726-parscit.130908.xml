<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.021489">
<title confidence="0.956686">
Boosting Trees for Clause Splitting
</title>
<author confidence="0.839697">
Xavier Carreras and Lluis Marquez
</author>
<affiliation confidence="0.681358">
TALP Research Center
</affiliation>
<address confidence="0.591272666666667">
LSI Department
Universitat Politecnica de Catalunya (UPC)
Jordi Girona Salgado 1-3. E-08034 Barcelona. Catalonia.
</address>
<email confidence="0.991962">
fcarreras,lluisml @lsi.upc.es
</email>
<sectionHeader confidence="0.999338" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99998675">
We present a system for the CoNLL-2001 shared
task: the clause splitting problem. Our ap-
proach consists in decomposing the clause split-
ting problem into a combination of binary &amp;quot;sim-
ple&amp;quot; decisions, which we solve with the Ada-
Boost learning algorithm. The whole problem
is decomposed in two levels, with two chained
decisions per level. The first level corresponds
to parts 1 and 2 presented in the introductory
document for the task. The second level corre-
sponds to the part 3, which we decompose in
two decisions and a combination procedure.
</bodyText>
<sectionHeader confidence="0.976722" genericHeader="keywords">
2 System Architecture
</sectionHeader>
<bodyText confidence="0.999799861111111">
AdaBoost (Freund and Schapire, 1997) is a gen-
eral method for obtaining a highly accurate clas-
sification rule by combining many weak classi-
fiers, each of which may be only moderately
accurate. In designing our system, a general-
ized version of the AdaBoost algorithm has been
used AdaBoost.MH, (Schapire and Singer,
1999), which works with very simple domain
partitioning weak hypotheses (decision stumps)
with confidence rated predictions.
This particular boosting algorithm is able to
work efficiently in very high dimensional feature
spaces, and has been applied, with significant
success, to a number of NLP disambiguation
tasks, such as: POS tagging, PP—attachment
disambiguation, text categorization, and word
sense disambiguation.
In our particular setting, weak rules are ex-
tended to arbitrarily deep decision trees fol-
lowing the suggestion of (Schapire and Singer,
1999) and the definition presented in (Carreras
and Marquez, 2001). These more complex weak
rules allow the algorithm to work in a higher di-
mensional feature space that contains conjunc-
tions of simple features, and this fact has turned
out to be crucial for improving results in the
present domain.
As a general guideline for all the parts, our
approach consists in giving the learning algo-
rithm a very large number of binary simple fea-
tures, assuming that the relevant features will
be identified for constructing the classifiers. In
section 2.1 the type of features used in the sys-
tem are described. The remaining sections ex-
plain how the specific parts of the problem are
solved.
</bodyText>
<subsectionHeader confidence="0.98818">
2.1 Type of features
</subsectionHeader>
<bodyText confidence="0.999324833333333">
In the system, 4 types of features are used,
which are the following. In the notation, in-
dexes relative to the sequence of words are
marked with the subscript w and indexes rela-
tive to the sequence of chunks are marked with
the subscript c.
</bodyText>
<listItem confidence="0.995873052631579">
• Word Window. A word window of con-
text size n centered in position i„, is the
sequence of words in the sentence placed
at i„, + jw positions, with jw E [—n, +n].
For each word in the window, its part-of-
speech (POS) together with its relative po-
sition jw forms a feature; for words whose
POS is CC, DT, EX, IN, MD, PDT, V* or W* the
word form with its relative position jw is
also a feature.
• Chunk window. A chunk window of con-
text size n centered in position i, is the se-
quence of chunks in the sentence placed at
ic + j, positions, with j, E [—n, +n]. The
chunk window features are the tags of the
chunks in the window together with the rel-
ative position j,.
• Sentence patterns. A pattern represents
the structure of the sentence which is rel-
</listItem>
<bodyText confidence="0.9963509">
evant for distinguishing clauses. The fol-
lowing elements are considered: a) Punc-
tuation marks ( &amp;quot;, &amp;quot;, (, ), „ : ) and
coordinate conjunctions (CC); b) The word
&amp;quot;that&amp;quot;, together with its POS; c) Rela-
tive pronouns, represented as W; and d) VP
chunks. Given two word positions i„, and
jw, the corresponding feature will be the
pattern representing the sentence from the
position i„, to the position
</bodyText>
<listItem confidence="0.8311065">
• Sentence features. These features count
the number of occurrences of relevant ele-
</listItem>
<bodyText confidence="0.94352675">
ments of the sentence. Specifically, we con-
sider the chunks with tag VP, WP or WP$, the
words whose POS is a punctuation mark
and the word that. When available, 5-
points and E-points of the clause structure
are also considered. Given a word in the
iw-th position, four features are generated
for each element: two indicating the count
of the element to the left and to the right
of the iw-th word, and two indicating the
existence or not of the element to the left
and to the right-hand side.
</bodyText>
<subsectionHeader confidence="0.999712">
2.2 Parts 1 and 2
</subsectionHeader>
<bodyText confidence="0.997779190476191">
The purpose of these parts is the identification
of the clause start positions (S-points) for the
first part and clause end positions (E-points)
for the second part. For each part, a classifier is
trained to decide whether a given word matches
the target of the part. Only words at the chunk
boundaries are considered candidates: words at
the beginning of chunks for S-points, and words
at the end of chunks for E-points. The rest are
always labeled as not S-points or not E-points,
respectively.&apos;
For a word candidate w the features consid-
ered are word and chunk windows of size 3, two
sentence patterns from the beginning to w,
and from w to the end and the sentence fea-
tures, which include the left-hand side already
identified S-points and E-points, respectively for
each part. When these parts are included as
subtasks of the part 3, sentence features for part
2 also include the S-points of the whole sen-
tence.
</bodyText>
<footnote confidence="0.766158333333333">
1-In the available data, a small proportion of words
which are not chunk boundaries are labeled as S-points
or E-points, but they correspond to annotation errors.
</footnote>
<subsectionHeader confidence="0.976375">
2.3 Part 3
</subsectionHeader>
<bodyText confidence="0.999993416666667">
The objective of this part is to build clauses,
given the S-points and E-points of the sentence
identified in the previous parts. Since this part
is too complex to be handled directly, we have
divided it into three chained subtasks: 1) Decide
how many open brackets correspond to each 5-
point. 2) For each open bracket and each E-
point to the right, i.e. a clause candidate, pre-
dict a confidence value for being a clause. 3)
Match each open bracket with an E-point pro-
ducing a consistent and confident clause split-
ting for the whole sentence.
</bodyText>
<subsectionHeader confidence="0.486642">
2.3.1 Open Brackets
</subsectionHeader>
<bodyText confidence="0.9999744">
In this subtask the number of open brackets for
each S-point must be predicted. Since our clas-
sification method only outputs binary decisions,
our approach consists in predicting whether in
a given S-point an additional bracket has to be
opened or not. We assume that each S-point
contains at least 1 open bracket. In practice,
this subtask consists of two classifiers: from 1
to 2, and from 2 to 3 open brackets. Higher
number of brackets are not frequent enough in
the data sets to be learned.
The features used for this task, given an 5-
point in the i-th position, are the following: 1)
Word and chunk windows of size 3. 2) Two
sentence patterns, from the beginning to i, and
from i to the end. 3) For each E-point to the
right, a sentence pattern from i to the E-point,
representing the possible clause starting at i. 4)
Sentence features, including counts for S-points
and E-points.
</bodyText>
<subsectionHeader confidence="0.917829">
2.3.2 Clause candidates
</subsectionHeader>
<bodyText confidence="0.999836266666667">
Each open bracket identified in the last subtask
paired with each E-point to the right forms a
clause candidate. The purpose of this subtask
is to predict a confidence value for the candi-
date to be a clause in the sentence. A classifier
learned for classifying candidates as clauses or
not will output such confidence values.
For each clause candidate starting at i and
ending at j, the features are the following: 1)
Word and chunk windows of size 3, centered in i.
2) One sentence pattern from i to j, represent-
ing the clause candidate. 3) Sentence features,
counting the relevant elements separately from
the beginning to i, from i to j and from j to the
end.
</bodyText>
<subsectionHeader confidence="0.893089">
2.3.3 Clause resolver
</subsectionHeader>
<bodyText confidence="0.997569875">
Given all clause candidates with its confidence
value, a clause split must be output for the
whole sentence, with the constraint that the
splitting must be consistent, according to the
definition of a clause. Given a list of clause
candidates, our strategy consists in applying re-
peatedly the following procedure until the list is
empty:
</bodyText>
<listItem confidence="0.997316777777778">
• Extract the clause candidate cmax with
highest confidence value, and output cmax
as a clause of the whole sentence.
• Remove from the list all clause candidate
which are not consistent with cam,: a)
candidates linked to the same cmax open
bracket. b) candidates which contain the
cmax open bracket or the cmax E-point, but
not both.
</listItem>
<sectionHeader confidence="0.871944" genericHeader="introduction">
3 Experiments and Results
</sectionHeader>
<bodyText confidence="0.9993964">
The system parameters (i.e., number and depth
of weak decision trees for the classifiers and win-
dow sizes for the features) are tuned using a
validation set consisting of a 15% of the train-
ing sentences, having learned with the remain-
ing 85%. All window sizes have been tuned to
3. The best performing depth for all tasks has
been 3. For tasks 1, 2 and 3.2 the best num-
ber of weak rules is 2000; for the task 3.1, 1,500
weak rules are needed for deciding among 1 or
3 brackets, and only 100 weak rules for deciding
among 2 and 3.
Table 1 shows for each task the number of
examples, the number of positive examples and
the number of generated features.
</bodyText>
<table confidence="0.999052833333333">
Part Examples Positive Features
1 138,069 22,950 43,356
2 138,069 17,150 42,853
3.1: 1 or 2 23,075 1,519 36,240
3.1: 2 or 3 1,600 81 5,361
3.2 39,209 16,294 14,040
</table>
<tableCaption confidence="0.999754">
Table 1: Dimensions of the decision problems
</tableCaption>
<bodyText confidence="0.904693833333333">
Table 2 shows the results obtained on the de-
velopment set and test set. The scores are lower
on the test set. For parts 1 and 2, the F1 scores
are about 2 points lower, but for the part 3 the
performance drops up to 6 points (this decrease
is specially noticeable in the recall rate). We
</bodyText>
<table confidence="0.985201375">
development precision recall F0-1
part 1 95.77% 92.08% 93.89%
part 2 91.27% 89.00% 90.12%
part 3 87.18% 82.48% 84.77%
test precision recall F0-1
part 1 93.96% 89.59% 91.72%
part 2 90.04% 88.41% 89.22%
part 3 84.82% 73.28% 78.63%
</table>
<tableCaption confidence="0.99644">
Table 2: Final results for the shared task
</tableCaption>
<bodyText confidence="0.9981625">
attribute that to the fact that part 3 strongly
depends on the results of the first two parts.
</bodyText>
<sectionHeader confidence="0.995428" genericHeader="method">
4 Some Comments
</sectionHeader>
<bodyText confidence="0.999984666666667">
The presented system follows a greedy approach
for solving the whole problem. At a first stage,
all S-points become one or more open brack-
ets. At the combination stage clause candidates
are selected greedily according to its confidence
value. Despite these simplifying criterions, we
think that the whole system performs quite well.
We are currently working on a system archi-
tecture which tries to overcome these limita-
tions, by simulating a bottom-up parsing ap-
proach by means of solving simple binary deci-
sions.
</bodyText>
<sectionHeader confidence="0.994331" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998674">
We would like to thank Montserrat Civit and
Victoria Arranz for their invaluable help in the
design of linguistically sensible features.
This research has been partially funded by
the European Commission (NAMIC — IST-
1999-12392) and the Spanish Research Depart-
ment (Hermes — TIC2000-0335-0O3-02). Xavier
Carreras holds a grant by the Department of
Universities, Research and Information Society
of the Catalan Government.
</bodyText>
<sectionHeader confidence="0.998953" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999828181818182">
Carreras and L. Marquez. 2001. Boosting Al-
gorithms for Anti—Spam E—mail Filtering. Sub-
mitted to Recent Advances in Natural Language
Processing RANLP&apos;01.
Freund and R. E. Schapire. 1997. A Decision—
Theoretic Generalization of On—line Learning and
an Application to Boosting. Journal of Computer
and System Sciences, 55(1):119-139.
E. Schapire and Y. Singer. 1999. Improved
Boosting Algorithms Using Confidence-rated Pre-
dictions. Machine Learning, 37(3):297-336.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.311918">
<title confidence="0.999946">Boosting Trees for Clause Splitting</title>
<author confidence="0.897151">Xavier Carreras</author>
<author confidence="0.897151">Lluis</author>
<affiliation confidence="0.863236">TALP Research LSI Universitat Politecnica de Catalunya Jordi Girona Salgado 1-3. E-08034 Barcelona.</affiliation>
<intro confidence="0.636248">fcarreras,lluisml @lsi.upc.es</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Carreras</author>
<author>L Marquez</author>
</authors>
<title>Boosting Algorithms for Anti—Spam E—mail Filtering. Submitted to Recent Advances in Natural Language Processing RANLP&apos;01.</title>
<date>2001</date>
<contexts>
<context position="1752" citStr="Carreras and Marquez, 2001" startWordPosition="261" endWordPosition="264">r, 1999), which works with very simple domain partitioning weak hypotheses (decision stumps) with confidence rated predictions. This particular boosting algorithm is able to work efficiently in very high dimensional feature spaces, and has been applied, with significant success, to a number of NLP disambiguation tasks, such as: POS tagging, PP—attachment disambiguation, text categorization, and word sense disambiguation. In our particular setting, weak rules are extended to arbitrarily deep decision trees following the suggestion of (Schapire and Singer, 1999) and the definition presented in (Carreras and Marquez, 2001). These more complex weak rules allow the algorithm to work in a higher dimensional feature space that contains conjunctions of simple features, and this fact has turned out to be crucial for improving results in the present domain. As a general guideline for all the parts, our approach consists in giving the learning algorithm a very large number of binary simple features, assuming that the relevant features will be identified for constructing the classifiers. In section 2.1 the type of features used in the system are described. The remaining sections explain how the specific parts of the pro</context>
</contexts>
<marker>Carreras, Marquez, 2001</marker>
<rawString>Carreras and L. Marquez. 2001. Boosting Algorithms for Anti—Spam E—mail Filtering. Submitted to Recent Advances in Natural Language Processing RANLP&apos;01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Freund</author>
<author>R E Schapire</author>
</authors>
<title>A Decision— Theoretic Generalization of On—line Learning and an Application to Boosting.</title>
<date>1997</date>
<journal>Journal of Computer and System Sciences,</journal>
<pages>55--1</pages>
<contexts>
<context position="850" citStr="Freund and Schapire, 1997" startWordPosition="126" endWordPosition="129">lsi.upc.es 1 Introduction We present a system for the CoNLL-2001 shared task: the clause splitting problem. Our approach consists in decomposing the clause splitting problem into a combination of binary &amp;quot;simple&amp;quot; decisions, which we solve with the AdaBoost learning algorithm. The whole problem is decomposed in two levels, with two chained decisions per level. The first level corresponds to parts 1 and 2 presented in the introductory document for the task. The second level corresponds to the part 3, which we decompose in two decisions and a combination procedure. 2 System Architecture AdaBoost (Freund and Schapire, 1997) is a general method for obtaining a highly accurate classification rule by combining many weak classifiers, each of which may be only moderately accurate. In designing our system, a generalized version of the AdaBoost algorithm has been used AdaBoost.MH, (Schapire and Singer, 1999), which works with very simple domain partitioning weak hypotheses (decision stumps) with confidence rated predictions. This particular boosting algorithm is able to work efficiently in very high dimensional feature spaces, and has been applied, with significant success, to a number of NLP disambiguation tasks, such</context>
</contexts>
<marker>Freund, Schapire, 1997</marker>
<rawString>Freund and R. E. Schapire. 1997. A Decision— Theoretic Generalization of On—line Learning and an Application to Boosting. Journal of Computer and System Sciences, 55(1):119-139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Schapire</author>
<author>Y Singer</author>
</authors>
<title>Improved Boosting Algorithms Using Confidence-rated Predictions.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>37--3</pages>
<contexts>
<context position="1133" citStr="Schapire and Singer, 1999" startWordPosition="173" endWordPosition="176">le problem is decomposed in two levels, with two chained decisions per level. The first level corresponds to parts 1 and 2 presented in the introductory document for the task. The second level corresponds to the part 3, which we decompose in two decisions and a combination procedure. 2 System Architecture AdaBoost (Freund and Schapire, 1997) is a general method for obtaining a highly accurate classification rule by combining many weak classifiers, each of which may be only moderately accurate. In designing our system, a generalized version of the AdaBoost algorithm has been used AdaBoost.MH, (Schapire and Singer, 1999), which works with very simple domain partitioning weak hypotheses (decision stumps) with confidence rated predictions. This particular boosting algorithm is able to work efficiently in very high dimensional feature spaces, and has been applied, with significant success, to a number of NLP disambiguation tasks, such as: POS tagging, PP—attachment disambiguation, text categorization, and word sense disambiguation. In our particular setting, weak rules are extended to arbitrarily deep decision trees following the suggestion of (Schapire and Singer, 1999) and the definition presented in (Carreras</context>
</contexts>
<marker>Schapire, Singer, 1999</marker>
<rawString>E. Schapire and Y. Singer. 1999. Improved Boosting Algorithms Using Confidence-rated Predictions. Machine Learning, 37(3):297-336.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>