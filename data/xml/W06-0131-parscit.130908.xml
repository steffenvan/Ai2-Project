<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001864">
<title confidence="0.978507">
POC-NLW Template for Chinese Word Segmentation
</title>
<author confidence="0.801934">
Bo Chen Weiran Xu
</author>
<email confidence="0.859121">
chb615@gmail.com xuweiran@263.net
</email>
<author confidence="0.930836">
Tao Peng Jun Guo
</author>
<email confidence="0.923649">
ppttbupt@gmail.com guojun@bupt.edu.cn
</email>
<author confidence="0.572469">
Pattern Recognition and Intelligent System Lab
</author>
<affiliation confidence="0.921913">
Beijing University of Posts and Telecommunications
</affiliation>
<address confidence="0.240007">
Beijing 100876, P. R. China
</address>
<sectionHeader confidence="0.97608" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999818">
In this paper, a language tagging tem-
plate named POC-NLW (position of a
character within an n-length word) is pre-
sented. Based on this template, a two-
stage statistical model for Chinese word
segmentation is constructed. In this
method, the basic word segmentation is
based on n-gram language model, and a
Hidden Markov tagger based on the
POC-NLW template is used to imple-
ment the out-of-vocabulary (OOV) word
identification. The system participated in
the MSRA_Close and UPUC_Close
word segmentation tracks at SIGHAN
Bakeoff 2006. Results returned by this
bakeoff are reported here.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999967818181818">
In Chinese word segmentation, there are two
problems still remain, one is the resolution of
ambiguity, and the other is the identification of
so-called out-of-vocabulary (OOV) or unknown
words. In order to resolve these two problems, a
two-stage statistical word segmentation strategy
is adopted in our system. The first stage is op-
tional, and the whole segmentation can be ac-
complished in the second stage. In the first stage,
the n-gram language model is employed to im-
plement basic word segmentation including dis-
ambiguation. In the second stage, a language
tagging template named POC-NLW (position of
a character within an n-length word) is intro-
duced to accomplish unknown word identifica-
tion as template-based character tagging.
The remainder of this paper is organized as
follows. In section 2 and section 3, a briefly de-
scription of the main methods adopted in our
system is given. Results of our system at this
bakeoff are reported in section 4. At last, conclu-
sions are derived in section 5.
</bodyText>
<sectionHeader confidence="0.809897" genericHeader="method">
2 The Basic Word Segmentation Stage
</sectionHeader>
<bodyText confidence="0.999220142857143">
In the first stage, the basic word segmentation is
accomplished. The key issue in this stage is the
ambiguity problem, which is mainly caused by
the fact that a Chinese character can occur in dif-
ferent word internal positions in different words
(Xue, 2003). A lot of machine learning tech-
niques have been applied to resolve this problem,
the n-gram language model is one of the most
popular ones among them (Fu and Luke, 2003;
Li et al., 2005). As such, we also employed n-
gram model in this stage.
When a sentence is inputted, it is first seg-
mented into a sequence of individual characters
(e.g. ASCII strings, basic Chinese characters,
punitions, numerals and so on), marked as C1,n.
According to the system’s dictionary, several
word sequences W1,m will be constructed as can-
didates. The function of the n-gram model is to
find out the best word sequence W* corresponds
to C1,n, which has the maximum integrated prob-
ability, i.e.,
</bodyText>
<equation confidence="0.996529388888889">
m
P W W
(  |) for bigram
i i −1
W1
m
for trigram
W1
W
* =
,
argmax
P(W1
 |C1
)
,
m
n
,m
W1
max
≅ arg
,m
11
i
=
1
max
≅ arg
,m
)
PW W
( |
i i −
, W
1 i − 2
</equation>
<page confidence="0.651219">
11
i
=
1
177
</page>
<bodyText confidence="0.959279142857143">
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 177–180,
Sydney, July 2006. c�2006 Association for Computational Linguistics
The Maximum Likelihood method was used to
estimate the word n-gram probabilities used in
our model, and the linear interpolation method
(Jelinek and Mercer, 1980) was applied to
smooth these estimated probabilities.
</bodyText>
<sectionHeader confidence="0.97528" genericHeader="method">
3 The OOV Word Identification Stage
</sectionHeader>
<bodyText confidence="0.999993166666667">
The n-gram method is based on the exiting grams
in the model, so it is good at judging the connect-
ing relationship among known words, but does
not have the ability to deal with unknown words
in substance. Therefore, another OOV word
identification model is required.
OOV words are regarded as words that do not
exist in a system’s machine-readable dictionary,
and a more detailed definition can be found in
(Wu and Jiang, 2000). In general, Chinese word
can be created through compounding or abbrevi-
ating of most of existing characters and words.
Thus, the key to solve the OOV word identifica-
tion lies on whether the new word creation
mechanisms in Chinese language can be ex-
tracted. Therefore, a POC-NLW language tag-
ging template is introduced to explore such in-
formation on the character-level within words.
</bodyText>
<subsectionHeader confidence="0.997936">
3.1 The POC-NLW Template
</subsectionHeader>
<bodyText confidence="0.965650666666667">
Many character-level based works have been
done for the Chinese word segmentation, includ-
ing the LMR tagging methods (Xue, 2003; Na-
kagawa. 2004), the IWP mechanism (Wu and
Jiang, 2000). Based on these previous works, this
POC-NLW template was derived. Assume that
the length of a word is the number of component
characters in it, the template is consist of two
component: Lmax and a Wl-Pn tag set. Lmax to de-
note the maximum length of a word expressed by
the template; a Wl-Pn tag denotes that this tag is
assigned to a character at the n-th position within
a l-length word, n =1,2,L,l . Apparently, the
size of this tag set is (Lmax + 1) x Lmax / 2
For example, the Chinese word “人 民 ” is
tagged as:
人 W2P1, 民 W2P2
and “中国人” is tagged as:
中 W3P1, 国 W3P2, 人 W3P3
In the example, two words are tagged by the
template respectively, and the Chinese character
“人” has been assigned two different tags.
In a sense, the Chinese word creation mecha-
nisms could be extracted through statistics of the
tags for each character on a certain large corpus.
On the other hand, while a character sequence
in a sentence is tagged by this template, the word
boundaries are obvious. Meanwhile, the word
segmentation is implemented.
In addition, in this template, known words and
unknown words are both regarded as sequences
of individual characters. Thus, the basic word
segmentation process, the disambiguation proc-
ess and the OOV word identification process can
be accomplished in a unified process. Thereby,
this model can also be used alone to implement
the word segmentation task. This characteristic
will make the word segmentation system much
more efficient.
</bodyText>
<subsectionHeader confidence="0.991663">
3.2 The HMM Tagger
</subsectionHeader>
<bodyText confidence="0.999927">
Form the description of POC-NLW template, it
can be found that the word segmentation could
be implemented as POC-NLW tagging, which is
similar to the so-called part-of-speech (POS)
tagging problem. In POS tagging, Hidden
Markov Model (HMM) was applied as one of the
most significant methods, as described in detail
in (Brants, 2000). The HMM method can achieve
high accuracy in tagging with low processing
costs, so it was adopted in our model.
According to the definition of POC-NLW
template, the state set of HMM corresponds to
the Wl-Pn tag set, and the symbol set is com-
posed of all characters. However, the initial state
probability matrix and the state transition prob-
ability matrix are not composed of all of the tags
in the state set. To express more clearly, we de-
fine two subset of the state set:
</bodyText>
<listItem confidence="0.961189857142857">
• Begin Tag Set (BTS): this set is con-
sisted of tag which can occur in the beg-
ging position in a word. Apparently, these
tags must have the Wl-P1 form.
• End Tag Set (ETS): correspond to BTS,
tags in this set should occur in the end po-
sition, and their form should be like Wl-Pl.
</listItem>
<bodyText confidence="0.999311545454546">
Apparently, the size of BTS is Lmax as well as
of ETS. Thus, the initial state probability matrix
corresponds to BTS instead of the whole state set.
On the other hand, because of the word internal
continuity, if the current tag Wl-Pn is not in ETS,
than the next tag must be Wl-P(n+1). In other
words, the case in which the transition probabil-
ity is need is that when the current tag is in ETS
and the next tag belongs to BTS. So, the state
transition matrix in our model corresponds to
ETS x BTS.
</bodyText>
<page confidence="0.996621">
178
</page>
<bodyText confidence="0.9998555625">
The probabilities used in HMM were defined
similarly to those in POS tagging, and were es-
timated using the Maximum Likelihood method
from the training corpus.
In the two-stage strategy, the output word se-
quence of the first stage is transferred into the
second stage. The items in the sequence, includ-
ing individual characters and words, which do
not have a bigram or trigram relationship with
the surrounding items, are picked out with its
surrounding items to compose several sequences
of items. These item sequences are processed by
the HMM tagger to form new item sequences. At
last, these processed items sequences are com-
bined into the whole word sequence as the final
output.
</bodyText>
<sectionHeader confidence="0.999645" genericHeader="evaluation">
4 Results and Analysis
</sectionHeader>
<subsectionHeader confidence="0.897202">
4.1 System
</subsectionHeader>
<bodyText confidence="0.999929125">
The system submitted at this bakeoff was a two-
stage one, as describe at beginning of this paper.
The model used in the first stage was trigram,
and the Lmax of the template used in the second
stage was set to 7.
In addition to the tags defined in the template
before, a special tag is introduced into our Wl-Pn
tag set to indicate all those characters that occur
after the Lmax-th position in an extremely long
(longer than Lmax) word., formulized as WLmax-
P(Lmax+1). And then, there are 28 basic tags
(from W1-P1 to W7-P7) and the special one W7-
P8.
For instance, using the special tag, the word
“中国共产党中央委员会” (form the MSRA
Corpus ) is tagged as:
</bodyText>
<equation confidence="0.557056">
中 W7-P1 国 W7-P2 共 W7-P3 产 W7-P4
党 W7-P5 中 W7-P6 央 W7-P7 委 W7-P8
员W7-P8 会 W7-P8
</equation>
<subsectionHeader confidence="0.926475">
4.2 Results at SIGHAN Bakeoff 2006
</subsectionHeader>
<bodyText confidence="0.997341666666667">
Our system participated in the MSRA_Close and
UPUC_Close track at the SIGHAN Bakeoff
2006. The test results are as showed in Table 1.
</bodyText>
<table confidence="0.995742714285714">
Corpus MSRA UPUC
F-measure 0.951 0.918
Recall 0.956 0.932
Precision 0.947 0.904
IV Recall 0.972 0.969
OOV Recall 0.493 0.546
OOV Precision 0.569 0.757
</table>
<tableCaption confidence="0.999937">
Table 1. Results at SIGHAN Bakeoff 2006
</tableCaption>
<bodyText confidence="0.999828736842105">
The performances of our system on the two
corpuses can rank in the half-top group among
the participated systems.
We notice that the accuracies on known word
segmentation are relatively better than on OOV
words segmentation. This appears somewhat un-
expected. In the close experiments we had done
on the PKU and MSR corpuses of SIGHAN
Bakeoff 2005, the relative performance of OOV
Recall was much more outstanding than of the F-
measure.
We think this is due to the inappropriate pa-
rameters used in n-gram model, which over-
guarantees the performance of basic word seg-
mentation. It can be seen on the IV Recall (high-
est in UPUC_Close track). For only the best out-
put sequence of the n-gram model is transferred
to the HMM tagger, some potential unknown
words may be miss-split in the early stage. Thus,
the OOV Recall is not very good, and this also
affects the overall performance.
On the other hand, the performances of OOV
identification on UPUC are much better than on
MSRA, while the performances of overall seg-
mentation accuracy on UPUC are worse than on
MSRA. This phenomenon also happened in our
experiments on the Bakeoff 2005 corpuses of
PKU and MSR. In the PKU test data, the rate of
OOV words according is 0.058 while in MSR is
0.026. Thus, it can be conclude that the more
unknown words occur, the more significant abil-
ity of OOV words identification appears.
In addition, the relative performance of OOV
Precision are much better. This demonstrates that
the OOV identification ability of our system is
appreciable. In other words, the POC-NLW tag-
ging method introduced is effective to some ex-
tent.
</bodyText>
<sectionHeader confidence="0.9992745" genericHeader="evaluation">
5 CONCLUSION AND FURTHER
WORK
</sectionHeader>
<bodyText confidence="0.999688166666667">
In this paper, a POC-NLW template is presented
for word segmentation, which aims at exploring
the word creation mechanisms in Chinese lan-
guage by utilizing the character-level informa-
tion to. A two-stage strategy was applied in our
system to combine the n-gram model based word
segmentation and OOV word identification im-
plemented by a HMM tagger. Test results show
that the method achieved high performance on
word segmentation, especially on unknown
words identification. Therefore, the method is a
practical one that can be implemented as an inte-
</bodyText>
<page confidence="0.996105">
179
</page>
<bodyText confidence="0.999880473684211">
gral component in actual Chinese NLP applica-
tions.
From the results, it can safely conclude that
method introduced here does find some charac-
ter-level information, and the information could
effectively conduct the word segmentation and
unknown words identification. For this is the first
time we participate in this bakeoff, and the work
has been done as a integral part of another sys-
tem during the past two months, the implementa-
tion of the segmentation system we submitted is
coarse. A lot of improvements, on either theo-
retical methods or implementation techniques,
are required in our future work, including the
smoothing techniques in the n-gram model and
the HMM model, the refine of the features ex-
traction method and the POC-NLW template it-
self, the more harmonious integration strategy
and so on.
</bodyText>
<sectionHeader confidence="0.997604" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998533833333333">
This work is partially supported by NSFC
(National Natural Science Foundation of China)
under Grant No.60475007, Key Project of Chi-
nese Ministry of Education under Grant
No.02029 and the Foundation of Chinese Minis-
try of Education for Century Spanning Talent.
</bodyText>
<sectionHeader confidence="0.999002" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9997576875">
Andi Wu, and Zixin Jiang. 2000. Statistically-
enhanced new word identification in a rule-based
Chinese system. Proceedings of the 2nd Chinese
Language Processing Workshop, 46-51.
Frederick Jelinek, and Robert L. Mercer. 1980. Inter-
polated Estimation of Markov Source Parameters
from Sparse Data. Proceedings of Workshop on
Pattern Recognition in Practice, Amsterdam, 381-
397.
Guohong Fu, and Kang-Kwong Luke. 2003. A Two-
stage Statistical Word Segmentation System for
Chinese. Proceedings of the Second SIGHAN
Workshop on Chinese Language Processing, 156-
159.
Heng Li, Yuan Dong, Xinnian Mao, Haila Wang, and
Wu Liu. 2005. Chinese Word Segmentation in
FTRD Beijing. Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing, 150-
153.
Nianwen Xue. 2003. Chinese Word Segmentation as
Character Tagging. International Journal of Com-
putational Linguistics and Chinese Language Pro-
cession, 8(1):29–48.
Tetsuji Nakagawa. 2004. Chinese and Japanese Word
Segmentation Using Word-Level and Character-
Level Information. Proceedings of the 20th Inter-
national Conference on Computational Linguistics,
466–472.
Thorsten Brants. 2000. TnT — A Statistical Part-of-
Speech Tagger. Proceedings of the Sixth Confer-
ence on Applied Natural Language Processing
ANLP-2000, 224–231.
</reference>
<page confidence="0.997748">
180
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.373902">
<title confidence="0.999947">POC-NLW Template for Chinese Word Segmentation</title>
<author confidence="0.999641">Bo Chen Weiran Xu</author>
<email confidence="0.800464">chb615@gmail.comxuweiran@263.net</email>
<author confidence="0.992762">Tao Peng Jun Guo</author>
<email confidence="0.606275">ppttbupt@gmail.comguojun@bupt.edu.cn</email>
<affiliation confidence="0.902779">Pattern Recognition and Intelligent System Lab Beijing University of Posts and Telecommunications</affiliation>
<address confidence="0.971864">Beijing 100876, P. R. China</address>
<abstract confidence="0.998468823529412">paper, a language tagging template named POC-NLW (position of a character within an n-length word) is pre- Based on this template, a stage statistical model for Chinese word segmentation is constructed. In this method, the basic word segmentation is on n-gram language model, Hidden Markov tagger based on the POC-NLW template is used to implement the out-of-vocabulary (OOV) word identification. The system participated in the MSRA_Close and UPUC_Close word segmentation tracks at SIGHAN Bakeoff 2006. Results returned by this bakeoff are reported here.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Andi Wu</author>
<author>Zixin Jiang</author>
</authors>
<title>Statisticallyenhanced new word identification in a rule-based Chinese system.</title>
<date>2000</date>
<booktitle>Proceedings of the 2nd Chinese Language Processing Workshop,</booktitle>
<pages>46--51</pages>
<contexts>
<context position="3860" citStr="Wu and Jiang, 2000" startWordPosition="650" endWordPosition="653">n-gram probabilities used in our model, and the linear interpolation method (Jelinek and Mercer, 1980) was applied to smooth these estimated probabilities. 3 The OOV Word Identification Stage The n-gram method is based on the exiting grams in the model, so it is good at judging the connecting relationship among known words, but does not have the ability to deal with unknown words in substance. Therefore, another OOV word identification model is required. OOV words are regarded as words that do not exist in a system’s machine-readable dictionary, and a more detailed definition can be found in (Wu and Jiang, 2000). In general, Chinese word can be created through compounding or abbreviating of most of existing characters and words. Thus, the key to solve the OOV word identification lies on whether the new word creation mechanisms in Chinese language can be extracted. Therefore, a POC-NLW language tagging template is introduced to explore such information on the character-level within words. 3.1 The POC-NLW Template Many character-level based works have been done for the Chinese word segmentation, including the LMR tagging methods (Xue, 2003; Nakagawa. 2004), the IWP mechanism (Wu and Jiang, 2000). Based</context>
</contexts>
<marker>Wu, Jiang, 2000</marker>
<rawString>Andi Wu, and Zixin Jiang. 2000. Statisticallyenhanced new word identification in a rule-based Chinese system. Proceedings of the 2nd Chinese Language Processing Workshop, 46-51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
<author>Robert L Mercer</author>
</authors>
<title>Interpolated Estimation of Markov Source Parameters from Sparse Data.</title>
<date>1980</date>
<booktitle>Proceedings of Workshop on Pattern Recognition in Practice,</booktitle>
<pages>381--397</pages>
<location>Amsterdam,</location>
<contexts>
<context position="3343" citStr="Jelinek and Mercer, 1980" startWordPosition="563" endWordPosition="566">nction of the n-gram model is to find out the best word sequence W* corresponds to C1,n, which has the maximum integrated probability, i.e., m P W W ( |) for bigram i i −1 W1 m for trigram W1 W * = , argmax P(W1 |C1 ) , m n ,m W1 max ≅ arg ,m 11 i = 1 max ≅ arg ,m ) PW W ( | i i − , W 1 i − 2 11 i = 1 177 Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 177–180, Sydney, July 2006. c�2006 Association for Computational Linguistics The Maximum Likelihood method was used to estimate the word n-gram probabilities used in our model, and the linear interpolation method (Jelinek and Mercer, 1980) was applied to smooth these estimated probabilities. 3 The OOV Word Identification Stage The n-gram method is based on the exiting grams in the model, so it is good at judging the connecting relationship among known words, but does not have the ability to deal with unknown words in substance. Therefore, another OOV word identification model is required. OOV words are regarded as words that do not exist in a system’s machine-readable dictionary, and a more detailed definition can be found in (Wu and Jiang, 2000). In general, Chinese word can be created through compounding or abbreviating of mo</context>
</contexts>
<marker>Jelinek, Mercer, 1980</marker>
<rawString>Frederick Jelinek, and Robert L. Mercer. 1980. Interpolated Estimation of Markov Source Parameters from Sparse Data. Proceedings of Workshop on Pattern Recognition in Practice, Amsterdam, 381-397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guohong Fu</author>
<author>Kang-Kwong Luke</author>
</authors>
<title>A Twostage Statistical Word Segmentation System for Chinese.</title>
<date>2003</date>
<booktitle>Proceedings of the Second SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>156--159</pages>
<contexts>
<context position="2351" citStr="Fu and Luke, 2003" startWordPosition="372" endWordPosition="375">ain methods adopted in our system is given. Results of our system at this bakeoff are reported in section 4. At last, conclusions are derived in section 5. 2 The Basic Word Segmentation Stage In the first stage, the basic word segmentation is accomplished. The key issue in this stage is the ambiguity problem, which is mainly caused by the fact that a Chinese character can occur in different word internal positions in different words (Xue, 2003). A lot of machine learning techniques have been applied to resolve this problem, the n-gram language model is one of the most popular ones among them (Fu and Luke, 2003; Li et al., 2005). As such, we also employed ngram model in this stage. When a sentence is inputted, it is first segmented into a sequence of individual characters (e.g. ASCII strings, basic Chinese characters, punitions, numerals and so on), marked as C1,n. According to the system’s dictionary, several word sequences W1,m will be constructed as candidates. The function of the n-gram model is to find out the best word sequence W* corresponds to C1,n, which has the maximum integrated probability, i.e., m P W W ( |) for bigram i i −1 W1 m for trigram W1 W * = , argmax P(W1 |C1 ) , m n ,m W1 max</context>
</contexts>
<marker>Fu, Luke, 2003</marker>
<rawString>Guohong Fu, and Kang-Kwong Luke. 2003. A Twostage Statistical Word Segmentation System for Chinese. Proceedings of the Second SIGHAN Workshop on Chinese Language Processing, 156-159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Li</author>
<author>Yuan Dong</author>
<author>Xinnian Mao</author>
<author>Haila Wang</author>
<author>Wu Liu</author>
</authors>
<date>2005</date>
<booktitle>Chinese Word Segmentation in FTRD Beijing. Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>150--153</pages>
<contexts>
<context position="2369" citStr="Li et al., 2005" startWordPosition="376" endWordPosition="379"> in our system is given. Results of our system at this bakeoff are reported in section 4. At last, conclusions are derived in section 5. 2 The Basic Word Segmentation Stage In the first stage, the basic word segmentation is accomplished. The key issue in this stage is the ambiguity problem, which is mainly caused by the fact that a Chinese character can occur in different word internal positions in different words (Xue, 2003). A lot of machine learning techniques have been applied to resolve this problem, the n-gram language model is one of the most popular ones among them (Fu and Luke, 2003; Li et al., 2005). As such, we also employed ngram model in this stage. When a sentence is inputted, it is first segmented into a sequence of individual characters (e.g. ASCII strings, basic Chinese characters, punitions, numerals and so on), marked as C1,n. According to the system’s dictionary, several word sequences W1,m will be constructed as candidates. The function of the n-gram model is to find out the best word sequence W* corresponds to C1,n, which has the maximum integrated probability, i.e., m P W W ( |) for bigram i i −1 W1 m for trigram W1 W * = , argmax P(W1 |C1 ) , m n ,m W1 max ≅ arg ,m 11 i = 1</context>
</contexts>
<marker>Li, Dong, Mao, Wang, Liu, 2005</marker>
<rawString>Heng Li, Yuan Dong, Xinnian Mao, Haila Wang, and Wu Liu. 2005. Chinese Word Segmentation in FTRD Beijing. Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, 150-153.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Chinese Word Segmentation as Character Tagging.</title>
<date>2003</date>
<journal>International Journal of Computational Linguistics and Chinese Language Procession,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="2182" citStr="Xue, 2003" startWordPosition="343" endWordPosition="344">dentification as template-based character tagging. The remainder of this paper is organized as follows. In section 2 and section 3, a briefly description of the main methods adopted in our system is given. Results of our system at this bakeoff are reported in section 4. At last, conclusions are derived in section 5. 2 The Basic Word Segmentation Stage In the first stage, the basic word segmentation is accomplished. The key issue in this stage is the ambiguity problem, which is mainly caused by the fact that a Chinese character can occur in different word internal positions in different words (Xue, 2003). A lot of machine learning techniques have been applied to resolve this problem, the n-gram language model is one of the most popular ones among them (Fu and Luke, 2003; Li et al., 2005). As such, we also employed ngram model in this stage. When a sentence is inputted, it is first segmented into a sequence of individual characters (e.g. ASCII strings, basic Chinese characters, punitions, numerals and so on), marked as C1,n. According to the system’s dictionary, several word sequences W1,m will be constructed as candidates. The function of the n-gram model is to find out the best word sequence</context>
<context position="4396" citStr="Xue, 2003" startWordPosition="739" endWordPosition="740">nary, and a more detailed definition can be found in (Wu and Jiang, 2000). In general, Chinese word can be created through compounding or abbreviating of most of existing characters and words. Thus, the key to solve the OOV word identification lies on whether the new word creation mechanisms in Chinese language can be extracted. Therefore, a POC-NLW language tagging template is introduced to explore such information on the character-level within words. 3.1 The POC-NLW Template Many character-level based works have been done for the Chinese word segmentation, including the LMR tagging methods (Xue, 2003; Nakagawa. 2004), the IWP mechanism (Wu and Jiang, 2000). Based on these previous works, this POC-NLW template was derived. Assume that the length of a word is the number of component characters in it, the template is consist of two component: Lmax and a Wl-Pn tag set. Lmax to denote the maximum length of a word expressed by the template; a Wl-Pn tag denotes that this tag is assigned to a character at the n-th position within a l-length word, n =1,2,L,l . Apparently, the size of this tag set is (Lmax + 1) x Lmax / 2 For example, the Chinese word “人 民 ” is tagged as: 人 W2P1, 民 W2P2 and “中国人” i</context>
</contexts>
<marker>Xue, 2003</marker>
<rawString>Nianwen Xue. 2003. Chinese Word Segmentation as Character Tagging. International Journal of Computational Linguistics and Chinese Language Procession, 8(1):29–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuji Nakagawa</author>
</authors>
<title>Chinese and Japanese Word Segmentation Using Word-Level and CharacterLevel Information.</title>
<date>2004</date>
<booktitle>Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<pages>466--472</pages>
<marker>Nakagawa, 2004</marker>
<rawString>Tetsuji Nakagawa. 2004. Chinese and Japanese Word Segmentation Using Word-Level and CharacterLevel Information. Proceedings of the 20th International Conference on Computational Linguistics, 466–472.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>TnT — A Statistical Part-ofSpeech Tagger.</title>
<date>2000</date>
<booktitle>Proceedings of the Sixth Conference on Applied Natural Language Processing ANLP-2000,</booktitle>
<pages>224--231</pages>
<contexts>
<context position="6267" citStr="Brants, 2000" startWordPosition="1062" endWordPosition="1063">guation process and the OOV word identification process can be accomplished in a unified process. Thereby, this model can also be used alone to implement the word segmentation task. This characteristic will make the word segmentation system much more efficient. 3.2 The HMM Tagger Form the description of POC-NLW template, it can be found that the word segmentation could be implemented as POC-NLW tagging, which is similar to the so-called part-of-speech (POS) tagging problem. In POS tagging, Hidden Markov Model (HMM) was applied as one of the most significant methods, as described in detail in (Brants, 2000). The HMM method can achieve high accuracy in tagging with low processing costs, so it was adopted in our model. According to the definition of POC-NLW template, the state set of HMM corresponds to the Wl-Pn tag set, and the symbol set is composed of all characters. However, the initial state probability matrix and the state transition probability matrix are not composed of all of the tags in the state set. To express more clearly, we define two subset of the state set: • Begin Tag Set (BTS): this set is consisted of tag which can occur in the begging position in a word. Apparently, these tags</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>Thorsten Brants. 2000. TnT — A Statistical Part-ofSpeech Tagger. Proceedings of the Sixth Conference on Applied Natural Language Processing ANLP-2000, 224–231.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>