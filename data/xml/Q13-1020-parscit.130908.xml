<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.992855">
Unsupervised Tree Induction for Tree-based Translation
</title>
<author confidence="0.953974">
Feifei Zhai, Jiajun Zhang, Yu Zhou and Chengqing Zong
</author>
<affiliation confidence="0.9335495">
National Laboratory of Pattern Recognition, Institute of Automation,
Chinese Academy of Sciences, Beijing, China
</affiliation>
<email confidence="0.976975">
{ffzhai,jjzhang,yzhou,cqzong}@nlpr.ia.ac.cn
</email>
<sectionHeader confidence="0.994409" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999701529411765">
In current research, most tree-based translation
models are built directly from parse trees. In
this study, we go in another direction and build
a translation model with an unsupervised tree
structure derived from a novel non-parametric
Bayesian model. In the model, we utilize
synchronous tree substitution grammars (STSG)
to capture the bilingual mapping between
language pairs. To train the model efficiently,
we develop a Gibbs sampler with three novel
Gibbs operators. The sampler is capable of
exploring the infinite space of tree structures by
performing local changes on the tree nodes.
Experimental results show that the string-to-
tree translation system using our Bayesian tree
structures significantly outperforms the strong
baseline string-to-tree system using parse trees.
</bodyText>
<sectionHeader confidence="0.998787" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9986444">
In recent years, tree-based translation models1 are
drawing more and more attention in the
community of statistical machine translation
(SMT). Due to their remarkable ability to
incorporate context structure information and long
distance reordering into the translation process,
tree-based translation models have shown
promising progress in improving translation
quality (Liu et al., 2006, 2009; Quirk et al., 2005;
Galley et al., 2004, 2006; Marcu et al., 2006; Shen
et al., 2008; Zhang et al., 2011b).
However, tree-based translation models always
suffer from two major challenges: 1) They are
usually built directly from parse trees, which are
generated by supervised linguistic parsers.
</bodyText>
<footnote confidence="0.6108915">
1 A tree-based translation model is defined as a model
using tree structures on one side or both sides.
</footnote>
<bodyText confidence="0.999968176470588">
However, for many language pairs, it is difficult to
acquire such corresponding linguistic parsers due
to the lack of Tree-bank resources for training. 2)
Parse trees are actually only used to model and
explain the monolingual structure, rather than the
bilingual mapping between language pairs. This
indicates that parse trees are usually not the
optimal choice for training tree-based translation
models (Wang et al., 2010).
Based on the above analysis, we can conclude
that the tree structure that is independent from
Tree-bank resources and simultaneously considers
the bilingual mapping inside the bilingual sentence
pairs would be a good choice for building tree-
based translation models.
Therefore, complying with the above conditions,
we propose an unsupervised tree structure for tree-
based translation models in this study. In the
structures, tree nodes are labeled by combining the
word classes of their boundary words rather than
by syntactic labels, such as NP, VP. Furthermore,
using these node labels, we design a generative
Bayesian model to infer the final tree structure
based on synchronous tree substitution grammars
(STSG) 2 . STSG is derived from the word
alignments and thus can grasp the bilingual
mapping effectively.
Training the Bayesian model is difficult due to
the exponential space of possible tree structures for
each training instance. We therefore develop an
efficient Gibbs sampler with three novel Gibbs
operators for training. The sampler is capable of
exploring the infinite space of tree structures by
performing local changes on the tree nodes.
</bodyText>
<footnote confidence="0.833627">
2 We believe it is possible to design a model to infer the
node label and tree structure jointly. We plan this as
future work, and here, we focus only on inferring the
tree structure in terms of the node labels derived from
word classes.
</footnote>
<page confidence="0.984449">
243
</page>
<bodyText confidence="0.908225740740741">
Transactions of the Association for Computational Linguistics, 1 (2013) 243–254. Action Editor: Philipp Koehn.
Submitted 12/2012; Revised 3/2013; Published 5/2013. c�2013 Association for Computational Linguistics.
The tree structure formed in this way is
independent from the Tree-bank resources and
simultaneously exploits the bilingual mapping
effectively. Experiments show that the proposed
unsupervised tree (U-tree) is more effective and
reasonable for tree-based translation than the parse
tree.
The main contributions of this study are as
follows:
1) Instead of the parse tree, we propose a
Bayesian model to induce a U-tree for tree-
based translation. The U-tree exploits the
bilingual mapping effectively and does not
rely on any Tree-bank resources.
2) We design a Gibbs sampler with three novel
Gibbs operators to train the Bayesian model
efficiently.
The remainder of the paper is organized as
follows. Section 2 introduces the related work.
Section 3 describes the STSG generation process,
and Section 4 depicts the adopted Bayesian model.
Section 5 describes the Gibbs sampling algorithm
and Gibbs operators. In Section 6, we analyze the
achieved U-trees and evaluate their effectiveness.
Finally, we conclude the paper in Section 7.
</bodyText>
<sectionHeader confidence="0.99979" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999840941176471">
In this study, we move in a new direction to build a
tree-based translation model with effective
unsupervised U-tree structures.
For unsupervised tree structure induction,
DeNero and Uszkoreit (2011) adopted a parallel
parsing model to induce unlabeled trees of source
sentences for syntactic pre-reordering. Our
previous work (Zhai et al., 2012) designed an EM-
based method to construct unsupervised trees for
tree-based translation models. This work differs
from the above work in that we design a novel
Bayesian model to induce unsupervised U-trees,
and prior knowledge can be encoded into the
model more freely and effectively.
Blunsom et al. (2008, 2009, 2010) utilized
Bayesian methods to learn synchronous context
free grammars (SCFG) from a parallel corpus. The
obtained SCFG is further used in a phrase-based
and hierarchical phrase-based system (Chiang,
2007). Levenberg et al. (2012) employed a
Bayesian method to learn discontinuous SCFG
rules. This study differs from their work because
we concentrate on constructing tree structures for
tree-based translation models. Our U-trees are
learned based on STSG, which is more appropriate
for tree-based translation models than SCFG.
Burkett and Klein (2008) and Burkett et al.
(2010) focused on joint parsing and alignment.
They utilized the bilingual Tree-bank to train a
joint model for both parsing and word alignment.
Cohn and Blunsom (2009) adopted a Bayesian
method to infer an STSG by exploring the space of
alignments based on parse trees. Liu et al. (2012)
re-trained the linguistic parsers bilingually based
on word alignment. Burkett and Klein (2012)
utilized a transformation-based method to learn a
sequence of monolingual tree transformations for
translation. Compared to their work, we do not rely
on any Tree-bank resources and focus on
generating effective unsupervised tree structures
for tree-based translation models.
Zollmann and Venugopal (2006) substituted the
non-terminal X in hierarchical phrase-based model
by extended syntactic categories. Zollmann and
Vogel (2011) further labeled the SCFG rules with
POS tags and unsupervised word classes. Our work
differs from theirs in that we present a Bayesian
model to learn effective STSG translation rules and
U-tree structures for tree-based translation models,
rather than designing a labeling strategy for
translation rules.
</bodyText>
<sectionHeader confidence="0.989161" genericHeader="method">
3 The STSG Generation Process
</sectionHeader>
<bodyText confidence="0.999879428571429">
In this work, we induce effective U-trees for the
string-to-tree translation model, which is based on
a synchronous tree substitution grammar (STSG)
between source strings and target tree fragments.
We take STSG as the generation grammar to match
the translation model. Typically, such an STSG3 is
a 5-tuple as follows:
</bodyText>
<equation confidence="0.877726">
G=E ( s,E t ,Nt,St,P)
</equation>
<bodyText confidence="0.630055">
where:
</bodyText>
<listItem confidence="0.9349704">
♦ Es and Et represent the set of source and
target words, respectively,
♦ Nt is the set of target non-terminals,
♦ St e Nt is the start root non-terminal, and
♦ P is the production rule set.
</listItem>
<footnote confidence="0.938779">
3 Generally, an STSG involves tree fragments on both
sides. Here we only consider the special case where the
source side is actually a string.
</footnote>
<page confidence="0.998294">
244
</page>
<bodyText confidence="0.9998812">
Apart from the start non-terminal St, we define
all the other non-terminals in Nt by word classes.
Inspired by (Zollmann and Vogel, 2011), we
divide these non-terminals into three categories:
one-word, two-word and multi-word non-terminals.
The one-word non-terminal is a word class, such as
C, meaning that it dominates a word whose word
class is C. Two-word non-terminals are used to
stand for two word strings. They are labeled in the
form of C1+C2, where C1 and C2 are the word
classes of the two words separately. Accordingly,
multi-word non-terminals represent the strings
containing more than two words. They are labeled
as C1...Cn, demanding that the word classes of the
leftmost word and the rightmost word are C1 and
Cn, respectively.
We use POS tag to play the role of word class4.
For example, the head node of the rule in Figure 1
is a multi-word non-terminal PRP...RB. It requires
that the POS tags of the leftmost and rightmost
word must be PRP and RB, respectively. Xiong et
al. (2006) showed that the boundary word is an
effective indicator for phrase reordering. Thus, we
believe that combining the word class of boundary
words can denote the whole phrase well.
</bodyText>
<equation confidence="0.446964">
PRP...RB
</equation>
<figureCaption confidence="0.998849">
Figure 1. An example of an STSG production rule.
</figureCaption>
<bodyText confidence="0.996317962962963">
Each production rule in P consists of a source
string and a target tree fragment. In the target tree
fragment, each internal node is labeled with a non-
terminal in Nt, and each leaf node is labeled with
either a target word in ¦t or a non-terminal in Nt .
The source string in a production rule comprises
source words and variables. Each variable
corresponds to a leaf non-terminal in the target tree
fragment. In the STSG, the production rule is used
to rewrite the root node into a string and a tree
fragment. For example, in Figure 1, the rule
rewrites the head node PRP...RB into the
corresponding string and fragment.
An STSG derivation refers to the process of
generating a specific source string and target tree
4 The demand of a POS tagger impairs the independence
from manual resources to some extent. In future, we
plan to design a method to learn effective unsupervised
labels for the non-terminals.
structure by production rules. This process begins
with the start non-terminal St and an empty source
string. We repeatedly choose production rules to
rewrite the leaf non-terminals and expand the
string until no leaf non-terminal is left. Finally, we
acquire a source string and a target tree structure
defined by the derivation. The probability of a
derivation is given as follows:
</bodyText>
<equation confidence="0.994479666666667">
n
p(d) p(p  |Ni) (1)
1
</equation>
<bodyText confidence="0.999898971428571">
where the derivation comprises a sequence of rules
d=(r1,É,rn), and Ni represents the root node of rule
ri. Hence, for a specific bilingual sentence pair, we
can generate the best target-side tree structure
based on the STSG, independent from the Tree-
bank resources. The STSG used in the above
process is learned by the Bayesian model that is
detailed in the next section.
Actually, SCFG can also be used to build the U-
trees. We do not use SCFG because most of the
tree-based models are based on STSG. In our
Bayesian model, the U-trees are optimized through
selecting a set of STSG rules. These STSG rules
are consistent with the translation rules used in the
tree-based models.
Another reason is that STSG has a stronger
expressive power on tree construction than SCFG.
In a STSG-based U-tree or a STSG rule, although
not linguistically informed, the nodes labeled by
POS tags are also effective on distinguishing
different ones. However, with SCFG, we have to
discard all the internal nodes (i.e., flattening the U-
trees or rules) to express the same sequence,
leading to a poor ability of distinguishing different
U-trees and production rules. Thus, using STSG,
we can build more specific U-trees for translation.
In addition, we find that the Bayesian SCFG
grammar cannot even significantly outperform the
heuristic SCFG grammar (Blunsom et al. 2009)5.
This would indicate that the SCFG-based
derivation tree as by-product is also not such good
for tree-based translation models. Considering the
above reasons, we believe that the STSG-based
learning procedure would result in a better
translation grammar for tree-based models.
</bodyText>
<footnote confidence="0.850052">
5 In (Blunsom et al., 2009), for Chinese-to-English
translation, the Bayesian SCFG grammar only
outperform the heuristic SCFG grammar by 0.1 BLEU
points on NIST MT 2004 and 0.6 BLEU points on NIST
MT 2005 in the NEWS domain.
</footnote>
<equation confidence="0.996974166666667">
ᡁԜ x1 x0
VBP+RB
wo-men
we VBP:x0 RB:x1
PRP
i
</equation>
<page confidence="0.997834">
245
</page>
<sectionHeader confidence="0.99775" genericHeader="method">
4 Bayesian Model
</sectionHeader>
<bodyText confidence="0.999369166666667">
In this section, we present a Bayesian model to
learn STSG defined in section 3. In the model, we
use șN to denote the probability distribution
p(r  |N) in Equation (1). șN follows a multinomial
distribution and we impose a Dirichlet prior (DP)
on it:
</bodyText>
<equation confidence="0.9866025">
r N
 |— ( )
Multi TN
TN |DN,P0 — DP(DN,P0(•  |N) )
</equation>
<bodyText confidence="0.987265727272727">
where P0(•  |N) (base distribution) is used to assign
prior probabilities to the STSG production rules. aN
controls the model’s tendency to either reuse
existing rules or create new ones using the base
distribution P0(•  |N) .
Instead of denoting the multinomial distribution
explicitly with a specific șN, we integrate over all
possible values of șN to achieve the probabilities of
rules. This integration results in the following
conditional probability for rule ri given the
previously observed rules r-i = r1 ,É, ri-1:
Where n-i
ri denotes the number of ri in r~ i , and n-iN
represents the total count of rules rewriting non-
terminal N in r~ i . Thanks to the exchangeability of
the model, all permutations of the rules are actually
equiprobable. This means that we can compute the
probability of each rule based on the previous and
subsequent rules (i.e. consider each rule as the last
one). This characteristic allows us to design an
efficient Gibbs sampling algorithm to train the
Bayesian model.
</bodyText>
<subsectionHeader confidence="0.99922">
4.1 Base Distribution
</subsectionHeader>
<bodyText confidence="0.977004571428571">
The base distribution P0(r  |N) is designed to
assign prior probabilities to the STSG production
rules. Because each rule r consists of a target tree
fragment frag and a source string str in the model,
we follow Cohn and Blunsom (2009) and
decompose the prior probability P0(r  |N) into two
factors as follows:
</bodyText>
<equation confidence="0.9669935">
P0(r  |N) P( frag
 |N) P(str  |frag) (4)
</equation>
<bodyText confidence="0.992630095238095">
where P( frag  |N) is the probability of
producing the target tree fragment frag. To
generate frag, Cohn and Blunsom (2009) used a
geometric prior to decide how many child nodes to
assign each node. Differently, we require that each
multi-word non-terminal node must have two child
nodes. This is because the binary structure has
been verified to be very effective for tree-based
translation (Wang et al., 2007; Zhang et al., 2011a).
The generation process starts at root node N. At
first, root node N is expanded into two child nodes.
Then, each newly generated node will be checked
to expand into two new child nodes with
probability pexpand. This process repeats until all the
new non-terminal nodes are checked. Obviously,
pexpand controls the scale of tree fragments, where a
large pexpand corresponds to large fragments6. The
new terminal nodes (words) are drawn uniformly
from the target-side vocabulary, and the non-
terminal nodes are created by asking two questions
as follows:
</bodyText>
<listItem confidence="0.826834">
1) What type is the node, one-word, two-
word or multi-word non-terminal?
2) What tag is used to label the node?
</listItem>
<bodyText confidence="0.9997575">
The answer to question 1) is chosen from a
uniform distribution, i.e., the probability is 1/3 for
each type of non-terminal. The entire generation
process is in a top-down manner, i.e., generating a
parent node first and then its children.
With respect to question 2), because the father
node has determined the POS tags of boundary
words, we only need one POS tag to generate the
label of the current node. For example, in Figure 1,
as the father node PRP...RB demands that the POS
tag of the rightmost word is RB, the right child of
PRP...RB must also satisfy this condition.
Therefore, we choose a POS tag VBP and obtain
the label VBP+RB. The POS tag is drawn
uniformly from the POS tag set. If the current node
is a one-word non-terminal, question 2) is
unnecessary. Similarly, with respect to the two-
word non-terminal node, questions 1) and 2) are
both unnecessary for its two child nodes because
they have already been defined by their father node.
As an example of the generative process, the
tree fragment in Figure 1 is created as follows:
</bodyText>
<listItem confidence="0.821502833333333">
a. Determine that the left child of PRP...RB is
a one-word non-terminal (labeled with PRP);
b. Expand PRP and generate the word “we” for
PRP;
6 In our experiment, we set pexpand to 1/3 to encourage
small tree fragments.
</listItem>
<equation confidence="0.9777504">
i
n � +D P 0
(  |, , , ) i
� i r N
p r r N P
D
i N 0
(  |)
r N
i
i
n � � D
N N
(3)
(2)
</equation>
<page confidence="0.979541">
246
</page>
<listItem confidence="0.980431142857143">
c. Determine that the right child of PRP...RB is
a two-word non-terminal;
d. Utilize the predetermined RB and a POS tag
VBP to form the tag of the two-word non-
terminal: VBP+RB;
e. Expand VBP+RB (to VBP and RB);
f. Do not expand VBP and RB.
</listItem>
<equation confidence="0.506689">
P(str  |frag) in Equation (4) is the probability of
</equation>
<bodyText confidence="0.999694842105263">
generating the source string, which contains
several source words and variables. Inspired by
(Blunsom et al., 2009) and (Cohn and Blunsom,
2009), we define P(str  |frag) as follows:
where csw is the number of words in the source
string. s means the source vocabulary set. Further,
cvar denotes the number of variables, which is
determined by the tree fragment frag.
As shown in Equation(5), we first determine
how many source words to generate using a
Poisson distribution Ppoisson(csw;1), which imposes a
stable preference for short source strings. Then, we
draw each source word from a uniform distribution
over s. Afterwards, we insert the variables into
the string. The variables are inserted one at a time
using a uniform distribution over the possible
positions. This factor discourages more variables.
For the example rule in Figure 1, the generative
process of the source string is:
</bodyText>
<listItem confidence="0.765428545454546">
a. Decide to generate one source word;
b. Generate the source word “ᡁԜ (wo-men) Ó;
c. Insert the first variable after the word;
d. Insert the second variable between the word
and the first variable.
Intuitively, a good translation grammar should
carry both small translation rules with enough
generality and large rules with enough context
information. DeNero and Klein (2007) proposed
this statement, and Cohn and Blunsom (2009) has
verified it in their experiments with parse trees.
</listItem>
<bodyText confidence="0.998799">
Our base distribution is also designed based on
this intuition. Considering the two factors in our
base distribution, we penalize both large target tree
fragments with many nodes and long source strings
with many words and variables. The Bayesian
model tends to select both small and frequent
STSG production rules to construct the U-trees.
With these types of trees, we can extract small
rules with good generality and simultaneously
obtain large rules with enough context information
by composition. We will show the effectiveness of
our U-trees in the verification experiment.
</bodyText>
<sectionHeader confidence="0.528257" genericHeader="method">
5 Model Training by Gibbs Sampling
</sectionHeader>
<bodyText confidence="0.999938">
In this section, we introduce a collapsed Gibbs
sampler, which enables us to train the Bayesian
model efficiently.
</bodyText>
<subsectionHeader confidence="0.996526">
5.1 Initialization State
</subsectionHeader>
<bodyText confidence="0.998943384615384">
At first, we use random binary trees to initialize the
sampler. To get the initial U-trees, we recursively
and randomly segment a sentence into two parts
and simultaneously create a tree node to dominate
each part. The created tree nodes are labeled by the
non-terminals described in section 3.
Using the initial target U-trees, source sentences
and word alignment, we extract minimal GHKM
translation rules7 in terms of frontier nodes (Galley
et al., 2004). Frontier nodes are the tree nodes that
can map onto contiguous substrings on the source
side via word alignment. For example, the bold
italic nodes with shadows in Figure 2 are frontier
nodes. In addition, it should be noted that the word
alignment is fixed8, and we only explore the entire
space of tree structures in our sampler. Differently,
Cohn and Blunsom (2009) designed a sampler to
infer an STSG by fixing the tree structure and
exploring the space of alignment. We believe that
it is possible to investigate the space of both tree
structure and alignment simultaneously. This
subject will be one of our future work topics.
For each training instance (a pair of source
sentence and target U-tree structure), the extracted
GHKM minimal translation rules compose a
unique STSG derivation9. Moreover, all the rules
developed from the training data constitute an
initial STSG for the Gibbs sampler.
7 We attach the unaligned word to the lowest frontier
node that can cover it in terms of word alignment.
8 The sampler might reinforce the frequent alignment
errors (AE), which would harm the translation model
(TM). Actually, the frequent AEs also greatly impair the
conventional TM. Besides, our sampler encourages the
correct alignments and simultaneously discourages the
infrequent AEs. Thus, compared with the conventional
TMs, we believe that our final TM would not be worse
due to AEs. Our final experiments verify this point and
we will conduct a much detailed analysis in future.
</bodyText>
<footnote confidence="0.5636435">
9 We only use the minimal GHKM rules (Galley et al.,
2004) here to reduce the complexity of the sampler.
</footnote>
<figure confidence="0.979487090909091">
c var
1�-7 1
P(str  |frag) = Ppoisson (csw; 1) x x 11
 ||c sw
E s i =1c sw
+i
(5)
247
NN...RB
jin-tian wo-men zai-ci jian-mian
Ӻཙ ᡁԜ ޽⅑ 㿱䶒
</figure>
<figureCaption confidence="0.9957685">
Figure 2. Illustration of an initial U-tree structure. The
bold italic nodes with shadows are frontier nodes.
</figureCaption>
<bodyText confidence="0.99980075">
Under this initial STSG, the sampler modifies
the initial U-trees (initial sample) to create a series
of new ones (new samples) by the Gibbs operators.
Consequently, new STSGs are created based on the
new U-trees simultaneously and used for the next
sampling operation. Repeatedly and after a number
of iterations, we can obtain the final U-trees for
building translation models.
</bodyText>
<subsectionHeader confidence="0.991668">
5.2 The Gibbs Operators
</subsectionHeader>
<bodyText confidence="0.999881190476191">
In this section, we develop three novel Gibbs
operators for the sampler. They explore the entire
space of the U-tree structures by performing local
changes on the tree nodes.
For a U-tree of a given sentence, we define s-
node as the non-root node covering at least two
words. Thus, the set of s-node contains all the tree
nodes except the root node, the pre-terminal nodes
and leaf nodes, which we call non-s-node. For
example, in Figure 2, PRBÉRB and PRP+VBP are
s-nodes, while NN and NNÉRB are non-s-nodes.
Since the POS tag sequence of the sentence is
fixed, all non-s-nodes would stay unchanged in all
possible U-trees of the sentence. Based on this fact,
our Gibbs operators work only on s-nodes.
Further, we assign 3 descendant candidates (DC)
for each s-node: its left child, right child and its
sibling. For example, in Figure 3, the 3 DCs for the
s-node are node PRP, VBP and RB respectively.
According to the different DCs it governs, every s-
node might be in one of the two different states:
</bodyText>
<listItem confidence="0.982822">
1) Left state: as Figure 3(a) shows, the s-node
governs the left two DCs, PRP and VBP,
and is labeled PRP+VBP.
2) Right state: as Figure 3(b) shows, the s-node
governs the right two DCs, VBP and RB, and
is labeled VBP+RB.
</listItem>
<bodyText confidence="0.999672611111111">
For a specific U-tree, the states of s-nodes are fixed.
Thus, by changing an s-node’s state, we can easily
transform this U-tree to another one, i.e., from the
current sample to a new one.
To formulate the U-tree transformation process,
we associate a binary variable `P䌜{0,1} with each
s-node, indicating whether the s-node is in the left
(`P=0) or right state (`P=1). Then we can change
the U-tree by changing value of the `P parameters.
Our first Gibbs operator, Rotate, just works by
sampling value of the `P parameters, one at a time,
and changing the U-tree accordingly. For example,
in Figure 3(a), the s-node is currently in the left
state (`P=0). We sample the `P of this node, and if
the sampled value of `P is 0, we keep the structure
unchanged, i.e., in the left state. Otherwise, we
change its state to the right state (`P=1), and
transform the U-tree to Figure 3(b) accordingly.
</bodyText>
<figureCaption confidence="0.99819025">
Figure 3. Illustration of the Rotate operator. In the
figure, (a) and (b) denote the s-node’s left state and right
state respectively. The bold italic nodes with shadows in
the figure are frontier nodes.
</figureCaption>
<bodyText confidence="0.999864846153846">
Obviously, towards an s-node for sampling, the
two values of `P would define two different U-trees.
Using the GHKM algorithm (Galley et al. 2004),
we can get two different STSG derivations from
the two U-trees based on the fixed word alignment.
Each derivation carries a set of STSG rules (i.e.,
minimal GHKM translation rules) of its own. In
the two derivations, the STSG rules defined by the
two states include the one rooted at the s-node’s
lowest ancestor frontier node, and the one rooted at
the s-node if it is a frontier node. For instance, in
Figure 3(a), as the s-node is not a frontier node, the
left state (`P=0) defines only one rule:
</bodyText>
<equation confidence="0.992997333333333">
rleft x x x
: 0 2 1
PRP ... RB(PRP + VBP(x0 : PRP x1: VBP) x2 : RB)
</equation>
<bodyText confidence="0.997051">
Differently, in Figure 3(b), the s-node is a
frontier node and thus the right state (`P=1) defines
two rules:
</bodyText>
<figure confidence="0.999190612903226">
PRP...RB
PRP+VBP
NN
PRP
VBP
RB
meet
again
we
today
PRP...RB
PRP...RB
PRP+VBP
VBP+RB
s-node
Rotate
s-node
VBP
RB
PRP
VBP
RB
again
again
meet
we
PRP
we meet
wo-men zai-ci jian-mian wo-men zai-ci jian-mian
ᡁԜ ޽⅑ 㿱䶒 ᡁԜ ޽⅑ 㿱䶒
(a) `P=0 (b) `P=1
</figure>
<page confidence="0.991172">
248
</page>
<bodyText confidence="0.999870333333333">
Using these STSG rules, the two derivations are
evaluated as follows (We use the value of `F to
denote the corresponding STSG derivation):
</bodyText>
<equation confidence="0.992222333333333">
p(T = 0) °C p(rleft  |r)
p (T =1) °C p( nght - 0, right-1  |r)
p( right -0  |r)p( nght -1  |right-0,r )
</equation>
<bodyText confidence="0.999556866666667">
Where r- refers to the conditional context, i.e., the
set of all other rules in the training data. All the
probabilities in the above formulas are computed
by Equation(3). We then normalize the two scores
and sample a value of `F based on them. With the
Bayesian model described in section 4, the sampler
will prefer the `F that produces small and frequent
STSG rules. This tendency results in more frontier
nodes in the U-tree (i.e., the s-node tends to be in
the state that is a frontier node), which will factor
the training instance into more small STSG rules.
In this way, the overall likelihood of the bilingual
data is improved by the sampler.
Theoretically, the Rotate operator is capable of
arriving at any possible U-tree from the initial U-
tree. This is because we can first convert the initial
U-tree to a left branch tree by the Rotate operator,
and then transform it to any other U-tree. However,
it may take a long time to do so. Thus, to speed up
the structure transformation process, we employ a
Two-level-Rotate operator, which takes a pair of s-
nodes in a parent-child relationship as a unit for
sampling. Similar to the Rotate operator, we also
assign a binary variable 4䌜{0,1} to each unit and
update the U-tree by sampling the value of 4. The
method of sampling 4 is similar to the one used for
`F. Figure 4 shows an example of the operator. As
shown in Figure 4(a), the unit NN...VBP and
PRP+VBP is in the left state (4=0), and governs
the left three descendants: NN, PRP, and VBP. By
the Two-level-Rotate operator, we can convert the
unit to Figure 4(b), i.e., the right state (4=1). Just as
Figure 4(b) shows, the governed descendants of the
unit are turned to PRP, VBP, and RB.
It may be confusing when choosing the parent-
child s-node pair for sampling because the parent
node always faces two choices: combining the left
child or right child for sampling. To avoid
confusion, we split the Two-level-Rotate operator
into two operators: Two-level-left-Rotate operator,
which works with the parent node and its left child,
and Two-level-right-Rotate operator, which only
considers the parent node and its right child 10.
Therefore, the operator used in Figure 4 is a Two-
level-right-Rotate operator.
</bodyText>
<figureCaption confidence="0.998002">
Figure 4. Illustration of the Two-level-Rotate operator.
The bold italic nodes with shadows in the Figure are
frontier nodes.
</figureCaption>
<bodyText confidence="0.999952571428571">
During sampling, for each training instance, the
sampler first applies the Two-level-left-Rotate
operator to all candidate pairs of s-nodes (parent s-
node and its left child s-node) in the U-tree. After
that, the Two-level-right-Rotate operator is applied
to all the candidate pairs of s-nodes (parent s-node
and its right child s-node). Then, we use the Rotate
operator on every s-node in the U-tree. By utilizing
the operators separately, we can guarantee that our
sampler satisfies detailed balance. We visit all the
training instances in a random order (one iteration).
After a number of iterations, we can obtain the
final U-tree structures and build the tree-based
translation model accordingly.
</bodyText>
<sectionHeader confidence="0.999844" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998438">
6.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.917420181818182">
The experiments are conducted on Chinese-to-
English translation. The training data are the FBIS
corpus with approximately 7.1 million Chinese
words and 9.2 million English words. We obtain
the bidirectional word alignment with GIZA++,
and then adopt the grow-diag-final-and strategy to
obtain the final symmetric alignment. We train a 5-
gram language model on the Xinhua portion of the
English Gigaword corpus and the English part of
10 We can also take more nodes as a unit for sampling,
but this would make the algorithm much more complex.
</bodyText>
<figure confidence="0.998298151515152">
NN...RB
NN...RB
NN...VBP
Two-level-right-Rotate
PRP...RB
PRP+VBP
VBP+RB
NN
PRP
VBP
RB
NN
PRP
VBP
RB
Today
jin-tian wo-men zai-ci jian-mian
hl�c MN NYt A_
again
jin-tian wo-men zai-ci jian-mian
hl�c MN NYt A_
we
meet
again
Today
we
meet
(a) 4=0 (b) 4=1
: x0 x1 —&gt; PRP ...RB(x0 : PRP x1 : VBP+ RB)
: x1 x0 —&gt; VBP+ RB(x0 : VBP x1: RB)
rright-0
rright
-1
</figure>
<page confidence="0.997268">
249
</page>
<bodyText confidence="0.999860318181818">
the training data. For tuning and testing, we use the
NIST MT 2003 evaluation data as the development
set, and use the NIST MT04 and MT05 data as the
test set. We use MERT (Och, 2004) to tune
parameters. Since MERT is prone to search errors,
we run MERT 5 times and select the best tuning
parameters in the tuning set. The translation quality
is evaluated by case-insensitive BLEU-4 with the
shortest length penalty. The statistical significance
test is performed by the re-sampling approach
(Koehn, 2004).
To create the baseline system, we use the open-
source Joshua 4.0 system (Ganitkevitch et al., 2012)
to build a hierarchical phrase-based (HPB) system,
and a syntax-augmented MT (SAMT) 11 system
(Zollmann and Venugopal, 2006) respectively.
The translation system used for testing the
effectiveness of our U-trees is our in-house string-
to-tree system (abbreviated as s2t). The system is
implemented based on (Galley et al., 2006) and
(Marcu et al. 2006). In the system, we extract both
the minimal GHKM rules (Galley et al., 2004), and
the rules of SPMT Model 1 (Galley et al., 2006)
with phrases up to length L=5 on the source side.
We then obtain the composed rules by composing
two or three adjacent minimal rules.
To build the above s2t system, we first use the
parse tree, which is generated by parsing the
English side of the bilingual data with the Berkeley
parser (Petrov et al., 2006). Then, we binarize the
English parse trees using the head binarization
approach (Wang et al., 2007) and use the resulting
binary parse trees to build another s2t system.
For the U-trees, we run the Gibbs sampler for
1000 iterations on the whole corpus. The sampler
uses 1,087s per iteration, on average, using a single
core, 2.3 GHz Intel Xeon machine. For the
hyperparameters, we set Į to 0.1 and pexpmrd = 1/3
to give a preference to the rules with small
fragments. We built an s2t translation system with
the achieved U-trees after the 1000th iteration. We
only use one sample to extract the translation
grammar because multiple samples would result in
a grammar that would be too large.
</bodyText>
<listItem confidence="0.8533222">
11 From (Zollmann and Vogel, 2011), we find that the
performance of SAMT system is similar with the
method of labeling SCFG rules with POS tags. Thus, to
be convenient, we only conduct experiments with the
SAMT system.
</listItem>
<subsectionHeader confidence="0.998809">
6.2 Analysis of The Gibbs Sampler
</subsectionHeader>
<bodyText confidence="0.997137333333333">
To evaluate the effectiveness of the Gibbs sampler,
we explore the change of the training data’s
likelihood with increasing sampling iterations.
</bodyText>
<figure confidence="0.931645">
100 200 300 400 500 600 700 800 900 1000
Number of Sampling Iterations
</figure>
<figureCaption confidence="0.95108025">
Figure 5. Histograms of the training data’s likelihood vs.
the number of sampling iterations. In the figure, random
1 to 3 refers to three independent runs of the sampler
with different initial U-trees as initialization states.
</figureCaption>
<bodyText confidence="0.997398538461538">
Figure 5 depicts the negative-log likelihood of
the training data after several sampling iterations.
The results show that the overall likelihood of the
training data is improved by the sampler. Moreover,
comparing the three independent runs, we see that
although the sampler begins with different initial
U-trees, the training data’s likelihood is always
similar during sampling. This demonstrates that
our sampler is not sensitive to the random initial
U-trees and can always arrive at a good final state
beginning from different initialization states. Thus,
we only utilize the U-trees from random 1 for
further analysis hereafter.
</bodyText>
<figure confidence="0.819515">
100 200 300 400 500 600 700 800 900 1000
Number of Sampling Iterations
</figure>
<figureCaption confidence="0.9377865">
Figure 6. The total number of frontier nodes for the
three independent runs.
</figureCaption>
<subsectionHeader confidence="0.999263">
6.3 Analysis of the U-tree Structure
</subsectionHeader>
<bodyText confidence="0.971836">
Acquiring better U-trees for translation is our final
purpose. However, are the U-trees achieved by the
</bodyText>
<figure confidence="0.850662722222222">
random 1
random 2
random 3
random 1
random 2
random 3
Negative-Log Likelihood 1.259E+08
1.255E+08
1.251E+08
1.247E+08
1.243E+08
1.239E+08
Total Number of Frontier Nodes 1.060E+07
1.055E+07
1.050E+07
1.045E+07
1.040E+07
1.035E+07
</figure>
<page confidence="0.991228">
250
</page>
<bodyText confidence="0.999802538461538">
Gibbs sampler appropriate for the tree-based
translation model?
To answer this question, we first analyze the
effect of the sampler on the U-trees. Figure 6
shows the total number of frontier nodes in the
training data during sampling. The results show
that the number of frontier nodes increases with
increased sampling. This tendency indicates that
our sampler prefers the tree structure with more
frontier nodes. Consequently, the final U-tree
structures can always be factored into many small
minimal translation rules. Just as we have argued
in section 4.1, this is beneficial for a good
translation grammar.
To demonstrate the above analysis, Figure 7
shows a visual comparison between our U-tree
(from random 1) and the binary parse tree (found
by head binarization). Because the traditional parse
tree is not binarized, we do not consider it for this
analysis. Figure 7 shows that whether it is the
target tree fragment or the source string of the rule,
our U-trees always tend to obtain the smaller
ones12. This comparison verifies that our Bayesian
tree induction model is effective in shifting the tree
structures away from complex minimal rules,
which tend to negatively affect translation.
</bodyText>
<figure confidence="0.993648866666667">
1000k
U-Tree
binary parse tree
600k
400k
200k
0
2 3 4 5 6 7 8 9 10 &gt;=11
Number of Nodes in the Target Tree Fragment
1200k
900k
600k
300k
0
1 2 3 4 5 6 7
</figure>
<subsectionHeader confidence="0.529667">
Number of Words and Variables in the Source String
</subsectionHeader>
<bodyText confidence="0.825225157894737">
Figure 7. Histograms over minimal translation rule
statistics comparing our U-trees and binary parse trees.
12 Binary parse trees get more tree fragments with two
nodes than U-trees. This is because there are many
unary edges in the binary parse trees, while no unary
edge exists in our U-trees.
Specifically, we show an example of a binary
parse tree and our U-tree in Figure 8. The example
U-tree is more conducive to extracting effective
translation rules. For example, to translate the
Chinese phrase “ӵ Ѫ”, we can extract a rule (R2
in Figure 9) directly from the U-tree because the
phrase “ӵ Ѫ” is governed by a frontier node, i.e.,
node “VBD+RB”. However, because no node
governs “ӵ Ѫ” in the binary parse tree, we can
only obtain a rule (R1 in Figure 9) with many extra
nodes and edges, such as node CD in R1. Due to
these extra things, R1 is too large to show good
generality.
</bodyText>
<figure confidence="0.9814568">
ӵ Ѫ аॳӄⲮ 㖾ݳ
(a) binary parse tree
VBD...NNS
ӵ Ѫ аॳӄⲮ 㖾ݳ
(b) U-tree
</figure>
<figureCaption confidence="0.975358666666667">
Figure 8. Example of different tree structures. The node
NP-COMP is achieved by head binarization. The bold
italic nodes with shadows denote frontier nodes.
Figure 9. Example rules to translate the Chinese phrase
“ӵ Ѫ.” R1 is extracted from Figure 8(a), i.e., the
binary parse tree. R2 is from Figure 8(b), i.e., the U-tree.
</figureCaption>
<figure confidence="0.999729852941177">
NP
NP
NP-COMP
VBD RB CD NNP NNS
QP
was
only
US dollars
1500
was only 1500 US dollars
VBD
VBD+RB
RB
CD
CD...NNS
NNP
NNP+NNS
NNS
was QP
NP-COMP:x1
RB
CD:x0
VBD
NP
ӵ Ѫ x0 x1
NP
only
VBD+RB
VBD RB
ӵ Ѫ
was only
Number of Rules
Number of Rules
800k
</figure>
<page confidence="0.992857">
251
</page>
<bodyText confidence="0.999885272727273">
Based on the above analysis, we can conclude
that our proposed U-tree structures are conducive
to extracting small, minimal translation rules. This
indicates that the U-trees are more consistent with
the word alignment and are good at capturing
bilingual mapping information. Therefore, because
parse trees are always constrained by cross-lingual
structure divergence, we believe that the proposed
U-trees would result in a better translation
grammar. We demonstrate this conclusion in the
next sub-section.
</bodyText>
<subsectionHeader confidence="0.970454">
6.4 Final Translation Results
</subsectionHeader>
<bodyText confidence="0.99994784">
The final translation results are shown in Table 1.
In the table, lines 3-6 refer to the string-to-tree
systems built with different types of tree structures.
Table 1 shows that all our s2t systems
outperform the Joshua (HPB) and Joshua (SAMT)
system significantly. This comparison verifies the
superiority of our in-house s2t system. Moreover,
the results shown in Table 1 also demonstrate the
effectiveness of head binarization, which helps to
improve the s2t system using parse trees in all
translation tasks.
To test the effectiveness of our U-trees, we give
the s2t translation system using the U-trees (from
random 1). The results show that the system using
U-trees achieves the best translation result from all
of the systems. It surpasses the s2t system using
parse trees by 1.47 BLEU points on MT04 and
1.44 BLEU points on MT05. Moreover, even using
the binary parse trees, the achieved s2t system is
still lower than our U-tree-based s2t system by
0.97 BLEU points on the combined test set. From
the translation results, we can validate our former
analysis that the U-trees generated by our Bayesian
tree induction model are more appropriate for
string-to-tree translation than parse trees.
</bodyText>
<table confidence="0.999550166666667">
System MT04 MT05 All
Joshua (HPB) 31.73 28.82 30.64
Joshua (SAMT) 32.48 29.77 31.56
s2t (parse-tree) 33.73* 30.25* 32.75*
s2t (binary-parse-tree) 34.09* 30.99*# 32.92*
s2t (U-tree) 35.20*# 31.69*# 33.89*#
</table>
<tableCaption confidence="0.7653038">
Table 1. Results (in case-insensitive BLEU-4 scores) of
s2t systems using different types of trees. The “*” and
“#” denote that the results are significantly better than
the Joshua (SAMT) system and the s2t system using
parse trees (p&lt;0.01).
</tableCaption>
<subsectionHeader confidence="0.990151">
6.5 Large Data
</subsectionHeader>
<bodyText confidence="0.999293333333333">
We also conduct an experiment on a larger
bilingual training data from the LDC corpus13. The
training corpus contains 2.1M sentence pairs with
approximately 27.7M Chinese words and 31.9M
English words. Similarly, we train a 5-gram
language model using the Xinhua portion of the
English Gigaword corpus and the English part of
the training corpus. With the same settings as
before, we run the Gibbs sampler for 1000
iterations and utilize the final U-tree structure to
build a string-to-tree translation system.
The final BLEU score results are shown in Table
2. In the scenario with a large data, the string-to-
tree system using our U-trees still significantly
outperforms the system using parse trees.
</bodyText>
<table confidence="0.9995265">
System MT04 MT05 All
Joshua (HPB) 34.55 33.11 34.01
Joshua (SAMT) 34.76 33.72 34.37
s2t (parse-tree) 36.40* 34.53* 35.70*
s2t (binary-parse-tree) 37.38*# 35.14*# 36.54*#
s2t (U-tree) 38.02*# 36.12*# 37.34*#
</table>
<tableCaption confidence="0.948511333333333">
Table 2. Results (in case-insensitive BLEU-4 scores) for
the large training data. The meaning of “*” and “#” are
similar to Table 1.
</tableCaption>
<sectionHeader confidence="0.979007" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.998735166666667">
In this paper, we explored a new direction to build
a tree-based model based on unsupervised
Bayesian trees rather than supervised parse trees.
To achieve this purpose, we have made two major
efforts in this paper:
(1) We have proposed a novel generative
Bayesian model to induce effective U-trees for
tree-based translation. We utilized STSG in the
model to grasp bilingual mapping information. We
further imposed a reasonable hierarchical prior on
the tree structures, encouraging small and frequent
minimal rules for translation.
(2) To train the Bayesian tree induction
model efficiently, we developed a Gibbs sampler
with three novel Gibbs operators. The operators are
designed specifically to explore the infinite space
of tree structures by performing local changes on
the tree structure.
</bodyText>
<footnote confidence="0.516219666666667">
13 LDC category number : LDC2000T50, LDC2002E18,
LDC2003E07, LDC2004T07, LDC2005T06,
LDC2002L27, LDC2005T10 and LDC2005T34.
</footnote>
<page confidence="0.994883">
252
</page>
<bodyText confidence="0.999937625">
Experiments on the string-to-tree translation
model demonstrated that our U-trees are better
than the parse trees. The translation results verify
that the well-designed unsupervised trees are
actually more appropriate for tree-based translation
than parse trees. Therefore, we believe that the
unsupervised tree structure would be a promising
research direction for tree-based translation.
In future, we plan to testify our sampler with
various initial trees, such as the tree structure
formed by (Zhang et al., 2008). We also plan to
perform a detailed empirical comparison between
STST and SCFG under our settings. Moreover, we
will further conduct experiments to compare our
methods with other relevant works, such as (Cohn
and Blunsom, 2009) and (Burkett and Klein, 2012).
</bodyText>
<sectionHeader confidence="0.998464" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999857">
We would like to thank Philipp Koehn and three
anonymous reviewers for their valuable comments
and suggestions. The research work has been
funded by the Hi-Tech Research and Development
Program (“863” Program) of China under Grant
No. 2011AA01A207, 2012AA011101, and
2012AA011102.
</bodyText>
<sectionHeader confidence="0.997102" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999812608695652">
Phil Blunsom, Trevor Cohn, Miles Osborne. 2008.
Bayesian synchronous grammar induction. In
Advances in Neural Information Processing Systems,
volume 21, pages 161-168.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles
Osborne. 2009. A gibbs sampler for phrasal
synchronous grammar induction. In Proc. of ACL
2009, pages 782-790.
Phil Blunsom and Trevor Cohn. 2010. Inducing
synchronous grammars with slice sampling. In Proc.
of NAACL 2010, pages 238-241.
David Burkett and Dan Klein. 2008. Two languages are
better than one (for syntactic Parsing). In Proc. of
EMNLP 2008, pages 877-886.
David Burkett, John Blitzer, and Dan Klein. 2010. Joint
parsing and alignment with weakly synchronized
grammars. In Proc. of NAACL 2010, pages 127-135.
David Burkett and Dan Klein. 2012. Transforming trees
to improve syntactic convergence. In Proc. of
EMNLP 2012, pages 863-872.
David Chiang. 2007. Hierarchical phrase-based
translation. Computational Linguistics, 33 (2). pages
201-228.
Dekai Wu. 1996. A polynomial-time algorithm for
statistical machine translation. In Proc. of ACL 1996,
pages 152-158.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23:377-404.
Trevor Cohn and Phil Blunsom. 2009. A bayesian
model of syntax-directed tree to string grammar
induction. In Proc. of EMNLP 2009, pages 352-361.
Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2010. Inducing tree-substitution grammars. Journal
of Machine Learning Research, pages 3053-3096.
Brooke Cowan, Ivona Kucerova and Michael Collins.
2006. A discriminative model for tree-to-tree
translation. In Proc. of EMNLP 2006, pages 232-241.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In Proc.
of ACL 2007, pages 17-24.
John DeNero and Jakob Uszkoreit. 2011. Inducing
sentence structure from parallel corpora for
reordering. In Proc. of EMNLP 2011, pages 193-203.
Chris Dyer. 2010. Two monolingual parses are better
than one (synchronous parse). In Proc. of NAACL
2010, pages 263-266.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proc. of ACL
2003, pages 205-208.
Michel Galley, Mark Hopkins, Kevin Knight and Daniel
Marcu. 2004. What’s in a translation rule. In Proc. of
HLT-NAACL 2004, pages 273–280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc. of
ACL-COLING 2006, pages 961-968.
Jonathan Weese, Juri Ganitkevitch, Chris Callison-
Burch, Matt Post and Adam Lopez. 2011. Joshua 3.0:
syntax-based machine translation with the thrax
Grammar Extractor. In Proc of WMT11, pages 478-
484.
Liang Huang, Kevin Knight and Aravind Joshi. 2006. A
syntax-directed translator with extended domain of
locality. In Proc. of AMTA 2006, pages 65-73.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical phrase-based translation, In Proc. of
HLT/NAACL 2003, pages 48-54.
</reference>
<page confidence="0.981783">
253
</page>
<reference confidence="0.999896806122449">
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. of EMNLP
2004, pages 388–395.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer and Ondrej Bojar. 2007.
Moses: open source toolkit for statistical machine
translation. In Proc. of ACL 2007, pages 177-180.
Abby Levenberg, Chris Dyer and Phil Blunsom. 2012.
A bayesian model for learning SCFGs with
discontiguous Rules. In Proc. of EMNLP 2012, pages
223-232.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren N.G. Thornton, Jonathan Weese and Omar F.
Zaidan. 2009. Joshua: An open source toolkit for
parsing-based machine translation. In Proc. of ACL
2009, pages 135-139.
Shujie Liu, Chi-Ho Li, Mu Li, Ming Zhou. 2012. Re-
training monolingual parser bilingually for syntactic
SMT. In Proc. of EMNLP 2012, pages 854-862.
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine
translation. In Proc. of ACL-COLING 2006, pages
609-616.
Yang Liu, Yajuan Lv and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Proc.
of ACL-IJCNLP 2009, pages 558-566.
Daniel Marcu, Wei Wang, Abdessamad Echihabi and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactified target language phrases.
In Proc. of EMNLP 2006, pages 44-52.
Franz Och, 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL 2003,
pages 160-167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proc. of ACL
2002, pages 311-318.
Slav Petrov, Leon Barrett, Romain Thibaux and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proc. of COLING-
ACL 2006, pages 433-440.
Chris Quirk, Arul Menezes and Colin Cherry. 2005.
Dependency treelet translation: syntactically
informed phrasal SMT. In Proc. of ACL 2005, pages
271-279.
Libin Shen, Jinxi Xu and Ralph Weischedel. 2008. A
new string-to-dependency machine translation
algorithm with a target dependency language model.
In Proc. of ACL-08, pages 577-585.
Wei Wang, Kevin Knight, and Daniel Marcu. 2007.
Binarizing syntax trees to improve syntax-based
machine translation accuracy. In Proc. of EMNLP
2007, pages 746-754.
Wei Wang, Jonathan May, Kevin Knight, and Daniel
Marcu. 2010. Re-structuring, re-labeling, and re-
aligning for syntax-based machine translation.
Computational Linguistics, 36(2):247–277.
Feifei Zhai, Jiajun Zhang, Yu Zhou and Chengqing
Zong. 2012. Tree-based translation without using
parse trees. In Proc. of COLING 2012, pages 3037-
3054.
Hao Zhang, Liang Huang, Daniel Gildea and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proc. of HLT-NAACL 2006, pages
256-263.
Hao Zhang, Daniel Gildea, and David Chiang. 2008.
Extracting synchronous grammars rules from word
level alignments in linear time. In Proc. of COLING
2008, pages 1081-1088.
Hao Zhang, Licheng Fang, Peng Xu, Xiaoyun Wu.
2011a. Binarized forest to string translation. In Proc.
of ACL 2011, pages 835-845.
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, Chew
Lim Tan. 2009. Forest-based tree sequence to string
translation model. In Proc. of ACL-IJCNLP 2009,
pages 172-180.
Jiajun Zhang, Feifei Zhai and Chengqing Zong. 2011b.
Augmenting string-to-tree translation models with
fuzzy use of source-side syntax. In Proc. of EMNLP
2011, pages 204-215.
Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Chew
Lim Tan and Sheng Li. 2007. A tree-to-tree
alignment-based model for statistical Machine
translation. MT-Summit-07. pages 535-542
Min Zhang, Hongfei Jiang, Ai ti Aw, Haizhou Li, Chew
Lim Tan and Sheng Li. 2008. A tree sequence
alignment-based tree-to-tree translation model. In
Proc. of ACL 2008, pages 559-567.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proc. of Workshop on Statistical Machine
Translation 2006, pages 138-141.
Andreas Zollmann and Stephan Vogel. 2011. A word-
class approach to labeling PSCFG rules for machine
translation. In Proc. of ACL 2011, pages 1-11.
</reference>
<page confidence="0.998643">
254
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.471412">
<title confidence="0.999165">Unsupervised Tree Induction for Tree-based Translation</title>
<author confidence="0.524136">Feifei Zhai</author>
<author confidence="0.524136">Jiajun Zhang</author>
<author confidence="0.524136">Yu Zhou</author>
<author confidence="0.524136">Chengqing</author>
<affiliation confidence="0.830577">National Laboratory of Pattern Recognition, Institute of Chinese Academy of Sciences, Beijing,</affiliation>
<email confidence="0.976088">ffzhai@nlpr.ia.ac.cn</email>
<email confidence="0.976088">jjzhang@nlpr.ia.ac.cn</email>
<email confidence="0.976088">yzhou@nlpr.ia.ac.cn</email>
<email confidence="0.976088">cqzong@nlpr.ia.ac.cn</email>
<abstract confidence="0.999889277777778">In current research, most tree-based translation models are built directly from parse trees. In this study, we go in another direction and build a translation model with an unsupervised tree structure derived from a novel non-parametric Bayesian model. In the model, we utilize synchronous tree substitution grammars (STSG) to capture the bilingual mapping between language pairs. To train the model efficiently, we develop a Gibbs sampler with three novel Gibbs operators. The sampler is capable of exploring the infinite space of tree structures by performing local changes on the tree nodes. Experimental results show that the string-totree translation system using our Bayesian tree structures significantly outperforms the strong baseline string-to-tree system using parse trees.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Miles Osborne</author>
</authors>
<title>Bayesian synchronous grammar induction.</title>
<date>2008</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<volume>21</volume>
<pages>161--168</pages>
<contexts>
<context position="5609" citStr="Blunsom et al. (2008" startWordPosition="842" endWordPosition="845"> direction to build a tree-based translation model with effective unsupervised U-tree structures. For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering. Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al. (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al. (2010) focused o</context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2008</marker>
<rawString>Phil Blunsom, Trevor Cohn, Miles Osborne. 2008. Bayesian synchronous grammar induction. In Advances in Neural Information Processing Systems, volume 21, pages 161-168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Chris Dyer</author>
<author>Miles Osborne</author>
</authors>
<title>A gibbs sampler for phrasal synchronous grammar induction.</title>
<date>2009</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>782--790</pages>
<contexts>
<context position="11941" citStr="Blunsom et al. 2009" startWordPosition="1880" endWordPosition="1883">power on tree construction than SCFG. In a STSG-based U-tree or a STSG rule, although not linguistically informed, the nodes labeled by POS tags are also effective on distinguishing different ones. However, with SCFG, we have to discard all the internal nodes (i.e., flattening the Utrees or rules) to express the same sequence, leading to a poor ability of distinguishing different U-trees and production rules. Thus, using STSG, we can build more specific U-trees for translation. In addition, we find that the Bayesian SCFG grammar cannot even significantly outperform the heuristic SCFG grammar (Blunsom et al. 2009)5. This would indicate that the SCFG-based derivation tree as by-product is also not such good for tree-based translation models. Considering the above reasons, we believe that the STSG-based learning procedure would result in a better translation grammar for tree-based models. 5 In (Blunsom et al., 2009), for Chinese-to-English translation, the Bayesian SCFG grammar only outperform the heuristic SCFG grammar by 0.1 BLEU points on NIST MT 2004 and 0.6 BLEU points on NIST MT 2005 in the NEWS domain. ᡁ x1 x0 VBP+RB wo-men we VBP:x0 RB:x1 PRP i 245 4 Bayesian Model In this section, we present a </context>
<context position="17044" citStr="Blunsom et al., 2009" startWordPosition="2782" endWordPosition="2785"> Expand PRP and generate the word “we” for PRP; 6 In our experiment, we set pexpand to 1/3 to encourage small tree fragments. i n � +D P 0 ( |, , , ) i � i r N p r r N P D i N 0 ( |) r N i i n � � D N N (3) (2) 246 c. Determine that the right child of PRP...RB is a two-word non-terminal; d. Utilize the predetermined RB and a POS tag VBP to form the tag of the two-word nonterminal: VBP+RB; e. Expand VBP+RB (to VBP and RB); f. Do not expand VBP and RB. P(str |frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables. Inspired by (Blunsom et al., 2009) and (Cohn and Blunsom, 2009), we define P(str |frag) as follows: where csw is the number of words in the source string. s means the source vocabulary set. Further, cvar denotes the number of variables, which is determined by the tree fragment frag. As shown in Equation(5), we first determine how many source words to generate using a Poisson distribution Ppoisson(csw;1), which imposes a stable preference for short source strings. Then, we draw each source word from a uniform distribution over s. Afterwards, we insert the variables into the string. The variables are inserted one at a time usi</context>
</contexts>
<marker>Blunsom, Cohn, Dyer, Osborne, 2009</marker>
<rawString>Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Osborne. 2009. A gibbs sampler for phrasal synchronous grammar induction. In Proc. of ACL 2009, pages 782-790.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
</authors>
<title>Inducing synchronous grammars with slice sampling.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL</booktitle>
<pages>238--241</pages>
<marker>Blunsom, Cohn, 2010</marker>
<rawString>Phil Blunsom and Trevor Cohn. 2010. Inducing synchronous grammars with slice sampling. In Proc. of NAACL 2010, pages 238-241.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Burkett</author>
<author>Dan Klein</author>
</authors>
<title>Two languages are better than one (for syntactic Parsing).</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>877--886</pages>
<contexts>
<context position="6173" citStr="Burkett and Klein (2008)" startWordPosition="924" endWordPosition="927"> the model more freely and effectively. Blunsom et al. (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. Cohn and Blunsom (2009) adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on gene</context>
</contexts>
<marker>Burkett, Klein, 2008</marker>
<rawString>David Burkett and Dan Klein. 2008. Two languages are better than one (for syntactic Parsing). In Proc. of EMNLP 2008, pages 877-886.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Burkett</author>
<author>John Blitzer</author>
<author>Dan Klein</author>
</authors>
<title>Joint parsing and alignment with weakly synchronized grammars.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL</booktitle>
<pages>127--135</pages>
<contexts>
<context position="6199" citStr="Burkett et al. (2010)" startWordPosition="929" endWordPosition="932">fectively. Blunsom et al. (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. Cohn and Blunsom (2009) adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervi</context>
</contexts>
<marker>Burkett, Blitzer, Klein, 2010</marker>
<rawString>David Burkett, John Blitzer, and Dan Klein. 2010. Joint parsing and alignment with weakly synchronized grammars. In Proc. of NAACL 2010, pages 127-135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Burkett</author>
<author>Dan Klein</author>
</authors>
<title>Transforming trees to improve syntactic convergence.</title>
<date>2012</date>
<booktitle>In Proc. of EMNLP 2012,</booktitle>
<pages>863--872</pages>
<contexts>
<context position="6577" citStr="Burkett and Klein (2012)" startWordPosition="990" endWordPosition="993">use we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. Cohn and Blunsom (2009) adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes. Our work differs from theirs in that we present a Bayesian model to learn effective STSG tr</context>
</contexts>
<marker>Burkett, Klein, 2012</marker>
<rawString>David Burkett and Dan Klein. 2012. Transforming trees to improve syntactic convergence. In Proc. of EMNLP 2012, pages 863-872.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<pages>201--228</pages>
<contexts>
<context position="5826" citStr="Chiang, 2007" startWordPosition="875" endWordPosition="876">ees of source sentences for syntactic pre-reordering. Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al. (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. Cohn and Blunsom (2009) adopted a Bayesian method to infer an STSG by exploring the spac</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33 (2). pages 201-228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>A polynomial-time algorithm for statistical machine translation.</title>
<date>1996</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>152--158</pages>
<marker>Wu, 1996</marker>
<rawString>Dekai Wu. 1996. A polynomial-time algorithm for statistical machine translation. In Proc. of ACL 1996, pages 152-158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--377</pages>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23:377-404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Phil Blunsom</author>
</authors>
<title>A bayesian model of syntax-directed tree to string grammar induction.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>352--361</pages>
<contexts>
<context position="6361" citStr="Cohn and Blunsom (2009)" startWordPosition="955" endWordPosition="958">FG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. Cohn and Blunsom (2009) adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended</context>
<context position="14069" citStr="Cohn and Blunsom (2009)" startWordPosition="2243" endWordPosition="2246"> N in r~ i . Thanks to the exchangeability of the model, all permutations of the rules are actually equiprobable. This means that we can compute the probability of each rule based on the previous and subsequent rules (i.e. consider each rule as the last one). This characteristic allows us to design an efficient Gibbs sampling algorithm to train the Bayesian model. 4.1 Base Distribution The base distribution P0(r |N) is designed to assign prior probabilities to the STSG production rules. Because each rule r consists of a target tree fragment frag and a source string str in the model, we follow Cohn and Blunsom (2009) and decompose the prior probability P0(r |N) into two factors as follows: P0(r |N) P( frag  |N) P(str |frag) (4) where P( frag |N) is the probability of producing the target tree fragment frag. To generate frag, Cohn and Blunsom (2009) used a geometric prior to decide how many child nodes to assign each node. Differently, we require that each multi-word non-terminal node must have two child nodes. This is because the binary structure has been verified to be very effective for tree-based translation (Wang et al., 2007; Zhang et al., 2011a). The generation process starts at root node N. At fir</context>
<context position="17073" citStr="Cohn and Blunsom, 2009" startWordPosition="2787" endWordPosition="2790">e word “we” for PRP; 6 In our experiment, we set pexpand to 1/3 to encourage small tree fragments. i n � +D P 0 ( |, , , ) i � i r N p r r N P D i N 0 ( |) r N i i n � � D N N (3) (2) 246 c. Determine that the right child of PRP...RB is a two-word non-terminal; d. Utilize the predetermined RB and a POS tag VBP to form the tag of the two-word nonterminal: VBP+RB; e. Expand VBP+RB (to VBP and RB); f. Do not expand VBP and RB. P(str |frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables. Inspired by (Blunsom et al., 2009) and (Cohn and Blunsom, 2009), we define P(str |frag) as follows: where csw is the number of words in the source string. s means the source vocabulary set. Further, cvar denotes the number of variables, which is determined by the tree fragment frag. As shown in Equation(5), we first determine how many source words to generate using a Poisson distribution Ppoisson(csw;1), which imposes a stable preference for short source strings. Then, we draw each source word from a uniform distribution over s. Afterwards, we insert the variables into the string. The variables are inserted one at a time using a uniform distribution ove</context>
<context position="19888" citStr="Cohn and Blunsom (2009)" startWordPosition="3239" endWordPosition="3242">each part. The created tree nodes are labeled by the non-terminals described in section 3. Using the initial target U-trees, source sentences and word alignment, we extract minimal GHKM translation rules7 in terms of frontier nodes (Galley et al., 2004). Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment. For example, the bold italic nodes with shadows in Figure 2 are frontier nodes. In addition, it should be noted that the word alignment is fixed8, and we only explore the entire space of tree structures in our sampler. Differently, Cohn and Blunsom (2009) designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment. We believe that it is possible to investigate the space of both tree structure and alignment simultaneously. This subject will be one of our future work topics. For each training instance (a pair of source sentence and target U-tree structure), the extracted GHKM minimal translation rules compose a unique STSG derivation9. Moreover, all the rules developed from the training data constitute an initial STSG for the Gibbs sampler. 7 We attach the unaligned word to the lowest frontier node that </context>
</contexts>
<marker>Cohn, Blunsom, 2009</marker>
<rawString>Trevor Cohn and Phil Blunsom. 2009. A bayesian model of syntax-directed tree to string grammar induction. In Proc. of EMNLP 2009, pages 352-361.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Phil Blunsom</author>
<author>Sharon Goldwater</author>
</authors>
<title>Inducing tree-substitution grammars.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3053--3096</pages>
<marker>Cohn, Blunsom, Goldwater, 2010</marker>
<rawString>Trevor Cohn, Phil Blunsom, and Sharon Goldwater. 2010. Inducing tree-substitution grammars. Journal of Machine Learning Research, pages 3053-3096.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brooke Cowan</author>
</authors>
<title>Ivona Kucerova and Michael Collins.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>232--241</pages>
<marker>Cowan, 2006</marker>
<rawString>Brooke Cowan, Ivona Kucerova and Michael Collins. 2006. A discriminative model for tree-to-tree translation. In Proc. of EMNLP 2006, pages 232-241.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Tailoring word alignments to syntactic machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>17--24</pages>
<contexts>
<context position="18197" citStr="DeNero and Klein (2007)" startWordPosition="2968" endWordPosition="2971">ariables into the string. The variables are inserted one at a time using a uniform distribution over the possible positions. This factor discourages more variables. For the example rule in Figure 1, the generative process of the source string is: a. Decide to generate one source word; b. Generate the source word “ᡁ (wo-men) Ó; c. Insert the first variable after the word; d. Insert the second variable between the word and the first variable. Intuitively, a good translation grammar should carry both small translation rules with enough generality and large rules with enough context information. DeNero and Klein (2007) proposed this statement, and Cohn and Blunsom (2009) has verified it in their experiments with parse trees. Our base distribution is also designed based on this intuition. Considering the two factors in our base distribution, we penalize both large target tree fragments with many nodes and long source strings with many words and variables. The Bayesian model tends to select both small and frequent STSG production rules to construct the U-trees. With these types of trees, we can extract small rules with good generality and simultaneously obtain large rules with enough context information by co</context>
</contexts>
<marker>DeNero, Klein, 2007</marker>
<rawString>John DeNero and Dan Klein. 2007. Tailoring word alignments to syntactic machine translation. In Proc. of ACL 2007, pages 17-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Jakob Uszkoreit</author>
</authors>
<title>Inducing sentence structure from parallel corpora for reordering.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>193--203</pages>
<contexts>
<context position="5157" citStr="DeNero and Uszkoreit (2011)" startWordPosition="771" endWordPosition="774">Bayesian model efficiently. The remainder of the paper is organized as follows. Section 2 introduces the related work. Section 3 describes the STSG generation process, and Section 4 depicts the adopted Bayesian model. Section 5 describes the Gibbs sampling algorithm and Gibbs operators. In Section 6, we analyze the achieved U-trees and evaluate their effectiveness. Finally, we conclude the paper in Section 7. 2 Related Work In this study, we move in a new direction to build a tree-based translation model with effective unsupervised U-tree structures. For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering. Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al. (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used </context>
</contexts>
<marker>DeNero, Uszkoreit, 2011</marker>
<rawString>John DeNero and Jakob Uszkoreit. 2011. Inducing sentence structure from parallel corpora for reordering. In Proc. of EMNLP 2011, pages 193-203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
</authors>
<title>Two monolingual parses are better than one (synchronous parse).</title>
<date>2010</date>
<booktitle>In Proc. of NAACL</booktitle>
<pages>263--266</pages>
<marker>Dyer, 2010</marker>
<rawString>Chris Dyer. 2010. Two monolingual parses are better than one (synchronous parse). In Proc. of NAACL 2010, pages 263-266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Learning non-isomorphic tree mappings for machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>205--208</pages>
<marker>Eisner, 2003</marker>
<rawString>Jason Eisner. 2003. Learning non-isomorphic tree mappings for machine translation. In Proc. of ACL 2003, pages 205-208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule.</title>
<date>2004</date>
<booktitle>In Proc. of HLT-NAACL</booktitle>
<pages>273--280</pages>
<contexts>
<context position="1511" citStr="Galley et al., 2004" startWordPosition="206" endWordPosition="209">how that the string-totree translation system using our Bayesian tree structures significantly outperforms the strong baseline string-to-tree system using parse trees. 1 Introduction In recent years, tree-based translation models1 are drawing more and more attention in the community of statistical machine translation (SMT). Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree-based translation models have shown promising progress in improving translation quality (Liu et al., 2006, 2009; Quirk et al., 2005; Galley et al., 2004, 2006; Marcu et al., 2006; Shen et al., 2008; Zhang et al., 2011b). However, tree-based translation models always suffer from two major challenges: 1) They are usually built directly from parse trees, which are generated by supervised linguistic parsers. 1 A tree-based translation model is defined as a model using tree structures on one side or both sides. However, for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of Tree-bank resources for training. 2) Parse trees are actually only used to model and explain the monolingual structure, ra</context>
<context position="19518" citStr="Galley et al., 2004" startWordPosition="3176" endWordPosition="3179">ning by Gibbs Sampling In this section, we introduce a collapsed Gibbs sampler, which enables us to train the Bayesian model efficiently. 5.1 Initialization State At first, we use random binary trees to initialize the sampler. To get the initial U-trees, we recursively and randomly segment a sentence into two parts and simultaneously create a tree node to dominate each part. The created tree nodes are labeled by the non-terminals described in section 3. Using the initial target U-trees, source sentences and word alignment, we extract minimal GHKM translation rules7 in terms of frontier nodes (Galley et al., 2004). Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment. For example, the bold italic nodes with shadows in Figure 2 are frontier nodes. In addition, it should be noted that the word alignment is fixed8, and we only explore the entire space of tree structures in our sampler. Differently, Cohn and Blunsom (2009) designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment. We believe that it is possible to investigate the space of both tree structure and alignment simultaneously. This subject will</context>
<context position="21070" citStr="Galley et al., 2004" startWordPosition="3432" endWordPosition="3435">word to the lowest frontier node that can cover it in terms of word alignment. 8 The sampler might reinforce the frequent alignment errors (AE), which would harm the translation model (TM). Actually, the frequent AEs also greatly impair the conventional TM. Besides, our sampler encourages the correct alignments and simultaneously discourages the infrequent AEs. Thus, compared with the conventional TMs, we believe that our final TM would not be worse due to AEs. Our final experiments verify this point and we will conduct a much detailed analysis in future. 9 We only use the minimal GHKM rules (Galley et al., 2004) here to reduce the complexity of the sampler. c var 1�-7 1 P(str |frag) = Ppoisson (csw; 1) x x 11 ||c sw E s i =1c sw +i (5) 247 NN...RB jin-tian wo-men zai-ci jian-mian Ӻཙ ᡁ  㿱䶒 Figure 2. Illustration of an initial U-tree structure. The bold italic nodes with shadows are frontier nodes. Under this initial STSG, the sampler modifies the initial U-trees (initial sample) to create a series of new ones (new samples) by the Gibbs operators. Consequently, new STSGs are created based on the new U-trees simultaneously and used for the next sampling operation. Repeatedly and after a number of ite</context>
<context position="24233" citStr="Galley et al. 2004" startWordPosition="3986" endWordPosition="3989">rently in the left state (`P=0). We sample the `P of this node, and if the sampled value of `P is 0, we keep the structure unchanged, i.e., in the left state. Otherwise, we change its state to the right state (`P=1), and transform the U-tree to Figure 3(b) accordingly. Figure 3. Illustration of the Rotate operator. In the figure, (a) and (b) denote the s-node’s left state and right state respectively. The bold italic nodes with shadows in the figure are frontier nodes. Obviously, towards an s-node for sampling, the two values of `P would define two different U-trees. Using the GHKM algorithm (Galley et al. 2004), we can get two different STSG derivations from the two U-trees based on the fixed word alignment. Each derivation carries a set of STSG rules (i.e., minimal GHKM translation rules) of its own. In the two derivations, the STSG rules defined by the two states include the one rooted at the s-node’s lowest ancestor frontier node, and the one rooted at the s-node if it is a frontier node. For instance, in Figure 3(a), as the s-node is not a frontier node, the left state (`P=0) defines only one rule: rleft x x x : 0 2 1 PRP ... RB(PRP + VBP(x0 : PRP x1: VBP) x2 : RB) Differently, in Figure 3(b), t</context>
<context position="30411" citStr="Galley et al., 2004" startWordPosition="5052" endWordPosition="5055">y. The statistical significance test is performed by the re-sampling approach (Koehn, 2004). To create the baseline system, we use the opensource Joshua 4.0 system (Ganitkevitch et al., 2012) to build a hierarchical phrase-based (HPB) system, and a syntax-augmented MT (SAMT) 11 system (Zollmann and Venugopal, 2006) respectively. The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t). The system is implemented based on (Galley et al., 2006) and (Marcu et al. 2006). In the system, we extract both the minimal GHKM rules (Galley et al., 2004), and the rules of SPMT Model 1 (Galley et al., 2006) with phrases up to length L=5 on the source side. We then obtain the composed rules by composing two or three adjacent minimal rules. To build the above s2t system, we first use the parse tree, which is generated by parsing the English side of the bilingual data with the Berkeley parser (Petrov et al., 2006). Then, we binarize the English parse trees using the head binarization approach (Wang et al., 2007) and use the resulting binary parse trees to build another s2t system. For the U-trees, we run the Gibbs sampler for 1000 iterations on t</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight and Daniel Marcu. 2004. What’s in a translation rule. In Proc. of HLT-NAACL 2004, pages 273–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proc. of ACL-COLING</booktitle>
<pages>961--968</pages>
<contexts>
<context position="30310" citStr="Galley et al., 2006" startWordPosition="5033" endWordPosition="5036"> set. The translation quality is evaluated by case-insensitive BLEU-4 with the shortest length penalty. The statistical significance test is performed by the re-sampling approach (Koehn, 2004). To create the baseline system, we use the opensource Joshua 4.0 system (Ganitkevitch et al., 2012) to build a hierarchical phrase-based (HPB) system, and a syntax-augmented MT (SAMT) 11 system (Zollmann and Venugopal, 2006) respectively. The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t). The system is implemented based on (Galley et al., 2006) and (Marcu et al. 2006). In the system, we extract both the minimal GHKM rules (Galley et al., 2004), and the rules of SPMT Model 1 (Galley et al., 2006) with phrases up to length L=5 on the source side. We then obtain the composed rules by composing two or three adjacent minimal rules. To build the above s2t system, we first use the parse tree, which is generated by parsing the English side of the bilingual data with the Berkeley parser (Petrov et al., 2006). Then, we binarize the English parse trees using the head binarization approach (Wang et al., 2007) and use the resulting binary parse </context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proc. of ACL-COLING 2006, pages 961-968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Weese</author>
<author>Juri Ganitkevitch</author>
<author>Chris CallisonBurch</author>
<author>Matt Post</author>
<author>Adam Lopez</author>
</authors>
<title>Joshua 3.0: syntax-based machine translation with the thrax Grammar Extractor.</title>
<date>2011</date>
<booktitle>In Proc of WMT11,</booktitle>
<pages>478--484</pages>
<marker>Weese, Ganitkevitch, CallisonBurch, Post, Lopez, 2011</marker>
<rawString>Jonathan Weese, Juri Ganitkevitch, Chris CallisonBurch, Matt Post and Adam Lopez. 2011. Joshua 3.0: syntax-based machine translation with the thrax Grammar Extractor. In Proc of WMT11, pages 478-484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>A syntax-directed translator with extended domain of locality.</title>
<date>2006</date>
<booktitle>In Proc. of AMTA</booktitle>
<pages>65--73</pages>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Liang Huang, Kevin Knight and Aravind Joshi. 2006. A syntax-directed translator with extended domain of locality. In Proc. of AMTA 2006, pages 65-73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation,</title>
<date>2003</date>
<booktitle>In Proc. of HLT/NAACL</booktitle>
<pages>48--54</pages>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Och, and Daniel Marcu. 2003. Statistical phrase-based translation, In Proc. of HLT/NAACL 2003, pages 48-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>388--395</pages>
<contexts>
<context position="29882" citStr="Koehn, 2004" startWordPosition="4969" endWordPosition="4970">4=1 : x0 x1 —&gt; PRP ...RB(x0 : PRP x1 : VBP+ RB) : x1 x0 —&gt; VBP+ RB(x0 : VBP x1: RB) rright-0 rright -1 249 the training data. For tuning and testing, we use the NIST MT 2003 evaluation data as the development set, and use the NIST MT04 and MT05 data as the test set. We use MERT (Och, 2004) to tune parameters. Since MERT is prone to search errors, we run MERT 5 times and select the best tuning parameters in the tuning set. The translation quality is evaluated by case-insensitive BLEU-4 with the shortest length penalty. The statistical significance test is performed by the re-sampling approach (Koehn, 2004). To create the baseline system, we use the opensource Joshua 4.0 system (Ganitkevitch et al., 2012) to build a hierarchical phrase-based (HPB) system, and a syntax-augmented MT (SAMT) 11 system (Zollmann and Venugopal, 2006) respectively. The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t). The system is implemented based on (Galley et al., 2006) and (Marcu et al. 2006). In the system, we extract both the minimal GHKM rules (Galley et al., 2004), and the rules of SPMT Model 1 (Galley et al., 2006) with phrases up t</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proc. of EMNLP 2004, pages 388–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondrej Bojar</author>
</authors>
<title>Moses: open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>177--180</pages>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer and Ondrej Bojar. 2007. Moses: open source toolkit for statistical machine translation. In Proc. of ACL 2007, pages 177-180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abby Levenberg</author>
<author>Chris Dyer</author>
<author>Phil Blunsom</author>
</authors>
<title>A bayesian model for learning SCFGs with discontiguous Rules.</title>
<date>2012</date>
<booktitle>In Proc. of EMNLP 2012,</booktitle>
<pages>223--232</pages>
<contexts>
<context position="5851" citStr="Levenberg et al. (2012)" startWordPosition="877" endWordPosition="880">entences for syntactic pre-reordering. Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al. (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. Cohn and Blunsom (2009) adopted a Bayesian method to infer an STSG by exploring the space of alignments based on </context>
</contexts>
<marker>Levenberg, Dyer, Blunsom, 2012</marker>
<rawString>Abby Levenberg, Chris Dyer and Phil Blunsom. 2012. A bayesian model for learning SCFGs with discontiguous Rules. In Proc. of EMNLP 2012, pages 223-232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Chris Callison-Burch</author>
<author>Chris Dyer</author>
<author>Juri Ganitkevitch</author>
<author>Sanjeev Khudanpur</author>
<author>Lane Schwartz</author>
<author>Wren N G Thornton</author>
<author>Jonathan Weese</author>
<author>Omar F Zaidan</author>
</authors>
<title>Joshua: An open source toolkit for parsing-based machine translation.</title>
<date>2009</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>135--139</pages>
<marker>Li, Callison-Burch, Dyer, Ganitkevitch, Khudanpur, Schwartz, Thornton, Weese, Zaidan, 2009</marker>
<rawString>Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren N.G. Thornton, Jonathan Weese and Omar F. Zaidan. 2009. Joshua: An open source toolkit for parsing-based machine translation. In Proc. of ACL 2009, pages 135-139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shujie Liu</author>
<author>Chi-Ho Li</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
</authors>
<title>Retraining monolingual parser bilingually for syntactic SMT.</title>
<date>2012</date>
<booktitle>In Proc. of EMNLP 2012,</booktitle>
<pages>854--862</pages>
<contexts>
<context position="6481" citStr="Liu et al. (2012)" startWordPosition="977" endWordPosition="980">ayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. Cohn and Blunsom (2009) adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word clas</context>
</contexts>
<marker>Liu, Li, Li, Zhou, 2012</marker>
<rawString>Shujie Liu, Chi-Ho Li, Mu Li, Ming Zhou. 2012. Retraining monolingual parser bilingually for syntactic SMT. In Proc. of EMNLP 2012, pages 854-862.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-tostring alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of ACL-COLING</booktitle>
<pages>609--616</pages>
<contexts>
<context position="1464" citStr="Liu et al., 2006" startWordPosition="197" endWordPosition="200">es on the tree nodes. Experimental results show that the string-totree translation system using our Bayesian tree structures significantly outperforms the strong baseline string-to-tree system using parse trees. 1 Introduction In recent years, tree-based translation models1 are drawing more and more attention in the community of statistical machine translation (SMT). Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree-based translation models have shown promising progress in improving translation quality (Liu et al., 2006, 2009; Quirk et al., 2005; Galley et al., 2004, 2006; Marcu et al., 2006; Shen et al., 2008; Zhang et al., 2011b). However, tree-based translation models always suffer from two major challenges: 1) They are usually built directly from parse trees, which are generated by supervised linguistic parsers. 1 A tree-based translation model is defined as a model using tree structures on one side or both sides. However, for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of Tree-bank resources for training. 2) Parse trees are actually only used to </context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-tostring alignment template for statistical machine translation. In Proc. of ACL-COLING 2006, pages 609-616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
</authors>
<title>Yajuan Lv and Qun Liu.</title>
<date>2009</date>
<booktitle>In Proc. of ACL-IJCNLP</booktitle>
<pages>558--566</pages>
<marker>Liu, 2009</marker>
<rawString>Yang Liu, Yajuan Lv and Qun Liu. 2009. Improving tree-to-tree translation with packed forests. In Proc. of ACL-IJCNLP 2009, pages 558-566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Wei Wang</author>
<author>Abdessamad Echihabi</author>
<author>Kevin Knight</author>
</authors>
<title>SPMT: Statistical machine translation with syntactified target language phrases.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>44--52</pages>
<contexts>
<context position="1537" citStr="Marcu et al., 2006" startWordPosition="211" endWordPosition="214">translation system using our Bayesian tree structures significantly outperforms the strong baseline string-to-tree system using parse trees. 1 Introduction In recent years, tree-based translation models1 are drawing more and more attention in the community of statistical machine translation (SMT). Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree-based translation models have shown promising progress in improving translation quality (Liu et al., 2006, 2009; Quirk et al., 2005; Galley et al., 2004, 2006; Marcu et al., 2006; Shen et al., 2008; Zhang et al., 2011b). However, tree-based translation models always suffer from two major challenges: 1) They are usually built directly from parse trees, which are generated by supervised linguistic parsers. 1 A tree-based translation model is defined as a model using tree structures on one side or both sides. However, for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of Tree-bank resources for training. 2) Parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual ma</context>
<context position="30334" citStr="Marcu et al. 2006" startWordPosition="5038" endWordPosition="5041">ity is evaluated by case-insensitive BLEU-4 with the shortest length penalty. The statistical significance test is performed by the re-sampling approach (Koehn, 2004). To create the baseline system, we use the opensource Joshua 4.0 system (Ganitkevitch et al., 2012) to build a hierarchical phrase-based (HPB) system, and a syntax-augmented MT (SAMT) 11 system (Zollmann and Venugopal, 2006) respectively. The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t). The system is implemented based on (Galley et al., 2006) and (Marcu et al. 2006). In the system, we extract both the minimal GHKM rules (Galley et al., 2004), and the rules of SPMT Model 1 (Galley et al., 2006) with phrases up to length L=5 on the source side. We then obtain the composed rules by composing two or three adjacent minimal rules. To build the above s2t system, we first use the parse tree, which is generated by parsing the English side of the bilingual data with the Berkeley parser (Petrov et al., 2006). Then, we binarize the English parse trees using the head binarization approach (Wang et al., 2007) and use the resulting binary parse trees to build another s</context>
</contexts>
<marker>Marcu, Wang, Echihabi, Knight, 2006</marker>
<rawString>Daniel Marcu, Wei Wang, Abdessamad Echihabi and Kevin Knight. 2006. SPMT: Statistical machine translation with syntactified target language phrases. In Proc. of EMNLP 2006, pages 44-52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>160--167</pages>
<marker>Och, 2003</marker>
<rawString>Franz Och, 2003. Minimum error rate training in statistical machine translation. In Proc. of ACL 2003, pages 160-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>311--318</pages>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proc. of ACL 2002, pages 311-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proc. of COLINGACL</booktitle>
<pages>433--440</pages>
<contexts>
<context position="30774" citStr="Petrov et al., 2006" startWordPosition="5119" endWordPosition="5122">testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t). The system is implemented based on (Galley et al., 2006) and (Marcu et al. 2006). In the system, we extract both the minimal GHKM rules (Galley et al., 2004), and the rules of SPMT Model 1 (Galley et al., 2006) with phrases up to length L=5 on the source side. We then obtain the composed rules by composing two or three adjacent minimal rules. To build the above s2t system, we first use the parse tree, which is generated by parsing the English side of the bilingual data with the Berkeley parser (Petrov et al., 2006). Then, we binarize the English parse trees using the head binarization approach (Wang et al., 2007) and use the resulting binary parse trees to build another s2t system. For the U-trees, we run the Gibbs sampler for 1000 iterations on the whole corpus. The sampler uses 1,087s per iteration, on average, using a single core, 2.3 GHz Intel Xeon machine. For the hyperparameters, we set Į to 0.1 and pexpmrd = 1/3 to give a preference to the rules with small fragments. We built an s2t translation system with the achieved U-trees after the 1000th iteration. We only use one sample to extract the tran</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proc. of COLINGACL 2006, pages 433-440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency treelet translation: syntactically informed phrasal SMT.</title>
<date>2005</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>271--279</pages>
<contexts>
<context position="1490" citStr="Quirk et al., 2005" startWordPosition="202" endWordPosition="205">perimental results show that the string-totree translation system using our Bayesian tree structures significantly outperforms the strong baseline string-to-tree system using parse trees. 1 Introduction In recent years, tree-based translation models1 are drawing more and more attention in the community of statistical machine translation (SMT). Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree-based translation models have shown promising progress in improving translation quality (Liu et al., 2006, 2009; Quirk et al., 2005; Galley et al., 2004, 2006; Marcu et al., 2006; Shen et al., 2008; Zhang et al., 2011b). However, tree-based translation models always suffer from two major challenges: 1) They are usually built directly from parse trees, which are generated by supervised linguistic parsers. 1 A tree-based translation model is defined as a model using tree structures on one side or both sides. However, for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of Tree-bank resources for training. 2) Parse trees are actually only used to model and explain the mono</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes and Colin Cherry. 2005. Dependency treelet translation: syntactically informed phrasal SMT. In Proc. of ACL 2005, pages 271-279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In Proc. of ACL-08,</booktitle>
<pages>577--585</pages>
<contexts>
<context position="1556" citStr="Shen et al., 2008" startWordPosition="215" endWordPosition="218">sing our Bayesian tree structures significantly outperforms the strong baseline string-to-tree system using parse trees. 1 Introduction In recent years, tree-based translation models1 are drawing more and more attention in the community of statistical machine translation (SMT). Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree-based translation models have shown promising progress in improving translation quality (Liu et al., 2006, 2009; Quirk et al., 2005; Galley et al., 2004, 2006; Marcu et al., 2006; Shen et al., 2008; Zhang et al., 2011b). However, tree-based translation models always suffer from two major challenges: 1) They are usually built directly from parse trees, which are generated by supervised linguistic parsers. 1 A tree-based translation model is defined as a model using tree structures on one side or both sides. However, for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of Tree-bank resources for training. 2) Parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual mapping between langu</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Libin Shen, Jinxi Xu and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proc. of ACL-08, pages 577-585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Wang</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Binarizing syntax trees to improve syntax-based machine translation accuracy.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>746--754</pages>
<contexts>
<context position="14593" citStr="Wang et al., 2007" startWordPosition="2332" endWordPosition="2335">et tree fragment frag and a source string str in the model, we follow Cohn and Blunsom (2009) and decompose the prior probability P0(r |N) into two factors as follows: P0(r |N) P( frag  |N) P(str |frag) (4) where P( frag |N) is the probability of producing the target tree fragment frag. To generate frag, Cohn and Blunsom (2009) used a geometric prior to decide how many child nodes to assign each node. Differently, we require that each multi-word non-terminal node must have two child nodes. This is because the binary structure has been verified to be very effective for tree-based translation (Wang et al., 2007; Zhang et al., 2011a). The generation process starts at root node N. At first, root node N is expanded into two child nodes. Then, each newly generated node will be checked to expand into two new child nodes with probability pexpand. This process repeats until all the new non-terminal nodes are checked. Obviously, pexpand controls the scale of tree fragments, where a large pexpand corresponds to large fragments6. The new terminal nodes (words) are drawn uniformly from the target-side vocabulary, and the nonterminal nodes are created by asking two questions as follows: 1) What type is the node</context>
<context position="30874" citStr="Wang et al., 2007" startWordPosition="5135" endWordPosition="5138">e system is implemented based on (Galley et al., 2006) and (Marcu et al. 2006). In the system, we extract both the minimal GHKM rules (Galley et al., 2004), and the rules of SPMT Model 1 (Galley et al., 2006) with phrases up to length L=5 on the source side. We then obtain the composed rules by composing two or three adjacent minimal rules. To build the above s2t system, we first use the parse tree, which is generated by parsing the English side of the bilingual data with the Berkeley parser (Petrov et al., 2006). Then, we binarize the English parse trees using the head binarization approach (Wang et al., 2007) and use the resulting binary parse trees to build another s2t system. For the U-trees, we run the Gibbs sampler for 1000 iterations on the whole corpus. The sampler uses 1,087s per iteration, on average, using a single core, 2.3 GHz Intel Xeon machine. For the hyperparameters, we set Į to 0.1 and pexpmrd = 1/3 to give a preference to the rules with small fragments. We built an s2t translation system with the achieved U-trees after the 1000th iteration. We only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large. 11 From </context>
</contexts>
<marker>Wang, Knight, Marcu, 2007</marker>
<rawString>Wei Wang, Kevin Knight, and Daniel Marcu. 2007. Binarizing syntax trees to improve syntax-based machine translation accuracy. In Proc. of EMNLP 2007, pages 746-754.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Wang</author>
<author>Jonathan May</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Re-structuring, re-labeling, and realigning for syntax-based machine translation.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>2</issue>
<contexts>
<context position="2296" citStr="Wang et al., 2010" startWordPosition="329" endWordPosition="332">y built directly from parse trees, which are generated by supervised linguistic parsers. 1 A tree-based translation model is defined as a model using tree structures on one side or both sides. However, for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of Tree-bank resources for training. 2) Parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual mapping between language pairs. This indicates that parse trees are usually not the optimal choice for training tree-based translation models (Wang et al., 2010). Based on the above analysis, we can conclude that the tree structure that is independent from Tree-bank resources and simultaneously considers the bilingual mapping inside the bilingual sentence pairs would be a good choice for building treebased translation models. Therefore, complying with the above conditions, we propose an unsupervised tree structure for treebased translation models in this study. In the structures, tree nodes are labeled by combining the word classes of their boundary words rather than by syntactic labels, such as NP, VP. Furthermore, using these node labels, we design </context>
</contexts>
<marker>Wang, May, Knight, Marcu, 2010</marker>
<rawString>Wei Wang, Jonathan May, Kevin Knight, and Daniel Marcu. 2010. Re-structuring, re-labeling, and realigning for syntax-based machine translation. Computational Linguistics, 36(2):247–277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Feifei Zhai</author>
<author>Jiajun Zhang</author>
<author>Yu Zhou</author>
<author>Chengqing Zong</author>
</authors>
<title>Tree-based translation without using parse trees.</title>
<date>2012</date>
<booktitle>In Proc. of COLING 2012,</booktitle>
<pages>3037--3054</pages>
<contexts>
<context position="5304" citStr="Zhai et al., 2012" startWordPosition="793" endWordPosition="796">on process, and Section 4 depicts the adopted Bayesian model. Section 5 describes the Gibbs sampling algorithm and Gibbs operators. In Section 6, we analyze the achieved U-trees and evaluate their effectiveness. Finally, we conclude the paper in Section 7. 2 Related Work In this study, we move in a new direction to build a tree-based translation model with effective unsupervised U-tree structures. For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering. Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al. (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SC</context>
</contexts>
<marker>Zhai, Zhang, Zhou, Zong, 2012</marker>
<rawString>Feifei Zhai, Jiajun Zhang, Yu Zhou and Chengqing Zong. 2012. Tree-based translation without using parse trees. In Proc. of COLING 2012, pages 3037-3054.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Liang Huang</author>
<author>Daniel Gildea</author>
<author>Kevin Knight</author>
</authors>
<title>Synchronous binarization for machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of HLT-NAACL</booktitle>
<pages>256--263</pages>
<marker>Zhang, Huang, Gildea, Knight, 2006</marker>
<rawString>Hao Zhang, Liang Huang, Daniel Gildea and Kevin Knight. 2006. Synchronous binarization for machine translation. In Proc. of HLT-NAACL 2006, pages 256-263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
<author>David Chiang</author>
</authors>
<title>Extracting synchronous grammars rules from word level alignments in linear time.</title>
<date>2008</date>
<booktitle>In Proc. of COLING</booktitle>
<pages>1081--1088</pages>
<marker>Zhang, Gildea, Chiang, 2008</marker>
<rawString>Hao Zhang, Daniel Gildea, and David Chiang. 2008. Extracting synchronous grammars rules from word level alignments in linear time. In Proc. of COLING 2008, pages 1081-1088.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
</authors>
<title>Licheng Fang, Peng Xu, Xiaoyun Wu. 2011a. Binarized forest to string translation.</title>
<date>2011</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>835--845</pages>
<marker>Zhang, 2011</marker>
<rawString>Hao Zhang, Licheng Fang, Peng Xu, Xiaoyun Wu. 2011a. Binarized forest to string translation. In Proc. of ACL 2011, pages 835-845.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Zhang</author>
<author>Min Zhang</author>
<author>Haizhou Li</author>
<author>Aiti Aw</author>
<author>Chew Lim Tan</author>
</authors>
<title>Forest-based tree sequence to string translation model.</title>
<date>2009</date>
<booktitle>In Proc. of ACL-IJCNLP</booktitle>
<pages>172--180</pages>
<marker>Zhang, Zhang, Li, Aw, Tan, 2009</marker>
<rawString>Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, Chew Lim Tan. 2009. Forest-based tree sequence to string translation model. In Proc. of ACL-IJCNLP 2009, pages 172-180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiajun Zhang</author>
</authors>
<title>Feifei Zhai and Chengqing Zong. 2011b. Augmenting string-to-tree translation models with fuzzy use of source-side syntax.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>204--215</pages>
<marker>Zhang, 2011</marker>
<rawString>Jiajun Zhang, Feifei Zhai and Chengqing Zong. 2011b. Augmenting string-to-tree translation models with fuzzy use of source-side syntax. In Proc. of EMNLP 2011, pages 204-215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Hongfei Jiang</author>
<author>Ai Ti Aw</author>
<author>Jun Sun</author>
<author>Chew Lim Tan</author>
<author>Sheng Li</author>
</authors>
<title>A tree-to-tree alignment-based model for statistical Machine translation.</title>
<date>2007</date>
<pages>07--535</pages>
<marker>Zhang, Jiang, Aw, Sun, Tan, Li, 2007</marker>
<rawString>Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Chew Lim Tan and Sheng Li. 2007. A tree-to-tree alignment-based model for statistical Machine translation. MT-Summit-07. pages 535-542</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Hongfei Jiang</author>
<author>Ai ti Aw</author>
<author>Haizhou Li</author>
<author>Chew Lim Tan</author>
<author>Sheng Li</author>
</authors>
<title>A tree sequence alignment-based tree-to-tree translation model.</title>
<date>2008</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>559--567</pages>
<marker>Zhang, Jiang, Aw, Li, Tan, Li, 2008</marker>
<rawString>Min Zhang, Hongfei Jiang, Ai ti Aw, Haizhou Li, Chew Lim Tan and Sheng Li. 2008. A tree sequence alignment-based tree-to-tree translation model. In Proc. of ACL 2008, pages 559-567.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proc. of Workshop on Statistical Machine Translation</booktitle>
<pages>138--141</pages>
<contexts>
<context position="6883" citStr="Zollmann and Venugopal (2006)" startWordPosition="1032" endWordPosition="1035">the bilingual Tree-bank to train a joint model for both parsing and word alignment. Cohn and Blunsom (2009) adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes. Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules. 3 The STSG Generation Process In this work, we induce effective U-trees for the string-to-tree translation model, which is based on a synchronous tree substitution gram</context>
<context position="30107" citStr="Zollmann and Venugopal, 2006" startWordPosition="5002" endWordPosition="5005"> set, and use the NIST MT04 and MT05 data as the test set. We use MERT (Och, 2004) to tune parameters. Since MERT is prone to search errors, we run MERT 5 times and select the best tuning parameters in the tuning set. The translation quality is evaluated by case-insensitive BLEU-4 with the shortest length penalty. The statistical significance test is performed by the re-sampling approach (Koehn, 2004). To create the baseline system, we use the opensource Joshua 4.0 system (Ganitkevitch et al., 2012) to build a hierarchical phrase-based (HPB) system, and a syntax-augmented MT (SAMT) 11 system (Zollmann and Venugopal, 2006) respectively. The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t). The system is implemented based on (Galley et al., 2006) and (Marcu et al. 2006). In the system, we extract both the minimal GHKM rules (Galley et al., 2004), and the rules of SPMT Model 1 (Galley et al., 2006) with phrases up to length L=5 on the source side. We then obtain the composed rules by composing two or three adjacent minimal rules. To build the above s2t system, we first use the parse tree, which is generated by parsing the English side o</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proc. of Workshop on Statistical Machine Translation 2006, pages 138-141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Stephan Vogel</author>
</authors>
<title>A wordclass approach to labeling PSCFG rules for machine translation.</title>
<date>2011</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>1--11</pages>
<contexts>
<context position="7009" citStr="Zollmann and Vogel (2011)" startWordPosition="1048" endWordPosition="1051">hod to infer an STSG by exploring the space of alignments based on parse trees. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes. Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules. 3 The STSG Generation Process In this work, we induce effective U-trees for the string-to-tree translation model, which is based on a synchronous tree substitution grammar (STSG) between source strings and target tree fragments. We take STSG as the generation grammar to match the translation m</context>
<context position="31500" citStr="Zollmann and Vogel, 2011" startWordPosition="5244" endWordPosition="5247">and use the resulting binary parse trees to build another s2t system. For the U-trees, we run the Gibbs sampler for 1000 iterations on the whole corpus. The sampler uses 1,087s per iteration, on average, using a single core, 2.3 GHz Intel Xeon machine. For the hyperparameters, we set Į to 0.1 and pexpmrd = 1/3 to give a preference to the rules with small fragments. We built an s2t translation system with the achieved U-trees after the 1000th iteration. We only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large. 11 From (Zollmann and Vogel, 2011), we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags. Thus, to be convenient, we only conduct experiments with the SAMT system. 6.2 Analysis of The Gibbs Sampler To evaluate the effectiveness of the Gibbs sampler, we explore the change of the training data’s likelihood with increasing sampling iterations. 100 200 300 400 500 600 700 800 900 1000 Number of Sampling Iterations Figure 5. Histograms of the training data’s likelihood vs. the number of sampling iterations. In the figure, random 1 to 3 refers to three independent runs of the sa</context>
</contexts>
<marker>Zollmann, Vogel, 2011</marker>
<rawString>Andreas Zollmann and Stephan Vogel. 2011. A wordclass approach to labeling PSCFG rules for machine translation. In Proc. of ACL 2011, pages 1-11.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>