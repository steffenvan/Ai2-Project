<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012359">
<title confidence="0.991935">
UNIBA: Distributional Semantics for Textual Similarity
</title>
<author confidence="0.991202">
Annalina Caputo Pierpaolo Basile Giovanni Semeraro
</author>
<affiliation confidence="0.9979245">
Department of Computer Science
University of Bari “Aldo Moro”
</affiliation>
<address confidence="0.885276">
Via E. Orabona, 4 - 70125 Bari, Italy
</address>
<email confidence="0.990447">
{acaputo, basilepp, semeraro}@di.uniba.it
</email>
<sectionHeader confidence="0.998575" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998514">
We report the results of UNIBA participation
in the first SemEval-2012 Semantic Textual
Similarity task. Our systems rely on distribu-
tional models of words automatically inferred
from a large corpus. We exploit three differ-
ent semantic word spaces: Random Indexing
(RI), Latent Semantic Analysis (LSA) over RI,
and vector permutations in RI. Runs based on
these spaces consistently outperform the base-
line on the proposed datasets.
</bodyText>
<sectionHeader confidence="0.89805" genericHeader="categories and subject descriptors">
1 Background and Related Research
</sectionHeader>
<bodyText confidence="0.990715571428572">
SemEval-2012 Semantic Textual Similarity (STS)
task (Agirre et al., 2012) aims at providing a gen-
eral framework to “examine the degree of semantic
equivalence between two sentences.”
We propose an approach to Semantic Textual
Similarity based on distributional models of words,
where the geometrical metaphor of meaning is ex-
ploited. Distributional models are grounded on the
distributional hypothesis (Harris, 1968), according
to which the meaning of a word is determined by
the set of textual contexts in which it appears. These
models represent words as vectors in a high dimen-
sional vector space. Word vectors are built from a
large corpus in such a way that vector dimensions
reflect the different uses (or contexts) of a word in
the corpus. Hence, the meaning of a word is de-
fined by its use, and words used in similar contexts
are represented by vectors near in the space. In this
way, semantically related words like “basketball”
and “volleyball”, which occur frequently in similar
contexts, say with words “court, play, player”, will
be represented by near points. Different definitions
of contexts give rise to different (semantic) spaces.
A context can be a document, a sentence or a fixed
window of surrounding words. Contexts and words
can be stored through a co-occurrence matrix, whose
columns correspond to contexts, and rows to words.
Therefore, the strength of the semantic association
between words can be computed as the cosine simi-
larity of their vector representations.
Latent Semantic Analysis (Deerwester et al.,
1990), BEAGLE (Jones and Mewhort, 2007),
Random Indexing (Kanerva, 1988), Hyperspace
Analogue to Language (Burgess et al., 1998),
WordSpace (Schütze and Pedersen, 1995) are all
techniques conceived to build up semantic spaces.
However, all of them intend to represent semantics at
a word scale. Although vectors addition and multi-
plication are two well defined operations suitable for
composing words in semantic spaces, they miss tak-
ing into account the underlying syntax, which regu-
lates the compositionality of words. Some efforts to-
ward this direction are emerging (Clark and Pulman,
2007; Clark et al., 2008; Mitchell and Lapata, 2010;
Coecke et al., 2010; Basile et al., 2011; Clarke,
2012), which resulted in theoretical work corrob-
orated by empirical evaluation on how small frag-
ments of text compose (e.g. noun-noun, adjective-
noun, and verb-noun pairs).
</bodyText>
<sectionHeader confidence="0.996872" genericHeader="method">
2 Methodology
</sectionHeader>
<bodyText confidence="0.9998475">
Our approach to STS is inspired by the latest devel-
opments about semantic compositionality and distri-
butional models. The general methodology is based
on the construction of a semantic space endowed
</bodyText>
<page confidence="0.972277">
591
</page>
<note confidence="0.5271985">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 591–596,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999984571428572">
with a vector addition operator. The vector addition
sums the word vectors of each pair of sentences in-
volved in the evaluation. The result consists of two
vectors whose similarity can be computed by co-
sine similarity. However, this simple methodology
translates a text into a mere bag-of-word representa-
tion, depriving the text of its syntactic construction,
which also influences the overall meaning of the sen-
tence. In order to deal with this limit, we experi-
ment two classical methods for building a semantic
space, namely Random Indexing and Latent Seman-
tic Analysis, along with a new method based on vec-
tor permutations, which tries to encompass syntactic
information directly into the resulting space.
</bodyText>
<subsectionHeader confidence="0.990229">
2.1 Random Indexing
</subsectionHeader>
<bodyText confidence="0.999923411764706">
Our first method is based on Random Indexing (RI),
introduced by Kanerva (Kanerva, 1988). This tech-
nique allows us to build a semantic space with no
need for (either term-document or term-term) ma-
trix factorization, because vectors are inferred by
using an incremental strategy. Moreover, it allows
us to solve efficiently the problem of reducing di-
mensions, which is one of the key features used to
uncover the “latent semantic dimensions” of a word
distribution.
RI1 (Widdows and Ferraro, 2008) is based on
the concept of Random Projection according to
which high dimensional vectors chosen randomly
are “nearly orthogonal”.
Formally, given an n x m matrix A and an m x
k matrix R made up of k m-dimensional random
vectors, we define a new n x k matrix B as follows:
</bodyText>
<equation confidence="0.824151">
Bn,k = An,m, Rm,k k &lt;&lt; m (1)
</equation>
<bodyText confidence="0.992525">
The new matrix B has the property to preserve the
distance between points scaled by a multiplicative
factor (Johnson and Lindenstrauss, 1984).
Specifically, RI creates the semantic space Bn,k
in two steps (we consider a fixed window w of terms
as context):
</bodyText>
<listItem confidence="0.524412666666667">
1. A context vector is assigned to each term. This
vector is sparse, high-dimensional and ternary,
which means that its elements can take values
</listItem>
<footnote confidence="0.921802">
1An implementation of RI can be found at:
http://code.google.com/p/semanticvectors/
</footnote>
<bodyText confidence="0.9137385">
in {-1, 0, 1}. A context vector contains a small
number of randomly distributed non-zero ele-
ments, and the structure of this vector follows
the hypothesis behind the concept of Random
Projection;
2. Context vectors are accumulated by analyzing
co-occurring terms in a window w. The seman-
tic vector for a term is computed as the sum of
the context vectors for terms which co-occur in
w.
</bodyText>
<subsectionHeader confidence="0.999902">
2.2 Latent Semantic Analysis
</subsectionHeader>
<bodyText confidence="0.999257375">
Latent Semantic Analysis (Deerwester et al., 1990)
relies on the Singular Value Decomposition (SVD)
of a term-document co-occurrence matrix. Given
a matrix M, it can be decomposed in the product
of three matrices UEVT, where U and V are the
orthonormal matrices and E is the diagonal matrix
of singular values of M placed in decreasing order.
Computing the LSA on the co-occurrence matrix M
can be a computationally expensive task, as a corpus
can contain thousands of terms. Hence, we decided
to apply LSA to the reduced approximation gener-
ated by RI. It is important to point out that no trun-
cation of singular values is performed. Since com-
puting the similarity between any two words is equal
to taking the corresponding entry in the MMT ma-
trix, we can exploit the relation
</bodyText>
<equation confidence="0.842113">
MMT = UEVTVETUT = UEETUT =
(UE)(UE)T
</equation>
<bodyText confidence="0.9999795625">
Hence, the application of LSA to RI makes possible
to represent each word in the UE space.
A similar approach was investigated by Sellberg
and Jönsson (2008) for retrieval of similar FAQs in
a Question Answering system. Authors showed that
halving the matrix dimension by applying the RI re-
sulted in a drastic reduction of LSA computation
time. Certainly there was also a performance price
to be paid, however general performance was bet-
ter than VSM and RI respectively. We also experi-
mented LSA computed on RI versus LSA applied to
the original matrix during the tuning of our systems.
Surprisingly, we found that LSA applied on the re-
duced matrix gives better results than LSA. How-
ever, these results are not reported as they are not
the focus of this evaluation.
</bodyText>
<page confidence="0.985925">
592
</page>
<subsectionHeader confidence="0.993781">
2.3 Vector Permutations in RI
</subsectionHeader>
<bodyText confidence="0.999954108108108">
The classical distributional models can handle only
one definition of context at a time, such as the whole
document or the window w. A method to add infor-
mation about context in RI is proposed in (Sahlgren
et al., 2008). The authors describe a strategy to en-
code word order in RI by the permutation of coor-
dinates in context vector. When the coordinates are
shuffled using a random permutation, the resulting
vector is nearly orthogonal to the original one. That
operation corresponds to the generation of a new
random vector. Moreover, by applying a predeter-
mined mechanism to obtain random permutations,
such as elements rotation, it is always possible to
reconstruct the original vector using the reverse per-
mutations. By exploiting this strategy it is possible
to obtain different random vectors for each context
in which the term occurs.
Our idea is to encode syntactic dependen-
cies using vector permutations. A syntactic
dependency between two words is defined as
dep(head, dependent), where dep is the syntac-
tic link which connects the dependent word to the
head word. Generally speaking, dependent is the
modifier, object or complement, while head plays a
key role in determining the behavior of the link. For
example, subj(eat, cat) means that “cat” is the sub-
ject of “eat”. In that case the head word is “eat”,
which plays the role of verb.
The key idea is to encode in the semantic space in-
formation about syntactic dependencies which link
words together. Rather than representing the kind
of dependency, our focus is to encompass informa-
tion about the existence of such a relation between
words in the construction of the space. The method
adopted to construct a semantic space that takes into
account both syntactic dependencies and Random
Indexing can be defined as follows:
</bodyText>
<listItem confidence="0.687087">
1. a context vector is assigned to each term, as de-
scribed in Section 2.1 (Random Indexing);
2. context vectors are accumulated by analyzing
terms which are linked by a dependency. In
particular the semantic vector for each term ti
is computed as the sum of the inverse-permuted
context vectors for the terms tj which are de-
pendents of ti, and the permuted vectors for
</listItem>
<bodyText confidence="0.999673307692308">
the terms tj which are heads of ti. Moreover,
the context vector of ti, and those of tj terms
which appears in a dependency relation with
it, are sum to the final semantic vector in or-
der to provide distributional evidence of co-
occurrence. Each permutation is computed as
a forward/backward rotation of one element. If
H1 is a permutation of one element, the inverse-
permutation is defined as H−1: the elements
rotation is performed by one left-shifting step.
Formally, denoting with x the context vector
for a term, we compute the semantic vector for
the term ti as follows:
</bodyText>
<equation confidence="0.996579">
�si = xi + (H−1xj + xj) +
j
ddep(ti,tj)
� (H1xk + xk)
k
ddep(tk,ti)
</equation>
<bodyText confidence="0.999942">
Adding permuted vectors to the head word and
inverse-permuted vectors to the corresponding de-
pendent word allows to encode the information
about both heads and dependents into the space.
This approach is similar to the one investigated by
(Cohen et al., 2010) to encode relations between
medical terms.
</bodyText>
<sectionHeader confidence="0.999297" genericHeader="evaluation">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999653111111111">
Dataset Description. SemEval-2012 STS is a first
attempt to provide a “unified framework for the eval-
uation of modular semantic components.” The task
consists in computing the similarity between pair
of texts, returning a similarity score. Sentences
are extracted from five publicly available datasets:
MSR (Paraphrase Microsoft Research Paraphrase
Corpus, 750 pairs), MSR (Video Microsoft Research
Video Description Corpus, 750 pairs), SMTeuroparl
(WMT2008 development dataset, Europarl section,
459 pairs), SMTnews (news conversation sentence
pairs from WMT, 399 pairs), and OnWN (pairs of
sentences from Ontonotes and WordNet definition,
750 pairs). Humans rated each pair with values from
0 to 5. The evaluation is performed by comparing
humans scores against systems performance through
Pearson’s correlation. The organizers propose three
different ways to aggregate values from the datasets:
</bodyText>
<page confidence="0.997792">
593
</page>
<table confidence="0.9995574">
ALL Rank-ALL ALLnrm Rank-ALLNrm Mean Rank-Mean
baseline .3110 87 .6732 85 .4356 70
UNIBA-RI .6285 41 .7951 43 .5651 45
UNIBA-LSARI .6221 44 .8079 30 .5728 40
UNIBA-DEPRI .6141 46 .8027 38 .5891 31
</table>
<tableCaption confidence="0.996404">
Table 1: Evaluation results of Pearson’s correlation.
</tableCaption>
<table confidence="0.9996098">
MSRpar MSRvid SMT-eur On-WN SMT-news
baseline .4334 .2996 .4542 .5864 .3908
UNIBA- RI .4128 .7612 .4531 .6306 .4887
UNIBA- LSARI .3886 .7908 .4679 .6826 .4238
UNIBA- DEPRI .4542 .7673 .5126 .6593 .4636
</table>
<tableCaption confidence="0.999612">
Table 2: Evaluation results of Pearson’s correlation for individual datasets.
</tableCaption>
<bodyText confidence="0.996688103448276">
ALL Pearson correlation with the gold standard for
the five datasets.
ALLnrm Pearson correlation after the system out-
puts for each dataset are fitted to the gold stan-
dard using least squares.
Mean Weighted mean across the five datasets,
where the weight depends on the number of
pairs in the dataset.
Experimental Setting. For the evaluation, we
built Distributional Spaces using the WaCkype-
dia_EN corpus2. WaCkypedia_EN is based on a
2009 dump of the English Wikipedia (about 800 mil-
lion tokens) and includes information about: part-of-
speech, lemma and a full dependency parsing per-
formed by MaltParser (Nivre et al., 2007). The three
spaces described in Section 2 are built exploiting
information about term windows and dependency
parsing supplied by WaCkypedia. The total number
of dependencies amounts to about 200 million.
The RI system is implemented in Java and re-
lies on some portions of code publicly available in
the Semantic Vectors package (Widdows and Fer-
raro, 2008), while for LSA we exploited the publicly
available C library SVDLIBC3.
We restricted the vocabulary to the 50,000 most
frequent terms, with stop words removal and forc-
ing the system to include terms which occur in the
dataset. Hence, the dimension of the original matrix
would have been 50,000×50,000.
</bodyText>
<footnote confidence="0.9999455">
2http://wacky.sslmit.unibo.it/doku.php?id=corpora
3http://tedlab.mit.edu/ dr/SVDLIBC/
</footnote>
<bodyText confidence="0.99991284375">
Our approach involves some parameters. In par-
ticular, each semantic space needs to set up the di-
mension k of the space. All spaces use a dimen-
sion of 500 (resulting in a 50,000×500 matrix). The
number of non-zero elements in the random vector
is set to 10. When we apply LSA to the output space
generated by the Random Indexing we hold all the
500 dimensions since during the tuning we observed
a drop in performance when a lower dimension was
set. The co-occurrence distance w between terms
was set up to 4.
In order to compute the similarity between the
vector representations of sentences we used the co-
sine similarity, and then we multiplied by 5 the ob-
tained value.
Results. Table 1 shows the overall results obtained
exploiting the different semantic spaces. We re-
port the three proposed evaluation measures with the
corresponding overall ranks with respect to the 89
runs submitted by participants. We submitted three
different runs, each exploring a different semantic
space: UNIBA-RI (based on Random Indexing),
UNIBA-LSARI (based on LSA performed over RI
outcome), and UNIBA-DEPRI (based on Random
Indexing and vector permutations). Each proposed
measure stresses different aspects. ALL is the Pear-
son’s correlation computed over the concatenated
dataset. As a consequence this measure ranks higher
systems which obtain consistent better results. Con-
versely, ALLNrm normalizes results by scaling val-
ues obtained from each dataset, in this way it tries
to give emphasis to systems trained on each dataset.
</bodyText>
<page confidence="0.996298">
594
</page>
<bodyText confidence="0.993451777777778">
The result of these different perspective is that our
three spaces rank differently according to each mea-
sure. It seems that UNIBA-RI is able to work better
across all datasets, while UNIBA-LSARI gives the
best results on specific datasets, even though all our
methods are unsupervised and do not need training
steps. A deeper analysis on each dataset is reported
on Table 2. Here results seem to be at odds with
Table 1.
Considering individual datasets, UNIBA-RI gives
only once the best result, while UNIBA-LSARI and
UNIBA-DEPRI are able to provide the best results
twice. Generally, all results outperform the base-
line, based on a simple keyword overlap. Lower re-
sults are obtained in MSRpar, we ascribe this result
to the notably long sentences here involved. In par-
ticular, UNIBA-LSARI gives a result lower than the
baseline, and in line with the one obtained by LSA
during the tuning. Hence, we ascribe this low per-
formance to the application of LSA method to this
specific dataset. Only UNIBA-DEPRI was able to
outperform the baseline in this dataset. This shows
the usefulness of encoding syntactic features in se-
mantic word space where longer sentences are in-
volved. Generally, it is interesting to be noticed that
our spaces perform rather well on short and similarly
structured sentences, such as MSRvid and On-WN.
</bodyText>
<sectionHeader confidence="0.999613" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999978272727273">
We reported evaluation results of our participation in
Semantic Textual Similarity task. Our systems ex-
ploit distributional models to represent the seman-
tics of words. Two of such spaces are based on a
classical definition of context, such as a fixed win-
dow of surrounding words. A third spaces tries to
encompass more definitions of context at once, as
the syntactic structure that relates words in a cor-
pus. Although simple, our methods have achieved
generally good results, outperforming the baseline
provided by the organizers.
</bodyText>
<sectionHeader confidence="0.999243" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998588913793103">
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot
on semantic textual similarity. In Proceedings of the
6th International Workshop on Semantic Evaluation
(SemEval 2012), in conjunction with the First Joint
Conference on Lexical and Computational Semantics
(*SEM 2012).
Pierpaolo Basile, Annalina Caputo, and Giovanni Semer-
aro. 2011. Encoding syntactic dependencies by vec-
tor permutation. In Proceedings of the EMNLP 2011
Workshop on GEometrical Models of Natural Lan-
guage Semantics, GEMS ’11, pages 43–51, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Curt Burgess, Kay Livesay, and Kevin Lund. 1998. Ex-
plorations in context space: Words, sentences, dis-
course. Discourse Processes, 25(2-3):211–257.
Stephen Clark and Stephen Pulman. 2007. Combining
symbolic and distributional models of meaning. In
Proceedings of the AAAI Spring Symposium on Quan-
tum Interaction, pages 52–55.
Stephen Clark, Bob Coecke, and Mehrnoosh Sadrzadeh.
2008. A compositional distributional model of mean-
ing. In Proceedings of the Second Quantum Interac-
tion Symposium (QI-2008), pages 133–140.
Daoud Clarke. 2012. A context–theoretic framework for
compositionality in distributional semantics. Compu-
tational Linguistics, 38(1):41–71.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark.
2010. Mathematical foundations for a composi-
tional distributional model of meaning. CoRR,
abs/1003.4394.
Trevor Cohen, Dominic Widdows, Roger W. Schvan-
eveldt, and Thomas C. Rindflesch. 2010. Logical
leaps and quantum connectives: Forging paths through
predication space. In AAAI-Fall 2010 Symposium on
Quantum Informatics for Cognitive, Social, and Se-
mantic Processes, pages 11–13.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society for Information Science, 41(6):391–
407.
Zellig Harris. 1968. Mathematical Structures of Lan-
guage. New York: Interscience.
William B. Johnson and Joram Lindenstrauss. 1984. Ex-
tensions of Lipschitz mappings into a Hilbert space.
Conference on Modern Analysis and Probability, Con-
temporary Mathematics, 26:189–206.
Michael N. Jones and Douglas J. K. Mewhort. 2007.
Representing word meaning and order information in
a composite holographic lexicon. Psychological Re-
view, 114(1):1–37.
Pentti Kanerva. 1988. Sparse Distributed Memory. MIT
Press.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388–1429.
</reference>
<page confidence="0.984071">
595
</page>
<reference confidence="0.998303885714286">
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gülsen Eryigit, Sandra Kübler, Svetoslav Marinov,
and Erwin Marsi. 2007. Maltparser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(02):95–135.
Magnus Sahlgren, Anders Holst, and Pentti Kanerva.
2008. Permutations as a means to encode order in
word space. In V. Sloutsky, B. Love, and K. Mcrae,
editors, Proceedings of the 30th Annual Meeting of
the Cognitive Science Society (CogSci’08), July 23-26,
Washington D.C., USA, pages 1300–1305. Cognitive
Science Society, Austin, TX.
Hinrich Schütze and Jan O. Pedersen. 1995. Informa-
tion retrieval based on word senses. In Proceedings of
the 4th Annual Symposium on Document Analysis and
Information Retrieval, pages 161–175.
Linus Sellberg and Arne Jönsson. 2008. Using random
indexing to improve singular value decomposition
for latent semantic analysis. In Nicoletta Calzolari,
Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan
Odjik, Stelios Piperidis, and Daniel Tapias, editors,
Proceedings of the Sixth International Conference
on Language Resources and Evaluation (LREC2008),
pages 2335–2338, Marrakech, Morocco. European
Language Resources Association (ELRA).
Dominic Widdows and Kathleen Ferraro. 2008. Se-
mantic Vectors: A Scalable Open Source Package
and Online Technology Management Application. In
Nicoletta Calzolari, Khalid Choukri, Bente Maegaard,
Joseph Mariani, Jan Odjik, Stelios Piperidis, and
Daniel Tapias, editors, Proceedings of the 6th Interna-
tional Conference on Language Resources and Eval-
uation (LREC2008), pages 1183–1190, Marrakech,
Morocco. European Language Resources Association
(ELRA).
</reference>
<page confidence="0.998641">
596
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000026">
<title confidence="0.998887">UNIBA: Distributional Semantics for Textual Similarity</title>
<author confidence="0.999892">Annalina Caputo Pierpaolo Basile Giovanni Semeraro</author>
<affiliation confidence="0.99964">Department of Computer University of Bari “Aldo</affiliation>
<address confidence="0.525162">Via E. Orabona, 4 - 70125 Bari,</address>
<email confidence="0.992037">acaputo@di.uniba.it</email>
<email confidence="0.992037">basilepp@di.uniba.it</email>
<email confidence="0.992037">semeraro@di.uniba.it</email>
<abstract confidence="0.994062095238095">We report the results of UNIBA participation in the first SemEval-2012 Semantic Textual Similarity task. Our systems rely on distributional models of words automatically inferred from a large corpus. We exploit three different semantic word spaces: Random Indexing (RI), Latent Semantic Analysis (LSA) over RI, and vector permutations in RI. Runs based on these spaces consistently outperform the baseline on the proposed datasets. 1 Background and Related Research SemEval-2012 Semantic Textual Similarity (STS) task (Agirre et al., 2012) aims at providing a genframework to the degree of semantic between two We propose an approach to Semantic Textual Similarity based on distributional models of words, where the geometrical metaphor of meaning is exploited. Distributional models are grounded on the hypothesis 1968), according to which the meaning of a word is determined by the set of textual contexts in which it appears. These models represent words as vectors in a high dimensional vector space. Word vectors are built from a large corpus in such a way that vector dimensions the different uses (or of a word in the corpus. Hence, the meaning of a word is defined by its use, and words used in similar contexts are represented by vectors near in the space. In this way, semantically related words like “basketball” and “volleyball”, which occur frequently in similar contexts, say with words “court, play, player”, will be represented by near points. Different definitions of contexts give rise to different (semantic) spaces. A context can be a document, a sentence or a fixed window of surrounding words. Contexts and words can be stored through a co-occurrence matrix, whose columns correspond to contexts, and rows to words. Therefore, the strength of the semantic association between words can be computed as the cosine similarity of their vector representations.</abstract>
<note confidence="0.7065025">Latent Semantic Analysis (Deerwester et al., 1990), BEAGLE (Jones and Mewhort, 2007), Random Indexing (Kanerva, 1988), Hyperspace Analogue to Language (Burgess et al., 1998),</note>
<abstract confidence="0.974863025559106">WordSpace (Schütze and Pedersen, 1995) are all techniques conceived to build up semantic spaces. However, all of them intend to represent semantics at a word scale. Although vectors addition and multiplication are two well defined operations suitable for composing words in semantic spaces, they miss taking into account the underlying syntax, which regulates the compositionality of words. Some efforts toward this direction are emerging (Clark and Pulman, 2007; Clark et al., 2008; Mitchell and Lapata, 2010; Coecke et al., 2010; Basile et al., 2011; Clarke, 2012), which resulted in theoretical work corroborated by empirical evaluation on how small fragments of text compose (e.g. noun-noun, adjectivenoun, and verb-noun pairs). 2 Methodology Our approach to STS is inspired by the latest developments about semantic compositionality and distributional models. The general methodology is based on the construction of a semantic space endowed 591 Joint Conference on Lexical and Computational Semantics pages 591–596, Canada, June 7-8, 2012. Association for Computational Linguistics with a vector addition operator. The vector addition sums the word vectors of each pair of sentences involved in the evaluation. The result consists of two vectors whose similarity can be computed by cosine similarity. However, this simple methodology translates a text into a mere bag-of-word representation, depriving the text of its syntactic construction, which also influences the overall meaning of the sentence. In order to deal with this limit, we experiment two classical methods for building a semantic space, namely Random Indexing and Latent Semantic Analysis, along with a new method based on vector permutations, which tries to encompass syntactic information directly into the resulting space. 2.1 Random Indexing Our first method is based on Random Indexing (RI), introduced by Kanerva (Kanerva, 1988). This technique allows us to build a semantic space with no need for (either term-document or term-term) matrix factorization, because vectors are inferred by using an incremental strategy. Moreover, it allows us to solve efficiently the problem of reducing dimensions, which is one of the key features used to uncover the “latent semantic dimensions” of a word distribution. (Widdows and Ferraro, 2008) is based on the concept of Random Projection according to which high dimensional vectors chosen randomly are “nearly orthogonal”. given an an up of random we define a new follows: k &lt;&lt; m new matrix the property to preserve the distance between points scaled by a multiplicative factor (Johnson and Lindenstrauss, 1984). RI creates the semantic space two steps (we consider a fixed window terms as context): A vector assigned to each term. This vector is sparse, high-dimensional and ternary, which means that its elements can take values implementation of RI can be found at: http://code.google.com/p/semanticvectors/ in {-1, 0, 1}. A context vector contains a small number of randomly distributed non-zero elements, and the structure of this vector follows the hypothesis behind the concept of Random Projection; 2. Context vectors are accumulated by analyzing terms in a window The semanvector a term is computed as the sum of the context vectors for terms which co-occur in 2.2 Latent Semantic Analysis Latent Semantic Analysis (Deerwester et al., 1990) relies on the Singular Value Decomposition (SVD) of a term-document co-occurrence matrix. Given a matrix M, it can be decomposed in the product three matrices where U and V are the orthonormal matrices and E is the diagonal matrix values M placed in decreasing order. Computing the LSA on the co-occurrence matrix M can be a computationally expensive task, as a corpus can contain thousands of terms. Hence, we decided to apply LSA to the reduced approximation generated by RI. It is important to point out that no truncation of singular values is performed. Since computing the similarity between any two words is equal taking the corresponding entry in the matrix, we can exploit the relation = = Hence, the application of LSA to RI makes possible to represent each word in the UE space. A similar approach was investigated by Sellberg and Jönsson (2008) for retrieval of similar FAQs in a Question Answering system. Authors showed that halving the matrix dimension by applying the RI resulted in a drastic reduction of LSA computation time. Certainly there was also a performance price to be paid, however general performance was better than VSM and RI respectively. We also experimented LSA computed on RI versus LSA applied to the original matrix during the tuning of our systems. Surprisingly, we found that LSA applied on the reduced matrix gives better results than LSA. However, these results are not reported as they are not the focus of this evaluation. 592 2.3 Vector Permutations in RI The classical distributional models can handle only one definition of context at a time, such as the whole or the window A method to add information about context in RI is proposed in (Sahlgren et al., 2008). The authors describe a strategy to encode word order in RI by the permutation of coorin When the coordinates are shuffled using a random permutation, the resulting vector is nearly orthogonal to the original one. That operation corresponds to the generation of a new random vector. Moreover, by applying a predetermined mechanism to obtain random permutations, such as elements rotation, it is always possible to reconstruct the original vector using the reverse permutations. By exploiting this strategy it is possible to obtain different random vectors for each context in which the term occurs. idea is to encode syntactic dependencies using vector permutations. A dependency between two words is defined as where the syntaclink which connects the to the Generally speaking, the object or complement, while a key role in determining the behavior of the link. For that “cat” is the subof “eat”. In that case the is “eat”, which plays the role of verb. The key idea is to encode in the semantic space information about syntactic dependencies which link words together. Rather than representing the kind of dependency, our focus is to encompass information about the existence of such a relation between words in the construction of the space. The method adopted to construct a semantic space that takes into account both syntactic dependencies and Random Indexing can be defined as follows: 1. a context vector is assigned to each term, as described in Section 2.1 (Random Indexing); 2. context vectors are accumulated by analyzing terms which are linked by a dependency. In the semantic vector for each term is computed as the sum of the inverse-permuted vectors for the terms are deof and the permuted vectors for terms are heads of Moreover, context vector of and those of which appears in a dependency relation with it, are sum to the final semantic vector in order to provide distributional evidence of cooccurrence. Each permutation is computed as a forward/backward rotation of one element. If a permutation of one element, the inverseis defined as the elements rotation is performed by one left-shifting step. denoting with context vector for a term, we compute the semantic vector for term follows: j k Adding permuted vectors to the head word and inverse-permuted vectors to the corresponding dependent word allows to encode the information about both heads and dependents into the space. This approach is similar to the one investigated by (Cohen et al., 2010) to encode relations between medical terms. 3 Evaluation Description. STS is a first to provide a framework for the evalof modular semantic The task consists in computing the similarity between pair of texts, returning a similarity score. Sentences are extracted from five publicly available datasets: MSR (Paraphrase Microsoft Research Paraphrase Corpus, 750 pairs), MSR (Video Microsoft Research Video Description Corpus, 750 pairs), SMTeuroparl (WMT2008 development dataset, Europarl section, 459 pairs), SMTnews (news conversation sentence pairs from WMT, 399 pairs), and OnWN (pairs of sentences from Ontonotes and WordNet definition, 750 pairs). Humans rated each pair with values from 0 to 5. The evaluation is performed by comparing humans scores against systems performance through Pearson’s correlation. The organizers propose three different ways to aggregate values from the datasets: 593 ALL Rank-ALL ALLnrm Rank-ALLNrm Mean Rank-Mean baseline .3110 87 .6732 85 .4356 70 UNIBA-RI .6285 41 .7951 43 .5651 45 UNIBA-LSARI .6221 44 .8079 30 .5728 40 UNIBA-DEPRI .6141 46 .8027 38 .5891 31 Table 1: Evaluation results of Pearson’s correlation. MSRpar MSRvid SMT-eur On-WN SMT-news baseline .4334 .2996 .4542 .5864 .3908 UNIBA- RI .4128 .7612 .4531 .6306 .4887 UNIBA- LSARI .3886 .7908 .4679 .6826 .4238 UNIBA- DEPRI .4542 .7673 .5126 .6593 .4636 Table 2: Evaluation results of Pearson’s correlation for individual datasets. correlation with the gold standard for the five datasets. correlation after the system outputs for each dataset are fitted to the gold standard using least squares. mean across the five datasets, where the weight depends on the number of pairs in the dataset. Setting. the evaluation, we built Distributional Spaces using the WaCkype- WaCkypedia_EN is based on a 2009 dump of the English Wikipedia (about 800 million tokens) and includes information about: part-ofspeech, lemma and a full dependency parsing performed by MaltParser (Nivre et al., 2007). The three spaces described in Section 2 are built exploiting information about term windows and dependency parsing supplied by WaCkypedia. The total number of dependencies amounts to about 200 million. The RI system is implemented in Java and relies on some portions of code publicly available in the Semantic Vectors package (Widdows and Ferraro, 2008), while for LSA we exploited the publicly C library We restricted the vocabulary to the 50,000 most frequent terms, with stop words removal and forcing the system to include terms which occur in the dataset. Hence, the dimension of the original matrix have been dr/SVDLIBC/ Our approach involves some parameters. In particular, each semantic space needs to set up the dithe space. All spaces use a dimenof 500 (resulting in a matrix). The number of non-zero elements in the random vector is set to 10. When we apply LSA to the output space generated by the Random Indexing we hold all the 500 dimensions since during the tuning we observed a drop in performance when a lower dimension was The co-occurrence distance terms was set up to 4. In order to compute the similarity between the vector representations of sentences we used the cosine similarity, and then we multiplied by 5 the obtained value. 1 shows the overall results obtained exploiting the different semantic spaces. We report the three proposed evaluation measures with the corresponding overall ranks with respect to the 89 runs submitted by participants. We submitted three different runs, each exploring a different semantic space: UNIBA-RI (based on Random Indexing), UNIBA-LSARI (based on LSA performed over RI outcome), and UNIBA-DEPRI (based on Random Indexing and vector permutations). Each proposed measure stresses different aspects. ALL is the Pearson’s correlation computed over the concatenated dataset. As a consequence this measure ranks higher systems which obtain consistent better results. Conversely, ALLNrm normalizes results by scaling values obtained from each dataset, in this way it tries to give emphasis to systems trained on each dataset. 594 The result of these different perspective is that our three spaces rank differently according to each measure. It seems that UNIBA-RI is able to work better across all datasets, while UNIBA-LSARI gives the best results on specific datasets, even though all our methods are unsupervised and do not need training steps. A deeper analysis on each dataset is reported on Table 2. Here results seem to be at odds with Table 1. Considering individual datasets, UNIBA-RI gives only once the best result, while UNIBA-LSARI and UNIBA-DEPRI are able to provide the best results twice. Generally, all results outperform the baseline, based on a simple keyword overlap. Lower results are obtained in MSRpar, we ascribe this result to the notably long sentences here involved. In particular, UNIBA-LSARI gives a result lower than the baseline, and in line with the one obtained by LSA during the tuning. Hence, we ascribe this low performance to the application of LSA method to this specific dataset. Only UNIBA-DEPRI was able to outperform the baseline in this dataset. This shows the usefulness of encoding syntactic features in semantic word space where longer sentences are involved. Generally, it is interesting to be noticed that our spaces perform rather well on short and similarly structured sentences, such as MSRvid and On-WN. 4 Conclusion We reported evaluation results of our participation in Semantic Textual Similarity task. Our systems exploit distributional models to represent the semantics of words. Two of such spaces are based on a classical definition of context, such as a fixed window of surrounding words. A third spaces tries to encompass more definitions of context at once, as the syntactic structure that relates words in a corpus. Although simple, our methods have achieved generally good results, outperforming the baseline provided by the organizers.</abstract>
<note confidence="0.835524515789474">References Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot semantic textual similarity. In of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics Pierpaolo Basile, Annalina Caputo, and Giovanni Semeraro. 2011. Encoding syntactic dependencies by vecpermutation. In of the EMNLP 2011 Workshop on GEometrical Models of Natural Lan- GEMS ’11, pages 43–51, Stroudsburg, PA, USA. Association for Computational Linguistics. Curt Burgess, Kay Livesay, and Kevin Lund. 1998. Explorations in context space: Words, sentences, dis- 25(2-3):211–257. Stephen Clark and Stephen Pulman. 2007. Combining symbolic and distributional models of meaning. In Proceedings of the AAAI Spring Symposium on Quanpages 52–55. Stephen Clark, Bob Coecke, and Mehrnoosh Sadrzadeh. 2008. A compositional distributional model of mean- In of the Second Quantum Interac- Symposium pages 133–140. Daoud Clarke. 2012. A context–theoretic framework for in distributional semantics. Compu- 38(1):41–71. Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark. Mathematical foundations for a composidistributional model of meaning. abs/1003.4394. Trevor Cohen, Dominic Widdows, Roger W. Schvaneveldt, and Thomas C. Rindflesch. 2010. Logical leaps and quantum connectives: Forging paths through space. In 2010 Symposium on Quantum Informatics for Cognitive, Social, and Sepages 11–13. Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. by latent semantic analysis. of the Society for Information 41(6):391– 407. Harris. 1968. Structures of Lan- New York: Interscience. William B. Johnson and Joram Lindenstrauss. 1984. Extensions of Lipschitz mappings into a Hilbert space. Conference on Modern Analysis and Probability, Con- 26:189–206. Michael N. Jones and Douglas J. K. Mewhort. 2007. Representing word meaning and order information in composite holographic lexicon. Re- 114(1):1–37. Kanerva. 1988. Distributed MIT Press. Jeff Mitchell and Mirella Lapata. 2010. Composition in models of semantics. 34(8):1388–1429. 595 Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, Gülsen Eryigit, Sandra Kübler, Svetoslav Marinov, and Erwin Marsi. 2007. Maltparser: A languageindependent system for data-driven dependency pars- Language 13(02):95–135. Magnus Sahlgren, Anders Holst, and Pentti Kanerva. 2008. Permutations as a means to encode order in word space. In V. Sloutsky, B. Love, and K. Mcrae, of the 30th Annual Meeting of the Cognitive Science Society (CogSci’08), July 23-26, D.C., pages 1300–1305. Cognitive Science Society, Austin, TX. Hinrich Schütze and Jan O. Pedersen. 1995. Informaretrieval based on word senses. In of the 4th Annual Symposium on Document Analysis and pages 161–175. Linus Sellberg and Arne Jönsson. 2008. Using random indexing to improve singular value decomposition for latent semantic analysis. In Nicoletta Calzolari, Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odjik, Stelios Piperidis, and Daniel Tapias, editors, Proceedings of the Sixth International Conference Language Resources and Evaluation pages 2335–2338, Marrakech, Morocco. European Language Resources Association (ELRA). Dominic Widdows and Kathleen Ferraro. 2008. Semantic Vectors: A Scalable Open Source Package and Online Technology Management Application. In Nicoletta Calzolari, Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odjik, Stelios Piperidis, and Tapias, editors, of the 6th International Conference on Language Resources and Evalpages 1183–1190, Marrakech, Morocco. European Language Resources Association (ELRA). 596</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<title>Semeval-2012 task 6: A pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics (*SEM</booktitle>
<contexts>
<context position="796" citStr="Agirre et al., 2012" startWordPosition="110" endWordPosition="113">rabona, 4 - 70125 Bari, Italy {acaputo, basilepp, semeraro}@di.uniba.it Abstract We report the results of UNIBA participation in the first SemEval-2012 Semantic Textual Similarity task. Our systems rely on distributional models of words automatically inferred from a large corpus. We exploit three different semantic word spaces: Random Indexing (RI), Latent Semantic Analysis (LSA) over RI, and vector permutations in RI. Runs based on these spaces consistently outperform the baseline on the proposed datasets. 1 Background and Related Research SemEval-2012 Semantic Textual Similarity (STS) task (Agirre et al., 2012) aims at providing a general framework to “examine the degree of semantic equivalence between two sentences.” We propose an approach to Semantic Textual Similarity based on distributional models of words, where the geometrical metaphor of meaning is exploited. Distributional models are grounded on the distributional hypothesis (Harris, 1968), according to which the meaning of a word is determined by the set of textual contexts in which it appears. These models represent words as vectors in a high dimensional vector space. Word vectors are built from a large corpus in such a way that vector dim</context>
</contexts>
<marker>Agirre, Cer, Diab, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot on semantic textual similarity. In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics (*SEM 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierpaolo Basile</author>
<author>Annalina Caputo</author>
<author>Giovanni Semeraro</author>
</authors>
<title>Encoding syntactic dependencies by vector permutation.</title>
<date>2011</date>
<booktitle>In Proceedings of the EMNLP 2011 Workshop on GEometrical Models of Natural Language Semantics, GEMS ’11,</booktitle>
<pages>43--51</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2941" citStr="Basile et al., 2011" startWordPosition="451" endWordPosition="454">rva, 1988), Hyperspace Analogue to Language (Burgess et al., 1998), WordSpace (Schütze and Pedersen, 1995) are all techniques conceived to build up semantic spaces. However, all of them intend to represent semantics at a word scale. Although vectors addition and multiplication are two well defined operations suitable for composing words in semantic spaces, they miss taking into account the underlying syntax, which regulates the compositionality of words. Some efforts toward this direction are emerging (Clark and Pulman, 2007; Clark et al., 2008; Mitchell and Lapata, 2010; Coecke et al., 2010; Basile et al., 2011; Clarke, 2012), which resulted in theoretical work corroborated by empirical evaluation on how small fragments of text compose (e.g. noun-noun, adjectivenoun, and verb-noun pairs). 2 Methodology Our approach to STS is inspired by the latest developments about semantic compositionality and distributional models. The general methodology is based on the construction of a semantic space endowed 591 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 591–596, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics with a vector addition operator.</context>
</contexts>
<marker>Basile, Caputo, Semeraro, 2011</marker>
<rawString>Pierpaolo Basile, Annalina Caputo, and Giovanni Semeraro. 2011. Encoding syntactic dependencies by vector permutation. In Proceedings of the EMNLP 2011 Workshop on GEometrical Models of Natural Language Semantics, GEMS ’11, pages 43–51, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Curt Burgess</author>
<author>Kay Livesay</author>
<author>Kevin Lund</author>
</authors>
<date>1998</date>
<booktitle>Explorations in context space: Words, sentences, discourse. Discourse Processes,</booktitle>
<pages>25--2</pages>
<contexts>
<context position="2388" citStr="Burgess et al., 1998" startWordPosition="363" endWordPosition="366">e represented by near points. Different definitions of contexts give rise to different (semantic) spaces. A context can be a document, a sentence or a fixed window of surrounding words. Contexts and words can be stored through a co-occurrence matrix, whose columns correspond to contexts, and rows to words. Therefore, the strength of the semantic association between words can be computed as the cosine similarity of their vector representations. Latent Semantic Analysis (Deerwester et al., 1990), BEAGLE (Jones and Mewhort, 2007), Random Indexing (Kanerva, 1988), Hyperspace Analogue to Language (Burgess et al., 1998), WordSpace (Schütze and Pedersen, 1995) are all techniques conceived to build up semantic spaces. However, all of them intend to represent semantics at a word scale. Although vectors addition and multiplication are two well defined operations suitable for composing words in semantic spaces, they miss taking into account the underlying syntax, which regulates the compositionality of words. Some efforts toward this direction are emerging (Clark and Pulman, 2007; Clark et al., 2008; Mitchell and Lapata, 2010; Coecke et al., 2010; Basile et al., 2011; Clarke, 2012), which resulted in theoretical </context>
</contexts>
<marker>Burgess, Livesay, Lund, 1998</marker>
<rawString>Curt Burgess, Kay Livesay, and Kevin Lund. 1998. Explorations in context space: Words, sentences, discourse. Discourse Processes, 25(2-3):211–257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>Stephen Pulman</author>
</authors>
<title>Combining symbolic and distributional models of meaning.</title>
<date>2007</date>
<booktitle>In Proceedings of the AAAI Spring Symposium on Quantum Interaction,</booktitle>
<pages>52--55</pages>
<contexts>
<context position="2852" citStr="Clark and Pulman, 2007" startWordPosition="435" endWordPosition="438"> Analysis (Deerwester et al., 1990), BEAGLE (Jones and Mewhort, 2007), Random Indexing (Kanerva, 1988), Hyperspace Analogue to Language (Burgess et al., 1998), WordSpace (Schütze and Pedersen, 1995) are all techniques conceived to build up semantic spaces. However, all of them intend to represent semantics at a word scale. Although vectors addition and multiplication are two well defined operations suitable for composing words in semantic spaces, they miss taking into account the underlying syntax, which regulates the compositionality of words. Some efforts toward this direction are emerging (Clark and Pulman, 2007; Clark et al., 2008; Mitchell and Lapata, 2010; Coecke et al., 2010; Basile et al., 2011; Clarke, 2012), which resulted in theoretical work corroborated by empirical evaluation on how small fragments of text compose (e.g. noun-noun, adjectivenoun, and verb-noun pairs). 2 Methodology Our approach to STS is inspired by the latest developments about semantic compositionality and distributional models. The general methodology is based on the construction of a semantic space endowed 591 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 591–596, Montr´eal, Canada, June 7-8</context>
</contexts>
<marker>Clark, Pulman, 2007</marker>
<rawString>Stephen Clark and Stephen Pulman. 2007. Combining symbolic and distributional models of meaning. In Proceedings of the AAAI Spring Symposium on Quantum Interaction, pages 52–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>Bob Coecke</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>A compositional distributional model of meaning.</title>
<date>2008</date>
<booktitle>In Proceedings of the Second Quantum Interaction Symposium (QI-2008),</booktitle>
<pages>133--140</pages>
<contexts>
<context position="2872" citStr="Clark et al., 2008" startWordPosition="439" endWordPosition="442"> al., 1990), BEAGLE (Jones and Mewhort, 2007), Random Indexing (Kanerva, 1988), Hyperspace Analogue to Language (Burgess et al., 1998), WordSpace (Schütze and Pedersen, 1995) are all techniques conceived to build up semantic spaces. However, all of them intend to represent semantics at a word scale. Although vectors addition and multiplication are two well defined operations suitable for composing words in semantic spaces, they miss taking into account the underlying syntax, which regulates the compositionality of words. Some efforts toward this direction are emerging (Clark and Pulman, 2007; Clark et al., 2008; Mitchell and Lapata, 2010; Coecke et al., 2010; Basile et al., 2011; Clarke, 2012), which resulted in theoretical work corroborated by empirical evaluation on how small fragments of text compose (e.g. noun-noun, adjectivenoun, and verb-noun pairs). 2 Methodology Our approach to STS is inspired by the latest developments about semantic compositionality and distributional models. The general methodology is based on the construction of a semantic space endowed 591 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 591–596, Montr´eal, Canada, June 7-8, 2012. c�2012 Assoc</context>
</contexts>
<marker>Clark, Coecke, Sadrzadeh, 2008</marker>
<rawString>Stephen Clark, Bob Coecke, and Mehrnoosh Sadrzadeh. 2008. A compositional distributional model of meaning. In Proceedings of the Second Quantum Interaction Symposium (QI-2008), pages 133–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daoud Clarke</author>
</authors>
<title>A context–theoretic framework for compositionality in distributional semantics.</title>
<date>2012</date>
<journal>Computational Linguistics,</journal>
<volume>38</volume>
<issue>1</issue>
<contexts>
<context position="2956" citStr="Clarke, 2012" startWordPosition="455" endWordPosition="456">e Analogue to Language (Burgess et al., 1998), WordSpace (Schütze and Pedersen, 1995) are all techniques conceived to build up semantic spaces. However, all of them intend to represent semantics at a word scale. Although vectors addition and multiplication are two well defined operations suitable for composing words in semantic spaces, they miss taking into account the underlying syntax, which regulates the compositionality of words. Some efforts toward this direction are emerging (Clark and Pulman, 2007; Clark et al., 2008; Mitchell and Lapata, 2010; Coecke et al., 2010; Basile et al., 2011; Clarke, 2012), which resulted in theoretical work corroborated by empirical evaluation on how small fragments of text compose (e.g. noun-noun, adjectivenoun, and verb-noun pairs). 2 Methodology Our approach to STS is inspired by the latest developments about semantic compositionality and distributional models. The general methodology is based on the construction of a semantic space endowed 591 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 591–596, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics with a vector addition operator. The vector add</context>
</contexts>
<marker>Clarke, 2012</marker>
<rawString>Daoud Clarke. 2012. A context–theoretic framework for compositionality in distributional semantics. Computational Linguistics, 38(1):41–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Coecke</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Stephen Clark</author>
</authors>
<title>Mathematical foundations for a compositional distributional model of meaning.</title>
<date>2010</date>
<journal>CoRR,</journal>
<pages>1003--4394</pages>
<contexts>
<context position="2920" citStr="Coecke et al., 2010" startWordPosition="447" endWordPosition="450">Random Indexing (Kanerva, 1988), Hyperspace Analogue to Language (Burgess et al., 1998), WordSpace (Schütze and Pedersen, 1995) are all techniques conceived to build up semantic spaces. However, all of them intend to represent semantics at a word scale. Although vectors addition and multiplication are two well defined operations suitable for composing words in semantic spaces, they miss taking into account the underlying syntax, which regulates the compositionality of words. Some efforts toward this direction are emerging (Clark and Pulman, 2007; Clark et al., 2008; Mitchell and Lapata, 2010; Coecke et al., 2010; Basile et al., 2011; Clarke, 2012), which resulted in theoretical work corroborated by empirical evaluation on how small fragments of text compose (e.g. noun-noun, adjectivenoun, and verb-noun pairs). 2 Methodology Our approach to STS is inspired by the latest developments about semantic compositionality and distributional models. The general methodology is based on the construction of a semantic space endowed 591 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 591–596, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics with a vect</context>
</contexts>
<marker>Coecke, Sadrzadeh, Clark, 2010</marker>
<rawString>Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark. 2010. Mathematical foundations for a compositional distributional model of meaning. CoRR, abs/1003.4394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohen</author>
<author>Dominic Widdows</author>
<author>Roger W Schvaneveldt</author>
<author>Thomas C Rindflesch</author>
</authors>
<title>Logical leaps and quantum connectives: Forging paths through predication space.</title>
<date>2010</date>
<booktitle>In AAAI-Fall 2010 Symposium on Quantum Informatics for Cognitive, Social, and Semantic Processes,</booktitle>
<pages>11--13</pages>
<contexts>
<context position="10611" citStr="Cohen et al., 2010" startWordPosition="1736" endWordPosition="1739">ackward rotation of one element. If H1 is a permutation of one element, the inversepermutation is defined as H−1: the elements rotation is performed by one left-shifting step. Formally, denoting with x the context vector for a term, we compute the semantic vector for the term ti as follows: �si = xi + (H−1xj + xj) + j ddep(ti,tj) � (H1xk + xk) k ddep(tk,ti) Adding permuted vectors to the head word and inverse-permuted vectors to the corresponding dependent word allows to encode the information about both heads and dependents into the space. This approach is similar to the one investigated by (Cohen et al., 2010) to encode relations between medical terms. 3 Evaluation Dataset Description. SemEval-2012 STS is a first attempt to provide a “unified framework for the evaluation of modular semantic components.” The task consists in computing the similarity between pair of texts, returning a similarity score. Sentences are extracted from five publicly available datasets: MSR (Paraphrase Microsoft Research Paraphrase Corpus, 750 pairs), MSR (Video Microsoft Research Video Description Corpus, 750 pairs), SMTeuroparl (WMT2008 development dataset, Europarl section, 459 pairs), SMTnews (news conversation sentenc</context>
</contexts>
<marker>Cohen, Widdows, Schvaneveldt, Rindflesch, 2010</marker>
<rawString>Trevor Cohen, Dominic Widdows, Roger W. Schvaneveldt, and Thomas C. Rindflesch. 2010. Logical leaps and quantum connectives: Forging paths through predication space. In AAAI-Fall 2010 Symposium on Quantum Informatics for Cognitive, Social, and Semantic Processes, pages 11–13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan T Dumais</author>
<author>George W Furnas</author>
<author>Thomas K Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<pages>407</pages>
<contexts>
<context position="2265" citStr="Deerwester et al., 1990" startWordPosition="346" endWordPosition="349">s like “basketball” and “volleyball”, which occur frequently in similar contexts, say with words “court, play, player”, will be represented by near points. Different definitions of contexts give rise to different (semantic) spaces. A context can be a document, a sentence or a fixed window of surrounding words. Contexts and words can be stored through a co-occurrence matrix, whose columns correspond to contexts, and rows to words. Therefore, the strength of the semantic association between words can be computed as the cosine similarity of their vector representations. Latent Semantic Analysis (Deerwester et al., 1990), BEAGLE (Jones and Mewhort, 2007), Random Indexing (Kanerva, 1988), Hyperspace Analogue to Language (Burgess et al., 1998), WordSpace (Schütze and Pedersen, 1995) are all techniques conceived to build up semantic spaces. However, all of them intend to represent semantics at a word scale. Although vectors addition and multiplication are two well defined operations suitable for composing words in semantic spaces, they miss taking into account the underlying syntax, which regulates the compositionality of words. Some efforts toward this direction are emerging (Clark and Pulman, 2007; Clark et al</context>
<context position="5987" citStr="Deerwester et al., 1990" startWordPosition="943" endWordPosition="946">igh-dimensional and ternary, which means that its elements can take values 1An implementation of RI can be found at: http://code.google.com/p/semanticvectors/ in {-1, 0, 1}. A context vector contains a small number of randomly distributed non-zero elements, and the structure of this vector follows the hypothesis behind the concept of Random Projection; 2. Context vectors are accumulated by analyzing co-occurring terms in a window w. The semantic vector for a term is computed as the sum of the context vectors for terms which co-occur in w. 2.2 Latent Semantic Analysis Latent Semantic Analysis (Deerwester et al., 1990) relies on the Singular Value Decomposition (SVD) of a term-document co-occurrence matrix. Given a matrix M, it can be decomposed in the product of three matrices UEVT, where U and V are the orthonormal matrices and E is the diagonal matrix of singular values of M placed in decreasing order. Computing the LSA on the co-occurrence matrix M can be a computationally expensive task, as a corpus can contain thousands of terms. Hence, we decided to apply LSA to the reduced approximation generated by RI. It is important to point out that no truncation of singular values is performed. Since computing </context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41(6):391– 407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<date>1968</date>
<booktitle>Mathematical Structures of Language.</booktitle>
<location>New York: Interscience.</location>
<contexts>
<context position="1139" citStr="Harris, 1968" startWordPosition="162" endWordPosition="163">, Latent Semantic Analysis (LSA) over RI, and vector permutations in RI. Runs based on these spaces consistently outperform the baseline on the proposed datasets. 1 Background and Related Research SemEval-2012 Semantic Textual Similarity (STS) task (Agirre et al., 2012) aims at providing a general framework to “examine the degree of semantic equivalence between two sentences.” We propose an approach to Semantic Textual Similarity based on distributional models of words, where the geometrical metaphor of meaning is exploited. Distributional models are grounded on the distributional hypothesis (Harris, 1968), according to which the meaning of a word is determined by the set of textual contexts in which it appears. These models represent words as vectors in a high dimensional vector space. Word vectors are built from a large corpus in such a way that vector dimensions reflect the different uses (or contexts) of a word in the corpus. Hence, the meaning of a word is defined by its use, and words used in similar contexts are represented by vectors near in the space. In this way, semantically related words like “basketball” and “volleyball”, which occur frequently in similar contexts, say with words “</context>
</contexts>
<marker>Harris, 1968</marker>
<rawString>Zellig Harris. 1968. Mathematical Structures of Language. New York: Interscience.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William B Johnson</author>
<author>Joram Lindenstrauss</author>
</authors>
<date>1984</date>
<booktitle>Extensions of Lipschitz mappings into a Hilbert space. Conference on Modern Analysis and Probability, Contemporary Mathematics,</booktitle>
<pages>26--189</pages>
<contexts>
<context position="5177" citStr="Johnson and Lindenstrauss, 1984" startWordPosition="812" endWordPosition="815">lve efficiently the problem of reducing dimensions, which is one of the key features used to uncover the “latent semantic dimensions” of a word distribution. RI1 (Widdows and Ferraro, 2008) is based on the concept of Random Projection according to which high dimensional vectors chosen randomly are “nearly orthogonal”. Formally, given an n x m matrix A and an m x k matrix R made up of k m-dimensional random vectors, we define a new n x k matrix B as follows: Bn,k = An,m, Rm,k k &lt;&lt; m (1) The new matrix B has the property to preserve the distance between points scaled by a multiplicative factor (Johnson and Lindenstrauss, 1984). Specifically, RI creates the semantic space Bn,k in two steps (we consider a fixed window w of terms as context): 1. A context vector is assigned to each term. This vector is sparse, high-dimensional and ternary, which means that its elements can take values 1An implementation of RI can be found at: http://code.google.com/p/semanticvectors/ in {-1, 0, 1}. A context vector contains a small number of randomly distributed non-zero elements, and the structure of this vector follows the hypothesis behind the concept of Random Projection; 2. Context vectors are accumulated by analyzing co-occurrin</context>
</contexts>
<marker>Johnson, Lindenstrauss, 1984</marker>
<rawString>William B. Johnson and Joram Lindenstrauss. 1984. Extensions of Lipschitz mappings into a Hilbert space. Conference on Modern Analysis and Probability, Contemporary Mathematics, 26:189–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael N Jones</author>
<author>Douglas J K Mewhort</author>
</authors>
<title>Representing word meaning and order information in a composite holographic lexicon.</title>
<date>2007</date>
<journal>Psychological Review,</journal>
<volume>114</volume>
<issue>1</issue>
<contexts>
<context position="2299" citStr="Jones and Mewhort, 2007" startWordPosition="351" endWordPosition="354">l”, which occur frequently in similar contexts, say with words “court, play, player”, will be represented by near points. Different definitions of contexts give rise to different (semantic) spaces. A context can be a document, a sentence or a fixed window of surrounding words. Contexts and words can be stored through a co-occurrence matrix, whose columns correspond to contexts, and rows to words. Therefore, the strength of the semantic association between words can be computed as the cosine similarity of their vector representations. Latent Semantic Analysis (Deerwester et al., 1990), BEAGLE (Jones and Mewhort, 2007), Random Indexing (Kanerva, 1988), Hyperspace Analogue to Language (Burgess et al., 1998), WordSpace (Schütze and Pedersen, 1995) are all techniques conceived to build up semantic spaces. However, all of them intend to represent semantics at a word scale. Although vectors addition and multiplication are two well defined operations suitable for composing words in semantic spaces, they miss taking into account the underlying syntax, which regulates the compositionality of words. Some efforts toward this direction are emerging (Clark and Pulman, 2007; Clark et al., 2008; Mitchell and Lapata, 2010</context>
</contexts>
<marker>Jones, Mewhort, 2007</marker>
<rawString>Michael N. Jones and Douglas J. K. Mewhort. 2007. Representing word meaning and order information in a composite holographic lexicon. Psychological Review, 114(1):1–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pentti Kanerva</author>
</authors>
<title>Sparse Distributed Memory.</title>
<date>1988</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2332" citStr="Kanerva, 1988" startWordPosition="357" endWordPosition="358">xts, say with words “court, play, player”, will be represented by near points. Different definitions of contexts give rise to different (semantic) spaces. A context can be a document, a sentence or a fixed window of surrounding words. Contexts and words can be stored through a co-occurrence matrix, whose columns correspond to contexts, and rows to words. Therefore, the strength of the semantic association between words can be computed as the cosine similarity of their vector representations. Latent Semantic Analysis (Deerwester et al., 1990), BEAGLE (Jones and Mewhort, 2007), Random Indexing (Kanerva, 1988), Hyperspace Analogue to Language (Burgess et al., 1998), WordSpace (Schütze and Pedersen, 1995) are all techniques conceived to build up semantic spaces. However, all of them intend to represent semantics at a word scale. Although vectors addition and multiplication are two well defined operations suitable for composing words in semantic spaces, they miss taking into account the underlying syntax, which regulates the compositionality of words. Some efforts toward this direction are emerging (Clark and Pulman, 2007; Clark et al., 2008; Mitchell and Lapata, 2010; Coecke et al., 2010; Basile et </context>
<context position="4326" citStr="Kanerva, 1988" startWordPosition="666" endWordPosition="667">ine similarity. However, this simple methodology translates a text into a mere bag-of-word representation, depriving the text of its syntactic construction, which also influences the overall meaning of the sentence. In order to deal with this limit, we experiment two classical methods for building a semantic space, namely Random Indexing and Latent Semantic Analysis, along with a new method based on vector permutations, which tries to encompass syntactic information directly into the resulting space. 2.1 Random Indexing Our first method is based on Random Indexing (RI), introduced by Kanerva (Kanerva, 1988). This technique allows us to build a semantic space with no need for (either term-document or term-term) matrix factorization, because vectors are inferred by using an incremental strategy. Moreover, it allows us to solve efficiently the problem of reducing dimensions, which is one of the key features used to uncover the “latent semantic dimensions” of a word distribution. RI1 (Widdows and Ferraro, 2008) is based on the concept of Random Projection according to which high dimensional vectors chosen randomly are “nearly orthogonal”. Formally, given an n x m matrix A and an m x k matrix R made </context>
</contexts>
<marker>Kanerva, 1988</marker>
<rawString>Pentti Kanerva. 1988. Sparse Distributed Memory. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="2899" citStr="Mitchell and Lapata, 2010" startWordPosition="443" endWordPosition="446">(Jones and Mewhort, 2007), Random Indexing (Kanerva, 1988), Hyperspace Analogue to Language (Burgess et al., 1998), WordSpace (Schütze and Pedersen, 1995) are all techniques conceived to build up semantic spaces. However, all of them intend to represent semantics at a word scale. Although vectors addition and multiplication are two well defined operations suitable for composing words in semantic spaces, they miss taking into account the underlying syntax, which regulates the compositionality of words. Some efforts toward this direction are emerging (Clark and Pulman, 2007; Clark et al., 2008; Mitchell and Lapata, 2010; Coecke et al., 2010; Basile et al., 2011; Clarke, 2012), which resulted in theoretical work corroborated by empirical evaluation on how small fragments of text compose (e.g. noun-noun, adjectivenoun, and verb-noun pairs). 2 Methodology Our approach to STS is inspired by the latest developments about semantic compositionality and distributional models. The general methodology is based on the construction of a semantic space endowed 591 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 591–596, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Li</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive Science, 34(8):1388–1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Atanas Chanev, Gülsen Eryigit, Sandra Kübler, Svetoslav Marinov, and Erwin Marsi.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>02</issue>
<contexts>
<context position="12725" citStr="Nivre et al., 2007" startWordPosition="2055" endWordPosition="2058">earson correlation with the gold standard for the five datasets. ALLnrm Pearson correlation after the system outputs for each dataset are fitted to the gold standard using least squares. Mean Weighted mean across the five datasets, where the weight depends on the number of pairs in the dataset. Experimental Setting. For the evaluation, we built Distributional Spaces using the WaCkypedia_EN corpus2. WaCkypedia_EN is based on a 2009 dump of the English Wikipedia (about 800 million tokens) and includes information about: part-ofspeech, lemma and a full dependency parsing performed by MaltParser (Nivre et al., 2007). The three spaces described in Section 2 are built exploiting information about term windows and dependency parsing supplied by WaCkypedia. The total number of dependencies amounts to about 200 million. The RI system is implemented in Java and relies on some portions of code publicly available in the Semantic Vectors package (Widdows and Ferraro, 2008), while for LSA we exploited the publicly available C library SVDLIBC3. We restricted the vocabulary to the 50,000 most frequent terms, with stop words removal and forcing the system to include terms which occur in the dataset. Hence, the dimens</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, Gülsen Eryigit, Sandra Kübler, Svetoslav Marinov, and Erwin Marsi. 2007. Maltparser: A languageindependent system for data-driven dependency parsing. Natural Language Engineering, 13(02):95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Sahlgren</author>
<author>Anders Holst</author>
<author>Pentti Kanerva</author>
</authors>
<title>Permutations as a means to encode order in word space.</title>
<date>2008</date>
<booktitle>Proceedings of the 30th Annual Meeting of the Cognitive Science Society (CogSci’08),</booktitle>
<pages>23--26</pages>
<editor>In V. Sloutsky, B. Love, and K. Mcrae, editors,</editor>
<publisher>Cognitive Science Society,</publisher>
<location>Washington D.C., USA,</location>
<contexts>
<context position="7770" citStr="Sahlgren et al., 2008" startWordPosition="1255" endWordPosition="1258">aid, however general performance was better than VSM and RI respectively. We also experimented LSA computed on RI versus LSA applied to the original matrix during the tuning of our systems. Surprisingly, we found that LSA applied on the reduced matrix gives better results than LSA. However, these results are not reported as they are not the focus of this evaluation. 592 2.3 Vector Permutations in RI The classical distributional models can handle only one definition of context at a time, such as the whole document or the window w. A method to add information about context in RI is proposed in (Sahlgren et al., 2008). The authors describe a strategy to encode word order in RI by the permutation of coordinates in context vector. When the coordinates are shuffled using a random permutation, the resulting vector is nearly orthogonal to the original one. That operation corresponds to the generation of a new random vector. Moreover, by applying a predetermined mechanism to obtain random permutations, such as elements rotation, it is always possible to reconstruct the original vector using the reverse permutations. By exploiting this strategy it is possible to obtain different random vectors for each context in</context>
</contexts>
<marker>Sahlgren, Holst, Kanerva, 2008</marker>
<rawString>Magnus Sahlgren, Anders Holst, and Pentti Kanerva. 2008. Permutations as a means to encode order in word space. In V. Sloutsky, B. Love, and K. Mcrae, editors, Proceedings of the 30th Annual Meeting of the Cognitive Science Society (CogSci’08), July 23-26, Washington D.C., USA, pages 1300–1305. Cognitive Science Society, Austin, TX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schütze</author>
<author>Jan O Pedersen</author>
</authors>
<title>Information retrieval based on word senses.</title>
<date>1995</date>
<booktitle>In Proceedings of the 4th Annual Symposium on Document Analysis and Information Retrieval,</booktitle>
<pages>161--175</pages>
<contexts>
<context position="2428" citStr="Schütze and Pedersen, 1995" startWordPosition="368" endWordPosition="371">erent definitions of contexts give rise to different (semantic) spaces. A context can be a document, a sentence or a fixed window of surrounding words. Contexts and words can be stored through a co-occurrence matrix, whose columns correspond to contexts, and rows to words. Therefore, the strength of the semantic association between words can be computed as the cosine similarity of their vector representations. Latent Semantic Analysis (Deerwester et al., 1990), BEAGLE (Jones and Mewhort, 2007), Random Indexing (Kanerva, 1988), Hyperspace Analogue to Language (Burgess et al., 1998), WordSpace (Schütze and Pedersen, 1995) are all techniques conceived to build up semantic spaces. However, all of them intend to represent semantics at a word scale. Although vectors addition and multiplication are two well defined operations suitable for composing words in semantic spaces, they miss taking into account the underlying syntax, which regulates the compositionality of words. Some efforts toward this direction are emerging (Clark and Pulman, 2007; Clark et al., 2008; Mitchell and Lapata, 2010; Coecke et al., 2010; Basile et al., 2011; Clarke, 2012), which resulted in theoretical work corroborated by empirical evaluatio</context>
</contexts>
<marker>Schütze, Pedersen, 1995</marker>
<rawString>Hinrich Schütze and Jan O. Pedersen. 1995. Information retrieval based on word senses. In Proceedings of the 4th Annual Symposium on Document Analysis and Information Retrieval, pages 161–175.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Linus Sellberg</author>
<author>Arne Jönsson</author>
</authors>
<title>Using random indexing to improve singular value decomposition for latent semantic analysis.</title>
<date>2008</date>
<booktitle>Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC2008),</booktitle>
<pages>2335--2338</pages>
<editor>In Nicoletta Calzolari, Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odjik, Stelios Piperidis, and Daniel Tapias, editors,</editor>
<location>Marrakech,</location>
<contexts>
<context position="6908" citStr="Sellberg and Jönsson (2008)" startWordPosition="1104" endWordPosition="1107">omputing the LSA on the co-occurrence matrix M can be a computationally expensive task, as a corpus can contain thousands of terms. Hence, we decided to apply LSA to the reduced approximation generated by RI. It is important to point out that no truncation of singular values is performed. Since computing the similarity between any two words is equal to taking the corresponding entry in the MMT matrix, we can exploit the relation MMT = UEVTVETUT = UEETUT = (UE)(UE)T Hence, the application of LSA to RI makes possible to represent each word in the UE space. A similar approach was investigated by Sellberg and Jönsson (2008) for retrieval of similar FAQs in a Question Answering system. Authors showed that halving the matrix dimension by applying the RI resulted in a drastic reduction of LSA computation time. Certainly there was also a performance price to be paid, however general performance was better than VSM and RI respectively. We also experimented LSA computed on RI versus LSA applied to the original matrix during the tuning of our systems. Surprisingly, we found that LSA applied on the reduced matrix gives better results than LSA. However, these results are not reported as they are not the focus of this eva</context>
</contexts>
<marker>Sellberg, Jönsson, 2008</marker>
<rawString>Linus Sellberg and Arne Jönsson. 2008. Using random indexing to improve singular value decomposition for latent semantic analysis. In Nicoletta Calzolari, Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odjik, Stelios Piperidis, and Daniel Tapias, editors, Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC2008), pages 2335–2338, Marrakech, Morocco. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Dominic Widdows</author>
<author>Kathleen Ferraro</author>
</authors>
<title>Semantic Vectors: A Scalable Open Source Package and Online Technology Management Application.</title>
<date>2008</date>
<booktitle>Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC2008),</booktitle>
<pages>1183--1190</pages>
<editor>In Nicoletta Calzolari, Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odjik, Stelios Piperidis, and Daniel Tapias, editors,</editor>
<location>Marrakech,</location>
<contexts>
<context position="4734" citStr="Widdows and Ferraro, 2008" startWordPosition="730" endWordPosition="733">sed on vector permutations, which tries to encompass syntactic information directly into the resulting space. 2.1 Random Indexing Our first method is based on Random Indexing (RI), introduced by Kanerva (Kanerva, 1988). This technique allows us to build a semantic space with no need for (either term-document or term-term) matrix factorization, because vectors are inferred by using an incremental strategy. Moreover, it allows us to solve efficiently the problem of reducing dimensions, which is one of the key features used to uncover the “latent semantic dimensions” of a word distribution. RI1 (Widdows and Ferraro, 2008) is based on the concept of Random Projection according to which high dimensional vectors chosen randomly are “nearly orthogonal”. Formally, given an n x m matrix A and an m x k matrix R made up of k m-dimensional random vectors, we define a new n x k matrix B as follows: Bn,k = An,m, Rm,k k &lt;&lt; m (1) The new matrix B has the property to preserve the distance between points scaled by a multiplicative factor (Johnson and Lindenstrauss, 1984). Specifically, RI creates the semantic space Bn,k in two steps (we consider a fixed window w of terms as context): 1. A context vector is assigned to each t</context>
<context position="13080" citStr="Widdows and Ferraro, 2008" startWordPosition="2111" endWordPosition="2115">istributional Spaces using the WaCkypedia_EN corpus2. WaCkypedia_EN is based on a 2009 dump of the English Wikipedia (about 800 million tokens) and includes information about: part-ofspeech, lemma and a full dependency parsing performed by MaltParser (Nivre et al., 2007). The three spaces described in Section 2 are built exploiting information about term windows and dependency parsing supplied by WaCkypedia. The total number of dependencies amounts to about 200 million. The RI system is implemented in Java and relies on some portions of code publicly available in the Semantic Vectors package (Widdows and Ferraro, 2008), while for LSA we exploited the publicly available C library SVDLIBC3. We restricted the vocabulary to the 50,000 most frequent terms, with stop words removal and forcing the system to include terms which occur in the dataset. Hence, the dimension of the original matrix would have been 50,000×50,000. 2http://wacky.sslmit.unibo.it/doku.php?id=corpora 3http://tedlab.mit.edu/ dr/SVDLIBC/ Our approach involves some parameters. In particular, each semantic space needs to set up the dimension k of the space. All spaces use a dimension of 500 (resulting in a 50,000×500 matrix). The number of non-zer</context>
</contexts>
<marker>Widdows, Ferraro, 2008</marker>
<rawString>Dominic Widdows and Kathleen Ferraro. 2008. Semantic Vectors: A Scalable Open Source Package and Online Technology Management Application. In Nicoletta Calzolari, Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odjik, Stelios Piperidis, and Daniel Tapias, editors, Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC2008), pages 1183–1190, Marrakech, Morocco. European Language Resources Association (ELRA).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>