<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000099">
<title confidence="0.998923">
A Lucene and Maximum Entropy Model Based Hedge Detection System
</title>
<author confidence="0.998664">
Lin Chen
</author>
<affiliation confidence="0.999264">
University of Illinois at Chicago
</affiliation>
<address confidence="0.704923">
Chicago, IL, USA
</address>
<email confidence="0.988663">
lin@chenlin.net
</email>
<author confidence="0.99749">
Barbara Di Eugenio
</author>
<affiliation confidence="0.998869">
University of Illinois at Chicago
</affiliation>
<address confidence="0.708964">
Chicago, IL, USA
</address>
<email confidence="0.997492">
bdieugen@uic.edu
</email>
<sectionHeader confidence="0.993863" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999674545454545">
This paper describes the approach to
hedge detection we developed, in order to
participate in the shared task at CoNLL-
2010. A supervised learning approach is
employed in our implementation. Hedge
cue annotations in the training data are
used as the seed to build a reliable hedge
cue set. Maximum Entropy (MaxEnt)
model is used as the learning technique to
determine uncertainty. By making use of
Apache Lucene, we are able to do fuzzy
string match to extract hedge cues, and
to incorporate part-of-speech (POS) tags
in hedge cues. Not only can our system
determine the certainty of the sentence,
but is also able to find all the contained
hedges. Our system was ranked third on
the Wikipedia dataset. In later experi-
ments with different parameters, we fur-
ther improved our results, with a 0.612
F-score on the Wikipedia dataset, and a
0.802 F-score on the biological dataset.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999910615384615">
A hedge is a mitigating device used to lessen the
impact of an utterance1. As a very important way
to precisely express the degree of accuracy and
truth assessment in human communication, hedg-
ing is widely used in both spoken and written lan-
guages. Detecting hedges in natural language text
can be very useful for areas like text mining and
information extraction. For example, in opinion
mining, hedges can be used to assess the degree
of sentiment, and refine sentiment classes from
{positive, negative, objective} to {positive, some-
how positive, objective, somehow objective, nega-
tive, somehow negative}.
</bodyText>
<footnote confidence="0.8082945">
1http://en.wikipedia.org/wiki/
Hedge(linguistics)
</footnote>
<bodyText confidence="0.999926380952381">
Hedge detection related work has been con-
ducted by several people. Light et al. (2004)
started to do annotations on biomedicine article
abstracts, and conducted the preliminary work of
automatic classification for uncertainty. Medlock
and Briscoe (2007) devised detailed guidelines
for hedge annotations, and used a probabilistic
weakly supervised learning approach to classify
hedges. Ganter and Strube (2009) took Wikipedia
articles as training corpus, used weasel words’ fre-
quency and syntactic patterns as features to clas-
sify uncertainty.
The rest of the paper is organized as follows.
Section 2 shows the architecture of our system.
Section 3 explains how we make use of Apache
Lucene to do fuzzy string match and incorporate
POS tag in hedge cues and our method to gener-
ate hedge cue candidates. Section 4 describes the
details of using MaxEnt model to classify uncer-
tainty. We present and discuss experiments and
results in section 5, and conclude in section 6.
</bodyText>
<sectionHeader confidence="0.941376" genericHeader="method">
2 System Architecture
</sectionHeader>
<bodyText confidence="0.99968675">
Our system is divided into training and testing
modules. The architecture of our system is shown
in Figure 1.
In the training module, we use the training cor-
pus to learn a reliable hedge cue set with bal-
anced support and confidence, then train a Max-
Ent model for each hedge cue to classify the un-
certainty for sentences matched by that hedge cue.
In the testing module, the learned hedge cues
are used to match the sentences to classify, then
each matched sentence is classified using the cor-
responding MaxEnt model. A sentence will be
classified as uncertain if the MaxEnt model deter-
mines it is. Because of this design, our system is
not only able to check if a sentence is uncertain,
but also can detect the contained hedges.
</bodyText>
<page confidence="0.981332">
114
</page>
<note confidence="0.616679">
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 114–119,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<figure confidence="0.999414">
Annotation Extender
POS Tag Replacer
MaxEnt Models
MaxEnt Trainer
Training Data
Lucene Indexer
Index with POS
Token Pruner
Hedge Cues
Hedge Cue
Candidates
Confidence?
Support?
Hedge Cue Candidate Generator
yes
Trainging
no
Mark Sentece Uncertainty
Get Matched Sentences
Get the MaxEnt Model
Get A Hedge Cue
Get A Sentence
More Sentence?
Hedge Cues
Marked
Uncertain?
Uncertain?
More Cue?
yes
yes
yes
no
Output
Marked
Uncertainty
yes
no
Lucene Indexer
Index with POS
Testing Data
Testing
</figure>
<figureCaption confidence="0.999948">
Figure 1: System Architecture
</figureCaption>
<sectionHeader confidence="0.95983" genericHeader="method">
3 Learn Hedge Cues
</sectionHeader>
<bodyText confidence="0.98683668">
The training data provided by CoNLL-2010
shared task contain “&lt;ccue&gt;&lt;/ccue&gt;” annota-
tions for uncertain sentences. Most of the annota-
tions are either too strict, which makes them hard
to use to match other sentences, or too general,
which means that most of the matched sentences
are not uncertain.
Similar to how Liu (2007) measures the useful-
ness of association rules, we use support and con-
fidence to measure the usefulness of a hedge cue.
Support is the ratio of sentences containing a
hedge cue to all sentences. Because in a train-
ing dataset, the number of all the sentences is a
fixed constant, we only use the number of sen-
tences containing the hedge cue as support, see
formula 1. In the other part of this paper, sentences
matched by hedge cues means sentences contains
hedge cues. We use support to measure the degree
of generality of a hedge cue.
sup = count of matched sentences (1)
Confidence is the ratio of sentences which con-
tain a hedge cue and are uncertain to all the sen-
tences containing the hedge cue, as formula 2.
We use confidence to measure the reliability for
a word or phrase to be a hedge cue.
</bodyText>
<subsectionHeader confidence="0.999985">
3.1 Usage of Apache Lucene
</subsectionHeader>
<bodyText confidence="0.997215125">
Apache Lucene2 is a full text indexing Java library
provided as an open source project of Apache
Foundation. It provides flexible indexing and
search capability for text documents, and it has
very high performance. To explain the integra-
tion of Lucene into our implementation, we need
to introduce several terms, some of which come
from McCandless et al. (2010).
</bodyText>
<listItem confidence="0.995228866666667">
• Analyzer: Raw texts are preprocessed before
being added to the index: text preprocessing
components such as tokenization, stop words
removal, and stemming are parts of an ana-
lyzer.
• Document: A document represents a collec-
tion of fields, it could be a web page, a text
file, or only a paragraph of an article.
• Field: A field represents a document or the
meta-data associated with that document, like
the author, type, URL. A field has a name and
a value, and a bunch of options to control how
Lucene will index its value.
• Term: The very basic unit of a search. It con-
tains a field name and a value to search.
</listItem>
<footnote confidence="0.643354333333333">
conf = (2)
count of matched sentences
2http://lucene.apache.org
</footnote>
<page confidence="0.959113">
115
</page>
<listItem confidence="0.9578285">
• Query: The root class used to do search upon
an index.
</listItem>
<bodyText confidence="0.9985855">
In our implementation, Lucene is used for the
following 3 purposes:
</bodyText>
<listItem confidence="0.968856666666667">
• Enable quick counting for combinations of
words and POS tags.
• Store the training and testing corpus for fast
counting and retrieval.
• Allow gap between words or POS tags in
hedge cues to match sentences.
</listItem>
<bodyText confidence="0.999661363636364">
Lucene provides the capability to build cus-
tomized analyzers for complex linguistics analy-
sis. Our customized Lucene analyzer employs to-
kenizer and POS tagger from OpenNLP tools3 to
do tokenization and POS tagging. For every word
in the sentence, we put two Lucene tokens in the
same position, by setting up the second token’s Po-
sitionIncremental attribute to be 0.
For example, for sentence it is believed to be
very good, our analyzer will make Lucene store it
as Figure 2 in its index.
</bodyText>
<figureCaption confidence="0.998774">
Figure 2: Customized Tokenizer Example
</figureCaption>
<bodyText confidence="0.993003473684211">
Indexing text in that way, we are able to match
sentences cross words and POS tags. For example,
the phrase it is believed will be matched by it is be-
lieved, it is VBN, it VBZ believed. This technique
enables us to generalize a hedge cue.
In our implementation, all the data for training
and testing are indexed. The indexing schema is: a
sentence is treated as a Lucene document; the con-
tent of the sentence is analyzed by our customized
analyzer; other information like sentence id, sen-
tence position, uncertainty is stored as fields of the
document. In this way, we can query all those
fields, and when we find a match, we can easily
get all the information out just from the index.
Lucene provides various types of queries to
search the indexed content. We use SpanNear-
Query and BooleanQuery to search the matched
sentences for hedge cues. We rely on SpanNear-
Query’s feature of allowing positional restriction
</bodyText>
<footnote confidence="0.66178">
3http://opennlp.sourceforge.net
</footnote>
<bodyText confidence="0.999522777777778">
when matching sentences. When building a Span-
NearQuery, we can specify the position gap al-
lowed among the terms in the query. We build a
SpanNearQuery from a hedge cue, put each token
as a term of the query, and set the position gap to
be 2. Take Figure 3 as an example, because the
gap between token is and said is 1, is less than the
specified gap setting 2, so It is widely said to be
good will count as a match with hedge cue is said.
</bodyText>
<figureCaption confidence="0.9979">
Figure 3: SpanNearQuery Matching Example
</figureCaption>
<bodyText confidence="0.99980425">
We use BooleanQuery with nested SpanNear-
Query and TermQuery to count uncertain sen-
tences, then to calculate the confidence of a hedge
cue.
</bodyText>
<subsectionHeader confidence="0.995526">
3.2 Hedge Cue Candidate Generation
</subsectionHeader>
<bodyText confidence="0.974130666666667">
We firstly tried to use the token as the basic unit for
hedge cues. However, several pieces of evidence
suggest it is not appropriate.
</bodyText>
<listItem confidence="0.8835995">
• Low Coverage. We only get 42 tokens in
Wikipedia training data, using 20, 0.4 as the
thresholds for support and confidence.
• Irrelevant words or stop words with lower
thresholds. When we use 5, 0.3 as the thresh-
olds for coverage and confidence, we get 279
</listItem>
<bodyText confidence="0.935452875">
tokens, however, words like is, his, musical,
voters, makers appear in the list.
We noticed that many phrases with similar
structures or fixed collocations appear very often
in the annotations, like it is believed, it is thought,
many of them, many of these and etc. Based on this
observation, we calculated the support and confi-
dence for some examples, see table 1.
</bodyText>
<table confidence="0.996685">
Hedge Cue Sup. Conf.
it is believed 14 .93
by some 30 .87
many of 135 .65
</table>
<tableCaption confidence="0.999811">
Table 1: Hedge Cue Examples
</tableCaption>
<bodyText confidence="0.998534">
We decided to use the phrase or collocation as
the basic unit for hedge cues. There are two prob-
lems in using the original annotations as hedge
cues:
</bodyText>
<figure confidence="0.82777075">
It is believed to be very good
PRP VBZ VBN TO VB RB VBN
0 1 2 3 4 5
6
</figure>
<page confidence="0.991121">
116
</page>
<listItem confidence="0.7079596">
• High confidence but low coverage: annota-
tions that contain proper nouns always have
very high confidence, usually 100%, how-
ever, they have very low support.
• High coverage but low confidence: annota-
</listItem>
<bodyText confidence="0.964216833333333">
tions with only one token are very frequent,
but only a few of them result in enough con-
fidence.
To balance confidence and support, we built our
hedge cue candidate generator. Its architecture is
presented in Figure 4.
</bodyText>
<figureCaption confidence="0.994611">
Figure 4: Hedge Cue Candidate Generator
</figureCaption>
<bodyText confidence="0.99927253125">
The three main components of the hedge cue
candidate generator are described below.
Annotation Extender: When the input hedge
cue annotation contains only 1 token, this compo-
nent will be used. It will generate 3 more hedge
cue candidates by adding the surrounding tokens.
We expect to discover candidates with higher con-
fidence.
Token Pruner: According to our observations,
proper nouns rarely contribute to the uncertainty
of a sentence, and our Lucene based string match-
ing method ensures that the matched sentences re-
main matched after we remove tokens from the
original cue annotation. So we remove proper
nouns in the original cue annotation to generate
hedge cue candidates. By using this component,
we expect to extract hedge cues with higher sup-
port.
POS Tag Replacer: This component is used to
generalize similar phrases, by using POS tags to
replace the concrete words. For example, we use
the POS tag VBN to replace believed in it is be-
lieved to generate it is VBN. Hence, when a sen-
tence contains it is thought in the testing dataset,
even if it is thought never appeared in the train-
ing data set, we will still be able to match it and
classify it against the trained MaxEnt model. We
expect that this component will be able to increase
support. Due to the O(2&apos;) time complexity, we did
not try the brute force approach to replace every
word, only the words with the POS tags in Table 2
are replaced in the process.
</bodyText>
<table confidence="0.9988042">
POS Description Example
VBN past participle verb it is believed
NNS plural common noun some countries
DT determiner some of those
CD numeral, cardinal one of the best
</table>
<tableCaption confidence="0.999392">
Table 2: POS Tag Replacer Examples
</tableCaption>
<bodyText confidence="0.9990702">
After hedge cue candidates are generated, we
convert them to Lucene queries to calculate their
confidence and support. We prune those that fall
below the predefined confidence and support set-
tings.
</bodyText>
<sectionHeader confidence="0.984823" genericHeader="method">
4 Learn Uncertainty
</sectionHeader>
<bodyText confidence="0.9905809">
Not all the learned hedge cues have 100% uncer-
tainty confidence, given a hedge cue, we need to
learn how to classify whether a matched sentence
is uncertain or not. The classification model is,
given a tuple of (Sentence, Hedge Cue), in which
the sentence contains the hedge cue, we classify it
to the outcome set {Certain, Uncertain}.
MaxEnt is a general purpose machine learn-
ing technique, it makes no assumptions in addi-
tion to what we know from the data. MaxEnt has
been widely used in Natural Language Processing
(NLP) tasks like POS tagging, word sense disam-
biguation, and proved its efficiency. Due to Max-
Ent’s capability to combine multiple and depen-
dent knowledge sources, we employed MaxEnt as
our machine learning model. Features we used to
train the model include meta information features
and collocation features.
Meta Information Features include three fea-
tures:
</bodyText>
<listItem confidence="0.976007">
• Sentence Location: The location of the sen-
tence in the article, whether in the title or in
the content. We observed sentences in the ti-
tle are rarely uncertain.
• Number of Tokens: The number of tokens
in the sentence. Title of article is usually
shorter, and more likely to be certain.
</listItem>
<figure confidence="0.97606275">
Cue Annotations
Tokens &gt; 1
NO
YES
POS Tag Replacer
Cue Candidates
Token Pruner
Annotation Extender
</figure>
<page confidence="0.972227">
117
</page>
<listItem confidence="0.835952277777778">
• Hedge Cue Location: The location of
matched tokens in a sentence. We consider
them to be in the beginning, if the first token
of the matched part is the first token in the
sentence; to be at the end, if the last token of
the matched part is the last token of the sen-
tence; otherwise, they are in the middle. We
were trying to use this feature as a simplified
version to model the syntactic role of hedge
cues in sentences.
Collocation Features include the word and POS
tag collocation features:
• Word Collocation: Using a window size of 5,
extract all the word within that window, ex-
cluding punctuation.
• POS Tag Collocation: Using a window size
of 5, extract all the POS tags of tokens within
that window, excluding punctuation.
</listItem>
<bodyText confidence="0.99959075">
We use the OpenNLP MaxEnt4 Java library as
the MaxEnt trainer and classifier. For each hedge
cue, the training is iterated 100 times, with no cut
off threshold for events.
</bodyText>
<sectionHeader confidence="0.989351" genericHeader="evaluation">
5 Experiments and Discussion
</sectionHeader>
<bodyText confidence="0.999894833333333">
We first ran experiments to evaluate the perfor-
mance of the entire system. We used official
dataset as training and testing, with different con-
fidence and support thresholds. The result on offi-
cial Wikipedia dataset is presented in Table 3. Re-
sult on the biological dataset is listed in Table 4.
In the result tables, the first 2 columns are the con-
fidence and support threshold; “Cues” is the num-
ber of generated hedge cues; the last 3 columns are
standard classifier evaluation measures.
Our submitted result used 0.35, 5 as the thresh-
olds for confidence and support. We officially
placed third on the Wikipedia dataset, with a
0.5741 F-score, and third from last on the biolog-
ical dataset, with a 0.7692 F-score. In later ex-
periments, we used different parameters, which re-
sulted in a 0.03 F-score improvement. We believe
the big difference of ranking on different datasets
comes from the incomplete training. Due to incor-
rect estimation of running time, we only used the
smaller training file in our submitted biological re-
sult.
From Table 3 and 4, we can see that a higher
confidence threshold gives higher precision, and
</bodyText>
<footnote confidence="0.944691">
4http://maxent.sourceforge.net
</footnote>
<table confidence="0.998364454545455">
Conf. Sup. Cues Prec. Recall F
10 360 0.658 0.561 0.606
0.4 15 254 0.672 0.534 0.595
20 186 0.682 0.508 0.582
10 293 0.7 0.534 0.606
0.45 15 190 0.717 0.503 0.591
20 137 0.732 0.476 0.577
5 480 0.712 0.536 0.612
0.5 10 222 0.736 0.492 0.590
15 149 0.746 0.468 0.575
20 112 0.758 0.443 0.559
</table>
<tableCaption confidence="0.998268">
Table 3: Evaluation Result on Wikipedia Dataset
</tableCaption>
<table confidence="0.999550181818182">
Conf. Sup. Cues Prec. Recall F
10 330 0.68 0.884 0.769
0.4 15 229 0.681 0.861 0.76
20 187 0.679 0.842 0.752
10 317 0.689 0.878 0.772
0.45 15 220 0.69 0.857 0.764
20 179 0.688 0.838 0.756
5 586 0.724 0.899 0.802
10 297 0.742 0.841 0.788
0.5 15 206 0.742 0.819 0.779
20 169 0.74 0.8 0.769
</table>
<tableCaption confidence="0.99989">
Table 4: Evaluation Result on Biological Dataset
</tableCaption>
<bodyText confidence="0.998457043478261">
a lower support threshold leads to higher recall.
Since the lower support threshold could generate
more hedge cues, it will generate less training in-
stances for hedge cues with both low confidence
and support, which affects the performance of the
MaxEnt classifier. In both datasets, it appears that
0.5 and 5 are the best thresholds for confidence
and support, respectively.
Beyond the performance of the entire system,
our hedge cue generator yields very promising re-
sults. Using the best parameters we just noted
above, our hedge cue generator generates 52 hedge
cues with confidence 100% on the Wikipedia
dataset, and 332 hedge cues in the biological
dataset. Some hedge cue examples are shown in
Table 5.
We also ran experiments to verify the perfor-
mance of our MaxEnt classifier. We used the same
setting of datasets as for the system performance
evaluation. Given a hedge cue, we extracted all
the matched sentences from the training set to train
a MaxEnt classifier, and used it to classify the
matched sentences by the hedge cue in testing set.
</bodyText>
<page confidence="0.994208">
118
</page>
<table confidence="0.999247666666667">
Hedge Cue Sup. Conf. TestSize Prec. Recall F
indicated that 63 0.984 6 1.0 1.0 1.0
by some 30 0.867 29 0.966 1.000 0.983
are considered 29 0.724 10 0.750 0.857 0.800
some of NNS 62 0.613 27 1.000 0.778 0.875
the most JJ 213 0.432 129 0.873 0.475 0.615
</table>
<tableCaption confidence="0.728672">
Table 6: MaxEnt Classifier Performance
</tableCaption>
<table confidence="0.999915857142857">
Hedge Cue Conf. Sup.
probably VBN 1.0 21
DT probably 1.0 15
many NNS believe 1.0 10
NNS suggested DT 1.0 248
results suggest 1.0 122
has VBN widely VBN 1.0 10
</table>
<tableCaption confidence="0.999669">
Table 5: Generated Hedge Cue Examples
</tableCaption>
<bodyText confidence="0.828172857142857">
Table 6 shows the results, the hedge cues were
manually chosen with relative higher support.
We can see that the performance of the MaxEnt
classifier correlates tightly with confidence and
support. Higher confidence means a more accu-
rate detection for a phrase to be hedge cue, while
higher support means more training instances for
the classifier: the best strategy would be to find
hedge cues with both high confidence and support.
While experimenting with the system, we found
several potential improvements.
• Normalize words. Take the word suggest as
an example. In the generated hedge cues,
we found that its other forms are everywhere,
like it suggested, NNS suggests a, and DT
suggesting that. As we put POS tags into
Lucene index, we can normalize words to
their base forms using a morphology parser,
and put base forms into index. After that, the
query with suggest will match all the forms.
• Use more sophisticated features to train the
MaxEnt classifier. Currently we only use
shallow linguistics information as features,
however we noticed that the role of the phrase
could be very important to decide whether it
indicates uncertainty. We can deep parse sen-
tences, extract the role information, and add
it to the feature list of classifier.
</bodyText>
<sectionHeader confidence="0.999491" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999759">
In this paper, we described the hedge detection
system we developed to participate in the shared
task of CoNLL-2010. Our system uses a heuristic
learner to learn hedge cues, and uses MaxEnt as
its machine learning model to classify uncertainty
for sentences matched by hedge cues. Hedge cues
in our system include both words and POS tags,
which make them more general. Apache Lucene is
integrated into our system to efficiently run com-
plex linguistic queries on the corpus.
</bodyText>
<sectionHeader confidence="0.998204" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9969515">
This work is supported by award IIS-0905593
from the National Science Foundation.
</bodyText>
<sectionHeader confidence="0.998699" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996245434782609">
Viola Ganter and Michael Strube. 2009. Finding
hedges by chasing weasels: Hedge detection using
Wikipedia tags and shallow linguistic features. In
Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, pages 173–176, Suntec, Singapore,
August. Association for Computational Linguistics.
Marc Light, Xin Ying Qiu, and Padmini Srinivasan.
2004. The language of bioscience: Facts, specula-
tions, and statements in between. In Proceedings of
BioLink 2004 Workshop on Linking Biological Lit-
erature, Ontologies and Databases: Tools for Users,
pages 17–24, Boston, Mass, May.
Bing Liu. 2007. Web data mining: exploring hyper-
links, contents, and usage data. Springer.
Michael McCandless, Erik Hatcher, and Otis Gospod-
neti. 2010. Lucene in action. Manning Publications
Co, 2nd edition.
Ben Medlock and Ted Briscoe. 2007. Weakly super-
vised learning for hedge classification in scientific
literature. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 992–999. Association for Computational Lin-
guistics, June.
</reference>
<page confidence="0.99909">
119
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.444608">
<title confidence="0.999933">A Lucene and Maximum Entropy Model Based Hedge Detection System</title>
<author confidence="0.997664">Lin</author>
<affiliation confidence="0.981594">University of Illinois at</affiliation>
<address confidence="0.58959">Chicago, IL,</address>
<email confidence="0.898248">lin@chenlin.net</email>
<author confidence="0.996">Barbara Di</author>
<affiliation confidence="0.9238225">University of Illinois at Chicago, IL,</affiliation>
<email confidence="0.999508">bdieugen@uic.edu</email>
<abstract confidence="0.997178913043479">This paper describes the approach to hedge detection we developed, in order to participate in the shared task at CoNLL- 2010. A supervised learning approach is employed in our implementation. Hedge cue annotations in the training data are used as the seed to build a reliable hedge cue set. Maximum Entropy (MaxEnt) model is used as the learning technique to determine uncertainty. By making use of Apache Lucene, we are able to do fuzzy string match to extract hedge cues, and to incorporate part-of-speech (POS) tags in hedge cues. Not only can our system determine the certainty of the sentence, but is also able to find all the contained hedges. Our system was ranked third on the Wikipedia dataset. In later experiments with different parameters, we further improved our results, with a 0.612 F-score on the Wikipedia dataset, and a 0.802 F-score on the biological dataset.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Viola Ganter</author>
<author>Michael Strube</author>
</authors>
<title>Finding hedges by chasing weasels: Hedge detection using Wikipedia tags and shallow linguistic features.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,</booktitle>
<pages>173--176</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="2196" citStr="Ganter and Strube (2009)" startWordPosition="337" endWordPosition="340">iment, and refine sentiment classes from {positive, negative, objective} to {positive, somehow positive, objective, somehow objective, negative, somehow negative}. 1http://en.wikipedia.org/wiki/ Hedge(linguistics) Hedge detection related work has been conducted by several people. Light et al. (2004) started to do annotations on biomedicine article abstracts, and conducted the preliminary work of automatic classification for uncertainty. Medlock and Briscoe (2007) devised detailed guidelines for hedge annotations, and used a probabilistic weakly supervised learning approach to classify hedges. Ganter and Strube (2009) took Wikipedia articles as training corpus, used weasel words’ frequency and syntactic patterns as features to classify uncertainty. The rest of the paper is organized as follows. Section 2 shows the architecture of our system. Section 3 explains how we make use of Apache Lucene to do fuzzy string match and incorporate POS tag in hedge cues and our method to generate hedge cue candidates. Section 4 describes the details of using MaxEnt model to classify uncertainty. We present and discuss experiments and results in section 5, and conclude in section 6. 2 System Architecture Our system is divi</context>
</contexts>
<marker>Ganter, Strube, 2009</marker>
<rawString>Viola Ganter and Michael Strube. 2009. Finding hedges by chasing weasels: Hedge detection using Wikipedia tags and shallow linguistic features. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 173–176, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Light</author>
<author>Xin Ying Qiu</author>
<author>Padmini Srinivasan</author>
</authors>
<title>The language of bioscience: Facts, speculations, and statements in between.</title>
<date>2004</date>
<booktitle>In Proceedings of BioLink 2004 Workshop on Linking Biological Literature, Ontologies and Databases: Tools for Users,</booktitle>
<pages>17--24</pages>
<location>Boston, Mass,</location>
<contexts>
<context position="1872" citStr="Light et al. (2004)" startWordPosition="294" endWordPosition="297">egree of accuracy and truth assessment in human communication, hedging is widely used in both spoken and written languages. Detecting hedges in natural language text can be very useful for areas like text mining and information extraction. For example, in opinion mining, hedges can be used to assess the degree of sentiment, and refine sentiment classes from {positive, negative, objective} to {positive, somehow positive, objective, somehow objective, negative, somehow negative}. 1http://en.wikipedia.org/wiki/ Hedge(linguistics) Hedge detection related work has been conducted by several people. Light et al. (2004) started to do annotations on biomedicine article abstracts, and conducted the preliminary work of automatic classification for uncertainty. Medlock and Briscoe (2007) devised detailed guidelines for hedge annotations, and used a probabilistic weakly supervised learning approach to classify hedges. Ganter and Strube (2009) took Wikipedia articles as training corpus, used weasel words’ frequency and syntactic patterns as features to classify uncertainty. The rest of the paper is organized as follows. Section 2 shows the architecture of our system. Section 3 explains how we make use of Apache Lu</context>
</contexts>
<marker>Light, Qiu, Srinivasan, 2004</marker>
<rawString>Marc Light, Xin Ying Qiu, and Padmini Srinivasan. 2004. The language of bioscience: Facts, speculations, and statements in between. In Proceedings of BioLink 2004 Workshop on Linking Biological Literature, Ontologies and Databases: Tools for Users, pages 17–24, Boston, Mass, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Web data mining: exploring hyperlinks, contents, and usage data.</title>
<date>2007</date>
<publisher>Springer.</publisher>
<contexts>
<context position="4565" citStr="Liu (2007)" startWordPosition="730" endWordPosition="731">Get Matched Sentences Get the MaxEnt Model Get A Hedge Cue Get A Sentence More Sentence? Hedge Cues Marked Uncertain? Uncertain? More Cue? yes yes yes no Output Marked Uncertainty yes no Lucene Indexer Index with POS Testing Data Testing Figure 1: System Architecture 3 Learn Hedge Cues The training data provided by CoNLL-2010 shared task contain “&lt;ccue&gt;&lt;/ccue&gt;” annotations for uncertain sentences. Most of the annotations are either too strict, which makes them hard to use to match other sentences, or too general, which means that most of the matched sentences are not uncertain. Similar to how Liu (2007) measures the usefulness of association rules, we use support and confidence to measure the usefulness of a hedge cue. Support is the ratio of sentences containing a hedge cue to all sentences. Because in a training dataset, the number of all the sentences is a fixed constant, we only use the number of sentences containing the hedge cue as support, see formula 1. In the other part of this paper, sentences matched by hedge cues means sentences contains hedge cues. We use support to measure the degree of generality of a hedge cue. sup = count of matched sentences (1) Confidence is the ratio of s</context>
</contexts>
<marker>Liu, 2007</marker>
<rawString>Bing Liu. 2007. Web data mining: exploring hyperlinks, contents, and usage data. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael McCandless</author>
<author>Erik Hatcher</author>
<author>Otis Gospodneti</author>
</authors>
<date>2010</date>
<note>Lucene in action. Manning Publications Co, 2nd edition.</note>
<contexts>
<context position="5752" citStr="McCandless et al. (2010)" startWordPosition="940" endWordPosition="943">nces (1) Confidence is the ratio of sentences which contain a hedge cue and are uncertain to all the sentences containing the hedge cue, as formula 2. We use confidence to measure the reliability for a word or phrase to be a hedge cue. 3.1 Usage of Apache Lucene Apache Lucene2 is a full text indexing Java library provided as an open source project of Apache Foundation. It provides flexible indexing and search capability for text documents, and it has very high performance. To explain the integration of Lucene into our implementation, we need to introduce several terms, some of which come from McCandless et al. (2010). • Analyzer: Raw texts are preprocessed before being added to the index: text preprocessing components such as tokenization, stop words removal, and stemming are parts of an analyzer. • Document: A document represents a collection of fields, it could be a web page, a text file, or only a paragraph of an article. • Field: A field represents a document or the meta-data associated with that document, like the author, type, URL. A field has a name and a value, and a bunch of options to control how Lucene will index its value. • Term: The very basic unit of a search. It contains a field name and a</context>
</contexts>
<marker>McCandless, Hatcher, Gospodneti, 2010</marker>
<rawString>Michael McCandless, Erik Hatcher, and Otis Gospodneti. 2010. Lucene in action. Manning Publications Co, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Medlock</author>
<author>Ted Briscoe</author>
</authors>
<title>Weakly supervised learning for hedge classification in scientific literature.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>992--999</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<contexts>
<context position="2039" citStr="Medlock and Briscoe (2007)" startWordPosition="316" endWordPosition="319"> text can be very useful for areas like text mining and information extraction. For example, in opinion mining, hedges can be used to assess the degree of sentiment, and refine sentiment classes from {positive, negative, objective} to {positive, somehow positive, objective, somehow objective, negative, somehow negative}. 1http://en.wikipedia.org/wiki/ Hedge(linguistics) Hedge detection related work has been conducted by several people. Light et al. (2004) started to do annotations on biomedicine article abstracts, and conducted the preliminary work of automatic classification for uncertainty. Medlock and Briscoe (2007) devised detailed guidelines for hedge annotations, and used a probabilistic weakly supervised learning approach to classify hedges. Ganter and Strube (2009) took Wikipedia articles as training corpus, used weasel words’ frequency and syntactic patterns as features to classify uncertainty. The rest of the paper is organized as follows. Section 2 shows the architecture of our system. Section 3 explains how we make use of Apache Lucene to do fuzzy string match and incorporate POS tag in hedge cues and our method to generate hedge cue candidates. Section 4 describes the details of using MaxEnt mo</context>
</contexts>
<marker>Medlock, Briscoe, 2007</marker>
<rawString>Ben Medlock and Ted Briscoe. 2007. Weakly supervised learning for hedge classification in scientific literature. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 992–999. Association for Computational Linguistics, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>