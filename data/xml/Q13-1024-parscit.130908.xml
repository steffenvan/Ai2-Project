<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.996999">
Large-scale Word Alignment Using Soft Dependency Cohesion
Constraints
</title>
<author confidence="0.993729">
Zhiguo Wang and Chengqing Zong
</author>
<affiliation confidence="0.988409">
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences
</affiliation>
<email confidence="0.984754">
{zgwang, cqzong}@nlpr.ia.ac.cn
</email>
<sectionHeader confidence="0.984529" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999930238095238">
Dependency cohesion refers to the
observation that phrases dominated by
disjoint dependency subtrees in the source
language generally do not overlap in the
target language. It has been verified to be a
useful constraint for word alignment.
However, previous work either treats this
as a hard constraint or uses it as a feature in
discriminative models, which is ineffective
for large-scale tasks. In this paper, we take
dependency cohesion as a soft constraint,
and integrate it into a generative model for
large-scale word alignment experiments.
We also propose an approximate EM
algorithm and a Gibbs sampling algorithm
to estimate model parameters in an
unsupervised manner. Experiments on
large-scale Chinese-English translation
tasks demonstrate that our model achieves
improvements in both alignment quality
and translation quality.
</bodyText>
<sectionHeader confidence="0.992478" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99989162264151">
Word alignment is the task of identifying word
correspondences between parallel sentence pairs.
Word alignment has become a vital component of
statistical machine translation (SMT) systems,
since it is required by almost all state-of-the-art
SMT systems for the purpose of extracting phrase
tables or even syntactic transformation rules
(Koehn et al., 2007; Galley et al., 2004).
During the past two decades, generative word
alignment models such as the IBM Models (Brown
et al., 1993) and the HMM model (Vogel et al.,
1996) have been widely used, primarily because
they are trained on bilingual sentences in an
unsupervised manner and the implementation is
freely available in the GIZA++ toolkit (Och and
Ney, 2003). However, the word alignment quality
of generative models is still far from satisfactory
for SMT systems. In recent years, discriminative
alignment models incorporating linguistically
motivated features have become increasingly
popular (Moore, 2005; Taskar et al., 2005; Riesa
and Marcu, 2010; Saers et al., 2010; Riesa et al.,
2011). These models are usually trained with
manually annotated parallel data. However, when
moving to a new language pair, large amount of
hand-aligned data are usually unavailable and
expensive to create.
A more practical way to improve large-scale
word alignment quality is to introduce syntactic
knowledge into a generative model and train the
model in an unsupervised manner (Wu, 1997;
Yamada and Knight, 2001; Lopez and Resnik,
2005; DeNero and Klein, 2007; Pauls et al., 2010).
In this paper, we take dependency cohesion (Fox,
2002) into account, which assumes phrases
dominated by disjoint dependency subtrees tend
not to overlap after translation. Instead of treating
dependency cohesion as a hard constraint (Lin and
Cherry, 2003) or using it as a feature in
discriminative models (Cherry and Lin, 2006b), we
treat dependency cohesion as a distortion
constraint, and integrate it into a modified HMM
word alignment model to softly influence the
probabilities of alignment candidates. We also
propose an approximate EM algorithm and an
explicit Gibbs sampling algorithm to train the
model in an unsupervised manner. Experiments on
a large-scale Chinese-English translation task
demonstrate that our model achieves
improvements in both word alignment quality and
machine translation quality.
The remainder of this paper is organized as
follows: Section 2 introduces dependency cohesion
</bodyText>
<note confidence="0.497272333333333">
291
Transactions of the Association for Computational Linguistics, 1 (2013) 291–300. Action Editor: Chris Callison-Burch.
Submitted 5/2013; Published 7/2013. c�2013 Association for Computational Linguistics.
</note>
<bodyText confidence="0.9997775">
constraint for word alignment. Section 3 presents
our generative model for word alignment using
dependency cohesion constraint. Section 4
describes algorithms for parameter estimation. We
discuss and analyze the experiments in Section 5.
Section 6 gives the related work. Finally, we
conclude this paper and mention future work in
Section 7.
</bodyText>
<sectionHeader confidence="0.7459435" genericHeader="introduction">
2 Dependency Cohesion Constraint for
Word Alignment
</sectionHeader>
<bodyText confidence="0.988281024390244">
Given a source (foreign) sentence f 1 = fi, f2, ... , fj
and a target (English) sentence ell = el, e2, ..., el,
the alignment A between fi and el is defined as a
subset of the Cartesian product of word positions:
A E {(j, i): j = 1,..., J; i = 1, ..., I)
When given the source side dependency tree T, we
can project dependency subtrees in T onto the
target sentence through the alignment A .
Dependency cohesion assumes projection spans of
disjoint subtrees tend not to overlap. Let T(fi) be
the subtree of T rooted at fi, we define two kinds
of projection span for the node fi: subtree span and
head span. The subtree span is the projection span
of the total subtree T(fi), while the head span is
the projection span of the node fi itself. Following
Fox (2002) and Lin and Cherry (2003), we
consider two types of dependency cohesion: head-
modifier cohesion and modifier-modifier cohesion.
Head-modifier cohesion is defined as the subtree
span of a node does not overlap with the head span
of its head (parent) node, while modifier-modifier
cohesion is defined as subtree spans of two nodes
under the same head node do not overlap each
other. We call a situation where cohesion is not
maintained crossing.
Using the dependency tree in Figure 1 as an
example, given the correct alignment “R”, the
subtree span of “有/have” is [8, 14] , and the head
span of its head node “之一/one of” is [3, 4]. They
do not overlap each other, so the head-modifier
cohesion is maintained. Similarly, the subtree span
of “少数/few” is [6, 6], and it does not overlap the
subtree span of “有/have”, so a modifier-modifier
cohesion is maintained. However, when “R” is
replaced with the incorrect alignment “W”, the
subtree span of “有/have” becomes [3, 14], and it
overlaps the head span of its head “之一/one of”,
so a head-modifier crossing occurs. Meanwhile,
the subtree spans of the two nodes “有/have” and
“ 少 数 /few” overlap each other, so a modifier-
modifier crossing occurs.
</bodyText>
<equation confidence="0.999051714285714">
是 与 北
韩
有 邦 的
数
国
家
之 。
</equation>
<figureCaption confidence="0.989766">
Figure 1: A Chinese-English sentence pair
</figureCaption>
<bodyText confidence="0.99933384">
including the word alignments and the Chinese
side dependency tree. The Chinese and English
words are listed horizontally and vertically,
respectively. The black grids are gold-standard
alignments. For the Chinese word “有/have”,
we give two alignment positions, where “R” is
the correct alignment and “W” is the incorrect
alignment.
Fox (2002) showed that dependency cohesion is
generally maintained between English and French.
To test how well this assumption holds between
Chinese and English, we measure the dependency
cohesion between the two languages with a
manually annotated bilingual Chinese-English data
set of 502 sentence pairs 1 . We use the head-
modifier cohesion percentage (HCP) and the
modifier-modifier cohesion percentage (MCP) to
measure the degree of cohesion in the corpus. HCP
(or MCP) is used for measuring how many head-
modifier (or modifier-modifier) pairs are actually
cohesive. Table 1 lists the relative percentages in
both Chinese-to-English (ch-en, using Chinese side
dependency trees) and English-to-Chinese (en-ch,
using English side dependency trees) directions.
As we see from Table 1, dependency cohesion is
</bodyText>
<figure confidence="0.992685">
1 The data set is the development set used in Section 5.
1 Australia
is
one
of
the
few
countries
that
have
diplomatic
relations
with
North
Korea
.
2
3
4
5
6
7
8
9
10
11
12
13
14
15
澳
洲
少
</figure>
<page confidence="0.603874">
一
交
292
</page>
<bodyText confidence="0.999919461538462">
generally maintained between Chinese and English.
So dependency cohesion would be helpful for
word alignment between Chinese and English.
However, there are still a number of crossings. If
we restrict alignment space with a hard cohesion
constraint, the correct alignments that result in
crossings will be ruled out directly. In the next
section, we describe an approach to integrating
dependency cohesion constraint into a generative
model to softly influence the probabilities of
alignment candidates. We show that our new
approach addresses the shortcomings of using
dependency cohesion as a hard constraint.
</bodyText>
<table confidence="0.992193">
ch-en en-ch
HCP MCP HCP MCP
88.43 95.82 81.53 91.62
</table>
<tableCaption confidence="0.9976395">
Table 1: Cohesion percentages (%) of a manually
annotated data set between Chinese and English.
</tableCaption>
<sectionHeader confidence="0.953473" genericHeader="method">
3 A Generative Word Alignment Model
with Dependency Cohesion Constraint
</sectionHeader>
<bodyText confidence="0.9999144375">
The most influential generative word alignment
models are the IBM Models 1-5 and the HMM
model (Brown et al., 1993; Vogel et al., 1996; Och
and Ney, 2003). These models can be classified
into sequence-based models (IBM Models 1, 2 and
HMM) and fertility-based models (IBM Models 3,
4 and 5). The sequence-based model is easier to
implement, and recent experiments have shown
that appropriately modified sequence-based model
can produce comparable performance with
fertility-based models (Lopez and Resnik, 2005;
Liang et al., 2006; DeNero and Klein, 2007; Zhao
and Gildea, 2010; Bansal et al., 2011). So we built
a generative word alignment model with
dependency cohesion constraint based on the
sequence-based model.
</bodyText>
<subsectionHeader confidence="0.998979">
3.1 The Sequence-based Alignment Model
</subsectionHeader>
<bodyText confidence="0.999609923076923">
According to Brown et al. (1993) and Och and Ney
(2003), the sequence-based model is built as a
noisy channel model, where the source sentence 𝒇1𝐽
and the alignment 𝒂1𝐽 are generated conditioning on
the target sentence 𝒆1𝐼 . The model assumes each
source word is assigned to exactly one target word,
and defines an asymmetric alignment for the
sentence pair as 𝒂1 𝐽 = 𝑎1, 𝑎2, ... , 𝑎𝑗, ... , 𝑎𝐽, where each
𝑎𝑗 ∈ [0, 𝐼] is an alignment from the source position j
to the target position 𝑎𝑗 , 𝑎𝑗 = 0 means 𝑓𝑗 is not
aligned with any target words. The sequence-based
model divides alignment procedure into two stages
(distortion and translation) and factors as:
</bodyText>
<equation confidence="0.993058333333333">
𝐽
𝑝(𝒇1 𝐽, 𝒂1 𝐽|𝒆1 𝐼 ) = ∏ 𝑝𝑑(𝑎𝑗|𝑎𝑗−1, 𝐼)𝑝𝑡(𝑓𝑗|𝑒𝑎𝑗) (1)
𝑗=1
</equation>
<bodyText confidence="0.9999875">
where 𝑝𝑑 is the distortion model and 𝑝𝑡 is the
translation model. IBM Models 1, 2 and the HMM
model all assume the same translation model
𝑝𝑡 (𝑓𝑗|𝑒𝑎𝑗) . However, they use three different
distortion models. IBM Model 1 assumes a
uniform distortion probability 1/(I+1), IBM Model
2 assumes 𝑝𝑑 (𝑎𝑗 |𝑗) that depends on word position j
and HMM model assumes 𝑝𝑑(𝑎𝑗|𝑎𝑗−1,𝐼) that
depends on the previous alignment 𝑎𝑗−1. Recently,
tree distance models (Lopez and Resnik, 2005;
DeNero and Klein, 2007) formulate the distortion
model as 𝑝𝑑(𝑎𝑗|𝑎𝑗−1,𝑇) , where the distance
between 𝑎𝑗 and 𝑎𝑗−1 are calculated by walking
through the phrase (or dependency) tree T.
</bodyText>
<subsectionHeader confidence="0.992056">
3.2 Proposed Model
</subsectionHeader>
<bodyText confidence="0.996700041666667">
To integrate dependency cohesion constraint into a
generative model, we refine the sequence-based
model in two ways with the help of the source side
dependency tree 𝑇𝑓.
First, we design a new word alignment order. In
the sequence-based model, source words are
aligned from left to right by taking source sentence
as a linear sequence. However, to apply
dependency cohesion constraint, the subtree span
of a head node is computed based on the
alignments of its children, so children must be
aligned before the head node. Riesa and Marcu
(2010) propose a hierarchical search procedure to
traverse all nodes in a phrase structure tree.
Similarly, we define a bottom-up topological order
(BUT-order) to traverse all words in the source
side dependency tree 𝑇𝑓 . In the BUT-order, tree
nodes are aligned bottom-up with 𝑇𝑓 as a backbone.
For all children under the same head node, left
children are aligned from right to left, and then
right children are aligned from left to right. For
example, the BUT-order for the following
dependency tree is “C B E F D A H G”.
A B C D E F G H
</bodyText>
<page confidence="0.63127">
293
</page>
<bodyText confidence="0.996727217391304">
For the sake of clarity, we define a function to
map all nodes in 𝑇𝑓 into their BUT-order, and
notate it as BUT(𝑇𝑓) = 𝜋1, 𝜋2, ..., 𝜋𝑗, ..., 𝜋𝐽, where 𝜋𝑗
means the j-th node in BUT-order is the 𝜋𝑗-th word
in the original source sentence. We arrange
alignment sequence 𝒂1𝐽 according the BUT-order
and notate it as 𝒂[1,𝐽] = 𝑎𝜋 1, ... , 𝑎𝜋𝑗, ... , 𝑎𝜋 𝐽 , where
𝑎𝜋𝑗 is the aligned position for a node 𝑓𝜋𝑗. We also
notate the sub-sequence 𝑎𝜋 𝑖, . .., 𝑎𝜋𝑗as 𝒂[𝑖 ,𝑗].
Second, we keep the same translation model as
the sequence-based model and integrate the
dependency cohesion constraints into the distortion
model. The main idea is to influence the distortion
procedure with the dependency cohesion
constraints. Assume node 𝑓ℎ and node 𝑓𝑚 are a
head-modifier pair in 𝑇𝑓, where 𝑓ℎ is the head and
𝑓𝑚 is the modifier. The head-modifier cohesion
relationship between them is notated as 𝒽ℎ,𝑚 ∈
{𝑐𝑜ℎ𝑒𝑠𝑖𝑜𝑛, 𝑐𝑟𝑜𝑠𝑠𝑖𝑛𝑔} . When the head-modifier
cohesion is maintained 𝒽ℎ,𝑚 = 𝑐𝑜ℎ𝑒𝑠𝑖𝑜𝑛, otherwise
𝒽ℎ,𝑚 = 𝑐𝑟𝑜𝑠𝑠𝑖𝑛𝑔. We represent the set of head-
modifier cohesion relationships for all the head-
modifier pairs in 𝑇𝑓 as:
</bodyText>
<equation confidence="0.894847">
𝑯 = {𝒽ℎ,𝑚  |ℎ ∈ [1,𝐽], 𝑚 ∈ [1,𝐽], ℎ ≠ 𝑚,
𝑓ℎ and 𝑓𝑚 are a head-modifier pair in 𝑇𝑓 }
</equation>
<bodyText confidence="0.999702">
The set of head-modifier cohesion relationships for
all the head-modifier pairs taking 𝑓ℎ as the head
node can be represented as:
</bodyText>
<equation confidence="0.99929">
𝓱ℎ = {𝒽ℎ,𝑚  |𝑚 ∈ [1,𝐽], 𝑚 ≠ ℎ,
𝑓ℎ and 𝑓𝑚 are a head-modifier pair in 𝑇𝑓 }
Obviously, 𝑯 = ⋃ 𝓱ℎ
𝐽 ℎ=0 .
</equation>
<bodyText confidence="0.999144818181818">
Similarly, we assume node 𝑓𝑘 and node 𝑓𝑙 are a
modifier-modifier pair in 𝑇𝑓. To avoid repetition,
we assume 𝑓𝑘 is the node sitting at the position
after 𝑓𝑙 in BUT-order and call 𝑓𝑘 as the higher-
order node of the pair. The modifier-modifier
cohesion relationship between them is notated as
𝓂𝑘,𝑙 ∈ {𝑐𝑜ℎ𝑒𝑠𝑖𝑜𝑛, 𝑐𝑟𝑜𝑠𝑠𝑖𝑛𝑔} . When the modifier-
modifier cohesion is maintained 𝓂𝑘,𝑙 = 𝑐𝑜ℎ𝑒𝑠𝑖𝑜𝑛,
otherwise 𝓂𝑘,𝑙 = 𝑐𝑟𝑜𝑠𝑠𝑖𝑛𝑔. We represent the set of
modifier-modifier cohesion relationships for all the
modifier-modifier pairs in 𝑇𝑓 as:
</bodyText>
<equation confidence="0.992306">
𝑴 = {𝓂𝑘,𝑙  |𝑘 ∈ [1,𝐽], 𝑙 ∈ [1,𝐽], 𝑘 ≠ 𝑙,
</equation>
<bodyText confidence="0.9810992">
𝑓𝑘 and 𝑓𝑙 are a modifier-modifier pair in 𝑇𝑓 }
The set of modifier-modifier cohesion
relationships for all the modifier-modifier pairs
taking 𝑓𝑘 as the higher-order node can be
represented as:
</bodyText>
<equation confidence="0.9992905">
𝓶𝑘 = {𝓂𝑘,𝑙  |𝑙 ∈ [1,𝐽], 𝑙 ≠ 𝑘,
𝑓𝑘 and 𝑓𝑙 are a modifier-modifier pair in 𝑇𝑓 }
Obviously, 𝑴 = ⋃ 𝓶𝑘
𝐽 𝑘=0 .
</equation>
<bodyText confidence="0.9987348">
With the above notations, we formulate the
distortion probability for a node 𝑓𝜋𝑗 as
𝑝𝑑 (𝑎𝜋𝑗, 𝓱𝜋𝑗,𝓶𝜋𝑗|𝒂[1,𝑗−1]).
According to Eq. (1) and the two improvements,
we formulated our model as:
</bodyText>
<equation confidence="0.9857315">
𝑝(𝒇1𝐽, 𝒂[1,𝐽]|𝒆1𝐼, 𝑇𝑓) = 𝑝(𝒂[1,𝐽], 𝑯, 𝑴, 𝒇1𝐽, |𝒆1𝐼, 𝑇𝑓 )
≈∏𝜋𝑗∈𝐵𝑈𝑇(𝑇𝑓)𝑝𝑑(𝑎𝜋𝑗,𝓱𝜋𝑗, 𝓶𝜋𝑗|𝒂[1,𝑗−1]) 𝑝𝑡(𝑓𝜋𝑗|𝑒𝑎𝜋𝑗)
</equation>
<bodyText confidence="0.99996625">
Here, we use the approximation symbol,
because the right hand side is not guaranteed to
be normalized. In practice, we only compute
ratios of these terms, so it is not actually a
problem. Such model is called deficient (Brown
et al., 1993), and many successful unsupervised
models are deficient, e.g., IBM model 3 and
IBM model 4.
</bodyText>
<subsectionHeader confidence="0.994688">
3.3 Dependency Cohesive Distortion Model
</subsectionHeader>
<bodyText confidence="0.9999656">
We assume the distortion procedure is influenced
by three factors: words distance, head-modifier
cohesion and modifier-modifier cohesion.
Therefore, we further decompose the distortion
model 𝑝𝑑 into three terms as follows:
</bodyText>
<equation confidence="0.984516333333333">
𝑝𝑑 (𝑎𝜋𝑗, 𝓱𝜋𝑗, 𝓶𝜋𝑗|𝒂[1,𝑗−1])
= 𝑝 (𝑎𝜋𝑗|𝒂[1,𝑗−1]) 𝑝 (𝓱𝜋𝑗 |𝒂[1,𝑗]) 𝑝 (𝓶𝜋𝑗 |𝒂[1,𝑗], 𝓱𝜋𝑗)
≈𝑝𝑤𝑑 (𝑎𝜋𝑗 |𝑎𝜋𝑗−1, 𝐼) 𝑝ℎ𝑐 (𝓱𝜋𝑗 |𝒂[1,𝑗]) 𝑝𝑚𝑐 (𝓶𝜋𝑗 |𝒂[1,𝑗])
</equation>
<bodyText confidence="0.999990454545455">
where 𝑝 𝑤𝑑 is the words distance term, 𝑝ℎ𝑐 is the
head-modifier cohesion term and 𝑝𝑚𝑐 is the
modifier-modifier cohesion term.
The word distance term 𝑝 𝑤𝑑 has been verified to
be very useful in the HMM alignment model.
However, in our model, the word distance is
calculated based on the previous node in BUT-
order rather than the previous word in the original
sentence. We follow the HMM word alignment
model (Vogel et al., 1996) and parameterize 𝑝 𝑤𝑑 in
terms of the jump width:
</bodyText>
<equation confidence="0.997818">
𝑝𝑤𝑑(𝑖|𝑖′,𝐼) = 𝑐(𝑖−𝑖′) ∑ 𝑐(𝑖′′−𝑖′)
𝑖′′ (4)
</equation>
<bodyText confidence="0.768786">
where 𝑐() is the count of jump width.
</bodyText>
<page confidence="0.53443">
294
</page>
<bodyText confidence="0.999944166666667">
The head-modifier cohesion term 𝑝ℎ𝑐 is used to
penalize the distortion probability according to
relationships between the head node and its
children (modifiers). Therefore, we define 𝑝ℎ𝑐 as
the product of probabilities for all head-modifier
pairs taking 𝑓𝜋𝑗 as head node:
</bodyText>
<equation confidence="0.914682">
𝑝ℎ𝑐 (𝓱𝜋𝑗|𝒂[1,𝑗]) = ∏ 𝒽𝜋𝑗,𝑐∈𝓱𝜋𝑗 𝑝ℎ (𝒽𝜋𝑗,𝑐|𝑓𝑐, 𝑒𝑎𝜋𝑗, 𝑒𝑎𝑐) (5)
</equation>
<bodyText confidence="0.996757">
where 𝒽𝜋𝑗,𝑐 ∈ {𝑐𝑜ℎ𝑒𝑠𝑖𝑜𝑛, 𝑐𝑟𝑜𝑠𝑠𝑖𝑛𝑔} is the head-
modifier cohesion relationship between 𝑓𝜋 𝑗 and
one of its child 𝑓𝑐 , 𝑝ℎ is the corresponding
probability, 𝑒𝑎𝜋 𝑗 and 𝑒𝑎𝑐 are the aligned words for
𝑓𝜋 𝑗 and 𝑓𝑐.
Similarly, the modifier-modifier cohesion term
𝑝𝑚𝑐 is used to penalize the distortion probability
according to relationships between 𝑓𝜋𝑗 and its
siblings. Therefore, we define 𝑝𝑚𝑐 as the product
of probabilities for all the modifier-modifier pairs
taking 𝑓𝜋𝑗 as the higher-order node:
</bodyText>
<equation confidence="0.9311865">
𝑝𝑚𝑐 (𝓶𝜋𝑗 |𝒂[1,𝑗]) = ∏𝓂𝜋𝑗,𝑠∈𝓶𝜋𝑗 𝑝𝑚 (𝓂𝜋𝑗,𝑠 |𝑓𝑠, 𝑒𝑎𝜋𝑗, 𝑒𝑎𝑠)
(6)
</equation>
<bodyText confidence="0.999372636363636">
where 𝓂𝜋𝑗,𝑠 ∈ {𝑐𝑜ℎ𝑒𝑠𝑖𝑜𝑛, 𝑐𝑟𝑜𝑠𝑠𝑖𝑛𝑔} is the modifier-
modifier cohesion relationship between 𝑓𝜋 𝑗 and
one of its sibling 𝑓𝑠 , 𝑝𝑚 is the corresponding
probability, 𝑒𝑎𝜋 𝑗 and 𝑒𝑎𝑠 are the aligned words for
𝑓𝜋𝑗 and 𝑓𝑠.
Both 𝑝ℎ and 𝑝𝑚 in Eq. (5) and Eq. (6) are
conditioned on three words, which would make
them very sparse. To cope with this problem, we
use the word clustering toolkit, mkcls (Och et al.,
1999), to cluster all words into 50 classes, and
replace the three words with their classes.
</bodyText>
<sectionHeader confidence="0.982801" genericHeader="method">
4 Parameter Estimation
</sectionHeader>
<bodyText confidence="0.9999939">
To align sentence pairs with the model in Eq. (2),
we have to estimate some parameters: 𝑝𝑡, 𝑝𝑤𝑑, 𝑝ℎ
and 𝑝𝑚. The traditional approach for sequence-
based models uses Expectation Maximization (EM)
algorithm to estimate parameters. However, in our
model, it is hard to find an efficient way to sum
over all the possible alignments, which is required
in the E-step of EM algorithm. Therefore, we
propose an approximate EM algorithm and a Gibbs
sampling algorithm for parameter estimation.
</bodyText>
<subsectionHeader confidence="0.993905">
4.1 Approximate EM Algorithm
</subsectionHeader>
<bodyText confidence="0.999987772727273">
The approximate EM algorithm is similar to the
training algorithm for fertility-based alignment
models (Och and Ney, 2003). The main idea is to
enumerate only a small subset of good alignments
in the E-step, then collect expectation counts and
estimate parameters among the small subset in M-
step. Following with Och and Ney (2003), we
employ neighbor alignments of the Viterbi
alignment as the small subset. Neighbor
alignments are obtained by performing one swap
or move operation over the Viterbi alignment.
Obtaining the Viterbi alignment itself is not so
easy for our model. Therefore, we take the Viterbi
alignment of the sequence-based model (HMM
model) as the starting point, and iterate the hill-
climbing algorithm (Brown et al., 1993) many
times to get the best alignment greedily. In each
iteration, we find the best alignment with Eq. (2)
among neighbor alignments of the initial point, and
then make the best alignment as the initial point for
the next iteration. The algorithm iterates until no
update could be made.
</bodyText>
<subsectionHeader confidence="0.997189">
4.2 Gibbs Sampling Algorithm
</subsectionHeader>
<bodyText confidence="0.99991944">
Gibbs sampling is another effective algorithm for
unsupervised learning problems. As is described in
the literatures (Johnson et al., 2007; Gao and
Johnson, 2008), there are two types of Gibbs
samplers: explicit and collapsed. An explicit
sampler represents and samples the model
parameters in addition to the word alignments,
while in a collapsed sampler the parameters are
integrated out and only alignments are sampled.
Mermer and Saraçlar (2011) proposed a collapsed
sampler for IBM Model 1. However, their sampler
updates parameters constantly and thus cannot run
efficiently on large-scale tasks. Instead, we take
advantage of explicit Gibbs sampling to make a
highly parallelizable sampler. Our Gibbs sampler
is similar to the MCMC algorithm in Zhao and
Gildea (2010), but we assume Dirichlet priors
when sampling model parameters and take a
different sampling approach based on the source
side dependency tree.
Our sampler performs a sequence of consecutive
iterations. Each iteration consists of two sampling
steps. The first step samples the aligned position
for each dependency node according to the BUT-
order. Concretely, when sampling the aligned
</bodyText>
<page confidence="0.501996">
295
</page>
<bodyText confidence="0.936509111111111">
position 𝑎𝜋𝑗
(𝑡+1) for node 𝑓𝜋𝑗 on iteration 𝑡+1, the
aligned positions for 𝒂[1,𝑗−1] are fixed on the new
sampling results 𝒂[1,𝑗−1]
(𝑡+1) on iteration 𝑡+1, and the
aligned positions for 𝒂[𝑗+1,𝐽] are fixed on the old
sampling results 𝒂[𝑗+1,𝐽]
(𝑡) on iteration 𝑡. Therefore,
we sample the aligned position 𝑎𝜋𝑗
</bodyText>
<equation confidence="0.988799222222222">
(𝑡+1) as follows:
𝑎𝜋𝑗
(𝑡+1) ~ 𝑝 (𝑎𝜋𝑗|𝒂[1,𝑗−1]
(𝑡+1) , 𝒂[𝑗+1,𝐽]
(𝑡) , 𝑓1 𝐽, 𝑒1 𝐼)
𝑝(𝒇1𝐽, 𝒂̂𝑎𝜋𝑗|𝒆1𝐼)
∑ 𝑝 (𝒇1 𝐽,𝒂̂𝑎𝜋𝑗|𝒆1 𝐼 )
𝑎𝜋𝑗∈{0,1,...,𝐼}
(7)
</equation>
<bodyText confidence="0.711057">
where 𝒂̂𝑎𝜋𝑗 = 𝒂[1,𝑗−1]
</bodyText>
<equation confidence="0.85698">
(𝑡+1) ∪ 𝑎𝜋𝑗 ∪ 𝒂[𝑗+1,𝐽]
</equation>
<bodyText confidence="0.878151666666667">
(𝑡) , the numerator
is the probability of aligning 𝑓𝜋𝑗 with 𝑒𝑎𝜋𝑗 (the
alignments for other nodes are fixed at 𝒂[1,𝑗−1]
</bodyText>
<equation confidence="0.833191">
(𝑡+1) and
𝒂[𝑗+1,𝐽]
</equation>
<bodyText confidence="0.999932352941177">
(𝑡) ) calculated with Eq. (2), and the
denominator is the summation of the probabilities
of aligning 𝑓𝜋𝑗 with each target word. The second
step of our sampler calculates parameters 𝑝𝑡, 𝑝𝑤𝑑,
𝑝ℎ and 𝑝𝑚 using their counts, where all these
counts can be easily collected during the first
sampling step. Because all these parameters follow
multinomial distributions, we consider Dirichlet
priors for them, which would greatly simplify the
inference procedure.
In the first sampling step, all the sentence pairs
are processed independently. So we can make this
step parallel and process all the sentence pairs
efficiently with multi-threads. When using the
Gibbs sampler for decoding, we just ignore the
second sampling step and iterate the first sampling
step many times.
</bodyText>
<sectionHeader confidence="0.998633" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.993873333333333">
We performed a series of experiments to evaluate
our model. All the experiments are conducted on
the Chinese-English language pair. We employ
two training sets: FBIS and LARGE. The size and
source corpus of these training sets are listed in
Table 2. We will use the smaller training set FBIS
to evaluate the characters of our model and use the
LARGE training set to evaluate whether our model
is adaptable for large-scale task. For word
alignment quality evaluation, we take the hand-
aligned data sets from SSMT20072, which contains
2 http://nlp.ict.ac.cn/guidelines/guidelines-2007-
SSMT(English).doc
505 sentence pairs in the testing set and 502
sentence pairs in the development set. Following
Och and Ney (2003), we evaluate word alignment
quality with the alignment error rate (AER), where
lower AER is better.
Because our model takes dependency trees as
input, we parse both sides of the two training sets,
the development set and the testing set with
Berkeley parser (Petrov et al., 2006), and then
convert the generated phrase trees into dependency
trees according to Wang and Zong (2010; 2011).
Our model is an asymmetric model, so we perform
word alignment in both forward (Chinese4English)
and reverse (English4Chinese) directions.
</bodyText>
<table confidence="0.992101142857143">
Train Set Source Corpus # Words
FBIS FBIS newswire data Ch: 7.1M
En: 9.1M
LARGE LDC2000T50, LDC2003E14, Ch: 27.6M
LDC2003E07, LDC2004T07, En: 31.8M
LDC2005T06, LDC2002L27,
LDC2005T10, LDC2005T34
</table>
<tableCaption confidence="0.992103">
Table 2: The size and the source corpus of the two
training sets.
</tableCaption>
<subsectionHeader confidence="0.998031">
5.1 Effectiveness of Cohesion Constraints
</subsectionHeader>
<bodyText confidence="0.9968106">
In Eq. (3), the distortion probability 𝑝𝑑 is
decomposed into three terms: 𝑝𝑤𝑑, 𝑝ℎ𝑐 and 𝑝𝑚𝑐 .
To study whether cohesion constraints are effective
for word alignment, we construct four sub-models
as follows:
</bodyText>
<listItem confidence="0.93700825">
(1) wd: 𝑝𝑑 = 𝑝𝑤𝑑;
(2) wd-hc: 𝑝𝑑 = 𝑝𝑤𝑑 ∙ 𝑝ℎ𝑐;
(3) wd-mc: 𝑝𝑑 = 𝑝𝑤𝑑 ∙ 𝑝𝑚𝑐;
(4) wd-hc-mc: 𝑝𝑑 = 𝑝𝑤𝑑 ∙ 𝑝ℎ𝑐 ∙ 𝑝𝑚𝑐.
</listItem>
<bodyText confidence="0.993445111111111">
We train these four models with the approximate
EM and the Gibbs sampling algorithms on the
FBIS training set. For approximate EM algorithm,
we first train a HMM model (with 5 iterations of
IBM model 1 and 5 iterations of HMM model),
then train these four sub-models with 10 iterations
of the approximate EM algorithm. For Gibbs
sampling, we choose symmetric Dirichlet priors
identically with all hyper-parameters equals 0.0001
to obtain a sparse Dirichlet prior. Then, we make
the alignments produced by the HMM model as the
initial points, and train these sub-models with 20
iterations of the Gibbs sampling.
AERs on the development set are listed in Table
3. We can easily find: 1) when employing the
head-modifier cohesion constraint, the wd-hc
model yields better AERs than the wd model; 2)
=
</bodyText>
<page confidence="0.754792">
296
</page>
<bodyText confidence="0.999395230769231">
when employing the modifier-modifier cohesion
constraint, the wd-mc model also yields better
AERs than the wd model; and 3) when employing
both head-modifier cohesion constraint and
modifier-modifier cohesion constraint together, the
wd-hc-mc model yields the best AERs among the
four sub-models. So both head-modifier cohesion
constraint and modifier-modifier cohesion
constraint are helpful for word alignment. Table 3
also shows that the approximate EM algorithm
yields better AERs in the forward direction than
reverse direction, while the Gibbs sampling
algorithm yields close AERs in both directions.
</bodyText>
<table confidence="0.998974833333333">
EM Gibbs
forward reverse forward reverse
wd 26.12 28.66 27.09 26.40
wd-hc 24.67 25.86 26.24 24.39
wd-mc 24.49 26.53 25.51 25.40
wd-hc-mc 23.63 25.17 24.65 24.33
</table>
<tableCaption confidence="0.9587345">
Table 3: AERs on the development set (trained
on the FBIS data set).
</tableCaption>
<subsectionHeader confidence="0.9994">
5.2 Comparison with State-of-the-Art Models
</subsectionHeader>
<bodyText confidence="0.999679">
To show the effectiveness of our model, we
compare our model with some of the state-of-the-
art models. All the systems are listed as follows:
</bodyText>
<listItem confidence="0.977607826086956">
1) IBM4: The fertility-based model (IBM model 4)
which is implemented in GIZA++ toolkit. The
training scheme is 5 iterations of IBM model 1,
5 iterations of the HMM model and 10
iterations of IBM model 4.
2) IBM4-L0: A modification to the GIZA++
toolkit which extends IBM models with ℓ0 -
norm (Vaswani et al., 2012). The training
scheme is the same as IBM4.
3) IBM4-Prior: A modification to the GIZA++
toolkit which extends the translation model of
IBM models with Dirichlet priors (Riley and
Gildea, 2012). The training scheme is the same
as IBM4.
4) Agree-HMM: The HMM alignment model by
jointly training the forward and reverse models
(Liang et al., 2006), which is implemented in
the BerkeleyAligner. The training scheme is 5
iterations of jointly training IBM model 1 and 5
iterations of jointly training HMM model.
5) Tree-Distance: The tree distance alignment
model proposed in DeNero and Klein (2007),
which is implemented in the BerkeleyAligner.
</listItem>
<bodyText confidence="0.999109666666667">
The training scheme is 5 iterations of jointly
training IBM model 1 and 5 iterations of jointly
training the tree distance model.
</bodyText>
<listItem confidence="0.925014466666667">
6) Hard-Cohesion: The implemented “Cohesion
Checking Algorithm” (Lin and Cherry, 2003)
which takes dependency cohesion as a hard
constraint during beam search word alignment
decoding. We use the model trained by the
Agree-HMM system to estimate alignment
candidates.
We also build two systems for our soft
dependency cohesion model:
7) Soft-Cohesion-EM: the wd-hc-mc sub-model
trained with the approximate EM algorithm as
described in sub-section 5.1.
8) Soft-Cohesion-Gibbs: the wd-hc-mc sub-model
trained with the Gibbs sampling algorithm as
described in sub-section 5.1.
</listItem>
<bodyText confidence="0.999571461538462">
We train all these systems on the FBIS training
set, and test them on the testing set. We also
combine the forward and reverse alignments with
the grow-diag-final-and (GDFA) heuristic (Koehn
et al., 2007). All AERs are listed in Table 4. We
find our soft cohesion systems produce better
AERs than the Hard-Cohesion system as well as
the other systems. Table 5 gives the head-modifier
cohesion percentage (HCP) and the modifier-
modifier cohesion percentage (MCP) of each
system. We find HCPs and MCPs of our soft
cohesion systems are much closer to the gold-
standard alignments.
</bodyText>
<table confidence="0.999270444444444">
forward reverse GDFA
IBM4 42.90 42.81 44.32
IBM4-L0 42.59 41.04 43.19
IBM4-Prior 41.94 40.46 42.44
Agree-HMM 38.03 37.91 41.01
Tree-Distance 34.21 37.22 38.42
Hard-Cohesion 37.32 38.92 38.92
Soft-Cohesion-EM 33.65 34.74 35.85
Soft-Cohesion-Gibbs 34.45 33.72 34.46
</table>
<tableCaption confidence="0.9890725">
Table 4: AERs on the testing set (trained on the
FBIS data set).
</tableCaption>
<bodyText confidence="0.9961675">
To evaluate whether our model is adaptable for
large-scale task, we retrained these systems using
the LARGE training set. AERs on the testing set
are listed in Table3 6. Compared with Table 4, we
</bodyText>
<footnote confidence="0.452550666666667">
3 Tree-Distance system requires too much memory to run on
our server when using the LARGE data set, so we can’t get the
result.
</footnote>
<page confidence="0.716353">
297
</page>
<bodyText confidence="0.9987402">
find all the systems yield better performance when
using more training data. Our soft cohesion
systems still produce better AERs than other
systems, suggesting that our soft cohesion model is
very effective for large-scale word alignment tasks.
</bodyText>
<table confidence="0.999216090909091">
forward reverse
HCP MCP HCP MCP
IBM4 60.53 63.94 56.15 64.80
IBM4-L0 60.57 62.53 66.49 65.68
IBM4-Prior 66.48 74.65 67.19 72.32
Agree-HMM 75.52 66.61 73.88 66.07
Tree-Distance 81.37 74.69 78.00 71.73
Hard-Cohesion 98.70 97.43 98.25 97.84
Soft-Cohesion-EM 85.21 81.96 82.96 81.36
Soft-Cohesion-Gibbs 88.74 85.55 87.81 84.83
gold-standard 88.43 95.82 81.53 91.62
</table>
<tableCaption confidence="0.9505355">
Table 5: HCPs and MCPs on the development
set.
</tableCaption>
<table confidence="0.99988625">
forward reverse GDFA
IBM4 37.45 39.18 40.52
IBM4-L0 38.17 38.88 39.82
IBM4-Prior 35.86 36.71 37.08
Agree-HMM 35.58 35.73 39.10
Hard-Cohesion 35.04 37.59 37.63
Soft-Cohesion-EM 30.93 32.67 33.65
Soft-Cohesion-Gibbs 32.07 32.68 32.28
</table>
<tableCaption confidence="0.9867235">
Table 6: AERs on the testing set (trained on the
LARGE data set).
</tableCaption>
<subsectionHeader confidence="0.972271">
5.3 Machine Translation Quality Comparison
</subsectionHeader>
<bodyText confidence="0.999651">
We then evaluate the effect of word alignment on
machine translation quality using the phrase-based
translation system Moses (Koehn et al., 2007). We
take NIST MT03 test data as the development set,
NIST MT05 test data as the testing set. We train a
5-gram language model with the Xinhua portion of
English Gigaword corpus and the English side of
the training set using the SRILM Toolkit (Stolcke,
2002).
We train machine translation models using
GDFA alignments of each system. BLEU scores
on NIST MT05 are listed in Table 7, where BLEU
scores are calculated using lowercased and
tokenized data (Papineni et al., 2002). Although
the IBM4-L0, Agree-HMM, Tree-Distance and
Hard-Cohesion systems improve word alignment
than IBM4, they fail to outperform the IBM4
system on machine translation. The BLEU score of
our Soft-Cohesion-EM system is better than the
IBM4 system when using the FBIS training set, but
worse when using the LARGE training set. Our
Soft-Cohesion-Gibbs system produces the best
BLEU score when using both training sets. We
also performed a statistical significance test using
bootstrap resampling with 1000 samples (Koehn,
2004; Zhang et al., 2004). Experimental results
show the Soft-Cohesion-Gibbs system is
significantly better (p&lt;0.05) than the IBM4 system.
The IBM4-Prior system slightly outperforms IBM4,
but it’s not significant.
</bodyText>
<table confidence="0.997579222222222">
FBIS LARGE
IBM4 30.7 33.1
IBM4-L0 30.4 32.3
IBM4-Prior 30.9 33.2
Agree-HMM 27.2 30.1
Tree-Distance 28.2 N/A
Hard-Cohesion 30.4 32.2
Soft-Cohesion-EM 30.9 33.1
Soft-Cohesion-Gibbs 31.6* 33.9*
</table>
<tableCaption confidence="0.9658485">
Table 7: BLEU scores, where * indicates
significantly better than IBM4 (p&lt;0.05).
</tableCaption>
<sectionHeader confidence="0.999568" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999972538461539">
There have been many proposals of integrating
syntactic knowledge into generative alignment
models. Wu (1997) proposed the inversion
transduction grammar (ITG) to model word
alignment as synchronous parsing for a sentence
pair. Yamada and Knight (2001) represented
translation as a sequence of re-ordering operations
over child nodes of a syntactic tree. Gildea (2003)
introduced a “loosely” tree-based alignment
technique, which allows alignments to violate
syntactic constraints by incurring a cost in
probability. Pauls et al. (2010) gave a new instance
of the ITG formalism, in which one side of the
synchronous derivation is constrained by the
syntactic tree.
Fox (2002) measured syntactic cohesion in gold
standard alignments and showed syntactic
cohesion is generally maintained between English
and French. She also compared three variant
syntactic representations (phrase tree, verb phrase
flattening tree and dependency tree), and found the
dependency tree produced the highest degree of
cohesion. So Cherry and Lin (2003; 2006a) used
dependency cohesion as a hard constraint to
restrict the alignment space, where all potential
alignments violating cohesion constraint are ruled
</bodyText>
<page confidence="0.663127">
298
</page>
<bodyText confidence="0.999961588235294">
out directly. Although the alignment quality is
improved, they ignored situations where a small set
of correct alignments can violate cohesion. To
address this limitation, Cherry and Lin (2006b)
proposed a soft constraint approach, which took
dependency cohesion as a feature of a
discriminative model, and verified that the soft
constraint works better than the hard constraint.
However, the training procedure is very time-
consuming, and they trained the model with only
100 hand-annotated sentence pairs. Therefore, their
method is not suitable for large-scale tasks. In this
paper, we also use dependency cohesion as a soft
constraint. But, unlike Cherry and Lin (2006b), we
integrate the soft dependency cohesion constraint
into a generative model that is more suitable for
large-scale word alignment tasks.
</bodyText>
<sectionHeader confidence="0.964251" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999986652173913">
We described a generative model for word
alignment that uses dependency cohesion as a soft
constraint. We proposed an approximate EM
algorithm and an explicit Gibbs sampling
algorithm for parameter estimation in an
unsupervised manner. Experimental results
performed on a large-scale data set show that our
model improves word alignment quality as well as
machine translation quality. Our experimental
results also indicate that the soft constraint
approach is much better than the hard constraint
approach.
It is possible that our word alignment model can
be improved further. First, we generated word
alignments in both forward and reverse directions
separately, but it might be helpful to use
dependency trees of the two sides simultaneously.
Second, we only used the one-best automatically
generated dependency trees in the model. However,
errors are inevitable in those trees, so we will
investigate how to use N-best dependency trees or
dependency forests (Hayashi et al., 2011) to see if
they can improve our model.
</bodyText>
<sectionHeader confidence="0.99547" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.979197466666667">
We would like to thank Nianwen Xue for
insightful discussions on writing this article. We
are grateful to anonymous reviewers for many
helpful suggestions that helped improve the final
version of this article. The research work has been
funded by the Hi-Tech Research and Development
Program (&amp;quot;863&amp;quot; Program) of China under Grant No.
2011AA01A207, 2012AA011101, and
2012AA011102 and also supported by the Key
Project of Knowledge Innovation Program of
Chinese Academy of Sciences under Grant
No.KGZD-EW-501. This work is also supported in
part by the DAPRA via contract HR0011-11-C-
0145 entitled &amp;quot;Linguistic Resources for
Multilingual Processing&amp;quot;.
</bodyText>
<sectionHeader confidence="0.977769" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999643">
Mohit Bansal, Chris Quirk, and Robert Moore, 2011.
Gappy Phrasal Alignment By Agreement. In Proc. of
ACL 2011.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra and Robert L. Mercer, 1993. The
mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics, 19
(2). pages 263-311.
C. Cherry and D. Lin, 2003. A probability model to
improve word alignment. In Proc. of ACL &apos;03, pages
88-95.
C. Cherry and D. Lin, 2006a. A comparison of
syntactically motivated word alignment spaces. In
Proc. of EACL &apos;06, pages 145-152.
C. Cherry and D. Lin, 2006b. Soft syntactic constraints
for word alignment through discriminative training.
In Proc. of COLING/ACL &apos;06, pages 105-112.
John DeNero and Dan Klein, 2007. Tailoring word
alignments to syntactic machine translation. In Proc.
of ACL &apos;07, pages 17.
C. Dyer, J. Clark, A. Lavie and N.A. Smith, 2011.
Unsupervised word alignment with arbitrary features.
In Proc. of ACL &apos;11, pages 409-419.
Heidi J. Fox, 2002. Phrasal cohesion and statistical
machine translation. In Proc. of EMNLP &apos;02, pages
304-3111.
Michel Galley, Mark Hopkins, Kevin Knight, Daniel
Marcu, 2004. What&apos;s in a translation rule? In Proc. of
NAACL &apos;04, pages 344-352.
J. Gao and M. Johnson, 2008. A comparison of
Bayesian estimators for unsupervised Hidden
Markov Model POS taggers. In Proc. of EMNLP &apos;08,
pages 344-352.
Daniel Gildea, 2003. Loosely Tree-Based Alignment for
Machine Translation. In Proc. of ACL&apos;03, pages 80-
87.
</reference>
<page confidence="0.521549">
299
</page>
<reference confidence="0.999965548387097">
K. Hayashi, T. Watanabe, M. Asahara and Y.
Matsumoto, 2011. Third-order Variational Reranking
on Packed-Shared Dependency Forests. In Proc. of
EMNLP &apos;11.
M. Johnson, T. Griffiths and S. Goldwater, 2007.
Bayesian inference for PCFGs via Markov chain
Monte Carlo. In Proc. of NAACL &apos;07, pages 139-146.
Philipp Koehn, 2004. Statistical significance tests for
machine translation evaluation. In Proc. of
EMNLP&apos;04.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M.
Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran
and R. Zens, 2007. Moses: Open source toolkit for
statistical machine translation. In Proc. of ACL &apos;07,
Demonstration Session, pages 177-180.
Percy Liang, Ben Taskar and Dan Klein, 2006.
Alignment by agreement. In Proc. of HLT-NAACL
06, pages 104-111.
D. Lin and C. Cherry, 2003. Word alignment with
cohesion constraint. In Proc. of NAACL &apos;03, pages
49-51.
Adam Lopez and Philip Resnik, 2005. Improved HMM
alignment models for languages with scarce
resources. In ACL Workshop on Building and Using
Parallel Texts &apos;05, pages 83-86.
Cos¸kun Mermer and Murat Saraçlar, 2011. Bayesian
word alignment for statistical machine translation. In
Proc. of ACL &apos;11, pages 182-187.
R.C. Moore, 2005. A discriminative framework for
bilingual word alignment. In Proc. of EMNLP &apos;05,
pages 81-88.
F.J. Och, C. Tillmann and H. Ney, 1999. Improved
alignment models for statistical machine translation.
In Proc. of EMNLP/WVLC &apos;99, pages 20-28.
Franz Josef Och and Hermann Ney, 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29 (1). pages 19-51.
K. Papineni, S. Roukos, T. Ward and W.J. Zhu, 2002.
BLEU: a method for automatic evaluation of
machine translation. In Proc. of ACL &apos;02, pages 311-
318.
Adam Pauls, Dan Klein, David Chiang and Kevin
Knight, 2010. Unsupervised Syntactic Alignment
with Inversion Transduction Grammars. In Proc. of
NAACL &apos;10.
Slav Petrov, Leon Barrett, Romain Thibaux and Dan
Klein, 2006. Learning accurate, compact, and
interpretable tree annotation. In Proc. of ACL 2006.
Jason Riesa and Daniel Marcu, 2010. Hierarchical
search for word alignment. In Proc. of ACL &apos;10,
pages 157-166.
Jason Riesa, Ann Irvine and Daniel Marcu, 2011.
Feature-Rich Language-Independent Syntax-Based
Alignment for Statistical Machine Translation. In
Proc. of EMNLP &apos;11.
Darcey Riley and Daniel Gildea, 2012. Improving the
IBM Alignment Models Using Variational Bayes. In
Proc. of ACL &apos;12.
M. Saers, J. Nivre and D. Wu, 2010. Word alignment
with stochastic bracketing linear inversion
transduction grammar. In Proc. of NAACL &apos;10, pages
341-344.
A. Stolcke, 2002. SRILM-an extensible language
modeling toolkit. In ICSLP &apos;02.
B. Taskar, S. Lacoste-Julien and D. Klein, 2005. A
discriminative matching approach to word alignment.
In Proc. of EMNLP &apos;05, pages 73-80.
Ashish Vaswani, Liang Huang, and David Chiang, 2012.
Smaller alignment models for better translations:
unsupervised word alignment with the l0 norm. In
Proc. ACL&apos;12, pages 311–319.
Stephan Vogel, Hermann Ney and Christoph Tillmann,
1996. HMM-based word alignment in statistical
translation. In Proc. of COLING-96, pages 836-841.
D. Wu, 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23 (3). pages 377-403.
Zhiguo Wang, Chengqing Zong, 2010. Phrase Structure
Parsing with Dependency Structure, In Proc. of
COLING 2010, pages 1292-1300.
Zhiguo Wang, Chengqing Zong, 2011. Parse Reranking
Based on Higher-Order Lexical Dependencies, In
Proc. Of IJCNLP 2011, pages 1251-1259.
Kenji Yamada and Kevin Knight, 2001. A syntax-based
statistical translation model. In Proc. of ACL &apos;01,
pages 523-530.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004.
Interpreting BLEU/NIST scores: How much
improvement do we need to have a better system? In
Proc. of LREC.
Shaojun Zhao and Daniel Gildea, 2010. A fast fertility
hidden Markov model for word alignment using
MCMC. In Proc. of EMNLP &apos;10, pages 596-605.
</reference>
<page confidence="0.886257">
300
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.704418">
<title confidence="0.997188">Large-scale Word Alignment Using Soft Dependency Cohesion Constraints</title>
<author confidence="0.989401">Wang</author>
<affiliation confidence="0.997661">National Laboratory of Pattern Institute of Automation, Chinese Academy of Sciences</affiliation>
<email confidence="0.722127">zgwang@nlpr.ia.ac.cn</email>
<email confidence="0.722127">cqzong@nlpr.ia.ac.cn</email>
<abstract confidence="0.999642181818182">Dependency cohesion refers to the observation that phrases dominated by disjoint dependency subtrees in the source language generally do not overlap in the target language. It has been verified to be a useful constraint for word alignment. However, previous work either treats this as a hard constraint or uses it as a feature in discriminative models, which is ineffective for large-scale tasks. In this paper, we take dependency cohesion as a soft constraint, and integrate it into a generative model for large-scale word alignment experiments. We also propose an approximate EM algorithm and a Gibbs sampling algorithm to estimate model parameters in an unsupervised manner. Experiments on large-scale Chinese-English translation tasks demonstrate that our model achieves improvements in both alignment quality and translation quality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Chris Quirk</author>
<author>Robert Moore</author>
</authors>
<title>Gappy Phrasal Alignment By Agreement.</title>
<date>2011</date>
<booktitle>In Proc. of ACL</booktitle>
<contexts>
<context position="8917" citStr="Bansal et al., 2011" startWordPosition="1406" endWordPosition="1409"> Cohesion Constraint The most influential generative word alignment models are the IBM Models 1-5 and the HMM model (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). These models can be classified into sequence-based models (IBM Models 1, 2 and HMM) and fertility-based models (IBM Models 3, 4 and 5). The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011). So we built a generative word alignment model with dependency cohesion constraint based on the sequence-based model. 3.1 The Sequence-based Alignment Model According to Brown et al. (1993) and Och and Ney (2003), the sequence-based model is built as a noisy channel model, where the source sentence 𝒇1𝐽 and the alignment 𝒂1𝐽 are generated conditioning on the target sentence 𝒆1𝐼 . The model assumes each source word is assigned to exactly one target word, and defines an asymmetric alignment for the sentence pair as 𝒂1 𝐽 = 𝑎1, 𝑎2, ... , 𝑎𝑗, ... , 𝑎𝐽, where each 𝑎𝑗 ∈ [0, 𝐼] is an alignment from th</context>
</contexts>
<marker>Bansal, Quirk, Moore, 2011</marker>
<rawString>Mohit Bansal, Chris Quirk, and Robert Moore, 2011. Gappy Phrasal Alignment By Agreement. In Proc. of ACL 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>263--311</pages>
<contexts>
<context position="1576" citStr="Brown et al., 1993" startWordPosition="224" endWordPosition="227"> translation tasks demonstrate that our model achieves improvements in both alignment quality and translation quality. 1 Introduction Word alignment is the task of identifying word correspondences between parallel sentence pairs. Word alignment has become a vital component of statistical machine translation (SMT) systems, since it is required by almost all state-of-the-art SMT systems for the purpose of extracting phrase tables or even syntactic transformation rules (Koehn et al., 2007; Galley et al., 2004). During the past two decades, generative word alignment models such as the IBM Models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) have been widely used, primarily because they are trained on bilingual sentences in an unsupervised manner and the implementation is freely available in the GIZA++ toolkit (Och and Ney, 2003). However, the word alignment quality of generative models is still far from satisfactory for SMT systems. In recent years, discriminative alignment models incorporating linguistically motivated features have become increasingly popular (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010; Saers et al., 2010; Riesa et al., 2011). These models are usually trained </context>
<context position="8432" citStr="Brown et al., 1993" startWordPosition="1331" endWordPosition="1334"> the next section, we describe an approach to integrating dependency cohesion constraint into a generative model to softly influence the probabilities of alignment candidates. We show that our new approach addresses the shortcomings of using dependency cohesion as a hard constraint. ch-en en-ch HCP MCP HCP MCP 88.43 95.82 81.53 91.62 Table 1: Cohesion percentages (%) of a manually annotated data set between Chinese and English. 3 A Generative Word Alignment Model with Dependency Cohesion Constraint The most influential generative word alignment models are the IBM Models 1-5 and the HMM model (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). These models can be classified into sequence-based models (IBM Models 1, 2 and HMM) and fertility-based models (IBM Models 3, 4 and 5). The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011). So we built a generative word alignment model with dependency cohesion constraint based on the sequence-based mod</context>
<context position="14346" citStr="Brown et al., 1993" startWordPosition="2368" endWordPosition="2371">], 𝑙 ≠ 𝑘, 𝑓𝑘 and 𝑓𝑙 are a modifier-modifier pair in 𝑇𝑓 } Obviously, 𝑴 = ⋃ 𝓶𝑘 𝐽 𝑘=0 . With the above notations, we formulate the distortion probability for a node 𝑓𝜋𝑗 as 𝑝𝑑 (𝑎𝜋𝑗, 𝓱𝜋𝑗,𝓶𝜋𝑗|𝒂[1,𝑗−1]). According to Eq. (1) and the two improvements, we formulated our model as: 𝑝(𝒇1𝐽, 𝒂[1,𝐽]|𝒆1𝐼, 𝑇𝑓) = 𝑝(𝒂[1,𝐽], 𝑯, 𝑴, 𝒇1𝐽, |𝒆1𝐼, 𝑇𝑓 ) ≈∏𝜋𝑗∈𝐵𝑈𝑇(𝑇𝑓)𝑝𝑑(𝑎𝜋𝑗,𝓱𝜋𝑗, 𝓶𝜋𝑗|𝒂[1,𝑗−1]) 𝑝𝑡(𝑓𝜋𝑗|𝑒𝑎𝜋𝑗) Here, we use the approximation symbol, because the right hand side is not guaranteed to be normalized. In practice, we only compute ratios of these terms, so it is not actually a problem. Such model is called deficient (Brown et al., 1993), and many successful unsupervised models are deficient, e.g., IBM model 3 and IBM model 4. 3.3 Dependency Cohesive Distortion Model We assume the distortion procedure is influenced by three factors: words distance, head-modifier cohesion and modifier-modifier cohesion. Therefore, we further decompose the distortion model 𝑝𝑑 into three terms as follows: 𝑝𝑑 (𝑎𝜋𝑗, 𝓱𝜋𝑗, 𝓶𝜋𝑗|𝒂[1,𝑗−1]) = 𝑝 (𝑎𝜋𝑗|𝒂[1,𝑗−1]) 𝑝 (𝓱𝜋𝑗 |𝒂[1,𝑗]) 𝑝 (𝓶𝜋𝑗 |𝒂[1,𝑗], 𝓱𝜋𝑗) ≈𝑝𝑤𝑑 (𝑎𝜋𝑗 |𝑎𝜋𝑗−1, 𝐼) 𝑝ℎ𝑐 (𝓱𝜋𝑗 |𝒂[1,𝑗]) 𝑝𝑚𝑐 (𝓶𝜋𝑗 |𝒂[1,𝑗]) where 𝑝 𝑤𝑑 is the words distance term, 𝑝ℎ𝑐 is the head-modifier cohesion term and 𝑝𝑚𝑐 is the modifier-m</context>
<context position="18058" citStr="Brown et al., 1993" startWordPosition="2981" endWordPosition="2984"> The main idea is to enumerate only a small subset of good alignments in the E-step, then collect expectation counts and estimate parameters among the small subset in Mstep. Following with Och and Ney (2003), we employ neighbor alignments of the Viterbi alignment as the small subset. Neighbor alignments are obtained by performing one swap or move operation over the Viterbi alignment. Obtaining the Viterbi alignment itself is not so easy for our model. Therefore, we take the Viterbi alignment of the sequence-based model (HMM model) as the starting point, and iterate the hillclimbing algorithm (Brown et al., 1993) many times to get the best alignment greedily. In each iteration, we find the best alignment with Eq. (2) among neighbor alignments of the initial point, and then make the best alignment as the initial point for the next iteration. The algorithm iterates until no update could be made. 4.2 Gibbs Sampling Algorithm Gibbs sampling is another effective algorithm for unsupervised learning problems. As is described in the literatures (Johnson et al., 2007; Gao and Johnson, 2008), there are two types of Gibbs samplers: explicit and collapsed. An explicit sampler represents and samples the model para</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra and Robert L. Mercer, 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19 (2). pages 263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cherry</author>
<author>D Lin</author>
</authors>
<title>A probability model to improve word alignment.</title>
<date>2003</date>
<booktitle>In Proc. of ACL &apos;03,</booktitle>
<pages>88--95</pages>
<contexts>
<context position="31195" citStr="Cherry and Lin (2003" startWordPosition="5045" endWordPosition="5048">nt technique, which allows alignments to violate syntactic constraints by incurring a cost in probability. Pauls et al. (2010) gave a new instance of the ITG formalism, in which one side of the synchronous derivation is constrained by the syntactic tree. Fox (2002) measured syntactic cohesion in gold standard alignments and showed syntactic cohesion is generally maintained between English and French. She also compared three variant syntactic representations (phrase tree, verb phrase flattening tree and dependency tree), and found the dependency tree produced the highest degree of cohesion. So Cherry and Lin (2003; 2006a) used dependency cohesion as a hard constraint to restrict the alignment space, where all potential alignments violating cohesion constraint are ruled 298 out directly. Although the alignment quality is improved, they ignored situations where a small set of correct alignments can violate cohesion. To address this limitation, Cherry and Lin (2006b) proposed a soft constraint approach, which took dependency cohesion as a feature of a discriminative model, and verified that the soft constraint works better than the hard constraint. However, the training procedure is very timeconsuming, an</context>
</contexts>
<marker>Cherry, Lin, 2003</marker>
<rawString>C. Cherry and D. Lin, 2003. A probability model to improve word alignment. In Proc. of ACL &apos;03, pages 88-95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cherry</author>
<author>D Lin</author>
</authors>
<title>A comparison of syntactically motivated word alignment spaces.</title>
<date>2006</date>
<booktitle>In Proc. of EACL &apos;06,</booktitle>
<pages>145--152</pages>
<contexts>
<context position="2948" citStr="Cherry and Lin, 2006" startWordPosition="436" endWordPosition="439">ive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency cohesion (Fox, 2002) into account, which assumes phrases dominated by disjoint dependency subtrees tend not to overlap after translation. Instead of treating dependency cohesion as a hard constraint (Lin and Cherry, 2003) or using it as a feature in discriminative models (Cherry and Lin, 2006b), we treat dependency cohesion as a distortion constraint, and integrate it into a modified HMM word alignment model to softly influence the probabilities of alignment candidates. We also propose an approximate EM algorithm and an explicit Gibbs sampling algorithm to train the model in an unsupervised manner. Experiments on a large-scale Chinese-English translation task demonstrate that our model achieves improvements in both word alignment quality and machine translation quality. The remainder of this paper is organized as follows: Section 2 introduces dependency cohesion 291 Transactions o</context>
<context position="31550" citStr="Cherry and Lin (2006" startWordPosition="5097" endWordPosition="5100">generally maintained between English and French. She also compared three variant syntactic representations (phrase tree, verb phrase flattening tree and dependency tree), and found the dependency tree produced the highest degree of cohesion. So Cherry and Lin (2003; 2006a) used dependency cohesion as a hard constraint to restrict the alignment space, where all potential alignments violating cohesion constraint are ruled 298 out directly. Although the alignment quality is improved, they ignored situations where a small set of correct alignments can violate cohesion. To address this limitation, Cherry and Lin (2006b) proposed a soft constraint approach, which took dependency cohesion as a feature of a discriminative model, and verified that the soft constraint works better than the hard constraint. However, the training procedure is very timeconsuming, and they trained the model with only 100 hand-annotated sentence pairs. Therefore, their method is not suitable for large-scale tasks. In this paper, we also use dependency cohesion as a soft constraint. But, unlike Cherry and Lin (2006b), we integrate the soft dependency cohesion constraint into a generative model that is more suitable for large-scale wo</context>
</contexts>
<marker>Cherry, Lin, 2006</marker>
<rawString>C. Cherry and D. Lin, 2006a. A comparison of syntactically motivated word alignment spaces. In Proc. of EACL &apos;06, pages 145-152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cherry</author>
<author>D Lin</author>
</authors>
<title>Soft syntactic constraints for word alignment through discriminative training.</title>
<date>2006</date>
<booktitle>In Proc. of COLING/ACL &apos;06,</booktitle>
<pages>105--112</pages>
<contexts>
<context position="2948" citStr="Cherry and Lin, 2006" startWordPosition="436" endWordPosition="439">ive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency cohesion (Fox, 2002) into account, which assumes phrases dominated by disjoint dependency subtrees tend not to overlap after translation. Instead of treating dependency cohesion as a hard constraint (Lin and Cherry, 2003) or using it as a feature in discriminative models (Cherry and Lin, 2006b), we treat dependency cohesion as a distortion constraint, and integrate it into a modified HMM word alignment model to softly influence the probabilities of alignment candidates. We also propose an approximate EM algorithm and an explicit Gibbs sampling algorithm to train the model in an unsupervised manner. Experiments on a large-scale Chinese-English translation task demonstrate that our model achieves improvements in both word alignment quality and machine translation quality. The remainder of this paper is organized as follows: Section 2 introduces dependency cohesion 291 Transactions o</context>
<context position="31550" citStr="Cherry and Lin (2006" startWordPosition="5097" endWordPosition="5100">generally maintained between English and French. She also compared three variant syntactic representations (phrase tree, verb phrase flattening tree and dependency tree), and found the dependency tree produced the highest degree of cohesion. So Cherry and Lin (2003; 2006a) used dependency cohesion as a hard constraint to restrict the alignment space, where all potential alignments violating cohesion constraint are ruled 298 out directly. Although the alignment quality is improved, they ignored situations where a small set of correct alignments can violate cohesion. To address this limitation, Cherry and Lin (2006b) proposed a soft constraint approach, which took dependency cohesion as a feature of a discriminative model, and verified that the soft constraint works better than the hard constraint. However, the training procedure is very timeconsuming, and they trained the model with only 100 hand-annotated sentence pairs. Therefore, their method is not suitable for large-scale tasks. In this paper, we also use dependency cohesion as a soft constraint. But, unlike Cherry and Lin (2006b), we integrate the soft dependency cohesion constraint into a generative model that is more suitable for large-scale wo</context>
</contexts>
<marker>Cherry, Lin, 2006</marker>
<rawString>C. Cherry and D. Lin, 2006b. Soft syntactic constraints for word alignment through discriminative training. In Proc. of COLING/ACL &apos;06, pages 105-112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Tailoring word alignments to syntactic machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL &apos;07,</booktitle>
<pages>17</pages>
<contexts>
<context position="2598" citStr="DeNero and Klein, 2007" startWordPosition="381" endWordPosition="384"> linguistically motivated features have become increasingly popular (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010; Saers et al., 2010; Riesa et al., 2011). These models are usually trained with manually annotated parallel data. However, when moving to a new language pair, large amount of hand-aligned data are usually unavailable and expensive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency cohesion (Fox, 2002) into account, which assumes phrases dominated by disjoint dependency subtrees tend not to overlap after translation. Instead of treating dependency cohesion as a hard constraint (Lin and Cherry, 2003) or using it as a feature in discriminative models (Cherry and Lin, 2006b), we treat dependency cohesion as a distortion constraint, and integrate it into a modified HMM word alignment model to softly influence the probabilities of alignment candidates. We also propose an approximate EM algorithm and an explicit Gibbs sa</context>
<context position="8872" citStr="DeNero and Klein, 2007" startWordPosition="1398" endWordPosition="1401">Generative Word Alignment Model with Dependency Cohesion Constraint The most influential generative word alignment models are the IBM Models 1-5 and the HMM model (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). These models can be classified into sequence-based models (IBM Models 1, 2 and HMM) and fertility-based models (IBM Models 3, 4 and 5). The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011). So we built a generative word alignment model with dependency cohesion constraint based on the sequence-based model. 3.1 The Sequence-based Alignment Model According to Brown et al. (1993) and Och and Ney (2003), the sequence-based model is built as a noisy channel model, where the source sentence 𝒇1𝐽 and the alignment 𝒂1𝐽 are generated conditioning on the target sentence 𝒆1𝐼 . The model assumes each source word is assigned to exactly one target word, and defines an asymmetric alignment for the sentence pair as 𝒂1 𝐽 = 𝑎1, 𝑎2, ... , 𝑎𝑗, ... , 𝑎𝐽, w</context>
<context position="10282" citStr="DeNero and Klein, 2007" startWordPosition="1643" endWordPosition="1646">nt procedure into two stages (distortion and translation) and factors as: 𝐽 𝑝(𝒇1 𝐽, 𝒂1 𝐽|𝒆1 𝐼 ) = ∏ 𝑝𝑑(𝑎𝑗|𝑎𝑗−1, 𝐼)𝑝𝑡(𝑓𝑗|𝑒𝑎𝑗) (1) 𝑗=1 where 𝑝𝑑 is the distortion model and 𝑝𝑡 is the translation model. IBM Models 1, 2 and the HMM model all assume the same translation model 𝑝𝑡 (𝑓𝑗|𝑒𝑎𝑗) . However, they use three different distortion models. IBM Model 1 assumes a uniform distortion probability 1/(I+1), IBM Model 2 assumes 𝑝𝑑 (𝑎𝑗 |𝑗) that depends on word position j and HMM model assumes 𝑝𝑑(𝑎𝑗|𝑎𝑗−1,𝐼) that depends on the previous alignment 𝑎𝑗−1. Recently, tree distance models (Lopez and Resnik, 2005; DeNero and Klein, 2007) formulate the distortion model as 𝑝𝑑(𝑎𝑗|𝑎𝑗−1,𝑇) , where the distance between 𝑎𝑗 and 𝑎𝑗−1 are calculated by walking through the phrase (or dependency) tree T. 3.2 Proposed Model To integrate dependency cohesion constraint into a generative model, we refine the sequence-based model in two ways with the help of the source side dependency tree 𝑇𝑓. First, we design a new word alignment order. In the sequence-based model, source words are aligned from left to right by taking source sentence as a linear sequence. However, to apply dependency cohesion constraint, the subtree span of a head node is co</context>
<context position="25544" citStr="DeNero and Klein (2007)" startWordPosition="4187" endWordPosition="4190"> - norm (Vaswani et al., 2012). The training scheme is the same as IBM4. 3) IBM4-Prior: A modification to the GIZA++ toolkit which extends the translation model of IBM models with Dirichlet priors (Riley and Gildea, 2012). The training scheme is the same as IBM4. 4) Agree-HMM: The HMM alignment model by jointly training the forward and reverse models (Liang et al., 2006), which is implemented in the BerkeleyAligner. The training scheme is 5 iterations of jointly training IBM model 1 and 5 iterations of jointly training HMM model. 5) Tree-Distance: The tree distance alignment model proposed in DeNero and Klein (2007), which is implemented in the BerkeleyAligner. The training scheme is 5 iterations of jointly training IBM model 1 and 5 iterations of jointly training the tree distance model. 6) Hard-Cohesion: The implemented “Cohesion Checking Algorithm” (Lin and Cherry, 2003) which takes dependency cohesion as a hard constraint during beam search word alignment decoding. We use the model trained by the Agree-HMM system to estimate alignment candidates. We also build two systems for our soft dependency cohesion model: 7) Soft-Cohesion-EM: the wd-hc-mc sub-model trained with the approximate EM algorithm as d</context>
</contexts>
<marker>DeNero, Klein, 2007</marker>
<rawString>John DeNero and Dan Klein, 2007. Tailoring word alignments to syntactic machine translation. In Proc. of ACL &apos;07, pages 17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Dyer</author>
<author>J Clark</author>
<author>A Lavie</author>
<author>N A Smith</author>
</authors>
<title>Unsupervised word alignment with arbitrary features.</title>
<date>2011</date>
<booktitle>In Proc. of ACL &apos;11,</booktitle>
<pages>409--419</pages>
<marker>Dyer, Clark, Lavie, Smith, 2011</marker>
<rawString>C. Dyer, J. Clark, A. Lavie and N.A. Smith, 2011. Unsupervised word alignment with arbitrary features. In Proc. of ACL &apos;11, pages 409-419.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heidi J Fox</author>
</authors>
<title>Phrasal cohesion and statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP &apos;02,</booktitle>
<pages>304--3111</pages>
<contexts>
<context position="2675" citStr="Fox, 2002" startWordPosition="396" endWordPosition="397">et al., 2005; Riesa and Marcu, 2010; Saers et al., 2010; Riesa et al., 2011). These models are usually trained with manually annotated parallel data. However, when moving to a new language pair, large amount of hand-aligned data are usually unavailable and expensive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency cohesion (Fox, 2002) into account, which assumes phrases dominated by disjoint dependency subtrees tend not to overlap after translation. Instead of treating dependency cohesion as a hard constraint (Lin and Cherry, 2003) or using it as a feature in discriminative models (Cherry and Lin, 2006b), we treat dependency cohesion as a distortion constraint, and integrate it into a modified HMM word alignment model to softly influence the probabilities of alignment candidates. We also propose an approximate EM algorithm and an explicit Gibbs sampling algorithm to train the model in an unsupervised manner. Experiments on</context>
<context position="4893" citStr="Fox (2002)" startWordPosition="749" endWordPosition="750">fined as a subset of the Cartesian product of word positions: A E {(j, i): j = 1,..., J; i = 1, ..., I) When given the source side dependency tree T, we can project dependency subtrees in T onto the target sentence through the alignment A . Dependency cohesion assumes projection spans of disjoint subtrees tend not to overlap. Let T(fi) be the subtree of T rooted at fi, we define two kinds of projection span for the node fi: subtree span and head span. The subtree span is the projection span of the total subtree T(fi), while the head span is the projection span of the node fi itself. Following Fox (2002) and Lin and Cherry (2003), we consider two types of dependency cohesion: headmodifier cohesion and modifier-modifier cohesion. Head-modifier cohesion is defined as the subtree span of a node does not overlap with the head span of its head (parent) node, while modifier-modifier cohesion is defined as subtree spans of two nodes under the same head node do not overlap each other. We call a situation where cohesion is not maintained crossing. Using the dependency tree in Figure 1 as an example, given the correct alignment “R”, the subtree span of “有/have” is [8, 14] , and the head span of its hea</context>
<context position="6492" citStr="Fox (2002)" startWordPosition="1021" endWordPosition="1022">head span of its head “之一/one of”, so a head-modifier crossing occurs. Meanwhile, the subtree spans of the two nodes “有/have” and “ 少 数 /few” overlap each other, so a modifiermodifier crossing occurs. 是 与 北 韩 有 邦 的 数 国 家 之 。 Figure 1: A Chinese-English sentence pair including the word alignments and the Chinese side dependency tree. The Chinese and English words are listed horizontally and vertically, respectively. The black grids are gold-standard alignments. For the Chinese word “有/have”, we give two alignment positions, where “R” is the correct alignment and “W” is the incorrect alignment. Fox (2002) showed that dependency cohesion is generally maintained between English and French. To test how well this assumption holds between Chinese and English, we measure the dependency cohesion between the two languages with a manually annotated bilingual Chinese-English data set of 502 sentence pairs 1 . We use the headmodifier cohesion percentage (HCP) and the modifier-modifier cohesion percentage (MCP) to measure the degree of cohesion in the corpus. HCP (or MCP) is used for measuring how many headmodifier (or modifier-modifier) pairs are actually cohesive. Table 1 lists the relative percentages </context>
<context position="30840" citStr="Fox (2002)" startWordPosition="4997" endWordPosition="4998"> into generative alignment models. Wu (1997) proposed the inversion transduction grammar (ITG) to model word alignment as synchronous parsing for a sentence pair. Yamada and Knight (2001) represented translation as a sequence of re-ordering operations over child nodes of a syntactic tree. Gildea (2003) introduced a “loosely” tree-based alignment technique, which allows alignments to violate syntactic constraints by incurring a cost in probability. Pauls et al. (2010) gave a new instance of the ITG formalism, in which one side of the synchronous derivation is constrained by the syntactic tree. Fox (2002) measured syntactic cohesion in gold standard alignments and showed syntactic cohesion is generally maintained between English and French. She also compared three variant syntactic representations (phrase tree, verb phrase flattening tree and dependency tree), and found the dependency tree produced the highest degree of cohesion. So Cherry and Lin (2003; 2006a) used dependency cohesion as a hard constraint to restrict the alignment space, where all potential alignments violating cohesion constraint are ruled 298 out directly. Although the alignment quality is improved, they ignored situations </context>
</contexts>
<marker>Fox, 2002</marker>
<rawString>Heidi J. Fox, 2002. Phrasal cohesion and statistical machine translation. In Proc. of EMNLP &apos;02, pages 304-3111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What&apos;s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proc. of NAACL &apos;04,</booktitle>
<pages>344--352</pages>
<contexts>
<context position="1469" citStr="Galley et al., 2004" startWordPosition="206" endWordPosition="209">algorithm to estimate model parameters in an unsupervised manner. Experiments on large-scale Chinese-English translation tasks demonstrate that our model achieves improvements in both alignment quality and translation quality. 1 Introduction Word alignment is the task of identifying word correspondences between parallel sentence pairs. Word alignment has become a vital component of statistical machine translation (SMT) systems, since it is required by almost all state-of-the-art SMT systems for the purpose of extracting phrase tables or even syntactic transformation rules (Koehn et al., 2007; Galley et al., 2004). During the past two decades, generative word alignment models such as the IBM Models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) have been widely used, primarily because they are trained on bilingual sentences in an unsupervised manner and the implementation is freely available in the GIZA++ toolkit (Och and Ney, 2003). However, the word alignment quality of generative models is still far from satisfactory for SMT systems. In recent years, discriminative alignment models incorporating linguistically motivated features have become increasingly popular (Moore, 2005; Taskar et a</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, Daniel Marcu, 2004. What&apos;s in a translation rule? In Proc. of NAACL &apos;04, pages 344-352.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gao</author>
<author>M Johnson</author>
</authors>
<title>A comparison of Bayesian estimators for unsupervised Hidden Markov Model POS taggers.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP &apos;08,</booktitle>
<pages>344--352</pages>
<contexts>
<context position="18536" citStr="Gao and Johnson, 2008" startWordPosition="3058" endWordPosition="3061">the Viterbi alignment of the sequence-based model (HMM model) as the starting point, and iterate the hillclimbing algorithm (Brown et al., 1993) many times to get the best alignment greedily. In each iteration, we find the best alignment with Eq. (2) among neighbor alignments of the initial point, and then make the best alignment as the initial point for the next iteration. The algorithm iterates until no update could be made. 4.2 Gibbs Sampling Algorithm Gibbs sampling is another effective algorithm for unsupervised learning problems. As is described in the literatures (Johnson et al., 2007; Gao and Johnson, 2008), there are two types of Gibbs samplers: explicit and collapsed. An explicit sampler represents and samples the model parameters in addition to the word alignments, while in a collapsed sampler the parameters are integrated out and only alignments are sampled. Mermer and Saraçlar (2011) proposed a collapsed sampler for IBM Model 1. However, their sampler updates parameters constantly and thus cannot run efficiently on large-scale tasks. Instead, we take advantage of explicit Gibbs sampling to make a highly parallelizable sampler. Our Gibbs sampler is similar to the MCMC algorithm in Zhao and G</context>
</contexts>
<marker>Gao, Johnson, 2008</marker>
<rawString>J. Gao and M. Johnson, 2008. A comparison of Bayesian estimators for unsupervised Hidden Markov Model POS taggers. In Proc. of EMNLP &apos;08, pages 344-352.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Loosely Tree-Based Alignment for Machine Translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL&apos;03,</booktitle>
<pages>80--87</pages>
<contexts>
<context position="30533" citStr="Gildea (2003)" startWordPosition="4950" endWordPosition="4951">3 IBM4-Prior 30.9 33.2 Agree-HMM 27.2 30.1 Tree-Distance 28.2 N/A Hard-Cohesion 30.4 32.2 Soft-Cohesion-EM 30.9 33.1 Soft-Cohesion-Gibbs 31.6* 33.9* Table 7: BLEU scores, where * indicates significantly better than IBM4 (p&lt;0.05). 6 Related Work There have been many proposals of integrating syntactic knowledge into generative alignment models. Wu (1997) proposed the inversion transduction grammar (ITG) to model word alignment as synchronous parsing for a sentence pair. Yamada and Knight (2001) represented translation as a sequence of re-ordering operations over child nodes of a syntactic tree. Gildea (2003) introduced a “loosely” tree-based alignment technique, which allows alignments to violate syntactic constraints by incurring a cost in probability. Pauls et al. (2010) gave a new instance of the ITG formalism, in which one side of the synchronous derivation is constrained by the syntactic tree. Fox (2002) measured syntactic cohesion in gold standard alignments and showed syntactic cohesion is generally maintained between English and French. She also compared three variant syntactic representations (phrase tree, verb phrase flattening tree and dependency tree), and found the dependency tree pr</context>
</contexts>
<marker>Gildea, 2003</marker>
<rawString>Daniel Gildea, 2003. Loosely Tree-Based Alignment for Machine Translation. In Proc. of ACL&apos;03, pages 80-87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hayashi</author>
<author>T Watanabe</author>
<author>M Asahara</author>
<author>Y Matsumoto</author>
</authors>
<title>Third-order Variational Reranking on Packed-Shared Dependency Forests.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP &apos;11.</booktitle>
<contexts>
<context position="33183" citStr="Hayashi et al., 2011" startWordPosition="5346" endWordPosition="5349"> translation quality. Our experimental results also indicate that the soft constraint approach is much better than the hard constraint approach. It is possible that our word alignment model can be improved further. First, we generated word alignments in both forward and reverse directions separately, but it might be helpful to use dependency trees of the two sides simultaneously. Second, we only used the one-best automatically generated dependency trees in the model. However, errors are inevitable in those trees, so we will investigate how to use N-best dependency trees or dependency forests (Hayashi et al., 2011) to see if they can improve our model. Acknowledgments We would like to thank Nianwen Xue for insightful discussions on writing this article. We are grateful to anonymous reviewers for many helpful suggestions that helped improve the final version of this article. The research work has been funded by the Hi-Tech Research and Development Program (&amp;quot;863&amp;quot; Program) of China under Grant No. 2011AA01A207, 2012AA011101, and 2012AA011102 and also supported by the Key Project of Knowledge Innovation Program of Chinese Academy of Sciences under Grant No.KGZD-EW-501. This work is also supported in part by</context>
</contexts>
<marker>Hayashi, Watanabe, Asahara, Matsumoto, 2011</marker>
<rawString>K. Hayashi, T. Watanabe, M. Asahara and Y. Matsumoto, 2011. Third-order Variational Reranking on Packed-Shared Dependency Forests. In Proc. of EMNLP &apos;11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
<author>T Griffiths</author>
<author>S Goldwater</author>
</authors>
<title>Bayesian inference for PCFGs via Markov chain Monte Carlo.</title>
<date>2007</date>
<booktitle>In Proc. of NAACL &apos;07,</booktitle>
<pages>139--146</pages>
<contexts>
<context position="18512" citStr="Johnson et al., 2007" startWordPosition="3054" endWordPosition="3057">l. Therefore, we take the Viterbi alignment of the sequence-based model (HMM model) as the starting point, and iterate the hillclimbing algorithm (Brown et al., 1993) many times to get the best alignment greedily. In each iteration, we find the best alignment with Eq. (2) among neighbor alignments of the initial point, and then make the best alignment as the initial point for the next iteration. The algorithm iterates until no update could be made. 4.2 Gibbs Sampling Algorithm Gibbs sampling is another effective algorithm for unsupervised learning problems. As is described in the literatures (Johnson et al., 2007; Gao and Johnson, 2008), there are two types of Gibbs samplers: explicit and collapsed. An explicit sampler represents and samples the model parameters in addition to the word alignments, while in a collapsed sampler the parameters are integrated out and only alignments are sampled. Mermer and Saraçlar (2011) proposed a collapsed sampler for IBM Model 1. However, their sampler updates parameters constantly and thus cannot run efficiently on large-scale tasks. Instead, we take advantage of explicit Gibbs sampling to make a highly parallelizable sampler. Our Gibbs sampler is similar to the MCMC</context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>M. Johnson, T. Griffiths and S. Goldwater, 2007. Bayesian inference for PCFGs via Markov chain Monte Carlo. In Proc. of NAACL &apos;07, pages 139-146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP&apos;04.</booktitle>
<contexts>
<context position="29668" citStr="Koehn, 2004" startWordPosition="4828" endWordPosition="4829">EU scores are calculated using lowercased and tokenized data (Papineni et al., 2002). Although the IBM4-L0, Agree-HMM, Tree-Distance and Hard-Cohesion systems improve word alignment than IBM4, they fail to outperform the IBM4 system on machine translation. The BLEU score of our Soft-Cohesion-EM system is better than the IBM4 system when using the FBIS training set, but worse when using the LARGE training set. Our Soft-Cohesion-Gibbs system produces the best BLEU score when using both training sets. We also performed a statistical significance test using bootstrap resampling with 1000 samples (Koehn, 2004; Zhang et al., 2004). Experimental results show the Soft-Cohesion-Gibbs system is significantly better (p&lt;0.05) than the IBM4 system. The IBM4-Prior system slightly outperforms IBM4, but it’s not significant. FBIS LARGE IBM4 30.7 33.1 IBM4-L0 30.4 32.3 IBM4-Prior 30.9 33.2 Agree-HMM 27.2 30.1 Tree-Distance 28.2 N/A Hard-Cohesion 30.4 32.2 Soft-Cohesion-EM 30.9 33.1 Soft-Cohesion-Gibbs 31.6* 33.9* Table 7: BLEU scores, where * indicates significantly better than IBM4 (p&lt;0.05). 6 Related Work There have been many proposals of integrating syntactic knowledge into generative alignment models. Wu </context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn, 2004. Statistical significance tests for machine translation evaluation. In Proc. of EMNLP&apos;04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL &apos;07, Demonstration Session,</booktitle>
<pages>177--180</pages>
<contexts>
<context position="1447" citStr="Koehn et al., 2007" startWordPosition="202" endWordPosition="205">nd a Gibbs sampling algorithm to estimate model parameters in an unsupervised manner. Experiments on large-scale Chinese-English translation tasks demonstrate that our model achieves improvements in both alignment quality and translation quality. 1 Introduction Word alignment is the task of identifying word correspondences between parallel sentence pairs. Word alignment has become a vital component of statistical machine translation (SMT) systems, since it is required by almost all state-of-the-art SMT systems for the purpose of extracting phrase tables or even syntactic transformation rules (Koehn et al., 2007; Galley et al., 2004). During the past two decades, generative word alignment models such as the IBM Models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) have been widely used, primarily because they are trained on bilingual sentences in an unsupervised manner and the implementation is freely available in the GIZA++ toolkit (Och and Ney, 2003). However, the word alignment quality of generative models is still far from satisfactory for SMT systems. In recent years, discriminative alignment models incorporating linguistically motivated features have become increasingly popular (Mo</context>
<context position="26499" citStr="Koehn et al., 2007" startWordPosition="4330" endWordPosition="4333">word alignment decoding. We use the model trained by the Agree-HMM system to estimate alignment candidates. We also build two systems for our soft dependency cohesion model: 7) Soft-Cohesion-EM: the wd-hc-mc sub-model trained with the approximate EM algorithm as described in sub-section 5.1. 8) Soft-Cohesion-Gibbs: the wd-hc-mc sub-model trained with the Gibbs sampling algorithm as described in sub-section 5.1. We train all these systems on the FBIS training set, and test them on the testing set. We also combine the forward and reverse alignments with the grow-diag-final-and (GDFA) heuristic (Koehn et al., 2007). All AERs are listed in Table 4. We find our soft cohesion systems produce better AERs than the Hard-Cohesion system as well as the other systems. Table 5 gives the head-modifier cohesion percentage (HCP) and the modifiermodifier cohesion percentage (MCP) of each system. We find HCPs and MCPs of our soft cohesion systems are much closer to the goldstandard alignments. forward reverse GDFA IBM4 42.90 42.81 44.32 IBM4-L0 42.59 41.04 43.19 IBM4-Prior 41.94 40.46 42.44 Agree-HMM 38.03 37.91 41.01 Tree-Distance 34.21 37.22 38.42 Hard-Cohesion 37.32 38.92 38.92 Soft-Cohesion-EM 33.65 34.74 35.85 So</context>
<context position="28667" citStr="Koehn et al., 2007" startWordPosition="4667" endWordPosition="4670">.36 Soft-Cohesion-Gibbs 88.74 85.55 87.81 84.83 gold-standard 88.43 95.82 81.53 91.62 Table 5: HCPs and MCPs on the development set. forward reverse GDFA IBM4 37.45 39.18 40.52 IBM4-L0 38.17 38.88 39.82 IBM4-Prior 35.86 36.71 37.08 Agree-HMM 35.58 35.73 39.10 Hard-Cohesion 35.04 37.59 37.63 Soft-Cohesion-EM 30.93 32.67 33.65 Soft-Cohesion-Gibbs 32.07 32.68 32.28 Table 6: AERs on the testing set (trained on the LARGE data set). 5.3 Machine Translation Quality Comparison We then evaluate the effect of word alignment on machine translation quality using the phrase-based translation system Moses (Koehn et al., 2007). We take NIST MT03 test data as the development set, NIST MT05 test data as the testing set. We train a 5-gram language model with the Xinhua portion of English Gigaword corpus and the English side of the training set using the SRILM Toolkit (Stolcke, 2002). We train machine translation models using GDFA alignments of each system. BLEU scores on NIST MT05 are listed in Table 7, where BLEU scores are calculated using lowercased and tokenized data (Papineni et al., 2002). Although the IBM4-L0, Agree-HMM, Tree-Distance and Hard-Cohesion systems improve word alignment than IBM4, they fail to outp</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran and R. Zens, 2007. Moses: Open source toolkit for statistical machine translation. In Proc. of ACL &apos;07, Demonstration Session, pages 177-180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proc. of HLT-NAACL 06,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="8848" citStr="Liang et al., 2006" startWordPosition="1394" endWordPosition="1397">se and English. 3 A Generative Word Alignment Model with Dependency Cohesion Constraint The most influential generative word alignment models are the IBM Models 1-5 and the HMM model (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). These models can be classified into sequence-based models (IBM Models 1, 2 and HMM) and fertility-based models (IBM Models 3, 4 and 5). The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011). So we built a generative word alignment model with dependency cohesion constraint based on the sequence-based model. 3.1 The Sequence-based Alignment Model According to Brown et al. (1993) and Och and Ney (2003), the sequence-based model is built as a noisy channel model, where the source sentence 𝒇1𝐽 and the alignment 𝒂1𝐽 are generated conditioning on the target sentence 𝒆1𝐼 . The model assumes each source word is assigned to exactly one target word, and defines an asymmetric alignment for the sentence pair as 𝒂1 𝐽 = 𝑎1, 𝑎</context>
<context position="25294" citStr="Liang et al., 2006" startWordPosition="4148" endWordPosition="4151">model 4) which is implemented in GIZA++ toolkit. The training scheme is 5 iterations of IBM model 1, 5 iterations of the HMM model and 10 iterations of IBM model 4. 2) IBM4-L0: A modification to the GIZA++ toolkit which extends IBM models with ℓ0 - norm (Vaswani et al., 2012). The training scheme is the same as IBM4. 3) IBM4-Prior: A modification to the GIZA++ toolkit which extends the translation model of IBM models with Dirichlet priors (Riley and Gildea, 2012). The training scheme is the same as IBM4. 4) Agree-HMM: The HMM alignment model by jointly training the forward and reverse models (Liang et al., 2006), which is implemented in the BerkeleyAligner. The training scheme is 5 iterations of jointly training IBM model 1 and 5 iterations of jointly training HMM model. 5) Tree-Distance: The tree distance alignment model proposed in DeNero and Klein (2007), which is implemented in the BerkeleyAligner. The training scheme is 5 iterations of jointly training IBM model 1 and 5 iterations of jointly training the tree distance model. 6) Hard-Cohesion: The implemented “Cohesion Checking Algorithm” (Lin and Cherry, 2003) which takes dependency cohesion as a hard constraint during beam search word alignment</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar and Dan Klein, 2006. Alignment by agreement. In Proc. of HLT-NAACL 06, pages 104-111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
<author>C Cherry</author>
</authors>
<title>Word alignment with cohesion constraint.</title>
<date>2003</date>
<booktitle>In Proc. of NAACL &apos;03,</booktitle>
<pages>49--51</pages>
<contexts>
<context position="2876" citStr="Lin and Cherry, 2003" startWordPosition="423" endWordPosition="426">air, large amount of hand-aligned data are usually unavailable and expensive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency cohesion (Fox, 2002) into account, which assumes phrases dominated by disjoint dependency subtrees tend not to overlap after translation. Instead of treating dependency cohesion as a hard constraint (Lin and Cherry, 2003) or using it as a feature in discriminative models (Cherry and Lin, 2006b), we treat dependency cohesion as a distortion constraint, and integrate it into a modified HMM word alignment model to softly influence the probabilities of alignment candidates. We also propose an approximate EM algorithm and an explicit Gibbs sampling algorithm to train the model in an unsupervised manner. Experiments on a large-scale Chinese-English translation task demonstrate that our model achieves improvements in both word alignment quality and machine translation quality. The remainder of this paper is organized</context>
<context position="4919" citStr="Lin and Cherry (2003)" startWordPosition="752" endWordPosition="755">et of the Cartesian product of word positions: A E {(j, i): j = 1,..., J; i = 1, ..., I) When given the source side dependency tree T, we can project dependency subtrees in T onto the target sentence through the alignment A . Dependency cohesion assumes projection spans of disjoint subtrees tend not to overlap. Let T(fi) be the subtree of T rooted at fi, we define two kinds of projection span for the node fi: subtree span and head span. The subtree span is the projection span of the total subtree T(fi), while the head span is the projection span of the node fi itself. Following Fox (2002) and Lin and Cherry (2003), we consider two types of dependency cohesion: headmodifier cohesion and modifier-modifier cohesion. Head-modifier cohesion is defined as the subtree span of a node does not overlap with the head span of its head (parent) node, while modifier-modifier cohesion is defined as subtree spans of two nodes under the same head node do not overlap each other. We call a situation where cohesion is not maintained crossing. Using the dependency tree in Figure 1 as an example, given the correct alignment “R”, the subtree span of “有/have” is [8, 14] , and the head span of its head node “之一/one of” is [3, </context>
<context position="25807" citStr="Lin and Cherry, 2003" startWordPosition="4226" endWordPosition="4229">) Agree-HMM: The HMM alignment model by jointly training the forward and reverse models (Liang et al., 2006), which is implemented in the BerkeleyAligner. The training scheme is 5 iterations of jointly training IBM model 1 and 5 iterations of jointly training HMM model. 5) Tree-Distance: The tree distance alignment model proposed in DeNero and Klein (2007), which is implemented in the BerkeleyAligner. The training scheme is 5 iterations of jointly training IBM model 1 and 5 iterations of jointly training the tree distance model. 6) Hard-Cohesion: The implemented “Cohesion Checking Algorithm” (Lin and Cherry, 2003) which takes dependency cohesion as a hard constraint during beam search word alignment decoding. We use the model trained by the Agree-HMM system to estimate alignment candidates. We also build two systems for our soft dependency cohesion model: 7) Soft-Cohesion-EM: the wd-hc-mc sub-model trained with the approximate EM algorithm as described in sub-section 5.1. 8) Soft-Cohesion-Gibbs: the wd-hc-mc sub-model trained with the Gibbs sampling algorithm as described in sub-section 5.1. We train all these systems on the FBIS training set, and test them on the testing set. We also combine the forwa</context>
</contexts>
<marker>Lin, Cherry, 2003</marker>
<rawString>D. Lin and C. Cherry, 2003. Word alignment with cohesion constraint. In Proc. of NAACL &apos;03, pages 49-51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
<author>Philip Resnik</author>
</authors>
<title>Improved HMM alignment models for languages with scarce resources.</title>
<date>2005</date>
<booktitle>In ACL Workshop on Building and Using Parallel Texts &apos;05,</booktitle>
<pages>83--86</pages>
<contexts>
<context position="2574" citStr="Lopez and Resnik, 2005" startWordPosition="377" endWordPosition="380">ent models incorporating linguistically motivated features have become increasingly popular (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010; Saers et al., 2010; Riesa et al., 2011). These models are usually trained with manually annotated parallel data. However, when moving to a new language pair, large amount of hand-aligned data are usually unavailable and expensive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency cohesion (Fox, 2002) into account, which assumes phrases dominated by disjoint dependency subtrees tend not to overlap after translation. Instead of treating dependency cohesion as a hard constraint (Lin and Cherry, 2003) or using it as a feature in discriminative models (Cherry and Lin, 2006b), we treat dependency cohesion as a distortion constraint, and integrate it into a modified HMM word alignment model to softly influence the probabilities of alignment candidates. We also propose an approximate EM algorithm </context>
<context position="8828" citStr="Lopez and Resnik, 2005" startWordPosition="1390" endWordPosition="1393">d data set between Chinese and English. 3 A Generative Word Alignment Model with Dependency Cohesion Constraint The most influential generative word alignment models are the IBM Models 1-5 and the HMM model (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). These models can be classified into sequence-based models (IBM Models 1, 2 and HMM) and fertility-based models (IBM Models 3, 4 and 5). The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011). So we built a generative word alignment model with dependency cohesion constraint based on the sequence-based model. 3.1 The Sequence-based Alignment Model According to Brown et al. (1993) and Och and Ney (2003), the sequence-based model is built as a noisy channel model, where the source sentence 𝒇1𝐽 and the alignment 𝒂1𝐽 are generated conditioning on the target sentence 𝒆1𝐼 . The model assumes each source word is assigned to exactly one target word, and defines an asymmetric alignment for the sentence </context>
<context position="10257" citStr="Lopez and Resnik, 2005" startWordPosition="1639" endWordPosition="1642">ed model divides alignment procedure into two stages (distortion and translation) and factors as: 𝐽 𝑝(𝒇1 𝐽, 𝒂1 𝐽|𝒆1 𝐼 ) = ∏ 𝑝𝑑(𝑎𝑗|𝑎𝑗−1, 𝐼)𝑝𝑡(𝑓𝑗|𝑒𝑎𝑗) (1) 𝑗=1 where 𝑝𝑑 is the distortion model and 𝑝𝑡 is the translation model. IBM Models 1, 2 and the HMM model all assume the same translation model 𝑝𝑡 (𝑓𝑗|𝑒𝑎𝑗) . However, they use three different distortion models. IBM Model 1 assumes a uniform distortion probability 1/(I+1), IBM Model 2 assumes 𝑝𝑑 (𝑎𝑗 |𝑗) that depends on word position j and HMM model assumes 𝑝𝑑(𝑎𝑗|𝑎𝑗−1,𝐼) that depends on the previous alignment 𝑎𝑗−1. Recently, tree distance models (Lopez and Resnik, 2005; DeNero and Klein, 2007) formulate the distortion model as 𝑝𝑑(𝑎𝑗|𝑎𝑗−1,𝑇) , where the distance between 𝑎𝑗 and 𝑎𝑗−1 are calculated by walking through the phrase (or dependency) tree T. 3.2 Proposed Model To integrate dependency cohesion constraint into a generative model, we refine the sequence-based model in two ways with the help of the source side dependency tree 𝑇𝑓. First, we design a new word alignment order. In the sequence-based model, source words are aligned from left to right by taking source sentence as a linear sequence. However, to apply dependency cohesion constraint, the subtree </context>
</contexts>
<marker>Lopez, Resnik, 2005</marker>
<rawString>Adam Lopez and Philip Resnik, 2005. Improved HMM alignment models for languages with scarce resources. In ACL Workshop on Building and Using Parallel Texts &apos;05, pages 83-86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cos¸kun Mermer</author>
<author>Murat Saraçlar</author>
</authors>
<title>Bayesian word alignment for statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proc. of ACL &apos;11,</booktitle>
<pages>182--187</pages>
<contexts>
<context position="18823" citStr="Mermer and Saraçlar (2011)" startWordPosition="3102" endWordPosition="3105"> initial point, and then make the best alignment as the initial point for the next iteration. The algorithm iterates until no update could be made. 4.2 Gibbs Sampling Algorithm Gibbs sampling is another effective algorithm for unsupervised learning problems. As is described in the literatures (Johnson et al., 2007; Gao and Johnson, 2008), there are two types of Gibbs samplers: explicit and collapsed. An explicit sampler represents and samples the model parameters in addition to the word alignments, while in a collapsed sampler the parameters are integrated out and only alignments are sampled. Mermer and Saraçlar (2011) proposed a collapsed sampler for IBM Model 1. However, their sampler updates parameters constantly and thus cannot run efficiently on large-scale tasks. Instead, we take advantage of explicit Gibbs sampling to make a highly parallelizable sampler. Our Gibbs sampler is similar to the MCMC algorithm in Zhao and Gildea (2010), but we assume Dirichlet priors when sampling model parameters and take a different sampling approach based on the source side dependency tree. Our sampler performs a sequence of consecutive iterations. Each iteration consists of two sampling steps. The first step samples t</context>
</contexts>
<marker>Mermer, Saraçlar, 2011</marker>
<rawString>Cos¸kun Mermer and Murat Saraçlar, 2011. Bayesian word alignment for statistical machine translation. In Proc. of ACL &apos;11, pages 182-187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Moore</author>
</authors>
<title>A discriminative framework for bilingual word alignment.</title>
<date>2005</date>
<booktitle>In Proc. of EMNLP &apos;05,</booktitle>
<pages>81--88</pages>
<contexts>
<context position="2056" citStr="Moore, 2005" startWordPosition="296" endWordPosition="297">07; Galley et al., 2004). During the past two decades, generative word alignment models such as the IBM Models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) have been widely used, primarily because they are trained on bilingual sentences in an unsupervised manner and the implementation is freely available in the GIZA++ toolkit (Och and Ney, 2003). However, the word alignment quality of generative models is still far from satisfactory for SMT systems. In recent years, discriminative alignment models incorporating linguistically motivated features have become increasingly popular (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010; Saers et al., 2010; Riesa et al., 2011). These models are usually trained with manually annotated parallel data. However, when moving to a new language pair, large amount of hand-aligned data are usually unavailable and expensive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency c</context>
</contexts>
<marker>Moore, 2005</marker>
<rawString>R.C. Moore, 2005. A discriminative framework for bilingual word alignment. In Proc. of EMNLP &apos;05, pages 81-88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>C Tillmann</author>
<author>H Ney</author>
</authors>
<title>Improved alignment models for statistical machine translation.</title>
<date>1999</date>
<booktitle>In Proc. of EMNLP/WVLC &apos;99,</booktitle>
<pages>20--28</pages>
<contexts>
<context position="16693" citStr="Och et al., 1999" startWordPosition="2761" endWordPosition="2764"> 𝑓𝜋𝑗 and its siblings. Therefore, we define 𝑝𝑚𝑐 as the product of probabilities for all the modifier-modifier pairs taking 𝑓𝜋𝑗 as the higher-order node: 𝑝𝑚𝑐 (𝓶𝜋𝑗 |𝒂[1,𝑗]) = ∏𝓂𝜋𝑗,𝑠∈𝓶𝜋𝑗 𝑝𝑚 (𝓂𝜋𝑗,𝑠 |𝑓𝑠, 𝑒𝑎𝜋𝑗, 𝑒𝑎𝑠) (6) where 𝓂𝜋𝑗,𝑠 ∈ {𝑐𝑜ℎ𝑒𝑠𝑖𝑜𝑛, 𝑐𝑟𝑜𝑠𝑠𝑖𝑛𝑔} is the modifiermodifier cohesion relationship between 𝑓𝜋 𝑗 and one of its sibling 𝑓𝑠 , 𝑝𝑚 is the corresponding probability, 𝑒𝑎𝜋 𝑗 and 𝑒𝑎𝑠 are the aligned words for 𝑓𝜋𝑗 and 𝑓𝑠. Both 𝑝ℎ and 𝑝𝑚 in Eq. (5) and Eq. (6) are conditioned on three words, which would make them very sparse. To cope with this problem, we use the word clustering toolkit, mkcls (Och et al., 1999), to cluster all words into 50 classes, and replace the three words with their classes. 4 Parameter Estimation To align sentence pairs with the model in Eq. (2), we have to estimate some parameters: 𝑝𝑡, 𝑝𝑤𝑑, 𝑝ℎ and 𝑝𝑚. The traditional approach for sequencebased models uses Expectation Maximization (EM) algorithm to estimate parameters. However, in our model, it is hard to find an efficient way to sum over all the possible alignments, which is required in the E-step of EM algorithm. Therefore, we propose an approximate EM algorithm and a Gibbs sampling algorithm for parameter estimation. 4.1 Ap</context>
</contexts>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>F.J. Och, C. Tillmann and H. Ney, 1999. Improved alignment models for statistical machine translation. In Proc. of EMNLP/WVLC &apos;99, pages 20-28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<pages>pages</pages>
<contexts>
<context position="1807" citStr="Och and Ney, 2003" startWordPosition="262" endWordPosition="265">rd alignment has become a vital component of statistical machine translation (SMT) systems, since it is required by almost all state-of-the-art SMT systems for the purpose of extracting phrase tables or even syntactic transformation rules (Koehn et al., 2007; Galley et al., 2004). During the past two decades, generative word alignment models such as the IBM Models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) have been widely used, primarily because they are trained on bilingual sentences in an unsupervised manner and the implementation is freely available in the GIZA++ toolkit (Och and Ney, 2003). However, the word alignment quality of generative models is still far from satisfactory for SMT systems. In recent years, discriminative alignment models incorporating linguistically motivated features have become increasingly popular (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010; Saers et al., 2010; Riesa et al., 2011). These models are usually trained with manually annotated parallel data. However, when moving to a new language pair, large amount of hand-aligned data are usually unavailable and expensive to create. A more practical way to improve large-scale word alignment quali</context>
<context position="8472" citStr="Och and Ney, 2003" startWordPosition="1339" endWordPosition="1342">ch to integrating dependency cohesion constraint into a generative model to softly influence the probabilities of alignment candidates. We show that our new approach addresses the shortcomings of using dependency cohesion as a hard constraint. ch-en en-ch HCP MCP HCP MCP 88.43 95.82 81.53 91.62 Table 1: Cohesion percentages (%) of a manually annotated data set between Chinese and English. 3 A Generative Word Alignment Model with Dependency Cohesion Constraint The most influential generative word alignment models are the IBM Models 1-5 and the HMM model (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). These models can be classified into sequence-based models (IBM Models 1, 2 and HMM) and fertility-based models (IBM Models 3, 4 and 5). The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011). So we built a generative word alignment model with dependency cohesion constraint based on the sequence-based model. 3.1 The Sequence-based Alignment Mod</context>
<context position="17438" citStr="Och and Ney, 2003" startWordPosition="2880" endWordPosition="2883">e pairs with the model in Eq. (2), we have to estimate some parameters: 𝑝𝑡, 𝑝𝑤𝑑, 𝑝ℎ and 𝑝𝑚. The traditional approach for sequencebased models uses Expectation Maximization (EM) algorithm to estimate parameters. However, in our model, it is hard to find an efficient way to sum over all the possible alignments, which is required in the E-step of EM algorithm. Therefore, we propose an approximate EM algorithm and a Gibbs sampling algorithm for parameter estimation. 4.1 Approximate EM Algorithm The approximate EM algorithm is similar to the training algorithm for fertility-based alignment models (Och and Ney, 2003). The main idea is to enumerate only a small subset of good alignments in the E-step, then collect expectation counts and estimate parameters among the small subset in Mstep. Following with Och and Ney (2003), we employ neighbor alignments of the Viterbi alignment as the small subset. Neighbor alignments are obtained by performing one swap or move operation over the Viterbi alignment. Obtaining the Viterbi alignment itself is not so easy for our model. Therefore, we take the Viterbi alignment of the sequence-based model (HMM model) as the starting point, and iterate the hillclimbing algorithm </context>
<context position="21666" citStr="Och and Ney (2003)" startWordPosition="3557" endWordPosition="3560"> the Chinese-English language pair. We employ two training sets: FBIS and LARGE. The size and source corpus of these training sets are listed in Table 2. We will use the smaller training set FBIS to evaluate the characters of our model and use the LARGE training set to evaluate whether our model is adaptable for large-scale task. For word alignment quality evaluation, we take the handaligned data sets from SSMT20072, which contains 2 http://nlp.ict.ac.cn/guidelines/guidelines-2007- SSMT(English).doc 505 sentence pairs in the testing set and 502 sentence pairs in the development set. Following Och and Ney (2003), we evaluate word alignment quality with the alignment error rate (AER), where lower AER is better. Because our model takes dependency trees as input, we parse both sides of the two training sets, the development set and the testing set with Berkeley parser (Petrov et al., 2006), and then convert the generated phrase trees into dependency trees according to Wang and Zong (2010; 2011). Our model is an asymmetric model, so we perform word alignment in both forward (Chinese4English) and reverse (English4Chinese) directions. Train Set Source Corpus # Words FBIS FBIS newswire data Ch: 7.1M En: 9.1</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney, 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29 (1). pages 19-51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W J Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL &apos;02,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="29141" citStr="Papineni et al., 2002" startWordPosition="4748" endWordPosition="4751">on We then evaluate the effect of word alignment on machine translation quality using the phrase-based translation system Moses (Koehn et al., 2007). We take NIST MT03 test data as the development set, NIST MT05 test data as the testing set. We train a 5-gram language model with the Xinhua portion of English Gigaword corpus and the English side of the training set using the SRILM Toolkit (Stolcke, 2002). We train machine translation models using GDFA alignments of each system. BLEU scores on NIST MT05 are listed in Table 7, where BLEU scores are calculated using lowercased and tokenized data (Papineni et al., 2002). Although the IBM4-L0, Agree-HMM, Tree-Distance and Hard-Cohesion systems improve word alignment than IBM4, they fail to outperform the IBM4 system on machine translation. The BLEU score of our Soft-Cohesion-EM system is better than the IBM4 system when using the FBIS training set, but worse when using the LARGE training set. Our Soft-Cohesion-Gibbs system produces the best BLEU score when using both training sets. We also performed a statistical significance test using bootstrap resampling with 1000 samples (Koehn, 2004; Zhang et al., 2004). Experimental results show the Soft-Cohesion-Gibbs </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward and W.J. Zhu, 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. of ACL &apos;02, pages 311-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Pauls</author>
<author>Dan Klein</author>
<author>David Chiang</author>
<author>Kevin Knight</author>
</authors>
<title>Unsupervised Syntactic Alignment with Inversion Transduction Grammars.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL &apos;10.</booktitle>
<contexts>
<context position="2619" citStr="Pauls et al., 2010" startWordPosition="385" endWordPosition="388">d features have become increasingly popular (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010; Saers et al., 2010; Riesa et al., 2011). These models are usually trained with manually annotated parallel data. However, when moving to a new language pair, large amount of hand-aligned data are usually unavailable and expensive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency cohesion (Fox, 2002) into account, which assumes phrases dominated by disjoint dependency subtrees tend not to overlap after translation. Instead of treating dependency cohesion as a hard constraint (Lin and Cherry, 2003) or using it as a feature in discriminative models (Cherry and Lin, 2006b), we treat dependency cohesion as a distortion constraint, and integrate it into a modified HMM word alignment model to softly influence the probabilities of alignment candidates. We also propose an approximate EM algorithm and an explicit Gibbs sampling algorithm to t</context>
<context position="30701" citStr="Pauls et al. (2010)" startWordPosition="4971" endWordPosition="4974"> scores, where * indicates significantly better than IBM4 (p&lt;0.05). 6 Related Work There have been many proposals of integrating syntactic knowledge into generative alignment models. Wu (1997) proposed the inversion transduction grammar (ITG) to model word alignment as synchronous parsing for a sentence pair. Yamada and Knight (2001) represented translation as a sequence of re-ordering operations over child nodes of a syntactic tree. Gildea (2003) introduced a “loosely” tree-based alignment technique, which allows alignments to violate syntactic constraints by incurring a cost in probability. Pauls et al. (2010) gave a new instance of the ITG formalism, in which one side of the synchronous derivation is constrained by the syntactic tree. Fox (2002) measured syntactic cohesion in gold standard alignments and showed syntactic cohesion is generally maintained between English and French. She also compared three variant syntactic representations (phrase tree, verb phrase flattening tree and dependency tree), and found the dependency tree produced the highest degree of cohesion. So Cherry and Lin (2003; 2006a) used dependency cohesion as a hard constraint to restrict the alignment space, where all potentia</context>
</contexts>
<marker>Pauls, Klein, Chiang, Knight, 2010</marker>
<rawString>Adam Pauls, Dan Klein, David Chiang and Kevin Knight, 2010. Unsupervised Syntactic Alignment with Inversion Transduction Grammars. In Proc. of NAACL &apos;10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proc. of ACL</booktitle>
<contexts>
<context position="21946" citStr="Petrov et al., 2006" startWordPosition="3604" endWordPosition="3607"> whether our model is adaptable for large-scale task. For word alignment quality evaluation, we take the handaligned data sets from SSMT20072, which contains 2 http://nlp.ict.ac.cn/guidelines/guidelines-2007- SSMT(English).doc 505 sentence pairs in the testing set and 502 sentence pairs in the development set. Following Och and Ney (2003), we evaluate word alignment quality with the alignment error rate (AER), where lower AER is better. Because our model takes dependency trees as input, we parse both sides of the two training sets, the development set and the testing set with Berkeley parser (Petrov et al., 2006), and then convert the generated phrase trees into dependency trees according to Wang and Zong (2010; 2011). Our model is an asymmetric model, so we perform word alignment in both forward (Chinese4English) and reverse (English4Chinese) directions. Train Set Source Corpus # Words FBIS FBIS newswire data Ch: 7.1M En: 9.1M LARGE LDC2000T50, LDC2003E14, Ch: 27.6M LDC2003E07, LDC2004T07, En: 31.8M LDC2005T06, LDC2002L27, LDC2005T10, LDC2005T34 Table 2: The size and the source corpus of the two training sets. 5.1 Effectiveness of Cohesion Constraints In Eq. (3), the distortion probability 𝑝𝑑 is deco</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux and Dan Klein, 2006. Learning accurate, compact, and interpretable tree annotation. In Proc. of ACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Riesa</author>
<author>Daniel Marcu</author>
</authors>
<title>Hierarchical search for word alignment.</title>
<date>2010</date>
<booktitle>In Proc. of ACL &apos;10,</booktitle>
<pages>157--166</pages>
<contexts>
<context position="2100" citStr="Riesa and Marcu, 2010" startWordPosition="302" endWordPosition="305">he past two decades, generative word alignment models such as the IBM Models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) have been widely used, primarily because they are trained on bilingual sentences in an unsupervised manner and the implementation is freely available in the GIZA++ toolkit (Och and Ney, 2003). However, the word alignment quality of generative models is still far from satisfactory for SMT systems. In recent years, discriminative alignment models incorporating linguistically motivated features have become increasingly popular (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010; Saers et al., 2010; Riesa et al., 2011). These models are usually trained with manually annotated parallel data. However, when moving to a new language pair, large amount of hand-aligned data are usually unavailable and expensive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency cohesion (Fox, 2002) into account, which assu</context>
<context position="11002" citStr="Riesa and Marcu (2010)" startWordPosition="1761" endWordPosition="1764">ulated by walking through the phrase (or dependency) tree T. 3.2 Proposed Model To integrate dependency cohesion constraint into a generative model, we refine the sequence-based model in two ways with the help of the source side dependency tree 𝑇𝑓. First, we design a new word alignment order. In the sequence-based model, source words are aligned from left to right by taking source sentence as a linear sequence. However, to apply dependency cohesion constraint, the subtree span of a head node is computed based on the alignments of its children, so children must be aligned before the head node. Riesa and Marcu (2010) propose a hierarchical search procedure to traverse all nodes in a phrase structure tree. Similarly, we define a bottom-up topological order (BUT-order) to traverse all words in the source side dependency tree 𝑇𝑓 . In the BUT-order, tree nodes are aligned bottom-up with 𝑇𝑓 as a backbone. For all children under the same head node, left children are aligned from right to left, and then right children are aligned from left to right. For example, the BUT-order for the following dependency tree is “C B E F D A H G”. A B C D E F G H 293 For the sake of clarity, we define a function to map all nodes</context>
</contexts>
<marker>Riesa, Marcu, 2010</marker>
<rawString>Jason Riesa and Daniel Marcu, 2010. Hierarchical search for word alignment. In Proc. of ACL &apos;10, pages 157-166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Riesa</author>
<author>Ann Irvine</author>
<author>Daniel Marcu</author>
</authors>
<title>Feature-Rich Language-Independent Syntax-Based Alignment for Statistical Machine Translation.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP &apos;11.</booktitle>
<contexts>
<context position="2141" citStr="Riesa et al., 2011" startWordPosition="310" endWordPosition="313">ent models such as the IBM Models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) have been widely used, primarily because they are trained on bilingual sentences in an unsupervised manner and the implementation is freely available in the GIZA++ toolkit (Och and Ney, 2003). However, the word alignment quality of generative models is still far from satisfactory for SMT systems. In recent years, discriminative alignment models incorporating linguistically motivated features have become increasingly popular (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010; Saers et al., 2010; Riesa et al., 2011). These models are usually trained with manually annotated parallel data. However, when moving to a new language pair, large amount of hand-aligned data are usually unavailable and expensive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency cohesion (Fox, 2002) into account, which assumes phrases dominated by disjoint depende</context>
</contexts>
<marker>Riesa, Irvine, Marcu, 2011</marker>
<rawString>Jason Riesa, Ann Irvine and Daniel Marcu, 2011. Feature-Rich Language-Independent Syntax-Based Alignment for Statistical Machine Translation. In Proc. of EMNLP &apos;11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Darcey Riley</author>
<author>Daniel Gildea</author>
</authors>
<title>Improving the IBM Alignment Models Using Variational Bayes.</title>
<date>2012</date>
<booktitle>In Proc. of ACL &apos;12.</booktitle>
<contexts>
<context position="25142" citStr="Riley and Gildea, 2012" startWordPosition="4122" endWordPosition="4125"> of our model, we compare our model with some of the state-of-theart models. All the systems are listed as follows: 1) IBM4: The fertility-based model (IBM model 4) which is implemented in GIZA++ toolkit. The training scheme is 5 iterations of IBM model 1, 5 iterations of the HMM model and 10 iterations of IBM model 4. 2) IBM4-L0: A modification to the GIZA++ toolkit which extends IBM models with ℓ0 - norm (Vaswani et al., 2012). The training scheme is the same as IBM4. 3) IBM4-Prior: A modification to the GIZA++ toolkit which extends the translation model of IBM models with Dirichlet priors (Riley and Gildea, 2012). The training scheme is the same as IBM4. 4) Agree-HMM: The HMM alignment model by jointly training the forward and reverse models (Liang et al., 2006), which is implemented in the BerkeleyAligner. The training scheme is 5 iterations of jointly training IBM model 1 and 5 iterations of jointly training HMM model. 5) Tree-Distance: The tree distance alignment model proposed in DeNero and Klein (2007), which is implemented in the BerkeleyAligner. The training scheme is 5 iterations of jointly training IBM model 1 and 5 iterations of jointly training the tree distance model. 6) Hard-Cohesion: The</context>
</contexts>
<marker>Riley, Gildea, 2012</marker>
<rawString>Darcey Riley and Daniel Gildea, 2012. Improving the IBM Alignment Models Using Variational Bayes. In Proc. of ACL &apos;12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Saers</author>
<author>J Nivre</author>
<author>D Wu</author>
</authors>
<title>Word alignment with stochastic bracketing linear inversion transduction grammar.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL &apos;10,</booktitle>
<pages>341--344</pages>
<contexts>
<context position="2120" citStr="Saers et al., 2010" startWordPosition="306" endWordPosition="309">nerative word alignment models such as the IBM Models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) have been widely used, primarily because they are trained on bilingual sentences in an unsupervised manner and the implementation is freely available in the GIZA++ toolkit (Och and Ney, 2003). However, the word alignment quality of generative models is still far from satisfactory for SMT systems. In recent years, discriminative alignment models incorporating linguistically motivated features have become increasingly popular (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010; Saers et al., 2010; Riesa et al., 2011). These models are usually trained with manually annotated parallel data. However, when moving to a new language pair, large amount of hand-aligned data are usually unavailable and expensive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency cohesion (Fox, 2002) into account, which assumes phrases dominate</context>
</contexts>
<marker>Saers, Nivre, Wu, 2010</marker>
<rawString>M. Saers, J. Nivre and D. Wu, 2010. Word alignment with stochastic bracketing linear inversion transduction grammar. In Proc. of NAACL &apos;10, pages 341-344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM-an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In ICSLP &apos;02.</booktitle>
<contexts>
<context position="28925" citStr="Stolcke, 2002" startWordPosition="4715" endWordPosition="4716">d-Cohesion 35.04 37.59 37.63 Soft-Cohesion-EM 30.93 32.67 33.65 Soft-Cohesion-Gibbs 32.07 32.68 32.28 Table 6: AERs on the testing set (trained on the LARGE data set). 5.3 Machine Translation Quality Comparison We then evaluate the effect of word alignment on machine translation quality using the phrase-based translation system Moses (Koehn et al., 2007). We take NIST MT03 test data as the development set, NIST MT05 test data as the testing set. We train a 5-gram language model with the Xinhua portion of English Gigaword corpus and the English side of the training set using the SRILM Toolkit (Stolcke, 2002). We train machine translation models using GDFA alignments of each system. BLEU scores on NIST MT05 are listed in Table 7, where BLEU scores are calculated using lowercased and tokenized data (Papineni et al., 2002). Although the IBM4-L0, Agree-HMM, Tree-Distance and Hard-Cohesion systems improve word alignment than IBM4, they fail to outperform the IBM4 system on machine translation. The BLEU score of our Soft-Cohesion-EM system is better than the IBM4 system when using the FBIS training set, but worse when using the LARGE training set. Our Soft-Cohesion-Gibbs system produces the best BLEU s</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke, 2002. SRILM-an extensible language modeling toolkit. In ICSLP &apos;02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>S Lacoste-Julien</author>
<author>D Klein</author>
</authors>
<title>A discriminative matching approach to word alignment.</title>
<date>2005</date>
<booktitle>In Proc. of EMNLP &apos;05,</booktitle>
<pages>73--80</pages>
<contexts>
<context position="2077" citStr="Taskar et al., 2005" startWordPosition="298" endWordPosition="301"> al., 2004). During the past two decades, generative word alignment models such as the IBM Models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) have been widely used, primarily because they are trained on bilingual sentences in an unsupervised manner and the implementation is freely available in the GIZA++ toolkit (Och and Ney, 2003). However, the word alignment quality of generative models is still far from satisfactory for SMT systems. In recent years, discriminative alignment models incorporating linguistically motivated features have become increasingly popular (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010; Saers et al., 2010; Riesa et al., 2011). These models are usually trained with manually annotated parallel data. However, when moving to a new language pair, large amount of hand-aligned data are usually unavailable and expensive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency cohesion (Fox, 2002) i</context>
</contexts>
<marker>Taskar, Lacoste-Julien, Klein, 2005</marker>
<rawString>B. Taskar, S. Lacoste-Julien and D. Klein, 2005. A discriminative matching approach to word alignment. In Proc. of EMNLP &apos;05, pages 73-80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Vaswani</author>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Smaller alignment models for better translations: unsupervised word alignment with the l0 norm.</title>
<date>2012</date>
<booktitle>In Proc. ACL&apos;12,</booktitle>
<pages>311--319</pages>
<contexts>
<context position="24951" citStr="Vaswani et al., 2012" startWordPosition="4091" endWordPosition="4094">26.53 25.51 25.40 wd-hc-mc 23.63 25.17 24.65 24.33 Table 3: AERs on the development set (trained on the FBIS data set). 5.2 Comparison with State-of-the-Art Models To show the effectiveness of our model, we compare our model with some of the state-of-theart models. All the systems are listed as follows: 1) IBM4: The fertility-based model (IBM model 4) which is implemented in GIZA++ toolkit. The training scheme is 5 iterations of IBM model 1, 5 iterations of the HMM model and 10 iterations of IBM model 4. 2) IBM4-L0: A modification to the GIZA++ toolkit which extends IBM models with ℓ0 - norm (Vaswani et al., 2012). The training scheme is the same as IBM4. 3) IBM4-Prior: A modification to the GIZA++ toolkit which extends the translation model of IBM models with Dirichlet priors (Riley and Gildea, 2012). The training scheme is the same as IBM4. 4) Agree-HMM: The HMM alignment model by jointly training the forward and reverse models (Liang et al., 2006), which is implemented in the BerkeleyAligner. The training scheme is 5 iterations of jointly training IBM model 1 and 5 iterations of jointly training HMM model. 5) Tree-Distance: The tree distance alignment model proposed in DeNero and Klein (2007), which</context>
</contexts>
<marker>Vaswani, Huang, Chiang, 2012</marker>
<rawString>Ashish Vaswani, Liang Huang, and David Chiang, 2012. Smaller alignment models for better translations: unsupervised word alignment with the l0 norm. In Proc. ACL&apos;12, pages 311–319.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proc. of COLING-96,</booktitle>
<pages>836--841</pages>
<contexts>
<context position="1615" citStr="Vogel et al., 1996" startWordPosition="232" endWordPosition="235"> model achieves improvements in both alignment quality and translation quality. 1 Introduction Word alignment is the task of identifying word correspondences between parallel sentence pairs. Word alignment has become a vital component of statistical machine translation (SMT) systems, since it is required by almost all state-of-the-art SMT systems for the purpose of extracting phrase tables or even syntactic transformation rules (Koehn et al., 2007; Galley et al., 2004). During the past two decades, generative word alignment models such as the IBM Models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) have been widely used, primarily because they are trained on bilingual sentences in an unsupervised manner and the implementation is freely available in the GIZA++ toolkit (Och and Ney, 2003). However, the word alignment quality of generative models is still far from satisfactory for SMT systems. In recent years, discriminative alignment models incorporating linguistically motivated features have become increasingly popular (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010; Saers et al., 2010; Riesa et al., 2011). These models are usually trained with manually annotated parallel data. </context>
<context position="8452" citStr="Vogel et al., 1996" startWordPosition="1335" endWordPosition="1338">e describe an approach to integrating dependency cohesion constraint into a generative model to softly influence the probabilities of alignment candidates. We show that our new approach addresses the shortcomings of using dependency cohesion as a hard constraint. ch-en en-ch HCP MCP HCP MCP 88.43 95.82 81.53 91.62 Table 1: Cohesion percentages (%) of a manually annotated data set between Chinese and English. 3 A Generative Word Alignment Model with Dependency Cohesion Constraint The most influential generative word alignment models are the IBM Models 1-5 and the HMM model (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). These models can be classified into sequence-based models (IBM Models 1, 2 and HMM) and fertility-based models (IBM Models 3, 4 and 5). The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011). So we built a generative word alignment model with dependency cohesion constraint based on the sequence-based model. 3.1 The Sequence</context>
<context position="15270" citStr="Vogel et al., 1996" startWordPosition="2518" endWordPosition="2521">e distortion model 𝑝𝑑 into three terms as follows: 𝑝𝑑 (𝑎𝜋𝑗, 𝓱𝜋𝑗, 𝓶𝜋𝑗|𝒂[1,𝑗−1]) = 𝑝 (𝑎𝜋𝑗|𝒂[1,𝑗−1]) 𝑝 (𝓱𝜋𝑗 |𝒂[1,𝑗]) 𝑝 (𝓶𝜋𝑗 |𝒂[1,𝑗], 𝓱𝜋𝑗) ≈𝑝𝑤𝑑 (𝑎𝜋𝑗 |𝑎𝜋𝑗−1, 𝐼) 𝑝ℎ𝑐 (𝓱𝜋𝑗 |𝒂[1,𝑗]) 𝑝𝑚𝑐 (𝓶𝜋𝑗 |𝒂[1,𝑗]) where 𝑝 𝑤𝑑 is the words distance term, 𝑝ℎ𝑐 is the head-modifier cohesion term and 𝑝𝑚𝑐 is the modifier-modifier cohesion term. The word distance term 𝑝 𝑤𝑑 has been verified to be very useful in the HMM alignment model. However, in our model, the word distance is calculated based on the previous node in BUTorder rather than the previous word in the original sentence. We follow the HMM word alignment model (Vogel et al., 1996) and parameterize 𝑝 𝑤𝑑 in terms of the jump width: 𝑝𝑤𝑑(𝑖|𝑖′,𝐼) = 𝑐(𝑖−𝑖′) ∑ 𝑐(𝑖′′−𝑖′) 𝑖′′ (4) where 𝑐() is the count of jump width. 294 The head-modifier cohesion term 𝑝ℎ𝑐 is used to penalize the distortion probability according to relationships between the head node and its children (modifiers). Therefore, we define 𝑝ℎ𝑐 as the product of probabilities for all head-modifier pairs taking 𝑓𝜋𝑗 as head node: 𝑝ℎ𝑐 (𝓱𝜋𝑗|𝒂[1,𝑗]) = ∏ 𝒽𝜋𝑗,𝑐∈𝓱𝜋𝑗 𝑝ℎ (𝒽𝜋𝑗,𝑐|𝑓𝑐, 𝑒𝑎𝜋𝑗, 𝑒𝑎𝑐) (5) where 𝒽𝜋𝑗,𝑐 ∈ {𝑐𝑜ℎ𝑒𝑠𝑖𝑜𝑛, 𝑐𝑟𝑜𝑠𝑠𝑖𝑛𝑔} is the headmodifier cohesion relationship between 𝑓𝜋 𝑗 and one of its child 𝑓𝑐 , 𝑝ℎ is the corres</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney and Christoph Tillmann, 1996. HMM-based word alignment in statistical translation. In Proc. of COLING-96, pages 836-841.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<pages>377--403</pages>
<contexts>
<context position="2525" citStr="Wu, 1997" startWordPosition="371" endWordPosition="372">recent years, discriminative alignment models incorporating linguistically motivated features have become increasingly popular (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010; Saers et al., 2010; Riesa et al., 2011). These models are usually trained with manually annotated parallel data. However, when moving to a new language pair, large amount of hand-aligned data are usually unavailable and expensive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency cohesion (Fox, 2002) into account, which assumes phrases dominated by disjoint dependency subtrees tend not to overlap after translation. Instead of treating dependency cohesion as a hard constraint (Lin and Cherry, 2003) or using it as a feature in discriminative models (Cherry and Lin, 2006b), we treat dependency cohesion as a distortion constraint, and integrate it into a modified HMM word alignment model to softly influence the probabilities of alignment candida</context>
<context position="30274" citStr="Wu (1997)" startWordPosition="4912" endWordPosition="4913">004; Zhang et al., 2004). Experimental results show the Soft-Cohesion-Gibbs system is significantly better (p&lt;0.05) than the IBM4 system. The IBM4-Prior system slightly outperforms IBM4, but it’s not significant. FBIS LARGE IBM4 30.7 33.1 IBM4-L0 30.4 32.3 IBM4-Prior 30.9 33.2 Agree-HMM 27.2 30.1 Tree-Distance 28.2 N/A Hard-Cohesion 30.4 32.2 Soft-Cohesion-EM 30.9 33.1 Soft-Cohesion-Gibbs 31.6* 33.9* Table 7: BLEU scores, where * indicates significantly better than IBM4 (p&lt;0.05). 6 Related Work There have been many proposals of integrating syntactic knowledge into generative alignment models. Wu (1997) proposed the inversion transduction grammar (ITG) to model word alignment as synchronous parsing for a sentence pair. Yamada and Knight (2001) represented translation as a sequence of re-ordering operations over child nodes of a syntactic tree. Gildea (2003) introduced a “loosely” tree-based alignment technique, which allows alignments to violate syntactic constraints by incurring a cost in probability. Pauls et al. (2010) gave a new instance of the ITG formalism, in which one side of the synchronous derivation is constrained by the syntactic tree. Fox (2002) measured syntactic cohesion in go</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>D. Wu, 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23 (3). pages 377-403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiguo Wang</author>
</authors>
<title>Chengqing Zong,</title>
<date>2010</date>
<booktitle>In Proc. of COLING</booktitle>
<pages>1292--1300</pages>
<marker>Wang, 2010</marker>
<rawString>Zhiguo Wang, Chengqing Zong, 2010. Phrase Structure Parsing with Dependency Structure, In Proc. of COLING 2010, pages 1292-1300.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiguo Wang</author>
</authors>
<title>Chengqing Zong,</title>
<date>2011</date>
<booktitle>In Proc. Of IJCNLP</booktitle>
<pages>1251--1259</pages>
<marker>Wang, 2011</marker>
<rawString>Zhiguo Wang, Chengqing Zong, 2011. Parse Reranking Based on Higher-Order Lexical Dependencies, In Proc. Of IJCNLP 2011, pages 1251-1259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In Proc. of ACL &apos;01,</booktitle>
<pages>523--530</pages>
<contexts>
<context position="2550" citStr="Yamada and Knight, 2001" startWordPosition="373" endWordPosition="376">rs, discriminative alignment models incorporating linguistically motivated features have become increasingly popular (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010; Saers et al., 2010; Riesa et al., 2011). These models are usually trained with manually annotated parallel data. However, when moving to a new language pair, large amount of hand-aligned data are usually unavailable and expensive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency cohesion (Fox, 2002) into account, which assumes phrases dominated by disjoint dependency subtrees tend not to overlap after translation. Instead of treating dependency cohesion as a hard constraint (Lin and Cherry, 2003) or using it as a feature in discriminative models (Cherry and Lin, 2006b), we treat dependency cohesion as a distortion constraint, and integrate it into a modified HMM word alignment model to softly influence the probabilities of alignment candidates. We also propose an a</context>
<context position="30417" citStr="Yamada and Knight (2001)" startWordPosition="4931" endWordPosition="4934">4 system. The IBM4-Prior system slightly outperforms IBM4, but it’s not significant. FBIS LARGE IBM4 30.7 33.1 IBM4-L0 30.4 32.3 IBM4-Prior 30.9 33.2 Agree-HMM 27.2 30.1 Tree-Distance 28.2 N/A Hard-Cohesion 30.4 32.2 Soft-Cohesion-EM 30.9 33.1 Soft-Cohesion-Gibbs 31.6* 33.9* Table 7: BLEU scores, where * indicates significantly better than IBM4 (p&lt;0.05). 6 Related Work There have been many proposals of integrating syntactic knowledge into generative alignment models. Wu (1997) proposed the inversion transduction grammar (ITG) to model word alignment as synchronous parsing for a sentence pair. Yamada and Knight (2001) represented translation as a sequence of re-ordering operations over child nodes of a syntactic tree. Gildea (2003) introduced a “loosely” tree-based alignment technique, which allows alignments to violate syntactic constraints by incurring a cost in probability. Pauls et al. (2010) gave a new instance of the ITG formalism, in which one side of the synchronous derivation is constrained by the syntactic tree. Fox (2002) measured syntactic cohesion in gold standard alignments and showed syntactic cohesion is generally maintained between English and French. She also compared three variant syntac</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight, 2001. A syntax-based statistical translation model. In Proc. of ACL &apos;01, pages 523-530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhang</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Interpreting BLEU/NIST scores: How much improvement do we need to have a better system?</title>
<date>2004</date>
<booktitle>In Proc. of LREC.</booktitle>
<contexts>
<context position="29689" citStr="Zhang et al., 2004" startWordPosition="4830" endWordPosition="4833"> calculated using lowercased and tokenized data (Papineni et al., 2002). Although the IBM4-L0, Agree-HMM, Tree-Distance and Hard-Cohesion systems improve word alignment than IBM4, they fail to outperform the IBM4 system on machine translation. The BLEU score of our Soft-Cohesion-EM system is better than the IBM4 system when using the FBIS training set, but worse when using the LARGE training set. Our Soft-Cohesion-Gibbs system produces the best BLEU score when using both training sets. We also performed a statistical significance test using bootstrap resampling with 1000 samples (Koehn, 2004; Zhang et al., 2004). Experimental results show the Soft-Cohesion-Gibbs system is significantly better (p&lt;0.05) than the IBM4 system. The IBM4-Prior system slightly outperforms IBM4, but it’s not significant. FBIS LARGE IBM4 30.7 33.1 IBM4-L0 30.4 32.3 IBM4-Prior 30.9 33.2 Agree-HMM 27.2 30.1 Tree-Distance 28.2 N/A Hard-Cohesion 30.4 32.2 Soft-Cohesion-EM 30.9 33.1 Soft-Cohesion-Gibbs 31.6* 33.9* Table 7: BLEU scores, where * indicates significantly better than IBM4 (p&lt;0.05). 6 Related Work There have been many proposals of integrating syntactic knowledge into generative alignment models. Wu (1997) proposed the i</context>
</contexts>
<marker>Zhang, Vogel, Waibel, 2004</marker>
<rawString>Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. Interpreting BLEU/NIST scores: How much improvement do we need to have a better system? In Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shaojun Zhao</author>
<author>Daniel Gildea</author>
</authors>
<title>A fast fertility hidden Markov model for word alignment using MCMC.</title>
<date>2010</date>
<booktitle>In Proc. of EMNLP &apos;10,</booktitle>
<pages>596--605</pages>
<contexts>
<context position="8895" citStr="Zhao and Gildea, 2010" startWordPosition="1402" endWordPosition="1405">t Model with Dependency Cohesion Constraint The most influential generative word alignment models are the IBM Models 1-5 and the HMM model (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). These models can be classified into sequence-based models (IBM Models 1, 2 and HMM) and fertility-based models (IBM Models 3, 4 and 5). The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011). So we built a generative word alignment model with dependency cohesion constraint based on the sequence-based model. 3.1 The Sequence-based Alignment Model According to Brown et al. (1993) and Och and Ney (2003), the sequence-based model is built as a noisy channel model, where the source sentence 𝒇1𝐽 and the alignment 𝒂1𝐽 are generated conditioning on the target sentence 𝒆1𝐼 . The model assumes each source word is assigned to exactly one target word, and defines an asymmetric alignment for the sentence pair as 𝒂1 𝐽 = 𝑎1, 𝑎2, ... , 𝑎𝑗, ... , 𝑎𝐽, where each 𝑎𝑗 ∈ [0, 𝐼] i</context>
<context position="19148" citStr="Zhao and Gildea (2010)" startWordPosition="3152" endWordPosition="3155">son, 2008), there are two types of Gibbs samplers: explicit and collapsed. An explicit sampler represents and samples the model parameters in addition to the word alignments, while in a collapsed sampler the parameters are integrated out and only alignments are sampled. Mermer and Saraçlar (2011) proposed a collapsed sampler for IBM Model 1. However, their sampler updates parameters constantly and thus cannot run efficiently on large-scale tasks. Instead, we take advantage of explicit Gibbs sampling to make a highly parallelizable sampler. Our Gibbs sampler is similar to the MCMC algorithm in Zhao and Gildea (2010), but we assume Dirichlet priors when sampling model parameters and take a different sampling approach based on the source side dependency tree. Our sampler performs a sequence of consecutive iterations. Each iteration consists of two sampling steps. The first step samples the aligned position for each dependency node according to the BUTorder. Concretely, when sampling the aligned 295 position 𝑎𝜋𝑗 (𝑡+1) for node 𝑓𝜋𝑗 on iteration 𝑡+1, the aligned positions for 𝒂[1,𝑗−1] are fixed on the new sampling results 𝒂[1,𝑗−1] (𝑡+1) on iteration 𝑡+1, and the aligned positions for 𝒂[𝑗+1,𝐽] are fixed on the</context>
</contexts>
<marker>Zhao, Gildea, 2010</marker>
<rawString>Shaojun Zhao and Daniel Gildea, 2010. A fast fertility hidden Markov model for word alignment using MCMC. In Proc. of EMNLP &apos;10, pages 596-605.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>