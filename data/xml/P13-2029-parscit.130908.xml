<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000717">
<title confidence="0.985206">
Automatic Coupling of Answer Extraction and Information Retrieval
</title>
<author confidence="0.979487">
Xuchen Yao and Benjamin Van Durme Peter Clark
</author>
<affiliation confidence="0.970914">
Johns Hopkins University Vulcan Inc.
</affiliation>
<address confidence="0.56436">
Baltimore, MD, USA Seattle, WA, USA
</address>
<sectionHeader confidence="0.949025" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999105235294118">
Information Retrieval (IR) and Answer
Extraction are often designed as isolated
or loosely connected components in Ques-
tion Answering (QA), with repeated over-
engineering on IR, and not necessarily per-
formance gain for QA. We propose to
tightly integrate them by coupling auto-
matically learned features for answer ex-
traction to a shallow-structured IR model.
Our method is very quick to implement,
and significantly improves IR for QA
(measured in Mean Average Precision and
Mean Reciprocal Rank) by 10%-20%
against an uncoupled retrieval baseline
in both document and passage retrieval,
which further leads to a downstream 20%
improvement in QA F1.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999978882352941">
The overall performance of a Question Answer-
ing system is bounded by its Information Re-
trieval (IR) front end, resulting in research specif-
ically on Information Retrieval for Question An-
swering (IR4QA) (Greenwood, 2008; Sakai et al.,
2010). Common approaches such as query expan-
sion, structured retrieval, and translation models
show patterns of complicated engineering on the
IR side, or isolate the upstream passage retrieval
from downstream answer extraction. We argue
that: 1. an IR front end should deliver exactly
what a QA1 back end needs; 2. many intuitions
employed by QA should be and can be re-used in
IR, rather than re-invented. We propose a coupled
retrieval method with prior knowledge of its down-
stream QA component, that feeds QA with exactly
the information needed.
</bodyText>
<footnote confidence="0.810751">
1After this point in the paper we use the term QA in a
narrow sense: QA without the IR component, i.e., answer
extraction.
</footnote>
<bodyText confidence="0.993021473684211">
As a motivating example, using the ques-
tion When was Alaska purchased from
the TREC 2002 QA track as the query to the In-
dri search engine, the top sentence retrieved from
the accompanying AQUAINT corpus is:
Eventually Alaska Airlines will
allow all travelers who have
purchased electronic tickets
through any means.
While this relates Alaska and purchased, it
is not a useful passage for the given question.2 It
is apparent that the question asks for a date. Prior
work proposed predictive annotation (Prager et al.,
2000; Prager et al., 2006): text is first annotated in
a predictive manner (of what types of questions it
might answer) with 20 answer types and then in-
dexed. A question analysis component (consisting
of 400 question templates) maps the desired an-
swer type to one of the 20 existing answer types.
Retrieval is then performed with both the question
and predicated answer types in the query.
However, predictive annotation has the limita-
tion of being labor intensive and assuming the un-
derlying NLP pipeline to be accurate. We avoid
these limitations by directly asking the down-
stream QA system for the information about which
entities answer which questions, via two steps:
1. reusing the question analysis components from
QA; 2. forming a query based on the most relevant
answer features given a question from the learned
QA model. There is no query-time overhead and
no manual template creation. Moreover, this ap-
proach is more robust against, e.g., entity recog-
nition errors, because answer typing knowledge is
learned from how the data was actually labeled,
not from how the data was assumed to be labeled
(e.g., manual templates usually assume perfect la-
beling of named entities, but often it is not the case
</bodyText>
<footnote confidence="0.996942">
2Based on a non-optimized IR configuration, none of the
top 1000 returned passages contained the correct answer:
1867.
</footnote>
<page confidence="0.954918">
159
</page>
<bodyText confidence="0.971115240740741">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 159–165,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
in practice).
We use our statistically-trained QA system (Yao
et al., 2013) that recognizes the association be-
tween question type and expected answer types
through various features. The QA system employs
a linear chain Conditional Random Field (CRF)
(Lafferty et al., 2001) and tags each token as either
an answer (ANS) or not (O). This will be our off-
the-shelf QA system, which recognizes the associ-
ation between question type and expected answer
types through various features based on e.g., part-
of-speech tagging (POS) and named entity recog-
nition (NER).
With weights optimized by CRF training (Ta-
ble 1), we can learn how answer features are cor-
related with question features. These features,
whose weights are optimized by the CRF train-
ing, directly reflect what the most important an-
swer types associated with each question type are.
For instance, line 2 in Table 1 says that if there is a
when question, and the current token’s NER label
is DATE, then it is likely that this token is tagged
as ANS. IR can easily make use of this knowledge:
for a when question, IR retrieves sentences with
tokens labeled as DATE by NER, or POS tagged as
CD. The only extra processing is to pre-tag and
index the text with POS and NER labels. The ana-
lyzing power of discriminative answer features for
IR comes for free from a trained QA system. Un-
like predictive annotation, statistical evidence de-
termines the best answer features given the ques-
tion, with no manual pattern or templates needed.
To compare again predictive annotation with
our approach: predictive annotation works in a
forward mode, downstream QA is tailored for up-
stream IR, i.e., QA works on whatever IR re-
trieves. Our method works in reverse (backward):
downstream QA dictates upstream IR, i.e., IR re-
trieves what QA wants. Moreover, our approach
extends easily beyond fixed answer types such as
named entities: we are already using POS tags as a
demonstration. We can potentially use any helpful
answer features in retrieval. For instance, if the
QA system learns that in order to is highly
correlated with why question through lexicalized
features, or some certain dependency relations are
helpful in answering questions with specific struc-
tures, then it is natural and easy for the IR compo-
nent to incorporate them.
There is also a distinction between our method
and the technique of learning to rank applied in
feature label weight
</bodyText>
<equation confidence="0.693491333333333">
qword=whenjPOSo=CD ANS 0.86
qword=whenjNER0=DATE ANS 0.79
qword=whenjPOSo=CD O -0.74
</equation>
<tableCaption confidence="0.968712">
Table 1: Learned weights for sampled features with respect
</tableCaption>
<bodyText confidence="0.992409230769231">
to the label of current token (indexed by [0]) in a CRF. The
larger the weight, the more “important” is this feature to help
tag the current token with the corresponding label. For in-
stance, line 1 says when answering a when question, and
the POS of current token is CD (cardinal number), it is likely
(large weight) that the token is tagged as ANS.
QA (Bilotti et al., 2010; Agarwal et al., 2012). Our
method is a QA-driven approach that provides su-
pervision for IR from a learned QA model, while
learning to rank is essentially an IR-driven ap-
proach: the supervision for IR comes from a la-
beled ranking list of retrieval results.
Overall, we make the following contributions:
</bodyText>
<listItem confidence="0.9847685">
• Our proposed method tightly integrates QA
with IR and the reuse of analysis from QA does
not put extra overhead on the IR queries. This
QA-driven approach provides a holistic solution
to the task of IR4QA.
• We learn statistical evidence about what the
</listItem>
<bodyText confidence="0.979073222222222">
form of answers to different questions look like,
rather than using manually authored templates.
This provides great flexibility in using answer
features in IR queries.
We give a full spectrum evaluation of all three
stages of IR+QA: document retrieval, passage re-
trieval and answer extraction, to examine thor-
oughly the effectiveness of the method.3 All of
our code and datasets are publicly available.4
</bodyText>
<sectionHeader confidence="0.952148" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999004230769231">
Besides Predictive Annotation, our work is closest
to structured retrieval, which covers techniques of
dependency path mapping (Lin and Pantel, 2001;
Cui et al., 2005; Kaisser, 2012), graph matching
with Semantic Role Labeling (Shen and Lapata,
2007) and answer type checking (Pinchak et al.,
2009), etc. Specifically, Bilotti et al. (2007) pro-
posed indexing text with their semantic roles and
named entities. Queries then include constraints
of semantic roles and named entities for the pred-
icate and its arguments in the question. Improve-
ments in recall of answer-bearing sentences were
shown over the bag-of-words baseline. Zhao and
</bodyText>
<footnote confidence="0.999875">
3Rarely are all three aspects presented in concert (see §2).
4http://code.google.com/p/jacana/
</footnote>
<page confidence="0.995537">
160
</page>
<bodyText confidence="0.999877666666667">
Callan (2008) extended this work with approx-
imate matching and smoothing. Most research
uses parsing to assign deep structures. Com-
pared to shallow (POS, NER) structured retrieval,
deep structures need more processing power and
smoothing, but might also be more precise. 5
Most of the above (except Kaisser (2012)) only
reported on IR or QA, but not both, assuming that
improvement in one naturally improves the other.
Bilotti and Nyberg (2008) challenged this assump-
tion and called for tighter coupling between IR and
QA. This paper is aimed at that challenge.
</bodyText>
<sectionHeader confidence="0.978232" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.999273444444444">
Table 1 already shows some examples of features
associating question types with answer types. We
store the features and their learned weights from
the trained model for IR usage.
We let the trained QA system guide the query
formulation when performing coupled retrieval
with Indri (Strohman et al., 2005), given a corpus
already annotated with POS tags and NER labels.
Then retrieval runs in four steps (Figure 1):
</bodyText>
<listItem confidence="0.957755230769231">
1. Question Analysis. The question analysis com-
ponent from QA is reused here. In this imple-
mentation, the only information we have cho-
sen to use from the question is the question
word (e.g., how, who) and the lexical answer
types (LAT) in case of what/which questions.
2. Answer Feature Selection. Given the question
word, we select the 5 highest weighted features
(e.g., POS[0]=CD for a when question).
3. Query Formulation. The original question is
combined with the top features as the query.
4. Coupled Retrieval. Indri retrieves a ranked list
of documents or passages.
</listItem>
<bodyText confidence="0.988727179104478">
As motivated in the introduction, this framework
is aimed at providing the following benefits:
Reuse of QA components on the IR side. IR
reuses both code for question analysis and top
weighted features from QA.
Statistical selection of answer features. For in-
stance, the NER tagger we used divides location
into two categories: GPE (geo locations) and LOC
5Ogilvie (2010) showed in chapter 4.3 that keyword and
named entities based retrieval actually outperformed SRL-
based structured retrieval in MAP for the answer-bearing sen-
tence retrieval task in their setting. In this paper we do not
intend to re-invent another parse-based structure matching al-
gorithm, but only use shallow structures to show the idea of
coupling QA with IR; in the future this might be extended to
incorporate “deeper” structure.
(non-GPE ). Both of them are learned to be impor-
tant to where questions.
Error tolerance along the NLP pipeline. IR
and QA share the same processing pipeline. Sys-
tematic errors made by the processing tools are
tolerated, in the sense that if the same pre-
processing error is made on both the question
and sentence, an answer may still be found.
Take the previous where question, besides
NER[0]=GPE and NER[0]=LOC, we also found
oddly NER[0]=PERSON an important feature, due
to that the NER tool sometimes mistakes PERSON
for LOC. For instance, the volcano name Mauna
Loa is labeled as a PERSON instead of a LOC. But
since the importance of this feature is recognized
by downstream QA, the upstream IR is still moti-
vated to retrieve it.
Queries were lightly optimized using the fol-
lowing strategies:
Query Weighting In practice query words are
weighted:
#weight(1.0 When 1.0 was 1.0 Alaska 1.0 purchased
α #max(#any:CD #any:DATE))
with a weight α for the answer types tuned via
cross-validation.
Since NER and POS tags are not lexicalized
they accumulate many more counts (i.e. term fre-
quency) than individual words, thus we in gen-
eral downweight by setting α &lt; 1.0, giving the
expected answer types “enough say” but not “too
much say”:
NER Types First We found NER labels better in-
dicators of expected answer types than POS tags.
The reasons are two-fold: 1. In general POS tags
are too coarse-grained in answer types than NER
labels. E.g., NNP can answer who and where
questions, but is not as precise as PERSON and
GPE. 2. POS tags accumulate even more counts
than NER labels, thus they need separate down-
weighting. Learning the interplay of these weights
in a joint IR/QA model, is an interesting path for
future work. If the top-weighted features are based
on NER, then we do not include POS tags for that
question. Otherwise POS tags are useful, for in-
stance, in answering how questions.
Unigram QA Model The QA system uses up to
trigram features (Table 1 shows examples of uni-
gram and bigram features). Thus it is able to learn,
for instance, that a POS sequence of IN CD NNS
is likely an answer to a when question (such as:
in 5 years). This requires that the IR queries
</bodyText>
<page confidence="0.979255">
161
</page>
<figure confidence="0.700538">
When was Alaska purchased?
</figure>
<figureCaption confidence="0.984828">
Figure 1: Coupled retrieval with queries directly con-
structed from highest weighted features of downstream QA.
The retrieved and ranked list of sentences is POS and NER
tagged, but only query-relevant tags are shown due to space
limit. A bag-of-words retrieval approach would have the sen-
tence shown above at rank 50 at its top position instead.
</figureCaption>
<bodyText confidence="0.999954285714286">
look for a consecutive IN CD NNS sequence. We
drop this strict constraint (which may need further
smoothing) and only use unigram features, not by
simply extracting “good” unigram features from
the trained model, but by re-training the model
with only unigram features. In answer extraction,
we still use up to trigram features. 6
</bodyText>
<sectionHeader confidence="0.999733" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9993095">
We want to measure and compare the performance
of the following retrieval techniques:
</bodyText>
<listItem confidence="0.996993692307692">
1. uncoupled retrieval with an off-the-shelf IR en-
gine by using the question as query (baseline),
2. QA-driven coupled retrieval (proposed), and
3. answer-bearing retrieval by using both the
question and known answer as query, only eval-
uated for answer extraction (upper bound),
at the three stages of question answering:
1. Document retrieval (for relevant docs from cor-
pus), measured by Mean Average Precision
(MAP) and Mean Reciprocal Rank (MRR).
2. Passage retrieval (finding relevant sentences
from the document), also by MAP and MRR.
3. Answer extraction, measured by Fl.
</listItem>
<footnote confidence="0.982902875">
6This is because the weights of unigram to trigram fea-
tures in a loglinear CRF model is a balanced consequence for
maximization. A unigram feature might end up with lower
weight because another trigram containing this unigram gets
a higher weight. Then we would have missed this feature
if we only used top unigram features. Thus we re-train the
model with only unigram features to make sure weights are
“assigned properly” among only unigram features.
</footnote>
<table confidence="0.8595306">
questions sentences
set
#all #pos. #all #pos.
TRAIN 2205 1756 (80%) 22043 7637 (35%)
TESTgold 99 88 (89%) 990 368 (37%)
</table>
<tableCaption confidence="0.9859935">
Table 2: Statistics for AMT-collected data (total cost was
around $800 for paying three Turkers per sentence). Positive
questions are those with an answer found. Positive sentences
are those bearing an answer.
</tableCaption>
<bodyText confidence="0.9963885">
All coupled and uncoupled queries are performed
with Indri v5.3 (Strohman et al., 2005).
</bodyText>
<subsectionHeader confidence="0.9792">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.99958803030303">
Test Set for IR and QA The MIT109 test col-
lection by Lin and Katz (2006) contains 109
questions from TREC 2002 and provides a near-
exhaustive judgment of relevant documents for
each question. We removed 10 questions that do
not have an answer by matching the TREC answer
patterns. Then we call this test set MIT99.
Training Set for QA We used Amazon Mechani-
cal Turk to collect training data for the QA system
by issuing answer-bearing queries for TREC1999-
2003 questions. For the top 10 retrieved sen-
tences for each question, three Turkers judged
whether each sentence contained the answer. The
inter-coder agreement rate was 0.81 (Krippen-
dorff, 2004; Artstein and Poesio, 2008).
The 99 questions of MIT99 were extracted from
the Turk collection as our TESTgold with the re-
maining as TRAIN, with statistics shown in Table
2. Note that only 88 questions out of MIT99 have
an answer from the top 10 query results.
Finally both the training and test data were
sentence-segmented and word-tokenized by
NLTK (Bird and Loper, 2004), dependency-
parsed by the Stanford Parser (Klein and
Manning, 2003), and NER-tagged by the Illinois
Named Entity Tagger (Ratinov and Roth, 2009)
with an 18-label type set.
Corpus Preprocessing for IR The AQUAINT
(LDC2002T31) corpus, on which the MIT99
questions are based, was processed in exactly the
same manner as was the QA training set. But
only sentence boundaries, POS tags and NER la-
bels were kept as the annotation of the corpus.
</bodyText>
<subsectionHeader confidence="0.991183">
4.2 Document and Passage Retrieval
</subsectionHeader>
<bodyText confidence="0.999897">
We issued uncoupled queries consisting of ques-
tion words, and QA-driven coupled queries con-
sisting of both the question and expected answer
types, then retrieved the top 1000 documents, and
</bodyText>
<figure confidence="0.97562584">
1
2
...
50
2. Get top weighted
features w.r.t qword
(from trained QA model)
1. Simple question analysis
(reuse from QA)
qword=when|POS[0]=CD → ANS: 0.86
qword=when|NER[0]=DATE → ANS: 0.79
...
qword=when
3. Query formulation
4. Coupled retrieval
#combine(Alaska purchased
#max(#any:CD #any:DATE))
On &lt;DATE&gt;March 30, &lt;CD&gt; 1867 &lt;/CD&gt; &lt;/DATE&gt;,
U.S. ... reached agreement ... to purchase ... Alaska ...
The islands were sold to the United States in
&lt;CD&gt;1867&lt;/CD&gt; with the purchase of Alaska.
...
...
Eventually Alaska Airlines will allow all travelers who
have purchased electronic tickets ...
</figure>
<page confidence="0.895322">
162
</page>
<table confidence="0.82724475">
coupled uncoupled
MAP MRR MAP MRR
document 0.2524 0.4835 0.2110 0.4298
sentence 0.1375 0.2987 0.1200 0.2544
</table>
<tableCaption confidence="0.9798085">
Table 3: Coupled vs. uncoupled document/sentence re-
trieval in MAP and MRR on MIT99. Significance level
(Smucker et al., 2007) for both MAP: P &lt; 0.001 and for
both MRR: P &lt; 0.05.
</tableCaption>
<bodyText confidence="0.999207571428571">
finally computed MAP and MRR against the gold-
standard MIT99 per-document judgment.
To find the best weighting α for coupled re-
trieval, we used 5-fold cross-validation and final-
ized at α = 0.1. Table 3 shows the results.
Coupled retrieval outperforms (20% by MAP with
p &lt; 0.001 and 12% by MRR with p &lt; 0.01) un-
coupled retrieval significantly according to paired
randomization test (Smucker et al., 2007).
For passage retrieval, we extracted relevant sin-
gle sentences. Recall that MIT99 only contains
document-level judgment. To generate a test set
for sentence retrieval, we matched each sentence
from relevant documents provided by MIT99 for
each question against the TREC answer patterns.
We found no significant difference between re-
trieving sentences from the documents returned
by document retrieval or directly from the corpus.
Numbers of the latter are shown in Table 3. Still,
coupled retrieval is significantly better by about
10% in MAP and 17% in MRR.
</bodyText>
<subsectionHeader confidence="0.999546">
4.3 Answer Extraction
</subsectionHeader>
<bodyText confidence="0.999945875">
Lastly we sent the sentences to the downstream
QA engine (trained on TRAIN) and computed F1
per K for the top K retrieved sentences, 7 shown
in Figure 2. The best F1 with coupled sentence re-
trieval is 0.231, 20% better than F1 of 0.192 with
uncoupled retrieval, both at K = 1.
The two descending lines at the bottom reflect
the fact that the majority-voting mechanism from
the QA system was too simple: F1 drops as K in-
creases. Thus we also computed F1’s assuming
perfect voting: a voting oracle that always selects
the correct answer as long as the QA system pro-
duces one, thus the two ascending lines in the cen-
ter of Figure 2. Still, F1 with coupled retrieval is
always better: reiterating the fact that coupled re-
trieval covers more answer-bearing sentences.
</bodyText>
<footnote confidence="0.67320875">
7Lin (2007), Zhang et al. (2007), and Kaisser (2012) also
evaluated on MIT109. However their QA engines used web-
based search engines, thus leading to results that are neither
reproducible nor directly comparable with ours.
</footnote>
<bodyText confidence="0.999106375">
Finally, to find the upper bound for QA, we
drew the two upper lines, testing on TESTgold de-
scribed in Table 2. The test sentences were ob-
tained with answer-bearing queries. This is as-
suming almost perfect IR. The gap between the
top two and other lines signals more room for im-
provements for IR in terms of better coverage and
better rank for answer-bearing sentences.
</bodyText>
<figure confidence="0.631173">
Top K Sentences Retrieved
</figure>
<figureCaption confidence="0.861679">
Figure 2: F1 values for answer extraction on MIT99. Best
</figureCaption>
<bodyText confidence="0.7524725">
F1’s for each method are parenthesized in the legend. “Or-
acle” methods assumed perfect voting of answer candidates
(a question is answered correctly if the system ever produced
one correct answer for it). “Gold” was tested on TESTgold.
</bodyText>
<sectionHeader confidence="0.993542" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999991181818182">
We described a method to perform coupled in-
formation retrieval with a prior knowledge of the
downstream QA system. Specifically, we coupled
IR queries with automatically learned answer fea-
tures from QA and observed significant improve-
ments in document/passage retrieval and boosted
F1 in answer extraction. This method has the mer-
its of not requiring hand-built question and answer
templates and being flexible in incorporating vari-
ous answer features automatically learned and op-
timized from the downstream QA system.
</bodyText>
<sectionHeader confidence="0.969853" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9936215">
We thank Vulcan Inc. for funding this work. We
also thank Paul Ogilvie, James Mayfield, Paul Mc-
Namee, Jason Eisner and the three anonymous re-
viewers for insightful comments.
</bodyText>
<figure confidence="0.999119">
0.8
0.7
Gold Oracle (0.755)
Gold (0.596)
Coupled Oracle (0.609)
Uncoupled Oracle (0.569)
0.6
0.5
0.4
0.3
0.2
F1
0.1
Coupled (0.231)
Uncoupled (0.192)
0.0
type
</figure>
<page confidence="0.9961">
163
</page>
<sectionHeader confidence="0.988546" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999338">
Arvind Agarwal, Hema Raghavan, Karthik Subbian,
Prem Melville, Richard D. Lawrence, David C.
Gondek, and James Fan. 2012. Learning to rank
for robust question answering. In Proceedings of
the 21st ACM international conference on Informa-
tion and knowledge management, CIKM ’12, pages
833–842, New York, NY, USA. ACM.
Ron Artstein and Massimo Poesio. 2008. Inter-Coder
Agreement for Computational Linguistics. Compu-
tational Linguistics, 34(4):555–596.
M.W. Bilotti and E. Nyberg. 2008. Improving text
retrieval precision and answer accuracy in question
answering systems. In Coling 2008: Proceedings
of the 2nd workshop on Information Retrieval for
Question Answering, pages 1–8.
M.W. Bilotti, P. Ogilvie, J. Callan, and E. Nyberg.
2007. Structured retrieval for question answer-
ing. In Proceedings of the 30th annual international
ACM SIGIR conference on Research and develop-
ment in information retrieval, pages 351–358. ACM.
M.W. Bilotti, J. Elsas, J. Carbonell, and E. Nyberg.
2010. Rank learning for factoid question answer-
ing with linguistic and semantic constraints. In Pro-
ceedings of the 19th ACM international conference
on Information and knowledge management, pages
459–468. ACM.
Steven Bird and Edward Loper. 2004. Nltk: The nat-
ural language toolkit. In The Companion Volume to
the Proceedings of 42st Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 214–
217, Barcelona, Spain, July.
Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and
Tat-Seng Chua. 2005. Question answering passage
retrieval using dependency relations. In Proceed-
ings of the 28th annual international ACM SIGIR
conference on Research and development in infor-
mation retrieval, SIGIR ’05, pages 400–407, New
York, NY, USA. ACM.
Mark A. Greenwood, editor. 2008. Coling 2008: Pro-
ceedings of the 2nd workshop on Information Re-
trieval for Question Answering. Coling 2008 Orga-
nizing Committee, Manchester, UK, August.
Michael Kaisser. 2012. Answer Sentence Retrieval by
Matching Dependency Paths acquired from Ques-
tion/Answer Sentence Pairs. In EACL, pages 88–98.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In In Proc. the 41st An-
nual Meeting of the Association for Computational
Linguistics.
Klaus H. Krippendorff. 2004. Content Analysis: An
Introduction to Its Methodology. Sage Publications,
Inc, 2nd edition.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning, ICML
’01, pages 282–289, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.
J. Lin and B. Katz. 2006. Building a reusable test
collection for question answering. Journal of the
American Society for Information Science and Tech-
nology, 57(7):851–861.
D. Lin and P. Pantel. 2001. Discovery of inference
rules for question-answering. Natural Language
Engineering, 7(4):343–360.
Jimmy Lin. 2007. An exploration of the principles un-
derlying redundancy-based factoid question answer-
ing. ACM Trans. Inf. Syst., 25(2), April.
P. Ogilvie. 2010. Retrieval using Document Struc-
ture and Annotations. Ph.D. thesis, Carnegie Mellon
University.
Christopher Pinchak, Davood Rafiei, and Dekang Lin.
2009. Answer typing for information retrieval. In
Proceedings of the 18th ACM conference on In-
formation and knowledge management, CIKM ’09,
pages 1955–1958, New York, NY, USA. ACM.
John Prager, Eric Brown, Anni Coden, and Dragomir
Radev. 2000. Question-answering by predictive an-
notation. In Proceedings of the 23rd annual inter-
national ACM SIGIR conference on Research and
development in information retrieval, SIGIR ’00,
pages 184–191, New York, NY, USA. ACM.
J. Prager, J. Chu-Carroll, E. Brown, and K. Czuba.
2006. Question answering by predictive annota-
tion. Advances in Open Domain Question Answer-
ing, pages 307–347.
L. Ratinov and D. Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
CoNLL, 6.
Tetsuya Sakai, Hideki Shima, Noriko Kando, Rui-
hua Song, Chuan-Jie Lin, Teruko Mitamura, Miho
Sugimito, and Cheng-Wei Lee. 2010. Overview
of the ntcir-7 aclia ir4qa task. In Proceedings of
NTCIR-8 Workshop Meeting, Tokyo, Japan.
D. Shen and M. Lapata. 2007. Using semantic roles
to improve question answering. In Proceedings of
EMNLP-CoNLL, pages 12–21.
M.D. Smucker, J. Allan, and B. Carterette. 2007. A
comparison of statistical significance tests for in-
formation retrieval evaluation. In Proceedings of
the sixteenth ACM conference on Conference on in-
formation and knowledge management, pages 623–
632. ACM.
</reference>
<page confidence="0.985083">
164
</page>
<reference confidence="0.999505842105263">
T. Strohman, D. Metzler, H. Turtle, and W.B. Croft.
2005. Indri: A language model-based search engine
for complex queries. In Proceedings of the Interna-
tional Conference on Intelligent Analysis, volume 2,
pages 2–6. Citeseer.
Xuchen Yao, Benjamin Van Durme, Peter Clark, and
Chris Callison-Burch. 2013. Answer Extraction as
Sequence Tagging with Tree Edit Distance. In Pro-
ceedings of NAACL 2013.
Xian Zhang, Yu Hao, Xiaoyan Zhu, Ming Li, and
David R. Cheriton. 2007. Information distance
from a question to an answer. In Proceedings of
the 13th ACM SIGKDD international conference on
Knowledge discovery and data mining, KDD ’07,
pages 874–883, New York, NY, USA. ACM.
L. Zhao and J. Callan. 2008. A generative retrieval
model for structured documents. In Proceedings of
the 17th ACM conference on Information and knowl-
edge management, pages 1163–1172. ACM.
</reference>
<page confidence="0.998737">
165
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.774685">
<title confidence="0.99859">Automatic Coupling of Answer Extraction and Information Retrieval</title>
<author confidence="0.999312">Yao Van_Durme Peter Clark</author>
<affiliation confidence="0.999402">Johns Hopkins University Vulcan Inc.</affiliation>
<address confidence="0.99977">Baltimore, MD, USA Seattle, WA, USA</address>
<abstract confidence="0.999531294117647">Information Retrieval (IR) and Answer Extraction are often designed as isolated or loosely connected components in Question Answering (QA), with repeated overengineering on IR, and not necessarily performance gain for QA. We propose to tightly integrate them by coupling automatically learned features for answer extraction to a shallow-structured IR model. Our method is very quick to implement, and significantly improves IR for QA (measured in Mean Average Precision and Mean Reciprocal Rank) by 10%-20% against an uncoupled retrieval baseline in both document and passage retrieval, which further leads to a downstream 20%</abstract>
<intro confidence="0.781713">in QA</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Arvind Agarwal</author>
<author>Hema Raghavan</author>
<author>Karthik Subbian</author>
<author>Prem Melville</author>
<author>Richard D Lawrence</author>
<author>David C Gondek</author>
<author>James Fan</author>
</authors>
<title>Learning to rank for robust question answering.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st ACM international conference on Information and knowledge management, CIKM ’12,</booktitle>
<pages>833--842</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="6742" citStr="Agarwal et al., 2012" startWordPosition="1104" endWordPosition="1107">ur method and the technique of learning to rank applied in feature label weight qword=whenjPOSo=CD ANS 0.86 qword=whenjNER0=DATE ANS 0.79 qword=whenjPOSo=CD O -0.74 Table 1: Learned weights for sampled features with respect to the label of current token (indexed by [0]) in a CRF. The larger the weight, the more “important” is this feature to help tag the current token with the corresponding label. For instance, line 1 says when answering a when question, and the POS of current token is CD (cardinal number), it is likely (large weight) that the token is tagged as ANS. QA (Bilotti et al., 2010; Agarwal et al., 2012). Our method is a QA-driven approach that provides supervision for IR from a learned QA model, while learning to rank is essentially an IR-driven approach: the supervision for IR comes from a labeled ranking list of retrieval results. Overall, we make the following contributions: • Our proposed method tightly integrates QA with IR and the reuse of analysis from QA does not put extra overhead on the IR queries. This QA-driven approach provides a holistic solution to the task of IR4QA. • We learn statistical evidence about what the form of answers to different questions look like, rather than us</context>
</contexts>
<marker>Agarwal, Raghavan, Subbian, Melville, Lawrence, Gondek, Fan, 2012</marker>
<rawString>Arvind Agarwal, Hema Raghavan, Karthik Subbian, Prem Melville, Richard D. Lawrence, David C. Gondek, and James Fan. 2012. Learning to rank for robust question answering. In Proceedings of the 21st ACM international conference on Information and knowledge management, CIKM ’12, pages 833–842, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ron Artstein</author>
<author>Massimo Poesio</author>
</authors>
<title>Inter-Coder Agreement for Computational Linguistics.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="15918" citStr="Artstein and Poesio, 2008" startWordPosition="2617" endWordPosition="2620"> Lin and Katz (2006) contains 109 questions from TREC 2002 and provides a nearexhaustive judgment of relevant documents for each question. We removed 10 questions that do not have an answer by matching the TREC answer patterns. Then we call this test set MIT99. Training Set for QA We used Amazon Mechanical Turk to collect training data for the QA system by issuing answer-bearing queries for TREC1999- 2003 questions. For the top 10 retrieved sentences for each question, three Turkers judged whether each sentence contained the answer. The inter-coder agreement rate was 0.81 (Krippendorff, 2004; Artstein and Poesio, 2008). The 99 questions of MIT99 were extracted from the Turk collection as our TESTgold with the remaining as TRAIN, with statistics shown in Table 2. Note that only 88 questions out of MIT99 have an answer from the top 10 query results. Finally both the training and test data were sentence-segmented and word-tokenized by NLTK (Bird and Loper, 2004), dependencyparsed by the Stanford Parser (Klein and Manning, 2003), and NER-tagged by the Illinois Named Entity Tagger (Ratinov and Roth, 2009) with an 18-label type set. Corpus Preprocessing for IR The AQUAINT (LDC2002T31) corpus, on which the MIT99 q</context>
</contexts>
<marker>Artstein, Poesio, 2008</marker>
<rawString>Ron Artstein and Massimo Poesio. 2008. Inter-Coder Agreement for Computational Linguistics. Computational Linguistics, 34(4):555–596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M W Bilotti</author>
<author>E Nyberg</author>
</authors>
<title>Improving text retrieval precision and answer accuracy in question answering systems.</title>
<date>2008</date>
<booktitle>In Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="8875" citStr="Bilotti and Nyberg (2008)" startWordPosition="1441" endWordPosition="1444">l of answer-bearing sentences were shown over the bag-of-words baseline. Zhao and 3Rarely are all three aspects presented in concert (see §2). 4http://code.google.com/p/jacana/ 160 Callan (2008) extended this work with approximate matching and smoothing. Most research uses parsing to assign deep structures. Compared to shallow (POS, NER) structured retrieval, deep structures need more processing power and smoothing, but might also be more precise. 5 Most of the above (except Kaisser (2012)) only reported on IR or QA, but not both, assuming that improvement in one naturally improves the other. Bilotti and Nyberg (2008) challenged this assumption and called for tighter coupling between IR and QA. This paper is aimed at that challenge. 3 Method Table 1 already shows some examples of features associating question types with answer types. We store the features and their learned weights from the trained model for IR usage. We let the trained QA system guide the query formulation when performing coupled retrieval with Indri (Strohman et al., 2005), given a corpus already annotated with POS tags and NER labels. Then retrieval runs in four steps (Figure 1): 1. Question Analysis. The question analysis component from</context>
</contexts>
<marker>Bilotti, Nyberg, 2008</marker>
<rawString>M.W. Bilotti and E. Nyberg. 2008. Improving text retrieval precision and answer accuracy in question answering systems. In Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M W Bilotti</author>
<author>P Ogilvie</author>
<author>J Callan</author>
<author>E Nyberg</author>
</authors>
<title>Structured retrieval for question answering.</title>
<date>2007</date>
<booktitle>In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>351--358</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="8036" citStr="Bilotti et al. (2007)" startWordPosition="1312" endWordPosition="1315"> answer features in IR queries. We give a full spectrum evaluation of all three stages of IR+QA: document retrieval, passage retrieval and answer extraction, to examine thoroughly the effectiveness of the method.3 All of our code and datasets are publicly available.4 2 Background Besides Predictive Annotation, our work is closest to structured retrieval, which covers techniques of dependency path mapping (Lin and Pantel, 2001; Cui et al., 2005; Kaisser, 2012), graph matching with Semantic Role Labeling (Shen and Lapata, 2007) and answer type checking (Pinchak et al., 2009), etc. Specifically, Bilotti et al. (2007) proposed indexing text with their semantic roles and named entities. Queries then include constraints of semantic roles and named entities for the predicate and its arguments in the question. Improvements in recall of answer-bearing sentences were shown over the bag-of-words baseline. Zhao and 3Rarely are all three aspects presented in concert (see §2). 4http://code.google.com/p/jacana/ 160 Callan (2008) extended this work with approximate matching and smoothing. Most research uses parsing to assign deep structures. Compared to shallow (POS, NER) structured retrieval, deep structures need mor</context>
</contexts>
<marker>Bilotti, Ogilvie, Callan, Nyberg, 2007</marker>
<rawString>M.W. Bilotti, P. Ogilvie, J. Callan, and E. Nyberg. 2007. Structured retrieval for question answering. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pages 351–358. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M W Bilotti</author>
<author>J Elsas</author>
<author>J Carbonell</author>
<author>E Nyberg</author>
</authors>
<title>Rank learning for factoid question answering with linguistic and semantic constraints.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th ACM international conference on Information and knowledge management,</booktitle>
<pages>459--468</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6719" citStr="Bilotti et al., 2010" startWordPosition="1100" endWordPosition="1103"> distinction between our method and the technique of learning to rank applied in feature label weight qword=whenjPOSo=CD ANS 0.86 qword=whenjNER0=DATE ANS 0.79 qword=whenjPOSo=CD O -0.74 Table 1: Learned weights for sampled features with respect to the label of current token (indexed by [0]) in a CRF. The larger the weight, the more “important” is this feature to help tag the current token with the corresponding label. For instance, line 1 says when answering a when question, and the POS of current token is CD (cardinal number), it is likely (large weight) that the token is tagged as ANS. QA (Bilotti et al., 2010; Agarwal et al., 2012). Our method is a QA-driven approach that provides supervision for IR from a learned QA model, while learning to rank is essentially an IR-driven approach: the supervision for IR comes from a labeled ranking list of retrieval results. Overall, we make the following contributions: • Our proposed method tightly integrates QA with IR and the reuse of analysis from QA does not put extra overhead on the IR queries. This QA-driven approach provides a holistic solution to the task of IR4QA. • We learn statistical evidence about what the form of answers to different questions lo</context>
</contexts>
<marker>Bilotti, Elsas, Carbonell, Nyberg, 2010</marker>
<rawString>M.W. Bilotti, J. Elsas, J. Carbonell, and E. Nyberg. 2010. Rank learning for factoid question answering with linguistic and semantic constraints. In Proceedings of the 19th ACM international conference on Information and knowledge management, pages 459–468. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Edward Loper</author>
</authors>
<title>Nltk: The natural language toolkit.</title>
<date>2004</date>
<booktitle>In The Companion Volume to the Proceedings of 42st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>214--217</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="16265" citStr="Bird and Loper, 2004" startWordPosition="2677" endWordPosition="2680">tem by issuing answer-bearing queries for TREC1999- 2003 questions. For the top 10 retrieved sentences for each question, three Turkers judged whether each sentence contained the answer. The inter-coder agreement rate was 0.81 (Krippendorff, 2004; Artstein and Poesio, 2008). The 99 questions of MIT99 were extracted from the Turk collection as our TESTgold with the remaining as TRAIN, with statistics shown in Table 2. Note that only 88 questions out of MIT99 have an answer from the top 10 query results. Finally both the training and test data were sentence-segmented and word-tokenized by NLTK (Bird and Loper, 2004), dependencyparsed by the Stanford Parser (Klein and Manning, 2003), and NER-tagged by the Illinois Named Entity Tagger (Ratinov and Roth, 2009) with an 18-label type set. Corpus Preprocessing for IR The AQUAINT (LDC2002T31) corpus, on which the MIT99 questions are based, was processed in exactly the same manner as was the QA training set. But only sentence boundaries, POS tags and NER labels were kept as the annotation of the corpus. 4.2 Document and Passage Retrieval We issued uncoupled queries consisting of question words, and QA-driven coupled queries consisting of both the question and ex</context>
</contexts>
<marker>Bird, Loper, 2004</marker>
<rawString>Steven Bird and Edward Loper. 2004. Nltk: The natural language toolkit. In The Companion Volume to the Proceedings of 42st Annual Meeting of the Association for Computational Linguistics, pages 214– 217, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Cui</author>
<author>Renxu Sun</author>
<author>Keya Li</author>
<author>Min-Yen Kan</author>
<author>Tat-Seng Chua</author>
</authors>
<title>Question answering passage retrieval using dependency relations.</title>
<date>2005</date>
<booktitle>In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’05,</booktitle>
<pages>400--407</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="7862" citStr="Cui et al., 2005" startWordPosition="1286" endWordPosition="1289">istical evidence about what the form of answers to different questions look like, rather than using manually authored templates. This provides great flexibility in using answer features in IR queries. We give a full spectrum evaluation of all three stages of IR+QA: document retrieval, passage retrieval and answer extraction, to examine thoroughly the effectiveness of the method.3 All of our code and datasets are publicly available.4 2 Background Besides Predictive Annotation, our work is closest to structured retrieval, which covers techniques of dependency path mapping (Lin and Pantel, 2001; Cui et al., 2005; Kaisser, 2012), graph matching with Semantic Role Labeling (Shen and Lapata, 2007) and answer type checking (Pinchak et al., 2009), etc. Specifically, Bilotti et al. (2007) proposed indexing text with their semantic roles and named entities. Queries then include constraints of semantic roles and named entities for the predicate and its arguments in the question. Improvements in recall of answer-bearing sentences were shown over the bag-of-words baseline. Zhao and 3Rarely are all three aspects presented in concert (see §2). 4http://code.google.com/p/jacana/ 160 Callan (2008) extended this wor</context>
</contexts>
<marker>Cui, Sun, Li, Kan, Chua, 2005</marker>
<rawString>Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and Tat-Seng Chua. 2005. Question answering passage retrieval using dependency relations. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’05, pages 400–407, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark A Greenwood</author>
<author>editor</author>
</authors>
<date>2008</date>
<booktitle>Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering. Coling 2008 Organizing Committee,</booktitle>
<location>Manchester, UK,</location>
<marker>Greenwood, editor, 2008</marker>
<rawString>Mark A. Greenwood, editor. 2008. Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering. Coling 2008 Organizing Committee, Manchester, UK, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Kaisser</author>
</authors>
<title>Answer Sentence Retrieval by Matching Dependency Paths acquired from Question/Answer Sentence Pairs. In</title>
<date>2012</date>
<booktitle>EACL,</booktitle>
<pages>88--98</pages>
<contexts>
<context position="7878" citStr="Kaisser, 2012" startWordPosition="1290" endWordPosition="1291">bout what the form of answers to different questions look like, rather than using manually authored templates. This provides great flexibility in using answer features in IR queries. We give a full spectrum evaluation of all three stages of IR+QA: document retrieval, passage retrieval and answer extraction, to examine thoroughly the effectiveness of the method.3 All of our code and datasets are publicly available.4 2 Background Besides Predictive Annotation, our work is closest to structured retrieval, which covers techniques of dependency path mapping (Lin and Pantel, 2001; Cui et al., 2005; Kaisser, 2012), graph matching with Semantic Role Labeling (Shen and Lapata, 2007) and answer type checking (Pinchak et al., 2009), etc. Specifically, Bilotti et al. (2007) proposed indexing text with their semantic roles and named entities. Queries then include constraints of semantic roles and named entities for the predicate and its arguments in the question. Improvements in recall of answer-bearing sentences were shown over the bag-of-words baseline. Zhao and 3Rarely are all three aspects presented in concert (see §2). 4http://code.google.com/p/jacana/ 160 Callan (2008) extended this work with approxima</context>
<context position="19607" citStr="Kaisser (2012)" startWordPosition="3231" endWordPosition="3232">0.231, 20% better than F1 of 0.192 with uncoupled retrieval, both at K = 1. The two descending lines at the bottom reflect the fact that the majority-voting mechanism from the QA system was too simple: F1 drops as K increases. Thus we also computed F1’s assuming perfect voting: a voting oracle that always selects the correct answer as long as the QA system produces one, thus the two ascending lines in the center of Figure 2. Still, F1 with coupled retrieval is always better: reiterating the fact that coupled retrieval covers more answer-bearing sentences. 7Lin (2007), Zhang et al. (2007), and Kaisser (2012) also evaluated on MIT109. However their QA engines used webbased search engines, thus leading to results that are neither reproducible nor directly comparable with ours. Finally, to find the upper bound for QA, we drew the two upper lines, testing on TESTgold described in Table 2. The test sentences were obtained with answer-bearing queries. This is assuming almost perfect IR. The gap between the top two and other lines signals more room for improvements for IR in terms of better coverage and better rank for answer-bearing sentences. Top K Sentences Retrieved Figure 2: F1 values for answer ex</context>
</contexts>
<marker>Kaisser, 2012</marker>
<rawString>Michael Kaisser. 2012. Answer Sentence Retrieval by Matching Dependency Paths acquired from Question/Answer Sentence Pairs. In EACL, pages 88–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing. In</title>
<date>2003</date>
<booktitle>In Proc. the 41st Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="16332" citStr="Klein and Manning, 2003" startWordPosition="2687" endWordPosition="2690">ns. For the top 10 retrieved sentences for each question, three Turkers judged whether each sentence contained the answer. The inter-coder agreement rate was 0.81 (Krippendorff, 2004; Artstein and Poesio, 2008). The 99 questions of MIT99 were extracted from the Turk collection as our TESTgold with the remaining as TRAIN, with statistics shown in Table 2. Note that only 88 questions out of MIT99 have an answer from the top 10 query results. Finally both the training and test data were sentence-segmented and word-tokenized by NLTK (Bird and Loper, 2004), dependencyparsed by the Stanford Parser (Klein and Manning, 2003), and NER-tagged by the Illinois Named Entity Tagger (Ratinov and Roth, 2009) with an 18-label type set. Corpus Preprocessing for IR The AQUAINT (LDC2002T31) corpus, on which the MIT99 questions are based, was processed in exactly the same manner as was the QA training set. But only sentence boundaries, POS tags and NER labels were kept as the annotation of the corpus. 4.2 Document and Passage Retrieval We issued uncoupled queries consisting of question words, and QA-driven coupled queries consisting of both the question and expected answer types, then retrieved the top 1000 documents, and 1 2</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate Unlexicalized Parsing. In In Proc. the 41st Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus H Krippendorff</author>
</authors>
<title>Content Analysis: An Introduction to Its Methodology.</title>
<date>2004</date>
<publisher>Sage Publications, Inc,</publisher>
<note>2nd edition.</note>
<contexts>
<context position="15890" citStr="Krippendorff, 2004" startWordPosition="2614" endWordPosition="2616">9 test collection by Lin and Katz (2006) contains 109 questions from TREC 2002 and provides a nearexhaustive judgment of relevant documents for each question. We removed 10 questions that do not have an answer by matching the TREC answer patterns. Then we call this test set MIT99. Training Set for QA We used Amazon Mechanical Turk to collect training data for the QA system by issuing answer-bearing queries for TREC1999- 2003 questions. For the top 10 retrieved sentences for each question, three Turkers judged whether each sentence contained the answer. The inter-coder agreement rate was 0.81 (Krippendorff, 2004; Artstein and Poesio, 2008). The 99 questions of MIT99 were extracted from the Turk collection as our TESTgold with the remaining as TRAIN, with statistics shown in Table 2. Note that only 88 questions out of MIT99 have an answer from the top 10 query results. Finally both the training and test data were sentence-segmented and word-tokenized by NLTK (Bird and Loper, 2004), dependencyparsed by the Stanford Parser (Klein and Manning, 2003), and NER-tagged by the Illinois Named Entity Tagger (Ratinov and Roth, 2009) with an 18-label type set. Corpus Preprocessing for IR The AQUAINT (LDC2002T31) </context>
</contexts>
<marker>Krippendorff, 2004</marker>
<rawString>Klaus H. Krippendorff. 2004. Content Analysis: An Introduction to Its Methodology. Sage Publications, Inc, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01,</booktitle>
<pages>282--289</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="4078" citStr="Lafferty et al., 2001" startWordPosition="650" endWordPosition="653">entities, but often it is not the case 2Based on a non-optimized IR configuration, none of the top 1000 returned passages contained the correct answer: 1867. 159 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 159–165, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics in practice). We use our statistically-trained QA system (Yao et al., 2013) that recognizes the association between question type and expected answer types through various features. The QA system employs a linear chain Conditional Random Field (CRF) (Lafferty et al., 2001) and tags each token as either an answer (ANS) or not (O). This will be our offthe-shelf QA system, which recognizes the association between question type and expected answer types through various features based on e.g., partof-speech tagging (POS) and named entity recognition (NER). With weights optimized by CRF training (Table 1), we can learn how answer features are correlated with question features. These features, whose weights are optimized by the CRF training, directly reflect what the most important answer types associated with each question type are. For instance, line 2 in Table 1 sa</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01, pages 282–289, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lin</author>
<author>B Katz</author>
</authors>
<title>Building a reusable test collection for question answering.</title>
<date>2006</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>57</volume>
<issue>7</issue>
<contexts>
<context position="15312" citStr="Lin and Katz (2006)" startWordPosition="2518" endWordPosition="2521">e-train the model with only unigram features to make sure weights are “assigned properly” among only unigram features. questions sentences set #all #pos. #all #pos. TRAIN 2205 1756 (80%) 22043 7637 (35%) TESTgold 99 88 (89%) 990 368 (37%) Table 2: Statistics for AMT-collected data (total cost was around $800 for paying three Turkers per sentence). Positive questions are those with an answer found. Positive sentences are those bearing an answer. All coupled and uncoupled queries are performed with Indri v5.3 (Strohman et al., 2005). 4.1 Data Test Set for IR and QA The MIT109 test collection by Lin and Katz (2006) contains 109 questions from TREC 2002 and provides a nearexhaustive judgment of relevant documents for each question. We removed 10 questions that do not have an answer by matching the TREC answer patterns. Then we call this test set MIT99. Training Set for QA We used Amazon Mechanical Turk to collect training data for the QA system by issuing answer-bearing queries for TREC1999- 2003 questions. For the top 10 retrieved sentences for each question, three Turkers judged whether each sentence contained the answer. The inter-coder agreement rate was 0.81 (Krippendorff, 2004; Artstein and Poesio,</context>
</contexts>
<marker>Lin, Katz, 2006</marker>
<rawString>J. Lin and B. Katz. 2006. Building a reusable test collection for question answering. Journal of the American Society for Information Science and Technology, 57(7):851–861.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
<author>P Pantel</author>
</authors>
<title>Discovery of inference rules for question-answering.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>4</issue>
<contexts>
<context position="7844" citStr="Lin and Pantel, 2001" startWordPosition="1282" endWordPosition="1285">IR4QA. • We learn statistical evidence about what the form of answers to different questions look like, rather than using manually authored templates. This provides great flexibility in using answer features in IR queries. We give a full spectrum evaluation of all three stages of IR+QA: document retrieval, passage retrieval and answer extraction, to examine thoroughly the effectiveness of the method.3 All of our code and datasets are publicly available.4 2 Background Besides Predictive Annotation, our work is closest to structured retrieval, which covers techniques of dependency path mapping (Lin and Pantel, 2001; Cui et al., 2005; Kaisser, 2012), graph matching with Semantic Role Labeling (Shen and Lapata, 2007) and answer type checking (Pinchak et al., 2009), etc. Specifically, Bilotti et al. (2007) proposed indexing text with their semantic roles and named entities. Queries then include constraints of semantic roles and named entities for the predicate and its arguments in the question. Improvements in recall of answer-bearing sentences were shown over the bag-of-words baseline. Zhao and 3Rarely are all three aspects presented in concert (see §2). 4http://code.google.com/p/jacana/ 160 Callan (2008)</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>D. Lin and P. Pantel. 2001. Discovery of inference rules for question-answering. Natural Language Engineering, 7(4):343–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jimmy Lin</author>
</authors>
<title>An exploration of the principles underlying redundancy-based factoid question answering.</title>
<date>2007</date>
<journal>ACM Trans. Inf. Syst.,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="19566" citStr="Lin (2007)" startWordPosition="3224" endWordPosition="3225">1 with coupled sentence retrieval is 0.231, 20% better than F1 of 0.192 with uncoupled retrieval, both at K = 1. The two descending lines at the bottom reflect the fact that the majority-voting mechanism from the QA system was too simple: F1 drops as K increases. Thus we also computed F1’s assuming perfect voting: a voting oracle that always selects the correct answer as long as the QA system produces one, thus the two ascending lines in the center of Figure 2. Still, F1 with coupled retrieval is always better: reiterating the fact that coupled retrieval covers more answer-bearing sentences. 7Lin (2007), Zhang et al. (2007), and Kaisser (2012) also evaluated on MIT109. However their QA engines used webbased search engines, thus leading to results that are neither reproducible nor directly comparable with ours. Finally, to find the upper bound for QA, we drew the two upper lines, testing on TESTgold described in Table 2. The test sentences were obtained with answer-bearing queries. This is assuming almost perfect IR. The gap between the top two and other lines signals more room for improvements for IR in terms of better coverage and better rank for answer-bearing sentences. Top K Sentences Re</context>
</contexts>
<marker>Lin, 2007</marker>
<rawString>Jimmy Lin. 2007. An exploration of the principles underlying redundancy-based factoid question answering. ACM Trans. Inf. Syst., 25(2), April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ogilvie</author>
</authors>
<title>Retrieval using Document Structure and Annotations.</title>
<date>2010</date>
<tech>Ph.D. thesis,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="10362" citStr="Ogilvie (2010)" startWordPosition="1688" endWordPosition="1689">t the 5 highest weighted features (e.g., POS[0]=CD for a when question). 3. Query Formulation. The original question is combined with the top features as the query. 4. Coupled Retrieval. Indri retrieves a ranked list of documents or passages. As motivated in the introduction, this framework is aimed at providing the following benefits: Reuse of QA components on the IR side. IR reuses both code for question analysis and top weighted features from QA. Statistical selection of answer features. For instance, the NER tagger we used divides location into two categories: GPE (geo locations) and LOC 5Ogilvie (2010) showed in chapter 4.3 that keyword and named entities based retrieval actually outperformed SRLbased structured retrieval in MAP for the answer-bearing sentence retrieval task in their setting. In this paper we do not intend to re-invent another parse-based structure matching algorithm, but only use shallow structures to show the idea of coupling QA with IR; in the future this might be extended to incorporate “deeper” structure. (non-GPE ). Both of them are learned to be important to where questions. Error tolerance along the NLP pipeline. IR and QA share the same processing pipeline. Systema</context>
</contexts>
<marker>Ogilvie, 2010</marker>
<rawString>P. Ogilvie. 2010. Retrieval using Document Structure and Annotations. Ph.D. thesis, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Pinchak</author>
<author>Davood Rafiei</author>
<author>Dekang Lin</author>
</authors>
<title>Answer typing for information retrieval.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th ACM conference on Information and knowledge management, CIKM ’09,</booktitle>
<pages>1955--1958</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="7994" citStr="Pinchak et al., 2009" startWordPosition="1306" endWordPosition="1309">. This provides great flexibility in using answer features in IR queries. We give a full spectrum evaluation of all three stages of IR+QA: document retrieval, passage retrieval and answer extraction, to examine thoroughly the effectiveness of the method.3 All of our code and datasets are publicly available.4 2 Background Besides Predictive Annotation, our work is closest to structured retrieval, which covers techniques of dependency path mapping (Lin and Pantel, 2001; Cui et al., 2005; Kaisser, 2012), graph matching with Semantic Role Labeling (Shen and Lapata, 2007) and answer type checking (Pinchak et al., 2009), etc. Specifically, Bilotti et al. (2007) proposed indexing text with their semantic roles and named entities. Queries then include constraints of semantic roles and named entities for the predicate and its arguments in the question. Improvements in recall of answer-bearing sentences were shown over the bag-of-words baseline. Zhao and 3Rarely are all three aspects presented in concert (see §2). 4http://code.google.com/p/jacana/ 160 Callan (2008) extended this work with approximate matching and smoothing. Most research uses parsing to assign deep structures. Compared to shallow (POS, NER) stru</context>
</contexts>
<marker>Pinchak, Rafiei, Lin, 2009</marker>
<rawString>Christopher Pinchak, Davood Rafiei, and Dekang Lin. 2009. Answer typing for information retrieval. In Proceedings of the 18th ACM conference on Information and knowledge management, CIKM ’09, pages 1955–1958, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Prager</author>
<author>Eric Brown</author>
<author>Anni Coden</author>
<author>Dragomir Radev</author>
</authors>
<title>Question-answering by predictive annotation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’00,</booktitle>
<pages>184--191</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2286" citStr="Prager et al., 2000" startWordPosition="363" endWordPosition="366">paper we use the term QA in a narrow sense: QA without the IR component, i.e., answer extraction. As a motivating example, using the question When was Alaska purchased from the TREC 2002 QA track as the query to the Indri search engine, the top sentence retrieved from the accompanying AQUAINT corpus is: Eventually Alaska Airlines will allow all travelers who have purchased electronic tickets through any means. While this relates Alaska and purchased, it is not a useful passage for the given question.2 It is apparent that the question asks for a date. Prior work proposed predictive annotation (Prager et al., 2000; Prager et al., 2006): text is first annotated in a predictive manner (of what types of questions it might answer) with 20 answer types and then indexed. A question analysis component (consisting of 400 question templates) maps the desired answer type to one of the 20 existing answer types. Retrieval is then performed with both the question and predicated answer types in the query. However, predictive annotation has the limitation of being labor intensive and assuming the underlying NLP pipeline to be accurate. We avoid these limitations by directly asking the downstream QA system for the inf</context>
</contexts>
<marker>Prager, Brown, Coden, Radev, 2000</marker>
<rawString>John Prager, Eric Brown, Anni Coden, and Dragomir Radev. 2000. Question-answering by predictive annotation. In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’00, pages 184–191, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Prager</author>
<author>J Chu-Carroll</author>
<author>E Brown</author>
<author>K Czuba</author>
</authors>
<title>Question answering by predictive annotation.</title>
<date>2006</date>
<booktitle>Advances in Open Domain Question Answering,</booktitle>
<pages>307--347</pages>
<contexts>
<context position="2308" citStr="Prager et al., 2006" startWordPosition="367" endWordPosition="370"> QA in a narrow sense: QA without the IR component, i.e., answer extraction. As a motivating example, using the question When was Alaska purchased from the TREC 2002 QA track as the query to the Indri search engine, the top sentence retrieved from the accompanying AQUAINT corpus is: Eventually Alaska Airlines will allow all travelers who have purchased electronic tickets through any means. While this relates Alaska and purchased, it is not a useful passage for the given question.2 It is apparent that the question asks for a date. Prior work proposed predictive annotation (Prager et al., 2000; Prager et al., 2006): text is first annotated in a predictive manner (of what types of questions it might answer) with 20 answer types and then indexed. A question analysis component (consisting of 400 question templates) maps the desired answer type to one of the 20 existing answer types. Retrieval is then performed with both the question and predicated answer types in the query. However, predictive annotation has the limitation of being labor intensive and assuming the underlying NLP pipeline to be accurate. We avoid these limitations by directly asking the downstream QA system for the information about which e</context>
</contexts>
<marker>Prager, Chu-Carroll, Brown, Czuba, 2006</marker>
<rawString>J. Prager, J. Chu-Carroll, E. Brown, and K. Czuba. 2006. Question answering by predictive annotation. Advances in Open Domain Question Answering, pages 307–347.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ratinov</author>
<author>D Roth</author>
</authors>
<title>Design challenges and misconceptions in named entity recognition.</title>
<date>2009</date>
<journal>In CoNLL,</journal>
<volume>6</volume>
<contexts>
<context position="16409" citStr="Ratinov and Roth, 2009" startWordPosition="2699" endWordPosition="2702"> whether each sentence contained the answer. The inter-coder agreement rate was 0.81 (Krippendorff, 2004; Artstein and Poesio, 2008). The 99 questions of MIT99 were extracted from the Turk collection as our TESTgold with the remaining as TRAIN, with statistics shown in Table 2. Note that only 88 questions out of MIT99 have an answer from the top 10 query results. Finally both the training and test data were sentence-segmented and word-tokenized by NLTK (Bird and Loper, 2004), dependencyparsed by the Stanford Parser (Klein and Manning, 2003), and NER-tagged by the Illinois Named Entity Tagger (Ratinov and Roth, 2009) with an 18-label type set. Corpus Preprocessing for IR The AQUAINT (LDC2002T31) corpus, on which the MIT99 questions are based, was processed in exactly the same manner as was the QA training set. But only sentence boundaries, POS tags and NER labels were kept as the annotation of the corpus. 4.2 Document and Passage Retrieval We issued uncoupled queries consisting of question words, and QA-driven coupled queries consisting of both the question and expected answer types, then retrieved the top 1000 documents, and 1 2 ... 50 2. Get top weighted features w.r.t qword (from trained QA model) 1. S</context>
</contexts>
<marker>Ratinov, Roth, 2009</marker>
<rawString>L. Ratinov and D. Roth. 2009. Design challenges and misconceptions in named entity recognition. In CoNLL, 6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuya Sakai</author>
<author>Hideki Shima</author>
</authors>
<title>Noriko Kando, Ruihua Song, Chuan-Jie Lin, Teruko Mitamura, Miho Sugimito, and Cheng-Wei Lee.</title>
<date>2010</date>
<booktitle>In Proceedings of NTCIR-8 Workshop Meeting,</booktitle>
<location>Tokyo, Japan.</location>
<marker>Sakai, Shima, 2010</marker>
<rawString>Tetsuya Sakai, Hideki Shima, Noriko Kando, Ruihua Song, Chuan-Jie Lin, Teruko Mitamura, Miho Sugimito, and Cheng-Wei Lee. 2010. Overview of the ntcir-7 aclia ir4qa task. In Proceedings of NTCIR-8 Workshop Meeting, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Shen</author>
<author>M Lapata</author>
</authors>
<title>Using semantic roles to improve question answering.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<pages>12--21</pages>
<contexts>
<context position="7946" citStr="Shen and Lapata, 2007" startWordPosition="1298" endWordPosition="1301">ke, rather than using manually authored templates. This provides great flexibility in using answer features in IR queries. We give a full spectrum evaluation of all three stages of IR+QA: document retrieval, passage retrieval and answer extraction, to examine thoroughly the effectiveness of the method.3 All of our code and datasets are publicly available.4 2 Background Besides Predictive Annotation, our work is closest to structured retrieval, which covers techniques of dependency path mapping (Lin and Pantel, 2001; Cui et al., 2005; Kaisser, 2012), graph matching with Semantic Role Labeling (Shen and Lapata, 2007) and answer type checking (Pinchak et al., 2009), etc. Specifically, Bilotti et al. (2007) proposed indexing text with their semantic roles and named entities. Queries then include constraints of semantic roles and named entities for the predicate and its arguments in the question. Improvements in recall of answer-bearing sentences were shown over the bag-of-words baseline. Zhao and 3Rarely are all three aspects presented in concert (see §2). 4http://code.google.com/p/jacana/ 160 Callan (2008) extended this work with approximate matching and smoothing. Most research uses parsing to assign deep</context>
</contexts>
<marker>Shen, Lapata, 2007</marker>
<rawString>D. Shen and M. Lapata. 2007. Using semantic roles to improve question answering. In Proceedings of EMNLP-CoNLL, pages 12–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M D Smucker</author>
<author>J Allan</author>
<author>B Carterette</author>
</authors>
<title>A comparison of statistical significance tests for information retrieval evaluation.</title>
<date>2007</date>
<booktitle>In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management,</booktitle>
<pages>623--632</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="17755" citStr="Smucker et al., 2007" startWordPosition="2911" endWordPosition="2914"> Query formulation 4. Coupled retrieval #combine(Alaska purchased #max(#any:CD #any:DATE)) On &lt;DATE&gt;March 30, &lt;CD&gt; 1867 &lt;/CD&gt; &lt;/DATE&gt;, U.S. ... reached agreement ... to purchase ... Alaska ... The islands were sold to the United States in &lt;CD&gt;1867&lt;/CD&gt; with the purchase of Alaska. ... ... Eventually Alaska Airlines will allow all travelers who have purchased electronic tickets ... 162 coupled uncoupled MAP MRR MAP MRR document 0.2524 0.4835 0.2110 0.4298 sentence 0.1375 0.2987 0.1200 0.2544 Table 3: Coupled vs. uncoupled document/sentence retrieval in MAP and MRR on MIT99. Significance level (Smucker et al., 2007) for both MAP: P &lt; 0.001 and for both MRR: P &lt; 0.05. finally computed MAP and MRR against the goldstandard MIT99 per-document judgment. To find the best weighting α for coupled retrieval, we used 5-fold cross-validation and finalized at α = 0.1. Table 3 shows the results. Coupled retrieval outperforms (20% by MAP with p &lt; 0.001 and 12% by MRR with p &lt; 0.01) uncoupled retrieval significantly according to paired randomization test (Smucker et al., 2007). For passage retrieval, we extracted relevant single sentences. Recall that MIT99 only contains document-level judgment. To generate a test set </context>
</contexts>
<marker>Smucker, Allan, Carterette, 2007</marker>
<rawString>M.D. Smucker, J. Allan, and B. Carterette. 2007. A comparison of statistical significance tests for information retrieval evaluation. In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management, pages 623– 632. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Strohman</author>
<author>D Metzler</author>
<author>H Turtle</author>
<author>W B Croft</author>
</authors>
<title>Indri: A language model-based search engine for complex queries.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Conference on Intelligent Analysis,</booktitle>
<volume>2</volume>
<pages>2--6</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="9306" citStr="Strohman et al., 2005" startWordPosition="1512" endWordPosition="1515"> be more precise. 5 Most of the above (except Kaisser (2012)) only reported on IR or QA, but not both, assuming that improvement in one naturally improves the other. Bilotti and Nyberg (2008) challenged this assumption and called for tighter coupling between IR and QA. This paper is aimed at that challenge. 3 Method Table 1 already shows some examples of features associating question types with answer types. We store the features and their learned weights from the trained model for IR usage. We let the trained QA system guide the query formulation when performing coupled retrieval with Indri (Strohman et al., 2005), given a corpus already annotated with POS tags and NER labels. Then retrieval runs in four steps (Figure 1): 1. Question Analysis. The question analysis component from QA is reused here. In this implementation, the only information we have chosen to use from the question is the question word (e.g., how, who) and the lexical answer types (LAT) in case of what/which questions. 2. Answer Feature Selection. Given the question word, we select the 5 highest weighted features (e.g., POS[0]=CD for a when question). 3. Query Formulation. The original question is combined with the top features as the </context>
<context position="15229" citStr="Strohman et al., 2005" startWordPosition="2500" endWordPosition="2503">Then we would have missed this feature if we only used top unigram features. Thus we re-train the model with only unigram features to make sure weights are “assigned properly” among only unigram features. questions sentences set #all #pos. #all #pos. TRAIN 2205 1756 (80%) 22043 7637 (35%) TESTgold 99 88 (89%) 990 368 (37%) Table 2: Statistics for AMT-collected data (total cost was around $800 for paying three Turkers per sentence). Positive questions are those with an answer found. Positive sentences are those bearing an answer. All coupled and uncoupled queries are performed with Indri v5.3 (Strohman et al., 2005). 4.1 Data Test Set for IR and QA The MIT109 test collection by Lin and Katz (2006) contains 109 questions from TREC 2002 and provides a nearexhaustive judgment of relevant documents for each question. We removed 10 questions that do not have an answer by matching the TREC answer patterns. Then we call this test set MIT99. Training Set for QA We used Amazon Mechanical Turk to collect training data for the QA system by issuing answer-bearing queries for TREC1999- 2003 questions. For the top 10 retrieved sentences for each question, three Turkers judged whether each sentence contained the answer</context>
</contexts>
<marker>Strohman, Metzler, Turtle, Croft, 2005</marker>
<rawString>T. Strohman, D. Metzler, H. Turtle, and W.B. Croft. 2005. Indri: A language model-based search engine for complex queries. In Proceedings of the International Conference on Intelligent Analysis, volume 2, pages 2–6. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuchen Yao</author>
<author>Benjamin Van Durme</author>
<author>Peter Clark</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Answer Extraction as Sequence Tagging with Tree Edit Distance.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL</booktitle>
<marker>Yao, Van Durme, Clark, Callison-Burch, 2013</marker>
<rawString>Xuchen Yao, Benjamin Van Durme, Peter Clark, and Chris Callison-Burch. 2013. Answer Extraction as Sequence Tagging with Tree Edit Distance. In Proceedings of NAACL 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xian Zhang</author>
<author>Yu Hao</author>
<author>Xiaoyan Zhu</author>
<author>Ming Li</author>
<author>David R Cheriton</author>
</authors>
<title>Information distance from a question to an answer.</title>
<date>2007</date>
<booktitle>In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’07,</booktitle>
<pages>874--883</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="19587" citStr="Zhang et al. (2007)" startWordPosition="3226" endWordPosition="3229">ed sentence retrieval is 0.231, 20% better than F1 of 0.192 with uncoupled retrieval, both at K = 1. The two descending lines at the bottom reflect the fact that the majority-voting mechanism from the QA system was too simple: F1 drops as K increases. Thus we also computed F1’s assuming perfect voting: a voting oracle that always selects the correct answer as long as the QA system produces one, thus the two ascending lines in the center of Figure 2. Still, F1 with coupled retrieval is always better: reiterating the fact that coupled retrieval covers more answer-bearing sentences. 7Lin (2007), Zhang et al. (2007), and Kaisser (2012) also evaluated on MIT109. However their QA engines used webbased search engines, thus leading to results that are neither reproducible nor directly comparable with ours. Finally, to find the upper bound for QA, we drew the two upper lines, testing on TESTgold described in Table 2. The test sentences were obtained with answer-bearing queries. This is assuming almost perfect IR. The gap between the top two and other lines signals more room for improvements for IR in terms of better coverage and better rank for answer-bearing sentences. Top K Sentences Retrieved Figure 2: F1 </context>
</contexts>
<marker>Zhang, Hao, Zhu, Li, Cheriton, 2007</marker>
<rawString>Xian Zhang, Yu Hao, Xiaoyan Zhu, Ming Li, and David R. Cheriton. 2007. Information distance from a question to an answer. In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’07, pages 874–883, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Zhao</author>
<author>J Callan</author>
</authors>
<title>A generative retrieval model for structured documents.</title>
<date>2008</date>
<booktitle>In Proceedings of the 17th ACM conference on Information and knowledge management,</booktitle>
<pages>1163--1172</pages>
<publisher>ACM.</publisher>
<marker>Zhao, Callan, 2008</marker>
<rawString>L. Zhao and J. Callan. 2008. A generative retrieval model for structured documents. In Proceedings of the 17th ACM conference on Information and knowledge management, pages 1163–1172. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>