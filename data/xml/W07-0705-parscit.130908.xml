<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010929">
<author confidence="0.7625435">
Can We Translate Letters?
David Vilar, Jan-T. Peter and Hermann Ney
</author>
<affiliation confidence="0.8483185">
Lehrstuhl f¨ur Informatik 6
RWTH Aachen University
</affiliation>
<address confidence="0.598042">
D-52056 Aachen, Germany
</address>
<email confidence="0.997305">
{vilar,peter,ney}@cs.rwth-aachen.de
</email>
<sectionHeader confidence="0.996634" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99970147368421">
Current statistical machine translation sys-
tems handle the translation process as the
transformation of a string of symbols into
another string of symbols. Normally the
symbols dealt with are the words in differ-
ent languages, sometimes with some addi-
tional information included, like morpho-
logical data. In this work we try to push
the approach to the limit, working not on the
level of words, but treating both the source
and target sentences as a string of letters.
We try to find out if a nearly unmodified
state-of-the-art translation system is able to
cope with the problem and whether it is ca-
pable to further generalize translation rules,
for example at the level of word suffixes and
translation of unseen words. Experiments
are carried out for the translation of Catalan
to Spanish.
</bodyText>
<sectionHeader confidence="0.99888" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999618333333333">
Most current statistical machine translation systems
handle the translation process as a “blind” transfor-
mation of a sequence of symbols, which represent
the words in a source language, to another sequence
of symbols, which represent words in a target lan-
guage. This approach allows for a relative simplic-
ity of the models, but also has drawbacks, as re-
lated word forms, like different verb tenses or plural-
singular word pairs, are treated as completely differ-
ent entities.
Some efforts have been made e.g. to integrate
more information about the words in the form of Part
</bodyText>
<page confidence="0.986663">
33
</page>
<bodyText confidence="0.9999255">
Of Speech tags (Popovi´c and Ney, 2005), using addi-
tional information about stems and suffixes (Popovi´c
and Ney, 2004) or to reduce the morphological vari-
ability of the words (de Gispert, 2006). State of the
art decoders provide the ability of handling different
word forms directly in what has been called factored
translation models (Shen et al., 2006).
In this work, we try to go a step further and treat
the words (and thus whole sentences) as sequences
of letters, which have to be translated into a new se-
quence of letters. We try to find out if the trans-
lation models can generalize and generate correct
words out of the stream of letters. For this approach
to work we need to translate between two related
languages, in which a correspondence between the
structure of the words can be found.
For this experiment we chose a Catalan-Spanish
corpus. Catalan is a romance language spoken in the
north-east of Spain and Andorra and is considered
by some authors as a transitional language between
the Iberian Romance languages (e.g. Spanish) and
Gallo-Romance languages (e.g. French). A common
origin and geographic proximity result in a similar-
ity between Spanish and Catalan, albeit with enough
differences to be considered different languages. In
particular, the sentence structure is quite similar in
both languages and many times a nearly monotoni-
cal word to word correspondence between sentences
can be found. An example of Catalan and Spanish
sentences is given in Figure 1.
The structure of the paper is as follows: In Sec-
tion 2 we review the statistical approach to machine
translation and consider how the usual techniques
can be adapted to the letter translation task. In Sec-
</bodyText>
<note confidence="0.4896715">
Proceedings of the Second Workshop on Statistical Machine Translation, pages 33–39,
Prague, June 2007. c�2007 Association for Computational Linguistics
</note>
<table confidence="0.950776">
Catalan Perqu`e a mi m’agradaria estar-hi dues, una o dues setmanes, m´es o menys, depenent del
preu i cada hotel.
Spanish Porque a mime gustaria quedarme dos, una o dos semanas, m´as o menos, dependiendo del
precio y cada hotel.
English Because I would like to be there two, one or two weeks, more or less, depending on the
price of each hotel.
Catalan Si baixa aquitenim una guia de la ciutat que li podem facilitar en la que surt informaci´o
sobre els llocs m´es interessants de la ciutat.
Spanish Si baja aquitenemos una guia de la ciudad que le podemos facilitar en la que sale infor-
maci´on sobre los sitios m´as interesantes de la ciudad.
</table>
<bodyText confidence="0.72644">
English If you come down here we have a guide book of the city that you can use, in there is
information about the most interesting places in the city.
</bodyText>
<figureCaption confidence="0.998842">
Figure 1: Example Spanish and Catalan sentences (the English translation is provided for clarity).
</figureCaption>
<bodyText confidence="0.999881666666667">
tion 3 we present the results of the letter-based trans-
lation and show how to use it for improving transla-
tion quality. Although the interest of this work is
more academical, in Section 4 we discuss possible
practical applications for this approach. The paper
concludes in Section 5.
</bodyText>
<sectionHeader confidence="0.965781" genericHeader="method">
2 From Words To Letters
</sectionHeader>
<bodyText confidence="0.999439571428572">
In the standard approach to statistical machine trans-
lation we are given a sentence (sequence of words)
fJ1 = f1 ... fJ in a source language which is to be
translated into a sentence ˆeI1 = ˆe1 ... ˆeI in a target
language. Bayes decision rule states that we should
choose the sentence which maximizes the posterior
probability
</bodyText>
<equation confidence="0.9788965">
ˆeI1 = argmax p(eI1|fJ1 ), (1)
el
</equation>
<bodyText confidence="0.999666714285714">
where the argmax operator denotes the search pro-
cess. In the original work (Brown et al., 1993) the
posterior probability p(eI1|fJ1 ) is decomposed fol-
lowing a noisy-channel approach, but current state-
of-the-art systems model the translation probabil-
ity directly using a log-linear model(Och and Ney,
2002):
</bodyText>
<equation confidence="0.921779">
IJexp (EM1 λmhm(e1,f1 ))
J
� (EM � ,
˜eI 1 exp m=1 λmhm(˜eI 1, fJ 1 )
(2)
</equation>
<bodyText confidence="0.999963961538462">
with hm different models, λm scaling factors and
the denominator a normalization factor that can be
ignored in the maximization process. The λm are
usually chosen by optimizing a performance mea-
sure over a development corpus using a numerical
optimization algorithm like the downhill simplex al-
gorithm (Press et al., 2002).
The most widely used models in the log lin-
ear combination are phrase-based models in source-
to-target and target-to-source directions, ibm1-like
scores computed at phrase level, also in source-to-
target and target-to-source directions, a target lan-
guage model and different penalties, like phrase
penalty and word penalty.
This same approach can be directly adapted to the
letter-based translation framework. In this case we
are given a sequence of letters F 1corresponding
to a source (word) string fJ1 , which is to be trans-
lated into a sequence of letters £i corresponding to
a string eI1 in a target language. Note that in this case
whitespaces are also part of the vocabulary and have
to be generated as any other letter. It is also impor-
tant to remark that, without any further restrictions,
the word sequences eI1 corresponding to a generated
letter sequence £I1 are not even composed of actual
words.
</bodyText>
<subsectionHeader confidence="0.998494">
2.1 Details of the Letter-Based System
</subsectionHeader>
<bodyText confidence="0.924227285714286">
The vocabulary of the letter-based translation sys-
tem is some orders of magnitude smaller than the
vocabulary of a full word-based translation system,
at least for European languages. A typical vocabu-
lary size for a letter-based system would be around
70, considering upper- and lowercase letter, digits,
p(eI1|fJ1 ) =
</bodyText>
<page confidence="0.97343">
34
</page>
<bodyText confidence="0.999989190476191">
whitespace and punctuation marks, while the vocab-
ulary size of a word-based system like the ones used
in current evaluation campaigns is in the range of
tens or hundreds of thousands words. In a normal
situation there are no unknowns when carrying out
the actual translation of a given test corpus. The sit-
uation can be very different if we consider languages
like Chinese or Japanese.
This small vocabulary size allows us to deal with
a larger context in the models used. For the phrase-
based models we extract all phrases that can be used
when translating a given test corpus, without any
restriction on the length of the source or the tar-
get part1. For the language model we were able to
use a high-order n-gram model. In fact in our ex-
periments a 16-gram letter-based language model is
used, while state-of-the-art translation systems nor-
mally use 3 or 4-grams (word-based).
In order to better try to generate “actual words”
in the letter-based system, a new model was added
in the log-linear combination, namely the count of
words generated that have been seen in the training
corpus, normalized with the length of the input sen-
tence. Note however that this models enters as an ad-
ditional feature function in the model and it does not
constitute a restriction of the generalization capabil-
ities the model can have in creating “new words”.
Somehow surprisingly, an additional word language
model did not help.
While the vocabulary size is reduced, the average
sentence length increases, as we consider each let-
ter to be a unit by itself. This has a negative impact
in the running time of the actual implementation of
the algorithms, specially for the alignment process.
In order to alleviate this, the alignment process was
split into two passes. In the first part, a word align-
ment was computed (using the GIZA++ toolkit (Och
and Ney, 2003)). Then the training sentences were
split according to this alignment (in a similar way to
the standard phrase extraction algorithm), so that the
length of the source and target part is around thirty
letters. Then, a letter-based alignment is computed.
</bodyText>
<subsectionHeader confidence="0.988204">
2.2 Efficiency Issues
</subsectionHeader>
<bodyText confidence="0.9433765">
Somewhat counter-intuitively, the reduced vocabu-
lary size does not necessarily imply a reduced mem-
</bodyText>
<footnote confidence="0.918064">
1For the word-based system this is also the case.
</footnote>
<bodyText confidence="0.999728428571429">
ory footprint, at least not without a dedicated pro-
gram optimization. As in a sensible implementa-
tions of nearly all natural language processing tools,
the words are mapped to integers and handled as
such. A typical implementation of a phrase table is
then a prefix-tree, which is accessed through these
word indices. In the case of the letter-based transla-
tion, the phrases extracted are much larger than the
word-based ones, in terms of elements. Thus the to-
tal size of the phrase table increases.
The size of the search graph is also larger for
the letter-based system. In most current systems
the generation algorithm is a beam search algorithm
with a “source synchronous” search organization.
As the length of the source sentence is dramatically
increased when considering letters instead of words,
the total size of the search graph is also increased, as
is the running time of the translation process.
The memory usage for the letter system can ac-
tually be optimized, in the sense that the letters can
act as “indices” themselves for addressing the phrase
table and the auxiliary mapping structure is not nec-
essary any more. Furthermore the characters can be
stored in only one byte, which provides a signifi-
cant memory gain over the word based system where
normally four bytes are used for storing the indices.
These gains however are not expected to counteract
the other issues presented in this section.
</bodyText>
<sectionHeader confidence="0.997827" genericHeader="method">
3 Experimental Results
</sectionHeader>
<bodyText confidence="0.999689266666667">
The corpus used for our experiment was built in the
framework of the LC-STAR project (Conejero et al.,
2003). It consists of spontaneous dialogues in Span-
ish, Catalan and English2 in the tourism and travel-
ling domain. The test corpus (and an additional de-
velopment corpus for parameter optimization) was
randomly extracted, the rest of the sentences were
used as training data. Statistics for the corpus can
be seen in Table 1. Details of the translation system
used can be found in (Mauser et al., 2006).
The results of the word-based and letter-based
approaches can be seen in Table 2 (rows with la-
bel “Full Corpus”). The high BLEU scores (up to
nearly 80%) denote that the quality of the trans-
lation is quite good for both systems. The word-
</bodyText>
<footnote confidence="0.995338">
2The English part of the corpus was not used in our experi-
ments.
</footnote>
<page confidence="0.991992">
35
</page>
<table confidence="0.999408">
Spanish Catalan
Training Sentences 40 574
Running Words 482 290 485 514
Vocabulary 14 327 12 772
Singletons 6 743 5 930
Test Sentences 972
Running Words 12 771 12 973
OOVs [%] 1.4 1.3
</table>
<tableCaption confidence="0.999588">
Table 1: Corpus Statistics
</tableCaption>
<bodyText confidence="0.999731408163266">
based system outperforms the letter-based one, as
expected, but the letter-based system also achieves
quite a good translation quality. Example transla-
tions for both systems can be found in Figure 2. It
can be observed that most of the words generated
by the letter based system are correct words, and in
many cases the “false” words that the system gen-
erates are very close to actual words (e.g. “elos” in-
stead of “los” in the second example of Figure 2).
We also investigated the generalization capabili-
ties of both systems under scarce training data con-
ditions. It was expected that the greater flexibility
of the letter-based system would provide an advan-
tage of the approach when compared to the word-
based approach. We randomly selected subsets of
the training corpus of different sizes ranging from
1000 sentences to 40 000 (i.e. the full corpus) and
computed the translation quality on the same test
corpus as before. Contrary to our hopes, however,
the difference in BLEU score between the word-
based and the letter-based system remained fairly
constant, as can be seen in Figure 3, and Table 2
for representative training corpus sizes.
Nevertheless, the second example in Figure 2 pro-
vides an interesting insight into one of the possi-
ble practical applications of this approach. In the
example translation of the word-based system, the
word “centreamericans” was not known to the sys-
tem (and has been explicitly marked as unknown in
Figure 2). The letter-based system, however, was
able to correctly learn the translation from “centre-”
to “centro-” and that the ending “-ans” in Catalan
is often translated as “-anos” in Spanish, and thus
a correct translation has been found. We thus chose
to combine both systems, the word-based system do-
ing most of the translation work, but using the letter-
based system for the translation of unknown words.
The results of this combined approach can be found
in Table 2 under the label “Combined System”. The
combination of both approaches leads to a 0.5% in-
crease in BLEU using the full corpus as training ma-
terial. This increase is not very big, but is it over a
quite strong baseline and the percentage of out-of-
vocabulary words in this corpus is around 1% of the
total words (see Table 1). When the corpus size is
reduced, the gain in BLEU score becomes more im-
portant, and for the small corpus size of 1000 sen-
tences the gain is 2.5% BLEU. Table 2 and Figure 3
show more details.
</bodyText>
<sectionHeader confidence="0.98868" genericHeader="method">
4 Practical Applications
</sectionHeader>
<bodyText confidence="0.99994725">
The approach described in this paper is mainly of
academical interest. We have shown that letter-
based translation is in principle possible between
similar languages, in our case between Catalan and
Spanish, but can be applied to other closely related
language pairs like Spanish and Portuguese or Ger-
man and Dutch. The approach can be interesting for
languages where very few parallel training data is
available.
The idea of translating unknown words in a letter-
based fashion can also have applications to state-of-
the-art translation systems. Nowadays most auto-
matic translation projects and evaluations deal with
translation from Chinese or Arabic to English. For
these language pairs the translation of named en-
tities poses an additional problem, as many times
they were not previously seen in the training data
and they are actually one of the most informative
words in the texts. The “translation” of these enti-
ties is in most cases actually a (more or less pho-
netic) transliteration, see for example (Al-Onaizan
and Knight, 2002). Using the proposed approach for
the translation of these words can provide a tighter
integration in the translation process and hopefully
increase the translation performance, in the same
way as it helps for the case of the Catalan-Spanish
translation for unseen words.
Somewhat related to this problem, we can find an
additional application in the field of speech recog-
nition. The task of grapheme-to-phoneme conver-
sion aims at increasing the vocabulary an ASR sys-
tem can recognize, without the need for additional
</bodyText>
<page confidence="0.994309">
36
</page>
<table confidence="0.9997107">
BLEU WER PER
Word-Based System Full Corpus 78.9 11.4 10.6
10k 74.0 13.9 13.2
1k 60.0 21.3 20.1
Letter-Based System Full Corpus 72.9 14.7 13.5
10k 69.8 16.5 15.1
1k 55.8 24.3 22.8
Combined System Full Corpus 79.4 11.2 10.4
10k 75.2 13.4 12.6
1k 62.5 20.2 19.0
</table>
<tableCaption confidence="0.964322">
Table 2: Translation results for selected corpus sizes. All measures are percentages.
</tableCaption>
<table confidence="0.999319555555555">
Source (Cat) B´e, en principi seria per a les vacances de Setmana Santa que s´on les seg¨uents que tenim
ara, entrant a juliol.
Word-Based Bueno, en principio ser´ıa para las vacaciones de Semana Santa que son las siguientes que
tenemos ahora, entrando en julio.
Letter-Based Bueno, en principio ser´ıa para las vacaciones de Semana Santa que son las siguientes que
tenemos ahora, entrando bamos en julio .
Reference Bueno, en principio ser´ıa para las vacaciones de Semana Santa que son las siguientes que
tenemos ahora, entrando julio.
Source (Cat) Jo li recomanaria per exemple que intent´es apropar-se a algun pais veitamb´e com poden ser
els paisos centreamericans, una mica m´es al nord Panam´a.
Word-Based Yo le recomendaria por ejemplo que intentase acercarse a alg´un pais vecino tambi´en como
pueden ser los paises UNKNOWN centreamericans, un poco m´as al norte Panam´a.
Letter-Based Yo le recomendaria por ejemplo que intentaseo acercarse a alg´un pais veitambi´en como
pueden ser elos paises centroamericanos, un poco m´as al norte Panam´a.
Combined Yo le recomendaria por ejemplo que intentase acercarse a alg´un pais vecino tambi´en como
pueden ser los paises centroamericanos, un poco m´as al norte Panam´a.
Reference Yo le recomendaria por ejemplo que intentase acercarse a alg´un pais vecino tambi´en como
pueden ser los paises centroamericanos, un poco m´as al norte Panam´a.
</table>
<figureCaption confidence="0.9840285">
Figure 2: Example translations of the different approaches. For the word-based system an unknown word
has been explicitly marked.
</figureCaption>
<page confidence="0.938156">
37
</page>
<figure confidence="0.998707090909091">
Word-Based
Letter-Based
Combined
80
75
70
65
60
55
50
0 5000 10000 15000 20000 25000 30000 35000 40000
</figure>
<figureCaption confidence="0.999979">
Figure 3: Translation quality depending of the corpus size.
</figureCaption>
<bodyText confidence="0.9916038">
acoustic data. The problem can be formulated as a
translation from graphemes (“letters”) to a sequence
of graphones (“pronunciations”), see for example
(Bisani and Ney, 2002). The proposed letter-based
approach can also be adapted to this task.
Lastly, a combination of both, word-based and
letter-based models, working in parallel and perhaps
taking into account additional information like base
forms, can be helpful when translating from or into
rich inflexional languages, like for example Spanish.
</bodyText>
<sectionHeader confidence="0.999237" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999984">
We have investigated the possibility of building a
letter-based system for translation between related
languages. The performance of the approach is quite
acceptable, although, as expected, the quality of the
word-based approach is superior. The combination
of both techniques, however, allows the system to
translate words not seen in the training corpus and
thus increase the translation quality. The gain is spe-
cially important when the training material is scarce.
While the experiments carried out in this work are
more interesting from an academical point of view,
several practical applications has been discussed and
will be the object of future work.
</bodyText>
<sectionHeader confidence="0.995145" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.996282666666667">
This work was partly funded by the Deutsche
Forschungsgemeinschaft (DFG) under the project
“Statistische Text¨ubersetzung” (NE 572/5-3).
</bodyText>
<sectionHeader confidence="0.996888" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.990032928571429">
Yaser Al-Onaizan and Kevin Knight. 2002. Machine
transliteration of names in arabic text. In Proceed-
ings of the ACL-02 workshop on Computational ap-
proaches to semitic languages, pages 1–13, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Max Bisani and Hermann Ney. 2002. Investigations
on joint-multigram models for grapheme-to-phoneme
conversion. In Proceedings of the 7th International
Conference on Spoken Language Processing, vol-
ume 1, pages 105–108, Denver, CO, September.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
</reference>
<page confidence="0.98537">
38
</page>
<reference confidence="0.999783956521739">
mation. Computational Linguistics, 19(2):263–311,
June.
D. Conejero, J. Gimnez, V. Arranz, A. Bonafonte, N. Pas-
cual, N. Castell, and A. Moreno. 2003. Lexica and
corpora for speech-to-speech translation: A trilingual
approach. In European Conf. on Speech Commu-
nication and Technology, pages 1593–1596, Geneva,
Switzerland, September.
Adri`a de Gispert. 2006. Introducing Linguistic Knowl-
edge into Statistical Machine Translation. Ph.D. the-
sis, Universitat Polit`ecnica de Catalunya, Barcelona,
October.
Arne Mauser, Richard Zens, Evgeny Matusov, Saˇsa
Hasan, and Hermann Ney. 2006. The RWTH Statisti-
cal Machine Translation System for the IWSLT 2006
Evaluation. In Proc. of the International Workshop on
Spoken Language Translation, pages 103–110, Kyoto,
Japan.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proc. of the 40th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 295–302, Philadelphia, PA, July.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51, March.
Maja Popovi´c and Hermann Ney. 2004. Towards the
Use of Word Stems and Suffixes for Statistical Ma-
chine Translation. In 4th International Conference on
Language Resources and Evaluation (LREC), pages
1585–1588, Lisbon, Portugal, May.
Maja Popovi´c and Hermann Ney. 2005. Exploiting
Phrasal Lexica and Additional Morpho-syntactic Lan-
guage Resources for Statistical Machine Translation
with Scarce Training Data. In 10th Annual Conference
of the European Association for Machine Translation
(EAMT), pages 212–218, Budapest, Hungary, May.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 2002. Numerical Recipes
in C++. Cambridge University Press, Cambridge,
UK.
Wade Shen, Richard Zens, Nicola Bertoldi, and Marcello
Federico. 2006. The JHU Workshop 2006 IWSLT
System. In Proc. of the International Workshop on
Spoken Language Translation, pages 59–63, Kyoto,
Japan.
</reference>
<page confidence="0.999528">
39
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.310641">
<title confidence="0.999591">Can We Translate Letters?</title>
<author confidence="0.999601">David Vilar</author>
<author confidence="0.999601">Jan-T Peter</author>
<author confidence="0.999601">Hermann Ney</author>
<affiliation confidence="0.754107">Lehrstuhl f¨ur Informatik 6 RWTH Aachen University</affiliation>
<address confidence="0.999748">D-52056 Aachen, Germany</address>
<abstract confidence="0.97902885">Current statistical machine translation systems handle the translation process as the transformation of a string of symbols into another string of symbols. Normally the symbols dealt with are the words in different languages, sometimes with some additional information included, like morphological data. In this work we try to push the approach to the limit, working not on the level of words, but treating both the source and target sentences as a string of letters. We try to find out if a nearly unmodified state-of-the-art translation system is able to cope with the problem and whether it is capable to further generalize translation rules, for example at the level of word suffixes and translation of unseen words. Experiments are carried out for the translation of Catalan to Spanish.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Kevin Knight</author>
</authors>
<title>Machine transliteration of names in arabic text.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 workshop on Computational approaches to semitic languages,</booktitle>
<pages>1--13</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="15169" citStr="Al-Onaizan and Knight, 2002" startWordPosition="2546" endWordPosition="2549">lable. The idea of translating unknown words in a letterbased fashion can also have applications to state-ofthe-art translation systems. Nowadays most automatic translation projects and evaluations deal with translation from Chinese or Arabic to English. For these language pairs the translation of named entities poses an additional problem, as many times they were not previously seen in the training data and they are actually one of the most informative words in the texts. The “translation” of these entities is in most cases actually a (more or less phonetic) transliteration, see for example (Al-Onaizan and Knight, 2002). Using the proposed approach for the translation of these words can provide a tighter integration in the translation process and hopefully increase the translation performance, in the same way as it helps for the case of the Catalan-Spanish translation for unseen words. Somewhat related to this problem, we can find an additional application in the field of speech recognition. The task of grapheme-to-phoneme conversion aims at increasing the vocabulary an ASR system can recognize, without the need for additional 36 BLEU WER PER Word-Based System Full Corpus 78.9 11.4 10.6 10k 74.0 13.9 13.2 1k</context>
</contexts>
<marker>Al-Onaizan, Knight, 2002</marker>
<rawString>Yaser Al-Onaizan and Kevin Knight. 2002. Machine transliteration of names in arabic text. In Proceedings of the ACL-02 workshop on Computational approaches to semitic languages, pages 1–13, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Max Bisani</author>
<author>Hermann Ney</author>
</authors>
<title>Investigations on joint-multigram models for grapheme-to-phoneme conversion.</title>
<date>2002</date>
<booktitle>In Proceedings of the 7th International Conference on Spoken Language Processing,</booktitle>
<volume>1</volume>
<pages>105--108</pages>
<location>Denver, CO,</location>
<contexts>
<context position="17899" citStr="Bisani and Ney, 2002" startWordPosition="2980" endWordPosition="2983">mendaria por ejemplo que intentase acercarse a alg´un pais vecino tambi´en como pueden ser los paises centroamericanos, un poco m´as al norte Panam´a. Figure 2: Example translations of the different approaches. For the word-based system an unknown word has been explicitly marked. 37 Word-Based Letter-Based Combined 80 75 70 65 60 55 50 0 5000 10000 15000 20000 25000 30000 35000 40000 Figure 3: Translation quality depending of the corpus size. acoustic data. The problem can be formulated as a translation from graphemes (“letters”) to a sequence of graphones (“pronunciations”), see for example (Bisani and Ney, 2002). The proposed letter-based approach can also be adapted to this task. Lastly, a combination of both, word-based and letter-based models, working in parallel and perhaps taking into account additional information like base forms, can be helpful when translating from or into rich inflexional languages, like for example Spanish. 5 Conclusions We have investigated the possibility of building a letter-based system for translation between related languages. The performance of the approach is quite acceptable, although, as expected, the quality of the word-based approach is superior. The combination</context>
</contexts>
<marker>Bisani, Ney, 2002</marker>
<rawString>Max Bisani and Hermann Ney. 2002. Investigations on joint-multigram models for grapheme-to-phoneme conversion. In Proceedings of the 7th International Conference on Spoken Language Processing, volume 1, pages 105–108, Denver, CO, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="5074" citStr="Brown et al., 1993" startWordPosition="845" endWordPosition="848">his work is more academical, in Section 4 we discuss possible practical applications for this approach. The paper concludes in Section 5. 2 From Words To Letters In the standard approach to statistical machine translation we are given a sentence (sequence of words) fJ1 = f1 ... fJ in a source language which is to be translated into a sentence ˆeI1 = ˆe1 ... ˆeI in a target language. Bayes decision rule states that we should choose the sentence which maximizes the posterior probability ˆeI1 = argmax p(eI1|fJ1 ), (1) el where the argmax operator denotes the search process. In the original work (Brown et al., 1993) the posterior probability p(eI1|fJ1 ) is decomposed following a noisy-channel approach, but current stateof-the-art systems model the translation probability directly using a log-linear model(Och and Ney, 2002): IJexp (EM1 λmhm(e1,f1 )) J � (EM � , ˜eI 1 exp m=1 λmhm(˜eI 1, fJ 1 ) (2) with hm different models, λm scaling factors and the denominator a normalization factor that can be ignored in the maximization process. The λm are usually chosen by optimizing a performance measure over a development corpus using a numerical optimization algorithm like the downhill simplex algorithm (Press et a</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Conejero</author>
<author>J Gimnez</author>
<author>V Arranz</author>
<author>A Bonafonte</author>
<author>N Pascual</author>
<author>N Castell</author>
<author>A Moreno</author>
</authors>
<title>Lexica and corpora for speech-to-speech translation: A trilingual approach.</title>
<date>2003</date>
<booktitle>In European Conf. on Speech Communication and Technology,</booktitle>
<pages>1593--1596</pages>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="10765" citStr="Conejero et al., 2003" startWordPosition="1793" endWordPosition="1796">ge for the letter system can actually be optimized, in the sense that the letters can act as “indices” themselves for addressing the phrase table and the auxiliary mapping structure is not necessary any more. Furthermore the characters can be stored in only one byte, which provides a significant memory gain over the word based system where normally four bytes are used for storing the indices. These gains however are not expected to counteract the other issues presented in this section. 3 Experimental Results The corpus used for our experiment was built in the framework of the LC-STAR project (Conejero et al., 2003). It consists of spontaneous dialogues in Spanish, Catalan and English2 in the tourism and travelling domain. The test corpus (and an additional development corpus for parameter optimization) was randomly extracted, the rest of the sentences were used as training data. Statistics for the corpus can be seen in Table 1. Details of the translation system used can be found in (Mauser et al., 2006). The results of the word-based and letter-based approaches can be seen in Table 2 (rows with label “Full Corpus”). The high BLEU scores (up to nearly 80%) denote that the quality of the translation is qu</context>
</contexts>
<marker>Conejero, Gimnez, Arranz, Bonafonte, Pascual, Castell, Moreno, 2003</marker>
<rawString>D. Conejero, J. Gimnez, V. Arranz, A. Bonafonte, N. Pascual, N. Castell, and A. Moreno. 2003. Lexica and corpora for speech-to-speech translation: A trilingual approach. In European Conf. on Speech Communication and Technology, pages 1593–1596, Geneva, Switzerland, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adri`a de Gispert</author>
</authors>
<title>Introducing Linguistic Knowledge into Statistical Machine Translation.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>Universitat Polit`ecnica de Catalunya,</institution>
<location>Barcelona,</location>
<marker>de Gispert, 2006</marker>
<rawString>Adri`a de Gispert. 2006. Introducing Linguistic Knowledge into Statistical Machine Translation. Ph.D. thesis, Universitat Polit`ecnica de Catalunya, Barcelona, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arne Mauser</author>
<author>Richard Zens</author>
<author>Evgeny Matusov</author>
<author>Saˇsa Hasan</author>
<author>Hermann Ney</author>
</authors>
<title>Evaluation.</title>
<date>2006</date>
<booktitle>The RWTH Statistical Machine Translation System for the IWSLT</booktitle>
<pages>103--110</pages>
<location>Kyoto, Japan.</location>
<contexts>
<context position="11161" citStr="Mauser et al., 2006" startWordPosition="1860" endWordPosition="1863">ese gains however are not expected to counteract the other issues presented in this section. 3 Experimental Results The corpus used for our experiment was built in the framework of the LC-STAR project (Conejero et al., 2003). It consists of spontaneous dialogues in Spanish, Catalan and English2 in the tourism and travelling domain. The test corpus (and an additional development corpus for parameter optimization) was randomly extracted, the rest of the sentences were used as training data. Statistics for the corpus can be seen in Table 1. Details of the translation system used can be found in (Mauser et al., 2006). The results of the word-based and letter-based approaches can be seen in Table 2 (rows with label “Full Corpus”). The high BLEU scores (up to nearly 80%) denote that the quality of the translation is quite good for both systems. The word2The English part of the corpus was not used in our experiments. 35 Spanish Catalan Training Sentences 40 574 Running Words 482 290 485 514 Vocabulary 14 327 12 772 Singletons 6 743 5 930 Test Sentences 972 Running Words 12 771 12 973 OOVs [%] 1.4 1.3 Table 1: Corpus Statistics based system outperforms the letter-based one, as expected, but the letter-based s</context>
</contexts>
<marker>Mauser, Zens, Matusov, Hasan, Ney, 2006</marker>
<rawString>Arne Mauser, Richard Zens, Evgeny Matusov, Saˇsa Hasan, and Hermann Ney. 2006. The RWTH Statistical Machine Translation System for the IWSLT 2006 Evaluation. In Proc. of the International Workshop on Spoken Language Translation, pages 103–110, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>295--302</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="5285" citStr="Och and Ney, 2002" startWordPosition="875" endWordPosition="878">anslation we are given a sentence (sequence of words) fJ1 = f1 ... fJ in a source language which is to be translated into a sentence ˆeI1 = ˆe1 ... ˆeI in a target language. Bayes decision rule states that we should choose the sentence which maximizes the posterior probability ˆeI1 = argmax p(eI1|fJ1 ), (1) el where the argmax operator denotes the search process. In the original work (Brown et al., 1993) the posterior probability p(eI1|fJ1 ) is decomposed following a noisy-channel approach, but current stateof-the-art systems model the translation probability directly using a log-linear model(Och and Ney, 2002): IJexp (EM1 λmhm(e1,f1 )) J � (EM � , ˜eI 1 exp m=1 λmhm(˜eI 1, fJ 1 ) (2) with hm different models, λm scaling factors and the denominator a normalization factor that can be ignored in the maximization process. The λm are usually chosen by optimizing a performance measure over a development corpus using a numerical optimization algorithm like the downhill simplex algorithm (Press et al., 2002). The most widely used models in the log linear combination are phrase-based models in sourceto-target and target-to-source directions, ibm1-like scores computed at phrase level, also in source-totarget</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 295–302, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="8797" citStr="Och and Ney, 2003" startWordPosition="1469" endWordPosition="1472"> it does not constitute a restriction of the generalization capabilities the model can have in creating “new words”. Somehow surprisingly, an additional word language model did not help. While the vocabulary size is reduced, the average sentence length increases, as we consider each letter to be a unit by itself. This has a negative impact in the running time of the actual implementation of the algorithms, specially for the alignment process. In order to alleviate this, the alignment process was split into two passes. In the first part, a word alignment was computed (using the GIZA++ toolkit (Och and Ney, 2003)). Then the training sentences were split according to this alignment (in a similar way to the standard phrase extraction algorithm), so that the length of the source and target part is around thirty letters. Then, a letter-based alignment is computed. 2.2 Efficiency Issues Somewhat counter-intuitively, the reduced vocabulary size does not necessarily imply a reduced mem1For the word-based system this is also the case. ory footprint, at least not without a dedicated program optimization. As in a sensible implementations of nearly all natural language processing tools, the words are mapped to i</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maja Popovi´c</author>
<author>Hermann Ney</author>
</authors>
<title>Towards the Use of Word Stems and Suffixes for Statistical Machine Translation.</title>
<date>2004</date>
<booktitle>In 4th International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>1585--1588</pages>
<location>Lisbon, Portugal,</location>
<marker>Popovi´c, Ney, 2004</marker>
<rawString>Maja Popovi´c and Hermann Ney. 2004. Towards the Use of Word Stems and Suffixes for Statistical Machine Translation. In 4th International Conference on Language Resources and Evaluation (LREC), pages 1585–1588, Lisbon, Portugal, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maja Popovi´c</author>
<author>Hermann Ney</author>
</authors>
<title>Exploiting Phrasal Lexica and Additional Morpho-syntactic Language Resources for Statistical Machine Translation with Scarce Training Data.</title>
<date>2005</date>
<booktitle>In 10th Annual Conference of the European Association for Machine Translation (EAMT),</booktitle>
<pages>212--218</pages>
<location>Budapest, Hungary,</location>
<marker>Popovi´c, Ney, 2005</marker>
<rawString>Maja Popovi´c and Hermann Ney. 2005. Exploiting Phrasal Lexica and Additional Morpho-syntactic Language Resources for Statistical Machine Translation with Scarce Training Data. In 10th Annual Conference of the European Association for Machine Translation (EAMT), pages 212–218, Budapest, Hungary, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William H Press</author>
<author>Saul A Teukolsky</author>
<author>William T Vetterling</author>
<author>Brian P Flannery</author>
</authors>
<title>Numerical Recipes in C++.</title>
<date>2002</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="5683" citStr="Press et al., 2002" startWordPosition="945" endWordPosition="948">al., 1993) the posterior probability p(eI1|fJ1 ) is decomposed following a noisy-channel approach, but current stateof-the-art systems model the translation probability directly using a log-linear model(Och and Ney, 2002): IJexp (EM1 λmhm(e1,f1 )) J � (EM � , ˜eI 1 exp m=1 λmhm(˜eI 1, fJ 1 ) (2) with hm different models, λm scaling factors and the denominator a normalization factor that can be ignored in the maximization process. The λm are usually chosen by optimizing a performance measure over a development corpus using a numerical optimization algorithm like the downhill simplex algorithm (Press et al., 2002). The most widely used models in the log linear combination are phrase-based models in sourceto-target and target-to-source directions, ibm1-like scores computed at phrase level, also in source-totarget and target-to-source directions, a target language model and different penalties, like phrase penalty and word penalty. This same approach can be directly adapted to the letter-based translation framework. In this case we are given a sequence of letters F 1corresponding to a source (word) string fJ1 , which is to be translated into a sequence of letters £i corresponding to a string eI1 in a tar</context>
</contexts>
<marker>Press, Teukolsky, Vetterling, Flannery, 2002</marker>
<rawString>William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. 2002. Numerical Recipes in C++. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wade Shen</author>
<author>Richard Zens</author>
<author>Nicola Bertoldi</author>
<author>Marcello Federico</author>
</authors>
<title>IWSLT System.</title>
<date>2006</date>
<booktitle>The JHU Workshop</booktitle>
<pages>59--63</pages>
<location>Kyoto, Japan.</location>
<contexts>
<context position="1926" citStr="Shen et al., 2006" startWordPosition="307" endWordPosition="310">he models, but also has drawbacks, as related word forms, like different verb tenses or pluralsingular word pairs, are treated as completely different entities. Some efforts have been made e.g. to integrate more information about the words in the form of Part 33 Of Speech tags (Popovi´c and Ney, 2005), using additional information about stems and suffixes (Popovi´c and Ney, 2004) or to reduce the morphological variability of the words (de Gispert, 2006). State of the art decoders provide the ability of handling different word forms directly in what has been called factored translation models (Shen et al., 2006). In this work, we try to go a step further and treat the words (and thus whole sentences) as sequences of letters, which have to be translated into a new sequence of letters. We try to find out if the translation models can generalize and generate correct words out of the stream of letters. For this approach to work we need to translate between two related languages, in which a correspondence between the structure of the words can be found. For this experiment we chose a Catalan-Spanish corpus. Catalan is a romance language spoken in the north-east of Spain and Andorra and is considered by so</context>
</contexts>
<marker>Shen, Zens, Bertoldi, Federico, 2006</marker>
<rawString>Wade Shen, Richard Zens, Nicola Bertoldi, and Marcello Federico. 2006. The JHU Workshop 2006 IWSLT System. In Proc. of the International Workshop on Spoken Language Translation, pages 59–63, Kyoto, Japan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>