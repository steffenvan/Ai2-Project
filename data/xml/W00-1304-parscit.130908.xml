<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000352">
<title confidence="0.9984785">
Coaxing Confidences from an Old Friend:
Probabilistic Classifications from Transformation Rule Lists
</title>
<author confidence="0.992622">
Radu Florian* John C. Hendersont Grace Ngai*
</author>
<affiliation confidence="0.769741333333333">
*Department of Computer Science tThe MITRE Corporation
Johns Hopkins University 202 Burlington Road
Baltimore, MD 21218, USA Bedford, MA 01730, USA
</affiliation>
<email confidence="0.998147">
{rflorian,gyn}@cs.jhu.edu jhndrsn@mitre.org
</email>
<sectionHeader confidence="0.997374" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999388722222222">
Transformation-based learning has been success-
fully employed to solve many natural language
processing problems. It has many positive fea-
tures, but one drawback is that it does not provide
estimates of class membership probabilities.
In this paper, we present a novel method for
obtaining class membership probabilities from a
transformation-based rule list classifier. Three ex-
periments are presented which measure the model-
ing accuracy and cross-entropy of the probabilistic
classifier on unseen data and the degree to which
the output probabilities from the classifier can be
used to estimate confidences in its classification
decisions.
The results of these experiments show that, for
the task of text chunking&apos;, the estimates produced
by this technique are more informative than those
generated by a state-of-the-art decision tree.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997821297297297">
In natural language processing, a great amount of
work has gone into the development of machine
learning algorithms which extract useful linguistic
information from resources such as dictionaries,
newswire feeds, manually annotated corpora and
web pages. Most of the effective methods can
be roughly divided into rule-based and proba-
bilistic algorithms. In general, the rule-based
methods have the advantage of capturing the
necessary information in a small and concise set
of rules. In part-of-speech tagging, for exam-
ple, rule-based and probabilistic methods achieve
comparable accuracies, but rule-based methods
capture the knowledge in a hundred or so simple
rules, while the probabilistic methods have a
very high-dimensional parameter space (millions
of parameters).
One of the main advantages of probabilistic
methods, on the other hand, is that they include a
measure of uncertainty in their output. This can
take the form of a probability distribution over
potential outputs, or it may be a ranked list of
1A11 the experiments are performed on text chunking.
The technique presented is general-purpose, however, and
can be applied to many tasks for which transformation-
based learning performs well, without changing the inter-
nals of the learner.
candidate outputs. These uncertainty measures
are useful in situations where both the classifi-
cation of an sample and the system&apos;s confidence
in that classification are needed. An example of
this is a situation in an ensemble system where
ensemble members disagree and a decision must
be made about how to resolve the disagreement.
A similar situation arises in pipeline systems, such
as a system which performs parsing on the output
of a probabilistic part-of-speech tagging.
Transformation-based learning (TBL) (Brill,
1995) is a successful rule-based machine learning
algorithm in natural language processing. It has
been applied to a wide variety of tasks, including
part of speech tagging (Roche and Schabes, 1995;
Brill, 1995), noun phrase chunking (Ramshaw and
Marcus, 1999), parsing (Brill, 1996; Vilain and
Day, 1996), spelling correction (Mangu and Brill,
1997), prepositional phrase attachment (Brill and
Resnik, 1994), dialog act tagging (Samuel et
al., 1998), segmentation and message understand-
ing (Day et al., 1997), often achieving state-
of-the-art performance with a small and easily-
understandable list of rules.
In this paper, we describe a novel method
which enables a transformation-based classifier to
generate a probability distribution on the class
labels. Application of the method allows the
transformation rule list to retain the robustness of
the transformation-based algorithms, while bene-
fitting from the advantages of a probabilistic clas-
sifier. The usefulness of the resulting probabilities
is demonstrated by comparison with another state-
of-the-art classifier, the C4.5 decision tree (Quin-
lan, 1993). The performance of our algorithm
compares favorably across many dimensions: it
obtains better perplexity and cross-entropy; an
active learning algorithm using our system outper-
forms a similar algorithm using decision trees; and
finally, our algorithm has better rejection curves
than a similar decision tree. Section 2 presents the
transformation based learning paradigm; Section
3 describes the algorithm for construction of the
decision tree associated with the transformation
based list; Section 4 describes the experiments
in detail and Section 5 concludes the paper and
outlines the future work.
</bodyText>
<page confidence="0.99542">
26
</page>
<sectionHeader confidence="0.837467" genericHeader="introduction">
2 Transformation rule lists
</sectionHeader>
<bodyText confidence="0.996664555555556">
The central idea of transformation-based learn-
ing is to learn an ordered list of rules which
progressively improve upon the current state of
the training set. An initial assignment is made
based on simple statistics, and then rules are
greedily learned to correct the mistakes, until no
net improvement can be made.
These definitions and notation will be used
throughout the paper:
</bodyText>
<listItem confidence="0.995749">
• X denotes the sample space;
• C denotes the set of possible classifications of
the samples;
• The state space is defined as $ = X x C.
• 71- will usually denote a predicate defined on
X;
• A rule r is defined as a predicate — class label
— time tuple, (ir, c, t), c E C, t E N, where t is
the learning iteration in which when the rule
was learned, its position in the list.
• A rule r = c, t) applies to a state (x, y) if
ir(x) = true and c y.
Using a TBL framework to solve a problem as-
sumes the existence of:
• An initial class assignment (mapping from X
to S). This can be as simple as the most
common class label in the training set, or it
can be the output from another classifier.
• A set of allowable templates for rules. These
templates determine the predicates the rules
will test, and they have the biggest influence
over the behavior of the system.
• An objective function for learning. Unlike in
many other learning algorithms, the objective
function for TBL will typically optimize the
evaluation function. An often-used method is
the difference in performance resulting from
applying the rule.
</listItem>
<bodyText confidence="0.99483775">
At the beginning of the learning phase, the
training set is first given an initial class assign-
ment. The system then iteratively executes the
following steps:
</bodyText>
<listItem confidence="0.980712166666667">
1. Generate all productive rules.
2. For each rule:
(a) Apply to a copy of the most recent state
of the training set.
(b) Score the result using the objective func-
tion.
3. Select the rule with the best score.
4. Apply the rule to the current state of the
training set, updating it to reflect this change.
5. Stop if the score is smaller than some pre-set
threshold T.
6. Repeat from Step 1.
</listItem>
<bodyText confidence="0.9998278">
The system thus learns a list of rules in a greedy
fashion, according to the objective function. When
no rule that improves the current state of the
training set beyond the pre-set threshold can
be found, the training phase ends. During the
evaluation phase, the evaluation set is initialized
with the same initial class assignment. Each rule
is then applied, in the order it was learned, to the
evaluation set. The final classification is the one
attained when all rules have been applied.
</bodyText>
<sectionHeader confidence="0.953273" genericHeader="method">
3 Probability estimation with
transformation rule lists
</sectionHeader>
<bodyText confidence="0.999878642857143">
Rule lists are infamous for making hard decisions,
decisions which adhere entirely to one possibility,
excluding all others. These hard decisions are
often accurate and outperform other types of
classifiers in terms of exact-match accuracy, but
because they do not have an associated proba-
bility, they give no hint as to when they might
fail. In contrast, probabilistic systems make soft
decisions by assigning a probability distribution
over all possible classes.
There are many applications where soft deci-
sions prove useful. In situations such as active
learning, where a small number of samples are
selected for annotation, the probabilities can be
used to determine which examples the classifier
was most unsure of, and hence should provide the
most extra information. A probabilistic system
can also act as a filter for a more expensive
system or a human expert when it is permitted
to reject samples. Soft decision-making is also
useful when the system is one of the components
in a larger decision-making process, as is the case
in speech recognition systems (Bahl et al., 1989),
or in an ensemble system like AdaBoost (Freund
and Schapire, 1997). There are many other
applications in which a probabilistic classifier is
necessary, and a non-probabilistic classifier cannot
be used instead.
</bodyText>
<subsectionHeader confidence="0.9951825">
3.1 Estimation via conversion to decision
tree
</subsectionHeader>
<bodyText confidence="0.99995075">
The method we propose to obtain probabilis-
tic classifications from a transformation rule list
involves dividing the samples into equivalence
classes and computing distributions over each
equivalence class. At any given point in time i,
each sample x in the training set has an associated
state s•(x) = (x, y). Let R(x) to be the set of rules
ri that applies to the state si(x),
</bodyText>
<equation confidence="0.94397">
R(x) = {ri E Riri applies to si(x)}
</equation>
<bodyText confidence="0.9982975">
An equivalence class consists of all the samples
x that have the same R(x). Class probability
assignments are then estimated using statistics
computed on the equivalence classes.
</bodyText>
<page confidence="0.996435">
27
</page>
<bodyText confidence="0.918342454545455">
An illustration of the conversion from a rule
list to a decision tree is shown below. Table 1
shows an example transformation rule list. It is
straightforward to convert this rule list into a de-
cision pylon (Bahl et al., 1989), which can be used
to represent all the possible sequences of labels
assigned to a sample during the application of the
TBL algorithm. The decision pylon associated
with this particular rule list is displayed on the left
side of Figure 1. The decision tree shown on the
right side of Figure 1 is constructed such that the
samples stored in any leaf have the same class label
sequence as in the displayed decision pylon. In
the decision pylon, &amp;quot;no&amp;quot; answers go straight down;
in the decision tree, &amp;quot;yes&amp;quot; answers take the right
branch. Note that a one rule in the transformation
rule list can often correspond to more than one
node in the decision tree.
Initial label = A
If Q1 and label=A then labe14--B
If Q2 and label=A then labeli–B
If Q3 and label=B then labeli--A
</bodyText>
<tableCaption confidence="0.991313">
Table 1: Example of a Transformation Rule List.
</tableCaption>
<figureCaption confidence="0.8889405">
Figure 1: Converting the transformation rule list
from Table 1 to a decision tree.
</figureCaption>
<bodyText confidence="0.999974909090909">
The conversion from a transformation rule list
to a decision tree is presented as a recursive
procedure. The set of samples in the training set
is transformed to a set of states by applying the
initial class assignments. A node 71 is created for
each of the initial class label assignments c and all
states labeled c are assigned to n.
The following recursive procedure is invoked
with an initial &amp;quot;root&amp;quot; node, the complete set of
states (from the corpus) and the whole sequence
of rules learned during training:
</bodyText>
<sectionHeader confidence="0.432583" genericHeader="method">
Algorithm: RuleListToDecisionTree
(RLTDT)
Input:
</sectionHeader>
<listItem confidence="0.997409375">
• A set B of N states ((xi, yi) (xN, yN)) with
labels yi E C;
• A set R of M rules (ro, rm) where ri =
yi, i).
Do:
1. If 72. is empty, the end of the rule list has been
reached. Create a leaf node, n, and estimate
the probability class distribution based on the
true classifications of the states in B. Return
n.
2. Let rj = (Tri, j) be the lowest-indexed rule
in R,. Remove it from R.
3. Split the data in B using the predicate rj and
the current hypothesis such that samples on
which it1 returns true are on the right of the
split:
</listItem>
<equation confidence="0.9876475">
= E Biri(x) = false}
BR = {X E Blri(x) = true}
</equation>
<listItem confidence="0.997150333333333">
4. If IBL, I &gt; K and IBRI &gt; K, the split is
acceptable:
(a) Create a new internal node, n;
(b) Set the question: q(n) = ri;
(c) Create the left child of n using a recursive
call to RLTDT(BL, R);
(d) Create the right child of n using a recur-
sive call to RLTDT(BR,R.);
(e) Return node n.
</listItem>
<bodyText confidence="0.998269333333333">
Otherwise, no split is performed using r.
Repeat from Step 1.
The parameter K is a constant that determines the
minimum weight that a leaf is permitted to have,
effectively pruning the tree during construction.
In all the experiments, K was set to 5.
</bodyText>
<subsectionHeader confidence="0.998845">
3.2 Further growth of the decision tree
</subsectionHeader>
<bodyText confidence="0.9999355">
When a rule list is converted into a decision tree,
there are often leaves that are inordinately heavy
because they contain a large number of samples.
Examples of such leaves are those containing
samples which were never transformed by any
of the rules in the rule list. These populations
exist either because they could not be split up
during the rule list learning without incurring a
net penalty, or because any rule that acts on them
has an objective function score of less than the
threshold T. This is sub-optimal for estimation
because when a large portion of the corpus falls
into the same equivalence class, the distribution
assigned to it reflects only the mean of those
samples. The undesirable consequence is that all
of those samples are given the same probability
distribution.
To ameliorate this problem, those samples are
partitioned into smaller equivalence classes by
further growing the decision tree. Since a decision
tree does not place all the samples with the same
current label into a single equivalence class, it does
not get stuck in the same situation as a rule list
— in which no change in the current state of
corpus can be made without incurring a net loss
in performance.
</bodyText>
<page confidence="0.991348">
28
</page>
<bodyText confidence="0.992963233333333">
Continuing to grow the decision tree that was
converted from a rule list can be viewed from
another angle. A highly accurate prefix tree
for the final decision tree is created by tying
questions together during the first phase of the
growth process (TBL). Unlike traditional decision
trees which select splitting questions for a node
by looking only at the samples contained in the
local node, this decision tree selects questions by
looking at samples contained in all nodes on the
frontier whose paths have a suffix in common. An
illustration of this phenomenon can be seen in
Figure 1, where the choice to split on Question
3 was made from samples which tested false
on the predicate of Question 1, together with
samples which tested false on the predicate of
Question 2. The result of this is that questions
are chosen based on a much larger population than
in standard decision tree growth, and therefore
have a much greater chance of being useful and
generalizable. This alleviates the problem of over-
partitioning of data, which is a widely-recognized
concern during decision tree growth.
The decision tree obtained from this conversion
can be grown further. When the rule list R. is
exhausted at Step 1, instead of creating a leaf
node, continue splitting the samples contained in
the node with a decision tree induction algorithm.
The splitting criterion used in the experiments is
the information gain measure.
</bodyText>
<sectionHeader confidence="0.999495" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999899137931035">
Three experiments that demonstrate the effec-
tiveness and appropriateness of our probability
estimates are presented in this section. The
experiments are performed on text chunking, a
subproblem of syntactic parsing. Unlike full pars-
ing, the sentences are divided into non-overlapping
phrases, where each word belongs to the lowest
parse constituent that dominates it.
The data used in all of these experiments is
the CoNLL-2000 phrase chunking corpus (CoNLL,
2000). The corpus consists of sections 15-18 and
section 20 of the Penn Treebank (Marcus et al.,
1993), and is pre-divided into a 8936-sentence
(211727 tokens) training set and a 2012-sentence
(47377 tokens) test set. The chunk tags are
derived from the parse tree constituents, and the
part-of-speech tags were generated by the Brill
tagger (Brill, 1995).
As was noted by Ramshaw &amp; Marcus (1999),
text chunking can be mapped to a tagging task,
where each word is tagged with a chunk tag
representing the phrase that it belongs to. An
example sentence from the corpus is shown in
Table 4. As a contrasting system, our results
are compared with those produced by a 04.5
decision tree system (henceforth C4.5). The
reason for using C4.5 is twofold: firstly, it is a
widely-used algorithm which achieves state-of-the-
art performance on a broad variety of tasks; and
</bodyText>
<table confidence="0.999463333333333">
Word POS tag Chunk Tag
A.P. NNP B-NP
Green NNP I-NP
currently RB B-ADVP
has VBZ B-VP
2,664,098 CD B-NP
shares NNS I-NP
outstanding JJ B-ADJP
. 0
</table>
<tableCaption confidence="0.999584">
Table 2: Example of a sentence with chunk tags
</tableCaption>
<bodyText confidence="0.997446760869565">
secondly, it belongs to the same class of classifiers
as our converted transformation-based rule list
(henceforth TBLDT).
To perform a fair evaluation, extra care was
taken to ensure that both 04.5 and TBLDT
explore as similar a sample space as possible. The
systems were allowed to consult the word, the
part-of-speech, and the chunk tag of all examples
within a window of 5 positions (2 words on either
side) of each target example.2 Since multiple
features covering the entire vocabulary of the
training set would be too large a space for 04.5
to deal with, in all of experiments where TBLDT
is directly compared with 04.5, the word types
that both systems can include in their predicates
are restricted to the most &amp;quot;ambiguous&amp;quot; 100 words
in the training set, as measured by the number of
chunk tag types that are assigned to them. The
initial prediction was made for both systems using
a class assignment based solely on the part-of-
speech tag of the word.
Considering chunk tags within a contextual win-
dow of the target word raises a problem with 04.5.
A decision tree generally trains on independent
samples and does not take into account changes
of any features in the context. In our case, the
samples are dependent; the classification of sample
i is a feature for sample i + 1, which means that
changing the classification for sample i affects
the context of sample i + 1. To address this
problem, the 04.5 systems are trained with the
correct chunks in the left context. When the
system is used for classification, input is processed
in a left-to-right manner; and the output of the
system is fed forward to be used as features
in the left context of following samples. Since
04.5 generates probabilities for each classification
decision, they can be redirected into the input for
the next position. Providing the decision tree with
this confidence information effectively allows it to
perform a limited search over the entire sentence.
C4.5 does have one advantage over TBLDT,
however. A decision tree can be trained using the
subsetting feature, where questions asked are of
the form: &amp;quot;does feature f belong to the set F?&amp;quot;.
This is not something that a TBL can do readily,
</bodyText>
<footnote confidence="0.9621355">
2The TBL templates are similar to those used in
Ramshaw and Marcus (1999).
</footnote>
<page confidence="0.99848">
29
</page>
<bodyText confidence="0.999478666666667">
but since the objective is in comparing TBLDT to
another state-of-the-art system, this feature was
enabled.
</bodyText>
<subsectionHeader confidence="0.976614">
4.1 Evaluation Measures
</subsectionHeader>
<bodyText confidence="0.874946695652174">
The most commonly used measure for evaluating
tagging tasks is tag accuracy. It is defined as
# of correctly tagged examples
Accuracy =
# of examples
In syntactic parsing, though, since the task is
to identify the phrasal components, it is more
appropriate to measure the precision and recall:
# of correct proposed phrases
# of proposed phrases
# of correct proposed phrases
# of correct phrases
To facilitate the comparison of systems with dif-
ferent precision and recall, the F-measure metric
is computed as a weighted harmonic mean of
precision and recall:
The /3 parameter is used to give more weight to
precision or recall, as the task at hand requires.
In all our experiments, is set to 1, giving equal
weight to precision and recall.
The reported performances are all measured
with the evaluation tool provided with the CoNLL
corpus (CoNLL, 2000).
</bodyText>
<subsectionHeader confidence="0.992912">
4.2 Active Learning
</subsectionHeader>
<bodyText confidence="0.999895648648649">
To demonstrate the usefulness of obtaining proba-
bilities from a transformation rule list, this section
describes an application which utilizes these prob-
abilities, and compare the resulting performance
of the system with that achieved by C4.5.
Natural language processing has traditionally
required large amounts of annotated data from
which to extract linguistic properties. However,
not all data is created equal: a normal distribu-
tion of annotated data contains much redundant
information. Seung et al. (1992) and Freund et
al. (1997) proposed a theoretical active learning
approach, where samples are intelligently selected
for annotation. By eliminating redundant infor-
mation, the same performance can be achieved
while using fewer resources. Empirically, active
learning has been applied to various NLP tasks
such as text categorization (Lewis and Gale, 1994;
Lewis and Catlett, 1994; Liere and Tadepalli,
1997), part-of-speech tagging (Dagan and Engel-
son, 1995; Engelson and Dagan, 1996), and base
noun phrase chunking (Ngai and Yarowsky, 2000),
resulting in significantly large reductions in the
quantity of data needed to achieve comparable
performance.
This section presents two experimental results
which show the effectiveness of the probabilities
generated by the TBLDT. The first experiment
compares the performance achieved by the active
learning algorithm using TBLDT with the perfor-
mance obtained by selecting samples sequentially
from the training set. The second experiment
compares the performances achieved by TBLDT
and C4.5 training on samples selected by active
learning.
The following describes the active learning algo-
rithm used in the experiments:
</bodyText>
<listItem confidence="0.9951726">
1. Label an initial T1 sentences of the corpus;
2. Use the machine learning algorithm (C4.5 or
TBLDT) to obtain chunk probabilities on the
rest of the training data;
3. Choose Ty samples from the rest of the train-
</listItem>
<bodyText confidence="0.882225333333333">
ing set, specifically the samples that optimize
an evaluation function f, based on the class
distribution probability of each sample;
</bodyText>
<listItem confidence="0.7323318">
4. Add the samples, including their &amp;quot;true&amp;quot; classi-
fication3 to the training pool and retrain the
system;
5. If a desired number of samples is reached,
stop, otherwise repeat from Step 2.
</listItem>
<bodyText confidence="0.998815">
The evaluation function f that was used in our
experiments is:
</bodyText>
<equation confidence="0.862814">
1
f (S) = E H (CIS,i)
</equation>
<bodyText confidence="0.999268952380952">
where H (CS, i) is the entropy of the chunk
probability distribution associated with the word
index i in sentence S.
Figure 2 displays the performance (F-measure
and chunk accuracy) of a TBLDT system trained
on samples selected by active learning and the
same system trained on samples selected sequen-
tially from the corpus versus the number of words
in the annotated training set. At each step of
the iteration, the active learning-trained TBLDT
system achieves a higher accuracy/F-measure, or,
conversely, is able to obtain the same performance
level with less training data. Overall, our system
can yield the same performance as the sequential
system with 45% less data, a significant reduction
in the annotation effort.
Figure 3 shows a comparison between two active
learning experiments: one using TBLDT and the
other using C4.5.4 For completeness, a sequential
run using C4.5 is also presented. Even though
C4.5 examines a larger space than TBLDT by
</bodyText>
<tableCaption confidence="0.623236333333333">
3The true (reference or gold standard) classification is
available in this experiment. In an annotation situation,
the samples are sent to human annotators for labeling.
4As mentioned earlier, both the TBLDT and C4.5 were
limited to the same 100 most ambiguous words in the
corpus to ensure comparability.
</tableCaption>
<equation confidence="0.9100254">
Precision
Recall =
1}3 =
p x Precision + Recall
(132 + 1) x Precision x Recall
</equation>
<page confidence="0.930315">
30
</page>
<figure confidence="0.998129">
(a) F-measure vs. number of words in training set (b) Chunk Accuracy vs. number of words in training
set
</figure>
<figureCaption confidence="0.988703">
Figure 2: Performance of the TBLDT system versus sequential choice.
</figureCaption>
<figure confidence="0.994302692307692">
88
87
85
85
84
87
82
0 5CCO 10033 15070 20000
Norm, of womb 1 treirrg rt
87
92.5
995
80
89.5
89 o 5003 10:00 75070 20000
Huffer 41 words in trairo set
25CCO
30800
25070
30X0
50:10 79880 19370 20070 25003 30000
810t wore In rhino rt
(a) F-measure vs. number of words in training set
8000 10000 15000 20003 29:00 30090
Krim el wort in Wining dor
(b) Accuracy vs. number of words in training set
</figure>
<figureCaption confidence="0.999989">
Figure 3: Performance of the TBLDT system versus the DT system
</figureCaption>
<bodyText confidence="0.999955">
utilizing the feature subset predicates, TBLDT
still performs better. The difference in accuracy at
26200 words (at the end of the active learning run
for TBLDT) is statistically significant at a 0.0003
level.
As a final remark on this experiment, note that
at an annotation level of 19000 words, the fully
lexicalized TBLDT outperformed the 04.5 system
by making 15% fewer errors.
</bodyText>
<subsectionHeader confidence="0.999121">
4.3 Rejection curves
</subsectionHeader>
<bodyText confidence="0.999852846153846">
It is often very useful for a classifier to be able
to offer confidence scores associated with its deci-
sions. Confidence scores are associated with the
probability P(C(x) correctlx) where C(x) is the
classification of sample x. These scores can be
used in real-life problems to reject samples that
the the classifier is not sure about, in which case
a better observation, or a human decision, might
be requested. The performance of the classifier
is then evaluated on the samples that were not
rejected. This experiment framework is well-
established in machine learning and optimization
research (Dietterich and Bakiri, 1995; Priebe et
al., 1999).
Since non-probabilistic classifiers do not offer
any insights into how sure they are about a
particular classification, it is not easy to obtain
confidence scores from them. A probabilistic
classifier, in contrast, offers information about the
class probability distribution of a given sample.
Two measures that can be used in generating
confidence scores are proposed in this section.
The first measure, the entropy H of the class
probability distribution of a sample x, C(x) =
{p(cilx),p(c21x)...p(ck lx)}, is a measure of the
uncertainty in the distribution:
</bodyText>
<equation confidence="0.904773">
H(C(x)) = — Ep(cilx) log2 p(ci Ix)
</equation>
<bodyText confidence="0.9983415">
The higher the entropy of the distribution of
class probability estimates, the more uncertain the
</bodyText>
<page confidence="0.999876">
31
</page>
<figureCaption confidence="0.999896">
Figure 4: Rejection curves.
</figureCaption>
<figure confidence="0.999849090909091">
0.1
02 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Percent of rejected data
0.99
.5 (soft
0.98
Accuracy Residual
0.94
0.93
0
TA 0.97
2 0.96
0.95
TBL-DT
C43 (soft decision)r-Y-
....••
0.935
0.94 ...............................................
, C4.5 (hard decision) ,
0 02 0.4 0.6 0.8
Probability of the most Rely tag
0.99
0.985
0.98
0.975
0.97
0.965 -
0.96 -
0.955 -
0.95 -
0.945
(a) Subcorpus (batch) rejection
(b) Threshold (online) rejection
</figure>
<bodyText confidence="0.994987463414634">
classifier is of its classification. The samples se-
lected for rejection are chosen by sorting the data
using the entropies of the estimated probabilities,
and then selecting the ones with highest entropies.
The resulting curve is a measure of the correlation
between the true probability distribution and the
one given by the classifier.
Figure 4(a) shows the rejection curves for the
TBLDT system and two C4.5 decision trees - one
which receives a probability distribution as input
(&amp;quot;soft&amp;quot; decisions on the left context) , and one
which receives classifications (&amp;quot;hard&amp;quot; decisions on
all fields). At the left of the curve, no samples
are rejected; at the right side, only the samples
about which the classifiers were most certain are
kept (the samples with minimum entropy). Note
that the y-values on the right side of the curve are
based on less data, effectively introducing wider
variance in the curve as it moves right.
As shown in Figure 4(a), the C4.5 classifier
that has access to the left context chunk tag
probability distributions behaves better than the
other C4.5 system, because this information about
the surrounding context allows it to effectively
perform a shallow search of the classification
space. The TBLDT system, which also receives
a probability distribution on the chunk tags in
the left context, clearly outperforms both C4.5
systems at all rejection levels.
The second proposed measure is based on the
probability of the most likely tag. The assumption
here is that this probability is representative of
how certain the system is about the classifica-
tion. The samples are put in bins based on
the probability of the most likely chunk tag, and
accuracies are computed for each bin (these bins
are cumulative, meaning that a sample will be
included in all the bins that have a lower threshold
than the probability of its most likely chunk
tag). At each accuracy level, a sample will be
rejected if the probability of its most likely chunk
</bodyText>
<table confidence="0.998842">
Model Perplexity Cross Entropy
TBLDT 1.2944 0.2580
DT+probs 1.4150 0.3471
DT 1.4568 0.3763
</table>
<tableCaption confidence="0.8373545">
Table 3: Cross entropy and perplexities for two
C4.5 systems and the TBLDT system
</tableCaption>
<bodyText confidence="0.867739235294118">
is below the accuracy level. The resulting curve
is a measure of the correlation between the true
distribution probability and the probability of the
most likely chunk tag, i.e. how appropriate those
probabilities are as confidence measures. Unlike
the first measure mentioned before, a threshold
obtained using this measure can be used in an
online manner to identify the samples of whose
classification the system is confident.
Figure 4(b) displays the rejection curve for
the second measure and the same three systems.
TBLDT again outperforms both C4.5 systems, at
all levels of confidence.
In summary, the TBLDT system outperforms
both C4.5 systems presented, resulting in fewer re-
jections for the same performance, or, conversely,
better performance at the same rejection rate.
</bodyText>
<subsectionHeader confidence="0.999434">
4.4 Perplexity and Cross Entropy
</subsectionHeader>
<bodyText confidence="0.999733333333333">
Cross entropy is a goodness measure for probabil-
ity estimates that takes into account the accuracy
of the estimates as well as the classification accu-
racy of the system. It measures the performance
of a system trained on a set of samples distributed
according to the probability distribution p when
tested on a set following a probability distribution
q. More specifically, we utilize conditional cross
entropy, which is defined as
</bodyText>
<equation confidence="0.9534505">
H (CIX) = - E q(x) E q(clx) log2 p(clx)
x EX cEC
</equation>
<bodyText confidence="0.9269235">
where X is the set of examples and C is the set of
chunk tags, q is the probability distribution on the
</bodyText>
<page confidence="0.995038">
32
</page>
<table confidence="0.999842769230769">
Chunk Accuracy Precision Recall F1
Type (%) (%) (%)
Overall 95.23 92.02 92.50 92.26
ADJP - 75.69 68.95 72.16
ADVP - 80.88 78.64 79.74
CONJP - 40.00 44.44 42.11
INTJ - 50.00 50.00 50.00
LST - 0.00 0.00 0.00
NP - 92.18 92.72 92.45
PP - 95.89 97.90 96.88
PRT - 67.80 75.47 71.43
SBAR - 88.71 82.24 85.35
VP - 92.00 92.87 92.44
</table>
<tableCaption confidence="0.965732">
Table 4: Performance of TBLDT on the CoNLL
Test Set
</tableCaption>
<bodyText confidence="0.9935522">
test document and pis the probability distribution
on the train corpus.
The cross entropy metric fails if any outcome is
given zero probability by the estimator. To avoid
this problem, estimators are &amp;quot;smoothed&amp;quot;, ensuring
that novel events receive non-zero probabilities.
A very simple smoothing technique (interpolation
with a constant) was used for all of these systems.
A closely related measure is perplexity, defined
as
</bodyText>
<equation confidence="0.981511">
P = 2/1(clx)
</equation>
<bodyText confidence="0.999933">
The cross entropy and perplexity results for the
various estimation schemes are presented in Table
3. The TBLDT outperforms both 04.5 systems,
obtaining better cross-entropy and chunk tag per-
plexity. This shows that the overall probability
distribution obtained from the TBLDT system
better matches the true probability distribution.
This strongly suggests that probabilities generated
this way can be used successfully in system com-
bination techniques such as voting or boosting.
</bodyText>
<subsectionHeader confidence="0.998839">
4.5 Chunking performance
</subsectionHeader>
<bodyText confidence="0.999879">
It is worth noting that the transformation-based
system used in the comparative graphs in Figure
3 was not running at full potential. As described
earlier, the TBLDT system was only allowed to
consider words that 04.5 had access to. However,
a comparison between the corresponding TBLDT
curves in Figures 2 (where the system is given
access to all the words) and 3 show that a
transformation-based system given access to all
the words performs better than the one with a
restricted lexicon, which in turn outperforms the
best 04.5 decision tree system both in terms of
accuracy and F-measure.
Table 4 shows the performance of the TBLDT
system on the full CoNLL test set, broken down
by chunk type. Even though the TBLDT results
could not be compared with other published re-
sults on the same task and data (CoNLL will
not take place until September 2000), our system
significantly outperforms a similar system trained
with a C4.5 decision tree, shown in Table 5, both
in chunk accuracy and F-measure.
</bodyText>
<table confidence="0.999409266666667">
Chunk Accuracy Precision Recall PI
Type (%) (%) (%)
Overall 93.80 90.02 90.26 90.14
ADJP - 65.58 64.38 64.98.
75.44-
ADVP - 74.14 76.79
CONJP - 33.33 33.33 33.33
INTJ - 50.00 50.00 50.00
LST - 0.00 0.00 0.00
NP - 91.00 90.93 90.96
PP - 92.70 96.36 94.50
67.9g-
PRT - 71.13 65.09
SBAR - 86.35 61.50 71.83
VP - 90.71 91.22 90.97
</table>
<tableCaption confidence="0.9412235">
Table 5: Performance of C4.5 on the CoNLL Test
Set
</tableCaption>
<sectionHeader confidence="0.998964" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.99981703030303">
In this paper we presented a novel way to convert
transformation rule lists, a common paradigm in
natural language processing, into a form that is
equivalent in its classification behavior, but is
capable of providing probability estimates. Using
this approach, favorable properties of transfor-
mation rule lists that makes them popular for
language processing are retained, while the many
advantages of a probabilistic system are gained.
To demonstrate the efficacy of this approach,
the resulting probabilities were tested in three
ways: directly measuring the modeling accuracy
on the test set via cross entropy, testing the
goodness of the output probabilities in a active
learning algorithm, and observing the rejection
curves attained from these probability estimates.
The experiments clearly demonstrate that the
resulting probabilities perform at least as well as
the ones generated by 04.5 decision trees, resulting
in better performance in all cases. This proves that
the resulting probabilistic classifier is as least as
good as other state-of-the-art probabilistic models.
The positive results obtained suggest that the
probabilistic classifier obtained from transforma-
tion rule lists can be successfully used in machine
learning algorithms that require soft-decision clas-
sifiers, such as boosting or voting. Future research
will include testing the behavior of the system
under AdaBoost (Freund and Schapire, 1997). We
also intend to investigate the effects that other
decision tree growth and smoothing techniques
may have on continued refinement of the converted
rule list.
</bodyText>
<sectionHeader confidence="0.999615" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999861625">
We thank Eric Brill, Fred Jelinek and David
Yarowsky for their invaluable advice and sugges-
tions. In addition we would like to thank David
Day, Ben Wellner and the anonymous reviewers
for their useful comments and suggestions on the
paper.
The views expressed in this paper are those of
the authors and do not necessarily reflect the views
</bodyText>
<page confidence="0.99653">
33
</page>
<bodyText confidence="0.999836">
of the MITRE Corporation. It was performed
as a collaborative effort at both MITRE and
the Center for Language and Speech Processing,
Johns Hopkins University, Baltimore, MD. It was
supported by NSF grants numbered IRI-9502312
and IRI-9618874, as well as the MITRE-Sponsored
Research program.
</bodyText>
<sectionHeader confidence="0.999001" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999608816326531">
L. Bahl, P. Brown, P. de Souza, and R. Mercer. 1989.
A tree-based statistical language model for natural
language speech recognition. IEEE Transactions on
Acoustics, Speech and Signal Processing, 37:1001-
1008.
E. Brill and P. Resnik. 1994. A rule-based approach
to prepositional phrase attachment disambiguation.
In Proceedings of the Fifteenth International Con-
ference on Computational Linguistics (COLING-
1994), Pages 1198-4204, Kyoto.
E. Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part of speech tagging. Computational
Linguistics, 21(4):543-565.
E. Brill, 1996. Learning to Parse with Transforma-
tions. In H. Bunt and M. Tomita (eds.) Recent
Advances in Parsing Technology, Kluwer.
CoNLL. 2000. Shared task for computational natu-
ral language learning (CoNLL), 2000. http://lcg-
www.uia.ac.be/con112000/chunking.
I. Dagan and S. Engelson. 1995. Committee-based
sampling for training probabilistic classifiers. In
Proceedings of International Conference on Machine
Learning (ICML) 1995, pages 150-157.
D. Day, J. Aberdeen, L. Hirschman, R. Kozierok,
P. Robinson, and M. Vilain. 1997. Mixed-initiative
development of language processing systems. In
Fifth Conference on Applied Natural Language Pro-
cessing, pages 348-355. Association for Computa-
tional Linguistics, March.
T. G. Dietterich and G. Bakiri. 1995. Solving multi-
class learning problems via error-correcting output
codes. Journal of Artificial Intelligence Research,
2:263-286.
S. Engelson and I. Dagan. 1996. Minirni7ing manual
annotation cost in supervised training from corpora.
In Proceedings of ACL 1996, pages 319-326, Santa
Cruz, CA. Association for Computational Linguis-
tics.
Y. Freund and R.E. Schapire. 1997. A decision-
theoretic generalization of on-line learning and an
application to boosting. Journal of Computer and
System Sciences, 55(1):119-139.
Y. Freund, H. S. Seung, E. Shamir, and N. Tishby.
1997. Selective sampling using the query by com-
mittee algorithm. Machine Learning, 28:133-168.
D. Lewis and J. Catlett. 1994. Heterogeneous un-
certainty sampling for supervised learning. In Pro-
ceedings of the 11th International Conference on
Machine Learning, pages 139-147.
D. Lewis and W. Gale. 1994. A sequential algorithm
for training text classifiers. In Proceedings of ACM-
SIGIR 1994, pages 3-12. ACM-SIGIR.
R. Liere and P. TadepaIli. 1997. Active learning with
committees for text categorization. In Proceedings
of the Fourteenth National Conference on Artificial
Intelligence, pages 591-596. AAAI.
L. Mangu and E. Brill. 1997. Automatic rule acquisi-
tion for spelling correction. In Proceedings of the
Fourteenth International Conference on Machine
Learning, pages 734-741, Nashville, Tennessee.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of english:
The Penn Treebank. Computational Linguistics,
19 (2):313-330.
G. Ngai and D. Yarowsky. 2000. Rule writing or
annotation: Cost-efficient resource usage for base
noun phrase chunking. In Proceedings of ACL 2000.
Association for Computational Linguistics.
C. E. Priebe, J.-S. Pang, and T. Olson. 1999. Optimiz-
ing mine classification performance. In Proceedings
of the JSM. American Statistical Association.
J. R. Quinlan. 1993. C4.5: Programs for machine
learning. Morgan Kaufmann, San Mateo, CA.
L. Ra.mshaw and M. Marcus, 1999. Text Chunk-
ing Using Transformation-based Learning. In S.
Armstrong, K.W. Church, P. Isabelle, S. Manzi,
E. Tzoukermann and D. Yarowsky (eds.) Natural
Language Processing Using Very Large Corpora,
Kluwer.
E. Roche and Y. Schabes. 1995. Computational
linguistics. Deterministic Part of Speech Tagging
with Finite State Transducers, 21(2):227-253.
K. Samuel, S. Carberry, and K. Vijay-Shanker. 1998.
Dialogue act tagging with transformation-based
learning. In Proceedings of the 17th International
Conference on Computational Linguistics and the
36th Annual Meeting of the Association for Com-
putational Linguistics, pages 1150-1156, Montreal,
Quebec, Canada.
H. S. Seung, M. Opper, and H. Sompolinsky. 1992.
Query by committee. In Proceedings of the Fifth
Annual ACM Workshop on Computational Learning
Theory, pages 287-294. ACM.
M. Vilain and D. Day. 1996. Finite-state parsing
by rule sequences. In International Conference on
Computational Linguistics, pages 274-279, Copen-
hagen, Denmark, August.
</reference>
<page confidence="0.999323">
34
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.947123">
<title confidence="0.9986245">Coaxing Confidences from an Old Probabilistic Classifications from Transformation Rule Lists</title>
<author confidence="0.998413">Radu Florian John C Hendersont Grace Ngai</author>
<affiliation confidence="0.999833">Department of Computer Science tThe MITRE Corporation</affiliation>
<address confidence="0.9884625">Johns Hopkins University 202 Burlington Road Baltimore, MD 21218, USA Bedford, MA 01730, USA</address>
<email confidence="0.993202">rflorian@cs.jhu.edujhndrsn@mitre.org</email>
<email confidence="0.993202">gyn@cs.jhu.edujhndrsn@mitre.org</email>
<abstract confidence="0.99889647368421">Transformation-based learning has been successfully employed to solve many natural language processing problems. It has many positive features, but one drawback is that it does not provide estimates of class membership probabilities. In this paper, we present a novel method for obtaining class membership probabilities from a transformation-based rule list classifier. Three experiments are presented which measure the modeling accuracy and cross-entropy of the probabilistic classifier on unseen data and the degree to which the output probabilities from the classifier can be used to estimate confidences in its classification decisions. The results of these experiments show that, for the task of text chunking&apos;, the estimates produced by this technique are more informative than those generated by a state-of-the-art decision tree.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Bahl</author>
<author>P Brown</author>
<author>P de Souza</author>
<author>R Mercer</author>
</authors>
<title>A tree-based statistical language model for natural language speech recognition.</title>
<date>1989</date>
<journal>IEEE Transactions on Acoustics, Speech and Signal Processing,</journal>
<pages>37--1001</pages>
<marker>Bahl, Brown, de Souza, Mercer, 1989</marker>
<rawString>L. Bahl, P. Brown, P. de Souza, and R. Mercer. 1989. A tree-based statistical language model for natural language speech recognition. IEEE Transactions on Acoustics, Speech and Signal Processing, 37:1001-1008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
<author>P Resnik</author>
</authors>
<title>A rule-based approach to prepositional phrase attachment disambiguation.</title>
<date>1994</date>
<booktitle>In Proceedings of the Fifteenth International Conference on Computational Linguistics (COLING1994),</booktitle>
<pages>1198--4204</pages>
<location>Kyoto.</location>
<contexts>
<context position="3383" citStr="Brill and Resnik, 1994" startWordPosition="494" endWordPosition="497">resolve the disagreement. A similar situation arises in pipeline systems, such as a system which performs parsing on the output of a probabilistic part-of-speech tagging. Transformation-based learning (TBL) (Brill, 1995) is a successful rule-based machine learning algorithm in natural language processing. It has been applied to a wide variety of tasks, including part of speech tagging (Roche and Schabes, 1995; Brill, 1995), noun phrase chunking (Ramshaw and Marcus, 1999), parsing (Brill, 1996; Vilain and Day, 1996), spelling correction (Mangu and Brill, 1997), prepositional phrase attachment (Brill and Resnik, 1994), dialog act tagging (Samuel et al., 1998), segmentation and message understanding (Day et al., 1997), often achieving stateof-the-art performance with a small and easilyunderstandable list of rules. In this paper, we describe a novel method which enables a transformation-based classifier to generate a probability distribution on the class labels. Application of the method allows the transformation rule list to retain the robustness of the transformation-based algorithms, while benefitting from the advantages of a probabilistic classifier. The usefulness of the resulting probabilities is demon</context>
</contexts>
<marker>Brill, Resnik, 1994</marker>
<rawString>E. Brill and P. Resnik. 1994. A rule-based approach to prepositional phrase attachment disambiguation. In Proceedings of the Fifteenth International Conference on Computational Linguistics (COLING1994), Pages 1198-4204, Kyoto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: A case study in part of speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>21--4</pages>
<contexts>
<context position="2980" citStr="Brill, 1995" startWordPosition="437" endWordPosition="438">formationbased learning performs well, without changing the internals of the learner. candidate outputs. These uncertainty measures are useful in situations where both the classification of an sample and the system&apos;s confidence in that classification are needed. An example of this is a situation in an ensemble system where ensemble members disagree and a decision must be made about how to resolve the disagreement. A similar situation arises in pipeline systems, such as a system which performs parsing on the output of a probabilistic part-of-speech tagging. Transformation-based learning (TBL) (Brill, 1995) is a successful rule-based machine learning algorithm in natural language processing. It has been applied to a wide variety of tasks, including part of speech tagging (Roche and Schabes, 1995; Brill, 1995), noun phrase chunking (Ramshaw and Marcus, 1999), parsing (Brill, 1996; Vilain and Day, 1996), spelling correction (Mangu and Brill, 1997), prepositional phrase attachment (Brill and Resnik, 1994), dialog act tagging (Samuel et al., 1998), segmentation and message understanding (Day et al., 1997), often achieving stateof-the-art performance with a small and easilyunderstandable list of rule</context>
<context position="15547" citStr="Brill, 1995" startWordPosition="2569" endWordPosition="2570">ctic parsing. Unlike full parsing, the sentences are divided into non-overlapping phrases, where each word belongs to the lowest parse constituent that dominates it. The data used in all of these experiments is the CoNLL-2000 phrase chunking corpus (CoNLL, 2000). The corpus consists of sections 15-18 and section 20 of the Penn Treebank (Marcus et al., 1993), and is pre-divided into a 8936-sentence (211727 tokens) training set and a 2012-sentence (47377 tokens) test set. The chunk tags are derived from the parse tree constituents, and the part-of-speech tags were generated by the Brill tagger (Brill, 1995). As was noted by Ramshaw &amp; Marcus (1999), text chunking can be mapped to a tagging task, where each word is tagged with a chunk tag representing the phrase that it belongs to. An example sentence from the corpus is shown in Table 4. As a contrasting system, our results are compared with those produced by a 04.5 decision tree system (henceforth C4.5). The reason for using C4.5 is twofold: firstly, it is a widely-used algorithm which achieves state-of-theart performance on a broad variety of tasks; and Word POS tag Chunk Tag A.P. NNP B-NP Green NNP I-NP currently RB B-ADVP has VBZ B-VP 2,664,09</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>E. Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study in part of speech tagging. Computational Linguistics, 21(4):543-565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Learning to Parse with Transformations.</title>
<date>1996</date>
<booktitle>Recent Advances in Parsing Technology,</booktitle>
<editor>In H. Bunt and M. Tomita (eds.)</editor>
<publisher>Kluwer.</publisher>
<contexts>
<context position="3257" citStr="Brill, 1996" startWordPosition="479" endWordPosition="480">his is a situation in an ensemble system where ensemble members disagree and a decision must be made about how to resolve the disagreement. A similar situation arises in pipeline systems, such as a system which performs parsing on the output of a probabilistic part-of-speech tagging. Transformation-based learning (TBL) (Brill, 1995) is a successful rule-based machine learning algorithm in natural language processing. It has been applied to a wide variety of tasks, including part of speech tagging (Roche and Schabes, 1995; Brill, 1995), noun phrase chunking (Ramshaw and Marcus, 1999), parsing (Brill, 1996; Vilain and Day, 1996), spelling correction (Mangu and Brill, 1997), prepositional phrase attachment (Brill and Resnik, 1994), dialog act tagging (Samuel et al., 1998), segmentation and message understanding (Day et al., 1997), often achieving stateof-the-art performance with a small and easilyunderstandable list of rules. In this paper, we describe a novel method which enables a transformation-based classifier to generate a probability distribution on the class labels. Application of the method allows the transformation rule list to retain the robustness of the transformation-based algorithm</context>
</contexts>
<marker>Brill, 1996</marker>
<rawString>E. Brill, 1996. Learning to Parse with Transformations. In H. Bunt and M. Tomita (eds.) Recent Advances in Parsing Technology, Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>CoNLL</author>
</authors>
<title>Shared task for computational natural language learning (CoNLL),</title>
<date>2000</date>
<note>http://lcgwww.uia.ac.be/con112000/chunking.</note>
<contexts>
<context position="15197" citStr="CoNLL, 2000" startWordPosition="2513" endWordPosition="2514">n the node with a decision tree induction algorithm. The splitting criterion used in the experiments is the information gain measure. 4 Experiments Three experiments that demonstrate the effectiveness and appropriateness of our probability estimates are presented in this section. The experiments are performed on text chunking, a subproblem of syntactic parsing. Unlike full parsing, the sentences are divided into non-overlapping phrases, where each word belongs to the lowest parse constituent that dominates it. The data used in all of these experiments is the CoNLL-2000 phrase chunking corpus (CoNLL, 2000). The corpus consists of sections 15-18 and section 20 of the Penn Treebank (Marcus et al., 1993), and is pre-divided into a 8936-sentence (211727 tokens) training set and a 2012-sentence (47377 tokens) test set. The chunk tags are derived from the parse tree constituents, and the part-of-speech tags were generated by the Brill tagger (Brill, 1995). As was noted by Ramshaw &amp; Marcus (1999), text chunking can be mapped to a tagging task, where each word is tagged with a chunk tag representing the phrase that it belongs to. An example sentence from the corpus is shown in Table 4. As a contrasting</context>
<context position="19492" citStr="CoNLL, 2000" startWordPosition="3245" endWordPosition="3246">ore appropriate to measure the precision and recall: # of correct proposed phrases # of proposed phrases # of correct proposed phrases # of correct phrases To facilitate the comparison of systems with different precision and recall, the F-measure metric is computed as a weighted harmonic mean of precision and recall: The /3 parameter is used to give more weight to precision or recall, as the task at hand requires. In all our experiments, is set to 1, giving equal weight to precision and recall. The reported performances are all measured with the evaluation tool provided with the CoNLL corpus (CoNLL, 2000). 4.2 Active Learning To demonstrate the usefulness of obtaining probabilities from a transformation rule list, this section describes an application which utilizes these probabilities, and compare the resulting performance of the system with that achieved by C4.5. Natural language processing has traditionally required large amounts of annotated data from which to extract linguistic properties. However, not all data is created equal: a normal distribution of annotated data contains much redundant information. Seung et al. (1992) and Freund et al. (1997) proposed a theoretical active learning a</context>
</contexts>
<marker>CoNLL, 2000</marker>
<rawString>CoNLL. 2000. Shared task for computational natural language learning (CoNLL), 2000. http://lcgwww.uia.ac.be/con112000/chunking.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>S Engelson</author>
</authors>
<title>Committee-based sampling for training probabilistic classifiers.</title>
<date>1995</date>
<booktitle>In Proceedings of International Conference on Machine Learning (ICML)</booktitle>
<pages>150--157</pages>
<contexts>
<context position="20481" citStr="Dagan and Engelson, 1995" startWordPosition="3386" endWordPosition="3390">o extract linguistic properties. However, not all data is created equal: a normal distribution of annotated data contains much redundant information. Seung et al. (1992) and Freund et al. (1997) proposed a theoretical active learning approach, where samples are intelligently selected for annotation. By eliminating redundant information, the same performance can be achieved while using fewer resources. Empirically, active learning has been applied to various NLP tasks such as text categorization (Lewis and Gale, 1994; Lewis and Catlett, 1994; Liere and Tadepalli, 1997), part-of-speech tagging (Dagan and Engelson, 1995; Engelson and Dagan, 1996), and base noun phrase chunking (Ngai and Yarowsky, 2000), resulting in significantly large reductions in the quantity of data needed to achieve comparable performance. This section presents two experimental results which show the effectiveness of the probabilities generated by the TBLDT. The first experiment compares the performance achieved by the active learning algorithm using TBLDT with the performance obtained by selecting samples sequentially from the training set. The second experiment compares the performances achieved by TBLDT and C4.5 training on samples s</context>
</contexts>
<marker>Dagan, Engelson, 1995</marker>
<rawString>I. Dagan and S. Engelson. 1995. Committee-based sampling for training probabilistic classifiers. In Proceedings of International Conference on Machine Learning (ICML) 1995, pages 150-157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Day</author>
<author>J Aberdeen</author>
<author>L Hirschman</author>
<author>R Kozierok</author>
<author>P Robinson</author>
<author>M Vilain</author>
</authors>
<title>Mixed-initiative development of language processing systems.</title>
<date>1997</date>
<booktitle>In Fifth Conference on Applied Natural Language Processing,</booktitle>
<pages>348--355</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<contexts>
<context position="3484" citStr="Day et al., 1997" startWordPosition="510" endWordPosition="513">arsing on the output of a probabilistic part-of-speech tagging. Transformation-based learning (TBL) (Brill, 1995) is a successful rule-based machine learning algorithm in natural language processing. It has been applied to a wide variety of tasks, including part of speech tagging (Roche and Schabes, 1995; Brill, 1995), noun phrase chunking (Ramshaw and Marcus, 1999), parsing (Brill, 1996; Vilain and Day, 1996), spelling correction (Mangu and Brill, 1997), prepositional phrase attachment (Brill and Resnik, 1994), dialog act tagging (Samuel et al., 1998), segmentation and message understanding (Day et al., 1997), often achieving stateof-the-art performance with a small and easilyunderstandable list of rules. In this paper, we describe a novel method which enables a transformation-based classifier to generate a probability distribution on the class labels. Application of the method allows the transformation rule list to retain the robustness of the transformation-based algorithms, while benefitting from the advantages of a probabilistic classifier. The usefulness of the resulting probabilities is demonstrated by comparison with another stateof-the-art classifier, the C4.5 decision tree (Quinlan, 1993)</context>
</contexts>
<marker>Day, Aberdeen, Hirschman, Kozierok, Robinson, Vilain, 1997</marker>
<rawString>D. Day, J. Aberdeen, L. Hirschman, R. Kozierok, P. Robinson, and M. Vilain. 1997. Mixed-initiative development of language processing systems. In Fifth Conference on Applied Natural Language Processing, pages 348-355. Association for Computational Linguistics, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T G Dietterich</author>
<author>G Bakiri</author>
</authors>
<title>Solving multiclass learning problems via error-correcting output codes.</title>
<date>1995</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>2--263</pages>
<contexts>
<context position="24805" citStr="Dietterich and Bakiri, 1995" startWordPosition="4103" endWordPosition="4106">ves It is often very useful for a classifier to be able to offer confidence scores associated with its decisions. Confidence scores are associated with the probability P(C(x) correctlx) where C(x) is the classification of sample x. These scores can be used in real-life problems to reject samples that the the classifier is not sure about, in which case a better observation, or a human decision, might be requested. The performance of the classifier is then evaluated on the samples that were not rejected. This experiment framework is wellestablished in machine learning and optimization research (Dietterich and Bakiri, 1995; Priebe et al., 1999). Since non-probabilistic classifiers do not offer any insights into how sure they are about a particular classification, it is not easy to obtain confidence scores from them. A probabilistic classifier, in contrast, offers information about the class probability distribution of a given sample. Two measures that can be used in generating confidence scores are proposed in this section. The first measure, the entropy H of the class probability distribution of a sample x, C(x) = {p(cilx),p(c21x)...p(ck lx)}, is a measure of the uncertainty in the distribution: H(C(x)) = — Ep</context>
</contexts>
<marker>Dietterich, Bakiri, 1995</marker>
<rawString>T. G. Dietterich and G. Bakiri. 1995. Solving multiclass learning problems via error-correcting output codes. Journal of Artificial Intelligence Research, 2:263-286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Engelson</author>
<author>I Dagan</author>
</authors>
<title>Minirni7ing manual annotation cost in supervised training from corpora.</title>
<date>1996</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>319--326</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Santa Cruz, CA.</location>
<contexts>
<context position="20508" citStr="Engelson and Dagan, 1996" startWordPosition="3391" endWordPosition="3394">rties. However, not all data is created equal: a normal distribution of annotated data contains much redundant information. Seung et al. (1992) and Freund et al. (1997) proposed a theoretical active learning approach, where samples are intelligently selected for annotation. By eliminating redundant information, the same performance can be achieved while using fewer resources. Empirically, active learning has been applied to various NLP tasks such as text categorization (Lewis and Gale, 1994; Lewis and Catlett, 1994; Liere and Tadepalli, 1997), part-of-speech tagging (Dagan and Engelson, 1995; Engelson and Dagan, 1996), and base noun phrase chunking (Ngai and Yarowsky, 2000), resulting in significantly large reductions in the quantity of data needed to achieve comparable performance. This section presents two experimental results which show the effectiveness of the probabilities generated by the TBLDT. The first experiment compares the performance achieved by the active learning algorithm using TBLDT with the performance obtained by selecting samples sequentially from the training set. The second experiment compares the performances achieved by TBLDT and C4.5 training on samples selected by active learning.</context>
</contexts>
<marker>Engelson, Dagan, 1996</marker>
<rawString>S. Engelson and I. Dagan. 1996. Minirni7ing manual annotation cost in supervised training from corpora. In Proceedings of ACL 1996, pages 319-326, Santa Cruz, CA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R E Schapire</author>
</authors>
<title>A decisiontheoretic generalization of on-line learning and an application to boosting.</title>
<date>1997</date>
<journal>Journal of Computer and System Sciences,</journal>
<pages>55--1</pages>
<contexts>
<context position="8461" citStr="Freund and Schapire, 1997" startWordPosition="1340" endWordPosition="1343">In situations such as active learning, where a small number of samples are selected for annotation, the probabilities can be used to determine which examples the classifier was most unsure of, and hence should provide the most extra information. A probabilistic system can also act as a filter for a more expensive system or a human expert when it is permitted to reject samples. Soft decision-making is also useful when the system is one of the components in a larger decision-making process, as is the case in speech recognition systems (Bahl et al., 1989), or in an ensemble system like AdaBoost (Freund and Schapire, 1997). There are many other applications in which a probabilistic classifier is necessary, and a non-probabilistic classifier cannot be used instead. 3.1 Estimation via conversion to decision tree The method we propose to obtain probabilistic classifications from a transformation rule list involves dividing the samples into equivalence classes and computing distributions over each equivalence class. At any given point in time i, each sample x in the training set has an associated state s•(x) = (x, y). Let R(x) to be the set of rules ri that applies to the state si(x), R(x) = {ri E Riri applies to s</context>
<context position="33658" citStr="Freund and Schapire, 1997" startWordPosition="5546" endWordPosition="5549">monstrate that the resulting probabilities perform at least as well as the ones generated by 04.5 decision trees, resulting in better performance in all cases. This proves that the resulting probabilistic classifier is as least as good as other state-of-the-art probabilistic models. The positive results obtained suggest that the probabilistic classifier obtained from transformation rule lists can be successfully used in machine learning algorithms that require soft-decision classifiers, such as boosting or voting. Future research will include testing the behavior of the system under AdaBoost (Freund and Schapire, 1997). We also intend to investigate the effects that other decision tree growth and smoothing techniques may have on continued refinement of the converted rule list. 6 Acknowledgements We thank Eric Brill, Fred Jelinek and David Yarowsky for their invaluable advice and suggestions. In addition we would like to thank David Day, Ben Wellner and the anonymous reviewers for their useful comments and suggestions on the paper. The views expressed in this paper are those of the authors and do not necessarily reflect the views 33 of the MITRE Corporation. It was performed as a collaborative effort at both</context>
</contexts>
<marker>Freund, Schapire, 1997</marker>
<rawString>Y. Freund and R.E. Schapire. 1997. A decisiontheoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1):119-139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>H S Seung</author>
<author>E Shamir</author>
<author>N Tishby</author>
</authors>
<title>Selective sampling using the query by committee algorithm.</title>
<date>1997</date>
<booktitle>Machine Learning,</booktitle>
<pages>28--133</pages>
<contexts>
<context position="20051" citStr="Freund et al. (1997)" startWordPosition="3326" endWordPosition="3329"> evaluation tool provided with the CoNLL corpus (CoNLL, 2000). 4.2 Active Learning To demonstrate the usefulness of obtaining probabilities from a transformation rule list, this section describes an application which utilizes these probabilities, and compare the resulting performance of the system with that achieved by C4.5. Natural language processing has traditionally required large amounts of annotated data from which to extract linguistic properties. However, not all data is created equal: a normal distribution of annotated data contains much redundant information. Seung et al. (1992) and Freund et al. (1997) proposed a theoretical active learning approach, where samples are intelligently selected for annotation. By eliminating redundant information, the same performance can be achieved while using fewer resources. Empirically, active learning has been applied to various NLP tasks such as text categorization (Lewis and Gale, 1994; Lewis and Catlett, 1994; Liere and Tadepalli, 1997), part-of-speech tagging (Dagan and Engelson, 1995; Engelson and Dagan, 1996), and base noun phrase chunking (Ngai and Yarowsky, 2000), resulting in significantly large reductions in the quantity of data needed to achiev</context>
</contexts>
<marker>Freund, Seung, Shamir, Tishby, 1997</marker>
<rawString>Y. Freund, H. S. Seung, E. Shamir, and N. Tishby. 1997. Selective sampling using the query by committee algorithm. Machine Learning, 28:133-168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lewis</author>
<author>J Catlett</author>
</authors>
<title>Heterogeneous uncertainty sampling for supervised learning.</title>
<date>1994</date>
<booktitle>In Proceedings of the 11th International Conference on Machine Learning,</booktitle>
<pages>139--147</pages>
<contexts>
<context position="20403" citStr="Lewis and Catlett, 1994" startWordPosition="3376" endWordPosition="3379">ssing has traditionally required large amounts of annotated data from which to extract linguistic properties. However, not all data is created equal: a normal distribution of annotated data contains much redundant information. Seung et al. (1992) and Freund et al. (1997) proposed a theoretical active learning approach, where samples are intelligently selected for annotation. By eliminating redundant information, the same performance can be achieved while using fewer resources. Empirically, active learning has been applied to various NLP tasks such as text categorization (Lewis and Gale, 1994; Lewis and Catlett, 1994; Liere and Tadepalli, 1997), part-of-speech tagging (Dagan and Engelson, 1995; Engelson and Dagan, 1996), and base noun phrase chunking (Ngai and Yarowsky, 2000), resulting in significantly large reductions in the quantity of data needed to achieve comparable performance. This section presents two experimental results which show the effectiveness of the probabilities generated by the TBLDT. The first experiment compares the performance achieved by the active learning algorithm using TBLDT with the performance obtained by selecting samples sequentially from the training set. The second experim</context>
</contexts>
<marker>Lewis, Catlett, 1994</marker>
<rawString>D. Lewis and J. Catlett. 1994. Heterogeneous uncertainty sampling for supervised learning. In Proceedings of the 11th International Conference on Machine Learning, pages 139-147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lewis</author>
<author>W Gale</author>
</authors>
<title>A sequential algorithm for training text classifiers.</title>
<date>1994</date>
<booktitle>In Proceedings of ACMSIGIR</booktitle>
<pages>3--12</pages>
<publisher>ACM-SIGIR.</publisher>
<contexts>
<context position="20378" citStr="Lewis and Gale, 1994" startWordPosition="3372" endWordPosition="3375">Natural language processing has traditionally required large amounts of annotated data from which to extract linguistic properties. However, not all data is created equal: a normal distribution of annotated data contains much redundant information. Seung et al. (1992) and Freund et al. (1997) proposed a theoretical active learning approach, where samples are intelligently selected for annotation. By eliminating redundant information, the same performance can be achieved while using fewer resources. Empirically, active learning has been applied to various NLP tasks such as text categorization (Lewis and Gale, 1994; Lewis and Catlett, 1994; Liere and Tadepalli, 1997), part-of-speech tagging (Dagan and Engelson, 1995; Engelson and Dagan, 1996), and base noun phrase chunking (Ngai and Yarowsky, 2000), resulting in significantly large reductions in the quantity of data needed to achieve comparable performance. This section presents two experimental results which show the effectiveness of the probabilities generated by the TBLDT. The first experiment compares the performance achieved by the active learning algorithm using TBLDT with the performance obtained by selecting samples sequentially from the trainin</context>
</contexts>
<marker>Lewis, Gale, 1994</marker>
<rawString>D. Lewis and W. Gale. 1994. A sequential algorithm for training text classifiers. In Proceedings of ACMSIGIR 1994, pages 3-12. ACM-SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Liere</author>
<author>P TadepaIli</author>
</authors>
<title>Active learning with committees for text categorization.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fourteenth National Conference on Artificial Intelligence,</booktitle>
<pages>591--596</pages>
<publisher>AAAI.</publisher>
<marker>Liere, TadepaIli, 1997</marker>
<rawString>R. Liere and P. TadepaIli. 1997. Active learning with committees for text categorization. In Proceedings of the Fourteenth National Conference on Artificial Intelligence, pages 591-596. AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Mangu</author>
<author>E Brill</author>
</authors>
<title>Automatic rule acquisition for spelling correction.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fourteenth International Conference on Machine Learning,</booktitle>
<pages>734--741</pages>
<location>Nashville, Tennessee.</location>
<contexts>
<context position="3325" citStr="Mangu and Brill, 1997" startWordPosition="487" endWordPosition="490">embers disagree and a decision must be made about how to resolve the disagreement. A similar situation arises in pipeline systems, such as a system which performs parsing on the output of a probabilistic part-of-speech tagging. Transformation-based learning (TBL) (Brill, 1995) is a successful rule-based machine learning algorithm in natural language processing. It has been applied to a wide variety of tasks, including part of speech tagging (Roche and Schabes, 1995; Brill, 1995), noun phrase chunking (Ramshaw and Marcus, 1999), parsing (Brill, 1996; Vilain and Day, 1996), spelling correction (Mangu and Brill, 1997), prepositional phrase attachment (Brill and Resnik, 1994), dialog act tagging (Samuel et al., 1998), segmentation and message understanding (Day et al., 1997), often achieving stateof-the-art performance with a small and easilyunderstandable list of rules. In this paper, we describe a novel method which enables a transformation-based classifier to generate a probability distribution on the class labels. Application of the method allows the transformation rule list to retain the robustness of the transformation-based algorithms, while benefitting from the advantages of a probabilistic classifi</context>
</contexts>
<marker>Mangu, Brill, 1997</marker>
<rawString>L. Mangu and E. Brill. 1997. Automatic rule acquisition for spelling correction. In Proceedings of the Fourteenth International Conference on Machine Learning, pages 734-741, Nashville, Tennessee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<pages>2--313</pages>
<contexts>
<context position="15294" citStr="Marcus et al., 1993" startWordPosition="2528" endWordPosition="2531">experiments is the information gain measure. 4 Experiments Three experiments that demonstrate the effectiveness and appropriateness of our probability estimates are presented in this section. The experiments are performed on text chunking, a subproblem of syntactic parsing. Unlike full parsing, the sentences are divided into non-overlapping phrases, where each word belongs to the lowest parse constituent that dominates it. The data used in all of these experiments is the CoNLL-2000 phrase chunking corpus (CoNLL, 2000). The corpus consists of sections 15-18 and section 20 of the Penn Treebank (Marcus et al., 1993), and is pre-divided into a 8936-sentence (211727 tokens) training set and a 2012-sentence (47377 tokens) test set. The chunk tags are derived from the parse tree constituents, and the part-of-speech tags were generated by the Brill tagger (Brill, 1995). As was noted by Ramshaw &amp; Marcus (1999), text chunking can be mapped to a tagging task, where each word is tagged with a chunk tag representing the phrase that it belongs to. An example sentence from the corpus is shown in Table 4. As a contrasting system, our results are compared with those produced by a 04.5 decision tree system (henceforth </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large annotated corpus of english: The Penn Treebank. Computational Linguistics, 19 (2):313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Ngai</author>
<author>D Yarowsky</author>
</authors>
<title>Rule writing or annotation: Cost-efficient resource usage for base noun phrase chunking.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="20565" citStr="Ngai and Yarowsky, 2000" startWordPosition="3400" endWordPosition="3403">stribution of annotated data contains much redundant information. Seung et al. (1992) and Freund et al. (1997) proposed a theoretical active learning approach, where samples are intelligently selected for annotation. By eliminating redundant information, the same performance can be achieved while using fewer resources. Empirically, active learning has been applied to various NLP tasks such as text categorization (Lewis and Gale, 1994; Lewis and Catlett, 1994; Liere and Tadepalli, 1997), part-of-speech tagging (Dagan and Engelson, 1995; Engelson and Dagan, 1996), and base noun phrase chunking (Ngai and Yarowsky, 2000), resulting in significantly large reductions in the quantity of data needed to achieve comparable performance. This section presents two experimental results which show the effectiveness of the probabilities generated by the TBLDT. The first experiment compares the performance achieved by the active learning algorithm using TBLDT with the performance obtained by selecting samples sequentially from the training set. The second experiment compares the performances achieved by TBLDT and C4.5 training on samples selected by active learning. The following describes the active learning algorithm us</context>
</contexts>
<marker>Ngai, Yarowsky, 2000</marker>
<rawString>G. Ngai and D. Yarowsky. 2000. Rule writing or annotation: Cost-efficient resource usage for base noun phrase chunking. In Proceedings of ACL 2000. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C E Priebe</author>
<author>J-S Pang</author>
<author>T Olson</author>
</authors>
<title>Optimizing mine classification performance.</title>
<date>1999</date>
<booktitle>In Proceedings of the JSM.</booktitle>
<publisher>American Statistical Association.</publisher>
<contexts>
<context position="24827" citStr="Priebe et al., 1999" startWordPosition="4107" endWordPosition="4110">or a classifier to be able to offer confidence scores associated with its decisions. Confidence scores are associated with the probability P(C(x) correctlx) where C(x) is the classification of sample x. These scores can be used in real-life problems to reject samples that the the classifier is not sure about, in which case a better observation, or a human decision, might be requested. The performance of the classifier is then evaluated on the samples that were not rejected. This experiment framework is wellestablished in machine learning and optimization research (Dietterich and Bakiri, 1995; Priebe et al., 1999). Since non-probabilistic classifiers do not offer any insights into how sure they are about a particular classification, it is not easy to obtain confidence scores from them. A probabilistic classifier, in contrast, offers information about the class probability distribution of a given sample. Two measures that can be used in generating confidence scores are proposed in this section. The first measure, the entropy H of the class probability distribution of a sample x, C(x) = {p(cilx),p(c21x)...p(ck lx)}, is a measure of the uncertainty in the distribution: H(C(x)) = — Ep(cilx) log2 p(ci Ix) T</context>
</contexts>
<marker>Priebe, Pang, Olson, 1999</marker>
<rawString>C. E. Priebe, J.-S. Pang, and T. Olson. 1999. Optimizing mine classification performance. In Proceedings of the JSM. American Statistical Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<title>C4.5: Programs for machine learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Mateo, CA.</location>
<contexts>
<context position="4084" citStr="Quinlan, 1993" startWordPosition="597" endWordPosition="599"> et al., 1997), often achieving stateof-the-art performance with a small and easilyunderstandable list of rules. In this paper, we describe a novel method which enables a transformation-based classifier to generate a probability distribution on the class labels. Application of the method allows the transformation rule list to retain the robustness of the transformation-based algorithms, while benefitting from the advantages of a probabilistic classifier. The usefulness of the resulting probabilities is demonstrated by comparison with another stateof-the-art classifier, the C4.5 decision tree (Quinlan, 1993). The performance of our algorithm compares favorably across many dimensions: it obtains better perplexity and cross-entropy; an active learning algorithm using our system outperforms a similar algorithm using decision trees; and finally, our algorithm has better rejection curves than a similar decision tree. Section 2 presents the transformation based learning paradigm; Section 3 describes the algorithm for construction of the decision tree associated with the transformation based list; Section 4 describes the experiments in detail and Section 5 concludes the paper and outlines the future wor</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>J. R. Quinlan. 1993. C4.5: Programs for machine learning. Morgan Kaufmann, San Mateo, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ra mshaw</author>
<author>M Marcus</author>
</authors>
<title>Text Chunking Using Transformation-based Learning.</title>
<date>1999</date>
<booktitle>Natural Language Processing Using Very Large Corpora,</booktitle>
<editor>In S. Armstrong, K.W. Church, P. Isabelle, S. Manzi, E. Tzoukermann and D. Yarowsky (eds.)</editor>
<publisher>Kluwer.</publisher>
<contexts>
<context position="3235" citStr="mshaw and Marcus, 1999" startWordPosition="474" endWordPosition="477">cation are needed. An example of this is a situation in an ensemble system where ensemble members disagree and a decision must be made about how to resolve the disagreement. A similar situation arises in pipeline systems, such as a system which performs parsing on the output of a probabilistic part-of-speech tagging. Transformation-based learning (TBL) (Brill, 1995) is a successful rule-based machine learning algorithm in natural language processing. It has been applied to a wide variety of tasks, including part of speech tagging (Roche and Schabes, 1995; Brill, 1995), noun phrase chunking (Ramshaw and Marcus, 1999), parsing (Brill, 1996; Vilain and Day, 1996), spelling correction (Mangu and Brill, 1997), prepositional phrase attachment (Brill and Resnik, 1994), dialog act tagging (Samuel et al., 1998), segmentation and message understanding (Day et al., 1997), often achieving stateof-the-art performance with a small and easilyunderstandable list of rules. In this paper, we describe a novel method which enables a transformation-based classifier to generate a probability distribution on the class labels. Application of the method allows the transformation rule list to retain the robustness of the transfor</context>
<context position="18502" citStr="mshaw and Marcus (1999)" startWordPosition="3079" endWordPosition="3082">es in the left context of following samples. Since 04.5 generates probabilities for each classification decision, they can be redirected into the input for the next position. Providing the decision tree with this confidence information effectively allows it to perform a limited search over the entire sentence. C4.5 does have one advantage over TBLDT, however. A decision tree can be trained using the subsetting feature, where questions asked are of the form: &amp;quot;does feature f belong to the set F?&amp;quot;. This is not something that a TBL can do readily, 2The TBL templates are similar to those used in Ramshaw and Marcus (1999). 29 but since the objective is in comparing TBLDT to another state-of-the-art system, this feature was enabled. 4.1 Evaluation Measures The most commonly used measure for evaluating tagging tasks is tag accuracy. It is defined as # of correctly tagged examples Accuracy = # of examples In syntactic parsing, though, since the task is to identify the phrasal components, it is more appropriate to measure the precision and recall: # of correct proposed phrases # of proposed phrases # of correct proposed phrases # of correct phrases To facilitate the comparison of systems with different precision a</context>
<context position="15588" citStr="mshaw &amp; Marcus (1999)" startWordPosition="2575" endWordPosition="2578">g, the sentences are divided into non-overlapping phrases, where each word belongs to the lowest parse constituent that dominates it. The data used in all of these experiments is the CoNLL-2000 phrase chunking corpus (CoNLL, 2000). The corpus consists of sections 15-18 and section 20 of the Penn Treebank (Marcus et al., 1993), and is pre-divided into a 8936-sentence (211727 tokens) training set and a 2012-sentence (47377 tokens) test set. The chunk tags are derived from the parse tree constituents, and the part-of-speech tags were generated by the Brill tagger (Brill, 1995). As was noted by Ramshaw &amp; Marcus (1999), text chunking can be mapped to a tagging task, where each word is tagged with a chunk tag representing the phrase that it belongs to. An example sentence from the corpus is shown in Table 4. As a contrasting system, our results are compared with those produced by a 04.5 decision tree system (henceforth C4.5). The reason for using C4.5 is twofold: firstly, it is a widely-used algorithm which achieves state-of-theart performance on a broad variety of tasks; and Word POS tag Chunk Tag A.P. NNP B-NP Green NNP I-NP currently RB B-ADVP has VBZ B-VP 2,664,098 CD B-NP shares NNS I-NP outstanding JJ </context>
</contexts>
<marker>mshaw, Marcus, 1999</marker>
<rawString>L. Ra.mshaw and M. Marcus, 1999. Text Chunking Using Transformation-based Learning. In S. Armstrong, K.W. Church, P. Isabelle, S. Manzi, E. Tzoukermann and D. Yarowsky (eds.) Natural Language Processing Using Very Large Corpora, Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Roche</author>
<author>Y Schabes</author>
</authors>
<title>Computational linguistics. Deterministic Part of Speech Tagging with Finite State Transducers,</title>
<date>1995</date>
<pages>21--2</pages>
<contexts>
<context position="3172" citStr="Roche and Schabes, 1995" startWordPosition="465" endWordPosition="468">ation of an sample and the system&apos;s confidence in that classification are needed. An example of this is a situation in an ensemble system where ensemble members disagree and a decision must be made about how to resolve the disagreement. A similar situation arises in pipeline systems, such as a system which performs parsing on the output of a probabilistic part-of-speech tagging. Transformation-based learning (TBL) (Brill, 1995) is a successful rule-based machine learning algorithm in natural language processing. It has been applied to a wide variety of tasks, including part of speech tagging (Roche and Schabes, 1995; Brill, 1995), noun phrase chunking (Ramshaw and Marcus, 1999), parsing (Brill, 1996; Vilain and Day, 1996), spelling correction (Mangu and Brill, 1997), prepositional phrase attachment (Brill and Resnik, 1994), dialog act tagging (Samuel et al., 1998), segmentation and message understanding (Day et al., 1997), often achieving stateof-the-art performance with a small and easilyunderstandable list of rules. In this paper, we describe a novel method which enables a transformation-based classifier to generate a probability distribution on the class labels. Application of the method allows the tr</context>
</contexts>
<marker>Roche, Schabes, 1995</marker>
<rawString>E. Roche and Y. Schabes. 1995. Computational linguistics. Deterministic Part of Speech Tagging with Finite State Transducers, 21(2):227-253.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Samuel</author>
<author>S Carberry</author>
<author>K Vijay-Shanker</author>
</authors>
<title>Dialogue act tagging with transformation-based learning.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th International Conference on Computational Linguistics and the 36th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1150--1156</pages>
<location>Montreal, Quebec, Canada.</location>
<contexts>
<context position="3425" citStr="Samuel et al., 1998" startWordPosition="501" endWordPosition="504"> arises in pipeline systems, such as a system which performs parsing on the output of a probabilistic part-of-speech tagging. Transformation-based learning (TBL) (Brill, 1995) is a successful rule-based machine learning algorithm in natural language processing. It has been applied to a wide variety of tasks, including part of speech tagging (Roche and Schabes, 1995; Brill, 1995), noun phrase chunking (Ramshaw and Marcus, 1999), parsing (Brill, 1996; Vilain and Day, 1996), spelling correction (Mangu and Brill, 1997), prepositional phrase attachment (Brill and Resnik, 1994), dialog act tagging (Samuel et al., 1998), segmentation and message understanding (Day et al., 1997), often achieving stateof-the-art performance with a small and easilyunderstandable list of rules. In this paper, we describe a novel method which enables a transformation-based classifier to generate a probability distribution on the class labels. Application of the method allows the transformation rule list to retain the robustness of the transformation-based algorithms, while benefitting from the advantages of a probabilistic classifier. The usefulness of the resulting probabilities is demonstrated by comparison with another stateof</context>
</contexts>
<marker>Samuel, Carberry, Vijay-Shanker, 1998</marker>
<rawString>K. Samuel, S. Carberry, and K. Vijay-Shanker. 1998. Dialogue act tagging with transformation-based learning. In Proceedings of the 17th International Conference on Computational Linguistics and the 36th Annual Meeting of the Association for Computational Linguistics, pages 1150-1156, Montreal, Quebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H S Seung</author>
<author>M Opper</author>
<author>H Sompolinsky</author>
</authors>
<title>Query by committee.</title>
<date>1992</date>
<booktitle>In Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory,</booktitle>
<pages>287--294</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="20026" citStr="Seung et al. (1992)" startWordPosition="3321" endWordPosition="3324">re all measured with the evaluation tool provided with the CoNLL corpus (CoNLL, 2000). 4.2 Active Learning To demonstrate the usefulness of obtaining probabilities from a transformation rule list, this section describes an application which utilizes these probabilities, and compare the resulting performance of the system with that achieved by C4.5. Natural language processing has traditionally required large amounts of annotated data from which to extract linguistic properties. However, not all data is created equal: a normal distribution of annotated data contains much redundant information. Seung et al. (1992) and Freund et al. (1997) proposed a theoretical active learning approach, where samples are intelligently selected for annotation. By eliminating redundant information, the same performance can be achieved while using fewer resources. Empirically, active learning has been applied to various NLP tasks such as text categorization (Lewis and Gale, 1994; Lewis and Catlett, 1994; Liere and Tadepalli, 1997), part-of-speech tagging (Dagan and Engelson, 1995; Engelson and Dagan, 1996), and base noun phrase chunking (Ngai and Yarowsky, 2000), resulting in significantly large reductions in the quantity</context>
</contexts>
<marker>Seung, Opper, Sompolinsky, 1992</marker>
<rawString>H. S. Seung, M. Opper, and H. Sompolinsky. 1992. Query by committee. In Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, pages 287-294. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Vilain</author>
<author>D Day</author>
</authors>
<title>Finite-state parsing by rule sequences.</title>
<date>1996</date>
<booktitle>In International Conference on Computational Linguistics,</booktitle>
<pages>274--279</pages>
<location>Copenhagen, Denmark,</location>
<contexts>
<context position="3280" citStr="Vilain and Day, 1996" startWordPosition="481" endWordPosition="484">ation in an ensemble system where ensemble members disagree and a decision must be made about how to resolve the disagreement. A similar situation arises in pipeline systems, such as a system which performs parsing on the output of a probabilistic part-of-speech tagging. Transformation-based learning (TBL) (Brill, 1995) is a successful rule-based machine learning algorithm in natural language processing. It has been applied to a wide variety of tasks, including part of speech tagging (Roche and Schabes, 1995; Brill, 1995), noun phrase chunking (Ramshaw and Marcus, 1999), parsing (Brill, 1996; Vilain and Day, 1996), spelling correction (Mangu and Brill, 1997), prepositional phrase attachment (Brill and Resnik, 1994), dialog act tagging (Samuel et al., 1998), segmentation and message understanding (Day et al., 1997), often achieving stateof-the-art performance with a small and easilyunderstandable list of rules. In this paper, we describe a novel method which enables a transformation-based classifier to generate a probability distribution on the class labels. Application of the method allows the transformation rule list to retain the robustness of the transformation-based algorithms, while benefitting fr</context>
</contexts>
<marker>Vilain, Day, 1996</marker>
<rawString>M. Vilain and D. Day. 1996. Finite-state parsing by rule sequences. In International Conference on Computational Linguistics, pages 274-279, Copenhagen, Denmark, August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>