<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.082963">
<title confidence="0.9115845">
AT&amp;T: The Tag&amp;Parse Approach to Semantic Parsing of Robot Spatial
Commands
</title>
<author confidence="0.973226">
Svetlana Stoyanchev, Hyuckchul Jung, John Chen, Srinivas Bangalore
</author>
<affiliation confidence="0.938038">
AT&amp;T Labs Research
</affiliation>
<address confidence="0.78799">
1 AT&amp;T Way Bedminster NJ 07921
</address>
<email confidence="0.999049">
{sveta,hjung,jchen,srini}@research.att.com
</email>
<sectionHeader confidence="0.997392" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999960230769231">
The Tag&amp;Parse approach to semantic
parsing first assigns semantic tags to each
word in a sentence and then parses the
tag sequence into a semantic tree. We
use statistical approach for tagging, pars-
ing, and reference resolution stages. Each
stage produces multiple hypotheses which
are re-ranked using spatial validation. We
evaluate the Tag&amp;Parse approach on a cor-
pus of Robotic Spatial Commands as part
of the SemEval Task6 exercise. Our sys-
tem accuracy is 87.35% and 60.84% with
and without spatial validation.
</bodyText>
<sectionHeader confidence="0.999396" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999607318181818">
In this paper we describe a system participating
in the SemEval2014 Task-6 on Supervised Seman-
tic Parsing of Robotic Spatial Commands. It pro-
duces a semantic parse of natural language com-
mands addressed to a robot arm designed to move
objects on a grid surface. Each command directs
a robot to change position of an object given a
current configuration. A command uniquely iden-
tifies an object and its destination, for example
“Move the turquoise pyramid above the yellow
cube”. System output is a Robot Control Lan-
guage (RCL) parse (see Figure 1) which is pro-
cessed by the robot arm simulator. The Robot Spa-
tial Commands dataset (Dukes, 2013) is used for
training and testing.
Our system uses a Tag&amp;Parse approach which
separates semantic tagging and semantic parsing
stages. It has four components: 1) semantic tag-
ging, 2) parsing, 3) reference resolution, and 4)
spatial validation. The first three are trained using
LLAMA (Haffner, 2006), a supervised machine
learning toolkit, on the RCL-parsed sentences.
</bodyText>
<footnote confidence="0.9053655">
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
</footnote>
<bodyText confidence="0.99987">
For semantic tagging, we train a maximum en-
tropy sequence tagger for assigning a semantic la-
bel and value to each word in a sentence, such as
type cube or color blue. For parsing, we train a
constituency parser on non-lexical RCL semantic
trees. For reference resolution, we train a maxi-
mum entropy model that identifies entities for ref-
erence tags found by previous components. All of
these components can generate multiple hypothe-
ses. Spatial validation re-ranks these hypotheses
by validating them against the input spatial con-
figuration. The top hypothesis after re-ranking is
returned by the system.
Separating tagging and parsing stages has sev-
eral advantages. A tagging stage allows the system
flexibility to abstract from possible grammatical or
spelling errors in a command. It assigns a seman-
tic category to each word in a sentence. Words not
contributing to the semantic meaning are assigned
‘O’ label by the tagger and are ignored in the fur-
ther processing. Words that are misspelled can po-
tentially receive a correct tag when a word simi-
larity feature is used in building a tagging model.
This will be especially important when process-
ing output of spoken commands that may contain
recognition errors.
The remainder of the paper is organized thusly.
In Section 2 we describe each of the components
used in our system. In Section 3 we describe the
results reported for SemEval2014 and evaluation
of each system component. We summarize our
findings and present future work in Section 4.
</bodyText>
<sectionHeader confidence="0.999041" genericHeader="introduction">
2 System
</sectionHeader>
<subsectionHeader confidence="0.999827">
2.1 Sequence Tagging
</subsectionHeader>
<bodyText confidence="0.999929833333333">
A sequence tagging approach is used for condi-
tional inference of tags given a word sequence.
It is used for many natural language tasks, such
as part of speech (POS) and named entity tag-
ging (Toutanova and others, 2003; Carreras et al.,
2003). We train a sequence tagger for assign-
</bodyText>
<page confidence="0.987989">
109
</page>
<note confidence="0.8309215">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 109–113,
Dublin, Ireland, August 23-24, 2014.
</note>
<figureCaption confidence="0.9989519">
Figure 1: RCL tree for a sentence Move the turquoise pyramid above the yellow cube.
Word index tag label
Move 1 action move
the 2 O -
turquoise 3 color cyan
pyramid 4 type prism
above 5 relation above
the 6 O -
yellow 7 color yellow
cube 8 type cube
</figureCaption>
<tableCaption confidence="0.95465">
Table 1: Tagging labels for a sentence Move the
</tableCaption>
<bodyText confidence="0.986434590909091">
turquoise pyramid above the yellow cube.
ing a combined semantic tag and label (such as
type cube) to each word in a command. The tags
used for training are extracted from the leaf-level
nodes of the RCL trees. Table 2 shows tags and
labels for a sample sentence “Move the turquoise
pyramid above the yellow cube” extracted from
the RCL parse tree (see Figure 1). In some cases,
a label is the same as a word (yellow, cube) while
in other cases, it differs (turquoise - cyan, pyramid
- prism).
We train a sequence tagger using LLAMA max-
imum entropy (maxent) classification (Haffner,
2006) to predict the combined semantic tag and
label of each word. Neighboring words, immedi-
ately neighboring semantic tags, and POS tags are
used as features, where the POS tagger is another
sequence tagging model trained on the Penn Tree-
bank (Marcus et al., 1993). We also experimented
with a tagger that assigns tags and labels in sep-
arate sequence tagging models, but it performed
poorly.
</bodyText>
<subsectionHeader confidence="0.998826">
2.2 Parsing
</subsectionHeader>
<bodyText confidence="0.9970084">
We use a constituency parser for building RCL
trees. The input to the parser is a sequence of
tags assigned by a sequence tagger, such as “ac-
tion color type relation color type” for the exam-
ple in Figure 1.
The parser generates multiple RCL parse tree
hypotheses sorted in the order of their likelihood.
The likelihood of a tree T given a sequence of tags
T is determined using a probabilistic context free
grammar (PCFG) G:
</bodyText>
<equation confidence="0.9930625">
P(T|S) = 11 PG(r) (1)
r∈T
</equation>
<bodyText confidence="0.99990775">
The n-best parses are obtained using the CKY
algorithm, recording the n-best hyperedge back-
pointers per constituent along the lines of (Huang
and Chiang, 2005). G was obtained and PG was
estimated from a corpus of non-lexical RCL trees
generated by removing all nodes descendant from
the tag nodes (action, color, etc.). Parses may con-
tain empty nodes not corresponding to any tag in
the input sequence. These are hypothesized by the
parser at positions in between input tags and in-
serted as edges according to the PCFG, which has
probabilistic rules for generating empty nodes.
</bodyText>
<subsectionHeader confidence="0.998188">
2.3 Reference Resolution
</subsectionHeader>
<bodyText confidence="0.999948823529412">
Reference resolution identifies the most prob-
able antecedent for each anaphor within a
text (Hirschman and Chinchor, 1997). It applies
when multiple candidates antecedents are present.
For example, in a sentence “Pick up the red cube
standing on a grey cube and place it on top of
the yellow one”, the anaphor it has two candidate
antecedents corresponding to entity segments the
red cube and a grey cube. In our system, anaphor
and antecedents are represented by reference tags
occurring in one sentence. A reference tag is ei-
ther assigned by a sequence tagger to one of the
words (e.g. to a pronoun) or is inserted into a
tree by the parser (e.g. ellipsis). We train a bi-
nary maxent model for this task using LLAMA.
The input is a pair consisting of an anaphor and
a candidate antecedent, along with their features.
</bodyText>
<page confidence="0.989535">
110
</page>
<bodyText confidence="0.9999194">
Features that are used include the preceding and
following words as well as the tags/labels of both
the anaphor and candidate antecedent. The refer-
ence resolution component selects the antecedent
for which the model returns the highest score.
</bodyText>
<subsectionHeader confidence="0.98494">
2.4 Spatial Validation
</subsectionHeader>
<bodyText confidence="0.999910566666667">
SemEval2014 Task6 provided a spatial planner
which takes an RCL command as an input and
determines if that command is executable in the
given spatial context. At each step described in
2.1∼2.3, due to the statistical nature of our ap-
proach, multiple hypotheses can be easily com-
puted with different confidence values. We used
the spatial planner to validate the final output RCL
commands from the three steps by checking if the
RCLs are executable or not. We generate multi-
ple tagger output hypotheses. For each tagger out-
put hypothesis, we generate multiple parser out-
put hypotheses. For each parser output hypothe-
sis, we generate multiple reference resolution out-
put hypotheses. The resulting output hypotheses
are ranked in the order of confidence scores with
the highest tagging output scores ranked first, fol-
lowed by the parsing output scores, and, finally,
reference resolution output scores. The system re-
turns the result of the top scored command that is
valid according to the spatial validator.
In many applications, there can be a tool or
method to validate tag/parse/reference outputs
fully or partially. Note that in our system the val-
idation is performed after all output is generated.
Tightly coupled validation, such as checking va-
lidity of a tagged entity or a parse constituent,
could help in computing hypotheses at each step
(e.g., feature values based on possible entities or
actions) and it remains as future work.
</bodyText>
<sectionHeader confidence="0.999962" genericHeader="method">
3 Results
</sectionHeader>
<bodyText confidence="0.999814153846154">
In this section, we present evaluation results on the
three subsets of the data summarized in Table 3. In
the TEST2500 data set, the models are trained on
the initial 2500 sentences of the Robot Commands
Treebank and evaluated on the last 909 sentences
(this corresponds to the data split of the SemEval
task). In TEST500 data set, the models are trained
on the initial 500 sentences of the training set and
evaluated on the last 909 test sentences. We re-
port these results to analyze the models’ perfor-
mance on a reduced training size. In DEV2500
data set, models are trained on 90% of the initial
2500 sentences and evaluated on 10% of the 2500
</bodyText>
<table confidence="0.998839571428571">
# Dataset Avg # hyp Accuracy
1 TEST25001-best 1 86.0%
2 TEST2500 max-5 3.34 95.2%
3 TEST5001-best 1 67.9%
4 TEST500 max-5 4.25 83.8%
5 DEV25001-best 1 90.8%
6 DEV2500 max-5 2.9 98.0%
</table>
<tableCaption confidence="0.7873085">
Table 3: Tagger accuracy for 1-best and maximum
of 5-best hypotheses (max-5).
</tableCaption>
<bodyText confidence="0.991865">
sentences using a random data split. We observe
that sentence length and standard deviation of test
sentences in the TEST2500 data set is higher than
on the training sentences while in the DEV2500
data set training and test sentence length and stan-
dard deviation are comparable.
</bodyText>
<subsectionHeader confidence="0.998651">
3.1 Semantic Tagging
</subsectionHeader>
<bodyText confidence="0.99994965">
Table 3 presents sentence accuracy of the seman-
tic tagging stage. Tagging accuracy is evaluated
on 1-best and on max-5 best tagger outputs. In
the max-5 setting the number of hypotheses gen-
erated by the tagger varies for each input with the
average numbers reported in Table 3. Tagging ac-
curacy on TEST2500 using 1-best is 86.0%. Con-
sidering max-5 best tagging sequences, the accu-
racy is 95.2%. On the TEST500 data set tagging
accuracy is 67.9% and 83.8% on 1-best and max-
5 best sequences respectively, approximately 8%
points lower than on TEST2500 data set. On the
DEV2500 data set tagging accuracy is 90.8% and
98.0% on 1-best and max-5 best sequences, 4.8%
and 2.8% points higher than on the TEST2500
data set. The higher performance on DEV2500 in
comparison to the TEST2500 can be explained by
the higher complexity of the test sentences in com-
parison to the training sentences in the TEST2500
data set.
</bodyText>
<subsectionHeader confidence="0.999262">
3.2 RCL Parsing
</subsectionHeader>
<bodyText confidence="0.999655125">
Parsing was evaluated using the EVALB scoring
metric (Collins, 1997). Its 1-best F-measure accu-
racy on gold standard TEST2500 and DEV2500
semantic tag sequences was 96.17% and 95.20%,
respectively. On TEST500, its accuracy remained
95.20%. On TEST2500 with system provided in-
put sequences, its accuracy was 94.79% for 869
out of 909 sentences that were tagged correctly.
</bodyText>
<subsectionHeader confidence="0.995652">
3.3 System Accuracy
</subsectionHeader>
<bodyText confidence="0.9149245">
Table 4 presents string accuracy of automatically
generated RCL parse trees on each data set. The
</bodyText>
<page confidence="0.995994">
111
</page>
<table confidence="0.99483325">
Name Train #sent Train Sent. len. (stdev) Test #sent Test Sent. Len. (stdev)
TEST2500 2500 13.44 (5.50) 909 13.96 (5.59)
TEST500 500 14.62(5.66) 909 13.96 (5.59)
DEV2500 2250 13.43 ( 5.53) 250 13.57 (5.27)
</table>
<tableCaption confidence="0.998691">
Table 2: Number of sentences, average length and standard deviation of the data sets.
</tableCaption>
<bodyText confidence="0.998133065217391">
results are obtained by comparing system output
RCL parse string with the reference RCL parse
string. For each data set, we ran the system
with and without spatial validation. We ran RCL
parser and reference resolution on automatically
assigned semantic tags (Auto) and oracle tagging
(Orcl). We observed that some tag labels can be
verified systematically and corrected them with
simple rules: e.g., change “front” to “forward”
because relation specification in (Dukes, 2013)
doesn’t have “front” even though annotations in-
cluded cases with “front” as relation.
The system performance on TEST2500 data
set using automatically assigned tags and no spa-
tial validation is 60.84%. In this mode, the sys-
tem uses 1-best parser and 1-best tagger output.
With spatial validation, which allows the system to
re-rank parser and tagger hypotheses, the perfor-
mance increases by 27% points to 87.35%. This
indicates that the parser and the tagger component
often produce a correct output which is not ranked
first. Using oracle tags without / with spatial vali-
dation on TEST2500 data set the system accuracy
is 67.55% / 94.83%, 7% points above the accuracy
using predicted tags.
The system performance on TEST500 data set
using automatically assigned tags with / with-
out spatial validation is 48.95% / 74.92%, ap-
proximately 12% points below the performance
on TEST2500 (Row 1). Using oracle tags with-
out / with spatial validation the performance on
TEST500 data set is 63.89% / 94.94%. The per-
formance without spatial validation is only 4% be-
low TEST2500, while with spatial validation the
performance on TEST2500 and TEST500 is the
same. These results indicate that most perfor-
mance degradation on a smaller data set is due to
the semantic tagger.
The system performance on DEV2500 data set
using automatically assigned tags without / with
spatial validation is 68.0% / 96.80% (Row 5), 8%
points above the performance on TEST2500 (Row
1). With oracle tags, the performance is 69.60%
/ 98.0%, which is 2-3% points above TEST2500
(Row 2). These results indicate that most perfor-
mance improvement on a better balanced data set
</bodyText>
<table confidence="0.993738375">
# Dataset Tag Accuracy without / with
spatial validation
1 TEST2500 Auto 60.84 / 87.35
2 TEST2500 Orcl 67.55 / 94.83
3 TEST500 Auto 48.95 / 74.92
4 TEST500 Orcl 63.89 / 94.94
5 DEV2500 Auto 68.00 / 96.80
6 DEV2500 Orcl 69.60 / 98.00
</table>
<tableCaption confidence="0.977922">
Table 4: System accuracy with and without spatial
validation using automatically assigned tags and
oracle tags (OT).
</tableCaption>
<bodyText confidence="0.668348">
DEV2500 is due to better semantic tagging.
</bodyText>
<sectionHeader confidence="0.988757" genericHeader="conclusions">
4 Summary and Future Work
</sectionHeader>
<bodyText confidence="0.999947333333333">
In this paper, we present the results of semantic
processing for natural language robot commands
using Tag&amp;Parse approach. The system first tags
the input sentence and then applies non-lexical
parsing to the tag sequence. Reference resolution
is applied to the resulting parse trees. We com-
pare the results of the models trained on the data
sets of size 500 (TEST500) and 2500 (TEST2500)
sentences. We observe that sequence tagging
model degrades significantly on a smaller data set.
Parsing and reference resolution models, on the
other hand, perform nearly as well on both train-
ing sizes. We compare the results of the models
trained on more (DEV2500) and less (TEST2500)
homogeneous training/testing data sets. We ob-
serve that a semantic tagging model is more sen-
sitive to the difference between training and test
set than parsing model degrading significantly a
less homogeneous data set. Our results show that
1) both tagging and parsing models will benefit
from an improved re-ranking, and 2) our parsing
model is robust to a data size reduction while tag-
ging model requires a larger training data set.
In future work we plan to explore how
Tag&amp;Parse approach will generalize in other do-
mains. In particular, we are interested in using
a combination of domain-specific tagging models
and generic semantic parsing (Das et al., 2010) for
processing spoken commands in a dialogue sys-
tem.
</bodyText>
<page confidence="0.997592">
112
</page>
<sectionHeader confidence="0.998336" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999935823529412">
Xavier Carreras, Llu´ıs M`arquez, and Llu´ıs Padr´o.
2003. A Simple Named Entity Extractor Using Ad-
aBoost. In Proceedings of the CoNLL, pages 152–
157, Edmonton, Canada.
Michael Collins. 1997. Three Generative Lexicalized
Models for Statistical Parsing. In Proceedings of the
35th Annual Meeting of the ACL, pages 16–23.
Dipanjan Das, Nathan Schneider, Desai Chen, and
Noah A. Smith. 2010. Probabilistic Frame-
Semantic Parsing. In HLT-NAACL, pages 948–956.
Kais Dukes. 2013. Semantic Annotation of Robotic
Spatial Commands. In Language and Technology
Conference (LTC).
Patrick Haffner. 2006. Scaling large margin classifiers
for spoken language understanding. Speech Com-
munication, 48(3-4):239–261.
Lynette Hirschman and Nancy Chinchor. 1997. MUC-
7 Coreference Task Definition. In Proceedings of
the Message Understanding Conference (MUC-7).
Science Applications International Corporation.
Liang Huang and David Chiang. 2005. Better K-
best Parsing. In Proceedings of the Ninth Inter-
national Workshop on Parsing Technology, Parsing
’05, pages 53–64, Stroudsburg, PA, USA.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313–330.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-Rich Part-
of-Speech Tagging with a Cyclic Dependency Net-
work. In Proceedings of the 2003 Conference of the
NAACL on Human Language Technology - Volume
1, pages 173–180.
</reference>
<page confidence="0.999319">
113
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.523204">
<title confidence="0.9910775">AT&amp;T: The Tag&amp;Parse Approach to Semantic Parsing of Robot Spatial Commands</title>
<author confidence="0.972039">Svetlana Stoyanchev</author>
<author confidence="0.972039">Hyuckchul Jung</author>
<author confidence="0.972039">John Chen</author>
<author confidence="0.972039">Srinivas</author>
<affiliation confidence="0.958557">AT&amp;T Labs</affiliation>
<address confidence="0.533409">1 AT&amp;T Way Bedminster NJ</address>
<abstract confidence="0.999435571428571">to semantic parsing first assigns semantic tags to each word in a sentence and then parses the tag sequence into a semantic tree. We use statistical approach for tagging, parsing, and reference resolution stages. Each stage produces multiple hypotheses which are re-ranked using spatial validation. We the on a corpus of Robotic Spatial Commands as part of the SemEval Task6 exercise. Our system accuracy is 87.35% and 60.84% with and without spatial validation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Llu´ıs M`arquez</author>
<author>Llu´ıs Padr´o</author>
</authors>
<title>A Simple Named Entity Extractor Using AdaBoost.</title>
<date>2003</date>
<booktitle>In Proceedings of the CoNLL,</booktitle>
<pages>152--157</pages>
<location>Edmonton, Canada.</location>
<marker>Carreras, M`arquez, Padr´o, 2003</marker>
<rawString>Xavier Carreras, Llu´ıs M`arquez, and Llu´ıs Padr´o. 2003. A Simple Named Entity Extractor Using AdaBoost. In Proceedings of the CoNLL, pages 152– 157, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three Generative Lexicalized Models for Statistical Parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the ACL,</booktitle>
<pages>16--23</pages>
<contexts>
<context position="11025" citStr="Collins, 1997" startWordPosition="1839" endWordPosition="1840">uracy is 95.2%. On the TEST500 data set tagging accuracy is 67.9% and 83.8% on 1-best and max5 best sequences respectively, approximately 8% points lower than on TEST2500 data set. On the DEV2500 data set tagging accuracy is 90.8% and 98.0% on 1-best and max-5 best sequences, 4.8% and 2.8% points higher than on the TEST2500 data set. The higher performance on DEV2500 in comparison to the TEST2500 can be explained by the higher complexity of the test sentences in comparison to the training sentences in the TEST2500 data set. 3.2 RCL Parsing Parsing was evaluated using the EVALB scoring metric (Collins, 1997). Its 1-best F-measure accuracy on gold standard TEST2500 and DEV2500 semantic tag sequences was 96.17% and 95.20%, respectively. On TEST500, its accuracy remained 95.20%. On TEST2500 with system provided input sequences, its accuracy was 94.79% for 869 out of 909 sentences that were tagged correctly. 3.3 System Accuracy Table 4 presents string accuracy of automatically generated RCL parse trees on each data set. The 111 Name Train #sent Train Sent. len. (stdev) Test #sent Test Sent. Len. (stdev) TEST2500 2500 13.44 (5.50) 909 13.96 (5.59) TEST500 500 14.62(5.66) 909 13.96 (5.59) DEV2500 2250 </context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three Generative Lexicalized Models for Statistical Parsing. In Proceedings of the 35th Annual Meeting of the ACL, pages 16–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Nathan Schneider</author>
<author>Desai Chen</author>
<author>Noah A Smith</author>
</authors>
<title>Probabilistic FrameSemantic Parsing. In</title>
<date>2010</date>
<booktitle>HLT-NAACL,</booktitle>
<pages>948--956</pages>
<marker>Das, Schneider, Chen, Smith, 2010</marker>
<rawString>Dipanjan Das, Nathan Schneider, Desai Chen, and Noah A. Smith. 2010. Probabilistic FrameSemantic Parsing. In HLT-NAACL, pages 948–956.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kais Dukes</author>
</authors>
<title>Semantic Annotation of Robotic Spatial Commands.</title>
<date>2013</date>
<booktitle>In Language and Technology Conference (LTC).</booktitle>
<contexts>
<context position="1416" citStr="Dukes, 2013" startWordPosition="222" endWordPosition="223">tem participating in the SemEval2014 Task-6 on Supervised Semantic Parsing of Robotic Spatial Commands. It produces a semantic parse of natural language commands addressed to a robot arm designed to move objects on a grid surface. Each command directs a robot to change position of an object given a current configuration. A command uniquely identifies an object and its destination, for example “Move the turquoise pyramid above the yellow cube”. System output is a Robot Control Language (RCL) parse (see Figure 1) which is processed by the robot arm simulator. The Robot Spatial Commands dataset (Dukes, 2013) is used for training and testing. Our system uses a Tag&amp;Parse approach which separates semantic tagging and semantic parsing stages. It has four components: 1) semantic tagging, 2) parsing, 3) reference resolution, and 4) spatial validation. The first three are trained using LLAMA (Haffner, 2006), a supervised machine learning toolkit, on the RCL-parsed sentences. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http: //creativecommons.org/licenses/by/4.0/ For semantic taggin</context>
<context position="12218" citStr="Dukes, 2013" startWordPosition="2027" endWordPosition="2028">5.59) DEV2500 2250 13.43 ( 5.53) 250 13.57 (5.27) Table 2: Number of sentences, average length and standard deviation of the data sets. results are obtained by comparing system output RCL parse string with the reference RCL parse string. For each data set, we ran the system with and without spatial validation. We ran RCL parser and reference resolution on automatically assigned semantic tags (Auto) and oracle tagging (Orcl). We observed that some tag labels can be verified systematically and corrected them with simple rules: e.g., change “front” to “forward” because relation specification in (Dukes, 2013) doesn’t have “front” even though annotations included cases with “front” as relation. The system performance on TEST2500 data set using automatically assigned tags and no spatial validation is 60.84%. In this mode, the system uses 1-best parser and 1-best tagger output. With spatial validation, which allows the system to re-rank parser and tagger hypotheses, the performance increases by 27% points to 87.35%. This indicates that the parser and the tagger component often produce a correct output which is not ranked first. Using oracle tags without / with spatial validation on TEST2500 data set </context>
</contexts>
<marker>Dukes, 2013</marker>
<rawString>Kais Dukes. 2013. Semantic Annotation of Robotic Spatial Commands. In Language and Technology Conference (LTC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Haffner</author>
</authors>
<title>Scaling large margin classifiers for spoken language understanding.</title>
<date>2006</date>
<journal>Speech Communication,</journal>
<pages>48--3</pages>
<contexts>
<context position="1714" citStr="Haffner, 2006" startWordPosition="268" endWordPosition="269">n a current configuration. A command uniquely identifies an object and its destination, for example “Move the turquoise pyramid above the yellow cube”. System output is a Robot Control Language (RCL) parse (see Figure 1) which is processed by the robot arm simulator. The Robot Spatial Commands dataset (Dukes, 2013) is used for training and testing. Our system uses a Tag&amp;Parse approach which separates semantic tagging and semantic parsing stages. It has four components: 1) semantic tagging, 2) parsing, 3) reference resolution, and 4) spatial validation. The first three are trained using LLAMA (Haffner, 2006), a supervised machine learning toolkit, on the RCL-parsed sentences. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http: //creativecommons.org/licenses/by/4.0/ For semantic tagging, we train a maximum entropy sequence tagger for assigning a semantic label and value to each word in a sentence, such as type cube or color blue. For parsing, we train a constituency parser on non-lexical RCL semantic trees. For reference resolution, we train a maximum entropy model that identif</context>
<context position="4833" citStr="Haffner, 2006" startWordPosition="789" endWordPosition="790">s for a sentence Move the turquoise pyramid above the yellow cube. ing a combined semantic tag and label (such as type cube) to each word in a command. The tags used for training are extracted from the leaf-level nodes of the RCL trees. Table 2 shows tags and labels for a sample sentence “Move the turquoise pyramid above the yellow cube” extracted from the RCL parse tree (see Figure 1). In some cases, a label is the same as a word (yellow, cube) while in other cases, it differs (turquoise - cyan, pyramid - prism). We train a sequence tagger using LLAMA maximum entropy (maxent) classification (Haffner, 2006) to predict the combined semantic tag and label of each word. Neighboring words, immediately neighboring semantic tags, and POS tags are used as features, where the POS tagger is another sequence tagging model trained on the Penn Treebank (Marcus et al., 1993). We also experimented with a tagger that assigns tags and labels in separate sequence tagging models, but it performed poorly. 2.2 Parsing We use a constituency parser for building RCL trees. The input to the parser is a sequence of tags assigned by a sequence tagger, such as “action color type relation color type” for the example in Fig</context>
</contexts>
<marker>Haffner, 2006</marker>
<rawString>Patrick Haffner. 2006. Scaling large margin classifiers for spoken language understanding. Speech Communication, 48(3-4):239–261.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynette Hirschman</author>
<author>Nancy Chinchor</author>
</authors>
<title>MUC7 Coreference Task Definition.</title>
<date>1997</date>
<booktitle>In Proceedings of the Message Understanding Conference (MUC-7). Science Applications International Corporation.</booktitle>
<contexts>
<context position="6410" citStr="Hirschman and Chinchor, 1997" startWordPosition="1054" endWordPosition="1057">rs per constituent along the lines of (Huang and Chiang, 2005). G was obtained and PG was estimated from a corpus of non-lexical RCL trees generated by removing all nodes descendant from the tag nodes (action, color, etc.). Parses may contain empty nodes not corresponding to any tag in the input sequence. These are hypothesized by the parser at positions in between input tags and inserted as edges according to the PCFG, which has probabilistic rules for generating empty nodes. 2.3 Reference Resolution Reference resolution identifies the most probable antecedent for each anaphor within a text (Hirschman and Chinchor, 1997). It applies when multiple candidates antecedents are present. For example, in a sentence “Pick up the red cube standing on a grey cube and place it on top of the yellow one”, the anaphor it has two candidate antecedents corresponding to entity segments the red cube and a grey cube. In our system, anaphor and antecedents are represented by reference tags occurring in one sentence. A reference tag is either assigned by a sequence tagger to one of the words (e.g. to a pronoun) or is inserted into a tree by the parser (e.g. ellipsis). We train a binary maxent model for this task using LLAMA. The </context>
</contexts>
<marker>Hirschman, Chinchor, 1997</marker>
<rawString>Lynette Hirschman and Nancy Chinchor. 1997. MUC7 Coreference Task Definition. In Proceedings of the Message Understanding Conference (MUC-7). Science Applications International Corporation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better Kbest Parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth International Workshop on Parsing Technology, Parsing ’05,</booktitle>
<pages>53--64</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5843" citStr="Huang and Chiang, 2005" startWordPosition="962" endWordPosition="965"> 2.2 Parsing We use a constituency parser for building RCL trees. The input to the parser is a sequence of tags assigned by a sequence tagger, such as “action color type relation color type” for the example in Figure 1. The parser generates multiple RCL parse tree hypotheses sorted in the order of their likelihood. The likelihood of a tree T given a sequence of tags T is determined using a probabilistic context free grammar (PCFG) G: P(T|S) = 11 PG(r) (1) r∈T The n-best parses are obtained using the CKY algorithm, recording the n-best hyperedge backpointers per constituent along the lines of (Huang and Chiang, 2005). G was obtained and PG was estimated from a corpus of non-lexical RCL trees generated by removing all nodes descendant from the tag nodes (action, color, etc.). Parses may contain empty nodes not corresponding to any tag in the input sequence. These are hypothesized by the parser at positions in between input tags and inserted as edges according to the PCFG, which has probabilistic rules for generating empty nodes. 2.3 Reference Resolution Reference resolution identifies the most probable antecedent for each anaphor within a text (Hirschman and Chinchor, 1997). It applies when multiple candid</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better Kbest Parsing. In Proceedings of the Ninth International Workshop on Parsing Technology, Parsing ’05, pages 53–64, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="5093" citStr="Marcus et al., 1993" startWordPosition="831" endWordPosition="834">s and labels for a sample sentence “Move the turquoise pyramid above the yellow cube” extracted from the RCL parse tree (see Figure 1). In some cases, a label is the same as a word (yellow, cube) while in other cases, it differs (turquoise - cyan, pyramid - prism). We train a sequence tagger using LLAMA maximum entropy (maxent) classification (Haffner, 2006) to predict the combined semantic tag and label of each word. Neighboring words, immediately neighboring semantic tags, and POS tags are used as features, where the POS tagger is another sequence tagging model trained on the Penn Treebank (Marcus et al., 1993). We also experimented with a tagger that assigns tags and labels in separate sequence tagging models, but it performed poorly. 2.2 Parsing We use a constituency parser for building RCL trees. The input to the parser is a sequence of tags assigned by a sequence tagger, such as “action color type relation color type” for the example in Figure 1. The parser generates multiple RCL parse tree hypotheses sorted in the order of their likelihood. The likelihood of a tree T given a sequence of tags T is determined using a probabilistic context free grammar (PCFG) G: P(T|S) = 11 PG(r) (1) r∈T The n-bes</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-Rich Partof-Speech Tagging with a Cyclic Dependency Network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the NAACL on Human Language Technology -</booktitle>
<volume>1</volume>
<pages>173--180</pages>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-Rich Partof-Speech Tagging with a Cyclic Dependency Network. In Proceedings of the 2003 Conference of the NAACL on Human Language Technology - Volume 1, pages 173–180.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>