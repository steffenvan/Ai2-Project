<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000009">
<title confidence="0.998496">
Towards Robust Abstractive Multi-Document Summarization: A
Caseframe Analysis of Centrality and Domain
</title>
<author confidence="0.999556">
Jackie Chi Kit Cheung
</author>
<affiliation confidence="0.999442">
University of Toronto
</affiliation>
<address confidence="0.995357">
10 King’s College Rd., Room 3302
Toronto, ON, Canada M5S 3G4
</address>
<email confidence="0.999492">
jcheung@cs.toronto.edu
</email>
<author confidence="0.995128">
Gerald Penn
</author>
<affiliation confidence="0.998794">
University of Toronto
</affiliation>
<address confidence="0.995339">
10 King’s College Rd., Room 3302
Toronto, ON, Canada M5S 3G4
</address>
<email confidence="0.999519">
gpenn@cs.toronto.edu
</email>
<sectionHeader confidence="0.99391" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99997184">
In automatic summarization, centrality is
the notion that a summary should contain
the core parts of the source text. Cur-
rent systems use centrality, along with re-
dundancy avoidance and some sentence
compression, to produce mostly extrac-
tive summaries. In this paper, we investi-
gate how summarization can advance past
this paradigm towards robust abstraction
by making greater use of the domain of
the source text. We conduct a series of
studies comparing human-written model
summaries to system summaries at the se-
mantic level of caseframes. We show that
model summaries (1) are more abstrac-
tive and make use of more sentence aggre-
gation, (2) do not contain as many topi-
cal caseframes as system summaries, and
(3) cannot be reconstructed solely from
the source text, but can be if texts from
in-domain documents are added. These
results suggest that substantial improve-
ments are unlikely to result from better
optimizing centrality-based criteria, but
rather more domain knowledge is needed.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999959808510639">
In automatic summarization, centrality has been
one of the guiding principles for content selection
in extractive systems. We define centrality to be
the idea that a summary should contain the parts
of the source text that are most similar or repre-
sentative of the source text. This is most trans-
parently illustrated by the Maximal Marginal Rel-
evance (MMR) system of Carbonell and Goldstein
(1998), which defines the summarization objective
to be a linear combination of a centrality term and
a non-redundancy term.
Since MMR, much progress has been made on
more sophisticated methods of measuring central-
ity and integrating it with non-redundancy (See
Nenkova and McKeown (2011) for a recent sur-
vey). For example, term weighting methods such
as the signature term method of Lin and Hovy
(2000) pick out salient terms that occur more often
than would be expected in the source text based on
frequencies in a background corpus. This method
is a core component of the most successful sum-
marization methods (Conroy et al., 2006).
While extractive methods based on centrality
have thus achieved success, there has long been
recognition that abstractive methods are ultimately
more desirable. One line of work is in text simpli-
fication and sentence fusion, which focus on the
ability of abstraction to achieve a higher compres-
sion ratio (Knight and Marcu, 2000; Barzilay and
McKeown, 2005). A less examined issue is that of
aggregation and information synthesis. A key part
of the usefulness of summaries is that they provide
some synthesis or analysis of the source text and
make a more general statement that is of direct rel-
evance to the user. For example, a series of related
events can be aggregated and expressed as a trend.
The position of this paper is that centrality is
not enough to make substantial progress towards
abstractive summarization that is capable of this
type of semantic inference. Instead, summariza-
tion systems need to make more use of domain
knowledge. We provide evidence for this in a se-
ries of studies on the TAC 2010 guided summa-
rization data set that examines how the behaviour
of automatic summarizers can or cannot be dis-
tinguished from human summarizers. First, we
confirm that abstraction is a desirable goal, and
</bodyText>
<page confidence="0.846665">
1233
</page>
<note confidence="0.9133295">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1233–1242,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999954961538462">
provide a quantitative measure of the degree of
sentence aggregation in a summarization system.
Second, we show that centrality-based measures
are unlikely to lead to substantial progress towards
abstractive summarization, because current top-
performing systems already produce summaries
that are more “central” than humans do. Third, we
consider how domain knowledge may be useful as
a resource for an abstractive system, by showing
that key parts of model summaries can be recon-
structed from the source plus related in-domain
documents.
Our contributions are novel in the following re-
spects. First, our analyses are performed at the
level of caseframes, rather at the level of words or
syntactic dependencies as in previous work. Case-
frames are shallow approximations of semantic
roles which are well suited to characterizing a do-
main by its slots. Furthermore, we take a devel-
opmental rather than evaluative perspective—our
goal is not to develop a new evaluation measure as
defined by correlation with human responsiveness
judgments. Instead, our studies reveal useful cri-
teria with which to distinguish human-written and
system summaries, helping to guide the develop-
ment of future summarization systems.
</bodyText>
<sectionHeader confidence="0.999692" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999926791666667">
Domain-dependent template-based summariza-
tion systems have been an alternative to extractive
systems which make use of rich knowledge about
a domain and information extraction techniques to
generate a summary, possibly using a natural lan-
guage generation system (Radev and McKeown,
1998; White et al., 2001; McKeown et al., 2002).
This paper can be seen as a first step towards
reconciling the advantages of domain knowledge
with the resource-lean extraction approaches pop-
ular today.
As noted above, Lin and Hovy’s (2000) sig-
nature terms have been successful in discovering
terms that are specific to the source text. These
terms are identified by a log-likelihood ratio test
based on their relative frequencies in relevant and
irrelevant documents. They were originally pro-
posed in the context of single-document summa-
rization, where they were calculated using in-
domain (relevant) vs. out-of-domain (irrelevant)
text. In multi-document summarization, the in-
domain text has been replaced by the source text
cluster (Conroy et al., 2006), thus they are now
used as a form of centrality-based features. In
this paper, we use guided summarization data as
an opportunity to reopen the investigation into the
effect of domain, because multiple document clus-
ters from the same domain are available.
Summarization evaluation is typically done by
comparing system output to human-written model
summaries, and are validated by their correlation
with user responsiveness judgments. The compar-
ison can be done at the word level, as in ROUGE
(Lin, 2004), at the syntactic level, as in Basic
Elements (Hovy et al., 2006), or at the level of
summary content units, as in the Pyramid method
(Nenkova and Passonneau, 2004). There are also
automatic measures which do not require model
summaries, but compare against the source text in-
stead (Louis and Nenkova, 2009; Saggion et al.,
2010).
Several studies complement this paper by ex-
amining the best possible extractive system us-
ing current evaluation measures, such as ROUGE
(Lin and Hovy, 2003; Conroy et al., 2006). They
find that the best possible extractive systems score
higher or as highly than human summarizers, but
it is unclear whether this means the oracle sum-
maries are actually as useful as human ones in
an extrinsic setting. Genest et al. (2009) ask hu-
mans to create extractive summaries, and find that
they score in between current automatic systems
and human-written abstracts on responsiveness,
linguistic quality, and Pyramid score. In the lec-
ture domain, He et al. (1999; 2000) find that
lecture transcripts that have been manually high-
lighted with key points improve students’ quiz
scores more than when using automated summa-
rization techniques or when providing only the
lecture transcript or slides.
Jing and McKeown (2000) manually analyzed
30 human-written summaries, and find that 19%
of sentences cannot be explained by cut-and-paste
operations from the source text. Saggion and La-
palme (2002) similarly define a list of transfor-
mations necessary to convert source text to sum-
mary text, and manually analyzed their frequen-
cies. Copeck and Szpakowicz (2004) find that
at most 55% of vocabulary items found in model
summaries occur in the source text, but they do
not investigate where the other vocabulary items
might be found.
</bodyText>
<page confidence="0.957538">
1234
</page>
<bodyText confidence="0.707088">
Sentence:
At one point, two bomb squad trucks sped to
the school after a backpack scare.
</bodyText>
<equation confidence="0.6798011">
Dependencies:
num(point, one) prep at(sped, point)
num(trucks, two) nn(trucks, bomb)
nn(trucks, squad) nsubj(sped, trucks)
root(ROOT, sped) det(school, the)
prep to(sped, school) det(scare, a)
nn(scare, backpack) prep after(sped, scare)
Caseframes:
(speed, prep at) (speed, nsubj)
(speed, prep to) (speed, prep after)
</equation>
<tableCaption confidence="0.726407333333333">
Table 1: A sentence decomposed into its depen-
dency edges, and the caseframes derived from
those edges that we consider (in black).
</tableCaption>
<sectionHeader confidence="0.953445" genericHeader="method">
3 Theoretical basis of our analysis
</sectionHeader>
<bodyText confidence="0.999962925925926">
Many existing summarization evaluation methods
rely on word or N-gram overlap measures, but
these measures are not appropriate for our anal-
ysis. Word overlap can occur due to shared proper
nouns or entity mentions. Good summaries should
certainly contain the salient entities in the source
text, but when assessing the effect of the domain,
different domain instances (i.e., different docu-
ment clusters in the same domain) would be ex-
pected to contain different salient entities. Also,
the realization of entities as noun phrases depends
strongly on context, which would confound our
analysis if we do not also correctly resolve corefer-
ence, a difficult problem in its own right. We leave
such issues to other work (Nenkova and McKe-
own, 2003, e.g.).
Domains would rather be expected to share slots
(a.k.a. aspects), which require a more semantic
level of analysis that can account for the various
ways in which a particular slot can be expressed.
Another consideration is that the structures to be
analyzed should be extracted automatically. Based
on these criteria, we selected caseframes to be the
appropriate unit of analysis. A caseframe is a shal-
low approximation of the semantic role structure
of a proposition-bearing unit like a verb, and are
derived from the dependency parse of a sentence1.
</bodyText>
<figure confidence="0.6899512">
1Note that caseframes are distinct from (though directly
Relation Caseframe Pair Sim.
Degree (kill, dobj) (wound, dobj) 0.82
Causal (kill, dobj) (die, nsubj) 0.80
Type (rise, dobj) (drop, prep to) 0.81
</figure>
<figureCaption confidence="0.91226">
Figure 1: Sample pairs of similar caseframes by
</figureCaption>
<bodyText confidence="0.970377517241379">
relation type, and the similarity score assigned to
them by our distributional model.
In particular, they are (gov, role) pairs, where gov
is a proposition-bearing element, and role is an
approximation of a semantic role with gov as its
head (See Figure 1 for examples). Caseframes do
not consider the dependents of the semantic role
approximations.
The use of caseframes is well grounded in a va-
riety of NLP tasks relevant to summarization such
as coreference resolution (Bean and Riloff, 2004),
and information extraction (Chambers and Juraf-
sky, 2011), where they serve the central unit of se-
mantic analysis. Related semantic representations
are popular in Case Grammar and its derivative
formalisms such as frame semantics (Fillmore,
1982).
We use the following algorithm to extract case-
frames from dependency parses. First, we extract
those dependency edges with a relation type of
subject, direct object, indirect object, or prepo-
sitional object (with the preposition indicated),
along with their governors. The governor must be
a verb, event noun (as defined by the hyponyms
of the WordNet EVENT synset), or nominal or ad-
jectival predicate. Then, a series of deterministic
transformations are applied to the syntactic rela-
tions to account for voicing alternations, control,
raising, and copular constructions.
</bodyText>
<subsectionHeader confidence="0.999726">
3.1 Caseframe Similarity
</subsectionHeader>
<bodyText confidence="0.951827">
Direct caseframe matches account for some vari-
ation in the expression of slots, such as voicing
alternations, but there are other reasons different
caseframes may indicate the same slot (Figure 1).
For example, (kill, dobj) and (wound, dobj) both
indicate the victim of an attack, but differ by
the degree of injury to the victim. (kill, dobj)
and (die, nsubj) also refer to a victim, but are
linked by a causal relation. (rise, dobj) and
inspired by) the similarly named case frames of Case Gram-
mar (Fillmore, 1968).
</bodyText>
<page confidence="0.953896">
1235
</page>
<bodyText confidence="0.999961444444445">
(drop, prep to) on the other hand simply share a
named entity type (in this case, numbers). To ac-
count for these issues, we measure caseframe sim-
ilarity based on their distributional similarity in a
large training corpus.
First, we construct vector representations of
each caseframe, where the dimensions of the vec-
tor correspond to the lemma of the head word that
fills the caseframe in the training corpus. For ex-
ample, kicked the ball would result in a count of
1 added to the caseframe (kick, dobj) for the con-
text word ball. Then, we rescale the counts into
pointwise mutual information values, which has
been shown to be more effective than raw counts
at detecting semantic relatedness (Turney, 2001).
Similarity between caseframes can then be com-
pared by cosine similarity between the their vector
representations.
For training, we use the AFP portion of the
Gigaword corpus (Graff et al., 2005), which we
parsed using the Stanford parser’s typed depen-
dency tree representation with collapsed conjunc-
tions (de Marneffe et al., 2006). For reasons of
sparsity, we only considered caseframes that ap-
pear at least five times in the guided summariza-
tion corpus, and only the 3000 most common lem-
mata in Gigaword as context words.
</bodyText>
<subsectionHeader confidence="0.999891">
3.2 An Example
</subsectionHeader>
<bodyText confidence="0.97764545">
To illustrate how caseframes indicate the slots in a
summary, we provide the following fragment of a
model summary from TAC about the Unabomber
trial:
(1) In Sacramento, Theodore Kaczynski faces a
10-count federal indictment for 4 of the 16
mail bomb attacks attributed to the
Unabomber in which two people were killed.
Iffound guilty, he faces a death penalty....
He has pleaded innocent to all charges. U.S.
District Judge Garland Burrell Jr. presides
in Sacramento.
All of the slots provided by TAC for the Inves-
tigations and Trials domain can be identified by
one or more caseframes. The DEFENDANT can be
identified by (face, nsubj), and (plead, nsubj);
the CHARGES by (face, dobj); the REASON
by (indictment, prep for); the SENTENCE by
(face, dobj); the PLEAD by (plead, dobj); and
the INVESTIGATOR by (preside, nsubj).
</bodyText>
<sectionHeader confidence="0.998715" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.988811479166667">
We conducted our experiments on the data and re-
sults of the TAC 2010 summarization workshop.
This data set contains 920 newspaper articles in
46 topics of 20 documents each. Ten are used in
an initial guided summarization task, and ten are
used in an update summarization task, in which
a summary must be produced assuming that the
original ten documents had already been read. All
summaries have a word length limit of 100 words.
We analyzed the results of the two summarization
tasks separately in our experiments.
The 46 topics belong to five different cate-
gories or domains: Accidents and natural dis-
asters, Criminal or terrorist attacks, Health and
safety, Endangered resources, and Investigations
and trials. Each domain is associated with a tem-
plate specifying the type of information that is ex-
pected in the domain, such as the participants in
the event or the time that the event occurred.
In our study, we compared the characteristics of
summaries generated by the eight human summa-
rizers with those generated by the peer summaries,
which are basically extractive systems. There
are 43 peer summarization systems, including two
baselines defined by NIST. We refer to systems
by their ID given by NIST, which are alphabetical
for the human summarizers (A to H), and numeric
for the peer summarizers (1 to 43). We removed
two peer systems (systems 29 and 43) which did
not generate any summary text in the workshop,
presumably due to software problems. For each
measure that we consider, we compare the average
among the human-written summaries to the three
individual peer systems, which we chose in order
to provide a representative sample of the average
and best performance of the automatic systems ac-
cording to current evaluation methods. These sys-
tems are all primarily extractive, like most of the
systems in the workshop:
Peer average The average of the measure
among the 41 peer summarizers.
Peer 16 This system scored the highest in re-
sponsiveness scores on the original summarization
task and in ROUGE-2, responsiveness, and Pyra-
mid score in the update task.
Peer 22 This system scored the highest in
ROUGE-2 and Pyramid score in the original sum-
marization task.
</bodyText>
<page confidence="0.942957">
1236
</page>
<figure confidence="0.999738454545454">
Number of sentences
System IDs
(a) Initial guided summarization task
(b) Update summarization task
1 428251210153136184281430373313192416214041327381735233934227 6202611325 9G2 FBEADCH
2.0
1.5
1.0
0.5
0.0
281311842410153624251213163034273 9 81437334071139381935262317222163241520FG2 ECBAHD
1.8
1.6
1.4
Number of sentences
1.2
1.0
0.8
0.6
0.4
0.2
0.0
</figure>
<figureCaption confidence="0.963511666666667">
Figure 2: Average sentence cover size: the average number of sentences needed to generate the case-
frames in a summary sentence (Study 1). Model summaries are shown in darker bars. Peer system
numbers that we focus on are in bold.
</figureCaption>
<table confidence="0.960323666666667">
Condition Initial Update
Model average 1.58 1.57
Peer average 1.06 1.06
Peer 1 1.00 1.00
Peer 16 1.04 1.04
Peer 22 1.08 1.09
</table>
<tableCaption confidence="0.852367">
Table 2: The average number of source text sen-
</tableCaption>
<bodyText confidence="0.961108444444445">
tences needed to cover a summary sentence. The
model average is statistically significantly differ-
ent from all the other conditions p &lt; 10−7
(Study 1).
Peer 1 The NIST-defined baseline, which is the
leading sentence baseline from the most recent
document in the source text cluster. This system
scored the highest on linguistic quality in both
tasks.
</bodyText>
<subsectionHeader confidence="0.996415">
4.1 Study 1: Sentence aggregation
</subsectionHeader>
<bodyText confidence="0.9998528">
We first confirm that human summarizers are more
prone to sentence aggregation than system sum-
marizers, showing that abstraction is indeed a de-
sirable goal. To do so, we propose a measure to
quantify the degree of sentence aggregation exhib-
ited by a summarizer, which we call average sen-
tence cover size. This is defined to be the min-
imum number of sentences from the source text
needed to cover all of the caseframes found in a
summary sentence (for those caseframes that can
be found in the source text at all), averaged over all
of the summary sentences. Purely extractive sys-
tems would thus be expected to score 1.0, as would
systems that perform text compression by remov-
ing constituents of a source text sentence. Human
summarizers would be expected to score higher, if
they actually aggregate information from multiple
points in the source text.
To illustrate, suppose we assign arbitrary in-
dices to caseframes, a summary sentence con-
tains caseframes {1, 2, 3, 4, 5}, and the source
text contains three sentences with caseframes,
which can be represented as a nested set
{{1, 3, 4}, {2, 5, 6}, {1, 4, 7}}. Then, the sum-
mary sentence can be covered by two sentences
from the source text, namely {{1, 3, 4}, {2, 5, 6}}.
This problem is actually an instance of the min-
imum set cover problem, in which sentences are
sets, and caseframes are set elements. Minimum
set cover is NP-hard in general, but the standard
integer programming formulation of set cover suf-
ficed for our data set; we used ILOG CPLEX
12.4’s mixed integer programming mode to solve
all the set cover problems optimally.
Results Figure 2 shows the ranking of the sum-
marizers by this measure. Most peer systems have
a low average sentence cover size of close to 1,
which reflects the fact that they are purely or al-
most purely extractive. Human model summariz-
ers show a higher degree of aggregation in their
summaries. The averages of the tested condi-
tions are shown in Table 2, and are statistically
significant. Peer 2 shows a relatively high level
of aggregation despite being an extractive system.
Upon inspection of its summaries, it appears that
Peer 2 tends to select many datelines, and without
punctuation to separate them from the rest of the
summary, our automatic analysis tools incorrectly
merged many sentences together, resulting in in-
correct parses and novel caseframes not found in
</bodyText>
<page confidence="0.971568">
1237
</page>
<figure confidence="0.99682685">
Per word density
System IDs
(a) Initial guided summarization task
EAGB37133C122726423911H28F152D3220355407 41081914303641183 921243413222516311 762338
System IDs
(b) Update summarization task
A32B1242273733G1 5287392EFH352615CD112093614194013168304 61031841213424172531222338
0.12
0.10
0.08
0.06
0.04
0.02
0.00
Per word density 0.10
0.08
0.06
0.04
0.02
0.00
</figure>
<figureCaption confidence="0.997804">
Figure 3: Density of signature caseframes (Study 2).
</figureCaption>
<figure confidence="0.798757142857143">
Topic: Unabomber trial
(charge, dobj), (kill, dobj),
(trial, prep of), (bombing, prep in)
Topic: Mangrove forests
(beach, prep of), (save, dobj)
(development, prep of), (recover, nsubj)
Topic: Bird Flu
</figure>
<figureCaption confidence="0.704019">
(infect, prep with), (die, nsubj)
(contact, dobj), (import, prep from)
Figure 4: Examples of signature caseframes found
in Study 2.
</figureCaption>
<bodyText confidence="0.853446">
the source text.
</bodyText>
<subsectionHeader confidence="0.987444">
4.2 Study 2: Signature caseframe density
</subsectionHeader>
<bodyText confidence="0.99952295">
Study 1 shows that human summarizers are more
abstractive in that they aggregate information from
multiple sentences in the source text, but how is
this aggregation performed? One possibility is
that human summary writers are able to pack a
greater number of salient caseframes into their
summaries. That is, humans are fundamentally re-
lying on centrality just as automatic summarizers
do, and are simply able to achieve higher compres-
sion ratios by being more succinct. If this is true,
then sentence fusion methods over the source text
alone might be able to solve the problem. Unfor-
tunately, we show that this is false and that system
summaries are actually more central than model
ones.
To extract topical caseframes, we use Lin and
Hovy’s (2000) method of calculating signature
terms, but extend the method to apply it at the
caseframe rather than the word level. We fol-
low Lin and Hovy (2000) in using a significance
</bodyText>
<table confidence="0.997970333333333">
Condition Initial Update
Model average 0.065 0.052
Peer average 0.080* 0.072*
Peer 1 0.066 0.050
Peer 16 0.083* 0.085*
Peer 22 0.101* 0.084*
</table>
<tableCaption confidence="0.998372">
Table 3: Signature caseframe densities for differ-
</tableCaption>
<bodyText confidence="0.94886332">
ent sets of summarizers, for the initial and update
guided summarization tasks (Study 2). *: p &lt;
0.005.
threshold of 0.001 to determine signature case-
frames2. Figure 4 shows examples of signature
caseframes for several topics. Then, we calculate
the signature caseframe density of each of the
summarization systems. This is defined to be the
number of signature caseframes in the set of sum-
maries divided by the number of words in that set
of summaries.
Results Figure 3 shows the density for all of the
summarizers, in ascending order of density. As
can be seen, the human abstractors actually tend to
use fewer signature caseframes in their summaries
than automatic systems. Only the leading baseline
is indistinguishable from the model average. Ta-
ble 3 shows the densities for the conditions that
we described earlier. The differences in density
between the human average and the non-baseline
conditions are highly statistically significant, ac-
cording to paired two-tailed Wilcoxon signed-rank
tests for the statistic calculated for each topic clus-
ter.
These results show that human abstractors do
</bodyText>
<footnote confidence="0.991072">
2We tried various other thresholds, but the results were
much the same.
</footnote>
<page confidence="0.950455">
1238
</page>
<table confidence="0.999673285714286">
Threshold 0.9 0.8
Condition Init. Up. Init. Up.
Model average 0.066 0.052 0.062 0.047
Peer average 0.080 0.071 0.071 0.063
Peer 1 0.068 0.050 0.060 0.044
Peer 16 0.083 0.086 0.072 0.077
Peer 22 0.100 0.086 0.084 0.075
</table>
<tableCaption confidence="0.729129666666667">
Table 4: Density of signature caseframes after
merging to various threshold for the initial (Init.)
and update (Up.) summarization tasks (Study 2).
</tableCaption>
<bodyText confidence="0.999849166666667">
not merely repeat the caseframes that are indica-
tive of a topic cluster or use minor grammatical
alternations in their summaries. Rather, a genuine
sort of abstraction or distillation has taken place,
either through paraphrasing or semantic inference,
to transform the source text into the final informa-
tive summary.
Merging Caseframes We next investigate
whether simple paraphrasing could account for
the above results; it may be the case that human
summarizers simply replace words in the source
text with synonyms, which can be detected with
distributional similarity. Thus, we merged similar
caseframes into clusters according to the distribu-
tional semantic similarity defined in Section 3.1,
and then repeated the previous experiment. We
chose two relatively high levels of similarity (0.8
and 0.9), and used complete-link agglomerative
(i.e., bottom-up) clustering to merge similar
caseframes. That is, each caseframe begins as a
separate cluster, and the two most similar clusters
are merged at each step until the desired similarity
threshold is reached. Cluster similarity is defined
to be the minimum similarity (or equivalently,
maximum distance) between elements in the
two clusters; that is, maxC∈C1,C′∈C2 −sim(c, c′).
Complete-link agglomerative clustering tends to
form coherent clusters where the similarity be-
tween any pair within a cluster is high (Manning
et al., 2008).
Cluster Results Table 4 shows the results after
the clustering step, with similarity thresholds of
0.9 and 0.8. Once again, model summaries contain
a lower density of signature caseframes. The sta-
tistical significance results are unchanged. This in-
dicates that simple paraphrasing alone cannot ac-
count for the difference in the signature caseframe
densities, and that some deeper abstraction or se-
mantic inference has occurred.
Note that we are not claiming that a lower den-
sity of signature caseframes necessarily correlates
with a more informative summary. For example,
some automatic summarizers are comparable to
the human abstractors in their relatively low den-
sity of signature caseframes, but these turn out to
be the lowest performing summarization systems
by all measures in the workshop, and they are un-
likely to rival human abstractors in any reasonable
evaluation of summary informativeness. It does,
however, appear that further optimizing centrality-
based measures alone is unlikely to produce bet-
ter informative summaries, even if we analyze the
summary at a syntactic/semantic rather than lexi-
cal level.
</bodyText>
<subsectionHeader confidence="0.995433">
4.3 Study 3: Summary Reconstruction
</subsectionHeader>
<bodyText confidence="0.99135684375">
The above studies show that the higher degree
of abstraction in model summaries cannot be ex-
plained by better compression of topically salient
caseframes alone. We now switch perspectives to
ask how model summaries might be automatically
generated at all. We will show that they cannot
be reconstructed solely from the source text, ex-
tending Copeck and Szpakowicz (2004)’s result to
caseframes. However, we also show that if articles
from the same domain are added, reconstruction
then becomes possible. Our measure of whether
a model summary can be reconstructed is case-
frame coverage. We define this to be the propor-
tion of caseframes in a summary that is contained
by some reference set. This is thus a score be-
tween 0 and 1. Unlike in the previous study, we
use the full set of caseframes, not just signature
caseframes, because our goal now to create a hy-
pothesis space from which it is in principle possi-
ble to generate the model summaries.
Results We first calculated caseframe coverage
with respect to the source text alone (Figure 5).
As expected, automatic systems show close to per-
fect coverage, because of their basically extractive
nature, while model summaries show much lower
coverage. These statistics are summarized by Ta-
ble 5. These results present a fundamental limit
to extractive systems, and also text simplification
and sentence fusion methods based solely on the
source text.
The Impact of Domain Knowledge How might
automatic summarizers be able to acquire these
</bodyText>
<page confidence="0.984446">
1239
</page>
<figure confidence="0.998666285714286">
1.0
0.8
Coverage
0.6
0.4
0.2
0.0
1.0
0.8
Coverage
0.6
0.4
0.2
0.0
EBHFCD381723220639405 93414233519733411211372642212732428104 81316313025221151836
System IDs
GABEHCFD 2381 7321 1413920 19 215231 37 27 12 4 6337 83022 110241141 ?
1316 1 3 91618
System IDs
AG
(a) Initial guided summarization task (b) Update summarization task
</figure>
<figureCaption confidence="0.999935">
Figure 5: Coverage of summary text caseframes in source text (Study 3).
</figureCaption>
<table confidence="0.944982333333333">
Condition Initial Update
Model average 0.77 0.75
Peer average 0.99 0.99
Peer 1 1.00 1.00
Peer 16 1.00 1.00
Peer 22 1.00 1.00
</table>
<tableCaption confidence="0.916798">
Table 5: Coverage of caseframes in summaries
</tableCaption>
<bodyText confidence="0.998535222222222">
with respect to the source text. The model aver-
age is statistically significantly different from all
the other conditions p &lt; 10−8 (Study 3).
caseframes from other sources? Traditional sys-
tems that perform semantic inference do so from a
set of known facts about the domain in the form of
a knowledge base, but as we have seen, most ex-
tractive summarization systems do not make much
use of in-domain corpora. We examine adding
in-domain text to the source text to see how this
would affect coverage.
Recall that the 46 topics in TAC 2010 are cat-
egorized into five domains. To calculate the im-
pact of domain knowledge, we add all the docu-
ments that belong in the same domain to the source
text to calculate coverage. To ensure that coverage
does not increase simply due to increasing the size
of the reference set, we compare to the baseline of
adding the same number of documents that belong
to another domain. As shown in Table 6, the ef-
fect of adding more in-domain text on caseframe
coverage is substantial, and noticeably more than
using out-of-domain text. In fact, nearly all case-
frames can be found in the expanded set of arti-
cles. The implication of this result is that it may
be possible to generate better summaries by min-
ing in-domain text for relevant caseframes.
</bodyText>
<sectionHeader confidence="0.559151" genericHeader="method">
Reference corpus Initial Update
</sectionHeader>
<bodyText confidence="0.839804857142857">
Source text only 0.77 0.75
+out-of-domain 0.91 0.91
+in-domain 0.98 0.97
Table 6: The effect on caseframe coverage of
adding in-domain and out-of-domain documents.
The difference between adding in-domain and out-
of-domain text is significant p &lt; 10−3 (Study 3).
</bodyText>
<sectionHeader confidence="0.998824" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999902">
We have presented a series of studies to distin-
guish human-written informative summaries from
the summaries produced by current systems. Our
studies are performed at the level of caseframes,
which are able to characterize a domain in terms of
its slots. First, we confirm that model summaries
are more abstractive and aggregate information
from multiple source text sentences. Then, we
show that this is not simply due to summary writ-
ers packing together source text sentences contain-
ing topical caseframes to achieve a higher com-
pression ratio, even if paraphrasing is taken into
account. Indeed, model summaries cannot be re-
constructed from the source text alone. How-
ever, our results are also positive in that we find
that nearly all model summary caseframes can be
found in the source text together with some in-
domain documents.
Current summarization systems have been
heavily optimized towards centrality and lexical-
semantical reasoning, but we are nearing the bot-
tom of the barrel. Domain inference, on the other
hand, and a greater use of in-domain documents
as a knowledge source for domain inference, are
very promising indeed. Mining useful caseframes
</bodyText>
<page confidence="0.961843">
1240
</page>
<bodyText confidence="0.99995925">
for a sentence fusion-based approach has the po-
tential, as our experiments have shown, to deliver
results in just the areas where current approaches
are weakest.
</bodyText>
<sectionHeader confidence="0.990295" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9814155">
This work is supported by the Natural Sciences
and Engineering Research Council of Canada.
</bodyText>
<sectionHeader confidence="0.998442" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999454670000001">
Regina Barzilay and Kathleen R. McKeown. 2005.
Sentence fusion for multidocument news summa-
rization. Computational Linguistics, 31(3):297–
328.
David Bean and Ellen Riloff. 2004. Unsupervised
learning of contextual role knowledge for corefer-
ence resolution. In Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: HLT-NAACL 2004.
Jaime Carbonell and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering doc-
uments and producing summaries. In Proceedings
of the 21st Annual International ACM SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval, pages 335–336. ACM.
Nathanael Chambers and Dan Jurafsky. 2011.
Template-based information extraction without the
templates. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 976–
986, Portland, Oregon, USA, June. Association for
Computational Linguistics.
John M. Conroy, Judith D. Schlesinger, and Dianne P.
O’Leary. 2006. Topic-focused multi-document
summarization using an approximate oracle score.
In Proceedings of the COLING/ACL 2006 Main
Conference Poster Sessions, pages 152–159, Syd-
ney, Australia, July. Association for Computational
Linguistics.
Terry Copeck and Stan Szpakowicz. 2004. Vocabu-
lary agreement among model summaries and source
documents. In Proceedings of the 2004 Document
Understanding Conference (DUC).
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
In LREC 2006.
Charles Fillmore. 1968. The case for case. In E. Bach
and R. T. Harms, editors, Universals in Linguistic
Theory, pages 1–88. Holt, Reinhart, and Winston,
New York.
Charles J. Fillmore. 1982. Frame semantics. Linguis-
tics in the Morning Calm, pages 111–137.
Pierre-Etienne Genest, Guy Lapalme, and Mehdi
Yousfi-Monod. 2009. Hextac: the creation of a
manual extractive run. In Proceedings of the Second
Text Analysis Conference, Gaithersburg, Maryland,
USA. National Institute of Standards and Technol-
ogy.
David Graff, Junbo Kong, Ke Chen, and Kazuaki
Maeda. 2005. English gigaword second edition.
Linguistic Data Consortium, Philadelphia.
Liwei He, Elizabeth Sanocki, Anoop Gupta, and
Jonathan Grudin. 1999. Auto-summarization of
audio-video presentations. In Proceedings of the
Seventh ACM International Conference on Multime-
dia. ACM.
Liwei He, Elizabeth Sanocki, Anoop Gupta, and
Jonathan Grudin. 2000. Comparing presentation
summaries: slides vs. reading vs. listening. In Pro-
ceedings of the SIGCHI Conference on Human Fac-
tors in Computing Systems, CHI ’00, pages 177–
184, New York, NY, USA. ACM.
Eduard Hovy, Chin-Yew Lin, Liang Zhou, and Junichi
Fukumoto. 2006. Automated summarization evalu-
ation with Basic Elements. In Proceedings ofthe 5th
International Conference on Language Resources
and Evaluation (LREC), pages 899–902.
IBM. IBMILOG CPLEX Optimization Studio V12.4.
Hongyan Jing and Kathleen R. McKeown. 2000. Cut
and paste based text summarization. In Proceed-
ings of the 1st North American Chapter of the As-
sociationfor Computational Linguistics Conference,
pages 178–185.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization-step one: Sentence compres-
sion. In Proceedings of the National Conference on
Artificial Intelligence.
Chin-Yew Lin and Eduard Hovy. 2000. The auto-
mated acquisition of topic signatures for text sum-
marization. In Proceedings of the 18th Conference
on Computational Linguistics - Volume 1, COLING
’00, pages 495–501, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Chin-Yew Lin and Eduard Hovy. 2003. The potential
and limitations of automatic sentence extraction for
summarization. In Proceedings of the HLT-NAACL
03 on Text Summarization Workshop. Association
for Computational Linguistics.
Chin Y. Lin. 2004. ROUGE: A package for automatic
evaluation of summaries. In Stan Szpakowicz and
Marie-Francine Moens, editors, Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74–81, Barcelona, Spain, July. Associa-
tion for Computational Linguistics.
Annie Louis and Ani Nenkova. 2009. Automatically
evaluating content selection in summarization with-
out human models. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
</reference>
<page confidence="0.778877">
1241
</page>
<reference confidence="0.999801450980392">
Processing. Association for Computational Linguis-
tics.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch¨utze, 2008. Introduction to Information
Retrieval, chapter 17. Cambridge University Press.
Kathleen R. McKeown, Regina Barzilay, David Evans,
Vasileios Hatzivassiloglou, Judith L. Klavans, Ani
Nenkova, Carl Sable, Barry Schiffman, and Sergey
Sigelman. 2002. Tracking and summarizing news
on a daily basis with Columbia’s Newsblaster. In
Proceedings of the Second International Conference
on Human Language Technology Research, pages
280–285. Morgan Kaufmann Publishers Inc.
Ani Nenkova and Kathleen McKeown. 2003. Refer-
ences to named entities: a corpus study. In Com-
panion Volume of the Proceedings of HLT-NAACL
2003 - Short Papers. Association for Computational
Linguistics.
Ani Nenkova and Kathleen McKeown. 2011. Auto-
matic summarization. Foundations and Trends in
Information Retrieval, 5(2):103–233.
Ani Nenkova and Rebecca Passonneau. 2004. Evalu-
ating content selection in summarization: The pyra-
mid method. In Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: HLT-NAACL 2004, volume 2004, pages
145–152.
Dragomir R. Radev and Kathleen R. McKeown. 1998.
Generating natural language summaries from mul-
tiple on-line sources. Computational Linguistics,
24(3):470–500.
Horacio Saggion and Guy Lapalme. 2002. Generat-
ing indicative-informative summaries with SumUM.
Computational linguistics, 28(4):497–526.
Horacio Saggion, Juan-Manuel Torres-Moreno, Iria
Cunha, and Eric SanJuan. 2010. Multilingual sum-
marization evaluation without human models. In
Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, pages 1059–
1067. Association for Computational Linguistics.
Peter Turney. 2001. Mining the web for synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
the Twelth European Conference on Machine Learn-
ing (ECML-2001), pages 491–502.
Michael White, Tanya Korelsky, Claire Cardie, Vincent
Ng, David Pierce, and Kiri Wagstaff. 2001. Mul-
tidocument summarization via information extrac-
tion. In Proceedings of the First International Con-
ference on Human Language Technology Research.
Association for Computational Linguistics.
</reference>
<page confidence="0.993122">
1242
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.815368">
<title confidence="0.9977085">Towards Robust Abstractive Multi-Document Summarization: Caseframe Analysis of Centrality and Domain</title>
<author confidence="0.999368">Jackie Chi Kit</author>
<affiliation confidence="0.999157">University of</affiliation>
<address confidence="0.9678985">10 King’s College Rd., Room Toronto, ON, Canada M5S</address>
<email confidence="0.998998">jcheung@cs.toronto.edu</email>
<author confidence="0.956374">Gerald</author>
<affiliation confidence="0.999196">University of</affiliation>
<address confidence="0.9583025">10 King’s College Rd., Room Toronto, ON, Canada M5S</address>
<email confidence="0.99876">gpenn@cs.toronto.edu</email>
<abstract confidence="0.999528807692308">automatic summarization, the notion that a summary should contain the core parts of the source text. Current systems use centrality, along with redundancy avoidance and some sentence compression, to produce mostly extractive summaries. In this paper, we investigate how summarization can advance past this paradigm towards robust abstraction by making greater use of the domain of the source text. We conduct a series of studies comparing human-written model summaries to system summaries at the selevel of We show that model summaries (1) are more abstractive and make use of more sentence aggregation, (2) do not contain as many topical caseframes as system summaries, and (3) cannot be reconstructed solely from the source text, but can be if texts from in-domain documents are added. These results suggest that substantial improvements are unlikely to result from better optimizing centrality-based criteria, but rather more domain knowledge is needed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Sentence fusion for multidocument news summarization.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>3</issue>
<pages>328</pages>
<contexts>
<context position="2748" citStr="Barzilay and McKeown, 2005" startWordPosition="431" endWordPosition="434">re term method of Lin and Hovy (2000) pick out salient terms that occur more often than would be expected in the source text based on frequencies in a background corpus. This method is a core component of the most successful summarization methods (Conroy et al., 2006). While extractive methods based on centrality have thus achieved success, there has long been recognition that abstractive methods are ultimately more desirable. One line of work is in text simplification and sentence fusion, which focus on the ability of abstraction to achieve a higher compression ratio (Knight and Marcu, 2000; Barzilay and McKeown, 2005). A less examined issue is that of aggregation and information synthesis. A key part of the usefulness of summaries is that they provide some synthesis or analysis of the source text and make a more general statement that is of direct relevance to the user. For example, a series of related events can be aggregated and expressed as a trend. The position of this paper is that centrality is not enough to make substantial progress towards abstractive summarization that is capable of this type of semantic inference. Instead, summarization systems need to make more use of domain knowledge. We provid</context>
</contexts>
<marker>Barzilay, McKeown, 2005</marker>
<rawString>Regina Barzilay and Kathleen R. McKeown. 2005. Sentence fusion for multidocument news summarization. Computational Linguistics, 31(3):297– 328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Bean</author>
<author>Ellen Riloff</author>
</authors>
<title>Unsupervised learning of contextual role knowledge for coreference resolution.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL</booktitle>
<contexts>
<context position="10941" citStr="Bean and Riloff, 2004" startWordPosition="1727" endWordPosition="1730">2 Causal (kill, dobj) (die, nsubj) 0.80 Type (rise, dobj) (drop, prep to) 0.81 Figure 1: Sample pairs of similar caseframes by relation type, and the similarity score assigned to them by our distributional model. In particular, they are (gov, role) pairs, where gov is a proposition-bearing element, and role is an approximation of a semantic role with gov as its head (See Figure 1 for examples). Caseframes do not consider the dependents of the semantic role approximations. The use of caseframes is well grounded in a variety of NLP tasks relevant to summarization such as coreference resolution (Bean and Riloff, 2004), and information extraction (Chambers and Jurafsky, 2011), where they serve the central unit of semantic analysis. Related semantic representations are popular in Case Grammar and its derivative formalisms such as frame semantics (Fillmore, 1982). We use the following algorithm to extract caseframes from dependency parses. First, we extract those dependency edges with a relation type of subject, direct object, indirect object, or prepositional object (with the preposition indicated), along with their governors. The governor must be a verb, event noun (as defined by the hyponyms of the WordNet</context>
</contexts>
<marker>Bean, Riloff, 2004</marker>
<rawString>David Bean and Ellen Riloff. 2004. Unsupervised learning of contextual role knowledge for coreference resolution. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaime Carbonell</author>
<author>Jade Goldstein</author>
</authors>
<title>The use of MMR, diversity-based reranking for reordering documents and producing summaries.</title>
<date>1998</date>
<booktitle>In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>335--336</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1761" citStr="Carbonell and Goldstein (1998)" startWordPosition="271" endWordPosition="274"> be if texts from in-domain documents are added. These results suggest that substantial improvements are unlikely to result from better optimizing centrality-based criteria, but rather more domain knowledge is needed. 1 Introduction In automatic summarization, centrality has been one of the guiding principles for content selection in extractive systems. We define centrality to be the idea that a summary should contain the parts of the source text that are most similar or representative of the source text. This is most transparently illustrated by the Maximal Marginal Relevance (MMR) system of Carbonell and Goldstein (1998), which defines the summarization objective to be a linear combination of a centrality term and a non-redundancy term. Since MMR, much progress has been made on more sophisticated methods of measuring centrality and integrating it with non-redundancy (See Nenkova and McKeown (2011) for a recent survey). For example, term weighting methods such as the signature term method of Lin and Hovy (2000) pick out salient terms that occur more often than would be expected in the source text based on frequencies in a background corpus. This method is a core component of the most successful summarization m</context>
</contexts>
<marker>Carbonell, Goldstein, 1998</marker>
<rawString>Jaime Carbonell and Jade Goldstein. 1998. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 335–336. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Template-based information extraction without the templates.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>976--986</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="10999" citStr="Chambers and Jurafsky, 2011" startWordPosition="1734" endWordPosition="1738">dobj) (drop, prep to) 0.81 Figure 1: Sample pairs of similar caseframes by relation type, and the similarity score assigned to them by our distributional model. In particular, they are (gov, role) pairs, where gov is a proposition-bearing element, and role is an approximation of a semantic role with gov as its head (See Figure 1 for examples). Caseframes do not consider the dependents of the semantic role approximations. The use of caseframes is well grounded in a variety of NLP tasks relevant to summarization such as coreference resolution (Bean and Riloff, 2004), and information extraction (Chambers and Jurafsky, 2011), where they serve the central unit of semantic analysis. Related semantic representations are popular in Case Grammar and its derivative formalisms such as frame semantics (Fillmore, 1982). We use the following algorithm to extract caseframes from dependency parses. First, we extract those dependency edges with a relation type of subject, direct object, indirect object, or prepositional object (with the preposition indicated), along with their governors. The governor must be a verb, event noun (as defined by the hyponyms of the WordNet EVENT synset), or nominal or adjectival predicate. Then, </context>
</contexts>
<marker>Chambers, Jurafsky, 2011</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2011. Template-based information extraction without the templates. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 976– 986, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Conroy</author>
<author>Judith D Schlesinger</author>
<author>Dianne P O’Leary</author>
</authors>
<title>Topic-focused multi-document summarization using an approximate oracle score.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions,</booktitle>
<pages>152--159</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<marker>Conroy, Schlesinger, O’Leary, 2006</marker>
<rawString>John M. Conroy, Judith D. Schlesinger, and Dianne P. O’Leary. 2006. Topic-focused multi-document summarization using an approximate oracle score. In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 152–159, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Copeck</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Vocabulary agreement among model summaries and source documents.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Document Understanding Conference (DUC).</booktitle>
<contexts>
<context position="8145" citStr="Copeck and Szpakowicz (2004)" startWordPosition="1282" endWordPosition="1285">the lecture domain, He et al. (1999; 2000) find that lecture transcripts that have been manually highlighted with key points improve students’ quiz scores more than when using automated summarization techniques or when providing only the lecture transcript or slides. Jing and McKeown (2000) manually analyzed 30 human-written summaries, and find that 19% of sentences cannot be explained by cut-and-paste operations from the source text. Saggion and Lapalme (2002) similarly define a list of transformations necessary to convert source text to summary text, and manually analyzed their frequencies. Copeck and Szpakowicz (2004) find that at most 55% of vocabulary items found in model summaries occur in the source text, but they do not investigate where the other vocabulary items might be found. 1234 Sentence: At one point, two bomb squad trucks sped to the school after a backpack scare. Dependencies: num(point, one) prep at(sped, point) num(trucks, two) nn(trucks, bomb) nn(trucks, squad) nsubj(sped, trucks) root(ROOT, sped) det(school, the) prep to(sped, school) det(scare, a) nn(scare, backpack) prep after(sped, scare) Caseframes: (speed, prep at) (speed, nsubj) (speed, prep to) (speed, prep after) Table 1: A senten</context>
<context position="26468" citStr="Copeck and Szpakowicz (2004)" startWordPosition="4219" endWordPosition="4222">eness. It does, however, appear that further optimizing centralitybased measures alone is unlikely to produce better informative summaries, even if we analyze the summary at a syntactic/semantic rather than lexical level. 4.3 Study 3: Summary Reconstruction The above studies show that the higher degree of abstraction in model summaries cannot be explained by better compression of topically salient caseframes alone. We now switch perspectives to ask how model summaries might be automatically generated at all. We will show that they cannot be reconstructed solely from the source text, extending Copeck and Szpakowicz (2004)’s result to caseframes. However, we also show that if articles from the same domain are added, reconstruction then becomes possible. Our measure of whether a model summary can be reconstructed is caseframe coverage. We define this to be the proportion of caseframes in a summary that is contained by some reference set. This is thus a score between 0 and 1. Unlike in the previous study, we use the full set of caseframes, not just signature caseframes, because our goal now to create a hypothesis space from which it is in principle possible to generate the model summaries. Results We first calcul</context>
</contexts>
<marker>Copeck, Szpakowicz, 2004</marker>
<rawString>Terry Copeck and Stan Szpakowicz. 2004. Vocabulary agreement among model summaries and source documents. In Proceedings of the 2004 Document Understanding Conference (DUC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses. In</title>
<date>2006</date>
<booktitle>In LREC</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In In LREC 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Fillmore</author>
</authors>
<title>The case for case.</title>
<date>1968</date>
<booktitle>Universals in Linguistic Theory,</booktitle>
<pages>1--88</pages>
<editor>In E. Bach and R. T. Harms, editors,</editor>
<location>Holt, Reinhart, and Winston, New York.</location>
<contexts>
<context position="12303" citStr="Fillmore, 1968" startWordPosition="1943" endWordPosition="1944">nt for voicing alternations, control, raising, and copular constructions. 3.1 Caseframe Similarity Direct caseframe matches account for some variation in the expression of slots, such as voicing alternations, but there are other reasons different caseframes may indicate the same slot (Figure 1). For example, (kill, dobj) and (wound, dobj) both indicate the victim of an attack, but differ by the degree of injury to the victim. (kill, dobj) and (die, nsubj) also refer to a victim, but are linked by a causal relation. (rise, dobj) and inspired by) the similarly named case frames of Case Grammar (Fillmore, 1968). 1235 (drop, prep to) on the other hand simply share a named entity type (in this case, numbers). To account for these issues, we measure caseframe similarity based on their distributional similarity in a large training corpus. First, we construct vector representations of each caseframe, where the dimensions of the vector correspond to the lemma of the head word that fills the caseframe in the training corpus. For example, kicked the ball would result in a count of 1 added to the caseframe (kick, dobj) for the context word ball. Then, we rescale the counts into pointwise mutual information v</context>
</contexts>
<marker>Fillmore, 1968</marker>
<rawString>Charles Fillmore. 1968. The case for case. In E. Bach and R. T. Harms, editors, Universals in Linguistic Theory, pages 1–88. Holt, Reinhart, and Winston, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles J Fillmore</author>
</authors>
<title>Frame semantics. Linguistics in the Morning Calm,</title>
<date>1982</date>
<pages>111--137</pages>
<contexts>
<context position="11188" citStr="Fillmore, 1982" startWordPosition="1765" endWordPosition="1766">, where gov is a proposition-bearing element, and role is an approximation of a semantic role with gov as its head (See Figure 1 for examples). Caseframes do not consider the dependents of the semantic role approximations. The use of caseframes is well grounded in a variety of NLP tasks relevant to summarization such as coreference resolution (Bean and Riloff, 2004), and information extraction (Chambers and Jurafsky, 2011), where they serve the central unit of semantic analysis. Related semantic representations are popular in Case Grammar and its derivative formalisms such as frame semantics (Fillmore, 1982). We use the following algorithm to extract caseframes from dependency parses. First, we extract those dependency edges with a relation type of subject, direct object, indirect object, or prepositional object (with the preposition indicated), along with their governors. The governor must be a verb, event noun (as defined by the hyponyms of the WordNet EVENT synset), or nominal or adjectival predicate. Then, a series of deterministic transformations are applied to the syntactic relations to account for voicing alternations, control, raising, and copular constructions. 3.1 Caseframe Similarity D</context>
</contexts>
<marker>Fillmore, 1982</marker>
<rawString>Charles J. Fillmore. 1982. Frame semantics. Linguistics in the Morning Calm, pages 111–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre-Etienne Genest</author>
<author>Guy Lapalme</author>
<author>Mehdi Yousfi-Monod</author>
</authors>
<title>Hextac: the creation of a manual extractive run.</title>
<date>2009</date>
<booktitle>In Proceedings of the Second Text Analysis Conference,</booktitle>
<institution>National Institute of Standards and Technology.</institution>
<location>Gaithersburg, Maryland, USA.</location>
<contexts>
<context position="7322" citStr="Genest et al. (2009)" startWordPosition="1155" endWordPosition="1158">d (Nenkova and Passonneau, 2004). There are also automatic measures which do not require model summaries, but compare against the source text instead (Louis and Nenkova, 2009; Saggion et al., 2010). Several studies complement this paper by examining the best possible extractive system using current evaluation measures, such as ROUGE (Lin and Hovy, 2003; Conroy et al., 2006). They find that the best possible extractive systems score higher or as highly than human summarizers, but it is unclear whether this means the oracle summaries are actually as useful as human ones in an extrinsic setting. Genest et al. (2009) ask humans to create extractive summaries, and find that they score in between current automatic systems and human-written abstracts on responsiveness, linguistic quality, and Pyramid score. In the lecture domain, He et al. (1999; 2000) find that lecture transcripts that have been manually highlighted with key points improve students’ quiz scores more than when using automated summarization techniques or when providing only the lecture transcript or slides. Jing and McKeown (2000) manually analyzed 30 human-written summaries, and find that 19% of sentences cannot be explained by cut-and-paste</context>
</contexts>
<marker>Genest, Lapalme, Yousfi-Monod, 2009</marker>
<rawString>Pierre-Etienne Genest, Guy Lapalme, and Mehdi Yousfi-Monod. 2009. Hextac: the creation of a manual extractive run. In Proceedings of the Second Text Analysis Conference, Gaithersburg, Maryland, USA. National Institute of Standards and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
<author>Junbo Kong</author>
<author>Ke Chen</author>
<author>Kazuaki Maeda</author>
</authors>
<title>English gigaword second edition. Linguistic Data Consortium,</title>
<date>2005</date>
<location>Philadelphia.</location>
<contexts>
<context position="13212" citStr="Graff et al., 2005" startWordPosition="2094" endWordPosition="2097"> where the dimensions of the vector correspond to the lemma of the head word that fills the caseframe in the training corpus. For example, kicked the ball would result in a count of 1 added to the caseframe (kick, dobj) for the context word ball. Then, we rescale the counts into pointwise mutual information values, which has been shown to be more effective than raw counts at detecting semantic relatedness (Turney, 2001). Similarity between caseframes can then be compared by cosine similarity between the their vector representations. For training, we use the AFP portion of the Gigaword corpus (Graff et al., 2005), which we parsed using the Stanford parser’s typed dependency tree representation with collapsed conjunctions (de Marneffe et al., 2006). For reasons of sparsity, we only considered caseframes that appear at least five times in the guided summarization corpus, and only the 3000 most common lemmata in Gigaword as context words. 3.2 An Example To illustrate how caseframes indicate the slots in a summary, we provide the following fragment of a model summary from TAC about the Unabomber trial: (1) In Sacramento, Theodore Kaczynski faces a 10-count federal indictment for 4 of the 16 mail bomb atta</context>
</contexts>
<marker>Graff, Kong, Chen, Maeda, 2005</marker>
<rawString>David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2005. English gigaword second edition. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liwei He</author>
<author>Elizabeth Sanocki</author>
<author>Anoop Gupta</author>
<author>Jonathan Grudin</author>
</authors>
<title>Auto-summarization of audio-video presentations.</title>
<date>1999</date>
<booktitle>In Proceedings of the Seventh ACM International Conference on Multimedia.</booktitle>
<publisher>ACM.</publisher>
<contexts>
<context position="7552" citStr="He et al. (1999" startWordPosition="1191" endWordPosition="1194">r by examining the best possible extractive system using current evaluation measures, such as ROUGE (Lin and Hovy, 2003; Conroy et al., 2006). They find that the best possible extractive systems score higher or as highly than human summarizers, but it is unclear whether this means the oracle summaries are actually as useful as human ones in an extrinsic setting. Genest et al. (2009) ask humans to create extractive summaries, and find that they score in between current automatic systems and human-written abstracts on responsiveness, linguistic quality, and Pyramid score. In the lecture domain, He et al. (1999; 2000) find that lecture transcripts that have been manually highlighted with key points improve students’ quiz scores more than when using automated summarization techniques or when providing only the lecture transcript or slides. Jing and McKeown (2000) manually analyzed 30 human-written summaries, and find that 19% of sentences cannot be explained by cut-and-paste operations from the source text. Saggion and Lapalme (2002) similarly define a list of transformations necessary to convert source text to summary text, and manually analyzed their frequencies. Copeck and Szpakowicz (2004) find t</context>
</contexts>
<marker>He, Sanocki, Gupta, Grudin, 1999</marker>
<rawString>Liwei He, Elizabeth Sanocki, Anoop Gupta, and Jonathan Grudin. 1999. Auto-summarization of audio-video presentations. In Proceedings of the Seventh ACM International Conference on Multimedia. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liwei He</author>
<author>Elizabeth Sanocki</author>
<author>Anoop Gupta</author>
<author>Jonathan Grudin</author>
</authors>
<title>Comparing presentation summaries: slides vs. reading vs. listening.</title>
<date>2000</date>
<booktitle>In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’00,</booktitle>
<pages>177--184</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>He, Sanocki, Gupta, Grudin, 2000</marker>
<rawString>Liwei He, Elizabeth Sanocki, Anoop Gupta, and Jonathan Grudin. 2000. Comparing presentation summaries: slides vs. reading vs. listening. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’00, pages 177– 184, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Chin-Yew Lin</author>
<author>Liang Zhou</author>
<author>Junichi Fukumoto</author>
</authors>
<title>Automated summarization evaluation with Basic Elements.</title>
<date>2006</date>
<booktitle>In Proceedings ofthe 5th International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>899--902</pages>
<contexts>
<context position="6635" citStr="Hovy et al., 2006" startWordPosition="1040" endWordPosition="1043">y the source text cluster (Conroy et al., 2006), thus they are now used as a form of centrality-based features. In this paper, we use guided summarization data as an opportunity to reopen the investigation into the effect of domain, because multiple document clusters from the same domain are available. Summarization evaluation is typically done by comparing system output to human-written model summaries, and are validated by their correlation with user responsiveness judgments. The comparison can be done at the word level, as in ROUGE (Lin, 2004), at the syntactic level, as in Basic Elements (Hovy et al., 2006), or at the level of summary content units, as in the Pyramid method (Nenkova and Passonneau, 2004). There are also automatic measures which do not require model summaries, but compare against the source text instead (Louis and Nenkova, 2009; Saggion et al., 2010). Several studies complement this paper by examining the best possible extractive system using current evaluation measures, such as ROUGE (Lin and Hovy, 2003; Conroy et al., 2006). They find that the best possible extractive systems score higher or as highly than human summarizers, but it is unclear whether this means the oracle summa</context>
</contexts>
<marker>Hovy, Lin, Zhou, Fukumoto, 2006</marker>
<rawString>Eduard Hovy, Chin-Yew Lin, Liang Zhou, and Junichi Fukumoto. 2006. Automated summarization evaluation with Basic Elements. In Proceedings ofthe 5th International Conference on Language Resources and Evaluation (LREC), pages 899–902. IBM. IBMILOG CPLEX Optimization Studio V12.4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Cut and paste based text summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st North American Chapter of the Associationfor Computational Linguistics Conference,</booktitle>
<pages>178--185</pages>
<contexts>
<context position="7808" citStr="Jing and McKeown (2000)" startWordPosition="1230" endWordPosition="1233"> it is unclear whether this means the oracle summaries are actually as useful as human ones in an extrinsic setting. Genest et al. (2009) ask humans to create extractive summaries, and find that they score in between current automatic systems and human-written abstracts on responsiveness, linguistic quality, and Pyramid score. In the lecture domain, He et al. (1999; 2000) find that lecture transcripts that have been manually highlighted with key points improve students’ quiz scores more than when using automated summarization techniques or when providing only the lecture transcript or slides. Jing and McKeown (2000) manually analyzed 30 human-written summaries, and find that 19% of sentences cannot be explained by cut-and-paste operations from the source text. Saggion and Lapalme (2002) similarly define a list of transformations necessary to convert source text to summary text, and manually analyzed their frequencies. Copeck and Szpakowicz (2004) find that at most 55% of vocabulary items found in model summaries occur in the source text, but they do not investigate where the other vocabulary items might be found. 1234 Sentence: At one point, two bomb squad trucks sped to the school after a backpack scare</context>
</contexts>
<marker>Jing, McKeown, 2000</marker>
<rawString>Hongyan Jing and Kathleen R. McKeown. 2000. Cut and paste based text summarization. In Proceedings of the 1st North American Chapter of the Associationfor Computational Linguistics Conference, pages 178–185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Statisticsbased summarization-step one: Sentence compression.</title>
<date>2000</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="2719" citStr="Knight and Marcu, 2000" startWordPosition="427" endWordPosition="430">hods such as the signature term method of Lin and Hovy (2000) pick out salient terms that occur more often than would be expected in the source text based on frequencies in a background corpus. This method is a core component of the most successful summarization methods (Conroy et al., 2006). While extractive methods based on centrality have thus achieved success, there has long been recognition that abstractive methods are ultimately more desirable. One line of work is in text simplification and sentence fusion, which focus on the ability of abstraction to achieve a higher compression ratio (Knight and Marcu, 2000; Barzilay and McKeown, 2005). A less examined issue is that of aggregation and information synthesis. A key part of the usefulness of summaries is that they provide some synthesis or analysis of the source text and make a more general statement that is of direct relevance to the user. For example, a series of related events can be aggregated and expressed as a trend. The position of this paper is that centrality is not enough to make substantial progress towards abstractive summarization that is capable of this type of semantic inference. Instead, summarization systems need to make more use o</context>
</contexts>
<marker>Knight, Marcu, 2000</marker>
<rawString>Kevin Knight and Daniel Marcu. 2000. Statisticsbased summarization-step one: Sentence compression. In Proceedings of the National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>The automated acquisition of topic signatures for text summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th Conference on Computational Linguistics - Volume 1, COLING ’00,</booktitle>
<pages>495--501</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2158" citStr="Lin and Hovy (2000)" startWordPosition="335" endWordPosition="338">hould contain the parts of the source text that are most similar or representative of the source text. This is most transparently illustrated by the Maximal Marginal Relevance (MMR) system of Carbonell and Goldstein (1998), which defines the summarization objective to be a linear combination of a centrality term and a non-redundancy term. Since MMR, much progress has been made on more sophisticated methods of measuring centrality and integrating it with non-redundancy (See Nenkova and McKeown (2011) for a recent survey). For example, term weighting methods such as the signature term method of Lin and Hovy (2000) pick out salient terms that occur more often than would be expected in the source text based on frequencies in a background corpus. This method is a core component of the most successful summarization methods (Conroy et al., 2006). While extractive methods based on centrality have thus achieved success, there has long been recognition that abstractive methods are ultimately more desirable. One line of work is in text simplification and sentence fusion, which focus on the ability of abstraction to achieve a higher compression ratio (Knight and Marcu, 2000; Barzilay and McKeown, 2005). A less e</context>
<context position="21794" citStr="Lin and Hovy (2000)" startWordPosition="3493" endWordPosition="3496">nto their summaries. That is, humans are fundamentally relying on centrality just as automatic summarizers do, and are simply able to achieve higher compression ratios by being more succinct. If this is true, then sentence fusion methods over the source text alone might be able to solve the problem. Unfortunately, we show that this is false and that system summaries are actually more central than model ones. To extract topical caseframes, we use Lin and Hovy’s (2000) method of calculating signature terms, but extend the method to apply it at the caseframe rather than the word level. We follow Lin and Hovy (2000) in using a significance Condition Initial Update Model average 0.065 0.052 Peer average 0.080* 0.072* Peer 1 0.066 0.050 Peer 16 0.083* 0.085* Peer 22 0.101* 0.084* Table 3: Signature caseframe densities for different sets of summarizers, for the initial and update guided summarization tasks (Study 2). *: p &lt; 0.005. threshold of 0.001 to determine signature caseframes2. Figure 4 shows examples of signature caseframes for several topics. Then, we calculate the signature caseframe density of each of the summarization systems. This is defined to be the number of signature caseframes in the set o</context>
</contexts>
<marker>Lin, Hovy, 2000</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2000. The automated acquisition of topic signatures for text summarization. In Proceedings of the 18th Conference on Computational Linguistics - Volume 1, COLING ’00, pages 495–501, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>The potential and limitations of automatic sentence extraction for summarization.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT-NAACL 03 on Text Summarization Workshop. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7056" citStr="Lin and Hovy, 2003" startWordPosition="1109" endWordPosition="1112">ated by their correlation with user responsiveness judgments. The comparison can be done at the word level, as in ROUGE (Lin, 2004), at the syntactic level, as in Basic Elements (Hovy et al., 2006), or at the level of summary content units, as in the Pyramid method (Nenkova and Passonneau, 2004). There are also automatic measures which do not require model summaries, but compare against the source text instead (Louis and Nenkova, 2009; Saggion et al., 2010). Several studies complement this paper by examining the best possible extractive system using current evaluation measures, such as ROUGE (Lin and Hovy, 2003; Conroy et al., 2006). They find that the best possible extractive systems score higher or as highly than human summarizers, but it is unclear whether this means the oracle summaries are actually as useful as human ones in an extrinsic setting. Genest et al. (2009) ask humans to create extractive summaries, and find that they score in between current automatic systems and human-written abstracts on responsiveness, linguistic quality, and Pyramid score. In the lecture domain, He et al. (1999; 2000) find that lecture transcripts that have been manually highlighted with key points improve studen</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2003. The potential and limitations of automatic sentence extraction for summarization. In Proceedings of the HLT-NAACL 03 on Text Summarization Workshop. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin Y Lin</author>
</authors>
<title>ROUGE: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Stan Szpakowicz and Marie-Francine Moens, editors, Text Summarization Branches Out: Proceedings of the ACL-04 Workshop,</booktitle>
<pages>74--81</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="6569" citStr="Lin, 2004" startWordPosition="1030" endWordPosition="1031">ument summarization, the indomain text has been replaced by the source text cluster (Conroy et al., 2006), thus they are now used as a form of centrality-based features. In this paper, we use guided summarization data as an opportunity to reopen the investigation into the effect of domain, because multiple document clusters from the same domain are available. Summarization evaluation is typically done by comparing system output to human-written model summaries, and are validated by their correlation with user responsiveness judgments. The comparison can be done at the word level, as in ROUGE (Lin, 2004), at the syntactic level, as in Basic Elements (Hovy et al., 2006), or at the level of summary content units, as in the Pyramid method (Nenkova and Passonneau, 2004). There are also automatic measures which do not require model summaries, but compare against the source text instead (Louis and Nenkova, 2009; Saggion et al., 2010). Several studies complement this paper by examining the best possible extractive system using current evaluation measures, such as ROUGE (Lin and Hovy, 2003; Conroy et al., 2006). They find that the best possible extractive systems score higher or as highly than human </context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin Y. Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Stan Szpakowicz and Marie-Francine Moens, editors, Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74–81, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annie Louis</author>
<author>Ani Nenkova</author>
</authors>
<title>Automatically evaluating content selection in summarization without human models.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6876" citStr="Louis and Nenkova, 2009" startWordPosition="1080" endWordPosition="1083">ause multiple document clusters from the same domain are available. Summarization evaluation is typically done by comparing system output to human-written model summaries, and are validated by their correlation with user responsiveness judgments. The comparison can be done at the word level, as in ROUGE (Lin, 2004), at the syntactic level, as in Basic Elements (Hovy et al., 2006), or at the level of summary content units, as in the Pyramid method (Nenkova and Passonneau, 2004). There are also automatic measures which do not require model summaries, but compare against the source text instead (Louis and Nenkova, 2009; Saggion et al., 2010). Several studies complement this paper by examining the best possible extractive system using current evaluation measures, such as ROUGE (Lin and Hovy, 2003; Conroy et al., 2006). They find that the best possible extractive systems score higher or as highly than human summarizers, but it is unclear whether this means the oracle summaries are actually as useful as human ones in an extrinsic setting. Genest et al. (2009) ask humans to create extractive summaries, and find that they score in between current automatic systems and human-written abstracts on responsiveness, l</context>
</contexts>
<marker>Louis, Nenkova, 2009</marker>
<rawString>Annie Louis and Ani Nenkova. 2009. Automatically evaluating content selection in summarization without human models. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Introduction to Information Retrieval, chapter 17.</title>
<date>2008</date>
<publisher>Cambridge University Press.</publisher>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch¨utze, 2008. Introduction to Information Retrieval, chapter 17. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen R McKeown</author>
<author>Regina Barzilay</author>
<author>David Evans</author>
<author>Vasileios Hatzivassiloglou</author>
<author>Judith L Klavans</author>
<author>Ani Nenkova</author>
<author>Carl Sable</author>
<author>Barry Schiffman</author>
<author>Sergey Sigelman</author>
</authors>
<title>Tracking and summarizing news on a daily basis with Columbia’s Newsblaster.</title>
<date>2002</date>
<booktitle>In Proceedings of the Second International Conference on Human Language Technology Research,</booktitle>
<pages>280--285</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<contexts>
<context position="5356" citStr="McKeown et al., 2002" startWordPosition="839" endWordPosition="842">s not to develop a new evaluation measure as defined by correlation with human responsiveness judgments. Instead, our studies reveal useful criteria with which to distinguish human-written and system summaries, helping to guide the development of future summarization systems. 2 Related Work Domain-dependent template-based summarization systems have been an alternative to extractive systems which make use of rich knowledge about a domain and information extraction techniques to generate a summary, possibly using a natural language generation system (Radev and McKeown, 1998; White et al., 2001; McKeown et al., 2002). This paper can be seen as a first step towards reconciling the advantages of domain knowledge with the resource-lean extraction approaches popular today. As noted above, Lin and Hovy’s (2000) signature terms have been successful in discovering terms that are specific to the source text. These terms are identified by a log-likelihood ratio test based on their relative frequencies in relevant and irrelevant documents. They were originally proposed in the context of single-document summarization, where they were calculated using indomain (relevant) vs. out-of-domain (irrelevant) text. In multi-</context>
</contexts>
<marker>McKeown, Barzilay, Evans, Hatzivassiloglou, Klavans, Nenkova, Sable, Schiffman, Sigelman, 2002</marker>
<rawString>Kathleen R. McKeown, Regina Barzilay, David Evans, Vasileios Hatzivassiloglou, Judith L. Klavans, Ani Nenkova, Carl Sable, Barry Schiffman, and Sergey Sigelman. 2002. Tracking and summarizing news on a daily basis with Columbia’s Newsblaster. In Proceedings of the Second International Conference on Human Language Technology Research, pages 280–285. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Kathleen McKeown</author>
</authors>
<title>References to named entities: a corpus study.</title>
<date>2003</date>
<booktitle>In Companion Volume of the Proceedings of HLT-NAACL</booktitle>
<contexts>
<context position="9636" citStr="Nenkova and McKeown, 2003" startWordPosition="1516" endWordPosition="1520">ppropriate for our analysis. Word overlap can occur due to shared proper nouns or entity mentions. Good summaries should certainly contain the salient entities in the source text, but when assessing the effect of the domain, different domain instances (i.e., different document clusters in the same domain) would be expected to contain different salient entities. Also, the realization of entities as noun phrases depends strongly on context, which would confound our analysis if we do not also correctly resolve coreference, a difficult problem in its own right. We leave such issues to other work (Nenkova and McKeown, 2003, e.g.). Domains would rather be expected to share slots (a.k.a. aspects), which require a more semantic level of analysis that can account for the various ways in which a particular slot can be expressed. Another consideration is that the structures to be analyzed should be extracted automatically. Based on these criteria, we selected caseframes to be the appropriate unit of analysis. A caseframe is a shallow approximation of the semantic role structure of a proposition-bearing unit like a verb, and are derived from the dependency parse of a sentence1. 1Note that caseframes are distinct from </context>
</contexts>
<marker>Nenkova, McKeown, 2003</marker>
<rawString>Ani Nenkova and Kathleen McKeown. 2003. References to named entities: a corpus study. In Companion Volume of the Proceedings of HLT-NAACL 2003 - Short Papers. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Kathleen McKeown</author>
</authors>
<date>2011</date>
<booktitle>Automatic summarization. Foundations and Trends in Information Retrieval,</booktitle>
<volume>5</volume>
<issue>2</issue>
<contexts>
<context position="2043" citStr="Nenkova and McKeown (2011)" startWordPosition="314" endWordPosition="317">f the guiding principles for content selection in extractive systems. We define centrality to be the idea that a summary should contain the parts of the source text that are most similar or representative of the source text. This is most transparently illustrated by the Maximal Marginal Relevance (MMR) system of Carbonell and Goldstein (1998), which defines the summarization objective to be a linear combination of a centrality term and a non-redundancy term. Since MMR, much progress has been made on more sophisticated methods of measuring centrality and integrating it with non-redundancy (See Nenkova and McKeown (2011) for a recent survey). For example, term weighting methods such as the signature term method of Lin and Hovy (2000) pick out salient terms that occur more often than would be expected in the source text based on frequencies in a background corpus. This method is a core component of the most successful summarization methods (Conroy et al., 2006). While extractive methods based on centrality have thus achieved success, there has long been recognition that abstractive methods are ultimately more desirable. One line of work is in text simplification and sentence fusion, which focus on the ability </context>
</contexts>
<marker>Nenkova, McKeown, 2011</marker>
<rawString>Ani Nenkova and Kathleen McKeown. 2011. Automatic summarization. Foundations and Trends in Information Retrieval, 5(2):103–233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Rebecca Passonneau</author>
</authors>
<title>Evaluating content selection in summarization: The pyramid method.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL</booktitle>
<volume>volume</volume>
<pages>145--152</pages>
<contexts>
<context position="6734" citStr="Nenkova and Passonneau, 2004" startWordPosition="1057" endWordPosition="1060">trality-based features. In this paper, we use guided summarization data as an opportunity to reopen the investigation into the effect of domain, because multiple document clusters from the same domain are available. Summarization evaluation is typically done by comparing system output to human-written model summaries, and are validated by their correlation with user responsiveness judgments. The comparison can be done at the word level, as in ROUGE (Lin, 2004), at the syntactic level, as in Basic Elements (Hovy et al., 2006), or at the level of summary content units, as in the Pyramid method (Nenkova and Passonneau, 2004). There are also automatic measures which do not require model summaries, but compare against the source text instead (Louis and Nenkova, 2009; Saggion et al., 2010). Several studies complement this paper by examining the best possible extractive system using current evaluation measures, such as ROUGE (Lin and Hovy, 2003; Conroy et al., 2006). They find that the best possible extractive systems score higher or as highly than human summarizers, but it is unclear whether this means the oracle summaries are actually as useful as human ones in an extrinsic setting. Genest et al. (2009) ask humans </context>
</contexts>
<marker>Nenkova, Passonneau, 2004</marker>
<rawString>Ani Nenkova and Rebecca Passonneau. 2004. Evaluating content selection in summarization: The pyramid method. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004, volume 2004, pages 145–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Generating natural language summaries from multiple on-line sources.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>3</issue>
<contexts>
<context position="5313" citStr="Radev and McKeown, 1998" startWordPosition="831" endWordPosition="834">rather than evaluative perspective—our goal is not to develop a new evaluation measure as defined by correlation with human responsiveness judgments. Instead, our studies reveal useful criteria with which to distinguish human-written and system summaries, helping to guide the development of future summarization systems. 2 Related Work Domain-dependent template-based summarization systems have been an alternative to extractive systems which make use of rich knowledge about a domain and information extraction techniques to generate a summary, possibly using a natural language generation system (Radev and McKeown, 1998; White et al., 2001; McKeown et al., 2002). This paper can be seen as a first step towards reconciling the advantages of domain knowledge with the resource-lean extraction approaches popular today. As noted above, Lin and Hovy’s (2000) signature terms have been successful in discovering terms that are specific to the source text. These terms are identified by a log-likelihood ratio test based on their relative frequencies in relevant and irrelevant documents. They were originally proposed in the context of single-document summarization, where they were calculated using indomain (relevant) vs.</context>
</contexts>
<marker>Radev, McKeown, 1998</marker>
<rawString>Dragomir R. Radev and Kathleen R. McKeown. 1998. Generating natural language summaries from multiple on-line sources. Computational Linguistics, 24(3):470–500.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Horacio Saggion</author>
<author>Guy Lapalme</author>
</authors>
<date>2002</date>
<booktitle>Generating indicative-informative summaries with SumUM. Computational linguistics,</booktitle>
<pages>28--4</pages>
<contexts>
<context position="7982" citStr="Saggion and Lapalme (2002)" startWordPosition="1255" endWordPosition="1259">ummaries, and find that they score in between current automatic systems and human-written abstracts on responsiveness, linguistic quality, and Pyramid score. In the lecture domain, He et al. (1999; 2000) find that lecture transcripts that have been manually highlighted with key points improve students’ quiz scores more than when using automated summarization techniques or when providing only the lecture transcript or slides. Jing and McKeown (2000) manually analyzed 30 human-written summaries, and find that 19% of sentences cannot be explained by cut-and-paste operations from the source text. Saggion and Lapalme (2002) similarly define a list of transformations necessary to convert source text to summary text, and manually analyzed their frequencies. Copeck and Szpakowicz (2004) find that at most 55% of vocabulary items found in model summaries occur in the source text, but they do not investigate where the other vocabulary items might be found. 1234 Sentence: At one point, two bomb squad trucks sped to the school after a backpack scare. Dependencies: num(point, one) prep at(sped, point) num(trucks, two) nn(trucks, bomb) nn(trucks, squad) nsubj(sped, trucks) root(ROOT, sped) det(school, the) prep to(sped, s</context>
</contexts>
<marker>Saggion, Lapalme, 2002</marker>
<rawString>Horacio Saggion and Guy Lapalme. 2002. Generating indicative-informative summaries with SumUM. Computational linguistics, 28(4):497–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Horacio Saggion</author>
<author>Juan-Manuel Torres-Moreno</author>
<author>Iria Cunha</author>
<author>Eric SanJuan</author>
</authors>
<title>Multilingual summarization evaluation without human models.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters,</booktitle>
<pages>1059--1067</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="6899" citStr="Saggion et al., 2010" startWordPosition="1084" endWordPosition="1087">usters from the same domain are available. Summarization evaluation is typically done by comparing system output to human-written model summaries, and are validated by their correlation with user responsiveness judgments. The comparison can be done at the word level, as in ROUGE (Lin, 2004), at the syntactic level, as in Basic Elements (Hovy et al., 2006), or at the level of summary content units, as in the Pyramid method (Nenkova and Passonneau, 2004). There are also automatic measures which do not require model summaries, but compare against the source text instead (Louis and Nenkova, 2009; Saggion et al., 2010). Several studies complement this paper by examining the best possible extractive system using current evaluation measures, such as ROUGE (Lin and Hovy, 2003; Conroy et al., 2006). They find that the best possible extractive systems score higher or as highly than human summarizers, but it is unclear whether this means the oracle summaries are actually as useful as human ones in an extrinsic setting. Genest et al. (2009) ask humans to create extractive summaries, and find that they score in between current automatic systems and human-written abstracts on responsiveness, linguistic quality, and </context>
</contexts>
<marker>Saggion, Torres-Moreno, Cunha, SanJuan, 2010</marker>
<rawString>Horacio Saggion, Juan-Manuel Torres-Moreno, Iria Cunha, and Eric SanJuan. 2010. Multilingual summarization evaluation without human models. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, pages 1059– 1067. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Mining the web for synonyms: PMI-IR versus LSA on TOEFL.</title>
<date>2001</date>
<booktitle>In Proceedings of the Twelth European Conference on Machine Learning (ECML-2001),</booktitle>
<pages>491--502</pages>
<contexts>
<context position="13016" citStr="Turney, 2001" startWordPosition="2065" endWordPosition="2066">o account for these issues, we measure caseframe similarity based on their distributional similarity in a large training corpus. First, we construct vector representations of each caseframe, where the dimensions of the vector correspond to the lemma of the head word that fills the caseframe in the training corpus. For example, kicked the ball would result in a count of 1 added to the caseframe (kick, dobj) for the context word ball. Then, we rescale the counts into pointwise mutual information values, which has been shown to be more effective than raw counts at detecting semantic relatedness (Turney, 2001). Similarity between caseframes can then be compared by cosine similarity between the their vector representations. For training, we use the AFP portion of the Gigaword corpus (Graff et al., 2005), which we parsed using the Stanford parser’s typed dependency tree representation with collapsed conjunctions (de Marneffe et al., 2006). For reasons of sparsity, we only considered caseframes that appear at least five times in the guided summarization corpus, and only the 3000 most common lemmata in Gigaword as context words. 3.2 An Example To illustrate how caseframes indicate the slots in a summar</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>Peter Turney. 2001. Mining the web for synonyms: PMI-IR versus LSA on TOEFL. In Proceedings of the Twelth European Conference on Machine Learning (ECML-2001), pages 491–502.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
<author>Tanya Korelsky</author>
<author>Claire Cardie</author>
<author>Vincent Ng</author>
<author>David Pierce</author>
<author>Kiri Wagstaff</author>
</authors>
<title>Multidocument summarization via information extraction.</title>
<date>2001</date>
<booktitle>In Proceedings of the First International Conference on Human Language Technology Research. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5333" citStr="White et al., 2001" startWordPosition="835" endWordPosition="838">rspective—our goal is not to develop a new evaluation measure as defined by correlation with human responsiveness judgments. Instead, our studies reveal useful criteria with which to distinguish human-written and system summaries, helping to guide the development of future summarization systems. 2 Related Work Domain-dependent template-based summarization systems have been an alternative to extractive systems which make use of rich knowledge about a domain and information extraction techniques to generate a summary, possibly using a natural language generation system (Radev and McKeown, 1998; White et al., 2001; McKeown et al., 2002). This paper can be seen as a first step towards reconciling the advantages of domain knowledge with the resource-lean extraction approaches popular today. As noted above, Lin and Hovy’s (2000) signature terms have been successful in discovering terms that are specific to the source text. These terms are identified by a log-likelihood ratio test based on their relative frequencies in relevant and irrelevant documents. They were originally proposed in the context of single-document summarization, where they were calculated using indomain (relevant) vs. out-of-domain (irre</context>
</contexts>
<marker>White, Korelsky, Cardie, Ng, Pierce, Wagstaff, 2001</marker>
<rawString>Michael White, Tanya Korelsky, Claire Cardie, Vincent Ng, David Pierce, and Kiri Wagstaff. 2001. Multidocument summarization via information extraction. In Proceedings of the First International Conference on Human Language Technology Research. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>