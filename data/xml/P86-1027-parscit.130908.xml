<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.591768">
<title confidence="0.969386">
FORUM ON CONNECTIONISM
Language Learning in Massively-Parallel Networks
</title>
<note confidence="0.95829675">
Terrence J. Sejnowski
Biophysics Department
Johns Hopkins University
Baltimore, MD 21218
</note>
<sectionHeader confidence="0.846171" genericHeader="abstract">
PANELIST STATEMENT
</sectionHeader>
<bodyText confidence="0.999908984615385">
Massively-parallel connectionist networks have tradition-
ally been applied to constraint-satisfaction in early visual
processing (Ballard, Hinton &amp; Sejnowski, 1983), but are now
being applied to problems ranging from the Traveling-
Salesman Problem to language acquisition (Rumelhart &amp;
McClelland, 1986). In these networks, knowledge is
represented by the distributed pattern of activity in a large
number of relatively simple neuron-like processing units, and
computation is performed in parallel by the use of connec-
tions between the units.
A network model can be &amp;quot;programmed&amp;quot; by specifying the
strengths of the connections, or weights, on all the links be-
tween the processing units. In vision, it is sometimes possible
to design networks from a task analysis of the problem, aided
by the homogeneity of the domain. For example, Sejnowski
&amp; Hinton (1986) designed a network that can separate figure
from ground for shapes with incomplete bounding contours.
Constructing a network is much more difficult in an in-
homogeneous domain like natural language. This problem
has been partially overcome by the discovery of powerful
learning algorithms that allow the strengths of connection in
a network to be shaped by experience; that is, a good set of
weights can be found to solve a problem given only examples
of typical inputs and the desired outputs (Sejnowski, Kienker
&amp; Hinton, 1986; Rumelhart, Hinton &amp; Williams, 1986).
Network learning will be demonstrated for the problem of
converting unrestricted English text to phonemes. NETtalk
is a network of 309 processing units connected by 18,629
weights (Sejnowski &amp; Rosenberg, 1986). It was trained on
the 1,000 most common words in English taken from the
Brown corpus and achieved 98% accuracy. The same net-
work was then tested for generalization on a 20,000 word
dictionary: without further training it was 80% accurate and
reached 92% with additional training. The network mas-
tered different letter-to-sound correspondence rules in vary-
ing lengths. of time; for example, the &amp;quot;hard c rule&amp;quot;, c -&gt; /k/,
was learned much faster than the &amp;quot;soft c rule&amp;quot;, c -&gt; /5/.
NETtalk demonstrably learns the regular patterns of
English pronunciation and also copes with the problem of ir-
regularity in the corpus. Irregular words are learned not by
creating a look-up table of exceptions, as is common in com-
mercial text-to-speech systems such as DECtalk, but by pat-
tern recognition. As a consequence, exceptional words are in-
corporated into the network as easily as words with a regular
pronunciation. NETtalk is being used as a research tool to
study phonology; it can also be used as a model for studying
acquired dyslexia and recovery from brain damage; several
interesting phenomena in human learning and memory such
as the power law for practice and the spacing effect are in-
herent properties of the distributed form of knowledge
representation used by NETtalk (Rosenberg &amp; Sejnowski,
1986).
NETtalk has no access to syntactic or semantic infor-
mation and cannot, for example, disambiguate the two
pronunciations of &amp;quot;read&amp;quot;. Grammatical analysis requires
longer range interactions at the level of word representations.
However, it may be possible to train larger and more sophis-
ticated networks on problems in these domains and incor-
porate them into a system of networks that form a highly
modularized and distributed language analyzer. At present
there is no way to assess the computational complexity of
these tasks for network models; the experience with NETtalk
suggests that conventional measures of complexity derived
from rule-based models of language are not accurate in-
dicators.
</bodyText>
<sectionHeader confidence="0.998756" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.905790083333334">
Ballard, D. H., Hinton, G. E., &amp; Sejnowski, T. J., 1983.
Parallel visual computation, Nature 306: 21-26.
Rosenberg, C. R. &amp; Sejnowski, T. J. 1986. The effects of
distributed vs massed practice on NETtalk, a massively-
parallel network that learns to read aloud, (submitted for
publication).
Rumelhart, D. E., Hinton, G. E. &amp; Williams, R. J. 1986.
In: Parallel Distributed Processing: Explorations in
the Microstructure of Cognition. Edited by Rumelhart,
D. E. &amp; McClelland, J. L. (Cambridge: MIT Press.)
Rumelhart, D. E. &amp; McClelland, J. L. (Eds.) 1986.
Parallel Distributed Processing: Explorations in the
Microstructure of Cognition. (Cambridge: MIT Press.)
Sejnowski, T. J., Kienker, P. K. &amp; Hinton, G. E. (in
press) Learning symmetry groups with hidden units: Beyond
the perceptron, Physics D.
Sejnowski, T. J. &amp; Hinton, G. E. 1986. Separating figure
from ground with a Boltzmann Machine, In: Vision, Brain
&amp; Cooperative Computation, Edited by M. A. Arbib &amp;
A. R. Hanson (Cambridge: MIT Press).
Sejnowski, T. J. &amp; Rosenberg, C. R. 1986. NETtalk: A
parallel network that learns to read aloud, Johns Hopkins
University Department of Electrical Engineering and Com-
puter Science Technical Report 86/01.
</reference>
<page confidence="0.998665">
184
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.003993">
<title confidence="0.997936">FORUM ON CONNECTIONISM Language Learning in Massively-Parallel Networks</title>
<author confidence="0.999997">Terrence J Sejnowski</author>
<affiliation confidence="0.995862">Biophysics Department Johns Hopkins University</affiliation>
<address confidence="0.999886">Baltimore, MD 21218</address>
<abstract confidence="0.986320393939394">PANELIST STATEMENT Massively-parallel connectionist networks have traditionally been applied to constraint-satisfaction in early visual processing (Ballard, Hinton &amp; Sejnowski, 1983), but are now being applied to problems ranging from the Traveling- Salesman Problem to language acquisition (Rumelhart &amp; McClelland, 1986). In these networks, knowledge is represented by the distributed pattern of activity in a large number of relatively simple neuron-like processing units, and computation is performed in parallel by the use of connections between the units. A network model can be &amp;quot;programmed&amp;quot; by specifying the strengths of the connections, or weights, on all the links between the processing units. In vision, it is sometimes possible to design networks from a task analysis of the problem, aided by the homogeneity of the domain. For example, Sejnowski &amp; Hinton (1986) designed a network that can separate figure from ground for shapes with incomplete bounding contours. Constructing a network is much more difficult in an inhomogeneous domain like natural language. This problem has been partially overcome by the discovery of powerful learning algorithms that allow the strengths of connection in a network to be shaped by experience; that is, a good set of weights can be found to solve a problem given only examples of typical inputs and the desired outputs (Sejnowski, Kienker Hinton, Hinton &amp; Williams, 1986). Network learning will be demonstrated for the problem of converting unrestricted English text to phonemes. NETtalk a network of 309 processing units connected by weights (Sejnowski &amp; Rosenberg, 1986). It was trained on the 1,000 most common words in English taken from the corpus and achieved The same network was then tested for generalization on a 20,000 word dictionary: without further training it was 80% accurate and reached 92% with additional training. The network mastered different letter-to-sound correspondence rules in varying lengths. of time; for example, the &amp;quot;hard c rule&amp;quot;, c -&gt; /k/, was learned much faster than the &amp;quot;soft c rule&amp;quot;, c -&gt; /5/. NETtalk demonstrably learns the regular patterns of English pronunciation and also copes with the problem of irregularity in the corpus. Irregular words are learned not by creating a look-up table of exceptions, as is common in commercial text-to-speech systems such as DECtalk, but by pattern recognition. As a consequence, exceptional words are incorporated into the network as easily as words with a regular pronunciation. NETtalk is being used as a research tool to study phonology; it can also be used as a model for studying acquired dyslexia and recovery from brain damage; several interesting phenomena in human learning and memory such as the power law for practice and the spacing effect are inherent properties of the distributed form of knowledge representation used by NETtalk (Rosenberg &amp; Sejnowski, 1986). NETtalk has no access to syntactic or semantic information and cannot, for example, disambiguate the two pronunciations of &amp;quot;read&amp;quot;. Grammatical analysis requires longer range interactions at the level of word representations. However, it may be possible to train larger and more sophisticated networks on problems in these domains and incorporate them into a system of networks that form a highly modularized and distributed language analyzer. At present there is no way to assess the computational complexity of these tasks for network models; the experience with NETtalk suggests that conventional measures of complexity derived from rule-based models of language are not accurate indicators.</abstract>
<note confidence="0.828515642857143">REFERENCES Ballard, D. H., Hinton, G. E., &amp; Sejnowski, T. J., 1983. visual computation, 21-26. C. R. &amp; Sejnowski, T. 1986. effects of distributed vs massed practice on NETtalk, a massivelyparallel network that learns to read aloud, (submitted for publication). D. E., Hinton, G. E. &amp; Williams, J. Distributed Processing: Explorations in Microstructure of Cognition. by Rumelhart, D. E. &amp; McClelland, J. L. (Cambridge: MIT Press.) Rumelhart, D. E. &amp; McClelland, J. L. (Eds.) 1986. Parallel Distributed Processing: Explorations in the of Cognition. MIT Press.</note>
<author confidence="0.61805">T J Sejnowski</author>
<author confidence="0.61805">P K Kienker</author>
<author confidence="0.61805">G E Hinton</author>
<affiliation confidence="0.442157">press) Learning symmetry groups with hidden units: Beyond</affiliation>
<keyword confidence="0.576492">perceptron, D.</keyword>
<note confidence="0.491722777777778">T. J. &amp; G. E. 1986. Separating figure ground with a Boltzmann Machine, In: Brain Cooperative Computation, by M. A. Arbib &amp; A. R. Hanson (Cambridge: MIT Press). Sejnowski, T. J. &amp; Rosenberg, C. R. 1986. NETtalk: A parallel network that learns to read aloud, Johns Hopkins University Department of Electrical Engineering and Computer Science Technical Report 86/01. 184</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D H Ballard</author>
<author>G E Hinton</author>
<author>T J Sejnowski</author>
</authors>
<title>Parallel visual computation,</title>
<date>1983</date>
<journal>Nature</journal>
<volume>306</volume>
<pages>21--26</pages>
<marker>Ballard, Hinton, Sejnowski, 1983</marker>
<rawString>Ballard, D. H., Hinton, G. E., &amp; Sejnowski, T. J., 1983. Parallel visual computation, Nature 306: 21-26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C R Rosenberg</author>
<author>T J Sejnowski</author>
</authors>
<title>The effects of distributed vs massed practice on NETtalk, a massivelyparallel network that learns to read aloud,</title>
<date>1986</date>
<note>(submitted for publication).</note>
<contexts>
<context position="3107" citStr="Rosenberg &amp; Sejnowski, 1986" startWordPosition="483" endWordPosition="486">table of exceptions, as is common in commercial text-to-speech systems such as DECtalk, but by pattern recognition. As a consequence, exceptional words are incorporated into the network as easily as words with a regular pronunciation. NETtalk is being used as a research tool to study phonology; it can also be used as a model for studying acquired dyslexia and recovery from brain damage; several interesting phenomena in human learning and memory such as the power law for practice and the spacing effect are inherent properties of the distributed form of knowledge representation used by NETtalk (Rosenberg &amp; Sejnowski, 1986). NETtalk has no access to syntactic or semantic information and cannot, for example, disambiguate the two pronunciations of &amp;quot;read&amp;quot;. Grammatical analysis requires longer range interactions at the level of word representations. However, it may be possible to train larger and more sophisticated networks on problems in these domains and incorporate them into a system of networks that form a highly modularized and distributed language analyzer. At present there is no way to assess the computational complexity of these tasks for network models; the experience with NETtalk suggests that conventional</context>
</contexts>
<marker>Rosenberg, Sejnowski, 1986</marker>
<rawString>Rosenberg, C. R. &amp; Sejnowski, T. J. 1986. The effects of distributed vs massed practice on NETtalk, a massivelyparallel network that learns to read aloud, (submitted for publication).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Rumelhart</author>
<author>G E Hinton</author>
<author>R J Williams</author>
</authors>
<title>In:</title>
<date>1986</date>
<booktitle>Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Edited</booktitle>
<publisher>MIT Press.</publisher>
<marker>Rumelhart, Hinton, Williams, 1986</marker>
<rawString>Rumelhart, D. E., Hinton, G. E. &amp; Williams, R. J. 1986. In: Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Edited by Rumelhart, D. E. &amp; McClelland, J. L. (Cambridge: MIT Press.)</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Rumelhart</author>
<author>J L McClelland</author>
</authors>
<title>Parallel Distributed Processing: Explorations in the Microstructure of Cognition.</title>
<date>1986</date>
<publisher>MIT Press.</publisher>
<location>Cambridge:</location>
<marker>Rumelhart, McClelland, 1986</marker>
<rawString>Rumelhart, D. E. &amp; McClelland, J. L. (Eds.) 1986. Parallel Distributed Processing: Explorations in the Microstructure of Cognition. (Cambridge: MIT Press.) Sejnowski, T. J., Kienker, P. K. &amp; Hinton, G. E. (in press) Learning symmetry groups with hidden units: Beyond the perceptron, Physics D.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T J Sejnowski</author>
<author>G E Hinton</author>
</authors>
<title>Separating figure from ground with a Boltzmann Machine, In: Vision,</title>
<date>1986</date>
<journal>Brain &amp; Cooperative Computation, Edited</journal>
<publisher>MIT Press).</publisher>
<contexts>
<context position="1033" citStr="Sejnowski &amp; Hinton (1986)" startWordPosition="146" endWordPosition="149">gSalesman Problem to language acquisition (Rumelhart &amp; McClelland, 1986). In these networks, knowledge is represented by the distributed pattern of activity in a large number of relatively simple neuron-like processing units, and computation is performed in parallel by the use of connections between the units. A network model can be &amp;quot;programmed&amp;quot; by specifying the strengths of the connections, or weights, on all the links between the processing units. In vision, it is sometimes possible to design networks from a task analysis of the problem, aided by the homogeneity of the domain. For example, Sejnowski &amp; Hinton (1986) designed a network that can separate figure from ground for shapes with incomplete bounding contours. Constructing a network is much more difficult in an inhomogeneous domain like natural language. This problem has been partially overcome by the discovery of powerful learning algorithms that allow the strengths of connection in a network to be shaped by experience; that is, a good set of weights can be found to solve a problem given only examples of typical inputs and the desired outputs (Sejnowski, Kienker &amp; Hinton, 1986; Rumelhart, Hinton &amp; Williams, 1986). Network learning will be demonstr</context>
</contexts>
<marker>Sejnowski, Hinton, 1986</marker>
<rawString>Sejnowski, T. J. &amp; Hinton, G. E. 1986. Separating figure from ground with a Boltzmann Machine, In: Vision, Brain &amp; Cooperative Computation, Edited by M. A. Arbib &amp; A. R. Hanson (Cambridge: MIT Press).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T J Sejnowski</author>
<author>C R Rosenberg</author>
</authors>
<title>NETtalk: A parallel network that learns to read aloud,</title>
<date>1986</date>
<tech>Technical Report 86/01.</tech>
<institution>Johns Hopkins University Department of Electrical Engineering and Computer Science</institution>
<contexts>
<context position="1809" citStr="Sejnowski &amp; Rosenberg, 1986" startWordPosition="268" endWordPosition="271"> in an inhomogeneous domain like natural language. This problem has been partially overcome by the discovery of powerful learning algorithms that allow the strengths of connection in a network to be shaped by experience; that is, a good set of weights can be found to solve a problem given only examples of typical inputs and the desired outputs (Sejnowski, Kienker &amp; Hinton, 1986; Rumelhart, Hinton &amp; Williams, 1986). Network learning will be demonstrated for the problem of converting unrestricted English text to phonemes. NETtalk is a network of 309 processing units connected by 18,629 weights (Sejnowski &amp; Rosenberg, 1986). It was trained on the 1,000 most common words in English taken from the Brown corpus and achieved 98% accuracy. The same network was then tested for generalization on a 20,000 word dictionary: without further training it was 80% accurate and reached 92% with additional training. The network mastered different letter-to-sound correspondence rules in varying lengths. of time; for example, the &amp;quot;hard c rule&amp;quot;, c -&gt; /k/, was learned much faster than the &amp;quot;soft c rule&amp;quot;, c -&gt; /5/. NETtalk demonstrably learns the regular patterns of English pronunciation and also copes with the problem of irregularity</context>
</contexts>
<marker>Sejnowski, Rosenberg, 1986</marker>
<rawString>Sejnowski, T. J. &amp; Rosenberg, C. R. 1986. NETtalk: A parallel network that learns to read aloud, Johns Hopkins University Department of Electrical Engineering and Computer Science Technical Report 86/01.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>