<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000089">
<title confidence="0.9752885">
Machine Learning Methods for
Chinese Web Page Categorization
</title>
<author confidence="0.998499">
Ji He&apos;, Ah-Hwee Tan&apos; and Chew-Lim Tan&amp;quot;
</author>
<affiliation confidence="0.998812">
&apos;School of Computing, National University of Singapore
</affiliation>
<address confidence="0.800045">
10 Kent Ridge Crescent, Singapore 119260
fheji,tancll©comp.nus.edu.sg
&apos;Kent Ridge Digital Labs
21 Heng Mui Keng Terrace, Singapore 119613
</address>
<email confidence="0.842102">
ahhweealcrdl.org.sg
</email>
<sectionHeader confidence="0.989966" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999694733333333">
This paper reports our evaluation of
k Nearest Neighbor (kNN), Support
Vector Machines (SVM), and Adap-
tive Resonance Associative Map
(ARAM) on Chinese web page clas-
sification. Benchmark experiments
based on a Chinese web corpus
showed that their predictive per-
formance were roughly comparable
although ARAM and kNN slightly
outperformed SVM in small cate-
gories. In addition, inserting rules
into ARAM helped to improve per-
formance, especially for small well-
defined categories.
</bodyText>
<sectionHeader confidence="0.998786" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999966678571429">
Text categorization refers to the task of au-
tomatically assigning one or multiple pre-
defined category labels to free text docu-
ments. Whereas an extensive range of meth-
ods has been applied to English text cate-
gorization, relatively few have been bench-
marked for Chinese text categorization. Typi-
cal approaches to Chinese text categorization,
such as Naive Bayes (NB) (Zhu, 1987), Vector
Space Model (VSM) (Zou et al., 1998; Zou et
al., 1999) and Linear List Square Fit (LLSF)
(Cao et al., 1999; Yang, 1994), have well stud-
ied theoretical basis derived from the informa-
tion retrieval research, but are not known to
be the best classifiers (Yang and Liu, 1999;
Yang, 1999). In addition, there is a lack of
publicly available Chinese corpus for evaluat-
ing Chinese text categorization systems.
This paper reports our applications of
three statistical machine learning methods,
namely k Nearest Neighbor system (kNN)
(Dasarathy, 1991), Support Vector Machines
(SVM) (Cortes and Vapnik, 1995), and Adap-
tive Resonance Associative Map (ARAM)
(Tan, 1995) to Chinese web page categoriza-
tion. kNN and SVM have been reported as
the top performing methods for English text
categorization (Yang and Liu, 1999). ARAM
belongs to a popularly known family of pre-
dictive self-organizing neural networks which
until recently has not been used for docu-
ment classification. The trio has been eval-
uated based on a Chinese corpus consisting
of news articles extracted from People&apos;s Daily
(He et al., 2000). This article reports the ex-
periments of a much more challenging task in
classifying Chinese web pages. The Chinese
web corpus was created by downloading from
various Chinese web sites covering a wide vari-
ety of topics. There is a great diversity among
the web pages in terms of document length,
style, and content. The objectives of our ex-
periments are two-folded. First, we examine
and compare the capabilities of these meth-
ods in learning categorization knowledge from
real-life web documents. Second, we investi-
gate if incorporating domain knowledge de-
rived from the category description can en-
hance ARAM&apos;s predictive performance.
The rest of this article is organized as fol-
lows. Section 2 describes our choice of the fea-
ture selection and extraction methods. Sec-
tion 3 gives a summary of the kNN and SVM,
and presents the less familiar ARAM algo-
rithm in more details. Section 4 presents our
evaluation paradigm and reports the experi-
</bodyText>
<page confidence="0.996903">
93
</page>
<bodyText confidence="0.857509">
mental results. vector
</bodyText>
<note confidence="0.983721333333333">
2 Feature Selection and Extraction (Xi, X2, ...,XM) (1)
x-
max{xi}
</note>
<bodyText confidence="0.997312489361702">
A pre-requisite of text categorization is to ex-
tract a suitable feature representation of the
documents. Typically, word stems are sug-
gested as the representation units by infor-
mation retrieval research. However, unlike
English and other Indo-European languages,
Chinese text does not have a natural delim-
iter between words. As a consequence, word
segmentation is a major issue in Chinese doc-
ument processing. Chinese word segmenta-
tion methods have been extensively discussed
in the literature. Unfortunately perfect preci-
sion and disambiguation cannot be reached.
As a result, the inherent errors caused by
word segmentation always remains as a prob-
lem in Chinese information processing.
In our experiments, a word-class bi-gram
model is adopted to segment each training
document into a set of tokens. The lexi-
con used by the segmentation model contains
64,000 words in 1,006 classes. High precision
segmentation is not the focus of our work. In-
stead we aim to compare different classifier&apos;s
performance on noisy document set as long as
the errors caused by word segmentation are
reasonably low.
To select keyword features for classifica-
tion, x (CHI) statistics is adopted as the
ranking metric in our experiments. A prior
study on several well-known corpora in-
cluding Reuters-21578 and OHSUMED has
proven that CHI statistics generally outper-
forms other feature ranking measures, such
as term strength (TS), document frequency
(DF), mutual information (MI), and informa-
tion gain (IG) (Yang and J.P, 1997).
During keyword extraction, the document
is first segmented and converted into a
keyword frequency vector (tfi, tf2, , tfm),
where tfi is the in-document term frequency
of keyword w, and M is the number of the
keyword features selected. A term weight-
ing method based on inverse document fre-
quency (IDF) (Salton, 1988) and the Li-
normalization are then applied on the fre-
quency vector to produce the keyword feature
in which xi is computed by
</bodyText>
<equation confidence="0.892426">
xi = (1 + log2 tfi) log2 — (2)
ni
</equation>
<bodyText confidence="0.99712325">
where n is the number of documents in the
whole training set, and ni is the number of
training documents in which the keyword wi
occurs at least once.
</bodyText>
<sectionHeader confidence="0.99531" genericHeader="method">
3 The Classifiers
</sectionHeader>
<subsectionHeader confidence="0.996374">
3.1 k Nearest Neighbor
</subsectionHeader>
<bodyText confidence="0.999824">
k Nearest Neighbor (kNN) is a tradi-
tional statistical pattern recognition algo-
rithm (Dasarathy, 1991). It has been studied
extensively for text categorization (Yang and
Liu, 1999). In essence, kNN makes the predic-
tion based on the k training patterns that are
closest to the unseen (test) pattern, accord-
ing to a distance metric. The distance metric
that measures the similarity between two nor-
malized patterns can be either a simple LI-
distance function or a L2-distance function,
such as the plain Euclidean distance defined
by
</bodyText>
<equation confidence="0.871678">
D(a, b) = 1/E (ai –b1)2. (3)
</equation>
<bodyText confidence="0.999175142857143">
The class assignment to the test pattern is
based on the class assignment of the closest k
training patterns. A commonly used method
is to label the test pattern with the class that
has the most instances among the k nearest
neighbors. Specifically, the class index y(x)
assigned to the test pattern x is given by
</bodyText>
<equation confidence="0.840857">
y(x) = argmaxi{n(dj, ci)idjEkNN}, (4)
</equation>
<bodyText confidence="0.999592857142857">
where n(di, ci) is the number of training pat-
tern dj in the k nearest neighbor set that are
associated with class
The drawback of kNN is the difficulty in
deciding a optimal k value. Typically it has
to be determined through conducting a series
of experiments using different values.
</bodyText>
<page confidence="0.99873">
94
</page>
<subsectionHeader confidence="0.999787">
3.2 Support Vector Machines
</subsectionHeader>
<bodyText confidence="0.999918777777778">
Support Vector Machines (SVM) is a rela-
tively new class of machine learning tech-
niques first introduced by Vapnik (Cortes
and Vapnik, 1995). Based on the structural
risk minimization principle from the compu-
tational learning theory, SVM seeks a decision
surface to separate the training data points
into two classes and makes decisions based on
the support vectors that are selected as the
only effective elements from the training set.
Given a set of linearly separable points
S = {xiERnli = 1, 2, ... ,N}, each point xi
belongs to one of the two classes, labeled as
ye{—1, +1}. A separating hyper-plane di-
vides S into two sides, each side containing
points with the same class label only. The
separating hyper-plane can be identified by
the pair (w, b) that satisfies
</bodyText>
<equation confidence="0.8059018">
w-x+b= 0 (5)
and yi (w + b)&gt;1
for i = 1, 2, ... , N; where the dot product op-
eration is defined by
w • x wixi (6)
</equation>
<bodyText confidence="0.999877882352941">
for vectors w and x. Thus the goal of the
SVM learning is to find the optimal separat-
ing hyper-plane (OSH) that has the maximal
margin to both sides. This can be formula-
rized as:
The points that are closest to the OSH are
termed support vectors (Fig. 1).
The SVM problem can be extended to lin-
early non-separable case and non-linear case.
Various quadratic programming algorithms
have been proposed and extensively studied
to solve the SVM problem (Cortes and \Tap-
nik, 1995; Joachims, 1998; Joachims, 1999).
During classification, SVM makes decision
based on the OSH instead of the whole
training set. It simply finds out on which
side of the OSH the test pattern is located.
</bodyText>
<figureCaption confidence="0.7340166">
Figure 1: Separating hyperplanes (the set
of solid lines), optimal separating hyperplane
(the bold solid line), and support vectors (data
points on the dashed lines). The dashed lines
identify the max margin.
</figureCaption>
<bodyText confidence="0.999903916666667">
This property makes SVM highly compet-
itive, compared with other traditional pat-
tern recognition methods, in terms of com-
putational efficiency and predictive accuracy
(Yang and Liu, 1999).
In recent years, Joachims has done much re-
search on the application of SVM to text cat-
egorization (Joachims, 1998). His SVMlight
system published via http://www-ai.cs.uni-
dortmund.de/FORSCHUNG/VERFAHREN/
SVM_LIGHT/svmlight.eng.html is used in
our benchmark experiments.
</bodyText>
<subsectionHeader confidence="0.997707">
3.3 Adaptive Resonance Associative
Map
</subsectionHeader>
<bodyText confidence="0.999667722222222">
Adaptive Resonance Associative Map
(ARAM) is a class of predictive self-
organizing neural networks that performs
incremental supervised learning of recog-
nition categories (pattern classes) and
multidimensional maps of patterns. An
ARAM system can be visualized as two
overlapping Adaptive Resonance Theory
(ART) modules consisting of two input fields
Ff and Ft with an F2 category field (Tan,
1995; Tan, 1997) (Fig. 2). For classification
problems, the IT field serves as the input
field containing the document feature vector
and the FP field serves as the output field
containing the class prediction vector. The
F2 field contains the activities of the recogni-
tion categories that are used to encode the
patterns.
</bodyText>
<equation confidence="0.89069">
(7)
subject to yi(w • xi + b)&gt;1
minimize lw • w
</equation>
<page confidence="0.973094">
95
</page>
<figureCaption confidence="0.9916005">
Figure 2: The Adaptive Resonance Associa-
tive Map architecture
</figureCaption>
<bodyText confidence="0.97418425">
When performing classification tasks,
ARAM formulates recognition categories of
input patterns, and associates each cate-
gory with its respective prediction. During
learning, given an input pattern (document
feature) presented at the Fif&apos; input layer
and an output pattern (known class label)
presented at the FP output field, the category
field F2 selects a winner that receives the
largest overall input. The winning node se-
lected in F2 then triggers a top-down priming
on Ff and Fp, monitored by separate reset
mechanisms. Code stabilization is ensured
by restricting encoding to states where
resonance are reached in both modules.
By synchronizing the unsupervised catego-
rization of two pattern sets, ARAM learns
supervised mapping between the pattern sets.
Due to the code stabilization mechanism,
fast learning in a real-time environment is
feasible.
The knowledge that ARAM discovers dur-
ing learning is compatible with IF-THEN
rule-based presentation. Specifically, each
node in the F2 field represents a recognition
category associating the Ff patterns with the
Fib output vectors. Learned weight vectors,
one for each F2 node, constitute a set of rules
that link antecedents to consequences. At any
point during the incremental learning process,
the system architecture can be translated into
a compact set of rules. Similarly, domain
knowledge in the form of IF-THEN rules can
be inserted into ARAM architecture.
The ART modules used in ARAM can be
ART 1, which categorizes binary patterns, or
analog ART modules such as ART 2, ART 2-
A, and fuzzy ART, which categorize both bi-
nary and analog patterns. The fuzzy ARAM
(Tan, 1995) algorithm based on fuzzy ART
(Carpenter et al., 1991) is introduced below.
Parameters: Fuzzy ARAM dynamics are
determined by the choice parameters a. &gt; 0
and ab &gt; 0; the learning rates fla E [0,1] and
13b E [0, 1]; the vigilance parameters Pa E [0,1]
and Pb E [0,1]; and the contribution parame-
ter -y E [0,1].
Weight vectors: Each F2 category node j
is associated with two adaptive weight tem-
plates wl and w. Initially, all category nodes
are uncommitted and all weights equal ones.
After a category node is selected for encoding,
it becomes committed.
Category choice: Given the Ft and Fp in-
put vectors A and B, for each F2 node j, the
choice function Tj is defined by
</bodyText>
<equation confidence="0.895999333333333">
IA A wql IB A wig
Tj =
, aa + iwII+ (1 -y) ab + (8)
</equation>
<bodyText confidence="0.99844125">
where the fuzzy AND operation A is defined
by
(p A q)i -=- qi), (9)
and where the norm 1.1 is defined by
</bodyText>
<subsectionHeader confidence="0.657428">
IPI Pi (10)
</subsectionHeader>
<bodyText confidence="0.936922">
for vectors p and q.
The system is said to make a choice when
at most one F2 node can become active. The
choice is indexed at J where
Tj = max{Ti :for all F2 node j}. (11)
When a category choice is made at node J,
yj = 1; and yj = 0 for all j 0 J.
Resonance or reset: Resonance occurs if
the match functions, m5 and mbj, meet the
vigilance criteria in their respective modules:
</bodyText>
<figure confidence="0.922195428571429">
a IA A w,1
m j = (12)
and
b IB A wbjl
m j —
IBI
(13)
</figure>
<page confidence="0.983891">
96
</page>
<bodyText confidence="0.999317">
Learning then ensues, as defined below. If
any of the vigilance constraints is violated,
mismatch reset occurs in which the value of
the choice function Tj is set to 0 for the du-
ration of the input presentation. The search
process repeats to select another new index J
until resonance is achieved.
Learning: Once the search ends, the weight
vectors wfj, and wbj are updated according to
the equations
</bodyText>
<equation confidence="0.89661975">
weinew) = (1_ fia)wcal(old) + it (A. A waj(old))
and
wb(new) old) b(old)
= (1 — )3b)Wb( )3b(B A w
</equation>
<bodyText confidence="0.7336685">
respectively. Fast learning corresponds to set-
ting f3a = 13b = 1 for committed nodes.
Classification: During classification, using
the choice rule, only the F2 node J that re-
ceives maximal IT -4 F2 input Ti predicts
ARTb output. In simulations,
</bodyText>
<equation confidence="0.9699376">
0 Yj = if j = J where Tj &gt; Tk
for all k J (16)
otherwise.
The F1 activity vector Xb is given by
xb = = wbj. (17)
</equation>
<bodyText confidence="0.7196085">
The output prediction vector B is then given
by
</bodyText>
<equation confidence="0.776306">
B (bi , b2, bN) Xb (18)
</equation>
<bodyText confidence="0.982752702702703">
where bi indicates the likelihood or confidence
of assigning a pattern to category i.
Rule insertion: Rule insertion proceeds in
two phases. The first phase parses the rules
for keyword features. When a new keyword is
encountered, it is added to a keyword feature
table containing keywords obtained through
automatic feature selection from training
documents. Based on the keyword feature
table, the second phase of rule insertion
translates each rule into a M-dimensional
vector a and a N-dimensional vector b, where
M is the total number of features in the
keyword feature table and N is the number
of categories. Given a rule of the following
format, Xi,X2,•••,Xm
IF 7JllY2)- • • ,Yn
THEN ,
where xi,... 5m are antecedents and
Yi, • • • Yn are consequences, the algorithm
derives a pair of vectors a and b such that
for each index
if wi = xi for some j E {1, ... ,m}
otherwise
where wi is the ith entry in the keyword fea-
ture table; and for each index i = 1,. , N,
where wi is the class label of the category i.
The vector pairs derived from the rules are
then used as training patterns to initialize a
ARAM network. During rule insertion, the
vigilance parameters pa and pb are each set
to 1 to ensure that only identical attribute
vectors are grouped into one recognition cat-
egory. Contradictory symbolic rules are de-
tected during rule insertion when identical in-
put attribute vectors are associated with dis-
tinct output attribute vectors.
</bodyText>
<sectionHeader confidence="0.999266" genericHeader="method">
4 Empirical Evaluation
</sectionHeader>
<subsectionHeader confidence="0.999521">
4.1 The Chinese Web Corpus
</subsectionHeader>
<bodyText confidence="0.970700933333333">
The Chinese web corpus, collected in-house,
consists of web pages downloaded from vari-
ous Chinese web sites covering a wide variety
of topics. Our experiments are based on a
subset of the corpus consisting of 8 top-level
categories and over 6,000 documents. For
each category, we conduct binary classifica-
tion experiments in which we tag the cur-
rent category as the positive category and the
other seven categories as the negative cate-
gories. The corpus is further partitioned into
training and testing data such that the num-
ber of the training documents is at least 2.5
times of that of the testing documents (Table
1).
</bodyText>
<equation confidence="0.4887862">
=
{
=
1 1 if wi —y3 for some j E {1, ... , n}
0 otherwise
</equation>
<page confidence="0.998576">
97
</page>
<tableCaption confidence="0.978643333333333">
Table 1: The eight top-level categories in the
Chinese web corpus, and the training and test
samples by category.
</tableCaption>
<table confidence="0.999794933333333">
Category Description Train Test
Art Topic regarding 325 102
literature, art
Belief Philosophy and 131 40
religious beliefs
Biz Business 2647 727
Edu Education 205 77
IT Computer and 1085 309
internet informatics
Joy Online fresh, 636 216
interesting info
Med Medical care 155 57
related web sites
Sci Various kinds 119 39
of science
</table>
<tableCaption confidence="0.9712125">
Table 2: A sample set of 19 rules generated
based on the accompanied description of the
</tableCaption>
<figure confidence="0.8211160625">
Chinese web categories.
Art :- EC (Chinese painting)
Belief :- 4A-K (pray) irkfffi (rabbi)
Biz :- gEM (promotion) Sial (real estate)
Edu (client)
2M- (undergraduate) 4ff (supervisor)
IT Mil (campus)
Ag* (version) a (virus)
(firewall) 3/1 (program)
Oil (lantern riddle)
igft (health care) Zh (prescription)
(medical jurisprudence)
Sci e tt (supernaturalism)
XR* (high technology)
Joy
Med
</figure>
<subsectionHeader confidence="0.992961">
4.2 Experiment Paradigm
</subsectionHeader>
<bodyText confidence="0.99991625">
kNN experiments used the plain Euclidean
distance defined by equation (3) as the simi-
larity measure. On each pattern set contain-
ing a varying number of documents, different
values of k ranging from 1 to 29 were tested
and the best results were recorded. Only odd
k were used to ensure that a prediction can
always be made.
SVM experiments used the default built-in
inductive SVM parameter set in SVMlight,
which is described in detail on the web site
and elsewhere (Joachims, 1999).
ARAM experiments employed a standard
set of parameter values of fuzzy ARAM. In
addition, using a voting strategy, 5 ARAM
systems were trained using the same set of
patterns in different orders of presentation
and were combined to yield a final prediction
vector.
To derive domain theory on web page clas-
sification, a varying number (ranging from 10
to 30) of training documents from each cate-
gory were reviewed. A set of domain knowl-
edge consists of 56 rules with about one to 10
rules for each category was generated. Only
positive rules that link keyword antecedents
to positive category consequences were in-
cluded (Table 2).
</bodyText>
<subsectionHeader confidence="0.99712">
4.3 Performance Measures
</subsectionHeader>
<bodyText confidence="0.9999364">
Our experiments adopt the most commonly
used performance measures, including the re-
call, precision, and F1 measures. Recall (R) is
the percentage of the documents for a given
category that are classified correctly. Preci-
sion (P) is the percentage of the predicted
documents for a given category that are clas-
sified correctly. F1 rating is one of the com-
monly used measures to combine R and Pinto
a single rating, defined as
</bodyText>
<equation confidence="0.926544333333333">
2RP (21)
—
(R +P)
</equation>
<bodyText confidence="0.999900529411765">
These scores are calculated for a series of bi-
nary classification experiments, one for each
category. Micro-averaged scores and macro-
averaged scores on the whole corpus are
then produced across the experiments. With
micro-averaging, the performance measures
are produced across the documents by adding
up all the documents counts across the differ-
ent tests, and calculating using these summed
values. With macro-averaging, each category
is assigned with the same weight and per-
formance measures are calculated across the
categories. It is understandable that micro-
averaged scores and macro-averaged scores re-
flect a classifier&apos;s performance on large cate-
gories and small categories respectively (Yang
and Liu, 1999).
</bodyText>
<page confidence="0.988115">
98
</page>
<tableCaption confidence="0.9926655">
Table 3: Predictive performance of the four
classifiers on the Chinese web co us.
</tableCaption>
<table confidence="0.999869833333333">
Category P kNN F1 P SVM Fi
R R
Art .795 .304 .440 .398 .402 .400
Belief .773 .425 .548 .556 .500 .526
Biz .724 .689 .706 .692 .703 .698
Edu .380 .351 .365 .602 .074 .180
IT .309 .333 .321 .394 .307 .345
Joy .381 .236 .291 .462 .255 .328
Med .833 .351 .494 .330 .544 .411
Sci .625 .128 .213 .137 .179 .156
Micro-ave. .584 .482 .528 .523 .521 .522
Macro-ave. .600 .352 .422 .384 .454 .380
ARAM ARAM w/rules
Category P R F1 P R F3,
Art .653 .461 .540 .706 .471 .565
Belief .750 .750 .750 .714 .750 .732
Biz .742 .622 .677 .745 .604 .667
Edu .421 .312 .358 .420 .273 .331
IT .444 .259 .327 .437 .291 .350
Joy .600 .208 .309 .618 .194 .296
Med .421 .421 .421 .448 .456 .452
Sci .292 .179 .222 .409 .231 .295
Micro-ave. .619 .453 .523 .628 .450 .524
Macro-ave. .540 .402 .451 .562 .409 .461
</table>
<sectionHeader confidence="0.518444" genericHeader="evaluation">
4.4 Results and Discussions
</sectionHeader>
<bodyText confidence="0.999910689189189">
Table 3 summarizes the three classifier&apos;s per-
formances on the test corpus in terms of pre-
cision, recall, and F1 measures. The micro-
averaged scores produced by the trio, which
were predominantly determined by the clas-
sifiers&apos; performance on the large categories
(such as Biz, IT, and Joy), were roughly com-
parable. Among the three, kNN seemed to
be marginally better than SVM and ARAM.
Inserting rules into ARAM did not have a
significant impact. This showed that do-
main knowledge was not very useful for cat-
egories that already have a large number of
training examples. The differences in the
macro-averaged scores produced by the three
classifiers, however, were much more signifi-
cant. The macro-averaged F1 score obtained
by ARAM was noticeably better than that of
kNN, which in turn was higher than that of
SVM. This indicates that ARAM (and kNN)
tends to outperform SVM in small categories
that have a smaller number of training pat-
terns.
We are particularly interested in the classi-
fier&apos;s learning ability on small categories. In
certain applications, such as personalized con-
tent delivery, a large pre-labeled training cor-
pus may not be available. Therefore, a classi-
fier&apos;s ability of learning from a small training
pattern set is a major concern. The different
approaches adopted by these three classifiers
in learning categorization knowledge are best
seen in the light of the distinct learning pe-
culiarities they exhibit on the small training
sets.
kNN is a lazy learning method in the sense
that it does not carry out any off-line learning
to generate a particular category knowledge
representation. Instead, kNN performs on-
line scoring to find the training patterns that
are nearest to a test pattern and makes the
decision based on the statistical presumption
that patterns in the same category have simi-
lar feature representations. The presumption
is basically true to most pattern instances.
Thus kNN exhibits a relatively stable perfor-
mance across small and large categories.
SVM identifies optimal separating hyper-
plane (OSH) across the training data points
and makes classification decisions based on
the representative data instances (known as
support vectors). Compared with kNN, SVM
is more computationally efficient during clas-
sification for large-scale training sets. How-
ever, the OSH generated using small train-
ing sets may not be very representative, espe-
cially when the training patterns are sparsely
distributed and there is a relatively narrow
margin between the positive and negative pat-
terns. In our experiments on small train-
ing sets including Art, Belief, Edu, and Sci,
SVM&apos;s performance were generally lower than
those of kNN and ARAM.
ARAM generates recognition categories
from the input training patterns. The incre-
mentally learned rules abstract the major rep-
resentations of the training patterns and elim-
inate minor inconsistencies in the data pat-
terns. During classifying, it works in a sim-
ilar fashion as kNN. The major difference is
that ARAM uses the learned recognition cat-
egories as the similarity-scoring unit whereas
kNN uses the raw in-processed training pat-
terns as the distance-scoring unit. It follows
</bodyText>
<page confidence="0.995469">
99
</page>
<bodyText confidence="0.999636125">
that ARAM is notably more scalable than
Id•IN by its pattern abstraction capability and
therefore is more suitable for handling very
large data sets.
The overall improvement in predictive per-
formance obtained by inserting rules into
ARAM is also of particular interest to us.
ARAM&apos;s performance was more likely to be
improved by rule insertion in categories that
are well defined and have relatively fewer
numbers of training patterns. As long as a
user is able to abstract the category knowl-
edge into certain specific rule representa-
tion, domain knowledge could complement
the limited knowledge acquired through a
small training set quite effectively.
</bodyText>
<sectionHeader confidence="0.997972" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999992857142857">
We would like to thank our colleagues, Jian
Su and Guo-Dong Zhou, for providing the
Chinese segmentation software and Fon-Lin
Lai for his valuable suggestions in designing
the experiment system. In addition, we thank
T. Joachims at the University of Dortmund
for making SVMlight available.
</bodyText>
<sectionHeader confidence="0.99896" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999689695652174">
Suqing Cao, Fuhu Zeng, and Huanguang Cao.
1999. A mathematical model for automatic
Chinese text categorization. Journal of the
China Society for Scientific and Technical In-
formation [in Chinese], 1999(1).
G.A. Carpenter, S. Grossberg, and D.B. Rosen.
1991. Fuzzy ART: Fast stable learning and cat-
egorization of analog patterns by an adaptive
resonance system. Neural Networks, 4:759-771.
C. Cortes and V. Vapnik. 1995. Support vector
networks. Machine learning, 20:273-297.
Belur V. Dasarathy. 1991. Nearest Neighbor (NN)
Norms: NN Pattern Classification Techniques.
IEEE Computer Society Press, Las Alamitos,
California.
Ji He, A.-H. Tan, and Chew-Lim Tan. 2000.
A comparative study on Chinese text catego-
rization methods. In PRICAI&apos;2000 Interna-
tional Workshop on Text and Web Mining, Mel-
bourne, August.
T. Joachims. 1998. Text categorization with sup-
port vector machines: Learning with many rel-
evant features. In Proceedings of the European
Conference on Machine Learning, Springer.
T. Joachims. 1999. Making large-Scales SVM
learning Pracical. Advances in Kernel Methods
- Support Vector Learning. B. Scholkopf, C.
Burges and A. Smola (ed.), MIT Press.
Salton. 1988. Term weighting approaches in au-
tomatic text retrieval. Information Processing
and Management, 24(5):513-523.
A.-H. Tan. 1995. Adaptive resonance associative
map. Neural Networks, 8(3):437-446.
A.-H. Tan. 1997. Cascade ARTMAP: Integrat-
ing neural computation and symbolic knowl-
edge processing. IEEE Transactions on Neural
Networks, 8(2):237-235.
Y. Yang and Pedersen J.P. 1997. A comparative
study on feature selection in text categoriza-
tion. In the Fourteenth International Confer-
ence on Machine Learning (ICML&apos;97), pages
412-420.
Y. Yang and X. Liu. 1999. A re-examination
of text categorization methods. In 22nd An-
nual International ACM SIGIR Conference on
Research and Development in Information Re-
trieval (SIGIR &apos;99), pages 42-49.
Y. Yang. 1994. Expert network: Effective and ef-
ficient learning from human decisions in text
categorization and retrieval. In 17th Annual
International ACM SIGIR Conference on Re-
search and Development in Information Re-
trieval (SIGIR &apos;94).
Y. Yang. 1999. An evaluation of statistical ap-
proaches to text categorization. Journal of In-
formation Retrieval, 1(1/2):67-88.
Lanjuan Zhu. 1987. The theory and experiments
on automatic Chinese documents classification.
Journal of the China Society for Scientific and
Technical Information [in Chinese], 1987(6).
Tao Zou, Ji-Cheng Wang, Yuan Huang, and Fu-
Yan Zhang. 1998. The design and implementa-
tion of an automatic Chinese documents classi-
fication system. Journal for Chinese Informa-
tion [in Chinese], 1998(2).
Tao Zou, Yuan Huang, and Fuyan Zhang. 1999.
Technology of information mining on WWW.
Journal of the China Society for Scientific and
Technical Information [in Chinese], 1999(4).
</reference>
<page confidence="0.990687">
100
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.324543">
<title confidence="0.873255">Machine Learning Methods Chinese Web Page Categorization</title>
<author confidence="0.7602">Ah-Hwee Tan&apos; He&apos;</author>
<affiliation confidence="0.997991">apos;School of Computing, National University of</affiliation>
<address confidence="0.845486333333333">10 Kent Ridge Crescent, Singapore &apos;Kent Ridge Digital 21 Heng Mui Keng Terrace, Singapore</address>
<email confidence="0.994769">ahhweealcrdl.org.sg</email>
<abstract confidence="0.9961194375">This paper reports our evaluation of k Nearest Neighbor (kNN), Support Vector Machines (SVM), and Adaptive Resonance Associative Map (ARAM) on Chinese web page classification. Benchmark experiments based on a Chinese web corpus showed that their predictive performance were roughly comparable although ARAM and kNN slightly outperformed SVM in small categories. In addition, inserting rules into ARAM helped to improve performance, especially for small welldefined categories.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Suqing Cao</author>
<author>Fuhu Zeng</author>
<author>Huanguang Cao</author>
</authors>
<title>A mathematical model for automatic Chinese text categorization.</title>
<date>1999</date>
<journal>Journal of the China Society for Scientific and Technical Information [in Chinese],</journal>
<volume>1999</volume>
<issue>1</issue>
<contexts>
<context position="1306" citStr="Cao et al., 1999" startWordPosition="195" endWordPosition="198">ition, inserting rules into ARAM helped to improve performance, especially for small welldefined categories. 1 Introduction Text categorization refers to the task of automatically assigning one or multiple predefined category labels to free text documents. Whereas an extensive range of methods has been applied to English text categorization, relatively few have been benchmarked for Chinese text categorization. Typical approaches to Chinese text categorization, such as Naive Bayes (NB) (Zhu, 1987), Vector Space Model (VSM) (Zou et al., 1998; Zou et al., 1999) and Linear List Square Fit (LLSF) (Cao et al., 1999; Yang, 1994), have well studied theoretical basis derived from the information retrieval research, but are not known to be the best classifiers (Yang and Liu, 1999; Yang, 1999). In addition, there is a lack of publicly available Chinese corpus for evaluating Chinese text categorization systems. This paper reports our applications of three statistical machine learning methods, namely k Nearest Neighbor system (kNN) (Dasarathy, 1991), Support Vector Machines (SVM) (Cortes and Vapnik, 1995), and Adaptive Resonance Associative Map (ARAM) (Tan, 1995) to Chinese web page categorization. kNN and SVM</context>
</contexts>
<marker>Cao, Zeng, Cao, 1999</marker>
<rawString>Suqing Cao, Fuhu Zeng, and Huanguang Cao. 1999. A mathematical model for automatic Chinese text categorization. Journal of the China Society for Scientific and Technical Information [in Chinese], 1999(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Carpenter</author>
<author>S Grossberg</author>
<author>D B Rosen</author>
</authors>
<title>Fuzzy ART: Fast stable learning and categorization of analog patterns by an adaptive resonance system.</title>
<date>1991</date>
<journal>Neural Networks,</journal>
<pages>4--759</pages>
<contexts>
<context position="11508" citStr="Carpenter et al., 1991" startWordPosition="1864" endWordPosition="1867">Fib output vectors. Learned weight vectors, one for each F2 node, constitute a set of rules that link antecedents to consequences. At any point during the incremental learning process, the system architecture can be translated into a compact set of rules. Similarly, domain knowledge in the form of IF-THEN rules can be inserted into ARAM architecture. The ART modules used in ARAM can be ART 1, which categorizes binary patterns, or analog ART modules such as ART 2, ART 2- A, and fuzzy ART, which categorize both binary and analog patterns. The fuzzy ARAM (Tan, 1995) algorithm based on fuzzy ART (Carpenter et al., 1991) is introduced below. Parameters: Fuzzy ARAM dynamics are determined by the choice parameters a. &gt; 0 and ab &gt; 0; the learning rates fla E [0,1] and 13b E [0, 1]; the vigilance parameters Pa E [0,1] and Pb E [0,1]; and the contribution parameter -y E [0,1]. Weight vectors: Each F2 category node j is associated with two adaptive weight templates wl and w. Initially, all category nodes are uncommitted and all weights equal ones. After a category node is selected for encoding, it becomes committed. Category choice: Given the Ft and Fp input vectors A and B, for each F2 node j, the choice function </context>
</contexts>
<marker>Carpenter, Grossberg, Rosen, 1991</marker>
<rawString>G.A. Carpenter, S. Grossberg, and D.B. Rosen. 1991. Fuzzy ART: Fast stable learning and categorization of analog patterns by an adaptive resonance system. Neural Networks, 4:759-771.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cortes</author>
<author>V Vapnik</author>
</authors>
<title>Support vector networks.</title>
<date>1995</date>
<booktitle>Machine learning,</booktitle>
<pages>20--273</pages>
<contexts>
<context position="1799" citStr="Cortes and Vapnik, 1995" startWordPosition="270" endWordPosition="273">(NB) (Zhu, 1987), Vector Space Model (VSM) (Zou et al., 1998; Zou et al., 1999) and Linear List Square Fit (LLSF) (Cao et al., 1999; Yang, 1994), have well studied theoretical basis derived from the information retrieval research, but are not known to be the best classifiers (Yang and Liu, 1999; Yang, 1999). In addition, there is a lack of publicly available Chinese corpus for evaluating Chinese text categorization systems. This paper reports our applications of three statistical machine learning methods, namely k Nearest Neighbor system (kNN) (Dasarathy, 1991), Support Vector Machines (SVM) (Cortes and Vapnik, 1995), and Adaptive Resonance Associative Map (ARAM) (Tan, 1995) to Chinese web page categorization. kNN and SVM have been reported as the top performing methods for English text categorization (Yang and Liu, 1999). ARAM belongs to a popularly known family of predictive self-organizing neural networks which until recently has not been used for document classification. The trio has been evaluated based on a Chinese corpus consisting of news articles extracted from People&apos;s Daily (He et al., 2000). This article reports the experiments of a much more challenging task in classifying Chinese web pages. </context>
<context position="6862" citStr="Cortes and Vapnik, 1995" startWordPosition="1111" endWordPosition="1114">has the most instances among the k nearest neighbors. Specifically, the class index y(x) assigned to the test pattern x is given by y(x) = argmaxi{n(dj, ci)idjEkNN}, (4) where n(di, ci) is the number of training pattern dj in the k nearest neighbor set that are associated with class The drawback of kNN is the difficulty in deciding a optimal k value. Typically it has to be determined through conducting a series of experiments using different values. 94 3.2 Support Vector Machines Support Vector Machines (SVM) is a relatively new class of machine learning techniques first introduced by Vapnik (Cortes and Vapnik, 1995). Based on the structural risk minimization principle from the computational learning theory, SVM seeks a decision surface to separate the training data points into two classes and makes decisions based on the support vectors that are selected as the only effective elements from the training set. Given a set of linearly separable points S = {xiERnli = 1, 2, ... ,N}, each point xi belongs to one of the two classes, labeled as ye{—1, +1}. A separating hyper-plane divides S into two sides, each side containing points with the same class label only. The separating hyper-plane can be identified by </context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>C. Cortes and V. Vapnik. 1995. Support vector networks. Machine learning, 20:273-297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Belur V Dasarathy</author>
</authors>
<title>Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques.</title>
<date>1991</date>
<publisher>IEEE Computer Society Press,</publisher>
<location>Las Alamitos, California.</location>
<contexts>
<context position="1742" citStr="Dasarathy, 1991" startWordPosition="264" endWordPosition="265">Chinese text categorization, such as Naive Bayes (NB) (Zhu, 1987), Vector Space Model (VSM) (Zou et al., 1998; Zou et al., 1999) and Linear List Square Fit (LLSF) (Cao et al., 1999; Yang, 1994), have well studied theoretical basis derived from the information retrieval research, but are not known to be the best classifiers (Yang and Liu, 1999; Yang, 1999). In addition, there is a lack of publicly available Chinese corpus for evaluating Chinese text categorization systems. This paper reports our applications of three statistical machine learning methods, namely k Nearest Neighbor system (kNN) (Dasarathy, 1991), Support Vector Machines (SVM) (Cortes and Vapnik, 1995), and Adaptive Resonance Associative Map (ARAM) (Tan, 1995) to Chinese web page categorization. kNN and SVM have been reported as the top performing methods for English text categorization (Yang and Liu, 1999). ARAM belongs to a popularly known family of predictive self-organizing neural networks which until recently has not been used for document classification. The trio has been evaluated based on a Chinese corpus consisting of news articles extracted from People&apos;s Daily (He et al., 2000). This article reports the experiments of a much</context>
<context position="5599" citStr="Dasarathy, 1991" startWordPosition="896" endWordPosition="897">t term frequency of keyword w, and M is the number of the keyword features selected. A term weighting method based on inverse document frequency (IDF) (Salton, 1988) and the Linormalization are then applied on the frequency vector to produce the keyword feature in which xi is computed by xi = (1 + log2 tfi) log2 — (2) ni where n is the number of documents in the whole training set, and ni is the number of training documents in which the keyword wi occurs at least once. 3 The Classifiers 3.1 k Nearest Neighbor k Nearest Neighbor (kNN) is a traditional statistical pattern recognition algorithm (Dasarathy, 1991). It has been studied extensively for text categorization (Yang and Liu, 1999). In essence, kNN makes the prediction based on the k training patterns that are closest to the unseen (test) pattern, according to a distance metric. The distance metric that measures the similarity between two normalized patterns can be either a simple LIdistance function or a L2-distance function, such as the plain Euclidean distance defined by D(a, b) = 1/E (ai –b1)2. (3) The class assignment to the test pattern is based on the class assignment of the closest k training patterns. A commonly used method is to labe</context>
</contexts>
<marker>Dasarathy, 1991</marker>
<rawString>Belur V. Dasarathy. 1991. Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques. IEEE Computer Society Press, Las Alamitos, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ji He</author>
<author>A-H Tan</author>
<author>Chew-Lim Tan</author>
</authors>
<title>A comparative study on Chinese text categorization methods.</title>
<date>2000</date>
<booktitle>In PRICAI&apos;2000 International Workshop on Text and Web Mining,</booktitle>
<location>Melbourne,</location>
<contexts>
<context position="2294" citStr="He et al., 2000" startWordPosition="351" endWordPosition="354">ethods, namely k Nearest Neighbor system (kNN) (Dasarathy, 1991), Support Vector Machines (SVM) (Cortes and Vapnik, 1995), and Adaptive Resonance Associative Map (ARAM) (Tan, 1995) to Chinese web page categorization. kNN and SVM have been reported as the top performing methods for English text categorization (Yang and Liu, 1999). ARAM belongs to a popularly known family of predictive self-organizing neural networks which until recently has not been used for document classification. The trio has been evaluated based on a Chinese corpus consisting of news articles extracted from People&apos;s Daily (He et al., 2000). This article reports the experiments of a much more challenging task in classifying Chinese web pages. The Chinese web corpus was created by downloading from various Chinese web sites covering a wide variety of topics. There is a great diversity among the web pages in terms of document length, style, and content. The objectives of our experiments are two-folded. First, we examine and compare the capabilities of these methods in learning categorization knowledge from real-life web documents. Second, we investigate if incorporating domain knowledge derived from the category description can enh</context>
</contexts>
<marker>He, Tan, Tan, 2000</marker>
<rawString>Ji He, A.-H. Tan, and Chew-Lim Tan. 2000. A comparative study on Chinese text categorization methods. In PRICAI&apos;2000 International Workshop on Text and Web Mining, Melbourne, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Text categorization with support vector machines: Learning with many relevant features.</title>
<date>1998</date>
<booktitle>In Proceedings of the European Conference on Machine Learning,</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="8096" citStr="Joachims, 1998" startWordPosition="1336" endWordPosition="1337"> satisfies w-x+b= 0 (5) and yi (w + b)&gt;1 for i = 1, 2, ... , N; where the dot product operation is defined by w • x wixi (6) for vectors w and x. Thus the goal of the SVM learning is to find the optimal separating hyper-plane (OSH) that has the maximal margin to both sides. This can be formularized as: The points that are closest to the OSH are termed support vectors (Fig. 1). The SVM problem can be extended to linearly non-separable case and non-linear case. Various quadratic programming algorithms have been proposed and extensively studied to solve the SVM problem (Cortes and \Tapnik, 1995; Joachims, 1998; Joachims, 1999). During classification, SVM makes decision based on the OSH instead of the whole training set. It simply finds out on which side of the OSH the test pattern is located. Figure 1: Separating hyperplanes (the set of solid lines), optimal separating hyperplane (the bold solid line), and support vectors (data points on the dashed lines). The dashed lines identify the max margin. This property makes SVM highly competitive, compared with other traditional pattern recognition methods, in terms of computational efficiency and predictive accuracy (Yang and Liu, 1999). In recent years,</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>T. Joachims. 1998. Text categorization with support vector machines: Learning with many relevant features. In Proceedings of the European Conference on Machine Learning, Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<date>1999</date>
<booktitle>Making large-Scales SVM learning Pracical. Advances in Kernel Methods - Support Vector Learning.</booktitle>
<editor>B. Scholkopf, C. Burges and A. Smola (ed.),</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="8113" citStr="Joachims, 1999" startWordPosition="1338" endWordPosition="1339">= 0 (5) and yi (w + b)&gt;1 for i = 1, 2, ... , N; where the dot product operation is defined by w • x wixi (6) for vectors w and x. Thus the goal of the SVM learning is to find the optimal separating hyper-plane (OSH) that has the maximal margin to both sides. This can be formularized as: The points that are closest to the OSH are termed support vectors (Fig. 1). The SVM problem can be extended to linearly non-separable case and non-linear case. Various quadratic programming algorithms have been proposed and extensively studied to solve the SVM problem (Cortes and \Tapnik, 1995; Joachims, 1998; Joachims, 1999). During classification, SVM makes decision based on the OSH instead of the whole training set. It simply finds out on which side of the OSH the test pattern is located. Figure 1: Separating hyperplanes (the set of solid lines), optimal separating hyperplane (the bold solid line), and support vectors (data points on the dashed lines). The dashed lines identify the max margin. This property makes SVM highly competitive, compared with other traditional pattern recognition methods, in terms of computational efficiency and predictive accuracy (Yang and Liu, 1999). In recent years, Joachims has don</context>
<context position="17261" citStr="Joachims, 1999" startWordPosition="2916" endWordPosition="2917">alth care) Zh (prescription) (medical jurisprudence) Sci e tt (supernaturalism) XR* (high technology) Joy Med 4.2 Experiment Paradigm kNN experiments used the plain Euclidean distance defined by equation (3) as the similarity measure. On each pattern set containing a varying number of documents, different values of k ranging from 1 to 29 were tested and the best results were recorded. Only odd k were used to ensure that a prediction can always be made. SVM experiments used the default built-in inductive SVM parameter set in SVMlight, which is described in detail on the web site and elsewhere (Joachims, 1999). ARAM experiments employed a standard set of parameter values of fuzzy ARAM. In addition, using a voting strategy, 5 ARAM systems were trained using the same set of patterns in different orders of presentation and were combined to yield a final prediction vector. To derive domain theory on web page classification, a varying number (ranging from 10 to 30) of training documents from each category were reviewed. A set of domain knowledge consists of 56 rules with about one to 10 rules for each category was generated. Only positive rules that link keyword antecedents to positive category conseque</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>T. Joachims. 1999. Making large-Scales SVM learning Pracical. Advances in Kernel Methods - Support Vector Learning. B. Scholkopf, C. Burges and A. Smola (ed.), MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Salton</author>
</authors>
<title>Term weighting approaches in automatic text retrieval.</title>
<date>1988</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>24--5</pages>
<contexts>
<context position="5148" citStr="Salton, 1988" startWordPosition="812" endWordPosition="813">ents. A prior study on several well-known corpora including Reuters-21578 and OHSUMED has proven that CHI statistics generally outperforms other feature ranking measures, such as term strength (TS), document frequency (DF), mutual information (MI), and information gain (IG) (Yang and J.P, 1997). During keyword extraction, the document is first segmented and converted into a keyword frequency vector (tfi, tf2, , tfm), where tfi is the in-document term frequency of keyword w, and M is the number of the keyword features selected. A term weighting method based on inverse document frequency (IDF) (Salton, 1988) and the Linormalization are then applied on the frequency vector to produce the keyword feature in which xi is computed by xi = (1 + log2 tfi) log2 — (2) ni where n is the number of documents in the whole training set, and ni is the number of training documents in which the keyword wi occurs at least once. 3 The Classifiers 3.1 k Nearest Neighbor k Nearest Neighbor (kNN) is a traditional statistical pattern recognition algorithm (Dasarathy, 1991). It has been studied extensively for text categorization (Yang and Liu, 1999). In essence, kNN makes the prediction based on the k training patterns</context>
</contexts>
<marker>Salton, 1988</marker>
<rawString>Salton. 1988. Term weighting approaches in automatic text retrieval. Information Processing and Management, 24(5):513-523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A-H Tan</author>
</authors>
<title>Adaptive resonance associative map.</title>
<date>1995</date>
<journal>Neural Networks,</journal>
<pages>8--3</pages>
<contexts>
<context position="1858" citStr="Tan, 1995" startWordPosition="281" endWordPosition="282"> 1999) and Linear List Square Fit (LLSF) (Cao et al., 1999; Yang, 1994), have well studied theoretical basis derived from the information retrieval research, but are not known to be the best classifiers (Yang and Liu, 1999; Yang, 1999). In addition, there is a lack of publicly available Chinese corpus for evaluating Chinese text categorization systems. This paper reports our applications of three statistical machine learning methods, namely k Nearest Neighbor system (kNN) (Dasarathy, 1991), Support Vector Machines (SVM) (Cortes and Vapnik, 1995), and Adaptive Resonance Associative Map (ARAM) (Tan, 1995) to Chinese web page categorization. kNN and SVM have been reported as the top performing methods for English text categorization (Yang and Liu, 1999). ARAM belongs to a popularly known family of predictive self-organizing neural networks which until recently has not been used for document classification. The trio has been evaluated based on a Chinese corpus consisting of news articles extracted from People&apos;s Daily (He et al., 2000). This article reports the experiments of a much more challenging task in classifying Chinese web pages. The Chinese web corpus was created by downloading from vari</context>
<context position="9384" citStr="Tan, 1995" startWordPosition="1526" endWordPosition="1527">on (Joachims, 1998). His SVMlight system published via http://www-ai.cs.unidortmund.de/FORSCHUNG/VERFAHREN/ SVM_LIGHT/svmlight.eng.html is used in our benchmark experiments. 3.3 Adaptive Resonance Associative Map Adaptive Resonance Associative Map (ARAM) is a class of predictive selforganizing neural networks that performs incremental supervised learning of recognition categories (pattern classes) and multidimensional maps of patterns. An ARAM system can be visualized as two overlapping Adaptive Resonance Theory (ART) modules consisting of two input fields Ff and Ft with an F2 category field (Tan, 1995; Tan, 1997) (Fig. 2). For classification problems, the IT field serves as the input field containing the document feature vector and the FP field serves as the output field containing the class prediction vector. The F2 field contains the activities of the recognition categories that are used to encode the patterns. (7) subject to yi(w • xi + b)&gt;1 minimize lw • w 95 Figure 2: The Adaptive Resonance Associative Map architecture When performing classification tasks, ARAM formulates recognition categories of input patterns, and associates each category with its respective prediction. During lear</context>
<context position="11454" citStr="Tan, 1995" startWordPosition="1857" endWordPosition="1858">ory associating the Ff patterns with the Fib output vectors. Learned weight vectors, one for each F2 node, constitute a set of rules that link antecedents to consequences. At any point during the incremental learning process, the system architecture can be translated into a compact set of rules. Similarly, domain knowledge in the form of IF-THEN rules can be inserted into ARAM architecture. The ART modules used in ARAM can be ART 1, which categorizes binary patterns, or analog ART modules such as ART 2, ART 2- A, and fuzzy ART, which categorize both binary and analog patterns. The fuzzy ARAM (Tan, 1995) algorithm based on fuzzy ART (Carpenter et al., 1991) is introduced below. Parameters: Fuzzy ARAM dynamics are determined by the choice parameters a. &gt; 0 and ab &gt; 0; the learning rates fla E [0,1] and 13b E [0, 1]; the vigilance parameters Pa E [0,1] and Pb E [0,1]; and the contribution parameter -y E [0,1]. Weight vectors: Each F2 category node j is associated with two adaptive weight templates wl and w. Initially, all category nodes are uncommitted and all weights equal ones. After a category node is selected for encoding, it becomes committed. Category choice: Given the Ft and Fp input vec</context>
</contexts>
<marker>Tan, 1995</marker>
<rawString>A.-H. Tan. 1995. Adaptive resonance associative map. Neural Networks, 8(3):437-446.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A-H Tan</author>
</authors>
<title>Cascade ARTMAP: Integrating neural computation and symbolic knowledge processing.</title>
<date>1997</date>
<journal>IEEE Transactions on Neural Networks,</journal>
<pages>8--2</pages>
<contexts>
<context position="9396" citStr="Tan, 1997" startWordPosition="1528" endWordPosition="1529">s, 1998). His SVMlight system published via http://www-ai.cs.unidortmund.de/FORSCHUNG/VERFAHREN/ SVM_LIGHT/svmlight.eng.html is used in our benchmark experiments. 3.3 Adaptive Resonance Associative Map Adaptive Resonance Associative Map (ARAM) is a class of predictive selforganizing neural networks that performs incremental supervised learning of recognition categories (pattern classes) and multidimensional maps of patterns. An ARAM system can be visualized as two overlapping Adaptive Resonance Theory (ART) modules consisting of two input fields Ff and Ft with an F2 category field (Tan, 1995; Tan, 1997) (Fig. 2). For classification problems, the IT field serves as the input field containing the document feature vector and the FP field serves as the output field containing the class prediction vector. The F2 field contains the activities of the recognition categories that are used to encode the patterns. (7) subject to yi(w • xi + b)&gt;1 minimize lw • w 95 Figure 2: The Adaptive Resonance Associative Map architecture When performing classification tasks, ARAM formulates recognition categories of input patterns, and associates each category with its respective prediction. During learning, given </context>
</contexts>
<marker>Tan, 1997</marker>
<rawString>A.-H. Tan. 1997. Cascade ARTMAP: Integrating neural computation and symbolic knowledge processing. IEEE Transactions on Neural Networks, 8(2):237-235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yang</author>
<author>J P Pedersen</author>
</authors>
<title>A comparative study on feature selection in text categorization.</title>
<date>1997</date>
<booktitle>In the Fourteenth International Conference on Machine Learning (ICML&apos;97),</booktitle>
<pages>412--420</pages>
<marker>Yang, Pedersen, 1997</marker>
<rawString>Y. Yang and Pedersen J.P. 1997. A comparative study on feature selection in text categorization. In the Fourteenth International Conference on Machine Learning (ICML&apos;97), pages 412-420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yang</author>
<author>X Liu</author>
</authors>
<title>A re-examination of text categorization methods.</title>
<date>1999</date>
<booktitle>In 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;99),</booktitle>
<pages>42--49</pages>
<contexts>
<context position="1470" citStr="Yang and Liu, 1999" startWordPosition="223" endWordPosition="226"> of automatically assigning one or multiple predefined category labels to free text documents. Whereas an extensive range of methods has been applied to English text categorization, relatively few have been benchmarked for Chinese text categorization. Typical approaches to Chinese text categorization, such as Naive Bayes (NB) (Zhu, 1987), Vector Space Model (VSM) (Zou et al., 1998; Zou et al., 1999) and Linear List Square Fit (LLSF) (Cao et al., 1999; Yang, 1994), have well studied theoretical basis derived from the information retrieval research, but are not known to be the best classifiers (Yang and Liu, 1999; Yang, 1999). In addition, there is a lack of publicly available Chinese corpus for evaluating Chinese text categorization systems. This paper reports our applications of three statistical machine learning methods, namely k Nearest Neighbor system (kNN) (Dasarathy, 1991), Support Vector Machines (SVM) (Cortes and Vapnik, 1995), and Adaptive Resonance Associative Map (ARAM) (Tan, 1995) to Chinese web page categorization. kNN and SVM have been reported as the top performing methods for English text categorization (Yang and Liu, 1999). ARAM belongs to a popularly known family of predictive self-</context>
<context position="5677" citStr="Yang and Liu, 1999" startWordPosition="906" endWordPosition="909">selected. A term weighting method based on inverse document frequency (IDF) (Salton, 1988) and the Linormalization are then applied on the frequency vector to produce the keyword feature in which xi is computed by xi = (1 + log2 tfi) log2 — (2) ni where n is the number of documents in the whole training set, and ni is the number of training documents in which the keyword wi occurs at least once. 3 The Classifiers 3.1 k Nearest Neighbor k Nearest Neighbor (kNN) is a traditional statistical pattern recognition algorithm (Dasarathy, 1991). It has been studied extensively for text categorization (Yang and Liu, 1999). In essence, kNN makes the prediction based on the k training patterns that are closest to the unseen (test) pattern, according to a distance metric. The distance metric that measures the similarity between two normalized patterns can be either a simple LIdistance function or a L2-distance function, such as the plain Euclidean distance defined by D(a, b) = 1/E (ai –b1)2. (3) The class assignment to the test pattern is based on the class assignment of the closest k training patterns. A commonly used method is to label the test pattern with the class that has the most instances among the k near</context>
<context position="8678" citStr="Yang and Liu, 1999" startWordPosition="1426" endWordPosition="1429">tes and \Tapnik, 1995; Joachims, 1998; Joachims, 1999). During classification, SVM makes decision based on the OSH instead of the whole training set. It simply finds out on which side of the OSH the test pattern is located. Figure 1: Separating hyperplanes (the set of solid lines), optimal separating hyperplane (the bold solid line), and support vectors (data points on the dashed lines). The dashed lines identify the max margin. This property makes SVM highly competitive, compared with other traditional pattern recognition methods, in terms of computational efficiency and predictive accuracy (Yang and Liu, 1999). In recent years, Joachims has done much research on the application of SVM to text categorization (Joachims, 1998). His SVMlight system published via http://www-ai.cs.unidortmund.de/FORSCHUNG/VERFAHREN/ SVM_LIGHT/svmlight.eng.html is used in our benchmark experiments. 3.3 Adaptive Resonance Associative Map Adaptive Resonance Associative Map (ARAM) is a class of predictive selforganizing neural networks that performs incremental supervised learning of recognition categories (pattern classes) and multidimensional maps of patterns. An ARAM system can be visualized as two overlapping Adaptive Re</context>
<context position="19073" citStr="Yang and Liu, 1999" startWordPosition="3206" endWordPosition="3209">gory. Micro-averaged scores and macroaveraged scores on the whole corpus are then produced across the experiments. With micro-averaging, the performance measures are produced across the documents by adding up all the documents counts across the different tests, and calculating using these summed values. With macro-averaging, each category is assigned with the same weight and performance measures are calculated across the categories. It is understandable that microaveraged scores and macro-averaged scores reflect a classifier&apos;s performance on large categories and small categories respectively (Yang and Liu, 1999). 98 Table 3: Predictive performance of the four classifiers on the Chinese web co us. Category P kNN F1 P SVM Fi R R Art .795 .304 .440 .398 .402 .400 Belief .773 .425 .548 .556 .500 .526 Biz .724 .689 .706 .692 .703 .698 Edu .380 .351 .365 .602 .074 .180 IT .309 .333 .321 .394 .307 .345 Joy .381 .236 .291 .462 .255 .328 Med .833 .351 .494 .330 .544 .411 Sci .625 .128 .213 .137 .179 .156 Micro-ave. .584 .482 .528 .523 .521 .522 Macro-ave. .600 .352 .422 .384 .454 .380 ARAM ARAM w/rules Category P R F1 P R F3, Art .653 .461 .540 .706 .471 .565 Belief .750 .750 .750 .714 .750 .732 Biz .742 .622</context>
</contexts>
<marker>Yang, Liu, 1999</marker>
<rawString>Y. Yang and X. Liu. 1999. A re-examination of text categorization methods. In 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;99), pages 42-49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yang</author>
</authors>
<title>Expert network: Effective and efficient learning from human decisions in text categorization and retrieval.</title>
<date>1994</date>
<booktitle>In 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;94).</booktitle>
<contexts>
<context position="1319" citStr="Yang, 1994" startWordPosition="199" endWordPosition="200">ules into ARAM helped to improve performance, especially for small welldefined categories. 1 Introduction Text categorization refers to the task of automatically assigning one or multiple predefined category labels to free text documents. Whereas an extensive range of methods has been applied to English text categorization, relatively few have been benchmarked for Chinese text categorization. Typical approaches to Chinese text categorization, such as Naive Bayes (NB) (Zhu, 1987), Vector Space Model (VSM) (Zou et al., 1998; Zou et al., 1999) and Linear List Square Fit (LLSF) (Cao et al., 1999; Yang, 1994), have well studied theoretical basis derived from the information retrieval research, but are not known to be the best classifiers (Yang and Liu, 1999; Yang, 1999). In addition, there is a lack of publicly available Chinese corpus for evaluating Chinese text categorization systems. This paper reports our applications of three statistical machine learning methods, namely k Nearest Neighbor system (kNN) (Dasarathy, 1991), Support Vector Machines (SVM) (Cortes and Vapnik, 1995), and Adaptive Resonance Associative Map (ARAM) (Tan, 1995) to Chinese web page categorization. kNN and SVM have been re</context>
</contexts>
<marker>Yang, 1994</marker>
<rawString>Y. Yang. 1994. Expert network: Effective and efficient learning from human decisions in text categorization and retrieval. In 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;94).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yang</author>
</authors>
<title>An evaluation of statistical approaches to text categorization.</title>
<date>1999</date>
<journal>Journal of Information Retrieval,</journal>
<pages>1--1</pages>
<contexts>
<context position="1483" citStr="Yang, 1999" startWordPosition="227" endWordPosition="228">signing one or multiple predefined category labels to free text documents. Whereas an extensive range of methods has been applied to English text categorization, relatively few have been benchmarked for Chinese text categorization. Typical approaches to Chinese text categorization, such as Naive Bayes (NB) (Zhu, 1987), Vector Space Model (VSM) (Zou et al., 1998; Zou et al., 1999) and Linear List Square Fit (LLSF) (Cao et al., 1999; Yang, 1994), have well studied theoretical basis derived from the information retrieval research, but are not known to be the best classifiers (Yang and Liu, 1999; Yang, 1999). In addition, there is a lack of publicly available Chinese corpus for evaluating Chinese text categorization systems. This paper reports our applications of three statistical machine learning methods, namely k Nearest Neighbor system (kNN) (Dasarathy, 1991), Support Vector Machines (SVM) (Cortes and Vapnik, 1995), and Adaptive Resonance Associative Map (ARAM) (Tan, 1995) to Chinese web page categorization. kNN and SVM have been reported as the top performing methods for English text categorization (Yang and Liu, 1999). ARAM belongs to a popularly known family of predictive self-organizing ne</context>
</contexts>
<marker>Yang, 1999</marker>
<rawString>Y. Yang. 1999. An evaluation of statistical approaches to text categorization. Journal of Information Retrieval, 1(1/2):67-88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lanjuan Zhu</author>
</authors>
<title>The theory and experiments on automatic Chinese documents classification.</title>
<date>1987</date>
<journal>Journal of the China Society for Scientific and Technical Information [in Chinese],</journal>
<volume>1987</volume>
<issue>6</issue>
<contexts>
<context position="1191" citStr="Zhu, 1987" startWordPosition="175" endWordPosition="176">rformance were roughly comparable although ARAM and kNN slightly outperformed SVM in small categories. In addition, inserting rules into ARAM helped to improve performance, especially for small welldefined categories. 1 Introduction Text categorization refers to the task of automatically assigning one or multiple predefined category labels to free text documents. Whereas an extensive range of methods has been applied to English text categorization, relatively few have been benchmarked for Chinese text categorization. Typical approaches to Chinese text categorization, such as Naive Bayes (NB) (Zhu, 1987), Vector Space Model (VSM) (Zou et al., 1998; Zou et al., 1999) and Linear List Square Fit (LLSF) (Cao et al., 1999; Yang, 1994), have well studied theoretical basis derived from the information retrieval research, but are not known to be the best classifiers (Yang and Liu, 1999; Yang, 1999). In addition, there is a lack of publicly available Chinese corpus for evaluating Chinese text categorization systems. This paper reports our applications of three statistical machine learning methods, namely k Nearest Neighbor system (kNN) (Dasarathy, 1991), Support Vector Machines (SVM) (Cortes and Vapni</context>
</contexts>
<marker>Zhu, 1987</marker>
<rawString>Lanjuan Zhu. 1987. The theory and experiments on automatic Chinese documents classification. Journal of the China Society for Scientific and Technical Information [in Chinese], 1987(6).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Zou</author>
<author>Ji-Cheng Wang</author>
<author>Yuan Huang</author>
<author>FuYan Zhang</author>
</authors>
<title>The design and implementation of an automatic Chinese documents classification system. Journal for Chinese Information [in Chinese],</title>
<date>1998</date>
<contexts>
<context position="1235" citStr="Zou et al., 1998" startWordPosition="181" endWordPosition="184">ough ARAM and kNN slightly outperformed SVM in small categories. In addition, inserting rules into ARAM helped to improve performance, especially for small welldefined categories. 1 Introduction Text categorization refers to the task of automatically assigning one or multiple predefined category labels to free text documents. Whereas an extensive range of methods has been applied to English text categorization, relatively few have been benchmarked for Chinese text categorization. Typical approaches to Chinese text categorization, such as Naive Bayes (NB) (Zhu, 1987), Vector Space Model (VSM) (Zou et al., 1998; Zou et al., 1999) and Linear List Square Fit (LLSF) (Cao et al., 1999; Yang, 1994), have well studied theoretical basis derived from the information retrieval research, but are not known to be the best classifiers (Yang and Liu, 1999; Yang, 1999). In addition, there is a lack of publicly available Chinese corpus for evaluating Chinese text categorization systems. This paper reports our applications of three statistical machine learning methods, namely k Nearest Neighbor system (kNN) (Dasarathy, 1991), Support Vector Machines (SVM) (Cortes and Vapnik, 1995), and Adaptive Resonance Associative</context>
</contexts>
<marker>Zou, Wang, Huang, Zhang, 1998</marker>
<rawString>Tao Zou, Ji-Cheng Wang, Yuan Huang, and FuYan Zhang. 1998. The design and implementation of an automatic Chinese documents classification system. Journal for Chinese Information [in Chinese], 1998(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Zou</author>
<author>Yuan Huang</author>
<author>Fuyan Zhang</author>
</authors>
<date>1999</date>
<booktitle>Technology of information mining on WWW. Journal of the China Society for Scientific and Technical Information [in Chinese],</booktitle>
<contexts>
<context position="1254" citStr="Zou et al., 1999" startWordPosition="185" endWordPosition="188">slightly outperformed SVM in small categories. In addition, inserting rules into ARAM helped to improve performance, especially for small welldefined categories. 1 Introduction Text categorization refers to the task of automatically assigning one or multiple predefined category labels to free text documents. Whereas an extensive range of methods has been applied to English text categorization, relatively few have been benchmarked for Chinese text categorization. Typical approaches to Chinese text categorization, such as Naive Bayes (NB) (Zhu, 1987), Vector Space Model (VSM) (Zou et al., 1998; Zou et al., 1999) and Linear List Square Fit (LLSF) (Cao et al., 1999; Yang, 1994), have well studied theoretical basis derived from the information retrieval research, but are not known to be the best classifiers (Yang and Liu, 1999; Yang, 1999). In addition, there is a lack of publicly available Chinese corpus for evaluating Chinese text categorization systems. This paper reports our applications of three statistical machine learning methods, namely k Nearest Neighbor system (kNN) (Dasarathy, 1991), Support Vector Machines (SVM) (Cortes and Vapnik, 1995), and Adaptive Resonance Associative Map (ARAM) (Tan, 1</context>
</contexts>
<marker>Zou, Huang, Zhang, 1999</marker>
<rawString>Tao Zou, Yuan Huang, and Fuyan Zhang. 1999. Technology of information mining on WWW. Journal of the China Society for Scientific and Technical Information [in Chinese], 1999(4).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>