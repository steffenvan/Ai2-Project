<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000124">
<title confidence="0.9994295">
Bigram HMM with Context Distribution Clustering for Unsupervised
Chinese Part-of-Speech tagging
</title>
<author confidence="0.997742">
Lidan Zhang
</author>
<affiliation confidence="0.9989785">
Department of Computer Science
the University of Hong Kong
</affiliation>
<address confidence="0.406647">
Hong Kong
</address>
<email confidence="0.984368">
lzhang@cs.hku.hk
</email>
<author confidence="0.982971">
Kwok-Ping Chan
</author>
<affiliation confidence="0.9988725">
Department of Computer Science
the University of Hong Kong
</affiliation>
<address confidence="0.409663">
Hong Kong
</address>
<email confidence="0.992541">
kpchan@cs.hku.hk
</email>
<sectionHeader confidence="0.99732" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999909409090909">
This paper presents an unsupervised
Chinese Part-of-Speech (POS) tagging
model based on the first-order HMM.
Unlike the conventional HMM, the num-
ber of hidden states is not fixed and will
be increased to fit the training data. In
favor of sparse distribution, the Dirich-
let priors are introduced with variational
inference method. To reduce the emis-
sion variables, words are represented by
their contexts and clustered based on the
distributional similarities between con-
texts. Experiment results show the out-
put state sequence of HMM are highly
correlated to the latent annotations of
gold POS tags, in context of clustering
similarity measures. The other exper-
iments on a real application, unsuper-
vised dependency parsing, reveal that the
output sequence can replace the manu-
ally annotated tags without loss of accu-
racies.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999947758241759">
Recently latent variable model has shown great
potential in recovering the underlying structures.
For example, the task of POS tagging is to re-
cover the appropriate sequence structure given
the input word sequence (Goldwater and Grif-
fiths, 2007). One of the most popular exam-
ple of latent models is Hidden Markov Model
(HMM), which has been extensively studied for
many years (Rabiner, 1989). The key problem
of HMM is how to find an optimal hidden state
number and the topology appropriately.
In most cases, the topology of HMM is pre-
defined by exploiting the domain or empirical
knowledge. This topology will be fixed during
the whole process. Therefore how to select the
optimal topology for a certain application or a set
of training data is still a problem, because many
researches show that varying the size of the state
space greatly affects the performance of HMM.
Generally there are two ways to adjust the state
number: top-down and bottom-up methods. In
the bottom-up methods (Brand, 1999), the state
number is initialized with a relatively large num-
ber. During the training, the states are merged or
trimmed and ended with a small set of states. On
the other hand, the top-down methods (Siddiqi et
al., 2007) start from a small state set and split one
or some states until no further improvement can
be obtained. The bottom-up approaches require
huge computational cost in deciding the states to
be merged, which makes it impractical for appli-
cations with large state space. In this paper, we
focus on the latter approaches.
Another problem in HMM is that EM algo-
rithm might yield local maximum value. John-
son (2007) points out that training HMM with
EM gives poor results because it leads to a fairly
flat distribution of hidden states when the empiri-
cal distribution is highly skewed. A multinomial
prior, which favors sparse distribution, is a good
choice for natural language tasks. In this paper,
we proposed a new procedure for inferring the
HMM topology and estimating its parameters si-
multaneously. Gibbs sampling has been used in
infinite HMM (iHMM) (Beal et al., 2001; Fox et
al., 2008; Van Gael et al., 2008) for inference.
Unfortunately Gibbs sampling is slow and diffi-
cult to be converged. In this paper, we proposed
the variational Bayesian inference for the adap-
tive HMM model with Dirichlet prior. It involves
a modification to the Baum-Welch algorithm. In
each iteration, we replaced only one hidden state
with two new states until convergence.
To reduce the number of observation vari-
ables, the words are pre-clustered and repre-
sented by the exemplar within the same clus-
ter. It is a one-to-many clustering, because the
same word play different roles under different
contexts. We evaluate the similarity between the
distribution of contexts, with the assumption that
the context distribution implies syntactic pattern
of the given word (Zelling, 1968; Weeds and
Weir, 2003). With this clustering, more contex-
tual information can be considered without in-
creasing the model complexity. A relatively sim-
ple model is important for unsupervised task in
terms of computational burden and data sparse-
ness. This is the reason why we do not increase
the order of HMM(Kaji and Kitsuregawa, 2008;
Headden et al., 2008).
With unsupervised algorithms, there are two
aspects to be evaluated (Van Gael et al., 2009).
Fist one is how good the outcome clusters are.
We compare the HMM results with the manu-
ally POS tags and report the similarity measures
based on information theory. On the other hand,
we test how good the outputs act as an interme-
diate results. In many natural language tasks, the
inputs are word class, not the actual lexical item,
for reason of sparsity. In this paper, we choose
the unsupervised dependency parsing as the ap-
plication to investigate whether our clusters can
replace the manual labeled tags or not.
The paper is organized as below: in section 2,
we describe the definition of HMM and its vari-
ance inference. We present our dynamic HMM
in section 3. To overcome the context limitation
in the first-order HMM, we present our distribu-
tional similarity clustering in section 4. In sec-
tion 5, we reported the results of the mentioned
experiments while section 6 concludes the paper.
</bodyText>
<sectionHeader confidence="0.99672" genericHeader="introduction">
2 Terminology
</sectionHeader>
<bodyText confidence="0.9997804">
The task of POS tagging is to assign a syntac-
tic category sequence to the input words. Let
S be defined as the set of all possible hidden
states, which are expected to be highly correlated
to POS tags.  represents the set of all words.
Therefore the task is to find a sequence of tag
sequence S = s1...sn ∈ S given a sequence of
words (i.e. a sentence, W = w1...wn ∈ ). The
optimal tags is to maximize the conditional prob-
ability p(S |W), which is equal to:
</bodyText>
<equation confidence="0.99753125">
max p(S |W) = max p(S)p(W|S )
S S
= max p(W, S )
S (1)
</equation>
<bodyText confidence="0.998092714285714">
In this paper,we consider the first-order HMM,
where the POS tags are regarded as hidden states
and words as observed variables. According to
the Markov assumption, the best sequence of
tags S for a given sequence of words W is done
by maximizing (with s0 = 0) the joint probabil-
ity:
</bodyText>
<equation confidence="0.991229">
p(si|si−1)p(wi|si) (2)
</equation>
<bodyText confidence="0.998956">
where w0 is the special boundary marker of sen-
tences.
</bodyText>
<subsectionHeader confidence="0.988823">
2.1 Variational Inference for HMM
</subsectionHeader>
<bodyText confidence="0.999877333333333">
Let the HMM be modeled with parameter  =
(A, B, ), where A = {aij} = {P(st = j|st−1 = i)}
is the transition matrix governing the dynamic of
the HMM. B = {bt(i)} = {P(wt = i|st}) is the state
emission matrix and  = {i} = {P(s1 = i)} as-
signs the initial probabilities to all hidden states.
In favor of sparse distributions, a natural choice
is to encode Dirichlet prior into parameters p().
In particular, we have:
</bodyText>
<equation confidence="0.998444">
N
p(A) = H Dir({ai1, ..., aiN} |u(A))
i=1
p(B) = N Dir({bi1, ..., biN} |u(B)) (3)
i=1
p() = Dir({1, ..., N} |u(70)
n
H
i=1
p(W, S) =
</equation>
<bodyText confidence="0.8953175">
where the Dirichlet distribution of order N with
hyperparameter vector u is defined as:
</bodyText>
<equation confidence="0.995059">
Dir(x|u) =
</equation>
<bodyText confidence="0.889815333333333">
In this paper, we consider the symmetric
Dirichlet distribution with a fixed length, i.e.
u = [∑Ni=1 ui/N, ..., ∑Ni=1 ui/N].
In the Bayesian framework, the model param-
eters are also regarded as hidden variables. The
marginal likelihood can be calculated by sum-
ming up all hidden variables. According to the
Jensen’s inequality, the lower bound of marginal
likelihood is defined as:
</bodyText>
<equation confidence="0.997586714285714">
∫ ∑
ln p(W) = ln p()p(W, S |)d
S
∫ ∑ q(, S) ln p(W, S, )
&gt;_ d
S q(,S)
= T
</equation>
<bodyText confidence="0.9981375">
Generally, Variational Bayesian Inference
aims to find a tractable distribution q(, s) that
maximizes the lower bound T . To make infer-
ence flexible, the posterior distribution can be
assumed to be factorized according to the mean-
field assumption. We have:
</bodyText>
<equation confidence="0.983632">
p(W, S, ) 7z� q(S, ) = qe()qS (S) (6)
</equation>
<bodyText confidence="0.9997495">
Then an extension of EM algorithm (called
Baum-Welch algorithm) can be used to alter-
nately optimize the qS and qe. The EM process
is described as follows:
</bodyText>
<listItem confidence="0.9740646">
• E Step: Forward-Backward algorithm to
find the optimal state sequence S(t+1) =
arg max p(S (t)|W, (t))
• M Step: The parameters (t+1) are re-
estimated given the optimal state S(t+1)
</listItem>
<bodyText confidence="0.9996988">
The E and M steps are repeated until a conver-
gence criteria is satisfied. Beal (2003) proved
that only need to do minor modifications in M
step (in 1) is needed, when Dirichlet prior is in-
troduced.
</bodyText>
<sectionHeader confidence="0.987437" genericHeader="method">
3 Adaptive Hidden Markov Model
</sectionHeader>
<bodyText confidence="0.941266714285714">
As aforementioned, the key problem of HMM is
how to initialize the number of hidden states and
select the topology of HMM. In this paper, we
use the top-down scheme: starting from a small
number of states, only one state is chosen in each
step and splitted into two new states. This binary
split scheme is described in Figure 1.
Algorithm 1 Outline of our adpative HMM
Initialization: Initialize: t = 0, N(t)
repeat
Optimization: Find the optimal parameters
for current Nt
Candidate Generation: Split states and
generate candidate HMMs
Candidate Selection: Select the optimal
HMM from the candidates, whose hidden
state number is Nt+1
until No further improvement can be achieved
after splitting
In the following, we will discuss the details of
each step one by one.
</bodyText>
<subsectionHeader confidence="0.999303">
3.1 Candidate Generation
</subsectionHeader>
<bodyText confidence="0.9997105">
Let N(t) represent the number of hidden states at
timestep t. The problem is how to choose the
states for splitting. A straightforward way is to
select all states and generate N(t) + 1 candidate
HMMs, including the original un-splitted one.
Obviously the exhaustive search is inefficient es-
pecially for large state space. To make the algo-
rithm more efficient, some constraints must be
set to narrow the search space.
Intuitively entropy implies uncertainty. So
hidden states with large conditional entropies are
desirable to be splitted. We can define the con-
ditional entropy of the state sequences given ob-
servation W as:
</bodyText>
<subsubsectionHeader confidence="0.400608">
[P(S |W)log P(S |W)] (8)
</subsubsectionHeader>
<bodyText confidence="0.9985128">
Our assumption is the state to be splitted must
be the states sequence with the highest condi-
tional entropy value. This entropy can be recur-
sively calculated with complexity O(N2T) (Her-
nando et al., 2005). Here N is the number of
</bodyText>
<equation confidence="0.997756852941176">
r(∑Ni=1 ui) N x`.``−1. (4)
∏Ni=1 r(ui) ∏ t
i=1
(5)
∑
H(S |W) = −
S
A(t+1) = {a(t+1)
i j } = exp[(( ij
O( )) − (
B(t+1) = {b(t+1)} = exp[W(W(B)) − (
ik ik
7r(t+1) = {7r(t+1)
i } = exp[(W(&amp;quot;)
i ) − (
W (A))]
))] ; W(A) i j= u(A) j+ Eq(s)[nij]
W(B)
ik )] ; W(B)
ik = u(B)
k + Eq(s)[n′ik]
Wj )]; W(&amp;quot;)
(&amp;quot;) i = u(&amp;quot;)
i + Eq(s)[n′′
i ]
N
L
j=1
T
Z
k=1
N
i=1
(7)
</equation>
<figureCaption confidence="0.996263333333333">
Figure 1: Parameters update equations in M-step. Here E is the expectation with respect to the
model parameters. And nij is the expected number of transition from state si to state sj; n′ik is the
expected number of times word wk occurs with state si; n′′
</figureCaption>
<bodyText confidence="0.989581">
i is the occurrence of s0 = i
states and T is the length of sequence. Using
this entropy constraint, the size of candidate state
set is always smaller than the minimal value be-
tween N and T.
</bodyText>
<subsectionHeader confidence="0.999362">
3.2 Candidate Selection
</subsectionHeader>
<bodyText confidence="0.9989701875">
Given the above candidate set, the parameters of
each HMM are to be updated. Note that we just
update the parameters related to the split state,
whilst keep the others fixed. Suppose the i-th
hidden state is replaced by two new states. First
the transition matrix is enlarged from N(t) x N(t)
dimension to (N(t) + 1) x (N(t) + 1) dimension,
by inserting one column and row after the i-th
column and row. In the process of update, we
only change the items in the two (i and i + 1)
rows and columns. The other elements irrelevant
to the split state are not involved in the update
procedure. Similarly EM algorithm is used to
find the optimal parameters. Note that most of
the calculations can be skipped by making use
of the forward and backward probability matrix
achieved in the previous step. Therefore the con-
vergence is fast.
Given the candidate selection, we can use a
modified Baum-Welch algorithm to find optimal
states and parameters. Here we use the algorithm
in (Siddiqi et al., 2007) with some modifications
for the Dirichlet prior. In particular, in E step,
we follow their partial Forward-Background al-
gorithm to calculate E[nij] and E[n′ik], if si or sj
is candidate state to be splitted. Then in M-step,
only rows and columns related to the candidate
state are updated according to equation (7). The
detailed description is given as appendix.
Finally it is natural to use variational bound
of marginal likelihood in equation (5) for model
scoring and convergence criterion.
</bodyText>
<sectionHeader confidence="0.998118" genericHeader="method">
4 Distributional Clustering
</sectionHeader>
<bodyText confidence="0.999910272727273">
To reduce the number of observation variables,
the words are clustered before HMM training.
Intuitively, the words share the similar contexts
have similar syntactic property. The categories
of many words are varied in different contexts.
In other words, the cluster of a given word is
heavily dependent on the context it appears. For
example, 发现 can be a noun (meaning: discov-
ery) if it acts as an object, or a verb (meaning: to
discover) if it is followed with a noun. Further-
more the introduction of context can overcome
the limited context in the first-order HMM.
The underlying hypothesis of clustering based
on distributional similarity is that the words oc-
curring in similar contexts behave as similar syn-
tactic roles. In this work, the context of a word
is a trigram consist of the word immediately pre-
ceding the target and the word immediately fol-
lowing it. The similarity between two words
is measured by Pointwise Mutual Information
(PMI) between the context pair in which they ap-
pear:
</bodyText>
<equation confidence="0.952813">
PMI(wi, wj) = log P(ci, cj) (9)
P(ci)P(cj)
</equation>
<bodyText confidence="0.99988">
where ci denotes the context of wi. P(ci, cj) is
the co-occurrence probability of ci and cj, and
</bodyText>
<equation confidence="0.843513">
P(ci) = Ej P(ci, cj) is the occurrence probabil-
</equation>
<bodyText confidence="0.999395733333334">
ity of ci. In our experiments, the cutoff context
count is set to 10, which means the frequency
less than the threshold is labeled as the unknown
context.
The above distributional similarity can be
used as a distance measure. Hence any cluster-
ing algorithm can be adopted. In this paper, we
use the affinity propagation algorithm (Frey and
Dueck, 2007). Its parameter ‘dampfact’ is set
to 0.9, and the other parameters are set as de-
fault. After running the clustering algorithm, the
contexts are clustered into 1869 clusterings. It
is noted that one word might be classified into
several clusters , if its contexts are clustered into
several clusters.
</bodyText>
<sectionHeader confidence="0.999792" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999994647058824">
As aforementioned, the outputs of our HMM
model are evaluated in two ways, clustering met-
ric and parsing performance. The data used in all
experiments are the Chinese data set in CoNLL-
2007 shared task. The number of tokens in
training, development and test sets are 609,060,
49,620 and 73,153 respectively. We use all train-
ing data set for training the model, whose maxi-
mum length is 242.
The hyper parameters of Dirichlet priors are
initialized in a homogeneous way. The initial
hidden state is set to 40 in all experiments. After
several iterations, the hidden states number con-
verged to 247, which is much larger than the size
of the manually defined POS tags. Our expec-
tation is the refinement variables can reveal the
deep granularity of the POS tags.
</bodyText>
<subsectionHeader confidence="0.998423">
5.1 Clustering Evaluation
</subsectionHeader>
<bodyText confidence="0.999988625">
In this paper, we use information theoretic based
metrics to quantify the information shared by
two clusters. The most common information-
based clustering metric is the variational of In-
formation (VI)(Meil˘a, 2007). Given the cluster-
ing result Cr and the gold clustering Cg, VI sums
up the conditional entropy of one cluster distri-
bution given the other one:
</bodyText>
<equation confidence="0.998465">
VI(Cr, Cg) = H(Cr) + H(Cg) − 2I(Cr, Cg) (10)
= H(Cr|Cg) + H(Cg|Cr)
</equation>
<bodyText confidence="0.999350222222222">
where H(Cr) is the entropy associated with the
clustering Cr, and mutual information I(Cr, Cg)
quantifies the mutual dependence between two
clusterings, or say the shared information be-
tween two variables. It is easy to see that
VIE [0, log(N)], where N is the number of data
points. However, the standard VI is not normal-
ized, which favors clusterings with a small num-
ber of clusters. It can be normalized by divid-
ing by log(N), because the number of training
instances are fixed. However the normalized VI
score is misleadingly large, if the N is very large
which is the case in our task. In this paper only
un-normalized VI scores are reported to show the
score ranking.
To standardize the measures to have fixed
bounds, (Strehl and Ghosh, 2003) defined the
normalized Mutual Information (NMI) as:
</bodyText>
<equation confidence="0.9980405">
NMI(Cr, Cg) = �H(Cr)H(Cg)
I(Cr, Cg) (11)
</equation>
<bodyText confidence="0.989275111111111">
NMI takes its lower bound of 0 if no information
is shared by two clusters and the upper bound
of 1 if two clusterings are identical. The NMI
however, still has problems, whose variation is
sensitive to the choice of the number of clusters.
Rosenberg and Hirschberg (2007) proposed
V-measure to combine two desirable properties
of clustering: homogeneity (h) and completeness
(c) as follows:
</bodyText>
<equation confidence="0.999925333333333">
h = 1 − H(Cg|Cr)/H(Cg)
c = 1 − H(Cr|Cg)/H(Cr) (12)
V = 2hc/(h + c)
</equation>
<bodyText confidence="0.999884692307692">
Generally homogeneity and completeness
runs in opposite way, whose harmonic mean (i.e.
V-measure) is a comprise score, just like F-score
for the precision and recall.
Let us first examine the contextual word clus-
tering performance. The VI score between dis-
tributional word categories and gold standard is
2.39. The NMI and V-measure score are 0.53
and 0.48, respectively.
The clustering performance of the HMM out-
puts are reported in Figure 2. The best VI
score achieved was 3.9524, while V-measure
was 62.09% and NMI reached 0.8051. Previous
</bodyText>
<figure confidence="0.999041347826087">
(a) VI score
(b) normalized scores
5
4.8
4.6
4.4
4.2
4
3.8
40 60 80 100 120 140 160 180 200 220 240
0.7
0.6
0.5
0.4
0.3
NMI
homogeneity
completeness
V−measure
0.1
0
40 60 80 100 120 140 160 180 200 220 240
0.2
</figure>
<figureCaption confidence="0.999457">
Figure 2: Clustering evaluation metrics against number of hidden states
</figureCaption>
<bodyText confidence="0.999901538461539">
work of Chinese tagging focuses on the tagging
accuracies, e.g. Wang (Wang and Schuurmans,
) and Huang et al. (Huang et al., 2007). To
our knowledge, this is the first work to report
the distributional clustering similarity measures
based on informatics view for Chinese . Simi-
lar works can be found on English of WSJ cor-
pus (Van Gael et al., 2009). Their best results of
VI, V-measure, achieved with Pitman-Yor prior,
were 3.73 and 59%. We believe the Chinese re-
sults are not good as English correspondences
because of the rich unknown words in Chinese
(Tseng et al., 2005).
</bodyText>
<subsectionHeader confidence="0.998876">
5.2 Dependency Parsing Evaluation
</subsectionHeader>
<bodyText confidence="0.996435444444444">
The next experiment is to test the goodness of the
outcome states of our model in the context of real
tasks. In this work, we consider unsupervised
dependency parsing for a fully unsupervised sys-
tem. The dependency parsing is to extract the
dependency graph whose nodes are the words of
the given sentence. The dependency graph is a
directed acyclic graph in which every edge links
from a head word to its dependent. Because we
work on unsupervised methods in this paper, we
choose a simple generative head-outward model
(Dependency Model with Valence, DMV) (Klein
and Manning, 2004; Headden III et al., 2009) for
parsing. The data through the experiment is re-
stricted to the sentences up to length 10 (exclud-
ing punctuation).
Because the main purpose is to test the HMM
output rather than to improve the parsing perfor-
mance, we select the original DMV model with-
out extensions or modifications. Starting from
the root, DMV generates the head, and then each
head recursively generates its left and right de-
pendents. In each direction, the possible depen-
dents are repeatedly chosen until a STOP marker
is seen. DMV use inside-outside algorithm for
re-estimation. We choose the “harmonic” ini-
tializer proposed in (Klein and Manning, 2004)
for initialization. The valence information is the
simplest binary value indicating the adjacency.
For different HMM candidates with varied hid-
den state number, we directly use the outputs as
the input of the DMV and trained a set of models.
Performing test on these individual models, we
report the directed dependency accuracies (the
fraction of words assigned the correct parent) in
Figure 3.
</bodyText>
<page confidence="0.839595333333333">
55
50
45
40
35
40 60 80 100 120 140 160 180 200 220 240
</page>
<figureCaption confidence="0.9930355">
Figure 3: Directed accuracies for different hid-
den states
</figureCaption>
<bodyText confidence="0.997163388888889">
It is noted that the accuracy monotonically
increases when the number of states increases.
The most drastic increase happened when state
changes from 40 to 120. The accuracy increased
from 38.56% to 50.60%. If the state number is
larger than 180, the increase is not obvious. The
final best accuracy is 54.20%, which improve the
standard DMV model by 5.6%. Therefore we
can see that the introduction of more annotations
can help the parsing results. However, the im-
provement is limited and stable when the num-
ber of state number is large. To further improve
the parsing performance, one might turn to the
extension of DMV model, e.g. introducing more
knowledge (prior or lexical information) or more
sophistical smoothing techniques. However, the
development of parser is not the focus of this pa-
per.
</bodyText>
<sectionHeader confidence="0.998116" genericHeader="method">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.99997380952381">
This paper works on the unsupervised Chinese
POS tagging based on the first-order HMM. Our
contributions are: 1). The number of hidden
states can be adjusted to fit the data. 2). For in-
ference, we use the variational inference, which
is faster and is guaranteed theoretically to con-
vergence. 3). To overcome the context limitation
in HMM, the words are clustered based on dis-
tributional similarities. It is a 1-to-many cluster-
ing, which means one word might be classified
into different clusters under different contexts.
Finally, experiments show the hidden states are
correlated to the latent annotations of the stan-
dard POS tags.
The future work includes to improve the per-
formance by incorporating a small amount of su-
pervision. The typical supervision used before
is dictionary extracted from a large corpus like
Chinese Gigaword. Another interesting idea is
to select some exemplars (Haghighi and Klein,
2006).
</bodyText>
<sectionHeader confidence="0.999243" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.9996058125">
Beal, Matthew J., Zoubin Ghahramani, and Carl Ed-
ward Rasmussen. 2001. The infinite hidden
markov model. In NIPS, pages 577–584.
Beal, M. J. 2003. Variational algorithms for
approximate bayesian inference. Phd Thesis.
Gatsby Computational Neuroscience Unit, Uni-
versity College London.
Brand, Matthew. 1999. An entropic estimator for
structure discovery. In Proceedings of the 1998
conference on Advances in neural information pro-
cessing systems II, pages 723–729, Cambridge,
MA, USA. MIT Press.
Fox, Emily B., Erik B. Sudderth, Michael I. Jordan,
and Alan S. Willsky. 2008. An hdp-hmm for sys-
tems with state persistence. In ICML ’08: Pro-
ceedings of the 25th international conference on
Machine learning.
Frey, Brendan J. and Delbert Dueck. 2007. Clus-
tering by passing messages between data points.
Science, 315:972–976.
Goldwater, Sharon and Tom Griffiths. 2007. A
fully bayesian approach to unsupervised part-of-
speech tagging. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics, pages 744–751, Prague, Czech Republic,
June. Association for Computational Linguistics.
Haghighi, Aria and Dan Klein. 2006. Prototype-
driven learning for sequence models. In Pro-
ceedings of the main conference on Human Lan-
guage Technology Conference of the North Amer-
ican Chapter of the Association of Computational
Linguistics, pages 320–327.
Headden, III, William P., David McClosky, and Eu-
gene Charniak. 2008. Evaluating unsupervised
part-of-speech tagging for grammar induction. In
COLING ’08: Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics,
pages 329–336, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Headden III, William P., Mark Johnson, and David
McClosky. 2009. Improving unsupervised depen-
dency parsing with richer contexts and smoothing.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 101–109, Boulder, Colorado,
June. Association for Computational Linguistics.
Hernando, D., V. Crespi, and G. Cybenko. 2005. Ef-
ficient computation of the hidden markov model
entropy for a given observation sequence. vol-
ume 51, pages 2681–2685.
Huang, Zhongqiang, Mary Harper, and Wen Wang.
2007. Mandarin part-of-speech tagging and dis-
criminative reranking. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Nat-
ural Language Learning (EMNLP-CoNLL), pages
1093–1102, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
Johnson, Mark. 2007. Why doesn’t EM find good
HMM POS-taggers? In Proceedings of the 2007
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Nat-
ural Language Learning (EMNLP-CoNLL), pages
296–305, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Kaji, Nobuhiro and Masaru Kitsuregawa. 2008. Us-
ing hidden markov random fields to combine dis-
tributional and pattern-based word clustering. In
COLING ’08: Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics,
pages 401–408, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Klein, Dan and Christopher Manning. 2004. Corpus-
based induction of syntactic structure: Models of
dependency and constituency. In Proceedings of
the 42nd Meeting of the Association for Computa-
tional Linguistics (ACL’04), Main Volume, pages
478–485, Barcelona, Spain, July.
Meil˘a, Marina. 2007. Comparing clusterings—an in-
formation based distance. volume 98, pages 873–
895.
Rabiner, Lawrence R. 1989. A tutorial on hidden
markov models and selected applications in speech
recognition. In Proceedings of the IEEE, pages
257–286.
Rosenberg, Andrew and Julia Hirschberg. 2007.
V-measure: A conditional entropy-based exter-
nal cluster evaluation measure. In Proceedings
of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 410–420.
Siddiqi, Sajid, Geoffrey Gordon, and Andrew Moore.
2007. Fast state discovery for hmm model selec-
tion and learning. In Proceedings of the Eleventh
International Conference on Artificial Intelligence
and Statistics (AI-STATS).
Strehl, Alexander and Joydeep Ghosh. 2003. Clus-
ter ensembles — a knowledge reuse framework
for combining multiple partitions. Journal of Ma-
chine Learning Research, 3:583–617.
Tseng, Huihsin, Daniel Jurafsky, and Christopher
Manning. 2005. Morphological features help pos
tagging of unknown words across language vari-
eties. pages 32–39.
Van Gael, Jurgen, Yunus Saatci, Yee Whye Teh, and
Zoubin Ghahramani. 2008. Beam sampling for
the infinite hidden markov model. In ICML ’08:
Proceedings of the 25th international conference
on Machine learning.
Van Gael, Jurgen, Andreas Vlachos, and Zoubin
Ghahramani. 2009. The infinite HMM for unsu-
pervised PoS tagging. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 678–687, Singapore, Au-
gust. Association for Computational Linguistics.
Wang, Qin Iris and Dale Schuurmans. Improved es-
timation for unsupervised part-of-speech tagging.
page 2005, Wuhan, China.
Weeds, Julie and David Weir. 2003. A general
framework for distributional similarity. In Pro-
ceedings of the 2003 conference on Empirical
methods in natural language processing, pages
81–88, Morristown, NJ, USA. Association for
Computational Linguistics.
Zelling, Harris. 1968. Mathematical sturcture of lan-
guage. NewYork:Wiley.
</reference>
<sectionHeader confidence="0.970338" genericHeader="conclusions">
APPENDIX
</sectionHeader>
<bodyText confidence="0.6282308">
Pseudo-code of the extended Baum-Welch Al-
gorithm in our dynamic HMM
Input: Time step t:
State Candidate: k —� (k(1), k(2)) ;
Sate Number: Nt;
</bodyText>
<equation confidence="0.909215625">
Model Parameter: 0(t) = (A(t), B(t), 7r(t));
Initialize
u(l)[k(1), k(2)] [&amp;quot;(�)[k]
2 , &amp;quot;(�)[k]
2 ], l E {A, B, 7r}
7rk(1) 1,7rk;7rk(2) 127rk
a¯kk(,) `i2a¯kkP; ak(�)¯k ak()¯k;
ak(�)k(�) 1 2ak(q(�), here i, j E 1, 2, k¯ # k
</equation>
<sectionHeader confidence="0.380797" genericHeader="references">
repeat
</sectionHeader>
<reference confidence="0.9296021">
E step:
update forward: at(k(1)) and at(k(2))
backward: /3t(k(1)) and /3t(k(2))
update et(i, j) and yt(i); if i, j E {k(1), k(2)}
update ]E[nij] = Et et(i, j)/ Et yt(i)
E&apos; [nik] = Et,w,=k yt(j)/ Et yt(j)
M step:
update 0(t+1) using equation (7)
until (AT-&lt; s)
Output: 0(t+1), T-
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.160564">
<title confidence="0.991193">Bigram HMM with Context Distribution Clustering for Unsupervised Chinese Part-of-Speech tagging</title>
<author confidence="0.963376">Lidan</author>
<affiliation confidence="0.992982">Department of Computer the University of Hong</affiliation>
<address confidence="0.662419">Hong</address>
<email confidence="0.694462">lzhang@cs.hku.hkKwok-Ping</email>
<affiliation confidence="0.9905165">Department of Computer the University of Hong</affiliation>
<address confidence="0.686108">Hong</address>
<email confidence="0.968517">kpchan@cs.hku.hk</email>
<abstract confidence="0.993605347826087">This paper presents an unsupervised Chinese Part-of-Speech (POS) tagging model based on the first-order HMM. Unlike the conventional HMM, the number of hidden states is not fixed and will be increased to fit the training data. In favor of sparse distribution, the Dirichlet priors are introduced with variational inference method. To reduce the emission variables, words are represented by their contexts and clustered based on the distributional similarities between contexts. Experiment results show the output state sequence of HMM are highly correlated to the latent annotations of gold POS tags, in context of clustering similarity measures. The other experiments on a real application, unsupervised dependency parsing, reveal that the output sequence can replace the manually annotated tags without loss of accuracies.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Matthew J Beal</author>
<author>Zoubin Ghahramani</author>
<author>Carl Edward Rasmussen</author>
</authors>
<title>The infinite hidden markov model.</title>
<date>2001</date>
<booktitle>In NIPS,</booktitle>
<pages>577--584</pages>
<contexts>
<context position="3233" citStr="Beal et al., 2001" startWordPosition="519" endWordPosition="522">e state space. In this paper, we focus on the latter approaches. Another problem in HMM is that EM algorithm might yield local maximum value. Johnson (2007) points out that training HMM with EM gives poor results because it leads to a fairly flat distribution of hidden states when the empirical distribution is highly skewed. A multinomial prior, which favors sparse distribution, is a good choice for natural language tasks. In this paper, we proposed a new procedure for inferring the HMM topology and estimating its parameters simultaneously. Gibbs sampling has been used in infinite HMM (iHMM) (Beal et al., 2001; Fox et al., 2008; Van Gael et al., 2008) for inference. Unfortunately Gibbs sampling is slow and difficult to be converged. In this paper, we proposed the variational Bayesian inference for the adaptive HMM model with Dirichlet prior. It involves a modification to the Baum-Welch algorithm. In each iteration, we replaced only one hidden state with two new states until convergence. To reduce the number of observation variables, the words are pre-clustered and represented by the exemplar within the same cluster. It is a one-to-many clustering, because the same word play different roles under di</context>
</contexts>
<marker>Beal, Ghahramani, Rasmussen, 2001</marker>
<rawString>Beal, Matthew J., Zoubin Ghahramani, and Carl Edward Rasmussen. 2001. The infinite hidden markov model. In NIPS, pages 577–584.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Beal</author>
</authors>
<title>Variational algorithms for approximate bayesian inference. Phd Thesis.</title>
<date>2003</date>
<institution>Gatsby Computational Neuroscience Unit, University College London.</institution>
<contexts>
<context position="8136" citStr="Beal (2003)" startWordPosition="1392" endWordPosition="1393">bound T . To make inference flexible, the posterior distribution can be assumed to be factorized according to the meanfield assumption. We have: p(W, S, ) 7z� q(S, ) = qe()qS (S) (6) Then an extension of EM algorithm (called Baum-Welch algorithm) can be used to alternately optimize the qS and qe. The EM process is described as follows: • E Step: Forward-Backward algorithm to find the optimal state sequence S(t+1) = arg max p(S (t)|W, (t)) • M Step: The parameters (t+1) are reestimated given the optimal state S(t+1) The E and M steps are repeated until a convergence criteria is satisfied. Beal (2003) proved that only need to do minor modifications in M step (in 1) is needed, when Dirichlet prior is introduced. 3 Adaptive Hidden Markov Model As aforementioned, the key problem of HMM is how to initialize the number of hidden states and select the topology of HMM. In this paper, we use the top-down scheme: starting from a small number of states, only one state is chosen in each step and splitted into two new states. This binary split scheme is described in Figure 1. Algorithm 1 Outline of our adpative HMM Initialization: Initialize: t = 0, N(t) repeat Optimization: Find the optimal parameter</context>
</contexts>
<marker>Beal, 2003</marker>
<rawString>Beal, M. J. 2003. Variational algorithms for approximate bayesian inference. Phd Thesis. Gatsby Computational Neuroscience Unit, University College London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Brand</author>
</authors>
<title>An entropic estimator for structure discovery.</title>
<date>1999</date>
<booktitle>In Proceedings of the 1998 conference on Advances in neural information processing systems II,</booktitle>
<pages>723--729</pages>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="2144" citStr="Brand, 1999" startWordPosition="336" endWordPosition="337">89). The key problem of HMM is how to find an optimal hidden state number and the topology appropriately. In most cases, the topology of HMM is predefined by exploiting the domain or empirical knowledge. This topology will be fixed during the whole process. Therefore how to select the optimal topology for a certain application or a set of training data is still a problem, because many researches show that varying the size of the state space greatly affects the performance of HMM. Generally there are two ways to adjust the state number: top-down and bottom-up methods. In the bottom-up methods (Brand, 1999), the state number is initialized with a relatively large number. During the training, the states are merged or trimmed and ended with a small set of states. On the other hand, the top-down methods (Siddiqi et al., 2007) start from a small state set and split one or some states until no further improvement can be obtained. The bottom-up approaches require huge computational cost in deciding the states to be merged, which makes it impractical for applications with large state space. In this paper, we focus on the latter approaches. Another problem in HMM is that EM algorithm might yield local m</context>
</contexts>
<marker>Brand, 1999</marker>
<rawString>Brand, Matthew. 1999. An entropic estimator for structure discovery. In Proceedings of the 1998 conference on Advances in neural information processing systems II, pages 723–729, Cambridge, MA, USA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily B Fox</author>
<author>Erik B Sudderth</author>
<author>Michael I Jordan</author>
<author>Alan S Willsky</author>
</authors>
<title>An hdp-hmm for systems with state persistence.</title>
<date>2008</date>
<booktitle>In ICML ’08: Proceedings of the 25th international conference on Machine learning.</booktitle>
<contexts>
<context position="3251" citStr="Fox et al., 2008" startWordPosition="523" endWordPosition="526">his paper, we focus on the latter approaches. Another problem in HMM is that EM algorithm might yield local maximum value. Johnson (2007) points out that training HMM with EM gives poor results because it leads to a fairly flat distribution of hidden states when the empirical distribution is highly skewed. A multinomial prior, which favors sparse distribution, is a good choice for natural language tasks. In this paper, we proposed a new procedure for inferring the HMM topology and estimating its parameters simultaneously. Gibbs sampling has been used in infinite HMM (iHMM) (Beal et al., 2001; Fox et al., 2008; Van Gael et al., 2008) for inference. Unfortunately Gibbs sampling is slow and difficult to be converged. In this paper, we proposed the variational Bayesian inference for the adaptive HMM model with Dirichlet prior. It involves a modification to the Baum-Welch algorithm. In each iteration, we replaced only one hidden state with two new states until convergence. To reduce the number of observation variables, the words are pre-clustered and represented by the exemplar within the same cluster. It is a one-to-many clustering, because the same word play different roles under different contexts. </context>
</contexts>
<marker>Fox, Sudderth, Jordan, Willsky, 2008</marker>
<rawString>Fox, Emily B., Erik B. Sudderth, Michael I. Jordan, and Alan S. Willsky. 2008. An hdp-hmm for systems with state persistence. In ICML ’08: Proceedings of the 25th international conference on Machine learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan J Frey</author>
<author>Delbert Dueck</author>
</authors>
<title>Clustering by passing messages between data points.</title>
<date>2007</date>
<journal>Science,</journal>
<pages>315--972</pages>
<contexts>
<context position="13830" citStr="Frey and Dueck, 2007" startWordPosition="2396" endWordPosition="2399">Pointwise Mutual Information (PMI) between the context pair in which they appear: PMI(wi, wj) = log P(ci, cj) (9) P(ci)P(cj) where ci denotes the context of wi. P(ci, cj) is the co-occurrence probability of ci and cj, and P(ci) = Ej P(ci, cj) is the occurrence probability of ci. In our experiments, the cutoff context count is set to 10, which means the frequency less than the threshold is labeled as the unknown context. The above distributional similarity can be used as a distance measure. Hence any clustering algorithm can be adopted. In this paper, we use the affinity propagation algorithm (Frey and Dueck, 2007). Its parameter ‘dampfact’ is set to 0.9, and the other parameters are set as default. After running the clustering algorithm, the contexts are clustered into 1869 clusterings. It is noted that one word might be classified into several clusters , if its contexts are clustered into several clusters. 5 Experiments As aforementioned, the outputs of our HMM model are evaluated in two ways, clustering metric and parsing performance. The data used in all experiments are the Chinese data set in CoNLL2007 shared task. The number of tokens in training, development and test sets are 609,060, 49,620 and </context>
</contexts>
<marker>Frey, Dueck, 2007</marker>
<rawString>Frey, Brendan J. and Delbert Dueck. 2007. Clustering by passing messages between data points. Science, 315:972–976.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Tom Griffiths</author>
</authors>
<title>A fully bayesian approach to unsupervised part-ofspeech tagging.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>744--751</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1389" citStr="Goldwater and Griffiths, 2007" startWordPosition="205" endWordPosition="209">ties between contexts. Experiment results show the output state sequence of HMM are highly correlated to the latent annotations of gold POS tags, in context of clustering similarity measures. The other experiments on a real application, unsupervised dependency parsing, reveal that the output sequence can replace the manually annotated tags without loss of accuracies. 1 Introduction Recently latent variable model has shown great potential in recovering the underlying structures. For example, the task of POS tagging is to recover the appropriate sequence structure given the input word sequence (Goldwater and Griffiths, 2007). One of the most popular example of latent models is Hidden Markov Model (HMM), which has been extensively studied for many years (Rabiner, 1989). The key problem of HMM is how to find an optimal hidden state number and the topology appropriately. In most cases, the topology of HMM is predefined by exploiting the domain or empirical knowledge. This topology will be fixed during the whole process. Therefore how to select the optimal topology for a certain application or a set of training data is still a problem, because many researches show that varying the size of the state space greatly affe</context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>Goldwater, Sharon and Tom Griffiths. 2007. A fully bayesian approach to unsupervised part-ofspeech tagging. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 744–751, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Prototypedriven learning for sequence models.</title>
<date>2006</date>
<booktitle>In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>320--327</pages>
<marker>Haghighi, Klein, 2006</marker>
<rawString>Haghighi, Aria and Dan Klein. 2006. Prototypedriven learning for sequence models. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pages 320–327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William P Headden</author>
<author>David McClosky</author>
<author>Eugene Charniak</author>
</authors>
<title>Evaluating unsupervised part-of-speech tagging for grammar induction.</title>
<date>2008</date>
<booktitle>In COLING ’08: Proceedings of the 22nd International Conference on Computational Linguistics,</booktitle>
<pages>329--336</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="4381" citStr="Headden et al., 2008" startWordPosition="704" endWordPosition="707">e-to-many clustering, because the same word play different roles under different contexts. We evaluate the similarity between the distribution of contexts, with the assumption that the context distribution implies syntactic pattern of the given word (Zelling, 1968; Weeds and Weir, 2003). With this clustering, more contextual information can be considered without increasing the model complexity. A relatively simple model is important for unsupervised task in terms of computational burden and data sparseness. This is the reason why we do not increase the order of HMM(Kaji and Kitsuregawa, 2008; Headden et al., 2008). With unsupervised algorithms, there are two aspects to be evaluated (Van Gael et al., 2009). Fist one is how good the outcome clusters are. We compare the HMM results with the manually POS tags and report the similarity measures based on information theory. On the other hand, we test how good the outputs act as an intermediate results. In many natural language tasks, the inputs are word class, not the actual lexical item, for reason of sparsity. In this paper, we choose the unsupervised dependency parsing as the application to investigate whether our clusters can replace the manual labeled t</context>
</contexts>
<marker>Headden, McClosky, Charniak, 2008</marker>
<rawString>Headden, III, William P., David McClosky, and Eugene Charniak. 2008. Evaluating unsupervised part-of-speech tagging for grammar induction. In COLING ’08: Proceedings of the 22nd International Conference on Computational Linguistics, pages 329–336, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Headden William P</author>
<author>Mark Johnson</author>
<author>David McClosky</author>
</authors>
<title>Improving unsupervised dependency parsing with richer contexts and smoothing.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>101--109</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<marker>P, Johnson, McClosky, 2009</marker>
<rawString>Headden III, William P., Mark Johnson, and David McClosky. 2009. Improving unsupervised dependency parsing with richer contexts and smoothing. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 101–109, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hernando</author>
<author>V Crespi</author>
<author>G Cybenko</author>
</authors>
<title>Efficient computation of the hidden markov model entropy for a given observation sequence.</title>
<date>2005</date>
<volume>51</volume>
<pages>2681--2685</pages>
<contexts>
<context position="9924" citStr="Hernando et al., 2005" startWordPosition="1688" endWordPosition="1692">itted one. Obviously the exhaustive search is inefficient especially for large state space. To make the algorithm more efficient, some constraints must be set to narrow the search space. Intuitively entropy implies uncertainty. So hidden states with large conditional entropies are desirable to be splitted. We can define the conditional entropy of the state sequences given observation W as: [P(S |W)log P(S |W)] (8) Our assumption is the state to be splitted must be the states sequence with the highest conditional entropy value. This entropy can be recursively calculated with complexity O(N2T) (Hernando et al., 2005). Here N is the number of r(∑Ni=1 ui) N x`.``−1. (4) ∏Ni=1 r(ui) ∏ t i=1 (5) ∑ H(S |W) = − S A(t+1) = {a(t+1) i j } = exp[(( ij O( )) − ( B(t+1) = {b(t+1)} = exp[W(W(B)) − ( ik ik 7r(t+1) = {7r(t+1) i } = exp[(W(&amp;quot;) i ) − ( W (A))] ))] ; W(A) i j= u(A) j+ Eq(s)[nij] W(B) ik )] ; W(B) ik = u(B) k + Eq(s)[n′ik] Wj )]; W(&amp;quot;) (&amp;quot;) i = u(&amp;quot;) i + Eq(s)[n′′ i ] N L j=1 T Z k=1 N i=1 (7) Figure 1: Parameters update equations in M-step. Here E is the expectation with respect to the model parameters. And nij is the expected number of transition from state si to state sj; n′ik is the expected number of </context>
</contexts>
<marker>Hernando, Crespi, Cybenko, 2005</marker>
<rawString>Hernando, D., V. Crespi, and G. Cybenko. 2005. Efficient computation of the hidden markov model entropy for a given observation sequence. volume 51, pages 2681–2685.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Mary Harper</author>
<author>Wen Wang</author>
</authors>
<title>Mandarin part-of-speech tagging and discriminative reranking.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>1093--1102</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="17608" citStr="Huang et al., 2007" startWordPosition="3044" endWordPosition="3047">re are 0.53 and 0.48, respectively. The clustering performance of the HMM outputs are reported in Figure 2. The best VI score achieved was 3.9524, while V-measure was 62.09% and NMI reached 0.8051. Previous (a) VI score (b) normalized scores 5 4.8 4.6 4.4 4.2 4 3.8 40 60 80 100 120 140 160 180 200 220 240 0.7 0.6 0.5 0.4 0.3 NMI homogeneity completeness V−measure 0.1 0 40 60 80 100 120 140 160 180 200 220 240 0.2 Figure 2: Clustering evaluation metrics against number of hidden states work of Chinese tagging focuses on the tagging accuracies, e.g. Wang (Wang and Schuurmans, ) and Huang et al. (Huang et al., 2007). To our knowledge, this is the first work to report the distributional clustering similarity measures based on informatics view for Chinese . Similar works can be found on English of WSJ corpus (Van Gael et al., 2009). Their best results of VI, V-measure, achieved with Pitman-Yor prior, were 3.73 and 59%. We believe the Chinese results are not good as English correspondences because of the rich unknown words in Chinese (Tseng et al., 2005). 5.2 Dependency Parsing Evaluation The next experiment is to test the goodness of the outcome states of our model in the context of real tasks. In this wor</context>
</contexts>
<marker>Huang, Harper, Wang, 2007</marker>
<rawString>Huang, Zhongqiang, Mary Harper, and Wen Wang. 2007. Mandarin part-of-speech tagging and discriminative reranking. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 1093–1102, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Why doesn’t EM find good HMM POS-taggers?</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>296--305</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2772" citStr="Johnson (2007)" startWordPosition="444" endWordPosition="446">mber is initialized with a relatively large number. During the training, the states are merged or trimmed and ended with a small set of states. On the other hand, the top-down methods (Siddiqi et al., 2007) start from a small state set and split one or some states until no further improvement can be obtained. The bottom-up approaches require huge computational cost in deciding the states to be merged, which makes it impractical for applications with large state space. In this paper, we focus on the latter approaches. Another problem in HMM is that EM algorithm might yield local maximum value. Johnson (2007) points out that training HMM with EM gives poor results because it leads to a fairly flat distribution of hidden states when the empirical distribution is highly skewed. A multinomial prior, which favors sparse distribution, is a good choice for natural language tasks. In this paper, we proposed a new procedure for inferring the HMM topology and estimating its parameters simultaneously. Gibbs sampling has been used in infinite HMM (iHMM) (Beal et al., 2001; Fox et al., 2008; Van Gael et al., 2008) for inference. Unfortunately Gibbs sampling is slow and difficult to be converged. In this paper</context>
</contexts>
<marker>Johnson, 2007</marker>
<rawString>Johnson, Mark. 2007. Why doesn’t EM find good HMM POS-taggers? In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 296–305, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nobuhiro Kaji</author>
<author>Masaru Kitsuregawa</author>
</authors>
<title>Using hidden markov random fields to combine distributional and pattern-based word clustering.</title>
<date>2008</date>
<booktitle>In COLING ’08: Proceedings of the 22nd International Conference on Computational Linguistics,</booktitle>
<pages>401--408</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="4358" citStr="Kaji and Kitsuregawa, 2008" startWordPosition="700" endWordPosition="703">the same cluster. It is a one-to-many clustering, because the same word play different roles under different contexts. We evaluate the similarity between the distribution of contexts, with the assumption that the context distribution implies syntactic pattern of the given word (Zelling, 1968; Weeds and Weir, 2003). With this clustering, more contextual information can be considered without increasing the model complexity. A relatively simple model is important for unsupervised task in terms of computational burden and data sparseness. This is the reason why we do not increase the order of HMM(Kaji and Kitsuregawa, 2008; Headden et al., 2008). With unsupervised algorithms, there are two aspects to be evaluated (Van Gael et al., 2009). Fist one is how good the outcome clusters are. We compare the HMM results with the manually POS tags and report the similarity measures based on information theory. On the other hand, we test how good the outputs act as an intermediate results. In many natural language tasks, the inputs are word class, not the actual lexical item, for reason of sparsity. In this paper, we choose the unsupervised dependency parsing as the application to investigate whether our clusters can repla</context>
</contexts>
<marker>Kaji, Kitsuregawa, 2008</marker>
<rawString>Kaji, Nobuhiro and Masaru Kitsuregawa. 2008. Using hidden markov random fields to combine distributional and pattern-based word clustering. In COLING ’08: Proceedings of the 22nd International Conference on Computational Linguistics, pages 401–408, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher Manning</author>
</authors>
<title>Corpusbased induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume,</booktitle>
<pages>478--485</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="18670" citStr="Klein and Manning, 2004" startWordPosition="3223" endWordPosition="3226">2005). 5.2 Dependency Parsing Evaluation The next experiment is to test the goodness of the outcome states of our model in the context of real tasks. In this work, we consider unsupervised dependency parsing for a fully unsupervised system. The dependency parsing is to extract the dependency graph whose nodes are the words of the given sentence. The dependency graph is a directed acyclic graph in which every edge links from a head word to its dependent. Because we work on unsupervised methods in this paper, we choose a simple generative head-outward model (Dependency Model with Valence, DMV) (Klein and Manning, 2004; Headden III et al., 2009) for parsing. The data through the experiment is restricted to the sentences up to length 10 (excluding punctuation). Because the main purpose is to test the HMM output rather than to improve the parsing performance, we select the original DMV model without extensions or modifications. Starting from the root, DMV generates the head, and then each head recursively generates its left and right dependents. In each direction, the possible dependents are repeatedly chosen until a STOP marker is seen. DMV use inside-outside algorithm for re-estimation. We choose the “harmo</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>Klein, Dan and Christopher Manning. 2004. Corpusbased induction of syntactic structure: Models of dependency and constituency. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume, pages 478–485, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marina Meil˘a</author>
</authors>
<title>Comparing clusterings—an information based distance.</title>
<date>2007</date>
<volume>98</volume>
<pages>873--895</pages>
<marker>Meil˘a, 2007</marker>
<rawString>Meil˘a, Marina. 2007. Comparing clusterings—an information based distance. volume 98, pages 873– 895.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence R Rabiner</author>
</authors>
<title>A tutorial on hidden markov models and selected applications in speech recognition.</title>
<date>1989</date>
<booktitle>In Proceedings of the IEEE,</booktitle>
<pages>257--286</pages>
<contexts>
<context position="1535" citStr="Rabiner, 1989" startWordPosition="233" endWordPosition="234">ustering similarity measures. The other experiments on a real application, unsupervised dependency parsing, reveal that the output sequence can replace the manually annotated tags without loss of accuracies. 1 Introduction Recently latent variable model has shown great potential in recovering the underlying structures. For example, the task of POS tagging is to recover the appropriate sequence structure given the input word sequence (Goldwater and Griffiths, 2007). One of the most popular example of latent models is Hidden Markov Model (HMM), which has been extensively studied for many years (Rabiner, 1989). The key problem of HMM is how to find an optimal hidden state number and the topology appropriately. In most cases, the topology of HMM is predefined by exploiting the domain or empirical knowledge. This topology will be fixed during the whole process. Therefore how to select the optimal topology for a certain application or a set of training data is still a problem, because many researches show that varying the size of the state space greatly affects the performance of HMM. Generally there are two ways to adjust the state number: top-down and bottom-up methods. In the bottom-up methods (Bra</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>Rabiner, Lawrence R. 1989. A tutorial on hidden markov models and selected applications in speech recognition. In Proceedings of the IEEE, pages 257–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Rosenberg</author>
<author>Julia Hirschberg</author>
</authors>
<title>V-measure: A conditional entropy-based external cluster evaluation measure.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>410--420</pages>
<contexts>
<context position="16466" citStr="Rosenberg and Hirschberg (2007)" startWordPosition="2845" endWordPosition="2848">owever the normalized VI score is misleadingly large, if the N is very large which is the case in our task. In this paper only un-normalized VI scores are reported to show the score ranking. To standardize the measures to have fixed bounds, (Strehl and Ghosh, 2003) defined the normalized Mutual Information (NMI) as: NMI(Cr, Cg) = �H(Cr)H(Cg) I(Cr, Cg) (11) NMI takes its lower bound of 0 if no information is shared by two clusters and the upper bound of 1 if two clusterings are identical. The NMI however, still has problems, whose variation is sensitive to the choice of the number of clusters. Rosenberg and Hirschberg (2007) proposed V-measure to combine two desirable properties of clustering: homogeneity (h) and completeness (c) as follows: h = 1 − H(Cg|Cr)/H(Cg) c = 1 − H(Cr|Cg)/H(Cr) (12) V = 2hc/(h + c) Generally homogeneity and completeness runs in opposite way, whose harmonic mean (i.e. V-measure) is a comprise score, just like F-score for the precision and recall. Let us first examine the contextual word clustering performance. The VI score between distributional word categories and gold standard is 2.39. The NMI and V-measure score are 0.53 and 0.48, respectively. The clustering performance of the HMM out</context>
</contexts>
<marker>Rosenberg, Hirschberg, 2007</marker>
<rawString>Rosenberg, Andrew and Julia Hirschberg. 2007. V-measure: A conditional entropy-based external cluster evaluation measure. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 410–420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sajid Siddiqi</author>
<author>Geoffrey Gordon</author>
<author>Andrew Moore</author>
</authors>
<title>Fast state discovery for hmm model selection and learning.</title>
<date>2007</date>
<booktitle>In Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics (AI-STATS).</booktitle>
<contexts>
<context position="2364" citStr="Siddiqi et al., 2007" startWordPosition="373" endWordPosition="376">pology will be fixed during the whole process. Therefore how to select the optimal topology for a certain application or a set of training data is still a problem, because many researches show that varying the size of the state space greatly affects the performance of HMM. Generally there are two ways to adjust the state number: top-down and bottom-up methods. In the bottom-up methods (Brand, 1999), the state number is initialized with a relatively large number. During the training, the states are merged or trimmed and ended with a small set of states. On the other hand, the top-down methods (Siddiqi et al., 2007) start from a small state set and split one or some states until no further improvement can be obtained. The bottom-up approaches require huge computational cost in deciding the states to be merged, which makes it impractical for applications with large state space. In this paper, we focus on the latter approaches. Another problem in HMM is that EM algorithm might yield local maximum value. Johnson (2007) points out that training HMM with EM gives poor results because it leads to a fairly flat distribution of hidden states when the empirical distribution is highly skewed. A multinomial prior, </context>
<context position="11772" citStr="Siddiqi et al., 2007" startWordPosition="2051" endWordPosition="2054"> after the i-th column and row. In the process of update, we only change the items in the two (i and i + 1) rows and columns. The other elements irrelevant to the split state are not involved in the update procedure. Similarly EM algorithm is used to find the optimal parameters. Note that most of the calculations can be skipped by making use of the forward and backward probability matrix achieved in the previous step. Therefore the convergence is fast. Given the candidate selection, we can use a modified Baum-Welch algorithm to find optimal states and parameters. Here we use the algorithm in (Siddiqi et al., 2007) with some modifications for the Dirichlet prior. In particular, in E step, we follow their partial Forward-Background algorithm to calculate E[nij] and E[n′ik], if si or sj is candidate state to be splitted. Then in M-step, only rows and columns related to the candidate state are updated according to equation (7). The detailed description is given as appendix. Finally it is natural to use variational bound of marginal likelihood in equation (5) for model scoring and convergence criterion. 4 Distributional Clustering To reduce the number of observation variables, the words are clustered before</context>
</contexts>
<marker>Siddiqi, Gordon, Moore, 2007</marker>
<rawString>Siddiqi, Sajid, Geoffrey Gordon, and Andrew Moore. 2007. Fast state discovery for hmm model selection and learning. In Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics (AI-STATS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Strehl</author>
<author>Joydeep Ghosh</author>
</authors>
<title>Cluster ensembles — a knowledge reuse framework for combining multiple partitions.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--583</pages>
<contexts>
<context position="16100" citStr="Strehl and Ghosh, 2003" startWordPosition="2783" endWordPosition="2786">ce between two clusterings, or say the shared information between two variables. It is easy to see that VIE [0, log(N)], where N is the number of data points. However, the standard VI is not normalized, which favors clusterings with a small number of clusters. It can be normalized by dividing by log(N), because the number of training instances are fixed. However the normalized VI score is misleadingly large, if the N is very large which is the case in our task. In this paper only un-normalized VI scores are reported to show the score ranking. To standardize the measures to have fixed bounds, (Strehl and Ghosh, 2003) defined the normalized Mutual Information (NMI) as: NMI(Cr, Cg) = �H(Cr)H(Cg) I(Cr, Cg) (11) NMI takes its lower bound of 0 if no information is shared by two clusters and the upper bound of 1 if two clusterings are identical. The NMI however, still has problems, whose variation is sensitive to the choice of the number of clusters. Rosenberg and Hirschberg (2007) proposed V-measure to combine two desirable properties of clustering: homogeneity (h) and completeness (c) as follows: h = 1 − H(Cg|Cr)/H(Cg) c = 1 − H(Cr|Cg)/H(Cr) (12) V = 2hc/(h + c) Generally homogeneity and completeness runs in </context>
</contexts>
<marker>Strehl, Ghosh, 2003</marker>
<rawString>Strehl, Alexander and Joydeep Ghosh. 2003. Cluster ensembles — a knowledge reuse framework for combining multiple partitions. Journal of Machine Learning Research, 3:583–617.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huihsin Tseng</author>
<author>Daniel Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>Morphological features help pos tagging of unknown words across language varieties.</title>
<date>2005</date>
<pages>32--39</pages>
<contexts>
<context position="18052" citStr="Tseng et al., 2005" startWordPosition="3121" endWordPosition="3124">tion metrics against number of hidden states work of Chinese tagging focuses on the tagging accuracies, e.g. Wang (Wang and Schuurmans, ) and Huang et al. (Huang et al., 2007). To our knowledge, this is the first work to report the distributional clustering similarity measures based on informatics view for Chinese . Similar works can be found on English of WSJ corpus (Van Gael et al., 2009). Their best results of VI, V-measure, achieved with Pitman-Yor prior, were 3.73 and 59%. We believe the Chinese results are not good as English correspondences because of the rich unknown words in Chinese (Tseng et al., 2005). 5.2 Dependency Parsing Evaluation The next experiment is to test the goodness of the outcome states of our model in the context of real tasks. In this work, we consider unsupervised dependency parsing for a fully unsupervised system. The dependency parsing is to extract the dependency graph whose nodes are the words of the given sentence. The dependency graph is a directed acyclic graph in which every edge links from a head word to its dependent. Because we work on unsupervised methods in this paper, we choose a simple generative head-outward model (Dependency Model with Valence, DMV) (Klein</context>
</contexts>
<marker>Tseng, Jurafsky, Manning, 2005</marker>
<rawString>Tseng, Huihsin, Daniel Jurafsky, and Christopher Manning. 2005. Morphological features help pos tagging of unknown words across language varieties. pages 32–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jurgen Van Gael</author>
<author>Yunus Saatci</author>
<author>Yee Whye Teh</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Beam sampling for the infinite hidden markov model.</title>
<date>2008</date>
<booktitle>In ICML ’08: Proceedings of the 25th international conference on Machine learning.</booktitle>
<marker>Van Gael, Saatci, Teh, Ghahramani, 2008</marker>
<rawString>Van Gael, Jurgen, Yunus Saatci, Yee Whye Teh, and Zoubin Ghahramani. 2008. Beam sampling for the infinite hidden markov model. In ICML ’08: Proceedings of the 25th international conference on Machine learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jurgen Van Gael</author>
<author>Andreas Vlachos</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>The infinite HMM for unsupervised PoS tagging.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>678--687</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Van Gael, Vlachos, Ghahramani, 2009</marker>
<rawString>Van Gael, Jurgen, Andreas Vlachos, and Zoubin Ghahramani. 2009. The infinite HMM for unsupervised PoS tagging. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 678–687, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qin Iris Wang</author>
<author>Dale Schuurmans</author>
</authors>
<title>Improved estimation for unsupervised part-of-speech tagging.</title>
<date>2005</date>
<pages>page</pages>
<location>Wuhan, China.</location>
<marker>Wang, Schuurmans, 2005</marker>
<rawString>Wang, Qin Iris and Dale Schuurmans. Improved estimation for unsupervised part-of-speech tagging. page 2005, Wuhan, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
</authors>
<title>A general framework for distributional similarity.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 conference on Empirical methods in natural language processing,</booktitle>
<pages>81--88</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="4047" citStr="Weeds and Weir, 2003" startWordPosition="649" endWordPosition="652">he adaptive HMM model with Dirichlet prior. It involves a modification to the Baum-Welch algorithm. In each iteration, we replaced only one hidden state with two new states until convergence. To reduce the number of observation variables, the words are pre-clustered and represented by the exemplar within the same cluster. It is a one-to-many clustering, because the same word play different roles under different contexts. We evaluate the similarity between the distribution of contexts, with the assumption that the context distribution implies syntactic pattern of the given word (Zelling, 1968; Weeds and Weir, 2003). With this clustering, more contextual information can be considered without increasing the model complexity. A relatively simple model is important for unsupervised task in terms of computational burden and data sparseness. This is the reason why we do not increase the order of HMM(Kaji and Kitsuregawa, 2008; Headden et al., 2008). With unsupervised algorithms, there are two aspects to be evaluated (Van Gael et al., 2009). Fist one is how good the outcome clusters are. We compare the HMM results with the manually POS tags and report the similarity measures based on information theory. On the</context>
</contexts>
<marker>Weeds, Weir, 2003</marker>
<rawString>Weeds, Julie and David Weir. 2003. A general framework for distributional similarity. In Proceedings of the 2003 conference on Empirical methods in natural language processing, pages 81–88, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harris Zelling</author>
</authors>
<date>1968</date>
<journal>Mathematical sturcture of language. NewYork:Wiley.</journal>
<contexts>
<context position="4024" citStr="Zelling, 1968" startWordPosition="647" endWordPosition="648">inference for the adaptive HMM model with Dirichlet prior. It involves a modification to the Baum-Welch algorithm. In each iteration, we replaced only one hidden state with two new states until convergence. To reduce the number of observation variables, the words are pre-clustered and represented by the exemplar within the same cluster. It is a one-to-many clustering, because the same word play different roles under different contexts. We evaluate the similarity between the distribution of contexts, with the assumption that the context distribution implies syntactic pattern of the given word (Zelling, 1968; Weeds and Weir, 2003). With this clustering, more contextual information can be considered without increasing the model complexity. A relatively simple model is important for unsupervised task in terms of computational burden and data sparseness. This is the reason why we do not increase the order of HMM(Kaji and Kitsuregawa, 2008; Headden et al., 2008). With unsupervised algorithms, there are two aspects to be evaluated (Van Gael et al., 2009). Fist one is how good the outcome clusters are. We compare the HMM results with the manually POS tags and report the similarity measures based on inf</context>
</contexts>
<marker>Zelling, 1968</marker>
<rawString>Zelling, Harris. 1968. Mathematical sturcture of language. NewYork:Wiley.</rawString>
</citation>
<citation valid="false">
<authors>
<author>E step</author>
</authors>
<title>update forward: at(k(1)) and at(k(2)) backward: /3t(k(1)) and /3t(k(2)) update et(i, j) and yt(i); if i, j E {k(1), k(2)} update ]E[nij]</title>
<journal>Et et(i, j)/ Et yt(i) E&apos; [nik] = Et,w,=k yt(j)/ Et yt(j) M</journal>
<publisher>(AT-&lt; s)</publisher>
<marker>step, </marker>
<rawString>E step: update forward: at(k(1)) and at(k(2)) backward: /3t(k(1)) and /3t(k(2)) update et(i, j) and yt(i); if i, j E {k(1), k(2)} update ]E[nij] = Et et(i, j)/ Et yt(i) E&apos; [nik] = Et,w,=k yt(j)/ Et yt(j) M step: update 0(t+1) using equation (7) until (AT-&lt; s)</rawString>
</citation>
<citation valid="false">
<note>Output: 0(t+1), T-</note>
<marker></marker>
<rawString>Output: 0(t+1), T-</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>