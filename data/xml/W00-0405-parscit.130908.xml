<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000023">
<title confidence="0.939097">
Multi-Document Summarization By Sentence Extraction
</title>
<author confidence="0.537091">
Jade Goldstein* Vibhu Mittalt Jaime Carbonell* Mark Kantrowitzt
</author>
<email confidence="0.76156">
jade@cs.cmu.edu mittal@jprc.com jgc@cs.cmu.edu mkant@jprc.com
</email>
<affiliation confidence="0.94016425">
*Language Technologies Institute tJust Research
Carnegie Mellon University 4616 Henry Street
Pittsburgh, PA 15213 Pittsburgh, PA 15213
U.S.A. U.S.A.
</affiliation>
<sectionHeader confidence="0.98867" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999936428571429">
This paper discusses a text extraction approach to multi-
document summarization that builds on single-document
summarization methods by using additional, available in-,
formation about the document set as a whole and the
relationships between the documents. Multi-document
summarization differs from single in that the issues
of compression, speed, redundancy and passage selec-
tion are critical in the formation of useful summaries.
Our approach addresses these issues by using domain-
independent techniques based mainly on fast, statistical
processing, a metric for reducing redundancy and maxi-
mizing diversity in the selected passages, and a modular
framework to allow easy parameterization for different
genres, corpora characteristics and user requirements.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.977306042857143">
With the continuing growth of online information, it
has become increasingly important to provide improved
mechanisms to find and present textual information ef-
fectively. Conventional IR systems find and rank docu-
ments based on maximizing relevance to the user query
(Salton, 1970; van Rijsbergen, 1979; Buckley, 1985;
Salton, 1989). Some systems also include sub-document
relevance assessments and convey this information to the
user. More recently, single document summarization sys-
tems provide an automated generic abstract or a query-
relevant summary (TIPSTER, 1998a),I However, large-
scale IR and summarization have not yet been truly in-
tegrated, and the functionality challenges on a summa-
rization system are greater in a true IR or topic-detection
context (Yang et al., 1998; Allan etal., 1998).
Consider the situation where the user issues a search
query, for instance on a news topic, and the retrieval sys-
tem finds hundreds of closely-ranked documents in re-
sponse. Many of these documents are likely to repeat
much the same information, while differing in certain
&apos;Most of these were based on statistical techniques applied to var-
ious document entities; examples include (Tait, 1983; Kupiec et al.,
1995; Paice, 1990; Klavans and Shaw, 1995; McKeown et al., 1995;
Shaw, 1995; Aone et al., 1997; Boguraev and Kennedy, 1997; Hovy
and Lin, 1997; Mitra et al., 1997; Teufel and Moens, 1997; Barzilay
and Elhadad, 1997; Carbonell and Goldstein, 1998; Baldwin and Mor-
ton, 1998; Radev and McKeown, 1998; Strzalkowski et al., 1998).
parts. Summaries of the individual documents would
help, but are likely to be very similar to each other, un-
less the summarization system takes into account other
summaries that have already been generated. Multi-
document summarization — capable of summarizing ei-
ther complete documents sets, or single documents in the
context of previously summarized ones — are likely to
be essential in such situations. Ideally, multi-document
summaries should contain the key shared relevant infor-
mation among all the documents only once, plus other
information unique to some of the individual documents
that are directly relevant to the user&apos;s query.
Though many of the same techniques used in single-
document summarization can also be used in multi-
document summarization, there are at least four signif-
icant differences:
I. The degree of redundancy in information contained
within a group of topically-related articles is much
higher than the degree of redundancy within an arti-
cle, as each article is apt to describe the main point
as well as necessary shared background. Hence
anti-redundancy methods are more crucial.
2. A group of articles may contain a temporal dimen-
sion, typical in a stream of news reports about an
unfolding event. Here later information may over-
ride earlier more tentative or incomplete accounts.
3. The compression ratio (i.e. the size of the summary
with respect to the size of the document set) will
typically be much smaller for collections of dozens
or hundreds of topically related documents than
for single document summaries. The SUMMAC
evaluation (TIPSTER, 1998a) tested 10% compres-
sion summaries, but in our work summarizing 200-
document clusters, we find that compression to the
1% or 0.1% level is required. Summarization be-
comes significantly more difficult when compres-
sion demands increase.
4. The co-reference problem in summarization
presents even greater challenges for multi-
document than for single-document summariza-
tion (Baldwin and Morton, 1998).
This paper discusses an approach to multi-document
summarization that builds on previous work in single-
</bodyText>
<page confidence="0.996919">
40
</page>
<bodyText confidence="0.99984875">
document summarization by using additional, available
information about the document set as a whole, the re-
lationships between the documents, as well as individual
documents.
</bodyText>
<sectionHeader confidence="0.657141" genericHeader="introduction">
2 Background and Related Work
</sectionHeader>
<bodyText confidence="0.999838555555556">
Generating an effective summary requires the summa-
rizer to select, evaluate, order and aggregate items of
information according to their relevance to a particular
subject or purpose. These tasks can either be approx-
imated by IR techniques or done in greater depth with
fuller natural language processing. Most previous work
in summarization has attempted to deal with the issues by
focusing more on a related, but simpler, problem. With
text-span deletion the system attempts to delete &amp;quot;less im-
portant&amp;quot; spans of text from the original document; the
text that remains is deemed a summary. Work on auto-
mated document summarization by text span extraction
dates back at least to work at IBM in the fifties (Luhn,
1958). Most of the work in sentence extraction applied
statistical techniques (frequency analysis, variance anal-
ysis, etc.) to linguistic units such as tokens, names,
anaphora, etc. More recently, other approaches have
investigated the utility of discourse structure (Marcu,
1997), the combination of information extraction and
language generation (Klavans and Shaw, 1995; McKe-
own et al., 1995), and using machine learning to find
patterns in text (Teufel and Moens, 1997; Barzilay and
Elhadad, 1997; Strzalkowski et al., 1998).
Some of these approaches to single document summa-
rization have been extended to deal with multi-document
summarization (Mani and Bloedern, 1997; Goldstein and
Carbonell, 1998; TIPSTER, 1998b; Radev and McKe-
own, 1998; Mani and Bloedorn, 1999; McKeown et al.,
,1999; Stein et al., 1999). These include comparing tem-
plates filled in by extracting information — using special-
ized, domain specific knowledge sources — from the doc-
&apos;ument, and then generating natural language summaries
from the templates (Radev and McKeown, 1998), corn—
. paring named-entities — extracted using specialized lists
— between documents and selecting the most relevant
section (TIPSTER, 1998b), finding co-reference chains
in the document set to identify common sections of inter-
est (TIPSTER, 1998b), or building activation networks
of related lexical items (identity mappings, synonyms,
hypernyms, etc.) to extract text spans from the document
set (Mani and Bloedern, 1997). Another system (Stein et
al., 1999) creates a multi-document summary from mul-
tiple single document summaries, an approach that can
be sub-optimal in some cases, due to the fact that the
process of generating the final multi-document summary
takes as input the individual summaries and not the com-
plete documents. (Particularly if the single-document
summaries can contain much overlapping information.)
The Columbia University system (McKeown etal., 1999)
creates a multi-document summary using machine learn-
ing and statistical techniques to identify similar sections
and language generation to reformulate the summary.
The focus of our approach is a multi-document system
that can quickly summarize large clusters of similar doc-
uments (on the order of thousands) while providing the
key relevant useful information or pointers to such in-
formation. Our system (I) primarily uses only domain-
independent techniques, based mainly on fast, statistical
processing, (2) explicitly deals with the issue of reducing
redundancy without eliminating potential relevant infor-
mation, and (3) contains parameterized modules, so that
different genres or corpora characteristics can be taken
into account easily.
</bodyText>
<sectionHeader confidence="0.970558" genericHeader="method">
3 Requirements for Multi-Document
Summarization
</sectionHeader>
<bodyText confidence="0.993393685714286">
There are two types of situations in which multi-
document summarization would be useful: (1) the user
is faced with a collection of dis-similar documents and
wishes to assess the information landscape contained in
the collection, or (2) there is a collection of topically-
related documents, extracted from a larger more diverse
collection as the result of a query, or a topically-cohesive
cluster. In the first case, if the collection is large enough,
it only makes sense to first cluster and categorize the doc-
uments (Yang et al., 1999), and then sample from, or
summarize each cohesive cluster. Hence, a &amp;quot;summary&amp;quot;
would constitute of a visualization of the information
landscape, where features could be clusters or summaries
thereof. In the second case, it is possible to build a syn-
thetic textual summary containing the main point(s) of
the topic, augmented with non-redundant background in-
formation and/or query-relevant elaborations. This is the
focus of our work reported here, including the necessity
to eliminate redundancy among the information content
of multiple related documents.
Users&apos; information seeking needs and goals vary
tremendously. When a group of three people created a
multi-document summarization of 10 articles about the
Microsoft Trial from a given day, one summary focused
on the details presented in court, one on an overall gist
of the day&apos;s events, and the third on a high level view of
the goals and outcome of the trial. Thus, an ideal multi-
document summarization would be able to address the
different levels of detail, which is difficult without natu-
ral language understanding. An interface for the summa-
rization system needs to be able to permit the user to en-
ter information seeking goals, via a query, a background
interest profile and/or a relevance feedback mechanism.
Following is a list of requirements for multi-document
summarization:
</bodyText>
<listItem confidence="0.9957125">
• clustering: The ability to cluster similar documents
and passages to find related information.
• coverage: The ability to find and extract the main
points across documents.
• anti-redundancy: The ability to minimize redun-
dancy between passages in the summary.
</listItem>
<page confidence="0.99473">
41
</page>
<bodyText confidence="0.8694294">
fp, summary cohesion criteria: The ability to combine
text passages in a useful manner for the reader. This
may include:
— document ordering: All text segments of high-
est ranking document, then all segments from
the next highest ranking document, etc.
— news-story principle (rank ordering): present
the most relevant and diverse information first
so that the reader gets the maximal information
content even if they stop reading the summary.
</bodyText>
<listItem confidence="0.986679342105263">
— topic-cohesion: Group together the passages
by topic clustering using passage similarity cri-
teria and present the information by the cluster -
centroid passage rank.
— time line ordering: Text passages ordered
based on the occurrence of events in time. ,
• coherence: Summaries generated should be read-
able and relevant to the user.
• context: Include sufficient context so that the sum-
mary is understandable to the reader.
• identification of source inconsistencies: Articles of-
ten have errors (such as billion reported as million,
etc.); multi-document summarization must be able
to recognize and report source inconsistencies.
• summary updates: A new multi-document summary
must take into account previous summaries in gen-
erating new summaries. In such cases, the system
needs to be able to track and categorize events.
• effective user interfaces:
— Attributability: The user needs to be able to
easily access the source of a given passage.
This could be the single document summary.
— Relationship: The user needs to view related
passages to the text passage shown, which can
highlight source inconsistencies.
— Source Selection: The user needs to be able to
select or eliminate various sources. For exam-
ple, the user may want to eliminate information
from some less reliable foreign news reporting
sources.
— Context: The user needs to be able to zoom
in on the context surrounding the chosen pas-
sages.
— Redirection: The user should be able to high-
light certain parts of the synthetic summary
and give a command to the system indicating
that these parts are to be weighted heavily and
that other parts are to be given a lesser weight.
</listItem>
<sectionHeader confidence="0.948289" genericHeader="method">
4 Types of Multi-Document Summarizers
</sectionHeader>
<bodyText confidence="0.999876777777778">
In the previous section we discussed the requirements
for a multi-document summarization system. Depend-
ing on a user&apos;s information seeking goals, the user may
want to create summaries that contain primarily the com-
mon portions of the documents (their intersection) or an
overview of the entire cluster of documents (a sampling.
of the space that the documents span). A user may also
want to have a highly readable summary, an overview of
pointers (sentences or word lists) to further information,
</bodyText>
<listItem confidence="0.845750361111111">
• or a combination of the two. Following is a list of var-
ious methods of creating multi-document summaries by
extraction:
I. Summary from Common Sections of Documents:
Find the important relevant parts that the cluster of
documents have in common (their intersection) and
use that as a summary.
2. Summary from Common Sections and Unique Sec-
tions of Documents: Find the important relevant
parts that the cluster of documents have in common
and the relevant parts that are unique and use that as
a summary.
3. Centroid Document Summary: Create a single doc-
ument summary from the centroid document in the
cluster.
4. Centroid Document plus Outliers Summary: Cre-
ate a single document summary from the centroid
document in the cluster and add some representa-
tion from outlier documents (passages or keyword
extraction) to provide a fuller coverage of the docu-
ment set.2
5. Latest Document plus Outliers Summary: Create
a single document summary from the latest time
stamped document in the cluster (most recent in-
formation) and add some representation of outlier
documents to provide a fuller coverage of the docu-
ment set.
6. Summary from Common Sections and Unique Sec-
tions of Documents with Time Weighting Factor:
Find the important relevant parts that the cluster of
documents have in common and the relevant parts
that are unique and weight all the information by
the time sequence of the documents in which they
appear and use the result as a summary. This al-
lows the more recent, often updated information to
be more likely to be included in the summary.
</listItem>
<bodyText confidence="0.999945125">
There are also much more complicated types of sum-
mary extracts which involve natural language process-
ing and/or understanding. These types of summaries in-
clude: (1) differing points of view within the document
collection, (2) updates of information within the doc-
ument collection, (3) updates of information from the
document collection with respect to an already provided
summary, (4) the development of an event or subtopic of
</bodyText>
<footnote confidence="0.984521">
2This is similar to the approach of Textwise (TIPSTER, 1998b),
whose multi-document summary consists of the most relevant para-
graph and specialized word lists.
</footnote>
<page confidence="0.998511">
42
</page>
<bodyText confidence="0.999899142857143">
an event (e.g., death tolls) over time, and (5) a compara-
tive development of an event.
Naturally, an ideal multi-document summary would
include a natural language generation component to cre-
ate cohesive readable summaries (Radev and McKeown,
1998; McKeown et al., 1999). Our current focus is on
the extraction of the relevant passages.
</bodyText>
<sectionHeader confidence="0.989901" genericHeader="method">
5 System Design
</sectionHeader>
<bodyText confidence="0.999966840909091">
In the previous sections we discussed the requirements
and types of multi-document summarization systems.
This section discusses our current implementation of
a multi-document summarization system which is de-
signed to produce summaries that emphasize &amp;quot;relevant
novelty.&amp;quot; Relevant novelty is a metric for minimizing re-
dundancy and maximizing both relevance and diversity.
A first approximation to measuring relevant novelty is to
measure relevance and novelty independently and pro-
vide a linear combination as the metric. We call this lin-
ear combination &amp;quot;marginal relevance&amp;quot; , i.e., a text pas-
sage has high marginal relevance if it is both relevant to
the query and useful for a summary, while having mini-
mal similarity to previously selected passages. Using this
metric one can maximize marginal relevance in retrieval
and summarization, hence we label our method &amp;quot;maxi-
mal marginal relevance&amp;quot; (MMR) (Carbonell and Gold-
stein, 1998).
The Maximal Marginal Relevance Multi-Document
(MMR-MD) metric is defined in Figure 1. Simi and
Sim2 cover some of the properties that we discussed in
Section 3.3
For Simi, the first term is the cosine similarity metric
for query and document. The second term computes a
coverage score for the passage by whether the passage
is in one or more clusters and the size of the cluster.
The third term reflects the information content of the pas-
sage by taking into account both statistical and linguis-
tic features for summary inclusion (such as query expan-
.sion, position of the passage in the document and pres-
ence/absence of named-entities in the passage). The final
term indicates the temporal sequence of the document in
the collection allowing for more recent information to
have higher weights.
For Sim2, the first term uses the cosine similarity met-
ric to compute the similarity between the passage and
previously selected passages. (This helps the system to
minimize the possibility of including passages similar to
ones already selected.) The second term penalizes pas-
sages that are part of clusters from which other passages
have already been chosen. The third term penalizes doc-
uments from which passages have already been selected;
however, the penalty is inversely proportional to docu-
ment length, to allow the possibility of longer documents
</bodyText>
<figureCaption confidence="0.472728">
3Sim1 and Sim2 as previously defined in MMR for single-
document summarization contained only the first term of each equa-
tion:
</figureCaption>
<bodyText confidence="0.9959813">
contributing more passages. These latter two terms allow
for a fuller coverage of the clusters and documents.
Given the above definition, MMR-MD incrementally
computes the standard relevance-ranked list — plus some
additional scoring factors — when the parameter A=1, and
computes a maximal diversity ranking among the pas-
sages in the documents when A=0. For intermediate val-
ues of A in the interval [0,1], a linear combination of both
criteria is optimized. In order to sample the information
space in the general vicinity of the query, small values of
A can be used; to focus on multiple, potentially overlap-
ping or reinforcing relevant passages, A can be set to a
value closer to 1. We found that a particularly effective
search strategy for document retrieval is to start with a
small A (e.g., A = .3) in order to understand the informa-
tion space in the region of the query, and then to focus
, on the most important parts using a reformulated query
(possibly via relevance feedback) and a larger value of A
(e.g., A = .7) (Carbonell and Goldstein, 1998).
Our multi-document summarizer works as follows:
</bodyText>
<listItem confidence="0.963843411764706">
• Segment the documents into passages, and index
them using inverted indices (as used by the IR
engine). Passages may be phrases, sentences, n-
sentence chunks, or paragraphs.
• Identify the passages relevant to the query using
cosine similarity with a threshold below which the
passages are discarded.
• Apply the MMR-MD metric as defined above. De-
pending on the desired length of the summary, se-
lect a number of passages to compute passage re-
dundancy using the cosine similarity metric and use
the passage similarity scoring as a method of clus-
tering passages. Users can select the number of pas-
sages or the amount of compression.
• Reassemble the selected passages into a summary
document using one of the summary-cohesion cri-
teria (see Section 3).
</listItem>
<bodyText confidence="0.9999164">
The results reported in this paper are based on the use
of the SMART search engine (Buckley, 1985) to compute
cosine similarities (with a SMART weighting of inn for
both queries and passages), stopwords eliminated from
the indexed data and stemming turned on.
</bodyText>
<sectionHeader confidence="0.999637" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.9999344">
The TIPSTER evaluation corpus provided several sets of
topical clusters to which we applied MMR-MD summa-
rization. As an example, consider a set of 200 apartheid-
related news-wire documents from the Associated Press
and the Wall Street Journal, spanning the period from
1988 to 1992. We used the TIPSTER provided topic de-
scription as the query. These 200 documents were on
an average 31 sentences in length, with a total of 6115
sentences. We used the sentence as our summary unit.
Generating a summary 10 sentences long resulted in a
</bodyText>
<page confidence="0.988988">
43
</page>
<equation confidence="0.954636538461538">
MMR—MD Arg max [A(Simi , Q, C1,, Di, D)) — (1 — A) max Sirn2 Pnrn) C7 S, DO)]
PiiER\S P„,„ES
Simi (Pii , D) = w1*(P1i -Q)+w2*coverage(Pii ,Cii)+w3*content(Pii)-Fw4*time_seguence(Di , D)
SEM2(P13)Pnm, C, S, Di) = wa * Pnm)+ Wb* clusters_selected(C-ij , S)+ we* documents_selected(Di , S)
coverage(Pii , C) = E wk*Ikl
_ kEc,,
content(P13) =- E Wtype(W)
WE)&apos;,,
iirneSiaMp(Dynartime) — timestamp(A)
time_seguence(Di , =
v,w:P„,„ES
1
documents_selected(Di , S) = *E[Pia, E
</equation>
<bodyText confidence="0.9734436">
where
Simi is the similarity metric for relevance ranking
Sim2 is the anti-redundancy metric
D is a document collection
P is the passages from the documents in that collection (e.g., Pii is passage j from document DO
Q is a query or user profile
R = I R(D, P, Q, 0), i.e., the ranked list of passages from documents retrieved by an IR system, given D, P,Q and a
relevance threshold 0, below which it will not retrieve passages (0 can be degree of match or number of passages)
S is the subset of passages in R already selected
R\S is the set difference, i.e., the set of as yet unselected passages in R
</bodyText>
<listItem confidence="0.866668">
• C is the set of passage clusters for the set of documents
</listItem>
<bodyText confidence="0.959339">
C„,„ is the subset of clusters of C that contains passage
C,, is the subset of clusters that contain passages from document Dv
IkI is the number of passages in the individual cluster k
IC„,„ n ciii is the number of clusters in the intersection of C„andCii
wi..are weights for the terms, which can be optimized
W is a word in the passage Pii
type is a particular type of word, e.g., city name
IDi I is the length of document i.
</bodyText>
<figureCaption confidence="0.995081">
Figure 1: Definition of multi-document summarization algorithm - MMR-MD
</figureCaption>
<equation confidence="0.9884905">
timestamp(Dmaxtime) — timestamp(Dmintime)
clusters_selected(Cii , S) = ICii n UCvwd
</equation>
<bodyText confidence="0.999867416666667">
sentence compression ratio of 0.2% and a character com-
pression of 0.3%, approximately two orders of magni-
tude different with compression ratios used in single doc-
ument summarization. The results of summarizing this
document set with a value of A set to 1 (effectively query
relevance, but no MMR-MD) and A set to 0.3 (both query
relevance and MMR-MD anti-redundancy) are shown in
Figures 2 and 3 respectively. The summary in Figure 2
clearly illustrates the need for reducing redundancy and
maximizing novel information.
Consider for instance, the summary shown in Figure 2.
The fact that the ANC is fighting to overthrow the gov-
</bodyText>
<page confidence="0.996401">
44
</page>
<listItem confidence="0.989492">
1. WSJ910204-0176: 1 CAPE TOWN, South Africa — President F.W. de Klerk&apos;s proposal to repeal the major pillars
of apartheid drew a generally positive response from black leaders, but African National Congress leader Nelson
Mandela called on the international community to continue economic sanctions against South Africa until the
government takes further steps.
2. AP880803-0082: 25 Three Canadian anti-apartheid groups issued a statement urging the government to sever
diplomatic and economic links with South Africa and aid the African National Congress, the banned group fighting
the white-dominated government in South Africa.
3. AP880803-0080: 25 Three Canadian anti-apartheid groups issued a statement urging the government to sever
diplomatic and economic links with South Africa and aid the African National Congress, the banned group fighting
the white-dominated government in South Africa.
4. AP880802-0165: 23 South Africa says the ANC, the main black group fighting to overthrow South Africa&apos;s white
government, has seven major military bases in Angola, and the Pretoria government wants those bases closed
down.
5. AP880212-0060: 14 ANGOP quoted the Angolan statement as saying the main causes of confict in the region
are South Africa&apos;s &amp;quot;illegal occupation&amp;quot; of Namibia, South African attacks against its black-ruled neighbors and
its alleged creation of armed groups to carry out &amp;quot;terrorist aCtivities&amp;quot; in those countries, and the denial of political
rights to the black majority in South Africa.
6. AP880823-0069: 17 The ANC is the main guerrilla group fighting to overthrow the South African government
and end apartheid, the system of racial segregation in which South Africa&apos;s black majority has no vote in national
affairs.
7. AP880803-0158: 26 South Africa says the ANC, the main black group fighting to overthrow South Africa&apos;s white-
led government, has seven major military bases in Angola, and it wants those bases closed down.
8. AP880613-0126: 15 The ANC is fighting to topple the South African government and its policy of apartheid,
under which the nation&apos;s 26 million blacks have no voice in national affairs and the 5 million whites control the
economy and dominate government.
9. AP880212-0060: 13 The African National Congress is the main rebel movement fighting South Africa&apos;s white-led
government and SWAPO is a black guerrilla group fighting for independence for Namibia, which is administered
by South Africa.
10. WSJ870129-0051: 1 Secretary of State George Shultz, in a meeting with Oliver Tambo, head of the African
National Congress, voiced concerns about Soviet influence on the black South African group and the ANC&apos;s use
of violence in the struggle against apartheid.
</listItem>
<figureCaption confidence="0.708777">
Figure 2: Sample multi-document summary with A = I, news-story-principle ordering (rank order)
</figureCaption>
<listItem confidence="0.860493166666667">
• ernment is mentioned seven times (sentences #2,—#4,#6—
#9);-which constitutes 70% of the sentences in the sum-
mary. Furthermore, sentence #3 is an exact duplicate of
sentence #2, and sentence #7 is almost identical to sen-
tence #4. In contrast, the summary in Figure 3, generated
using MMR-MD with a value of A set to 0.3 shows sig-
nificant improvements in eliminating redundancy. The
fact that the ANC is fighting to overthrow the govern-
ment is mentioned only twice (sentences #3,#7), and one
of these sentences has additional information in it. The
new summary retained only three of the sentences from
the earlier summary.
</listItem>
<bodyText confidence="0.999737041666667">
Counting clearly distinct propositions in both cases,
yields a 60% greater information content for the MMR-
MD case, though both summaries are equivalent in
length.
When these 200 documents were added to a set of 4
other topics of 200 documents, yielding a document-set
with 1000 documents, the query relevant multi-document
summarization system produced exactly the same re-
sults.
We are currently working on constructing datasets for
experimental evaluations of multi-document summariza-
tion. In order to construct these data sets, we attempted
to categorize user&apos;s information seeking goals for multi-
document summarization (see Section 3). As can be seen
in Figure 2, the standard IR technique of using a query to
extract relevant passages is no longer sufficient for multi-
document summarization due to redundancy. In addi-
tion, query relevant extractions cannot capture temporal
sequencing. The data sets will allow us to measure the
effects of these, and other features, on multi-document
summarization quality.
Specifically, we are constructing sets of 10 documents,
which either contain a snapshot of an event from mul-
tiple sources or the unfoldment of an event over time.
</bodyText>
<page confidence="0.996874">
45
</page>
<listItem confidence="0.8998050625">
1. WSJ870129-0051 1 Secretary of State George Shultz, in a meeting with Oliver Tambo, head of the African Na-
tional Congress, voiced concerns about Soviet influence on the black South African group and the ANC&apos;s use of
violence in the struggle against apartheid.
2. WSJ880422-0133 44 (See related story: &amp;quot;ANC: Apartheid&apos;s Foes – The Long Struggle: The ANC Is Banned,
But It Is in the Hearts of a Nation&apos;s Blacks — In South Africa, the Group Survives Assassinations, Government
Crackdowns— The Black, Green and Gold&amp;quot; – WSJ April 22, 1988)
3. AP880803-0158 26 South Africa says the ANC, the main black group fighting to overthrow South Africa&apos;s white-
led government, has seven major military bases in Angola, and it wants those bases closed down. •
4. AP880919-0052 5 But activist clergymen from South Africa said the pontiff should have spoken out more force-
fully against their white-minority government&apos;s policies of apartheid, under which 26 million blacks have no say
in national affairs.
5. AP890821-0092 10 Besides ending the emergency and lifting bans on anti- apartheid groups and individual ac-
tivists, the Harare summit&apos;s conditions included the removal of all troops from South Africa&apos;s black townships,
releasing all political prisoners and ending political trials and executions, and a government commitment to free
political discussion.
6. WSJ900503-0041 11 Pretoria and the ANC remainfar apart on their vision s for a post-apartheid South Africa:
</listItem>
<bodyText confidence="0.780756307692308">
The ANC wants a simple one-man, one-vote majority rule system, while the government claims that will lead to
black domination and insists on constitutional protection of the rights of minorities, including the whites.
7. W5J900807-0037 1 JOHANNESBURG, South Africa – The African National Congress suspended its 30-year
armed struggle against the white minority government, clearing the way for the start of negotiations over a new
constitution based on black-white power sharing.
8. WSJ900924-0119 20 The African National Congress, South Africa&apos;s main black liberation group, forged its sanc-
tions strategy as a means of pressuring the government to abandon white-minority rule.
9. WSJ910702-0053 36 At a‘ meeting in South Africa this week, the African National Congress, the major black
group, is expected to take a tough line again St the white-run government.
10. WSJ910204-0176 1 CAPE TOWN, South Africa – President F.W. de Klerk&apos;s proposal to repeal the major pillars
of apartheid drew a generally positive response from black leaders, but African National Congress leader Nelson
Mandela called on the international community to continue economic sanctions against South Africa until the
government takes further steps.
</bodyText>
<figureCaption confidence="0.997544">
Figure 3: Sample multi-document summary with A = 0.3, time-line ordering
</figureCaption>
<bodyText confidence="0.999958190476191">
From these sets we are performing two types of exper-
iments. In the first, we are examining how users put
sentences into pre-defined clusters and how they create
sentence based multi-document summaries. The result
will also serve as a gold standard for system generated
summaries - do our systems pick the same summary sen-
tences as humans and are they picking sentences from
the same clusters as humans? The second type of exper-
iment is designed to determine how users perceive the
output summary quality. In this experiment, users are
asked to rate the output sentences from the summarizer
as good, okay or bad. For the okay or bad sentences,
they are asked to provide a summary sentence from the
document set that is &amp;quot;better&amp;quot;, i.e., that makes a better set
of sentences to represent the information content of the
document set. We are comparing our proposed summa-
rizer #6 in Section 4 to summarizer #1, the common por-
tions of the document sets with no anti-redundancy and
summarizer #3, single document summary of a centroid
document using our single document summarizer (Gold-
stein et al., 1999).
</bodyText>
<sectionHeader confidence="0.997257" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9999942">
This paper presented a statistical method of generating
extraction based multi-document summaries. It builds
upon previous work in single-document summarization
and takes into account some of the major differences be-
tween single-document and multi-document summariza-
tion: (i) the need to carefully eliminate redundant infor-
mation from multiple documents, and achieve high com-
pression ratios, (ii) take into account information about
document and passage similarities, and weight different
passages accordingly, and (iii) take temporal information
into account.
Our approach differs from others in several ways: it
is completely domain-independent, is based mainly on
fast, statistical processing, it attempts to maximize the
novelty of the information being selected, and different
</bodyText>
<page confidence="0.997689">
46
</page>
<bodyText confidence="0.999968857142857">
genres or corpora characteristics can be taken into ac-
count easily. Since our system is not based on the use of
sophisticated natural language understanding or informa-
tion extraction techniques, summaries lack co-reference
resolution, passages may be disjoint from one another,
and in some cases may have false implicature.
In future work, we will integrate work on. multi-
document summarization with work on clustering to pro-
vide summaries for clusters produced by topic detection
and tracking. We also plan to investigate how to gen-
erate coherent temporally based event summaries. We
will also investigate how users can effectively use multi-
document summarization through interactive interfaces
to browse and explore large document sets.
</bodyText>
<sectionHeader confidence="0.999263" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.89417784375">
James Allan, Jaime Carbonell, George Doddington,
Jonathan Yamron, and Yiming Yang. 1998. Topic de-
tection and tracking pilot study: Final report. In Pro-
ceedings of the DARPA Broadcast News Transcription
and Understanding Workshop.
Chinatsu Aone, M. E. Okurowski, J. Gorlinslcy, and
B. Larsen. 1997. A scalable summarization sys-
tem using robust NLP. In Proceedings of the
ACL&apos;97/EACL&apos;97 Workshop on Intelligent Scalable
Text Summarization, pages 66-73, Madrid, Spain.
Breck Baldwin and Thomas S. Morton. 1998. Dy-
namic coreference-based summarization. In Proceed-
ings of the Third Conference on Empirical Methods in
Natural Language Processing (EMNLP-3), Granada,
Spain, June.
Regina Barzilay and Michael Elhadad. 1997. Using lex-
ical chains for text summarization. In Proceedings of
the ACL&apos;97/EACL&apos;97 Workshop on Intelligent Scal-
able Text Summarization, pages 10-17, Madrid, Spain.
Branimir Boguraev and Chris Kennedy. 1997. Salience
based content characterization of text documents. In
Proceedings of the ACL&apos;97/EACL&apos;97 Workshop on
Intelligent Scalable Text Summarization, pages 2-9,.
Madrid, Spain.
Chris Buckley. 1985. Implementation of the SMART in-
formation retrieval system. Technical Report TR 85-
686, Cornell University.
Jaime G. Carbonell and Jade Goldstein. 1998. The
use of MMR, diversity-based reranking for reordering
documents and producing summaries. In Proceedings
of SIGIR-98, Melbourne, Australia, August.
Jade Goldstein and Jaime Carbonell. 1998. The use
of mmr and diversity-based reranking in document
reranking and summarization. In Proceedings of the
14th Twente Workshop on Language Technology in
Multimedia Information Retrieval, pages 152-166,
Enschede, the Netherlands, December.
Jade Goldstein, Mark Kantrowitz, Vibhu 0. Mittal, and
Jaime G. Carbonell. 1999. Summarizing Text Doc-
uments: Sentence Selection and Evaluation Metrics.
In Proceedings of the 22nd International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval (SIGIR-99), pages 121-128, Berkeley,
CA.
Eduard Hovy and Chin-Yew Lin. 1997. Automated text
summarization in SUMMARIST. In ACL/EACL-97
Workshop on Intelligent Scalable Text Summarization,
pages 18-24, Madrid, Spain, July.
Judith L. Klavans and James Shaw. 1995. Lexical se-
mantics in summarization. In Proceedings of the First
Annual Workshop of the IFIP Working Group FOR
NLP and KR, Nantes, France, April.
Julian M. Kupiec, Jan Pedersen, and Francine Chen.
1995. A trainable document summarizer. In Proceed-
ings of the 18th Annual Int. ACM/SIGIR Conference
on Research and Development in IR, pages 68-73,
Seattle, WA, July.
P. H. Luhn. 1958. Automatic creation of literature ab-
stracts. IBM Journal, pages 159-165.
Inderjeet Mani and Eric Bloedern. 1997. Multi-
document summarization by graph search and merg-
ing. In Proceedings of AAAI-97, pages 622-628.
AAAI.
Inderjeet Mani and Eric Bloedorn. 1999. Summarizing
similarities and differences among related documents.
Information Retrieval, 1:35-67.
Daniel Marcu. 1997. From discourse structures to text
summaries. In Proceedings of the ACL&apos;97/EACL&apos;97
Workshop on Intelligent Scalable Text Summarization,
pages 82-88, Madrid, Spain.
Kathleen McKeown, Jacques Robin, and Karen Kukich.
1995. Designing and evaluating a new revision-based
model for summary generation. Info. Proc. and Man-
agement, 31(5).
Kathleen McKeown, Judith Klavans, Vasileios Hatzivas-
siloglou, Regina Barzilay, and Eleazar Eslcin. 1999.
Towards Multidocument Summarization by Reformu-
lation: Progress and Prospects. In Proceedings of
AAAI-99, pages 453-460, Orlando, FL, July.
Mandar Mitra, Amit Singhal, and Chris Buckley. 1997.
Automatic text summarization by paragraph extrac-
tion. In ACL/EACL-97 Workshop on Intelligent Scal-
able Text Summarization, pages 31-36, Madrid, Spain,
July.
Chris D. Paice. 1990. Constructing literature abstracts
by computer: Techniques and prospects. Info. Proc.
and Management, 26:171-186.
Dragomir Radev and Kathy McKeown. 1998. Generat-
ing natural language summaries from multiple online
sources. Compuutational Linguistics.
Gerald Salton. 1970. Automatic processing of foreign
language docuemnts. Journal of American Society for
Information Sciences, 21:187-194.
Gerald Salton. 1989. Automatic Text Processing: The
Transformation, Analysis, and Retrieval of Informa-
tion by Computer. Addison-Wesley.
</reference>
<page confidence="0.986648">
47
</page>
<reference confidence="0.999639324324324">
James Shaw. 1995. Conciseness through aggregation in
text generation. In Proceedings of 33rd Association
for Computational Linguistics, pages 329-331.
Gees C. Stein, Tomek Strzalkowski, and G. Bowden
Wise. 1999. Summarizing Multiple Documents Us-
ing Text Extraction and Interactive Clustering. In Pro-
ceedings of PacLing-99: The Pacific Rim Conference
on Computational Linguistics, pages 200-208, Water-
loo, Canada.
Tomek Strzalkowski, Jin Wang, and Bowden Wise.
1998. A robust practical text summarization system.
In AAAI Intelligent Text Summarization Workshop,
pages 26-30, Stanford, CA, March.
J. I. Tait. 1983. Automatic Summarizing of English
Texts. Ph.D. thesis, University of Cambridge, Cam-
bridge, UK.
Simone Teufel and Marc Moens. 1997. Sentence ex-
traction as a classification task. In ACIJEACL-97
Workshop on Intelligent Scalable Text Summarization,
pages 58-65, Madrid, Spain, July.
TIPSTER. 1998a. Tipster text phase III 18-month work-
shop notes, May. Fairfax, VA.
TIPSTER. 1998b. Tipster text phase III 24-month work-
shop notes, October. Baltimore, MD.
Charles J. van Rijsbergen. 1979. Information Retrieval.
Butterworths, London.
Yiming Yang, Tom Pierce, and Jaime G. Carbonell.
1998. A study on retrospective and on-line event de-
tection. In Proceedings of the 21th Ann Int ACM SI-
GIR Conference on Research and Development in In-
formation Retrieval (SIGIR&apos;98), pages 28-36.
,Yiming Yang, Jaime G. Carbonell, Ralf D. Brown,
Tom Pierce, Brian T. Archibald, and Xin Liu. 1999.
Learning approaches for topic detection and tracking
news events. IEEE intelligent Systems, Special Issue
on Applications of Intelligent Information Retrieval,
14(4):32-43, July/August.
</reference>
<page confidence="0.999351">
48
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.374611">
<title confidence="0.999526">Multi-Document Summarization By Sentence Extraction</title>
<author confidence="0.999921">Jade Goldstein Vibhu Mittalt Jaime Carbonell Mark Kantrowitzt</author>
<email confidence="0.994642">jade@cs.cmu.edumittal@jprc.comjgc@cs.cmu.edumkant@jprc.com</email>
<affiliation confidence="0.8354545">Language Technologies Institute tJust Research Carnegie Mellon University 4616 Henry Street</affiliation>
<address confidence="0.9992">Pittsburgh, PA 15213 Pittsburgh, PA 15213</address>
<affiliation confidence="0.56308">U.S.A. U.S.A.</affiliation>
<abstract confidence="0.997345133333333">This paper discusses a text extraction approach to multidocument summarization that builds on single-document methods by using additional, available formation about the document set as a whole and the relationships between the documents. Multi-document summarization differs from single in that the issues of compression, speed, redundancy and passage selection are critical in the formation of useful summaries. Our approach addresses these issues by using domainindependent techniques based mainly on fast, statistical processing, a metric for reducing redundancy and maximizing diversity in the selected passages, and a modular framework to allow easy parameterization for different genres, corpora characteristics and user requirements.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Allan</author>
<author>Jaime Carbonell</author>
<author>George Doddington</author>
<author>Jonathan Yamron</author>
<author>Yiming Yang</author>
</authors>
<title>Topic detection and tracking pilot study: Final report.</title>
<date>1998</date>
<booktitle>In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop.</booktitle>
<marker>Allan, Carbonell, Doddington, Yamron, Yang, 1998</marker>
<rawString>James Allan, Jaime Carbonell, George Doddington, Jonathan Yamron, and Yiming Yang. 1998. Topic detection and tracking pilot study: Final report. In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chinatsu Aone</author>
<author>M E Okurowski</author>
<author>J Gorlinslcy</author>
<author>B Larsen</author>
</authors>
<title>A scalable summarization system using robust NLP.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL&apos;97/EACL&apos;97 Workshop on Intelligent Scalable Text Summarization,</booktitle>
<pages>66--73</pages>
<location>Madrid,</location>
<contexts>
<context position="2412" citStr="Aone et al., 1997" startWordPosition="344" endWordPosition="347">n a summarization system are greater in a true IR or topic-detection context (Yang et al., 1998; Allan etal., 1998). Consider the situation where the user issues a search query, for instance on a news topic, and the retrieval system finds hundreds of closely-ranked documents in response. Many of these documents are likely to repeat much the same information, while differing in certain &apos;Most of these were based on statistical techniques applied to various document entities; examples include (Tait, 1983; Kupiec et al., 1995; Paice, 1990; Klavans and Shaw, 1995; McKeown et al., 1995; Shaw, 1995; Aone et al., 1997; Boguraev and Kennedy, 1997; Hovy and Lin, 1997; Mitra et al., 1997; Teufel and Moens, 1997; Barzilay and Elhadad, 1997; Carbonell and Goldstein, 1998; Baldwin and Morton, 1998; Radev and McKeown, 1998; Strzalkowski et al., 1998). parts. Summaries of the individual documents would help, but are likely to be very similar to each other, unless the summarization system takes into account other summaries that have already been generated. Multidocument summarization — capable of summarizing either complete documents sets, or single documents in the context of previously summarized ones — are likel</context>
</contexts>
<marker>Aone, Okurowski, Gorlinslcy, Larsen, 1997</marker>
<rawString>Chinatsu Aone, M. E. Okurowski, J. Gorlinslcy, and B. Larsen. 1997. A scalable summarization system using robust NLP. In Proceedings of the ACL&apos;97/EACL&apos;97 Workshop on Intelligent Scalable Text Summarization, pages 66-73, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Breck Baldwin</author>
<author>Thomas S Morton</author>
</authors>
<title>Dynamic coreference-based summarization.</title>
<date>1998</date>
<booktitle>In Proceedings of the Third Conference on Empirical Methods in Natural Language Processing (EMNLP-3),</booktitle>
<location>Granada, Spain,</location>
<contexts>
<context position="2589" citStr="Baldwin and Morton, 1998" startWordPosition="372" endWordPosition="376"> query, for instance on a news topic, and the retrieval system finds hundreds of closely-ranked documents in response. Many of these documents are likely to repeat much the same information, while differing in certain &apos;Most of these were based on statistical techniques applied to various document entities; examples include (Tait, 1983; Kupiec et al., 1995; Paice, 1990; Klavans and Shaw, 1995; McKeown et al., 1995; Shaw, 1995; Aone et al., 1997; Boguraev and Kennedy, 1997; Hovy and Lin, 1997; Mitra et al., 1997; Teufel and Moens, 1997; Barzilay and Elhadad, 1997; Carbonell and Goldstein, 1998; Baldwin and Morton, 1998; Radev and McKeown, 1998; Strzalkowski et al., 1998). parts. Summaries of the individual documents would help, but are likely to be very similar to each other, unless the summarization system takes into account other summaries that have already been generated. Multidocument summarization — capable of summarizing either complete documents sets, or single documents in the context of previously summarized ones — are likely to be essential in such situations. Ideally, multi-document summaries should contain the key shared relevant information among all the documents only once, plus other informat</context>
<context position="4627" citStr="Baldwin and Morton, 1998" startWordPosition="691" endWordPosition="694"> summary with respect to the size of the document set) will typically be much smaller for collections of dozens or hundreds of topically related documents than for single document summaries. The SUMMAC evaluation (TIPSTER, 1998a) tested 10% compression summaries, but in our work summarizing 200- document clusters, we find that compression to the 1% or 0.1% level is required. Summarization becomes significantly more difficult when compression demands increase. 4. The co-reference problem in summarization presents even greater challenges for multidocument than for single-document summarization (Baldwin and Morton, 1998). This paper discusses an approach to multi-document summarization that builds on previous work in single40 document summarization by using additional, available information about the document set as a whole, the relationships between the documents, as well as individual documents. 2 Background and Related Work Generating an effective summary requires the summarizer to select, evaluate, order and aggregate items of information according to their relevance to a particular subject or purpose. These tasks can either be approximated by IR techniques or done in greater depth with fuller natural lan</context>
</contexts>
<marker>Baldwin, Morton, 1998</marker>
<rawString>Breck Baldwin and Thomas S. Morton. 1998. Dynamic coreference-based summarization. In Proceedings of the Third Conference on Empirical Methods in Natural Language Processing (EMNLP-3), Granada, Spain, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Michael Elhadad</author>
</authors>
<title>Using lexical chains for text summarization.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL&apos;97/EACL&apos;97 Workshop on Intelligent Scalable Text Summarization,</booktitle>
<pages>10--17</pages>
<location>Madrid,</location>
<contexts>
<context position="2532" citStr="Barzilay and Elhadad, 1997" startWordPosition="364" endWordPosition="367">998). Consider the situation where the user issues a search query, for instance on a news topic, and the retrieval system finds hundreds of closely-ranked documents in response. Many of these documents are likely to repeat much the same information, while differing in certain &apos;Most of these were based on statistical techniques applied to various document entities; examples include (Tait, 1983; Kupiec et al., 1995; Paice, 1990; Klavans and Shaw, 1995; McKeown et al., 1995; Shaw, 1995; Aone et al., 1997; Boguraev and Kennedy, 1997; Hovy and Lin, 1997; Mitra et al., 1997; Teufel and Moens, 1997; Barzilay and Elhadad, 1997; Carbonell and Goldstein, 1998; Baldwin and Morton, 1998; Radev and McKeown, 1998; Strzalkowski et al., 1998). parts. Summaries of the individual documents would help, but are likely to be very similar to each other, unless the summarization system takes into account other summaries that have already been generated. Multidocument summarization — capable of summarizing either complete documents sets, or single documents in the context of previously summarized ones — are likely to be essential in such situations. Ideally, multi-document summaries should contain the key shared relevant informati</context>
<context position="6149" citStr="Barzilay and Elhadad, 1997" startWordPosition="926" endWordPosition="929">ary. Work on automated document summarization by text span extraction dates back at least to work at IBM in the fifties (Luhn, 1958). Most of the work in sentence extraction applied statistical techniques (frequency analysis, variance analysis, etc.) to linguistic units such as tokens, names, anaphora, etc. More recently, other approaches have investigated the utility of discourse structure (Marcu, 1997), the combination of information extraction and language generation (Klavans and Shaw, 1995; McKeown et al., 1995), and using machine learning to find patterns in text (Teufel and Moens, 1997; Barzilay and Elhadad, 1997; Strzalkowski et al., 1998). Some of these approaches to single document summarization have been extended to deal with multi-document summarization (Mani and Bloedern, 1997; Goldstein and Carbonell, 1998; TIPSTER, 1998b; Radev and McKeown, 1998; Mani and Bloedorn, 1999; McKeown et al., ,1999; Stein et al., 1999). These include comparing templates filled in by extracting information — using specialized, domain specific knowledge sources — from the doc&apos;ument, and then generating natural language summaries from the templates (Radev and McKeown, 1998), corn— . paring named-entities — extracted us</context>
</contexts>
<marker>Barzilay, Elhadad, 1997</marker>
<rawString>Regina Barzilay and Michael Elhadad. 1997. Using lexical chains for text summarization. In Proceedings of the ACL&apos;97/EACL&apos;97 Workshop on Intelligent Scalable Text Summarization, pages 10-17, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Branimir Boguraev</author>
<author>Chris Kennedy</author>
</authors>
<title>Salience based content characterization of text documents.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL&apos;97/EACL&apos;97 Workshop on Intelligent Scalable Text Summarization,</booktitle>
<pages>2--9</pages>
<location>Madrid,</location>
<contexts>
<context position="2440" citStr="Boguraev and Kennedy, 1997" startWordPosition="348" endWordPosition="351">ystem are greater in a true IR or topic-detection context (Yang et al., 1998; Allan etal., 1998). Consider the situation where the user issues a search query, for instance on a news topic, and the retrieval system finds hundreds of closely-ranked documents in response. Many of these documents are likely to repeat much the same information, while differing in certain &apos;Most of these were based on statistical techniques applied to various document entities; examples include (Tait, 1983; Kupiec et al., 1995; Paice, 1990; Klavans and Shaw, 1995; McKeown et al., 1995; Shaw, 1995; Aone et al., 1997; Boguraev and Kennedy, 1997; Hovy and Lin, 1997; Mitra et al., 1997; Teufel and Moens, 1997; Barzilay and Elhadad, 1997; Carbonell and Goldstein, 1998; Baldwin and Morton, 1998; Radev and McKeown, 1998; Strzalkowski et al., 1998). parts. Summaries of the individual documents would help, but are likely to be very similar to each other, unless the summarization system takes into account other summaries that have already been generated. Multidocument summarization — capable of summarizing either complete documents sets, or single documents in the context of previously summarized ones — are likely to be essential in such si</context>
</contexts>
<marker>Boguraev, Kennedy, 1997</marker>
<rawString>Branimir Boguraev and Chris Kennedy. 1997. Salience based content characterization of text documents. In Proceedings of the ACL&apos;97/EACL&apos;97 Workshop on Intelligent Scalable Text Summarization, pages 2-9,. Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Buckley</author>
</authors>
<title>Implementation of the SMART information retrieval system.</title>
<date>1985</date>
<tech>Technical Report TR 85-686,</tech>
<institution>Cornell University.</institution>
<contexts>
<context position="1428" citStr="Buckley, 1985" startWordPosition="190" endWordPosition="191">mainindependent techniques based mainly on fast, statistical processing, a metric for reducing redundancy and maximizing diversity in the selected passages, and a modular framework to allow easy parameterization for different genres, corpora characteristics and user requirements. 1 Introduction With the continuing growth of online information, it has become increasingly important to provide improved mechanisms to find and present textual information effectively. Conventional IR systems find and rank documents based on maximizing relevance to the user query (Salton, 1970; van Rijsbergen, 1979; Buckley, 1985; Salton, 1989). Some systems also include sub-document relevance assessments and convey this information to the user. More recently, single document summarization systems provide an automated generic abstract or a queryrelevant summary (TIPSTER, 1998a),I However, largescale IR and summarization have not yet been truly integrated, and the functionality challenges on a summarization system are greater in a true IR or topic-detection context (Yang et al., 1998; Allan etal., 1998). Consider the situation where the user issues a search query, for instance on a news topic, and the retrieval system </context>
<context position="20005" citStr="Buckley, 1985" startWordPosition="3142" endWordPosition="3143">imilarity with a threshold below which the passages are discarded. • Apply the MMR-MD metric as defined above. Depending on the desired length of the summary, select a number of passages to compute passage redundancy using the cosine similarity metric and use the passage similarity scoring as a method of clustering passages. Users can select the number of passages or the amount of compression. • Reassemble the selected passages into a summary document using one of the summary-cohesion criteria (see Section 3). The results reported in this paper are based on the use of the SMART search engine (Buckley, 1985) to compute cosine similarities (with a SMART weighting of inn for both queries and passages), stopwords eliminated from the indexed data and stemming turned on. 6 Discussion The TIPSTER evaluation corpus provided several sets of topical clusters to which we applied MMR-MD summarization. As an example, consider a set of 200 apartheidrelated news-wire documents from the Associated Press and the Wall Street Journal, spanning the period from 1988 to 1992. We used the TIPSTER provided topic description as the query. These 200 documents were on an average 31 sentences in length, with a total of 611</context>
</contexts>
<marker>Buckley, 1985</marker>
<rawString>Chris Buckley. 1985. Implementation of the SMART information retrieval system. Technical Report TR 85-686, Cornell University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaime G Carbonell</author>
<author>Jade Goldstein</author>
</authors>
<title>The use of MMR, diversity-based reranking for reordering documents and producing summaries.</title>
<date>1998</date>
<booktitle>In Proceedings of SIGIR-98,</booktitle>
<location>Melbourne, Australia,</location>
<contexts>
<context position="2563" citStr="Carbonell and Goldstein, 1998" startWordPosition="368" endWordPosition="371"> where the user issues a search query, for instance on a news topic, and the retrieval system finds hundreds of closely-ranked documents in response. Many of these documents are likely to repeat much the same information, while differing in certain &apos;Most of these were based on statistical techniques applied to various document entities; examples include (Tait, 1983; Kupiec et al., 1995; Paice, 1990; Klavans and Shaw, 1995; McKeown et al., 1995; Shaw, 1995; Aone et al., 1997; Boguraev and Kennedy, 1997; Hovy and Lin, 1997; Mitra et al., 1997; Teufel and Moens, 1997; Barzilay and Elhadad, 1997; Carbonell and Goldstein, 1998; Baldwin and Morton, 1998; Radev and McKeown, 1998; Strzalkowski et al., 1998). parts. Summaries of the individual documents would help, but are likely to be very similar to each other, unless the summarization system takes into account other summaries that have already been generated. Multidocument summarization — capable of summarizing either complete documents sets, or single documents in the context of previously summarized ones — are likely to be essential in such situations. Ideally, multi-document summaries should contain the key shared relevant information among all the documents only</context>
<context position="16573" citStr="Carbonell and Goldstein, 1998" startWordPosition="2567" endWordPosition="2571">for minimizing redundancy and maximizing both relevance and diversity. A first approximation to measuring relevant novelty is to measure relevance and novelty independently and provide a linear combination as the metric. We call this linear combination &amp;quot;marginal relevance&amp;quot; , i.e., a text passage has high marginal relevance if it is both relevant to the query and useful for a summary, while having minimal similarity to previously selected passages. Using this metric one can maximize marginal relevance in retrieval and summarization, hence we label our method &amp;quot;maximal marginal relevance&amp;quot; (MMR) (Carbonell and Goldstein, 1998). The Maximal Marginal Relevance Multi-Document (MMR-MD) metric is defined in Figure 1. Simi and Sim2 cover some of the properties that we discussed in Section 3.3 For Simi, the first term is the cosine similarity metric for query and document. The second term computes a coverage score for the passage by whether the passage is in one or more clusters and the size of the cluster. The third term reflects the information content of the passage by taking into account both statistical and linguistic features for summary inclusion (such as query expan.sion, position of the passage in the document an</context>
<context position="19107" citStr="Carbonell and Goldstein, 1998" startWordPosition="2990" endWordPosition="2993">of both criteria is optimized. In order to sample the information space in the general vicinity of the query, small values of A can be used; to focus on multiple, potentially overlapping or reinforcing relevant passages, A can be set to a value closer to 1. We found that a particularly effective search strategy for document retrieval is to start with a small A (e.g., A = .3) in order to understand the information space in the region of the query, and then to focus , on the most important parts using a reformulated query (possibly via relevance feedback) and a larger value of A (e.g., A = .7) (Carbonell and Goldstein, 1998). Our multi-document summarizer works as follows: • Segment the documents into passages, and index them using inverted indices (as used by the IR engine). Passages may be phrases, sentences, nsentence chunks, or paragraphs. • Identify the passages relevant to the query using cosine similarity with a threshold below which the passages are discarded. • Apply the MMR-MD metric as defined above. Depending on the desired length of the summary, select a number of passages to compute passage redundancy using the cosine similarity metric and use the passage similarity scoring as a method of clustering</context>
</contexts>
<marker>Carbonell, Goldstein, 1998</marker>
<rawString>Jaime G. Carbonell and Jade Goldstein. 1998. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In Proceedings of SIGIR-98, Melbourne, Australia, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jade Goldstein</author>
<author>Jaime Carbonell</author>
</authors>
<title>The use of mmr and diversity-based reranking in document reranking and summarization.</title>
<date>1998</date>
<booktitle>In Proceedings of the 14th Twente Workshop on Language Technology in Multimedia Information Retrieval,</booktitle>
<pages>152--166</pages>
<location>Enschede, the Netherlands,</location>
<contexts>
<context position="6353" citStr="Goldstein and Carbonell, 1998" startWordPosition="955" endWordPosition="958">ues (frequency analysis, variance analysis, etc.) to linguistic units such as tokens, names, anaphora, etc. More recently, other approaches have investigated the utility of discourse structure (Marcu, 1997), the combination of information extraction and language generation (Klavans and Shaw, 1995; McKeown et al., 1995), and using machine learning to find patterns in text (Teufel and Moens, 1997; Barzilay and Elhadad, 1997; Strzalkowski et al., 1998). Some of these approaches to single document summarization have been extended to deal with multi-document summarization (Mani and Bloedern, 1997; Goldstein and Carbonell, 1998; TIPSTER, 1998b; Radev and McKeown, 1998; Mani and Bloedorn, 1999; McKeown et al., ,1999; Stein et al., 1999). These include comparing templates filled in by extracting information — using specialized, domain specific knowledge sources — from the doc&apos;ument, and then generating natural language summaries from the templates (Radev and McKeown, 1998), corn— . paring named-entities — extracted using specialized lists — between documents and selecting the most relevant section (TIPSTER, 1998b), finding co-reference chains in the document set to identify common sections of interest (TIPSTER, 1998b)</context>
</contexts>
<marker>Goldstein, Carbonell, 1998</marker>
<rawString>Jade Goldstein and Jaime Carbonell. 1998. The use of mmr and diversity-based reranking in document reranking and summarization. In Proceedings of the 14th Twente Workshop on Language Technology in Multimedia Information Retrieval, pages 152-166, Enschede, the Netherlands, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jade Goldstein</author>
<author>Mark Kantrowitz</author>
<author>Vibhu</author>
</authors>
<title>Summarizing Text Documents: Sentence Selection and Evaluation Metrics.</title>
<date>1999</date>
<booktitle>In Proceedings of the 22nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR-99),</booktitle>
<pages>121--128</pages>
<location>Berkeley, CA.</location>
<contexts>
<context position="31517" citStr="Goldstein et al., 1999" startWordPosition="4991" endWordPosition="4995">ive the output summary quality. In this experiment, users are asked to rate the output sentences from the summarizer as good, okay or bad. For the okay or bad sentences, they are asked to provide a summary sentence from the document set that is &amp;quot;better&amp;quot;, i.e., that makes a better set of sentences to represent the information content of the document set. We are comparing our proposed summarizer #6 in Section 4 to summarizer #1, the common portions of the document sets with no anti-redundancy and summarizer #3, single document summary of a centroid document using our single document summarizer (Goldstein et al., 1999). 7 Conclusions and Future Work This paper presented a statistical method of generating extraction based multi-document summaries. It builds upon previous work in single-document summarization and takes into account some of the major differences between single-document and multi-document summarization: (i) the need to carefully eliminate redundant information from multiple documents, and achieve high compression ratios, (ii) take into account information about document and passage similarities, and weight different passages accordingly, and (iii) take temporal information into account. Our app</context>
</contexts>
<marker>Goldstein, Kantrowitz, Vibhu, 1999</marker>
<rawString>Jade Goldstein, Mark Kantrowitz, Vibhu 0. Mittal, and Jaime G. Carbonell. 1999. Summarizing Text Documents: Sentence Selection and Evaluation Metrics. In Proceedings of the 22nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR-99), pages 121-128, Berkeley, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Chin-Yew Lin</author>
</authors>
<title>Automated text summarization in SUMMARIST.</title>
<date>1997</date>
<booktitle>In ACL/EACL-97 Workshop on Intelligent Scalable Text Summarization,</booktitle>
<pages>18--24</pages>
<location>Madrid, Spain,</location>
<contexts>
<context position="2460" citStr="Hovy and Lin, 1997" startWordPosition="352" endWordPosition="355">IR or topic-detection context (Yang et al., 1998; Allan etal., 1998). Consider the situation where the user issues a search query, for instance on a news topic, and the retrieval system finds hundreds of closely-ranked documents in response. Many of these documents are likely to repeat much the same information, while differing in certain &apos;Most of these were based on statistical techniques applied to various document entities; examples include (Tait, 1983; Kupiec et al., 1995; Paice, 1990; Klavans and Shaw, 1995; McKeown et al., 1995; Shaw, 1995; Aone et al., 1997; Boguraev and Kennedy, 1997; Hovy and Lin, 1997; Mitra et al., 1997; Teufel and Moens, 1997; Barzilay and Elhadad, 1997; Carbonell and Goldstein, 1998; Baldwin and Morton, 1998; Radev and McKeown, 1998; Strzalkowski et al., 1998). parts. Summaries of the individual documents would help, but are likely to be very similar to each other, unless the summarization system takes into account other summaries that have already been generated. Multidocument summarization — capable of summarizing either complete documents sets, or single documents in the context of previously summarized ones — are likely to be essential in such situations. Ideally, m</context>
</contexts>
<marker>Hovy, Lin, 1997</marker>
<rawString>Eduard Hovy and Chin-Yew Lin. 1997. Automated text summarization in SUMMARIST. In ACL/EACL-97 Workshop on Intelligent Scalable Text Summarization, pages 18-24, Madrid, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith L Klavans</author>
<author>James Shaw</author>
</authors>
<title>Lexical semantics in summarization.</title>
<date>1995</date>
<booktitle>In Proceedings of the First Annual Workshop of the IFIP Working Group FOR NLP and KR,</booktitle>
<location>Nantes, France,</location>
<contexts>
<context position="2359" citStr="Klavans and Shaw, 1995" startWordPosition="334" endWordPosition="337"> been truly integrated, and the functionality challenges on a summarization system are greater in a true IR or topic-detection context (Yang et al., 1998; Allan etal., 1998). Consider the situation where the user issues a search query, for instance on a news topic, and the retrieval system finds hundreds of closely-ranked documents in response. Many of these documents are likely to repeat much the same information, while differing in certain &apos;Most of these were based on statistical techniques applied to various document entities; examples include (Tait, 1983; Kupiec et al., 1995; Paice, 1990; Klavans and Shaw, 1995; McKeown et al., 1995; Shaw, 1995; Aone et al., 1997; Boguraev and Kennedy, 1997; Hovy and Lin, 1997; Mitra et al., 1997; Teufel and Moens, 1997; Barzilay and Elhadad, 1997; Carbonell and Goldstein, 1998; Baldwin and Morton, 1998; Radev and McKeown, 1998; Strzalkowski et al., 1998). parts. Summaries of the individual documents would help, but are likely to be very similar to each other, unless the summarization system takes into account other summaries that have already been generated. Multidocument summarization — capable of summarizing either complete documents sets, or single documents in </context>
<context position="6021" citStr="Klavans and Shaw, 1995" startWordPosition="904" endWordPosition="907"> system attempts to delete &amp;quot;less important&amp;quot; spans of text from the original document; the text that remains is deemed a summary. Work on automated document summarization by text span extraction dates back at least to work at IBM in the fifties (Luhn, 1958). Most of the work in sentence extraction applied statistical techniques (frequency analysis, variance analysis, etc.) to linguistic units such as tokens, names, anaphora, etc. More recently, other approaches have investigated the utility of discourse structure (Marcu, 1997), the combination of information extraction and language generation (Klavans and Shaw, 1995; McKeown et al., 1995), and using machine learning to find patterns in text (Teufel and Moens, 1997; Barzilay and Elhadad, 1997; Strzalkowski et al., 1998). Some of these approaches to single document summarization have been extended to deal with multi-document summarization (Mani and Bloedern, 1997; Goldstein and Carbonell, 1998; TIPSTER, 1998b; Radev and McKeown, 1998; Mani and Bloedorn, 1999; McKeown et al., ,1999; Stein et al., 1999). These include comparing templates filled in by extracting information — using specialized, domain specific knowledge sources — from the doc&apos;ument, and then </context>
</contexts>
<marker>Klavans, Shaw, 1995</marker>
<rawString>Judith L. Klavans and James Shaw. 1995. Lexical semantics in summarization. In Proceedings of the First Annual Workshop of the IFIP Working Group FOR NLP and KR, Nantes, France, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian M Kupiec</author>
<author>Jan Pedersen</author>
<author>Francine Chen</author>
</authors>
<title>A trainable document summarizer.</title>
<date>1995</date>
<booktitle>In Proceedings of the 18th Annual Int. ACM/SIGIR Conference on Research and Development in IR,</booktitle>
<pages>68--73</pages>
<location>Seattle, WA,</location>
<contexts>
<context position="2322" citStr="Kupiec et al., 1995" startWordPosition="328" endWordPosition="331"> IR and summarization have not yet been truly integrated, and the functionality challenges on a summarization system are greater in a true IR or topic-detection context (Yang et al., 1998; Allan etal., 1998). Consider the situation where the user issues a search query, for instance on a news topic, and the retrieval system finds hundreds of closely-ranked documents in response. Many of these documents are likely to repeat much the same information, while differing in certain &apos;Most of these were based on statistical techniques applied to various document entities; examples include (Tait, 1983; Kupiec et al., 1995; Paice, 1990; Klavans and Shaw, 1995; McKeown et al., 1995; Shaw, 1995; Aone et al., 1997; Boguraev and Kennedy, 1997; Hovy and Lin, 1997; Mitra et al., 1997; Teufel and Moens, 1997; Barzilay and Elhadad, 1997; Carbonell and Goldstein, 1998; Baldwin and Morton, 1998; Radev and McKeown, 1998; Strzalkowski et al., 1998). parts. Summaries of the individual documents would help, but are likely to be very similar to each other, unless the summarization system takes into account other summaries that have already been generated. Multidocument summarization — capable of summarizing either complete do</context>
</contexts>
<marker>Kupiec, Pedersen, Chen, 1995</marker>
<rawString>Julian M. Kupiec, Jan Pedersen, and Francine Chen. 1995. A trainable document summarizer. In Proceedings of the 18th Annual Int. ACM/SIGIR Conference on Research and Development in IR, pages 68-73, Seattle, WA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P H Luhn</author>
</authors>
<title>Automatic creation of literature abstracts.</title>
<date>1958</date>
<journal>IBM Journal,</journal>
<pages>159--165</pages>
<contexts>
<context position="5655" citStr="Luhn, 1958" startWordPosition="856" endWordPosition="857">f information according to their relevance to a particular subject or purpose. These tasks can either be approximated by IR techniques or done in greater depth with fuller natural language processing. Most previous work in summarization has attempted to deal with the issues by focusing more on a related, but simpler, problem. With text-span deletion the system attempts to delete &amp;quot;less important&amp;quot; spans of text from the original document; the text that remains is deemed a summary. Work on automated document summarization by text span extraction dates back at least to work at IBM in the fifties (Luhn, 1958). Most of the work in sentence extraction applied statistical techniques (frequency analysis, variance analysis, etc.) to linguistic units such as tokens, names, anaphora, etc. More recently, other approaches have investigated the utility of discourse structure (Marcu, 1997), the combination of information extraction and language generation (Klavans and Shaw, 1995; McKeown et al., 1995), and using machine learning to find patterns in text (Teufel and Moens, 1997; Barzilay and Elhadad, 1997; Strzalkowski et al., 1998). Some of these approaches to single document summarization have been extended</context>
</contexts>
<marker>Luhn, 1958</marker>
<rawString>P. H. Luhn. 1958. Automatic creation of literature abstracts. IBM Journal, pages 159-165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
<author>Eric Bloedern</author>
</authors>
<title>Multidocument summarization by graph search and merging.</title>
<date>1997</date>
<booktitle>In Proceedings of AAAI-97,</booktitle>
<pages>622--628</pages>
<publisher>AAAI.</publisher>
<contexts>
<context position="6322" citStr="Mani and Bloedern, 1997" startWordPosition="951" endWordPosition="954">plied statistical techniques (frequency analysis, variance analysis, etc.) to linguistic units such as tokens, names, anaphora, etc. More recently, other approaches have investigated the utility of discourse structure (Marcu, 1997), the combination of information extraction and language generation (Klavans and Shaw, 1995; McKeown et al., 1995), and using machine learning to find patterns in text (Teufel and Moens, 1997; Barzilay and Elhadad, 1997; Strzalkowski et al., 1998). Some of these approaches to single document summarization have been extended to deal with multi-document summarization (Mani and Bloedern, 1997; Goldstein and Carbonell, 1998; TIPSTER, 1998b; Radev and McKeown, 1998; Mani and Bloedorn, 1999; McKeown et al., ,1999; Stein et al., 1999). These include comparing templates filled in by extracting information — using specialized, domain specific knowledge sources — from the doc&apos;ument, and then generating natural language summaries from the templates (Radev and McKeown, 1998), corn— . paring named-entities — extracted using specialized lists — between documents and selecting the most relevant section (TIPSTER, 1998b), finding co-reference chains in the document set to identify common sectio</context>
</contexts>
<marker>Mani, Bloedern, 1997</marker>
<rawString>Inderjeet Mani and Eric Bloedern. 1997. Multidocument summarization by graph search and merging. In Proceedings of AAAI-97, pages 622-628. AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
<author>Eric Bloedorn</author>
</authors>
<title>Summarizing similarities and differences among related documents. Information Retrieval,</title>
<date>1999</date>
<pages>1--35</pages>
<contexts>
<context position="6419" citStr="Mani and Bloedorn, 1999" startWordPosition="966" endWordPosition="969">ch as tokens, names, anaphora, etc. More recently, other approaches have investigated the utility of discourse structure (Marcu, 1997), the combination of information extraction and language generation (Klavans and Shaw, 1995; McKeown et al., 1995), and using machine learning to find patterns in text (Teufel and Moens, 1997; Barzilay and Elhadad, 1997; Strzalkowski et al., 1998). Some of these approaches to single document summarization have been extended to deal with multi-document summarization (Mani and Bloedern, 1997; Goldstein and Carbonell, 1998; TIPSTER, 1998b; Radev and McKeown, 1998; Mani and Bloedorn, 1999; McKeown et al., ,1999; Stein et al., 1999). These include comparing templates filled in by extracting information — using specialized, domain specific knowledge sources — from the doc&apos;ument, and then generating natural language summaries from the templates (Radev and McKeown, 1998), corn— . paring named-entities — extracted using specialized lists — between documents and selecting the most relevant section (TIPSTER, 1998b), finding co-reference chains in the document set to identify common sections of interest (TIPSTER, 1998b), or building activation networks of related lexical items (identi</context>
</contexts>
<marker>Mani, Bloedorn, 1999</marker>
<rawString>Inderjeet Mani and Eric Bloedorn. 1999. Summarizing similarities and differences among related documents. Information Retrieval, 1:35-67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>From discourse structures to text summaries.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL&apos;97/EACL&apos;97 Workshop on Intelligent Scalable Text Summarization,</booktitle>
<pages>82--88</pages>
<location>Madrid,</location>
<contexts>
<context position="5930" citStr="Marcu, 1997" startWordPosition="894" endWordPosition="895"> by focusing more on a related, but simpler, problem. With text-span deletion the system attempts to delete &amp;quot;less important&amp;quot; spans of text from the original document; the text that remains is deemed a summary. Work on automated document summarization by text span extraction dates back at least to work at IBM in the fifties (Luhn, 1958). Most of the work in sentence extraction applied statistical techniques (frequency analysis, variance analysis, etc.) to linguistic units such as tokens, names, anaphora, etc. More recently, other approaches have investigated the utility of discourse structure (Marcu, 1997), the combination of information extraction and language generation (Klavans and Shaw, 1995; McKeown et al., 1995), and using machine learning to find patterns in text (Teufel and Moens, 1997; Barzilay and Elhadad, 1997; Strzalkowski et al., 1998). Some of these approaches to single document summarization have been extended to deal with multi-document summarization (Mani and Bloedern, 1997; Goldstein and Carbonell, 1998; TIPSTER, 1998b; Radev and McKeown, 1998; Mani and Bloedorn, 1999; McKeown et al., ,1999; Stein et al., 1999). These include comparing templates filled in by extracting informa</context>
</contexts>
<marker>Marcu, 1997</marker>
<rawString>Daniel Marcu. 1997. From discourse structures to text summaries. In Proceedings of the ACL&apos;97/EACL&apos;97 Workshop on Intelligent Scalable Text Summarization, pages 82-88, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen McKeown</author>
<author>Jacques Robin</author>
<author>Karen Kukich</author>
</authors>
<title>Designing and evaluating a new revision-based model for summary generation.</title>
<date>1995</date>
<booktitle>Info. Proc. and Management,</booktitle>
<volume>31</volume>
<issue>5</issue>
<contexts>
<context position="2381" citStr="McKeown et al., 1995" startWordPosition="338" endWordPosition="341">and the functionality challenges on a summarization system are greater in a true IR or topic-detection context (Yang et al., 1998; Allan etal., 1998). Consider the situation where the user issues a search query, for instance on a news topic, and the retrieval system finds hundreds of closely-ranked documents in response. Many of these documents are likely to repeat much the same information, while differing in certain &apos;Most of these were based on statistical techniques applied to various document entities; examples include (Tait, 1983; Kupiec et al., 1995; Paice, 1990; Klavans and Shaw, 1995; McKeown et al., 1995; Shaw, 1995; Aone et al., 1997; Boguraev and Kennedy, 1997; Hovy and Lin, 1997; Mitra et al., 1997; Teufel and Moens, 1997; Barzilay and Elhadad, 1997; Carbonell and Goldstein, 1998; Baldwin and Morton, 1998; Radev and McKeown, 1998; Strzalkowski et al., 1998). parts. Summaries of the individual documents would help, but are likely to be very similar to each other, unless the summarization system takes into account other summaries that have already been generated. Multidocument summarization — capable of summarizing either complete documents sets, or single documents in the context of previou</context>
<context position="6044" citStr="McKeown et al., 1995" startWordPosition="908" endWordPosition="912">te &amp;quot;less important&amp;quot; spans of text from the original document; the text that remains is deemed a summary. Work on automated document summarization by text span extraction dates back at least to work at IBM in the fifties (Luhn, 1958). Most of the work in sentence extraction applied statistical techniques (frequency analysis, variance analysis, etc.) to linguistic units such as tokens, names, anaphora, etc. More recently, other approaches have investigated the utility of discourse structure (Marcu, 1997), the combination of information extraction and language generation (Klavans and Shaw, 1995; McKeown et al., 1995), and using machine learning to find patterns in text (Teufel and Moens, 1997; Barzilay and Elhadad, 1997; Strzalkowski et al., 1998). Some of these approaches to single document summarization have been extended to deal with multi-document summarization (Mani and Bloedern, 1997; Goldstein and Carbonell, 1998; TIPSTER, 1998b; Radev and McKeown, 1998; Mani and Bloedorn, 1999; McKeown et al., ,1999; Stein et al., 1999). These include comparing templates filled in by extracting information — using specialized, domain specific knowledge sources — from the doc&apos;ument, and then generating natural lang</context>
</contexts>
<marker>McKeown, Robin, Kukich, 1995</marker>
<rawString>Kathleen McKeown, Jacques Robin, and Karen Kukich. 1995. Designing and evaluating a new revision-based model for summary generation. Info. Proc. and Management, 31(5).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen McKeown</author>
</authors>
<title>Judith Klavans, Vasileios Hatzivassiloglou, Regina Barzilay, and Eleazar Eslcin.</title>
<date>1999</date>
<booktitle>In Proceedings of AAAI-99,</booktitle>
<pages>453--460</pages>
<location>Orlando, FL,</location>
<marker>McKeown, 1999</marker>
<rawString>Kathleen McKeown, Judith Klavans, Vasileios Hatzivassiloglou, Regina Barzilay, and Eleazar Eslcin. 1999. Towards Multidocument Summarization by Reformulation: Progress and Prospects. In Proceedings of AAAI-99, pages 453-460, Orlando, FL, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mandar Mitra</author>
<author>Amit Singhal</author>
<author>Chris Buckley</author>
</authors>
<title>Automatic text summarization by paragraph extraction.</title>
<date>1997</date>
<booktitle>In ACL/EACL-97 Workshop on Intelligent Scalable Text Summarization,</booktitle>
<pages>31--36</pages>
<location>Madrid, Spain,</location>
<contexts>
<context position="2480" citStr="Mitra et al., 1997" startWordPosition="356" endWordPosition="359">n context (Yang et al., 1998; Allan etal., 1998). Consider the situation where the user issues a search query, for instance on a news topic, and the retrieval system finds hundreds of closely-ranked documents in response. Many of these documents are likely to repeat much the same information, while differing in certain &apos;Most of these were based on statistical techniques applied to various document entities; examples include (Tait, 1983; Kupiec et al., 1995; Paice, 1990; Klavans and Shaw, 1995; McKeown et al., 1995; Shaw, 1995; Aone et al., 1997; Boguraev and Kennedy, 1997; Hovy and Lin, 1997; Mitra et al., 1997; Teufel and Moens, 1997; Barzilay and Elhadad, 1997; Carbonell and Goldstein, 1998; Baldwin and Morton, 1998; Radev and McKeown, 1998; Strzalkowski et al., 1998). parts. Summaries of the individual documents would help, but are likely to be very similar to each other, unless the summarization system takes into account other summaries that have already been generated. Multidocument summarization — capable of summarizing either complete documents sets, or single documents in the context of previously summarized ones — are likely to be essential in such situations. Ideally, multi-document summar</context>
</contexts>
<marker>Mitra, Singhal, Buckley, 1997</marker>
<rawString>Mandar Mitra, Amit Singhal, and Chris Buckley. 1997. Automatic text summarization by paragraph extraction. In ACL/EACL-97 Workshop on Intelligent Scalable Text Summarization, pages 31-36, Madrid, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris D Paice</author>
</authors>
<title>Constructing literature abstracts by computer: Techniques and prospects.</title>
<date>1990</date>
<booktitle>Info. Proc. and Management,</booktitle>
<pages>26--171</pages>
<contexts>
<context position="2335" citStr="Paice, 1990" startWordPosition="332" endWordPosition="333"> have not yet been truly integrated, and the functionality challenges on a summarization system are greater in a true IR or topic-detection context (Yang et al., 1998; Allan etal., 1998). Consider the situation where the user issues a search query, for instance on a news topic, and the retrieval system finds hundreds of closely-ranked documents in response. Many of these documents are likely to repeat much the same information, while differing in certain &apos;Most of these were based on statistical techniques applied to various document entities; examples include (Tait, 1983; Kupiec et al., 1995; Paice, 1990; Klavans and Shaw, 1995; McKeown et al., 1995; Shaw, 1995; Aone et al., 1997; Boguraev and Kennedy, 1997; Hovy and Lin, 1997; Mitra et al., 1997; Teufel and Moens, 1997; Barzilay and Elhadad, 1997; Carbonell and Goldstein, 1998; Baldwin and Morton, 1998; Radev and McKeown, 1998; Strzalkowski et al., 1998). parts. Summaries of the individual documents would help, but are likely to be very similar to each other, unless the summarization system takes into account other summaries that have already been generated. Multidocument summarization — capable of summarizing either complete documents sets,</context>
</contexts>
<marker>Paice, 1990</marker>
<rawString>Chris D. Paice. 1990. Constructing literature abstracts by computer: Techniques and prospects. Info. Proc. and Management, 26:171-186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir Radev</author>
<author>Kathy McKeown</author>
</authors>
<title>Generating natural language summaries from multiple online sources. Compuutational Linguistics.</title>
<date>1998</date>
<contexts>
<context position="2614" citStr="Radev and McKeown, 1998" startWordPosition="377" endWordPosition="380">news topic, and the retrieval system finds hundreds of closely-ranked documents in response. Many of these documents are likely to repeat much the same information, while differing in certain &apos;Most of these were based on statistical techniques applied to various document entities; examples include (Tait, 1983; Kupiec et al., 1995; Paice, 1990; Klavans and Shaw, 1995; McKeown et al., 1995; Shaw, 1995; Aone et al., 1997; Boguraev and Kennedy, 1997; Hovy and Lin, 1997; Mitra et al., 1997; Teufel and Moens, 1997; Barzilay and Elhadad, 1997; Carbonell and Goldstein, 1998; Baldwin and Morton, 1998; Radev and McKeown, 1998; Strzalkowski et al., 1998). parts. Summaries of the individual documents would help, but are likely to be very similar to each other, unless the summarization system takes into account other summaries that have already been generated. Multidocument summarization — capable of summarizing either complete documents sets, or single documents in the context of previously summarized ones — are likely to be essential in such situations. Ideally, multi-document summaries should contain the key shared relevant information among all the documents only once, plus other information unique to some of the</context>
<context position="6394" citStr="Radev and McKeown, 1998" startWordPosition="961" endWordPosition="965">.) to linguistic units such as tokens, names, anaphora, etc. More recently, other approaches have investigated the utility of discourse structure (Marcu, 1997), the combination of information extraction and language generation (Klavans and Shaw, 1995; McKeown et al., 1995), and using machine learning to find patterns in text (Teufel and Moens, 1997; Barzilay and Elhadad, 1997; Strzalkowski et al., 1998). Some of these approaches to single document summarization have been extended to deal with multi-document summarization (Mani and Bloedern, 1997; Goldstein and Carbonell, 1998; TIPSTER, 1998b; Radev and McKeown, 1998; Mani and Bloedorn, 1999; McKeown et al., ,1999; Stein et al., 1999). These include comparing templates filled in by extracting information — using specialized, domain specific knowledge sources — from the doc&apos;ument, and then generating natural language summaries from the templates (Radev and McKeown, 1998), corn— . paring named-entities — extracted using specialized lists — between documents and selecting the most relevant section (TIPSTER, 1998b), finding co-reference chains in the document set to identify common sections of interest (TIPSTER, 1998b), or building activation networks of rela</context>
<context position="15537" citStr="Radev and McKeown, 1998" startWordPosition="2409" endWordPosition="2412">ction, (2) updates of information within the document collection, (3) updates of information from the document collection with respect to an already provided summary, (4) the development of an event or subtopic of 2This is similar to the approach of Textwise (TIPSTER, 1998b), whose multi-document summary consists of the most relevant paragraph and specialized word lists. 42 an event (e.g., death tolls) over time, and (5) a comparative development of an event. Naturally, an ideal multi-document summary would include a natural language generation component to create cohesive readable summaries (Radev and McKeown, 1998; McKeown et al., 1999). Our current focus is on the extraction of the relevant passages. 5 System Design In the previous sections we discussed the requirements and types of multi-document summarization systems. This section discusses our current implementation of a multi-document summarization system which is designed to produce summaries that emphasize &amp;quot;relevant novelty.&amp;quot; Relevant novelty is a metric for minimizing redundancy and maximizing both relevance and diversity. A first approximation to measuring relevant novelty is to measure relevance and novelty independently and provide a linear </context>
</contexts>
<marker>Radev, McKeown, 1998</marker>
<rawString>Dragomir Radev and Kathy McKeown. 1998. Generating natural language summaries from multiple online sources. Compuutational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Salton</author>
</authors>
<title>Automatic processing of foreign language docuemnts.</title>
<date>1970</date>
<journal>Journal of American Society for Information Sciences,</journal>
<pages>21--187</pages>
<contexts>
<context position="1391" citStr="Salton, 1970" startWordPosition="185" endWordPosition="186">h addresses these issues by using domainindependent techniques based mainly on fast, statistical processing, a metric for reducing redundancy and maximizing diversity in the selected passages, and a modular framework to allow easy parameterization for different genres, corpora characteristics and user requirements. 1 Introduction With the continuing growth of online information, it has become increasingly important to provide improved mechanisms to find and present textual information effectively. Conventional IR systems find and rank documents based on maximizing relevance to the user query (Salton, 1970; van Rijsbergen, 1979; Buckley, 1985; Salton, 1989). Some systems also include sub-document relevance assessments and convey this information to the user. More recently, single document summarization systems provide an automated generic abstract or a queryrelevant summary (TIPSTER, 1998a),I However, largescale IR and summarization have not yet been truly integrated, and the functionality challenges on a summarization system are greater in a true IR or topic-detection context (Yang et al., 1998; Allan etal., 1998). Consider the situation where the user issues a search query, for instance on a </context>
</contexts>
<marker>Salton, 1970</marker>
<rawString>Gerald Salton. 1970. Automatic processing of foreign language docuemnts. Journal of American Society for Information Sciences, 21:187-194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Salton</author>
</authors>
<title>Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer.</title>
<date>1989</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="1443" citStr="Salton, 1989" startWordPosition="192" endWordPosition="193"> techniques based mainly on fast, statistical processing, a metric for reducing redundancy and maximizing diversity in the selected passages, and a modular framework to allow easy parameterization for different genres, corpora characteristics and user requirements. 1 Introduction With the continuing growth of online information, it has become increasingly important to provide improved mechanisms to find and present textual information effectively. Conventional IR systems find and rank documents based on maximizing relevance to the user query (Salton, 1970; van Rijsbergen, 1979; Buckley, 1985; Salton, 1989). Some systems also include sub-document relevance assessments and convey this information to the user. More recently, single document summarization systems provide an automated generic abstract or a queryrelevant summary (TIPSTER, 1998a),I However, largescale IR and summarization have not yet been truly integrated, and the functionality challenges on a summarization system are greater in a true IR or topic-detection context (Yang et al., 1998; Allan etal., 1998). Consider the situation where the user issues a search query, for instance on a news topic, and the retrieval system finds hundreds </context>
</contexts>
<marker>Salton, 1989</marker>
<rawString>Gerald Salton. 1989. Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Shaw</author>
</authors>
<title>Conciseness through aggregation in text generation.</title>
<date>1995</date>
<booktitle>In Proceedings of 33rd Association for Computational Linguistics,</booktitle>
<pages>329--331</pages>
<contexts>
<context position="2359" citStr="Shaw, 1995" startWordPosition="336" endWordPosition="337">integrated, and the functionality challenges on a summarization system are greater in a true IR or topic-detection context (Yang et al., 1998; Allan etal., 1998). Consider the situation where the user issues a search query, for instance on a news topic, and the retrieval system finds hundreds of closely-ranked documents in response. Many of these documents are likely to repeat much the same information, while differing in certain &apos;Most of these were based on statistical techniques applied to various document entities; examples include (Tait, 1983; Kupiec et al., 1995; Paice, 1990; Klavans and Shaw, 1995; McKeown et al., 1995; Shaw, 1995; Aone et al., 1997; Boguraev and Kennedy, 1997; Hovy and Lin, 1997; Mitra et al., 1997; Teufel and Moens, 1997; Barzilay and Elhadad, 1997; Carbonell and Goldstein, 1998; Baldwin and Morton, 1998; Radev and McKeown, 1998; Strzalkowski et al., 1998). parts. Summaries of the individual documents would help, but are likely to be very similar to each other, unless the summarization system takes into account other summaries that have already been generated. Multidocument summarization — capable of summarizing either complete documents sets, or single documents in </context>
<context position="6021" citStr="Shaw, 1995" startWordPosition="906" endWordPosition="907">mpts to delete &amp;quot;less important&amp;quot; spans of text from the original document; the text that remains is deemed a summary. Work on automated document summarization by text span extraction dates back at least to work at IBM in the fifties (Luhn, 1958). Most of the work in sentence extraction applied statistical techniques (frequency analysis, variance analysis, etc.) to linguistic units such as tokens, names, anaphora, etc. More recently, other approaches have investigated the utility of discourse structure (Marcu, 1997), the combination of information extraction and language generation (Klavans and Shaw, 1995; McKeown et al., 1995), and using machine learning to find patterns in text (Teufel and Moens, 1997; Barzilay and Elhadad, 1997; Strzalkowski et al., 1998). Some of these approaches to single document summarization have been extended to deal with multi-document summarization (Mani and Bloedern, 1997; Goldstein and Carbonell, 1998; TIPSTER, 1998b; Radev and McKeown, 1998; Mani and Bloedorn, 1999; McKeown et al., ,1999; Stein et al., 1999). These include comparing templates filled in by extracting information — using specialized, domain specific knowledge sources — from the doc&apos;ument, and then </context>
</contexts>
<marker>Shaw, 1995</marker>
<rawString>James Shaw. 1995. Conciseness through aggregation in text generation. In Proceedings of 33rd Association for Computational Linguistics, pages 329-331.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gees C Stein</author>
<author>Tomek Strzalkowski</author>
<author>G Bowden Wise</author>
</authors>
<title>Summarizing Multiple Documents Using Text Extraction and Interactive Clustering.</title>
<date>1999</date>
<booktitle>In Proceedings of PacLing-99: The Pacific Rim Conference on Computational Linguistics,</booktitle>
<pages>200--208</pages>
<location>Waterloo, Canada.</location>
<contexts>
<context position="6463" citStr="Stein et al., 1999" startWordPosition="974" endWordPosition="977">y, other approaches have investigated the utility of discourse structure (Marcu, 1997), the combination of information extraction and language generation (Klavans and Shaw, 1995; McKeown et al., 1995), and using machine learning to find patterns in text (Teufel and Moens, 1997; Barzilay and Elhadad, 1997; Strzalkowski et al., 1998). Some of these approaches to single document summarization have been extended to deal with multi-document summarization (Mani and Bloedern, 1997; Goldstein and Carbonell, 1998; TIPSTER, 1998b; Radev and McKeown, 1998; Mani and Bloedorn, 1999; McKeown et al., ,1999; Stein et al., 1999). These include comparing templates filled in by extracting information — using specialized, domain specific knowledge sources — from the doc&apos;ument, and then generating natural language summaries from the templates (Radev and McKeown, 1998), corn— . paring named-entities — extracted using specialized lists — between documents and selecting the most relevant section (TIPSTER, 1998b), finding co-reference chains in the document set to identify common sections of interest (TIPSTER, 1998b), or building activation networks of related lexical items (identity mappings, synonyms, hypernyms, etc.) to e</context>
</contexts>
<marker>Stein, Strzalkowski, Wise, 1999</marker>
<rawString>Gees C. Stein, Tomek Strzalkowski, and G. Bowden Wise. 1999. Summarizing Multiple Documents Using Text Extraction and Interactive Clustering. In Proceedings of PacLing-99: The Pacific Rim Conference on Computational Linguistics, pages 200-208, Waterloo, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomek Strzalkowski</author>
<author>Jin Wang</author>
<author>Bowden Wise</author>
</authors>
<title>A robust practical text summarization system.</title>
<date>1998</date>
<booktitle>In AAAI Intelligent Text Summarization Workshop,</booktitle>
<pages>26--30</pages>
<location>Stanford, CA,</location>
<contexts>
<context position="2642" citStr="Strzalkowski et al., 1998" startWordPosition="381" endWordPosition="384">eval system finds hundreds of closely-ranked documents in response. Many of these documents are likely to repeat much the same information, while differing in certain &apos;Most of these were based on statistical techniques applied to various document entities; examples include (Tait, 1983; Kupiec et al., 1995; Paice, 1990; Klavans and Shaw, 1995; McKeown et al., 1995; Shaw, 1995; Aone et al., 1997; Boguraev and Kennedy, 1997; Hovy and Lin, 1997; Mitra et al., 1997; Teufel and Moens, 1997; Barzilay and Elhadad, 1997; Carbonell and Goldstein, 1998; Baldwin and Morton, 1998; Radev and McKeown, 1998; Strzalkowski et al., 1998). parts. Summaries of the individual documents would help, but are likely to be very similar to each other, unless the summarization system takes into account other summaries that have already been generated. Multidocument summarization — capable of summarizing either complete documents sets, or single documents in the context of previously summarized ones — are likely to be essential in such situations. Ideally, multi-document summaries should contain the key shared relevant information among all the documents only once, plus other information unique to some of the individual documents that a</context>
<context position="6177" citStr="Strzalkowski et al., 1998" startWordPosition="930" endWordPosition="933">ent summarization by text span extraction dates back at least to work at IBM in the fifties (Luhn, 1958). Most of the work in sentence extraction applied statistical techniques (frequency analysis, variance analysis, etc.) to linguistic units such as tokens, names, anaphora, etc. More recently, other approaches have investigated the utility of discourse structure (Marcu, 1997), the combination of information extraction and language generation (Klavans and Shaw, 1995; McKeown et al., 1995), and using machine learning to find patterns in text (Teufel and Moens, 1997; Barzilay and Elhadad, 1997; Strzalkowski et al., 1998). Some of these approaches to single document summarization have been extended to deal with multi-document summarization (Mani and Bloedern, 1997; Goldstein and Carbonell, 1998; TIPSTER, 1998b; Radev and McKeown, 1998; Mani and Bloedorn, 1999; McKeown et al., ,1999; Stein et al., 1999). These include comparing templates filled in by extracting information — using specialized, domain specific knowledge sources — from the doc&apos;ument, and then generating natural language summaries from the templates (Radev and McKeown, 1998), corn— . paring named-entities — extracted using specialized lists — betw</context>
</contexts>
<marker>Strzalkowski, Wang, Wise, 1998</marker>
<rawString>Tomek Strzalkowski, Jin Wang, and Bowden Wise. 1998. A robust practical text summarization system. In AAAI Intelligent Text Summarization Workshop, pages 26-30, Stanford, CA, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J I Tait</author>
</authors>
<title>Automatic Summarizing of English Texts.</title>
<date>1983</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Cambridge,</institution>
<location>Cambridge, UK.</location>
<contexts>
<context position="2301" citStr="Tait, 1983" startWordPosition="326" endWordPosition="327">, largescale IR and summarization have not yet been truly integrated, and the functionality challenges on a summarization system are greater in a true IR or topic-detection context (Yang et al., 1998; Allan etal., 1998). Consider the situation where the user issues a search query, for instance on a news topic, and the retrieval system finds hundreds of closely-ranked documents in response. Many of these documents are likely to repeat much the same information, while differing in certain &apos;Most of these were based on statistical techniques applied to various document entities; examples include (Tait, 1983; Kupiec et al., 1995; Paice, 1990; Klavans and Shaw, 1995; McKeown et al., 1995; Shaw, 1995; Aone et al., 1997; Boguraev and Kennedy, 1997; Hovy and Lin, 1997; Mitra et al., 1997; Teufel and Moens, 1997; Barzilay and Elhadad, 1997; Carbonell and Goldstein, 1998; Baldwin and Morton, 1998; Radev and McKeown, 1998; Strzalkowski et al., 1998). parts. Summaries of the individual documents would help, but are likely to be very similar to each other, unless the summarization system takes into account other summaries that have already been generated. Multidocument summarization — capable of summarizi</context>
</contexts>
<marker>Tait, 1983</marker>
<rawString>J. I. Tait. 1983. Automatic Summarizing of English Texts. Ph.D. thesis, University of Cambridge, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
<author>Marc Moens</author>
</authors>
<title>Sentence extraction as a classification task.</title>
<date>1997</date>
<booktitle>In ACIJEACL-97 Workshop on Intelligent Scalable Text Summarization,</booktitle>
<pages>58--65</pages>
<location>Madrid, Spain,</location>
<contexts>
<context position="2504" citStr="Teufel and Moens, 1997" startWordPosition="360" endWordPosition="363">l., 1998; Allan etal., 1998). Consider the situation where the user issues a search query, for instance on a news topic, and the retrieval system finds hundreds of closely-ranked documents in response. Many of these documents are likely to repeat much the same information, while differing in certain &apos;Most of these were based on statistical techniques applied to various document entities; examples include (Tait, 1983; Kupiec et al., 1995; Paice, 1990; Klavans and Shaw, 1995; McKeown et al., 1995; Shaw, 1995; Aone et al., 1997; Boguraev and Kennedy, 1997; Hovy and Lin, 1997; Mitra et al., 1997; Teufel and Moens, 1997; Barzilay and Elhadad, 1997; Carbonell and Goldstein, 1998; Baldwin and Morton, 1998; Radev and McKeown, 1998; Strzalkowski et al., 1998). parts. Summaries of the individual documents would help, but are likely to be very similar to each other, unless the summarization system takes into account other summaries that have already been generated. Multidocument summarization — capable of summarizing either complete documents sets, or single documents in the context of previously summarized ones — are likely to be essential in such situations. Ideally, multi-document summaries should contain the k</context>
<context position="6121" citStr="Teufel and Moens, 1997" startWordPosition="922" endWordPosition="925">remains is deemed a summary. Work on automated document summarization by text span extraction dates back at least to work at IBM in the fifties (Luhn, 1958). Most of the work in sentence extraction applied statistical techniques (frequency analysis, variance analysis, etc.) to linguistic units such as tokens, names, anaphora, etc. More recently, other approaches have investigated the utility of discourse structure (Marcu, 1997), the combination of information extraction and language generation (Klavans and Shaw, 1995; McKeown et al., 1995), and using machine learning to find patterns in text (Teufel and Moens, 1997; Barzilay and Elhadad, 1997; Strzalkowski et al., 1998). Some of these approaches to single document summarization have been extended to deal with multi-document summarization (Mani and Bloedern, 1997; Goldstein and Carbonell, 1998; TIPSTER, 1998b; Radev and McKeown, 1998; Mani and Bloedorn, 1999; McKeown et al., ,1999; Stein et al., 1999). These include comparing templates filled in by extracting information — using specialized, domain specific knowledge sources — from the doc&apos;ument, and then generating natural language summaries from the templates (Radev and McKeown, 1998), corn— . paring n</context>
</contexts>
<marker>Teufel, Moens, 1997</marker>
<rawString>Simone Teufel and Marc Moens. 1997. Sentence extraction as a classification task. In ACIJEACL-97 Workshop on Intelligent Scalable Text Summarization, pages 58-65, Madrid, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>TIPSTER</author>
</authors>
<date>1998</date>
<booktitle>Tipster text phase III 18-month workshop notes, May.</booktitle>
<location>Fairfax, VA.</location>
<contexts>
<context position="1679" citStr="TIPSTER, 1998" startWordPosition="225" endWordPosition="226">istics and user requirements. 1 Introduction With the continuing growth of online information, it has become increasingly important to provide improved mechanisms to find and present textual information effectively. Conventional IR systems find and rank documents based on maximizing relevance to the user query (Salton, 1970; van Rijsbergen, 1979; Buckley, 1985; Salton, 1989). Some systems also include sub-document relevance assessments and convey this information to the user. More recently, single document summarization systems provide an automated generic abstract or a queryrelevant summary (TIPSTER, 1998a),I However, largescale IR and summarization have not yet been truly integrated, and the functionality challenges on a summarization system are greater in a true IR or topic-detection context (Yang et al., 1998; Allan etal., 1998). Consider the situation where the user issues a search query, for instance on a news topic, and the retrieval system finds hundreds of closely-ranked documents in response. Many of these documents are likely to repeat much the same information, while differing in certain &apos;Most of these were based on statistical techniques applied to various document entities; exampl</context>
<context position="4229" citStr="TIPSTER, 1998" startWordPosition="635" endWordPosition="636">as each article is apt to describe the main point as well as necessary shared background. Hence anti-redundancy methods are more crucial. 2. A group of articles may contain a temporal dimension, typical in a stream of news reports about an unfolding event. Here later information may override earlier more tentative or incomplete accounts. 3. The compression ratio (i.e. the size of the summary with respect to the size of the document set) will typically be much smaller for collections of dozens or hundreds of topically related documents than for single document summaries. The SUMMAC evaluation (TIPSTER, 1998a) tested 10% compression summaries, but in our work summarizing 200- document clusters, we find that compression to the 1% or 0.1% level is required. Summarization becomes significantly more difficult when compression demands increase. 4. The co-reference problem in summarization presents even greater challenges for multidocument than for single-document summarization (Baldwin and Morton, 1998). This paper discusses an approach to multi-document summarization that builds on previous work in single40 document summarization by using additional, available information about the document set as a </context>
<context position="6368" citStr="TIPSTER, 1998" startWordPosition="959" endWordPosition="960">ce analysis, etc.) to linguistic units such as tokens, names, anaphora, etc. More recently, other approaches have investigated the utility of discourse structure (Marcu, 1997), the combination of information extraction and language generation (Klavans and Shaw, 1995; McKeown et al., 1995), and using machine learning to find patterns in text (Teufel and Moens, 1997; Barzilay and Elhadad, 1997; Strzalkowski et al., 1998). Some of these approaches to single document summarization have been extended to deal with multi-document summarization (Mani and Bloedern, 1997; Goldstein and Carbonell, 1998; TIPSTER, 1998b; Radev and McKeown, 1998; Mani and Bloedorn, 1999; McKeown et al., ,1999; Stein et al., 1999). These include comparing templates filled in by extracting information — using specialized, domain specific knowledge sources — from the doc&apos;ument, and then generating natural language summaries from the templates (Radev and McKeown, 1998), corn— . paring named-entities — extracted using specialized lists — between documents and selecting the most relevant section (TIPSTER, 1998b), finding co-reference chains in the document set to identify common sections of interest (TIPSTER, 1998b), or building a</context>
<context position="15187" citStr="TIPSTER, 1998" startWordPosition="2358" endWordPosition="2359">result as a summary. This allows the more recent, often updated information to be more likely to be included in the summary. There are also much more complicated types of summary extracts which involve natural language processing and/or understanding. These types of summaries include: (1) differing points of view within the document collection, (2) updates of information within the document collection, (3) updates of information from the document collection with respect to an already provided summary, (4) the development of an event or subtopic of 2This is similar to the approach of Textwise (TIPSTER, 1998b), whose multi-document summary consists of the most relevant paragraph and specialized word lists. 42 an event (e.g., death tolls) over time, and (5) a comparative development of an event. Naturally, an ideal multi-document summary would include a natural language generation component to create cohesive readable summaries (Radev and McKeown, 1998; McKeown et al., 1999). Our current focus is on the extraction of the relevant passages. 5 System Design In the previous sections we discussed the requirements and types of multi-document summarization systems. This section discusses our current imp</context>
</contexts>
<marker>TIPSTER, 1998</marker>
<rawString>TIPSTER. 1998a. Tipster text phase III 18-month workshop notes, May. Fairfax, VA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>TIPSTER</author>
</authors>
<date>1998</date>
<booktitle>Tipster text phase III 24-month workshop notes, October.</booktitle>
<location>Baltimore, MD.</location>
<contexts>
<context position="1679" citStr="TIPSTER, 1998" startWordPosition="225" endWordPosition="226">istics and user requirements. 1 Introduction With the continuing growth of online information, it has become increasingly important to provide improved mechanisms to find and present textual information effectively. Conventional IR systems find and rank documents based on maximizing relevance to the user query (Salton, 1970; van Rijsbergen, 1979; Buckley, 1985; Salton, 1989). Some systems also include sub-document relevance assessments and convey this information to the user. More recently, single document summarization systems provide an automated generic abstract or a queryrelevant summary (TIPSTER, 1998a),I However, largescale IR and summarization have not yet been truly integrated, and the functionality challenges on a summarization system are greater in a true IR or topic-detection context (Yang et al., 1998; Allan etal., 1998). Consider the situation where the user issues a search query, for instance on a news topic, and the retrieval system finds hundreds of closely-ranked documents in response. Many of these documents are likely to repeat much the same information, while differing in certain &apos;Most of these were based on statistical techniques applied to various document entities; exampl</context>
<context position="4229" citStr="TIPSTER, 1998" startWordPosition="635" endWordPosition="636">as each article is apt to describe the main point as well as necessary shared background. Hence anti-redundancy methods are more crucial. 2. A group of articles may contain a temporal dimension, typical in a stream of news reports about an unfolding event. Here later information may override earlier more tentative or incomplete accounts. 3. The compression ratio (i.e. the size of the summary with respect to the size of the document set) will typically be much smaller for collections of dozens or hundreds of topically related documents than for single document summaries. The SUMMAC evaluation (TIPSTER, 1998a) tested 10% compression summaries, but in our work summarizing 200- document clusters, we find that compression to the 1% or 0.1% level is required. Summarization becomes significantly more difficult when compression demands increase. 4. The co-reference problem in summarization presents even greater challenges for multidocument than for single-document summarization (Baldwin and Morton, 1998). This paper discusses an approach to multi-document summarization that builds on previous work in single40 document summarization by using additional, available information about the document set as a </context>
<context position="6368" citStr="TIPSTER, 1998" startWordPosition="959" endWordPosition="960">ce analysis, etc.) to linguistic units such as tokens, names, anaphora, etc. More recently, other approaches have investigated the utility of discourse structure (Marcu, 1997), the combination of information extraction and language generation (Klavans and Shaw, 1995; McKeown et al., 1995), and using machine learning to find patterns in text (Teufel and Moens, 1997; Barzilay and Elhadad, 1997; Strzalkowski et al., 1998). Some of these approaches to single document summarization have been extended to deal with multi-document summarization (Mani and Bloedern, 1997; Goldstein and Carbonell, 1998; TIPSTER, 1998b; Radev and McKeown, 1998; Mani and Bloedorn, 1999; McKeown et al., ,1999; Stein et al., 1999). These include comparing templates filled in by extracting information — using specialized, domain specific knowledge sources — from the doc&apos;ument, and then generating natural language summaries from the templates (Radev and McKeown, 1998), corn— . paring named-entities — extracted using specialized lists — between documents and selecting the most relevant section (TIPSTER, 1998b), finding co-reference chains in the document set to identify common sections of interest (TIPSTER, 1998b), or building a</context>
<context position="15187" citStr="TIPSTER, 1998" startWordPosition="2358" endWordPosition="2359">result as a summary. This allows the more recent, often updated information to be more likely to be included in the summary. There are also much more complicated types of summary extracts which involve natural language processing and/or understanding. These types of summaries include: (1) differing points of view within the document collection, (2) updates of information within the document collection, (3) updates of information from the document collection with respect to an already provided summary, (4) the development of an event or subtopic of 2This is similar to the approach of Textwise (TIPSTER, 1998b), whose multi-document summary consists of the most relevant paragraph and specialized word lists. 42 an event (e.g., death tolls) over time, and (5) a comparative development of an event. Naturally, an ideal multi-document summary would include a natural language generation component to create cohesive readable summaries (Radev and McKeown, 1998; McKeown et al., 1999). Our current focus is on the extraction of the relevant passages. 5 System Design In the previous sections we discussed the requirements and types of multi-document summarization systems. This section discusses our current imp</context>
</contexts>
<marker>TIPSTER, 1998</marker>
<rawString>TIPSTER. 1998b. Tipster text phase III 24-month workshop notes, October. Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles J van Rijsbergen</author>
</authors>
<title>Information Retrieval.</title>
<date>1979</date>
<publisher>Butterworths,</publisher>
<location>London.</location>
<marker>van Rijsbergen, 1979</marker>
<rawString>Charles J. van Rijsbergen. 1979. Information Retrieval. Butterworths, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiming Yang</author>
<author>Tom Pierce</author>
<author>Jaime G Carbonell</author>
</authors>
<title>A study on retrospective and on-line event detection.</title>
<date>1998</date>
<booktitle>In Proceedings of the 21th Ann Int ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;98),</booktitle>
<pages>28--36</pages>
<contexts>
<context position="1890" citStr="Yang et al., 1998" startWordPosition="258" endWordPosition="261">ctively. Conventional IR systems find and rank documents based on maximizing relevance to the user query (Salton, 1970; van Rijsbergen, 1979; Buckley, 1985; Salton, 1989). Some systems also include sub-document relevance assessments and convey this information to the user. More recently, single document summarization systems provide an automated generic abstract or a queryrelevant summary (TIPSTER, 1998a),I However, largescale IR and summarization have not yet been truly integrated, and the functionality challenges on a summarization system are greater in a true IR or topic-detection context (Yang et al., 1998; Allan etal., 1998). Consider the situation where the user issues a search query, for instance on a news topic, and the retrieval system finds hundreds of closely-ranked documents in response. Many of these documents are likely to repeat much the same information, while differing in certain &apos;Most of these were based on statistical techniques applied to various document entities; examples include (Tait, 1983; Kupiec et al., 1995; Paice, 1990; Klavans and Shaw, 1995; McKeown et al., 1995; Shaw, 1995; Aone et al., 1997; Boguraev and Kennedy, 1997; Hovy and Lin, 1997; Mitra et al., 1997; Teufel a</context>
</contexts>
<marker>Yang, Pierce, Carbonell, 1998</marker>
<rawString>Yiming Yang, Tom Pierce, and Jaime G. Carbonell. 1998. A study on retrospective and on-line event detection. In Proceedings of the 21th Ann Int ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;98), pages 28-36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiming Yang</author>
<author>Jaime G Carbonell</author>
<author>Ralf D Brown</author>
<author>Tom Pierce</author>
<author>Brian T Archibald</author>
<author>Xin Liu</author>
</authors>
<title>Learning approaches for topic detection and tracking news events.</title>
<date>1999</date>
<journal>IEEE intelligent Systems, Special Issue on Applications of Intelligent Information Retrieval,</journal>
<pages>14--4</pages>
<contexts>
<context position="8910" citStr="Yang et al., 1999" startWordPosition="1339" endWordPosition="1342"> can be taken into account easily. 3 Requirements for Multi-Document Summarization There are two types of situations in which multidocument summarization would be useful: (1) the user is faced with a collection of dis-similar documents and wishes to assess the information landscape contained in the collection, or (2) there is a collection of topicallyrelated documents, extracted from a larger more diverse collection as the result of a query, or a topically-cohesive cluster. In the first case, if the collection is large enough, it only makes sense to first cluster and categorize the documents (Yang et al., 1999), and then sample from, or summarize each cohesive cluster. Hence, a &amp;quot;summary&amp;quot; would constitute of a visualization of the information landscape, where features could be clusters or summaries thereof. In the second case, it is possible to build a synthetic textual summary containing the main point(s) of the topic, augmented with non-redundant background information and/or query-relevant elaborations. This is the focus of our work reported here, including the necessity to eliminate redundancy among the information content of multiple related documents. Users&apos; information seeking needs and goals </context>
</contexts>
<marker>Yang, Carbonell, Brown, Pierce, Archibald, Liu, 1999</marker>
<rawString>,Yiming Yang, Jaime G. Carbonell, Ralf D. Brown, Tom Pierce, Brian T. Archibald, and Xin Liu. 1999. Learning approaches for topic detection and tracking news events. IEEE intelligent Systems, Special Issue on Applications of Intelligent Information Retrieval, 14(4):32-43, July/August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>