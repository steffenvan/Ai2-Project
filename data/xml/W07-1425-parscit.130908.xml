<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.983617">
A Compositional Approach toward Dynamic Phrasal Thesaurus
</title>
<author confidence="0.97104">
Atsushi Fujita Shuhei Kato Naoki Kato Satoshi Sato
</author>
<affiliation confidence="0.965175">
Graduate School of Engineering, Nagoya University
</affiliation>
<email confidence="0.9582535">
Ifujita,ssatol@nuee.nagoya-u.ac.jp
Ishuhei,naokil@sslab.nuee.nagoya-u.ac.jp
</email>
<sectionHeader confidence="0.997161" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9987986">
To enhance the technology for computing
semantic equivalence, we introduce the no-
tion of phrasal thesaurus which is a natural
extension of conventional word-based the-
saurus. Among a variety of phrases that
conveys the same meaning, i.e., paraphrases,
we focus on syntactic variants that are com-
positionally explainable using a small num-
ber of atomic knowledge, and develop a sys-
tem which dynamically generates such vari-
ants. This paper describes the proposed sys-
tem and three sorts of knowledge developed
for dynamic phrasal thesaurus in Japanese:
(i) transformation pattern, (ii) generation
function, and (iii) lexical function.
</bodyText>
<sectionHeader confidence="0.99951" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999810936170213">
Linguistic expressions that convey the same mean-
ing are called paraphrases. Handling paraphrases
is one of the key issues in a broad range of nat-
ural language processing tasks, including machine
translation, information retrieval, information ex-
traction, question answering, summarization, text
mining, and natural language generation.
Conventional approaches to computing semantic
equivalence between two expressions are five-fold.
The first approximates it based on the similarities
between their constituent words. If two words be-
long to closer nodes in a thesaurus or semantic net-
work, they are considered more likely to be similar.
The second uses the family of tree kernels (Collins
and Duffy, 2001; Takahashi, 2005). The degree of
equivalence of two trees (sentences) is defined as
the number of common subtrees included in both
trees. The third estimates the equivalence based on
word alignment composed using templates or trans-
lation probabilities derived from a set of parallel
text (Barzilay and Lee, 2003; Brockett and Dolan,
2005). The fourth espouses the distributional hy-
pothesis (Harris, 1968): given two words are likely
to be equivalent if distributions of their surrounding
words are similar (Lin and Pantel, 2001; Weeds et
al., 2005). The final regards two expressions equiva-
lent if they can be associated by using a set of lexico-
syntactic paraphrase patterns (Mel’cuk, 1996; Dras,
1999; Yoshikane et al., 1999; Takahashi, 2005).
Despite the results previous work has achieved,
no system that robustly recognizes and generates
paraphrases is established. We are not convinced of
a hypothesis underlying the word-based approaches
because the structure of words also conveys some
meaning. Even tree kernels, which take structures
into account, do not have a mechanism for iden-
tifying typical equivalents: e.g., dative alternation
and passivization, and abilities to generate para-
phrases. Contrary to the theoretical basis, the two
lines of corpus-based approaches have problems in
practice, i.e., data sparseness and computation cost.
The pattern-based approaches seem steadiest. Yet
no complete resource or methodology for handling
a wide variety of paraphrases has been developed.
On the basis of this recognition, we introduce the
notion of phrasal thesaurus to directly compute se-
mantic equivalence of phrases such as follows.
</bodyText>
<listItem confidence="0.933272">
(1) a. be in our favor / be favorable for us
b. its reproducibility / if it is reproducible
</listItem>
<bodyText confidence="0.8887292">
c. decrease sharply / show a sharp decrease
d. investigate the cause of a fire /
investigate why there was a fire /
investigate what started a fire /
make an investigation into the cause of a fire
</bodyText>
<page confidence="0.978065">
151
</page>
<note confidence="0.785112">
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 151–158,
Prague, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999877192307692">
Phrasal thesaurus is a natural extension of conven-
tional word-based thesaurus. It is thus promised that
it will bring us the following benefits:
Enhancement of NLP applications: As conven-
tional thesauri, phrasal thesaurus must be
useful to handle paraphrases having different
structures in a wide range of NLP applications.
Reading and writing aids: Showing more appro-
priate alternative phrases must be a power-
ful aid at certain situations such as writing
text. Controlling readability of text by altering
phrases must also be beneficial to readers.
Our aim is to develop resources and mechanisms
for computing semantic equivalence on the working
hypothesis that phrase is the appropriate unit for that
purpose. This paper describes the first version of our
paraphrase generation system and reports on our on-
going work on constructing resources for realizing
phrasal thesaurus.
The following sections describe the range of phe-
nomena we treat (Section 2), the overall architec-
ture of our paraphrase generation system which
functions as phrasal thesaurus (Section 3), the im-
plementation of knowledge bases (Section 4) fol-
lowed by discussion (Section 5), and conclusion
(Section 6).
</bodyText>
<sectionHeader confidence="0.931394" genericHeader="method">
2 Dynamic phrasal thesaurus
</sectionHeader>
<subsectionHeader confidence="0.799016">
2.1 Issue
</subsectionHeader>
<bodyText confidence="0.999422">
Toward realizing phrasal thesaurus, the following
two issues should be discussed.
</bodyText>
<listItem confidence="0.999878">
• What sorts of phrases should be treated
• How to cope with a variety of expressions
</listItem>
<bodyText confidence="0.999931257142857">
Although technologies of shallow parsing have
been dramatically improved in the last decade, it
is still difficult to represent arbitrary expression in
logical form. We therefore think it is reasonable to
define the range relying on lexico-syntactic struc-
ture instead of using particular semantic representa-
tion. According to the work of (Chklovski and Pan-
tel, 2004; Torisawa, 2006), predicate phrase (sim-
ple sentence) is a reasonable unit because it approx-
imately corresponds to the meaning of single event.
Combination of words and a variety of construc-
tion coerce us into handling an enormous number
of expressions than word-based approaches. One
may think taking phrase is like treading a thorny
path because one of the arguments in Section 1 is
about coverage. On this issue, we speculate that
one of the feasible approach to realize a robust sys-
tem is to divide phenomena into compositional and
non-compositional (idiosyncratic) ones1, and sepa-
rately develop resources to handle them as described
in (Fujita and Inui, 2005).
To compute semantic equivalence of idiosyncratic
paraphrases, pairs or groups of paraphrases have to
be statically compiled into a dictionary as word-
based thesaurus. The corpus-based approach is valu-
able for that purpose, although they are not guaran-
teed to collect all idiosyncratic paraphrases. On the
other hand, compositional paraphrases can be cap-
tured by a relatively small number of rules. Thus it
seems tolerable approach to generate them dynam-
ically by applying such rules. Our work is targeted
at compositional paraphrases and the system can be
called dynamic phrasal thesaurus. Hereafter, we
refer to paraphrases that are likely to be explained
compositionally as syntactic variants.
</bodyText>
<subsectionHeader confidence="0.997522">
2.2 Target language: Japanese
</subsectionHeader>
<bodyText confidence="0.992466428571428">
While the discussion above does not depend on par-
ticular language, our implementation of dynamic
phrasal thesaurus is targeted at Japanese. Sev-
eral methods for paraphrasing Japanese predicate
phrases have been proposed (Kondo et al., 1999;
Kondo et al., 2001; Kaji et al., 2002; Fujita et al.,
2005). The range they treat is, however, relatively
narrow because they tend to focus on particular para-
phrase phenomena or to rely on existing resources.
On the other hand, we define the range of phenom-
ena from a top-down viewpoint. As a concrete defi-
nition of predicate phrase in Japanese,
noun phrase + case marker + predicate
is employed which is hereafter referred to “phrase.”
Noun phrase and predicate in Japanese them-
selves subcategorize various syntactic variants as
shown in Figure 1 and paraphrase phenomena for
above phrase also involve those focused on their in-
teraction. Thus the range of phenomena is not so
narrow, and intriguing ones, such as shown in exam-
ples2 (2) and (3), are included.
</bodyText>
<footnote confidence="0.998856285714286">
1We regard lexical paraphrases (e.g., “scope” ⇔ “range”)
and idiomatic paraphrases (e.g., “get the sack” ⇔ “be dismissed
from employment”) as idiosyncratic.
2In each example, “s” and “t” denote an original sentence
and its paraphrase, respectively. SMALLCAPS strings indicate
the syntactic role of their corresponding Japanese expressions.
[N] indicates a nominalizer.
</footnote>
<page confidence="0.995398">
152
</page>
<figure confidence="0.7416512">
(2) Head switching
s. kakunin-o isogu.
checking-ACC to hurry-PRES
We hurry checking it.
t. isoide kakunin-suru.
</figure>
<bodyText confidence="0.743157">
in a hurry to check-PRES
We check it in a hurry.
</bodyText>
<listItem confidence="0.6152765">
(3) Noun phrase ⇔ sub-clause
s. kekka-no saigensei-o kenshou-suru.
result-GEN reproducibility-ACC to validate-PRES
We validate its reproducibility.
t. [ kekka-o saigen-dekiru]
result-ACC to reproduce-to be able
</listItem>
<bodyText confidence="0.853821111111111">
ka-douka-o kenshou-suru.
[N]-whether-ACC to validate-PRES
We validate whether it is reproducible.
We focus on syntactic variants at least one side of
which is subcategorized into the definition of phrase
above. For the sake of simplicity, we hereafter rep-
resent those expressions using part-of-speech (POS)
patterns. For instance, (2s) is called N : C : V type,
and (3s) is N1 : no : N2 : C : V type.
</bodyText>
<sectionHeader confidence="0.956078" genericHeader="method">
3 Paraphrase generation system
</sectionHeader>
<bodyText confidence="0.991308">
Given a phrase, the proposed system generates its
syntactic variants in the following four steps:
</bodyText>
<listItem confidence="0.9945925">
1. Morphological analysis
2. Syntactic transformation
3. Surface generation with lexical choice
4. SLM-based filtering
</listItem>
<bodyText confidence="0.999271142857143">
where no particular domain, occasion, and media is
assumed3. Candidates of syntactic variants are first
over-generated in step 2 and then anomalies among
them are filtered out in steps 3 and 4 using rule-based
lexical choice and statistical language model.
The rest of this section elaborates on each compo-
nent in turn.
</bodyText>
<subsectionHeader confidence="0.99997">
3.1 Morphological analysis
</subsectionHeader>
<bodyText confidence="0.999786142857143">
Technologies of morphological analysis in Japanese
have matured by introducing machine learning tech-
niques and large-scale annotated corpus, and there
are freely available tools. Since the structure of input
phrase is assumed to be quite simple, employment of
dependency analyzer was put off. We simply use a
morphological analyzer MeCab4.
</bodyText>
<footnote confidence="0.967580333333333">
3This corresponds to the linguistic transformation layer of
KURA (Takahashi et al., 2001).
4http://mecab.sourceforge.net/
</footnote>
<figure confidence="0.873478391304348">
“koto”
“mono”
“no”
f N1 N2
l N + sufÞxes
N1 + “no” + N2
Adj + N
Adjectival verb + N
clause + N
original verb
Sino-Japanese verb
lexical compound
light verb
Adv + “suru”
original + original
Sino + original
Sino + Sino
N + Sino
Adj f single word
l compound
Adjectival verb + “da”
Adv + “da”
Copula
</figure>
<figureCaption confidence="0.8743815">
Figure 1: Classification of syntactic variants of noun
phrase and predicate in Japanese.
</figureCaption>
<bodyText confidence="0.999990625">
Our system has a post-analysis processing. If ei-
ther of Sino-Japanese verbal nouns (e.g., “kenshou
(validation)” and “kandou (impression)”) or translit-
eration of verbs in foreign language (e.g., “doraibu
(to drive)” and “shifuto (to shift)”) is immediately
followed by “suru (to do)” or “dekiru (to be able),”
these adjacent two morphemes are joined into a sin-
gle morpheme to avoid incorrect transformation.
</bodyText>
<subsectionHeader confidence="0.99943">
3.2 Syntactic transformation
</subsectionHeader>
<bodyText confidence="0.9995965">
The second step over-generates syntactic variants
using the following three sorts of knowledge:
</bodyText>
<listItem confidence="0.840821222222222">
(i) Transformation pattern: It gives skeletons of
syntactic variants. Each variant is represented
by POS symbols designating the input con-
stituents and triggers of the generation function
and lexical function below.
(ii) Generation function: It enumerates different
expressions that are constituted with the same
set of words and subcategorized into the re-
quired syntactic category. Some of generation
</listItem>
<bodyText confidence="0.752484142857143">
functions handle base phrases, while the rest
generates functional words. Base phrases the
former generates are smaller than that transfor-
mation patterns treat. Since some functional
words are disjunctive, the latter generates all
candidates with a separator “/” and leaves the
selection to the following step.
</bodyText>
<figure confidence="0.975171772727273">
noun phrase 8 formal noun 8
&lt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &gt; content &lt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;: :
8
&lt;&gt;&gt;&gt;&gt;&gt;&gt; &gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt;:
single word
compound
8
&lt;&gt;
&gt;:
modiÞed
8
&lt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;:
predicate
verb phrase 8 single word 8
&lt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; compound &lt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;: &gt;&gt;&gt;:
8
&lt;&gt;
&gt;:
</figure>
<page confidence="0.988647">
153
</page>
<tableCaption confidence="0.999913">
Table 1: Grammar in Backus-Naur form, example, and instantiation for each knowledge.
</tableCaption>
<table confidence="0.957660054054054">
Knowledge type Grammar / Example / Instantiation
(i) Transformation &lt;transformation pattern&gt; ::= &lt;left pattern&gt; ⇒ &lt;right pattern&gt;
pattern &lt;left pattern&gt; ::= (&lt;POS symbol&gt;|&lt;word form&gt;)+
&lt;POS symbol&gt; ::= (N|C|V |Adj|Adv)
&lt;word form&gt; ::= (&lt;hiragana&gt;|&lt;katakana&gt;|&lt;kanji&gt;)+
&lt;right pattern&gt; ::=
(&lt;POS symbol&gt;|&lt;word form&gt;|&lt;function definition&gt;|&lt;lexical function&gt;)+
(a) N : C : V ⇒ adv(V ) : vp(N)
(b) N1 : no : N2 : C : V ⇒ N1 : genCase() : vp(N2) : ka-douka : C : V
(a) kakunin : o : isogu ⇒ adv(isogu) : vp(kakunin)
checking ACC to hurry adv(to hurry) vp(checking)
(b) kekka : no : saigensei : o : kenshou-suru
result GEN reproducibility ACC to validate-PRES
⇒ kekka : genCase() : vp(saigensei) : ka-douka : o : kenshou-suru
result case marker vp(reproducibility) [N]-whether ACC to validate-PRES
(ii) Generation &lt;generation function&gt; ::= &lt;function definition&gt; ⇒ ’{’&lt;right pattern&gt;+’ }’
function &lt;function definition&gt; ::= &lt;syntactic category&gt;’(’&lt;POS symbol&gt;*’)’
&lt;syntactic category&gt; ::= (np  |vp  |lvc)
(a) vp(N) ⇒ {v(N) : genVoice() : genTense()}
(b) np(N1, N2) ⇒ {N1, N2, N1 : N2, N1 : no : N2, vp(N1) : N2, wh(N2) : vp(N1) : ka, ...}
(a) vp(kakunin) ⇒ { v(kakunin) : genVoice() : genTense() }
vp(verification) v(verification) verbal suffix for voice verbal suffix for tense
(b) np(shukka, gen-in)
np(starting fire, reason)
⇒ { shukka , gen-in , shukka : gen-in , shukka : no : gen-in ,
starting fire reason starting fire reason starting fire GEN reason
vp(shukka) : gen-in , wh(gen-in) : vp(shukka) : ka , ... }
vp(starting fire) reason wh(reason) vp(starting fire) [N]
(iii) Lexical &lt;lexical function&gt; ::= &lt;relation&gt;’(’&lt;POS symbol&gt;’)’
function &lt;relation&gt; ::= (n  |v  |adj  |adjv  |adv  |wh)
(a) adv(V)
(b) wh(N)
(a) adv(isogu) ⇒ isoide (given by a verb–adverb dictionary)
adv(to hurry) in a hurry
� �
� (b) wh(gen-in) ⇒ { naze , doushite } (given by a noun–interrogative dictionary)1
wh(reason) why why JJ
</table>
<bodyText confidence="0.932908666666667">
(iii) Lexical function: It generates different lexi-
cal items in certain semantic relations, such
as derivative form, from a given lexical item.
The back-end of this knowledge is a set
of pre-compiled dictionaries as described in
Section 4.2.
Table 1 gives a summary of grammar in Backus-
Naur form, examples, and instantiations of each
knowledge. Figure 2 illustrates an example of
knowledge application flow for transforming (4s)
into (4t), where “:” denotes delimiter of con-
stituents.
</bodyText>
<listItem confidence="0.9743215">
(4) s. “kakunin:o:isogu”
t. “isoide:{kakunin-suru:
</listItem>
<bodyText confidence="0.9737849375">
{0, reru/rareru, seru/saseru}:{0, ta/da}}”
First, transformation patterns that match to the given
input are applied. Then, the skeletons of syntactic
variants given by the pattern are lexicalized by con-
secutively invoking generation functions and lexical
functions. Plural number of expressions that gen-
eration function and lexical function generate are
enumerated within curly brackets. Transformation
is ended when the skeletons are fully lexicalized.
In fact, knowledge design for realizing the trans-
formation is not really new, because we have been
inspired by the previous pattern-based approaches.
Transformation pattern is thus alike that in the
Meaning-Text Theory (MTT) (Mel’ˇcuk, 1996), Syn-
chronous Tree Adjoining Grammar (STAG) (Dras,
1999), meta-rule for Fastr (Yoshikane et al., 1999),
</bodyText>
<page confidence="0.99954">
154
</page>
<figureCaption confidence="0.999482">
Figure 2: Syntactic transformation (for (2)).
</figureCaption>
<bodyText confidence="0.999833125">
and transfer pattern for KURA (Takahashi et al.,
2001). Lexical function is also alike that in MTT.
However, our aim in this research is beyond the
design. In other words, as described in Section 1,
we are aiming at the following two: to develop re-
sources for handling syntactic variants in Japanese,
and to conÞrm if phrasal thesaurus really contribute
to computing semantic equivalence.
</bodyText>
<subsectionHeader confidence="0.994209">
3.3 Surface generation with lexical choice
</subsectionHeader>
<bodyText confidence="0.997723923076923">
The input of the third component is a bunch of candi-
date phrases such as shown in (4t). This component
does the following three processes in turn:
Step 1. Unfolding: All word sequences are gener-
ated by removing curly brackets one by one.
Step 2. Lexical choice: Disjunctive words are con-
catenated with “/” (e.g., “reru/rareru” in (4t)).
One of them is selected based on POS and con-
jugation types of the preceding word.
Step 3. Conjugation: In the transformation step,
conjugative words are moved to different po-
sitions and some of them are newly generated.
Inappropriate conjugation forms are corrected.
</bodyText>
<subsectionHeader confidence="0.996208">
3.4 SLM-based Þltering
</subsectionHeader>
<bodyText confidence="0.997623714285714">
In the Þnal step, we assess the correctness of each
candidate of syntactic variants using a statistical lan-
guage model. Our model simply rejects candidate
phrases that never appear in a large size of raw text
corpus consisting of 15 years of newspaper articles
(Mainichi 1991–2005, approximately 1.8GB). Al-
though it is said that Japanese language has a degree
</bodyText>
<figureCaption confidence="0.999306">
Figure 3: Derivations of phrase types.
</figureCaption>
<bodyText confidence="0.999346333333333">
of freedom in word ordering, current implementa-
tion does not yet employ structured language models
because phrases we handle are simple.
</bodyText>
<sectionHeader confidence="0.998794" genericHeader="method">
4 Knowledge implementation
</sectionHeader>
<subsectionHeader confidence="0.9898435">
4.1 Transformation patterns and generation
functions
</subsectionHeader>
<bodyText confidence="0.999811">
An issue of developing resources is how to ensure
their coverage. Our approach to this issue is to de-
scribe transformation patterns by extending those for
simpler phrases. We Þrst described following three
patterns for N : C : V type phrases which we con-
sider the simplest according to Figure 1.
</bodyText>
<figure confidence="0.633801666666667">
(5) a. N : C : V vp(N)
b. N : C : V N : genCase() : lvc(V)
c. N : C : V adv(V ) : vp(N)
</figure>
<figureCaption confidence="0.445238666666667">
While the pattern (5c) is induced from example (2),
the patterns (5a-b) are derived from examples (6)
and (7), respectively.
</figureCaption>
<figure confidence="0.469258166666667">
(6) s. shigeki-o ukeru
inspiration-ACC to receive
to receive an inspiration
t. shigeki-sareru
to inspire-PASS
to be inspired
</figure>
<figureCaption confidence="0.428361333333333">
(7) s. hada-o shigeki-suru
skin-ACC to stimulate
to stimulate skin
</figureCaption>
<bodyText confidence="0.970651857142857">
t. hada-ni shigeki-o ataeru
skin-DAT stimulus-ACC to give
to give skin a stimulus
Regarding the patterns in (8) as the entire set of
compositional paraphrases for N : C : V type
phrases, we then extended them to a bit more com-
plex phrases as in Figure 3. For instance, 10 patterns
</bodyText>
<figure confidence="0.995902727272727">
+N
N1:N2:C:V
+V
N:C:V1:V2
N:C:V
+Adv
N:C:Adv:V
+Adj
switch V with Adj
Adj:N:C:V
N:C:Adj
</figure>
<page confidence="0.995824">
155
</page>
<tableCaption confidence="0.998207">
Table 2: Transformation patterns.
</tableCaption>
<bodyText confidence="0.369317">
Target phrase # of patterns
</bodyText>
<equation confidence="0.956692142857143">
N : C : V 3
N1 : N2 : C : V 10
N : C : V1 : V2 10
N : C : Adv : V 7
Adj:N:C:V 4
N:C:Adj 3
Total 37
</equation>
<tableCaption confidence="0.95142">
Table 3: Generation functions.
</tableCaption>
<table confidence="0.8211929">
Definition Syntactic category # of returned value
np(N1, N2) noun phrase 9
vp(N) verb phrase 1
vp(N1, N2) verb phrase 2
vp(V1, V2) verb phrase 3
lvc(V) light verb construction 1
genCase() case marker 4
genVoice() verbal suffix for voice 3
genTense() verbal suffix for tense 2
genAspect() verbal suffix for aspect 2
</table>
<bodyText confidence="0.949399">
for N1 : N2 : C : V type phrases shown in (8) have
been described based on patterns in (5), mainly fo-
cusing on interactions between newly introduced N1
and other constituents.
</bodyText>
<equation confidence="0.991068157894737">
(8) a. N1 : N2 : C : V ⇒ vp(N1, N2) (5a)
b. N1 : N2 : C : V ⇒
N1 : genCase() : vp(N2) (5a)
c. N1 : N2 : C : V ⇒
N2 : genCase() : vp(N1) (5a)
d. N1 : N2 : C : V ⇒
np(N1, N2) : genCase() : lvc(V) (5b)
e. N1 : N2 : C : V ⇒ N1:genCase():
N2 : genCase() : lvc(V) (5b)
f. N1 : N2 : C : V ⇒ N2:genCase():
N1 : genCase() : lvc(V) (5b)
g. N1 : N2 : C : V ⇒
adv(V ) : vp(N1, N2) (5c)
h. N1 : N2 : C : V ⇒
adv(V ) : N1 : genCase() : vp(N2) (5c)
i. N1 : N2 : C : V ⇒
adv(V ) : N2 : genCase() : vp(N1) (5c)
j. N1 : N2 : C : V ⇒
np(N1, N2) : C : V (new)
</equation>
<bodyText confidence="0.999716857142857">
The number of transformation patterns we have so
far developed is shown in Table 2.
Generation functions shown in Table 3 are devel-
oped along with creating transformation patterns.
Although this is the heart of the proposed model,
two problems are remained: (i) the granularity of
each generation function is determined according to
</bodyText>
<tableCaption confidence="0.844185">
Table 4: Dictionaries for lexical functions.
</tableCaption>
<figure confidence="0.694379416666667">
ID POS-pair |D ||C ||D u C ||S|
(a) noun–verb 3,431 - 3,431 3,431
(b) noun–adjective 308 667 906 475 †
(c) noun–adjectival verb 1,579 - 1,579 1,579
(d) noun–adverb 271 - 271 271
(e) verb–adjective 252 - 252 192 †
(f) verb–adjectival verb 74 - 74 68 †
(g) verb–adverb 74 - 74 64 †
(h) adjective–adjectival verb 66 95 159 146 †
(i) adjective–adverb 33 - 33 26 †
(j) adjectival verb–adverb 70 - 70 70
Total 6,158 762 6,849 6,322
</figure>
<bodyText confidence="0.99301">
our linguistic intuition, and (ii) they do not ensure of
generating all possible phrases. Therefore, we have
to establish the methodology to create this knowl-
edge more precisely.
</bodyText>
<subsectionHeader confidence="0.997894">
4.2 Lexical functions
</subsectionHeader>
<bodyText confidence="0.999978666666667">
Except wh(N), which generates interrogatives as
shown in the bottom line of Table 1, the relations
we have so far implemented are lexical derivations.
These roughly correspond to S, V, A, and Adv in
MTT. The back-end of these lexical functions is a
set of dictionaries built by the following two steps:
</bodyText>
<subsectionHeader confidence="0.615555">
Step 1. Automatic candidate collection: Most
</subsectionHeader>
<bodyText confidence="0.999892">
derivatives in Japanese share the beginning
of words and are characterized by the corre-
spondences of their suffixes. For example,
“amai (be sweet)” and “amami (sweetness)”
has a typical suffix correspondence “∗-i:∗-mi”
of adjective–noun derivation. Using this clue,
candidates are collected by two methods.
</bodyText>
<listItem confidence="0.992559083333333">
• From dictionary: Retrieve all word pairs from
the given set of words those satisfying the
following four conditions: (i) beginning with
kanji character, (ii) having different POSs,
(iii) sharing at least the first character and the
first sound, and (iv) having a suffix pattern
which corresponds to at least two pairs.
• Using dictionary and corpus: Generate candi-
dates from a set of words by applying a set of
typical suffix patterns, and then check if each
candidate is an actual word using corpus. This
is based on (Langkilde and Knight, 1998).
</listItem>
<bodyText confidence="0.97876275">
Step 2. Manual selection: The set of word pairs
collected in the previous step includes those do
not have particular semantic relationship. This
step involves human to discard noises.
</bodyText>
<page confidence="0.997375">
156
</page>
<bodyText confidence="0.999754222222222">
Table 4 shows the size of 10 dictionaries, where
each column denotes the number of word pairs re-
trieved from IPADIC5 (|D|), those using IPADIC,
seven patterns and the same corpus as in Section 3.4
(|C|), their union (|D U C|), and those manu-
ally judged correct (|J|), respectively. The sets of
word pairs J are used as bi-directional lexical func-
tions, although manual screening for four dictionar-
ies without dagger (†) are still in process.
</bodyText>
<sectionHeader confidence="0.999556" genericHeader="method">
5 Discussion
</sectionHeader>
<subsectionHeader confidence="0.999904">
5.1 Unit of processing
</subsectionHeader>
<bodyText confidence="0.999996260869565">
The working hypothesis underlying our work is that
phrase is the appropriate unit for computing seman-
tic equivalence. In addition to the arguments in
Section 1, the hypothesis is supported by what is
done in practice. Let us see two related fields.
The first is the task of word sense disambigua-
tion (WSD). State-of-the-art WSD techniques refer
to context as a clue. However, the range of context
is usually not so wide: words and their POSs within
small window centered the target word and content
words within the same sentence of the target word.
The task therefore can be viewed as determining the
meaning of phrase based on its constituent words
and surrounding content words.
Statistical language model (SLM) is another field.
SLMs usually deal with various things within word
sequence (or structure) at the same time. How-
ever, relations within a phrase should be differen-
tiated from that between phrases, because checking
the former is for grammaticality, while the latter for
cohesion. We think SLMs should take the phrase to
determine boundaries for assessing the correctness
of generated expressions more accurately.
</bodyText>
<subsectionHeader confidence="0.990114">
5.2 Compositionality
</subsectionHeader>
<bodyText confidence="0.953393444444445">
We examined how large part of manually created
paraphrases could be generated in our compositional
approach. First, a set of paraphrase examples were
created in the following procedure:
Step 1. Most frequent 400 phrases typed N1 : N2 :
C : V were sampled from one year of newspa-
per articles (Mainichi 1991).
Step 2. An annotator produced paraphrases for each
phrase. We allowed to record more than one
</bodyText>
<footnote confidence="0.890234">
5http://mecab.sourceforge.jp/
</footnote>
<bodyText confidence="0.9980108">
paraphrase for a given phrase and to give up
producing paraphrases. As a result, we ob-
tained 211 paraphrases for 170 input phrases.
Manual classification revealed that 42% (88 /211)
of paraphrases could be compositionally explain-
able, and the (theoretical) coverage increases to 86%
(182 /211) if we have a synonym dictionary. This
ratio is enough high to give these phenomena pref-
erence as the research target, although we cannot re-
ject a possibility that data has been biased.
</bodyText>
<subsectionHeader confidence="0.996478">
5.3 SufÞcient condition of equivalence
</subsectionHeader>
<bodyText confidence="0.99999568">
In our system, transformation patterns and genera-
tion functions offer necessary conditions for gener-
ating syntactic variants for given input. However,
we have no sufficient condition to control the appli-
cation of such a knowledge.
It has not been thoroughly clarified what clue can
be sufficient condition to ensure semantic equiva-
lence, even in a number of previous work. Though,
at least, roles of participants in the event have to be
preserved by some means, such as the way presented
in (Pantel et al., 2007). Kaji et al. (2002) introduced
a method of case frame alignment in paraphrase gen-
eration. In the model, arguments of main verb in the
source are taken over by that of the target according
to the similarities between arguments of the source
and target. Fujita et al. (2005) employed a semantic
representation of verb to realize the alignment of the
role of participants governed by the source and tar-
get verbs. According to an empirical experiment in
(Fujita et al., 2005), statistical language models do
not contribute to calculating semantic equivalence,
but to filtering out anomalies. We therefore plan to
incorporate above alignment-based models into our
system, for example, within or after the syntactic
transformation step (Figure 2).
</bodyText>
<sectionHeader confidence="0.780384" genericHeader="method">
5.4 Ideas for improvement
</sectionHeader>
<bodyText confidence="0.999876">
The knowledge and system presented in Section 3
are quite simple. Thus the following features should
be incorporated to improve the system in addition to
the one described in Section 5.3.
</bodyText>
<listItem confidence="0.980931">
• Dependency structure: To enable flexible
matching, such as Adv : N : C : V type input
and transformation pattern for N : C : Adv :
V type phrases.
• Sophisticated SLM: The generation phase
should also take the structure into account to
</listItem>
<page confidence="0.994948">
157
</page>
<bodyText confidence="0.70351">
evaluate generated expressions flexibly.
• Knowledge development: Although we have
not done intrinsic evaluation of knowledge, we
are aware of its incompleteness. We are con-
tinuing manual screening for the dictionaries
and planning to enhance the methodology of
knowledge development.
</bodyText>
<sectionHeader confidence="0.998641" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999995">
To enhance the technology for computing seman-
tic equivalence, we have introduced the notion of
phrasal thesaurus, which is a natural extension of
conventional word-based thesaurus. Plausibility of
taking phrase as the unit of processing has been dis-
cussed from several viewpoints. On the basis of
that, we have been developing a system to dynam-
ically generate syntactic variants in Japanese predi-
cate phrases which utilizes three sorts of knowledge
that are inspired by MTT, STAG, Fastr, and KURA.
Future work includes implementing more precise
features and larger resources to compute semantic
equivalence. We also plan to conduct an empirical
evaluation of the resources and the overall system.
</bodyText>
<sectionHeader confidence="0.998302" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998507">
This work was supported in part by MEXT Grants-
in-Aid for Young Scientists (B) 18700143, and for
Scientific Research (A) 16200009, Japan.
</bodyText>
<sectionHeader confidence="0.998831" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999704218390805">
Regina Barzilay and Lillian Lee. 2003. Learning to paraphrase:
an unsupervised approach using multiple-sequence align-
ment. In Proceedings of the 2003 Human Language Tech-
nology Conference and the North American Chapter of the
Association for Computational Linguistics (HLT-NAACL),
pages 16–23.
Chris Brockett and William B. Dolan. 2005. Support Vector
Machines for paraphrase identification and corpus construc-
tion. In Proceedings of the 3rd International Workshop on
Paraphrasing (IWP), pages 1–8.
Timothy Chklovski and Patrick Pantel. 2004. VerbOcean: min-
ing the Web for fine-grained semantic verb relations. In Pro-
ceedings of the 2004 Conference on Empirical Methods in
Natural Language Processing (EMNLP), pages 33–40.
Michael Collins and Nigel Duffy. 2001. Convolution kernels
for natural language. In Advances in Neural Information
Processing Systems 14: Proceedings of the 2001 Confer-
ence, pages 625–632.
Mark Dras. 1999. Tree adjoining grammar and the reluctant
paraphrasing of text. Ph.D. thesis, Division of Information
and Communication Science, Macquarie University.
Atsushi Fujita, Kentaro Inui, and Yuji Matsumoto. 2005. Ex-
ploiting Lexical Conceptual Structure for paraphrase gener-
ation. In Proceedings of the 2nd International Joint Con-
ference on Natural Language Processing (IJCNLP), pages
908–919.
Atsushi Fujita and Kentaro Inui. 2005. A class-oriented ap-
proach to building a paraphrase corpus. In Proceedings
of the 3rd International Workshop on Paraphrasing (IWP),
pages 25–32.
Zellig Harris. 1968. Mathematical structures of language.
New York: Interscience Publishers.
Nobuhiro Kaji, Daisuke Kawahara, Sadao Kurohashi, and
Satoshi Sato. 2002. Verb paraphrase based on case frame
alignment. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics (ACL), pages
215–222.
Keiko Kondo, Satoshi Sato, and Manabu Okumura. 1999.
Paraphrasing of “sahen-noun + suru”. IPSJ Journal,
40(11):4064–4074. (in Japanese).
Keiko Kondo, Satoshi Sato, and Manabu Okumura. 2001.
Paraphrasing by case alternation. IPSJ Journal, 42(3):465–
477. (in Japanese).
Irene Langkilde and Kevin Knight. 1998. Generation that ex-
ploits corpus-based statistical knowledge. In Proceedings of
the 36th Annual Meeting of the Association for Computa-
tional Linguistics and the 17th International Conference on
Computational Linguistics (COLING-ACL), pages 704–710.
Dekang Lin and Patrick Pantel. 2001. Discovery of inference
rules for question answering. Natural Language Engineer-
ing, 7(4):343–360.
Igor Mel’ÿcuk. 1996. Lexical functions: a tool for the descrip-
tion of lexical relations in a lexicon. In Leo Wanner, editor,
Lexical Functions in Lexicography and Natural Language
Processing, pages 37–102. John Benjamin Publishing Com-
pany.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola, Timothy
Chklovski, and Eduard Hovy. 2007. Isp: Learning infer-
ential selectional preferences. In Proceedings of Human
Language Technologies 2007: The Conference of the North
American Chapter of the Association for Computational Lin-
guistics (NAACL-HLT), pages 564–571.
Tetsuro Takahashi, Tomoya Iwakura, Ryu Iida, Atsushi Fujita,
and Kentaro Inui. 2001. KURA: a transfer-based lexico-
structural paraphrasing engine. In Proceedings of the 6th
Natural Language Processing PaciÞc Rim Symposium (NL-
PRS) Workshop on Automatic Paraphrasing: Theories and
Applications, pages 37–46.
Tetsuro Takahashi. 2005. Computation of semantic equiva-
lence for question answering. Ph.D. thesis, Graduate School
of Information Science, Nara Institute of Science and Tech-
nology.
Kentaro Torisawa. 2006. Acquiring inference rules with tem-
poral constraints by using Japanese coordinated sentences
and noun-verb co-occurrences. In Proceedings of the Hu-
man Language Technology Conference of the North Ameri-
can Chapter of the Association for Computational Linguis-
tics (HLT-NAACL), pages 57–64.
Julie Weeds, David Weir, and Bill Keller. 2005. The distribu-
tional similarity of sub-parses. In Proceedings of the ACL
Workshop on Empirical Modeling of Semantic Equivalence
and Entailment, pages 7–12.
Fuyuki Yoshikane, Keita Tsuji, Kyo Kageura, and Christian
Jacquemin. 1999. Detecting Japanese term variation in tex-
tual corpus. In Proceedings of the 4th International Work-
shop on Information Retrieval with Asian Languages, pages
97–108.
</reference>
<page confidence="0.997073">
158
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.748196">
<title confidence="0.999867">A Compositional Approach toward Dynamic Phrasal Thesaurus</title>
<author confidence="0.988648">Atsushi Fujita Shuhei Kato Naoki Kato Satoshi</author>
<affiliation confidence="0.856856">Graduate School of Engineering, Nagoya</affiliation>
<abstract confidence="0.9901899375">To enhance the technology for computing semantic equivalence, we introduce the notion of phrasal thesaurus which is a natural extension of conventional word-based thesaurus. Among a variety of phrases that conveys the same meaning, i.e., paraphrases, we focus on syntactic variants that are compositionally explainable using a small number of atomic knowledge, and develop a system which dynamically generates such variants. This paper describes the proposed system and three sorts of knowledge developed for dynamic phrasal thesaurus in Japanese: (i) transformation pattern, (ii) generation function, and (iii) lexical function.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Learning to paraphrase: an unsupervised approach using multiple-sequence alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Human Language Technology Conference and the North American Chapter of the Association for Computational Linguistics (HLT-NAACL),</booktitle>
<pages>16--23</pages>
<contexts>
<context position="1905" citStr="Barzilay and Lee, 2003" startWordPosition="271" endWordPosition="274">between two expressions are five-fold. The first approximates it based on the similarities between their constituent words. If two words belong to closer nodes in a thesaurus or semantic network, they are considered more likely to be similar. The second uses the family of tree kernels (Collins and Duffy, 2001; Takahashi, 2005). The degree of equivalence of two trees (sentences) is defined as the number of common subtrees included in both trees. The third estimates the equivalence based on word alignment composed using templates or translation probabilities derived from a set of parallel text (Barzilay and Lee, 2003; Brockett and Dolan, 2005). The fourth espouses the distributional hypothesis (Harris, 1968): given two words are likely to be equivalent if distributions of their surrounding words are similar (Lin and Pantel, 2001; Weeds et al., 2005). The final regards two expressions equivalent if they can be associated by using a set of lexicosyntactic paraphrase patterns (Mel’cuk, 1996; Dras, 1999; Yoshikane et al., 1999; Takahashi, 2005). Despite the results previous work has achieved, no system that robustly recognizes and generates paraphrases is established. We are not convinced of a hypothesis unde</context>
</contexts>
<marker>Barzilay, Lee, 2003</marker>
<rawString>Regina Barzilay and Lillian Lee. 2003. Learning to paraphrase: an unsupervised approach using multiple-sequence alignment. In Proceedings of the 2003 Human Language Technology Conference and the North American Chapter of the Association for Computational Linguistics (HLT-NAACL), pages 16–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Brockett</author>
<author>William B Dolan</author>
</authors>
<title>Support Vector Machines for paraphrase identification and corpus construction.</title>
<date>2005</date>
<booktitle>In Proceedings of the 3rd International Workshop on Paraphrasing (IWP),</booktitle>
<pages>1--8</pages>
<contexts>
<context position="1932" citStr="Brockett and Dolan, 2005" startWordPosition="275" endWordPosition="278">are five-fold. The first approximates it based on the similarities between their constituent words. If two words belong to closer nodes in a thesaurus or semantic network, they are considered more likely to be similar. The second uses the family of tree kernels (Collins and Duffy, 2001; Takahashi, 2005). The degree of equivalence of two trees (sentences) is defined as the number of common subtrees included in both trees. The third estimates the equivalence based on word alignment composed using templates or translation probabilities derived from a set of parallel text (Barzilay and Lee, 2003; Brockett and Dolan, 2005). The fourth espouses the distributional hypothesis (Harris, 1968): given two words are likely to be equivalent if distributions of their surrounding words are similar (Lin and Pantel, 2001; Weeds et al., 2005). The final regards two expressions equivalent if they can be associated by using a set of lexicosyntactic paraphrase patterns (Mel’cuk, 1996; Dras, 1999; Yoshikane et al., 1999; Takahashi, 2005). Despite the results previous work has achieved, no system that robustly recognizes and generates paraphrases is established. We are not convinced of a hypothesis underlying the word-based appro</context>
</contexts>
<marker>Brockett, Dolan, 2005</marker>
<rawString>Chris Brockett and William B. Dolan. 2005. Support Vector Machines for paraphrase identification and corpus construction. In Proceedings of the 3rd International Workshop on Paraphrasing (IWP), pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Chklovski</author>
<author>Patrick Pantel</author>
</authors>
<title>VerbOcean: mining the Web for fine-grained semantic verb relations.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>33--40</pages>
<contexts>
<context position="5433" citStr="Chklovski and Pantel, 2004" startWordPosition="819" endWordPosition="823">by discussion (Section 5), and conclusion (Section 6). 2 Dynamic phrasal thesaurus 2.1 Issue Toward realizing phrasal thesaurus, the following two issues should be discussed. • What sorts of phrases should be treated • How to cope with a variety of expressions Although technologies of shallow parsing have been dramatically improved in the last decade, it is still difficult to represent arbitrary expression in logical form. We therefore think it is reasonable to define the range relying on lexico-syntactic structure instead of using particular semantic representation. According to the work of (Chklovski and Pantel, 2004; Torisawa, 2006), predicate phrase (simple sentence) is a reasonable unit because it approximately corresponds to the meaning of single event. Combination of words and a variety of construction coerce us into handling an enormous number of expressions than word-based approaches. One may think taking phrase is like treading a thorny path because one of the arguments in Section 1 is about coverage. On this issue, we speculate that one of the feasible approach to realize a robust system is to divide phenomena into compositional and non-compositional (idiosyncratic) ones1, and separately develop </context>
</contexts>
<marker>Chklovski, Pantel, 2004</marker>
<rawString>Timothy Chklovski and Patrick Pantel. 2004. VerbOcean: mining the Web for fine-grained semantic verb relations. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>Convolution kernels for natural language.</title>
<date>2001</date>
<booktitle>In Advances in Neural Information Processing Systems 14: Proceedings of the 2001 Conference,</booktitle>
<pages>625--632</pages>
<contexts>
<context position="1593" citStr="Collins and Duffy, 2001" startWordPosition="222" endWordPosition="225">ing paraphrases is one of the key issues in a broad range of natural language processing tasks, including machine translation, information retrieval, information extraction, question answering, summarization, text mining, and natural language generation. Conventional approaches to computing semantic equivalence between two expressions are five-fold. The first approximates it based on the similarities between their constituent words. If two words belong to closer nodes in a thesaurus or semantic network, they are considered more likely to be similar. The second uses the family of tree kernels (Collins and Duffy, 2001; Takahashi, 2005). The degree of equivalence of two trees (sentences) is defined as the number of common subtrees included in both trees. The third estimates the equivalence based on word alignment composed using templates or translation probabilities derived from a set of parallel text (Barzilay and Lee, 2003; Brockett and Dolan, 2005). The fourth espouses the distributional hypothesis (Harris, 1968): given two words are likely to be equivalent if distributions of their surrounding words are similar (Lin and Pantel, 2001; Weeds et al., 2005). The final regards two expressions equivalent if t</context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>Michael Collins and Nigel Duffy. 2001. Convolution kernels for natural language. In Advances in Neural Information Processing Systems 14: Proceedings of the 2001 Conference, pages 625–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dras</author>
</authors>
<title>Tree adjoining grammar and the reluctant paraphrasing of text.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>Division of Information and Communication Science, Macquarie University.</institution>
<contexts>
<context position="2295" citStr="Dras, 1999" startWordPosition="335" endWordPosition="336">ber of common subtrees included in both trees. The third estimates the equivalence based on word alignment composed using templates or translation probabilities derived from a set of parallel text (Barzilay and Lee, 2003; Brockett and Dolan, 2005). The fourth espouses the distributional hypothesis (Harris, 1968): given two words are likely to be equivalent if distributions of their surrounding words are similar (Lin and Pantel, 2001; Weeds et al., 2005). The final regards two expressions equivalent if they can be associated by using a set of lexicosyntactic paraphrase patterns (Mel’cuk, 1996; Dras, 1999; Yoshikane et al., 1999; Takahashi, 2005). Despite the results previous work has achieved, no system that robustly recognizes and generates paraphrases is established. We are not convinced of a hypothesis underlying the word-based approaches because the structure of words also conveys some meaning. Even tree kernels, which take structures into account, do not have a mechanism for identifying typical equivalents: e.g., dative alternation and passivization, and abilities to generate paraphrases. Contrary to the theoretical basis, the two lines of corpus-based approaches have problems in practic</context>
<context position="15195" citStr="Dras, 1999" startWordPosition="2337" endWordPosition="2338">syntactic variants given by the pattern are lexicalized by consecutively invoking generation functions and lexical functions. Plural number of expressions that generation function and lexical function generate are enumerated within curly brackets. Transformation is ended when the skeletons are fully lexicalized. In fact, knowledge design for realizing the transformation is not really new, because we have been inspired by the previous pattern-based approaches. Transformation pattern is thus alike that in the Meaning-Text Theory (MTT) (Mel’ˇcuk, 1996), Synchronous Tree Adjoining Grammar (STAG) (Dras, 1999), meta-rule for Fastr (Yoshikane et al., 1999), 154 Figure 2: Syntactic transformation (for (2)). and transfer pattern for KURA (Takahashi et al., 2001). Lexical function is also alike that in MTT. However, our aim in this research is beyond the design. In other words, as described in Section 1, we are aiming at the following two: to develop resources for handling syntactic variants in Japanese, and to conÞrm if phrasal thesaurus really contribute to computing semantic equivalence. 3.3 Surface generation with lexical choice The input of the third component is a bunch of candidate phrases such </context>
</contexts>
<marker>Dras, 1999</marker>
<rawString>Mark Dras. 1999. Tree adjoining grammar and the reluctant paraphrasing of text. Ph.D. thesis, Division of Information and Communication Science, Macquarie University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atsushi Fujita</author>
<author>Kentaro Inui</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Exploiting Lexical Conceptual Structure for paraphrase generation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2nd International Joint Conference on Natural Language Processing (IJCNLP),</booktitle>
<pages>908--919</pages>
<contexts>
<context position="7123" citStr="Fujita et al., 2005" startWordPosition="1085" endWordPosition="1088"> seems tolerable approach to generate them dynamically by applying such rules. Our work is targeted at compositional paraphrases and the system can be called dynamic phrasal thesaurus. Hereafter, we refer to paraphrases that are likely to be explained compositionally as syntactic variants. 2.2 Target language: Japanese While the discussion above does not depend on particular language, our implementation of dynamic phrasal thesaurus is targeted at Japanese. Several methods for paraphrasing Japanese predicate phrases have been proposed (Kondo et al., 1999; Kondo et al., 2001; Kaji et al., 2002; Fujita et al., 2005). The range they treat is, however, relatively narrow because they tend to focus on particular paraphrase phenomena or to rely on existing resources. On the other hand, we define the range of phenomena from a top-down viewpoint. As a concrete definition of predicate phrase in Japanese, noun phrase + case marker + predicate is employed which is hereafter referred to “phrase.” Noun phrase and predicate in Japanese themselves subcategorize various syntactic variants as shown in Figure 1 and paraphrase phenomena for above phrase also involve those focused on their interaction. Thus the range of ph</context>
<context position="25024" citStr="Fujita et al. (2005)" startWordPosition="4058" endWordPosition="4061">ficient condition to control the application of such a knowledge. It has not been thoroughly clarified what clue can be sufficient condition to ensure semantic equivalence, even in a number of previous work. Though, at least, roles of participants in the event have to be preserved by some means, such as the way presented in (Pantel et al., 2007). Kaji et al. (2002) introduced a method of case frame alignment in paraphrase generation. In the model, arguments of main verb in the source are taken over by that of the target according to the similarities between arguments of the source and target. Fujita et al. (2005) employed a semantic representation of verb to realize the alignment of the role of participants governed by the source and target verbs. According to an empirical experiment in (Fujita et al., 2005), statistical language models do not contribute to calculating semantic equivalence, but to filtering out anomalies. We therefore plan to incorporate above alignment-based models into our system, for example, within or after the syntactic transformation step (Figure 2). 5.4 Ideas for improvement The knowledge and system presented in Section 3 are quite simple. Thus the following features should be </context>
</contexts>
<marker>Fujita, Inui, Matsumoto, 2005</marker>
<rawString>Atsushi Fujita, Kentaro Inui, and Yuji Matsumoto. 2005. Exploiting Lexical Conceptual Structure for paraphrase generation. In Proceedings of the 2nd International Joint Conference on Natural Language Processing (IJCNLP), pages 908–919.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atsushi Fujita</author>
<author>Kentaro Inui</author>
</authors>
<title>A class-oriented approach to building a paraphrase corpus.</title>
<date>2005</date>
<booktitle>In Proceedings of the 3rd International Workshop on Paraphrasing (IWP),</booktitle>
<pages>25--32</pages>
<contexts>
<context position="6097" citStr="Fujita and Inui, 2005" startWordPosition="927" endWordPosition="930">e sentence) is a reasonable unit because it approximately corresponds to the meaning of single event. Combination of words and a variety of construction coerce us into handling an enormous number of expressions than word-based approaches. One may think taking phrase is like treading a thorny path because one of the arguments in Section 1 is about coverage. On this issue, we speculate that one of the feasible approach to realize a robust system is to divide phenomena into compositional and non-compositional (idiosyncratic) ones1, and separately develop resources to handle them as described in (Fujita and Inui, 2005). To compute semantic equivalence of idiosyncratic paraphrases, pairs or groups of paraphrases have to be statically compiled into a dictionary as wordbased thesaurus. The corpus-based approach is valuable for that purpose, although they are not guaranteed to collect all idiosyncratic paraphrases. On the other hand, compositional paraphrases can be captured by a relatively small number of rules. Thus it seems tolerable approach to generate them dynamically by applying such rules. Our work is targeted at compositional paraphrases and the system can be called dynamic phrasal thesaurus. Hereafter</context>
</contexts>
<marker>Fujita, Inui, 2005</marker>
<rawString>Atsushi Fujita and Kentaro Inui. 2005. A class-oriented approach to building a paraphrase corpus. In Proceedings of the 3rd International Workshop on Paraphrasing (IWP), pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<title>Mathematical structures of language.</title>
<date>1968</date>
<publisher>Interscience Publishers.</publisher>
<location>New York:</location>
<contexts>
<context position="1998" citStr="Harris, 1968" startWordPosition="286" endWordPosition="287">ir constituent words. If two words belong to closer nodes in a thesaurus or semantic network, they are considered more likely to be similar. The second uses the family of tree kernels (Collins and Duffy, 2001; Takahashi, 2005). The degree of equivalence of two trees (sentences) is defined as the number of common subtrees included in both trees. The third estimates the equivalence based on word alignment composed using templates or translation probabilities derived from a set of parallel text (Barzilay and Lee, 2003; Brockett and Dolan, 2005). The fourth espouses the distributional hypothesis (Harris, 1968): given two words are likely to be equivalent if distributions of their surrounding words are similar (Lin and Pantel, 2001; Weeds et al., 2005). The final regards two expressions equivalent if they can be associated by using a set of lexicosyntactic paraphrase patterns (Mel’cuk, 1996; Dras, 1999; Yoshikane et al., 1999; Takahashi, 2005). Despite the results previous work has achieved, no system that robustly recognizes and generates paraphrases is established. We are not convinced of a hypothesis underlying the word-based approaches because the structure of words also conveys some meaning. Ev</context>
</contexts>
<marker>Harris, 1968</marker>
<rawString>Zellig Harris. 1968. Mathematical structures of language. New York: Interscience Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nobuhiro Kaji</author>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
<author>Satoshi Sato</author>
</authors>
<title>Verb paraphrase based on case frame alignment.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>215--222</pages>
<contexts>
<context position="7101" citStr="Kaji et al., 2002" startWordPosition="1081" endWordPosition="1084">r of rules. Thus it seems tolerable approach to generate them dynamically by applying such rules. Our work is targeted at compositional paraphrases and the system can be called dynamic phrasal thesaurus. Hereafter, we refer to paraphrases that are likely to be explained compositionally as syntactic variants. 2.2 Target language: Japanese While the discussion above does not depend on particular language, our implementation of dynamic phrasal thesaurus is targeted at Japanese. Several methods for paraphrasing Japanese predicate phrases have been proposed (Kondo et al., 1999; Kondo et al., 2001; Kaji et al., 2002; Fujita et al., 2005). The range they treat is, however, relatively narrow because they tend to focus on particular paraphrase phenomena or to rely on existing resources. On the other hand, we define the range of phenomena from a top-down viewpoint. As a concrete definition of predicate phrase in Japanese, noun phrase + case marker + predicate is employed which is hereafter referred to “phrase.” Noun phrase and predicate in Japanese themselves subcategorize various syntactic variants as shown in Figure 1 and paraphrase phenomena for above phrase also involve those focused on their interaction</context>
<context position="24771" citStr="Kaji et al. (2002)" startWordPosition="4014" endWordPosition="4017">eject a possibility that data has been biased. 5.3 SufÞcient condition of equivalence In our system, transformation patterns and generation functions offer necessary conditions for generating syntactic variants for given input. However, we have no sufficient condition to control the application of such a knowledge. It has not been thoroughly clarified what clue can be sufficient condition to ensure semantic equivalence, even in a number of previous work. Though, at least, roles of participants in the event have to be preserved by some means, such as the way presented in (Pantel et al., 2007). Kaji et al. (2002) introduced a method of case frame alignment in paraphrase generation. In the model, arguments of main verb in the source are taken over by that of the target according to the similarities between arguments of the source and target. Fujita et al. (2005) employed a semantic representation of verb to realize the alignment of the role of participants governed by the source and target verbs. According to an empirical experiment in (Fujita et al., 2005), statistical language models do not contribute to calculating semantic equivalence, but to filtering out anomalies. We therefore plan to incorporat</context>
</contexts>
<marker>Kaji, Kawahara, Kurohashi, Sato, 2002</marker>
<rawString>Nobuhiro Kaji, Daisuke Kawahara, Sadao Kurohashi, and Satoshi Sato. 2002. Verb paraphrase based on case frame alignment. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 215–222.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keiko Kondo</author>
<author>Satoshi Sato</author>
<author>Manabu Okumura</author>
</authors>
<date>1999</date>
<journal>Paraphrasing of “sahen-noun + suru”. IPSJ Journal,</journal>
<volume>40</volume>
<issue>11</issue>
<note>(in Japanese).</note>
<contexts>
<context position="7062" citStr="Kondo et al., 1999" startWordPosition="1073" endWordPosition="1076"> be captured by a relatively small number of rules. Thus it seems tolerable approach to generate them dynamically by applying such rules. Our work is targeted at compositional paraphrases and the system can be called dynamic phrasal thesaurus. Hereafter, we refer to paraphrases that are likely to be explained compositionally as syntactic variants. 2.2 Target language: Japanese While the discussion above does not depend on particular language, our implementation of dynamic phrasal thesaurus is targeted at Japanese. Several methods for paraphrasing Japanese predicate phrases have been proposed (Kondo et al., 1999; Kondo et al., 2001; Kaji et al., 2002; Fujita et al., 2005). The range they treat is, however, relatively narrow because they tend to focus on particular paraphrase phenomena or to rely on existing resources. On the other hand, we define the range of phenomena from a top-down viewpoint. As a concrete definition of predicate phrase in Japanese, noun phrase + case marker + predicate is employed which is hereafter referred to “phrase.” Noun phrase and predicate in Japanese themselves subcategorize various syntactic variants as shown in Figure 1 and paraphrase phenomena for above phrase also inv</context>
</contexts>
<marker>Kondo, Sato, Okumura, 1999</marker>
<rawString>Keiko Kondo, Satoshi Sato, and Manabu Okumura. 1999. Paraphrasing of “sahen-noun + suru”. IPSJ Journal, 40(11):4064–4074. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keiko Kondo</author>
<author>Satoshi Sato</author>
<author>Manabu Okumura</author>
</authors>
<title>Paraphrasing by case alternation.</title>
<date>2001</date>
<journal>IPSJ Journal,</journal>
<volume>42</volume>
<issue>3</issue>
<pages>477</pages>
<note>(in Japanese).</note>
<contexts>
<context position="7082" citStr="Kondo et al., 2001" startWordPosition="1077" endWordPosition="1080">latively small number of rules. Thus it seems tolerable approach to generate them dynamically by applying such rules. Our work is targeted at compositional paraphrases and the system can be called dynamic phrasal thesaurus. Hereafter, we refer to paraphrases that are likely to be explained compositionally as syntactic variants. 2.2 Target language: Japanese While the discussion above does not depend on particular language, our implementation of dynamic phrasal thesaurus is targeted at Japanese. Several methods for paraphrasing Japanese predicate phrases have been proposed (Kondo et al., 1999; Kondo et al., 2001; Kaji et al., 2002; Fujita et al., 2005). The range they treat is, however, relatively narrow because they tend to focus on particular paraphrase phenomena or to rely on existing resources. On the other hand, we define the range of phenomena from a top-down viewpoint. As a concrete definition of predicate phrase in Japanese, noun phrase + case marker + predicate is employed which is hereafter referred to “phrase.” Noun phrase and predicate in Japanese themselves subcategorize various syntactic variants as shown in Figure 1 and paraphrase phenomena for above phrase also involve those focused o</context>
</contexts>
<marker>Kondo, Sato, Okumura, 2001</marker>
<rawString>Keiko Kondo, Satoshi Sato, and Manabu Okumura. 2001. Paraphrasing by case alternation. IPSJ Journal, 42(3):465– 477. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde</author>
<author>Kevin Knight</author>
</authors>
<title>Generation that exploits corpus-based statistical knowledge.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics (COLING-ACL),</booktitle>
<pages>704--710</pages>
<contexts>
<context position="21473" citStr="Langkilde and Knight, 1998" startWordPosition="3475" endWordPosition="3478">ive–noun derivation. Using this clue, candidates are collected by two methods. • From dictionary: Retrieve all word pairs from the given set of words those satisfying the following four conditions: (i) beginning with kanji character, (ii) having different POSs, (iii) sharing at least the first character and the first sound, and (iv) having a suffix pattern which corresponds to at least two pairs. • Using dictionary and corpus: Generate candidates from a set of words by applying a set of typical suffix patterns, and then check if each candidate is an actual word using corpus. This is based on (Langkilde and Knight, 1998). Step 2. Manual selection: The set of word pairs collected in the previous step includes those do not have particular semantic relationship. This step involves human to discard noises. 156 Table 4 shows the size of 10 dictionaries, where each column denotes the number of word pairs retrieved from IPADIC5 (|D|), those using IPADIC, seven patterns and the same corpus as in Section 3.4 (|C|), their union (|D U C|), and those manually judged correct (|J|), respectively. The sets of word pairs J are used as bi-directional lexical functions, although manual screening for four dictionaries without d</context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>Irene Langkilde and Kevin Knight. 1998. Generation that exploits corpus-based statistical knowledge. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics (COLING-ACL), pages 704–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Discovery of inference rules for question answering.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>4</issue>
<contexts>
<context position="2121" citStr="Lin and Pantel, 2001" startWordPosition="304" endWordPosition="307">re likely to be similar. The second uses the family of tree kernels (Collins and Duffy, 2001; Takahashi, 2005). The degree of equivalence of two trees (sentences) is defined as the number of common subtrees included in both trees. The third estimates the equivalence based on word alignment composed using templates or translation probabilities derived from a set of parallel text (Barzilay and Lee, 2003; Brockett and Dolan, 2005). The fourth espouses the distributional hypothesis (Harris, 1968): given two words are likely to be equivalent if distributions of their surrounding words are similar (Lin and Pantel, 2001; Weeds et al., 2005). The final regards two expressions equivalent if they can be associated by using a set of lexicosyntactic paraphrase patterns (Mel’cuk, 1996; Dras, 1999; Yoshikane et al., 1999; Takahashi, 2005). Despite the results previous work has achieved, no system that robustly recognizes and generates paraphrases is established. We are not convinced of a hypothesis underlying the word-based approaches because the structure of words also conveys some meaning. Even tree kernels, which take structures into account, do not have a mechanism for identifying typical equivalents: e.g., dat</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Discovery of inference rules for question answering. Natural Language Engineering, 7(4):343–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Mel’ÿcuk</author>
</authors>
<title>Lexical functions: a tool for the description of lexical relations in a lexicon.</title>
<date>1996</date>
<booktitle>Lexical Functions in Lexicography and Natural Language Processing,</booktitle>
<pages>37--102</pages>
<editor>In Leo Wanner, editor,</editor>
<publisher>John Benjamin Publishing Company.</publisher>
<marker>Mel’ÿcuk, 1996</marker>
<rawString>Igor Mel’ÿcuk. 1996. Lexical functions: a tool for the description of lexical relations in a lexicon. In Leo Wanner, editor, Lexical Functions in Lexicography and Natural Language Processing, pages 37–102. John Benjamin Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Rahul Bhagat</author>
<author>Bonaventura Coppola</author>
<author>Timothy Chklovski</author>
<author>Eduard Hovy</author>
</authors>
<title>Isp: Learning inferential selectional preferences.</title>
<date>2007</date>
<booktitle>In Proceedings of Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT),</booktitle>
<pages>564--571</pages>
<contexts>
<context position="24751" citStr="Pantel et al., 2007" startWordPosition="4010" endWordPosition="4013">, although we cannot reject a possibility that data has been biased. 5.3 SufÞcient condition of equivalence In our system, transformation patterns and generation functions offer necessary conditions for generating syntactic variants for given input. However, we have no sufficient condition to control the application of such a knowledge. It has not been thoroughly clarified what clue can be sufficient condition to ensure semantic equivalence, even in a number of previous work. Though, at least, roles of participants in the event have to be preserved by some means, such as the way presented in (Pantel et al., 2007). Kaji et al. (2002) introduced a method of case frame alignment in paraphrase generation. In the model, arguments of main verb in the source are taken over by that of the target according to the similarities between arguments of the source and target. Fujita et al. (2005) employed a semantic representation of verb to realize the alignment of the role of participants governed by the source and target verbs. According to an empirical experiment in (Fujita et al., 2005), statistical language models do not contribute to calculating semantic equivalence, but to filtering out anomalies. We therefor</context>
</contexts>
<marker>Pantel, Bhagat, Coppola, Chklovski, Hovy, 2007</marker>
<rawString>Patrick Pantel, Rahul Bhagat, Bonaventura Coppola, Timothy Chklovski, and Eduard Hovy. 2007. Isp: Learning inferential selectional preferences. In Proceedings of Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT), pages 564–571.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuro Takahashi</author>
<author>Tomoya Iwakura</author>
<author>Ryu Iida</author>
<author>Atsushi Fujita</author>
<author>Kentaro Inui</author>
</authors>
<title>KURA: a transfer-based lexicostructural paraphrasing engine.</title>
<date>2001</date>
<booktitle>In Proceedings of the 6th Natural Language Processing PaciÞc Rim Symposium (NLPRS) Workshop on Automatic Paraphrasing: Theories and Applications,</booktitle>
<pages>37--46</pages>
<contexts>
<context position="9994" citStr="Takahashi et al., 2001" startWordPosition="1529" endWordPosition="1532"> them are filtered out in steps 3 and 4 using rule-based lexical choice and statistical language model. The rest of this section elaborates on each component in turn. 3.1 Morphological analysis Technologies of morphological analysis in Japanese have matured by introducing machine learning techniques and large-scale annotated corpus, and there are freely available tools. Since the structure of input phrase is assumed to be quite simple, employment of dependency analyzer was put off. We simply use a morphological analyzer MeCab4. 3This corresponds to the linguistic transformation layer of KURA (Takahashi et al., 2001). 4http://mecab.sourceforge.net/ “koto” “mono” “no” f N1 N2 l N + sufÞxes N1 + “no” + N2 Adj + N Adjectival verb + N clause + N original verb Sino-Japanese verb lexical compound light verb Adv + “suru” original + original Sino + original Sino + Sino N + Sino Adj f single word l compound Adjectival verb + “da” Adv + “da” Copula Figure 1: Classification of syntactic variants of noun phrase and predicate in Japanese. Our system has a post-analysis processing. If either of Sino-Japanese verbal nouns (e.g., “kenshou (validation)” and “kandou (impression)”) or transliteration of verbs in foreign lan</context>
<context position="15347" citStr="Takahashi et al., 2001" startWordPosition="2358" endWordPosition="2361">of expressions that generation function and lexical function generate are enumerated within curly brackets. Transformation is ended when the skeletons are fully lexicalized. In fact, knowledge design for realizing the transformation is not really new, because we have been inspired by the previous pattern-based approaches. Transformation pattern is thus alike that in the Meaning-Text Theory (MTT) (Mel’ˇcuk, 1996), Synchronous Tree Adjoining Grammar (STAG) (Dras, 1999), meta-rule for Fastr (Yoshikane et al., 1999), 154 Figure 2: Syntactic transformation (for (2)). and transfer pattern for KURA (Takahashi et al., 2001). Lexical function is also alike that in MTT. However, our aim in this research is beyond the design. In other words, as described in Section 1, we are aiming at the following two: to develop resources for handling syntactic variants in Japanese, and to conÞrm if phrasal thesaurus really contribute to computing semantic equivalence. 3.3 Surface generation with lexical choice The input of the third component is a bunch of candidate phrases such as shown in (4t). This component does the following three processes in turn: Step 1. Unfolding: All word sequences are generated by removing curly brack</context>
</contexts>
<marker>Takahashi, Iwakura, Iida, Fujita, Inui, 2001</marker>
<rawString>Tetsuro Takahashi, Tomoya Iwakura, Ryu Iida, Atsushi Fujita, and Kentaro Inui. 2001. KURA: a transfer-based lexicostructural paraphrasing engine. In Proceedings of the 6th Natural Language Processing PaciÞc Rim Symposium (NLPRS) Workshop on Automatic Paraphrasing: Theories and Applications, pages 37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuro Takahashi</author>
</authors>
<title>Computation of semantic equivalence for question answering.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>Graduate School of Information Science, Nara Institute of Science and Technology.</institution>
<contexts>
<context position="1611" citStr="Takahashi, 2005" startWordPosition="226" endWordPosition="227"> the key issues in a broad range of natural language processing tasks, including machine translation, information retrieval, information extraction, question answering, summarization, text mining, and natural language generation. Conventional approaches to computing semantic equivalence between two expressions are five-fold. The first approximates it based on the similarities between their constituent words. If two words belong to closer nodes in a thesaurus or semantic network, they are considered more likely to be similar. The second uses the family of tree kernels (Collins and Duffy, 2001; Takahashi, 2005). The degree of equivalence of two trees (sentences) is defined as the number of common subtrees included in both trees. The third estimates the equivalence based on word alignment composed using templates or translation probabilities derived from a set of parallel text (Barzilay and Lee, 2003; Brockett and Dolan, 2005). The fourth espouses the distributional hypothesis (Harris, 1968): given two words are likely to be equivalent if distributions of their surrounding words are similar (Lin and Pantel, 2001; Weeds et al., 2005). The final regards two expressions equivalent if they can be associa</context>
</contexts>
<marker>Takahashi, 2005</marker>
<rawString>Tetsuro Takahashi. 2005. Computation of semantic equivalence for question answering. Ph.D. thesis, Graduate School of Information Science, Nara Institute of Science and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kentaro Torisawa</author>
</authors>
<title>Acquiring inference rules with temporal constraints by using Japanese coordinated sentences and noun-verb co-occurrences.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL),</booktitle>
<pages>57--64</pages>
<contexts>
<context position="5450" citStr="Torisawa, 2006" startWordPosition="824" endWordPosition="825">nd conclusion (Section 6). 2 Dynamic phrasal thesaurus 2.1 Issue Toward realizing phrasal thesaurus, the following two issues should be discussed. • What sorts of phrases should be treated • How to cope with a variety of expressions Although technologies of shallow parsing have been dramatically improved in the last decade, it is still difficult to represent arbitrary expression in logical form. We therefore think it is reasonable to define the range relying on lexico-syntactic structure instead of using particular semantic representation. According to the work of (Chklovski and Pantel, 2004; Torisawa, 2006), predicate phrase (simple sentence) is a reasonable unit because it approximately corresponds to the meaning of single event. Combination of words and a variety of construction coerce us into handling an enormous number of expressions than word-based approaches. One may think taking phrase is like treading a thorny path because one of the arguments in Section 1 is about coverage. On this issue, we speculate that one of the feasible approach to realize a robust system is to divide phenomena into compositional and non-compositional (idiosyncratic) ones1, and separately develop resources to hand</context>
</contexts>
<marker>Torisawa, 2006</marker>
<rawString>Kentaro Torisawa. 2006. Acquiring inference rules with temporal constraints by using Japanese coordinated sentences and noun-verb co-occurrences. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL), pages 57–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
<author>Bill Keller</author>
</authors>
<title>The distributional similarity of sub-parses.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment,</booktitle>
<pages>7--12</pages>
<contexts>
<context position="2142" citStr="Weeds et al., 2005" startWordPosition="308" endWordPosition="311">r. The second uses the family of tree kernels (Collins and Duffy, 2001; Takahashi, 2005). The degree of equivalence of two trees (sentences) is defined as the number of common subtrees included in both trees. The third estimates the equivalence based on word alignment composed using templates or translation probabilities derived from a set of parallel text (Barzilay and Lee, 2003; Brockett and Dolan, 2005). The fourth espouses the distributional hypothesis (Harris, 1968): given two words are likely to be equivalent if distributions of their surrounding words are similar (Lin and Pantel, 2001; Weeds et al., 2005). The final regards two expressions equivalent if they can be associated by using a set of lexicosyntactic paraphrase patterns (Mel’cuk, 1996; Dras, 1999; Yoshikane et al., 1999; Takahashi, 2005). Despite the results previous work has achieved, no system that robustly recognizes and generates paraphrases is established. We are not convinced of a hypothesis underlying the word-based approaches because the structure of words also conveys some meaning. Even tree kernels, which take structures into account, do not have a mechanism for identifying typical equivalents: e.g., dative alternation and p</context>
</contexts>
<marker>Weeds, Weir, Keller, 2005</marker>
<rawString>Julie Weeds, David Weir, and Bill Keller. 2005. The distributional similarity of sub-parses. In Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 7–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuyuki Yoshikane</author>
<author>Keita Tsuji</author>
<author>Kyo Kageura</author>
<author>Christian Jacquemin</author>
</authors>
<title>Detecting Japanese term variation in textual corpus.</title>
<date>1999</date>
<booktitle>In Proceedings of the 4th International Workshop on Information Retrieval with Asian Languages,</booktitle>
<pages>97--108</pages>
<contexts>
<context position="2319" citStr="Yoshikane et al., 1999" startWordPosition="337" endWordPosition="340">n subtrees included in both trees. The third estimates the equivalence based on word alignment composed using templates or translation probabilities derived from a set of parallel text (Barzilay and Lee, 2003; Brockett and Dolan, 2005). The fourth espouses the distributional hypothesis (Harris, 1968): given two words are likely to be equivalent if distributions of their surrounding words are similar (Lin and Pantel, 2001; Weeds et al., 2005). The final regards two expressions equivalent if they can be associated by using a set of lexicosyntactic paraphrase patterns (Mel’cuk, 1996; Dras, 1999; Yoshikane et al., 1999; Takahashi, 2005). Despite the results previous work has achieved, no system that robustly recognizes and generates paraphrases is established. We are not convinced of a hypothesis underlying the word-based approaches because the structure of words also conveys some meaning. Even tree kernels, which take structures into account, do not have a mechanism for identifying typical equivalents: e.g., dative alternation and passivization, and abilities to generate paraphrases. Contrary to the theoretical basis, the two lines of corpus-based approaches have problems in practice, i.e., data sparseness</context>
<context position="15241" citStr="Yoshikane et al., 1999" startWordPosition="2342" endWordPosition="2345">ttern are lexicalized by consecutively invoking generation functions and lexical functions. Plural number of expressions that generation function and lexical function generate are enumerated within curly brackets. Transformation is ended when the skeletons are fully lexicalized. In fact, knowledge design for realizing the transformation is not really new, because we have been inspired by the previous pattern-based approaches. Transformation pattern is thus alike that in the Meaning-Text Theory (MTT) (Mel’ˇcuk, 1996), Synchronous Tree Adjoining Grammar (STAG) (Dras, 1999), meta-rule for Fastr (Yoshikane et al., 1999), 154 Figure 2: Syntactic transformation (for (2)). and transfer pattern for KURA (Takahashi et al., 2001). Lexical function is also alike that in MTT. However, our aim in this research is beyond the design. In other words, as described in Section 1, we are aiming at the following two: to develop resources for handling syntactic variants in Japanese, and to conÞrm if phrasal thesaurus really contribute to computing semantic equivalence. 3.3 Surface generation with lexical choice The input of the third component is a bunch of candidate phrases such as shown in (4t). This component does the foll</context>
</contexts>
<marker>Yoshikane, Tsuji, Kageura, Jacquemin, 1999</marker>
<rawString>Fuyuki Yoshikane, Keita Tsuji, Kyo Kageura, and Christian Jacquemin. 1999. Detecting Japanese term variation in textual corpus. In Proceedings of the 4th International Workshop on Information Retrieval with Asian Languages, pages 97–108.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>