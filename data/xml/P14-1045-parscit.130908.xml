<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.9975435">
Predicting the relevance of distributional semantic similarity with
contextual information
</title>
<author confidence="0.993724">
Philippe Muller
</author>
<affiliation confidence="0.994975">
IRIT, Toulouse University
</affiliation>
<address confidence="0.745826">
Universit´e Paul Sabatier
118 Route de Narbonne
31062 Toulouse Cedex 04
</address>
<email confidence="0.998638">
philippe.muller@irit.fr
</email>
<author confidence="0.938584">
C´ecile Fabre
</author>
<affiliation confidence="0.8612815">
CLLE, Toulouse University
Universit´e Toulouse-Le Mirail
</affiliation>
<address confidence="0.653211">
5 alles A. Machado
31058 Toulouse Cedex
</address>
<email confidence="0.770457">
cecile.fabre@univ-tlse2.fr
</email>
<author confidence="0.885053">
Cl´ementine Adam
</author>
<affiliation confidence="0.828226">
CLLE, Toulouse University
Universit´e Toulouse-Le Mirail
</affiliation>
<address confidence="0.581626">
5 alles A. Machado
31058 Toulouse Cedex
</address>
<email confidence="0.654674">
clementine.adam@univ-tlse2.fr
</email>
<sectionHeader confidence="0.992539" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999685538461539">
Using distributional analysis methods to
compute semantic proximity links be-
tween words has become commonplace
in NLP. The resulting relations are often
noisy or difficult to interpret in general.
This paper focuses on the issues of eval-
uating a distributional resource and filter-
ing the relations it contains, but instead
of considering it in abstracto, we focus
on pairs of words in context. In a dis-
course, we are interested in knowing if the
semantic link between two items is a by-
product of textual coherence or is irrele-
vant. We first set up a human annotation
of semantic links with or without contex-
tual information to show the importance of
the textual context in evaluating the rele-
vance of semantic similarity, and to assess
the prevalence of actual semantic relations
between word tokens. We then built an ex-
periment to automatically predict this rel-
evance, evaluated on the reliable reference
data set which was the outcome of the first
annotation. We show that in-document in-
formation greatly improve the prediction
made by the similarity level alone.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999944326923077">
The goal of the work presented in this paper is to
improve distributional thesauri, and to help evalu-
ate the content of such resources. A distributional
thesaurus is a lexical network that lists semantic
neighbours, computed from a corpus and a simi-
larity measure between lexical items, which gen-
erally captures the similarity of contexts in which
the items occur. This way of building a seman-
tic network has been very popular since (Grefen-
stette, 1994; Lin, 1998), even though the nature of
the information it contains is hard to define, and
its evaluation is far from obvious. A distributional
thesaurus includes a lot of “noise” from a seman-
tic point of view, but also lists relevant lexical pairs
that escape classical lexical relations such as syn-
onymy or hypernymy.
There is a classical dichotomy when evaluat-
ing NLP components between extrinsic and in-
trinsic evaluations (Jones, 1994), and this applies
to distributional thesauri (Curran, 2004; Poibeau
and Messiant, 2008). Extrinsic evaluations mea-
sure the capacity of a system in which a resource
or a component to evaluate has been used, for in-
stance in this case information retrieval (van der
Plas, 2008) or word sense disambiguation (Weeds
and Weir, 2005). Intrinsic evaluations try to mea-
sure the resource itself with respect to some hu-
man standard or judgment, for instance by com-
paring a distributional resource with respect to an
existing synonym dictionary or similarity judg-
ment produced by human subjects (Pado and La-
pata, 2007; Baroni and Lenci, 2010). The short-
comings of these methods have been underlined
in (Baroni and Lenci, 2011). Lexical resources
designed for other objectives put the spotlight on
specific areas of the distributional thesaurus. They
are not suitable for the evaluation of the whole
range of semantic relatedness that is exhibited by
distributional similarities, which exceeds the lim-
its of classical lexical relations, even though re-
searchers have tried to collect equivalent resources
manually, to be used as a gold standard (Weeds,
2003; Bordag, 2008; Anguiano et al., 2011). One
advantage of distributional similarities is to exhibit
a lot of different semantic relations, not necessar-
ily standard lexical relations. Even with respect
to established lexical resources, distributional ap-
proaches may improve coverage, complicating the
evaluation even more.
The method we propose here has been de-
signed as an intrinsic evaluation with a view to
validate semantic proximity links in a broad per-
</bodyText>
<page confidence="0.988152">
479
</page>
<note confidence="0.831433">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 479–488,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.9998113">
spective, to cover what (Morris and Hirst, 2004)
call “non classical lexical semantic relations”.
For instance, agentive relations (author/publish,
author/publication) or associative relations (ac-
tor/cinema) should be considered. At the same
time, we want to filter associations that can be
considered as accidental in a semantic perspective
(e.g. flag and composer are similar because they
appear a lot with nationality names). We do this
by judging the relevance of a lexical relation in a
context where both elements of a lexical pair oc-
cur. We show not only that this improves the relia-
bility of human judgments, but also that it gives a
framework where this relevance can be predicted
automatically. We hypothetize that evaluating and
filtering semantic relations in texts where lexical
items occur would help tasks that naturally make
use of semantic similarity relations, but assessing
this goes beyond the present work.
In the rest of this paper, we describe the re-
source we used as a case study, and the data we
collected to evaluate its content (section 2). We
present the experiments we set up to automatically
filter semantic relations in context, with various
groups of features that take into account informa-
tion from the corpus used to build the thesaurus
and contextual information related to occurrences
of semantic neighbours 3). Finally we discuss
some related work on the evaluation and improve-
ment of distributional resources (section 4).
</bodyText>
<sectionHeader confidence="0.990564" genericHeader="introduction">
2 Evaluation of lexical similarity in
context
</sectionHeader>
<subsectionHeader confidence="0.997385">
2.1 Data
</subsectionHeader>
<bodyText confidence="0.999945">
We use a distributional resource for French, built
on a 200M word corpus extracted from the French
Wikipedia, following principles laid out in (Bouri-
gault, 2002) from a structured model (Baroni
and Lenci, 2010), i.e. using syntactic con-
texts. In this approach, contexts are triples (gover-
nor,relation,dependent) derived from syntactic de-
pendency structures. Governors and dependents
are verbs, adjectives and nouns. Multiword units
are available, but they form a very small subset
of the resulting neighbours. Base elements in the
thesaurus are of two types: arguments (depen-
dents’ lemma) and predicates (governor+relation).
This is to keep the predicate/argument distinction
since similarities will be computed between pred-
icate pairs or argument pairs, and a lexical item
can appear in many predicates and as an argument
(e.g. interest as argument, interest for as one pred-
icate). The similarity of distributions was com-
puted with Lin’s score (Lin, 1998).
We will talk of lexical neighbours or distribu-
tional neighbours to label pairs of predicates or ar-
guments, and in the rest of the paper we consider
only lexical pairs with a Lin score of at least 0.1,
which means about 1.4M pairs. This somewhat
arbitrary level is an a priori threshold to limit the
resulting database, and it is conservative enough
not to exclude potential interesting relations. The
distribution of scores is given figure 1; 97% of the
selected pairs have a score between 0.1 and 0.29.
</bodyText>
<figureCaption confidence="0.9270835">
Figure 1: Histogram of Lin scores for pairs con-
sidered.
</figureCaption>
<bodyText confidence="0.99947">
To ease the use of lexical neighbours in our ex-
periments, we merged together predicates that in-
clude the same lexical unit, a posteriori. Thus
there is no need for a syntactic analysis of the con-
text considered when exploiting the resource, and
sparsity is less of an issue1.
</bodyText>
<subsectionHeader confidence="0.999421">
2.2 Annotation
</subsectionHeader>
<bodyText confidence="0.959493357142857">
In order to evaluate the resource, we set up an an-
notation in context: pairs of lexical items are to
be judged in their context of use, in texts where
they occur together. To verify that this method-
ology is useful, we did a preliminary annotation
to contrast judgment on lexical pairs with or with-
out this contextual information. Then we made a
larger annotation in context once we were assured
of the reliability of the methodology.
For the preliminary test, we asked three annota-
tors to judge the similarity of pairs of lexical items
without any context (no-context), and to judge the
1Whenever two predicates with the same lemma have
common neighbours, we average the score of the pairs.
</bodyText>
<page confidence="0.99636">
480
</page>
<bodyText confidence="0.889734363636364">
[...] Le ventre de l’impala de mˆeme que ses l`evres et sa queue sont blancs. Il faut aussi mentionner leurs lignes noires uniques
a` chaque individu au bout des oreilles , sur le dos de la queue et sur le front. Ces lignes noires sont tr`es utiles aux impalas
puisque ce sont des signes qui leur permettent de se reconnaitre entre eux. Ils poss`edent aussi des glandes s´ecr´etant des odeurs
sur les pattes arri`eres et sur le front. Ces odeurs permettent ´egalement aux individus de se reconnaitre entre eux. Il a ´egalement
des coussinets noirs situ´es, a` l’arri`ere de ses pattes . Les impalas mˆales et femelles ont une morphologie diff´erente. En effet,
on peut facilement distinguer un mˆale par ses cornes en forme de S qui mesurent de 40 a` 90 cm de long.
Les impalas vivent dans les savanes o`u l’ herbe (courte ou moyenne) abonde. Bien qu’ils appr´ecient la proximit´e d’une source
d’eau, celle-ci n’est g´en´eralement pas essentielle aux impalas puisqu’ils peuvent se satisfaire de l’eau contenue dans l’ herbe
qu’ils consomment. Leur environnement est relativement peu accident´e et n’est compos´e que d’ herbes , de buissons ainsi que
de quelques arbres.
[...]
</bodyText>
<figureCaption confidence="0.8673835">
Figure 2: Example excerpt during the annotation of lexical pairs: annotators focus on a target item (here
corne, horn, in blue) and must judge yellow words (pending: oreille/queue, ear/tail), either validating
their relevance (green words: pattes, legs) or rejecting them (red words: herbe, grass). The text describes
the morphology of the impala, and its habitat.
</figureCaption>
<bodyText confidence="0.999902666666667">
similarity of pairs presented within a paragraph
where they both occur (in context). The three an-
notators were linguists, and two of them (1 and
3) knew about the resource and how it was built.
For each annotation, 100 pairs were randomly se-
lected, with the following constraints:
</bodyText>
<listItem confidence="0.985107555555555">
• for the no-context annotation, candidate pairs
had a Lin score above 0.2, which placed them
in the top 14% of lexical neighbours with re-
spect to the similarity level.
• for the in context annotation, the only con-
straint was that the pairs occur in the same
paragraph somewhere in the corpus used to
build the resource. The example paragraph
was chosen at random.
</listItem>
<bodyText confidence="0.999977057692308">
The guidelines given in both cases were the
same: “Do you think the two words are seman-
tically close ? In other words, is there a seman-
tic relation between them, either classical (syn-
onymy, hypernymy, co-hyponymy, meronymy, co-
meronymy) or not (the relation can be paraphrased
but does not belong to the previous cases) ?”
For the pre-test, agreement was rather moderate
without context (the average of pairwise kappas
was .46), and much better with a context (aver-
age = .68), with agreement rates above 90%. This
seems to validate the feasability of a reliable anno-
tation of relatedness in context, so we went on for
a larger annotation with two of the previous anno-
tators.
For the larger annotation, the protocol was
slightly changed: two annotators were given 42
full texts from the original corpus where lexical
neighbours occurred. They were asked to judge
the relation between two items types, regardless of
the number of occurrences in the text. This time
there was no filtering of the lexical pairs beyond
the 0.1 threshold of the original resource. We fol-
lowed the well-known postulate (Gale et al., 1992)
that all occurrences of a word in the same dis-
course tend to have the same sense (“one sense
per discourse”), in order to decrease the annotator
workload. We also assumed that the relation be-
tween these items remain stable within the docu-
ment, an arguably strong hypothesis that needed to
be checked against inter-annotator agreement be-
fore beginning the final annotation . It turns out
that the kappa score (0.80) shows a better inter-
annotator agreement than during the preliminary
test, which can be explained by the larger context
given to the annotator (the whole text), and thus
more occurrences of each element in the pair to
judge, and also because the annotators were more
experienced after the preliminary test. Agreement
measures are summed-up table 1. An excerpt of an
example text, as it was presented to the annotators,
is shown figure 2.
Overall, it took only a few days to annotate
9885 pairs of lexical items. Among the pairs that
were presented to the annotators, about 11% were
judged as relevant by the annotators. It is not
easy to decide if the non-relevant pairs are just
noise, or context-dependent associations that were
not present in the actual text considered (for pol-
ysemy reasons for instance), or just low-level as-
sociations. An important aspect is thus to guar-
antee that there is a correlation between the sim-
</bodyText>
<page confidence="0.995844">
481
</page>
<table confidence="0.999843285714286">
Annotators Non-contextual Contextual
Agreement rate Kappa Agreement rate Kappa
N1+N2 77% 0.52 91% 0.66
N1+N3 70% 0.36 92% 0.69
N2+N3 79% 0.50 92% 0.69
Average 75,3% 0,46 91,7% 0,68
Experts NA NA 90.8% 0.80
</table>
<tableCaption confidence="0.997505">
Table 1: Inter-annotator agreements with Cohen’s Kappa for contextual and non-contextual annotations.
</tableCaption>
<bodyText confidence="0.986016903225806">
N1, N2, N3 were annotators during the pre-test; expert annotation was made on a different dataset from
the same corpus, only with the full discourse context.
ilarity score (Lin’s score here), and the evaluated
relevance of the neighbour pairs. Pearson corre-
lation factor shows that Lin score is indeed sig-
nificantly correlated to the annotated relevance of
lexical pairs, albeit not strongly (r = 0.159).
The produced annotation2 can be used as a ref-
erence to explore various aspects of distributional
resources, with the caveat that it is as such a bit
dependent on the particular resource used. We
nonetheless assume that some of the relevant pairs
would appear in other thesauri, or would be of in-
terest in an evaluation of another resource.
The first thing we can analyse from the anno-
tated data is the impact of a threshold on Lin’s
score to select relevant lexical pairs. The resource
itself is built by choosing a cut-off which is sup-
posed to keep pairs with a satisfactory similar-
ity, but this threshold is rather arbitrary. Figure
3 shows the influence of the threshold value to se-
lect relevant pairs, when considering precision and
recall of the pairs that are kept when choosing the
threshold, evaluated against the human annotation
of relevance in context. In case one wants to opti-
mize the F-score (the harmonic mean of precision
and recall) when extracting relevant pairs, we can
see that the optimal point is at .24 for a threshold
of .22 on Lin’s score. This can be considered as a
baseline for extraction of relevant lexical pairs, to
which we turn in the following section.
</bodyText>
<sectionHeader confidence="0.985939" genericHeader="method">
3 Experiments: predicting relevance in
context
</sectionHeader>
<bodyText confidence="0.9997075">
The outcome of the contextual annotation pre-
sented above is a rather sizeable dataset of val-
idated semantic links, and we showed these lin-
guistic judgments to be reliable. We used this
</bodyText>
<footnote confidence="0.9396675">
2Freely available here http://www.irit.fr/
˜Philippe.Muller/resources.html.
</footnote>
<figureCaption confidence="0.982191">
Figure 3: Precision and recall on relevant links
</figureCaption>
<bodyText confidence="0.991699727272727">
with respect to a threshold on the similarity mea-
sure (Lin’s score)
dataset to set up a supervised classification exper-
iment in order to automatically predict the rele-
vance of a semantic link in a given discourse. We
present now the list of features that were used for
the model. They can be divided in three groups,
according to their origin: they are computed from
the whole corpus, gathered from the distributional
resource, or extracted from the considered text
which contains the semantic pair to be evaluated.
</bodyText>
<subsectionHeader confidence="0.964002">
3.1 Features
</subsectionHeader>
<bodyText confidence="0.9998514">
For each pair neighboura/neighbourb, we com-
puted a set of features from Wikipedia (the corpus
used to derive the distributional similarity): We
first computed the frequencies of each item in the
corpus, freqa and freqb, from which we derive
</bodyText>
<listItem confidence="0.9632335">
• freqmin, freqmax : the min and max of
freqa and freqb ;
• freq×: the combination of the two, or
log(freqa × freqb)
</listItem>
<page confidence="0.993616">
482
</page>
<bodyText confidence="0.999209642857143">
We also measured the syntagmatic association of
neighboura and neighbourb, with a mutual infor-
mation measure (Church and Hanks, 1990), com-
puted from the cooccurrence of two tokens within
the same paragraph in Wikipedia. This is a rather
large window, and thus gives a good coverage
with respect to the neighbour database (70% of all
pairs).
A straightforward parameter to include to pre-
dict the relevance of a link is of course the simi-
larity measure itself, here Lin’s information mea-
sure. But this can be complemented by additional
information on the similarity of the neighbours,
namely:
</bodyText>
<listItem confidence="0.97131675">
• each neighbour productivity : proda and
prodb are defined as the numbers of
neighbours of respectively neighboura and
neighbourb in the database (thus related to-
kens with a similarity above the threshold),
from which we derive three features as for
frequencies: the min, the max, and the log
of the product. The idea is that neighbours
whith very high productivity give rise to less
reliable relations.
• the ranks of tokens in other related items
neighbours: ranka−b is defined as the rank of
</listItem>
<bodyText confidence="0.899498142857143">
neighboura among neighbours of neighbourb
ordered with respect to Lin’s score; rangb−a
is defined similarly and again we consider
as features the min, max and log-product of
these ranks.
We add two categorial features, of a more linguis-
tic nature:
</bodyText>
<listItem confidence="0.917968166666667">
• cats is the pair of part-of-speech for the re-
lated items, e.g. to distinguish the relevance
of NN or VV pairs.
• predarg is related to the predicate/argument
distinction: are the related items predicates or
arguments ?
</listItem>
<bodyText confidence="0.9694803">
The last set of features derive from the occur-
rences of related tokens in the considered dis-
courses:
First, we take into account the frequencies of
items within the text, with three features as before:
the min of the frequencies of the two related items,
the max, and the log-product. Then we consider a
tf·idf (Salton et al., 1975) measure, to evaluate the
specificity and arguably the importance of a word
Feature Description
</bodyText>
<equation confidence="0.7972015">
freqmin min(freqa, freqb)
freqmax max(freqa, freqb)
freqx log(freqa x freqb)
im im =log P(a,b)
P (a)�P (b)
lin Lin’s score
rankmin min(ranka−b, rankb−a)
rankmax max(ranka−b, rankb−a)
rankx log(ranka−b x rankb−a)
prodmin min(proda, prodb)
prodmax max(proda, prodb)
prodx log(proda x prodb)
cats neighbour pos pair
predarg predicate or argument
freqtxtmin min(freqtxta, freqtxtb)
freqtxtmax max(freqtxta, freqtxtb)
freqtxtx log(freqtxta x freqstxtb)
tf·ipf tf·ipf (neighboura)xtf·ipf (neighbourb)
coprph copresence in a sentence
coprpara copresence in a paragraph
</equation>
<bodyText confidence="0.955393666666667">
sd smallest distance between
neighboura and neighbourb
gd highest distance between neighboura
and neighbourb
ad average distance between neighboura
and neighbourb
</bodyText>
<equation confidence="0.669426">
prodtxtmin min(proda, prodb)
prodtxtmax max(proda, prodb)
prodtxtx log(proda x prodb)
</equation>
<bodyText confidence="0.985043111111111">
cc belong to the same lexical connected
component
Table 2: Summary of features used in the super-
vised model, with respect to two lexical items a
and b. The first group is corpus related, the second
group is related to the distributional database, the
third group is related to the textual context. Freq
is related to the frequencies in the corpus, Freqtext
the frequencies in the considered text.
</bodyText>
<page confidence="0.997057">
483
</page>
<bodyText confidence="0.999414928571428">
in a document or within a document. Several vari-
ants of tf·idf have been proposed to adapt the mea-
sure to more local areas in a text with respect to the
whole document. For instance (Dias et al., 2007)
propose a tf·isf (term frequency · inverse sentence
frequency), for topic segmentation. We similarly
defined a tf·ipfmeasure based on the frequency of
a word within a paragraph with respect to its fre-
quency within the text. The resulting feature we
used is the product of this measure for neighboura
and neighbourb.
A few other contextual features are included in
the model: the distances between pairs of related
items, instantiated as:
</bodyText>
<listItem confidence="0.987827454545455">
• distance in words between occurrences of re-
lated word types:
– minimal distance between two occur-
rences (sd)
– maximal distance between two occur-
rences (gd)
– average distance (ad) ;
• boolean features indicating whether
neighboura and neighbourb appear in
the same sentence (coprs) or the same
paragraph (coprpara).
</listItem>
<bodyText confidence="0.904958115384615">
Finally, we took into account the network of re-
lated lexical items, by considering the largest sets
of words present in the text and connected in the
database (self-connected components), by adding
the following features:
• the degree of each lemma, seen as a node
in this similarity graph, combined as above
in minimal degree of the pair, maximal de-
gree, and product of degrees (prodtxtmin,
prodtxtmax, prodtxt×). This is the number
of pairs (present in the text) where a lemma
appears in.
• a boolean feature cc saying whether a lexi-
cal pair belongs to a connected component of
the text, except the largest. This reflects the
fact that a small component may concern a
lexical field which is more specific and thus
more relevant to the text.
Figure 4 shows examples of self-connected
components in an excerpt of the page on Go-
rille (gorilla), e.g. the set {pelage, dos, four-
rure} (coat, back, fur).
The last feature is probably not entirely indepen-
dent from the productivity of an item, or from the
tf.ipf measure.
Table 2 sums up the features used in our model.
</bodyText>
<subsectionHeader confidence="0.998797">
3.2 Model
</subsectionHeader>
<bodyText confidence="0.999988222222222">
Our task is to identify relevant similarities between
lexical items, between all possible related pairs,
and we want to train an inductive model, a clas-
sifier, to extract the relevant links. We have seen
that the relevant/not relevant classification is very
imbalanced, biased towards the “not relevant” cat-
egory (about 11%/89%), so we applied methods
dedicated to counter-balance this, and will focus
on the precision and recall of the predicted rele-
vant links.
Following a classical methodology, we made a
10-fold cross-validation to evaluate robustly the
performance of the classifiers. We tested a few
popular machine learning methods, and report on
two of them, a naive bayes model and the best
method on our dataset, the Random Forest clas-
sifier (Breiman, 2001). Other popular methods
(maximum entropy, SVM) have shown slightly in-
ferior combined F-score, even though precision
and recall might yield more important variations.
As a baseline, we can also consider a simple
threshold on the lexical similarity score, in our
case Lin’s measure, which we have shown to yield
the best F-score of 24% when set at 0.22.
To address class imbalance, two broad types of
methods can be applied to help the model focus
on the minority class. The first one is to resam-
ple the training data to balance the two classes,
the second one is to penalize differently the two
classes during training when the model makes a
mistake (a mistake on the minority class being
made more costly than on the majority class). We
tested the two strategies, by applying the classical
Smote method of (Chawla et al., 2002) as a kind
of resampling, and the ensemble method Meta-
Cost of (Domingos, 1999) as a cost-aware learn-
ing method. Smote synthetizes and adds new in-
stances similar to the minority class instances and
is more efficient than a mere resampling. Meta-
Cost is an interesting meta-learner that can use
any classifier as a base classifier. We used Weka’s
implementations of these methods (Frank et al.,
2004), and our experiments and comparisons are
thus easily replicated on our dataset, provided with
this paper, even though they can be improved by
</bodyText>
<page confidence="0.994755">
484
</page>
<figure confidence="0.7786975">
de vue
g ´en´etique
</figure>
<bodyText confidence="0.771713235294118">
Le gorille est apr`es le bonobo et le chimpanz´e , du point
, l’ animal le plus proche
de l’ humain . Cette parent´e a ´et´e confirm´ee par les similitudes entre les chromosomes et les
groupes
sanguins . Notre g ´enome ne diff`ere que de 2 % de celui du gorille .
Redress´es , les gorilles atteignent une taille de 1,75 m`etre , mais ils sont en fait un peu plus grands car
ils ont les genoux fl´echis . L’ envergure des bras d´epasse la longueur du corps et peut atteindre 2,75
m`etres .
Il existe une grande diff´erence de masse entre les sexes : les femelles p`esent de 90 a` 150 kilogrammes
et les mˆales jusqu’ a` 275. En captivit´e , particuli`erement bien nourris , ils atteignent 350 kilogrammes
.
Le pelage d´epend du sexe et de l’ ˆage . Chez les mˆales les plus ˆag´es se d´eveloppe sur le dos une
fourrure gris argent´e , d’ o`u leur nom de “dos argent´es” . Le pelage des gorilles de montagne est
particuli`erement long et soyeux .
Comme tous les anthropodes , les gorilles sont d´epourvus de queue . Leur
anatomie est puissante , le
visage et les oreilles sont glabres et ils pr´esentent des torus supra-orbitaires marqu´es .
</bodyText>
<figureCaption confidence="0.8501746">
Figure 4: A few connected lexical components of the similarity graph, projected on a text, each in a
different color. The groups are, in order of appearance of the first element: {genetic, close, human},
{similarity, kinship}, {chromosome, genome}, {male, female}, {coat, back, fur}, {age/N, aged/A},
{ear, tail, face}. The text describes the gorilla species, more particularly its morphology. Gray words are
other lexical elements in the neighbour database.
</figureCaption>
<bodyText confidence="0.9999848125">
refinements of these techniques. We chose the
following settings for the different models: naive
bayes uses a kernel density estimation for numer-
ical features, as this generally improves perfor-
mance. For Random Forests, we chose to have ten
trees, and each decision is taken on a randomly
chosen set of five features. For resampling, Smote
advises to double the number of instances of the
minority class, and we observed that a bigger re-
sampling degrades performances. For cost-aware
learning, a sensible choice is to invert the class ra-
tio for the cost ratio, i.e. here the cost of a mistake
on a relevant link (false negative) is exactly 8.5
times higher than the cost on a non-relevant link
(false positive), as non-relevant instances are 8.5
times more present than relevant ones.
</bodyText>
<subsectionHeader confidence="0.965811">
3.3 Results
</subsectionHeader>
<bodyText confidence="0.9999615">
We are interested in the precision and recall for
the “relevant” class. If we take the best simple
classifier (random forests), the precision and re-
call are 68.1% and 24.2% for an F-score of 35.7%,
and this is significantly beaten by the Naive Bayes
method as precision and recall are more even (F-
score of 41.5%). This is already a big improve-
ment on the use of the similarity measure alone
(24%). Also note that predicting every link as rel-
evant would result in a 2.6% precision, and thus a
5% F-score. The random forest model is signifi-
cantly improved by the balancing techniques: the
overall best F-score of 46.3% is reached with Ran-
dom Forests and the cost-aware learning method.
Table 3 sums up the scores for the different con-
figurations, with precision, recall, F-score and the
confidence interval on the F-score. We analysed
the learning curve by doing a cross-validation on
reduced set of instances (from 10% to 90%); F1-
scores range from 37.3% with 10% of instances
and stabilize at 80%, with small increment in ev-
ery case.
The filtering approach we propose seems to
yield good results, by augmenting the similarity
built on the whole corpus with signals from the lo-
cal contexts and documents where related lexical
items appear together.
To try to analyse the role of each set of fea-
tures, we repeated the experiment but changed the
set of features used during training, and results are
shown table 4 for the best method (RF with cost-
aware learning).
We can see that similarity-related features (mea-
sures, ranks) have the biggest impact, but the other
ones also seem to play a significant role. We can
draw the tentative conclusion that the quality of
distributional relations depends on the contextual-
izing of the related lexical items, beyond just the
similarity score and the ranks of items as neigh-
bours of other items.
</bodyText>
<page confidence="0.99714">
485
</page>
<table confidence="0.999872625">
Method Precision Recall F-score CI
Baseline (Lin threshold) 24.0 24.0 24.0
RF 68.1 24.2 35.7 f 3.4
NB 34.8 51.3 41.5 f 2.6
RF+resampling 56.6 32.0 40.9 f 3.3
NB+resampling 32.8 54.0 40.7 f 2.5
RF+cost aware learning 40.4 54.3 46.3 f 2.7
NB+cost aware learning 27.3 61.5 37.8 f 2.2
</table>
<tableCaption confidence="0.9911475">
Table 3: Classification scores (%) on the relevant class. CI is the confidence interval on the F-score (RF
= Random Forest, NB= naive bayes).
Table 4: Impact of each group of features on the best scores (%) : the lowest the results, the bigger the
impact of the removed group of features.
</tableCaption>
<table confidence="0.864240333333333">
Features Prec. Recall F-score
all
all − corpus feat.
all − similarity feat.
all − contextual feat.
40.4 54.3 46.3
37.4 52.8 43.8
36.1 49.5 41.8
36.5 54.8 43.8
</table>
<sectionHeader confidence="0.999181" genericHeader="method">
4 Related work
</sectionHeader>
<bodyText confidence="0.999987625">
Our work is related to two issues: evaluating dis-
tributional resources, and improving them. Eval-
uating distributional resources is the subject of a
lot of methodological reflection (Sahlgren, 2006),
and as we said in the introduction, evaluations can
be divided between extrinsic and intrinsic evalua-
tions. In extrinsic evaluations, models are evalu-
ated against benchmarks focusing on a single task
or a single aspect of a resource: either discrimina-
tive, TOEFL-like tests (Freitag et al., 2005), anal-
ogy production (Turney, 2008), or synonym selec-
tion (Weeds, 2003; Anguiano et al., 2011; Fer-
ret, 2013; Curran and Moens, 2002). In intrin-
sic evaluations, associations norms are used, such
as the 353 word-similarity dataset (Finkelstein et
al., 2002), e.g. (Pado and Lapata, 2007; Agirre et
al., 2009), or specifically designed test cases, as
in (Baroni and Lenci, 2011). We differ from all
these evaluation procedures as we do not focus on
an essential view of the relatedness of two lexical
items, but evaluate the link in a context where the
relevance of the link is in question, an “existential”
view of semantic relatedness.
As for improving distributional thesauri, out-
side of numerous alternate approaches to the
construction, there is a body of work focusing
on improving an existing resource, for instance
reweighting context features once an initial the-
saurus is built (Zhitomirsky-Geffet and Dagan,
2009), or post-processing the resource to filter bad
neighbours or re-ranking neighbours of a given
target (Ferret, 2013). They still use “essential”
evaluation measures (mostly synonym extraction),
although the latter comes close to our work since
it also trains a model to detect (intrinsically) bad
neighbours by using example sentences with the
words to discriminate. We are not aware of any
work that would try to evaluate differently seman-
tic neighbours according to the context they ap-
pear in.
</bodyText>
<sectionHeader confidence="0.999301" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999983285714286">
We proposed a method to reliably evaluate distri-
butional semantic similarity in a broad sense by
considering the validation of lexical pairs in con-
texts where they both appear. This helps cover non
classical semantic relations which are hard to eval-
uate with classical resources. We also presented a
supervised learning model which combines global
features from the corpus used to built a distribu-
tional thesaurus and local features from the text
where similarities are to be judged as relevant or
not to the coherence of a document. It seems
from these experiments that the quality of distri-
butional relations depends on the contextualizing
of the related lexical items, beyond just the simi-
</bodyText>
<page confidence="0.997084">
486
</page>
<bodyText confidence="0.999956391304348">
larity score and the ranks of items as neighbours of
other items. This can hopefully help filter out lex-
ical pairs when word lexical similarity is used as
an information source where context is important:
lexical disambiguation (Miller et al., 2012), topic
segmentation (Guinaudeau et al., 2012). This can
also be a preprocessing step when looking for sim-
ilarities at higher levels, for instance at the sen-
tence level (Mihalcea et al., 2006) or other macro-
textual level (Agirre et al., 2013), since these are
always aggregation functions of word similarities.
There are limits to what is presented here: we need
to evaluate the importance of the level of noise in
the distributional neighbours database, or at least
the quantity of non-semantic relations present, and
this depends on the way the database is built. Our
starting corpus is relatively small compared to cur-
rent efforts in this framework. We are confident
that the same methodology can be followed, even
though the quantitative results may vary, since it
is independent of the particular distributional the-
saurus we used, and the way the similarities are
computed.
</bodyText>
<sectionHeader confidence="0.998934" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999211560975609">
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova,
M. Pas¸ca, and A. Soroa. 2009. A study on similar-
ity and relatedness using distributional and wordnet-
based approaches. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 19–27. Asso-
ciation for Computational Linguistics.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *sem 2013 shared
task: Semantic textual similarity. In Second Joint
Conference on Lexical and Computational Seman-
tics (*SEM), Volume 1: Proceedings of the Main
Conference and the Shared Task: Semantic Textual
Similarity, pages 32–43, Atlanta, Georgia, USA,
June. Association for Computational Linguistics.
E.H. Anguiano, P. Denis, et al. 2011. FreDist: Au-
tomatic construction of distributional thesauri for
French. In Actes de la 18eme conf´erence sur
le traitement automatique des langues naturelles,
pages 119–124.
M. Baroni and A. Lenci. 2010. Distributional mem-
ory: A general framework for corpus-based seman-
tics. Computational Linguistics, 36(4):673–721.
M. Baroni and A. Lenci. 2011. How we BLESSed dis-
tributional semantic evaluation. GEMS 2011, pages
1–10.
Stefan Bordag. 2008. A comparison of co-occurrence
and similarity measures as simulations of context.
In Alexander F. Gelbukh, editor, CICLing, volume
4919 of Lecture Notes in Computer Science, pages
52–63. Springer.
D. Bourigault. 2002. UPERY : un outil
d’analyse distributionnelle tendue pour la construc-
tion d’ontologies partir de corpus. In Actes de la
9e confrence sur le Traitement Automatique de la
Langue Naturelle, pages 75–84, Nancy.
Leo Breiman. 2001. Random forests. Machine Learn-
ing, 45(1):5–32.
Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O.
Hall, and W. Philip Kegelmeyer. 2002. Smote: Syn-
thetic minority over-sampling technique. J. Artif. In-
tell. Res. (JAIR), 16:321–357.
Kenneth Church and Patrick Hanks. 1990. Word as-
sociation norms, mutual information, and lexicogra-
phy. Computational Linguistics, 16(1):pp. 22–29.
James R. Curran and Marc Moens. 2002. Improve-
ments in automatic thesaurus extraction. In Pro-
ceedings of the ACL-02 Workshop on Unsupervised
Lexical Acquisition, pages 59–66.
J.R. Curran. 2004. From distributional to semantic
similarity. Ph.D. thesis, University of Edinburgh.
Ga¨el Dias, Elsa Alves, and Jos´e Gabriel Pereira Lopes.
2007. Topic segmentation algorithms for text sum-
marization and passage retrieval: an exhaustive eval-
uation. In Proceedings of the 22nd national confer-
ence on Artificial intelligence - Volume 2, AAAI’07,
pages 1334–1339. AAAI Press.
Pedro Domingos. 1999. Metacost: A general method
for making classifiers cost-sensitive. In Usama M.
Fayyad, Surajit Chaudhuri, and David Madigan, ed-
itors, KDD, pages 155–164. ACM.
Olivier Ferret. 2013. Identifying bad semantic neigh-
bors for improving distributional thesauri. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 561–571, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: the
concept revisited. ACM Trans. Inf. Syst., 20(1):116–
131.
Eibe Frank, Mark Hall, , and Len Trigg. 2004.
Weka 3.3: Data mining software in java.
www.cs.waikato.ac.nz/ml/weka/.
Dayne Freitag, Matthias Blume, John Byrnes, Ed-
mond Chow, Sadik Kapadia, Richard Rohwer, and
Zhiqiang Wang. 2005. New experiments in distri-
butional representations of synonymy. In Proceed-
ings of CoNLL, pages 25–32, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
</reference>
<page confidence="0.980319">
487
</page>
<reference confidence="0.999924405797102">
W. Gale, K. Church, and D. Yarowsky. 1992. One
sense per discourse. In In Proceedings of the 4th
DARPA Speech and Natural Language Workshop,
New-York, pages 233–237.
G. Grefenstette. 1994. Explorations in automatic the-
saurus discovery. Kluwer Academic Pub., Boston.
Camille Guinaudeau, Guillaume Gravier, and Pascale
S´ebillot. 2012. Enhancing lexical cohesion measure
with confidence measures, semantic relations and
language model interpolation for multimedia spo-
ken content topic segmentation. Computer Speech
&amp; Language, 26(2):90–104.
Karen Sparck Jones. 1994. Towards better NLP sys-
tem evaluation. In Proceedings of the Human Lan-
guage Technology Conference, pages 102–107. As-
sociation for Computational Linguistics.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proceedings of the 15th International
Conference on Machine Learning, pages 296–304,
Madison.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In Proceedings
of the 21st national conference on Artificial intel-
ligence, AAAI06, volume 1, pages 775–780. AAAI
Press.
Tristan Miller, Chris Biemann, Torsten Zesch, and
Iryna Gurevych. 2012. Using distributional similar-
ity for lexical expansion in knowledge-based word
sense disambiguation. In Proceedings of COLING
2012, pages 1781–1796, Mumbai, India, December.
The COLING 2012 Organizing Committee.
J. Morris and G. Hirst. 2004. Non-classical lexical se-
mantic relations. In Proceedings of the HLT Work-
shop on Computational Lexical Semantics, pages
46–51, Boston.
Sebastian Pado and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161–199.
Thierry Poibeau and C´edric Messiant. 2008. Do we
still Need Gold Standards for Evaluation? In Pro-
ceedings of the Language Resource and Evaluation
Conference.
Magnus Sahlgren. 2006. Towards pertinent evalua-
tion methodologies for word-space models. In In
Proceedings of the 5th International Conference on
Language Resources and Evaluation.
G. Salton, C. S. Yang, and C. T. Yu. 1975. A theory
of term importance in automatic text analysis. Jour-
nal of the American Society for Information Science,
26(1):33–44.
Peter D. Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. In
Proceedings of the 22nd International Conference
on Computational Linguistics - Volume 1, COLING
’08, pages 905–912, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
L. van der Plas. 2008. Automatic Lexico-Semantic Ac-
quisition for Question Answering. Ph.D. thesis, Uni-
versity of Groningen.
J. Weeds and D. Weir. 2005. Co-occurrence retrieval:
A flexible framework for lexical distributional simi-
larity. Computational Linguistics, 31(4):439–475.
Julie Elizabeth Weeds. 2003. Measures and Appli-
cations of Lexical Distributional Similarity. Ph.D.
thesis, University of Sussex.
Maayan Zhitomirsky-Geffet and Ido Dagan. 2009.
Bootstrapping distributional feature vector quality.
Computational Linguistics, 35(3):435–461.
</reference>
<page confidence="0.997986">
488
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.066714">
<title confidence="0.9896465">Predicting the relevance of distributional semantic similarity contextual information</title>
<author confidence="0.99785">Philippe Muller</author>
<affiliation confidence="0.999916">IRIT, Toulouse University</affiliation>
<address confidence="0.947265">Universit´e Paul Sabatier 118 Route de Narbonne 31062 Toulouse Cedex 04</address>
<email confidence="0.987051">philippe.muller@irit.fr</email>
<author confidence="0.601101">C´ecile</author>
<affiliation confidence="0.9290115">CLLE, Toulouse Universit´e Toulouse-Le</affiliation>
<address confidence="0.806182">5 alles A. 31058 Toulouse Cedex</address>
<email confidence="0.51264">cecile.fabre@univ-tlse2.fr</email>
<affiliation confidence="0.61765">Cl´ementine CLLE, Toulouse Universit´e Toulouse-Le</affiliation>
<address confidence="0.818807">5 alles A. 31058 Toulouse Cedex</address>
<email confidence="0.74227">clementine.adam@univ-tlse2.fr</email>
<abstract confidence="0.998429296296296">Using distributional analysis methods to compute semantic proximity links between words has become commonplace in NLP. The resulting relations are often noisy or difficult to interpret in general. This paper focuses on the issues of evaluating a distributional resource and filtering the relations it contains, but instead of considering it in abstracto, we focus on pairs of words in context. In a discourse, we are interested in knowing if the semantic link between two items is a byproduct of textual coherence or is irrelevant. We first set up a human annotation of semantic links with or without contextual information to show the importance of the textual context in evaluating the relevance of semantic similarity, and to assess the prevalence of actual semantic relations between word tokens. We then built an experiment to automatically predict this relevance, evaluated on the reliable reference data set which was the outcome of the first annotation. We show that in-document information greatly improve the prediction made by the similarity level alone.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>E Alfonseca</author>
<author>K Hall</author>
<author>J Kravalova</author>
<author>M Pas¸ca</author>
<author>A Soroa</author>
</authors>
<title>A study on similarity and relatedness using distributional and wordnetbased approaches.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>pages</pages>
<marker>Agirre, Alfonseca, Hall, Kravalova, Pas¸ca, Soroa, 2009</marker>
<rawString>E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pas¸ca, and A. Soroa. 2009. A study on similarity and relatedness using distributional and wordnetbased approaches. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 19–27. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor GonzalezAgirre</author>
<author>Weiwei Guo</author>
</authors>
<title>sem 2013 shared task: Semantic textual similarity.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity,</booktitle>
<pages>32--43</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="31581" citStr="Agirre et al., 2013" startWordPosition="5196" endWordPosition="5199">hat the quality of distributional relations depends on the contextualizing of the related lexical items, beyond just the simi486 larity score and the ranks of items as neighbours of other items. This can hopefully help filter out lexical pairs when word lexical similarity is used as an information source where context is important: lexical disambiguation (Miller et al., 2012), topic segmentation (Guinaudeau et al., 2012). This can also be a preprocessing step when looking for similarities at higher levels, for instance at the sentence level (Mihalcea et al., 2006) or other macrotextual level (Agirre et al., 2013), since these are always aggregation functions of word similarities. There are limits to what is presented here: we need to evaluate the importance of the level of noise in the distributional neighbours database, or at least the quantity of non-semantic relations present, and this depends on the way the database is built. Our starting corpus is relatively small compared to current efforts in this framework. We are confident that the same methodology can be followed, even though the quantitative results may vary, since it is independent of the particular distributional thesaurus we used, and th</context>
</contexts>
<marker>Agirre, Cer, Diab, GonzalezAgirre, Guo, 2013</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, Aitor GonzalezAgirre, and Weiwei Guo. 2013. *sem 2013 shared task: Semantic textual similarity. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity, pages 32–43, Atlanta, Georgia, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E H Anguiano</author>
<author>P Denis</author>
</authors>
<title>FreDist: Automatic construction of distributional thesauri for French.</title>
<date>2011</date>
<booktitle>In Actes de la 18eme</booktitle>
<pages>119--124</pages>
<marker>Anguiano, Denis, 2011</marker>
<rawString>E.H. Anguiano, P. Denis, et al. 2011. FreDist: Automatic construction of distributional thesauri for French. In Actes de la 18eme conf´erence sur le traitement automatique des langues naturelles, pages 119–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>A Lenci</author>
</authors>
<title>Distributional memory: A general framework for corpus-based semantics.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>4</issue>
<contexts>
<context position="3123" citStr="Baroni and Lenci, 2010" startWordPosition="486" endWordPosition="489">is applies to distributional thesauri (Curran, 2004; Poibeau and Messiant, 2008). Extrinsic evaluations measure the capacity of a system in which a resource or a component to evaluate has been used, for instance in this case information retrieval (van der Plas, 2008) or word sense disambiguation (Weeds and Weir, 2005). Intrinsic evaluations try to measure the resource itself with respect to some human standard or judgment, for instance by comparing a distributional resource with respect to an existing synonym dictionary or similarity judgment produced by human subjects (Pado and Lapata, 2007; Baroni and Lenci, 2010). The shortcomings of these methods have been underlined in (Baroni and Lenci, 2011). Lexical resources designed for other objectives put the spotlight on specific areas of the distributional thesaurus. They are not suitable for the evaluation of the whole range of semantic relatedness that is exhibited by distributional similarities, which exceeds the limits of classical lexical relations, even though researchers have tried to collect equivalent resources manually, to be used as a gold standard (Weeds, 2003; Bordag, 2008; Anguiano et al., 2011). One advantage of distributional similarities is</context>
<context position="6017" citStr="Baroni and Lenci, 2010" startWordPosition="935" endWordPosition="938">up to automatically filter semantic relations in context, with various groups of features that take into account information from the corpus used to build the thesaurus and contextual information related to occurrences of semantic neighbours 3). Finally we discuss some related work on the evaluation and improvement of distributional resources (section 4). 2 Evaluation of lexical similarity in context 2.1 Data We use a distributional resource for French, built on a 200M word corpus extracted from the French Wikipedia, following principles laid out in (Bourigault, 2002) from a structured model (Baroni and Lenci, 2010), i.e. using syntactic contexts. In this approach, contexts are triples (governor,relation,dependent) derived from syntactic dependency structures. Governors and dependents are verbs, adjectives and nouns. Multiword units are available, but they form a very small subset of the resulting neighbours. Base elements in the thesaurus are of two types: arguments (dependents’ lemma) and predicates (governor+relation). This is to keep the predicate/argument distinction since similarities will be computed between predicate pairs or argument pairs, and a lexical item can appear in many predicates and as</context>
</contexts>
<marker>Baroni, Lenci, 2010</marker>
<rawString>M. Baroni and A. Lenci. 2010. Distributional memory: A general framework for corpus-based semantics. Computational Linguistics, 36(4):673–721.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>A Lenci</author>
</authors>
<title>How we BLESSed distributional semantic evaluation. GEMS</title>
<date>2011</date>
<pages>1--10</pages>
<contexts>
<context position="3207" citStr="Baroni and Lenci, 2011" startWordPosition="500" endWordPosition="503">trinsic evaluations measure the capacity of a system in which a resource or a component to evaluate has been used, for instance in this case information retrieval (van der Plas, 2008) or word sense disambiguation (Weeds and Weir, 2005). Intrinsic evaluations try to measure the resource itself with respect to some human standard or judgment, for instance by comparing a distributional resource with respect to an existing synonym dictionary or similarity judgment produced by human subjects (Pado and Lapata, 2007; Baroni and Lenci, 2010). The shortcomings of these methods have been underlined in (Baroni and Lenci, 2011). Lexical resources designed for other objectives put the spotlight on specific areas of the distributional thesaurus. They are not suitable for the evaluation of the whole range of semantic relatedness that is exhibited by distributional similarities, which exceeds the limits of classical lexical relations, even though researchers have tried to collect equivalent resources manually, to be used as a gold standard (Weeds, 2003; Bordag, 2008; Anguiano et al., 2011). One advantage of distributional similarities is to exhibit a lot of different semantic relations, not necessarily standard lexical </context>
<context position="29340" citStr="Baroni and Lenci, 2011" startWordPosition="4833" endWordPosition="4836">can be divided between extrinsic and intrinsic evaluations. In extrinsic evaluations, models are evaluated against benchmarks focusing on a single task or a single aspect of a resource: either discriminative, TOEFL-like tests (Freitag et al., 2005), analogy production (Turney, 2008), or synonym selection (Weeds, 2003; Anguiano et al., 2011; Ferret, 2013; Curran and Moens, 2002). In intrinsic evaluations, associations norms are used, such as the 353 word-similarity dataset (Finkelstein et al., 2002), e.g. (Pado and Lapata, 2007; Agirre et al., 2009), or specifically designed test cases, as in (Baroni and Lenci, 2011). We differ from all these evaluation procedures as we do not focus on an essential view of the relatedness of two lexical items, but evaluate the link in a context where the relevance of the link is in question, an “existential” view of semantic relatedness. As for improving distributional thesauri, outside of numerous alternate approaches to the construction, there is a body of work focusing on improving an existing resource, for instance reweighting context features once an initial thesaurus is built (Zhitomirsky-Geffet and Dagan, 2009), or post-processing the resource to filter bad neighbo</context>
</contexts>
<marker>Baroni, Lenci, 2011</marker>
<rawString>M. Baroni and A. Lenci. 2011. How we BLESSed distributional semantic evaluation. GEMS 2011, pages 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Bordag</author>
</authors>
<title>A comparison of co-occurrence and similarity measures as simulations of context.</title>
<date>2008</date>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>4919</volume>
<pages>52--63</pages>
<editor>In Alexander F. Gelbukh, editor, CICLing,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="3650" citStr="Bordag, 2008" startWordPosition="569" endWordPosition="570">y judgment produced by human subjects (Pado and Lapata, 2007; Baroni and Lenci, 2010). The shortcomings of these methods have been underlined in (Baroni and Lenci, 2011). Lexical resources designed for other objectives put the spotlight on specific areas of the distributional thesaurus. They are not suitable for the evaluation of the whole range of semantic relatedness that is exhibited by distributional similarities, which exceeds the limits of classical lexical relations, even though researchers have tried to collect equivalent resources manually, to be used as a gold standard (Weeds, 2003; Bordag, 2008; Anguiano et al., 2011). One advantage of distributional similarities is to exhibit a lot of different semantic relations, not necessarily standard lexical relations. Even with respect to established lexical resources, distributional approaches may improve coverage, complicating the evaluation even more. The method we propose here has been designed as an intrinsic evaluation with a view to validate semantic proximity links in a broad per479 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 479–488, Baltimore, Maryland, USA, June 23-25 2014. c�2014 </context>
</contexts>
<marker>Bordag, 2008</marker>
<rawString>Stefan Bordag. 2008. A comparison of co-occurrence and similarity measures as simulations of context. In Alexander F. Gelbukh, editor, CICLing, volume 4919 of Lecture Notes in Computer Science, pages 52–63. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bourigault</author>
</authors>
<title>UPERY : un outil d’analyse distributionnelle tendue pour la construction d’ontologies partir de corpus.</title>
<date>2002</date>
<booktitle>In Actes de la 9e confrence sur le Traitement Automatique de la Langue Naturelle,</booktitle>
<pages>75--84</pages>
<location>Nancy.</location>
<contexts>
<context position="5968" citStr="Bourigault, 2002" startWordPosition="928" endWordPosition="930">tion 2). We present the experiments we set up to automatically filter semantic relations in context, with various groups of features that take into account information from the corpus used to build the thesaurus and contextual information related to occurrences of semantic neighbours 3). Finally we discuss some related work on the evaluation and improvement of distributional resources (section 4). 2 Evaluation of lexical similarity in context 2.1 Data We use a distributional resource for French, built on a 200M word corpus extracted from the French Wikipedia, following principles laid out in (Bourigault, 2002) from a structured model (Baroni and Lenci, 2010), i.e. using syntactic contexts. In this approach, contexts are triples (governor,relation,dependent) derived from syntactic dependency structures. Governors and dependents are verbs, adjectives and nouns. Multiword units are available, but they form a very small subset of the resulting neighbours. Base elements in the thesaurus are of two types: arguments (dependents’ lemma) and predicates (governor+relation). This is to keep the predicate/argument distinction since similarities will be computed between predicate pairs or argument pairs, and a </context>
</contexts>
<marker>Bourigault, 2002</marker>
<rawString>D. Bourigault. 2002. UPERY : un outil d’analyse distributionnelle tendue pour la construction d’ontologies partir de corpus. In Actes de la 9e confrence sur le Traitement Automatique de la Langue Naturelle, pages 75–84, Nancy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
</authors>
<title>Random forests.</title>
<date>2001</date>
<booktitle>Machine Learning,</booktitle>
<volume>45</volume>
<issue>1</issue>
<contexts>
<context position="22100" citStr="Breiman, 2001" startWordPosition="3598" endWordPosition="3599">ifier, to extract the relevant links. We have seen that the relevant/not relevant classification is very imbalanced, biased towards the “not relevant” category (about 11%/89%), so we applied methods dedicated to counter-balance this, and will focus on the precision and recall of the predicted relevant links. Following a classical methodology, we made a 10-fold cross-validation to evaluate robustly the performance of the classifiers. We tested a few popular machine learning methods, and report on two of them, a naive bayes model and the best method on our dataset, the Random Forest classifier (Breiman, 2001). Other popular methods (maximum entropy, SVM) have shown slightly inferior combined F-score, even though precision and recall might yield more important variations. As a baseline, we can also consider a simple threshold on the lexical similarity score, in our case Lin’s measure, which we have shown to yield the best F-score of 24% when set at 0.22. To address class imbalance, two broad types of methods can be applied to help the model focus on the minority class. The first one is to resample the training data to balance the two classes, the second one is to penalize differently the two classe</context>
</contexts>
<marker>Breiman, 2001</marker>
<rawString>Leo Breiman. 2001. Random forests. Machine Learning, 45(1):5–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitesh V Chawla</author>
<author>Kevin W Bowyer</author>
<author>Lawrence O Hall</author>
<author>W Philip Kegelmeyer</author>
</authors>
<title>Smote: Synthetic minority over-sampling technique.</title>
<date>2002</date>
<journal>J. Artif. Intell. Res. (JAIR),</journal>
<pages>16--321</pages>
<contexts>
<context position="22927" citStr="Chawla et al., 2002" startWordPosition="3739" endWordPosition="3742">ple threshold on the lexical similarity score, in our case Lin’s measure, which we have shown to yield the best F-score of 24% when set at 0.22. To address class imbalance, two broad types of methods can be applied to help the model focus on the minority class. The first one is to resample the training data to balance the two classes, the second one is to penalize differently the two classes during training when the model makes a mistake (a mistake on the minority class being made more costly than on the majority class). We tested the two strategies, by applying the classical Smote method of (Chawla et al., 2002) as a kind of resampling, and the ensemble method MetaCost of (Domingos, 1999) as a cost-aware learning method. Smote synthetizes and adds new instances similar to the minority class instances and is more efficient than a mere resampling. MetaCost is an interesting meta-learner that can use any classifier as a base classifier. We used Weka’s implementations of these methods (Frank et al., 2004), and our experiments and comparisons are thus easily replicated on our dataset, provided with this paper, even though they can be improved by 484 de vue g ´en´etique Le gorille est apr`es le bonobo et l</context>
</contexts>
<marker>Chawla, Bowyer, Hall, Kegelmeyer, 2002</marker>
<rawString>Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O. Hall, and W. Philip Kegelmeyer. 2002. Smote: Synthetic minority over-sampling technique. J. Artif. Intell. Res. (JAIR), 16:321–357.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<pages>22--29</pages>
<contexts>
<context position="16237" citStr="Church and Hanks, 1990" startWordPosition="2642" endWordPosition="2645">e distributional resource, or extracted from the considered text which contains the semantic pair to be evaluated. 3.1 Features For each pair neighboura/neighbourb, we computed a set of features from Wikipedia (the corpus used to derive the distributional similarity): We first computed the frequencies of each item in the corpus, freqa and freqb, from which we derive • freqmin, freqmax : the min and max of freqa and freqb ; • freq×: the combination of the two, or log(freqa × freqb) 482 We also measured the syntagmatic association of neighboura and neighbourb, with a mutual information measure (Church and Hanks, 1990), computed from the cooccurrence of two tokens within the same paragraph in Wikipedia. This is a rather large window, and thus gives a good coverage with respect to the neighbour database (70% of all pairs). A straightforward parameter to include to predict the relevance of a link is of course the similarity measure itself, here Lin’s information measure. But this can be complemented by additional information on the similarity of the neighbours, namely: • each neighbour productivity : proda and prodb are defined as the numbers of neighbours of respectively neighboura and neighbourb in the data</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Computational Linguistics, 16(1):pp. 22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Marc Moens</author>
</authors>
<title>Improvements in automatic thesaurus extraction.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 Workshop on Unsupervised Lexical Acquisition,</booktitle>
<pages>59--66</pages>
<contexts>
<context position="29097" citStr="Curran and Moens, 2002" startWordPosition="4795" endWordPosition="4798">work is related to two issues: evaluating distributional resources, and improving them. Evaluating distributional resources is the subject of a lot of methodological reflection (Sahlgren, 2006), and as we said in the introduction, evaluations can be divided between extrinsic and intrinsic evaluations. In extrinsic evaluations, models are evaluated against benchmarks focusing on a single task or a single aspect of a resource: either discriminative, TOEFL-like tests (Freitag et al., 2005), analogy production (Turney, 2008), or synonym selection (Weeds, 2003; Anguiano et al., 2011; Ferret, 2013; Curran and Moens, 2002). In intrinsic evaluations, associations norms are used, such as the 353 word-similarity dataset (Finkelstein et al., 2002), e.g. (Pado and Lapata, 2007; Agirre et al., 2009), or specifically designed test cases, as in (Baroni and Lenci, 2011). We differ from all these evaluation procedures as we do not focus on an essential view of the relatedness of two lexical items, but evaluate the link in a context where the relevance of the link is in question, an “existential” view of semantic relatedness. As for improving distributional thesauri, outside of numerous alternate approaches to the constru</context>
</contexts>
<marker>Curran, Moens, 2002</marker>
<rawString>James R. Curran and Marc Moens. 2002. Improvements in automatic thesaurus extraction. In Proceedings of the ACL-02 Workshop on Unsupervised Lexical Acquisition, pages 59–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Curran</author>
</authors>
<title>From distributional to semantic similarity.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="2551" citStr="Curran, 2004" startWordPosition="393" endWordPosition="394"> in which the items occur. This way of building a semantic network has been very popular since (Grefenstette, 1994; Lin, 1998), even though the nature of the information it contains is hard to define, and its evaluation is far from obvious. A distributional thesaurus includes a lot of “noise” from a semantic point of view, but also lists relevant lexical pairs that escape classical lexical relations such as synonymy or hypernymy. There is a classical dichotomy when evaluating NLP components between extrinsic and intrinsic evaluations (Jones, 1994), and this applies to distributional thesauri (Curran, 2004; Poibeau and Messiant, 2008). Extrinsic evaluations measure the capacity of a system in which a resource or a component to evaluate has been used, for instance in this case information retrieval (van der Plas, 2008) or word sense disambiguation (Weeds and Weir, 2005). Intrinsic evaluations try to measure the resource itself with respect to some human standard or judgment, for instance by comparing a distributional resource with respect to an existing synonym dictionary or similarity judgment produced by human subjects (Pado and Lapata, 2007; Baroni and Lenci, 2010). The shortcomings of these </context>
</contexts>
<marker>Curran, 2004</marker>
<rawString>J.R. Curran. 2004. From distributional to semantic similarity. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elsa Alves Ga¨el Dias</author>
<author>Jos´e Gabriel Pereira Lopes</author>
</authors>
<title>Topic segmentation algorithms for text summarization and passage retrieval: an exhaustive evaluation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 22nd national conference on Artificial intelligence - Volume 2, AAAI’07,</booktitle>
<pages>1334--1339</pages>
<publisher>AAAI Press.</publisher>
<marker>Ga¨el Dias, Lopes, 2007</marker>
<rawString>Ga¨el Dias, Elsa Alves, and Jos´e Gabriel Pereira Lopes. 2007. Topic segmentation algorithms for text summarization and passage retrieval: an exhaustive evaluation. In Proceedings of the 22nd national conference on Artificial intelligence - Volume 2, AAAI’07, pages 1334–1339. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro Domingos</author>
</authors>
<title>Metacost: A general method for making classifiers cost-sensitive.</title>
<date>1999</date>
<pages>155--164</pages>
<editor>In Usama M. Fayyad, Surajit Chaudhuri, and David Madigan, editors, KDD,</editor>
<publisher>ACM.</publisher>
<contexts>
<context position="23005" citStr="Domingos, 1999" startWordPosition="3755" endWordPosition="3756">have shown to yield the best F-score of 24% when set at 0.22. To address class imbalance, two broad types of methods can be applied to help the model focus on the minority class. The first one is to resample the training data to balance the two classes, the second one is to penalize differently the two classes during training when the model makes a mistake (a mistake on the minority class being made more costly than on the majority class). We tested the two strategies, by applying the classical Smote method of (Chawla et al., 2002) as a kind of resampling, and the ensemble method MetaCost of (Domingos, 1999) as a cost-aware learning method. Smote synthetizes and adds new instances similar to the minority class instances and is more efficient than a mere resampling. MetaCost is an interesting meta-learner that can use any classifier as a base classifier. We used Weka’s implementations of these methods (Frank et al., 2004), and our experiments and comparisons are thus easily replicated on our dataset, provided with this paper, even though they can be improved by 484 de vue g ´en´etique Le gorille est apr`es le bonobo et le chimpanz´e , du point , l’ animal le plus proche de l’ humain . Cette parent</context>
</contexts>
<marker>Domingos, 1999</marker>
<rawString>Pedro Domingos. 1999. Metacost: A general method for making classifiers cost-sensitive. In Usama M. Fayyad, Surajit Chaudhuri, and David Madigan, editors, KDD, pages 155–164. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Ferret</author>
</authors>
<title>Identifying bad semantic neighbors for improving distributional thesauri.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>561--571</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="29072" citStr="Ferret, 2013" startWordPosition="4792" endWordPosition="4794">ated work Our work is related to two issues: evaluating distributional resources, and improving them. Evaluating distributional resources is the subject of a lot of methodological reflection (Sahlgren, 2006), and as we said in the introduction, evaluations can be divided between extrinsic and intrinsic evaluations. In extrinsic evaluations, models are evaluated against benchmarks focusing on a single task or a single aspect of a resource: either discriminative, TOEFL-like tests (Freitag et al., 2005), analogy production (Turney, 2008), or synonym selection (Weeds, 2003; Anguiano et al., 2011; Ferret, 2013; Curran and Moens, 2002). In intrinsic evaluations, associations norms are used, such as the 353 word-similarity dataset (Finkelstein et al., 2002), e.g. (Pado and Lapata, 2007; Agirre et al., 2009), or specifically designed test cases, as in (Baroni and Lenci, 2011). We differ from all these evaluation procedures as we do not focus on an essential view of the relatedness of two lexical items, but evaluate the link in a context where the relevance of the link is in question, an “existential” view of semantic relatedness. As for improving distributional thesauri, outside of numerous alternate </context>
</contexts>
<marker>Ferret, 2013</marker>
<rawString>Olivier Ferret. 2013. Identifying bad semantic neighbors for improving distributional thesauri. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 561–571, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: the concept revisited.</title>
<date>2002</date>
<journal>ACM Trans. Inf. Syst.,</journal>
<volume>20</volume>
<issue>1</issue>
<pages>131</pages>
<contexts>
<context position="29220" citStr="Finkelstein et al., 2002" startWordPosition="4813" endWordPosition="4816">es is the subject of a lot of methodological reflection (Sahlgren, 2006), and as we said in the introduction, evaluations can be divided between extrinsic and intrinsic evaluations. In extrinsic evaluations, models are evaluated against benchmarks focusing on a single task or a single aspect of a resource: either discriminative, TOEFL-like tests (Freitag et al., 2005), analogy production (Turney, 2008), or synonym selection (Weeds, 2003; Anguiano et al., 2011; Ferret, 2013; Curran and Moens, 2002). In intrinsic evaluations, associations norms are used, such as the 353 word-similarity dataset (Finkelstein et al., 2002), e.g. (Pado and Lapata, 2007; Agirre et al., 2009), or specifically designed test cases, as in (Baroni and Lenci, 2011). We differ from all these evaluation procedures as we do not focus on an essential view of the relatedness of two lexical items, but evaluate the link in a context where the relevance of the link is in question, an “existential” view of semantic relatedness. As for improving distributional thesauri, outside of numerous alternate approaches to the construction, there is a body of work focusing on improving an existing resource, for instance reweighting context features once a</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2002</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2002. Placing search in context: the concept revisited. ACM Trans. Inf. Syst., 20(1):116– 131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eibe Frank</author>
<author>Mark Hall</author>
</authors>
<date>2004</date>
<note>Weka 3.3: Data mining software in java. www.cs.waikato.ac.nz/ml/weka/.</note>
<marker>Frank, Hall, 2004</marker>
<rawString>Eibe Frank, Mark Hall, , and Len Trigg. 2004. Weka 3.3: Data mining software in java. www.cs.waikato.ac.nz/ml/weka/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dayne Freitag</author>
<author>Matthias Blume</author>
<author>John Byrnes</author>
<author>Edmond Chow</author>
<author>Sadik Kapadia</author>
<author>Richard Rohwer</author>
<author>Zhiqiang Wang</author>
</authors>
<title>New experiments in distributional representations of synonymy.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>25--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="28965" citStr="Freitag et al., 2005" startWordPosition="4773" endWordPosition="4776">eat. all − similarity feat. all − contextual feat. 40.4 54.3 46.3 37.4 52.8 43.8 36.1 49.5 41.8 36.5 54.8 43.8 4 Related work Our work is related to two issues: evaluating distributional resources, and improving them. Evaluating distributional resources is the subject of a lot of methodological reflection (Sahlgren, 2006), and as we said in the introduction, evaluations can be divided between extrinsic and intrinsic evaluations. In extrinsic evaluations, models are evaluated against benchmarks focusing on a single task or a single aspect of a resource: either discriminative, TOEFL-like tests (Freitag et al., 2005), analogy production (Turney, 2008), or synonym selection (Weeds, 2003; Anguiano et al., 2011; Ferret, 2013; Curran and Moens, 2002). In intrinsic evaluations, associations norms are used, such as the 353 word-similarity dataset (Finkelstein et al., 2002), e.g. (Pado and Lapata, 2007; Agirre et al., 2009), or specifically designed test cases, as in (Baroni and Lenci, 2011). We differ from all these evaluation procedures as we do not focus on an essential view of the relatedness of two lexical items, but evaluate the link in a context where the relevance of the link is in question, an “existent</context>
</contexts>
<marker>Freitag, Blume, Byrnes, Chow, Kapadia, Rohwer, Wang, 2005</marker>
<rawString>Dayne Freitag, Matthias Blume, John Byrnes, Edmond Chow, Sadik Kapadia, Richard Rohwer, and Zhiqiang Wang. 2005. New experiments in distributional representations of synonymy. In Proceedings of CoNLL, pages 25–32, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Gale</author>
<author>K Church</author>
<author>D Yarowsky</author>
</authors>
<title>One sense per discourse. In</title>
<date>1992</date>
<booktitle>In Proceedings of the 4th DARPA Speech and Natural Language Workshop, New-York,</booktitle>
<pages>233--237</pages>
<contexts>
<context position="11611" citStr="Gale et al., 1992" startWordPosition="1870" endWordPosition="1873">s above 90%. This seems to validate the feasability of a reliable annotation of relatedness in context, so we went on for a larger annotation with two of the previous annotators. For the larger annotation, the protocol was slightly changed: two annotators were given 42 full texts from the original corpus where lexical neighbours occurred. They were asked to judge the relation between two items types, regardless of the number of occurrences in the text. This time there was no filtering of the lexical pairs beyond the 0.1 threshold of the original resource. We followed the well-known postulate (Gale et al., 1992) that all occurrences of a word in the same discourse tend to have the same sense (“one sense per discourse”), in order to decrease the annotator workload. We also assumed that the relation between these items remain stable within the document, an arguably strong hypothesis that needed to be checked against inter-annotator agreement before beginning the final annotation . It turns out that the kappa score (0.80) shows a better interannotator agreement than during the preliminary test, which can be explained by the larger context given to the annotator (the whole text), and thus more occurrence</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>W. Gale, K. Church, and D. Yarowsky. 1992. One sense per discourse. In In Proceedings of the 4th DARPA Speech and Natural Language Workshop, New-York, pages 233–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>Explorations in automatic thesaurus discovery.</title>
<date>1994</date>
<publisher>Kluwer Academic Pub.,</publisher>
<location>Boston.</location>
<contexts>
<context position="2053" citStr="Grefenstette, 1994" startWordPosition="312" endWordPosition="314">which was the outcome of the first annotation. We show that in-document information greatly improve the prediction made by the similarity level alone. 1 Introduction The goal of the work presented in this paper is to improve distributional thesauri, and to help evaluate the content of such resources. A distributional thesaurus is a lexical network that lists semantic neighbours, computed from a corpus and a similarity measure between lexical items, which generally captures the similarity of contexts in which the items occur. This way of building a semantic network has been very popular since (Grefenstette, 1994; Lin, 1998), even though the nature of the information it contains is hard to define, and its evaluation is far from obvious. A distributional thesaurus includes a lot of “noise” from a semantic point of view, but also lists relevant lexical pairs that escape classical lexical relations such as synonymy or hypernymy. There is a classical dichotomy when evaluating NLP components between extrinsic and intrinsic evaluations (Jones, 1994), and this applies to distributional thesauri (Curran, 2004; Poibeau and Messiant, 2008). Extrinsic evaluations measure the capacity of a system in which a resou</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>G. Grefenstette. 1994. Explorations in automatic thesaurus discovery. Kluwer Academic Pub., Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Camille Guinaudeau</author>
<author>Guillaume Gravier</author>
<author>Pascale S´ebillot</author>
</authors>
<title>Enhancing lexical cohesion measure with confidence measures, semantic relations and language model interpolation for multimedia spoken content topic segmentation.</title>
<date>2012</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>26</volume>
<issue>2</issue>
<marker>Guinaudeau, Gravier, S´ebillot, 2012</marker>
<rawString>Camille Guinaudeau, Guillaume Gravier, and Pascale S´ebillot. 2012. Enhancing lexical cohesion measure with confidence measures, semantic relations and language model interpolation for multimedia spoken content topic segmentation. Computer Speech &amp; Language, 26(2):90–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Sparck Jones</author>
</authors>
<title>Towards better NLP system evaluation.</title>
<date>1994</date>
<booktitle>In Proceedings of the Human Language Technology Conference,</booktitle>
<pages>102--107</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2492" citStr="Jones, 1994" startWordPosition="385" endWordPosition="386"> items, which generally captures the similarity of contexts in which the items occur. This way of building a semantic network has been very popular since (Grefenstette, 1994; Lin, 1998), even though the nature of the information it contains is hard to define, and its evaluation is far from obvious. A distributional thesaurus includes a lot of “noise” from a semantic point of view, but also lists relevant lexical pairs that escape classical lexical relations such as synonymy or hypernymy. There is a classical dichotomy when evaluating NLP components between extrinsic and intrinsic evaluations (Jones, 1994), and this applies to distributional thesauri (Curran, 2004; Poibeau and Messiant, 2008). Extrinsic evaluations measure the capacity of a system in which a resource or a component to evaluate has been used, for instance in this case information retrieval (van der Plas, 2008) or word sense disambiguation (Weeds and Weir, 2005). Intrinsic evaluations try to measure the resource itself with respect to some human standard or judgment, for instance by comparing a distributional resource with respect to an existing synonym dictionary or similarity judgment produced by human subjects (Pado and Lapata</context>
</contexts>
<marker>Jones, 1994</marker>
<rawString>Karen Sparck Jones. 1994. Towards better NLP system evaluation. In Proceedings of the Human Language Technology Conference, pages 102–107. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of the 15th International Conference on Machine Learning,</booktitle>
<pages>296--304</pages>
<location>Madison.</location>
<contexts>
<context position="2065" citStr="Lin, 1998" startWordPosition="315" endWordPosition="316">e of the first annotation. We show that in-document information greatly improve the prediction made by the similarity level alone. 1 Introduction The goal of the work presented in this paper is to improve distributional thesauri, and to help evaluate the content of such resources. A distributional thesaurus is a lexical network that lists semantic neighbours, computed from a corpus and a similarity measure between lexical items, which generally captures the similarity of contexts in which the items occur. This way of building a semantic network has been very popular since (Grefenstette, 1994; Lin, 1998), even though the nature of the information it contains is hard to define, and its evaluation is far from obvious. A distributional thesaurus includes a lot of “noise” from a semantic point of view, but also lists relevant lexical pairs that escape classical lexical relations such as synonymy or hypernymy. There is a classical dichotomy when evaluating NLP components between extrinsic and intrinsic evaluations (Jones, 1994), and this applies to distributional thesauri (Curran, 2004; Poibeau and Messiant, 2008). Extrinsic evaluations measure the capacity of a system in which a resource or a com</context>
<context position="6763" citStr="Lin, 1998" startWordPosition="1049" endWordPosition="1050">structures. Governors and dependents are verbs, adjectives and nouns. Multiword units are available, but they form a very small subset of the resulting neighbours. Base elements in the thesaurus are of two types: arguments (dependents’ lemma) and predicates (governor+relation). This is to keep the predicate/argument distinction since similarities will be computed between predicate pairs or argument pairs, and a lexical item can appear in many predicates and as an argument (e.g. interest as argument, interest for as one predicate). The similarity of distributions was computed with Lin’s score (Lin, 1998). We will talk of lexical neighbours or distributional neighbours to label pairs of predicates or arguments, and in the rest of the paper we consider only lexical pairs with a Lin score of at least 0.1, which means about 1.4M pairs. This somewhat arbitrary level is an a priori threshold to limit the resulting database, and it is conservative enough not to exclude potential interesting relations. The distribution of scores is given figure 1; 97% of the selected pairs have a score between 0.1 and 0.29. Figure 1: Histogram of Lin scores for pairs considered. To ease the use of lexical neighbours </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998. An information-theoretic definition of similarity. In Proceedings of the 15th International Conference on Machine Learning, pages 296–304, Madison.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st national conference on Artificial intelligence, AAAI06,</booktitle>
<volume>1</volume>
<pages>775--780</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="31531" citStr="Mihalcea et al., 2006" startWordPosition="5187" endWordPosition="5190">nce of a document. It seems from these experiments that the quality of distributional relations depends on the contextualizing of the related lexical items, beyond just the simi486 larity score and the ranks of items as neighbours of other items. This can hopefully help filter out lexical pairs when word lexical similarity is used as an information source where context is important: lexical disambiguation (Miller et al., 2012), topic segmentation (Guinaudeau et al., 2012). This can also be a preprocessing step when looking for similarities at higher levels, for instance at the sentence level (Mihalcea et al., 2006) or other macrotextual level (Agirre et al., 2013), since these are always aggregation functions of word similarities. There are limits to what is presented here: we need to evaluate the importance of the level of noise in the distributional neighbours database, or at least the quantity of non-semantic relations present, and this depends on the way the database is built. Our starting corpus is relatively small compared to current efforts in this framework. We are confident that the same methodology can be followed, even though the quantitative results may vary, since it is independent of the p</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and knowledge-based measures of text semantic similarity. In Proceedings of the 21st national conference on Artificial intelligence, AAAI06, volume 1, pages 775–780. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tristan Miller</author>
<author>Chris Biemann</author>
<author>Torsten Zesch</author>
<author>Iryna Gurevych</author>
</authors>
<title>Using distributional similarity for lexical expansion in knowledge-based word sense disambiguation.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012,</booktitle>
<pages>1781--1796</pages>
<location>Mumbai, India,</location>
<contexts>
<context position="31339" citStr="Miller et al., 2012" startWordPosition="5155" endWordPosition="5158"> which combines global features from the corpus used to built a distributional thesaurus and local features from the text where similarities are to be judged as relevant or not to the coherence of a document. It seems from these experiments that the quality of distributional relations depends on the contextualizing of the related lexical items, beyond just the simi486 larity score and the ranks of items as neighbours of other items. This can hopefully help filter out lexical pairs when word lexical similarity is used as an information source where context is important: lexical disambiguation (Miller et al., 2012), topic segmentation (Guinaudeau et al., 2012). This can also be a preprocessing step when looking for similarities at higher levels, for instance at the sentence level (Mihalcea et al., 2006) or other macrotextual level (Agirre et al., 2013), since these are always aggregation functions of word similarities. There are limits to what is presented here: we need to evaluate the importance of the level of noise in the distributional neighbours database, or at least the quantity of non-semantic relations present, and this depends on the way the database is built. Our starting corpus is relatively </context>
</contexts>
<marker>Miller, Biemann, Zesch, Gurevych, 2012</marker>
<rawString>Tristan Miller, Chris Biemann, Torsten Zesch, and Iryna Gurevych. 2012. Using distributional similarity for lexical expansion in knowledge-based word sense disambiguation. In Proceedings of COLING 2012, pages 1781–1796, Mumbai, India, December. The COLING 2012 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Morris</author>
<author>G Hirst</author>
</authors>
<title>Non-classical lexical semantic relations.</title>
<date>2004</date>
<booktitle>In Proceedings of the HLT Workshop on Computational Lexical Semantics,</booktitle>
<pages>46--51</pages>
<location>Boston.</location>
<contexts>
<context position="4340" citStr="Morris and Hirst, 2004" startWordPosition="668" endWordPosition="671">ies is to exhibit a lot of different semantic relations, not necessarily standard lexical relations. Even with respect to established lexical resources, distributional approaches may improve coverage, complicating the evaluation even more. The method we propose here has been designed as an intrinsic evaluation with a view to validate semantic proximity links in a broad per479 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 479–488, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics spective, to cover what (Morris and Hirst, 2004) call “non classical lexical semantic relations”. For instance, agentive relations (author/publish, author/publication) or associative relations (actor/cinema) should be considered. At the same time, we want to filter associations that can be considered as accidental in a semantic perspective (e.g. flag and composer are similar because they appear a lot with nationality names). We do this by judging the relevance of a lexical relation in a context where both elements of a lexical pair occur. We show not only that this improves the reliability of human judgments, but also that it gives a framew</context>
</contexts>
<marker>Morris, Hirst, 2004</marker>
<rawString>J. Morris and G. Hirst. 2004. Non-classical lexical semantic relations. In Proceedings of the HLT Workshop on Computational Lexical Semantics, pages 46–51, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pado</author>
<author>Mirella Lapata</author>
</authors>
<title>Dependency-based construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="3098" citStr="Pado and Lapata, 2007" startWordPosition="481" endWordPosition="485">s (Jones, 1994), and this applies to distributional thesauri (Curran, 2004; Poibeau and Messiant, 2008). Extrinsic evaluations measure the capacity of a system in which a resource or a component to evaluate has been used, for instance in this case information retrieval (van der Plas, 2008) or word sense disambiguation (Weeds and Weir, 2005). Intrinsic evaluations try to measure the resource itself with respect to some human standard or judgment, for instance by comparing a distributional resource with respect to an existing synonym dictionary or similarity judgment produced by human subjects (Pado and Lapata, 2007; Baroni and Lenci, 2010). The shortcomings of these methods have been underlined in (Baroni and Lenci, 2011). Lexical resources designed for other objectives put the spotlight on specific areas of the distributional thesaurus. They are not suitable for the evaluation of the whole range of semantic relatedness that is exhibited by distributional similarities, which exceeds the limits of classical lexical relations, even though researchers have tried to collect equivalent resources manually, to be used as a gold standard (Weeds, 2003; Bordag, 2008; Anguiano et al., 2011). One advantage of distr</context>
<context position="29249" citStr="Pado and Lapata, 2007" startWordPosition="4818" endWordPosition="4821">hodological reflection (Sahlgren, 2006), and as we said in the introduction, evaluations can be divided between extrinsic and intrinsic evaluations. In extrinsic evaluations, models are evaluated against benchmarks focusing on a single task or a single aspect of a resource: either discriminative, TOEFL-like tests (Freitag et al., 2005), analogy production (Turney, 2008), or synonym selection (Weeds, 2003; Anguiano et al., 2011; Ferret, 2013; Curran and Moens, 2002). In intrinsic evaluations, associations norms are used, such as the 353 word-similarity dataset (Finkelstein et al., 2002), e.g. (Pado and Lapata, 2007; Agirre et al., 2009), or specifically designed test cases, as in (Baroni and Lenci, 2011). We differ from all these evaluation procedures as we do not focus on an essential view of the relatedness of two lexical items, but evaluate the link in a context where the relevance of the link is in question, an “existential” view of semantic relatedness. As for improving distributional thesauri, outside of numerous alternate approaches to the construction, there is a body of work focusing on improving an existing resource, for instance reweighting context features once an initial thesaurus is built </context>
</contexts>
<marker>Pado, Lapata, 2007</marker>
<rawString>Sebastian Pado and Mirella Lapata. 2007. Dependency-based construction of semantic space models. Computational Linguistics, 33(2):161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thierry Poibeau</author>
<author>C´edric Messiant</author>
</authors>
<title>Do we still Need Gold Standards for Evaluation?</title>
<date>2008</date>
<booktitle>In Proceedings of the Language Resource and Evaluation Conference.</booktitle>
<contexts>
<context position="2580" citStr="Poibeau and Messiant, 2008" startWordPosition="395" endWordPosition="398">items occur. This way of building a semantic network has been very popular since (Grefenstette, 1994; Lin, 1998), even though the nature of the information it contains is hard to define, and its evaluation is far from obvious. A distributional thesaurus includes a lot of “noise” from a semantic point of view, but also lists relevant lexical pairs that escape classical lexical relations such as synonymy or hypernymy. There is a classical dichotomy when evaluating NLP components between extrinsic and intrinsic evaluations (Jones, 1994), and this applies to distributional thesauri (Curran, 2004; Poibeau and Messiant, 2008). Extrinsic evaluations measure the capacity of a system in which a resource or a component to evaluate has been used, for instance in this case information retrieval (van der Plas, 2008) or word sense disambiguation (Weeds and Weir, 2005). Intrinsic evaluations try to measure the resource itself with respect to some human standard or judgment, for instance by comparing a distributional resource with respect to an existing synonym dictionary or similarity judgment produced by human subjects (Pado and Lapata, 2007; Baroni and Lenci, 2010). The shortcomings of these methods have been underlined </context>
</contexts>
<marker>Poibeau, Messiant, 2008</marker>
<rawString>Thierry Poibeau and C´edric Messiant. 2008. Do we still Need Gold Standards for Evaluation? In Proceedings of the Language Resource and Evaluation Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Sahlgren</author>
</authors>
<title>Towards pertinent evaluation methodologies for word-space models. In</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation.</booktitle>
<contexts>
<context position="28667" citStr="Sahlgren, 2006" startWordPosition="4728" endWordPosition="4729">evant class. CI is the confidence interval on the F-score (RF = Random Forest, NB= naive bayes). Table 4: Impact of each group of features on the best scores (%) : the lowest the results, the bigger the impact of the removed group of features. Features Prec. Recall F-score all all − corpus feat. all − similarity feat. all − contextual feat. 40.4 54.3 46.3 37.4 52.8 43.8 36.1 49.5 41.8 36.5 54.8 43.8 4 Related work Our work is related to two issues: evaluating distributional resources, and improving them. Evaluating distributional resources is the subject of a lot of methodological reflection (Sahlgren, 2006), and as we said in the introduction, evaluations can be divided between extrinsic and intrinsic evaluations. In extrinsic evaluations, models are evaluated against benchmarks focusing on a single task or a single aspect of a resource: either discriminative, TOEFL-like tests (Freitag et al., 2005), analogy production (Turney, 2008), or synonym selection (Weeds, 2003; Anguiano et al., 2011; Ferret, 2013; Curran and Moens, 2002). In intrinsic evaluations, associations norms are used, such as the 353 word-similarity dataset (Finkelstein et al., 2002), e.g. (Pado and Lapata, 2007; Agirre et al., 2</context>
</contexts>
<marker>Sahlgren, 2006</marker>
<rawString>Magnus Sahlgren. 2006. Towards pertinent evaluation methodologies for word-space models. In In Proceedings of the 5th International Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>C S Yang</author>
<author>C T Yu</author>
</authors>
<title>A theory of term importance in automatic text analysis.</title>
<date>1975</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>26</volume>
<issue>1</issue>
<contexts>
<context position="17992" citStr="Salton et al., 1975" startWordPosition="2940" endWordPosition="2943">anks. We add two categorial features, of a more linguistic nature: • cats is the pair of part-of-speech for the related items, e.g. to distinguish the relevance of NN or VV pairs. • predarg is related to the predicate/argument distinction: are the related items predicates or arguments ? The last set of features derive from the occurrences of related tokens in the considered discourses: First, we take into account the frequencies of items within the text, with three features as before: the min of the frequencies of the two related items, the max, and the log-product. Then we consider a tf·idf (Salton et al., 1975) measure, to evaluate the specificity and arguably the importance of a word Feature Description freqmin min(freqa, freqb) freqmax max(freqa, freqb) freqx log(freqa x freqb) im im =log P(a,b) P (a)�P (b) lin Lin’s score rankmin min(ranka−b, rankb−a) rankmax max(ranka−b, rankb−a) rankx log(ranka−b x rankb−a) prodmin min(proda, prodb) prodmax max(proda, prodb) prodx log(proda x prodb) cats neighbour pos pair predarg predicate or argument freqtxtmin min(freqtxta, freqtxtb) freqtxtmax max(freqtxta, freqtxtb) freqtxtx log(freqtxta x freqstxtb) tf·ipf tf·ipf (neighboura)xtf·ipf (neighbourb) coprph co</context>
</contexts>
<marker>Salton, Yang, Yu, 1975</marker>
<rawString>G. Salton, C. S. Yang, and C. T. Yu. 1975. A theory of term importance in automatic text analysis. Journal of the American Society for Information Science, 26(1):33–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>A uniform approach to analogies, synonyms, antonyms, and associations.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics - Volume 1, COLING ’08,</booktitle>
<pages>905--912</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="29000" citStr="Turney, 2008" startWordPosition="4780" endWordPosition="4781">l feat. 40.4 54.3 46.3 37.4 52.8 43.8 36.1 49.5 41.8 36.5 54.8 43.8 4 Related work Our work is related to two issues: evaluating distributional resources, and improving them. Evaluating distributional resources is the subject of a lot of methodological reflection (Sahlgren, 2006), and as we said in the introduction, evaluations can be divided between extrinsic and intrinsic evaluations. In extrinsic evaluations, models are evaluated against benchmarks focusing on a single task or a single aspect of a resource: either discriminative, TOEFL-like tests (Freitag et al., 2005), analogy production (Turney, 2008), or synonym selection (Weeds, 2003; Anguiano et al., 2011; Ferret, 2013; Curran and Moens, 2002). In intrinsic evaluations, associations norms are used, such as the 353 word-similarity dataset (Finkelstein et al., 2002), e.g. (Pado and Lapata, 2007; Agirre et al., 2009), or specifically designed test cases, as in (Baroni and Lenci, 2011). We differ from all these evaluation procedures as we do not focus on an essential view of the relatedness of two lexical items, but evaluate the link in a context where the relevance of the link is in question, an “existential” view of semantic relatedness. </context>
</contexts>
<marker>Turney, 2008</marker>
<rawString>Peter D. Turney. 2008. A uniform approach to analogies, synonyms, antonyms, and associations. In Proceedings of the 22nd International Conference on Computational Linguistics - Volume 1, COLING ’08, pages 905–912, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L van der Plas</author>
</authors>
<title>Automatic Lexico-Semantic Acquisition for Question Answering.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Groningen.</institution>
<marker>van der Plas, 2008</marker>
<rawString>L. van der Plas. 2008. Automatic Lexico-Semantic Acquisition for Question Answering. Ph.D. thesis, University of Groningen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Weeds</author>
<author>D Weir</author>
</authors>
<title>Co-occurrence retrieval: A flexible framework for lexical distributional similarity.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>4</issue>
<contexts>
<context position="2819" citStr="Weeds and Weir, 2005" startWordPosition="436" endWordPosition="439">hesaurus includes a lot of “noise” from a semantic point of view, but also lists relevant lexical pairs that escape classical lexical relations such as synonymy or hypernymy. There is a classical dichotomy when evaluating NLP components between extrinsic and intrinsic evaluations (Jones, 1994), and this applies to distributional thesauri (Curran, 2004; Poibeau and Messiant, 2008). Extrinsic evaluations measure the capacity of a system in which a resource or a component to evaluate has been used, for instance in this case information retrieval (van der Plas, 2008) or word sense disambiguation (Weeds and Weir, 2005). Intrinsic evaluations try to measure the resource itself with respect to some human standard or judgment, for instance by comparing a distributional resource with respect to an existing synonym dictionary or similarity judgment produced by human subjects (Pado and Lapata, 2007; Baroni and Lenci, 2010). The shortcomings of these methods have been underlined in (Baroni and Lenci, 2011). Lexical resources designed for other objectives put the spotlight on specific areas of the distributional thesaurus. They are not suitable for the evaluation of the whole range of semantic relatedness that is e</context>
</contexts>
<marker>Weeds, Weir, 2005</marker>
<rawString>J. Weeds and D. Weir. 2005. Co-occurrence retrieval: A flexible framework for lexical distributional similarity. Computational Linguistics, 31(4):439–475.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Elizabeth Weeds</author>
</authors>
<date>2003</date>
<booktitle>Measures and Applications of Lexical Distributional Similarity. Ph.D. thesis,</booktitle>
<institution>University of Sussex.</institution>
<contexts>
<context position="3636" citStr="Weeds, 2003" startWordPosition="567" endWordPosition="568"> or similarity judgment produced by human subjects (Pado and Lapata, 2007; Baroni and Lenci, 2010). The shortcomings of these methods have been underlined in (Baroni and Lenci, 2011). Lexical resources designed for other objectives put the spotlight on specific areas of the distributional thesaurus. They are not suitable for the evaluation of the whole range of semantic relatedness that is exhibited by distributional similarities, which exceeds the limits of classical lexical relations, even though researchers have tried to collect equivalent resources manually, to be used as a gold standard (Weeds, 2003; Bordag, 2008; Anguiano et al., 2011). One advantage of distributional similarities is to exhibit a lot of different semantic relations, not necessarily standard lexical relations. Even with respect to established lexical resources, distributional approaches may improve coverage, complicating the evaluation even more. The method we propose here has been designed as an intrinsic evaluation with a view to validate semantic proximity links in a broad per479 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 479–488, Baltimore, Maryland, USA, June 23-25</context>
<context position="29035" citStr="Weeds, 2003" startWordPosition="4786" endWordPosition="4787"> 36.1 49.5 41.8 36.5 54.8 43.8 4 Related work Our work is related to two issues: evaluating distributional resources, and improving them. Evaluating distributional resources is the subject of a lot of methodological reflection (Sahlgren, 2006), and as we said in the introduction, evaluations can be divided between extrinsic and intrinsic evaluations. In extrinsic evaluations, models are evaluated against benchmarks focusing on a single task or a single aspect of a resource: either discriminative, TOEFL-like tests (Freitag et al., 2005), analogy production (Turney, 2008), or synonym selection (Weeds, 2003; Anguiano et al., 2011; Ferret, 2013; Curran and Moens, 2002). In intrinsic evaluations, associations norms are used, such as the 353 word-similarity dataset (Finkelstein et al., 2002), e.g. (Pado and Lapata, 2007; Agirre et al., 2009), or specifically designed test cases, as in (Baroni and Lenci, 2011). We differ from all these evaluation procedures as we do not focus on an essential view of the relatedness of two lexical items, but evaluate the link in a context where the relevance of the link is in question, an “existential” view of semantic relatedness. As for improving distributional the</context>
</contexts>
<marker>Weeds, 2003</marker>
<rawString>Julie Elizabeth Weeds. 2003. Measures and Applications of Lexical Distributional Similarity. Ph.D. thesis, University of Sussex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maayan Zhitomirsky-Geffet</author>
<author>Ido Dagan</author>
</authors>
<title>Bootstrapping distributional feature vector quality.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>3</issue>
<contexts>
<context position="29885" citStr="Zhitomirsky-Geffet and Dagan, 2009" startWordPosition="4920" endWordPosition="4923"> Agirre et al., 2009), or specifically designed test cases, as in (Baroni and Lenci, 2011). We differ from all these evaluation procedures as we do not focus on an essential view of the relatedness of two lexical items, but evaluate the link in a context where the relevance of the link is in question, an “existential” view of semantic relatedness. As for improving distributional thesauri, outside of numerous alternate approaches to the construction, there is a body of work focusing on improving an existing resource, for instance reweighting context features once an initial thesaurus is built (Zhitomirsky-Geffet and Dagan, 2009), or post-processing the resource to filter bad neighbours or re-ranking neighbours of a given target (Ferret, 2013). They still use “essential” evaluation measures (mostly synonym extraction), although the latter comes close to our work since it also trains a model to detect (intrinsically) bad neighbours by using example sentences with the words to discriminate. We are not aware of any work that would try to evaluate differently semantic neighbours according to the context they appear in. 5 Conclusion We proposed a method to reliably evaluate distributional semantic similarity in a broad sen</context>
</contexts>
<marker>Zhitomirsky-Geffet, Dagan, 2009</marker>
<rawString>Maayan Zhitomirsky-Geffet and Ido Dagan. 2009. Bootstrapping distributional feature vector quality. Computational Linguistics, 35(3):435–461.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>