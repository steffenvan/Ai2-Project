<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.062345">
<title confidence="0.9552815">
A Multi-layer Chinese Word Segmentation System Optimized for
Out-of-domain Tasks
</title>
<author confidence="0.99348">
Qin Gao Stephan Vogel
</author>
<affiliation confidence="0.9933495">
Language Technologies Institute Language Technologies Institute
Carnegie Mellon University Carnegie Mellon University
</affiliation>
<email confidence="0.996766">
qing@cs.cmu.edu stephan.vogel@cs.cmu.edu
</email>
<sectionHeader confidence="0.994738" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999916888888889">
State-of-the-art Chinese word segmenta-
tion systems have achieved high perfor-
mance when training data and testing data
are from the same domain. However, they
suffer from the generalizability problem
when applied on test data from different
domains. We introduce a multi-layer Chi-
nese word segmentation system which can
integrate the outputs from multiple hetero-
geneous segmentation systems. By train-
ing a second layer of large margin clas-
sifier on top of the outputs from several
Conditional Random Fields classifiers, it
can utilize a small amount of in-domain
training data to improve the performance.
Experimental results show consistent im-
provement on F1 scores and OOV recall
rates by applying the approach.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999921148936171">
The Chinese word segmentation problem has been
intensively investigated in the past two decades.
From lexicon-based methods such as Bi-Directed
Maximum Match (BDMM) (Chen et al., 2005) to
statistical models such as Hidden Markove Model
(HMM) (Zhang et al., 2003), a broad spectrum
of approaches have been experimented. By cast-
ing the problem as a character labeling task, se-
quence labeling models such as Conditional Ran-
dom Fields can be applied on the problem (Xue
and Shen, 2003). State-of-the-art CRF-based sys-
tems have achieved good performance. However,
like many machine learning problems, generaliz-
ability is crucial for a domain-independent seg-
mentation system. Because the training data usu-
ally come from limited domains, when the domain
of test data is different from the training data, the
results are still not satisfactory.
A straight-forward solution is to obtain more la-
beled data in the domain we want to test. However
this is not easily achievable because the amount
of data needed to train a segmentation system are
large. In this paper, we focus on improving the
system performance by using a relatively small
amount of manually labeled in-domain data to-
gether with larger out-of-domain corpus1. The
effect of mingling the small in-domain data into
large out-of-domain data may be neglectable due
to the difference in data size. Hence, we try to
explore an alternative way that put a second layer
of classifier on top of the segmentation systems
built on out-of-domain corpus (we will call them
sub-systems). The classifier should be able to uti-
lize the information from the sub-systems and op-
timize the performance with a small amount of in-
domain data.
The basic idea of our method is to integrate
a number of different sub-systems whose per-
formance varies on the new domain. Figure 1
demonstrates the system architecture. There are
two layers in the system. In the lower layer,
the out-of-domain corpora are used, together with
other resources to produce heterogeneous sub-
systems. In the second layer the outputs of the
sub-systems in the first layer are treated as input
to the classifier. We train the classifier with small
in-domain data. All the sub-systems should have
</bodyText>
<footnote confidence="0.9773114">
1From this point, we use the term out-of-domain corpus
to refer to the general and large training data that are not
related to the test domain, and the term in-domain corpus
to refer to small amount of data that comes from the same
domain of the test data
</footnote>
<bodyText confidence="0.9707892">
reasonable performance on all domains, but their
performance on different domains may vary. The
job of the second layer is to find the best decision
boundary on the target domain, in presence of all
the decisions made by the sub-systems.
</bodyText>
<figureCaption confidence="0.986763">
Figure 1: The architecture of the system, the first
layer (sub-systems) is trained on general out-of-
domain corpus and various resources, while the
second layer of the classifier is trained on in-
domain corpus.
</figureCaption>
<bodyText confidence="0.998107142857143">
Conditional Random Fields (CRF) (Lafferty et
al., 2001) has been applied on Chinese word seg-
mentation and achieved high performance. How-
ever, because of its conditional nature the small
amount of in-domain corpus will not significantly
change the distributions of the model parame-
ters trained on out-of-domain corpus, it is more
suitable to be used in the sub-systems than in
the second-layer classifier. Large margin models
such as Support Vector Machine (SVM) (Vapnik,
1995) can be trained on small corpus and gener-
alize well. Therefore we chose to use CRF in
building sub-systems and SVM in building the
second-layer. We built multiple CRF-based Chi-
nese word segmentation systems using different
features, and then use the marginal probability of
each tag of all the systems as features in SVM.
The SVM is then trained on small in-domain cor-
pus, results in a decision hyperplane that mini-
mizes the loss in the small training data. To in-
tegrate the dependencies of output tags, we use
SVM-HMM (Altun et al., 2003) to capture the in-
teractions between tags and features. By apply-
ing SVM-HMM we can bias our decision towards
most informative CRF-based system w.r.t. the tar-
get domain. Our methodology is similar to (Co-
hen and Carvalho, 2005), who applied a cross-
validation-like method to train sequential stacking
models, while we directly use small amount of in-
domain data to train the second-layer classifiers.
The paper is organized as follows, first we will
discuss the CRF-based sub-systems we used in
section 2, and then the SVM-based system com-
bination method in section 3. Finally, in section 4
the experimental results are presented.
</bodyText>
<sectionHeader confidence="0.710778" genericHeader="method">
2 CRF-based sub-systems
</sectionHeader>
<bodyText confidence="0.9999515">
In this section we describe the sub-systems we
used in system. All of the sub-systems are based
on CRF with different features. The tag set we
use is the 6-tag (B1, B2, B3, M, E, S) set pro-
posed by Zhao et al (2006). All of the sub-systems
use the same tag set, however as we will see later,
the second-layer classifier in our system does not
require the sub-systems to have a common tag
set. Also, all of the sub-systems include a com-
mon set of character features proposed in (Zhao
and Kit, 2008). The offsets and concatenations
of the six n-gram features (the feature template)
are: C−1, C0, C1, C−1C0, C0C1, C−1C1. In the
remaining part of the section we will introduce
other features that we employed in different sub-
systems.
</bodyText>
<subsectionHeader confidence="0.996117">
2.1 Character type features
</subsectionHeader>
<bodyText confidence="0.999811363636364">
By simply classify the characters into four types:
Punctuation (P), Digits (D), Roman Letters (L)
and Chinese characters (C), we can assign char-
acter type tags to every character. The idea is
straight-forward. We denote the feature as CTF.
Similar to character feature, we also use differ-
ent offsets and concatenations for character type
features. The feature template is identical to
character feature, i.e. CTF−1, CTF0, CTF1,
CTF−1CTF0, CTF0CTF1, CTF−1CTF1 are
used as features in CRF training.
</bodyText>
<figure confidence="0.998876">
Training data
(out-of-
domain)
Number
Tag Feature
Character
Type Feature
Word list 1
Word list 2
Word list 2
Entropy
Feature
Classifier 1
Classifier 2
Classifier 3
Classifier 4
Classifier 5
Training data
(in-domain)
Integrated
classifier
</figure>
<subsectionHeader confidence="0.786564">
2.2 Number tag feature
</subsectionHeader>
<bodyText confidence="0.999378375">
Numbers take a large portion of the OOV words,
which can easily be detected by regular expres-
sions or Finite State Automata. However there
are often ambiguities on the boundary of numbers.
Therefore, instead of using detected numbers as
final answers, we use them as features. The num-
ber detector we developed finds the longest sub-
strings in a sentence that are:
</bodyText>
<listItem confidence="0.999969666666667">
• Chinese Numbers (N)
• Chinese Ordinals (O)
• Chinese Dates (D)
</listItem>
<bodyText confidence="0.997365">
For each character of the detected num-
bers/ordinal/date, we assign a tag that reflects the
position of the character in the detected num-
ber/ordinal/date. We adopt the four-tag set (B, M,
E, S). The position tags are appended to end of
the number/ordinal/date tags to form the number
tag feature of that character. I.e. there are totally
13 possible values for the number tag feature, as
listed in Table 1.2
</bodyText>
<table confidence="0.9988778">
Number Ordinal Date Other
Begin NB OB DB XX
Middle NM OM DM
End NE OE DE
Single NS O5* D5*
</table>
<tableCaption confidence="0.998924">
Table 1: The feature values used in the number tag
</tableCaption>
<bodyText confidence="0.969518857142857">
feature, note that OS and DS are never observed
because there is no single character ordinal/date
by our definition.
Similar to character feature and character type
feature, the feature template mention before is
also applied on the number tag feature. We de-
note the number tag features as NF.
</bodyText>
<subsectionHeader confidence="0.995357">
2.3 Conditional Entropy Feature
</subsectionHeader>
<bodyText confidence="0.988387714285714">
We define the Forward Conditional Entropy of
a character C by the entropy of all the charac-
ters that follow C in a given corpus, and the
Backward Conditional Entropy as the entropy
of all the characters that precede C in a given
corpus. The conditional entropy can be com-
puted easily from a character bigram list gener-
ated from the corpus. Assume we have a bigram
2Two of the tags, OS and DS are never observed.
list B = {B1, B2, · · · , BN}, where every bigram
entry Bk = {cik, cjk, nk} is a triplet of the two
consecutive characters cik and cjk and the count of
the bigram in the corpus, nk. The Forward Condi-
tional Entropy of the character C is defined by:
</bodyText>
<equation confidence="0.977678">
�
Hf(C) :=
cik =C
</equation>
<bodyText confidence="0.996399857142857">
where Z = Ecik=C nk is the normalization fac-
tor.
And the Backward Conditional Entropy can be
computed similarly.
We assign labels to every character based on
the conditional entropy of it. If the conditional
entropy value is less than 1.0, we assign fea-
ture value 0 to the character, and for region
[1.0, 2.0), we assign feature value 1. Similarly we
define the region-to-value mappings as follows:
[2.0, 3.5) 2, [3.5, 5.0) 4, [5.0, 7.0) 5,
[7.0, +oo) 6. The forward and backward con-
ditional entropy forms two features. We will refer
to these features as EF.
</bodyText>
<subsectionHeader confidence="0.993935">
2.4 Lexical Features
</subsectionHeader>
<bodyText confidence="0.999981888888889">
Lexical features are the most important features to
make sub-systems output different results on dif-
ferent domains. We adopt the definition of the fea-
tures partially from (Shi and Wang, 2007). In our
system we use only the Lbegin(C0) and Lend(C0)
features, omitting the LmidC0 feature. The two
features represent the maximum length of words
found in the lexicon that contain the current char-
acter as the first or last character, correspondingly.
For feature values equal or greater than 6, we
group them into one value.
Although we can find a number of Chinese lex-
icons available, they may or may not be gener-
ated according to the same standard as the train-
ing data. Concatenating them into one may bring
in noise and undermine the performance. There-
fore, every lexicon will generate its own lexical
features.
</bodyText>
<sectionHeader confidence="0.958767" genericHeader="method">
3 SVM-based System Combination
</sectionHeader>
<bodyText confidence="0.99984175">
Generalization is a fundamental problem of Chi-
nese word segmentation. Since the training data
may come from different domains than the test
data, the vocabulary and the distribution can also
</bodyText>
<equation confidence="0.976199">
nk nk
Z log
Z
</equation>
<bodyText confidence="0.999914729166667">
be different. Ideally, if we can have labeled data
from the same domain, we can train segmenters
specific to the domain. However obtaining suffi-
cient amount of labeled data in the target domain
is time-consuming and expensive. In the mean
time, if we only label a small amount of data in the
target domain and put them into the training data,
the effect may be too small because the size of
out-of-domain data can overwhelm the in-domain
data.
In this paper we propose a different way of
utilizing small amount of in-domain corpus. We
put a second-layer classifier on top of the CRF-
based sub-systems, the output of CRF-based sub-
systems are treated as features in an SVM-HMM
(Altun et al., 2003) classifier. We can train the
SVM-HMM classifier on a small amount of in-
domain data. The training procedure can be
viewed as finding the optimal decision boundary
that minimize the hinge loss on the in-domain
data. Because the number of features for SVM-
HMM is significantly smaller than CRF, we can
train the model with as few as several hundred
sentences.
Similar to CRF, the SVM-HMM classifier still
treats the Chinese word segmentation problem as
character tagging. However, because of the limi-
tation of training data size, we try to minimize the
number of classes. We chose to adopt the two-tag
set, i.e. class 1 indicates the character is the end of
a word and class 2 means otherwise. Also, due to
limited amount of training data, we do not use any
character features, instead, the features comes di-
rectly from the output of sub-systems. The SVM-
HMM can use any real value features, which en-
ables integration of a wide range of segmenters.
In this paper we use only the CRF-based seg-
menters, and the features are the marginal prob-
abilities (Sutton and McCallum, 2006) of all the
tags in the tag set for each character. As an ex-
ample, for a CRF-based sub-system that outputs
six tags, it will output six features for each char-
acter for the SVM-HMM classifier, corresponding
to the marginal probability of the character given
the CRF model. The marginal probabilities for
the same tag (e.g. B1, S, etc) come from differ-
ent CRF-based sub-systems are treated as distinct
features.
</bodyText>
<table confidence="0.893219222222222">
Features Lexicons
S1 CF, CTF None
S2 CF, NF ADSO, CTB6
S3 CF, CTF, NF ADSO
S4 CF, CTF, NF, EF ADSO, CTB6
S5 CF, EF None
S6 CF, NF None
S7 CF, CTF ADSO
S8 CF, CTF CTB6
</table>
<tableCaption confidence="0.993111">
Table 2: The configurations of CRF-based sub-
</tableCaption>
<bodyText confidence="0.988501888888889">
systems. S1 to S4 are used in the final submission
of the Bake-off, S5 through S8 are also presented
to show the effects of individual features.
When we encounter data from a new domain,
we first use one of the CRF-based sub-system to
segment a portion of the data, and manually cor-
rect obvious segmentation errors. The manually
labeled data are then processed by all the CRF-
based sub-systems, so as to obtain features of ev-
ery character. After that, we train the SVM-HMM
model using these features.
During decoding, the Chinese input will also be
processed by all of the CRF-based sub-systems,
and the outputs will be fed into the SVM-HMM
classifier. The final decisions of word boundaries
are based solely on the classified labels of SVM-
HMM model.
For the Bake-off system, we labeled two hun-
dred sentences in each of the unsegmented train-
ing set (A and B). Since only one submission is
allowed, the SVM-HMM model of the final sys-
tem was trained on the concatenation of the two
training sets, i.e. four hundred sentences.
The CRF-based sub-systems are trained using
CRF++ toolkit (Kudo, 2003), and the SVM-HMM
trained by the SVM&amp;quot;&amp;quot;&apos; toolkit (Joachims et al.,
2009).
</bodyText>
<sectionHeader confidence="0.999073" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.996660571428571">
To evaluate the effectiveness of the proposed sys-
tem combination method, we performed two ex-
periments. First, we evaluate the system combina-
tion method on provided training data in the way
that is similar to cross-validation. Second, we ex-
perimented with training the SVM-HMM model
with the manually labeled data come from cor-
</bodyText>
<table confidence="0.999726727272727">
Micro-Average Macro-Average
P R F1 OOV-R P R F1 OOV-R
S1 0.962 0.960 0.961 0.722 0.962 0.960 0.960 0.720
S2 0.965 0.966 0.966 0.725 0.965 0.966 0.966 0.723
S3 0.966 0.967 0.967 0.731 0.966 0.967 0.967 0.729
S4 0.968 0.969 0.968 0.731 0.967 0.969 0.969 0.729
S5 0.962 0.960 0.961 0.720 0.962 0.960 0.960 0.718
S6 0.963 0.961 0.962 0.730 0.963 0.961 0.961 0.729
S7 0.966 0.967 0.966 0.723 0.966 0.967 0.967 0.720
S8 0.963 0.960 0.962 0.727 0.963 0.960 0.960 0.726
CB 0.969 0.969 0.969 0.741 0.969 0.969 0.969 0.739
</table>
<tableCaption confidence="0.99080475">
Table 3: The performance of individual sub-systems and combined system. The Micro-Average results
come from concatenating all the outputs of the ten-fold systems and then compute the scores, and the
Macro-Average results are calculated by first compute the scores in every of the ten-fold systems and
then average the scores.
</tableCaption>
<table confidence="0.999835333333333">
Set A Set B
P R F1 OOV-R P R F1 OOV-R
S1 0.925 0.920 0.923 0.625 0.936 0.938 0.937 0.805
S2 0.934 0.934 0.934 0.641 0.941 0.930 0.935 0.751
S3 0.940 0.937 0.938 0.677 0.938 0.926 0.932 0.752
S4 0.942 0.940 0.941 0.688 0.944 0.929 0.936 0.776
CB1 0.943 0.941 0.942 0.688 0.948 0.936 0.942 0.794
CB2 0.941 0.940 0.941 0.692 0.939 0.949 0.944 0.821
CB3 0.943 0.939 0.941 0.699 0.950 0.950 0.950 0.820
</table>
<tableCaption confidence="0.9069765">
Table 4: The performance of individual systems and system combination on Bake-off test data, CB1,
CB2, and CB3 are system combination trained on labeled data from domain A, B, and the concatenation
</tableCaption>
<bodyText confidence="0.993897016949153">
of the data from both domains.
responding domains, and tested the resulting sys-
tems on the Bake-off test data.
For experiment 1, We divide the training set
into 11 segments, segment 0 through 9 contains
1733 sentences, and segment 10 has 1724 sen-
tence. We perform 10-fold cross-validation on
segment 0 to 9. Every time we pick one segment
from segment 0 to 9 as test set and the remain-
ing 9 segments are used to train CRF-based sub-
systems. Segment 10 is used as the training set for
SVM-HMM model. The sub-systems we used is
listed in Table 2.
In Table 3 we provide the micro-level and
macro-level average of performance the ten-fold
evaluation, including both the combined system
and all the individual sub-systems. Because
the system combination uses more data than its
sub-systems (segment 10), in order to have a
fair comparison, when evaluating individual sub-
systems, segment 10 is appended to the training
data of CRF model. Therefore, the individual sub-
systems and system combination have exactly the
same set of training data.
As we can see in the results in Table 3, the sys-
tem combination method (Row CB) has improve-
ment over the best sub-system (S4) on both F1
and OOV recall rate, and the OOV recall rate im-
proved by 1%. We should notice that in this exper-
iment we actually did not deal with any data from
different domains, the advantage of the proposed
method is therefore not prominent.
We continue to present the experiment results
of the second experiment. In the experiment
we labeled 200 sentences from each of the unla-
beled bake-off training set A and B, and trained
the SVM-HMM model on the labeled data. We
compare the performance of the four sub-systems
and the performance of the system combination
method trained on: 1) 200 sentences from A, 2)
200 sentences from B, and 3) the concatenation
of the 400 sentences from both A and B. We show
the scores on the bake-off test set A and B in Table
4.
As we can see from the results in Table 4, the
system combination method outperforms all the
individual systems, and the best performance is
observed when using both of the labeled data from
domain A and B, which indicates the potential of
further improvement by increasing the amount of
in-domain training data. Also, the individual sub-
systems with the best performance on the two do-
mains are different. System 1 performs well on
Set B but not on Set A, so does System 4, which
tops on Set A but not as good as System 1 on Set
B. The system combination results appear to be
much more stable on the two domains, which is a
preferable characteristic if the segmentation sys-
tem needs to deal with data from various domains.
</bodyText>
<sectionHeader confidence="0.998929" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9996388125">
In this paper we discussed a system combina-
tion method based on SVM-HMM for the Chinese
word segmentation problem. The method can uti-
lize small amount of training data in target do-
mains to improve the performance over individ-
ual sub-systems trained on data from different do-
mains. Experimental results show that the method
is effective in improving the performance with a
small amount of in-domain training data.
Future work includes adding more heteroge-
neous sub-systems other than CRF-based ones
into the system and investigate the effects on the
performance. Automatic domain adaptation for
Chinese word segmentation can also be an out-
come of the method, which may be an interesting
research topic in the future.
</bodyText>
<sectionHeader confidence="0.998926" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999715490566038">
Altun, Yasemin, Ioannis Tsochantaridis, and Thomas
Hofmann. 2003. Hidden markov support vector
machines. In Proceedings of International Confer-
ence on Machine Learning (ICML).
Chen, Yaodong, Ting Wang, and Huowang Chen.
2005. Using directed graph based bdmm algorithm
for chinese word segmentation. pages 214–217.
Cohen, William W. and Vitor Carvalho. 2005. Stacked
sequential learning. In Proceedings of the Inter-
national Joint Conference on Artificial Intelligence
(IJ-CAI).
Joachims, Thorsten, Thomas Finley, and Chun-
Nam John Yu. 2009. Cutting-plane training of
structural svms. Machine Learning, 77(1):27–59.
Kudo, Taku. 2003. CRF++: Yet another crf toolkit.
Web page: http://crfpp.sourceforge.net/.
Lafferty, John, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of International Con-
ference on Machine Learning (ICML).
Shi, Yanxin and Mengqiu Wang. 2007. A dual-layer
crfs based joint decoding method for cascaded seg-
mentation and labeling tasks. In Proceedings of the
International Joint Conference on Artificial Intelli-
gence (IJ-CAI).
Sutton, Charles and Andrew McCallum, 2006. Intro-
duction to Statistical Relational Learning, chapter
An Introduction to Conditional Random Fields for
Relational Learning. MIT Press.
Vapnik, Vladimir N. 1995. The Nature of Statistical
Learning Theory. Springer.
Xue, Nianwen and Libin Shen. 2003. Chinese word
segmentation as lmr tagging. In Proceedings of
the second SIGHAN workshop on Chinese language
processing, pages 176–179.
Zhang, Huaping, Qun Liu, Xueqi Cheng, Hao Zhang,
and Hongkui Yu. 2003. Chinese lexical analysis us-
ing hierarchical hidden markov model. In Proceed-
ings of the second SIGHAN workshop on Chinese
language processing, pages 63–70.
Zhao, Hai and Chunyu Kit. 2008. Unsupervised
segmentation helps supervised learning of charac-
ter tagging for word segmentation and named entity
recognition. In The Sixth SIGHAN Workshop on
Chinese Language Processing (SIGHAN-6), pages
106–111.
Zhao, Hai, Chang-Ning Huang, Mu Li, and Bao-Liang
Lu. 2006. Effective tag set selection in chinese
word segmentation via conditional random field
modeling. In Proceedings of the 20th Pacific Asia
Conference on Language, Information and Compu-
tation (PACLIC-20), pages 87–94.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.961736">
<title confidence="0.999167">A Multi-layer Chinese Word Segmentation System Optimized Out-of-domain Tasks</title>
<author confidence="0.999574">Qin Gao Stephan Vogel</author>
<affiliation confidence="0.9983335">Language Technologies Institute Language Technologies Institute Carnegie Mellon University Carnegie Mellon University</affiliation>
<email confidence="0.998758">qing@cs.cmu.edustephan.vogel@cs.cmu.edu</email>
<abstract confidence="0.998183526315789">State-of-the-art Chinese word segmentation systems have achieved high performance when training data and testing data are from the same domain. However, they suffer from the generalizability problem when applied on test data from different domains. We introduce a multi-layer Chinese word segmentation system which can integrate the outputs from multiple heterogeneous segmentation systems. By training a second layer of large margin classifier on top of the outputs from several Conditional Random Fields classifiers, it can utilize a small amount of in-domain training data to improve the performance. Experimental results show consistent improvement on F1 scores and OOV recall rates by applying the approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yasemin Altun</author>
<author>Ioannis Tsochantaridis</author>
<author>Thomas Hofmann</author>
</authors>
<title>Hidden markov support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="4912" citStr="Altun et al., 2003" startWordPosition="784" endWordPosition="787">ssifier. Large margin models such as Support Vector Machine (SVM) (Vapnik, 1995) can be trained on small corpus and generalize well. Therefore we chose to use CRF in building sub-systems and SVM in building the second-layer. We built multiple CRF-based Chinese word segmentation systems using different features, and then use the marginal probability of each tag of all the systems as features in SVM. The SVM is then trained on small in-domain corpus, results in a decision hyperplane that minimizes the loss in the small training data. To integrate the dependencies of output tags, we use SVM-HMM (Altun et al., 2003) to capture the interactions between tags and features. By applying SVM-HMM we can bias our decision towards most informative CRF-based system w.r.t. the target domain. Our methodology is similar to (Cohen and Carvalho, 2005), who applied a crossvalidation-like method to train sequential stacking models, while we directly use small amount of indomain data to train the second-layer classifiers. The paper is organized as follows, first we will discuss the CRF-based sub-systems we used in section 2, and then the SVM-based system combination method in section 3. Finally, in section 4 the experimen</context>
<context position="11378" citStr="Altun et al., 2003" startWordPosition="1904" endWordPosition="1907"> can train segmenters specific to the domain. However obtaining sufficient amount of labeled data in the target domain is time-consuming and expensive. In the mean time, if we only label a small amount of data in the target domain and put them into the training data, the effect may be too small because the size of out-of-domain data can overwhelm the in-domain data. In this paper we propose a different way of utilizing small amount of in-domain corpus. We put a second-layer classifier on top of the CRFbased sub-systems, the output of CRF-based subsystems are treated as features in an SVM-HMM (Altun et al., 2003) classifier. We can train the SVM-HMM classifier on a small amount of indomain data. The training procedure can be viewed as finding the optimal decision boundary that minimize the hinge loss on the in-domain data. Because the number of features for SVMHMM is significantly smaller than CRF, we can train the model with as few as several hundred sentences. Similar to CRF, the SVM-HMM classifier still treats the Chinese word segmentation problem as character tagging. However, because of the limitation of training data size, we try to minimize the number of classes. We chose to adopt the two-tag s</context>
</contexts>
<marker>Altun, Tsochantaridis, Hofmann, 2003</marker>
<rawString>Altun, Yasemin, Ioannis Tsochantaridis, and Thomas Hofmann. 2003. Hidden markov support vector machines. In Proceedings of International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaodong Chen</author>
<author>Ting Wang</author>
<author>Huowang Chen</author>
</authors>
<title>Using directed graph based bdmm algorithm for chinese word segmentation.</title>
<date>2005</date>
<pages>214--217</pages>
<contexts>
<context position="1183" citStr="Chen et al., 2005" startWordPosition="164" endWordPosition="167">ntation system which can integrate the outputs from multiple heterogeneous segmentation systems. By training a second layer of large margin classifier on top of the outputs from several Conditional Random Fields classifiers, it can utilize a small amount of in-domain training data to improve the performance. Experimental results show consistent improvement on F1 scores and OOV recall rates by applying the approach. 1 Introduction The Chinese word segmentation problem has been intensively investigated in the past two decades. From lexicon-based methods such as Bi-Directed Maximum Match (BDMM) (Chen et al., 2005) to statistical models such as Hidden Markove Model (HMM) (Zhang et al., 2003), a broad spectrum of approaches have been experimented. By casting the problem as a character labeling task, sequence labeling models such as Conditional Random Fields can be applied on the problem (Xue and Shen, 2003). State-of-the-art CRF-based systems have achieved good performance. However, like many machine learning problems, generalizability is crucial for a domain-independent segmentation system. Because the training data usually come from limited domains, when the domain of test data is different from the tr</context>
</contexts>
<marker>Chen, Wang, Chen, 2005</marker>
<rawString>Chen, Yaodong, Ting Wang, and Huowang Chen. 2005. Using directed graph based bdmm algorithm for chinese word segmentation. pages 214–217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
<author>Vitor Carvalho</author>
</authors>
<title>Stacked sequential learning.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Joint Conference on Artificial Intelligence (IJ-CAI).</booktitle>
<contexts>
<context position="5137" citStr="Cohen and Carvalho, 2005" startWordPosition="821" endWordPosition="825">ayer. We built multiple CRF-based Chinese word segmentation systems using different features, and then use the marginal probability of each tag of all the systems as features in SVM. The SVM is then trained on small in-domain corpus, results in a decision hyperplane that minimizes the loss in the small training data. To integrate the dependencies of output tags, we use SVM-HMM (Altun et al., 2003) to capture the interactions between tags and features. By applying SVM-HMM we can bias our decision towards most informative CRF-based system w.r.t. the target domain. Our methodology is similar to (Cohen and Carvalho, 2005), who applied a crossvalidation-like method to train sequential stacking models, while we directly use small amount of indomain data to train the second-layer classifiers. The paper is organized as follows, first we will discuss the CRF-based sub-systems we used in section 2, and then the SVM-based system combination method in section 3. Finally, in section 4 the experimental results are presented. 2 CRF-based sub-systems In this section we describe the sub-systems we used in system. All of the sub-systems are based on CRF with different features. The tag set we use is the 6-tag (B1, B2, B3, M</context>
</contexts>
<marker>Cohen, Carvalho, 2005</marker>
<rawString>Cohen, William W. and Vitor Carvalho. 2005. Stacked sequential learning. In Proceedings of the International Joint Conference on Artificial Intelligence (IJ-CAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
<author>Thomas Finley</author>
<author>ChunNam John Yu</author>
</authors>
<title>Cutting-plane training of structural svms.</title>
<date>2009</date>
<booktitle>Machine Learning,</booktitle>
<volume>77</volume>
<issue>1</issue>
<contexts>
<context position="14235" citStr="Joachims et al., 2009" startWordPosition="2407" endWordPosition="2410">so be processed by all of the CRF-based sub-systems, and the outputs will be fed into the SVM-HMM classifier. The final decisions of word boundaries are based solely on the classified labels of SVMHMM model. For the Bake-off system, we labeled two hundred sentences in each of the unsegmented training set (A and B). Since only one submission is allowed, the SVM-HMM model of the final system was trained on the concatenation of the two training sets, i.e. four hundred sentences. The CRF-based sub-systems are trained using CRF++ toolkit (Kudo, 2003), and the SVM-HMM trained by the SVM&amp;quot;&amp;quot;&apos; toolkit (Joachims et al., 2009). 4 Experiments To evaluate the effectiveness of the proposed system combination method, we performed two experiments. First, we evaluate the system combination method on provided training data in the way that is similar to cross-validation. Second, we experimented with training the SVM-HMM model with the manually labeled data come from corMicro-Average Macro-Average P R F1 OOV-R P R F1 OOV-R S1 0.962 0.960 0.961 0.722 0.962 0.960 0.960 0.720 S2 0.965 0.966 0.966 0.725 0.965 0.966 0.966 0.723 S3 0.966 0.967 0.967 0.731 0.966 0.967 0.967 0.729 S4 0.968 0.969 0.968 0.731 0.967 0.969 0.969 0.729 </context>
</contexts>
<marker>Joachims, Finley, Yu, 2009</marker>
<rawString>Joachims, Thorsten, Thomas Finley, and ChunNam John Yu. 2009. Cutting-plane training of structural svms. Machine Learning, 77(1):27–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
</authors>
<title>CRF++: Yet another crf toolkit. Web page: http://crfpp.sourceforge.net/.</title>
<date>2003</date>
<contexts>
<context position="14164" citStr="Kudo, 2003" startWordPosition="2397" endWordPosition="2398">g these features. During decoding, the Chinese input will also be processed by all of the CRF-based sub-systems, and the outputs will be fed into the SVM-HMM classifier. The final decisions of word boundaries are based solely on the classified labels of SVMHMM model. For the Bake-off system, we labeled two hundred sentences in each of the unsegmented training set (A and B). Since only one submission is allowed, the SVM-HMM model of the final system was trained on the concatenation of the two training sets, i.e. four hundred sentences. The CRF-based sub-systems are trained using CRF++ toolkit (Kudo, 2003), and the SVM-HMM trained by the SVM&amp;quot;&amp;quot;&apos; toolkit (Joachims et al., 2009). 4 Experiments To evaluate the effectiveness of the proposed system combination method, we performed two experiments. First, we evaluate the system combination method on provided training data in the way that is similar to cross-validation. Second, we experimented with training the SVM-HMM model with the manually labeled data come from corMicro-Average Macro-Average P R F1 OOV-R P R F1 OOV-R S1 0.962 0.960 0.961 0.722 0.962 0.960 0.960 0.720 S2 0.965 0.966 0.966 0.725 0.965 0.966 0.966 0.723 S3 0.966 0.967 0.967 0.731 0.96</context>
</contexts>
<marker>Kudo, 2003</marker>
<rawString>Kudo, Taku. 2003. CRF++: Yet another crf toolkit. Web page: http://crfpp.sourceforge.net/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>Proceedings of International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="3952" citStr="Lafferty et al., 2001" startWordPosition="624" endWordPosition="627">domain, and the term in-domain corpus to refer to small amount of data that comes from the same domain of the test data reasonable performance on all domains, but their performance on different domains may vary. The job of the second layer is to find the best decision boundary on the target domain, in presence of all the decisions made by the sub-systems. Figure 1: The architecture of the system, the first layer (sub-systems) is trained on general out-ofdomain corpus and various resources, while the second layer of the classifier is trained on indomain corpus. Conditional Random Fields (CRF) (Lafferty et al., 2001) has been applied on Chinese word segmentation and achieved high performance. However, because of its conditional nature the small amount of in-domain corpus will not significantly change the distributions of the model parameters trained on out-of-domain corpus, it is more suitable to be used in the sub-systems than in the second-layer classifier. Large margin models such as Support Vector Machine (SVM) (Vapnik, 1995) can be trained on small corpus and generalize well. Therefore we chose to use CRF in building sub-systems and SVM in building the second-layer. We built multiple CRF-based Chines</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>Lafferty, John, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanxin Shi</author>
<author>Mengqiu Wang</author>
</authors>
<title>A dual-layer crfs based joint decoding method for cascaded segmentation and labeling tasks.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Joint Conference on Artificial Intelligence (IJ-CAI).</booktitle>
<contexts>
<context position="9829" citStr="Shi and Wang, 2007" startWordPosition="1637" endWordPosition="1640">ed on the conditional entropy of it. If the conditional entropy value is less than 1.0, we assign feature value 0 to the character, and for region [1.0, 2.0), we assign feature value 1. Similarly we define the region-to-value mappings as follows: [2.0, 3.5) 2, [3.5, 5.0) 4, [5.0, 7.0) 5, [7.0, +oo) 6. The forward and backward conditional entropy forms two features. We will refer to these features as EF. 2.4 Lexical Features Lexical features are the most important features to make sub-systems output different results on different domains. We adopt the definition of the features partially from (Shi and Wang, 2007). In our system we use only the Lbegin(C0) and Lend(C0) features, omitting the LmidC0 feature. The two features represent the maximum length of words found in the lexicon that contain the current character as the first or last character, correspondingly. For feature values equal or greater than 6, we group them into one value. Although we can find a number of Chinese lexicons available, they may or may not be generated according to the same standard as the training data. Concatenating them into one may bring in noise and undermine the performance. Therefore, every lexicon will generate its own</context>
</contexts>
<marker>Shi, Wang, 2007</marker>
<rawString>Shi, Yanxin and Mengqiu Wang. 2007. A dual-layer crfs based joint decoding method for cascaded segmentation and labeling tasks. In Proceedings of the International Joint Conference on Artificial Intelligence (IJ-CAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Andrew McCallum</author>
</authors>
<title>Introduction to Statistical Relational Learning, chapter An Introduction to Conditional Random Fields for Relational Learning.</title>
<date>2006</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="12450" citStr="Sutton and McCallum, 2006" startWordPosition="2090" endWordPosition="2093"> as character tagging. However, because of the limitation of training data size, we try to minimize the number of classes. We chose to adopt the two-tag set, i.e. class 1 indicates the character is the end of a word and class 2 means otherwise. Also, due to limited amount of training data, we do not use any character features, instead, the features comes directly from the output of sub-systems. The SVMHMM can use any real value features, which enables integration of a wide range of segmenters. In this paper we use only the CRF-based segmenters, and the features are the marginal probabilities (Sutton and McCallum, 2006) of all the tags in the tag set for each character. As an example, for a CRF-based sub-system that outputs six tags, it will output six features for each character for the SVM-HMM classifier, corresponding to the marginal probability of the character given the CRF model. The marginal probabilities for the same tag (e.g. B1, S, etc) come from different CRF-based sub-systems are treated as distinct features. Features Lexicons S1 CF, CTF None S2 CF, NF ADSO, CTB6 S3 CF, CTF, NF ADSO S4 CF, CTF, NF, EF ADSO, CTB6 S5 CF, EF None S6 CF, NF None S7 CF, CTF ADSO S8 CF, CTF CTB6 Table 2: The configurat</context>
</contexts>
<marker>Sutton, McCallum, 2006</marker>
<rawString>Sutton, Charles and Andrew McCallum, 2006. Introduction to Statistical Relational Learning, chapter An Introduction to Conditional Random Fields for Relational Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer.</publisher>
<contexts>
<context position="4373" citStr="Vapnik, 1995" startWordPosition="692" endWordPosition="693">is trained on general out-ofdomain corpus and various resources, while the second layer of the classifier is trained on indomain corpus. Conditional Random Fields (CRF) (Lafferty et al., 2001) has been applied on Chinese word segmentation and achieved high performance. However, because of its conditional nature the small amount of in-domain corpus will not significantly change the distributions of the model parameters trained on out-of-domain corpus, it is more suitable to be used in the sub-systems than in the second-layer classifier. Large margin models such as Support Vector Machine (SVM) (Vapnik, 1995) can be trained on small corpus and generalize well. Therefore we chose to use CRF in building sub-systems and SVM in building the second-layer. We built multiple CRF-based Chinese word segmentation systems using different features, and then use the marginal probability of each tag of all the systems as features in SVM. The SVM is then trained on small in-domain corpus, results in a decision hyperplane that minimizes the loss in the small training data. To integrate the dependencies of output tags, we use SVM-HMM (Altun et al., 2003) to capture the interactions between tags and features. By ap</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vapnik, Vladimir N. 1995. The Nature of Statistical Learning Theory. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Libin Shen</author>
</authors>
<title>Chinese word segmentation as lmr tagging.</title>
<date>2003</date>
<booktitle>In Proceedings of the second SIGHAN workshop on Chinese language processing,</booktitle>
<pages>176--179</pages>
<contexts>
<context position="1480" citStr="Xue and Shen, 2003" startWordPosition="215" endWordPosition="218"> performance. Experimental results show consistent improvement on F1 scores and OOV recall rates by applying the approach. 1 Introduction The Chinese word segmentation problem has been intensively investigated in the past two decades. From lexicon-based methods such as Bi-Directed Maximum Match (BDMM) (Chen et al., 2005) to statistical models such as Hidden Markove Model (HMM) (Zhang et al., 2003), a broad spectrum of approaches have been experimented. By casting the problem as a character labeling task, sequence labeling models such as Conditional Random Fields can be applied on the problem (Xue and Shen, 2003). State-of-the-art CRF-based systems have achieved good performance. However, like many machine learning problems, generalizability is crucial for a domain-independent segmentation system. Because the training data usually come from limited domains, when the domain of test data is different from the training data, the results are still not satisfactory. A straight-forward solution is to obtain more labeled data in the domain we want to test. However this is not easily achievable because the amount of data needed to train a segmentation system are large. In this paper, we focus on improving the</context>
</contexts>
<marker>Xue, Shen, 2003</marker>
<rawString>Xue, Nianwen and Libin Shen. 2003. Chinese word segmentation as lmr tagging. In Proceedings of the second SIGHAN workshop on Chinese language processing, pages 176–179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huaping Zhang</author>
<author>Qun Liu</author>
<author>Xueqi Cheng</author>
<author>Hao Zhang</author>
<author>Hongkui Yu</author>
</authors>
<title>Chinese lexical analysis using hierarchical hidden markov model.</title>
<date>2003</date>
<booktitle>In Proceedings of the second SIGHAN workshop on Chinese language processing,</booktitle>
<pages>63--70</pages>
<contexts>
<context position="1261" citStr="Zhang et al., 2003" startWordPosition="177" endWordPosition="180">gmentation systems. By training a second layer of large margin classifier on top of the outputs from several Conditional Random Fields classifiers, it can utilize a small amount of in-domain training data to improve the performance. Experimental results show consistent improvement on F1 scores and OOV recall rates by applying the approach. 1 Introduction The Chinese word segmentation problem has been intensively investigated in the past two decades. From lexicon-based methods such as Bi-Directed Maximum Match (BDMM) (Chen et al., 2005) to statistical models such as Hidden Markove Model (HMM) (Zhang et al., 2003), a broad spectrum of approaches have been experimented. By casting the problem as a character labeling task, sequence labeling models such as Conditional Random Fields can be applied on the problem (Xue and Shen, 2003). State-of-the-art CRF-based systems have achieved good performance. However, like many machine learning problems, generalizability is crucial for a domain-independent segmentation system. Because the training data usually come from limited domains, when the domain of test data is different from the training data, the results are still not satisfactory. A straight-forward soluti</context>
</contexts>
<marker>Zhang, Liu, Cheng, Zhang, Yu, 2003</marker>
<rawString>Zhang, Huaping, Qun Liu, Xueqi Cheng, Hao Zhang, and Hongkui Yu. 2003. Chinese lexical analysis using hierarchical hidden markov model. In Proceedings of the second SIGHAN workshop on Chinese language processing, pages 63–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chunyu Kit</author>
</authors>
<title>Unsupervised segmentation helps supervised learning of character tagging for word segmentation and named entity recognition.</title>
<date>2008</date>
<booktitle>In The Sixth SIGHAN Workshop on Chinese Language Processing (SIGHAN-6),</booktitle>
<pages>106--111</pages>
<contexts>
<context position="6060" citStr="Zhao and Kit, 2008" startWordPosition="984" endWordPosition="987">em combination method in section 3. Finally, in section 4 the experimental results are presented. 2 CRF-based sub-systems In this section we describe the sub-systems we used in system. All of the sub-systems are based on CRF with different features. The tag set we use is the 6-tag (B1, B2, B3, M, E, S) set proposed by Zhao et al (2006). All of the sub-systems use the same tag set, however as we will see later, the second-layer classifier in our system does not require the sub-systems to have a common tag set. Also, all of the sub-systems include a common set of character features proposed in (Zhao and Kit, 2008). The offsets and concatenations of the six n-gram features (the feature template) are: C−1, C0, C1, C−1C0, C0C1, C−1C1. In the remaining part of the section we will introduce other features that we employed in different subsystems. 2.1 Character type features By simply classify the characters into four types: Punctuation (P), Digits (D), Roman Letters (L) and Chinese characters (C), we can assign character type tags to every character. The idea is straight-forward. We denote the feature as CTF. Similar to character feature, we also use different offsets and concatenations for character type f</context>
</contexts>
<marker>Zhao, Kit, 2008</marker>
<rawString>Zhao, Hai and Chunyu Kit. 2008. Unsupervised segmentation helps supervised learning of character tagging for word segmentation and named entity recognition. In The Sixth SIGHAN Workshop on Chinese Language Processing (SIGHAN-6), pages 106–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chang-Ning Huang</author>
<author>Mu Li</author>
<author>Bao-Liang Lu</author>
</authors>
<title>Effective tag set selection in chinese word segmentation via conditional random field modeling.</title>
<date>2006</date>
<booktitle>In Proceedings of the 20th Pacific Asia Conference on Language, Information and Computation (PACLIC-20),</booktitle>
<pages>87--94</pages>
<contexts>
<context position="5778" citStr="Zhao et al (2006)" startWordPosition="933" endWordPosition="936">idation-like method to train sequential stacking models, while we directly use small amount of indomain data to train the second-layer classifiers. The paper is organized as follows, first we will discuss the CRF-based sub-systems we used in section 2, and then the SVM-based system combination method in section 3. Finally, in section 4 the experimental results are presented. 2 CRF-based sub-systems In this section we describe the sub-systems we used in system. All of the sub-systems are based on CRF with different features. The tag set we use is the 6-tag (B1, B2, B3, M, E, S) set proposed by Zhao et al (2006). All of the sub-systems use the same tag set, however as we will see later, the second-layer classifier in our system does not require the sub-systems to have a common tag set. Also, all of the sub-systems include a common set of character features proposed in (Zhao and Kit, 2008). The offsets and concatenations of the six n-gram features (the feature template) are: C−1, C0, C1, C−1C0, C0C1, C−1C1. In the remaining part of the section we will introduce other features that we employed in different subsystems. 2.1 Character type features By simply classify the characters into four types: Punctu</context>
</contexts>
<marker>Zhao, Huang, Li, Lu, 2006</marker>
<rawString>Zhao, Hai, Chang-Ning Huang, Mu Li, and Bao-Liang Lu. 2006. Effective tag set selection in chinese word segmentation via conditional random field modeling. In Proceedings of the 20th Pacific Asia Conference on Language, Information and Computation (PACLIC-20), pages 87–94.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>