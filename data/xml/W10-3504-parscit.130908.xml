<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000156">
<title confidence="0.990168">
Expanding textual entailment corpora from Wikipedia using co-training
</title>
<author confidence="0.997224">
Fabio Massimo Zanzotto Marco Pennacchiotti
</author>
<affiliation confidence="0.998189">
University of Rome ”Tor Vergata” Yahoo! Lab
</affiliation>
<email confidence="0.996348">
zanzotto@info.uniroma2.it pennac@yahoo-inc.com
</email>
<sectionHeader confidence="0.993849" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998854166666667">
In this paper we propose a novel method
to automatically extract large textual en-
tailment datasets homogeneous to existing
ones. The key idea is the combination of
two intuitions: (1) the use of Wikipedia
to extract a large set of textual entail-
ment pairs; (2) the application of semi-
supervised machine learning methods to
make the extracted dataset homogeneous
to the existing ones. We report empirical
evidence that our method successfully ex-
pands existing textual entailment corpora.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999954303571429">
Despite the growing success of the Recognizing
Textual Entailment (RTE) challenges (Dagan et
al., 2006; Bar-Haim et al., 2006; Giampiccolo et
al., 2007), the accuracy of most textual entailment
recognition systems are still below 60%. An in-
tuitive way to improve performance is to provide
systems with larger annotated datasets. This is es-
pecially true for machine learning systems, where
the size of the training corpus is an important fac-
tor. As a consequence, several attempts have been
made to train systems using larger datasets ob-
tained by merging RTE corpora of different chal-
lenges. Unfortunately, experimental results show
a significant decrease in accuracy (de Marneffe et
al., 2006). There are two major reasons for this
counter-intuitive result:
Homogeneity. As indicated by many studies (e.g.
(Siefkes, 2008)), homogeneity of the training cor-
pus is an important factor for the applicability of
supervised machine learning models, since exam-
ples with similar properties often imply more ef-
fective models. Unfortunately, the corpora of the
four RTE challenges are not homogenous. Indeed,
they model different properties of the textual en-
tailment phenomenon, as they have been created
using slightly (but significantly) different method-
ologies. For example, part of the RTE-1 dataset
(Dagan et al., 2006) was created using compara-
ble documents, where positive entailments have a
lexical overlap higher than negative ones (Nichol-
son et al., 2006; Dagan et al., 2006). Comparable
documents have not been used as a source of later
RTE corpora, making RTE-1 odd with respect to
other datasets.
Corpus size. RTE corpora are relatively small
in size (typically 800 pairs). The increase in
size obtained by merging corpora from different
challenges is not a viable solution. Much larger
datasets, of one or more order of magnitude, are
needed to capture the complex properties charac-
terizing entailment.
A key issue for the future development of RTE
is then the creation of datasets fulfilling two prop-
erties: (1) large size; (2) homogeneity wrt. ex-
isting RTE corpora. The task of creating large
datasets is unfeasible for human annotators. Col-
laborative annotation environments such as the
Amazon Mechanical Turk1 can help to annotate
pairs of sentences in positive or negative entail-
ment (Zaenen, submitted; Snow et al., 2008). Yet,
these environments can hardly solve the problem
of finding relevant pairs of sentences. Completely
automatic processes of dataset creation have been
proposed (Burger and Ferro, 2005; Hickl et al.,
2006). Unfortunately, these datasets are not ho-
mogeneous wrt. to the RTE datasets, as they are
</bodyText>
<footnote confidence="0.995417">
1http://mturk.com
</footnote>
<page confidence="0.977797">
28
</page>
<note confidence="0.5598105">
Beijing, August 2010
Proceedings of the 2nd Workshop on “Collaboratively Constructed Semantic Resources”, Coling 2010, pages 28–36,
</note>
<bodyText confidence="0.999984695652174">
created using different methodologies. In this pa-
per we propose a novel method to automatically
extract entailment datasets which are guaranteed
to be large and homogeneous to RTE ones. The
key idea is the combination of two factors: (1) the
use of Wikipedia as source of a large set of tex-
tual entailment pairs; (2) the application of semi-
supervised machine learning methods, namely co-
training, to make corpora homogeneous to RTE.
The paper is organized as follows. In Section 2
we report on previous attempts in automatically
creating RTE corpora. In Section 3 we outline im-
portant properties that these corpora should have,
and introduce our methodology to extract an RTE
corpus from Wikipedia (the WIKI corpus), con-
forming to these properties. In Section 4 we de-
scribe how co-training techniques can be lever-
aged to make the WIKI corpus homogeneous to
existing RTE corpora. In Section 5 we report em-
pirical evidence that the combination of the WIKI
corpus and co-training is successful. Finally, in
Section 6 we draw final conclusions and outline
future work.
</bodyText>
<sectionHeader confidence="0.999748" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999941528301887">
The first attempt to automatically create large
RTE corpora was proposed by Burger and
Ferro (Burger and Ferro, 2005), with the MITRE
corpus, a corpus of positive entailment examples
extracted from the XIE section of the Gigaword
news collection (Graff, 2003). The idea of the ap-
proach is that the headline and the first paragraph
of a news article should be (near-)paraphrase. Au-
thors then collect paragraph-headline pairs as Text
(T) - Hypothesis (H) examples, where the head-
lines plays the role of H. The final corpus con-
sists of 100,000 pairs, with an estimated accuracy
of 70% – i.e. two annotators checked a sample
of about 500 pairs, and verified that 30% of these
were either false entailments or noisy pairs. The
major limitation of the Burger and Ferro (Burger
and Ferro, 2005)’s approach is that the final cor-
pus consist only of positive examples. Because
of this imbalance, the corpus cannot be positively
used by RTE learning systems.
Hickl et al. (2006) propose a solution to the
problem, providing a methodology to extract both
positive and negative pairs (the LCC corpus). A
positive corpus consisting of 101,000 pairs is ex-
tracted similarly to (Burger and Ferro, 2005). Cor-
pus accuracy is estimated on a sample of 2,500 ex-
amples, achieving 92% (i.e. almost all examples
are positives), 22 points higher than Burger and
Ferro. A negative corpus of 119,000 is extracted
either: (1) selecting sequential sentences includ-
ing mentions of a same named entity (98.000
pairs); (2) selecting pairs of sentences connected
by words such as even though, although, other-
wise, but (21,000 pairs). Estimated accuracy for
the two techniques is respectively 97% and 94%.
Hickl and colleagues show that expanding the
RTE-2 training set with the LCC corpus (the ex-
pansion factor is 125), their RTE system im-
proves 10% accuracy. This suggests that by ex-
panding with a large and balanced corpus, en-
tailment recognition performance drastically im-
proves. This intuition is later contradicted in a
second experiment by Hickl and Bensley (2007).
Authors use the LCC corpus with the RTE-3 train-
ing set to train a new RTE system, showing an im-
provement in accuracy of less than 1% wrt. the
RTE-3 training alone.
Overall, evidence suggests that automatic ex-
pansion of the RTE corpora do not always lead to
performance improvement. This highly depends
on how balanced the corpus is, on the RTE system
adopted, and on the specific RTE dataset that is
expanded.
</bodyText>
<sectionHeader confidence="0.975264" genericHeader="method">
3 Extracting the WIKI corpus
</sectionHeader>
<bodyText confidence="0.9997506">
In this section we outline some of the properties
that a reliable corpus for RTE should have (Sec-
tion 3.1), and show that a corpus extracted from
Wikipedia conforms to these properties (Sec-
tion 3.2).
</bodyText>
<subsectionHeader confidence="0.999788">
3.1 Good practices in building RTE corpora
</subsectionHeader>
<bodyText confidence="0.989296888888889">
Previous work in Section 2 and the vast literature
on RTE suggest that a “reliable” corpus for RTE
should have, among others, the following proper-
ties:
(1) Not artificial. Textual entailment is a complex
phenomenon which encompasses different lin-
guistic levels. Entailment types range from very
simple polarity mismatches and syntactic alterna-
tions, to very complex semantic and knowledge-
</bodyText>
<page confidence="0.993082">
29
</page>
<footnote confidence="0.442635">
Si In this regard, some have charged the New World Translation Committee with being inconsistent.
S2 In this regard, some have charged the New World Translation Committee with not be consistent.
</footnote>
<note confidence="0.55186825">
Si The ’Stockholm Network’ is Europe’s only dedicated service organisation for market-oriented think tanks and
thinkers.
S2 The ’Stockholm Network’ is, according to its own site, Europe’s only dedicated service organisation for market-
oriented think tanks and thinkers.
</note>
<figureCaption confidence="0.99955">
Figure 1: Sentence pairs from the Wikipedia revision corpus
</figureCaption>
<bodyText confidence="0.909356695652174">
based inferences. These different types of en-
tailments are naturally distributed in texts, such
as news and every day conversations. A reliable
RTE corpus should preserve this important prop-
erty, i.e. it should be rich in entailment types
whose distribution in the corpus is similar to that
in real texts; and should not include unrepresenta-
tive hand-crafted prototypical examples.
(2) Balanced and consistent. A reliable corpus
should be balanced, i.e. composed by an equal or
comparable number of positive and negative ex-
amples. This is particularly critical for RTE sys-
tems based on machine learning: highly imbal-
anced class distributions often result in poor learn-
ing performance (Japkowicz and Stephen, 2002;
Kubat and Matwin, 1997). Also, the positive and
negative subsets of the corpus should be consis-
tent, i.e. created using the same methodology. If
this property is not preserved, the risk is a learning
system building a model which separates positive
and negatives according to the properties charac-
terizing the two methodologies, instead of those
of the entailment phenomenon.
</bodyText>
<listItem confidence="0.993838384615385">
(3) Not biased on lexical overlap. A major criti-
cism on the RTE-1 dataset was that it contained
too many positive examples with high lexical
overlap wrt. negative examples (Nicholson et al.,
2006). Glickman et al. (2005) show that an RTE
system using word overlap to decide entailment,
surprisingly achieves an accuracy of 0.57 on RTE-
1 test set. These performances are comparable to
those obtained on the same dataset by more so-
phisticated and principled systems. Learning from
this experience, a good corpus for RTE should
avoid imbalances on lexical overlap.
(4) Homogeneous to existing RTE corpora.
</listItem>
<bodyText confidence="0.97881405">
Corpus homogeneity is a key property for any ma-
chine learning approach (Siefkes, 2008). A new
corpus for RTE should then model the same or
similar entailments types of the reliable existing
ones (e.g., those of the RTE challenges). If this is
not the case, RTE system will be unable to learn
a coherent model, thus resulting in a decrease in
performance.
The MITRE corpus satisfies property (1), but
does not (2) and (3), as it is highly imbalanced
(it contains mostly positive examples), and is
fairly biased on lexical overlap, as most examples
of headline-paragraph pairs have many words in
common. The LCC corpus suffers the problem of
inconsistency, as positive and negative examples
are derived with radically different methodolo-
gies. Both the MITRE and the LCC corpora are
difficult to merge with the RTE challenge datasets,
as they are not homogeneous – i.e. they have been
built using very different methodologies.
</bodyText>
<subsectionHeader confidence="0.968535">
3.2 Extracting the corpus from Wikipedia
revisions
</subsectionHeader>
<bodyText confidence="0.999905095238095">
Our main intuition in using Wikipedia to build
an entailment corpus is that the wiki framework
should provide a natural source of non-artificial
examples of true and false entailments, through
its revision system. Wikipedia is an open ency-
clopedia, where every person can behave as an
author, inserting new entries or modifying exist-
ing ones. We call original entry S1 a piece of
text in Wikipedia before it is modified by an au-
thor, and revision S2 the modified text. The pri-
mary concern of Wikipedia authors is to reshape
a document according to their intent, by adding
or replacing pieces of text. Excluding vandalism,
there are several reasons for making a revision:
missing information, misspelling, syntactic errors,
and, more importantly, disagreement on the con-
tent. For example, in Fig. 1, Si is revised to S2,
as the author disagrees on the content of S1.
Our hypothesis is that (S1, S2) pairs represent
good candidates of both true and false entailment
pairs (T, H), as they represent semantically close
</bodyText>
<page confidence="0.990195">
30
</page>
<bodyText confidence="0.998497054054054">
pieces of texts. Also, Wikipedia pairs conform to
the properties listed in the previous section, as de-
scribed in the following.
(S1, S2) pairs are not artificial, as we extract
them from pieces of original texts, without any
modification or post-processing. Also, pairs are
rich of different entailment types, whose distribu-
tion is a reliable sample of language in use2. As
shown later in the paper, a collection of (S1, S2)
pairs is likely balanced on positive and negative
examples, as authors either contradict the content
of the original entry (false entailment) or add new
information to the existing content (true entail-
ment). Positive and negative pairs are guaranteed
to be consistent, as they are drawn from the same
Wikipedia source. Finally, the Wikipedia is not
biased in lexical overlap: A sentence S2 replac-
ing S1, usually changes only a few words. Yet,
the meaning of S2 may or may not change wrt.
the meaning of S1 – i.e. the lexical overlap of
the two sentences is very high, but the entailment
relation between S1 and S2 may be either posi-
tive or negative. For example, in Fig. 1 both pairs
have high overlap, but the first is a positive en-
tailment (S01 → S02), while the second is negative
(S00 1 → S002 ).
An additional interesting property of Wikipedia
revisions is that the transition from S1 to S2 is
commented by the author. The comment is a
piece of text where authors explain and motivate
the change (e.g. “general cleanup of spelling and
grammar”, “revision: Eysenck died in 1997!!”).
Even if very small, the comment can be used to
determine if S1 and S2 are in entailment or not.
In the following section we show how we lever-
age comments to make the WIKI corpus homoge-
neous to those of the RTE challenges.
</bodyText>
<sectionHeader confidence="0.7646505" genericHeader="method">
4 Expanding the RTE corpus with WIKI
using co-training
</sectionHeader>
<bodyText confidence="0.969673571428571">
Unlike the LCC corpus where negative and posi-
tive examples are clearly separated, the WIKI cor-
pus mixes the two sets – i.e. it is unlabelled. In
order to exploit the WIKI corpus in the RTE task,
one should either manually annotate the corpus,
2It has been shown that web documents (as Wikipedia)
are reliable samples of language (Keller and Lapata, 2003).
</bodyText>
<equation confidence="0.928681">
CO-TRAINING ALGORITHM(L,U,k)
returns h1,h2,L1,L2
set L1 = L2 = L
</equation>
<bodyText confidence="0.662329333333333">
while stopping condition is not met
– learn h1 on F1 from L1, and learn h2 on F1 from
L2,
</bodyText>
<figure confidence="0.4185692">
– classify U with h1 obtaining U1, and classify U
with h2 obtaining U2
– select and remove k-best classified examples u1
and u2 from respectively U1 and U2
– add u1 to L2 and u2 to L1
</figure>
<figureCaption confidence="0.995251">
Figure 2: General co-training algorithm
</figureCaption>
<bodyText confidence="0.999964583333333">
or find an alternative strategy to leverage the cor-
pus even if unlabelled. As manual annotation is
unfeasible, we choose the second solution. The
goal is then to expand a labelled RTE challenge
training set with the unlabelled WIKI, so that the
performance of an RTE system can increase over
an RTE test set.
In the literature, several techniques have been
proposed to use unlabelled data to expand a
training labelled corpus, e.g. Expectation-
Maximization (Dempster et al., 1977). We here
apply the co-training technique, first proposed by
(Blum and Mitchell, 1998) and then successfully
leveraged and analyzed in different settings (Ab-
ney, 2002). Co-training can be applied when the
unlabelled dataset allows two independent views
on its instances (applicability condition).
In this section, we first provide a short descrip-
tion of the co-training algorithm (Section 4.1). We
then investigate if different RTE corpora conform
to the applicability condition (Section 4.2). Fi-
nally, we show that our WIKI corpus conforms to
the condition, and then apply co-training by creat-
ing two independent views (Section 4.3).
</bodyText>
<subsectionHeader confidence="0.99917">
4.1 Co-training
</subsectionHeader>
<bodyText confidence="0.9999575">
The co-training algorithm uses unlabelled data to
increase classification performance, and to indi-
rectly increasing the size of labelled corpora. The
algorithm can be applied only under a specific ap-
plicability condition: corpus’ instances must have
two independent views, i.e. they can be modeled
by two independent feature sets.
We here adopt a slightly modified version of the
</bodyText>
<page confidence="0.999443">
31
</page>
<bodyText confidence="0.999970814814815">
cotraining algorithm, as described in Fig.2. Under
the applicability condition, instances are modeled
on a feature space F = F1 x F2 x C, where F1
and F2 are the two independent views and C is
the set of the target classes (in our case, true and
false entailment). The algorithm starts with an ini-
tial set of training labelled examples L and a set
of unlabelled examples U. The set L is copied
in two sets L1 and L2, used to train two differ-
ent classifiers h1 and h2, respectively using views
F1 and F2. The two classifiers are used to clas-
sify the unlabelled set U, obtaining two different
classifications, U1 and U2. Then comes the co-
training step: the k-best classified instances in U1
are added to L2 and feed the learning of a new
classifier h2 on the feature space F2. Similarly, the
k-best instances in U2 are added to L1 and train a
new classifier h1 on F1.
The procedure repeats until a stopping condi-
tion is met. This can be either a fixed number of
added unlabelled examples (Blum and Mitchell,
1998), the performance drop on a control set of
labelled instances, or a filter on the disagreement
of h1 and h2 in classifying U (Collins and Singer,
1999). The final outcome of co-training is the new
set of labelled examples L1 U L2 and the two clas-
sifier h1 and h2, obtained from the last iteration.
</bodyText>
<subsectionHeader confidence="0.989747">
4.2 Applicability condition on RTE corpora
</subsectionHeader>
<bodyText confidence="0.999901964285714">
In order to leverage co-training for homoge-
neously expanding an RTE corpus, it is neces-
sary to have a large unlabelled corpus which sat-
isfies the applicability condition. Unfortunately,
existing methodologies cannot guarantee the con-
dition.
For example, the corpora from which the
datasets of the RTE challenges were derived, were
created from the output of applications perform-
ing specific tasks (e.g., Question&amp;Answering, In-
formation Extraction, Machine Translation, etc.).
These corpora do not offer the possibility to cre-
ate two completely independent views. Indeed,
each extracted pair is composed only by the tex-
tual fragments of T and H, i.e. the only infor-
mation available are the two pieces of texts, from
which it is difficult to extract completely indepen-
dent sets of features, as linguistic features tend to
be dependent.
The MITRE corpus is extracted using two sub-
sequent sentences, the title and the first paragraph.
The LCC negative corpus is extracted using two
correlated sentences or subsentences. Also in
these two cases, it is very hard to find a view that is
independent from the space of the sentence pairs.
None of the existing RTE corpora can then be
used for co-training. In the next section we show
that this is not the case for the WIKI corpus.
</bodyText>
<subsectionHeader confidence="0.8754305">
4.3 Creating independent views on the WIKI
corpus
</subsectionHeader>
<bodyText confidence="0.999892">
The WIKI corpus is naturally suited for co-
training, as for each (S1, S2) pair, it is possible
to clearly define two independent views:
</bodyText>
<listItem confidence="0.973106555555555">
• content-pair view: a set of features modeling
the actual textual content of S1 and S2. This
view is typically available also in any other
RTE corpus.
• comment view: a set of features regarding the
revision comment inserted by an author. This
view represents “external” information (wrt.
to the text fragments) which are peculiar of
the WIKI corpus.
</listItem>
<bodyText confidence="0.999976333333333">
These two views are most likely independent.
Indeed, the content-pair view deals with the con-
tent of the Wikipedia revision, while the com-
ment view describes the reason why a revision
has been made. This setting is very similar to
the original one proposed for co-training by Blum
and Mitchell (Blum and Mitchell, 1998), where
the target problem was the classification of web
pages, and the two independent views on a page
were (1) its content and (2) its hyperlinks.
In the rest of this section we describe the feature
spaces we adopt for the two independent views.
</bodyText>
<subsectionHeader confidence="0.498249">
4.3.1 Content-pair view
</subsectionHeader>
<bodyText confidence="0.999974111111111">
The content-pair view is the classical view used
in RTE. The original entry S1 represents the Text
T, while the revision S2 is the Hypothesis H.
Any feature space of those reported in the textual
entailment literature could be applied. We here
adopt the space that represents first-order syntac-
tic rewrite rules (FOSR), as described in (Zan-
zotto and Moschitti, 2006). In this feature space,
each feature represents a syntactic first-order or
</bodyText>
<page confidence="0.998279">
32
</page>
<bodyText confidence="0.998137333333333">
grounded rewrite rule. For example, the rule:
is represented by the feature &lt; l, r &gt;. A (T, H)
pair activates a feature if it unifies with the related
rule. A detailed discussion of the FOSR feature
space is given in (Zanzotto et al., 2009) and ef-
ficient algorithms for the computation of the re-
lated kernel functions can be found in (Moschitti
and Zanzotto, 2007; Zanzotto and Dell’Arciprete,
2009).
</bodyText>
<subsectionHeader confidence="0.997258">
4.4 Comment view
</subsectionHeader>
<bodyText confidence="0.999993789473684">
A review comment is typically a textual fragment
describing the reason why an author has decided
to make a revision. In most cases the comment is
not a well-formed sentence, as authors tend to use
informal slang expressions and abbreviations (e.g.
“details: Trelew Massacre; cat: Dirty War, copy-
edit”, “removed a POV vandalism by Spylab”,
“dab ba:clean up using Project:AWB”). In these
cases, where syntactic analysis would mostly fail,
it is advisable to use simpler surface approaches
to build the feature space. We then use a stan-
dard bag-of-word space, combined with a bag-of-
2-grams space. For the first space we keep only
meaningful content words, by using a standard
stop-list including articles, prepositions, and very
frequent words such as be and have. The sec-
ond space should help in capturing small text frag-
ments containing functional words: we then keep
all words without using any stop-list.
</bodyText>
<sectionHeader confidence="0.999464" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9999371">
The goals of our experiments are the following:
(1) check the quality of the WIKI corpus, i.e. if
positive and negative examples well represent the
entailment phenomenon; (2) check if WIKI con-
tains examples similar to those of the RTE chal-
lenges, i.e. if the corpus is homogeneous to RTE;
(3) check if the WIKI corpus improves classifica-
tion performance when used to expand the RTE
datasets using the co-training technique described
in Section 4.
</bodyText>
<subsectionHeader confidence="0.96796">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999626974358974">
In order to check the above claims, we need
to experiment with both manually labelled and
unlabelled corpora. As unlabelled corpora we
adopt:
wiki unlabelled: An unlabelled WIKI corpus of
about 3,000 examples. The corpus has been built
by downloading 40,000 Wikipedia pages dealing
with 800 entries about politics, scientific theories,
and religion issues. We extracted original entries
and revisions from the XML and wiki code,
collecting an overall corpus of 20,000 (51, 52)
pairs. We then randomly selected the final 3,000
pairs.
news: A corpus of 1,600 examples obtained using
the methods adopted for the LCC corpus, both
for negative and positive examples (Hickl et al.,
2006).3 We randomly divided the corpus in two
parts: 800 training and 800 testing examples.
Each set contains an equal number of 400 positive
and negative pairs.
As labelled corpora we use:
RTE-1,RTE-2, and RTE-3: The corpora from
the first three RTE challenges (Dagan et al., 2006;
Bar-Haim et al., 2006; Giampiccolo et al., 2007).
We use the standard split between training and
testing.
wiki: A manually annotated corpus of 2,000
examples from the WIKI corpus. Pairs have been
annotated considering the original entry as the
H and the revision as T. Noisy pairs containing
vandalism or grammatical errors were removed
(these accounts for about 19% of the examples).
In all, the annotation produced 945 positive
examples (strict entailments and paraphrases) and
669 negative examples (reverse strict entailments
and contradictions). The annotation was carried
out by two experienced researchers, each one
annotating half of the corpus. Annotation guide-
lines follow those used for the RTE challenges.4
</bodyText>
<footnote confidence="0.684844">
3For negative examples, we adopt the headline - first para-
graph extraction methodology.
4Annotators were initially trained on a small development
corpus of 200 pairs. The inter-annotator agreement on this
set, computed using the Kappa-statistics (Siegel and Castel-
lan, 1988), was 0.60 corresponding to substantial agreement,
</footnote>
<figure confidence="0.929169333333333">
S
S
NP X VP
NP Y VP
→
p = l → r=
VBP
NP X
owns
bought
NP Y
VBP
</figure>
<page confidence="0.997255">
33
</page>
<bodyText confidence="0.999890272727273">
The corpus has been randomly split in three
equally numerous parts: development, training,
and testing. We kept aside the development to
design the features, while we used training and
testing for the experiments.
We use the Charniak Parser (Charniak, 2000)
for parsing sentences, and SVM-light (Joachims,
1999) extended with the syntactic first-order rule
kernels described in (Zanzotto and Moschitti,
2006; Moschitti and Zanzotto, 2007) for creating
the FOSR feature space.
</bodyText>
<subsectionHeader confidence="0.999152">
5.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.992744382352941">
The first experiment aims at checking the qual-
ity of the WIKI corpus, by comparing the perfor-
mance obtained by a standard RTE system over
the corpus in exam with those obtained over any
RTE challenge corpus. The hypothesis is that if
performance is comparable, then the corpus in
exam has the same complexity (and quality) as
the RTE challenge corpora. We then indepen-
dently experiment with the wiki and the news
corpora with the training-test splits reported in
Section 5.1. As RTE system we adopt an SVM
model learnt on the FOSR feature space described
in Section 4.3.1.
The accuracies of the system on the wiki
and news corpora are respectively 70.73% and
94.87%. The performance of the system on the
wiki corpus are in line with those obtained over
the RTE-2 dataset (60.62%). This suggests that
the WIKI corpus is at least as complex as the RTE
corpora (i.e. positive and negatives are not triv-
ially separable). On the contrary, the news cor-
pus is much easier to separate. Pilot experiments
show that increasing the size of the news corpus,
accuracy reaches nearly 100%. This indicates that
positive and negative examples in the news cor-
pus are extremely different. Indeed, as mentioned
in Section 3.1, news is not consistent – i.e. the
extraction methods for the positives and the neg-
atives are so different that the examples can be
easily recognized using evidence not representa-
tive of the entailment phenomenon (e.g. for nega-
tive examples, the lexical overlap is extremely low
wrt. positives).
in line with the RTE challenge annotation efforts.
</bodyText>
<table confidence="0.99167125">
Training Corpus Accuracy
RTE-2 60.62
RTE-1 51.25
RTE-3 57.25
wiki 56.00
news 53.25
RTE-2+RTE-1 58.5
RTE-2+RTE-3 59.62
RTE-2+news 56.75
RTE-2+wiki 59.25
RTE-1+wiki 53.37
RTE-3+wiki 59.00
</table>
<tableCaption confidence="0.992064">
Table 1: Accuracy of different training corpora over RTE-2
test.
</tableCaption>
<bodyText confidence="0.999944228571429">
In a second experiment we aim at checking if
WIKI is homogeneous to the RTE challenge cor-
pora – i.e. if it contains (T, H) pairs similar to
those of the RTE corpora. If this holds, we would
expect the performance of the RTE system to im-
prove (or at least not decrease) when expanding a
given RTE challenge corpus with WIKI. de Marn-
effe et al. (2006) already showed in their experi-
ment that it is extremely difficult to obtain better
performance by simply expanding an RTE chal-
lenge training corpus with corpora of other chal-
lenges, since different corpora are usually not ho-
mogeneous.
We here repeat a similar experiment: we ex-
periment with different combinations of training
sets, over the same test set (namely, RTE-2 test).
Results are reported in Table 1. The higher per-
formance is the one of the system when trained on
RTE-2 training set (second row) – i.e. a corpus
completely homogeneous to RTE-2 would pro-
duce the same performance as RTE-2 training.
As expected, the models learnt on RTE-1 and
RTE-3 perform worse (third and fourth rows): in
particular, RTE-1 seems extremely different from
RTE-2, as results show. The wiki corpus is more
similar to RTE-2 than the news corpus, i.e. per-
formance are higher. Yet, it is quite surprising that
the news corpus yields to a performance drop as
in (Hickl et al., 2006) it shows a high performance
increase.
The expansion of RTE-2 with the above cor-
pora (seventh-tenth rows) lead to a drop in per-
formance, suggesting that none of the corpora
is completely homogeneous to RTE-2. Yet, the
performance drop of the wiki corpus (RTE-2 +
</bodyText>
<page confidence="0.991306">
34
</page>
<figure confidence="0.9977355">
0 20 40 60 80 100
unlabelled examples
</figure>
<figureCaption confidence="0.99993">
Figure 3: Co-training accuracy curve on the two corpora.
</figureCaption>
<bodyText confidence="0.981588243243243">
wiki) is comparable to the performance drop ob-
tained using the other two RTE corpora (RTE-2 +
RTE-1 and RTE-2 + RTE-3). This indicates that
wiki is more homogeneous to RTE than news
– i.e. it contains (T, H) pairs that are similar to
the RTE examples. Interestingly, wiki combined
with other RTE corpora (RTE-1 + wiki and RTE-
3 + wiki) increases performance wrt. the models
obtained with RTE-1 and RTE-3 alone (last two
rows).
In a final experiment, we check if the WIKI
corpus improves the performance when combined
with the RTE-2 training in a co-training setting, as
described in Section 4. This would confirm that
WIKI is homogeneous to the RTE-2 corpus, and
could then be successfully adopted in future RTE
competitions. As test sets, we experiment both
with RTE-2 and RTE-3 test. In the co-training,
we use the RTE-2 training set as initial set L, and
wiki unlabelled as the unlabelled set U.5
Figure 3 reports the accuracy curves obtained
by the classifier hi learnt on the content view, at
each co-training iteration, both on the RTE-2 and
RTE-3 test sets. As the comment view is not avail-
able in the RTE sets, the comment-view classi-
fier become active only after the first 10 examples
are fed as training from the content view classi-
5Note that only wiki unlabelled allows both views de-
scribed in Section 4.3.
fier. As expected, performance increase for some
steps and then become stable for RTE-3 and de-
crease for RTE-2. This is the only case in which
we verified an increase in performance using cor-
pora other than the official ones from RTE chal-
lenges. This result suggests that the WIKI corpus
can successfully contribute to learn better textual
entailment models for RTE.
</bodyText>
<sectionHeader confidence="0.999466" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999433869565217">
In this paper we proposed a method for expanding
existing textual entailment corpora that leverages
Wikipedia. The method is extremely promising
as it allows building corpora homogeneous to ex-
isting ones. The model we have presented is not
strictly related to the RTE corpora. This method
can then be used to expand corpora such as the
Fracas test-suite (Cooper et al., 1996) which is
more oriented to specific semantic phenomena.
Even if the performance increase of the com-
pletely unsupervised cotraining method is not ex-
tremely high, this model can be used to semi-
automatically expanding corpora by using active
learning techniques (Cohn et al., 1996). The
initial increase of performances is an interesting
starting point.
In the future, we aim at releasing the annotated
portion of the WIKI corpus to the community; we
will also carry out further experiments and refine
the feature spaces. Finally, as Wikipedia is a mul-
tilingual resource, we will use the WIKI method-
ology to semi-automatically build RTE corpora
for other languages.
</bodyText>
<sectionHeader confidence="0.998617" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996292333333333">
Steven Abney. 2002. Bootstrapping. In Proceedings of
40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 360–367, Philadelphia, Pennsyl-
vania, USA, July. Association for Computational Linguis-
tics.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo
Giampiccolo, and Idan Magnini, Bernardo Szpektor.
2006. The second pascal recognising textual entail-
ment challenge. In Proceedings of the Second PASCAL
Challenges Workshop on Recognising Textual Entailment,
Venice, Italy.
Avrim Blum and Tom Mitchell. 1998. Combining labeled
and unlabeled data with co-training. In COLT: Proceed-
ings of the Conference on Computational Learning The-
ory. Morgan Kaufmann.
</reference>
<figure confidence="0.994940428571429">
RTE2
RTE3
62
61.5
accuracy 61
60.5
60
</figure>
<page confidence="0.986501">
35
</page>
<reference confidence="0.999790883495145">
John Burger and Lisa Ferro. 2005. Generating an entailment
corpus from news headlines. In Proceedings of the ACL
Workshop on Empirical Modeling of Semantic Equiva-
lence and Entailment, pages 49–54, Ann Arbor, Michi-
gan, June. Association for Computational Linguistics.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. of the 1st NAACL, pages 132–139, Seat-
tle, Washington.
David A. Cohn, Zoubin Ghahramani, and Michael I. Jordan.
1996. Active learning with statistical models. Journal of
Artificial Intelligence Research, 4:129–145.
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. In In Proceedings
of the Joint SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large Corpora,
pages 100–110.
Robin Cooper, Dick Crouch, Jan Van Eijck, Chris Fox, Jo-
han Van Genabith, Jan Jaspars, Hans Kamp, David Mil-
ward, Manfred Pinkal, Massimo Poesio, and Steve Pul-
man. 1996. Using the framework. Technical Report LRE
62-051 D-16, The FraCaS Consortium. Technical report.
Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006.
The pascal recognising textual entailment challenge. In
Quionero-Candela et al., editor, LNAI 3944: MLCW2005,
pages 177–190, Milan, Italy. Springer-Verlag.
Marie-Catherine de Marneffe, Bill MacCartney, Trond
Grenager, Daniel Cer, Anna Rafferty, and Christopher
D. Manning. 2006. Learning to distinguish valid tex-
tual entailments. In Proceedings of the Second PASCAL
Challenges Workshop on Recognising Textual Entailment,
Venice, Italy.
A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Max-
imum likelihood from incomplete data via the em algo-
rithm. Journal of the Royal Statistical Society, Series B,
39(1):1–38.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill
Dolan. 2007. The third pascal recognizing textual en-
tailment challenge. In Proceedings of the ACL-PASCAL
Workshop on Textual Entailment and Paraphrasing, pages
1–9, Prague, June. Association for Computational Lin-
guistics.
Oren Glickman, Ido Dagan, and Moshe Koppel. 2005. Web
based probabilistic textual entailment. In Proceedings of
the 1st Pascal Challenge Workshop, Southampton, UK.
David Graff. 2003. English gigaword.
Andrew Hickl and Jeremy Bensley. 2007. A discourse
commitment-based framework for recognizing textual en-
tailment. In Proceedings of the ACL-PASCAL Workshop
on Textual Entailment and Paraphrasing, pages 171–176,
Prague, June. ACL.
Andrew Hickl, John Williams, Jeremy Bensley, Kirk
Roberts, Bryan Rink, and Ying Shi. 2006. Recognizing
textual entailment with LCCs GROUNDHOG system. In
Proceedings of the 2nd PASCAL Challenge Workshop on
RTE, Venice, Italy.
N. Japkowicz and S. Stephen. 2002. The class imbalance
problem: A systematic study. Intelligent Data Analysis,
6(5).
Thorsten Joachims. 1999. Making large-scale svm learning
practical. In B. Schlkopf, C. Burges, and A. Smola, edi-
tors, Advances in Kernel Methods-Support Vector Learn-
ing. MIT Press.
Frank Keller and Mirella Lapata. 2003. Using the web to
obtain frequencies for unseen bigrams. Computational
Linguistics, 29(3), September.
M. Kubat and S. Matwin. 1997. Addressing the curse of in-
balanced data sets: One-side sampleing. In Proceedings
of the 14th International Conference on Machine Learn-
ing, pages 179–186. Morgan Kaufmann.
Alessandro Moschitti and Fabio Massimo Zanzotto. 2007.
Fast and effective kernels for relational learning from
texts. In Proceedings of the International Conference of
Machine Learning (ICML), Corvallis, Oregon.
Jeremy Nicholson, Nicola Stokes, and Timothy Baldwin.
2006. Detecting entailment using an extended imple-
mentation of the basic elements overlap metric. In Pro-
ceedings of the Second PASCAL Challenges Workshop on
Recognising Textual Entailment, Venice, Italy.
Christian Siefkes. 2008. An Incrementally Trainable Statis-
tical Approach to Information Extraction. VDM Verlag,
Saarbrucken, Germany.
S. Siegel and Jr. N. J. Castellan. 1988. Nonparametric
Statistics for the Behavioral Sciences. McGraw-Hill.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and fast – but is it good? eval-
uating non-expert annotations for natural language tasks.
In Proceedings of the 2008 Conference on EmNLP, pages
254–263, Honolulu, Hawaii. ACL.
Annie Zaenen. submitted. Do give a penny for their
thoughts. Journal ofNatural Language Engineering.
Fabio Massimo Zanzotto and Lorenzo Dell’Arciprete. 2009.
Efficient kernels for sentence pair classification. In Con-
ference on Empirical Methods on Natural Language Pro-
cessing, pages 91–100, 6-7 August.
Fabio Massimo Zanzotto and Alessandro Moschitti. 2006.
Automatic learning of textual entailments with cross-pair
similarities. In Proceedings of the 21st Coling and 44th
ACL, pages 401–408, Sydney, Australia, July.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and Alessan-
dro Moschitti. 2009. A machine learning approach to
textual entailment recognition. NATURAL LANGUAGE
ENGINEERING, 15-04:551–582. Accepted for pubblica-
tion.
</reference>
<page confidence="0.998934">
36
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.884830">
<title confidence="0.999917">Expanding textual entailment corpora from Wikipedia using co-training</title>
<author confidence="0.999962">Fabio Massimo Zanzotto Marco Pennacchiotti</author>
<affiliation confidence="0.999147">University of Rome ”Tor Vergata” Yahoo! Lab</affiliation>
<email confidence="0.946316">zanzotto@info.uniroma2.itpennac@yahoo-inc.com</email>
<abstract confidence="0.995051">In this paper we propose a novel method to automatically extract large textual entailment datasets homogeneous to existing ones. The key idea is the combination of two intuitions: (1) the use of Wikipedia to extract a large set of textual entailment pairs; (2) the application of semisupervised machine learning methods to make the extracted dataset homogeneous to the existing ones. We report empirical evidence that our method successfully expands existing textual entailment corpora.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Association for Computational Linguistics.</title>
<date>2002</date>
<booktitle>Bootstrapping. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>360--367</pages>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="15067" citStr="Abney, 2002" startWordPosition="2456" endWordPosition="2458"> leverage the corpus even if unlabelled. As manual annotation is unfeasible, we choose the second solution. The goal is then to expand a labelled RTE challenge training set with the unlabelled WIKI, so that the performance of an RTE system can increase over an RTE test set. In the literature, several techniques have been proposed to use unlabelled data to expand a training labelled corpus, e.g. ExpectationMaximization (Dempster et al., 1977). We here apply the co-training technique, first proposed by (Blum and Mitchell, 1998) and then successfully leveraged and analyzed in different settings (Abney, 2002). Co-training can be applied when the unlabelled dataset allows two independent views on its instances (applicability condition). In this section, we first provide a short description of the co-training algorithm (Section 4.1). We then investigate if different RTE corpora conform to the applicability condition (Section 4.2). Finally, we show that our WIKI corpus conforms to the condition, and then apply co-training by creating two independent views (Section 4.3). 4.1 Co-training The co-training algorithm uses unlabelled data to increase classification performance, and to indirectly increasing </context>
</contexts>
<marker>Abney, 2002</marker>
<rawString>Steven Abney. 2002. Bootstrapping. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 360–367, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Bar-Haim</author>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
<author>Lisa Ferro</author>
<author>Danilo Giampiccolo</author>
<author>Idan Magnini</author>
<author>Bernardo Szpektor</author>
</authors>
<title>The second pascal recognising textual entailment challenge.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment,</booktitle>
<location>Venice, Italy.</location>
<contexts>
<context position="840" citStr="Bar-Haim et al., 2006" startWordPosition="117" endWordPosition="120">s paper we propose a novel method to automatically extract large textual entailment datasets homogeneous to existing ones. The key idea is the combination of two intuitions: (1) the use of Wikipedia to extract a large set of textual entailment pairs; (2) the application of semisupervised machine learning methods to make the extracted dataset homogeneous to the existing ones. We report empirical evidence that our method successfully expands existing textual entailment corpora. 1 Introduction Despite the growing success of the Recognizing Textual Entailment (RTE) challenges (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007), the accuracy of most textual entailment recognition systems are still below 60%. An intuitive way to improve performance is to provide systems with larger annotated datasets. This is especially true for machine learning systems, where the size of the training corpus is an important factor. As a consequence, several attempts have been made to train systems using larger datasets obtained by merging RTE corpora of different challenges. Unfortunately, experimental results show a significant decrease in accuracy (de Marneffe et al., 2006). There are two major reasons fo</context>
<context position="22919" citStr="Bar-Haim et al., 2006" startWordPosition="3769" endWordPosition="3772"> extracted original entries and revisions from the XML and wiki code, collecting an overall corpus of 20,000 (51, 52) pairs. We then randomly selected the final 3,000 pairs. news: A corpus of 1,600 examples obtained using the methods adopted for the LCC corpus, both for negative and positive examples (Hickl et al., 2006).3 We randomly divided the corpus in two parts: 800 training and 800 testing examples. Each set contains an equal number of 400 positive and negative pairs. As labelled corpora we use: RTE-1,RTE-2, and RTE-3: The corpora from the first three RTE challenges (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007). We use the standard split between training and testing. wiki: A manually annotated corpus of 2,000 examples from the WIKI corpus. Pairs have been annotated considering the original entry as the H and the revision as T. Noisy pairs containing vandalism or grammatical errors were removed (these accounts for about 19% of the examples). In all, the annotation produced 945 positive examples (strict entailments and paraphrases) and 669 negative examples (reverse strict entailments and contradictions). The annotation was carried out by two experienced researchers, each on</context>
</contexts>
<marker>Bar-Haim, Dagan, Dolan, Ferro, Giampiccolo, Magnini, Szpektor, 2006</marker>
<rawString>Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, and Idan Magnini, Bernardo Szpektor. 2006. The second pascal recognising textual entailment challenge. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In COLT: Proceedings of the Conference on Computational Learning Theory.</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="14986" citStr="Blum and Mitchell, 1998" startWordPosition="2443" endWordPosition="2446">to L2 and u2 to L1 Figure 2: General co-training algorithm or find an alternative strategy to leverage the corpus even if unlabelled. As manual annotation is unfeasible, we choose the second solution. The goal is then to expand a labelled RTE challenge training set with the unlabelled WIKI, so that the performance of an RTE system can increase over an RTE test set. In the literature, several techniques have been proposed to use unlabelled data to expand a training labelled corpus, e.g. ExpectationMaximization (Dempster et al., 1977). We here apply the co-training technique, first proposed by (Blum and Mitchell, 1998) and then successfully leveraged and analyzed in different settings (Abney, 2002). Co-training can be applied when the unlabelled dataset allows two independent views on its instances (applicability condition). In this section, we first provide a short description of the co-training algorithm (Section 4.1). We then investigate if different RTE corpora conform to the applicability condition (Section 4.2). Finally, we show that our WIKI corpus conforms to the condition, and then apply co-training by creating two independent views (Section 4.3). 4.1 Co-training The co-training algorithm uses unla</context>
<context position="16945" citStr="Blum and Mitchell, 1998" startWordPosition="2775" endWordPosition="2778">copied in two sets L1 and L2, used to train two different classifiers h1 and h2, respectively using views F1 and F2. The two classifiers are used to classify the unlabelled set U, obtaining two different classifications, U1 and U2. Then comes the cotraining step: the k-best classified instances in U1 are added to L2 and feed the learning of a new classifier h2 on the feature space F2. Similarly, the k-best instances in U2 are added to L1 and train a new classifier h1 on F1. The procedure repeats until a stopping condition is met. This can be either a fixed number of added unlabelled examples (Blum and Mitchell, 1998), the performance drop on a control set of labelled instances, or a filter on the disagreement of h1 and h2 in classifying U (Collins and Singer, 1999). The final outcome of co-training is the new set of labelled examples L1 U L2 and the two classifier h1 and h2, obtained from the last iteration. 4.2 Applicability condition on RTE corpora In order to leverage co-training for homogeneously expanding an RTE corpus, it is necessary to have a large unlabelled corpus which satisfies the applicability condition. Unfortunately, existing methodologies cannot guarantee the condition. For example, the c</context>
<context position="19414" citStr="Blum and Mitchell, 1998" startWordPosition="3191" endWordPosition="3194"> the actual textual content of S1 and S2. This view is typically available also in any other RTE corpus. • comment view: a set of features regarding the revision comment inserted by an author. This view represents “external” information (wrt. to the text fragments) which are peculiar of the WIKI corpus. These two views are most likely independent. Indeed, the content-pair view deals with the content of the Wikipedia revision, while the comment view describes the reason why a revision has been made. This setting is very similar to the original one proposed for co-training by Blum and Mitchell (Blum and Mitchell, 1998), where the target problem was the classification of web pages, and the two independent views on a page were (1) its content and (2) its hyperlinks. In the rest of this section we describe the feature spaces we adopt for the two independent views. 4.3.1 Content-pair view The content-pair view is the classical view used in RTE. The original entry S1 represents the Text T, while the revision S2 is the Hypothesis H. Any feature space of those reported in the textual entailment literature could be applied. We here adopt the space that represents first-order syntactic rewrite rules (FOSR), as descr</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In COLT: Proceedings of the Conference on Computational Learning Theory. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Burger</author>
<author>Lisa Ferro</author>
</authors>
<title>Generating an entailment corpus from news headlines.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment,</booktitle>
<pages>49--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="3234" citStr="Burger and Ferro, 2005" startWordPosition="491" endWordPosition="494">tailment. A key issue for the future development of RTE is then the creation of datasets fulfilling two properties: (1) large size; (2) homogeneity wrt. existing RTE corpora. The task of creating large datasets is unfeasible for human annotators. Collaborative annotation environments such as the Amazon Mechanical Turk1 can help to annotate pairs of sentences in positive or negative entailment (Zaenen, submitted; Snow et al., 2008). Yet, these environments can hardly solve the problem of finding relevant pairs of sentences. Completely automatic processes of dataset creation have been proposed (Burger and Ferro, 2005; Hickl et al., 2006). Unfortunately, these datasets are not homogeneous wrt. to the RTE datasets, as they are 1http://mturk.com 28 Beijing, August 2010 Proceedings of the 2nd Workshop on “Collaboratively Constructed Semantic Resources”, Coling 2010, pages 28–36, created using different methodologies. In this paper we propose a novel method to automatically extract entailment datasets which are guaranteed to be large and homogeneous to RTE ones. The key idea is the combination of two factors: (1) the use of Wikipedia as source of a large set of textual entailment pairs; (2) the application of </context>
<context position="4694" citStr="Burger and Ferro, 2005" startWordPosition="728" endWordPosition="731"> important properties that these corpora should have, and introduce our methodology to extract an RTE corpus from Wikipedia (the WIKI corpus), conforming to these properties. In Section 4 we describe how co-training techniques can be leveraged to make the WIKI corpus homogeneous to existing RTE corpora. In Section 5 we report empirical evidence that the combination of the WIKI corpus and co-training is successful. Finally, in Section 6 we draw final conclusions and outline future work. 2 Related Work The first attempt to automatically create large RTE corpora was proposed by Burger and Ferro (Burger and Ferro, 2005), with the MITRE corpus, a corpus of positive entailment examples extracted from the XIE section of the Gigaword news collection (Graff, 2003). The idea of the approach is that the headline and the first paragraph of a news article should be (near-)paraphrase. Authors then collect paragraph-headline pairs as Text (T) - Hypothesis (H) examples, where the headlines plays the role of H. The final corpus consists of 100,000 pairs, with an estimated accuracy of 70% – i.e. two annotators checked a sample of about 500 pairs, and verified that 30% of these were either false entailments or noisy pairs.</context>
</contexts>
<marker>Burger, Ferro, 2005</marker>
<rawString>John Burger and Lisa Ferro. 2005. Generating an entailment corpus from news headlines. In Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 49–54, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proc. of the 1st NAACL,</booktitle>
<pages>132--139</pages>
<location>Seattle, Washington.</location>
<contexts>
<context position="24265" citStr="Charniak, 2000" startWordPosition="3985" endWordPosition="3986">the headline - first paragraph extraction methodology. 4Annotators were initially trained on a small development corpus of 200 pairs. The inter-annotator agreement on this set, computed using the Kappa-statistics (Siegel and Castellan, 1988), was 0.60 corresponding to substantial agreement, S S NP X VP NP Y VP → p = l → r= VBP NP X owns bought NP Y VBP 33 The corpus has been randomly split in three equally numerous parts: development, training, and testing. We kept aside the development to design the features, while we used training and testing for the experiments. We use the Charniak Parser (Charniak, 2000) for parsing sentences, and SVM-light (Joachims, 1999) extended with the syntactic first-order rule kernels described in (Zanzotto and Moschitti, 2006; Moschitti and Zanzotto, 2007) for creating the FOSR feature space. 5.2 Experimental Results The first experiment aims at checking the quality of the WIKI corpus, by comparing the performance obtained by a standard RTE system over the corpus in exam with those obtained over any RTE challenge corpus. The hypothesis is that if performance is comparable, then the corpus in exam has the same complexity (and quality) as the RTE challenge corpora. We </context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proc. of the 1st NAACL, pages 132–139, Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Cohn</author>
<author>Zoubin Ghahramani</author>
<author>Michael I Jordan</author>
</authors>
<title>Active learning with statistical models.</title>
<date>1996</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>4--129</pages>
<marker>Cohn, Ghahramani, Jordan, 1996</marker>
<rawString>David A. Cohn, Zoubin Ghahramani, and Michael I. Jordan. 1996. Active learning with statistical models. Journal of Artificial Intelligence Research, 4:129–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Yoram Singer</author>
</authors>
<title>Unsupervised models for named entity classification. In</title>
<date>1999</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>100--110</pages>
<contexts>
<context position="17096" citStr="Collins and Singer, 1999" startWordPosition="2802" endWordPosition="2805">lassify the unlabelled set U, obtaining two different classifications, U1 and U2. Then comes the cotraining step: the k-best classified instances in U1 are added to L2 and feed the learning of a new classifier h2 on the feature space F2. Similarly, the k-best instances in U2 are added to L1 and train a new classifier h1 on F1. The procedure repeats until a stopping condition is met. This can be either a fixed number of added unlabelled examples (Blum and Mitchell, 1998), the performance drop on a control set of labelled instances, or a filter on the disagreement of h1 and h2 in classifying U (Collins and Singer, 1999). The final outcome of co-training is the new set of labelled examples L1 U L2 and the two classifier h1 and h2, obtained from the last iteration. 4.2 Applicability condition on RTE corpora In order to leverage co-training for homogeneously expanding an RTE corpus, it is necessary to have a large unlabelled corpus which satisfies the applicability condition. Unfortunately, existing methodologies cannot guarantee the condition. For example, the corpora from which the datasets of the RTE challenges were derived, were created from the output of applications performing specific tasks (e.g., Questi</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>Michael Collins and Yoram Singer. 1999. Unsupervised models for named entity classification. In In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 100–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robin Cooper</author>
<author>Dick Crouch</author>
<author>Jan Van Eijck</author>
<author>Chris Fox</author>
<author>Johan Van Genabith</author>
<author>Jan Jaspars</author>
<author>Hans Kamp</author>
<author>David Milward</author>
<author>Manfred Pinkal</author>
<author>Massimo Poesio</author>
<author>Steve Pulman</author>
</authors>
<title>Using the framework.</title>
<date>1996</date>
<tech>Technical Report LRE 62-051 D-16, The FraCaS Consortium. Technical report.</tech>
<marker>Cooper, Crouch, Van Eijck, Fox, Van Genabith, Jaspars, Kamp, Milward, Pinkal, Poesio, Pulman, 1996</marker>
<rawString>Robin Cooper, Dick Crouch, Jan Van Eijck, Chris Fox, Johan Van Genabith, Jan Jaspars, Hans Kamp, David Milward, Manfred Pinkal, Massimo Poesio, and Steve Pulman. 1996. Using the framework. Technical Report LRE 62-051 D-16, The FraCaS Consortium. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The pascal recognising textual entailment challenge.</title>
<date>2006</date>
<booktitle>LNAI 3944: MLCW2005,</booktitle>
<pages>177--190</pages>
<editor>In Quionero-Candela et al., editor,</editor>
<publisher>Springer-Verlag.</publisher>
<location>Milan, Italy.</location>
<contexts>
<context position="817" citStr="Dagan et al., 2006" startWordPosition="113" endWordPosition="116">.com Abstract In this paper we propose a novel method to automatically extract large textual entailment datasets homogeneous to existing ones. The key idea is the combination of two intuitions: (1) the use of Wikipedia to extract a large set of textual entailment pairs; (2) the application of semisupervised machine learning methods to make the extracted dataset homogeneous to the existing ones. We report empirical evidence that our method successfully expands existing textual entailment corpora. 1 Introduction Despite the growing success of the Recognizing Textual Entailment (RTE) challenges (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007), the accuracy of most textual entailment recognition systems are still below 60%. An intuitive way to improve performance is to provide systems with larger annotated datasets. This is especially true for machine learning systems, where the size of the training corpus is an important factor. As a consequence, several attempts have been made to train systems using larger datasets obtained by merging RTE corpora of different challenges. Unfortunately, experimental results show a significant decrease in accuracy (de Marneffe et al., 2006). There a</context>
<context position="2190" citStr="Dagan et al., 2006" startWordPosition="326" endWordPosition="329">us is an important factor for the applicability of supervised machine learning models, since examples with similar properties often imply more effective models. Unfortunately, the corpora of the four RTE challenges are not homogenous. Indeed, they model different properties of the textual entailment phenomenon, as they have been created using slightly (but significantly) different methodologies. For example, part of the RTE-1 dataset (Dagan et al., 2006) was created using comparable documents, where positive entailments have a lexical overlap higher than negative ones (Nicholson et al., 2006; Dagan et al., 2006). Comparable documents have not been used as a source of later RTE corpora, making RTE-1 odd with respect to other datasets. Corpus size. RTE corpora are relatively small in size (typically 800 pairs). The increase in size obtained by merging corpora from different challenges is not a viable solution. Much larger datasets, of one or more order of magnitude, are needed to capture the complex properties characterizing entailment. A key issue for the future development of RTE is then the creation of datasets fulfilling two properties: (1) large size; (2) homogeneity wrt. existing RTE corpora. The</context>
<context position="22896" citStr="Dagan et al., 2006" startWordPosition="3765" endWordPosition="3768"> religion issues. We extracted original entries and revisions from the XML and wiki code, collecting an overall corpus of 20,000 (51, 52) pairs. We then randomly selected the final 3,000 pairs. news: A corpus of 1,600 examples obtained using the methods adopted for the LCC corpus, both for negative and positive examples (Hickl et al., 2006).3 We randomly divided the corpus in two parts: 800 training and 800 testing examples. Each set contains an equal number of 400 positive and negative pairs. As labelled corpora we use: RTE-1,RTE-2, and RTE-3: The corpora from the first three RTE challenges (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007). We use the standard split between training and testing. wiki: A manually annotated corpus of 2,000 examples from the WIKI corpus. Pairs have been annotated considering the original entry as the H and the revision as T. Noisy pairs containing vandalism or grammatical errors were removed (these accounts for about 19% of the examples). In all, the annotation produced 945 positive examples (strict entailments and paraphrases) and 669 negative examples (reverse strict entailments and contradictions). The annotation was carried out by two experienc</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The pascal recognising textual entailment challenge. In Quionero-Candela et al., editor, LNAI 3944: MLCW2005, pages 177–190, Milan, Italy. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Trond Grenager</author>
<author>Daniel Cer</author>
<author>Anna Rafferty</author>
<author>Christopher D Manning</author>
</authors>
<title>Learning to distinguish valid textual entailments.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment,</booktitle>
<location>Venice, Italy.</location>
<marker>de Marneffe, MacCartney, Grenager, Cer, Rafferty, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, Trond Grenager, Daniel Cer, Anna Rafferty, and Christopher D. Manning. 2006. Learning to distinguish valid textual entailments. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the em algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society, Series B,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="14900" citStr="Dempster et al., 1977" startWordPosition="2430" endWordPosition="2433">nd remove k-best classified examples u1 and u2 from respectively U1 and U2 – add u1 to L2 and u2 to L1 Figure 2: General co-training algorithm or find an alternative strategy to leverage the corpus even if unlabelled. As manual annotation is unfeasible, we choose the second solution. The goal is then to expand a labelled RTE challenge training set with the unlabelled WIKI, so that the performance of an RTE system can increase over an RTE test set. In the literature, several techniques have been proposed to use unlabelled data to expand a training labelled corpus, e.g. ExpectationMaximization (Dempster et al., 1977). We here apply the co-training technique, first proposed by (Blum and Mitchell, 1998) and then successfully leveraged and analyzed in different settings (Abney, 2002). Co-training can be applied when the unlabelled dataset allows two independent views on its instances (applicability condition). In this section, we first provide a short description of the co-training algorithm (Section 4.1). We then investigate if different RTE corpora conform to the applicability condition (Section 4.2). Finally, we show that our WIKI corpus conforms to the condition, and then apply co-training by creating tw</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society, Series B, 39(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Giampiccolo</author>
<author>Bernardo Magnini</author>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
</authors>
<title>The third pascal recognizing textual entailment challenge.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing,</booktitle>
<pages>1--9</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague,</location>
<contexts>
<context position="867" citStr="Giampiccolo et al., 2007" startWordPosition="121" endWordPosition="124">vel method to automatically extract large textual entailment datasets homogeneous to existing ones. The key idea is the combination of two intuitions: (1) the use of Wikipedia to extract a large set of textual entailment pairs; (2) the application of semisupervised machine learning methods to make the extracted dataset homogeneous to the existing ones. We report empirical evidence that our method successfully expands existing textual entailment corpora. 1 Introduction Despite the growing success of the Recognizing Textual Entailment (RTE) challenges (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007), the accuracy of most textual entailment recognition systems are still below 60%. An intuitive way to improve performance is to provide systems with larger annotated datasets. This is especially true for machine learning systems, where the size of the training corpus is an important factor. As a consequence, several attempts have been made to train systems using larger datasets obtained by merging RTE corpora of different challenges. Unfortunately, experimental results show a significant decrease in accuracy (de Marneffe et al., 2006). There are two major reasons for this counter-intuitive re</context>
<context position="22946" citStr="Giampiccolo et al., 2007" startWordPosition="3773" endWordPosition="3776">ries and revisions from the XML and wiki code, collecting an overall corpus of 20,000 (51, 52) pairs. We then randomly selected the final 3,000 pairs. news: A corpus of 1,600 examples obtained using the methods adopted for the LCC corpus, both for negative and positive examples (Hickl et al., 2006).3 We randomly divided the corpus in two parts: 800 training and 800 testing examples. Each set contains an equal number of 400 positive and negative pairs. As labelled corpora we use: RTE-1,RTE-2, and RTE-3: The corpora from the first three RTE challenges (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007). We use the standard split between training and testing. wiki: A manually annotated corpus of 2,000 examples from the WIKI corpus. Pairs have been annotated considering the original entry as the H and the revision as T. Noisy pairs containing vandalism or grammatical errors were removed (these accounts for about 19% of the examples). In all, the annotation produced 945 positive examples (strict entailments and paraphrases) and 669 negative examples (reverse strict entailments and contradictions). The annotation was carried out by two experienced researchers, each one annotating half of the co</context>
</contexts>
<marker>Giampiccolo, Magnini, Dagan, Dolan, 2007</marker>
<rawString>Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2007. The third pascal recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 1–9, Prague, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Glickman</author>
<author>Ido Dagan</author>
<author>Moshe Koppel</author>
</authors>
<title>Web based probabilistic textual entailment.</title>
<date>2005</date>
<booktitle>In Proceedings of the 1st Pascal Challenge Workshop,</booktitle>
<location>Southampton, UK. David Graff.</location>
<note>English gigaword.</note>
<contexts>
<context position="9522" citStr="Glickman et al. (2005)" startWordPosition="1511" endWordPosition="1514">kowicz and Stephen, 2002; Kubat and Matwin, 1997). Also, the positive and negative subsets of the corpus should be consistent, i.e. created using the same methodology. If this property is not preserved, the risk is a learning system building a model which separates positive and negatives according to the properties characterizing the two methodologies, instead of those of the entailment phenomenon. (3) Not biased on lexical overlap. A major criticism on the RTE-1 dataset was that it contained too many positive examples with high lexical overlap wrt. negative examples (Nicholson et al., 2006). Glickman et al. (2005) show that an RTE system using word overlap to decide entailment, surprisingly achieves an accuracy of 0.57 on RTE1 test set. These performances are comparable to those obtained on the same dataset by more sophisticated and principled systems. Learning from this experience, a good corpus for RTE should avoid imbalances on lexical overlap. (4) Homogeneous to existing RTE corpora. Corpus homogeneity is a key property for any machine learning approach (Siefkes, 2008). A new corpus for RTE should then model the same or similar entailments types of the reliable existing ones (e.g., those of the RTE</context>
</contexts>
<marker>Glickman, Dagan, Koppel, 2005</marker>
<rawString>Oren Glickman, Ido Dagan, and Moshe Koppel. 2005. Web based probabilistic textual entailment. In Proceedings of the 1st Pascal Challenge Workshop, Southampton, UK. David Graff. 2003. English gigaword.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Hickl</author>
<author>Jeremy Bensley</author>
</authors>
<title>A discourse commitment-based framework for recognizing textual entailment.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing,</booktitle>
<pages>171--176</pages>
<publisher>ACL.</publisher>
<location>Prague,</location>
<contexts>
<context position="6613" citStr="Hickl and Bensley (2007)" startWordPosition="1046" endWordPosition="1049">ntial sentences including mentions of a same named entity (98.000 pairs); (2) selecting pairs of sentences connected by words such as even though, although, otherwise, but (21,000 pairs). Estimated accuracy for the two techniques is respectively 97% and 94%. Hickl and colleagues show that expanding the RTE-2 training set with the LCC corpus (the expansion factor is 125), their RTE system improves 10% accuracy. This suggests that by expanding with a large and balanced corpus, entailment recognition performance drastically improves. This intuition is later contradicted in a second experiment by Hickl and Bensley (2007). Authors use the LCC corpus with the RTE-3 training set to train a new RTE system, showing an improvement in accuracy of less than 1% wrt. the RTE-3 training alone. Overall, evidence suggests that automatic expansion of the RTE corpora do not always lead to performance improvement. This highly depends on how balanced the corpus is, on the RTE system adopted, and on the specific RTE dataset that is expanded. 3 Extracting the WIKI corpus In this section we outline some of the properties that a reliable corpus for RTE should have (Section 3.1), and show that a corpus extracted from Wikipedia con</context>
</contexts>
<marker>Hickl, Bensley, 2007</marker>
<rawString>Andrew Hickl and Jeremy Bensley. 2007. A discourse commitment-based framework for recognizing textual entailment. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 171–176, Prague, June. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Hickl</author>
<author>John Williams</author>
<author>Jeremy Bensley</author>
<author>Kirk Roberts</author>
<author>Bryan Rink</author>
<author>Ying Shi</author>
</authors>
<title>Recognizing textual entailment with LCCs GROUNDHOG system.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2nd PASCAL Challenge Workshop on RTE,</booktitle>
<location>Venice, Italy.</location>
<contexts>
<context position="3255" citStr="Hickl et al., 2006" startWordPosition="495" endWordPosition="498">r the future development of RTE is then the creation of datasets fulfilling two properties: (1) large size; (2) homogeneity wrt. existing RTE corpora. The task of creating large datasets is unfeasible for human annotators. Collaborative annotation environments such as the Amazon Mechanical Turk1 can help to annotate pairs of sentences in positive or negative entailment (Zaenen, submitted; Snow et al., 2008). Yet, these environments can hardly solve the problem of finding relevant pairs of sentences. Completely automatic processes of dataset creation have been proposed (Burger and Ferro, 2005; Hickl et al., 2006). Unfortunately, these datasets are not homogeneous wrt. to the RTE datasets, as they are 1http://mturk.com 28 Beijing, August 2010 Proceedings of the 2nd Workshop on “Collaboratively Constructed Semantic Resources”, Coling 2010, pages 28–36, created using different methodologies. In this paper we propose a novel method to automatically extract entailment datasets which are guaranteed to be large and homogeneous to RTE ones. The key idea is the combination of two factors: (1) the use of Wikipedia as source of a large set of textual entailment pairs; (2) the application of semisupervised machin</context>
<context position="5544" citStr="Hickl et al. (2006)" startWordPosition="873" endWordPosition="876"> should be (near-)paraphrase. Authors then collect paragraph-headline pairs as Text (T) - Hypothesis (H) examples, where the headlines plays the role of H. The final corpus consists of 100,000 pairs, with an estimated accuracy of 70% – i.e. two annotators checked a sample of about 500 pairs, and verified that 30% of these were either false entailments or noisy pairs. The major limitation of the Burger and Ferro (Burger and Ferro, 2005)’s approach is that the final corpus consist only of positive examples. Because of this imbalance, the corpus cannot be positively used by RTE learning systems. Hickl et al. (2006) propose a solution to the problem, providing a methodology to extract both positive and negative pairs (the LCC corpus). A positive corpus consisting of 101,000 pairs is extracted similarly to (Burger and Ferro, 2005). Corpus accuracy is estimated on a sample of 2,500 examples, achieving 92% (i.e. almost all examples are positives), 22 points higher than Burger and Ferro. A negative corpus of 119,000 is extracted either: (1) selecting sequential sentences including mentions of a same named entity (98.000 pairs); (2) selecting pairs of sentences connected by words such as even though, although</context>
<context position="22620" citStr="Hickl et al., 2006" startWordPosition="3719" endWordPosition="3722"> both manually labelled and unlabelled corpora. As unlabelled corpora we adopt: wiki unlabelled: An unlabelled WIKI corpus of about 3,000 examples. The corpus has been built by downloading 40,000 Wikipedia pages dealing with 800 entries about politics, scientific theories, and religion issues. We extracted original entries and revisions from the XML and wiki code, collecting an overall corpus of 20,000 (51, 52) pairs. We then randomly selected the final 3,000 pairs. news: A corpus of 1,600 examples obtained using the methods adopted for the LCC corpus, both for negative and positive examples (Hickl et al., 2006).3 We randomly divided the corpus in two parts: 800 training and 800 testing examples. Each set contains an equal number of 400 positive and negative pairs. As labelled corpora we use: RTE-1,RTE-2, and RTE-3: The corpora from the first three RTE challenges (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007). We use the standard split between training and testing. wiki: A manually annotated corpus of 2,000 examples from the WIKI corpus. Pairs have been annotated considering the original entry as the H and the revision as T. Noisy pairs containing vandalism or grammatical error</context>
<context position="27633" citStr="Hickl et al., 2006" startWordPosition="4547" endWordPosition="4550"> set (namely, RTE-2 test). Results are reported in Table 1. The higher performance is the one of the system when trained on RTE-2 training set (second row) – i.e. a corpus completely homogeneous to RTE-2 would produce the same performance as RTE-2 training. As expected, the models learnt on RTE-1 and RTE-3 perform worse (third and fourth rows): in particular, RTE-1 seems extremely different from RTE-2, as results show. The wiki corpus is more similar to RTE-2 than the news corpus, i.e. performance are higher. Yet, it is quite surprising that the news corpus yields to a performance drop as in (Hickl et al., 2006) it shows a high performance increase. The expansion of RTE-2 with the above corpora (seventh-tenth rows) lead to a drop in performance, suggesting that none of the corpora is completely homogeneous to RTE-2. Yet, the performance drop of the wiki corpus (RTE-2 + 34 0 20 40 60 80 100 unlabelled examples Figure 3: Co-training accuracy curve on the two corpora. wiki) is comparable to the performance drop obtained using the other two RTE corpora (RTE-2 + RTE-1 and RTE-2 + RTE-3). This indicates that wiki is more homogeneous to RTE than news – i.e. it contains (T, H) pairs that are similar to the R</context>
</contexts>
<marker>Hickl, Williams, Bensley, Roberts, Rink, Shi, 2006</marker>
<rawString>Andrew Hickl, John Williams, Jeremy Bensley, Kirk Roberts, Bryan Rink, and Ying Shi. 2006. Recognizing textual entailment with LCCs GROUNDHOG system. In Proceedings of the 2nd PASCAL Challenge Workshop on RTE, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Japkowicz</author>
<author>S Stephen</author>
</authors>
<title>The class imbalance problem: A systematic study.</title>
<date>2002</date>
<journal>Intelligent Data Analysis,</journal>
<volume>6</volume>
<issue>5</issue>
<contexts>
<context position="8924" citStr="Japkowicz and Stephen, 2002" startWordPosition="1415" endWordPosition="1418">exts, such as news and every day conversations. A reliable RTE corpus should preserve this important property, i.e. it should be rich in entailment types whose distribution in the corpus is similar to that in real texts; and should not include unrepresentative hand-crafted prototypical examples. (2) Balanced and consistent. A reliable corpus should be balanced, i.e. composed by an equal or comparable number of positive and negative examples. This is particularly critical for RTE systems based on machine learning: highly imbalanced class distributions often result in poor learning performance (Japkowicz and Stephen, 2002; Kubat and Matwin, 1997). Also, the positive and negative subsets of the corpus should be consistent, i.e. created using the same methodology. If this property is not preserved, the risk is a learning system building a model which separates positive and negatives according to the properties characterizing the two methodologies, instead of those of the entailment phenomenon. (3) Not biased on lexical overlap. A major criticism on the RTE-1 dataset was that it contained too many positive examples with high lexical overlap wrt. negative examples (Nicholson et al., 2006). Glickman et al. (2005) s</context>
</contexts>
<marker>Japkowicz, Stephen, 2002</marker>
<rawString>N. Japkowicz and S. Stephen. 2002. The class imbalance problem: A systematic study. Intelligent Data Analysis, 6(5).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale svm learning practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods-Support Vector Learning.</booktitle>
<editor>In B. Schlkopf, C. Burges, and A. Smola, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="24319" citStr="Joachims, 1999" startWordPosition="3992" endWordPosition="3993"> 4Annotators were initially trained on a small development corpus of 200 pairs. The inter-annotator agreement on this set, computed using the Kappa-statistics (Siegel and Castellan, 1988), was 0.60 corresponding to substantial agreement, S S NP X VP NP Y VP → p = l → r= VBP NP X owns bought NP Y VBP 33 The corpus has been randomly split in three equally numerous parts: development, training, and testing. We kept aside the development to design the features, while we used training and testing for the experiments. We use the Charniak Parser (Charniak, 2000) for parsing sentences, and SVM-light (Joachims, 1999) extended with the syntactic first-order rule kernels described in (Zanzotto and Moschitti, 2006; Moschitti and Zanzotto, 2007) for creating the FOSR feature space. 5.2 Experimental Results The first experiment aims at checking the quality of the WIKI corpus, by comparing the performance obtained by a standard RTE system over the corpus in exam with those obtained over any RTE challenge corpus. The hypothesis is that if performance is comparable, then the corpus in exam has the same complexity (and quality) as the RTE challenge corpora. We then independently experiment with the wiki and the ne</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale svm learning practical. In B. Schlkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods-Support Vector Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Keller</author>
<author>Mirella Lapata</author>
</authors>
<title>Using the web to obtain frequencies for unseen bigrams.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>3</issue>
<contexts>
<context position="14040" citStr="Keller and Lapata, 2003" startWordPosition="2278" endWordPosition="2281"> very small, the comment can be used to determine if S1 and S2 are in entailment or not. In the following section we show how we leverage comments to make the WIKI corpus homogeneous to those of the RTE challenges. 4 Expanding the RTE corpus with WIKI using co-training Unlike the LCC corpus where negative and positive examples are clearly separated, the WIKI corpus mixes the two sets – i.e. it is unlabelled. In order to exploit the WIKI corpus in the RTE task, one should either manually annotate the corpus, 2It has been shown that web documents (as Wikipedia) are reliable samples of language (Keller and Lapata, 2003). CO-TRAINING ALGORITHM(L,U,k) returns h1,h2,L1,L2 set L1 = L2 = L while stopping condition is not met – learn h1 on F1 from L1, and learn h2 on F1 from L2, – classify U with h1 obtaining U1, and classify U with h2 obtaining U2 – select and remove k-best classified examples u1 and u2 from respectively U1 and U2 – add u1 to L2 and u2 to L1 Figure 2: General co-training algorithm or find an alternative strategy to leverage the corpus even if unlabelled. As manual annotation is unfeasible, we choose the second solution. The goal is then to expand a labelled RTE challenge training set with the unl</context>
</contexts>
<marker>Keller, Lapata, 2003</marker>
<rawString>Frank Keller and Mirella Lapata. 2003. Using the web to obtain frequencies for unseen bigrams. Computational Linguistics, 29(3), September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kubat</author>
<author>S Matwin</author>
</authors>
<title>Addressing the curse of inbalanced data sets: One-side sampleing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 14th International Conference on Machine Learning,</booktitle>
<pages>179--186</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="8949" citStr="Kubat and Matwin, 1997" startWordPosition="1419" endWordPosition="1422">day conversations. A reliable RTE corpus should preserve this important property, i.e. it should be rich in entailment types whose distribution in the corpus is similar to that in real texts; and should not include unrepresentative hand-crafted prototypical examples. (2) Balanced and consistent. A reliable corpus should be balanced, i.e. composed by an equal or comparable number of positive and negative examples. This is particularly critical for RTE systems based on machine learning: highly imbalanced class distributions often result in poor learning performance (Japkowicz and Stephen, 2002; Kubat and Matwin, 1997). Also, the positive and negative subsets of the corpus should be consistent, i.e. created using the same methodology. If this property is not preserved, the risk is a learning system building a model which separates positive and negatives according to the properties characterizing the two methodologies, instead of those of the entailment phenomenon. (3) Not biased on lexical overlap. A major criticism on the RTE-1 dataset was that it contained too many positive examples with high lexical overlap wrt. negative examples (Nicholson et al., 2006). Glickman et al. (2005) show that an RTE system us</context>
</contexts>
<marker>Kubat, Matwin, 1997</marker>
<rawString>M. Kubat and S. Matwin. 1997. Addressing the curse of inbalanced data sets: One-side sampleing. In Proceedings of the 14th International Conference on Machine Learning, pages 179–186. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Fabio Massimo Zanzotto</author>
</authors>
<title>Fast and effective kernels for relational learning from texts.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference of Machine Learning (ICML),</booktitle>
<location>Corvallis, Oregon.</location>
<contexts>
<context position="20494" citStr="Moschitti and Zanzotto, 2007" startWordPosition="3377" endWordPosition="3380"> in the textual entailment literature could be applied. We here adopt the space that represents first-order syntactic rewrite rules (FOSR), as described in (Zanzotto and Moschitti, 2006). In this feature space, each feature represents a syntactic first-order or 32 grounded rewrite rule. For example, the rule: is represented by the feature &lt; l, r &gt;. A (T, H) pair activates a feature if it unifies with the related rule. A detailed discussion of the FOSR feature space is given in (Zanzotto et al., 2009) and efficient algorithms for the computation of the related kernel functions can be found in (Moschitti and Zanzotto, 2007; Zanzotto and Dell’Arciprete, 2009). 4.4 Comment view A review comment is typically a textual fragment describing the reason why an author has decided to make a revision. In most cases the comment is not a well-formed sentence, as authors tend to use informal slang expressions and abbreviations (e.g. “details: Trelew Massacre; cat: Dirty War, copyedit”, “removed a POV vandalism by Spylab”, “dab ba:clean up using Project:AWB”). In these cases, where syntactic analysis would mostly fail, it is advisable to use simpler surface approaches to build the feature space. We then use a standard bag-of-</context>
<context position="24446" citStr="Moschitti and Zanzotto, 2007" startWordPosition="4007" endWordPosition="4010">this set, computed using the Kappa-statistics (Siegel and Castellan, 1988), was 0.60 corresponding to substantial agreement, S S NP X VP NP Y VP → p = l → r= VBP NP X owns bought NP Y VBP 33 The corpus has been randomly split in three equally numerous parts: development, training, and testing. We kept aside the development to design the features, while we used training and testing for the experiments. We use the Charniak Parser (Charniak, 2000) for parsing sentences, and SVM-light (Joachims, 1999) extended with the syntactic first-order rule kernels described in (Zanzotto and Moschitti, 2006; Moschitti and Zanzotto, 2007) for creating the FOSR feature space. 5.2 Experimental Results The first experiment aims at checking the quality of the WIKI corpus, by comparing the performance obtained by a standard RTE system over the corpus in exam with those obtained over any RTE challenge corpus. The hypothesis is that if performance is comparable, then the corpus in exam has the same complexity (and quality) as the RTE challenge corpora. We then independently experiment with the wiki and the news corpora with the training-test splits reported in Section 5.1. As RTE system we adopt an SVM model learnt on the FOSR featur</context>
</contexts>
<marker>Moschitti, Zanzotto, 2007</marker>
<rawString>Alessandro Moschitti and Fabio Massimo Zanzotto. 2007. Fast and effective kernels for relational learning from texts. In Proceedings of the International Conference of Machine Learning (ICML), Corvallis, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeremy Nicholson</author>
<author>Nicola Stokes</author>
<author>Timothy Baldwin</author>
</authors>
<title>Detecting entailment using an extended implementation of the basic elements overlap metric.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment,</booktitle>
<location>Venice, Italy.</location>
<contexts>
<context position="2169" citStr="Nicholson et al., 2006" startWordPosition="321" endWordPosition="325">ity of the training corpus is an important factor for the applicability of supervised machine learning models, since examples with similar properties often imply more effective models. Unfortunately, the corpora of the four RTE challenges are not homogenous. Indeed, they model different properties of the textual entailment phenomenon, as they have been created using slightly (but significantly) different methodologies. For example, part of the RTE-1 dataset (Dagan et al., 2006) was created using comparable documents, where positive entailments have a lexical overlap higher than negative ones (Nicholson et al., 2006; Dagan et al., 2006). Comparable documents have not been used as a source of later RTE corpora, making RTE-1 odd with respect to other datasets. Corpus size. RTE corpora are relatively small in size (typically 800 pairs). The increase in size obtained by merging corpora from different challenges is not a viable solution. Much larger datasets, of one or more order of magnitude, are needed to capture the complex properties characterizing entailment. A key issue for the future development of RTE is then the creation of datasets fulfilling two properties: (1) large size; (2) homogeneity wrt. exis</context>
<context position="9498" citStr="Nicholson et al., 2006" startWordPosition="1507" endWordPosition="1510">learning performance (Japkowicz and Stephen, 2002; Kubat and Matwin, 1997). Also, the positive and negative subsets of the corpus should be consistent, i.e. created using the same methodology. If this property is not preserved, the risk is a learning system building a model which separates positive and negatives according to the properties characterizing the two methodologies, instead of those of the entailment phenomenon. (3) Not biased on lexical overlap. A major criticism on the RTE-1 dataset was that it contained too many positive examples with high lexical overlap wrt. negative examples (Nicholson et al., 2006). Glickman et al. (2005) show that an RTE system using word overlap to decide entailment, surprisingly achieves an accuracy of 0.57 on RTE1 test set. These performances are comparable to those obtained on the same dataset by more sophisticated and principled systems. Learning from this experience, a good corpus for RTE should avoid imbalances on lexical overlap. (4) Homogeneous to existing RTE corpora. Corpus homogeneity is a key property for any machine learning approach (Siefkes, 2008). A new corpus for RTE should then model the same or similar entailments types of the reliable existing ones</context>
</contexts>
<marker>Nicholson, Stokes, Baldwin, 2006</marker>
<rawString>Jeremy Nicholson, Nicola Stokes, and Timothy Baldwin. 2006. Detecting entailment using an extended implementation of the basic elements overlap metric. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Siefkes</author>
</authors>
<title>An Incrementally Trainable Statistical Approach to Information Extraction.</title>
<date>2008</date>
<publisher>VDM Verlag,</publisher>
<location>Saarbrucken, Germany.</location>
<contexts>
<context position="1536" citStr="Siefkes, 2008" startWordPosition="227" endWordPosition="228">ystems are still below 60%. An intuitive way to improve performance is to provide systems with larger annotated datasets. This is especially true for machine learning systems, where the size of the training corpus is an important factor. As a consequence, several attempts have been made to train systems using larger datasets obtained by merging RTE corpora of different challenges. Unfortunately, experimental results show a significant decrease in accuracy (de Marneffe et al., 2006). There are two major reasons for this counter-intuitive result: Homogeneity. As indicated by many studies (e.g. (Siefkes, 2008)), homogeneity of the training corpus is an important factor for the applicability of supervised machine learning models, since examples with similar properties often imply more effective models. Unfortunately, the corpora of the four RTE challenges are not homogenous. Indeed, they model different properties of the textual entailment phenomenon, as they have been created using slightly (but significantly) different methodologies. For example, part of the RTE-1 dataset (Dagan et al., 2006) was created using comparable documents, where positive entailments have a lexical overlap higher than nega</context>
<context position="9990" citStr="Siefkes, 2008" startWordPosition="1588" endWordPosition="1589">was that it contained too many positive examples with high lexical overlap wrt. negative examples (Nicholson et al., 2006). Glickman et al. (2005) show that an RTE system using word overlap to decide entailment, surprisingly achieves an accuracy of 0.57 on RTE1 test set. These performances are comparable to those obtained on the same dataset by more sophisticated and principled systems. Learning from this experience, a good corpus for RTE should avoid imbalances on lexical overlap. (4) Homogeneous to existing RTE corpora. Corpus homogeneity is a key property for any machine learning approach (Siefkes, 2008). A new corpus for RTE should then model the same or similar entailments types of the reliable existing ones (e.g., those of the RTE challenges). If this is not the case, RTE system will be unable to learn a coherent model, thus resulting in a decrease in performance. The MITRE corpus satisfies property (1), but does not (2) and (3), as it is highly imbalanced (it contains mostly positive examples), and is fairly biased on lexical overlap, as most examples of headline-paragraph pairs have many words in common. The LCC corpus suffers the problem of inconsistency, as positive and negative exampl</context>
</contexts>
<marker>Siefkes, 2008</marker>
<rawString>Christian Siefkes. 2008. An Incrementally Trainable Statistical Approach to Information Extraction. VDM Verlag, Saarbrucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N J Castellan</author>
</authors>
<title>Nonparametric Statistics for the Behavioral Sciences.</title>
<date>1988</date>
<publisher>McGraw-Hill.</publisher>
<contexts>
<context position="23891" citStr="Castellan, 1988" startWordPosition="3915" endWordPosition="3917">mples). In all, the annotation produced 945 positive examples (strict entailments and paraphrases) and 669 negative examples (reverse strict entailments and contradictions). The annotation was carried out by two experienced researchers, each one annotating half of the corpus. Annotation guidelines follow those used for the RTE challenges.4 3For negative examples, we adopt the headline - first paragraph extraction methodology. 4Annotators were initially trained on a small development corpus of 200 pairs. The inter-annotator agreement on this set, computed using the Kappa-statistics (Siegel and Castellan, 1988), was 0.60 corresponding to substantial agreement, S S NP X VP NP Y VP → p = l → r= VBP NP X owns bought NP Y VBP 33 The corpus has been randomly split in three equally numerous parts: development, training, and testing. We kept aside the development to design the features, while we used training and testing for the experiments. We use the Charniak Parser (Charniak, 2000) for parsing sentences, and SVM-light (Joachims, 1999) extended with the syntactic first-order rule kernels described in (Zanzotto and Moschitti, 2006; Moschitti and Zanzotto, 2007) for creating the FOSR feature space. 5.2 Exp</context>
</contexts>
<marker>Castellan, 1988</marker>
<rawString>S. Siegel and Jr. N. J. Castellan. 1988. Nonparametric Statistics for the Behavioral Sciences. McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Ng</author>
</authors>
<title>Cheap and fast – but is it good? evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on EmNLP,</booktitle>
<pages>254--263</pages>
<publisher>ACL.</publisher>
<location>Honolulu, Hawaii.</location>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Ng. 2008. Cheap and fast – but is it good? evaluating non-expert annotations for natural language tasks. In Proceedings of the 2008 Conference on EmNLP, pages 254–263, Honolulu, Hawaii. ACL.</rawString>
</citation>
<citation valid="false">
<authors>
<author>submitted</author>
</authors>
<title>Do give a penny for their thoughts.</title>
<journal>Journal ofNatural Language Engineering.</journal>
<marker>submitted, </marker>
<rawString>Annie Zaenen. submitted. Do give a penny for their thoughts. Journal ofNatural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Lorenzo Dell’Arciprete</author>
</authors>
<title>Efficient kernels for sentence pair classification.</title>
<date>2009</date>
<booktitle>In Conference on Empirical Methods on Natural Language Processing,</booktitle>
<pages>91--100</pages>
<marker>Zanzotto, Dell’Arciprete, 2009</marker>
<rawString>Fabio Massimo Zanzotto and Lorenzo Dell’Arciprete. 2009. Efficient kernels for sentence pair classification. In Conference on Empirical Methods on Natural Language Processing, pages 91–100, 6-7 August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Automatic learning of textual entailments with cross-pair similarities.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st Coling and 44th ACL,</booktitle>
<pages>401--408</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="20052" citStr="Zanzotto and Moschitti, 2006" startWordPosition="3298" endWordPosition="3302">he target problem was the classification of web pages, and the two independent views on a page were (1) its content and (2) its hyperlinks. In the rest of this section we describe the feature spaces we adopt for the two independent views. 4.3.1 Content-pair view The content-pair view is the classical view used in RTE. The original entry S1 represents the Text T, while the revision S2 is the Hypothesis H. Any feature space of those reported in the textual entailment literature could be applied. We here adopt the space that represents first-order syntactic rewrite rules (FOSR), as described in (Zanzotto and Moschitti, 2006). In this feature space, each feature represents a syntactic first-order or 32 grounded rewrite rule. For example, the rule: is represented by the feature &lt; l, r &gt;. A (T, H) pair activates a feature if it unifies with the related rule. A detailed discussion of the FOSR feature space is given in (Zanzotto et al., 2009) and efficient algorithms for the computation of the related kernel functions can be found in (Moschitti and Zanzotto, 2007; Zanzotto and Dell’Arciprete, 2009). 4.4 Comment view A review comment is typically a textual fragment describing the reason why an author has decided to mak</context>
<context position="24415" citStr="Zanzotto and Moschitti, 2006" startWordPosition="4003" endWordPosition="4006"> inter-annotator agreement on this set, computed using the Kappa-statistics (Siegel and Castellan, 1988), was 0.60 corresponding to substantial agreement, S S NP X VP NP Y VP → p = l → r= VBP NP X owns bought NP Y VBP 33 The corpus has been randomly split in three equally numerous parts: development, training, and testing. We kept aside the development to design the features, while we used training and testing for the experiments. We use the Charniak Parser (Charniak, 2000) for parsing sentences, and SVM-light (Joachims, 1999) extended with the syntactic first-order rule kernels described in (Zanzotto and Moschitti, 2006; Moschitti and Zanzotto, 2007) for creating the FOSR feature space. 5.2 Experimental Results The first experiment aims at checking the quality of the WIKI corpus, by comparing the performance obtained by a standard RTE system over the corpus in exam with those obtained over any RTE challenge corpus. The hypothesis is that if performance is comparable, then the corpus in exam has the same complexity (and quality) as the RTE challenge corpora. We then independently experiment with the wiki and the news corpora with the training-test splits reported in Section 5.1. As RTE system we adopt an SVM </context>
</contexts>
<marker>Zanzotto, Moschitti, 2006</marker>
<rawString>Fabio Massimo Zanzotto and Alessandro Moschitti. 2006. Automatic learning of textual entailments with cross-pair similarities. In Proceedings of the 21st Coling and 44th ACL, pages 401–408, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Marco Pennacchiotti</author>
<author>Alessandro Moschitti</author>
</authors>
<title>A machine learning approach to textual entailment recognition.</title>
<date>2009</date>
<journal>NATURAL LANGUAGE ENGINEERING,</journal>
<pages>15--04</pages>
<note>Accepted for pubblication.</note>
<contexts>
<context position="20371" citStr="Zanzotto et al., 2009" startWordPosition="3356" endWordPosition="3359">iginal entry S1 represents the Text T, while the revision S2 is the Hypothesis H. Any feature space of those reported in the textual entailment literature could be applied. We here adopt the space that represents first-order syntactic rewrite rules (FOSR), as described in (Zanzotto and Moschitti, 2006). In this feature space, each feature represents a syntactic first-order or 32 grounded rewrite rule. For example, the rule: is represented by the feature &lt; l, r &gt;. A (T, H) pair activates a feature if it unifies with the related rule. A detailed discussion of the FOSR feature space is given in (Zanzotto et al., 2009) and efficient algorithms for the computation of the related kernel functions can be found in (Moschitti and Zanzotto, 2007; Zanzotto and Dell’Arciprete, 2009). 4.4 Comment view A review comment is typically a textual fragment describing the reason why an author has decided to make a revision. In most cases the comment is not a well-formed sentence, as authors tend to use informal slang expressions and abbreviations (e.g. “details: Trelew Massacre; cat: Dirty War, copyedit”, “removed a POV vandalism by Spylab”, “dab ba:clean up using Project:AWB”). In these cases, where syntactic analysis woul</context>
</contexts>
<marker>Zanzotto, Pennacchiotti, Moschitti, 2009</marker>
<rawString>Fabio Massimo Zanzotto, Marco Pennacchiotti, and Alessandro Moschitti. 2009. A machine learning approach to textual entailment recognition. NATURAL LANGUAGE ENGINEERING, 15-04:551–582. Accepted for pubblication.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>