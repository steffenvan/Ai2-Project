<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000175">
<title confidence="0.993957">
Response-Based Confidence Annotation for Spoken Dialogue Systems
</title>
<author confidence="0.984411">
Alexander Gruenstein
</author>
<affiliation confidence="0.9413245">
Spoken Language Systems Group
M.I.T. Computer Science and Artificial Intelligence Laboratory
</affiliation>
<address confidence="0.747882">
32 Vassar St, Cambridge, MA 02139 USA
</address>
<email confidence="0.99861">
alexgru@csail.mit.edu
</email>
<sectionHeader confidence="0.998599" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99964725">
Spoken and multimodal dialogue systems typ-
ically make use of confidence scores to choose
among (or reject) a speech recognizer’s N-
best hypotheses for a particular utterance. We
argue that it is beneficial to instead choose
among a list of candidate system responses.
We propose a novel method in which a con-
fidence score for each response is derived
from a classifier trained on acoustic and lex-
ical features emitted by the recognizer, as
well as features culled from the generation of
the candidate response itself. Our response-
based method yields statistically significant
improvements in F-measure over a baseline in
which hypotheses are chosen based on recog-
nition confidence scores only.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999960133333333">
The fundamental task for any spoken dialogue sys-
tem is to determine how to respond at any given time
to a user’s utterance. The challenge of understand-
ing and correctly responding to a user’s natural lan-
guage utterance is formidable even when the words
have been perfectly transcribed. However, dialogue
system designers face a greater challenge because
the speech recognition hypotheses which serve as
input to the natural language understanding compo-
nents of a system are often quite errorful; indeed, it
is not uncommon to find word error rates of 20-30%
for many dialogue systems under development in re-
search labs. Such high error rates often arise due to
the use of out-of-vocabulary words, noise, and the
increasingly large vocabularies of more capable sys-
</bodyText>
<page confidence="0.986324">
11
</page>
<bodyText confidence="0.999918676470588">
tems which try to allow for greater naturalness and
variation in user input.
Traditionally, dialogue systems have relied on
confidence scores assigned by the speech recognizer
to detect speech recognition errors. In a typical
setup, the dialogue system will choose to either ac-
cept (that is, attempt to understand and respond to)
or reject (that is, respond to the user with an indica-
tion of non-understanding) an utterance by thresh-
olding this confidence score.
Stating the problem in terms of choosing whether
or not to accept a particular utterance for process-
ing, however, misses the larger picture. From the
user’s perspective, what is truly important is whether
or not the system’s response to the utterance is cor-
rect. Sometimes, an errorful recognition hypothe-
sis may result in a correct response if, for example,
proper names are correctly recognized; conversely,
a near-perfect hypothesis may evoke an incorrect re-
sponse. In light of this, the problem at hand is better
formulated as one of assigning a confidence score
to a system’s candidate response which reflects the
probability that the response is an acceptable one.
If the system can’t formulate a response in which it
has high confidence, then it should clarify, indicate
non-understanding, and/or provide appropriate help.
In this paper, we present a method for assign-
ing confidence scores to candidate system responses
by making use not only of features obtained from
the speech recognizer, but also of features culled
from the process of generating a candidate system
response, and derived from the distribution of can-
didate responses themselves. We first compile a list
of unique candidate system responses by processing
</bodyText>
<note confidence="0.7100655">
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 11–20,
Columbus, June 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999974083333333">
each hypothesis on the recognizer’s N-best list. We
then train a Support Vector Machine (SVM) to iden-
tify acceptable responses. When given a novel ut-
terance, candidate responses are ranked with scores
output from the SVM. Based on the scores, the sys-
tem can then either respond with the highest-scoring
candidate, or reject all of the candidate responses
and respond by indicating non-understanding.
Part of the motivation for focusing our efforts on
selecting a system response, rather than a recogni-
tion hypothesis, can be demonstrated by counting
the number of unique responses which can be de-
rived from an N-best list. Figure 1 plots the mean
number of unique system responses, parses, and
recognition hypotheses given a particular maximum
N-best list length; it was generated using the data
described in section 3. Generally, we observe that
about half as many unique parses are generated as
recognition hypotheses, and then half again as many
unique responses. Since many hypotheses evoke the
same response, there is no value in discriminating
among these hypotheses. Instead, we should aim
to gain information about the quality of a response
by pooling knowledge gleaned from each hypothesis
evoking that response.
We expect a similar trend of multiple hypothe-
ses mapping to a single parse in any dialogue sys-
tem where parses contain a mixture of key syntac-
tic and semantic structure—as is the case here—or
where they contain only semantic information (e.g.,
slot/value pairs). Parsers which retain more syn-
tactic structure would likely generate more unique
parses, however many of these parses would prob-
ably map to the same system response since a re-
sponse doesn’t typically hinge on every syntactic de-
tail of an input utterance.
The remainder of our discussion proceeds as fol-
lows. In section 2 we place the method presented
here in context in relation to other research. In sec-
tion 3, we describe the City Browser multimodal di-
alogue system, and the process used to collect data
from users’ interactions with the system. We then
turn to our techniques for annotating the data in
section 4 and describe the features which are ex-
tracted from the labeled data in section 5. Finally,
we demonstrate how to build a classifier to rank can-
didate system responses in section 6, which we eval-
uate in section 7.
</bodyText>
<figure confidence="0.922473">
0 10 20 30 40 50
Maximum N−best length
</figure>
<figureCaption confidence="0.9695384">
Figure 1: The mean N-best recognition hypothesis list
length, mean number of unique parses derived from the
N-best list of recognition hypotheses, and mean number
of unique system responses derived from those parses,
given a maximum recognition N-best list length.
</figureCaption>
<sectionHeader confidence="0.999738" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.995471888888889">
There has been much research into deriving
utterance-level confidence scores based on features
derived from the process of speech recognition. The
baseline utterance-level confidence module we make
use of in this paper was introduced in (Hazen et al.,
2002); we use a subset of the recognizer-derived fea-
tures used by this module. In it, confidence scores
are derived by training a linear projection model to
differentiate utterances with high word error rates.
The utterance-level confidence scores are used to de-
cide whether or not the entire utterance should be
accepted or rejected, while the decision as to how
to respond is left out of the classification process.
Of course, most other recognizers make use of utter-
ance or hypothesis level confidence scores as well;
see, for example (San-Segundo et al., 2000; Chase,
1997).
(Litman et al., 2000) demonstrate the additional
use of prosodic features in deriving confidence
scores, and transition the problem from one of word
error rate to one involving concept error rate, which
is more appropriate in the context of spoken dia-
logue systems. However, they consider only the top
recognition hypothesis.
Our work has been heavily influenced by (Gabs-
dil and Lemon, 2004), (Bohus and Rudnicky, 2002),
(Walker et al., 2000), and (Chotimongkol and Rud-
</bodyText>
<figure confidence="0.962032444444444">
40
50
30
20
10
0
Mean N−best Length
Mean Unique Parses
Mean Unique Responses
</figure>
<page confidence="0.990164">
12
</page>
<bodyText confidence="0.999938309090909">
nicky, 2001) all of which demonstrate the utility of
training a classifier with features derived from the
natural language and dialogue management compo-
nents of a spoken dialogue system to better predict
the quality of speech recognition results. The work
described in (Gabsdil and Lemon, 2004) is espe-
cially relevant, because, as in our experiments, the
dialogue system of interest provides for map-based
multimodal dialogue. Indeed, we view the exper-
iments presented here as extending and validating
the techniques developed by Gabsdil and Lemon.
Our work is novel, however, in that we reframe
the problem as choosing among system responses,
rather than among recognizer hypotheses. By re-
casting the problem in these terms, we are able to
integrate information from all recognition hypothe-
ses which contribute to a single response, and to ex-
tract distributional features from the set of candi-
date responses. Another key difference is that our
method produces confidence scores for the candi-
date responses themselves, while the cited methods
produce a decision as to whether an utterance, or
a particular recognition hypothesis, should be ac-
cepted, rejected, or (in some cases), ignored by the
dialogue system.
In addition, because of the small size of the
dataset used in (Gabsdil and Lemon, 2004), the au-
thors were limited to testing their approach with
leave-one-out cross validation, which means that,
when testing a particular user’s utterance, other ut-
terances from the same user also contributed to
the training set. Their method also does not pro-
vide for optimizing a particular metric—such as F-
measure—although, it does solve a more difficult
3-class decision problem. Finally, another key dif-
ference is that we make use of an n-gram language
model with a large vocabulary of proper names,
whereas theirs is a context-free grammar with a
smaller vocabulary.
(Niemann et al., 2005) create a dialogue sys-
tem architecture in which uncertainty is propagated
across each layer of processing through the use of
probabilities, eventually leading to posterior proba-
bilities being assigned to candidate utterance inter-
pretations. Unlike our system, in which we train a
single classifier using arbitrary features derived from
each stage of processing, each component (recog-
nizer, parser, etc) is trained separately and must be
capable of assigning conditional probabilities to its
output given its input. The method hinges on proba-
bilistic inference, yet it is often problematic to map
a speech recognizer’s score to a probability as their
approach requires. In addition, the method is evalu-
ated only in a toy domain, using a few sample utter-
ances.
</bodyText>
<sectionHeader confidence="0.993923" genericHeader="method">
3 Experimental Data
</sectionHeader>
<bodyText confidence="0.999984176470588">
The data used for the experiments which follow
were collected from user interactions with City
Browser, a web-based, multimodal dialogue system.
A thorough description of the architecture and ca-
pabilities can be found in (Gruenstein et al., 2006;
Gruenstein and Seneff, 2007). Briefly, the version
of City Browser used for the experiments in this pa-
per allows users to access information about restau-
rants, museums, and subway stations by navigating
to a web page on their own computers. They can
also locate addresses on the map, and obtain driving
directions. Users can interact with City Browser’s
map-based graphical user interface by clicking and
drawing; and they can speak with it by talking into
their computer microphone and listening to a re-
sponse from their speakers. Speech recognition is
performed via the SUMMIT recognizer, using a tri-
gram language model with dynamically updatable
classes for proper nouns such as city, street, and
restaurant names—see (Chung et al., 2004) for a de-
scription of this capability. Speech recognition re-
sults were parsed by the TINA parser (Seneff, 1992)
using a hand-crafted grammar. A discourse mod-
ule (Filisko and Seneff, 2003) then integrates con-
textual knowledge. The fully formed request is sent
to the dialogue manager, which attempts to craft
an appropriate system response—both in terms of
a verbal and graphical response. The GENESIS
system (Seneff, 2002) uses hand-crafted generation
rules to produce a natural language string, which is
sent to an off-the-shelf text-to-speech synthesizer.
Finally, the user hears the response, and the graphi-
cal user interface is updated to show, for example, a
set of search results on the map.
</bodyText>
<subsectionHeader confidence="0.995986">
3.1 Data Collection
</subsectionHeader>
<bodyText confidence="0.9999545">
The set of data used in this paper was collected
as part of a controlled experiment in which users
</bodyText>
<page confidence="0.994504">
13
</page>
<bodyText confidence="0.999823">
worked through a set of scenarios by accessing the
City Browser web page from their own computers,
whenever and from wherever they liked. Interested
readers may refer to (Gruenstein and Seneff, 2007)
for more information on the experimental setup, as
well as for an initial analysis of a subset of the data
used here. Users completed a warmup scenario in
which they were simply told to utter “Hello City
Browser” to ensure that their audio setup and web
browser were working properly. They then worked
through ten scenarios presented sequentially, fol-
lowed by time for “free play” in which they could
use the system however they pleased.
As users interact with City Browser, logs are
made recording their interactions. In addition to
recording each utterance, every time a user clicks
or draws with the mouse, these actions are recorded
and time-stamped. The outputs of the various stages
of natural language processing are also logged, so
that the “dialogue state” of the system is tracked.
This means that, associated with each utterance in
the dataset is, among other things, the following in-
formation:
</bodyText>
<listItem confidence="0.978595416666667">
• a recording of the utterance;
• the current dialogue state, which includes in-
formation such as recently referred to entities
for anaphora resolution;
• the state of the GUI, including: the current po-
sition and bounds of the map, any points of in-
terest (POIs) displayed on the map, etc.;
• the contents of any dynamically updatable lan-
guage model classes; and
• time-stamped clicks, gestures, and other user
interface interaction performed by the user be-
fore and during speech.
</listItem>
<bodyText confidence="0.999801833333334">
The utterances of 38 users who attempted most
or all of the scenarios were transcribed, providing
1,912 utterances used in this study. The utterances
were drawn only from the 10 “real” scenarios; ut-
terances from the initial warmup and final free play
tasks were discarded. In addition, a small number of
utterances were eliminated because logging glitches
made it impossible to accurately recover the dia-
logue system’s state at the time of the utterance.
The class n-gram language model used for data
collection has a vocabulary of approximately 1,200
words, plus about 25,000 proper nouns.
</bodyText>
<sectionHeader confidence="0.992916" genericHeader="method">
4 Data Annotation
</sectionHeader>
<bodyText confidence="0.999441571428571">
Given the information associated with each utter-
ance in the dataset, it is possible to “replay” an ut-
terance to the dialogue system and obtain the same
response—both the spoken response and any up-
dates made to the GUI—which was originally pro-
vided to the user in response to the utterance. In
particular, we can replicate the reply frame which
is passed to GENESIS in order to produce a nat-
ural language response; and we can replicate the
gui reply frame which is sent to the GUI so that it
can be properly updated (e.g., to show the results of
a search on the map).
The ability to replicate the system’s response to
each utterance also gives us the flexibility to try out
alternative inputs to the dialogue system, given the
dialogue state at the time of the utterance. So, in ad-
dition to transcribing each utterance, we also passed
each transcript through the dialogue system, yield-
ing a system response. In the experiments that fol-
low, we considered the system’s response to the tran-
scribed utterance to be the correct response for that
utterance. It should be noted that in some cases,
even given the transcript, the dialogue system may
reject and respond by signally non-understanding—
if, for example, the utterance can’t be parsed. In
these cases, we take the response reject to be the
correct response.
We note that labeling the data in this fashion
has limitations. Most importantly, the system may
respond inappropriately even to a perfectly tran-
scribed utterance. Such responses, given our label-
ing methodology, would incorrectly be labeled as
correct. In addition, sometimes it may be the case
that there are actually several acceptable responses
to a particular utterances.
</bodyText>
<sectionHeader confidence="0.995537" genericHeader="method">
5 Feature Extraction
</sectionHeader>
<bodyText confidence="0.999967222222222">
For each utterance, our goal is to produce a set of
candidate system responses, where each response is
also associated with a vector of feature values to be
used to classify it as acceptable or unacceptable.
Responses are labeled as acceptable if they match
the system response produced from the transcrip-
tion, and as unacceptable otherwise.
We start with the N-best list output by the speech
recognizer. For each hypothesis, we extract a set
</bodyText>
<page confidence="0.994845">
14
</page>
<table confidence="0.98515125">
Recognition Distributional Response
(a) Best across hyps: (b) Drop: (c) Other: percent top 3 response type
total score per word total drop mean words percent top 5 num found
acoustic score per bound acoustic drop top rank percent top 10 POI type
lexical score per word lexical drop n-best length percent nbest is subset
top response type parse status
response rank geographical filter
num distinct
</table>
<tableCaption confidence="0.926662">
Table 1: Features used to train the acceptability classifier. Nine features are derived from the recognizer; seven have
to do with the distribution of responses; and six come from the process of generating the candidate response.
</tableCaption>
<bodyText confidence="0.999645232142857">
of acoustic, lexical, and total scores from the recog-
nizer. These scores are easily obtained, as they com-
prise a subset of the features used to train the rec-
ognizer’s existing confidence module; see (Hazen et
al., 2002). The features used are shown in Table 1a.
We then map each hypothesis to a candidate sys-
tem response, by running it through the dialogue
system given the original dialogue state. From these
outputs, we collect a list of unique responses, which
is typically shorter than the recognizer’s N-best list,
as multiple hypotheses typically map to the same re-
sponse.
We now derive a set of features for each unique
response. First, each response inherits the best value
for each recognizer score associated with a hypoth-
esis which evoked that response (see Table 1a). In
addition, the drop in score between the response’s
score for each recognition feature and the top value
occurring in the N-best list is used as a feature (see
Table 1b). Finally, the rank of the highest hypothe-
sis on the N-best list which evoked the response, the
mean number of words per hypothesis evoking the
responses, and the length of the recognizer’s N-best
list are used as features (see Table 1c).
Distributional features are also generated based
on the distribution of hypotheses on the N-best list
which evoked the same response. The percent of
times a particular response is evoked by the top 3,
top 5, top 10, and by all hypotheses on the N-best
list are used as features. Features are generated, as
well, based on the distribution of responses on the
list of unique responses. These features are: the ini-
tial ranking of this response on the list, the number
of distinct responses on the list, and the type of re-
sponse that was evoked by the top hypothesis on the
recognizer N-best list.
Finally, features derived from the response itself,
and natural language processing performed to de-
rive that response, are also calculated. The high-
level type of the response, as well as the type and
number of any POIs returned by a database query
are used as features if they exist, as is a boolean
indicator as to whether or not these results are a
subset of the results currently shown on the dis-
play. If any sort of “geographical filter”, such as
an address or circled region, is used to constrain the
search, then the type of this filter is also used as a
feature. Finally, the “best” parse status of any hy-
potheses leading to this response is also used, where
full parse &gt;- robust parse &gt;- no parse.
Table 1 lists all of the features used to train the
classifier, while Table 3 (in the appendix) lists the
possible values for the non-numerical features. Fig-
ure 3 (in the appendix) gives an overview of the fea-
ture extraction process, as well as the classification
method described in the next section.
</bodyText>
<sectionHeader confidence="0.925965" genericHeader="method">
6 Classifier Training and Scoring
</sectionHeader>
<bodyText confidence="0.999646076923077">
For a given utterance, we now have a candidate list
of responses derived from the speech recognizer’s
N-best list, a feature vector associated with each re-
sponse, and a label telling us the “correct” response,
as derived from the transcript. In order to build a
classifier, we first label each response as either ac-
ceptable or unacceptable by comparing it to the sys-
tem’s response to the transcribed utterance. If the
two responses are identical, then the response is la-
beled as acceptable; otherwise, it is labeled as un-
acceptable. This yields a binary decision problem
for each response, given a set of features. We train
a Support Vector Machine (SVM) to make this deci-
</bodyText>
<page confidence="0.995153">
15
</page>
<bodyText confidence="0.999983098039216">
sion, using the Weka toolkit, version 3.4.12 (Witten
and Frank, 2005).
Given a trained SVM model, the procedure for
processing a novel utterance is as follows. First,
classify each response (and its associated feature
vector) on the response list for that utterance using
the SVM. By using a logistic regression model fit on
the training data, an SVM score between −1 and 1
for each response is yielded, where responses with
positive scores are more likely to be acceptable, and
those with negative scores are more likely to be un-
acceptable.
Next, the SVM scores are used to rank the list of
responses. Given a ranked list of such responses, the
dialogue system has two options: it can choose the
top scoring response, or it can abstain from choos-
ing any response. The most straightforward method
for making such a decision is via a threshold: if the
score of the top response is above a certain thresh-
old, this response is accepted; otherwise, the system
abstains from choosing a response, and instead re-
sponds by indicating non-understanding. Figure 3
(in the appendix) provides a graphical overview of
the response confidence scoring process.
At first blush, a natural threshold to choose is 0,
as this marks the boundary between acceptable and
unacceptable. However, it may be desirable to opti-
mize this threshold based on the desired characteris-
tics of the dialogue system—in a mission-critical ap-
plication, for example, it may be preferable to accept
only high-confidence responses, and to clarify other-
wise. We can optimize the threshold as we like using
either the same training data, or a held-out develop-
ment set, so long as we have an objective function
with which to optimize. In the evaluation that fol-
lows, we optimize the threshold using the F-measure
on the training data as the objective function. It
would also be interesting to optimize the threshold
in a more sophisticated manner, such as that devel-
oped in (Bohus and Rudnicky, 2005) where task suc-
cess is used to derive the cost of misunderstandings
and false rejections, which in turn are used to set a
rejection threshold.
While a thresholding approach makes sense, other
approaches are feasible as well. For instance, a sec-
ond classifier could be used to decide whether or not
to accept the top ranking response. The classifier
could take into account such features as the spread
in scores among the responses, the number classi-
fied as acceptable, the drop between the top score
and the second-ranked score, etc.
</bodyText>
<sectionHeader confidence="0.997303" genericHeader="evaluation">
7 Evaluation
</sectionHeader>
<bodyText confidence="0.999938857142857">
We evaluated the response-based method using the
data described in section 3, N-best lists with a maxi-
mum length of 10, and an SVM with a linear kernel.
We note that, in the live system, two-pass recogni-
tion is performed for some utterances, in which a
key concept recognized in the first pass (e.g., a city
name) causes a dynamic update to the contents of
a class in the n-gram language model (e.g., a set
of street names) for the second pass—as in the ut-
terance Show me thirty two Vassar Street in Cam-
bridge where the city name (Cambridge) triggers
a second pass in which the streets in that city are
given a higher weight. This two-pass approach has
been shown previously to decrease word and con-
cept error rates (Gruenstein and Seneff, 2006), even
though it can be susceptible to errors in understand-
ing. However, since all street names, for example,
are active in the vocabulary at all times, the two-
pass approach is not strictly necessary to arrive at
the correct hypotheses. Hence, for simplicity, in the
experiments reported here, we do not integrate the
two-pass approach—as this would require us to po-
tentially do a second recognition pass for every can-
didate response. In a live system, a good strategy
might be to consider a second recognition pass based
on the top few candidate responses alone, which
would produce a new set of candidates to be scored.
We performed 38-fold cross validation, where in
each case the held-out test set was comprised of all
the utterances of a single user. This ensured that we
obtained an accurate prediction of a novel user’s ex-
perience, although it meant that the test sets were not
of equal size. We calculated F-measure for each test
set, using the methodology described in figure 4 (in
the appendix).
</bodyText>
<subsectionHeader confidence="0.979442">
7.1 Baseline
</subsectionHeader>
<bodyText confidence="0.9999862">
As a baseline, we made use of the existing confi-
dence module in the SUMMIT recognizer (Hazen
et al., 2002). The module uses a linear projection
model to produce an utterance level confidence score
based on 15 features derived from recognizer scores,
</bodyText>
<page confidence="0.992391">
16
</page>
<table confidence="0.999268166666667">
Method F
Recognition Confidence (Baseline) .62
Recog Features Only .62
Recog + Distributional .67
Recog + Response .71 *
Recog + Response + Distributional .72 **
</table>
<tableCaption confidence="0.601478111111111">
Table 2: Average F-measures obtained via per-user
cross-validation of the response-based confidence scor-
ing method using the feature sets described in Section 5,
as compared to a baseline system which chooses the top
hypothesis if the recognizer confidence score exceeds an
optimized rejection threshold. The starred scores are a
statistically significant (* indicates p &lt; .05, ** indicates
p &lt; .01) improvement over the baseline, as determined
by a paired t-test.
</tableCaption>
<bodyText confidence="0.9999373">
and from comparing hypotheses on the N-best list.
In our evaluation, the module was trained and tested
on the same data as the SVM model using cross-
validation.
An optimal rejection threshold was determined,
as for the SVM method, using the training data with
F-measure as the objective function. For each utter-
ance, if the confidence score exceeded the threshold,
then the response evoked from the top hypothesis on
the N-best list was chosen.
</bodyText>
<subsectionHeader confidence="0.788189">
7.2 Results
</subsectionHeader>
<bodyText confidence="0.918980631578947">
Table 2 compares the baseline recognizer confidence
module to our response-based confidence annotator.
The method was evaluated using several subsets of
the features listed in Table 1. Using features derived
from the recognizer only, we obtain results compa-
rable to the baseline. Adding the response and dis-
tributional features yields a 16% improvement over
the baseline system, which is statistically significant
with p &lt; .01 according to a paired t-test. While the
distributional features appear to be helpful, the fea-
ture values derived from the response itself are the
most beneficial, as they allow for a statistically sig-
nificant improvement over the baseline when paired
on their own with the recognizer-derived features.
Figure 2 plots ROC curves comparing the perfor-
mance of the baseline model to the best response-
based model. The curves were obtained by varying
the value of the rejection threshold. We observe that
the response-based model outperforms the baseline
</bodyText>
<subsectionHeader confidence="0.323198">
False Positive Rate
</subsectionHeader>
<figureCaption confidence="0.987438">
Figure 2: Receiver Operator Characteristic (ROC) curves
(averaged across each cross-validation fold) comparing
the baseline to the best response-based model.
</figureCaption>
<bodyText confidence="0.998432">
no matter what we set our tolerance for false posi-
tives to be.
The above results were obtained by using an SVM
with a linear kernel, where feature values were nor-
malized to be on the unit interval. We also tried
using a quadratic kernel, retaining the raw feature
values, and reducing the number of binary features
by manually binning the non-numeric feature val-
ues. Each change resulted in a slight decrease in
F-measure.
</bodyText>
<sectionHeader confidence="0.973222" genericHeader="conclusions">
8 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999994473684211">
We recast the problem of choosing among an N-best
list of recognition hypotheses as one of choosing the
best candidate system response which can be gen-
erated from the recognition hypotheses on that list.
We then demonstrated a framework for assigning
confidence scores to those responses, by using the
scores output by an SVM trained to discriminate be-
tween acceptable and unacceptable responses. The
classifier was trained using a set of features derived
from the speech recognizer, culled from the genera-
tion of each response, and calculated based on each
response’s distribution. We tested our methods us-
ing data collected by users interacting with the City
Browser multimodal dialogue system, and showed
that they lead to a significant improvement over a
baseline which makes an acceptance decision based
on an utterance-level recognizer confidence score.
The technique developed herein could be refined
in several ways. First and foremost, it may well be
</bodyText>
<figure confidence="0.99690825">
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
1
Recognition + Response + Distributional
Recognition Confidence (Baseline)
1
True Positive Rate
</figure>
<page confidence="0.994766">
17
</page>
<bodyText confidence="0.9998480625">
possible to find additional features with discrimina-
tory power. Also, the decision as to whether or not
to choose the top-scoring response could potentially
be improved by choosing a more appropriate metric
than F-measure as the objective function, or perhaps
by using a second classifier at this stage.
Finally, our experiments were performed off-line.
In order to better test the approach, we plan to de-
ploy the classifier as a component in the running di-
alogue system. This presents some processing time
constraints (as multiple candidate responses must be
generated); and it introduces the confounding factor
of working with a recognizer that can make multi-
ple recognition passes after language model recon-
figuration. These challenges should be tractable for
N-best lists of modest length.
</bodyText>
<sectionHeader confidence="0.999157" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999881777777778">
Thank you to Stephanie Seneff for her guidance
and advice. Thanks to Timothy J. Hazen for his
assistance with the confidence module. Thanks to
Ali Mohammad for discussions about the machine
learning aspects of this paper and his comments on
drafts. And thanks to four anonymous reviewers for
constructive criticism. This research is sponsored
by the T-Party Project, a joint research program be-
tween MIT and Quanta Computer Inc., Taiwan.
</bodyText>
<sectionHeader confidence="0.986031" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.90256761971831">
Dan Bohus and Alex Rudnicky. 2002. Integrating mul-
tiple knowledge sources for utterance-level confidence
annotation in the CMU Communicator spoken dialog
system. Technical Report CS-190, Carnegie Mellon
University.
Dan Bohus and Alexander I. Rudnicky. 2005. A princi-
pled approach for rejection threshold optimization in
spoken dialog systems. In Proc. ofINTERSPEECH.
Lin Chase. 1997. Word and acoustic confidence annota-
tion for large vocabulary speech recognition. In Proc.
of 5th European Conference on Speech Communica-
tion and Technology, pages 815–818.
Ananlada Chotimongkol and Alexander I. Rudnicky.
2001. N-best speech hypotheses reordering using lin-
ear regression. In Proc. of 7th European Conference
on Speech Communication and Technology.
Grace Chung, Stephanie Seneff, Chao Wang, and Lee
Hetherington. 2004. A dynamic vocabulary spoken
dialogue interface. In Proc. ofINTERSPEECH, pages
327–330.
Ed Filisko and Stephanie Seneff. 2003. A context res-
olution server for the Galaxy conversational systems.
In Proc. of EUROSPEECH.
Malte Gabsdil and Oliver Lemon. 2004. Combining
acoustic and pragmatic features to predict recognition
performance in spoken dialogue systems. In Proc. of
Association for Computational Linguistics.
Alexander Gruenstein and Stephanie Seneff. 2006.
Context-sensitive language modeling for large sets of
proper nouns in multimodal dialogue systems. In
Proc. of IEEE/ACL 2006 Workshop on Spoken Lan-
guage Technology.
Alexander Gruenstein and Stephanie Seneff. 2007. Re-
leasing a multimodal dialogue system into the wild:
User support mechanisms. In Proc. of the 8th SIGdial
Workshop on Discourse and Dialogue, pages 111–119.
Alexander Gruenstein, Stephanie Seneff, and Chao
Wang. 2006. Scalable and portable web-based
multimodal dialogue interaction with geographical
databases. In Proc. ofINTERSPEECH.
Timothy J. Hazen, Stephanie Seneff, and Joseph Po-
lifroni. 2002. Recognition confidence scoring and
its use in speech understanding systems. Computer
Speech and Language, 16:49–67.
Diane J. Litman, Julia Hirschberg, and Marc Swerts.
2000. Predicting automatic speech recognition perfor-
mance using prosodic cues. In Proc. ofNAACL, pages
218 – 225.
Michael Niemann, Sarah George, and Ingrid Zukerman.
2005. Towards a probabilistic, multi-layered spoken
language interpretation system. In Proc. of 4th IJCAI
Workshop on Knowledge and Reasoning in Practical
Dialogue Systems, pages 8–15.
Rub´en San-Segundo, Bryan Pellom, Wayne Ward, and
Jos´e M. Pardo. 2000. Confidence measures for dia-
logue management in the CU Communicator System.
In Proc. ofICASSP.
Stephanie Seneff. 1992. TINA: A natural language sys-
tem for spoken language applications. Computational
Linguistics, 18(1):61–86.
Stephanie Seneff. 2002. Response planning and gen-
eration in the MERCURY flight reservation system.
Computer Speech and Language, 16:283–312.
Marilyn Walker, Jerry Wright, and Irene Langkilde.
2000. Using natural language processing and dis-
course features to identify understanding errors in a
spoken dialogue system. In Proc. 17th International
Conf. on Machine Learning, pages 1111–1118.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical machine learning tools and techniques. Morgan
Kaufmann, San Francisco, 2nd edition.
</reference>
<page confidence="0.995028">
18
</page>
<figure confidence="0.991904958333333">
Recognition N-best
Hypothesis Rank St Sa Sl ...
thirty two vassal street in cambridge 0 45.3 28.5 26.5
thirty two vassar street in cambridge 1 45.0 27.1 30.5
thirty two vassar street in in cambridge 2 44.2 26.0 30.4
at thirty two vassar street &lt;noise&gt; 3 40.1 26.5 29.4
at thirty two vassal street in cambridge 4 39.5 26.3 29.0
thirty two vassar street cambridge &lt;noise&gt; 5 38.4 25.8 28.4
thirty two vassar street in canton 6 38.0 25.8 28.3
thirty two vassal street in in canton 7 33.5 22.5 27.5
twenty vassar in street in zoom 8 32.4 22.3 26.3
thirty two vassar street in cambridge &lt;noise&gt; 9 32.0 19.5 26.7
DS
4
4
4
4
4
4
4
4
4
4
Response List
</figure>
<figureCaption confidence="0.733899285714286">
Response Rank St Sa Sl %Top3 %Top5 Dist. Parse ...
R0 0 45.3 28.5 26.5 .33 .8 5 FULL
R1 1 45.0 27.1 30.5 .66 .2 5 FULL
R2 6 38.0 25.8 28.3 0.0 0.0 5 FULL
R3 7 33.5 22.5 27.5 0.0 0.0 5 ROBUST
R4 8 32.4 22.3 36.3 0.0 0.0 5 NONE
Figure 3: The feature extraction and classification process. The top half of the digram shows how an N-best list
</figureCaption>
<bodyText confidence="0.971894375">
of recognizer hypotheses, with associated scores from the recognizer, are processed by the dialogue system (DS) to
produce a list of responses. Associated with each response is a set of feature values derived from the response itself,
as well as the process of evoking the response (e.g. the parse status). The bottom half of the figure shows how the
unique responses are collapsed into a list. Each response in the list inherits the best recognition scores available from
hypotheses evoking that response; each also has feature values associated with it derived from the distribution of that
response on the recognizer N-best list. Each set of feature values is classified by a Support Vector Machine, and the
resulting score is used to rank the responses. If the highest scoring response exceeds the rejection threshold, then it is
chosen as the system’s response.
</bodyText>
<page confidence="0.997118">
19
</page>
<bodyText confidence="0.9539">
Feature Possible Values
response type geography, give directions, goodbye, greetings, help directions did not understand from place,
top response type help directions did not understand to place, help directions no to or from place,
help directions subway, hide subway map, history cleared, list cuisine, list name, list street,
no circled data, no data, no match near, non unique near, ok, panning down, panning east,
panning south, panning up, panning west, presupp failure, provide city for address, refined result,
reject or give help, show address, show subway map, speak properties, speak property,
speak verify false, speak verify true, welcome gui, zooming, zooming in, zooming out
POI type none, city, museum, neighborhood, restaurant, subway station
parse status no parse, robust parse, full parse
geographical filter none, address, circle, line, list item, map bounds, museum, neighborhood, point, polygon, restaurant,
subway station, city
</bodyText>
<tableCaption confidence="0.999052">
Table 3: The set of possible values for non-numerical features, which are converted to sets of binary features.
</tableCaption>
<figure confidence="0.954659966666667">
Case I
R0 is acceptable and is not reject
S0 &gt; T 4 T.P.
S0 &lt; T 4 F.N.
Response Score Type Label
R0 S0 speak_property acceptable
R1 S1 list_cuisine unacceptable
R2 S2 speak_property unacceptable
Case I: Example Ranked Response List
Case II
No candidate responses acceptable,
or acceptable response is reject
(a) (b)
R0 is not reject R0 is reject
S0 &gt; T 4 F.P. S0 &gt; T 4 T.N.
S0 &lt; T 4 T.N. S0 &lt; T 4 T.N.
Response Score Type Label
R0 S0 speak_property unacceptable
R1 S1 list_cuisine unacceptable
R2 S2 speak_property unacceptable
R3 S3 reject unacceptable
R4 S4 zooming_out unacceptable
Case II: Example Ranked Response List
Case III
Rn (with n &gt; 0) is acceptable
and is not reject
(a) (b)
R0 is not reject R0 is reject
S0 &gt; T 4 F.P. S0 &gt; T 4 F.N.
S0 &lt; T 4 F.N. S0 &lt; T 4 F.N.
</figure>
<table confidence="0.768059714285714">
Response Score Type Label
R0 S0 speak_property unacceptable
R1 S1 list_cuisine acceptable
R2 S2 speak_property unacceptable
R3 S3 reject unacceptable
R4 S4 zooming_out unacceptable
Case III: Example Ranked Response List
</table>
<figureCaption confidence="0.958184714285714">
Figure 4: Algorithm for calculating the F-measure confusion matrix of True Positives (T.P.), False Positives (F.P.),
True Negatives (T.N.), and False Negatives (F.N.). The ranking technique described in this paper creates a list of
candidate system responses ranked by their scores. The top scoring response is then accepted if its score exceeds a
threshold T, otherwise all candidate responses are rejected. As such, the problem is not a standard binary decision.
We show all possible outcomes from the ranking process, and note whether each case is counted as a T.P., F.P., T.N.,
or F.N. We note that given this algorithm for calculating the confusion matrix, no matter how we set the threshold T,
F-measure will always be penalized if Case III occurs.
</figureCaption>
<page confidence="0.963928">
20
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.485846">
<title confidence="0.998069">Response-Based Confidence Annotation for Spoken Dialogue Systems</title>
<author confidence="0.653922">Alexander</author>
<affiliation confidence="0.6919875">Spoken Language Systems M.I.T. Computer Science and Artificial Intelligence</affiliation>
<address confidence="0.990621">32 Vassar St, Cambridge, MA 02139</address>
<email confidence="0.999779">alexgru@csail.mit.edu</email>
<abstract confidence="0.995240294117647">Spoken and multimodal dialogue systems typically make use of confidence scores to choose among (or reject) a speech recognizer’s Nbest hypotheses for a particular utterance. We argue that it is beneficial to instead choose a list of candidate system We propose a novel method in which a confidence score for each response is derived from a classifier trained on acoustic and lexical features emitted by the recognizer, as well as features culled from the generation of the candidate response itself. Our responsebased method yields statistically significant improvements in F-measure over a baseline in which hypotheses are chosen based on recognition confidence scores only.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Dan Bohus</author>
<author>Alex Rudnicky</author>
</authors>
<title>Integrating multiple knowledge sources for utterance-level confidence annotation in the CMU Communicator spoken dialog system.</title>
<date>2002</date>
<tech>Technical Report CS-190,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="7448" citStr="Bohus and Rudnicky, 2002" startWordPosition="1188" endWordPosition="1191">d is left out of the classification process. Of course, most other recognizers make use of utterance or hypothesis level confidence scores as well; see, for example (San-Segundo et al., 2000; Chase, 1997). (Litman et al., 2000) demonstrate the additional use of prosodic features in deriving confidence scores, and transition the problem from one of word error rate to one involving concept error rate, which is more appropriate in the context of spoken dialogue systems. However, they consider only the top recognition hypothesis. Our work has been heavily influenced by (Gabsdil and Lemon, 2004), (Bohus and Rudnicky, 2002), (Walker et al., 2000), and (Chotimongkol and Rud40 50 30 20 10 0 Mean N−best Length Mean Unique Parses Mean Unique Responses 12 nicky, 2001) all of which demonstrate the utility of training a classifier with features derived from the natural language and dialogue management components of a spoken dialogue system to better predict the quality of speech recognition results. The work described in (Gabsdil and Lemon, 2004) is especially relevant, because, as in our experiments, the dialogue system of interest provides for map-based multimodal dialogue. Indeed, we view the experiments presented h</context>
</contexts>
<marker>Bohus, Rudnicky, 2002</marker>
<rawString>Dan Bohus and Alex Rudnicky. 2002. Integrating multiple knowledge sources for utterance-level confidence annotation in the CMU Communicator spoken dialog system. Technical Report CS-190, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Bohus</author>
<author>Alexander I Rudnicky</author>
</authors>
<title>A principled approach for rejection threshold optimization in spoken dialog systems.</title>
<date>2005</date>
<booktitle>In Proc. ofINTERSPEECH.</booktitle>
<contexts>
<context position="22486" citStr="Bohus and Rudnicky, 2005" startWordPosition="3690" endWordPosition="3693">e desired characteristics of the dialogue system—in a mission-critical application, for example, it may be preferable to accept only high-confidence responses, and to clarify otherwise. We can optimize the threshold as we like using either the same training data, or a held-out development set, so long as we have an objective function with which to optimize. In the evaluation that follows, we optimize the threshold using the F-measure on the training data as the objective function. It would also be interesting to optimize the threshold in a more sophisticated manner, such as that developed in (Bohus and Rudnicky, 2005) where task success is used to derive the cost of misunderstandings and false rejections, which in turn are used to set a rejection threshold. While a thresholding approach makes sense, other approaches are feasible as well. For instance, a second classifier could be used to decide whether or not to accept the top ranking response. The classifier could take into account such features as the spread in scores among the responses, the number classified as acceptable, the drop between the top score and the second-ranked score, etc. 7 Evaluation We evaluated the response-based method using the data</context>
</contexts>
<marker>Bohus, Rudnicky, 2005</marker>
<rawString>Dan Bohus and Alexander I. Rudnicky. 2005. A principled approach for rejection threshold optimization in spoken dialog systems. In Proc. ofINTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin Chase</author>
</authors>
<title>Word and acoustic confidence annotation for large vocabulary speech recognition.</title>
<date>1997</date>
<booktitle>In Proc. of 5th European Conference on Speech Communication and Technology,</booktitle>
<pages>815--818</pages>
<contexts>
<context position="7027" citStr="Chase, 1997" startWordPosition="1123" endWordPosition="1124">r was introduced in (Hazen et al., 2002); we use a subset of the recognizer-derived features used by this module. In it, confidence scores are derived by training a linear projection model to differentiate utterances with high word error rates. The utterance-level confidence scores are used to decide whether or not the entire utterance should be accepted or rejected, while the decision as to how to respond is left out of the classification process. Of course, most other recognizers make use of utterance or hypothesis level confidence scores as well; see, for example (San-Segundo et al., 2000; Chase, 1997). (Litman et al., 2000) demonstrate the additional use of prosodic features in deriving confidence scores, and transition the problem from one of word error rate to one involving concept error rate, which is more appropriate in the context of spoken dialogue systems. However, they consider only the top recognition hypothesis. Our work has been heavily influenced by (Gabsdil and Lemon, 2004), (Bohus and Rudnicky, 2002), (Walker et al., 2000), and (Chotimongkol and Rud40 50 30 20 10 0 Mean N−best Length Mean Unique Parses Mean Unique Responses 12 nicky, 2001) all of which demonstrate the utility</context>
</contexts>
<marker>Chase, 1997</marker>
<rawString>Lin Chase. 1997. Word and acoustic confidence annotation for large vocabulary speech recognition. In Proc. of 5th European Conference on Speech Communication and Technology, pages 815–818.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ananlada Chotimongkol</author>
<author>Alexander I Rudnicky</author>
</authors>
<title>N-best speech hypotheses reordering using linear regression.</title>
<date>2001</date>
<booktitle>In Proc. of 7th European Conference on Speech Communication and Technology.</booktitle>
<marker>Chotimongkol, Rudnicky, 2001</marker>
<rawString>Ananlada Chotimongkol and Alexander I. Rudnicky. 2001. N-best speech hypotheses reordering using linear regression. In Proc. of 7th European Conference on Speech Communication and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grace Chung</author>
<author>Stephanie Seneff</author>
<author>Chao Wang</author>
<author>Lee Hetherington</author>
</authors>
<title>A dynamic vocabulary spoken dialogue interface.</title>
<date>2004</date>
<booktitle>In Proc. ofINTERSPEECH,</booktitle>
<pages>327--330</pages>
<contexts>
<context position="11235" citStr="Chung et al., 2004" startWordPosition="1792" endWordPosition="1795">access information about restaurants, museums, and subway stations by navigating to a web page on their own computers. They can also locate addresses on the map, and obtain driving directions. Users can interact with City Browser’s map-based graphical user interface by clicking and drawing; and they can speak with it by talking into their computer microphone and listening to a response from their speakers. Speech recognition is performed via the SUMMIT recognizer, using a trigram language model with dynamically updatable classes for proper nouns such as city, street, and restaurant names—see (Chung et al., 2004) for a description of this capability. Speech recognition results were parsed by the TINA parser (Seneff, 1992) using a hand-crafted grammar. A discourse module (Filisko and Seneff, 2003) then integrates contextual knowledge. The fully formed request is sent to the dialogue manager, which attempts to craft an appropriate system response—both in terms of a verbal and graphical response. The GENESIS system (Seneff, 2002) uses hand-crafted generation rules to produce a natural language string, which is sent to an off-the-shelf text-to-speech synthesizer. Finally, the user hears the response, and </context>
</contexts>
<marker>Chung, Seneff, Wang, Hetherington, 2004</marker>
<rawString>Grace Chung, Stephanie Seneff, Chao Wang, and Lee Hetherington. 2004. A dynamic vocabulary spoken dialogue interface. In Proc. ofINTERSPEECH, pages 327–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ed Filisko</author>
<author>Stephanie Seneff</author>
</authors>
<title>A context resolution server for the Galaxy conversational systems.</title>
<date>2003</date>
<booktitle>In Proc. of EUROSPEECH.</booktitle>
<contexts>
<context position="11422" citStr="Filisko and Seneff, 2003" startWordPosition="1823" endWordPosition="1826">directions. Users can interact with City Browser’s map-based graphical user interface by clicking and drawing; and they can speak with it by talking into their computer microphone and listening to a response from their speakers. Speech recognition is performed via the SUMMIT recognizer, using a trigram language model with dynamically updatable classes for proper nouns such as city, street, and restaurant names—see (Chung et al., 2004) for a description of this capability. Speech recognition results were parsed by the TINA parser (Seneff, 1992) using a hand-crafted grammar. A discourse module (Filisko and Seneff, 2003) then integrates contextual knowledge. The fully formed request is sent to the dialogue manager, which attempts to craft an appropriate system response—both in terms of a verbal and graphical response. The GENESIS system (Seneff, 2002) uses hand-crafted generation rules to produce a natural language string, which is sent to an off-the-shelf text-to-speech synthesizer. Finally, the user hears the response, and the graphical user interface is updated to show, for example, a set of search results on the map. 3.1 Data Collection The set of data used in this paper was collected as part of a control</context>
</contexts>
<marker>Filisko, Seneff, 2003</marker>
<rawString>Ed Filisko and Stephanie Seneff. 2003. A context resolution server for the Galaxy conversational systems. In Proc. of EUROSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Malte Gabsdil</author>
<author>Oliver Lemon</author>
</authors>
<title>Combining acoustic and pragmatic features to predict recognition performance in spoken dialogue systems.</title>
<date>2004</date>
<booktitle>In Proc. of Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7420" citStr="Gabsdil and Lemon, 2004" startWordPosition="1183" endWordPosition="1187">ecision as to how to respond is left out of the classification process. Of course, most other recognizers make use of utterance or hypothesis level confidence scores as well; see, for example (San-Segundo et al., 2000; Chase, 1997). (Litman et al., 2000) demonstrate the additional use of prosodic features in deriving confidence scores, and transition the problem from one of word error rate to one involving concept error rate, which is more appropriate in the context of spoken dialogue systems. However, they consider only the top recognition hypothesis. Our work has been heavily influenced by (Gabsdil and Lemon, 2004), (Bohus and Rudnicky, 2002), (Walker et al., 2000), and (Chotimongkol and Rud40 50 30 20 10 0 Mean N−best Length Mean Unique Parses Mean Unique Responses 12 nicky, 2001) all of which demonstrate the utility of training a classifier with features derived from the natural language and dialogue management components of a spoken dialogue system to better predict the quality of speech recognition results. The work described in (Gabsdil and Lemon, 2004) is especially relevant, because, as in our experiments, the dialogue system of interest provides for map-based multimodal dialogue. Indeed, we view</context>
<context position="8877" citStr="Gabsdil and Lemon, 2004" startWordPosition="1418" endWordPosition="1421">potheses. By recasting the problem in these terms, we are able to integrate information from all recognition hypotheses which contribute to a single response, and to extract distributional features from the set of candidate responses. Another key difference is that our method produces confidence scores for the candidate responses themselves, while the cited methods produce a decision as to whether an utterance, or a particular recognition hypothesis, should be accepted, rejected, or (in some cases), ignored by the dialogue system. In addition, because of the small size of the dataset used in (Gabsdil and Lemon, 2004), the authors were limited to testing their approach with leave-one-out cross validation, which means that, when testing a particular user’s utterance, other utterances from the same user also contributed to the training set. Their method also does not provide for optimizing a particular metric—such as Fmeasure—although, it does solve a more difficult 3-class decision problem. Finally, another key difference is that we make use of an n-gram language model with a large vocabulary of proper names, whereas theirs is a context-free grammar with a smaller vocabulary. (Niemann et al., 2005) create a</context>
</contexts>
<marker>Gabsdil, Lemon, 2004</marker>
<rawString>Malte Gabsdil and Oliver Lemon. 2004. Combining acoustic and pragmatic features to predict recognition performance in spoken dialogue systems. In Proc. of Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Gruenstein</author>
<author>Stephanie Seneff</author>
</authors>
<title>Context-sensitive language modeling for large sets of proper nouns in multimodal dialogue systems.</title>
<date>2006</date>
<booktitle>In Proc. of IEEE/ACL 2006 Workshop on Spoken Language Technology.</booktitle>
<contexts>
<context position="23778" citStr="Gruenstein and Seneff, 2006" startWordPosition="3917" endWordPosition="3920">0, and an SVM with a linear kernel. We note that, in the live system, two-pass recognition is performed for some utterances, in which a key concept recognized in the first pass (e.g., a city name) causes a dynamic update to the contents of a class in the n-gram language model (e.g., a set of street names) for the second pass—as in the utterance Show me thirty two Vassar Street in Cambridge where the city name (Cambridge) triggers a second pass in which the streets in that city are given a higher weight. This two-pass approach has been shown previously to decrease word and concept error rates (Gruenstein and Seneff, 2006), even though it can be susceptible to errors in understanding. However, since all street names, for example, are active in the vocabulary at all times, the twopass approach is not strictly necessary to arrive at the correct hypotheses. Hence, for simplicity, in the experiments reported here, we do not integrate the two-pass approach—as this would require us to potentially do a second recognition pass for every candidate response. In a live system, a good strategy might be to consider a second recognition pass based on the top few candidate responses alone, which would produce a new set of can</context>
</contexts>
<marker>Gruenstein, Seneff, 2006</marker>
<rawString>Alexander Gruenstein and Stephanie Seneff. 2006. Context-sensitive language modeling for large sets of proper nouns in multimodal dialogue systems. In Proc. of IEEE/ACL 2006 Workshop on Spoken Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Gruenstein</author>
<author>Stephanie Seneff</author>
</authors>
<title>Releasing a multimodal dialogue system into the wild: User support mechanisms.</title>
<date>2007</date>
<booktitle>In Proc. of the 8th SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>111--119</pages>
<contexts>
<context position="10522" citStr="Gruenstein and Seneff, 2007" startWordPosition="1677" endWordPosition="1680">nd must be capable of assigning conditional probabilities to its output given its input. The method hinges on probabilistic inference, yet it is often problematic to map a speech recognizer’s score to a probability as their approach requires. In addition, the method is evaluated only in a toy domain, using a few sample utterances. 3 Experimental Data The data used for the experiments which follow were collected from user interactions with City Browser, a web-based, multimodal dialogue system. A thorough description of the architecture and capabilities can be found in (Gruenstein et al., 2006; Gruenstein and Seneff, 2007). Briefly, the version of City Browser used for the experiments in this paper allows users to access information about restaurants, museums, and subway stations by navigating to a web page on their own computers. They can also locate addresses on the map, and obtain driving directions. Users can interact with City Browser’s map-based graphical user interface by clicking and drawing; and they can speak with it by talking into their computer microphone and listening to a response from their speakers. Speech recognition is performed via the SUMMIT recognizer, using a trigram language model with d</context>
<context position="12254" citStr="Gruenstein and Seneff, 2007" startWordPosition="1958" endWordPosition="1961">e GENESIS system (Seneff, 2002) uses hand-crafted generation rules to produce a natural language string, which is sent to an off-the-shelf text-to-speech synthesizer. Finally, the user hears the response, and the graphical user interface is updated to show, for example, a set of search results on the map. 3.1 Data Collection The set of data used in this paper was collected as part of a controlled experiment in which users 13 worked through a set of scenarios by accessing the City Browser web page from their own computers, whenever and from wherever they liked. Interested readers may refer to (Gruenstein and Seneff, 2007) for more information on the experimental setup, as well as for an initial analysis of a subset of the data used here. Users completed a warmup scenario in which they were simply told to utter “Hello City Browser” to ensure that their audio setup and web browser were working properly. They then worked through ten scenarios presented sequentially, followed by time for “free play” in which they could use the system however they pleased. As users interact with City Browser, logs are made recording their interactions. In addition to recording each utterance, every time a user clicks or draws with </context>
</contexts>
<marker>Gruenstein, Seneff, 2007</marker>
<rawString>Alexander Gruenstein and Stephanie Seneff. 2007. Releasing a multimodal dialogue system into the wild: User support mechanisms. In Proc. of the 8th SIGdial Workshop on Discourse and Dialogue, pages 111–119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Gruenstein</author>
<author>Stephanie Seneff</author>
<author>Chao Wang</author>
</authors>
<title>Scalable and portable web-based multimodal dialogue interaction with geographical databases.</title>
<date>2006</date>
<booktitle>In Proc. ofINTERSPEECH.</booktitle>
<contexts>
<context position="10492" citStr="Gruenstein et al., 2006" startWordPosition="1673" endWordPosition="1676">) is trained separately and must be capable of assigning conditional probabilities to its output given its input. The method hinges on probabilistic inference, yet it is often problematic to map a speech recognizer’s score to a probability as their approach requires. In addition, the method is evaluated only in a toy domain, using a few sample utterances. 3 Experimental Data The data used for the experiments which follow were collected from user interactions with City Browser, a web-based, multimodal dialogue system. A thorough description of the architecture and capabilities can be found in (Gruenstein et al., 2006; Gruenstein and Seneff, 2007). Briefly, the version of City Browser used for the experiments in this paper allows users to access information about restaurants, museums, and subway stations by navigating to a web page on their own computers. They can also locate addresses on the map, and obtain driving directions. Users can interact with City Browser’s map-based graphical user interface by clicking and drawing; and they can speak with it by talking into their computer microphone and listening to a response from their speakers. Speech recognition is performed via the SUMMIT recognizer, using a</context>
</contexts>
<marker>Gruenstein, Seneff, Wang, 2006</marker>
<rawString>Alexander Gruenstein, Stephanie Seneff, and Chao Wang. 2006. Scalable and portable web-based multimodal dialogue interaction with geographical databases. In Proc. ofINTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy J Hazen</author>
<author>Stephanie Seneff</author>
<author>Joseph Polifroni</author>
</authors>
<title>Recognition confidence scoring and its use in speech understanding systems.</title>
<date>2002</date>
<journal>Computer Speech and Language,</journal>
<pages>16--49</pages>
<contexts>
<context position="6455" citStr="Hazen et al., 2002" startWordPosition="1027" endWordPosition="1030">section 6, which we evaluate in section 7. 0 10 20 30 40 50 Maximum N−best length Figure 1: The mean N-best recognition hypothesis list length, mean number of unique parses derived from the N-best list of recognition hypotheses, and mean number of unique system responses derived from those parses, given a maximum recognition N-best list length. 2 Related Work There has been much research into deriving utterance-level confidence scores based on features derived from the process of speech recognition. The baseline utterance-level confidence module we make use of in this paper was introduced in (Hazen et al., 2002); we use a subset of the recognizer-derived features used by this module. In it, confidence scores are derived by training a linear projection model to differentiate utterances with high word error rates. The utterance-level confidence scores are used to decide whether or not the entire utterance should be accepted or rejected, while the decision as to how to respond is left out of the classification process. Of course, most other recognizers make use of utterance or hypothesis level confidence scores as well; see, for example (San-Segundo et al., 2000; Chase, 1997). (Litman et al., 2000) demo</context>
<context position="17250" citStr="Hazen et al., 2002" startWordPosition="2789" endWordPosition="2792">t top 10 POI type lexical score per word lexical drop n-best length percent nbest is subset top response type parse status response rank geographical filter num distinct Table 1: Features used to train the acceptability classifier. Nine features are derived from the recognizer; seven have to do with the distribution of responses; and six come from the process of generating the candidate response. of acoustic, lexical, and total scores from the recognizer. These scores are easily obtained, as they comprise a subset of the features used to train the recognizer’s existing confidence module; see (Hazen et al., 2002). The features used are shown in Table 1a. We then map each hypothesis to a candidate system response, by running it through the dialogue system given the original dialogue state. From these outputs, we collect a list of unique responses, which is typically shorter than the recognizer’s N-best list, as multiple hypotheses typically map to the same response. We now derive a set of features for each unique response. First, each response inherits the best value for each recognizer score associated with a hypothesis which evoked that response (see Table 1a). In addition, the drop in score between </context>
<context position="24903" citStr="Hazen et al., 2002" startWordPosition="4111" endWordPosition="4114">tion pass based on the top few candidate responses alone, which would produce a new set of candidates to be scored. We performed 38-fold cross validation, where in each case the held-out test set was comprised of all the utterances of a single user. This ensured that we obtained an accurate prediction of a novel user’s experience, although it meant that the test sets were not of equal size. We calculated F-measure for each test set, using the methodology described in figure 4 (in the appendix). 7.1 Baseline As a baseline, we made use of the existing confidence module in the SUMMIT recognizer (Hazen et al., 2002). The module uses a linear projection model to produce an utterance level confidence score based on 15 features derived from recognizer scores, 16 Method F Recognition Confidence (Baseline) .62 Recog Features Only .62 Recog + Distributional .67 Recog + Response .71 * Recog + Response + Distributional .72 ** Table 2: Average F-measures obtained via per-user cross-validation of the response-based confidence scoring method using the feature sets described in Section 5, as compared to a baseline system which chooses the top hypothesis if the recognizer confidence score exceeds an optimized rejecti</context>
</contexts>
<marker>Hazen, Seneff, Polifroni, 2002</marker>
<rawString>Timothy J. Hazen, Stephanie Seneff, and Joseph Polifroni. 2002. Recognition confidence scoring and its use in speech understanding systems. Computer Speech and Language, 16:49–67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane J Litman</author>
<author>Julia Hirschberg</author>
<author>Marc Swerts</author>
</authors>
<title>Predicting automatic speech recognition performance using prosodic cues.</title>
<date>2000</date>
<booktitle>In Proc. ofNAACL,</booktitle>
<pages>218--225</pages>
<contexts>
<context position="7050" citStr="Litman et al., 2000" startWordPosition="1125" endWordPosition="1128">d in (Hazen et al., 2002); we use a subset of the recognizer-derived features used by this module. In it, confidence scores are derived by training a linear projection model to differentiate utterances with high word error rates. The utterance-level confidence scores are used to decide whether or not the entire utterance should be accepted or rejected, while the decision as to how to respond is left out of the classification process. Of course, most other recognizers make use of utterance or hypothesis level confidence scores as well; see, for example (San-Segundo et al., 2000; Chase, 1997). (Litman et al., 2000) demonstrate the additional use of prosodic features in deriving confidence scores, and transition the problem from one of word error rate to one involving concept error rate, which is more appropriate in the context of spoken dialogue systems. However, they consider only the top recognition hypothesis. Our work has been heavily influenced by (Gabsdil and Lemon, 2004), (Bohus and Rudnicky, 2002), (Walker et al., 2000), and (Chotimongkol and Rud40 50 30 20 10 0 Mean N−best Length Mean Unique Parses Mean Unique Responses 12 nicky, 2001) all of which demonstrate the utility of training a classifi</context>
</contexts>
<marker>Litman, Hirschberg, Swerts, 2000</marker>
<rawString>Diane J. Litman, Julia Hirschberg, and Marc Swerts. 2000. Predicting automatic speech recognition performance using prosodic cues. In Proc. ofNAACL, pages 218 – 225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Niemann</author>
<author>Sarah George</author>
<author>Ingrid Zukerman</author>
</authors>
<title>Towards a probabilistic, multi-layered spoken language interpretation system.</title>
<date>2005</date>
<booktitle>In Proc. of 4th IJCAI Workshop on Knowledge and Reasoning in Practical Dialogue Systems,</booktitle>
<pages>8--15</pages>
<contexts>
<context position="9468" citStr="Niemann et al., 2005" startWordPosition="1513" endWordPosition="1516">d in (Gabsdil and Lemon, 2004), the authors were limited to testing their approach with leave-one-out cross validation, which means that, when testing a particular user’s utterance, other utterances from the same user also contributed to the training set. Their method also does not provide for optimizing a particular metric—such as Fmeasure—although, it does solve a more difficult 3-class decision problem. Finally, another key difference is that we make use of an n-gram language model with a large vocabulary of proper names, whereas theirs is a context-free grammar with a smaller vocabulary. (Niemann et al., 2005) create a dialogue system architecture in which uncertainty is propagated across each layer of processing through the use of probabilities, eventually leading to posterior probabilities being assigned to candidate utterance interpretations. Unlike our system, in which we train a single classifier using arbitrary features derived from each stage of processing, each component (recognizer, parser, etc) is trained separately and must be capable of assigning conditional probabilities to its output given its input. The method hinges on probabilistic inference, yet it is often problematic to map a sp</context>
</contexts>
<marker>Niemann, George, Zukerman, 2005</marker>
<rawString>Michael Niemann, Sarah George, and Ingrid Zukerman. 2005. Towards a probabilistic, multi-layered spoken language interpretation system. In Proc. of 4th IJCAI Workshop on Knowledge and Reasoning in Practical Dialogue Systems, pages 8–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rub´en San-Segundo</author>
<author>Bryan Pellom</author>
<author>Wayne Ward</author>
<author>Jos´e M Pardo</author>
</authors>
<title>Confidence measures for dialogue management in the CU Communicator System. In</title>
<date>2000</date>
<booktitle>Proc. ofICASSP.</booktitle>
<contexts>
<context position="7013" citStr="San-Segundo et al., 2000" startWordPosition="1119" endWordPosition="1122">e make use of in this paper was introduced in (Hazen et al., 2002); we use a subset of the recognizer-derived features used by this module. In it, confidence scores are derived by training a linear projection model to differentiate utterances with high word error rates. The utterance-level confidence scores are used to decide whether or not the entire utterance should be accepted or rejected, while the decision as to how to respond is left out of the classification process. Of course, most other recognizers make use of utterance or hypothesis level confidence scores as well; see, for example (San-Segundo et al., 2000; Chase, 1997). (Litman et al., 2000) demonstrate the additional use of prosodic features in deriving confidence scores, and transition the problem from one of word error rate to one involving concept error rate, which is more appropriate in the context of spoken dialogue systems. However, they consider only the top recognition hypothesis. Our work has been heavily influenced by (Gabsdil and Lemon, 2004), (Bohus and Rudnicky, 2002), (Walker et al., 2000), and (Chotimongkol and Rud40 50 30 20 10 0 Mean N−best Length Mean Unique Parses Mean Unique Responses 12 nicky, 2001) all of which demonstra</context>
</contexts>
<marker>San-Segundo, Pellom, Ward, Pardo, 2000</marker>
<rawString>Rub´en San-Segundo, Bryan Pellom, Wayne Ward, and Jos´e M. Pardo. 2000. Confidence measures for dialogue management in the CU Communicator System. In Proc. ofICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephanie Seneff</author>
</authors>
<title>TINA: A natural language system for spoken language applications.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>1</issue>
<contexts>
<context position="11346" citStr="Seneff, 1992" startWordPosition="1813" endWordPosition="1814">. They can also locate addresses on the map, and obtain driving directions. Users can interact with City Browser’s map-based graphical user interface by clicking and drawing; and they can speak with it by talking into their computer microphone and listening to a response from their speakers. Speech recognition is performed via the SUMMIT recognizer, using a trigram language model with dynamically updatable classes for proper nouns such as city, street, and restaurant names—see (Chung et al., 2004) for a description of this capability. Speech recognition results were parsed by the TINA parser (Seneff, 1992) using a hand-crafted grammar. A discourse module (Filisko and Seneff, 2003) then integrates contextual knowledge. The fully formed request is sent to the dialogue manager, which attempts to craft an appropriate system response—both in terms of a verbal and graphical response. The GENESIS system (Seneff, 2002) uses hand-crafted generation rules to produce a natural language string, which is sent to an off-the-shelf text-to-speech synthesizer. Finally, the user hears the response, and the graphical user interface is updated to show, for example, a set of search results on the map. 3.1 Data Coll</context>
</contexts>
<marker>Seneff, 1992</marker>
<rawString>Stephanie Seneff. 1992. TINA: A natural language system for spoken language applications. Computational Linguistics, 18(1):61–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephanie Seneff</author>
</authors>
<title>Response planning and generation in the MERCURY flight reservation system. Computer Speech and Language,</title>
<date>2002</date>
<pages>16--283</pages>
<contexts>
<context position="11657" citStr="Seneff, 2002" startWordPosition="1861" endWordPosition="1862"> is performed via the SUMMIT recognizer, using a trigram language model with dynamically updatable classes for proper nouns such as city, street, and restaurant names—see (Chung et al., 2004) for a description of this capability. Speech recognition results were parsed by the TINA parser (Seneff, 1992) using a hand-crafted grammar. A discourse module (Filisko and Seneff, 2003) then integrates contextual knowledge. The fully formed request is sent to the dialogue manager, which attempts to craft an appropriate system response—both in terms of a verbal and graphical response. The GENESIS system (Seneff, 2002) uses hand-crafted generation rules to produce a natural language string, which is sent to an off-the-shelf text-to-speech synthesizer. Finally, the user hears the response, and the graphical user interface is updated to show, for example, a set of search results on the map. 3.1 Data Collection The set of data used in this paper was collected as part of a controlled experiment in which users 13 worked through a set of scenarios by accessing the City Browser web page from their own computers, whenever and from wherever they liked. Interested readers may refer to (Gruenstein and Seneff, 2007) fo</context>
</contexts>
<marker>Seneff, 2002</marker>
<rawString>Stephanie Seneff. 2002. Response planning and generation in the MERCURY flight reservation system. Computer Speech and Language, 16:283–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn Walker</author>
<author>Jerry Wright</author>
<author>Irene Langkilde</author>
</authors>
<title>Using natural language processing and discourse features to identify understanding errors in a spoken dialogue system.</title>
<date>2000</date>
<booktitle>In Proc. 17th International Conf. on Machine Learning,</booktitle>
<pages>1111--1118</pages>
<contexts>
<context position="7471" citStr="Walker et al., 2000" startWordPosition="1192" endWordPosition="1195">ication process. Of course, most other recognizers make use of utterance or hypothesis level confidence scores as well; see, for example (San-Segundo et al., 2000; Chase, 1997). (Litman et al., 2000) demonstrate the additional use of prosodic features in deriving confidence scores, and transition the problem from one of word error rate to one involving concept error rate, which is more appropriate in the context of spoken dialogue systems. However, they consider only the top recognition hypothesis. Our work has been heavily influenced by (Gabsdil and Lemon, 2004), (Bohus and Rudnicky, 2002), (Walker et al., 2000), and (Chotimongkol and Rud40 50 30 20 10 0 Mean N−best Length Mean Unique Parses Mean Unique Responses 12 nicky, 2001) all of which demonstrate the utility of training a classifier with features derived from the natural language and dialogue management components of a spoken dialogue system to better predict the quality of speech recognition results. The work described in (Gabsdil and Lemon, 2004) is especially relevant, because, as in our experiments, the dialogue system of interest provides for map-based multimodal dialogue. Indeed, we view the experiments presented here as extending and va</context>
</contexts>
<marker>Walker, Wright, Langkilde, 2000</marker>
<rawString>Marilyn Walker, Jerry Wright, and Irene Langkilde. 2000. Using natural language processing and discourse features to identify understanding errors in a spoken dialogue system. In Proc. 17th International Conf. on Machine Learning, pages 1111–1118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<title>Data Mining: Practical machine learning tools and techniques.</title>
<date>2005</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco,</location>
<contexts>
<context position="20601" citStr="Witten and Frank, 2005" startWordPosition="3373" endWordPosition="3376">ure vector associated with each response, and a label telling us the “correct” response, as derived from the transcript. In order to build a classifier, we first label each response as either acceptable or unacceptable by comparing it to the system’s response to the transcribed utterance. If the two responses are identical, then the response is labeled as acceptable; otherwise, it is labeled as unacceptable. This yields a binary decision problem for each response, given a set of features. We train a Support Vector Machine (SVM) to make this deci15 sion, using the Weka toolkit, version 3.4.12 (Witten and Frank, 2005). Given a trained SVM model, the procedure for processing a novel utterance is as follows. First, classify each response (and its associated feature vector) on the response list for that utterance using the SVM. By using a logistic regression model fit on the training data, an SVM score between −1 and 1 for each response is yielded, where responses with positive scores are more likely to be acceptable, and those with negative scores are more likely to be unacceptable. Next, the SVM scores are used to rank the list of responses. Given a ranked list of such responses, the dialogue system has two</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>Ian H. Witten and Eibe Frank. 2005. Data Mining: Practical machine learning tools and techniques. Morgan Kaufmann, San Francisco, 2nd edition.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>