<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000471">
<title confidence="0.904953">
An Iterative Algorithm to Build Chinese Language Models
</title>
<note confidence="0.912245166666667">
Xiaoqiang Luo Salim Roukos
Center for Language IBM T. J. Watson Research Center
and Speech Processing Yorktown Heights, NY 10598, USA
The Johns Hopkins University roukosOwatson.ibm.com
3400 N. Charles St.
Baltimore, MD21218, USA
</note>
<email confidence="0.903586">
xiao@jhu.edu
</email>
<sectionHeader confidence="0.996479" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99988448">
We present an iterative procedure to build
a Chinese language model (LM). We seg-
ment Chinese text into words based on a
word-based Chinese language model. How-
ever, the construction of a Chinese LM it-
self requires word boundaries. To get out
of the chicken-and-egg problem, we propose
an iterative procedure that alternates two
operations: segmenting text into words and
building an LM. Starting with an initial
segmented corpus and an LM based upon
it, we use a Viterbi-liek algorithm to seg-
ment another set of data. Then, we build
an LM based on the second set and use the
resulting LM to segment again the first cor-
pus. The alternating procedure provides a
self-organized way for the segmenter to de-
tect automatically unseen words and cor-
rect segmentation errors. Our prelimi-
nary experiment shows that the alternat-
ing procedure not only improves the accu-
racy of our segmentation, but discovers un-
seen words surprisingly well. The resulting
word-based LM has a perplexity of 188 for
a general Chinese corpus.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999989392156863">
In statistical speech recognition(Bahl et al., 1983),
it is necessary to build a language model(LM) for as-
signing probabilities to hypothesized sentences. The
LM is usually built by collecting statistics of words
over a large set of text data. While doing so is
straightforward for English, it is not trivial to collect
statistics for Chinese words since word boundaries
are not marked in written Chinese text. Chinese
is a morphosyllabic language (DeFrancis, 1984) in
that almost all Chinese characters represent a single
syllable and most Chinese characters are also mor-
phemes. Since a word can be multi-syllabic, it is gen-
erally non-trivial to segment a Chinese sentence into
words(Wu and Tseng, 1993). Since segmentation is
a fundamental problem in Chinese information pro-
cessing, there is a large literature to deal with the
problem. Recent work includes (Sproat et al., 1994)
and (Wang et al., 1992). In this paper, we adopt a
statistical approach to segment Chinese text based
on an LM because of its autonomous nature and its
capability to handle unseen words.
As far as speech recognition is concerned, what is
needed is a model to assign a probability to a string
of characters. One may argue that we could bypass
the segmentation problem by building a character-
based LM. However, we have a strong belief that a
word-based LM would be better than a character-
based&apos; one. In addition to speech recognition, the
use of word based models would have value in infor-
mation retrieval and other language processing ap-
plications.
If word boundaries are given, all established tech-
niques can be exploited to construct an LM (Jelinek
et al., 1992) just as is done for English. Therefore,
segmentation is a key issue in building the Chinese
LM. In this paper, we propose a segmentation al-
gorithm based on an LM. Since building an LM it-
self needs word boundaries, this is a chicken-and-egg
problem. To get out of this, we propose an iterative
procedure that alternates between the segmentation
of Chinese text and the construction of the LM. Our
preliminary experiments show that the iterative pro-
cedure is able to improve the segmentation accuracy
and more importantly, it can detect unseen words
automatically.
In section 2, the Viterbi-like segmentation algo-
rithm based on a LM is described. Then in sec-
tion section:iter-proc we discuss the alternating pro-
cedure of segmentation and building Chinese LMs.
We test the segmentation algorithm and the alter-
nating procedure and the results are reported in sec-
</bodyText>
<footnote confidence="0.941925">
IA character-based trigram model has a perplexity of
46 per character or 462 per word (a Chinese word has
an average length of 2 characters), while a word-based
trigram model has a perplexity 188 on the same set of
data. While the comparison would be fairer using a 5-
gram character model, that the word model would have
a lower perplexity as long as the coverage is high.
</footnote>
<page confidence="0.997956">
139
</page>
<bodyText confidence="0.957821272727273">
tion 4. Finally, the work is summarized in section 5.
2 segmentation based on LM
In this section, we assume there is a word-based Chi-
nese LM at our disposal so that we are able to com-
pute the probability of a sentence (with word bound-
aries). We use a Viterbi-like segmentation algorithm
based on the LM to segment texts.
Denote a sentence S by GIG • Cn - 1 Cn where
each Ci (1 &lt;i &lt; n } is a Chinese character. To seg-
ment a sentence into words is to group these char-
acters into words, i.e.
</bodyText>
<equation confidence="0.987281333333333">
S = C2 • • • Cn-lCn (1)
(C1 • • • Cr1)(Cx1+1 • • • Cx2) (2)
• • • (G1.+1 • • • C) (3)
</equation>
<bodyText confidence="0.9996278">
to trace back to find the segmentation points. There-
fore, it&apos;s necessary to record the segmentation points
in (10).
Let p(k) be the index of the last character in the
preceding word. Then
</bodyText>
<equation confidence="0.9781395">
p(k) = arg max [L(i) log P(Ci+1 • • • Cklhi)1 (11)
1&lt;i&lt;k-1
</equation>
<bodyText confidence="0.959650375">
that is, Cp(k)+1 • • • Ck comprises the last word of the
optimal segmentation up to the kth character.
A typical example of a six-character sentence is
shown in table 1. Since p(6) = 4, we know the last
word in the optimal segmentation is C5C6. Since
p(4) = 3, the second last word is C4. So on and so
forth. The optimal segmentation for this sentence is
(C1)(C2C3)(C4)(C5C6) .
</bodyText>
<equation confidence="0.792723">
= wiwz • • *Win (4)
</equation>
<bodyText confidence="0.997891857142857">
where xk is the index of the last character in kth
word wk, i,e wk = Cxk_1-1-1 • • • Cxk(k = 1,2, • • , m),
and of course, xo = 0, 5m = n.
Note that a segmentation of the sentence S can
be uniquely represented by an integer sequence
xi, • , Sm, so we will denote a segmentation by its
corresponding integer sequence thereafter. Let
</bodyText>
<equation confidence="0.642708">
G(S)= {(xi • • • x) :1 &lt; xi &lt; • • • &lt;xm,m &lt; n} (5)
</equation>
<bodyText confidence="0.9395885">
be the set of all possible segmentations of sentence
S. Suppose a word-based LM is given, then for a
segmentation g(S) = (Si • • • xm) E G(S), we can
assign a score to g(S) by
</bodyText>
<equation confidence="0.999888">
L(g(S)) = log Pg (wi • • • Wm) (6)
E log Pg(wilhi) (7)
</equation>
<bodyText confidence="0.9997775">
where wi = C.,_1+1 • • • Cx3(.i = 1,2, • • , m), and hi
is understood as the history words wi • • • wi_i. In
this paper the trigram model(Jelinek et al., 1992) is
used and therefore hi =
Among all possible segmentations, we pick the one
g* with the highest score as our result. That is,
</bodyText>
<equation confidence="0.9954985">
g* = arg max L(g(S)) (8)
gEG(s)
arg max log Pg(wi • • • wm.) (9)
gEG(S)
</equation>
<bodyText confidence="0.996821625">
Note the score depends on segmentation g and this
is emphasized by the subscript in (9). The optimal
segmentation g* can be obtained by dynamic pro-
gramming. With a slight abuse of notation, let L(k)
be the max accumulated score for the first k charac-
ters. L(k) is defined for k = 1, 2, • • • , n with L(1) = 0
and L(g*) = L(n). Given {L(i) : 1 &lt; i &lt; k - 1},
L(k) can be computed recursively as follows:
</bodyText>
<equation confidence="0.99009">
L(k) i&lt;rn&lt;akx i[L(i) + log P(C1+1 • • • Ch1)] (10)
</equation>
<bodyText confidence="0.9914425">
where hi is the history words ended with the
character C. At the end of the recursion, we need
</bodyText>
<tableCaption confidence="0.973522">
Table 1: A segmentation example
</tableCaption>
<table confidence="0.677561666666667">
chars C1 C2 C3 C4 C5 C6
1 2 3 4 5 6
p(k) 0 1 1 3 3 4
</table>
<bodyText confidence="0.999476176470588">
The searches in (10) and (11) are in general time-
consuming. Since long words are very rare in Chi-
nese(94% words are with three or less characters
(Wu and Tseng, 1993)), it won&apos;t hurt at all to limit
the search space in (10) and (11) by putting an up-
per bound(say, 10) to the length of the exploring
word, i.e, impose the constraint i &gt; maxl, k - d in
(10) and (11), where d is the upper bound of Chinese
word length. This will speed the dynamic program-
ming significantly for long sentences.
It is worth of pointing out that the algorithm in
(10) and (11) could pick an unseen word(i.e, a word
not included in the vocabulary on which the LM is
built on) in the optimal segmentation provided LM
assigns proper probabilities to unseen words. This is
the beauty of the algorithm that it is able to handle
unseen words automatically.
</bodyText>
<sectionHeader confidence="0.966164" genericHeader="method">
3 Iterative procedure to build LM
</sectionHeader>
<bodyText confidence="0.999234266666667">
In the previous section, we assumed there exists a
Chinese word LM at our disposal. However, this is
not true in reality. In this section, we discuss an it-
erative procedure that builds LM and automatically
appends the unseen words to the current vocabulary.
The procedure first splits the data into two parts,
set Ti and T2. We start from an initial segmenta-
tion of the set T1. This can be done, for instance,
by a simple greedy algorithm described in (Sproat
et al., 1994). With the segmented T1, we construct
a LA on it. Then we segment the set T2 by using
the LA and the algorithm described in section 2.
At the same time, we keep a counter for each unseen
word in optimal segmentations and increment the
counter whenever its associated word appears in an
</bodyText>
<page confidence="0.985341">
140
</page>
<bodyText confidence="0.999874923076923">
optimal segmentation. This gives us a measure to
tell whether an unseen word is an accidental charac-
ter string or a real word not included in our vocab-
ulary. The higher a counter is, the more likely it is
a word. After segmenting the set T2 we add to our
vocabulary all unseen words with its counter greater
than a threshold c. Then we use the augmented
vocabulary and construct another LMi+i using the
segmented T2. The pattern is clear now: LMi+i is
used to segment the set T1 again and the vocabulary
is further augmented.
To be more precise, the procedure can be written
in pseudo code as follows.
</bodyText>
<table confidence="0.618001714285714">
Step 0: Initially segment the set T1.
Construct an LM LM0 with an initial vocabu-
lary V0.
set i=1.
Step 1: Let j=i mod 2;
For each sentence S in the set T, do
1.1 segment it using LM_ 1.
</table>
<bodyText confidence="0.955770666666667">
1.2 for each unseen word in the optimal seg-
mentation, increment its counter by the
number of times it appears in the optimal
segmentation.
Step 2: Let A=the set of unseen words with
counter greater than c.
set Vi = UA.
Construct another LA using the segmented set
Ti and the vocabulary V.
Step 3: i=i+1 and goto step 1.
Unseen words, most of which are proper nouns,
pose a serious problem to Chinese text segmenta-
tion. In (Sproat et al,, 1994) a class based model was
proposed to identify personal names. In (Wang et
al., 1992), a title driven method was used to identify
personal names. The iterative procedure proposed
here provides a self-organized way to detect unseen
words, including proper nouns. The advantage is
that it needs little human intervention. The proce-
dure provides a chance for us to correct segmenting
errors.
</bodyText>
<sectionHeader confidence="0.997081" genericHeader="method">
4 Experiments and Evaluation
</sectionHeader>
<subsectionHeader confidence="0.999258">
4.1 Segmentation Accuracy
</subsectionHeader>
<bodyText confidence="0.991328125">
Our first attempt is to see how accurate the segmen-
tation algorithm proposed in section 2 is. To this
end, we split the whole data set 2 into two parts, half
for building LMs and half reserved for testing. The
trigram model used in this experiment is the stan-
dard deleted interpolation model described in (Je-
linek et al., 1992) with a vocabulary of 20K words.
Since we lack an objective criterion to measure
the accuracy of a segmentation system, we ask three
2The corpus has about 5 million characters and is
coarsely pre-segmented.
native speakers to segment manually 100 sentences
picked randomly from the test set and compare
them with segmentations by machine. The result is
summed in table 2, where ORG stands for the orig-
inal segmentation, P1, P2 and P3 for three human
subjects, and TRI and UNI stand for the segmen-
tations generated by trigram LM and unigram LM
respectively. The number reported here is the arith-
metic average of recall and precision, as was used in
(Sproat et al., 1994), i.e., 1/2(1,1x -i ), where 72,
is the number of common words in bah segmenta-
tions, n1 and n2 are the number of words in each of
the segmentations.
</bodyText>
<tableCaption confidence="0.991193">
Table 2: Segmentation Accuracy
</tableCaption>
<table confidence="0.999242">
ORG P1 P2 P3 TRI UNI
ORG 94.2 91.2
P1 85.9 85.3 87.4
P2 79.1 90.9 80.1 82.2
P3 87.4 85.7 82.2 85.6 85.7
</table>
<bodyText confidence="0.999614515151515">
We can make a few remarks about the result
in table 2. First of all, it is interesting to note
that the agreement of segmentations among human
subjects is roughly at the same level of that be-
tween human subjects and machine. This confirms
what reported in (Sproat et al., 1994). The major
disagreement for human subjects comes from com-
pound words, phrases and suffices. Since we don&apos;t
give any specific instructions to human subjects,
one of them tends to group consistently phrases
as words because he was implicitly using seman-
tics as his segmentation criterion. For example, he
segments the sentence 3 dao4 jial 1i2 chil dun4
fan4(see table 3) as two words dao4 jial 1i2(go
home) and chil dun4 fan4(have a meal) because
the two &amp;quot;words&amp;quot; are clearly two semantic units. The
other two subjects and machine segment it as dao4
/ jial 1i2/ dun4 / fan4.
Chinese has very limited morphology (Spencer,
1991) in that most grammatical concepts are con-
veyed by separate words and not by morphological
processes. The limited morphology includes some
ending morphemes to represent tenses of verbs, and
this is another source of disagreement. For exam-
ple, for the partial sentence zuo4 wan2 le, where
le functions as labeling the verb zuo4 wan2 as &amp;quot;per-
fect&amp;quot; tense, some subjects tend to segment it as two
words zuo4 wan2/ le while the other treat it as one
single word.
Second, the agreement of each of the subjects with
either the original, trigram, or unigram segmenta-
tion is quite high (see columns 2, 6, and 7 in Table 2)
and appears to be specific to the subject.
</bodyText>
<footnote confidence="0.741401">
3Here we use Pin Yin followed by its tone to represent
a character.
</footnote>
<page confidence="0.99699">
141
</page>
<bodyText confidence="0.999954923076923">
Third, it seems puzzling that the trigram LM
agrees with the original segmentation better than a
unigram model, but gives a worse result when com-
pared with manual segmentations. However, since
the LMs are trained using the presegmented data,
the trigram model tends to keep the original segmen-
tation because it takes the preceding two words into
account while the unigram model is less restricted
to deviate from the original segmentation. In other
words, if trained with &amp;quot;cleanly&amp;quot; segmented data, a
trigram model is more likely to produce a better seg-
mentation since it tends to preserve the nature of
training data.
</bodyText>
<subsectionHeader confidence="0.999709">
4.2 Experiment of the iterative procedure
</subsectionHeader>
<bodyText confidence="0.999996">
In addition to the 5 million characters of segmented
text, we had unsegmented data from various sources
reaching about 13 million characters. We applied
our iterative algorithm to that corpus.
Table 4 shows the figure of merit of the resulting
segmentation of the 100 sentence test set described
earlier. After one iteration, the agreement with
the original segmentation decreased by 3 percentage
points, while the agreement with the human segmen-
tation increased by less than one percentage point.
We ran our computation intensive procedure for one
iteration only. The results indicate that the impact
on segmentation accuracy would be small. However,
the new unsegmented corpus is a good source of au-
tomatically discovered words. A 20 examples picked
randomly from about 1500 unseen words are shown
in Table 5. 16 of them are reasonably good words
and are listed with their translated meanings. The
problematic words are marked with &amp;quot;?&amp;quot;.
</bodyText>
<subsectionHeader confidence="0.999456">
4.3 Perplexity of the language model
</subsectionHeader>
<bodyText confidence="0.999974461538461">
After each segmentation, an interpolated trigram
model is built, and an independent test set with
2.5 million characters is segmented and then used
to measure the quality of the model. We got a per-
plexity 188 for a vocabulary of 80K words, and the
alternating procedure has little impact on the per-
plexity. This can be explained by the fact that the
change of segmentation is very little ( which is re-
flected in table reftab:accuracy-iter ) and the addi-
tion of unseen words(1.5K) to the vocabulary is also
too little to affect the overall perplexity. The merit
of the alternating procedure is probably its ability
to detect unseen words.
</bodyText>
<sectionHeader confidence="0.999542" genericHeader="method">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999979904761905">
In this paper, we present an iterative procedure
to build Chinese language model(LM). We segment
Chinese text into words based on a word-based Chi-
nese language model. However, the construction of
a Chinese LM itself requires word boundaries. To
get out of the chicken-egg problem, we propose an
iterative procedure that alternates two operations:
segmenting text into words and building an LM.
Starting with an initial segmented corpus and an
LM based upon it, we use Viterbi-like algorithm to
segment another set of data. Then we build an LM
based on the second set and use the LM to seg-
ment again the first corpus. The alternating proce-
dure provides a self-organized way for the segmenter
to detect automatically unseen words and correct
segmentation errors. Our preliminary experiment
shows that the alternating procedure not only im-
proves the accuracy of our segmentation, but dis-
covers unseen words surprisingly well. We get a per-
plexity 188 for a general Chinese corpus with 2.5
million characters 4 .
</bodyText>
<sectionHeader confidence="0.999291" genericHeader="conclusions">
6 Acknowledgment
</sectionHeader>
<bodyText confidence="0.999937777777778">
The first author would like to thank various mem-
bers of the Human Language technologies Depart-
ment at the IBM T.J Watson center for their en-
couragement and helpful advice. Special thanks go
to Dr. Martin Franz for providing continuous help
in using the IBM language model tools. The authors
would also thank the comments and insight of two
anonymous reviewers which help improve the final
draft.
</bodyText>
<sectionHeader confidence="0.998594" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.982432714285714">
Richard Sproat, Chilin Shih, William Gale and
Nancy Chang. 1994. A stochastic finite-state
word segmentation algorithm for Chinese. In Pro-
ceedings of ACL&apos;94 , pages 66-73
Zimin Wu and Gwyneth Tseng 1993. Chinese Text
Segmentation for Text Retrieval: Achievements
and Problems Journal of the American Society
for Information Science, 44(9):532-542.
John DeFrancis. 1984. The Chinese Language. Uni-
versity of Hawaii Press, Honolulu.
Frederick Jelinek, Robert L. Mercer and Salim
Roukos. 1992. Principles of Lexical Language
Modeling for Speech recognition. In Advances in
Speech Signal Processing, pages 651-699, edited
by S. Furui and M. M. Sondhi. Marcel Dekker Inc.,
1992
L.R Bahl, Fred Jelinek and R.L. Mercer. 1983.
A Maximum Likelihood Approach to Continu-
ous Speech Recognition. In IEEE Transactions
on Pattern Analysis and Machine Intelligence,
1983,5(2):179-190
Liang-Jyh Wang, Wei-Chuan Li, and Chao-Huang
Chang. 1992. Recognizing unregistered names for
mandarin word identification. In Proceedings of
COLING-92, pages 1239-1243. COLING
4 Unfortunately, we could not find a report of Chinese
perplexity for comparison in the published literature con-
cerning Mandarin speech recognition
</reference>
<page confidence="0.994574">
142
</page>
<bodyText confidence="0.787291">
Andrew Spencer. 1992. Morphological theory :
an introduction to word structure in generative
grammar pages 38-39. Oxford, UK ; Cambridge,
Mass., USA. Basil Blackwell, 1991.
</bodyText>
<tableCaption confidence="0.997718">
Table 3: Segmentation of phrases
</tableCaption>
<table confidence="0.982072">
Chinese dao4 jial 112 chil dun4 fan4
Meaning go home eat a meal
</table>
<tableCaption confidence="0.9278695">
Table 4: Segmentation of accuracy after one itera-
tion
</tableCaption>
<table confidence="0.9981504">
TRO TR1
ORG .920 .890
P1 .863 .877
P2 .817 .832
P3 .850 .849
</table>
<tableCaption confidence="0.998252">
Table 5: Examples of unseen words
</tableCaption>
<figure confidence="0.6970425">
PinYin
Meaning
</figure>
<footnote confidence="0.97509545">
kui2 er2
he2 shi4 1u4 yinl dai4
shou2 dao3
ren4 zhong4
ji4 jian3
zi4 hai4
shuangl bao3
ji4 dongl
zi3 jiaol
xiaol long2 shi2
114 bo4 hai3
du4 shanl
shangl ban4
liu6 hai4
sa4 he4 1e4
kuai4 xun4
cheng4 jing3
huang2 du2
ba3 lian2
he2 dao3
</footnote>
<figure confidence="0.926964375">
last name of former US vice president
cassette of audio tape
(abbr)pretect (the) island
first name or part of a phrase
(abbr) discipline monitoring
double guarantee
(abbr) Eastern He Bei province
purple glue
personal name
(abbr) commercial oriented
six (types of) harms
translated name
fast news
train cop
yellow poison
a (biological) jargon
</figure>
<page confidence="0.981662">
143
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.549275">
<title confidence="0.999585">An Iterative Algorithm to Build Chinese Language Models</title>
<author confidence="0.995236">Xiaoqiang Luo Salim Roukos</author>
<affiliation confidence="0.842353">Center for Language J. Watson Research Center and Speech Processing Yorktown Heights, NY 10598, USA The Johns Hopkins University roukosOwatson.ibm.com</affiliation>
<address confidence="0.9969995">3400 N. Charles St. Baltimore, MD21218, USA</address>
<email confidence="0.999769">xiao@jhu.edu</email>
<abstract confidence="0.996591846153846">We present an iterative procedure to build a Chinese language model (LM). We segment Chinese text into words based on a word-based Chinese language model. However, the construction of a Chinese LM itself requires word boundaries. To get out of the chicken-and-egg problem, we propose an iterative procedure that alternates two operations: segmenting text into words and building an LM. Starting with an initial segmented corpus and an LM based upon it, we use a Viterbi-liek algorithm to segment another set of data. Then, we build an LM based on the second set and use the resulting LM to segment again the first corpus. The alternating procedure provides a self-organized way for the segmenter to detect automatically unseen words and correct segmentation errors. Our preliminary experiment shows that the alternating procedure not only improves the accuracy of our segmentation, but discovers unseen words surprisingly well. The resulting word-based LM has a perplexity of 188 for a general Chinese corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Chilin Shih</author>
<author>William Gale</author>
<author>Nancy Chang</author>
</authors>
<title>A stochastic finite-state word segmentation algorithm for Chinese.</title>
<date>1994</date>
<booktitle>In Proceedings of ACL&apos;94 ,</booktitle>
<pages>66--73</pages>
<contexts>
<context position="2213" citStr="Sproat et al., 1994" startWordPosition="351" endWordPosition="354"> straightforward for English, it is not trivial to collect statistics for Chinese words since word boundaries are not marked in written Chinese text. Chinese is a morphosyllabic language (DeFrancis, 1984) in that almost all Chinese characters represent a single syllable and most Chinese characters are also morphemes. Since a word can be multi-syllabic, it is generally non-trivial to segment a Chinese sentence into words(Wu and Tseng, 1993). Since segmentation is a fundamental problem in Chinese information processing, there is a large literature to deal with the problem. Recent work includes (Sproat et al., 1994) and (Wang et al., 1992). In this paper, we adopt a statistical approach to segment Chinese text based on an LM because of its autonomous nature and its capability to handle unseen words. As far as speech recognition is concerned, what is needed is a model to assign a probability to a string of characters. One may argue that we could bypass the segmentation problem by building a characterbased LM. However, we have a strong belief that a word-based LM would be better than a characterbased&apos; one. In addition to speech recognition, the use of word based models would have value in information retri</context>
<context position="8377" citStr="Sproat et al., 1994" startWordPosition="1541" endWordPosition="1544">ities to unseen words. This is the beauty of the algorithm that it is able to handle unseen words automatically. 3 Iterative procedure to build LM In the previous section, we assumed there exists a Chinese word LM at our disposal. However, this is not true in reality. In this section, we discuss an iterative procedure that builds LM and automatically appends the unseen words to the current vocabulary. The procedure first splits the data into two parts, set Ti and T2. We start from an initial segmentation of the set T1. This can be done, for instance, by a simple greedy algorithm described in (Sproat et al., 1994). With the segmented T1, we construct a LA on it. Then we segment the set T2 by using the LA and the algorithm described in section 2. At the same time, we keep a counter for each unseen word in optimal segmentations and increment the counter whenever its associated word appears in an 140 optimal segmentation. This gives us a measure to tell whether an unseen word is an accidental character string or a real word not included in our vocabulary. The higher a counter is, the more likely it is a word. After segmenting the set T2 we add to our vocabulary all unseen words with its counter greater th</context>
<context position="11337" citStr="Sproat et al., 1994" startWordPosition="2071" endWordPosition="2074"> objective criterion to measure the accuracy of a segmentation system, we ask three 2The corpus has about 5 million characters and is coarsely pre-segmented. native speakers to segment manually 100 sentences picked randomly from the test set and compare them with segmentations by machine. The result is summed in table 2, where ORG stands for the original segmentation, P1, P2 and P3 for three human subjects, and TRI and UNI stand for the segmentations generated by trigram LM and unigram LM respectively. The number reported here is the arithmetic average of recall and precision, as was used in (Sproat et al., 1994), i.e., 1/2(1,1x -i ), where 72, is the number of common words in bah segmentations, n1 and n2 are the number of words in each of the segmentations. Table 2: Segmentation Accuracy ORG P1 P2 P3 TRI UNI ORG 94.2 91.2 P1 85.9 85.3 87.4 P2 79.1 90.9 80.1 82.2 P3 87.4 85.7 82.2 85.6 85.7 We can make a few remarks about the result in table 2. First of all, it is interesting to note that the agreement of segmentations among human subjects is roughly at the same level of that between human subjects and machine. This confirms what reported in (Sproat et al., 1994). The major disagreement for human subj</context>
</contexts>
<marker>Sproat, Shih, Gale, Chang, 1994</marker>
<rawString>Richard Sproat, Chilin Shih, William Gale and Nancy Chang. 1994. A stochastic finite-state word segmentation algorithm for Chinese. In Proceedings of ACL&apos;94 , pages 66-73</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zimin Wu</author>
<author>Gwyneth Tseng</author>
</authors>
<title>Chinese Text Segmentation for Text Retrieval: Achievements and Problems</title>
<date>1993</date>
<journal>Journal of the American Society for Information Science,</journal>
<pages>44--9</pages>
<contexts>
<context position="2036" citStr="Wu and Tseng, 1993" startWordPosition="323" endWordPosition="326">uage model(LM) for assigning probabilities to hypothesized sentences. The LM is usually built by collecting statistics of words over a large set of text data. While doing so is straightforward for English, it is not trivial to collect statistics for Chinese words since word boundaries are not marked in written Chinese text. Chinese is a morphosyllabic language (DeFrancis, 1984) in that almost all Chinese characters represent a single syllable and most Chinese characters are also morphemes. Since a word can be multi-syllabic, it is generally non-trivial to segment a Chinese sentence into words(Wu and Tseng, 1993). Since segmentation is a fundamental problem in Chinese information processing, there is a large literature to deal with the problem. Recent work includes (Sproat et al., 1994) and (Wang et al., 1992). In this paper, we adopt a statistical approach to segment Chinese text based on an LM because of its autonomous nature and its capability to handle unseen words. As far as speech recognition is concerned, what is needed is a model to assign a probability to a string of characters. One may argue that we could bypass the segmentation problem by building a characterbased LM. However, we have a str</context>
<context position="7207" citStr="Wu and Tseng, 1993" startWordPosition="1327" endWordPosition="1330">otation, let L(k) be the max accumulated score for the first k characters. L(k) is defined for k = 1, 2, • • • , n with L(1) = 0 and L(g*) = L(n). Given {L(i) : 1 &lt; i &lt; k - 1}, L(k) can be computed recursively as follows: L(k) i&lt;rn&lt;akx i[L(i) + log P(C1+1 • • • Ch1)] (10) where hi is the history words ended with the character C. At the end of the recursion, we need Table 1: A segmentation example chars C1 C2 C3 C4 C5 C6 1 2 3 4 5 6 p(k) 0 1 1 3 3 4 The searches in (10) and (11) are in general timeconsuming. Since long words are very rare in Chinese(94% words are with three or less characters (Wu and Tseng, 1993)), it won&apos;t hurt at all to limit the search space in (10) and (11) by putting an upper bound(say, 10) to the length of the exploring word, i.e, impose the constraint i &gt; maxl, k - d in (10) and (11), where d is the upper bound of Chinese word length. This will speed the dynamic programming significantly for long sentences. It is worth of pointing out that the algorithm in (10) and (11) could pick an unseen word(i.e, a word not included in the vocabulary on which the LM is built on) in the optimal segmentation provided LM assigns proper probabilities to unseen words. This is the beauty of the a</context>
</contexts>
<marker>Wu, Tseng, 1993</marker>
<rawString>Zimin Wu and Gwyneth Tseng 1993. Chinese Text Segmentation for Text Retrieval: Achievements and Problems Journal of the American Society for Information Science, 44(9):532-542.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeFrancis</author>
</authors>
<title>The Chinese Language.</title>
<date>1984</date>
<publisher>University of Hawaii Press,</publisher>
<location>Honolulu.</location>
<contexts>
<context position="1797" citStr="DeFrancis, 1984" startWordPosition="286" endWordPosition="287">tation, but discovers unseen words surprisingly well. The resulting word-based LM has a perplexity of 188 for a general Chinese corpus. 1 Introduction In statistical speech recognition(Bahl et al., 1983), it is necessary to build a language model(LM) for assigning probabilities to hypothesized sentences. The LM is usually built by collecting statistics of words over a large set of text data. While doing so is straightforward for English, it is not trivial to collect statistics for Chinese words since word boundaries are not marked in written Chinese text. Chinese is a morphosyllabic language (DeFrancis, 1984) in that almost all Chinese characters represent a single syllable and most Chinese characters are also morphemes. Since a word can be multi-syllabic, it is generally non-trivial to segment a Chinese sentence into words(Wu and Tseng, 1993). Since segmentation is a fundamental problem in Chinese information processing, there is a large literature to deal with the problem. Recent work includes (Sproat et al., 1994) and (Wang et al., 1992). In this paper, we adopt a statistical approach to segment Chinese text based on an LM because of its autonomous nature and its capability to handle unseen wor</context>
</contexts>
<marker>DeFrancis, 1984</marker>
<rawString>John DeFrancis. 1984. The Chinese Language. University of Hawaii Press, Honolulu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
<author>Robert L Mercer</author>
<author>Salim Roukos</author>
</authors>
<title>Principles of Lexical Language Modeling for Speech recognition.</title>
<date>1992</date>
<booktitle>In Advances in Speech Signal Processing,</booktitle>
<pages>651--699</pages>
<publisher>Marcel Dekker Inc.,</publisher>
<note>edited by</note>
<contexts>
<context position="2977" citStr="Jelinek et al., 1992" startWordPosition="484" endWordPosition="487">e and its capability to handle unseen words. As far as speech recognition is concerned, what is needed is a model to assign a probability to a string of characters. One may argue that we could bypass the segmentation problem by building a characterbased LM. However, we have a strong belief that a word-based LM would be better than a characterbased&apos; one. In addition to speech recognition, the use of word based models would have value in information retrieval and other language processing applications. If word boundaries are given, all established techniques can be exploited to construct an LM (Jelinek et al., 1992) just as is done for English. Therefore, segmentation is a key issue in building the Chinese LM. In this paper, we propose a segmentation algorithm based on an LM. Since building an LM itself needs word boundaries, this is a chicken-and-egg problem. To get out of this, we propose an iterative procedure that alternates between the segmentation of Chinese text and the construction of the LM. Our preliminary experiments show that the iterative procedure is able to improve the segmentation accuracy and more importantly, it can detect unseen words automatically. In section 2, the Viterbi-like segme</context>
<context position="6207" citStr="Jelinek et al., 1992" startWordPosition="1116" endWordPosition="1119">egmentation of the sentence S can be uniquely represented by an integer sequence xi, • , Sm, so we will denote a segmentation by its corresponding integer sequence thereafter. Let G(S)= {(xi • • • x) :1 &lt; xi &lt; • • • &lt;xm,m &lt; n} (5) be the set of all possible segmentations of sentence S. Suppose a word-based LM is given, then for a segmentation g(S) = (Si • • • xm) E G(S), we can assign a score to g(S) by L(g(S)) = log Pg (wi • • • Wm) (6) E log Pg(wilhi) (7) where wi = C.,_1+1 • • • Cx3(.i = 1,2, • • , m), and hi is understood as the history words wi • • • wi_i. In this paper the trigram model(Jelinek et al., 1992) is used and therefore hi = Among all possible segmentations, we pick the one g* with the highest score as our result. That is, g* = arg max L(g(S)) (8) gEG(s) arg max log Pg(wi • • • wm.) (9) gEG(S) Note the score depends on segmentation g and this is emphasized by the subscript in (9). The optimal segmentation g* can be obtained by dynamic programming. With a slight abuse of notation, let L(k) be the max accumulated score for the first k characters. L(k) is defined for k = 1, 2, • • • , n with L(1) = 0 and L(g*) = L(n). Given {L(i) : 1 &lt; i &lt; k - 1}, L(k) can be computed recursively as follow</context>
<context position="10668" citStr="Jelinek et al., 1992" startWordPosition="1955" endWordPosition="1959">iterative procedure proposed here provides a self-organized way to detect unseen words, including proper nouns. The advantage is that it needs little human intervention. The procedure provides a chance for us to correct segmenting errors. 4 Experiments and Evaluation 4.1 Segmentation Accuracy Our first attempt is to see how accurate the segmentation algorithm proposed in section 2 is. To this end, we split the whole data set 2 into two parts, half for building LMs and half reserved for testing. The trigram model used in this experiment is the standard deleted interpolation model described in (Jelinek et al., 1992) with a vocabulary of 20K words. Since we lack an objective criterion to measure the accuracy of a segmentation system, we ask three 2The corpus has about 5 million characters and is coarsely pre-segmented. native speakers to segment manually 100 sentences picked randomly from the test set and compare them with segmentations by machine. The result is summed in table 2, where ORG stands for the original segmentation, P1, P2 and P3 for three human subjects, and TRI and UNI stand for the segmentations generated by trigram LM and unigram LM respectively. The number reported here is the arithmetic </context>
</contexts>
<marker>Jelinek, Mercer, Roukos, 1992</marker>
<rawString>Frederick Jelinek, Robert L. Mercer and Salim Roukos. 1992. Principles of Lexical Language Modeling for Speech recognition. In Advances in Speech Signal Processing, pages 651-699, edited by S. Furui and M. M. Sondhi. Marcel Dekker Inc., 1992</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Bahl</author>
<author>Fred Jelinek</author>
<author>R L Mercer</author>
</authors>
<title>A Maximum Likelihood Approach to Continuous Speech Recognition.</title>
<date>1983</date>
<booktitle>In IEEE Transactions on Pattern Analysis and Machine Intelligence,</booktitle>
<pages>1983--5</pages>
<contexts>
<context position="1384" citStr="Bahl et al., 1983" startWordPosition="218" endWordPosition="221"> a Viterbi-liek algorithm to segment another set of data. Then, we build an LM based on the second set and use the resulting LM to segment again the first corpus. The alternating procedure provides a self-organized way for the segmenter to detect automatically unseen words and correct segmentation errors. Our preliminary experiment shows that the alternating procedure not only improves the accuracy of our segmentation, but discovers unseen words surprisingly well. The resulting word-based LM has a perplexity of 188 for a general Chinese corpus. 1 Introduction In statistical speech recognition(Bahl et al., 1983), it is necessary to build a language model(LM) for assigning probabilities to hypothesized sentences. The LM is usually built by collecting statistics of words over a large set of text data. While doing so is straightforward for English, it is not trivial to collect statistics for Chinese words since word boundaries are not marked in written Chinese text. Chinese is a morphosyllabic language (DeFrancis, 1984) in that almost all Chinese characters represent a single syllable and most Chinese characters are also morphemes. Since a word can be multi-syllabic, it is generally non-trivial to segme</context>
</contexts>
<marker>Bahl, Jelinek, Mercer, 1983</marker>
<rawString>L.R Bahl, Fred Jelinek and R.L. Mercer. 1983. A Maximum Likelihood Approach to Continuous Speech Recognition. In IEEE Transactions on Pattern Analysis and Machine Intelligence, 1983,5(2):179-190</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang-Jyh Wang</author>
<author>Wei-Chuan Li</author>
<author>Chao-Huang Chang</author>
</authors>
<title>Recognizing unregistered names for mandarin word identification.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING-92,</booktitle>
<pages>1239--1243</pages>
<publisher>COLING</publisher>
<contexts>
<context position="2237" citStr="Wang et al., 1992" startWordPosition="356" endWordPosition="359">sh, it is not trivial to collect statistics for Chinese words since word boundaries are not marked in written Chinese text. Chinese is a morphosyllabic language (DeFrancis, 1984) in that almost all Chinese characters represent a single syllable and most Chinese characters are also morphemes. Since a word can be multi-syllabic, it is generally non-trivial to segment a Chinese sentence into words(Wu and Tseng, 1993). Since segmentation is a fundamental problem in Chinese information processing, there is a large literature to deal with the problem. Recent work includes (Sproat et al., 1994) and (Wang et al., 1992). In this paper, we adopt a statistical approach to segment Chinese text based on an LM because of its autonomous nature and its capability to handle unseen words. As far as speech recognition is concerned, what is needed is a model to assign a probability to a string of characters. One may argue that we could bypass the segmentation problem by building a characterbased LM. However, we have a strong belief that a word-based LM would be better than a characterbased&apos; one. In addition to speech recognition, the use of word based models would have value in information retrieval and other language </context>
<context position="9982" citStr="Wang et al., 1992" startWordPosition="1842" endWordPosition="1845">1. Step 1: Let j=i mod 2; For each sentence S in the set T, do 1.1 segment it using LM_ 1. 1.2 for each unseen word in the optimal segmentation, increment its counter by the number of times it appears in the optimal segmentation. Step 2: Let A=the set of unseen words with counter greater than c. set Vi = UA. Construct another LA using the segmented set Ti and the vocabulary V. Step 3: i=i+1 and goto step 1. Unseen words, most of which are proper nouns, pose a serious problem to Chinese text segmentation. In (Sproat et al,, 1994) a class based model was proposed to identify personal names. In (Wang et al., 1992), a title driven method was used to identify personal names. The iterative procedure proposed here provides a self-organized way to detect unseen words, including proper nouns. The advantage is that it needs little human intervention. The procedure provides a chance for us to correct segmenting errors. 4 Experiments and Evaluation 4.1 Segmentation Accuracy Our first attempt is to see how accurate the segmentation algorithm proposed in section 2 is. To this end, we split the whole data set 2 into two parts, half for building LMs and half reserved for testing. The trigram model used in this expe</context>
</contexts>
<marker>Wang, Li, Chang, 1992</marker>
<rawString>Liang-Jyh Wang, Wei-Chuan Li, and Chao-Huang Chang. 1992. Recognizing unregistered names for mandarin word identification. In Proceedings of COLING-92, pages 1239-1243. COLING</rawString>
</citation>
<citation valid="false">
<authors>
<author>Unfortunately</author>
</authors>
<title>we could not find a report of Chinese perplexity for comparison in the published literature concerning Mandarin speech recognition</title>
<marker>Unfortunately, </marker>
<rawString>4 Unfortunately, we could not find a report of Chinese perplexity for comparison in the published literature concerning Mandarin speech recognition</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>