<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001347">
<title confidence="0.998179">
A Practical Text Summarizer by Paragraph Extraction for Thai
</title>
<author confidence="0.97047">
Chuleerat Jaruskulchai and Canasai Kruengkrai
</author>
<affiliation confidence="0.989332333333333">
Intelligent Information Retrieval and Database Laboratory
Department of Computer Science, Faculty of Science
Kasetsart University, Bangkok, Thailand
</affiliation>
<email confidence="0.993061">
fscichj,g4364115@ku.ac.th
</email>
<sectionHeader confidence="0.995569" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999439071428572">
In this paper, we propose a practical ap-
proach for extracting the most relevant
paragraphs from the original document
to form a summary for Thai text. The
idea of our approach is to exploit both
the local and global properties of para-
graphs. The local property can be consid-
ered as clusters of significant words within
each paragraph, while the global property
can be though of as relations of all para-
graphs in a document. These two proper-
ties are combined for ranking and extract-
ing summaries. Experimental results on
real-world data sets are encouraging.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999964191176471">
The growth of electronic texts is becoming increas-
ingly common. Newspapers or magazines tend to
be available on the World-Wide Web. Summarizing
these texts can help users access to the information
content more quickly. However, doing this task by
humans is costly and time-consuming. Automatic
text summarization is a solution for dealing with this
problem.
Automatic text summarization can be broadly
classified into two approaches: abstraction and ex-
traction. In contrast to abstraction that requires using
heavy machinery from natural language processing
(NLP), including grammars and lexicons for pars-
ing and generation (Hahn and Mani, 2000), extrac-
tion can be easily viewed as the process of selecting
relevant excerpts (sentences, paragraphs, etc.) from
the original document and concatenating them into a
shorter form. Thus, most of recent works in this re-
search area are based on extraction (Goldstein et al.,
1999). Although one may argue that extraction ap-
proach makes the text hard to read due to the lack of
coherence, it also depends on the objective of sum-
marization. If we need to generate summaries that
can be used to indicative what topics are addressed
in the original document, and thus can be used to
alert the uses as the source content, i.e., the indica-
tive function (Mani et al., 1999), extraction approach
is capable of handling this kind of tasks.
There have been many researches on text sum-
marization problem. However, in Thai, we are in
the initial stage of developing mechanisms for au-
tomatically summarizing documents. It is a chal-
lenge to summarize these documents, since they are
extremely different from documents written in En-
glish. Similar to Chinese or Japanese, for the Thai
writing system, there are no boundaries between ad-
joining words, and also there are no explicit sen-
tences boundaries within the document. Fortunately,
there is the use of the paragraph structure in the
Thai writing system, which is indicated by inden-
tations and blank lines. Therefore, extracting text
spans from Thai documents at the paragraph level is
a more practical way.
In this paper, we propose a practical approach to
Thai text summarization by extracting the most rel-
evant paragraphs from the original document. Our
approach considers both the local and global prop-
erties of these paragraphs, which their meaning will
become clear later. We also present an efficient ap-
proach for solving Thai word segmentation problem,
which can enhance a basic word segmentation algo-
rithm yielding more useful output. We provide ex-
perimental evidence that our approach achieves ac-
ceptable performance. Furthermore, our approach
does not require the external knowledge other than
the document itself, and be able to summarize gen-
eral text documents.
The remainder of this paper is organized as fol-
lows. In Section 2, we review some related work
and contrast it with our work. Section 3 describes
the preprocessing for Thai text, particularly on word
segmentation. In Section 4, we present our approach
for extracting relevant paragraphs in detail, includ-
ing how to find clusters of significant words, how to
discover relations of paragraphs, and an algorithm
for combining these two approaches. Section 5 de-
scribes our experiments. Finally, we conclude in
Section 6 with some directions of future work.
</bodyText>
<sectionHeader confidence="0.999691" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999924245283019">
A comprehensive survey of text summarization ap-
proaches can be found in (Mani, 1999). We
briefly review here based on extraction approach.
Luhn (1959) proposed a simple but effective ap-
proach by using term frequencies and their related
positions to weight sentences that are extracted to
form a summary. Subsequent works have demon-
strated the success of Luhn’s approach (Buyukkok-
ten et al., 2001; Lam-Adesina and Jones, 2001;
Jaruskulchai et al., 2003). Edmunson (1969) pro-
posed the use of other features such as title words,
sentence locations, and bonus words to improve sen-
tence extraction. Goldstein et al. (1999) presented
an extraction technique that assigns weighted scores
for both statistical and linguistic features in the sen-
tence. Recently, Salton et al. (1999) have developed
a model for representing a document by using undi-
rected graphs. The basic idea is to consider vertices
as paragraphs and edges as the similarity between
two paragraphs. They suggested that the most im-
portant paragraphs should be linked to many other
paragraphs, which are likely to discuss topic covered
in those paragraphs.
Statistical learning approaches have also been
studied in text summarization problem. The first
known supervised learning algorithm was proposed
by Kupiec et al. (1995). Their approach estimates
the probability that a sentence should be included
in a summary given its feature values based on the
independent assumption of Bayes’ Rule. Other su-
pervised learning algorithms have already been in-
vestigated. Chuang and Yang (2000) studied several
algorithms for extracting sentence segments, such as
decision tree, naive Bayes classifier, and neural net-
work. They also used rhetorical relations for rep-
resenting features. One drawback of the supervised
learning algorithms is that they require an annotated
corpus to learn accurately. However, they may per-
form well for summarizing documents in a specific
domain.
This paper presents an approach for extracting the
most relevant paragraphs from the original docu-
ment to form a summary. The idea of our approach
is to exploit both the local and global properties of
paragraphs. The local property can be considered as
clusters of significant words within each paragraph,
while the global property can be though of as re-
lations of all paragraphs in the document. These
two properties can be combined and tuned to pro-
duce a single measure reflecting the informativeness
of each paragraph. Finally, we can apply this combi-
nation measure for ranking and extracting the most
relevant paragraphs.
</bodyText>
<sectionHeader confidence="0.997723" genericHeader="method">
3 Preprocessing for Thai Text
</sectionHeader>
<bodyText confidence="0.999968055555555">
The first step for working with Thai text is to tok-
enize a given text into meaningful words, since the
Thai writing system has no delimiters to indicate
word boundaries. Thai words are not delimited by
spaces. The spaces are only used to break the idea
or draw readers’ attention. In order to determine
word boundaries, we employed the longest matching
algorithm (Sornlertlamvanich, 1993). The longest
matching algorithm starts with a text span that could
be a phrase or a sentence. The algorithm tries to
align word boundaries according to the longest pos-
sible matching character compounds in a lexicon. If
no match is found in the lexicon, it drops the right-
most character in that text according to the morpho-
logical rules and begins the same search. If a word is
found, it marks a boundary at the end of the longest
word, and then begins the same search starting at the
remainder following the match.
</bodyText>
<equation confidence="0.546145">
w1w2 w1w2w3 ... w1w2w3 ... wn−1wn }
w2w3 ... w2w3 ... wn−1wn (1)
...
wn−1wn
</equation>
<bodyText confidence="0.9988976">
For example, if a phrase candidate consists
of four words, w1w2w3w4, we then obtain W =
{w1w2, w1w2w3, w1w2w3w4, w2w3, w2w3w4, w3w4}.
Let l be the number of set elements that can be
computed from l = (n · (n −1))/2 = (4·3)/2 = 6.
Since we use both stopwords and punctuation
for bounding the phrase candidate, this approach
produces a moderate number of set elements.
Let V be a temporary lexicon. After building
all the phrase candidates in the document and gen-
</bodyText>
<equation confidence="0.991616">
W = I
</equation>
<bodyText confidence="0.9983985">
In our work, the lexicon contained 32675 words.
However, the limitation of this algorithm is that if
the target words are compound words or unknown
words, it tends to produce incorrect results. For ex-
ample, a compound word is segmented as the fol-
lowing:
</bodyText>
<equation confidence="0.776389333333333">
องคกรสิทธิมนุษยชน
(Human Rights Organization)
องคกร_สิทธิ_มนุ_ษย_ชน
</equation>
<bodyText confidence="0.999956111111111">
Since this compound word does not appear in the
lexicon, it becomes small useless words after the
word segmentation process. We further describe an
efficient approach to alleviate this problem by using
an idea of phrase construction (Ohsawa et al., 1998).
Let wz be a word that is firstly tokenized by us-
ing the longest matching algorithm. We refer to
w1w2 ... wn as a phrase candidate, if n &gt; 1, and
no punctuation and stopwords occur between w1
and wn. It is well accepted in information retrieval
community that words can be broadly classified into
content-bearing words and stopwords. In Thai, we
found that words that perform as function words can
be used in place of stopwords similar to English.
We collected 253 most frequently occurred words
for making a list of Thai stopwords.
Given a phrase candidate consisting of n words,
we can generate a set of phrases in the following
form:
erating their sets of phrases, we can construct V
by adding phrases that the number of occurrences
exceeds some threshold. This idea is to exploit
redundancy of phrases occurring in the document.
If a generated phrase frequently occurs, this indi-
cates that it may be a meaningful phrase, and should
be included in the temporary lexicon using for re-
segmenting words.
We denote U to be a main lexicon. After obtain-
ing the temporary lexicon V , we then re-segment
words in the document by using U U V . With us-
ing the combination of these two lexicons, we can
recover some words from the first segmentation. Al-
though we have to do the word segmentation pro-
cess twice, the computation time is not prohibitive.
Furthermore, we obtain more meaningful words that
can be extracted to form keywords of the document.
</bodyText>
<sectionHeader confidence="0.972423" genericHeader="method">
4 Generating Summaries by Extraction
</sectionHeader>
<subsectionHeader confidence="0.997566">
4.1 Finding Clusters of Significant Words
</subsectionHeader>
<bodyText confidence="0.99886725">
In this section, we first describe an approach for
finding clusters of significant words in each para-
graph to calculate the local clustering score. Our
approach is reminiscent of Luhn’s approach (1959)
but uses the other term weighting technique instead
of the term frequency. Luhn suggested that the fre-
quency of a word occurrence in a document, as well
as its relative position determines its significance in
that document. More recent works have also em-
ployed Luhn’s approach as a basis component for
extracting relevant sentences (Buyukkokten et al.,
2001; Lam-Adesina and Jones, 2001). This ap-
proach performs well despite of its simplicity. In our
previous work (Jaruskulchai et al., 2003), we also
applied this approach for summarizing and brows-
ing Thai documents through PDAs.
Let Q be a subset of a continuous sequence of
words in a paragraph, {wu ... w„}. The subset Q
is called a cluster of significant words if it has these
characteristics:
</bodyText>
<listItem confidence="0.989483">
• The first word wu and the last word w„ in the
sequence are significant words.
• Significant words are separated by not more
than a predefined number of insignificant
words.
</listItem>
<bodyText confidence="0.999936744186047">
For example, we can partition a continuous se-
quence of words in a paragraph into clusters as
shown in Figure 1. The paragraph consists of twelve
words. We use the boldface to indicate positions
of significant words. Each cluster is enclosed with
brackets. In this example, we define that a cluster
is created whereby significant words are separated
by not more than three insignificant words. Note
that many clusters of significant words can be found
in the paragraph. The highest score of the clusters
found in the paragraph is selected to be the para-
graph score. Therefore, the local clustering score
for paragraph si can be calculated as follows:
where ns(β, si) is the number of bracketed signif-
icant words, and n(β, si) is the total number of
bracketed words.
We can see that the first important step in this pro-
cess is to mark positions of significant words for
identifying the clusters. Our goal is to find topical
words, which are indicative of the topics underly-
ing the document. According to Luhn’s approach,
the term frequencies is used to weight all the words.
The other term weighting scheme frequently used
is TFIDF (Term Frequency Inverse Document Fre-
quency) (Salton and Buckley, 1988). However, this
technique needs a corpus for computing IDF score,
causing the genre-dependent problem for generic
text summarization task.
In our work, we decide to use TLTF (Term Length
Term Frequency) term weighting technique (Banko
et al., 1999) for scoring words in the document in-
stead of TFIDF. TLTF multiplies a monotonic func-
tion of the term length by a monotonic function of
the term frequency. The basic idea of TLTF is based
on the assumption that words that are used more
frequently tend to be shorter. Such words are not
strongly indicative of the topics underlying in the
document, such as stopwords. In contrast, words
that are used less frequently tend to be longer. One
significant benefit of using TLTF term weighting
technique for our task is that it does not require
any external resources, only using the information
within the document.
</bodyText>
<equation confidence="0.402957">
w1[w2w3w4] w5w6w7w8[w9w10w11w12]
</equation>
<figureCaption confidence="0.999028">
Figure 1: Clusters of significant words.
</figureCaption>
<subsectionHeader confidence="0.992903">
4.2 Discovering Relations of Paragraphs
</subsectionHeader>
<bodyText confidence="0.998909181818182">
We now move on to describe an approach for dis-
covering relations of paragraphs. Given a docu-
ment D, we can represent it by an undirected graph
G = (V, E), where V = {s1, ... , sm} is the set of
paragraphs in that document. An edge (si, sj) is
in E, if the cosine similarity between paragraphs
si and sj is above a certain threshold, denoted α.
A paragraph si is considered to be a set of words
{wsi,1, wsi,2, ... , wsi,t}. The cosine similarity be-
tween two paragraphs can be calculated by the fol-
lowing formula:
</bodyText>
<equation confidence="0.9922058">
2
Etk=1 ws k i,kws�,
sim(si, sj) = (3)
Et 2 t 2
k=1 wsi k Ek=1 wsj,k
</equation>
<bodyText confidence="0.999904461538462">
The graph G is called the text relationship map of
D (Salton et al., 1999). Let dsi be the degree of node
si. We then refer to dsi as the global connectivity
score. Generating a summary for a given document
can be processed by sorting all the nodes with dsi in
decreasing order, and then extracting n top-ranked
nodes, where n is the target number of paragraphs
in the summary.
This idea is based on Salton et al.’s approach that
also performs extraction at the paragraph level. They
suggested that since a highly bushy node is linked
to a number of other nodes, it has an overlapping
vocabulary with several paragraphs, and is likely to
discuss topics covered in many other paragraphs.
Consequently, such nodes are good candidates for
extraction. They then used a global bushy path that
is constructed out of n most bushy nodes to form the
summary. Their experimental results on encyclope-
dia articles demonstrates reasonable results.
However, when we directly applied this approach
for extracting paragraphs from moderately-sized
documents, we found that using only the global con-
nectivity score is inadequate to measure the infor-
mativeness of paragraphs in some case. In order
to describe this situation, we consider an example
of a text relationship map in Figure 2. The map is
</bodyText>
<equation confidence="0.809221">
n(β, si) , (2)
ns(β, si)Lsi = argmaxβ
</equation>
<figureCaption confidence="0.91778175">
Figure 2: Text relationship map of an online news-
paper article using α = 0.10.
Figure 3: Text relationship map of the same article,
but using α = 0.20.
</figureCaption>
<bodyText confidence="0.999041416666667">
constructed from an online newspaper article.1 The
similarity threshold α is 0.1. As a result, edges with
similarities less than 0.1 do not appear on the map.
Node P4 obtains the maximum global connectivity
score at 9. However, the global connectivity score
of nodes P3, P5, and P6 is 7, and nodes P7 and P8 is
6, which are slightly different. When we increase the
threshold α = 0.2, we obtain a text relationship map
as shown in Figure 3. Nodes P4 and P7 now achieve
the same maximum global connectivity score at 5.
Nodes P3, P5, and P6 get the same score at 4.
From above example, it is hard to determine that
</bodyText>
<footnote confidence="0.987974">
1The article is available at: http://mickey.sci.ku.
ac.th/˜TextSumm/sample/t1.html
</footnote>
<bodyText confidence="0.998659090909091">
node P4 is more relevant than nodes such as P3 or
P5, since their scores are only different at 1 point.
Our preliminary experiments with many other docu-
ments lead to the suggestion that the global connec-
tivity score of nodes in the text relation map tends
to be slightly different on some document lengths.
Given a compression rate (ratio of the summary
length to the source length), if we immediately ex-
tract these nodes of paragraphs, many paragraphs
with the same score are also included in the sum-
mary.
</bodyText>
<subsectionHeader confidence="0.999867">
4.3 Combining Local and Global Properties
</subsectionHeader>
<bodyText confidence="0.999898058823529">
In this section, we present an algorithm that takes
advantage of both the local and global properties
of paragraphs for generating extractive summaries.
From previous sections, we describe two differ-
ent approaches that can be used to extract relevant
paragraphs. However, these extraction schemes are
based on different views and concepts. The local
clustering score only captures the content of infor-
mation within paragraphs, while the global connec-
tivity score mainly considers the structural aspect
of the document to evaluate the informativeness of
paragraphs. This leads to our motivation for uni-
fying good aspects of these two properties. We
can consider the local clustering score as the local
property of paragraphs, and the global connectivity
score as the global property. Here we propose an
algorithm that combines the local clustering score
with the global connectivity score to get a single
measure reflecting the informativeness of each para-
graph, which can be tuned according to the relative
importance of properties.
Our algorithm proceeds as follows. Given a doc-
ument, we start by eliminating stopwords and ex-
tracting all unique words in the document. These
unique words are used to be the document vocabu-
lary. Therefore, we can represent a paragraph si as a
vector. We then compute similarities between all the
paragraph vectors using equation (3), and eliminate
edges with similarities less than a threshold in order
to build the text relationship map. This process auto-
matically yields the global connectivity scores of the
paragraphs. Next, we weight each word in the doc-
ument vocabulary using TLTF term weighting tech-
nique. All the words are sorted by their TLTF scores,
</bodyText>
<equation confidence="0.997692111111111">
P4
P3
P2
P5
P6
P1
P7
P10
P8 P9
P4
P3
P2
P5
P1
P6
P7
P10
P8 P9
</equation>
<bodyText confidence="0.999855">
and top r words are selected to be significant words.
We mark positions of significant words in each para-
graph to calculate the local clustering score. After
obtaining both scores, for each paragraph sz, we can
compute the combination score by using the follow-
ing ranking function:
</bodyText>
<equation confidence="0.998972">
F(sz) = AG0 + (1 − A)L0 , (4)
</equation>
<bodyText confidence="0.9999165">
where G0 is the normalized global connectivity
score, and L0 is the normalized local clustering
score. The normalized global connectivity score G0
can be calculated as follows:
</bodyText>
<equation confidence="0.969676666666667">
dsi
G0 = ,(5)
dmax
</equation>
<bodyText confidence="0.9985675">
where dmax is the degree of the node that has the
maximum edges using for normalization, resulting
the score in the range of [0, 1]. Using equation (2),
L0 is given by:
</bodyText>
<equation confidence="0.978082">
Lsi
L0 = ,(6)
Lmax
</equation>
<bodyText confidence="0.9999608">
where Lmax is the maximum local clustering score
using for normalization. Similarly, it results this
score in the range of [0, 1]. The parameter A is var-
ied depending on the relative importance of the com-
ponents G0 and L0. Therefore, we can rank all the
paragraphs according to their combination scores in
decreasing order. We finally extract n top-ranked
paragraphs corresponding to the compression rate,
and rearrange them in chronological order to form
the output summary.
</bodyText>
<sectionHeader confidence="0.997875" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.996095">
5.1 Data Sets
</subsectionHeader>
<bodyText confidence="0.99995875">
The typical approach for testing a summarization
system is to create an “ideal” summary, either
by professional abstractors or merging summaries
provided by multiple human subjects using meth-
ods such as majority opinion, union, or intersec-
tion (Jing et al., 1998). This approach is known
as intrinsic method. Unlike in English, standard
data sets in Thai are not yet available for evaluat-
ing text summarization system. However, in order
to observe characteristics of our algorithm, we col-
lected Thai documents, including agricultural news
(D1.AN), general news (D2.GN), and columnist’s
articles (D3.CA) to make data sets. Each data set
consists of 10 documents, and document sizes range
from 1 to 4 pages. We asked a student in the Depart-
ment of Thais, Faculty of Liberal Arts, for manual
summarization by selecting the most relevant para-
graphs that can indicate the main points of the docu-
ment. These paragraphs are called extracts, and then
are used for evaluating our algorithm.
</bodyText>
<subsectionHeader confidence="0.999363">
5.2 Performance Evaluations
</subsectionHeader>
<bodyText confidence="0.99998675">
We evaluate results of summarization by using the
standard precision, recall, and F1. Let J be the num-
ber of extracts in the summary, K be the number of
selected paragraphs in the summary, and M be the
number of extracts in the test document. We then
refer to precision of the algorithm as the fraction be-
tween the number of extracts in the summary and the
number of selected paragraphs in the summary:
</bodyText>
<equation confidence="0.9826395">
J
Precision = K , (7)
</equation>
<bodyText confidence="0.9997825">
recall as the fraction between the number of extracts
in the summary and the number of extracts in the test
</bodyText>
<equation confidence="0.821265333333333">
document:
J
Recall = M . (8)
</equation>
<bodyText confidence="0.9414525">
Finally, F1, a combination of precision and recall,
can be calculated as follows:
</bodyText>
<equation confidence="0.986241">
2 · Precision · Recall
F1 = Precision + Recall .(9)
</equation>
<subsectionHeader confidence="0.978656">
5.3 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999869882352941">
In this section, we provide experimental evidence
that our algorithm gives acceptable performance.
The compression rate of paragraph extraction to
form a summary is 20% and 30%. These rates yield
the number of extracts in the summary comparable
to the number of actual extracts in a given test doc-
ument. The threshold α of the cosine similarity is
0.2. The parameter A for combining the local and
global properties is 0.5. For the distance between
significant words in a cluster, we set that significant
words are separated by not more than three insignif-
icant words.
Table 1 and 2 show a summary of precision, re-
call, and F1 for each compression rate, respectively.
We can see that average precision values of our al-
gorithm slightly decrease, but average recall val-
ues increase when we increase the compression rate.
</bodyText>
<table confidence="0.554386076923077">
t-rg�-r&amp;&amp; (Keywords):
เพนเทียมเอ็ม, โมบายลโปรเซสเซอร, โปรเซสเซอร, อินเทล, เครื่องโนตบุค, เดสกท็อป, เทคโนโลยี,
ประสิทธิภาพ, การใชพลังงาน, บริษัทอินเทล, รุนปจจุบัน
A-rn-rse�adang-rim 20% (Summarization result at 20%):
ที่นาสังเกตก็คือทั้ง “เพรสคอตต” และ “เพนเทียมเอ็ม” นั้น ไดรับการพัฒนาขึ้นมาบนพื้นฐาน
สถาปตยกรรมเดียวกัน นั่นหมายความวา ตอนนี้เดสกท็อปโปรเซสเซอรและโมบายลโปรเซสเซอรของ
อินเทลมีประสิทธิภาพและความสามารถทัดเทียมกันแลว หรือถาจะตางกันก็คงเล็กนอย
ในแงของเทคโนโลยี อินเทลโมบายลโปรเซสเซอรในปจจุบันจะเปนการพัฒนาตอยอดจากเดสกท็อป
โปรเซสเซอร โดยมีการปรับปรุงใหมีการใชพลังงานนอยลง กินไฟนอยลง ซึ่งเครื่องโนตบุคที่ใชอินเทล
โมบายลโปรเซสเซอรรุนปจจุบันจะสามารถรันบนแบตเตอรี่ไดนาน 1-4 ชั่วโมง
ในแงของการตลาด อินเทลจะทําตลาดโปรเซสเซอรเพนเทียมเอ็ม ภายใตชื่อ “เซนทริโน” (Centrino)
โดยวางจําหนวยเปนชุดแพ็คเกจ ที่นอกจากจะมีโปรเซสเซอรแลว ยังมีชิปเซ็ตและโมดูลระบบสื่อสารไร
สาย (WiFi) รวมอยูดวย
</table>
<figureCaption confidence="0.803788">
Figure 4: An example of keywords and extracted summaries in Thai.
</figureCaption>
<table confidence="0.99907275">
Data set Precision Recall Fi
D1.AN 0.600 0.448 0.509
D2.GN 0.518 0.385 0.431
D3.CA 0.530 0.330 0.404
</table>
<tableCaption confidence="0.841339">
Table 1: Evaluation results obtained by using com-
pression rate 20%.
</tableCaption>
<table confidence="0.9996755">
Data set Precision Recall Fi
D1.AN 0.550 0.577 0.555
D2.GN 0.464 0.467 0.453
D3.CA 0.523 0.462 0.488
</table>
<tableCaption confidence="0.7985385">
Table 2: Evaluation results obtained by using com-
pression rate 30%.
</tableCaption>
<bodyText confidence="0.999803470588235">
Since using higher compression rate tends to select
more paragraphs from the document, it increases the
chance that the selected paragraphs will be matched
with the target extracts. On the other hand, it also
selects irrelevant paragraphs to be included in the
summary, so precision can decrease. Further experi-
ments on larger text corpora are needed to determine
the performance of our summarizer. However, these
preliminary results are very encouraging. Figure
4 illustrates an example of keywords and extracted
summaries for a Thai document using compression
rate 20% . The implementation of our algorithm is
now available for user testing at http://mickey.
sci.ku.ac.th/˜TextSumm/index.html. The com-
putation time to summarize moderately-sized docu-
ments, such as newspaper articles, is less one sec-
ond.
</bodyText>
<sectionHeader confidence="0.995616" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.99991375">
In this paper, we have presented a practical ap-
proach to Thai text summarization by extracting the
most relevant paragraphs from the original docu-
ment. Our approach takes advantage of both the
local and global properties of paragraphs. The algo-
rithm that combines these two properties for ranking
and extracting paragraphs is given. Furthermore, the
algorithm does not require the external knowledge
other than the document itself, and be able to sum-
marize general text documents.
In future work, we intend to conduct experiments
with different document genres. We continue to fur-
ther develop standard data sets for evaluating Thai
text summarization system. Many research ques-
tions remain. Since extraction performs at the para-
graph level, the paragraph lengths may affect the
summarization results. The recent approach for
editing extracted text spans (Jing and McKeown,
2000) may also produce improvement for our algo-
rithm. We believe that our algorithm is language-
independent, which can summarize documents writ-
ten in many other languages. We plan to experimen-
tally test our algorithm with available standard data
sets in English.
</bodyText>
<sectionHeader confidence="0.996557" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99953875">
This research was supported by the grant of the Na-
tional Research Council of Thailand, 2002. Many
thanks to Tan Sinthurahat (Thammasat University)
for manual summarizing the data sets.
</bodyText>
<sectionHeader confidence="0.999181" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999509650793651">
Banko, M., Mittal, V., Kantrowitz, M., and Goldstein, J.
1999. Generating extraction-based summaries from
hand-written summaries by aligning text spans. In
Proceedings of PACLING’99.
Buyukkokten, O., Garcia-Molina, H., and Paepcke, A.
2001. Seeing the whole in parts: Text summarization
for web browsing on handheld devices. WWW10.
Chuang, W. T., and Yang, J. 2000. Extracting sentence
segments for text summarization: A machine learning
approach. In Proceedings of the 23rd ACM SIGIR,
152–159.
Edmundson, H. P. 1969. New methods in automatic ex-
traction. Journal of the ACM, 16(2):264–285.
Goldstein, J., Kantrowitz, M., Mittal, V., and Carbonell,
J. 1999. Summarizing text documents: Sentence se-
lection and evaluation metrics. In Proceedings of the
22nd ACM SIGIR, 121–128.
Hahn, U., and Mani, I. 2000. The challenges of auto-
matic summarization. IEEE Computer, 33(11):29–35.
Jaruskulchai, C., Khanthong, A., and Tantiprasongchai,
W. 2003. A Framework for Delivery of Thai Content
through Mobile Devices. Closing Gaps in the Digital
Divide Regional Conference on Digital GMS. Asian
Institute of Technology, 190–194.
Jing, H., Barzilay, R., McKeown, K., and Elhadad, M.
1998. Summarization evaluation methods: Experi-
ments and analysis. AAAI Intelligent Text Summariza-
tion Workshop, 60–68.
Jing, H., and McKeown, K. 2000. Cut and paste based
text summarization. In Proceedings of the 1st Confer-
ence of the North American Chapter of the Association
for Computational Linguistics.
Kupiec, J., Pedersen, J., and Chen, F. 1995. A train-
able document summarizer. In Proceedings of the 18th
ACM SIGIR, 68–73.
Lam-Adesina, M., and Jones, G. J. F. 2001. Applying
summarization techniques for term selection in rele-
vance feedback. In Proceedings of the 24th ACM SI-
GIR, 1–9.
Luhn, H. P. 1959. The automatic creation of literature
abstracts. IBMJournal of Research and Development,
159–165.
Mani, I., Firmin, T., House, D., Klein, G., Sundheim,
B., Hirschman, L. 1999. The TIPSTER SUMMAC
Text Summarization Evaluation. In Proceedings of
EACL’99.
Mani, I., and Maybury, M. T. 1999. Advances in ac-
tomatic text summarization. MIT Press.
Ohsawa, Y., Benson, N. E., and Yachida, M. 1998. Key-
Graph: Automatic indexing by cooccurrence graph
based on building construction metaphor. In Proceed-
ings of EAdvanced Digital Library Conference.
Salton, G., and Buckley, C. 1988. Term weighting ap-
proaches in automatic text retrieval. Information Pro-
cessing and Management, 24(5):513–523.
Salton, G., Singhal, A., Mitra, M., and Buckley, C. 1999.
Automatic text structuring and summarization. In
Mani, I. and Maybury, M. (Eds.), Advances in auto-
matic text summarization. MIT Press.
Sornlertlamvanich, V. 1993. Word segmentation for Thai
in machine translation system. Machine Translation,
National Electronics and Computer Technology Cen-
ter, 50–56.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.325254">
<title confidence="0.653067">A Practical Text Summarizer by Paragraph Extraction for Thai Chuleerat Jaruskulchai and Canasai Intelligent Information Retrieval and Database</title>
<affiliation confidence="0.983263">Department of Computer Science, Faculty of Kasetsart University, Bangkok,</affiliation>
<email confidence="0.965785">fscichj,g4364115@ku.ac.th</email>
<abstract confidence="0.9996498">In this paper, we propose a practical approach for extracting the most relevant paragraphs from the original document to form a summary for Thai text. The idea of our approach is to exploit both the local and global properties of paragraphs. The local property can be considered as clusters of significant words within each paragraph, while the global property can be though of as relations of all paragraphs in a document. These two properties are combined for ranking and extracting summaries. Experimental results on real-world data sets are encouraging.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>V Mittal</author>
<author>M Kantrowitz</author>
<author>J Goldstein</author>
</authors>
<title>Generating extraction-based summaries from hand-written summaries by aligning text spans.</title>
<date>1999</date>
<booktitle>In Proceedings of PACLING’99.</booktitle>
<contexts>
<context position="12892" citStr="Banko et al., 1999" startWordPosition="2110" endWordPosition="2113">ark positions of significant words for identifying the clusters. Our goal is to find topical words, which are indicative of the topics underlying the document. According to Luhn’s approach, the term frequencies is used to weight all the words. The other term weighting scheme frequently used is TFIDF (Term Frequency Inverse Document Frequency) (Salton and Buckley, 1988). However, this technique needs a corpus for computing IDF score, causing the genre-dependent problem for generic text summarization task. In our work, we decide to use TLTF (Term Length Term Frequency) term weighting technique (Banko et al., 1999) for scoring words in the document instead of TFIDF. TLTF multiplies a monotonic function of the term length by a monotonic function of the term frequency. The basic idea of TLTF is based on the assumption that words that are used more frequently tend to be shorter. Such words are not strongly indicative of the topics underlying in the document, such as stopwords. In contrast, words that are used less frequently tend to be longer. One significant benefit of using TLTF term weighting technique for our task is that it does not require any external resources, only using the information within the</context>
</contexts>
<marker>Banko, Mittal, Kantrowitz, Goldstein, 1999</marker>
<rawString>Banko, M., Mittal, V., Kantrowitz, M., and Goldstein, J. 1999. Generating extraction-based summaries from hand-written summaries by aligning text spans. In Proceedings of PACLING’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Buyukkokten</author>
<author>H Garcia-Molina</author>
<author>A Paepcke</author>
</authors>
<title>Seeing the whole in parts: Text summarization for web browsing on handheld devices.</title>
<date>2001</date>
<pages>10</pages>
<contexts>
<context position="4582" citStr="Buyukkokten et al., 2001" startWordPosition="724" endWordPosition="728">cant words, how to discover relations of paragraphs, and an algorithm for combining these two approaches. Section 5 describes our experiments. Finally, we conclude in Section 6 with some directions of future work. 2 Related Work A comprehensive survey of text summarization approaches can be found in (Mani, 1999). We briefly review here based on extraction approach. Luhn (1959) proposed a simple but effective approach by using term frequencies and their related positions to weight sentences that are extracted to form a summary. Subsequent works have demonstrated the success of Luhn’s approach (Buyukkokten et al., 2001; Lam-Adesina and Jones, 2001; Jaruskulchai et al., 2003). Edmunson (1969) proposed the use of other features such as title words, sentence locations, and bonus words to improve sentence extraction. Goldstein et al. (1999) presented an extraction technique that assigns weighted scores for both statistical and linguistic features in the sentence. Recently, Salton et al. (1999) have developed a model for representing a document by using undirected graphs. The basic idea is to consider vertices as paragraphs and edges as the similarity between two paragraphs. They suggested that the most importan</context>
<context position="10876" citStr="Buyukkokten et al., 2001" startWordPosition="1771" endWordPosition="1774">ummaries by Extraction 4.1 Finding Clusters of Significant Words In this section, we first describe an approach for finding clusters of significant words in each paragraph to calculate the local clustering score. Our approach is reminiscent of Luhn’s approach (1959) but uses the other term weighting technique instead of the term frequency. Luhn suggested that the frequency of a word occurrence in a document, as well as its relative position determines its significance in that document. More recent works have also employed Luhn’s approach as a basis component for extracting relevant sentences (Buyukkokten et al., 2001; Lam-Adesina and Jones, 2001). This approach performs well despite of its simplicity. In our previous work (Jaruskulchai et al., 2003), we also applied this approach for summarizing and browsing Thai documents through PDAs. Let Q be a subset of a continuous sequence of words in a paragraph, {wu ... w„}. The subset Q is called a cluster of significant words if it has these characteristics: • The first word wu and the last word w„ in the sequence are significant words. • Significant words are separated by not more than a predefined number of insignificant words. For example, we can partition a </context>
</contexts>
<marker>Buyukkokten, Garcia-Molina, Paepcke, 2001</marker>
<rawString>Buyukkokten, O., Garcia-Molina, H., and Paepcke, A. 2001. Seeing the whole in parts: Text summarization for web browsing on handheld devices. WWW10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W T Chuang</author>
<author>J Yang</author>
</authors>
<title>Extracting sentence segments for text summarization: A machine learning approach.</title>
<date>2000</date>
<booktitle>In Proceedings of the 23rd ACM SIGIR,</booktitle>
<pages>152--159</pages>
<contexts>
<context position="5728" citStr="Chuang and Yang (2000)" startWordPosition="902" endWordPosition="905">the similarity between two paragraphs. They suggested that the most important paragraphs should be linked to many other paragraphs, which are likely to discuss topic covered in those paragraphs. Statistical learning approaches have also been studied in text summarization problem. The first known supervised learning algorithm was proposed by Kupiec et al. (1995). Their approach estimates the probability that a sentence should be included in a summary given its feature values based on the independent assumption of Bayes’ Rule. Other supervised learning algorithms have already been investigated. Chuang and Yang (2000) studied several algorithms for extracting sentence segments, such as decision tree, naive Bayes classifier, and neural network. They also used rhetorical relations for representing features. One drawback of the supervised learning algorithms is that they require an annotated corpus to learn accurately. However, they may perform well for summarizing documents in a specific domain. This paper presents an approach for extracting the most relevant paragraphs from the original document to form a summary. The idea of our approach is to exploit both the local and global properties of paragraphs. The</context>
</contexts>
<marker>Chuang, Yang, 2000</marker>
<rawString>Chuang, W. T., and Yang, J. 2000. Extracting sentence segments for text summarization: A machine learning approach. In Proceedings of the 23rd ACM SIGIR, 152–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Edmundson</author>
</authors>
<title>New methods in automatic extraction.</title>
<date>1969</date>
<journal>Journal of the ACM,</journal>
<volume>16</volume>
<issue>2</issue>
<marker>Edmundson, 1969</marker>
<rawString>Edmundson, H. P. 1969. New methods in automatic extraction. Journal of the ACM, 16(2):264–285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goldstein</author>
<author>M Kantrowitz</author>
<author>V Mittal</author>
<author>J Carbonell</author>
</authors>
<title>Summarizing text documents: Sentence selection and evaluation metrics.</title>
<date>1999</date>
<booktitle>In Proceedings of the 22nd ACM SIGIR,</booktitle>
<pages>121--128</pages>
<contexts>
<context position="1787" citStr="Goldstein et al., 1999" startWordPosition="267" endWordPosition="270">ummarization is a solution for dealing with this problem. Automatic text summarization can be broadly classified into two approaches: abstraction and extraction. In contrast to abstraction that requires using heavy machinery from natural language processing (NLP), including grammars and lexicons for parsing and generation (Hahn and Mani, 2000), extraction can be easily viewed as the process of selecting relevant excerpts (sentences, paragraphs, etc.) from the original document and concatenating them into a shorter form. Thus, most of recent works in this research area are based on extraction (Goldstein et al., 1999). Although one may argue that extraction approach makes the text hard to read due to the lack of coherence, it also depends on the objective of summarization. If we need to generate summaries that can be used to indicative what topics are addressed in the original document, and thus can be used to alert the uses as the source content, i.e., the indicative function (Mani et al., 1999), extraction approach is capable of handling this kind of tasks. There have been many researches on text summarization problem. However, in Thai, we are in the initial stage of developing mechanisms for automatical</context>
<context position="4804" citStr="Goldstein et al. (1999)" startWordPosition="760" endWordPosition="763"> Work A comprehensive survey of text summarization approaches can be found in (Mani, 1999). We briefly review here based on extraction approach. Luhn (1959) proposed a simple but effective approach by using term frequencies and their related positions to weight sentences that are extracted to form a summary. Subsequent works have demonstrated the success of Luhn’s approach (Buyukkokten et al., 2001; Lam-Adesina and Jones, 2001; Jaruskulchai et al., 2003). Edmunson (1969) proposed the use of other features such as title words, sentence locations, and bonus words to improve sentence extraction. Goldstein et al. (1999) presented an extraction technique that assigns weighted scores for both statistical and linguistic features in the sentence. Recently, Salton et al. (1999) have developed a model for representing a document by using undirected graphs. The basic idea is to consider vertices as paragraphs and edges as the similarity between two paragraphs. They suggested that the most important paragraphs should be linked to many other paragraphs, which are likely to discuss topic covered in those paragraphs. Statistical learning approaches have also been studied in text summarization problem. The first known s</context>
</contexts>
<marker>Goldstein, Kantrowitz, Mittal, Carbonell, 1999</marker>
<rawString>Goldstein, J., Kantrowitz, M., Mittal, V., and Carbonell, J. 1999. Summarizing text documents: Sentence selection and evaluation metrics. In Proceedings of the 22nd ACM SIGIR, 121–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Hahn</author>
<author>I Mani</author>
</authors>
<title>The challenges of automatic summarization.</title>
<date>2000</date>
<journal>IEEE Computer,</journal>
<volume>33</volume>
<issue>11</issue>
<contexts>
<context position="1509" citStr="Hahn and Mani, 2000" startWordPosition="222" endWordPosition="225">texts is becoming increasingly common. Newspapers or magazines tend to be available on the World-Wide Web. Summarizing these texts can help users access to the information content more quickly. However, doing this task by humans is costly and time-consuming. Automatic text summarization is a solution for dealing with this problem. Automatic text summarization can be broadly classified into two approaches: abstraction and extraction. In contrast to abstraction that requires using heavy machinery from natural language processing (NLP), including grammars and lexicons for parsing and generation (Hahn and Mani, 2000), extraction can be easily viewed as the process of selecting relevant excerpts (sentences, paragraphs, etc.) from the original document and concatenating them into a shorter form. Thus, most of recent works in this research area are based on extraction (Goldstein et al., 1999). Although one may argue that extraction approach makes the text hard to read due to the lack of coherence, it also depends on the objective of summarization. If we need to generate summaries that can be used to indicative what topics are addressed in the original document, and thus can be used to alert the uses as the s</context>
</contexts>
<marker>Hahn, Mani, 2000</marker>
<rawString>Hahn, U., and Mani, I. 2000. The challenges of automatic summarization. IEEE Computer, 33(11):29–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Jaruskulchai</author>
<author>A Khanthong</author>
<author>W Tantiprasongchai</author>
</authors>
<title>A Framework for Delivery of Thai Content through Mobile Devices.</title>
<date>2003</date>
<booktitle>Closing Gaps in the Digital Divide Regional Conference on Digital GMS. Asian Institute of Technology,</booktitle>
<pages>190--194</pages>
<contexts>
<context position="4639" citStr="Jaruskulchai et al., 2003" startWordPosition="733" endWordPosition="736">d an algorithm for combining these two approaches. Section 5 describes our experiments. Finally, we conclude in Section 6 with some directions of future work. 2 Related Work A comprehensive survey of text summarization approaches can be found in (Mani, 1999). We briefly review here based on extraction approach. Luhn (1959) proposed a simple but effective approach by using term frequencies and their related positions to weight sentences that are extracted to form a summary. Subsequent works have demonstrated the success of Luhn’s approach (Buyukkokten et al., 2001; Lam-Adesina and Jones, 2001; Jaruskulchai et al., 2003). Edmunson (1969) proposed the use of other features such as title words, sentence locations, and bonus words to improve sentence extraction. Goldstein et al. (1999) presented an extraction technique that assigns weighted scores for both statistical and linguistic features in the sentence. Recently, Salton et al. (1999) have developed a model for representing a document by using undirected graphs. The basic idea is to consider vertices as paragraphs and edges as the similarity between two paragraphs. They suggested that the most important paragraphs should be linked to many other paragraphs, w</context>
<context position="11011" citStr="Jaruskulchai et al., 2003" startWordPosition="1792" endWordPosition="1795">of significant words in each paragraph to calculate the local clustering score. Our approach is reminiscent of Luhn’s approach (1959) but uses the other term weighting technique instead of the term frequency. Luhn suggested that the frequency of a word occurrence in a document, as well as its relative position determines its significance in that document. More recent works have also employed Luhn’s approach as a basis component for extracting relevant sentences (Buyukkokten et al., 2001; Lam-Adesina and Jones, 2001). This approach performs well despite of its simplicity. In our previous work (Jaruskulchai et al., 2003), we also applied this approach for summarizing and browsing Thai documents through PDAs. Let Q be a subset of a continuous sequence of words in a paragraph, {wu ... w„}. The subset Q is called a cluster of significant words if it has these characteristics: • The first word wu and the last word w„ in the sequence are significant words. • Significant words are separated by not more than a predefined number of insignificant words. For example, we can partition a continuous sequence of words in a paragraph into clusters as shown in Figure 1. The paragraph consists of twelve words. We use the bold</context>
</contexts>
<marker>Jaruskulchai, Khanthong, Tantiprasongchai, 2003</marker>
<rawString>Jaruskulchai, C., Khanthong, A., and Tantiprasongchai, W. 2003. A Framework for Delivery of Thai Content through Mobile Devices. Closing Gaps in the Digital Divide Regional Conference on Digital GMS. Asian Institute of Technology, 190–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Jing</author>
<author>R Barzilay</author>
<author>K McKeown</author>
<author>M Elhadad</author>
</authors>
<title>Summarization evaluation methods: Experiments and analysis.</title>
<date>1998</date>
<booktitle>AAAI Intelligent Text Summarization Workshop,</booktitle>
<pages>60--68</pages>
<contexts>
<context position="20131" citStr="Jing et al., 1998" startWordPosition="3353" endWordPosition="3356"> varied depending on the relative importance of the components G0 and L0. Therefore, we can rank all the paragraphs according to their combination scores in decreasing order. We finally extract n top-ranked paragraphs corresponding to the compression rate, and rearrange them in chronological order to form the output summary. 5 Experiments 5.1 Data Sets The typical approach for testing a summarization system is to create an “ideal” summary, either by professional abstractors or merging summaries provided by multiple human subjects using methods such as majority opinion, union, or intersection (Jing et al., 1998). This approach is known as intrinsic method. Unlike in English, standard data sets in Thai are not yet available for evaluating text summarization system. However, in order to observe characteristics of our algorithm, we collected Thai documents, including agricultural news (D1.AN), general news (D2.GN), and columnist’s articles (D3.CA) to make data sets. Each data set consists of 10 documents, and document sizes range from 1 to 4 pages. We asked a student in the Department of Thais, Faculty of Liberal Arts, for manual summarization by selecting the most relevant paragraphs that can indicate </context>
</contexts>
<marker>Jing, Barzilay, McKeown, Elhadad, 1998</marker>
<rawString>Jing, H., Barzilay, R., McKeown, K., and Elhadad, M. 1998. Summarization evaluation methods: Experiments and analysis. AAAI Intelligent Text Summarization Workshop, 60–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Jing</author>
<author>K McKeown</author>
</authors>
<title>Cut and paste based text summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<marker>Jing, McKeown, 2000</marker>
<rawString>Jing, H., and McKeown, K. 2000. Cut and paste based text summarization. In Proceedings of the 1st Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kupiec</author>
<author>J Pedersen</author>
<author>F Chen</author>
</authors>
<title>A trainable document summarizer.</title>
<date>1995</date>
<booktitle>In Proceedings of the 18th ACM SIGIR,</booktitle>
<pages>68--73</pages>
<contexts>
<context position="5469" citStr="Kupiec et al. (1995)" startWordPosition="862" endWordPosition="865">ns weighted scores for both statistical and linguistic features in the sentence. Recently, Salton et al. (1999) have developed a model for representing a document by using undirected graphs. The basic idea is to consider vertices as paragraphs and edges as the similarity between two paragraphs. They suggested that the most important paragraphs should be linked to many other paragraphs, which are likely to discuss topic covered in those paragraphs. Statistical learning approaches have also been studied in text summarization problem. The first known supervised learning algorithm was proposed by Kupiec et al. (1995). Their approach estimates the probability that a sentence should be included in a summary given its feature values based on the independent assumption of Bayes’ Rule. Other supervised learning algorithms have already been investigated. Chuang and Yang (2000) studied several algorithms for extracting sentence segments, such as decision tree, naive Bayes classifier, and neural network. They also used rhetorical relations for representing features. One drawback of the supervised learning algorithms is that they require an annotated corpus to learn accurately. However, they may perform well for s</context>
</contexts>
<marker>Kupiec, Pedersen, Chen, 1995</marker>
<rawString>Kupiec, J., Pedersen, J., and Chen, F. 1995. A trainable document summarizer. In Proceedings of the 18th ACM SIGIR, 68–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lam-Adesina</author>
<author>G J F Jones</author>
</authors>
<title>Applying summarization techniques for term selection in relevance feedback.</title>
<date>2001</date>
<booktitle>In Proceedings of the 24th ACM SIGIR,</booktitle>
<pages>1--9</pages>
<contexts>
<context position="4611" citStr="Lam-Adesina and Jones, 2001" startWordPosition="729" endWordPosition="732">r relations of paragraphs, and an algorithm for combining these two approaches. Section 5 describes our experiments. Finally, we conclude in Section 6 with some directions of future work. 2 Related Work A comprehensive survey of text summarization approaches can be found in (Mani, 1999). We briefly review here based on extraction approach. Luhn (1959) proposed a simple but effective approach by using term frequencies and their related positions to weight sentences that are extracted to form a summary. Subsequent works have demonstrated the success of Luhn’s approach (Buyukkokten et al., 2001; Lam-Adesina and Jones, 2001; Jaruskulchai et al., 2003). Edmunson (1969) proposed the use of other features such as title words, sentence locations, and bonus words to improve sentence extraction. Goldstein et al. (1999) presented an extraction technique that assigns weighted scores for both statistical and linguistic features in the sentence. Recently, Salton et al. (1999) have developed a model for representing a document by using undirected graphs. The basic idea is to consider vertices as paragraphs and edges as the similarity between two paragraphs. They suggested that the most important paragraphs should be linked</context>
<context position="10906" citStr="Lam-Adesina and Jones, 2001" startWordPosition="1775" endWordPosition="1778"> Finding Clusters of Significant Words In this section, we first describe an approach for finding clusters of significant words in each paragraph to calculate the local clustering score. Our approach is reminiscent of Luhn’s approach (1959) but uses the other term weighting technique instead of the term frequency. Luhn suggested that the frequency of a word occurrence in a document, as well as its relative position determines its significance in that document. More recent works have also employed Luhn’s approach as a basis component for extracting relevant sentences (Buyukkokten et al., 2001; Lam-Adesina and Jones, 2001). This approach performs well despite of its simplicity. In our previous work (Jaruskulchai et al., 2003), we also applied this approach for summarizing and browsing Thai documents through PDAs. Let Q be a subset of a continuous sequence of words in a paragraph, {wu ... w„}. The subset Q is called a cluster of significant words if it has these characteristics: • The first word wu and the last word w„ in the sequence are significant words. • Significant words are separated by not more than a predefined number of insignificant words. For example, we can partition a continuous sequence of words i</context>
</contexts>
<marker>Lam-Adesina, Jones, 2001</marker>
<rawString>Lam-Adesina, M., and Jones, G. J. F. 2001. Applying summarization techniques for term selection in relevance feedback. In Proceedings of the 24th ACM SIGIR, 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Luhn</author>
</authors>
<title>The automatic creation of literature abstracts.</title>
<date>1959</date>
<booktitle>IBMJournal of Research and Development,</booktitle>
<pages>159--165</pages>
<contexts>
<context position="4337" citStr="Luhn (1959)" startWordPosition="687" endWordPosition="688"> it with our work. Section 3 describes the preprocessing for Thai text, particularly on word segmentation. In Section 4, we present our approach for extracting relevant paragraphs in detail, including how to find clusters of significant words, how to discover relations of paragraphs, and an algorithm for combining these two approaches. Section 5 describes our experiments. Finally, we conclude in Section 6 with some directions of future work. 2 Related Work A comprehensive survey of text summarization approaches can be found in (Mani, 1999). We briefly review here based on extraction approach. Luhn (1959) proposed a simple but effective approach by using term frequencies and their related positions to weight sentences that are extracted to form a summary. Subsequent works have demonstrated the success of Luhn’s approach (Buyukkokten et al., 2001; Lam-Adesina and Jones, 2001; Jaruskulchai et al., 2003). Edmunson (1969) proposed the use of other features such as title words, sentence locations, and bonus words to improve sentence extraction. Goldstein et al. (1999) presented an extraction technique that assigns weighted scores for both statistical and linguistic features in the sentence. Recentl</context>
</contexts>
<marker>Luhn, 1959</marker>
<rawString>Luhn, H. P. 1959. The automatic creation of literature abstracts. IBMJournal of Research and Development, 159–165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
<author>T Firmin</author>
<author>D House</author>
<author>G Klein</author>
<author>B Sundheim</author>
<author>L Hirschman</author>
</authors>
<title>The TIPSTER SUMMAC Text Summarization Evaluation.</title>
<date>1999</date>
<booktitle>In Proceedings of EACL’99.</booktitle>
<contexts>
<context position="2173" citStr="Mani et al., 1999" startWordPosition="338" endWordPosition="341">ss of selecting relevant excerpts (sentences, paragraphs, etc.) from the original document and concatenating them into a shorter form. Thus, most of recent works in this research area are based on extraction (Goldstein et al., 1999). Although one may argue that extraction approach makes the text hard to read due to the lack of coherence, it also depends on the objective of summarization. If we need to generate summaries that can be used to indicative what topics are addressed in the original document, and thus can be used to alert the uses as the source content, i.e., the indicative function (Mani et al., 1999), extraction approach is capable of handling this kind of tasks. There have been many researches on text summarization problem. However, in Thai, we are in the initial stage of developing mechanisms for automatically summarizing documents. It is a challenge to summarize these documents, since they are extremely different from documents written in English. Similar to Chinese or Japanese, for the Thai writing system, there are no boundaries between adjoining words, and also there are no explicit sentences boundaries within the document. Fortunately, there is the use of the paragraph structure in</context>
</contexts>
<marker>Mani, Firmin, House, Klein, Sundheim, Hirschman, 1999</marker>
<rawString>Mani, I., Firmin, T., House, D., Klein, G., Sundheim, B., Hirschman, L. 1999. The TIPSTER SUMMAC Text Summarization Evaluation. In Proceedings of EACL’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
<author>M T Maybury</author>
</authors>
<title>Advances in actomatic text summarization.</title>
<date>1999</date>
<publisher>MIT Press.</publisher>
<marker>Mani, Maybury, 1999</marker>
<rawString>Mani, I., and Maybury, M. T. 1999. Advances in actomatic text summarization. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Ohsawa</author>
<author>N E Benson</author>
<author>M Yachida</author>
</authors>
<title>KeyGraph: Automatic indexing by cooccurrence graph based on building construction metaphor.</title>
<date>1998</date>
<booktitle>In Proceedings of EAdvanced Digital Library Conference.</booktitle>
<contexts>
<context position="8793" citStr="Ohsawa et al., 1998" startWordPosition="1415" endWordPosition="1418">e candidates in the document and genW = I In our work, the lexicon contained 32675 words. However, the limitation of this algorithm is that if the target words are compound words or unknown words, it tends to produce incorrect results. For example, a compound word is segmented as the following: องคกรสิทธิมนุษยชน (Human Rights Organization) องคกร_สิทธิ_มนุ_ษย_ชน Since this compound word does not appear in the lexicon, it becomes small useless words after the word segmentation process. We further describe an efficient approach to alleviate this problem by using an idea of phrase construction (Ohsawa et al., 1998). Let wz be a word that is firstly tokenized by using the longest matching algorithm. We refer to w1w2 ... wn as a phrase candidate, if n &gt; 1, and no punctuation and stopwords occur between w1 and wn. It is well accepted in information retrieval community that words can be broadly classified into content-bearing words and stopwords. In Thai, we found that words that perform as function words can be used in place of stopwords similar to English. We collected 253 most frequently occurred words for making a list of Thai stopwords. Given a phrase candidate consisting of n words, we can generate a </context>
</contexts>
<marker>Ohsawa, Benson, Yachida, 1998</marker>
<rawString>Ohsawa, Y., Benson, N. E., and Yachida, M. 1998. KeyGraph: Automatic indexing by cooccurrence graph based on building construction metaphor. In Proceedings of EAdvanced Digital Library Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>C Buckley</author>
</authors>
<title>Term weighting approaches in automatic text retrieval.</title>
<date>1988</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>24--5</pages>
<contexts>
<context position="12644" citStr="Salton and Buckley, 1988" startWordPosition="2072" endWordPosition="2075"> the local clustering score for paragraph si can be calculated as follows: where ns(β, si) is the number of bracketed significant words, and n(β, si) is the total number of bracketed words. We can see that the first important step in this process is to mark positions of significant words for identifying the clusters. Our goal is to find topical words, which are indicative of the topics underlying the document. According to Luhn’s approach, the term frequencies is used to weight all the words. The other term weighting scheme frequently used is TFIDF (Term Frequency Inverse Document Frequency) (Salton and Buckley, 1988). However, this technique needs a corpus for computing IDF score, causing the genre-dependent problem for generic text summarization task. In our work, we decide to use TLTF (Term Length Term Frequency) term weighting technique (Banko et al., 1999) for scoring words in the document instead of TFIDF. TLTF multiplies a monotonic function of the term length by a monotonic function of the term frequency. The basic idea of TLTF is based on the assumption that words that are used more frequently tend to be shorter. Such words are not strongly indicative of the topics underlying in the document, such</context>
</contexts>
<marker>Salton, Buckley, 1988</marker>
<rawString>Salton, G., and Buckley, C. 1988. Term weighting approaches in automatic text retrieval. Information Processing and Management, 24(5):513–523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>A Singhal</author>
<author>M Mitra</author>
<author>C Buckley</author>
</authors>
<title>Automatic text structuring and summarization.</title>
<date>1999</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4960" citStr="Salton et al. (1999)" startWordPosition="783" endWordPosition="786">oposed a simple but effective approach by using term frequencies and their related positions to weight sentences that are extracted to form a summary. Subsequent works have demonstrated the success of Luhn’s approach (Buyukkokten et al., 2001; Lam-Adesina and Jones, 2001; Jaruskulchai et al., 2003). Edmunson (1969) proposed the use of other features such as title words, sentence locations, and bonus words to improve sentence extraction. Goldstein et al. (1999) presented an extraction technique that assigns weighted scores for both statistical and linguistic features in the sentence. Recently, Salton et al. (1999) have developed a model for representing a document by using undirected graphs. The basic idea is to consider vertices as paragraphs and edges as the similarity between two paragraphs. They suggested that the most important paragraphs should be linked to many other paragraphs, which are likely to discuss topic covered in those paragraphs. Statistical learning approaches have also been studied in text summarization problem. The first known supervised learning algorithm was proposed by Kupiec et al. (1995). Their approach estimates the probability that a sentence should be included in a summary </context>
<context position="14272" citStr="Salton et al., 1999" startWordPosition="2361" endWordPosition="2364">ach for discovering relations of paragraphs. Given a document D, we can represent it by an undirected graph G = (V, E), where V = {s1, ... , sm} is the set of paragraphs in that document. An edge (si, sj) is in E, if the cosine similarity between paragraphs si and sj is above a certain threshold, denoted α. A paragraph si is considered to be a set of words {wsi,1, wsi,2, ... , wsi,t}. The cosine similarity between two paragraphs can be calculated by the following formula: 2 Etk=1 ws k i,kws�, sim(si, sj) = (3) Et 2 t 2 k=1 wsi k Ek=1 wsj,k The graph G is called the text relationship map of D (Salton et al., 1999). Let dsi be the degree of node si. We then refer to dsi as the global connectivity score. Generating a summary for a given document can be processed by sorting all the nodes with dsi in decreasing order, and then extracting n top-ranked nodes, where n is the target number of paragraphs in the summary. This idea is based on Salton et al.’s approach that also performs extraction at the paragraph level. They suggested that since a highly bushy node is linked to a number of other nodes, it has an overlapping vocabulary with several paragraphs, and is likely to discuss topics covered in many other</context>
</contexts>
<marker>Salton, Singhal, Mitra, Buckley, 1999</marker>
<rawString>Salton, G., Singhal, A., Mitra, M., and Buckley, C. 1999. Automatic text structuring and summarization. In Mani, I. and Maybury, M. (Eds.), Advances in automatic text summarization. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Sornlertlamvanich</author>
</authors>
<title>Word segmentation for Thai in machine translation system.</title>
<date>1993</date>
<booktitle>Machine Translation, National Electronics and Computer Technology Center,</booktitle>
<pages>50--56</pages>
<contexts>
<context position="7156" citStr="Sornlertlamvanich, 1993" startWordPosition="1133" endWordPosition="1134">n be combined and tuned to produce a single measure reflecting the informativeness of each paragraph. Finally, we can apply this combination measure for ranking and extracting the most relevant paragraphs. 3 Preprocessing for Thai Text The first step for working with Thai text is to tokenize a given text into meaningful words, since the Thai writing system has no delimiters to indicate word boundaries. Thai words are not delimited by spaces. The spaces are only used to break the idea or draw readers’ attention. In order to determine word boundaries, we employed the longest matching algorithm (Sornlertlamvanich, 1993). The longest matching algorithm starts with a text span that could be a phrase or a sentence. The algorithm tries to align word boundaries according to the longest possible matching character compounds in a lexicon. If no match is found in the lexicon, it drops the rightmost character in that text according to the morphological rules and begins the same search. If a word is found, it marks a boundary at the end of the longest word, and then begins the same search starting at the remainder following the match. w1w2 w1w2w3 ... w1w2w3 ... wn−1wn } w2w3 ... w2w3 ... wn−1wn (1) ... wn−1wn For exam</context>
</contexts>
<marker>Sornlertlamvanich, 1993</marker>
<rawString>Sornlertlamvanich, V. 1993. Word segmentation for Thai in machine translation system. Machine Translation, National Electronics and Computer Technology Center, 50–56.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>