<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002446">
<title confidence="0.988826">
Categorization of Turkish News Documents with Morphological Analysis
</title>
<author confidence="0.934566">
Burak Kerim Akkus¸
</author>
<affiliation confidence="0.822841333333333">
Computer Engineering Department
Middle East Technical University
Ankara, Turkey
</affiliation>
<email confidence="0.995817">
burakkerim@ceng.metu.edu.tr
</email>
<author confidence="0.732374">
Ruket C¸ akıcı
</author>
<affiliation confidence="0.743632">
Computer Engineering Department
Middle East Technical University
Ankara, Turkey
</affiliation>
<email confidence="0.997076">
ruken@ceng.metu.edu.tr
</email>
<sectionHeader confidence="0.993857" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999705461538462">
Morphologically rich languages such as
Turkish may benefit from morphological
analysis in natural language tasks. In this
study, we examine the effects of morpho-
logical analysis on text categorization task
in Turkish. We use stems and word cate-
gories that are extracted with morphologi-
cal analysis as main features and compare
them with fixed length stemmers in a bag
of words approach with several learning
algorithms. We aim to show the effects
of using varying degrees of morphological
information.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999977964912281">
The goal of text classification is to find the cat-
egory or the topic of a text. Text categorization
has popular applications in daily life such as email
routing, spam detection, language identification,
audience detection or genre detection and has ma-
jor part in information retrieval tasks.
The aim of this study is to explain the impact of
morphological analysis and POS tagging on Turk-
ish text classification task. We train various classi-
fiers such as k-Nearest Neighbours (kNN), Naive
Bayes (NB) and Support Vector Machines (SVM)
for this task. Turkish NLP tasks have been proven
to benefit from morphological analysis or segmen-
tation of some sort (Eryi˘git et al., 2008; C¸etinoˇglu
and Oflazer, 2006; C¸akıcı and Baldridge, 2006).
Two different settings are used throughout the pa-
per to represent different degrees of stemming and
involvement of morphological information. The
first one uses the first n-characters (prefixes) of
each word in a bag of words approach. A variety
of number of characters are compared from 4 to 7
to find the optimal length for data representation.
This acts as the baseline for word segmentation
in order to make the limited amount of data less
sparse. The second setting involves word stems
that are extracted with a morphological analysis
followed by disambiguation. The effects of part of
speech tagging are also explored. Disambiguated
morphological data are used along with the part of
speech tags as informative features about the word
category.
Extracting an n-character prefix is simple and
considerably cheap compared to complex state-
of-the-art morphological analysis and disambigua-
tion process. There is a trade-off between quality
and expense. Therefore, we may choose to use a
cheap approximation instead of a more accurate
representation if there is no significant sacrifice in
the success of the system. Turkish is an agglutina-
tive language that mostly uses suffixes1. There-
fore, approximate stems that are extracted with
fixed size stemming rarely contain any affixes.
The training data used in this study consist of
news articles taken from Milliyet Corpus that con-
tains 80293 news articles published in the news-
paper Milliyet (Hakkani-T¨ur et al., 2000) 2. The
articles we use for training contain a subset of doc-
uments indexed from 1000-5000 and have at least
500 characters. The test set is not included in the
original corpus, but it has also been downloaded
form Milliyet’s public website 3.
The data used in this study have been ana-
lyzed with the morphological analyser described
in Oflazer (1993) and disambiguated with Sak et
al. (2007)’s morphological disambiguator. The
data have been manually labelled for training and
test. The annotated data is made available for pub-
</bodyText>
<footnote confidence="0.912914222222222">
1It has only one prefix for intensifying adjectives and ad-
verbs (sımsıcak: very hot). It is just a modified version of the
first syllable of the original word and also it is not common.
There are other prefixes adopted from foreign languages such
as anormal (abnormal), antisosyal (antisocial) or namert (not
brave).
2Thanks to Kemal Oflazer for letting us use the corpus
3http://www.milliyet.com.tr
1
</footnote>
<note confidence="0.590617">
Proceedings of the ACL Student Research Workshop, pages 1–8,
</note>
<page confidence="0.395373">
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</page>
<bodyText confidence="0.99987975">
lic use 4. By making our manually annotated data
available, we hope to contribute to future work in
this area.
The rest of the paper is organized as follows.
Section 2 briefly describes the classification meth-
ods used, section 3 explains how these methods
are used in implementation and finally the paper is
concluded with experimental results.
</bodyText>
<sectionHeader confidence="0.974503" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999330571428572">
Supervised and unsupervised methods have been
used for text classification in different languages
(Amasyalı and Diri, 2006; Beil et al., 2002).
Among these are Naive Bayes classification (Mc-
Callum and Nigam, 1998; Schneider, 2005), deci-
sion trees (Johnson et al., 2002) , neural networks
(Ng et al., 1997), k-nearest neighbour classifiers
(Lim, 2004) and support-vector machines (Shana-
han and Roma, 2003).
Bag-of-words model is one of the more intu-
itive ways to represent text files in text classi-
fication. It is simple, it ignores syntax, gram-
mar and the relative positions of the words in
the text (Harris, 1970). Each document is repre-
sented with an unordered list of words and each of
the word frequencies in the collection becomes a
feature representing the document. Bag-of-words
approach is an intuitive way and popular among
document classification tasks (Scott and Matwin,
1998; Joachims, 1997).
Another way of representing documents with
term weights is to use term frequency - inverse
document frequency (Sparck Jones, 1988). TFIDF
is another way of saying that a term is valuable for
a document if it occurs frequently in that docu-
ment but it is not common in the rest of the collec-
tion. TFIDF score of a term t in a document d in a
collection D is calculated as below:
</bodyText>
<equation confidence="0.99356">
tfidft,d,D = tft,d × idft,D
</equation>
<bodyText confidence="0.998982666666667">
tft,d is the number of times t occurs in d and idft,D
is the number of documents in D over the number
of document that contain t.
The idea behind bag of words and TFIDF is to
find a mapping from words to numbers which can
also be described as finding a mathematical rep-
resentation for text files. The output is a matrix
representation of the collection. This is also called
vector space model representation of the collec-
</bodyText>
<footnote confidence="0.977439">
4http://www.ceng.metu.edu.tr/ burakkerim/text cat
</footnote>
<bodyText confidence="0.999410833333333">
tion in which we can define similarity and dis-
tance metrics for documents. One way is to use
dot product since each document is represented as
a vector (Manning et al., 2008). A number of dif-
ferent dimensions in vector spaces are compared
in this study to find the optimal performance.
</bodyText>
<subsectionHeader confidence="0.971059">
2.1 Morphology
</subsectionHeader>
<bodyText confidence="0.999953166666667">
Languages such as Turkish, Czech and Finnish
have more complex morphology and cause addi-
tional difficulties which requires special handling
on linguistic studies compared to languages such
as English (Sak et al., 2007). Morphemes may
carry semantic or syntactic information, but mor-
phological ambiguity make it hard to pass this in-
formation on to other level in a trivial manner es-
pecially for languages with productive morphol-
ogy such as Turkish. An example of possible mor-
phological analyses of a single word in Turkish is
presented in Table 1.
</bodyText>
<equation confidence="0.926927833333333">
alın+Noun+A3sg+Pnon+Nom (forehead)
al+Adj&amp;quot;DB+Noun+Zero+A3sg+P2sg+Nom (your red)
al+Adj&amp;quot;DB+Noun+Zero+A3sg+Pnon+Gen (of red)
al+Verb+Pos+Imp+A2pl ((you) take)
al+Verb&amp;quot;DB+Verb+Pass+Pos+Imp+A2sg ((you) be taken)
alın+Verb+Pos+Imp+A2sg ((you) be offended)
</equation>
<bodyText confidence="0.984631652173913">
Table 1: Morphological analysis of the word
”alın” in Turkish with the corresponding mean-
ings.
We aim to examine the effects of morpholog-
ical information in a bag-of-words model in the
context of text classification. A relevant study
explores the prefixing versus morphological anal-
ysis/stemming effect on information retrieval in
Can et al. (2008). Several stemmers for Turkish
are presented for the indexing problem for infor-
mation retrieval. They use Oflazer’s morphologi-
cal analyzer (Oflazer, 1993), however, they do not
use a disambiguator. Instead they choose the most
common analysis among the candidates. Their re-
sults show that among the fixed length stemmers
5-character prefix is the the best and the lemma-
tizer based stemmer is slightly better than the fixed
length stemmer with five characters. However,
they also note that the difference is statistically in-
significant. We use Sak et al. (2007)’s disambigua-
tor which is reported with a 96.45% accuracy in
their study and with a 87.67% accuracy by Eryi˘git
(2012)
</bodyText>
<page confidence="0.995416">
2
</page>
<figureCaption confidence="0.9999805">
Figure 1: Learning curves with first five characters
Figure 2: Learning curves with stems
</figureCaption>
<sectionHeader confidence="0.994361" genericHeader="method">
3 Implementation
</sectionHeader>
<bodyText confidence="0.999966857142857">
In the first setting, up to first N characters of each
word is extracted as the feature set. A compari-
son between 4, 5, 6 and 7 characters is performed
to choose the best N. In the second setting we
use morphological analysis. Each word in docu-
ments is analysed morphologically with morpho-
logical analyser from Oflazer (1993) and word
stems are extracted for each term. Sak’s mor-
phological disambiguator for Turkish is used at
this step to choose the correct analysis (Sak et
al., 2007). Stems are the primary features used
for classification. Finally, we add word categories
from this analysis as features as POS tags.
We compare these settings in order to see how
well morphological analysis with disambiguation
performs against a simple baseline of fixed length
stemming with a bag-of-words approach. Both
stem bags and the first N-character bags are trans-
formed into vector space with TFIDF scoring.
Then, different sizes of feature space dimensions
are used with ranking by the highest term fre-
quency scores. A range of different dimension
sizes from 1200 to 7200 were experimented on to
find the optimal dimension size for this study (Ta-
ble 2). After the collection is mapped into vector
space, several learning algorithms are applied for
classification. K-Nearest neighbours was imple-
mented with weighted voting of 25 nearest neigh-
bours based on distance and Support Vector Ma-
chine is implemented with linear kernel and de-
fault parameters. These methods are used with
Python, NLTK (Loper and Bird, 2002) and Sci-Kit
(Loper and Bird, 2002; Pedregosa et al., 2011).
Training data contains 872 articles labelled and
divided into four categories as follows: 235 ar-
ticles on politics, 258 articles about social news
such as culture, education or health, 177 arti-
cles on economics and 202 about sports. This
data are generated using bootstrapping. Docu-
ments are hand annotated with an initial classi-
fier that is trained on a smaller set of hand la-
belled data. Classifier is used on unknown sam-
</bodyText>
<page confidence="0.993546">
3
</page>
<bodyText confidence="0.9997035">
ples, then the predictions are manually checked to
gather enough data for each class. Test data con-
sists of 160 articles with 40 in each class. These
are also manually labelled.
</bodyText>
<sectionHeader confidence="0.999379" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999887875">
Experiments begin with searching the optimal pre-
fix length for words with different classifiers. Af-
ter that, stems are used as features and evaluated
with the same classifiers. Section 4.3 contains
the comparison of these two features. Finally,
morphological information is added to these fea-
tures and the effects of the extra information is in-
spected in Section 4.4 .
</bodyText>
<subsectionHeader confidence="0.992721">
4.1 Optimal Number of Characters
</subsectionHeader>
<bodyText confidence="0.999983863636364">
This experiment aims to find out the optimal pre-
fix length for the first N-character feature to rep-
resent text documents in Turkish. We conjecture
that we can simulate stemming by taking a fixed
length prefix of each word. This experiment was
performed with all of the 872 training files and
160 test files. Table 2 shows the results of the ex-
periments where columns represent the number of
characters used and rows represent the number of
features used for classification.
The best performance is acquired using the first
five characters of each word for TFIDF transfor-
mation for all classifiers. Can et al. (2008) also
reported that the five character prefix in the fixed
length stemmer performed the best in their ex-
periments. Learning curves for 5-character pre-
fixes are presented in Figure 1. Although, SVM
performs poorer on average compared to Naive
Bayes, their best performances show no signifi-
cant statistical difference according to McNemar’s
Test. On the other hand, kNN falls behind these
two on most of the configurations.
</bodyText>
<subsectionHeader confidence="0.957724">
4.2 Stems
</subsectionHeader>
<bodyText confidence="0.99998609375">
Another experiment was conducted with the word
stems extracted with a morphological analyser and
a disambiguator (Sak et al., 2007). kNN, Naive
Bayes and SVM were trained with different fea-
ture sizes with increasing training data sizes. The
learning curves are presented in Figure 2.
Naive Bayes performs best in this setting even
with a small feature set with few training sam-
ples. When the corpus size is small, using less
features gives better results in SVM and Naive
Bayes. As the number of features used in classi-
fication increases, the number of samples needed
for an adequate classification also increases for
Naive Bayes. The performance of SVM also in-
creases with the number of data used in training.
More documents leave space for repetitions for
stop words and common less informative words an
their TFIDF scores decrease and the get less im-
pact on the classification while informative words
in each category get relatively higher scores, there-
fore an increase in data size also increases perfor-
mance. As the training size increases feature space
dimension becomes irrelevant and the results con-
verge to a similar point for Naive Bayes. On the
other hand, 1200 features are not enough for kNN
and SVM. With larger feature sets kNN and SVM
also give similar results to Naive Bayes although
kNN is left behind especially with less number of
features since it directly relies on the similarity
based on these features in vector space and most of
them are same in each document since we choose
them with term frequency.
</bodyText>
<subsectionHeader confidence="0.981701">
4.3 5-Character Prefixes vs Stems
</subsectionHeader>
<bodyText confidence="0.999526647058823">
This section provides a comparison between two
main features used in this study with three differ-
ent classifiers. F1 scores for the best and worst
configurations with each of the three classifiers are
presented in Table 3. Using five character prefixes
gives better results than using stems. Naive Bayes
with stems and five character prefixes disagree
only on six instances out of 160 test instances with
F1 scores of 0.92 and 0.94 respectively in the best
configurations. There is no statistically significant
difference.
Similarly, results for SVM with stems for the
best and the worst configurations is considered to
be not statistically significant. McNemar’s Test
(McNemar, 1947) is shown to have low error in
detecting a significant difference when there is
none (Dietterich, 1998).
</bodyText>
<table confidence="0.9778846">
Worst Best
First 5 Stems First 5 Stems
KNN 91.250 86.875 92.500 91.875
NB 92.500 91.250 94.375 91.875
SVM 91.250 88.750 93.175 92.500
</table>
<tableCaption confidence="0.9917">
Table 3: Comparison of F1-scores for best and
worst results in each classifier with each feature.
</tableCaption>
<page confidence="0.940379">
4
</page>
<figure confidence="0.930828">
(a) Learning curves with word tags
</figure>
<figureCaption confidence="0.995906">
Figure 3: Learning curves for SVM
</figureCaption>
<figure confidence="0.9997685">
(a) Learning curves without tags
(b) Learning curves with stem tags
</figure>
<page confidence="0.883973">
5
</page>
<table confidence="0.982750125">
KNN NB SVM
4 5 6 7 4 5 6 7 4 5 6 7
1200 90.00 91.25 86.87 84.37 93.12 92.50 93.12 90.00 89.37 91.250 90.62 88.75
2400 89.37 91.25 87.50 86.62 89.37 91.25 87.50 86.62 90.62 91.87 90.00 88.12
3600 86.87 91.25 90.00 88.17 93.75 93.75 92.50 91.87 90.62 91.87 90.00 88.12
4800 90.00 91.87 91.25 88.17 93.12 93.75 91.87 91.25 90.62 91.87 90.00 88.12
6000 88.75 91.87 91.87 90.62 92.50 93.75 92.50 90.62 90.62 93.12 93.12 90.00
7200 89.37 92.50 91.25 89.37 90.62 94.37 91.87 91.25 90.62 92.50 91.25 90.62
</table>
<tableCaption confidence="0.999619">
Table 2: F1-scores with different prefix lengths and dimensions.
</tableCaption>
<subsectionHeader confidence="0.998031">
4.4 SVM with POS Tags
</subsectionHeader>
<bodyText confidence="0.999950533333333">
The final experiment examines the effects of POS
tags that are extracted via morphological analy-
sis. Two different features are extracted and com-
pared with the base lines of classifiers with stems
and first five characters without tags. Stem tag is
the first tag of the first derivation and the word
tag is the tag of the last derivation and example
features are given in Table 4. Since derivational
morphemes are also present in the morphological
analyses word tags may differ from stem tags. In
addition, words that are spelled in the same way
may belong to different categories or have dif-
ferent meanings that can be expressed with POS
tags. Al+Verb (take) and Al+Adj (red) are differ-
ent even though their surface forms are the same.
</bodyText>
<table confidence="0.991353625">
Analysis al+AdjˆDB+Noun+Zero+
A3sg+Pnon+Gen (of red)
First 5 characters. alın ( of red, forehead,
Stem (you) be taken, (you) be of-
Stem + Stem Tag fended ...)
Stem + Word Tag al ( red, take )
al+Adj ( red )
al+Noun ( red )
</table>
<tableCaption confidence="0.99844">
Table 4: Example features for word ”alın”.
</tableCaption>
<bodyText confidence="0.9999485">
Using POS tags with stems increases the suc-
cess rate especially when the number of features
is low. However, using tags of the stems does
not make significant changes on average. The best
and the worst results differ with baseline with less
than 0.01 points in F1 scores as seen in Figure 3.
This may be due to the fact that the same stem
has a higher chance of being in the same cate-
gory even though the derived final form is differ-
ent. Even though, this may add extra information
to the stems, results show no significant differ-
ence. Adding stem or word tags to the first five
characters increases the success when the number
of training instances are low, however, it has no
significant effect on the highest score. Using tags
with five characters has positive effects when the
number of features are low and negative effects
when the number of features are high.
</bodyText>
<sectionHeader confidence="0.996545" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999985433333334">
In this study, we use K-Nearest Neighbours, Naive
Bayes and Support Vector Machine classifiers for
examining the effects of morphological informa-
tion on the task of classifying Turkish news arti-
cles. We have compared their performances on
different sizes of training data, different number
of features and different feature sets. Results sug-
gest that the first five characters of each word can
be used for TFIDF transformation to represent text
documents in classification tasks. Another fea-
ture used in the study is word stems. Stems are
extracted with a morphological analyser which is
computationally expensive and takes a lot of time
compared to extracting first characters of a word.
Although different test sets and training data may
change the final results, using a simple approxi-
mation with first five characters to represent doc-
uments instead of results of an expensive morpho-
logical analysis process gives similar or better re-
sults with much less cost. Experiments also indi-
cate that there is more place for growth if more
training data is available as most of the learning
curves presented in the experiments point. We
particularly expect better results with POS tag
experiments with more data. Actual word cate-
gories and meanings may differ and using POS
tags may solve this problem but sparsity of the data
is more prominent at the moment. The future work
includes repeating these experiments with larger
data sets to explore the effects of the data size.
</bodyText>
<page confidence="0.998508">
6
</page>
<bodyText confidence="0.902382">
Zelig Harris. 1970. Distributional structure. In Pa-
pers in Structural and Transformational Linguis-
tics, pages 775–794. D. Reidel Publishing Company,
Dordrecht, Holland.
</bodyText>
<note confidence="0.60779775">
References
Charu C. Aggarwal and Philip S. Yu. 2000. Finding
generalized projected clusters in high dimensional
spaces. SIGMOD Rec., 29(2):70–81.
</note>
<reference confidence="0.972192639175257">
M. Fatih Amasyalı and Banu Diri. 2006. Automatic
Turkish text categorization in terms of author, genre
and gender. In Proceedings of the 11th international
conference on Applications of Natural Language
to Information Systems, NLDB’06, pages 221–226,
Berlin, Heidelberg. Springer-Verlag.
Florian Beil, Martin Ester, and Xiaowei Xu. 2002. Fre-
quent term-based text clustering. In Proceedings of
the eighth ACM SIGKDD international conference
on Knowledge discovery and data mining, KDD ’02,
pages 436–442, New York, NY, USA. ACM.
Fazlı Can, Seyit Koc¸berber, Erman Balc¸ık, Cihan Kay-
nak, H. C¸a˘gdas¸ ¨Ocalan, and Onur M. Vursavas¸.
2008. Information retrieval on turkish texts. JA-
SIST, 59(3):407–421.
Ruket C¸akıcı and Jason Baldridge. 2006. Projective
and non-projective Turkish parsing. In Proceedings
of the 5th International Treebanks and Linguistic
Theories Conference, pages 43–54.
¨Ozlem C¸etinoˇglu and Kemal Oflazer. 2006.
Morphology-syntax interface for Turkish LFG. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, ACL-44, pages 153–160, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Thomas G. Dietterich. 1998. Approximate statis-
tical tests for comparing supervised classification
learning algorithms. Neural Computation, 10:1895–
1923.
G¨uls¸en Eryi˘git. 2012. The impact of automatic mor-
phological analysis &amp; disambiguation on depen-
dency parsing of turkish. In Proceedings of the
Eighth International Conference on Language Re-
sources and Evaluation (LREC), Istanbul, Turkey,
23-25 May.
G¨uls¸en Eryi˘git, Joakim Nivre, and Kemal Oflazer.
2008. Dependency parsing of Turkish. Comput.
Linguist., 34(3):357–389, September.
George Forman. 2003. An extensive empirical study
of feature selection metrics for text classification. J.
Mach. Learn. Res., 3:1289–1305, March.
Dilek Z. Hakkani-T¨ur, Kemal Oflazer, and G¨okhan T¨ur.
2000. Statistical morphological disambiguation for
agglutinative languages. In Proceedings of the 18th
conference on Computational linguistics - Volume
1, COLING ’00, pages 285–291, Stroudsburg, PA,
USA. Association for Computational Linguistics.
M. Ikonomakis, S. Kotsiantis, and V. Tampakas. 2005.
Text classification: a recent overview. In Proceed-
ings of the 9th WSEAS International Conference on
Computers, ICCOMP’05, pages 1–6, Stevens Point,
Wisconsin, USA. World Scientific and Engineering
Academy and Society (WSEAS).
Thorsten Joachims. 1997. A probabilistic analysis of
the rocchio algorithm with tfidf for text categoriza-
tion. In Proceedings of the Fourteenth International
Conference on Machine Learning, ICML ’97, pages
143–151, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
D. E. Johnson, F. J. Oles, T. Zhang, and T. Goetz. 2002.
A decision-tree-based symbolic rule induction sys-
tem for text categorization. IBM Syst. J., 41(3):428–
437, July.
Heui-Seok Lim. 2004. Improving kNN based text
classification with well estimated parameters. In
Nikhil R. Pal, Nikola Kasabov, Rajani K. Mudi,
Srimanta Pal, and Swapan K. Parui, editors, Neu-
ral Information Processing, 11th International Con-
ference, ICONIP 2004, Calcutta, India, November
22-25, 2004, Proceedings, volume 3316 of Lec-
ture Notes in Computer Science, pages 516–523.
Springer.
Tao Liu, Shengping Liu, and Zheng Chen. 2003. An
evaluation on feature selection for text clustering. In
In ICML, pages 488–495.
Edward Loper and Steven Bird. 2002. Nltk: the nat-
ural language toolkit. In Proceedings of the ACL-
02 Workshop on Effective tools and methodologies
for teaching natural language processing and com-
putational linguistics - Volume 1, ETMTNLP ’02,
pages 63–70, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Christopher D. Manning and Hinrich Sch¨utze. 1999.
Foundations of statistical natural language process-
ing. MIT Press, Cambridge, MA, USA.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch¨utze. 2008. Introduction to Information
Retrieval. Cambridge University Press, New York,
NY, USA.
Andrew McCallum and Kamal Nigam. 1998. A com-
parison of event models for naive bayes text classifi-
cation. In Proceesings of the Workshop on learning
for text categorization, AAAI’98, pages 41–48.
Quinn McNemar. 1947. Note on the Sampling Error
of the Difference Between Correlated Proportions or
Percentages. Psychometrika, 12(2):153–157.
</reference>
<page confidence="0.990166">
7
</page>
<reference confidence="0.999639706896552">
412–420, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Hwee Tou Ng, Wei Boon Goh, and Kok Leong Low.
1997. Feature selection, perceptron learning, and a
usability case study for text categorization. In Pro-
ceedings of the 20th annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval, SIGIR ’97, pages 67–73, New
York, NY, USA. ACM.
Kemal Oflazer. 1993. Two-level description of Turk-
ish morphology. In Proceedings of the sixth con-
ference on European chapter of the Association for
Computational Linguistics, EACL ’93, pages 472–
472, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825–2830.
Has¸im Sak, Tunga G¨ung¨or, and Murat Sarac¸lar. 2007.
Morphological disambiguation of Turkish text with
perceptron algorithm. In Proceedings of the 8th
International Conference on Computational Lin-
guistics and Intelligent Text Processing, CICLing
’07, pages 107–118, Berlin, Heidelberg. Springer-
Verlag.
Karl-Michael Schneider. 2005. Techniques for im-
proving the performance of naive bayes for text
classification. In In Proceedings of CICLing 2005,
pages 682–693.
Sam Scott and Stan Matwin. 1998. Text classification
using WordNet hypernyms. In Workshop: Usage of
WordNet in Natural Language Processing Systems,
ACL’98, pages 45–52.
James G. Shanahan and Norbert Roma. 2003. Boost-
ing support vector machines for text classification
through parameter-free threshold relaxation. In
Proceedings of the twelfth international conference
on Information and knowledge management, CIKM
’03, pages 247–254, New York, NY, USA. ACM.
Karen Sparck Jones. 1988. A statistical interpretation
of term specificity and its application in retrieval.
In Peter Willett, editor, Document retrieval systems,
pages 132–142. Taylor Graham Publishing, London,
UK, UK.
Yiming Yang and Xin Liu. 1999. A re-examination
of text categorization methods. In Proceedings of
the 22nd annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, SIGIR ’99, pages 42–49, New York, NY,
USA. ACM.
Yiming Yang and Jan O. Pedersen. 1997. A compar-
ative study on feature selection in text categoriza-
tion. In Proceedings of the Fourteenth International
Conference on Machine Learning, ICML ’97, pages
</reference>
<page confidence="0.998492">
8
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.344312">
<title confidence="0.993409">Categorization of Turkish News Documents with Morphological Analysis</title>
<author confidence="0.529422">Kerim</author>
<affiliation confidence="0.878169">Computer Engineering</affiliation>
<address confidence="0.8595475">Middle East Technical Ankara,</address>
<email confidence="0.997058">burakkerim@ceng.metu.edu.tr</email>
<affiliation confidence="0.956954">Computer Engineering</affiliation>
<address confidence="0.8978735">Middle East Technical Ankara,</address>
<email confidence="0.997548">ruken@ceng.metu.edu.tr</email>
<abstract confidence="0.997162428571429">Morphologically rich languages such as Turkish may benefit from morphological analysis in natural language tasks. In this study, we examine the effects of morphological analysis on text categorization task in Turkish. We use stems and word categories that are extracted with morphological analysis as main features and compare them with fixed length stemmers in a bag of words approach with several learning algorithms. We aim to show the effects of using varying degrees of morphological information.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Fatih Amasyalı</author>
<author>Banu Diri</author>
</authors>
<title>Automatic Turkish text categorization in terms of author, genre and gender.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th international conference on Applications of Natural Language to Information Systems, NLDB’06,</booktitle>
<pages>221--226</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="4598" citStr="Amasyalı and Diri, 2006" startWordPosition="710" endWordPosition="713">roceedings of the ACL Student Research Workshop, pages 1–8, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics lic use 4. By making our manually annotated data available, we hope to contribute to future work in this area. The rest of the paper is organized as follows. Section 2 briefly describes the classification methods used, section 3 explains how these methods are used in implementation and finally the paper is concluded with experimental results. 2 Background Supervised and unsupervised methods have been used for text classification in different languages (Amasyalı and Diri, 2006; Beil et al., 2002). Among these are Naive Bayes classification (McCallum and Nigam, 1998; Schneider, 2005), decision trees (Johnson et al., 2002) , neural networks (Ng et al., 1997), k-nearest neighbour classifiers (Lim, 2004) and support-vector machines (Shanahan and Roma, 2003). Bag-of-words model is one of the more intuitive ways to represent text files in text classification. It is simple, it ignores syntax, grammar and the relative positions of the words in the text (Harris, 1970). Each document is represented with an unordered list of words and each of the word frequencies in the colle</context>
</contexts>
<marker>Amasyalı, Diri, 2006</marker>
<rawString>M. Fatih Amasyalı and Banu Diri. 2006. Automatic Turkish text categorization in terms of author, genre and gender. In Proceedings of the 11th international conference on Applications of Natural Language to Information Systems, NLDB’06, pages 221–226, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florian Beil</author>
<author>Martin Ester</author>
<author>Xiaowei Xu</author>
</authors>
<title>Frequent term-based text clustering.</title>
<date>2002</date>
<booktitle>In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’02,</booktitle>
<pages>436--442</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="4618" citStr="Beil et al., 2002" startWordPosition="714" endWordPosition="717">dent Research Workshop, pages 1–8, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics lic use 4. By making our manually annotated data available, we hope to contribute to future work in this area. The rest of the paper is organized as follows. Section 2 briefly describes the classification methods used, section 3 explains how these methods are used in implementation and finally the paper is concluded with experimental results. 2 Background Supervised and unsupervised methods have been used for text classification in different languages (Amasyalı and Diri, 2006; Beil et al., 2002). Among these are Naive Bayes classification (McCallum and Nigam, 1998; Schneider, 2005), decision trees (Johnson et al., 2002) , neural networks (Ng et al., 1997), k-nearest neighbour classifiers (Lim, 2004) and support-vector machines (Shanahan and Roma, 2003). Bag-of-words model is one of the more intuitive ways to represent text files in text classification. It is simple, it ignores syntax, grammar and the relative positions of the words in the text (Harris, 1970). Each document is represented with an unordered list of words and each of the word frequencies in the collection becomes a feat</context>
</contexts>
<marker>Beil, Ester, Xu, 2002</marker>
<rawString>Florian Beil, Martin Ester, and Xiaowei Xu. 2002. Frequent term-based text clustering. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’02, pages 436–442, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fazlı Can</author>
<author>Seyit Koc¸berber</author>
<author>Erman Balc¸ık</author>
<author>Cihan Kaynak</author>
<author>H C¸a˘gdas¸ ¨Ocalan</author>
<author>Onur M Vursavas¸</author>
</authors>
<title>Information retrieval on turkish texts.</title>
<date>2008</date>
<journal>JASIST,</journal>
<volume>59</volume>
<issue>3</issue>
<marker>Can, Koc¸berber, Balc¸ık, Kaynak, ¨Ocalan, Vursavas¸, 2008</marker>
<rawString>Fazlı Can, Seyit Koc¸berber, Erman Balc¸ık, Cihan Kaynak, H. C¸a˘gdas¸ ¨Ocalan, and Onur M. Vursavas¸. 2008. Information retrieval on turkish texts. JASIST, 59(3):407–421.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruket C¸akıcı</author>
<author>Jason Baldridge</author>
</authors>
<title>Projective and non-projective Turkish parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Treebanks and Linguistic Theories Conference,</booktitle>
<pages>43--54</pages>
<marker>C¸akıcı, Baldridge, 2006</marker>
<rawString>Ruket C¸akıcı and Jason Baldridge. 2006. Projective and non-projective Turkish parsing. In Proceedings of the 5th International Treebanks and Linguistic Theories Conference, pages 43–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>¨Ozlem C¸etinoˇglu</author>
<author>Kemal Oflazer</author>
</authors>
<title>Morphology-syntax interface for Turkish LFG.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44,</booktitle>
<pages>153--160</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>C¸etinoˇglu, Oflazer, 2006</marker>
<rawString>¨Ozlem C¸etinoˇglu and Kemal Oflazer. 2006. Morphology-syntax interface for Turkish LFG. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44, pages 153–160, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas G Dietterich</author>
</authors>
<title>Approximate statistical tests for comparing supervised classification learning algorithms.</title>
<date>1998</date>
<journal>Neural Computation,</journal>
<volume>10</volume>
<contexts>
<context position="14478" citStr="Dietterich, 1998" startWordPosition="2329" endWordPosition="2330">h of the three classifiers are presented in Table 3. Using five character prefixes gives better results than using stems. Naive Bayes with stems and five character prefixes disagree only on six instances out of 160 test instances with F1 scores of 0.92 and 0.94 respectively in the best configurations. There is no statistically significant difference. Similarly, results for SVM with stems for the best and the worst configurations is considered to be not statistically significant. McNemar’s Test (McNemar, 1947) is shown to have low error in detecting a significant difference when there is none (Dietterich, 1998). Worst Best First 5 Stems First 5 Stems KNN 91.250 86.875 92.500 91.875 NB 92.500 91.250 94.375 91.875 SVM 91.250 88.750 93.175 92.500 Table 3: Comparison of F1-scores for best and worst results in each classifier with each feature. 4 (a) Learning curves with word tags Figure 3: Learning curves for SVM (a) Learning curves without tags (b) Learning curves with stem tags 5 KNN NB SVM 4 5 6 7 4 5 6 7 4 5 6 7 1200 90.00 91.25 86.87 84.37 93.12 92.50 93.12 90.00 89.37 91.250 90.62 88.75 2400 89.37 91.25 87.50 86.62 89.37 91.25 87.50 86.62 90.62 91.87 90.00 88.12 3600 86.87 91.25 90.00 88.17 93.75 </context>
</contexts>
<marker>Dietterich, 1998</marker>
<rawString>Thomas G. Dietterich. 1998. Approximate statistical tests for comparing supervised classification learning algorithms. Neural Computation, 10:1895– 1923.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨uls¸en Eryi˘git</author>
</authors>
<title>The impact of automatic morphological analysis &amp; disambiguation on dependency parsing of turkish.</title>
<date>2012</date>
<booktitle>In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC),</booktitle>
<location>Istanbul,</location>
<marker>Eryi˘git, 2012</marker>
<rawString>G¨uls¸en Eryi˘git. 2012. The impact of automatic morphological analysis &amp; disambiguation on dependency parsing of turkish. In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC), Istanbul, Turkey, 23-25 May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨uls¸en Eryi˘git</author>
<author>Joakim Nivre</author>
<author>Kemal Oflazer</author>
</authors>
<title>Dependency parsing of Turkish.</title>
<date>2008</date>
<journal>Comput. Linguist.,</journal>
<volume>34</volume>
<issue>3</issue>
<marker>Eryi˘git, Nivre, Oflazer, 2008</marker>
<rawString>G¨uls¸en Eryi˘git, Joakim Nivre, and Kemal Oflazer. 2008. Dependency parsing of Turkish. Comput. Linguist., 34(3):357–389, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Forman</author>
</authors>
<title>An extensive empirical study of feature selection metrics for text classification.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--1289</pages>
<marker>Forman, 2003</marker>
<rawString>George Forman. 2003. An extensive empirical study of feature selection metrics for text classification. J. Mach. Learn. Res., 3:1289–1305, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dilek Z Hakkani-T¨ur</author>
<author>Kemal Oflazer</author>
<author>G¨okhan T¨ur</author>
</authors>
<title>Statistical morphological disambiguation for agglutinative languages.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th conference on Computational linguistics - Volume 1, COLING ’00,</booktitle>
<pages>285--291</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Hakkani-T¨ur, Oflazer, T¨ur, 2000</marker>
<rawString>Dilek Z. Hakkani-T¨ur, Kemal Oflazer, and G¨okhan T¨ur. 2000. Statistical morphological disambiguation for agglutinative languages. In Proceedings of the 18th conference on Computational linguistics - Volume 1, COLING ’00, pages 285–291, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ikonomakis</author>
<author>S Kotsiantis</author>
<author>V Tampakas</author>
</authors>
<title>Text classification: a recent overview.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th WSEAS International Conference on Computers, ICCOMP’05,</booktitle>
<pages>1--6</pages>
<location>Stevens Point, Wisconsin, USA.</location>
<marker>Ikonomakis, Kotsiantis, Tampakas, 2005</marker>
<rawString>M. Ikonomakis, S. Kotsiantis, and V. Tampakas. 2005. Text classification: a recent overview. In Proceedings of the 9th WSEAS International Conference on Computers, ICCOMP’05, pages 1–6, Stevens Point, Wisconsin, USA. World Scientific and Engineering Academy and Society (WSEAS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>A probabilistic analysis of the rocchio algorithm with tfidf for text categorization.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fourteenth International Conference on Machine Learning, ICML ’97,</booktitle>
<pages>143--151</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="5379" citStr="Joachims, 1997" startWordPosition="838" endWordPosition="839">(Ng et al., 1997), k-nearest neighbour classifiers (Lim, 2004) and support-vector machines (Shanahan and Roma, 2003). Bag-of-words model is one of the more intuitive ways to represent text files in text classification. It is simple, it ignores syntax, grammar and the relative positions of the words in the text (Harris, 1970). Each document is represented with an unordered list of words and each of the word frequencies in the collection becomes a feature representing the document. Bag-of-words approach is an intuitive way and popular among document classification tasks (Scott and Matwin, 1998; Joachims, 1997). Another way of representing documents with term weights is to use term frequency - inverse document frequency (Sparck Jones, 1988). TFIDF is another way of saying that a term is valuable for a document if it occurs frequently in that document but it is not common in the rest of the collection. TFIDF score of a term t in a document d in a collection D is calculated as below: tfidft,d,D = tft,d × idft,D tft,d is the number of times t occurs in d and idft,D is the number of documents in D over the number of document that contain t. The idea behind bag of words and TFIDF is to find a mapping fro</context>
</contexts>
<marker>Joachims, 1997</marker>
<rawString>Thorsten Joachims. 1997. A probabilistic analysis of the rocchio algorithm with tfidf for text categorization. In Proceedings of the Fourteenth International Conference on Machine Learning, ICML ’97, pages 143–151, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Johnson</author>
<author>F J Oles</author>
<author>T Zhang</author>
<author>T Goetz</author>
</authors>
<title>A decision-tree-based symbolic rule induction system for text categorization.</title>
<date>2002</date>
<journal>IBM Syst. J.,</journal>
<volume>41</volume>
<issue>3</issue>
<pages>437</pages>
<contexts>
<context position="4745" citStr="Johnson et al., 2002" startWordPosition="734" endWordPosition="737">e 4. By making our manually annotated data available, we hope to contribute to future work in this area. The rest of the paper is organized as follows. Section 2 briefly describes the classification methods used, section 3 explains how these methods are used in implementation and finally the paper is concluded with experimental results. 2 Background Supervised and unsupervised methods have been used for text classification in different languages (Amasyalı and Diri, 2006; Beil et al., 2002). Among these are Naive Bayes classification (McCallum and Nigam, 1998; Schneider, 2005), decision trees (Johnson et al., 2002) , neural networks (Ng et al., 1997), k-nearest neighbour classifiers (Lim, 2004) and support-vector machines (Shanahan and Roma, 2003). Bag-of-words model is one of the more intuitive ways to represent text files in text classification. It is simple, it ignores syntax, grammar and the relative positions of the words in the text (Harris, 1970). Each document is represented with an unordered list of words and each of the word frequencies in the collection becomes a feature representing the document. Bag-of-words approach is an intuitive way and popular among document classification tasks (Scott</context>
</contexts>
<marker>Johnson, Oles, Zhang, Goetz, 2002</marker>
<rawString>D. E. Johnson, F. J. Oles, T. Zhang, and T. Goetz. 2002. A decision-tree-based symbolic rule induction system for text categorization. IBM Syst. J., 41(3):428– 437, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heui-Seok Lim</author>
</authors>
<title>Improving kNN based text classification with well estimated parameters. In Nikhil</title>
<date>2004</date>
<booktitle>Neural Information Processing, 11th International Conference, ICONIP 2004,</booktitle>
<volume>3316</volume>
<pages>516--523</pages>
<editor>R. Pal, Nikola Kasabov, Rajani K. Mudi, Srimanta Pal, and Swapan K. Parui, editors,</editor>
<publisher>Springer.</publisher>
<location>Calcutta, India,</location>
<contexts>
<context position="4826" citStr="Lim, 2004" startWordPosition="748" endWordPosition="749">n this area. The rest of the paper is organized as follows. Section 2 briefly describes the classification methods used, section 3 explains how these methods are used in implementation and finally the paper is concluded with experimental results. 2 Background Supervised and unsupervised methods have been used for text classification in different languages (Amasyalı and Diri, 2006; Beil et al., 2002). Among these are Naive Bayes classification (McCallum and Nigam, 1998; Schneider, 2005), decision trees (Johnson et al., 2002) , neural networks (Ng et al., 1997), k-nearest neighbour classifiers (Lim, 2004) and support-vector machines (Shanahan and Roma, 2003). Bag-of-words model is one of the more intuitive ways to represent text files in text classification. It is simple, it ignores syntax, grammar and the relative positions of the words in the text (Harris, 1970). Each document is represented with an unordered list of words and each of the word frequencies in the collection becomes a feature representing the document. Bag-of-words approach is an intuitive way and popular among document classification tasks (Scott and Matwin, 1998; Joachims, 1997). Another way of representing documents with te</context>
</contexts>
<marker>Lim, 2004</marker>
<rawString>Heui-Seok Lim. 2004. Improving kNN based text classification with well estimated parameters. In Nikhil R. Pal, Nikola Kasabov, Rajani K. Mudi, Srimanta Pal, and Swapan K. Parui, editors, Neural Information Processing, 11th International Conference, ICONIP 2004, Calcutta, India, November 22-25, 2004, Proceedings, volume 3316 of Lecture Notes in Computer Science, pages 516–523. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Liu</author>
<author>Shengping Liu</author>
<author>Zheng Chen</author>
</authors>
<title>An evaluation on feature selection for text clustering.</title>
<date>2003</date>
<booktitle>In In ICML,</booktitle>
<pages>488--495</pages>
<marker>Liu, Liu, Chen, 2003</marker>
<rawString>Tao Liu, Shengping Liu, and Zheng Chen. 2003. An evaluation on feature selection for text clustering. In In ICML, pages 488–495.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Loper</author>
<author>Steven Bird</author>
</authors>
<title>Nltk: the natural language toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL02 Workshop on Effective tools and methodologies for teaching natural language processing and computational linguistics - Volume 1, ETMTNLP ’02,</booktitle>
<pages>63--70</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="10002" citStr="Loper and Bird, 2002" startWordPosition="1585" endWordPosition="1588">TFIDF scoring. Then, different sizes of feature space dimensions are used with ranking by the highest term frequency scores. A range of different dimension sizes from 1200 to 7200 were experimented on to find the optimal dimension size for this study (Table 2). After the collection is mapped into vector space, several learning algorithms are applied for classification. K-Nearest neighbours was implemented with weighted voting of 25 nearest neighbours based on distance and Support Vector Machine is implemented with linear kernel and default parameters. These methods are used with Python, NLTK (Loper and Bird, 2002) and Sci-Kit (Loper and Bird, 2002; Pedregosa et al., 2011). Training data contains 872 articles labelled and divided into four categories as follows: 235 articles on politics, 258 articles about social news such as culture, education or health, 177 articles on economics and 202 about sports. This data are generated using bootstrapping. Documents are hand annotated with an initial classifier that is trained on a smaller set of hand labelled data. Classifier is used on unknown sam3 ples, then the predictions are manually checked to gather enough data for each class. Test data consists of 160 ar</context>
</contexts>
<marker>Loper, Bird, 2002</marker>
<rawString>Edward Loper and Steven Bird. 2002. Nltk: the natural language toolkit. In Proceedings of the ACL02 Workshop on Effective tools and methodologies for teaching natural language processing and computational linguistics - Volume 1, ETMTNLP ’02, pages 63–70, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Foundations of statistical natural language processing.</title>
<date>1999</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Christopher D. Manning and Hinrich Sch¨utze. 1999. Foundations of statistical natural language processing. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA.</location>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch¨utze. 2008. Introduction to Information Retrieval. Cambridge University Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Kamal Nigam</author>
</authors>
<title>A comparison of event models for naive bayes text classification.</title>
<date>1998</date>
<booktitle>In Proceesings of the Workshop on learning for text categorization, AAAI’98,</booktitle>
<pages>41--48</pages>
<contexts>
<context position="4688" citStr="McCallum and Nigam, 1998" startWordPosition="724" endWordPosition="728">013. c�2013 Association for Computational Linguistics lic use 4. By making our manually annotated data available, we hope to contribute to future work in this area. The rest of the paper is organized as follows. Section 2 briefly describes the classification methods used, section 3 explains how these methods are used in implementation and finally the paper is concluded with experimental results. 2 Background Supervised and unsupervised methods have been used for text classification in different languages (Amasyalı and Diri, 2006; Beil et al., 2002). Among these are Naive Bayes classification (McCallum and Nigam, 1998; Schneider, 2005), decision trees (Johnson et al., 2002) , neural networks (Ng et al., 1997), k-nearest neighbour classifiers (Lim, 2004) and support-vector machines (Shanahan and Roma, 2003). Bag-of-words model is one of the more intuitive ways to represent text files in text classification. It is simple, it ignores syntax, grammar and the relative positions of the words in the text (Harris, 1970). Each document is represented with an unordered list of words and each of the word frequencies in the collection becomes a feature representing the document. Bag-of-words approach is an intuitive w</context>
</contexts>
<marker>McCallum, Nigam, 1998</marker>
<rawString>Andrew McCallum and Kamal Nigam. 1998. A comparison of event models for naive bayes text classification. In Proceesings of the Workshop on learning for text categorization, AAAI’98, pages 41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quinn McNemar</author>
</authors>
<title>Note on the Sampling Error of the Difference Between Correlated Proportions or Percentages.</title>
<date>1947</date>
<journal>Psychometrika,</journal>
<volume>12</volume>
<issue>2</issue>
<contexts>
<context position="14375" citStr="McNemar, 1947" startWordPosition="2312" endWordPosition="2313">his study with three different classifiers. F1 scores for the best and worst configurations with each of the three classifiers are presented in Table 3. Using five character prefixes gives better results than using stems. Naive Bayes with stems and five character prefixes disagree only on six instances out of 160 test instances with F1 scores of 0.92 and 0.94 respectively in the best configurations. There is no statistically significant difference. Similarly, results for SVM with stems for the best and the worst configurations is considered to be not statistically significant. McNemar’s Test (McNemar, 1947) is shown to have low error in detecting a significant difference when there is none (Dietterich, 1998). Worst Best First 5 Stems First 5 Stems KNN 91.250 86.875 92.500 91.875 NB 92.500 91.250 94.375 91.875 SVM 91.250 88.750 93.175 92.500 Table 3: Comparison of F1-scores for best and worst results in each classifier with each feature. 4 (a) Learning curves with word tags Figure 3: Learning curves for SVM (a) Learning curves without tags (b) Learning curves with stem tags 5 KNN NB SVM 4 5 6 7 4 5 6 7 4 5 6 7 1200 90.00 91.25 86.87 84.37 93.12 92.50 93.12 90.00 89.37 91.250 90.62 88.75 2400 89.3</context>
</contexts>
<marker>McNemar, 1947</marker>
<rawString>Quinn McNemar. 1947. Note on the Sampling Error of the Difference Between Correlated Proportions or Percentages. Psychometrika, 12(2):153–157.</rawString>
</citation>
<citation valid="false">
<authors>
<author>San Francisco</author>
</authors>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>CA, USA.</location>
<marker>Francisco, </marker>
<rawString>412–420, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Wei Boon Goh</author>
<author>Kok Leong Low</author>
</authors>
<title>Feature selection, perceptron learning, and a usability case study for text categorization.</title>
<date>1997</date>
<booktitle>In Proceedings of the 20th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’97,</booktitle>
<pages>67--73</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="4781" citStr="Ng et al., 1997" startWordPosition="741" endWordPosition="744">a available, we hope to contribute to future work in this area. The rest of the paper is organized as follows. Section 2 briefly describes the classification methods used, section 3 explains how these methods are used in implementation and finally the paper is concluded with experimental results. 2 Background Supervised and unsupervised methods have been used for text classification in different languages (Amasyalı and Diri, 2006; Beil et al., 2002). Among these are Naive Bayes classification (McCallum and Nigam, 1998; Schneider, 2005), decision trees (Johnson et al., 2002) , neural networks (Ng et al., 1997), k-nearest neighbour classifiers (Lim, 2004) and support-vector machines (Shanahan and Roma, 2003). Bag-of-words model is one of the more intuitive ways to represent text files in text classification. It is simple, it ignores syntax, grammar and the relative positions of the words in the text (Harris, 1970). Each document is represented with an unordered list of words and each of the word frequencies in the collection becomes a feature representing the document. Bag-of-words approach is an intuitive way and popular among document classification tasks (Scott and Matwin, 1998; Joachims, 1997). </context>
</contexts>
<marker>Ng, Goh, Low, 1997</marker>
<rawString>Hwee Tou Ng, Wei Boon Goh, and Kok Leong Low. 1997. Feature selection, perceptron learning, and a usability case study for text categorization. In Proceedings of the 20th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’97, pages 67–73, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kemal Oflazer</author>
</authors>
<title>Two-level description of Turkish morphology.</title>
<date>1993</date>
<booktitle>In Proceedings of the sixth conference on European chapter of the Association for Computational Linguistics, EACL ’93,</booktitle>
<pages>472--472</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3395" citStr="Oflazer (1993)" startWordPosition="526" endWordPosition="527"> stems that are extracted with fixed size stemming rarely contain any affixes. The training data used in this study consist of news articles taken from Milliyet Corpus that contains 80293 news articles published in the newspaper Milliyet (Hakkani-T¨ur et al., 2000) 2. The articles we use for training contain a subset of documents indexed from 1000-5000 and have at least 500 characters. The test set is not included in the original corpus, but it has also been downloaded form Milliyet’s public website 3. The data used in this study have been analyzed with the morphological analyser described in Oflazer (1993) and disambiguated with Sak et al. (2007)’s morphological disambiguator. The data have been manually labelled for training and test. The annotated data is made available for pub1It has only one prefix for intensifying adjectives and adverbs (sımsıcak: very hot). It is just a modified version of the first syllable of the original word and also it is not common. There are other prefixes adopted from foreign languages such as anormal (abnormal), antisosyal (antisocial) or namert (not brave). 2Thanks to Kemal Oflazer for letting us use the corpus 3http://www.milliyet.com.tr 1 Proceedings of the AC</context>
<context position="7858" citStr="Oflazer, 1993" startWordPosition="1233" endWordPosition="1234">A2pl ((you) take) al+Verb&amp;quot;DB+Verb+Pass+Pos+Imp+A2sg ((you) be taken) alın+Verb+Pos+Imp+A2sg ((you) be offended) Table 1: Morphological analysis of the word ”alın” in Turkish with the corresponding meanings. We aim to examine the effects of morphological information in a bag-of-words model in the context of text classification. A relevant study explores the prefixing versus morphological analysis/stemming effect on information retrieval in Can et al. (2008). Several stemmers for Turkish are presented for the indexing problem for information retrieval. They use Oflazer’s morphological analyzer (Oflazer, 1993), however, they do not use a disambiguator. Instead they choose the most common analysis among the candidates. Their results show that among the fixed length stemmers 5-character prefix is the the best and the lemmatizer based stemmer is slightly better than the fixed length stemmer with five characters. However, they also note that the difference is statistically insignificant. We use Sak et al. (2007)’s disambiguator which is reported with a 96.45% accuracy in their study and with a 87.67% accuracy by Eryi˘git (2012) 2 Figure 1: Learning curves with first five characters Figure 2: Learning c</context>
</contexts>
<marker>Oflazer, 1993</marker>
<rawString>Kemal Oflazer. 1993. Two-level description of Turkish morphology. In Proceedings of the sixth conference on European chapter of the Association for Computational Linguistics, EACL ’93, pages 472– 472, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pedregosa</author>
<author>G Varoquaux</author>
<author>A Gramfort</author>
<author>V Michel</author>
<author>B Thirion</author>
<author>O Grisel</author>
<author>M Blondel</author>
<author>P Prettenhofer</author>
<author>R Weiss</author>
<author>V Dubourg</author>
<author>J Vanderplas</author>
<author>A Passos</author>
<author>D Cournapeau</author>
<author>M Brucher</author>
<author>M Perrot</author>
<author>E Duchesnay</author>
</authors>
<title>Scikit-learn: Machine learning in Python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<contexts>
<context position="10061" citStr="Pedregosa et al., 2011" startWordPosition="1595" endWordPosition="1598">ensions are used with ranking by the highest term frequency scores. A range of different dimension sizes from 1200 to 7200 were experimented on to find the optimal dimension size for this study (Table 2). After the collection is mapped into vector space, several learning algorithms are applied for classification. K-Nearest neighbours was implemented with weighted voting of 25 nearest neighbours based on distance and Support Vector Machine is implemented with linear kernel and default parameters. These methods are used with Python, NLTK (Loper and Bird, 2002) and Sci-Kit (Loper and Bird, 2002; Pedregosa et al., 2011). Training data contains 872 articles labelled and divided into four categories as follows: 235 articles on politics, 258 articles about social news such as culture, education or health, 177 articles on economics and 202 about sports. This data are generated using bootstrapping. Documents are hand annotated with an initial classifier that is trained on a smaller set of hand labelled data. Classifier is used on unknown sam3 ples, then the predictions are manually checked to gather enough data for each class. Test data consists of 160 articles with 40 in each class. These are also manually label</context>
</contexts>
<marker>Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, Duchesnay, 2011</marker>
<rawString>F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Has¸im Sak</author>
<author>Tunga G¨ung¨or</author>
<author>Murat Sarac¸lar</author>
</authors>
<title>Morphological disambiguation of Turkish text with perceptron algorithm.</title>
<date>2007</date>
<booktitle>In Proceedings of the 8th International Conference on Computational Linguistics and Intelligent Text Processing, CICLing ’07,</booktitle>
<pages>107--118</pages>
<publisher>SpringerVerlag.</publisher>
<location>Berlin, Heidelberg.</location>
<marker>Sak, G¨ung¨or, Sarac¸lar, 2007</marker>
<rawString>Has¸im Sak, Tunga G¨ung¨or, and Murat Sarac¸lar. 2007. Morphological disambiguation of Turkish text with perceptron algorithm. In Proceedings of the 8th International Conference on Computational Linguistics and Intelligent Text Processing, CICLing ’07, pages 107–118, Berlin, Heidelberg. SpringerVerlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl-Michael Schneider</author>
</authors>
<title>Techniques for improving the performance of naive bayes for text classification. In</title>
<date>2005</date>
<booktitle>In Proceedings of CICLing</booktitle>
<pages>682--693</pages>
<contexts>
<context position="4706" citStr="Schneider, 2005" startWordPosition="729" endWordPosition="730">r Computational Linguistics lic use 4. By making our manually annotated data available, we hope to contribute to future work in this area. The rest of the paper is organized as follows. Section 2 briefly describes the classification methods used, section 3 explains how these methods are used in implementation and finally the paper is concluded with experimental results. 2 Background Supervised and unsupervised methods have been used for text classification in different languages (Amasyalı and Diri, 2006; Beil et al., 2002). Among these are Naive Bayes classification (McCallum and Nigam, 1998; Schneider, 2005), decision trees (Johnson et al., 2002) , neural networks (Ng et al., 1997), k-nearest neighbour classifiers (Lim, 2004) and support-vector machines (Shanahan and Roma, 2003). Bag-of-words model is one of the more intuitive ways to represent text files in text classification. It is simple, it ignores syntax, grammar and the relative positions of the words in the text (Harris, 1970). Each document is represented with an unordered list of words and each of the word frequencies in the collection becomes a feature representing the document. Bag-of-words approach is an intuitive way and popular amo</context>
</contexts>
<marker>Schneider, 2005</marker>
<rawString>Karl-Michael Schneider. 2005. Techniques for improving the performance of naive bayes for text classification. In In Proceedings of CICLing 2005, pages 682–693.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sam Scott</author>
<author>Stan Matwin</author>
</authors>
<title>Text classification using WordNet hypernyms.</title>
<date>1998</date>
<booktitle>In Workshop: Usage of WordNet in Natural Language Processing Systems, ACL’98,</booktitle>
<pages>45--52</pages>
<contexts>
<context position="5362" citStr="Scott and Matwin, 1998" startWordPosition="834" endWordPosition="837">2002) , neural networks (Ng et al., 1997), k-nearest neighbour classifiers (Lim, 2004) and support-vector machines (Shanahan and Roma, 2003). Bag-of-words model is one of the more intuitive ways to represent text files in text classification. It is simple, it ignores syntax, grammar and the relative positions of the words in the text (Harris, 1970). Each document is represented with an unordered list of words and each of the word frequencies in the collection becomes a feature representing the document. Bag-of-words approach is an intuitive way and popular among document classification tasks (Scott and Matwin, 1998; Joachims, 1997). Another way of representing documents with term weights is to use term frequency - inverse document frequency (Sparck Jones, 1988). TFIDF is another way of saying that a term is valuable for a document if it occurs frequently in that document but it is not common in the rest of the collection. TFIDF score of a term t in a document d in a collection D is calculated as below: tfidft,d,D = tft,d × idft,D tft,d is the number of times t occurs in d and idft,D is the number of documents in D over the number of document that contain t. The idea behind bag of words and TFIDF is to f</context>
</contexts>
<marker>Scott, Matwin, 1998</marker>
<rawString>Sam Scott and Stan Matwin. 1998. Text classification using WordNet hypernyms. In Workshop: Usage of WordNet in Natural Language Processing Systems, ACL’98, pages 45–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James G Shanahan</author>
<author>Norbert Roma</author>
</authors>
<title>Boosting support vector machines for text classification through parameter-free threshold relaxation.</title>
<date>2003</date>
<booktitle>In Proceedings of the twelfth international conference on Information and knowledge management, CIKM ’03,</booktitle>
<pages>247--254</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="4880" citStr="Shanahan and Roma, 2003" startWordPosition="753" endWordPosition="757">ganized as follows. Section 2 briefly describes the classification methods used, section 3 explains how these methods are used in implementation and finally the paper is concluded with experimental results. 2 Background Supervised and unsupervised methods have been used for text classification in different languages (Amasyalı and Diri, 2006; Beil et al., 2002). Among these are Naive Bayes classification (McCallum and Nigam, 1998; Schneider, 2005), decision trees (Johnson et al., 2002) , neural networks (Ng et al., 1997), k-nearest neighbour classifiers (Lim, 2004) and support-vector machines (Shanahan and Roma, 2003). Bag-of-words model is one of the more intuitive ways to represent text files in text classification. It is simple, it ignores syntax, grammar and the relative positions of the words in the text (Harris, 1970). Each document is represented with an unordered list of words and each of the word frequencies in the collection becomes a feature representing the document. Bag-of-words approach is an intuitive way and popular among document classification tasks (Scott and Matwin, 1998; Joachims, 1997). Another way of representing documents with term weights is to use term frequency - inverse document</context>
</contexts>
<marker>Shanahan, Roma, 2003</marker>
<rawString>James G. Shanahan and Norbert Roma. 2003. Boosting support vector machines for text classification through parameter-free threshold relaxation. In Proceedings of the twelfth international conference on Information and knowledge management, CIKM ’03, pages 247–254, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Sparck Jones</author>
</authors>
<title>A statistical interpretation of term specificity and its application in retrieval.</title>
<date>1988</date>
<booktitle>Document retrieval systems,</booktitle>
<pages>132--142</pages>
<editor>In Peter Willett, editor,</editor>
<publisher>Taylor Graham Publishing,</publisher>
<location>London, UK,</location>
<contexts>
<context position="5511" citStr="Jones, 1988" startWordPosition="858" endWordPosition="859">l is one of the more intuitive ways to represent text files in text classification. It is simple, it ignores syntax, grammar and the relative positions of the words in the text (Harris, 1970). Each document is represented with an unordered list of words and each of the word frequencies in the collection becomes a feature representing the document. Bag-of-words approach is an intuitive way and popular among document classification tasks (Scott and Matwin, 1998; Joachims, 1997). Another way of representing documents with term weights is to use term frequency - inverse document frequency (Sparck Jones, 1988). TFIDF is another way of saying that a term is valuable for a document if it occurs frequently in that document but it is not common in the rest of the collection. TFIDF score of a term t in a document d in a collection D is calculated as below: tfidft,d,D = tft,d × idft,D tft,d is the number of times t occurs in d and idft,D is the number of documents in D over the number of document that contain t. The idea behind bag of words and TFIDF is to find a mapping from words to numbers which can also be described as finding a mathematical representation for text files. The output is a matrix repre</context>
</contexts>
<marker>Jones, 1988</marker>
<rawString>Karen Sparck Jones. 1988. A statistical interpretation of term specificity and its application in retrieval. In Peter Willett, editor, Document retrieval systems, pages 132–142. Taylor Graham Publishing, London, UK, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiming Yang</author>
<author>Xin Liu</author>
</authors>
<title>A re-examination of text categorization methods.</title>
<date>1999</date>
<booktitle>In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’99,</booktitle>
<pages>42--49</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Yang, Liu, 1999</marker>
<rawString>Yiming Yang and Xin Liu. 1999. A re-examination of text categorization methods. In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’99, pages 42–49, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiming Yang</author>
<author>Jan O Pedersen</author>
</authors>
<title>A comparative study on feature selection in text categorization.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fourteenth International Conference on Machine Learning, ICML ’97,</booktitle>
<pages>pages</pages>
<marker>Yang, Pedersen, 1997</marker>
<rawString>Yiming Yang and Jan O. Pedersen. 1997. A comparative study on feature selection in text categorization. In Proceedings of the Fourteenth International Conference on Machine Learning, ICML ’97, pages</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>