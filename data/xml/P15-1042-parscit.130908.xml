<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000038">
<title confidence="0.9944675">
Learning Bilingual Sentiment Word Embeddings for Cross-language
Sentiment Classification
</title>
<author confidence="0.999646">
Huiwei Zhou, Long Chen, Fulin Shi, and Degen Huang
</author>
<affiliation confidence="0.9999305">
School of Computer Science and Technology
Dalian University of Technology, Dalian, P.R. China
</affiliation>
<email confidence="0.975297">
{zhouhuiwei,huangdg}@dlut.edu.cn
{chenlong.415,shi fl}@mail.dlut.edu.cn
</email>
<sectionHeader confidence="0.993734" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999889208333334">
The sentiment classification performance
relies on high-quality sentiment resources.
However, these resources are imbalanced
in different languages. Cross-language
sentiment classification (CLSC) can lever-
age the rich resources in one language
(source language) for sentiment classifica-
tion in a resource-scarce language (target
language). Bilingual embeddings could
eliminate the semantic gap between two
languages for CLSC, but ignore the senti-
ment information of text. This paper pro-
poses an approach to learning bilingual
sentiment word embeddings (BSWE) for
English-Chinese CLSC. The proposed B-
SWE incorporate sentiment information of
text into bilingual embeddings. Further-
more, we can learn high-quality BSWE
by simply employing labeled corpora and
their translations, without relying on large-
scale parallel corpora. Experiments on
NLP&amp;CC 2013 CLSC dataset show that
our approach outperforms the state-of-the-
art systems.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999942444444445">
Sentiment classification is a task of predicting sen-
timent polarity of text, which has attracted consid-
erable interest in the NLP field. To date, a num-
ber of corpus-based approaches (Pang et al., 2002;
Pang and Lee, 2004; Kennedy and Inkpen, 2006)
have been developed for sentiment classification.
The approaches heavily rely on quality and quan-
tity of the labeled corpora, which are considered
as the most valuable resources in sentiment classi-
fication task. However, such sentiment resources
are imbalanced in different languages. To leverage
resources in the source language to improve the
sentiment classification performance in the target
language, cross-language sentiment classification
(CLSC) approaches have been investigated.
The traditional CLSC approaches employ ma-
chine translation (MT) systems to translate corpo-
ra in the source language into the target language,
and train the sentiment classifiers in the target lan-
guage (Banea et al., 2008). Directly employing
the translated resources for sentiment classifica-
tion in the target language is simple and could
get acceptable results. However, the gap between
the source language and target language inevitably
impacts the performance of sentiment classifica-
tion. To improve the classification accuracy, multi-
view approaches have been proposed. In these ap-
proaches, the resources in the source language and
their translations in the target language are both
used to train sentiment classifiers in two indepen-
dent views (Wan, 2009; Gui et al., 2013; Zhou et
al., 2014a). The final results are determined by en-
semble classifiers in these two views to overcome
the weakness of monolingual classifiers. However,
learning language-specific classifiers in each view
fails to capture the common sentiment information
of two languages during training process.
With the revival of interest in deep learning
(Hinton and Salakhutdinov, 2006), shared deep
representations (or embeddings) (Bengio et al.,
2013) are employed for CLSC (Chandar A P et
al., 2013). Usually, paired sentences from par-
allel corpora are used to learn word embeddings
across languages (Chandar A P et al., 2013; Chan-
dar A P et al., 2014), eliminating the need of MT
systems. The learned bilingual embeddings could
easily project the training data and test data into a
common space, where training and testing are per-
formed. However, high-quality bilingual embed-
dings rely on the large-scale task-related parallel
corpora, which are not always readily available.
Meanwhile, though semantic similarities across
languages are captured during bilingual embed-
ding learning process, sentiment information of
</bodyText>
<page confidence="0.972998">
430
</page>
<note confidence="0.977656666666667">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 430–440,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999845390243902">
text is ignored. That is, bilingual embeddings
learned from unlabeled parallel corpora are not
effective enough for CLSC because of a lack of
explicit sentiment information. Tang and Wan
(2014) first proposed a bilingual sentiment embed-
ding model using the original training data and the
corresponding translations through a linear map-
ping rather than deep learning technique.
This paper proposes a denoising autoencoder
based approach to learning bilingual sentimen-
t word embeddings (BSWE) for CLSC, which
incorporates sentiment polarities of text into the
bilingual embeddings. The proposed approach
learns BSWE with the original labeled documents
and their translations instead of parallel corpo-
ra. The BSWE learning process consists of two
phases: the unsupervised phase of semantic learn-
ing and the supervised phase of sentiment learn-
ing. In the unsupervised phase, sentiment words
and their negation features are extracted from the
source training data and their translations to rep-
resent paired documents. These features are used
as inputs for a denoising autoencoder to learn the
bilingual embeddings. In the supervised phase,
sentiment polarity labels of documents are used to
guide BSWE learning for incorporating sentiment
information into the bilingual embeddings.
The learned BSWE are applied to project En-
glish training data and Chinese test data into a
common space. In this space, a linear support vec-
tor machine (SVM) is used to perform training and
testing. The experiments are carried on NLP&amp;CC
2013 CLSC dataset, including book, DVD and
music categories. Experimental results show that
our approach achieves 80.68% average accuracy,
which outperforms the state-of-the-art systems on
this dataset. Although the BSWE are only evaluat-
ed on English-Chinese CLSC here, it can be pop-
ularized to many other languages.
The major contributions of this work can be
summarized as follows:
</bodyText>
<listItem confidence="0.991221857142857">
• We propose bilingual sentiment word em-
beddings (BSWE) for CLSC based on deep
learning technique. Experimental results
show that the proposed BSWE significantly
outperform the bilingual embeddings by in-
corporating sentiment information.
• Instead of large-scale parallel corpora, on-
ly the labeled English corpora and English-
to-Chinese translations are required for B-
SWE learning. It is proved that in spite of
the small-scale of training set, our approach
outperforms the state-of-the-art systems in
NLP&amp;CC 2013 CLSC share task.
• We employ sentiment words and their nega-
</listItem>
<bodyText confidence="0.8675622">
tion features rather than all words in doc-
uments to learn sentiment-specific embed-
dings, which significantly reduces the dimen-
sion of input vectors as well as improves sen-
timent classification performance.
</bodyText>
<sectionHeader confidence="0.999657" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99987625">
In this section, we review the literature related to
this paper from two perspectives: cross-language
sentiment classification and embedding learning
for sentiment classification.
</bodyText>
<sectionHeader confidence="0.574638" genericHeader="method">
2.1 Cross-language Sentiment Classification
(CLSC)
</sectionHeader>
<bodyText confidence="0.999725424242424">
The critical problem of CLSC is how to bridge the
gap between the source language and target lan-
guage. Machine translations or parallel corpora
are usually employed to solve this problem. We
present a brief review of CLSC from two aspects:
machine translation based approaches and parallel
corpora based approaches.
Machine translation based approaches use MT
systems to project training data into the target lan-
guage or test data into the source language. Wan
(2009) proposed a co-training approach for CLSC.
The approach first translated Chinese test data in-
to English, and English training data into Chinese.
Then, they performed training and testing in t-
wo independent views: English view and Chinese
view. Gui et al. (2013) combined self-training
approach with co-training approach by estimating
the confidence of each monolingual system. Li
et al. (2013) selected the samples in the source
language that were similar to those in the target
language to decrease the gap between two lan-
guages. Zhou et al. (2014a) proposed a combi-
nation CLSC model, which adopted denoising au-
toencoders (Vincent et al., 2008) to enhance the
robustness to translation errors of the input.
Most recently, a number of studies adopt deep
learning technique to learn bilingual representa-
tions with parallel corpora. Bilingual represen-
tations have been successfully applied in many
NLP tasks, such as machine translation (Zou
et al., 2013), sentiment classification (Chan-
dar A P et al., 2013; Zhou et al., 2014b), tex-
t classification (Chandar A P et al., 2014), etc.
</bodyText>
<page confidence="0.998215">
431
</page>
<bodyText confidence="0.9995264">
Chandar A P et al. (2013) learned bilingual
representations with aligned sentences through-
out two phases: the language-specific represen-
tation learning phase and the shared representa-
tion learning phase. In the language-specific rep-
resentation learning phase, they applied autoen-
coders to obtain a language-specific representa-
tion for each entity in two languages respective-
ly. In shared representation learning phase, pairs
of parallel language-specific representations were
passed to an autoencoder to learn bilingual repre-
sentations. To joint language-specific representa-
tions and bilingual representations, Chandar A P
et al. (2014) integrated the two learning phases in-
to a unified process to learn bilingual embeddings.
Zhou et al. (2014b) employed bilingual represen-
tations for English-Chinese CLSC. The work men-
tioned above employed aligned sentences in bilin-
gual embedding learning process. However, in the
sentiment classification process, only representa-
tions in the source language are used for training,
and representations in the target language are used
for testing, which ignores the interactions of se-
mantic information between the source language
and target language.
</bodyText>
<subsectionHeader confidence="0.9956155">
2.2 Embedding Learning for Sentiment
Classification
</subsectionHeader>
<bodyText confidence="0.988030823529412">
Bilingual embedding learning algorithms focus
on capturing syntactic and semantic similarities
across languages, but ignore sentiment informa-
tion. To date, many embedding learning algo-
rithms have been developed for sentiment classi-
fication problem by incorporating sentiment in-
formation into word embeddings. Maas et al.
(2011) presented a probabilistic model that com-
bined unsupervised and supervised techniques to
learn word vectors, capturing semantic informa-
tion as well as sentiment information. Wang et
al. (2014) introduced sentiment labels into Neural
Network Language Models (Bengio et al., 2003)
to enhance sentiment expression ability of word
vectors. Tang et al. (2014) theoretically and em-
pirically analyzed the effects of the syntactic con-
text and sentiment information in word vectors,
and showed that the syntactic context and senti-
ment information were equally important to senti-
ment classification.
Recent years have seen a surge of interest in
word embeddings with deep learning technique
(Bespalov et al., 2011; Glorot et al., 2011; Socher
et al., 2011; Socher et al., 2012), which have been
empirically shown to preserve linguistic regulari-
ties (Mikolov et al., 2013). Our work focuses on
learning bilingual sentiment word embeddings (B-
SWE) with deep learning technique. Unlike the
work of Chandar A P et al. (2014) that adopt-
ed parallel corpora to learn bilingual embeddings,
we only use training data and their translations to
learn BSWE. More importantly, sentiment infor-
mation is integrated into bilingual embeddings to
improve their performance in CLSC.
</bodyText>
<sectionHeader confidence="0.996823666666667" genericHeader="method">
3 Bilingual Sentiment Word Embeddings
(BSWE) for Cross-language Sentiment
Classification
</sectionHeader>
<subsectionHeader confidence="0.999962">
3.1 Denoising Autoencoder
</subsectionHeader>
<bodyText confidence="0.98378664">
It has been demonstrated that the denoising au-
toencoder could decrease the effects of translation
errors on the performance of CLSC (Zhou et al.,
2014a). This paper proposes a deep learning based
approach, which employs the denoising autoen-
coder to learn the bilingual embeddings for CLSC.
A denoising autoencoder is the modification of
an autoencoder. The autoencoder (Bengio et al.,
2007) includes an encoder fθ and a decoder gθe.
The encoder maps a d-dimensional input vector
x E [0, 1]d to a hidden representation y E [0,1]d&apos;
through a deterministic mapping y = fθ(x) =
a(Wx + b), parameterized by 0 = {W, b}. W
is a weight matrix, b is a bias term, and a(x) is the
activation function. The decoder maps y back to a
reconstructed vector xˆ = gθl(y) = a(WTy + c),
parameterized by 0&apos; = {WT, c}, where c is the
bias term for reconstruction.
Through the process of encoding and decod-
ing, the parameters 0 and 0&apos; of the autoencoder
will be trained by gradient descent to minimize the
loss function. The sum of reconstruction cross-
entropies across the training set is usually used as
the loss function:
d
</bodyText>
<equation confidence="0.999398">
l(x) = − [xi log ˆxi+(1−xi) log(1−ˆxi)] (1)
i=1
</equation>
<bodyText confidence="0.999941285714286">
A denoising autoencoder enhances robustness
to noises by corrupting the input x to a partially
destroyed version ˜x. The desired noise level of the
input x can be changed by adjusting the destruc-
tion fraction v. For each input x, a fixed number
vd (d is the dimension of x) of components are
selected randomly, and their values are set to 0,
</bodyText>
<page confidence="0.993411">
432
</page>
<bodyText confidence="0.999973">
while the others are left untouched. Like an au-
toencoder, the destroyed input x˜ is mapped to a
latent representation y = fθ(˜x) = σ(W˜x + b).
Then y is mapped back to a reconstructed vector
xˆ through xˆ = gθ,(y) = σ(WT y + c). The loss
function of a denoising autoencoder is the same as
that of an autoencoder. Minimizing the loss makes
xˆ close to the input x rather than ˜x.
Our BSWE learning process can be divided in-
to two phases: the unsupervised phase of seman-
tic learning and the supervised phase of sentiment
learning. In the unsupervised phase, a denoising
autoencoder is employed to learn the bilingual em-
beddings. In the supervised phase, the sentiment
information is incorporated into the bilingual em-
beddings based on sentiment labels of documents
to obtain BSWE.
</bodyText>
<subsectionHeader confidence="0.9995155">
3.2 Unsupervised Phase of the Bilingual
Embedding Learning
</subsectionHeader>
<bodyText confidence="0.999313">
In the unsupervised phase, the English training
documents and their Chinese translations are em-
ployed to learn the bilingual embeddings (Sen-
timent polarity labels of documents are not em-
ployed in this phase). Based on the English docu-
ments, 2,000 English sentiment words in MPQA
subjectivity lexicon1 are extracted by the Chi-
square method (Galavotti et al., 2000). Their cor-
responding Chinese translations are used as Chi-
nese sentiment words. Besides, some sentimen-
t words are often modified by negation words,
which lead to inversion of their polarities. There-
fore, negation features are introduced to each sen-
timent word to represent its negative form.
We take into account 14 frequently-used nega-
tion words in English such as not and none; 5
negation words in Chinese such as T (no/not) and
;q (without). A sentiment word modified by
these negation words in the window [-2, 2] is con-
sidered as its negative form in this paper, while
sentiment word features remain the initial mean-
ing. Negation features use binary expressions. If a
sentiment word is not modified by negation words,
the value of its negation features is set to 0. Thus,
the sentiment words and their corresponding nega-
tion features in English and Chinese are adopted to
represent the document pairs (xE, xC).
We expect that pairs of documents could be
forced to capture the common semantic informa-
tion of two languages. To achieve this, a denoising
</bodyText>
<footnote confidence="0.990679">
1http://mpqa.cs.pitt.edu/lexicons/subj lexicon
</footnote>
<figureCaption confidence="0.857820333333333">
autoencoder is used to perform the reconstructions
of paired documents in both English and Chinese.
Figure 1 shows the framework of bilingual embed-
ding learning.
Figure 1: The framework of bilingual embedding
learning.
</figureCaption>
<bodyText confidence="0.991734">
For the corrupted versions ˜xE (˜xC) of the initial
input vector xE (xC), we use the sigmoid function
as the activation function to extract latent repre-
sentations:
</bodyText>
<equation confidence="0.999984">
yE = fθ(˜xE) = σ(WE˜xE + b) (2)
yC = fθ(˜xC) = σ(WC˜xC + b) (3)
</equation>
<bodyText confidence="0.999807923076923">
where WE and WC are the language-specific
word representation matrices, corresponding to
English and Chinese respectively. Notice that the
bias b is shared to ensure that the produced repre-
sentations in two languages are on the same scale.
For the latent representations in either language,
we would like two decoders to perform recon-
structions in English and Chinese respectively. As
shown in Figure 1(a), for the latent representation
yE in English, one decoder is used to map yE
back to a reconstruction ˆxE in English, and the
other is used to map yE back to a reconstruction
ˆxC in Chinese such that:
</bodyText>
<equation confidence="0.9999625">
ˆxE = gθ,(yE) = σ(WT EyE + cE) (4)
ˆxC = gθ,(yE) = σ(WTCyE + cC) (5)
</equation>
<bodyText confidence="0.9990378">
where cE and cC are the biases of the decoders in
English and Chinese, respectively. Similarly, the
same steps repeat for the latent representation yC
in Chinese, which are shown in Figure 1(b).
The encoder and decoder structures allow us
to learn a mapping within and across languages.
Specifically, for a given document pair (xE, xC),
we can learn bilingual embeddings to recon-
struct xE from itself (loss l(xE)), reconstruc-
t xC from itself (loss l(xC)), construct xC from
</bodyText>
<figure confidence="0.980022346153846">
[WE, WC ]T
[ W E ,WC]T
xE
xC
xE
xC
l(xE)
l ( x E , x C )
l(xC,xE)
l(xC
)
xˆE
xˆ
C
xˆE
xˆ
C
yE
WE
yC
WC
xE
xE xC
(a) reconstruction from xE (b) reconstruction from xC
C
x
</figure>
<page confidence="0.997054">
433
</page>
<bodyText confidence="0.9935304">
xE (loss l(xE, xC)), construct xE from xC
(loss l(xC, xE)) and reconstruct the concatena-
tion of xE and xC ([xE, xC]) from itself (loss
l([xE, xC], [ˆxE, ˆxC])). The sum of 5 losses is
used as the loss function of bilingual embeddings:
</bodyText>
<equation confidence="0.98253">
L =l(xE) + l(xC) + l(xE, xC) + l(xC, xE)
+ l([xE, xC], [ˆxE, ˆxC])
(6)
</equation>
<subsectionHeader confidence="0.993508">
3.3 Supervised Phase of Sentiment Learning
</subsectionHeader>
<bodyText confidence="0.997461928571429">
In the unsupervised phase, we have learned the
bilingual embeddings, which could capture the se-
mantic information within and across languages.
However, the sentiment polarities of text are ig-
nored in the unsupervised phase. Bilingual em-
beddings without sentiment information are not
effective enough for sentiment classification task.
This paper proposes an approach to learning B-
SWE for CLSC, which introduces a supervised
learning phase to incorporate sentiment informa-
tion into the bilingual embeddings. The process of
supervised phase is shown in Figure 2.
according to the sentiment polarity label si of doc-
ument di:
</bodyText>
<equation confidence="0.957093">
�ξ∗ = arg max
ξ
</equation>
<bodyText confidence="0.999914875">
Through the supervised learning phase,
[WE, WC] is optimized by maximizing senti-
ment polarity probability. Thus, rich sentiment
information is encoded into the bilingual embed-
dings.
The following experiments will prove that the
proposed BSWE outperform the traditional bilin-
gual embeddings significantly in CLSC.
</bodyText>
<subsectionHeader confidence="0.842644">
3.4 Bilingual Document Representation
Method (BDR)
</subsectionHeader>
<bodyText confidence="0.998869555555556">
Once we have learned BSWE [WE, WC], whose
columns are representations for sentiment words,
we can use them to represent documents in two
languages.
Given an English training document dE
containing 2,000 sentiment word features
s1, s2, · · · , s2,000 and 2,000 corresponding nega-
tion features, we represent it as the TF-IDF
weighted sum of BSWE:
</bodyText>
<equation confidence="0.993533333333333">
4,000
TF − IDF(si)WE.,si (9)
i=1
</equation>
<bodyText confidence="0.99849">
Similarly, for its Chinese translation dC containing
2,000 sentiment word features t1, t2, · · · , t2,000
and 2,000 corresponding negation features, we
represent it as:
</bodyText>
<figure confidence="0.947192833333334">
sentiment
Yb
p(s |d;)
max p ( s  |d ; ) label



[ WE,WC]
[XE,XC]
log p(si|di; ξ) (8)
i=1
φdE
</figure>
<figureCaption confidence="0.827114">
Figure 2: The supervised learning process. 4,000
φdC = TF − IDF(tj)WC.,tj (10)
</figureCaption>
<bodyText confidence="0.99793775">
For paired documents [xE, xC], the sigmoid
function is adopted as the activation function
to extract latent bilingual representations yb =
σ([WE, WC][xE, xC] + b), where [WE, WC] is
the concatenation of WE and WC.
The latent bilingual representation yb is used
to obtain the positive polarity probability p(s =
1|d; ξ) of a document through a sigmoid function:
</bodyText>
<equation confidence="0.991647">
p(s = 1|d; ξ) = σ(ϕTyb + bl) (7)
</equation>
<bodyText confidence="0.970897117647059">
where ϕ is the logistic regression weight vector
and bl is the bias of logistic regression. The senti-
ment label s is a Boolean value representing sen-
timent polarity of a document: s = 0 represents
negative polarity and s = 1 represents positive po-
larity. Parameter ξ∗ = {[WE, WC]∗, b∗, ϕ∗, b∗l }
is learned by maximizing the objective function
j=1
We propose a bilingual document representa-
tion method (BDR) in this paper, which represents
each document di with the concatenation of its En-
glish and Chinese representations [φdE, φdC]. B-
DR is expected to enhance the ability of sentiment
expression for further improving the classification
performance. Such bilingual document represen-
tations are fed to a linear SVM to perform senti-
ment classification.
</bodyText>
<sectionHeader confidence="0.999848" genericHeader="evaluation">
4 Experiment
</sectionHeader>
<subsectionHeader confidence="0.996087">
4.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.689507">
Data Set. The proposed approach is evaluated on
NLP&amp;CC 2013 CLSC dataset2 3. The dataset con-
</bodyText>
<footnote confidence="0.9998975">
2http://tcci.ccf.org.cn/conference/2013/dldoc/evsam03.zip
3http://tcci.ccf.org.cn/conference/2013/dldoc/evdata03.zip
</footnote>
<page confidence="0.998945">
434
</page>
<bodyText confidence="0.999972473684211">
sists of product reviews on three categories: book,
DVD, and music. Each category contains 4,000
English labeled data as training data (the ratio of
the number of positive and negative samples is
1:1) and 4,000 Chinese unlabeled data as test data.
Tools. In our experiments, Google Translate4 is
adopted for both English-to-Chinese and Chinese-
to-English translation. ICTCLAS (Zhang et al.,
2003) is used as Chinese word segmentation tool.
A denoising autoencoder is developed based on
Theano system (Bergstra et al., 2010). BSWE
are trained for 50 and 30 epochs in unsuper-
vised phase and supervised phases respectively.
SV M&apos;Zght (Joachims, 1999) is used to train lin-
ear SVM sentiment classifiers
Evaluation Metric. The performance is evalu-
ated by the classification accuracy for each cate-
gory, and the average accuracy of three categories,
respectively. The category accuracy is defined as:
</bodyText>
<equation confidence="0.9632115">
Accuracy, = #system correct, (11)
#system total,
</equation>
<bodyText confidence="0.9967305">
where c is one of the three categories, and
#system correct, and #system total, stand
for the number of being correctly classified re-
views and the number of total reviews in the cate-
gory c, respectively.
The average accuracy is shown as:
</bodyText>
<figure confidence="0.6135555">
1 � Accuracy, (12)
Average =
3
,
</figure>
<subsectionHeader confidence="0.984738">
4.2 Evaluations on BSWE
</subsectionHeader>
<bodyText confidence="0.914532266666667">
In this section, we evaluate the quality of BSWE
for CLSC. The dimension of bilingual embeddings
d is set to 50, and destruction fraction ν is set to
0.2.
Effects of Bilingual Embedding Learning
Methods
We first compare our unsupervised bilingual em-
bedding learning method with the parallel cor-
pora based method. The parallel corpora based
method uses the paired documents in the parallel
corpus5 to learn bilingual embeddings, while our
method only uses the English training documents
and their Chinese translations (Sentiment polari-
ty labels of documents are not employed here).
The Boolean feature weight calculation method is
</bodyText>
<footnote confidence="0.988516">
4http://translate.google.cn/
5http://www.datatang.com/data/45485
</footnote>
<bodyText confidence="0.998644545454546">
adopted to represent documents for bilingual em-
bedding learning and BDR is employed to rep-
resent training data and test data for sentimen-
t classification. To represent the paired docu-
ments in the parallel corpus, 27,597 English word-
s and 31,786 Chinese words are extracted for
bilingual embedding learning. Our method only
needs 2,000 English sentiment words, 2,000 Chi-
nese sentiment words, and their negation features,
which significantly reduces the dimension of input
vectors.
</bodyText>
<figure confidence="0.51747">
Corpus Scale
</figure>
<figureCaption confidence="0.753721666666667">
Figure 3: Our unsupervised bilingual embed-
ding learning method vs. Parallel corpora based
method.
</figureCaption>
<bodyText confidence="0.999935611111111">
The average accuracies on NLP&amp;CC 2013 test
data of the two bilingual embedding learning
methods are shown in Figure 3. As can be seen
from Figure 3, when the corpus scales of the two
methods are the same (4,000 paired documents),
our method (75.09% average accuracy) surpasses
the parallel corpora method (54.82% average ac-
curacy) by about 20%. With the scale of the par-
allel corpora increasing, the performance of par-
allel corpora based method is steadily improved.
However, the performance is not as good as our
bilingual embedding learning method. Though the
document number of the parallel corpus is up to
70,000 , the average accuracy is only 70.05%. It is
proved that our method is more suitable for learn-
ing bilingual embeddings for cross-language senti-
ment classification than the parallel corpora based
method.
</bodyText>
<subsectionHeader confidence="0.824327">
Effects of Feature Weight in Bilingual
Embeddings
</subsectionHeader>
<bodyText confidence="0.91934675">
In this part, we compare the Boolean and TF-
IDF feature weight calculation methods in bilin-
gual embedding learning process.
Table 1 shows the classification accuracy with
</bodyText>
<figure confidence="0.999218">
104 2×104 3×104 4×104 5×104 6×104 7×104
Average 0.8
0.75
0.7
0.65
0.6
0.55
0.5
Our method
Parallel corpora based method
</figure>
<page confidence="0.997316">
435
</page>
<table confidence="0.998250333333333">
Category book DVD music Average
Boolean 76.22% 74.30% 74.75% 75.09%
TF-IDF 76.65% 77.60% 74.50% 76.25%
</table>
<tableCaption confidence="0.9627895">
Table 1: The classification accuracy with the
Boolean and TF-IDF methods.
</tableCaption>
<bodyText confidence="0.999791454545455">
the Boolean and TF-IDF methods. Generally, the
TF-IDF method performs better than the Boolean
method. The average accuracy of the TF-IDF
method is 1.16% higher than the Boolean method,
which illustrates that the TF-IDF method could re-
flect the latent contribution of sentiment words to
each document effectively. The TF-IDF weight
calculation method is exploited in the following
experiments. Notice that sentiment information
is not yet introduced in the bilingual embeddings
here.
</bodyText>
<subsectionHeader confidence="0.921085">
Effects of Sentiment Information in BSWE
</subsectionHeader>
<bodyText confidence="0.999975">
Incorporating sentiment information in the bilin-
gual embeddings, the performance of bilingual
embeddings (without sentiment information) and
BSWE (with sentiment information) is compared
in Figure 4.
</bodyText>
<subsectionHeader confidence="0.662011">
Effects of Bilingual Document Representation
Method
</subsectionHeader>
<bodyText confidence="0.897353714285714">
In this experiment, our bilingual document rep-
resentation method (BDR) is compared with the
following monolingual document representation
methods.
En-En: This method represents training and
test documents in English only with WE. English
training documents and Chinese-to-English trans-
lations of test documents are both represented with
WE.
Cn-Cn: This method represents training and
test documents in Chinese only with WC.
English-to-Chinese translations of training docu-
ments and Chinese test documents are both repre-
sented with WC.
En-Cn: This method represents English train-
ing documents with WE, while represents Chi-
nese test documents with WC. Chandar A P et
al. (2014) employed this method in their work.
BDR: This method adopts our bilingual doc-
ument representation method, which represents
training and test documents with both WE and
</bodyText>
<figure confidence="0.982224814814815">
WC.
0.8
0.79
0.78
Average
0.77
0.76
0.75
0.74
0 0.2 0.4 0.6 0.8
ν
0.73
En-En
Cn-Cn
En-Cn
BDR
Accurary
0.79
0.78
0.77
0.76
0.75
0.74
0.8
Bilingual embeddings
BSWE
book DVD music Average
</figure>
<figureCaption confidence="0.9840595">
Figure 4: Performance comparison of the bilingual
embeddings and BSWE.
</figureCaption>
<bodyText confidence="0.9991435">
As can be seen from Figure 4, by encoding sen-
timent information in the bilingual embeddings,
the performance in book, DVD and music cate-
gories significantly improves to 79.47%, 78.72%
and 76.58% respectively (2.82% increase in book,
1.12% in DVD, and 2.08% in music). The av-
erage accuracy reaches 78.26%, which is 2.01%
higher than that of the bilingual embeddings. The
experimental results indicate the effectiveness of
sentiment information in the bilingual embedding
learning. The BSWE learning approach is em-
ployed for CLSC in the following experiments.
</bodyText>
<figureCaption confidence="0.7226185">
Figure 5: Effects of bilingual document represen-
tation method (BDR).
</figureCaption>
<bodyText confidence="0.949439636363636">
Figure 5 shows the average accuracy curves of
different document representation methods with d-
ifferent destruction fraction ν. We vary ν from 0
to 0.9 with an interval of 0.1.
From Figure 5 we can see that En-En, Cn-Cn,
and En-Cn get similar results. BDR performs con-
stantly better than the other representation meth-
ods throughout the interval [0, 0.9]. The absolute
superiority of BDR benefits from the enhanced a-
bility of sentiment expression.
Meanwhile, when the input x is partially de-
</bodyText>
<page confidence="0.998364">
436
</page>
<bodyText confidence="0.9995478">
stroyed (v varies from 0.1 to 0.9), the perfor-
mance of En-En, Cn-Cn and En-Cn remains sta-
ble, which illustrates the robustness of the denois-
ing autoencoder to corrupting noises. In addi-
tion, the average accuracies of BDR in the inter-
val v ∈ [0.1, 0.9] are all higher than the average
accuracy under the condition v = 0 (78.23%).
Therefore, adding noises properly to the training
data could improve the performance of BSWE for
CLSC.
</bodyText>
<subsectionHeader confidence="0.9935485">
4.3 Influences of Dimension d and
Destruction Fraction v
</subsectionHeader>
<bodyText confidence="0.999909681818182">
Figure 6 shows the relationship between accura-
cies and dimension d of BSWE as well as that be-
tween accuracies and destruction fraction v in au-
toencoders in different categories. Dimension of
embeddings d varies from 50 to 500, and destruc-
tion fraction v varies from 0.1 to 0.9.
As shown in Figure 6, the average accuracies
generally move upward as dimension of BSWE in-
creasing. Generally, the average accuracies keep
higher than 80% with v varying from 0.1 to 0.5
as well as dimension varying from 300 to 500.
When v = 0.1 and d = 400, the average accu-
racy reaches the peak value 80.68% (category ac-
curacy of 81.05% in book, 81.60% in DVD, and
79.40% in music). The experimental results show
that in BSWE learning process, increasing the di-
mension of embeddings or properly adding noises
to the training data helps improve the performance
of CLSC. In this paper, we only evaluate BSWE
when dimension d varies from 50 to 500. Howev-
er, there is still space for further improvement if d
continues to increase.
</bodyText>
<subsectionHeader confidence="0.999865">
4.4 Comparison with Related Work
</subsectionHeader>
<bodyText confidence="0.999140866666667">
Table 2 shows comparisons of the performance
between our approach and some state-of-the-art
systems on NLP&amp;CC 2013 CLSC dataset. Our
approach achieves the best performance with an
80.68% average accuracy. Compared with the re-
cent related work, our approach is more effective
and suitable for eliminating the language gap.
Chen et al. (2014) translated Chinese test da-
ta into English and then gave different weight-
s to sentiment words according to the subject-
predicate component of sentiment words. They
got 77.09% accuracy and took the 2nd place in
NLP&amp;CC 2013 CLSC share task. The machine
translation based approach was limited by the
translation errors.
</bodyText>
<table confidence="0.9980438">
System book DVD music Average
Chen et al. 77.00% 78.33% 75.95% 77.09%
(2014) 78.70% 79.65% 78.30% 78.89%
Gui et al. 80.10% 81.60% 78.60% 80.10%
(2013) 80.63% 80.95% 78.48% 80.02%
Gui et al. 81.05% 81.60% 79.40% 80.68%
(2014)
Zhou et al.
(2014a)
Our approach
</table>
<tableCaption confidence="0.9730795">
Table 2: Performance comparisons on the
NLP&amp;CC 2013 CLSC dataset.
</tableCaption>
<bodyText confidence="0.999700666666667">
Gui et al. (2013; 2014) and Zhou et al. (2014a)
adopted the multi-view approach to bridge the lan-
guage gap. Gui et al. (2013) proposed a mixed
CLSC model by combining co-training and trans-
fer learning strategies. They achieved the high-
est accuracy of 78.89% in NLP&amp;CC CLSC share
task. Gui et al. (2014) further improved the accu-
racy to 80.10% by removing noise from the trans-
ferred samples to avoid negative transfers. Zhou
et al. (2014a) built denoising autoencoders in t-
wo independent views to enhance the robustness
to translation errors in the inputs and achieved
80.02% accuracy. The multi-view approach learn-
s language-specific classifiers in each view dur-
ing training process, which is difficult to capture
the common sentiment information of the two lan-
guages. Our approach integrates the bilingual em-
bedding learning into a unified process, and out-
performs Chen et al. (2014), Gui et al. (2013), Gui
et al. (2014) and Zhou et al. (2014a) by 3.59%,
1.79%, 0.58%, and 0.66% respectively. The su-
periority of our approach benefits from the unified
bilingual embedding learning process and the in-
tegration of semantic and sentiment information.
</bodyText>
<sectionHeader confidence="0.993532" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999974714285714">
This paper proposes an approach to learning B-
SWE by incorporating sentiment information in-
to the bilingual embeddings for CLSC. The pro-
posed approach learns BSWE with the labeled
documents and their translations rather than par-
allel corpora. In addition, BDR is proposed to en-
hance the sentiment expression ability which com-
bines English and Chinese representations. Exper-
iments on the NLP&amp;CC 2013 CLSC dataset show
that our approach outperforms the previous state-
of-the-art systems as well as traditional bilingual
embedding systems. The proposed BSWE are on-
ly evaluated on English-Chinese CLSC in this pa-
per, but it can be popularized to other languages.
</bodyText>
<page confidence="0.997546">
437
</page>
<figureCaption confidence="0.980512">
Figure 6: The relationship between accuracies and dimension d as well as that between accuracies and
destruction fraction ν.
</figureCaption>
<bodyText confidence="0.9993475">
Both semantic and sentiment information play
an important role in sentiment classification. In
the following work, we will further investigate the
relationship between semantic and sentiment in-
formation for CLSC, and balance their functions
to optimize their combination for CLSC.
</bodyText>
<sectionHeader confidence="0.99764" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99913675">
We wish to thank the anonymous reviewers for
their valuable comments. This research is support-
ed by National Natural Science Foundation of Chi-
na (Grant No. 61272375).
</bodyText>
<sectionHeader confidence="0.99861" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.992204">
Carmen Banea, Rada Mihalcea, Janyce Wiebe and
Samer Hassan. 2008. Multilingual Subjectivity
Analysis Using Machine Translation. In Proceed-
ings of the 2008 Conference on Empirical Method-
s in Natural Language Processing, pages 127-135.
Association for Computational Linguistics.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A Neural Probabilistic Lan-
guage Model. The Journal of Machine Learning Re-
search, vol 3: 1137-1155.
Yoshua Bengio, Pascal Lamblin, Dan Popovici, and
Hugo Larochelle. 2007. Greedy layer-wise train-
ing of deep networks. In Proceedings of Advances
in Neural Information Processing Systems 19 (NIPS
06), pages 153-160. MIT Press.
Yoshua Bengio, Aaron Courville, and Pascal Vincent.
2013. Representation learning: A review and new
perspectives. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence 35(8): 1798-1828.
IEEE.
James Bergstra, Olivier Breuleux, Frederic Bastien,
Pascal Lamblin, Razvan Pascanu, Guillaume Des-
jardins, Joseph Turian, Yoshua Bengio. 2010.
Theano: a CPU and GPU math expression compiler.
In Proceedings of the Python for scientific comput-
ing conference (SciPy).
Dmitriy Bespalov, Bing Bai, Yanjun Qi, and Ali Shok-
oufandeh. 2011. Sentiment classification based on
supervised latent n-gram analysis. In Proceedings
of the Conference on Information and Knowledge
Management, pages 375-382. ACM.
Sarath Chandar A P, Mitesh M. Khapra, Balara-
man Ravindran, Vikas Raykar and Amrita Saha.
2013. Multilingual deep learning. In Deep Learning
Workshop at NIPS 2013.
Sarath Chandar A P, Stanislas Lauly, Hugo Larochelle,
Mitesh M Khapra, Balaraman Ravindran,
Vikas Raykar, and Amrita Saha. 2014. An
autoencoder approach to learning bilingual word
representations. In Advances in Neural Information
Processing Systems, pages 1853-1861.
</reference>
<page confidence="0.993954">
438
</page>
<reference confidence="0.996371972477065">
Qiang Chen, Yanxiang He, Xule Liu, Songtao Sun,
Min Peng, and Fei Li. 2014. Cross-Language Sen-
timent Analysis Based on Parser (in Chinese). Acta
Scientiarum Naturalium Universitatis Pekinensis, 50
(1): 55-60.
G. E. Hinton and R. R. Salakhutdinov. 2006. Reducing
the Dimensionality of Data with Neural Networks.
Science, vol 313: 504-507.
Luigi Galavotti, Fabrizio Sebastiani, and Maria Sim-
i. 2000. Feature Selection and Negative Evidence
in Automated Text Categorization. In Proceedings
of ECDL-00, 4th European Conference on Research
and Advanced Technology for Digital Libraries.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In Pro-
ceedings of 28th International Conference on Ma-
chine Learning, pages 513-520.
Lin Gui, Ruifeng Xu, Jun Xu, Li Yuan, Yuanlin Yao,
Jiyun Zhou, Qiaoyun Qiu, Shuwei Wang, Kam-
Fai Wong, and Ricky Cheung. 2013. A mixed mod-
el for cross lingual opinion analysis. In Proceedings
of Natural Language Processing and Chinese Com-
puting, pages 93-104. Springer Verlag.
Lin Gui, Ruifeng Xu, Qin Lu, Jun Xu, Jian Xu, Bin Liu,
and Xiaolong Wang. 2014. Cross-lingual Opinion
Analysis via Negative Transfer Detection. In Pro-
ceedings of the 52nd Annual Meeting of the Associ-
ation for Computational Linguistics (Short Papers),
pages 860-865. Association for Computational Lin-
guistics.
Thorsten Joachims. 1999. Making large-Scale SVM
Learning Practical. Universit¨at Dortmund.
Alistair Kennedy and Diana Inkpen. 2006. Sentiment
classification of movie reviews using contextual va-
lence shifters. Computational intelligence, 22(2):
110-125.
Shoushan Li, Rong Wang, Huanhuan Liu, and Chu-
Ren Huang. 2013. Active learning for cross-lingual
sentiment classification. In Proceedings of Natu-
ral Language Processing and Chinese Computing,
pages 236-246. Springer Verlag.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning Word Vectors for Sentiment Anal-
ysis. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics,
pages 142-150. Association for Computational Lin-
guistics.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space
word representations. In Proceedings of NAACL-
HLT, pages 746-751. Association for Computation-
al Linguistics.
Bo Pang, Lillian Lee and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 79-86. ACM.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Annual Meeting on Association for Com-
putational Linguistics, pages 271-278. Association
for Computational Linguistics.
Richard Socher, Cliff Chiung-Yu Lin, Andrew Y. Ng,
and Christopher D. Manning. 2011. Parsing natu-
ral scenes and natural language with recursive neu-
ral networks. In Proceedings of the International
Conference on Machine Learning, pages 129-136.
Bellevue.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic Com-
positionality through Recursive Matrix-Vector S-
paces. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1201-1211. Association for Computational Linguis-
tics.
Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, T-
ing Liu, and Bing Qin. 2014. Learning Sentiment-
Specific Word Embedding for Twitter Sentimen-
t Classification. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistic, pages 1555-1565. Association for Compu-
tational Linguistics.
Xuewei Tang and Xiaojun Wan. 2014. Learn-
ing Bilingual Embedding Model for Cross-language
Sentiment Classification. In Proceedings of 2014
IEEE/WIC/ACM International Joint Conferences on
Web Intelligence (WI) and Intelligent Agent Tech-
nologies (IAT), pages 134-141. IEEE.
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and
Pierre-Antoine Manzagol. 2008. Extracting and
composing robust features with denoising autoen-
coders. In Proceedings of the 25th international
conference on Machine learning, pages 1096-1103.
ACM.
Xiaojun Wan. 2009. Co-training for cross-lingual sen-
timent classification. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP, pages 235-
243. Association for Computational Linguistics.
Yuan Wang, Zhaohui Li, Jie Liu, Zhicheng He,
Yalou Huang, and Dong Li. 2014. Word Vec-
tor Modeling for Sentiment Analysis of Product Re-
views. In Proceedings of Natural Language Pro-
cessing and Chinese Computing, pages 168-180.
Springer Verlag.
</reference>
<page confidence="0.990644">
439
</page>
<reference confidence="0.998775045454545">
Huaping Zhang, Hongkui Yu, Deyi Xiong, and Qun Li-
u. 2003. HHMM-based Chinese Lexical Analyzer
ICTCLAS. In 2nd SIGHAN workshop affiliated with
41th ACL, pages 184-187. Association for Compu-
tational Linguistics.
Guangyou Zhou, Tingting He, and Jun Zhao. 2014b.
Bridging the Language Gap: Learning Distribut-
ed Semantics for Cross-Lingual Sentiment Classifi-
cation. In Proceedings of Natural Language Pro-
cessing and Chinese Computing, pages 138-149.
Springer Verlag.
Huiwei Zhou, Long Chen, and Degen Huang. 2014a.
Cross-lingual sentiment classification based on de-
noising autoencoder. In Proceedings of Natu-
ral Language Processing and Chinese Computing,
pages 181-192. Springer Verlag.
Will Y. Zou, Richard Socher, Daniel Cer, and Christo-
pher D. Manning. 2013. Bilingual Word Embed-
ding for Phrase-Based Machine Translation. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1393-
1398.
</reference>
<page confidence="0.998191">
440
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.907623">
<title confidence="0.9998655">Learning Bilingual Sentiment Word Embeddings for Sentiment Classification</title>
<author confidence="0.996501">Huiwei Zhou</author>
<author confidence="0.996501">Long Chen</author>
<author confidence="0.996501">Fulin Shi</author>
<author confidence="0.996501">Degen</author>
<affiliation confidence="0.9659855">School of Computer Science and Dalian University of Technology, Dalian, P.R.</affiliation>
<abstract confidence="0.99908888">The sentiment classification performance relies on high-quality sentiment resources. However, these resources are imbalanced in different languages. Cross-language sentiment classification (CLSC) can leverage the rich resources in one language (source language) for sentiment classification in a resource-scarce language (target language). Bilingual embeddings could eliminate the semantic gap between two languages for CLSC, but ignore the sentiment information of text. This paper proposes an approach to learning bilingual sentiment word embeddings (BSWE) for English-Chinese CLSC. The proposed B- SWE incorporate sentiment information of text into bilingual embeddings. Furthermore, we can learn high-quality BSWE by simply employing labeled corpora and their translations, without relying on largescale parallel corpora. Experiments on NLP&amp;CC 2013 CLSC dataset show that our approach outperforms the state-of-theart systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Carmen Banea</author>
<author>Rada Mihalcea</author>
<author>Janyce Wiebe</author>
<author>Samer Hassan</author>
</authors>
<title>Multilingual Subjectivity Analysis Using Machine Translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>127--135</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2215" citStr="Banea et al., 2008" startWordPosition="304" endWordPosition="307">nd quantity of the labeled corpora, which are considered as the most valuable resources in sentiment classification task. However, such sentiment resources are imbalanced in different languages. To leverage resources in the source language to improve the sentiment classification performance in the target language, cross-language sentiment classification (CLSC) approaches have been investigated. The traditional CLSC approaches employ machine translation (MT) systems to translate corpora in the source language into the target language, and train the sentiment classifiers in the target language (Banea et al., 2008). Directly employing the translated resources for sentiment classification in the target language is simple and could get acceptable results. However, the gap between the source language and target language inevitably impacts the performance of sentiment classification. To improve the classification accuracy, multiview approaches have been proposed. In these approaches, the resources in the source language and their translations in the target language are both used to train sentiment classifiers in two independent views (Wan, 2009; Gui et al., 2013; Zhou et al., 2014a). The final results are d</context>
</contexts>
<marker>Banea, Mihalcea, Wiebe, Hassan, 2008</marker>
<rawString>Carmen Banea, Rada Mihalcea, Janyce Wiebe and Samer Hassan. 2008. Multilingual Subjectivity Analysis Using Machine Translation. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 127-135. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Jauvin</author>
</authors>
<title>A Neural Probabilistic Language Model.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>3</volume>
<pages>1137--1155</pages>
<contexts>
<context position="10468" citStr="Bengio et al., 2003" startWordPosition="1538" endWordPosition="1541">Classification Bilingual embedding learning algorithms focus on capturing syntactic and semantic similarities across languages, but ignore sentiment information. To date, many embedding learning algorithms have been developed for sentiment classification problem by incorporating sentiment information into word embeddings. Maas et al. (2011) presented a probabilistic model that combined unsupervised and supervised techniques to learn word vectors, capturing semantic information as well as sentiment information. Wang et al. (2014) introduced sentiment labels into Neural Network Language Models (Bengio et al., 2003) to enhance sentiment expression ability of word vectors. Tang et al. (2014) theoretically and empirically analyzed the effects of the syntactic context and sentiment information in word vectors, and showed that the syntactic context and sentiment information were equally important to sentiment classification. Recent years have seen a surge of interest in word embeddings with deep learning technique (Bespalov et al., 2011; Glorot et al., 2011; Socher et al., 2011; Socher et al., 2012), which have been empirically shown to preserve linguistic regularities (Mikolov et al., 2013). Our work focuse</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A Neural Probabilistic Language Model. The Journal of Machine Learning Research, vol 3: 1137-1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Pascal Lamblin</author>
<author>Dan Popovici</author>
<author>Hugo Larochelle</author>
</authors>
<title>Greedy layer-wise training of deep networks.</title>
<date>2007</date>
<booktitle>In Proceedings of Advances in Neural Information Processing Systems 19 (NIPS 06),</booktitle>
<pages>153--160</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="11948" citStr="Bengio et al., 2007" startWordPosition="1765" endWordPosition="1768">e importantly, sentiment information is integrated into bilingual embeddings to improve their performance in CLSC. 3 Bilingual Sentiment Word Embeddings (BSWE) for Cross-language Sentiment Classification 3.1 Denoising Autoencoder It has been demonstrated that the denoising autoencoder could decrease the effects of translation errors on the performance of CLSC (Zhou et al., 2014a). This paper proposes a deep learning based approach, which employs the denoising autoencoder to learn the bilingual embeddings for CLSC. A denoising autoencoder is the modification of an autoencoder. The autoencoder (Bengio et al., 2007) includes an encoder fθ and a decoder gθe. The encoder maps a d-dimensional input vector x E [0, 1]d to a hidden representation y E [0,1]d&apos; through a deterministic mapping y = fθ(x) = a(Wx + b), parameterized by 0 = {W, b}. W is a weight matrix, b is a bias term, and a(x) is the activation function. The decoder maps y back to a reconstructed vector xˆ = gθl(y) = a(WTy + c), parameterized by 0&apos; = {WT, c}, where c is the bias term for reconstruction. Through the process of encoding and decoding, the parameters 0 and 0&apos; of the autoencoder will be trained by gradient descent to minimize the loss f</context>
</contexts>
<marker>Bengio, Lamblin, Popovici, Larochelle, 2007</marker>
<rawString>Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. 2007. Greedy layer-wise training of deep networks. In Proceedings of Advances in Neural Information Processing Systems 19 (NIPS 06), pages 153-160. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Aaron Courville</author>
<author>Pascal Vincent</author>
</authors>
<title>Representation learning: A review and new perspectives.</title>
<date>2013</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence</journal>
<volume>35</volume>
<issue>8</issue>
<pages>1798--1828</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="3219" citStr="Bengio et al., 2013" startWordPosition="453" endWordPosition="456">es in the source language and their translations in the target language are both used to train sentiment classifiers in two independent views (Wan, 2009; Gui et al., 2013; Zhou et al., 2014a). The final results are determined by ensemble classifiers in these two views to overcome the weakness of monolingual classifiers. However, learning language-specific classifiers in each view fails to capture the common sentiment information of two languages during training process. With the revival of interest in deep learning (Hinton and Salakhutdinov, 2006), shared deep representations (or embeddings) (Bengio et al., 2013) are employed for CLSC (Chandar A P et al., 2013). Usually, paired sentences from parallel corpora are used to learn word embeddings across languages (Chandar A P et al., 2013; Chandar A P et al., 2014), eliminating the need of MT systems. The learned bilingual embeddings could easily project the training data and test data into a common space, where training and testing are performed. However, high-quality bilingual embeddings rely on the large-scale task-related parallel corpora, which are not always readily available. Meanwhile, though semantic similarities across languages are captured dur</context>
</contexts>
<marker>Bengio, Courville, Vincent, 2013</marker>
<rawString>Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2013. Representation learning: A review and new perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence 35(8): 1798-1828. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Bergstra</author>
<author>Olivier Breuleux</author>
<author>Frederic Bastien</author>
<author>Pascal Lamblin</author>
<author>Razvan Pascanu</author>
<author>Guillaume Desjardins</author>
<author>Joseph Turian</author>
<author>Yoshua Bengio</author>
</authors>
<title>Theano: a CPU and GPU math expression compiler.</title>
<date>2010</date>
<booktitle>In Proceedings of the Python for scientific computing conference (SciPy).</booktitle>
<contexts>
<context position="21126" citStr="Bergstra et al., 2010" startWordPosition="3317" endWordPosition="3320">cn/conference/2013/dldoc/evsam03.zip 3http://tcci.ccf.org.cn/conference/2013/dldoc/evdata03.zip 434 sists of product reviews on three categories: book, DVD, and music. Each category contains 4,000 English labeled data as training data (the ratio of the number of positive and negative samples is 1:1) and 4,000 Chinese unlabeled data as test data. Tools. In our experiments, Google Translate4 is adopted for both English-to-Chinese and Chineseto-English translation. ICTCLAS (Zhang et al., 2003) is used as Chinese word segmentation tool. A denoising autoencoder is developed based on Theano system (Bergstra et al., 2010). BSWE are trained for 50 and 30 epochs in unsupervised phase and supervised phases respectively. SV M&apos;Zght (Joachims, 1999) is used to train linear SVM sentiment classifiers Evaluation Metric. The performance is evaluated by the classification accuracy for each category, and the average accuracy of three categories, respectively. The category accuracy is defined as: Accuracy, = #system correct, (11) #system total, where c is one of the three categories, and #system correct, and #system total, stand for the number of being correctly classified reviews and the number of total reviews in the cat</context>
</contexts>
<marker>Bergstra, Breuleux, Bastien, Lamblin, Pascanu, Desjardins, Turian, Bengio, 2010</marker>
<rawString>James Bergstra, Olivier Breuleux, Frederic Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, Yoshua Bengio. 2010. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for scientific computing conference (SciPy).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitriy Bespalov</author>
<author>Bing Bai</author>
<author>Yanjun Qi</author>
<author>Ali Shokoufandeh</author>
</authors>
<title>Sentiment classification based on supervised latent n-gram analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Information and Knowledge Management,</booktitle>
<pages>375--382</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="10893" citStr="Bespalov et al., 2011" startWordPosition="1604" endWordPosition="1607">ues to learn word vectors, capturing semantic information as well as sentiment information. Wang et al. (2014) introduced sentiment labels into Neural Network Language Models (Bengio et al., 2003) to enhance sentiment expression ability of word vectors. Tang et al. (2014) theoretically and empirically analyzed the effects of the syntactic context and sentiment information in word vectors, and showed that the syntactic context and sentiment information were equally important to sentiment classification. Recent years have seen a surge of interest in word embeddings with deep learning technique (Bespalov et al., 2011; Glorot et al., 2011; Socher et al., 2011; Socher et al., 2012), which have been empirically shown to preserve linguistic regularities (Mikolov et al., 2013). Our work focuses on learning bilingual sentiment word embeddings (BSWE) with deep learning technique. Unlike the work of Chandar A P et al. (2014) that adopted parallel corpora to learn bilingual embeddings, we only use training data and their translations to learn BSWE. More importantly, sentiment information is integrated into bilingual embeddings to improve their performance in CLSC. 3 Bilingual Sentiment Word Embeddings (BSWE) for C</context>
</contexts>
<marker>Bespalov, Bai, Qi, Shokoufandeh, 2011</marker>
<rawString>Dmitriy Bespalov, Bing Bai, Yanjun Qi, and Ali Shokoufandeh. 2011. Sentiment classification based on supervised latent n-gram analysis. In Proceedings of the Conference on Information and Knowledge Management, pages 375-382. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarath Chandar A P</author>
<author>Mitesh M Khapra</author>
</authors>
<title>Balaraman Ravindran, Vikas Raykar and Amrita Saha.</title>
<date>2013</date>
<booktitle>In Deep Learning Workshop at NIPS</booktitle>
<marker>P, Khapra, 2013</marker>
<rawString>Sarath Chandar A P, Mitesh M. Khapra, Balaraman Ravindran, Vikas Raykar and Amrita Saha. 2013. Multilingual deep learning. In Deep Learning Workshop at NIPS 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarath Chandar A P</author>
<author>Stanislas Lauly</author>
<author>Hugo Larochelle</author>
<author>Mitesh M Khapra</author>
<author>Balaraman Ravindran</author>
<author>Vikas Raykar</author>
<author>Amrita Saha</author>
</authors>
<title>An autoencoder approach to learning bilingual word representations.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>1853--1861</pages>
<contexts>
<context position="3421" citStr="P et al., 2014" startWordPosition="491" endWordPosition="494">lts are determined by ensemble classifiers in these two views to overcome the weakness of monolingual classifiers. However, learning language-specific classifiers in each view fails to capture the common sentiment information of two languages during training process. With the revival of interest in deep learning (Hinton and Salakhutdinov, 2006), shared deep representations (or embeddings) (Bengio et al., 2013) are employed for CLSC (Chandar A P et al., 2013). Usually, paired sentences from parallel corpora are used to learn word embeddings across languages (Chandar A P et al., 2013; Chandar A P et al., 2014), eliminating the need of MT systems. The learned bilingual embeddings could easily project the training data and test data into a common space, where training and testing are performed. However, high-quality bilingual embeddings rely on the large-scale task-related parallel corpora, which are not always readily available. Meanwhile, though semantic similarities across languages are captured during bilingual embedding learning process, sentiment information of 430 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference </context>
<context position="8614" citStr="P et al., 2014" startWordPosition="1276" endWordPosition="1279">those in the target language to decrease the gap between two languages. Zhou et al. (2014a) proposed a combination CLSC model, which adopted denoising autoencoders (Vincent et al., 2008) to enhance the robustness to translation errors of the input. Most recently, a number of studies adopt deep learning technique to learn bilingual representations with parallel corpora. Bilingual representations have been successfully applied in many NLP tasks, such as machine translation (Zou et al., 2013), sentiment classification (Chandar A P et al., 2013; Zhou et al., 2014b), text classification (Chandar A P et al., 2014), etc. 431 Chandar A P et al. (2013) learned bilingual representations with aligned sentences throughout two phases: the language-specific representation learning phase and the shared representation learning phase. In the language-specific representation learning phase, they applied autoencoders to obtain a language-specific representation for each entity in two languages respectively. In shared representation learning phase, pairs of parallel language-specific representations were passed to an autoencoder to learn bilingual representations. To joint language-specific representations and bilin</context>
<context position="11199" citStr="P et al. (2014)" startWordPosition="1655" endWordPosition="1658">the effects of the syntactic context and sentiment information in word vectors, and showed that the syntactic context and sentiment information were equally important to sentiment classification. Recent years have seen a surge of interest in word embeddings with deep learning technique (Bespalov et al., 2011; Glorot et al., 2011; Socher et al., 2011; Socher et al., 2012), which have been empirically shown to preserve linguistic regularities (Mikolov et al., 2013). Our work focuses on learning bilingual sentiment word embeddings (BSWE) with deep learning technique. Unlike the work of Chandar A P et al. (2014) that adopted parallel corpora to learn bilingual embeddings, we only use training data and their translations to learn BSWE. More importantly, sentiment information is integrated into bilingual embeddings to improve their performance in CLSC. 3 Bilingual Sentiment Word Embeddings (BSWE) for Cross-language Sentiment Classification 3.1 Denoising Autoencoder It has been demonstrated that the denoising autoencoder could decrease the effects of translation errors on the performance of CLSC (Zhou et al., 2014a). This paper proposes a deep learning based approach, which employs the denoising autoenc</context>
<context position="25918" citStr="P et al. (2014)" startWordPosition="4054" endWordPosition="4057">thod (BDR) is compared with the following monolingual document representation methods. En-En: This method represents training and test documents in English only with WE. English training documents and Chinese-to-English translations of test documents are both represented with WE. Cn-Cn: This method represents training and test documents in Chinese only with WC. English-to-Chinese translations of training documents and Chinese test documents are both represented with WC. En-Cn: This method represents English training documents with WE, while represents Chinese test documents with WC. Chandar A P et al. (2014) employed this method in their work. BDR: This method adopts our bilingual document representation method, which represents training and test documents with both WE and WC. 0.8 0.79 0.78 Average 0.77 0.76 0.75 0.74 0 0.2 0.4 0.6 0.8 ν 0.73 En-En Cn-Cn En-Cn BDR Accurary 0.79 0.78 0.77 0.76 0.75 0.74 0.8 Bilingual embeddings BSWE book DVD music Average Figure 4: Performance comparison of the bilingual embeddings and BSWE. As can be seen from Figure 4, by encoding sentiment information in the bilingual embeddings, the performance in book, DVD and music categories significantly improves to 79.47%</context>
</contexts>
<marker>P, Lauly, Larochelle, Khapra, Ravindran, Raykar, Saha, 2014</marker>
<rawString>Sarath Chandar A P, Stanislas Lauly, Hugo Larochelle, Mitesh M Khapra, Balaraman Ravindran, Vikas Raykar, and Amrita Saha. 2014. An autoencoder approach to learning bilingual word representations. In Advances in Neural Information Processing Systems, pages 1853-1861.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiang Chen</author>
<author>Yanxiang He</author>
<author>Xule Liu</author>
<author>Songtao Sun</author>
<author>Min Peng</author>
<author>Fei Li</author>
</authors>
<title>Cross-Language Sentiment Analysis Based on Parser (in Chinese). Acta Scientiarum Naturalium Universitatis Pekinensis,</title>
<date>2014</date>
<volume>50</volume>
<issue>1</issue>
<pages>55--60</pages>
<contexts>
<context position="29332" citStr="Chen et al. (2014)" startWordPosition="4627" endWordPosition="4630">or properly adding noises to the training data helps improve the performance of CLSC. In this paper, we only evaluate BSWE when dimension d varies from 50 to 500. However, there is still space for further improvement if d continues to increase. 4.4 Comparison with Related Work Table 2 shows comparisons of the performance between our approach and some state-of-the-art systems on NLP&amp;CC 2013 CLSC dataset. Our approach achieves the best performance with an 80.68% average accuracy. Compared with the recent related work, our approach is more effective and suitable for eliminating the language gap. Chen et al. (2014) translated Chinese test data into English and then gave different weights to sentiment words according to the subjectpredicate component of sentiment words. They got 77.09% accuracy and took the 2nd place in NLP&amp;CC 2013 CLSC share task. The machine translation based approach was limited by the translation errors. System book DVD music Average Chen et al. 77.00% 78.33% 75.95% 77.09% (2014) 78.70% 79.65% 78.30% 78.89% Gui et al. 80.10% 81.60% 78.60% 80.10% (2013) 80.63% 80.95% 78.48% 80.02% Gui et al. 81.05% 81.60% 79.40% 80.68% (2014) Zhou et al. (2014a) Our approach Table 2: Performance compa</context>
<context position="30856" citStr="Chen et al. (2014)" startWordPosition="4877" endWordPosition="4880">C CLSC share task. Gui et al. (2014) further improved the accuracy to 80.10% by removing noise from the transferred samples to avoid negative transfers. Zhou et al. (2014a) built denoising autoencoders in two independent views to enhance the robustness to translation errors in the inputs and achieved 80.02% accuracy. The multi-view approach learns language-specific classifiers in each view during training process, which is difficult to capture the common sentiment information of the two languages. Our approach integrates the bilingual embedding learning into a unified process, and outperforms Chen et al. (2014), Gui et al. (2013), Gui et al. (2014) and Zhou et al. (2014a) by 3.59%, 1.79%, 0.58%, and 0.66% respectively. The superiority of our approach benefits from the unified bilingual embedding learning process and the integration of semantic and sentiment information. 5 Conclusion and Future Work This paper proposes an approach to learning BSWE by incorporating sentiment information into the bilingual embeddings for CLSC. The proposed approach learns BSWE with the labeled documents and their translations rather than parallel corpora. In addition, BDR is proposed to enhance the sentiment expression</context>
</contexts>
<marker>Chen, He, Liu, Sun, Peng, Li, 2014</marker>
<rawString>Qiang Chen, Yanxiang He, Xule Liu, Songtao Sun, Min Peng, and Fei Li. 2014. Cross-Language Sentiment Analysis Based on Parser (in Chinese). Acta Scientiarum Naturalium Universitatis Pekinensis, 50 (1): 55-60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Hinton</author>
<author>R R Salakhutdinov</author>
</authors>
<date>2006</date>
<journal>Reducing the Dimensionality of Data with Neural Networks. Science,</journal>
<volume>313</volume>
<pages>504--507</pages>
<contexts>
<context position="3152" citStr="Hinton and Salakhutdinov, 2006" startWordPosition="444" endWordPosition="447">acy, multiview approaches have been proposed. In these approaches, the resources in the source language and their translations in the target language are both used to train sentiment classifiers in two independent views (Wan, 2009; Gui et al., 2013; Zhou et al., 2014a). The final results are determined by ensemble classifiers in these two views to overcome the weakness of monolingual classifiers. However, learning language-specific classifiers in each view fails to capture the common sentiment information of two languages during training process. With the revival of interest in deep learning (Hinton and Salakhutdinov, 2006), shared deep representations (or embeddings) (Bengio et al., 2013) are employed for CLSC (Chandar A P et al., 2013). Usually, paired sentences from parallel corpora are used to learn word embeddings across languages (Chandar A P et al., 2013; Chandar A P et al., 2014), eliminating the need of MT systems. The learned bilingual embeddings could easily project the training data and test data into a common space, where training and testing are performed. However, high-quality bilingual embeddings rely on the large-scale task-related parallel corpora, which are not always readily available. Meanwh</context>
</contexts>
<marker>Hinton, Salakhutdinov, 2006</marker>
<rawString>G. E. Hinton and R. R. Salakhutdinov. 2006. Reducing the Dimensionality of Data with Neural Networks. Science, vol 313: 504-507.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luigi Galavotti</author>
<author>Fabrizio Sebastiani</author>
<author>Maria Simi</author>
</authors>
<title>Feature Selection and Negative Evidence in Automated Text Categorization.</title>
<date>2000</date>
<booktitle>In Proceedings of ECDL-00, 4th European Conference on Research and Advanced Technology for Digital Libraries.</booktitle>
<contexts>
<context position="14257" citStr="Galavotti et al., 2000" startWordPosition="2174" endWordPosition="2177">r is employed to learn the bilingual embeddings. In the supervised phase, the sentiment information is incorporated into the bilingual embeddings based on sentiment labels of documents to obtain BSWE. 3.2 Unsupervised Phase of the Bilingual Embedding Learning In the unsupervised phase, the English training documents and their Chinese translations are employed to learn the bilingual embeddings (Sentiment polarity labels of documents are not employed in this phase). Based on the English documents, 2,000 English sentiment words in MPQA subjectivity lexicon1 are extracted by the Chisquare method (Galavotti et al., 2000). Their corresponding Chinese translations are used as Chinese sentiment words. Besides, some sentiment words are often modified by negation words, which lead to inversion of their polarities. Therefore, negation features are introduced to each sentiment word to represent its negative form. We take into account 14 frequently-used negation words in English such as not and none; 5 negation words in Chinese such as T (no/not) and ;q (without). A sentiment word modified by these negation words in the window [-2, 2] is considered as its negative form in this paper, while sentiment word features rem</context>
</contexts>
<marker>Galavotti, Sebastiani, Simi, 2000</marker>
<rawString>Luigi Galavotti, Fabrizio Sebastiani, and Maria Simi. 2000. Feature Selection and Negative Evidence in Automated Text Categorization. In Proceedings of ECDL-00, 4th European Conference on Research and Advanced Technology for Digital Libraries.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Glorot</author>
<author>Antoine Bordes</author>
<author>Yoshua Bengio</author>
</authors>
<title>Domain adaptation for large-scale sentiment classification: A deep learning approach.</title>
<date>2011</date>
<booktitle>In Proceedings of 28th International Conference on Machine Learning,</booktitle>
<pages>513--520</pages>
<contexts>
<context position="10914" citStr="Glorot et al., 2011" startWordPosition="1608" endWordPosition="1611">rs, capturing semantic information as well as sentiment information. Wang et al. (2014) introduced sentiment labels into Neural Network Language Models (Bengio et al., 2003) to enhance sentiment expression ability of word vectors. Tang et al. (2014) theoretically and empirically analyzed the effects of the syntactic context and sentiment information in word vectors, and showed that the syntactic context and sentiment information were equally important to sentiment classification. Recent years have seen a surge of interest in word embeddings with deep learning technique (Bespalov et al., 2011; Glorot et al., 2011; Socher et al., 2011; Socher et al., 2012), which have been empirically shown to preserve linguistic regularities (Mikolov et al., 2013). Our work focuses on learning bilingual sentiment word embeddings (BSWE) with deep learning technique. Unlike the work of Chandar A P et al. (2014) that adopted parallel corpora to learn bilingual embeddings, we only use training data and their translations to learn BSWE. More importantly, sentiment information is integrated into bilingual embeddings to improve their performance in CLSC. 3 Bilingual Sentiment Word Embeddings (BSWE) for Cross-language Sentime</context>
</contexts>
<marker>Glorot, Bordes, Bengio, 2011</marker>
<rawString>Xavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011. Domain adaptation for large-scale sentiment classification: A deep learning approach. In Proceedings of 28th International Conference on Machine Learning, pages 513-520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin Gui</author>
<author>Ruifeng Xu</author>
<author>Jun Xu</author>
<author>Li Yuan</author>
<author>Yuanlin Yao</author>
<author>Jiyun Zhou</author>
<author>Qiaoyun Qiu</author>
<author>Shuwei Wang</author>
<author>KamFai Wong</author>
<author>Ricky Cheung</author>
</authors>
<title>A mixed model for cross lingual opinion analysis.</title>
<date>2013</date>
<booktitle>In Proceedings of Natural Language Processing and Chinese Computing,</booktitle>
<pages>93--104</pages>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="2769" citStr="Gui et al., 2013" startWordPosition="388" endWordPosition="391">timent classifiers in the target language (Banea et al., 2008). Directly employing the translated resources for sentiment classification in the target language is simple and could get acceptable results. However, the gap between the source language and target language inevitably impacts the performance of sentiment classification. To improve the classification accuracy, multiview approaches have been proposed. In these approaches, the resources in the source language and their translations in the target language are both used to train sentiment classifiers in two independent views (Wan, 2009; Gui et al., 2013; Zhou et al., 2014a). The final results are determined by ensemble classifiers in these two views to overcome the weakness of monolingual classifiers. However, learning language-specific classifiers in each view fails to capture the common sentiment information of two languages during training process. With the revival of interest in deep learning (Hinton and Salakhutdinov, 2006), shared deep representations (or embeddings) (Bengio et al., 2013) are employed for CLSC (Chandar A P et al., 2013). Usually, paired sentences from parallel corpora are used to learn word embeddings across languages </context>
<context position="7801" citStr="Gui et al. (2013)" startWordPosition="1146" endWordPosition="1149">translations or parallel corpora are usually employed to solve this problem. We present a brief review of CLSC from two aspects: machine translation based approaches and parallel corpora based approaches. Machine translation based approaches use MT systems to project training data into the target language or test data into the source language. Wan (2009) proposed a co-training approach for CLSC. The approach first translated Chinese test data into English, and English training data into Chinese. Then, they performed training and testing in two independent views: English view and Chinese view. Gui et al. (2013) combined self-training approach with co-training approach by estimating the confidence of each monolingual system. Li et al. (2013) selected the samples in the source language that were similar to those in the target language to decrease the gap between two languages. Zhou et al. (2014a) proposed a combination CLSC model, which adopted denoising autoencoders (Vincent et al., 2008) to enhance the robustness to translation errors of the input. Most recently, a number of studies adopt deep learning technique to learn bilingual representations with parallel corpora. Bilingual representations have</context>
<context position="29988" citStr="Gui et al. (2013" startWordPosition="4735" endWordPosition="4738">sh and then gave different weights to sentiment words according to the subjectpredicate component of sentiment words. They got 77.09% accuracy and took the 2nd place in NLP&amp;CC 2013 CLSC share task. The machine translation based approach was limited by the translation errors. System book DVD music Average Chen et al. 77.00% 78.33% 75.95% 77.09% (2014) 78.70% 79.65% 78.30% 78.89% Gui et al. 80.10% 81.60% 78.60% 80.10% (2013) 80.63% 80.95% 78.48% 80.02% Gui et al. 81.05% 81.60% 79.40% 80.68% (2014) Zhou et al. (2014a) Our approach Table 2: Performance comparisons on the NLP&amp;CC 2013 CLSC dataset. Gui et al. (2013; 2014) and Zhou et al. (2014a) adopted the multi-view approach to bridge the language gap. Gui et al. (2013) proposed a mixed CLSC model by combining co-training and transfer learning strategies. They achieved the highest accuracy of 78.89% in NLP&amp;CC CLSC share task. Gui et al. (2014) further improved the accuracy to 80.10% by removing noise from the transferred samples to avoid negative transfers. Zhou et al. (2014a) built denoising autoencoders in two independent views to enhance the robustness to translation errors in the inputs and achieved 80.02% accuracy. The multi-view approach learns </context>
</contexts>
<marker>Gui, Xu, Xu, Yuan, Yao, Zhou, Qiu, Wang, Wong, Cheung, 2013</marker>
<rawString>Lin Gui, Ruifeng Xu, Jun Xu, Li Yuan, Yuanlin Yao, Jiyun Zhou, Qiaoyun Qiu, Shuwei Wang, KamFai Wong, and Ricky Cheung. 2013. A mixed model for cross lingual opinion analysis. In Proceedings of Natural Language Processing and Chinese Computing, pages 93-104. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin Gui</author>
<author>Ruifeng Xu</author>
<author>Qin Lu</author>
<author>Jun Xu</author>
<author>Jian Xu</author>
<author>Bin Liu</author>
<author>Xiaolong Wang</author>
</authors>
<title>Cross-lingual Opinion Analysis via Negative Transfer Detection.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers),</booktitle>
<pages>860--865</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="30274" citStr="Gui et al. (2014)" startWordPosition="4785" endWordPosition="4788">ok DVD music Average Chen et al. 77.00% 78.33% 75.95% 77.09% (2014) 78.70% 79.65% 78.30% 78.89% Gui et al. 80.10% 81.60% 78.60% 80.10% (2013) 80.63% 80.95% 78.48% 80.02% Gui et al. 81.05% 81.60% 79.40% 80.68% (2014) Zhou et al. (2014a) Our approach Table 2: Performance comparisons on the NLP&amp;CC 2013 CLSC dataset. Gui et al. (2013; 2014) and Zhou et al. (2014a) adopted the multi-view approach to bridge the language gap. Gui et al. (2013) proposed a mixed CLSC model by combining co-training and transfer learning strategies. They achieved the highest accuracy of 78.89% in NLP&amp;CC CLSC share task. Gui et al. (2014) further improved the accuracy to 80.10% by removing noise from the transferred samples to avoid negative transfers. Zhou et al. (2014a) built denoising autoencoders in two independent views to enhance the robustness to translation errors in the inputs and achieved 80.02% accuracy. The multi-view approach learns language-specific classifiers in each view during training process, which is difficult to capture the common sentiment information of the two languages. Our approach integrates the bilingual embedding learning into a unified process, and outperforms Chen et al. (2014), Gui et al. (2013</context>
</contexts>
<marker>Gui, Xu, Lu, Xu, Xu, Liu, Wang, 2014</marker>
<rawString>Lin Gui, Ruifeng Xu, Qin Lu, Jun Xu, Jian Xu, Bin Liu, and Xiaolong Wang. 2014. Cross-lingual Opinion Analysis via Negative Transfer Detection. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 860-865. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-Scale SVM Learning Practical.</title>
<date>1999</date>
<institution>Universit¨at Dortmund.</institution>
<contexts>
<context position="21250" citStr="Joachims, 1999" startWordPosition="3339" endWordPosition="3340">ee categories: book, DVD, and music. Each category contains 4,000 English labeled data as training data (the ratio of the number of positive and negative samples is 1:1) and 4,000 Chinese unlabeled data as test data. Tools. In our experiments, Google Translate4 is adopted for both English-to-Chinese and Chineseto-English translation. ICTCLAS (Zhang et al., 2003) is used as Chinese word segmentation tool. A denoising autoencoder is developed based on Theano system (Bergstra et al., 2010). BSWE are trained for 50 and 30 epochs in unsupervised phase and supervised phases respectively. SV M&apos;Zght (Joachims, 1999) is used to train linear SVM sentiment classifiers Evaluation Metric. The performance is evaluated by the classification accuracy for each category, and the average accuracy of three categories, respectively. The category accuracy is defined as: Accuracy, = #system correct, (11) #system total, where c is one of the three categories, and #system correct, and #system total, stand for the number of being correctly classified reviews and the number of total reviews in the category c, respectively. The average accuracy is shown as: 1 � Accuracy, (12) Average = 3 , 4.2 Evaluations on BSWE In this se</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-Scale SVM Learning Practical. Universit¨at Dortmund.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alistair Kennedy</author>
<author>Diana Inkpen</author>
</authors>
<title>Sentiment classification of movie reviews using contextual valence shifters.</title>
<date>2006</date>
<journal>Computational intelligence,</journal>
<volume>22</volume>
<issue>2</issue>
<pages>110--125</pages>
<contexts>
<context position="1505" citStr="Kennedy and Inkpen, 2006" startWordPosition="202" endWordPosition="205">glish-Chinese CLSC. The proposed BSWE incorporate sentiment information of text into bilingual embeddings. Furthermore, we can learn high-quality BSWE by simply employing labeled corpora and their translations, without relying on largescale parallel corpora. Experiments on NLP&amp;CC 2013 CLSC dataset show that our approach outperforms the state-of-theart systems. 1 Introduction Sentiment classification is a task of predicting sentiment polarity of text, which has attracted considerable interest in the NLP field. To date, a number of corpus-based approaches (Pang et al., 2002; Pang and Lee, 2004; Kennedy and Inkpen, 2006) have been developed for sentiment classification. The approaches heavily rely on quality and quantity of the labeled corpora, which are considered as the most valuable resources in sentiment classification task. However, such sentiment resources are imbalanced in different languages. To leverage resources in the source language to improve the sentiment classification performance in the target language, cross-language sentiment classification (CLSC) approaches have been investigated. The traditional CLSC approaches employ machine translation (MT) systems to translate corpora in the source lang</context>
</contexts>
<marker>Kennedy, Inkpen, 2006</marker>
<rawString>Alistair Kennedy and Diana Inkpen. 2006. Sentiment classification of movie reviews using contextual valence shifters. Computational intelligence, 22(2): 110-125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shoushan Li</author>
<author>Rong Wang</author>
<author>Huanhuan Liu</author>
<author>ChuRen Huang</author>
</authors>
<title>Active learning for cross-lingual sentiment classification.</title>
<date>2013</date>
<booktitle>In Proceedings of Natural Language Processing and Chinese Computing,</booktitle>
<pages>236--246</pages>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="7933" citStr="Li et al. (2013)" startWordPosition="1164" endWordPosition="1167">ine translation based approaches and parallel corpora based approaches. Machine translation based approaches use MT systems to project training data into the target language or test data into the source language. Wan (2009) proposed a co-training approach for CLSC. The approach first translated Chinese test data into English, and English training data into Chinese. Then, they performed training and testing in two independent views: English view and Chinese view. Gui et al. (2013) combined self-training approach with co-training approach by estimating the confidence of each monolingual system. Li et al. (2013) selected the samples in the source language that were similar to those in the target language to decrease the gap between two languages. Zhou et al. (2014a) proposed a combination CLSC model, which adopted denoising autoencoders (Vincent et al., 2008) to enhance the robustness to translation errors of the input. Most recently, a number of studies adopt deep learning technique to learn bilingual representations with parallel corpora. Bilingual representations have been successfully applied in many NLP tasks, such as machine translation (Zou et al., 2013), sentiment classification (Chandar A P </context>
</contexts>
<marker>Li, Wang, Liu, Huang, 2013</marker>
<rawString>Shoushan Li, Rong Wang, Huanhuan Liu, and ChuRen Huang. 2013. Active learning for cross-lingual sentiment classification. In Proceedings of Natural Language Processing and Chinese Computing, pages 236-246. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew L Maas</author>
<author>Raymond E Daly</author>
<author>Peter T Pham</author>
<author>Dan Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Learning Word Vectors for Sentiment Analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>142--150</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="10190" citStr="Maas et al. (2011)" startWordPosition="1498" endWordPosition="1501">s, only representations in the source language are used for training, and representations in the target language are used for testing, which ignores the interactions of semantic information between the source language and target language. 2.2 Embedding Learning for Sentiment Classification Bilingual embedding learning algorithms focus on capturing syntactic and semantic similarities across languages, but ignore sentiment information. To date, many embedding learning algorithms have been developed for sentiment classification problem by incorporating sentiment information into word embeddings. Maas et al. (2011) presented a probabilistic model that combined unsupervised and supervised techniques to learn word vectors, capturing semantic information as well as sentiment information. Wang et al. (2014) introduced sentiment labels into Neural Network Language Models (Bengio et al., 2003) to enhance sentiment expression ability of word vectors. Tang et al. (2014) theoretically and empirically analyzed the effects of the syntactic context and sentiment information in word vectors, and showed that the syntactic context and sentiment information were equally important to sentiment classification. Recent yea</context>
</contexts>
<marker>Maas, Daly, Pham, Huang, Ng, Potts, 2011</marker>
<rawString>Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning Word Vectors for Sentiment Analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 142-150. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACLHLT,</booktitle>
<pages>746--751</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11051" citStr="Mikolov et al., 2013" startWordPosition="1630" endWordPosition="1633"> Language Models (Bengio et al., 2003) to enhance sentiment expression ability of word vectors. Tang et al. (2014) theoretically and empirically analyzed the effects of the syntactic context and sentiment information in word vectors, and showed that the syntactic context and sentiment information were equally important to sentiment classification. Recent years have seen a surge of interest in word embeddings with deep learning technique (Bespalov et al., 2011; Glorot et al., 2011; Socher et al., 2011; Socher et al., 2012), which have been empirically shown to preserve linguistic regularities (Mikolov et al., 2013). Our work focuses on learning bilingual sentiment word embeddings (BSWE) with deep learning technique. Unlike the work of Chandar A P et al. (2014) that adopted parallel corpora to learn bilingual embeddings, we only use training data and their translations to learn BSWE. More importantly, sentiment information is integrated into bilingual embeddings to improve their performance in CLSC. 3 Bilingual Sentiment Word Embeddings (BSWE) for Cross-language Sentiment Classification 3.1 Denoising Autoencoder It has been demonstrated that the denoising autoencoder could decrease the effects of transla</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word representations. In Proceedings of NAACLHLT, pages 746-751. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>79--86</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1458" citStr="Pang et al., 2002" startWordPosition="194" endWordPosition="197">sentiment word embeddings (BSWE) for English-Chinese CLSC. The proposed BSWE incorporate sentiment information of text into bilingual embeddings. Furthermore, we can learn high-quality BSWE by simply employing labeled corpora and their translations, without relying on largescale parallel corpora. Experiments on NLP&amp;CC 2013 CLSC dataset show that our approach outperforms the state-of-theart systems. 1 Introduction Sentiment classification is a task of predicting sentiment polarity of text, which has attracted considerable interest in the NLP field. To date, a number of corpus-based approaches (Pang et al., 2002; Pang and Lee, 2004; Kennedy and Inkpen, 2006) have been developed for sentiment classification. The approaches heavily rely on quality and quantity of the labeled corpora, which are considered as the most valuable resources in sentiment classification task. However, such sentiment resources are imbalanced in different languages. To leverage resources in the source language to improve the sentiment classification performance in the target language, cross-language sentiment classification (CLSC) approaches have been investigated. The traditional CLSC approaches employ machine translation (MT) </context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 79-86. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>271--278</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1478" citStr="Pang and Lee, 2004" startWordPosition="198" endWordPosition="201">ddings (BSWE) for English-Chinese CLSC. The proposed BSWE incorporate sentiment information of text into bilingual embeddings. Furthermore, we can learn high-quality BSWE by simply employing labeled corpora and their translations, without relying on largescale parallel corpora. Experiments on NLP&amp;CC 2013 CLSC dataset show that our approach outperforms the state-of-theart systems. 1 Introduction Sentiment classification is a task of predicting sentiment polarity of text, which has attracted considerable interest in the NLP field. To date, a number of corpus-based approaches (Pang et al., 2002; Pang and Lee, 2004; Kennedy and Inkpen, 2006) have been developed for sentiment classification. The approaches heavily rely on quality and quantity of the labeled corpora, which are considered as the most valuable resources in sentiment classification task. However, such sentiment resources are imbalanced in different languages. To leverage resources in the source language to improve the sentiment classification performance in the target language, cross-language sentiment classification (CLSC) approaches have been investigated. The traditional CLSC approaches employ machine translation (MT) systems to translate</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lillian Lee. 2004. A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, pages 271-278. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Cliff Chiung-Yu Lin</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing natural scenes and natural language with recursive neural networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the International Conference on Machine Learning,</booktitle>
<pages>129--136</pages>
<location>Bellevue.</location>
<contexts>
<context position="10935" citStr="Socher et al., 2011" startWordPosition="1612" endWordPosition="1615">c information as well as sentiment information. Wang et al. (2014) introduced sentiment labels into Neural Network Language Models (Bengio et al., 2003) to enhance sentiment expression ability of word vectors. Tang et al. (2014) theoretically and empirically analyzed the effects of the syntactic context and sentiment information in word vectors, and showed that the syntactic context and sentiment information were equally important to sentiment classification. Recent years have seen a surge of interest in word embeddings with deep learning technique (Bespalov et al., 2011; Glorot et al., 2011; Socher et al., 2011; Socher et al., 2012), which have been empirically shown to preserve linguistic regularities (Mikolov et al., 2013). Our work focuses on learning bilingual sentiment word embeddings (BSWE) with deep learning technique. Unlike the work of Chandar A P et al. (2014) that adopted parallel corpora to learn bilingual embeddings, we only use training data and their translations to learn BSWE. More importantly, sentiment information is integrated into bilingual embeddings to improve their performance in CLSC. 3 Bilingual Sentiment Word Embeddings (BSWE) for Cross-language Sentiment Classification 3.1</context>
</contexts>
<marker>Socher, Lin, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Cliff Chiung-Yu Lin, Andrew Y. Ng, and Christopher D. Manning. 2011. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the International Conference on Machine Learning, pages 129-136. Bellevue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic Compositionality through Recursive Matrix-Vector Spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1201--1211</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="10957" citStr="Socher et al., 2012" startWordPosition="1616" endWordPosition="1619"> as sentiment information. Wang et al. (2014) introduced sentiment labels into Neural Network Language Models (Bengio et al., 2003) to enhance sentiment expression ability of word vectors. Tang et al. (2014) theoretically and empirically analyzed the effects of the syntactic context and sentiment information in word vectors, and showed that the syntactic context and sentiment information were equally important to sentiment classification. Recent years have seen a surge of interest in word embeddings with deep learning technique (Bespalov et al., 2011; Glorot et al., 2011; Socher et al., 2011; Socher et al., 2012), which have been empirically shown to preserve linguistic regularities (Mikolov et al., 2013). Our work focuses on learning bilingual sentiment word embeddings (BSWE) with deep learning technique. Unlike the work of Chandar A P et al. (2014) that adopted parallel corpora to learn bilingual embeddings, we only use training data and their translations to learn BSWE. More importantly, sentiment information is integrated into bilingual embeddings to improve their performance in CLSC. 3 Bilingual Sentiment Word Embeddings (BSWE) for Cross-language Sentiment Classification 3.1 Denoising Autoencoder</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic Compositionality through Recursive Matrix-Vector Spaces. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1201-1211. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duyu Tang</author>
<author>Furu Wei</author>
<author>Nan Yang</author>
<author>Ming Zhou</author>
<author>Ting Liu</author>
<author>Bing Qin</author>
</authors>
<title>Learning SentimentSpecific Word Embedding for Twitter Sentiment Classification.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistic,</booktitle>
<pages>1555--1565</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="10544" citStr="Tang et al. (2014)" startWordPosition="1550" endWordPosition="1553">actic and semantic similarities across languages, but ignore sentiment information. To date, many embedding learning algorithms have been developed for sentiment classification problem by incorporating sentiment information into word embeddings. Maas et al. (2011) presented a probabilistic model that combined unsupervised and supervised techniques to learn word vectors, capturing semantic information as well as sentiment information. Wang et al. (2014) introduced sentiment labels into Neural Network Language Models (Bengio et al., 2003) to enhance sentiment expression ability of word vectors. Tang et al. (2014) theoretically and empirically analyzed the effects of the syntactic context and sentiment information in word vectors, and showed that the syntactic context and sentiment information were equally important to sentiment classification. Recent years have seen a surge of interest in word embeddings with deep learning technique (Bespalov et al., 2011; Glorot et al., 2011; Socher et al., 2011; Socher et al., 2012), which have been empirically shown to preserve linguistic regularities (Mikolov et al., 2013). Our work focuses on learning bilingual sentiment word embeddings (BSWE) with deep learning </context>
</contexts>
<marker>Tang, Wei, Yang, Zhou, Liu, Qin, 2014</marker>
<rawString>Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu, and Bing Qin. 2014. Learning SentimentSpecific Word Embedding for Twitter Sentiment Classification. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistic, pages 1555-1565. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuewei Tang</author>
<author>Xiaojun Wan</author>
</authors>
<title>Learning Bilingual Embedding Model for Cross-language Sentiment Classification.</title>
<date>2014</date>
<booktitle>In Proceedings of 2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT),</booktitle>
<pages>134--141</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="4344" citStr="Tang and Wan (2014)" startWordPosition="621" endWordPosition="624">s readily available. Meanwhile, though semantic similarities across languages are captured during bilingual embedding learning process, sentiment information of 430 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 430–440, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics text is ignored. That is, bilingual embeddings learned from unlabeled parallel corpora are not effective enough for CLSC because of a lack of explicit sentiment information. Tang and Wan (2014) first proposed a bilingual sentiment embedding model using the original training data and the corresponding translations through a linear mapping rather than deep learning technique. This paper proposes a denoising autoencoder based approach to learning bilingual sentiment word embeddings (BSWE) for CLSC, which incorporates sentiment polarities of text into the bilingual embeddings. The proposed approach learns BSWE with the original labeled documents and their translations instead of parallel corpora. The BSWE learning process consists of two phases: the unsupervised phase of semantic learni</context>
</contexts>
<marker>Tang, Wan, 2014</marker>
<rawString>Xuewei Tang and Xiaojun Wan. 2014. Learning Bilingual Embedding Model for Cross-language Sentiment Classification. In Proceedings of 2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT), pages 134-141. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Vincent</author>
<author>Hugo Larochelle</author>
<author>Yoshua Bengio</author>
<author>Pierre-Antoine Manzagol</author>
</authors>
<title>Extracting and composing robust features with denoising autoencoders.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning,</booktitle>
<pages>1096--1103</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="8185" citStr="Vincent et al., 2008" startWordPosition="1207" endWordPosition="1210">approach for CLSC. The approach first translated Chinese test data into English, and English training data into Chinese. Then, they performed training and testing in two independent views: English view and Chinese view. Gui et al. (2013) combined self-training approach with co-training approach by estimating the confidence of each monolingual system. Li et al. (2013) selected the samples in the source language that were similar to those in the target language to decrease the gap between two languages. Zhou et al. (2014a) proposed a combination CLSC model, which adopted denoising autoencoders (Vincent et al., 2008) to enhance the robustness to translation errors of the input. Most recently, a number of studies adopt deep learning technique to learn bilingual representations with parallel corpora. Bilingual representations have been successfully applied in many NLP tasks, such as machine translation (Zou et al., 2013), sentiment classification (Chandar A P et al., 2013; Zhou et al., 2014b), text classification (Chandar A P et al., 2014), etc. 431 Chandar A P et al. (2013) learned bilingual representations with aligned sentences throughout two phases: the language-specific representation learning phase an</context>
</contexts>
<marker>Vincent, Larochelle, Bengio, Manzagol, 2008</marker>
<rawString>Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. 2008. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, pages 1096-1103. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
</authors>
<title>Co-training for cross-lingual sentiment classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>235--243</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2751" citStr="Wan, 2009" startWordPosition="386" endWordPosition="387">ain the sentiment classifiers in the target language (Banea et al., 2008). Directly employing the translated resources for sentiment classification in the target language is simple and could get acceptable results. However, the gap between the source language and target language inevitably impacts the performance of sentiment classification. To improve the classification accuracy, multiview approaches have been proposed. In these approaches, the resources in the source language and their translations in the target language are both used to train sentiment classifiers in two independent views (Wan, 2009; Gui et al., 2013; Zhou et al., 2014a). The final results are determined by ensemble classifiers in these two views to overcome the weakness of monolingual classifiers. However, learning language-specific classifiers in each view fails to capture the common sentiment information of two languages during training process. With the revival of interest in deep learning (Hinton and Salakhutdinov, 2006), shared deep representations (or embeddings) (Bengio et al., 2013) are employed for CLSC (Chandar A P et al., 2013). Usually, paired sentences from parallel corpora are used to learn word embeddings</context>
<context position="7540" citStr="Wan (2009)" startWordPosition="1106" endWordPosition="1107">ross-language sentiment classification and embedding learning for sentiment classification. 2.1 Cross-language Sentiment Classification (CLSC) The critical problem of CLSC is how to bridge the gap between the source language and target language. Machine translations or parallel corpora are usually employed to solve this problem. We present a brief review of CLSC from two aspects: machine translation based approaches and parallel corpora based approaches. Machine translation based approaches use MT systems to project training data into the target language or test data into the source language. Wan (2009) proposed a co-training approach for CLSC. The approach first translated Chinese test data into English, and English training data into Chinese. Then, they performed training and testing in two independent views: English view and Chinese view. Gui et al. (2013) combined self-training approach with co-training approach by estimating the confidence of each monolingual system. Li et al. (2013) selected the samples in the source language that were similar to those in the target language to decrease the gap between two languages. Zhou et al. (2014a) proposed a combination CLSC model, which adopted </context>
</contexts>
<marker>Wan, 2009</marker>
<rawString>Xiaojun Wan. 2009. Co-training for cross-lingual sentiment classification. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 235-243. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Wang</author>
<author>Zhaohui Li</author>
<author>Jie Liu</author>
<author>Zhicheng He</author>
<author>Yalou Huang</author>
<author>Dong Li</author>
</authors>
<title>Word Vector Modeling for Sentiment Analysis of Product Reviews.</title>
<date>2014</date>
<booktitle>In Proceedings of Natural Language Processing and Chinese Computing,</booktitle>
<pages>168--180</pages>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="10382" citStr="Wang et al. (2014)" startWordPosition="1526" endWordPosition="1529">tween the source language and target language. 2.2 Embedding Learning for Sentiment Classification Bilingual embedding learning algorithms focus on capturing syntactic and semantic similarities across languages, but ignore sentiment information. To date, many embedding learning algorithms have been developed for sentiment classification problem by incorporating sentiment information into word embeddings. Maas et al. (2011) presented a probabilistic model that combined unsupervised and supervised techniques to learn word vectors, capturing semantic information as well as sentiment information. Wang et al. (2014) introduced sentiment labels into Neural Network Language Models (Bengio et al., 2003) to enhance sentiment expression ability of word vectors. Tang et al. (2014) theoretically and empirically analyzed the effects of the syntactic context and sentiment information in word vectors, and showed that the syntactic context and sentiment information were equally important to sentiment classification. Recent years have seen a surge of interest in word embeddings with deep learning technique (Bespalov et al., 2011; Glorot et al., 2011; Socher et al., 2011; Socher et al., 2012), which have been empiric</context>
</contexts>
<marker>Wang, Li, Liu, He, Huang, Li, 2014</marker>
<rawString>Yuan Wang, Zhaohui Li, Jie Liu, Zhicheng He, Yalou Huang, and Dong Li. 2014. Word Vector Modeling for Sentiment Analysis of Product Reviews. In Proceedings of Natural Language Processing and Chinese Computing, pages 168-180. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huaping Zhang</author>
<author>Hongkui Yu</author>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
</authors>
<title>HHMM-based Chinese Lexical Analyzer ICTCLAS.</title>
<date>2003</date>
<booktitle>In 2nd SIGHAN workshop affiliated with 41th ACL,</booktitle>
<pages>184--187</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="20999" citStr="Zhang et al., 2003" startWordPosition="3297" endWordPosition="3300">l Settings Data Set. The proposed approach is evaluated on NLP&amp;CC 2013 CLSC dataset2 3. The dataset con2http://tcci.ccf.org.cn/conference/2013/dldoc/evsam03.zip 3http://tcci.ccf.org.cn/conference/2013/dldoc/evdata03.zip 434 sists of product reviews on three categories: book, DVD, and music. Each category contains 4,000 English labeled data as training data (the ratio of the number of positive and negative samples is 1:1) and 4,000 Chinese unlabeled data as test data. Tools. In our experiments, Google Translate4 is adopted for both English-to-Chinese and Chineseto-English translation. ICTCLAS (Zhang et al., 2003) is used as Chinese word segmentation tool. A denoising autoencoder is developed based on Theano system (Bergstra et al., 2010). BSWE are trained for 50 and 30 epochs in unsupervised phase and supervised phases respectively. SV M&apos;Zght (Joachims, 1999) is used to train linear SVM sentiment classifiers Evaluation Metric. The performance is evaluated by the classification accuracy for each category, and the average accuracy of three categories, respectively. The category accuracy is defined as: Accuracy, = #system correct, (11) #system total, where c is one of the three categories, and #system co</context>
</contexts>
<marker>Zhang, Yu, Xiong, Liu, 2003</marker>
<rawString>Huaping Zhang, Hongkui Yu, Deyi Xiong, and Qun Liu. 2003. HHMM-based Chinese Lexical Analyzer ICTCLAS. In 2nd SIGHAN workshop affiliated with 41th ACL, pages 184-187. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guangyou Zhou</author>
<author>Tingting He</author>
<author>Jun Zhao</author>
</authors>
<title>Bridging the Language Gap: Learning Distributed Semantics for Cross-Lingual Sentiment Classification.</title>
<date>2014</date>
<booktitle>In Proceedings of Natural Language Processing and Chinese Computing,</booktitle>
<pages>138--149</pages>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="2788" citStr="Zhou et al., 2014" startWordPosition="392" endWordPosition="395"> in the target language (Banea et al., 2008). Directly employing the translated resources for sentiment classification in the target language is simple and could get acceptable results. However, the gap between the source language and target language inevitably impacts the performance of sentiment classification. To improve the classification accuracy, multiview approaches have been proposed. In these approaches, the resources in the source language and their translations in the target language are both used to train sentiment classifiers in two independent views (Wan, 2009; Gui et al., 2013; Zhou et al., 2014a). The final results are determined by ensemble classifiers in these two views to overcome the weakness of monolingual classifiers. However, learning language-specific classifiers in each view fails to capture the common sentiment information of two languages during training process. With the revival of interest in deep learning (Hinton and Salakhutdinov, 2006), shared deep representations (or embeddings) (Bengio et al., 2013) are employed for CLSC (Chandar A P et al., 2013). Usually, paired sentences from parallel corpora are used to learn word embeddings across languages (Chandar A P et al.</context>
<context position="8088" citStr="Zhou et al. (2014" startWordPosition="1192" endWordPosition="1195">the target language or test data into the source language. Wan (2009) proposed a co-training approach for CLSC. The approach first translated Chinese test data into English, and English training data into Chinese. Then, they performed training and testing in two independent views: English view and Chinese view. Gui et al. (2013) combined self-training approach with co-training approach by estimating the confidence of each monolingual system. Li et al. (2013) selected the samples in the source language that were similar to those in the target language to decrease the gap between two languages. Zhou et al. (2014a) proposed a combination CLSC model, which adopted denoising autoencoders (Vincent et al., 2008) to enhance the robustness to translation errors of the input. Most recently, a number of studies adopt deep learning technique to learn bilingual representations with parallel corpora. Bilingual representations have been successfully applied in many NLP tasks, such as machine translation (Zou et al., 2013), sentiment classification (Chandar A P et al., 2013; Zhou et al., 2014b), text classification (Chandar A P et al., 2014), etc. 431 Chandar A P et al. (2013) learned bilingual representations wit</context>
<context position="9368" citStr="Zhou et al. (2014" startWordPosition="1384" endWordPosition="1387">c representation learning phase and the shared representation learning phase. In the language-specific representation learning phase, they applied autoencoders to obtain a language-specific representation for each entity in two languages respectively. In shared representation learning phase, pairs of parallel language-specific representations were passed to an autoencoder to learn bilingual representations. To joint language-specific representations and bilingual representations, Chandar A P et al. (2014) integrated the two learning phases into a unified process to learn bilingual embeddings. Zhou et al. (2014b) employed bilingual representations for English-Chinese CLSC. The work mentioned above employed aligned sentences in bilingual embedding learning process. However, in the sentiment classification process, only representations in the source language are used for training, and representations in the target language are used for testing, which ignores the interactions of semantic information between the source language and target language. 2.2 Embedding Learning for Sentiment Classification Bilingual embedding learning algorithms focus on capturing syntactic and semantic similarities across lan</context>
<context position="11708" citStr="Zhou et al., 2014" startWordPosition="1729" endWordPosition="1732"> sentiment word embeddings (BSWE) with deep learning technique. Unlike the work of Chandar A P et al. (2014) that adopted parallel corpora to learn bilingual embeddings, we only use training data and their translations to learn BSWE. More importantly, sentiment information is integrated into bilingual embeddings to improve their performance in CLSC. 3 Bilingual Sentiment Word Embeddings (BSWE) for Cross-language Sentiment Classification 3.1 Denoising Autoencoder It has been demonstrated that the denoising autoencoder could decrease the effects of translation errors on the performance of CLSC (Zhou et al., 2014a). This paper proposes a deep learning based approach, which employs the denoising autoencoder to learn the bilingual embeddings for CLSC. A denoising autoencoder is the modification of an autoencoder. The autoencoder (Bengio et al., 2007) includes an encoder fθ and a decoder gθe. The encoder maps a d-dimensional input vector x E [0, 1]d to a hidden representation y E [0,1]d&apos; through a deterministic mapping y = fθ(x) = a(Wx + b), parameterized by 0 = {W, b}. W is a weight matrix, b is a bias term, and a(x) is the activation function. The decoder maps y back to a reconstructed vector xˆ = gθl(</context>
<context position="29890" citStr="Zhou et al. (2014" startWordPosition="4719" endWordPosition="4722">itable for eliminating the language gap. Chen et al. (2014) translated Chinese test data into English and then gave different weights to sentiment words according to the subjectpredicate component of sentiment words. They got 77.09% accuracy and took the 2nd place in NLP&amp;CC 2013 CLSC share task. The machine translation based approach was limited by the translation errors. System book DVD music Average Chen et al. 77.00% 78.33% 75.95% 77.09% (2014) 78.70% 79.65% 78.30% 78.89% Gui et al. 80.10% 81.60% 78.60% 80.10% (2013) 80.63% 80.95% 78.48% 80.02% Gui et al. 81.05% 81.60% 79.40% 80.68% (2014) Zhou et al. (2014a) Our approach Table 2: Performance comparisons on the NLP&amp;CC 2013 CLSC dataset. Gui et al. (2013; 2014) and Zhou et al. (2014a) adopted the multi-view approach to bridge the language gap. Gui et al. (2013) proposed a mixed CLSC model by combining co-training and transfer learning strategies. They achieved the highest accuracy of 78.89% in NLP&amp;CC CLSC share task. Gui et al. (2014) further improved the accuracy to 80.10% by removing noise from the transferred samples to avoid negative transfers. Zhou et al. (2014a) built denoising autoencoders in two independent views to enhance the robustness</context>
</contexts>
<marker>Zhou, He, Zhao, 2014</marker>
<rawString>Guangyou Zhou, Tingting He, and Jun Zhao. 2014b. Bridging the Language Gap: Learning Distributed Semantics for Cross-Lingual Sentiment Classification. In Proceedings of Natural Language Processing and Chinese Computing, pages 138-149. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huiwei Zhou</author>
<author>Long Chen</author>
<author>Degen Huang</author>
</authors>
<title>Cross-lingual sentiment classification based on denoising autoencoder.</title>
<date>2014</date>
<booktitle>In Proceedings of Natural Language Processing and Chinese Computing,</booktitle>
<pages>181--192</pages>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="2788" citStr="Zhou et al., 2014" startWordPosition="392" endWordPosition="395"> in the target language (Banea et al., 2008). Directly employing the translated resources for sentiment classification in the target language is simple and could get acceptable results. However, the gap between the source language and target language inevitably impacts the performance of sentiment classification. To improve the classification accuracy, multiview approaches have been proposed. In these approaches, the resources in the source language and their translations in the target language are both used to train sentiment classifiers in two independent views (Wan, 2009; Gui et al., 2013; Zhou et al., 2014a). The final results are determined by ensemble classifiers in these two views to overcome the weakness of monolingual classifiers. However, learning language-specific classifiers in each view fails to capture the common sentiment information of two languages during training process. With the revival of interest in deep learning (Hinton and Salakhutdinov, 2006), shared deep representations (or embeddings) (Bengio et al., 2013) are employed for CLSC (Chandar A P et al., 2013). Usually, paired sentences from parallel corpora are used to learn word embeddings across languages (Chandar A P et al.</context>
<context position="8088" citStr="Zhou et al. (2014" startWordPosition="1192" endWordPosition="1195">the target language or test data into the source language. Wan (2009) proposed a co-training approach for CLSC. The approach first translated Chinese test data into English, and English training data into Chinese. Then, they performed training and testing in two independent views: English view and Chinese view. Gui et al. (2013) combined self-training approach with co-training approach by estimating the confidence of each monolingual system. Li et al. (2013) selected the samples in the source language that were similar to those in the target language to decrease the gap between two languages. Zhou et al. (2014a) proposed a combination CLSC model, which adopted denoising autoencoders (Vincent et al., 2008) to enhance the robustness to translation errors of the input. Most recently, a number of studies adopt deep learning technique to learn bilingual representations with parallel corpora. Bilingual representations have been successfully applied in many NLP tasks, such as machine translation (Zou et al., 2013), sentiment classification (Chandar A P et al., 2013; Zhou et al., 2014b), text classification (Chandar A P et al., 2014), etc. 431 Chandar A P et al. (2013) learned bilingual representations wit</context>
<context position="9368" citStr="Zhou et al. (2014" startWordPosition="1384" endWordPosition="1387">c representation learning phase and the shared representation learning phase. In the language-specific representation learning phase, they applied autoencoders to obtain a language-specific representation for each entity in two languages respectively. In shared representation learning phase, pairs of parallel language-specific representations were passed to an autoencoder to learn bilingual representations. To joint language-specific representations and bilingual representations, Chandar A P et al. (2014) integrated the two learning phases into a unified process to learn bilingual embeddings. Zhou et al. (2014b) employed bilingual representations for English-Chinese CLSC. The work mentioned above employed aligned sentences in bilingual embedding learning process. However, in the sentiment classification process, only representations in the source language are used for training, and representations in the target language are used for testing, which ignores the interactions of semantic information between the source language and target language. 2.2 Embedding Learning for Sentiment Classification Bilingual embedding learning algorithms focus on capturing syntactic and semantic similarities across lan</context>
<context position="11708" citStr="Zhou et al., 2014" startWordPosition="1729" endWordPosition="1732"> sentiment word embeddings (BSWE) with deep learning technique. Unlike the work of Chandar A P et al. (2014) that adopted parallel corpora to learn bilingual embeddings, we only use training data and their translations to learn BSWE. More importantly, sentiment information is integrated into bilingual embeddings to improve their performance in CLSC. 3 Bilingual Sentiment Word Embeddings (BSWE) for Cross-language Sentiment Classification 3.1 Denoising Autoencoder It has been demonstrated that the denoising autoencoder could decrease the effects of translation errors on the performance of CLSC (Zhou et al., 2014a). This paper proposes a deep learning based approach, which employs the denoising autoencoder to learn the bilingual embeddings for CLSC. A denoising autoencoder is the modification of an autoencoder. The autoencoder (Bengio et al., 2007) includes an encoder fθ and a decoder gθe. The encoder maps a d-dimensional input vector x E [0, 1]d to a hidden representation y E [0,1]d&apos; through a deterministic mapping y = fθ(x) = a(Wx + b), parameterized by 0 = {W, b}. W is a weight matrix, b is a bias term, and a(x) is the activation function. The decoder maps y back to a reconstructed vector xˆ = gθl(</context>
<context position="29890" citStr="Zhou et al. (2014" startWordPosition="4719" endWordPosition="4722">itable for eliminating the language gap. Chen et al. (2014) translated Chinese test data into English and then gave different weights to sentiment words according to the subjectpredicate component of sentiment words. They got 77.09% accuracy and took the 2nd place in NLP&amp;CC 2013 CLSC share task. The machine translation based approach was limited by the translation errors. System book DVD music Average Chen et al. 77.00% 78.33% 75.95% 77.09% (2014) 78.70% 79.65% 78.30% 78.89% Gui et al. 80.10% 81.60% 78.60% 80.10% (2013) 80.63% 80.95% 78.48% 80.02% Gui et al. 81.05% 81.60% 79.40% 80.68% (2014) Zhou et al. (2014a) Our approach Table 2: Performance comparisons on the NLP&amp;CC 2013 CLSC dataset. Gui et al. (2013; 2014) and Zhou et al. (2014a) adopted the multi-view approach to bridge the language gap. Gui et al. (2013) proposed a mixed CLSC model by combining co-training and transfer learning strategies. They achieved the highest accuracy of 78.89% in NLP&amp;CC CLSC share task. Gui et al. (2014) further improved the accuracy to 80.10% by removing noise from the transferred samples to avoid negative transfers. Zhou et al. (2014a) built denoising autoencoders in two independent views to enhance the robustness</context>
</contexts>
<marker>Zhou, Chen, Huang, 2014</marker>
<rawString>Huiwei Zhou, Long Chen, and Degen Huang. 2014a. Cross-lingual sentiment classification based on denoising autoencoder. In Proceedings of Natural Language Processing and Chinese Computing, pages 181-192. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Will Y Zou</author>
<author>Richard Socher</author>
<author>Daniel Cer</author>
<author>Christopher D Manning</author>
</authors>
<title>Bilingual Word Embedding for Phrase-Based Machine Translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1393--1398</pages>
<contexts>
<context position="8493" citStr="Zou et al., 2013" startWordPosition="1254" endWordPosition="1257">e confidence of each monolingual system. Li et al. (2013) selected the samples in the source language that were similar to those in the target language to decrease the gap between two languages. Zhou et al. (2014a) proposed a combination CLSC model, which adopted denoising autoencoders (Vincent et al., 2008) to enhance the robustness to translation errors of the input. Most recently, a number of studies adopt deep learning technique to learn bilingual representations with parallel corpora. Bilingual representations have been successfully applied in many NLP tasks, such as machine translation (Zou et al., 2013), sentiment classification (Chandar A P et al., 2013; Zhou et al., 2014b), text classification (Chandar A P et al., 2014), etc. 431 Chandar A P et al. (2013) learned bilingual representations with aligned sentences throughout two phases: the language-specific representation learning phase and the shared representation learning phase. In the language-specific representation learning phase, they applied autoencoders to obtain a language-specific representation for each entity in two languages respectively. In shared representation learning phase, pairs of parallel language-specific representatio</context>
</contexts>
<marker>Zou, Socher, Cer, Manning, 2013</marker>
<rawString>Will Y. Zou, Richard Socher, Daniel Cer, and Christopher D. Manning. 2013. Bilingual Word Embedding for Phrase-Based Machine Translation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1393-1398.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>