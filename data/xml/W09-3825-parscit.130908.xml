<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000838">
<title confidence="0.9982105">
Transition-Based Parsing of the Chinese Treebank using a Global
Discriminative Model
</title>
<author confidence="0.998649">
Yue Zhang
</author>
<affiliation confidence="0.9655155">
Oxford University
Computing Laboratory
</affiliation>
<email confidence="0.984236">
yue.zhang@comlab.ox.ac.uk
</email>
<author confidence="0.990256">
Stephen Clark
</author>
<affiliation confidence="0.982337">
Cambridge University
Computer Laboratory
</affiliation>
<email confidence="0.993066">
stephen.clark@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.997333" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999914866666667">
Transition-based approaches have shown
competitive performance on constituent
and dependency parsing of Chinese. State-
of-the-art accuracies have been achieved
by a deterministic shift-reduce parsing
model on parsing the Chinese Treebank 2
data (Wang et al., 2006). In this paper,
we propose a global discriminative model
based on the shift-reduce parsing process,
combined with a beam-search decoder, ob-
taining competitive accuracies on CTB2.
We also report the performance of the
parser on CTB5 data, obtaining the highest
scores in the literature for a dependency-
based evaluation.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999911365384616">
Transition-based statistical parsing associates
scores with each decision in the parsing process,
selecting the parse which is built by the highest
scoring sequence of decisions (Briscoe and Car-
roll, 1993; Nivre et al., 2006). The parsing algo-
rithm is typically some form of bottom-up shift-
reduce algorithm, so that scores are associated
with actions such as shift and reduce. One ad-
vantage of this approach is that the parsing can be
highly efficient, for example by pursuing a greedy
strategy in which a single action is chosen at each
decision point.
The alternative approach, exemplified by
Collins (1997) and Charniak (2000), is to use
a chart-based algorithm to build the space of
possible parses, together with pruning of low-
probability constituents and the Viterbi algorithm
to find the highest scoring parse. For English de-
pendency parsing, the two approaches give similar
results (McDonald et al., 2005; Nivre et al., 2006).
For English constituent-based parsing using the
Penn Treebank, the best performing transition-
based parser lags behind the current state-of-the-
art (Sagae and Lavie, 2005). In contrast, for Chi-
nese, the best dependency parsers are currently
transition-based (Duan et al., 2007; Zhang and
Clark, 2008). For constituent-based parsing using
the Chinese Treebank (CTB), Wang et al. (2006)
have shown that a shift-reduce parser can give
competitive accuracy scores together with high
speeds, by using an SVM to make a single decision
at each point in the parsing process.
In this paper we describe a global discrimina-
tive model for Chinese shift-reduce parsing, and
compare it with Wang et al.’s approach. We ap-
ply the same shift-reduce procedure as Wang et
al. (2006), but instead of using a local classifier
for each transition-based action, we train a gener-
alized perceptron model over complete sequences
of actions, so that the parameters are learned in
the context of complete parses. We apply beam
search to decoding instead of greedy search. The
parser still operates in linear time, but the use of
beam-search allows the correction of local deci-
sion errors by global comparison. Using CTB2,
our model achieved Parseval F-scores comparable
to Wang et al.’s approach. We also present accu-
racy scores for the much larger CTB5, using both
a constituent-based and dependency-based evalu-
ation. The scores for the dependency-based eval-
uation were higher than the state-of-the-art depen-
dency parsers for the CTB5 data.
</bodyText>
<sectionHeader confidence="0.98538" genericHeader="method">
2 The Shift-Reduce Parsing Process
</sectionHeader>
<bodyText confidence="0.996451666666667">
The shift-reduce process used by our beam-search
decoder is based on the greedy shift-reduce parsers
of Sagae and Lavie (2005) and Wang et al. (2006).
</bodyText>
<page confidence="0.962418">
162
</page>
<note confidence="0.8786325">
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 162–171,
Paris, October 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999851076923077">
The process assumes binary-branching trees; sec-
tion 2.1 explains how these are obtained from the
arbitrary-branching trees in the Chinese Treebank.
The input is assumed to be segmented and POS
tagged, and the word-POS pairs waiting to be pro-
cessed are stored in a queue. A stack holds the
partial parse trees that are built during the parsing
process. A parse state is defined as a (stack,queue)
pair. Parser actions, including SHIFT and various
kinds of REDUCE, define functions from states to
states by shifting word-POS pairs onto the stack
and building partial parse trees.
The actions used by the parser are:
</bodyText>
<listItem confidence="0.997138483870968">
• SHIFT, which pushes the next word-POS pair
in the queue onto the stack;
• REDUCE–unary–X, which makes a new
unary-branching node with label X; the stack
is popped and the popped node becomes the
child of the new node; the new node is pushed
onto the stack;
• REDUCE–binary–{L/R}–X, which makes a
new binary-branching node with label X; the
stack is popped twice, with the first popped
node becoming the right child of the new
node and the second popped node becoming
the left child; the new node is pushed onto the
stack;
• TERMINATE, which pops the root node off
the stack and ends parsing. This action
is novel in our parser. Sagae and Lavie
(2005) and Wang et al. (2006) only used the
first three transition actions, setting the fi-
nal state as all incoming words having been
processed, and the stack containing only one
node. However, there are a small number of
sentences (14 out of 3475 from the training
data) that have unary-branching roots. For
these sentences, Wang’s parser will be unable
to produce the unary-branching roots because
the parsing process terminates as soon as the
root is found. We define a separate action to
terminate parsing, allowing unary reduces to
be applied to the root item before parsing fin-
ishes.
</listItem>
<bodyText confidence="0.941688">
The trees built by the parser are lexicalized, us-
ing the head-finding rules from Zhang and Clark
(2008). The left (L) and right (R) versions of the
REDUCE-binary rules indicate whether the head of
for node Y = X1..Xm E T :
if m &gt; 2 :
find the head node Xk(1 &lt; k &lt; m) of Y
</bodyText>
<equation confidence="0.999434">
m′ = m
while m′ &gt; k and m′ &gt; 2 :
new node Y * = X1..Xm′_1
Y ← Y *Xm′
m′ = m′ − 1
n′ = 1
while n′ &lt; k and k − n′ &gt; 1 :
new node Y * = Xn′..Xk
Y ← Xn′Y *
n′ = n′ + 1
</equation>
<figureCaption confidence="0.992805">
Figure 2: the binarization algorithm with input T
</figureCaption>
<bodyText confidence="0.999983923076923">
the new node is to be taken from the left or right
child. Note also that, since the parser is building
binary trees, the X label in the REDUCE rules can
be one of the temporary constituent labels, such
as NP*, which are needed for the binarization pro-
cess described in Section 2.1. Hence the number
of left and right binary reduce rules is the number
of constituent labels in the binarized grammar.
Wang et al. (2006) give a detailed example
showing how a segmented and POS-tagged sen-
tence can be incrementally processed using the
shift-reduce actions to produce a binary tree. We
show this example in Figure 1.
</bodyText>
<subsectionHeader confidence="0.990274">
2.1 The binarization process
</subsectionHeader>
<bodyText confidence="0.999068444444444">
The algorithm in Figure 2 is used to map CTB
trees into binarized trees, which are required by
the shift-reduce parsing process. For any tree node
with more than two child nodes, the algorithm
works by first finding the head node, and then pro-
cessing its right-hand-side and left-hand-side, re-
spectively. The head-finding rules are taken from
Zhang and Clark (2008). Y = X1..Xm represents
a tree node Y with child nodes X1...Xm(m ≥ 1).
The label of the newly generated node Y * is
based on the constituent label of the original node
Y , but marked with an asterix. Hence binariza-
tion enlarges the set of constituent labels. We
call the constituents marked with * temporary con-
stituents. The binarization process is reversible, in
that output from the shift-reduce parser can be un-
binarized into CTB format, which is required for
evaluation.
</bodyText>
<page confidence="0.998241">
163
</page>
<figureCaption confidence="0.9828425">
Figure 1: An example shift-reduce parsing process, adopted from Wang et al. (2006)
2.2 Restrictions on the sequence of actions a partial parse consisting of all the subtrees on the
stack.
Not all sequences of actions produce valid bina- In our parser a set of restrictions is applied
rized trees. In the deterministic parser of Wang et which guarantees a valid parse tree. For example,
al. (2006), the highest scoring action predicted by two simple restrictions are that a SHIFT action can
the classifier may prevent a valid binary tree from only be applied if the queue of incoming words
being built. In this case, Wang et al. simply return
</figureCaption>
<figure confidence="0.95074975">
164
Variables: state item item = (S, Q), where
S is stack and Q is incoming queue;
the agenda agenda;
list of state items next;
Algorithm:
for item E agenda:
if item.score = agenda.bestScore and
item.isFinished:
rval = item
break
next = []
for move E item.legalMoves:
next.push(item.TakeAction(move))
agenda = next.getBBest()
Outputs: rval
</figure>
<figureCaption confidence="0.999737">
Figure 3: the beam-search decoding algorithm
</figureCaption>
<bodyText confidence="0.92336125">
is non-empty, and the binary reduce actions can
only be performed if the stack contains at least two
nodes. Some of the restrictions are more complex
than this; the full set is listed in the Appendix.
</bodyText>
<sectionHeader confidence="0.65108" genericHeader="method">
3 Decoding with Beam Search
</sectionHeader>
<bodyText confidence="0.995613166666667">
Our decoder is based on the incremental shift-
reduce parsing process described in Section 2. We
apply beam-search, keeping the B highest scoring
state items in an agenda during the parsing pro-
cess. The agenda is initialized with a state item
containing the starting state, i.e. an empty stack
and a queue consisting of all word-POS pairs from
the sentence.
At each stage in the decoding process, existing
items from the agenda are progressed by applying
legal parsing actions. From all newly generated
state items, the B highest scoring are put back on
the agenda. The decoding process is terminated
when the highest scored state item in the agenda
reaches the final state. If multiple state items have
the same highest score, parsing terminates if any
of them are finished. The algorithm is shown in
Figure 3.
</bodyText>
<sectionHeader confidence="0.963769" genericHeader="method">
4 Model and Learning Algorithm
</sectionHeader>
<bodyText confidence="0.977690571428571">
We use a linear model to score state items. Recall
that a parser state is a (stack,queue) pair, with the
stack holding subtrees and the queue holding in-
coming words waiting to be processed. The score
Inputs: training examples (xi, yi)
Initialization: set w� = 0
Algorithm:
</bodyText>
<equation confidence="0.8804844">
for t = 1..T, i = 1..N:
zi = parse(xi, w)
if zi =� yi:
w� = w� + `b(yi) − `b(zi)
Outputs: w�
</equation>
<figureCaption confidence="0.8704765">
Figure 4: the perceptron learning algorithm
for state item Y is defined by:
</figureCaption>
<equation confidence="0.9308845">
Score(Y ) = w� · `b(Y ) = � Ai fi(Y )
i
</equation>
<bodyText confidence="0.999990225806452">
where `b(Y ) is the global feature vector from Y ,
and w� is the weight vector defined by the model.
Each element from `b(Y ) represents the global
count of a particular feature from Y . The feature
set consists of a large number of features which
pick out various configurations from the stack and
queue, based on the words and subtrees in the state
item. The features are described in Section 4.1.
The weight values are set using the generalized
perceptron algorithm (Collins, 2002).
The perceptron algorithm is shown in Figure 4.
It initializes weight values as all zeros, and uses
the current model to decode training examples (the
parse function in the pseudo-code). If the output
is correct, it passes on to the next example. If
the output is incorrect, it adjusts the weight val-
ues by adding the feature vector from the gold-
standard output and subtracting the feature vector
from the parser output. Weight values are updated
for each example (making the process online) and
the training data is iterated over T times. In or-
der to avoid overfitting we used the now-standard
averaged version of this algorithm (Collins, 2002).
We also apply the early update modification
from Collins and Roark (2004). If the agenda, at
any point during the decoding process, does not
contain the correct partial parse, it is not possible
for the decoder to produce the correct output. In
this case, decoding is stopped early and the weight
values are updated using the highest scoring par-
tial parse on the agenda.
</bodyText>
<subsectionHeader confidence="0.990927">
4.1 Feature set
</subsectionHeader>
<bodyText confidence="0.9714285">
Table 1 shows the set of feature templates for the
model. Individual features are generated from
</bodyText>
<page confidence="0.961091">
165
</page>
<equation confidence="0.827311666666667">
Description
Unigrams
Feature templates
S0tc, S0wc, S1tc, S1wc,
S2tc, S2wc, S3tc, S3wc,
N0wt, N1wt, N2wt, N3wt,
S0lwc, S0rwc, S0uwc,
S1lwc, S1rwc, S1uwc,
S0wS1w, S0wS1c, S0cS1w, S0cS1c,
S0wN0w, S0wN0t, S0cN0w, S0cN0t,
N0wN1w, N0wN1t, N0tN1w, N0tN1t
S1wN0w, S1wN0t, S1cN0w, S1cN0t,
</equation>
<bodyText confidence="0.9993863">
““” and “V” and right brackets including “)”,
“”” and “》”. In the table, b represents the
matching status of the last left bracket (if any)
on the stack. It takes three different values:
1 (no matching right bracket has been pushed
onto stack), 2 (a matching right bracket has been
pushed onto stack) and 3 (a matching right bracket
has been pushed onto stack, but then popped off).
The “Separator” row shows features that in-
clude one of the separator punctuations (i.e. “,”,
</bodyText>
<figure confidence="0.908208">
Bigrams
Trigrams
Bracket
Separator
</figure>
<construct confidence="0.8941411">
S0cS1cS2c, S0wS1cS2c,
S0cS1wS2c, S0cS1cS2w,
S0cS1cN0t, S0wS1cN0t,
S0cS1wN0t, S0cS1cN0w
S0wb, S0cb
S0wS1cb, S0cS1wb, S0cS1cb
S0wN0tb, S0cN0wb, S0cN0tb
S0wp, S0wcp, S0wq, S0wcq,
S1wp, S1wcp, S1wq, S1wcq
S0cS1cp, S0cS1cq
</construct>
<tableCaption confidence="0.992445">
Table 1: Feature templates
</tableCaption>
<bodyText confidence="0.999967481481482">
these templates by first instantiating a template
with particular labels, words and tags, and then
pairing the instantiated template with a particu-
lar action. In the table, the symbols S0, S1, S2,
and S3 represent the top four nodes on the stack,
and the symbols N0, N1, N2 and N3 represent the
first four words in the incoming queue. S0L, S0R
and S0U represent the left and right child for bi-
nary branching S0, and the single child for unary
branching S0, respectively; w represents the lex-
ical head token for a node; c represents the label
for a node. When the corresponding node is a ter-
minal, c represents its POS-tag, whereas when the
corresponding node is non-terminal, c represents
its constituent label; t represents the POS-tag for a
word.
The context S0, S1, S2, S3 and N0, N1, N2, N3
for the feature templates is taken from Wang et al.
(2006). However, Wang et al. (2006) used a poly-
nomial kernel function with an SVM and did not
manually create feature combinations. Since we
used the linear perceptron algorithm we manually
combined Unigram features into Bigram and Tri-
gram features.
The “Bracket” row shows bracket-related fea-
tures, which were inspired by Wang et al. (2006).
Here brackets refer to left brackets including “(”,
“、” and “;”) between the head words of
S0 and S1. These templates apply only when
the stack contains at least two nodes; p repre-
sents a separator punctuation symbol. Each unique
separator punctuation between S0 and S1 is only
counted once when generating the global feature
vector. q represents the count of any separator
punctuation between S0 and S1.
Whenever an action is being considered at each
point in the beam-search process, templates from
Table 1 are matched with the context defined by
the parser state and combined with the action to
generate features. Negative features, which are the
features from incorrect parser outputs but not from
any training example, are included in the model.
There are around a million features in our experi-
ments with the CTB2 dataset.
Wang et al. (2006) used a range of other fea-
tures, including rhythmic features of S0 and S1
(Sun and Jurafsky, 2003), features from the most
recently found node that is to the left or right of S0
and S1, the number of words and the number of
punctuations in S0 and S1, the distance between
S0 and S1 and so on. We did not include these
features in our parser, because they did not lead to
improved performance during development exper-
iments.
</bodyText>
<sectionHeader confidence="0.999755" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999703272727273">
The experiments were performed using the Chi-
nese Treebank 2 and Chinese Treebank 5 data.
Standard data preparation was performed before
the experiments: empty terminal nodes were re-
moved; any non-terminal nodes with no children
were removed; any unary X → X nodes resulting
from the previous steps were collapsed into one X
node.
For all experiments, we used the EVALB tool1
for evaluation, and used labeled recall (LR), la-
beled precision (LP) and F1 score (which is the
</bodyText>
<footnote confidence="0.734939">
1http://nlp.cs.nyu.edu/evalb/
</footnote>
<figure confidence="0.470828">
“ ”
o
</figure>
<page confidence="0.867893">
166
</page>
<figureCaption confidence="0.999149">
Figure 5: The influence of beam-size
</figureCaption>
<table confidence="0.99885425">
Sections Sentences Words
Training 001–270 3475 85,058
Development 301–325 355 6,821
Test 271–300 348 8,008
</table>
<tableCaption confidence="0.999892">
Table 2: The standard split of CTB2 data
</tableCaption>
<bodyText confidence="0.7005185">
harmonic mean of LR and LP) to measure pars-
ing accuracy.
</bodyText>
<subsectionHeader confidence="0.964434">
5.1 The influence of beam-size
</subsectionHeader>
<bodyText confidence="0.9999313">
Figure 5 shows the accuracy curves using differ-
ent beam-sizes for the decoder. The number of
training iterations is on the x-axis with F-score
on the y-axis. The tests were performed using
the development test data and gold-standard POS-
tags. The figure shows the benefit of using a beam
size greater than 1, with comparatively little accu-
racy gain being obtained beyond a beam size of 8.
Hence we set the beam size to 16 for the rest of the
experiments.
</bodyText>
<subsectionHeader confidence="0.999783">
5.2 Test results on CTB2
</subsectionHeader>
<bodyText confidence="0.999949444444444">
The experiments in this section were performed
using CTB2 to allow comparison with previous
work, with the CTB2 data extracted from Chinese
Treebank 5 (CTB5). The data was split into train-
ing, development test and test sets, as shown in Ta-
ble 2, which is consistent with Wang et al. (2006)
and earlier work. The tests were performed us-
ing both gold-standard POS-tags and POS-tags au-
tomatically assigned by a POS-tagger. We used our
</bodyText>
<table confidence="0.9993116">
Model LR LP F1
Bikel Thesis 80.9% 84.5% 82.7%
Wang 2006 SVM 87.2% 88.3% 87.8%
Wang 2006 Stacked 88.3% 88.1% 88.2%
Our parser 89.4% 90.1% 89.8%
</table>
<tableCaption confidence="0.943691">
Table 3: Accuracies on CTB2 with gold-standard
POS-tags
</tableCaption>
<bodyText confidence="0.991472341463415">
own implementation of the perceptron-based tag-
ger from Collins (2002).
The results of various models measured using
sentences with less than 40 words and using gold-
standard POS-tags are shown in Table 3. The
rows represent the model from Bikel and Chiang
(2000), Bikel (2004), the SVM and ensemble mod-
els from Wang et al. (2006), and our parser, re-
spectively. The accuracy of our parser is competi-
tive using this test set.
The results of various models using automati-
cally assigned POS-tags are shown in Table 4. The
rows in the table represent the models from Bikel
and Chiang (2000), Levy and Manning (2003),
Xiong et al. (2005), Bikel (2004), Chiang and
Bikel (2002), the SVM model from Wang et al.
(2006) and the ensemble system from Wang et
al. (2006), and the parser of this paper, respec-
tively. Our parser gave comparable accuracies to
the SVM and ensemble models from Wang et al.
(2006). However, comparison with Table 3 shows
that our parser is more sensitive to POS-tagging er-
rors than some of the other models. One possible
reason is that some of the other parsers, e.g. Bikel
(2004), use the parser model itself to resolve tag-
ging ambiguities, whereas we rely on a POS tag-
ger to accurately assign a single tag to each word.
In fact, for the Chinese data, POS tagging accu-
racy is not very high, with the perceptron-based
tagger achieving an accuracy of only 93%. The
beam-search decoding framework we use could
accommodate joint parsing and tagging, although
the use of features based on the tags of incom-
ing words complicates matters somewhat, since
these features rely on tags having been assigned to
all words in a pre-processing step. We leave this
problem for future work.
In a recent paper, Petrov and Klein (2007) re-
ported LR and LP of 85.7% and 86.9% for sen-
tences with less than 40 words and 81.9% and
84.8% for all sentences on the CTB2 test set, re-
</bodyText>
<page confidence="0.993538">
167
</page>
<table confidence="0.9997862">
LR &lt; 40 words POS LR &lt; 100 words POS LR Unlimited POS
LP F1 LP F1 LP F1
Bikel 2000 76.8% 77.8% 77.3% - 73.3% 74.6% 74.0% - - - - -
Levy 2003 79.2% 78.4% 78.8% - - - - - - - - -
Xiong 2005 78.7% 80.1% 79.4% - - - - - - - - -
Bikel Thesis 78.0% 81.2% 79.6% - 74.4% 78.5% 76.4% - - - - -
Chiang 2002 78.8% 81.1% 79.9% - 75.2% 78.0% 76.6% - - - - -
Wang 2006 SVM 78.1% 81.1% 79.6% 92.5% 75.5% 78.5% 77.0% 92.2% 75.0% 78.0% 76.5% 92.1%
Wang 2006 Stacked 79.2% 81.1% 80.1% 92.5% 76.7% 78.4% 77.5% 92.2% 76.2% 78.0% 77.1% 92.1%
Our parser 80.2% 80.5% 80.4% 93.5% 76.5% 77.7% 77.1% 93.1% 76.1% 77.4% 76.7% 93.0%
</table>
<tableCaption confidence="0.998994">
Table 4: Accuracies on CTB2 with automatically assigned tags
</tableCaption>
<table confidence="0.99990275">
LR &lt; 40 words POS LR Unlimited POS
LP F1 LP F1
87.9% 87.5% 87.7% 100% 86.9% 86.7% 86.8% 100%
80.2% 79.1% 79.6% 94.1% 78.6% 78.0% 78.3% 93.9%
</table>
<tableCaption confidence="0.998416">
Table 5: Accuracies on CTB5 using gold-standard and automatically assigned POS-tags
</tableCaption>
<table confidence="0.99838">
Sections Sentences Words
Set A 001–270 3,484 84,873
Set B Set A; 400–699 6,567 161,893
Set C Set B; 700–931 9,707 236,051
</table>
<tableCaption confidence="0.999605">
Table 6: Training sets with different sizes
</tableCaption>
<bodyText confidence="0.999962142857143">
spectively. These results are significantly better
than any model from Table 4. However, we did
not include their scores in the table because they
used a different training set from CTB5, which is
much larger than the CTB2 training set used by all
parsers in the table. In order to make a compari-
son, we split the data in the same way as Petrov
and Klein (2007) and tested our parser using auto-
matically assigned POS-tags. It gave LR and LP
of 82.0% and 80.9% for sentences with less than
40 words and 77.8% and 77.4% for all sentences,
significantly lower than Petrov and Klein (2007),
which we partly attribute to the sensitivity of our
parser to pos tag errors (see Table 5).
</bodyText>
<subsectionHeader confidence="0.99511">
5.3 The effect of training data size
</subsectionHeader>
<bodyText confidence="0.9996876">
CTB2 is a relatively small corpus, and so we in-
vestigated the effect of adding more training data
from CTB5. Intuitively, more training data leads
to higher parsing accuracy. By using increased
amount of training sentences (Table 6) from CTB5
with the same development test data (Table 2),
we draw the accuracy curves with different num-
ber of training iterations (Figure 6). This exper-
iment confirmed that the accuracy increases with
the amount of training data.
</bodyText>
<figureCaption confidence="0.997576">
Figure 6: The influence of the size of training data
</figureCaption>
<bodyText confidence="0.999953666666667">
Another motivation for us to use more training
data is to reduce overfitting. We invested consid-
erable effort into feature engineering using CTB2,
and found that a small variation of feature tem-
plates (e.g. changing the feature template SocSic
from Table 1 to SotcSitc) can lead to a compar-
atively large change (up to 1%) in the accuracy.
One possible reason for this variation is the small
size of the CTB2 training data. When performing
experiments using the larger set B from Table 6,
we observed improved stability relative to small
feature changes.
</bodyText>
<page confidence="0.9952">
168
</page>
<table confidence="0.9663182">
Sections Sentences Words
001–815; 16,118
Training 437,859
1001–1136
886–931; 804
Dev 20,453
1148–1151
816–885; 1,915
Test 50,319
1137–1147
</table>
<tableCaption confidence="0.988499">
Table 7: Standard split of CTB5 data
</tableCaption>
<table confidence="0.984900333333333">
Non-root Root Complete
Zhang 2008 86.21% 76.26% 34.41%
Our parser 86.95% 79.19% 36.08%
</table>
<tableCaption confidence="0.995467">
Table 8: Comparison with state-of-the-art depen-
dency parsing using CTB5 data
</tableCaption>
<subsectionHeader confidence="0.997812">
5.4 Test accuracy using CTB5
</subsectionHeader>
<bodyText confidence="0.999974333333333">
Table 5 presents the performance of the parser on
CTB5. We adopt the data split from Zhang and
Clark (2008), as shown in Table 7. We used the
same parser configurations as Section 5.2.
As an additional evaluation we also produced
dependency output from the phrase-structure
trees, using the head-finding rules, so that we
can also compare with dependency parsers, for
which the highest scores in the literature are cur-
rently from our previous work in Zhang and Clark
(2008). We compare the dependencies read off our
constituent parser using CTB5 data with the depen-
dency parser from Zhang and Clark (2008). The
same measures are taken and the accuracies with
gold-standard POS-tags are shown in Table 8. Our
constituent parser gave higher accuracy than the
dependency parser. It is interesting that, though
the constituent parser uses many fewer feature
templates than the dependency parser, the features
do include constituent information, which is un-
available to dependency parsers.
</bodyText>
<sectionHeader confidence="0.999956" genericHeader="method">
6 Related work
</sectionHeader>
<bodyText confidence="0.999945918918919">
Our parser is based on the shift-reduce parsing
process from Sagae and Lavie (2005) and Wang
et al. (2006), and therefore it can be classified
as a transition-based parser (Nivre et al., 2006).
An important difference between our parser and
the Wang et al. (2006) parser is that our parser
is based on a discriminative learning model with
global features, whilst the parser from Wang et al.
(2006) is based on a local classifier that optimizes
each individual choice. Instead of greedy local de-
coding, we used beam search in the decoder.
An early work that applies beam search to con-
stituent parsing is Ratnaparkhi (1999). The main
difference between our parser and Ratnaparkhi’s is
that we use a global discriminative model, whereas
Ratnaparkhi’s parser has separate probabilities of
actions chained together in a conditional model.
Both our parser and the parser from Collins and
Roark (2004) use a global discriminative model
and an incremental parsing process. The major
difference is the use of different incremental pars-
ing processes. To achieve better performance for
Chinese parsing, our parser is based on the shift-
reduce parsing process. In addition, we did not in-
clude a generative baseline model in the discrimi-
native model, as did Collins and Roark (2004).
Our parser in this paper shares similarity
with our transition-based dependency parser from
Zhang and Clark (2008) in the use of a discrimina-
tive model and beam search. The main difference
is that our parser in this paper is for constituent
parsing. In fact, our parser is one of only a few
constituent parsers which have successfully ap-
plied global discriminative models, certainly with-
out a generative baseline as a feature, whereas
global models for dependency parsing have been
comparatively easier to develop.
</bodyText>
<sectionHeader confidence="0.999173" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999761761904762">
The contributions of this paper can be summarized
as follows. First, we defined a global discrimina-
tive model for Chinese constituent-based parsing,
continuing recent work in this area which has fo-
cused on English (Clark and Curran, 2007; Car-
reras et al., 2008; Finkel et al., 2008). Second, we
showed how such a model can be applied to shift-
reduce parsing and combined with beam search,
resulting in an accurate linear-time parser. In stan-
dard tests using CTB2 data, our parser achieved
comparable Parseval F-score to the state-of-the-
art systems. Moreover, we observed that more
training data lead to improvements on both accu-
racy and stability against feature variations, and
reported performance of the parser using CTB5
data. By converting constituent-based output to
dependency relations using standard head-finding
rules, our parser also obtained the highest scores
for a CTB5 dependency evaluation in the literature.
Due to the comparatively low accuracy for Chi-
nese POS-tagging, the parsing accuracy dropped
</bodyText>
<page confidence="0.9963">
169
</page>
<bodyText confidence="0.9999635">
significantly when using automatically assigned
POS-tags rather than gold-standard POS-tags. In
our further work, we plan to investigate possible
methods of joint POS-tagging and parsing under
the discriminative model and beam-search frame-
work.
A discriminative model allows consistent train-
ing of a wide range of different features. We
showed in Zhang and Clark (2008) that it was pos-
sible to combine graph and transition-based de-
pendency parser into the same global discrimina-
tive model. Our parser framework in this paper
allows the same integration of graph-based fea-
tures. However, preliminary experiments with fea-
tures based on graph information did not show
accuracy improvements for our parser. One pos-
sible reason is that the transition actions for the
parser in this paper already include graph infor-
mation, such as the label of the newly gener-
ated constituent, while for the dependency parser
in Zhang and Clark (2008), transition actions do
not contain graph information, and therefore the
use of transition-based features helped to make
larger improvements in accuracy. The integration
of graph-based features for our shift-reduce con-
stituent parser is worth further study.
The source code of our parser is publicly avail-
able at http://www.sourceforge.net/projects/zpar.2
</bodyText>
<sectionHeader confidence="0.993831" genericHeader="acknowledgments">
Appendix
</sectionHeader>
<bodyText confidence="0.999974285714286">
The set of restrictions which ensures a valid binary
tree is shown below. The restriction on the num-
ber of consecutive unary rule applications is taken
from Sagae and Lavie (2005); it prevents infinite
running of the parser by repetitive use of unary re-
duce actions, and ensures linear time complexity
in the length of the sentence.
</bodyText>
<listItem confidence="0.995714636363636">
• the shift action can only be performed when
the queue of incoming words is not empty;
• when the node on top of the stack is tempo-
rary and its head word is from the right child,
no shift action can be performed;
• the unary reduce actions can be performed
only when the stack is not empty;
• a unary reduce with the same constituent la-
bel (Y → Y ) is not allowed;
• no more than three unary reduce actions can
be performed consecutively;
</listItem>
<footnote confidence="0.9276065">
2The make target for the parser in this paper is chi-
nese.conparser.
</footnote>
<listItem confidence="0.984436928571429">
• the binary reduce actions can only be per-
formed when the stack contains at least two
nodes, with at least one of the two nodes on
top of stack (with R being the topmost and L
being the second) being non-temporary;
• if L is temporary with label X∗, the result-
ing node must be labeled X or X∗ and left-
headed (i.e. to take the head word from L);
similar restrictions apply when R is tempo-
rary;
• when the incoming queue is empty and the
stack contains only two nodes, binary reduce
can be applied only if the resulting node is
non-temporary;
• when the stack contains only two nodes, tem-
porary resulting nodes from binary reduce
must be left-headed;
• when the queue is empty and the stack con-
tains more than two nodes, with the third
node from the top being temporary, binary re-
duce can be applied only if the resulting node
is non-temporary;
• when the stack contains more than two nodes,
with the third node from the top being tempo-
rary, temporary resulting nodes from binary
reduce must be left-headed;
• the terminate action can be performed when
the queue is empty, and the stack size is one.
</listItem>
<page confidence="0.996778">
170
</page>
<sectionHeader confidence="0.998325" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999695348837209">
Daniel M. Bikel and David Chiang. 2000. Two sta-
tistical parsing models applied to the Chinese Tree-
bank. In Proceedings of SIGHAN Workshop, pages
1–6, Morristown, NJ, USA.
Daniel M. Bikel. 2004. On the Parameter Space of
Generative Lexicalized Statistical Parsing Models.
Ph.D. thesis, University of Pennsylvania.
Ted Briscoe and John Carroll. 1993. Generalized prob-
abilistic LR parsing of natural language (corpora)
with unification-based grammars. Computational
Linguistics, 19(1):25–59.
Xavier Carreras, Michael Collins, and Terry Koo.
2008. Tag, dynamic programming, and the percep-
tron for efficient, feature-rich parsing. In Proceed-
ings of CoNLL, pages 9–16, Manchester, England,
August.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of NAACL, pages
132–139, Seattle, WA.
David Chiang and Daniel M. Bikel. 2002. Recovering
latent information in treebanks. In Proceedings of
COLING.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493–552.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings ofACL, pages 111–118, Barcelona, Spain, July.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of
the 35th Meeting of the ACL, pages 16–23, Madrid,
Spain.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP, pages 1–8, Philadelphia, USA, July.
Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Proba-
bilistic models for action-based Chinese dependency
parsing. In Proceedings ofECML/ECPPKDD, War-
saw, Poland, September.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, con-
ditional random field parsing. In Proceedings of
ACL/HLT, pages 959–967, Columbus, Ohio, June.
Association for Computational Linguistics.
Roger Levy and Christopher D. Manning. 2003. Is it
harder to parse Chinese, or the Chinese Treebank?
In Proceedings of ACL, pages 439–446, Sapporo,
Japan, July.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings ofACL, pages 91–
98, Ann Arbor, Michigan, June.
Joakim Nivre, Johan Hall, Jens Nilsson, G¨uls¸en
Eryiˇgit, and Svetoslav Marinov. 2006. Labeled
pseudo-projective dependency parsing with support
vector machines. In Proceedings of CoNLL, pages
221–225, New York City, June.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of
HLT/NAACL, pages 404–411, Rochester, New York,
April. Association for Computational Linguistics.
Adwait Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
Learning, 34(1-3):151–175.
Kenji Sagae and Alon Lavie. 2005. A classifier-based
parser with linear run-time complexity. In Proceed-
ings of IWPT, pages 125–132, Vancouver, British
Columbia, October.
Honglin Sun and Daniel Jurafsky. 2003. The effect of
rhythm on structural disambiguation in Chinese. In
Proceedings of SIGHAN Workshop.
Mengqiu Wang, Kenji Sagae, and Teruko Mitamura.
2006. A fast, accurate deterministic parser for Chi-
nese. In Proceedings of COLING/ACL, pages 425–
432, Sydney, Australia.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin,
and Yueliang Qian. 2005. Parsing the Penn Chinese
Treebank with semantic knowledge. In Proceedings
ofIJCNLP.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In Proceedings of EMNLP, pages
562–571, Hawaii, USA, October.
</reference>
<page confidence="0.998207">
171
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.411169">
<title confidence="0.9493515">Transition-Based Parsing of the Chinese Treebank using a Discriminative Model</title>
<author confidence="0.804808">Yue</author>
<affiliation confidence="0.9010625">Oxford Computing</affiliation>
<email confidence="0.970263">yue.zhang@comlab.ox.ac.uk</email>
<author confidence="0.965425">Stephen</author>
<affiliation confidence="0.7876185">Cambridge Computer</affiliation>
<email confidence="0.992832">stephen.clark@cl.cam.ac.uk</email>
<abstract confidence="0.9951661875">Transition-based approaches have shown competitive performance on constituent and dependency parsing of Chinese. Stateof-the-art accuracies have been achieved by a deterministic shift-reduce parsing model on parsing the Chinese Treebank 2 data (Wang et al., 2006). In this paper, we propose a global discriminative model based on the shift-reduce parsing process, combined with a beam-search decoder, obcompetitive accuracies on We also report the performance of the on data, obtaining the highest scores in the literature for a dependencybased evaluation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
<author>David Chiang</author>
</authors>
<title>Two statistical parsing models applied to the Chinese Treebank.</title>
<date>2000</date>
<booktitle>In Proceedings of SIGHAN Workshop,</booktitle>
<pages>1--6</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="17339" citStr="Bikel and Chiang (2000)" startWordPosition="2928" endWordPosition="2931">h Wang et al. (2006) and earlier work. The tests were performed using both gold-standard POS-tags and POS-tags automatically assigned by a POS-tagger. We used our Model LR LP F1 Bikel Thesis 80.9% 84.5% 82.7% Wang 2006 SVM 87.2% 88.3% 87.8% Wang 2006 Stacked 88.3% 88.1% 88.2% Our parser 89.4% 90.1% 89.8% Table 3: Accuracies on CTB2 with gold-standard POS-tags own implementation of the perceptron-based tagger from Collins (2002). The results of various models measured using sentences with less than 40 words and using goldstandard POS-tags are shown in Table 3. The rows represent the model from Bikel and Chiang (2000), Bikel (2004), the SVM and ensemble models from Wang et al. (2006), and our parser, respectively. The accuracy of our parser is competitive using this test set. The results of various models using automatically assigned POS-tags are shown in Table 4. The rows in the table represent the models from Bikel and Chiang (2000), Levy and Manning (2003), Xiong et al. (2005), Bikel (2004), Chiang and Bikel (2002), the SVM model from Wang et al. (2006) and the ensemble system from Wang et al. (2006), and the parser of this paper, respectively. Our parser gave comparable accuracies to the SVM and ensemb</context>
</contexts>
<marker>Bikel, Chiang, 2000</marker>
<rawString>Daniel M. Bikel and David Chiang. 2000. Two statistical parsing models applied to the Chinese Treebank. In Proceedings of SIGHAN Workshop, pages 1–6, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
</authors>
<title>On the Parameter Space of Generative Lexicalized Statistical Parsing Models.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="17353" citStr="Bikel (2004)" startWordPosition="2932" endWordPosition="2933">earlier work. The tests were performed using both gold-standard POS-tags and POS-tags automatically assigned by a POS-tagger. We used our Model LR LP F1 Bikel Thesis 80.9% 84.5% 82.7% Wang 2006 SVM 87.2% 88.3% 87.8% Wang 2006 Stacked 88.3% 88.1% 88.2% Our parser 89.4% 90.1% 89.8% Table 3: Accuracies on CTB2 with gold-standard POS-tags own implementation of the perceptron-based tagger from Collins (2002). The results of various models measured using sentences with less than 40 words and using goldstandard POS-tags are shown in Table 3. The rows represent the model from Bikel and Chiang (2000), Bikel (2004), the SVM and ensemble models from Wang et al. (2006), and our parser, respectively. The accuracy of our parser is competitive using this test set. The results of various models using automatically assigned POS-tags are shown in Table 4. The rows in the table represent the models from Bikel and Chiang (2000), Levy and Manning (2003), Xiong et al. (2005), Bikel (2004), Chiang and Bikel (2002), the SVM model from Wang et al. (2006) and the ensemble system from Wang et al. (2006), and the parser of this paper, respectively. Our parser gave comparable accuracies to the SVM and ensemble models from</context>
</contexts>
<marker>Bikel, 2004</marker>
<rawString>Daniel M. Bikel. 2004. On the Parameter Space of Generative Lexicalized Statistical Parsing Models. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Generalized probabilistic LR parsing of natural language (corpora) with unification-based grammars.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="1053" citStr="Briscoe and Carroll, 1993" startWordPosition="138" endWordPosition="142"> shift-reduce parsing model on parsing the Chinese Treebank 2 data (Wang et al., 2006). In this paper, we propose a global discriminative model based on the shift-reduce parsing process, combined with a beam-search decoder, obtaining competitive accuracies on CTB2. We also report the performance of the parser on CTB5 data, obtaining the highest scores in the literature for a dependencybased evaluation. 1 Introduction Transition-based statistical parsing associates scores with each decision in the parsing process, selecting the parse which is built by the highest scoring sequence of decisions (Briscoe and Carroll, 1993; Nivre et al., 2006). The parsing algorithm is typically some form of bottom-up shiftreduce algorithm, so that scores are associated with actions such as shift and reduce. One advantage of this approach is that the parsing can be highly efficient, for example by pursuing a greedy strategy in which a single action is chosen at each decision point. The alternative approach, exemplified by Collins (1997) and Charniak (2000), is to use a chart-based algorithm to build the space of possible parses, together with pruning of lowprobability constituents and the Viterbi algorithm to find the highest s</context>
</contexts>
<marker>Briscoe, Carroll, 1993</marker>
<rawString>Ted Briscoe and John Carroll. 1993. Generalized probabilistic LR parsing of natural language (corpora) with unification-based grammars. Computational Linguistics, 19(1):25–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>Tag, dynamic programming, and the perceptron for efficient, feature-rich parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>9--16</pages>
<location>Manchester, England,</location>
<contexts>
<context position="25225" citStr="Carreras et al., 2008" startWordPosition="4287" endWordPosition="4291">. The main difference is that our parser in this paper is for constituent parsing. In fact, our parser is one of only a few constituent parsers which have successfully applied global discriminative models, certainly without a generative baseline as a feature, whereas global models for dependency parsing have been comparatively easier to develop. 7 Conclusion The contributions of this paper can be summarized as follows. First, we defined a global discriminative model for Chinese constituent-based parsing, continuing recent work in this area which has focused on English (Clark and Curran, 2007; Carreras et al., 2008; Finkel et al., 2008). Second, we showed how such a model can be applied to shiftreduce parsing and combined with beam search, resulting in an accurate linear-time parser. In standard tests using CTB2 data, our parser achieved comparable Parseval F-score to the state-of-theart systems. Moreover, we observed that more training data lead to improvements on both accuracy and stability against feature variations, and reported performance of the parser using CTB5 data. By converting constituent-based output to dependency relations using standard head-finding rules, our parser also obtained the hig</context>
</contexts>
<marker>Carreras, Collins, Koo, 2008</marker>
<rawString>Xavier Carreras, Michael Collins, and Terry Koo. 2008. Tag, dynamic programming, and the perceptron for efficient, feature-rich parsing. In Proceedings of CoNLL, pages 9–16, Manchester, England, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>132--139</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="1478" citStr="Charniak (2000)" startWordPosition="212" endWordPosition="213">ed statistical parsing associates scores with each decision in the parsing process, selecting the parse which is built by the highest scoring sequence of decisions (Briscoe and Carroll, 1993; Nivre et al., 2006). The parsing algorithm is typically some form of bottom-up shiftreduce algorithm, so that scores are associated with actions such as shift and reduce. One advantage of this approach is that the parsing can be highly efficient, for example by pursuing a greedy strategy in which a single action is chosen at each decision point. The alternative approach, exemplified by Collins (1997) and Charniak (2000), is to use a chart-based algorithm to build the space of possible parses, together with pruning of lowprobability constituents and the Viterbi algorithm to find the highest scoring parse. For English dependency parsing, the two approaches give similar results (McDonald et al., 2005; Nivre et al., 2006). For English constituent-based parsing using the Penn Treebank, the best performing transitionbased parser lags behind the current state-of-theart (Sagae and Lavie, 2005). In contrast, for Chinese, the best dependency parsers are currently transition-based (Duan et al., 2007; Zhang and Clark, 2</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropyinspired parser. In Proceedings of NAACL, pages 132–139, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Daniel M Bikel</author>
</authors>
<title>Recovering latent information in treebanks.</title>
<date>2002</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="17747" citStr="Chiang and Bikel (2002)" startWordPosition="3000" endWordPosition="3003">ger from Collins (2002). The results of various models measured using sentences with less than 40 words and using goldstandard POS-tags are shown in Table 3. The rows represent the model from Bikel and Chiang (2000), Bikel (2004), the SVM and ensemble models from Wang et al. (2006), and our parser, respectively. The accuracy of our parser is competitive using this test set. The results of various models using automatically assigned POS-tags are shown in Table 4. The rows in the table represent the models from Bikel and Chiang (2000), Levy and Manning (2003), Xiong et al. (2005), Bikel (2004), Chiang and Bikel (2002), the SVM model from Wang et al. (2006) and the ensemble system from Wang et al. (2006), and the parser of this paper, respectively. Our parser gave comparable accuracies to the SVM and ensemble models from Wang et al. (2006). However, comparison with Table 3 shows that our parser is more sensitive to POS-tagging errors than some of the other models. One possible reason is that some of the other parsers, e.g. Bikel (2004), use the parser model itself to resolve tagging ambiguities, whereas we rely on a POS tagger to accurately assign a single tag to each word. In fact, for the Chinese data, PO</context>
</contexts>
<marker>Chiang, Bikel, 2002</marker>
<rawString>David Chiang and Daniel M. Bikel. 2002. Recovering latent information in treebanks. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Widecoverage efficient statistical parsing with CCG and log-linear models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="25202" citStr="Clark and Curran, 2007" startWordPosition="4283" endWordPosition="4286">ve model and beam search. The main difference is that our parser in this paper is for constituent parsing. In fact, our parser is one of only a few constituent parsers which have successfully applied global discriminative models, certainly without a generative baseline as a feature, whereas global models for dependency parsing have been comparatively easier to develop. 7 Conclusion The contributions of this paper can be summarized as follows. First, we defined a global discriminative model for Chinese constituent-based parsing, continuing recent work in this area which has focused on English (Clark and Curran, 2007; Carreras et al., 2008; Finkel et al., 2008). Second, we showed how such a model can be applied to shiftreduce parsing and combined with beam search, resulting in an accurate linear-time parser. In standard tests using CTB2 data, our parser achieved comparable Parseval F-score to the state-of-theart systems. Moreover, we observed that more training data lead to improvements on both accuracy and stability against feature variations, and reported performance of the parser using CTB5 data. By converting constituent-based output to dependency relations using standard head-finding rules, our parse</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007. Widecoverage efficient statistical parsing with CCG and log-linear models. Computational Linguistics, 33(4):493–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>111--118</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="11258" citStr="Collins and Roark (2004)" startWordPosition="1904" endWordPosition="1907">ses the current model to decode training examples (the parse function in the pseudo-code). If the output is correct, it passes on to the next example. If the output is incorrect, it adjusts the weight values by adding the feature vector from the goldstandard output and subtracting the feature vector from the parser output. Weight values are updated for each example (making the process online) and the training data is iterated over T times. In order to avoid overfitting we used the now-standard averaged version of this algorithm (Collins, 2002). We also apply the early update modification from Collins and Roark (2004). If the agenda, at any point during the decoding process, does not contain the correct partial parse, it is not possible for the decoder to produce the correct output. In this case, decoding is stopped early and the weight values are updated using the highest scoring partial parse on the agenda. 4.1 Feature set Table 1 shows the set of feature templates for the model. Individual features are generated from 165 Description Unigrams Feature templates S0tc, S0wc, S1tc, S1wc, S2tc, S2wc, S3tc, S3wc, N0wt, N1wt, N2wt, N3wt, S0lwc, S0rwc, S0uwc, S1lwc, S1rwc, S1uwc, S0wS1w, S0wS1c, S0cS1w, S0cS1c, </context>
<context position="24061" citStr="Collins and Roark (2004)" startWordPosition="4099" endWordPosition="4102"> is that our parser is based on a discriminative learning model with global features, whilst the parser from Wang et al. (2006) is based on a local classifier that optimizes each individual choice. Instead of greedy local decoding, we used beam search in the decoder. An early work that applies beam search to constituent parsing is Ratnaparkhi (1999). The main difference between our parser and Ratnaparkhi’s is that we use a global discriminative model, whereas Ratnaparkhi’s parser has separate probabilities of actions chained together in a conditional model. Both our parser and the parser from Collins and Roark (2004) use a global discriminative model and an incremental parsing process. The major difference is the use of different incremental parsing processes. To achieve better performance for Chinese parsing, our parser is based on the shiftreduce parsing process. In addition, we did not include a generative baseline model in the discriminative model, as did Collins and Roark (2004). Our parser in this paper shares similarity with our transition-based dependency parser from Zhang and Clark (2008) in the use of a discriminative model and beam search. The main difference is that our parser in this paper is</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings ofACL, pages 111–118, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Meeting of the ACL,</booktitle>
<pages>16--23</pages>
<location>Madrid,</location>
<contexts>
<context position="1458" citStr="Collins (1997)" startWordPosition="209" endWordPosition="210">tion Transition-based statistical parsing associates scores with each decision in the parsing process, selecting the parse which is built by the highest scoring sequence of decisions (Briscoe and Carroll, 1993; Nivre et al., 2006). The parsing algorithm is typically some form of bottom-up shiftreduce algorithm, so that scores are associated with actions such as shift and reduce. One advantage of this approach is that the parsing can be highly efficient, for example by pursuing a greedy strategy in which a single action is chosen at each decision point. The alternative approach, exemplified by Collins (1997) and Charniak (2000), is to use a chart-based algorithm to build the space of possible parses, together with pruning of lowprobability constituents and the Viterbi algorithm to find the highest scoring parse. For English dependency parsing, the two approaches give similar results (McDonald et al., 2005; Nivre et al., 2006). For English constituent-based parsing using the Penn Treebank, the best performing transitionbased parser lags behind the current state-of-theart (Sagae and Lavie, 2005). In contrast, for Chinese, the best dependency parsers are currently transition-based (Duan et al., 2007</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Meeting of the ACL, pages 16–23, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1--8</pages>
<location>Philadelphia, USA,</location>
<contexts>
<context position="10537" citStr="Collins, 2002" startWordPosition="1785" endWordPosition="1786">) Outputs: w� Figure 4: the perceptron learning algorithm for state item Y is defined by: Score(Y ) = w� · `b(Y ) = � Ai fi(Y ) i where `b(Y ) is the global feature vector from Y , and w� is the weight vector defined by the model. Each element from `b(Y ) represents the global count of a particular feature from Y . The feature set consists of a large number of features which pick out various configurations from the stack and queue, based on the words and subtrees in the state item. The features are described in Section 4.1. The weight values are set using the generalized perceptron algorithm (Collins, 2002). The perceptron algorithm is shown in Figure 4. It initializes weight values as all zeros, and uses the current model to decode training examples (the parse function in the pseudo-code). If the output is correct, it passes on to the next example. If the output is incorrect, it adjusts the weight values by adding the feature vector from the goldstandard output and subtracting the feature vector from the parser output. Weight values are updated for each example (making the process online) and the training data is iterated over T times. In order to avoid overfitting we used the now-standard aver</context>
<context position="17147" citStr="Collins (2002)" startWordPosition="2897" endWordPosition="2898">vious work, with the CTB2 data extracted from Chinese Treebank 5 (CTB5). The data was split into training, development test and test sets, as shown in Table 2, which is consistent with Wang et al. (2006) and earlier work. The tests were performed using both gold-standard POS-tags and POS-tags automatically assigned by a POS-tagger. We used our Model LR LP F1 Bikel Thesis 80.9% 84.5% 82.7% Wang 2006 SVM 87.2% 88.3% 87.8% Wang 2006 Stacked 88.3% 88.1% 88.2% Our parser 89.4% 90.1% 89.8% Table 3: Accuracies on CTB2 with gold-standard POS-tags own implementation of the perceptron-based tagger from Collins (2002). The results of various models measured using sentences with less than 40 words and using goldstandard POS-tags are shown in Table 3. The rows represent the model from Bikel and Chiang (2000), Bikel (2004), the SVM and ensemble models from Wang et al. (2006), and our parser, respectively. The accuracy of our parser is competitive using this test set. The results of various models using automatically assigned POS-tags are shown in Table 4. The rows in the table represent the models from Bikel and Chiang (2000), Levy and Manning (2003), Xiong et al. (2005), Bikel (2004), Chiang and Bikel (2002)</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of EMNLP, pages 1–8, Philadelphia, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiangyu Duan</author>
<author>Jun Zhao</author>
<author>Bo Xu</author>
</authors>
<title>Probabilistic models for action-based Chinese dependency parsing.</title>
<date>2007</date>
<booktitle>In Proceedings ofECML/ECPPKDD,</booktitle>
<location>Warsaw, Poland,</location>
<contexts>
<context position="2058" citStr="Duan et al., 2007" startWordPosition="299" endWordPosition="302">by Collins (1997) and Charniak (2000), is to use a chart-based algorithm to build the space of possible parses, together with pruning of lowprobability constituents and the Viterbi algorithm to find the highest scoring parse. For English dependency parsing, the two approaches give similar results (McDonald et al., 2005; Nivre et al., 2006). For English constituent-based parsing using the Penn Treebank, the best performing transitionbased parser lags behind the current state-of-theart (Sagae and Lavie, 2005). In contrast, for Chinese, the best dependency parsers are currently transition-based (Duan et al., 2007; Zhang and Clark, 2008). For constituent-based parsing using the Chinese Treebank (CTB), Wang et al. (2006) have shown that a shift-reduce parser can give competitive accuracy scores together with high speeds, by using an SVM to make a single decision at each point in the parsing process. In this paper we describe a global discriminative model for Chinese shift-reduce parsing, and compare it with Wang et al.’s approach. We apply the same shift-reduce procedure as Wang et al. (2006), but instead of using a local classifier for each transition-based action, we train a generalized perceptron mod</context>
</contexts>
<marker>Duan, Zhao, Xu, 2007</marker>
<rawString>Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Probabilistic models for action-based Chinese dependency parsing. In Proceedings ofECML/ECPPKDD, Warsaw, Poland, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Alex Kleeman</author>
<author>Christopher D Manning</author>
</authors>
<title>Efficient, feature-based, conditional random field parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL/HLT,</booktitle>
<pages>959--967</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="25247" citStr="Finkel et al., 2008" startWordPosition="4292" endWordPosition="4295">s that our parser in this paper is for constituent parsing. In fact, our parser is one of only a few constituent parsers which have successfully applied global discriminative models, certainly without a generative baseline as a feature, whereas global models for dependency parsing have been comparatively easier to develop. 7 Conclusion The contributions of this paper can be summarized as follows. First, we defined a global discriminative model for Chinese constituent-based parsing, continuing recent work in this area which has focused on English (Clark and Curran, 2007; Carreras et al., 2008; Finkel et al., 2008). Second, we showed how such a model can be applied to shiftreduce parsing and combined with beam search, resulting in an accurate linear-time parser. In standard tests using CTB2 data, our parser achieved comparable Parseval F-score to the state-of-theart systems. Moreover, we observed that more training data lead to improvements on both accuracy and stability against feature variations, and reported performance of the parser using CTB5 data. By converting constituent-based output to dependency relations using standard head-finding rules, our parser also obtained the highest scores for a CTB5</context>
</contexts>
<marker>Finkel, Kleeman, Manning, 2008</marker>
<rawString>Jenny Rose Finkel, Alex Kleeman, and Christopher D. Manning. 2008. Efficient, feature-based, conditional random field parsing. In Proceedings of ACL/HLT, pages 959–967, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
<author>Christopher D Manning</author>
</authors>
<title>Is it harder to parse Chinese, or the Chinese Treebank?</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>439--446</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="17687" citStr="Levy and Manning (2003)" startWordPosition="2990" endWordPosition="2993">dard POS-tags own implementation of the perceptron-based tagger from Collins (2002). The results of various models measured using sentences with less than 40 words and using goldstandard POS-tags are shown in Table 3. The rows represent the model from Bikel and Chiang (2000), Bikel (2004), the SVM and ensemble models from Wang et al. (2006), and our parser, respectively. The accuracy of our parser is competitive using this test set. The results of various models using automatically assigned POS-tags are shown in Table 4. The rows in the table represent the models from Bikel and Chiang (2000), Levy and Manning (2003), Xiong et al. (2005), Bikel (2004), Chiang and Bikel (2002), the SVM model from Wang et al. (2006) and the ensemble system from Wang et al. (2006), and the parser of this paper, respectively. Our parser gave comparable accuracies to the SVM and ensemble models from Wang et al. (2006). However, comparison with Table 3 shows that our parser is more sensitive to POS-tagging errors than some of the other models. One possible reason is that some of the other parsers, e.g. Bikel (2004), use the parser model itself to resolve tagging ambiguities, whereas we rely on a POS tagger to accurately assign </context>
</contexts>
<marker>Levy, Manning, 2003</marker>
<rawString>Roger Levy and Christopher D. Manning. 2003. Is it harder to parse Chinese, or the Chinese Treebank? In Proceedings of ACL, pages 439–446, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL, pages 91– 98,</booktitle>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="1761" citStr="McDonald et al., 2005" startWordPosition="255" endWordPosition="258">educe algorithm, so that scores are associated with actions such as shift and reduce. One advantage of this approach is that the parsing can be highly efficient, for example by pursuing a greedy strategy in which a single action is chosen at each decision point. The alternative approach, exemplified by Collins (1997) and Charniak (2000), is to use a chart-based algorithm to build the space of possible parses, together with pruning of lowprobability constituents and the Viterbi algorithm to find the highest scoring parse. For English dependency parsing, the two approaches give similar results (McDonald et al., 2005; Nivre et al., 2006). For English constituent-based parsing using the Penn Treebank, the best performing transitionbased parser lags behind the current state-of-theart (Sagae and Lavie, 2005). In contrast, for Chinese, the best dependency parsers are currently transition-based (Duan et al., 2007; Zhang and Clark, 2008). For constituent-based parsing using the Chinese Treebank (CTB), Wang et al. (2006) have shown that a shift-reduce parser can give competitive accuracy scores together with high speeds, by using an SVM to make a single decision at each point in the parsing process. In this pape</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings ofACL, pages 91– 98, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
<author>G¨uls¸en Eryiˇgit</author>
<author>Svetoslav Marinov</author>
</authors>
<title>Labeled pseudo-projective dependency parsing with support vector machines.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>221--225</pages>
<location>New York City,</location>
<marker>Nivre, Hall, Nilsson, Eryiˇgit, Marinov, 2006</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, G¨uls¸en Eryiˇgit, and Svetoslav Marinov. 2006. Labeled pseudo-projective dependency parsing with support vector machines. In Proceedings of CoNLL, pages 221–225, New York City, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of HLT/NAACL,</booktitle>
<pages>404--411</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York,</location>
<contexts>
<context position="18809" citStr="Petrov and Klein (2007)" startWordPosition="3186" endWordPosition="3189">er model itself to resolve tagging ambiguities, whereas we rely on a POS tagger to accurately assign a single tag to each word. In fact, for the Chinese data, POS tagging accuracy is not very high, with the perceptron-based tagger achieving an accuracy of only 93%. The beam-search decoding framework we use could accommodate joint parsing and tagging, although the use of features based on the tags of incoming words complicates matters somewhat, since these features rely on tags having been assigned to all words in a pre-processing step. We leave this problem for future work. In a recent paper, Petrov and Klein (2007) reported LR and LP of 85.7% and 86.9% for sentences with less than 40 words and 81.9% and 84.8% for all sentences on the CTB2 test set, re167 LR &lt; 40 words POS LR &lt; 100 words POS LR Unlimited POS LP F1 LP F1 LP F1 Bikel 2000 76.8% 77.8% 77.3% - 73.3% 74.6% 74.0% - - - - - Levy 2003 79.2% 78.4% 78.8% - - - - - - - - - Xiong 2005 78.7% 80.1% 79.4% - - - - - - - - - Bikel Thesis 78.0% 81.2% 79.6% - 74.4% 78.5% 76.4% - - - - - Chiang 2002 78.8% 81.1% 79.9% - 75.2% 78.0% 76.6% - - - - - Wang 2006 SVM 78.1% 81.1% 79.6% 92.5% 75.5% 78.5% 77.0% 92.2% 75.0% 78.0% 76.5% 92.1% Wang 2006 Stacked 79.2% 81</context>
<context position="20369" citStr="Petrov and Klein (2007)" startWordPosition="3494" endWordPosition="3497">78.3% 93.9% Table 5: Accuracies on CTB5 using gold-standard and automatically assigned POS-tags Sections Sentences Words Set A 001–270 3,484 84,873 Set B Set A; 400–699 6,567 161,893 Set C Set B; 700–931 9,707 236,051 Table 6: Training sets with different sizes spectively. These results are significantly better than any model from Table 4. However, we did not include their scores in the table because they used a different training set from CTB5, which is much larger than the CTB2 training set used by all parsers in the table. In order to make a comparison, we split the data in the same way as Petrov and Klein (2007) and tested our parser using automatically assigned POS-tags. It gave LR and LP of 82.0% and 80.9% for sentences with less than 40 words and 77.8% and 77.4% for all sentences, significantly lower than Petrov and Klein (2007), which we partly attribute to the sensitivity of our parser to pos tag errors (see Table 5). 5.3 The effect of training data size CTB2 is a relatively small corpus, and so we investigated the effect of adding more training data from CTB5. Intuitively, more training data leads to higher parsing accuracy. By using increased amount of training sentences (Table 6) from CTB5 wi</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of HLT/NAACL, pages 404–411, Rochester, New York, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Learning to parse natural language with maximum entropy models.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="23788" citStr="Ratnaparkhi (1999)" startWordPosition="4060" endWordPosition="4061"> parser is based on the shift-reduce parsing process from Sagae and Lavie (2005) and Wang et al. (2006), and therefore it can be classified as a transition-based parser (Nivre et al., 2006). An important difference between our parser and the Wang et al. (2006) parser is that our parser is based on a discriminative learning model with global features, whilst the parser from Wang et al. (2006) is based on a local classifier that optimizes each individual choice. Instead of greedy local decoding, we used beam search in the decoder. An early work that applies beam search to constituent parsing is Ratnaparkhi (1999). The main difference between our parser and Ratnaparkhi’s is that we use a global discriminative model, whereas Ratnaparkhi’s parser has separate probabilities of actions chained together in a conditional model. Both our parser and the parser from Collins and Roark (2004) use a global discriminative model and an incremental parsing process. The major difference is the use of different incremental parsing processes. To achieve better performance for Chinese parsing, our parser is based on the shiftreduce parsing process. In addition, we did not include a generative baseline model in the discri</context>
</contexts>
<marker>Ratnaparkhi, 1999</marker>
<rawString>Adwait Ratnaparkhi. 1999. Learning to parse natural language with maximum entropy models. Machine Learning, 34(1-3):151–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Alon Lavie</author>
</authors>
<title>A classifier-based parser with linear run-time complexity.</title>
<date>2005</date>
<booktitle>In Proceedings of IWPT,</booktitle>
<pages>125--132</pages>
<location>Vancouver, British Columbia,</location>
<contexts>
<context position="1953" citStr="Sagae and Lavie, 2005" startWordPosition="283" endWordPosition="286">edy strategy in which a single action is chosen at each decision point. The alternative approach, exemplified by Collins (1997) and Charniak (2000), is to use a chart-based algorithm to build the space of possible parses, together with pruning of lowprobability constituents and the Viterbi algorithm to find the highest scoring parse. For English dependency parsing, the two approaches give similar results (McDonald et al., 2005; Nivre et al., 2006). For English constituent-based parsing using the Penn Treebank, the best performing transitionbased parser lags behind the current state-of-theart (Sagae and Lavie, 2005). In contrast, for Chinese, the best dependency parsers are currently transition-based (Duan et al., 2007; Zhang and Clark, 2008). For constituent-based parsing using the Chinese Treebank (CTB), Wang et al. (2006) have shown that a shift-reduce parser can give competitive accuracy scores together with high speeds, by using an SVM to make a single decision at each point in the parsing process. In this paper we describe a global discriminative model for Chinese shift-reduce parsing, and compare it with Wang et al.’s approach. We apply the same shift-reduce procedure as Wang et al. (2006), but in</context>
<context position="3457" citStr="Sagae and Lavie (2005)" startWordPosition="524" endWordPosition="527">r still operates in linear time, but the use of beam-search allows the correction of local decision errors by global comparison. Using CTB2, our model achieved Parseval F-scores comparable to Wang et al.’s approach. We also present accuracy scores for the much larger CTB5, using both a constituent-based and dependency-based evaluation. The scores for the dependency-based evaluation were higher than the state-of-the-art dependency parsers for the CTB5 data. 2 The Shift-Reduce Parsing Process The shift-reduce process used by our beam-search decoder is based on the greedy shift-reduce parsers of Sagae and Lavie (2005) and Wang et al. (2006). 162 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 162–171, Paris, October 2009. c�2009 Association for Computational Linguistics The process assumes binary-branching trees; section 2.1 explains how these are obtained from the arbitrary-branching trees in the Chinese Treebank. The input is assumed to be segmented and POS tagged, and the word-POS pairs waiting to be processed are stored in a queue. A stack holds the partial parse trees that are built during the parsing process. A parse state is defined as a (stack,queue) pair. Par</context>
<context position="4918" citStr="Sagae and Lavie (2005)" startWordPosition="770" endWordPosition="773"> word-POS pair in the queue onto the stack; • REDUCE–unary–X, which makes a new unary-branching node with label X; the stack is popped and the popped node becomes the child of the new node; the new node is pushed onto the stack; • REDUCE–binary–{L/R}–X, which makes a new binary-branching node with label X; the stack is popped twice, with the first popped node becoming the right child of the new node and the second popped node becoming the left child; the new node is pushed onto the stack; • TERMINATE, which pops the root node off the stack and ends parsing. This action is novel in our parser. Sagae and Lavie (2005) and Wang et al. (2006) only used the first three transition actions, setting the final state as all incoming words having been processed, and the stack containing only one node. However, there are a small number of sentences (14 out of 3475 from the training data) that have unary-branching roots. For these sentences, Wang’s parser will be unable to produce the unary-branching roots because the parsing process terminates as soon as the root is found. We define a separate action to terminate parsing, allowing unary reduces to be applied to the root item before parsing finishes. The trees built </context>
<context position="23250" citStr="Sagae and Lavie (2005)" startWordPosition="3966" endWordPosition="3969">Clark (2008). We compare the dependencies read off our constituent parser using CTB5 data with the dependency parser from Zhang and Clark (2008). The same measures are taken and the accuracies with gold-standard POS-tags are shown in Table 8. Our constituent parser gave higher accuracy than the dependency parser. It is interesting that, though the constituent parser uses many fewer feature templates than the dependency parser, the features do include constituent information, which is unavailable to dependency parsers. 6 Related work Our parser is based on the shift-reduce parsing process from Sagae and Lavie (2005) and Wang et al. (2006), and therefore it can be classified as a transition-based parser (Nivre et al., 2006). An important difference between our parser and the Wang et al. (2006) parser is that our parser is based on a discriminative learning model with global features, whilst the parser from Wang et al. (2006) is based on a local classifier that optimizes each individual choice. Instead of greedy local decoding, we used beam search in the decoder. An early work that applies beam search to constituent parsing is Ratnaparkhi (1999). The main difference between our parser and Ratnaparkhi’s is </context>
<context position="27458" citStr="Sagae and Lavie (2005)" startWordPosition="4630" endWordPosition="4633">nerated constituent, while for the dependency parser in Zhang and Clark (2008), transition actions do not contain graph information, and therefore the use of transition-based features helped to make larger improvements in accuracy. The integration of graph-based features for our shift-reduce constituent parser is worth further study. The source code of our parser is publicly available at http://www.sourceforge.net/projects/zpar.2 Appendix The set of restrictions which ensures a valid binary tree is shown below. The restriction on the number of consecutive unary rule applications is taken from Sagae and Lavie (2005); it prevents infinite running of the parser by repetitive use of unary reduce actions, and ensures linear time complexity in the length of the sentence. • the shift action can only be performed when the queue of incoming words is not empty; • when the node on top of the stack is temporary and its head word is from the right child, no shift action can be performed; • the unary reduce actions can be performed only when the stack is not empty; • a unary reduce with the same constituent label (Y → Y ) is not allowed; • no more than three unary reduce actions can be performed consecutively; 2The m</context>
</contexts>
<marker>Sagae, Lavie, 2005</marker>
<rawString>Kenji Sagae and Alon Lavie. 2005. A classifier-based parser with linear run-time complexity. In Proceedings of IWPT, pages 125–132, Vancouver, British Columbia, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Honglin Sun</author>
<author>Daniel Jurafsky</author>
</authors>
<title>The effect of rhythm on structural disambiguation in Chinese.</title>
<date>2003</date>
<booktitle>In Proceedings of SIGHAN Workshop.</booktitle>
<contexts>
<context position="14844" citStr="Sun and Jurafsky, 2003" startWordPosition="2499" endWordPosition="2502">ure vector. q represents the count of any separator punctuation between S0 and S1. Whenever an action is being considered at each point in the beam-search process, templates from Table 1 are matched with the context defined by the parser state and combined with the action to generate features. Negative features, which are the features from incorrect parser outputs but not from any training example, are included in the model. There are around a million features in our experiments with the CTB2 dataset. Wang et al. (2006) used a range of other features, including rhythmic features of S0 and S1 (Sun and Jurafsky, 2003), features from the most recently found node that is to the left or right of S0 and S1, the number of words and the number of punctuations in S0 and S1, the distance between S0 and S1 and so on. We did not include these features in our parser, because they did not lead to improved performance during development experiments. 5 Experiments The experiments were performed using the Chinese Treebank 2 and Chinese Treebank 5 data. Standard data preparation was performed before the experiments: empty terminal nodes were removed; any non-terminal nodes with no children were removed; any unary X → X no</context>
</contexts>
<marker>Sun, Jurafsky, 2003</marker>
<rawString>Honglin Sun and Daniel Jurafsky. 2003. The effect of rhythm on structural disambiguation in Chinese. In Proceedings of SIGHAN Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Kenji Sagae</author>
<author>Teruko Mitamura</author>
</authors>
<title>A fast, accurate deterministic parser for Chinese.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL,</booktitle>
<pages>425--432</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="2166" citStr="Wang et al. (2006)" startWordPosition="315" endWordPosition="318">ses, together with pruning of lowprobability constituents and the Viterbi algorithm to find the highest scoring parse. For English dependency parsing, the two approaches give similar results (McDonald et al., 2005; Nivre et al., 2006). For English constituent-based parsing using the Penn Treebank, the best performing transitionbased parser lags behind the current state-of-theart (Sagae and Lavie, 2005). In contrast, for Chinese, the best dependency parsers are currently transition-based (Duan et al., 2007; Zhang and Clark, 2008). For constituent-based parsing using the Chinese Treebank (CTB), Wang et al. (2006) have shown that a shift-reduce parser can give competitive accuracy scores together with high speeds, by using an SVM to make a single decision at each point in the parsing process. In this paper we describe a global discriminative model for Chinese shift-reduce parsing, and compare it with Wang et al.’s approach. We apply the same shift-reduce procedure as Wang et al. (2006), but instead of using a local classifier for each transition-based action, we train a generalized perceptron model over complete sequences of actions, so that the parameters are learned in the context of complete parses.</context>
<context position="3480" citStr="Wang et al. (2006)" startWordPosition="529" endWordPosition="532">time, but the use of beam-search allows the correction of local decision errors by global comparison. Using CTB2, our model achieved Parseval F-scores comparable to Wang et al.’s approach. We also present accuracy scores for the much larger CTB5, using both a constituent-based and dependency-based evaluation. The scores for the dependency-based evaluation were higher than the state-of-the-art dependency parsers for the CTB5 data. 2 The Shift-Reduce Parsing Process The shift-reduce process used by our beam-search decoder is based on the greedy shift-reduce parsers of Sagae and Lavie (2005) and Wang et al. (2006). 162 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 162–171, Paris, October 2009. c�2009 Association for Computational Linguistics The process assumes binary-branching trees; section 2.1 explains how these are obtained from the arbitrary-branching trees in the Chinese Treebank. The input is assumed to be segmented and POS tagged, and the word-POS pairs waiting to be processed are stored in a queue. A stack holds the partial parse trees that are built during the parsing process. A parse state is defined as a (stack,queue) pair. Parser actions, including </context>
<context position="4941" citStr="Wang et al. (2006)" startWordPosition="775" endWordPosition="778"> onto the stack; • REDUCE–unary–X, which makes a new unary-branching node with label X; the stack is popped and the popped node becomes the child of the new node; the new node is pushed onto the stack; • REDUCE–binary–{L/R}–X, which makes a new binary-branching node with label X; the stack is popped twice, with the first popped node becoming the right child of the new node and the second popped node becoming the left child; the new node is pushed onto the stack; • TERMINATE, which pops the root node off the stack and ends parsing. This action is novel in our parser. Sagae and Lavie (2005) and Wang et al. (2006) only used the first three transition actions, setting the final state as all incoming words having been processed, and the stack containing only one node. However, there are a small number of sentences (14 out of 3475 from the training data) that have unary-branching roots. For these sentences, Wang’s parser will be unable to produce the unary-branching roots because the parsing process terminates as soon as the root is found. We define a separate action to terminate parsing, allowing unary reduces to be applied to the root item before parsing finishes. The trees built by the parser are lexic</context>
<context position="6405" citStr="Wang et al. (2006)" startWordPosition="1067" endWordPosition="1070">m′ &gt; k and m′ &gt; 2 : new node Y * = X1..Xm′_1 Y ← Y *Xm′ m′ = m′ − 1 n′ = 1 while n′ &lt; k and k − n′ &gt; 1 : new node Y * = Xn′..Xk Y ← Xn′Y * n′ = n′ + 1 Figure 2: the binarization algorithm with input T the new node is to be taken from the left or right child. Note also that, since the parser is building binary trees, the X label in the REDUCE rules can be one of the temporary constituent labels, such as NP*, which are needed for the binarization process described in Section 2.1. Hence the number of left and right binary reduce rules is the number of constituent labels in the binarized grammar. Wang et al. (2006) give a detailed example showing how a segmented and POS-tagged sentence can be incrementally processed using the shift-reduce actions to produce a binary tree. We show this example in Figure 1. 2.1 The binarization process The algorithm in Figure 2 is used to map CTB trees into binarized trees, which are required by the shift-reduce parsing process. For any tree node with more than two child nodes, the algorithm works by first finding the head node, and then processing its right-hand-side and left-hand-side, respectively. The head-finding rules are taken from Zhang and Clark (2008). Y = X1..X</context>
<context position="13562" citStr="Wang et al. (2006)" startWordPosition="2285" endWordPosition="2288"> the stack, and the symbols N0, N1, N2 and N3 represent the first four words in the incoming queue. S0L, S0R and S0U represent the left and right child for binary branching S0, and the single child for unary branching S0, respectively; w represents the lexical head token for a node; c represents the label for a node. When the corresponding node is a terminal, c represents its POS-tag, whereas when the corresponding node is non-terminal, c represents its constituent label; t represents the POS-tag for a word. The context S0, S1, S2, S3 and N0, N1, N2, N3 for the feature templates is taken from Wang et al. (2006). However, Wang et al. (2006) used a polynomial kernel function with an SVM and did not manually create feature combinations. Since we used the linear perceptron algorithm we manually combined Unigram features into Bigram and Trigram features. The “Bracket” row shows bracket-related features, which were inspired by Wang et al. (2006). Here brackets refer to left brackets including “(”, “、” and “;”) between the head words of S0 and S1. These templates apply only when the stack contains at least two nodes; p represents a separator punctuation symbol. Each unique separator punctuation between S0 </context>
<context position="16736" citStr="Wang et al. (2006)" startWordPosition="2828" endWordPosition="2831">y-axis. The tests were performed using the development test data and gold-standard POStags. The figure shows the benefit of using a beam size greater than 1, with comparatively little accuracy gain being obtained beyond a beam size of 8. Hence we set the beam size to 16 for the rest of the experiments. 5.2 Test results on CTB2 The experiments in this section were performed using CTB2 to allow comparison with previous work, with the CTB2 data extracted from Chinese Treebank 5 (CTB5). The data was split into training, development test and test sets, as shown in Table 2, which is consistent with Wang et al. (2006) and earlier work. The tests were performed using both gold-standard POS-tags and POS-tags automatically assigned by a POS-tagger. We used our Model LR LP F1 Bikel Thesis 80.9% 84.5% 82.7% Wang 2006 SVM 87.2% 88.3% 87.8% Wang 2006 Stacked 88.3% 88.1% 88.2% Our parser 89.4% 90.1% 89.8% Table 3: Accuracies on CTB2 with gold-standard POS-tags own implementation of the perceptron-based tagger from Collins (2002). The results of various models measured using sentences with less than 40 words and using goldstandard POS-tags are shown in Table 3. The rows represent the model from Bikel and Chiang (20</context>
<context position="17972" citStr="Wang et al. (2006)" startWordPosition="3041" endWordPosition="3044"> the SVM and ensemble models from Wang et al. (2006), and our parser, respectively. The accuracy of our parser is competitive using this test set. The results of various models using automatically assigned POS-tags are shown in Table 4. The rows in the table represent the models from Bikel and Chiang (2000), Levy and Manning (2003), Xiong et al. (2005), Bikel (2004), Chiang and Bikel (2002), the SVM model from Wang et al. (2006) and the ensemble system from Wang et al. (2006), and the parser of this paper, respectively. Our parser gave comparable accuracies to the SVM and ensemble models from Wang et al. (2006). However, comparison with Table 3 shows that our parser is more sensitive to POS-tagging errors than some of the other models. One possible reason is that some of the other parsers, e.g. Bikel (2004), use the parser model itself to resolve tagging ambiguities, whereas we rely on a POS tagger to accurately assign a single tag to each word. In fact, for the Chinese data, POS tagging accuracy is not very high, with the perceptron-based tagger achieving an accuracy of only 93%. The beam-search decoding framework we use could accommodate joint parsing and tagging, although the use of features base</context>
<context position="23273" citStr="Wang et al. (2006)" startWordPosition="3971" endWordPosition="3974">e dependencies read off our constituent parser using CTB5 data with the dependency parser from Zhang and Clark (2008). The same measures are taken and the accuracies with gold-standard POS-tags are shown in Table 8. Our constituent parser gave higher accuracy than the dependency parser. It is interesting that, though the constituent parser uses many fewer feature templates than the dependency parser, the features do include constituent information, which is unavailable to dependency parsers. 6 Related work Our parser is based on the shift-reduce parsing process from Sagae and Lavie (2005) and Wang et al. (2006), and therefore it can be classified as a transition-based parser (Nivre et al., 2006). An important difference between our parser and the Wang et al. (2006) parser is that our parser is based on a discriminative learning model with global features, whilst the parser from Wang et al. (2006) is based on a local classifier that optimizes each individual choice. Instead of greedy local decoding, we used beam search in the decoder. An early work that applies beam search to constituent parsing is Ratnaparkhi (1999). The main difference between our parser and Ratnaparkhi’s is that we use a global di</context>
</contexts>
<marker>Wang, Sagae, Mitamura, 2006</marker>
<rawString>Mengqiu Wang, Kenji Sagae, and Teruko Mitamura. 2006. A fast, accurate deterministic parser for Chinese. In Proceedings of COLING/ACL, pages 425– 432, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Shuanglong Li</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
<author>Yueliang Qian</author>
</authors>
<title>Parsing the Penn Chinese Treebank with semantic knowledge.</title>
<date>2005</date>
<booktitle>In Proceedings ofIJCNLP.</booktitle>
<contexts>
<context position="17708" citStr="Xiong et al. (2005)" startWordPosition="2994" endWordPosition="2997">ntation of the perceptron-based tagger from Collins (2002). The results of various models measured using sentences with less than 40 words and using goldstandard POS-tags are shown in Table 3. The rows represent the model from Bikel and Chiang (2000), Bikel (2004), the SVM and ensemble models from Wang et al. (2006), and our parser, respectively. The accuracy of our parser is competitive using this test set. The results of various models using automatically assigned POS-tags are shown in Table 4. The rows in the table represent the models from Bikel and Chiang (2000), Levy and Manning (2003), Xiong et al. (2005), Bikel (2004), Chiang and Bikel (2002), the SVM model from Wang et al. (2006) and the ensemble system from Wang et al. (2006), and the parser of this paper, respectively. Our parser gave comparable accuracies to the SVM and ensemble models from Wang et al. (2006). However, comparison with Table 3 shows that our parser is more sensitive to POS-tagging errors than some of the other models. One possible reason is that some of the other parsers, e.g. Bikel (2004), use the parser model itself to resolve tagging ambiguities, whereas we rely on a POS tagger to accurately assign a single tag to each </context>
</contexts>
<marker>Xiong, Li, Liu, Lin, Qian, 2005</marker>
<rawString>Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, and Yueliang Qian. 2005. Parsing the Penn Chinese Treebank with semantic knowledge. In Proceedings ofIJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A tale of two parsers: investigating and combining graphbased and transition-based dependency parsing using beam-search.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>562--571</pages>
<location>Hawaii, USA,</location>
<contexts>
<context position="2082" citStr="Zhang and Clark, 2008" startWordPosition="303" endWordPosition="306">nd Charniak (2000), is to use a chart-based algorithm to build the space of possible parses, together with pruning of lowprobability constituents and the Viterbi algorithm to find the highest scoring parse. For English dependency parsing, the two approaches give similar results (McDonald et al., 2005; Nivre et al., 2006). For English constituent-based parsing using the Penn Treebank, the best performing transitionbased parser lags behind the current state-of-theart (Sagae and Lavie, 2005). In contrast, for Chinese, the best dependency parsers are currently transition-based (Duan et al., 2007; Zhang and Clark, 2008). For constituent-based parsing using the Chinese Treebank (CTB), Wang et al. (2006) have shown that a shift-reduce parser can give competitive accuracy scores together with high speeds, by using an SVM to make a single decision at each point in the parsing process. In this paper we describe a global discriminative model for Chinese shift-reduce parsing, and compare it with Wang et al.’s approach. We apply the same shift-reduce procedure as Wang et al. (2006), but instead of using a local classifier for each transition-based action, we train a generalized perceptron model over complete sequenc</context>
<context position="5605" citStr="Zhang and Clark (2008)" startWordPosition="886" endWordPosition="889">ons, setting the final state as all incoming words having been processed, and the stack containing only one node. However, there are a small number of sentences (14 out of 3475 from the training data) that have unary-branching roots. For these sentences, Wang’s parser will be unable to produce the unary-branching roots because the parsing process terminates as soon as the root is found. We define a separate action to terminate parsing, allowing unary reduces to be applied to the root item before parsing finishes. The trees built by the parser are lexicalized, using the head-finding rules from Zhang and Clark (2008). The left (L) and right (R) versions of the REDUCE-binary rules indicate whether the head of for node Y = X1..Xm E T : if m &gt; 2 : find the head node Xk(1 &lt; k &lt; m) of Y m′ = m while m′ &gt; k and m′ &gt; 2 : new node Y * = X1..Xm′_1 Y ← Y *Xm′ m′ = m′ − 1 n′ = 1 while n′ &lt; k and k − n′ &gt; 1 : new node Y * = Xn′..Xk Y ← Xn′Y * n′ = n′ + 1 Figure 2: the binarization algorithm with input T the new node is to be taken from the left or right child. Note also that, since the parser is building binary trees, the X label in the REDUCE rules can be one of the temporary constituent labels, such as NP*, which a</context>
<context position="6994" citStr="Zhang and Clark (2008)" startWordPosition="1164" endWordPosition="1167">rized grammar. Wang et al. (2006) give a detailed example showing how a segmented and POS-tagged sentence can be incrementally processed using the shift-reduce actions to produce a binary tree. We show this example in Figure 1. 2.1 The binarization process The algorithm in Figure 2 is used to map CTB trees into binarized trees, which are required by the shift-reduce parsing process. For any tree node with more than two child nodes, the algorithm works by first finding the head node, and then processing its right-hand-side and left-hand-side, respectively. The head-finding rules are taken from Zhang and Clark (2008). Y = X1..Xm represents a tree node Y with child nodes X1...Xm(m ≥ 1). The label of the newly generated node Y * is based on the constituent label of the original node Y , but marked with an asterix. Hence binarization enlarges the set of constituent labels. We call the constituents marked with * temporary constituents. The binarization process is reversible, in that output from the shift-reduce parser can be unbinarized into CTB format, which is required for evaluation. 163 Figure 1: An example shift-reduce parsing process, adopted from Wang et al. (2006) 2.2 Restrictions on the sequence of a</context>
<context position="22274" citStr="Zhang and Clark (2008)" startWordPosition="3811" endWordPosition="3814">ning data. When performing experiments using the larger set B from Table 6, we observed improved stability relative to small feature changes. 168 Sections Sentences Words 001–815; 16,118 Training 437,859 1001–1136 886–931; 804 Dev 20,453 1148–1151 816–885; 1,915 Test 50,319 1137–1147 Table 7: Standard split of CTB5 data Non-root Root Complete Zhang 2008 86.21% 76.26% 34.41% Our parser 86.95% 79.19% 36.08% Table 8: Comparison with state-of-the-art dependency parsing using CTB5 data 5.4 Test accuracy using CTB5 Table 5 presents the performance of the parser on CTB5. We adopt the data split from Zhang and Clark (2008), as shown in Table 7. We used the same parser configurations as Section 5.2. As an additional evaluation we also produced dependency output from the phrase-structure trees, using the head-finding rules, so that we can also compare with dependency parsers, for which the highest scores in the literature are currently from our previous work in Zhang and Clark (2008). We compare the dependencies read off our constituent parser using CTB5 data with the dependency parser from Zhang and Clark (2008). The same measures are taken and the accuracies with gold-standard POS-tags are shown in Table 8. Our</context>
<context position="24551" citStr="Zhang and Clark (2008)" startWordPosition="4177" endWordPosition="4180">arate probabilities of actions chained together in a conditional model. Both our parser and the parser from Collins and Roark (2004) use a global discriminative model and an incremental parsing process. The major difference is the use of different incremental parsing processes. To achieve better performance for Chinese parsing, our parser is based on the shiftreduce parsing process. In addition, we did not include a generative baseline model in the discriminative model, as did Collins and Roark (2004). Our parser in this paper shares similarity with our transition-based dependency parser from Zhang and Clark (2008) in the use of a discriminative model and beam search. The main difference is that our parser in this paper is for constituent parsing. In fact, our parser is one of only a few constituent parsers which have successfully applied global discriminative models, certainly without a generative baseline as a feature, whereas global models for dependency parsing have been comparatively easier to develop. 7 Conclusion The contributions of this paper can be summarized as follows. First, we defined a global discriminative model for Chinese constituent-based parsing, continuing recent work in this area w</context>
<context position="26354" citStr="Zhang and Clark (2008)" startWordPosition="4458" endWordPosition="4461">t to dependency relations using standard head-finding rules, our parser also obtained the highest scores for a CTB5 dependency evaluation in the literature. Due to the comparatively low accuracy for Chinese POS-tagging, the parsing accuracy dropped 169 significantly when using automatically assigned POS-tags rather than gold-standard POS-tags. In our further work, we plan to investigate possible methods of joint POS-tagging and parsing under the discriminative model and beam-search framework. A discriminative model allows consistent training of a wide range of different features. We showed in Zhang and Clark (2008) that it was possible to combine graph and transition-based dependency parser into the same global discriminative model. Our parser framework in this paper allows the same integration of graph-based features. However, preliminary experiments with features based on graph information did not show accuracy improvements for our parser. One possible reason is that the transition actions for the parser in this paper already include graph information, such as the label of the newly generated constituent, while for the dependency parser in Zhang and Clark (2008), transition actions do not contain grap</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. A tale of two parsers: investigating and combining graphbased and transition-based dependency parsing using beam-search. In Proceedings of EMNLP, pages 562–571, Hawaii, USA, October.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>