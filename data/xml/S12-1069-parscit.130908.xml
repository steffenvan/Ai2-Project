<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005301">
<title confidence="0.98845">
UNT-SIMPRANK: Systems for Lexical Simplification Ranking
</title>
<author confidence="0.99936">
Ravi Sinha
</author>
<affiliation confidence="0.99972">
University of North Texas
</affiliation>
<address confidence="0.866666666666667">
1155 Union Circle #311277
Denton, Texas
76203-5017
</address>
<email confidence="0.998719">
RaviSinha@my.unt.edu
</email>
<sectionHeader confidence="0.995628" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999731666666667">
This paper presents three systems that took
part in the lexical simplification task at SE-
MEVAL 2012. Speculating on what the con-
cept of simplicity might mean for a word,
the systems apply different approaches to rank
the given candidate lists. One of the systems
performs second-best (statistically significant)
and another one performs third-best out of 9
systems and 3 baselines. Notably, the third-
best system is very close to the second-best,
and at the same time much more resource-light
in comparison.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998940810810811">
Lexical simplification (described in (Specia et al.,
2012)) is a newer problem that has arisen follow-
ing a recent surge in interest in the related task of
lexical substitution (McCarthy et al., 2007). While
lexical substitution aims at making systems generate
suitable paraphrases for a target word in an instance,
which do not necessarily have to be simpler versions
of the original, it has been speculated that one pos-
sible use of the task could be lexical simplification,
in particular in the realm of making educational text
more readable for non-native speakers.
The task of lexical simplification, which thus
derives from lexical substitution, uses the same
data set, and has been introduced at the 6th
International Workshop on Semantic Evaluation
(SEMEVAL 2012), in conjunction with the First Joint
Conference on Lexical and Computational Seman-
tics (*SEM 2012). Instead of asking systems to pro-
vide substitutes, the task provides the systems with
all substitutes and asks them to be ranked.
The task provides several instances of triplets of a
context C, a target word T, and a set of gold stan-
dard substitutes S. The systems are supposed to
rank the substitutes si E S from the simplest to the
most difficult, and match their predictions against
the provided human annotations. The organizers de-
fine simple loosely as words that can be understood
by a wide variety of people, regardless of their lit-
eracy and cognitive levels, age, and regional back-
grounds.
The task is novel in that so far most work has been
done on syntactic simplification and not on lexical
simplification. Carroll et. al. (Carroll et al., 1998)
seem to have pioneered some methodology and eval-
uation metrics in this field. Yatskar et. al. (Yatskar et
al., 2010) use an unsupervised learning method and
metadata from the Simple English Wikipedia.
</bodyText>
<sectionHeader confidence="0.988865" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.999552545454545">
The data (trial and test, no training) have been
adopted from the original lexical substitution
task (McCarthy et al., 2007). The trial set has 300
examples, each with a context, a target word, and
a set of substitutions. The test set has 1710 exam-
ples. The organizers provide a scorer for the task,
the trial gold standard rankings, and three baselines.
The data is provided in XML format, with tags iden-
tifying the lemmas, parts of speech, instances, con-
texts and head words. The substitutions and gold
rankings are in plain text format.
</bodyText>
<page confidence="0.990718">
493
</page>
<note confidence="0.7145405">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 493–496,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.998589" genericHeader="method">
3 Resources
</sectionHeader>
<bodyText confidence="0.9997285">
Intuitively, a simple word is likely to have a high
frequency in a resource that is supposed to contain
simple words. Other factors that could intuitively in-
fluence simplicity would be the frequency in spoken
conversation, and whether the word is polysemous
or not. As such, the following resources have been
selected to contribute to the metric used in ranking
the substitutes.
</bodyText>
<subsectionHeader confidence="0.999465">
3.1 Simple English Wikipedia
</subsectionHeader>
<bodyText confidence="0.999027111111111">
Simple English Wikipedia has been used before in
simplicity analysis, as described in (Yatskar et al.,
2010). It is a publicly available, smaller Wikipedia
(298MB decompressed), which claims to only con-
sist of words that are somehow simple. For all the
substitute candidates, I count their frequencies of oc-
currence in this resource, and these counts serve as
a factor in computing the corresponding simplicity
scores (refer to Equation 1.)
</bodyText>
<subsectionHeader confidence="0.998972">
3.2 Transcribed Spoken English Corpus
</subsectionHeader>
<bodyText confidence="0.999977222222222">
A set of spoken dialogues is also utilized in this
project to measure simplicity. Spoken language in-
tuitively contains more conversational words, and
has the same kind of resolution power as the Sim-
ple English Wikipedia when it comes to the relative
simplicity of a word. Frequency counts of all the
substitute candidates in a set of dialogue corpora is
computed, and used as another factor in the Equa-
tions 1 and 3.
</bodyText>
<subsectionHeader confidence="0.996596">
3.3 WordIet
</subsectionHeader>
<bodyText confidence="0.999817692307692">
WordNet, as described in (Fellbaum, 1998), is a lex-
ical knowledge base that combines the properties of
a thesaurus with that of a semantic network. The ba-
sic entry in WordNet is a synset, which is defined as
a set of synonyms. I use WordNet 3.0, which has
over 150,000 unique words, over 110,000 synsets,
and over 200,000 word-sense pairs. For each substi-
tute, I extract the raw number of senses (for all parts
of speech possible) for that word present in Word-
Net. This count serves as yet another factor in the
proposed simplicity measure, under the hypothesis
that a simple word is used very frequently, and is
therefore polysemous.
</bodyText>
<subsectionHeader confidence="0.996451">
3.4 Web1T Google I-gram Corpus
</subsectionHeader>
<bodyText confidence="0.999988777777778">
The Google Web 1T corpus (Brants and Franz,
2006) is a collection of English N-grams, ranging
from one to five N-grams, and their respective fre-
quency counts observed on the Web. The corpus was
generated from approximately 1 trillion tokens of
words from the Web, predominantly English. This
corpus is also used in both SIMPRANK and SALSA
systems, with the intuition that simpler words will
have higher counts on the Web taken as a whole.
</bodyText>
<subsectionHeader confidence="0.706369">
3.5 SaLSA
</subsectionHeader>
<bodyText confidence="0.999759642857143">
SALSA (Stand-alone Lexical Substitution Ana-
lyzer) is an in-house application which accepts as in-
puts sentences with target words marked distinctly,
and then builds all possible 3-grams by substitut-
ing the target word with synonyms (and inflections
thereof). It then queries the Web1T corpus using an
in-house quick lookup application and gathers the
counts for all 3-grams. Finally, it sums the counts,
and assigns the aggregated scores to each corre-
sponding synonym and outputs a reverse-ranked list
of the synonyms. More detail about this method-
ology can be found in (Sinha and Mihalcea, 2009).
SALSA uses the exact same methodology described
in the paper, except that it is a stand-alone tool.
</bodyText>
<sectionHeader confidence="0.999278" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<bodyText confidence="0.997768888888889">
Figure 1 shows the general higher-level picture of
how the experiments have been performed. SIM-
PRANK uses five resources, including the unigram
frequency data, while SIMPRANKLIGHT does not
use the unigram frequencies.
I hypothesize that the simplicity of a word could
be represented as the Equation 1 (here cword() rep-
resents the frequency count of the word in a given
resource).
</bodyText>
<equation confidence="0.999114833333333">
simplicity(word) _
1
len(word)
+ cword(5impleWiki)
+ cword(Discourse) + cword(WordNet)
+ cword(Unigrams) (1)
</equation>
<bodyText confidence="0.96964">
This formula is very empirical in nature, in that it
has been found based on extensive experimentation
</bodyText>
<page confidence="0.997885">
494
</page>
<figureCaption confidence="0.9956">
Figure 1: High-level schematic diagram of the experi-
ments
</figureCaption>
<bodyText confidence="0.904020357142857">
(Table 1). It intuitively makes sense that a simple
word is supposed to have high frequency counts in
lexical resources that are meant to be simple by de-
sign. Formally,
bination found (experiment 8 in the table) is outlined
in Equation 1.
Note however, that the Google Web1T corpus is
expensive in terms of money, computation time and
storage space. Thus, another set of experiments was
performed (listed as experiments 1a in Table 1 leav-
ing the unigram counts out, and it was found to work
almost just as well. This system has been labeled
SIMPRANKLIGHT and uses the formula in Equa-
tion 3.
</bodyText>
<equation confidence="0.999197">
simplicity(word) =
1
len(word) + cword(SimpleWiki)
+ cword(Discourse) + cword(WordNet)
(3)
</equation>
<bodyText confidence="0.99882175">
The substitutes can then be sorted in the decreas-
ing order of simplicity scores. The substitute with
the highest simplicity score is hypothesized to be the
simplest.
</bodyText>
<tableCaption confidence="0.997572">
Table 1: Variants of the experiments performed
</tableCaption>
<figure confidence="0.8454905">
SN System components Method Remarks Score
simplicity(word)
a frequency(SimpleResource)
1
� (2)
length
</figure>
<bodyText confidence="0.999849071428571">
Here, SimpleResource could be any resource
that contains simple words. Apart from frequency
counts, we could possibly also leverage morphology
for finding simplicity. Intuitively, a 3-letter word or
a 4-letter word would most likely be simpler than a
word that has a longer length. This accounts for the
length factor in the equations.
As Table 1 depicts, a lot of experiments were per-
formed where the components (counts) were mul-
tiplied instead of being added, normalized instead
of adding without normalization1, and also experi-
ments where subsets of the resources were selected.
The scores obtained using the gold standard and the
trial data are also shown in the table. The best com-
</bodyText>
<footnote confidence="0.9902215">
1The normalization is done by dividing by the maximum
value obtained for that particular resource
</footnote>
<table confidence="0.897152875">
1 baseline no-change add normalize 0.05
1a baseline random add don’t normalize 0.01
baseline unigram count (Web1T) 0.39
len, simplewiki, discourse, wordnet 0.20
len, simplewiki, discourse, wordnet 0.37
2 len, simplewiki, discourse, wordnet add normalize, inc sort -0.20
3 len, simplewiki, discourse, wordnet multiply don’t normalize 0.25
4 simplewiki, discourse, wordnet add don’t normalize 0.36
</table>
<footnote confidence="0.700799722222222">
4a simplewiki, discourse, wordnet add normalize 0.22
4b simplewiki, discourse, wordnet multiply don’t normalize 0.26
5 len, simplewiki, wordnet add don’t normalize 0.36
5a len, simplewiki, wordnet add normalize 0.19
5b len, simplewiki, wordnet multiply don’t normalize 0.26
6 len, discourse, wordnet add don’t normalize 0.31
6a len, discourse, wordnet add normalize 0.20
6b len, discourse, wordnet multiply don’t normalize 0.25
7 len, simplewiki, discourse add don’t normalize 0.37
7a len, simplewiki, discourse add normalize 0.22
7b len, simplewiki, discourse multiply don’t normalize 0.32
8 len, simplewiki, discourse, word- add don’t normalize 0.39
net, unigrams
8a len, simplewiki, discourse, word- add normalize 0.22
net, unigrams
8b len, simplewiki, discourse, word- multiply don’t normalize 0.26
net, unigrams
9 SaLSA 0.36
</footnote>
<bodyText confidence="0.991197571428571">
Experiment 2 in Table 1 shows what happens
when an increasing-order ranking of the simplicity
scores is used. A negative score here underscores
the correctness of both the simplicity score as well
as that of the reverse-ranking.
The third system, SALSA (Stand-alone Lexical
Substitution Analyzer) is the only system out of the
</bodyText>
<page confidence="0.998271">
495
</page>
<bodyText confidence="0.999974875">
three that takes advantage of the context provided
with the data set. It builds all possible 3-grams from
the context, replacing the target word one-by-one by
a substitute candidate (and inflections of the substi-
tute candidates). It then sums their frequency counts
in the Web1T corpus and assigns the sum to the sim-
plicity score of a particular synonym. The synonyms
can then be reverse-ranked.
</bodyText>
<sectionHeader confidence="0.894155" genericHeader="method">
5 System Standings and Discussion
</sectionHeader>
<bodyText confidence="0.96659">
For the test data, Table 2 depicts the system stand-
ings, separated by statistical significance.
</bodyText>
<tableCaption confidence="0.99623">
Table 2: Test data system scores
</tableCaption>
<table confidence="0.981606846153846">
Rank Team ID System ID Score
1 WLV-SHEF SimpLex 0.496
2 baseline Sim Freq 0.471
2 UIT SimpRank 0.471
2 annlor simple 0.465
3 UIT SimpRankL 0.449
4 EMNLPCPH ORD1 0.405
5 EMNLPCPH ORD2 0.393
6 SB mmSystem 0.289
7 annlor lmbing 0.199
8 baseline No Change 0.106
9 baseline Rand 0.013
10 UNT SaLSA -0.082
</table>
<bodyText confidence="0.999732052631579">
Surprisingly, the systems SIMPRANK and SIM-
PRANKLIGHT, which do not use the contexts pro-
vided, score much better than SALSA, which does
use the contexts. Apparently simplicity is rather a
statistical concept even for humans (the annotators
for the gold standard) and not a contextual one. Also
surprisingly, SIMPRANKLIGHT, which does not use
Google Web1T data, performs extremely well and
within 0.02 of the raw scores.
What is also surprising is the inability of all-but-
one systems to beat the baseline of using simple fre-
quency counts from Web1T, which is in turn based
entirely on statistical counts and does not take the
context into account.
A major contribution of this paper is the discovery
that other, lighter, free resources work just as well
as the expensive (in money, time and space) Web1T
data when it comes to identifying which word is sim-
ple and which one is not.
</bodyText>
<sectionHeader confidence="0.999402" genericHeader="discussions">
6 Future Work
</sectionHeader>
<bodyText confidence="0.999923">
I plan to extend this experiment by performing ab-
lation studies of all the individual features, play-
ing with new features, and also performing machine
learning experiments to see if supervised experi-
ments are a better way of solving the problem of
lexical simplicity ranking.
</bodyText>
<sectionHeader confidence="0.99918" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999773">
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Version 1.
John Carroll, Guido Minnen, Yvonne Canning, Siobhan
Devlin, and John Tait. 1998. Practical simplification
of english newspaper text to assist aphasic readers. In
In Proc. ofAAAI-98 Workshop on Integrating Artificial
Intelligence and Assistive Technology, pages 7–10.
Christiane Fellbaum, editor. 1998. WordNet An Elec-
tronic Lexical Database. The MIT Press, Cambridge,
MA ; London, May.
Diana McCarthy, Falmer East Sussex, and Roberto Nav-
igli. 2007. Semeval-2007 task 10: English lexical
substitution task. In In Proceedings of the 4th work-
shop on Semantic Evaluations (SemEval-2007), pages
48–53.
Ravi Sinha and Rada Mihalcea. 2009. Combining lex-
ical resources for contextual synonym expansion. In
Proceedings of the International Conference RANLP-
2009, pages 404–410, Borovets, Bulgaria, September.
Association for Computational Linguistics.
Lucia Specia, Sujay K. Jauhar, and Rada Mihalcea.
2012. Semeval-2012 task 1: English lexical simplifi-
cation. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), Mon-
treal, Canada.
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of simplic-
ity: Unsupervised extraction of lexical simplifications
from Wikipedia. In Proceedings of the NAACL, pages
365–368.
</reference>
<page confidence="0.999126">
496
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.359015">
<title confidence="0.999815">Systems for Lexical Simplification Ranking</title>
<author confidence="0.998671">Ravi</author>
<affiliation confidence="0.999993">University of North</affiliation>
<address confidence="0.651615">1155 Union Circle</address>
<affiliation confidence="0.682509">Denton,</affiliation>
<email confidence="0.996526">RaviSinha@my.unt.edu</email>
<abstract confidence="0.998335307692308">This paper presents three systems that took in the lexical simplification task at Speculating on what the concept of simplicity might mean for a word, the systems apply different approaches to rank the given candidate lists. One of the systems performs second-best (statistically significant) and another one performs third-best out of 9 systems and 3 baselines. Notably, the thirdbest system is very close to the second-best, and at the same time much more resource-light in comparison.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<date>2006</date>
<booktitle>Web 1T 5-gram Version 1.</booktitle>
<contexts>
<context position="5287" citStr="Brants and Franz, 2006" startWordPosition="862" endWordPosition="865">perties of a thesaurus with that of a semantic network. The basic entry in WordNet is a synset, which is defined as a set of synonyms. I use WordNet 3.0, which has over 150,000 unique words, over 110,000 synsets, and over 200,000 word-sense pairs. For each substitute, I extract the raw number of senses (for all parts of speech possible) for that word present in WordNet. This count serves as yet another factor in the proposed simplicity measure, under the hypothesis that a simple word is used very frequently, and is therefore polysemous. 3.4 Web1T Google I-gram Corpus The Google Web 1T corpus (Brants and Franz, 2006) is a collection of English N-grams, ranging from one to five N-grams, and their respective frequency counts observed on the Web. The corpus was generated from approximately 1 trillion tokens of words from the Web, predominantly English. This corpus is also used in both SIMPRANK and SALSA systems, with the intuition that simpler words will have higher counts on the Web taken as a whole. 3.5 SaLSA SALSA (Stand-alone Lexical Substitution Analyzer) is an in-house application which accepts as inputs sentences with target words marked distinctly, and then builds all possible 3-grams by substituting</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram Version 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Guido Minnen</author>
<author>Yvonne Canning</author>
<author>Siobhan Devlin</author>
<author>John Tait</author>
</authors>
<title>Practical simplification of english newspaper text to assist aphasic readers. In</title>
<date>1998</date>
<booktitle>In Proc. ofAAAI-98 Workshop on Integrating Artificial Intelligence and Assistive Technology,</booktitle>
<pages>7--10</pages>
<contexts>
<context position="2322" citStr="Carroll et al., 1998" startWordPosition="370" endWordPosition="373">ask provides several instances of triplets of a context C, a target word T, and a set of gold standard substitutes S. The systems are supposed to rank the substitutes si E S from the simplest to the most difficult, and match their predictions against the provided human annotations. The organizers define simple loosely as words that can be understood by a wide variety of people, regardless of their literacy and cognitive levels, age, and regional backgrounds. The task is novel in that so far most work has been done on syntactic simplification and not on lexical simplification. Carroll et. al. (Carroll et al., 1998) seem to have pioneered some methodology and evaluation metrics in this field. Yatskar et. al. (Yatskar et al., 2010) use an unsupervised learning method and metadata from the Simple English Wikipedia. 2 Data The data (trial and test, no training) have been adopted from the original lexical substitution task (McCarthy et al., 2007). The trial set has 300 examples, each with a context, a target word, and a set of substitutions. The test set has 1710 examples. The organizers provide a scorer for the task, the trial gold standard rankings, and three baselines. The data is provided in XML format, </context>
</contexts>
<marker>Carroll, Minnen, Canning, Devlin, Tait, 1998</marker>
<rawString>John Carroll, Guido Minnen, Yvonne Canning, Siobhan Devlin, and John Tait. 1998. Practical simplification of english newspaper text to assist aphasic readers. In In Proc. ofAAAI-98 Workshop on Integrating Artificial Intelligence and Assistive Technology, pages 7–10.</rawString>
</citation>
<citation valid="true">
<title>WordNet An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA ; London,</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet An Electronic Lexical Database. The MIT Press, Cambridge, MA ; London, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Falmer East Sussex</author>
<author>Roberto Navigli</author>
</authors>
<title>Semeval-2007 task 10: English lexical substitution task. In</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>48--53</pages>
<contexts>
<context position="896" citStr="McCarthy et al., 2007" startWordPosition="134" endWordPosition="137">. Speculating on what the concept of simplicity might mean for a word, the systems apply different approaches to rank the given candidate lists. One of the systems performs second-best (statistically significant) and another one performs third-best out of 9 systems and 3 baselines. Notably, the thirdbest system is very close to the second-best, and at the same time much more resource-light in comparison. 1 Introduction Lexical simplification (described in (Specia et al., 2012)) is a newer problem that has arisen following a recent surge in interest in the related task of lexical substitution (McCarthy et al., 2007). While lexical substitution aims at making systems generate suitable paraphrases for a target word in an instance, which do not necessarily have to be simpler versions of the original, it has been speculated that one possible use of the task could be lexical simplification, in particular in the realm of making educational text more readable for non-native speakers. The task of lexical simplification, which thus derives from lexical substitution, uses the same data set, and has been introduced at the 6th International Workshop on Semantic Evaluation (SEMEVAL 2012), in conjunction with the Firs</context>
<context position="2655" citStr="McCarthy et al., 2007" startWordPosition="424" endWordPosition="427">t can be understood by a wide variety of people, regardless of their literacy and cognitive levels, age, and regional backgrounds. The task is novel in that so far most work has been done on syntactic simplification and not on lexical simplification. Carroll et. al. (Carroll et al., 1998) seem to have pioneered some methodology and evaluation metrics in this field. Yatskar et. al. (Yatskar et al., 2010) use an unsupervised learning method and metadata from the Simple English Wikipedia. 2 Data The data (trial and test, no training) have been adopted from the original lexical substitution task (McCarthy et al., 2007). The trial set has 300 examples, each with a context, a target word, and a set of substitutions. The test set has 1710 examples. The organizers provide a scorer for the task, the trial gold standard rankings, and three baselines. The data is provided in XML format, with tags identifying the lemmas, parts of speech, instances, contexts and head words. The substitutions and gold rankings are in plain text format. 493 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 493–496, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics 3 Resources</context>
</contexts>
<marker>McCarthy, Sussex, Navigli, 2007</marker>
<rawString>Diana McCarthy, Falmer East Sussex, and Roberto Navigli. 2007. Semeval-2007 task 10: English lexical substitution task. In In Proceedings of the 4th workshop on Semantic Evaluations (SemEval-2007), pages 48–53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ravi Sinha</author>
<author>Rada Mihalcea</author>
</authors>
<title>Combining lexical resources for contextual synonym expansion.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Conference RANLP2009,</booktitle>
<pages>404--410</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Borovets, Bulgaria,</location>
<contexts>
<context position="6282" citStr="Sinha and Mihalcea, 2009" startWordPosition="1024" endWordPosition="1027">b taken as a whole. 3.5 SaLSA SALSA (Stand-alone Lexical Substitution Analyzer) is an in-house application which accepts as inputs sentences with target words marked distinctly, and then builds all possible 3-grams by substituting the target word with synonyms (and inflections thereof). It then queries the Web1T corpus using an in-house quick lookup application and gathers the counts for all 3-grams. Finally, it sums the counts, and assigns the aggregated scores to each corresponding synonym and outputs a reverse-ranked list of the synonyms. More detail about this methodology can be found in (Sinha and Mihalcea, 2009). SALSA uses the exact same methodology described in the paper, except that it is a stand-alone tool. 4 Experimental Setup Figure 1 shows the general higher-level picture of how the experiments have been performed. SIMPRANK uses five resources, including the unigram frequency data, while SIMPRANKLIGHT does not use the unigram frequencies. I hypothesize that the simplicity of a word could be represented as the Equation 1 (here cword() represents the frequency count of the word in a given resource). simplicity(word) _ 1 len(word) + cword(5impleWiki) + cword(Discourse) + cword(WordNet) + cword(Un</context>
</contexts>
<marker>Sinha, Mihalcea, 2009</marker>
<rawString>Ravi Sinha and Rada Mihalcea. 2009. Combining lexical resources for contextual synonym expansion. In Proceedings of the International Conference RANLP2009, pages 404–410, Borovets, Bulgaria, September. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Sujay K Jauhar</author>
<author>Rada Mihalcea</author>
</authors>
<title>Semeval-2012 task 1: English lexical simplification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="755" citStr="Specia et al., 2012" startWordPosition="109" endWordPosition="112">6203-5017 RaviSinha@my.unt.edu Abstract This paper presents three systems that took part in the lexical simplification task at SEMEVAL 2012. Speculating on what the concept of simplicity might mean for a word, the systems apply different approaches to rank the given candidate lists. One of the systems performs second-best (statistically significant) and another one performs third-best out of 9 systems and 3 baselines. Notably, the thirdbest system is very close to the second-best, and at the same time much more resource-light in comparison. 1 Introduction Lexical simplification (described in (Specia et al., 2012)) is a newer problem that has arisen following a recent surge in interest in the related task of lexical substitution (McCarthy et al., 2007). While lexical substitution aims at making systems generate suitable paraphrases for a target word in an instance, which do not necessarily have to be simpler versions of the original, it has been speculated that one possible use of the task could be lexical simplification, in particular in the realm of making educational text more readable for non-native speakers. The task of lexical simplification, which thus derives from lexical substitution, uses the</context>
</contexts>
<marker>Specia, Jauhar, Mihalcea, 2012</marker>
<rawString>Lucia Specia, Sujay K. Jauhar, and Rada Mihalcea. 2012. Semeval-2012 task 1: English lexical simplification. In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Yatskar</author>
<author>Bo Pang</author>
<author>Cristian Danescu-NiculescuMizil</author>
<author>Lillian Lee</author>
</authors>
<title>For the sake of simplicity: Unsupervised extraction of lexical simplifications from Wikipedia.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL,</booktitle>
<pages>365--368</pages>
<contexts>
<context position="2439" citStr="Yatskar et al., 2010" startWordPosition="390" endWordPosition="393"> The systems are supposed to rank the substitutes si E S from the simplest to the most difficult, and match their predictions against the provided human annotations. The organizers define simple loosely as words that can be understood by a wide variety of people, regardless of their literacy and cognitive levels, age, and regional backgrounds. The task is novel in that so far most work has been done on syntactic simplification and not on lexical simplification. Carroll et. al. (Carroll et al., 1998) seem to have pioneered some methodology and evaluation metrics in this field. Yatskar et. al. (Yatskar et al., 2010) use an unsupervised learning method and metadata from the Simple English Wikipedia. 2 Data The data (trial and test, no training) have been adopted from the original lexical substitution task (McCarthy et al., 2007). The trial set has 300 examples, each with a context, a target word, and a set of substitutions. The test set has 1710 examples. The organizers provide a scorer for the task, the trial gold standard rankings, and three baselines. The data is provided in XML format, with tags identifying the lemmas, parts of speech, instances, contexts and head words. The substitutions and gold ran</context>
<context position="3772" citStr="Yatskar et al., 2010" startWordPosition="604" endWordPosition="607">493–496, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics 3 Resources Intuitively, a simple word is likely to have a high frequency in a resource that is supposed to contain simple words. Other factors that could intuitively influence simplicity would be the frequency in spoken conversation, and whether the word is polysemous or not. As such, the following resources have been selected to contribute to the metric used in ranking the substitutes. 3.1 Simple English Wikipedia Simple English Wikipedia has been used before in simplicity analysis, as described in (Yatskar et al., 2010). It is a publicly available, smaller Wikipedia (298MB decompressed), which claims to only consist of words that are somehow simple. For all the substitute candidates, I count their frequencies of occurrence in this resource, and these counts serve as a factor in computing the corresponding simplicity scores (refer to Equation 1.) 3.2 Transcribed Spoken English Corpus A set of spoken dialogues is also utilized in this project to measure simplicity. Spoken language intuitively contains more conversational words, and has the same kind of resolution power as the Simple English Wikipedia when it c</context>
</contexts>
<marker>Yatskar, Pang, Danescu-NiculescuMizil, Lee, 2010</marker>
<rawString>Mark Yatskar, Bo Pang, Cristian Danescu-NiculescuMizil, and Lillian Lee. 2010. For the sake of simplicity: Unsupervised extraction of lexical simplifications from Wikipedia. In Proceedings of the NAACL, pages 365–368.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>