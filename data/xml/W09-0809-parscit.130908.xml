<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.025371">
<title confidence="0.9869875">
Syntactic Reordering for English-Arabic
Phrase-Based Machine Translation
</title>
<author confidence="0.995777">
Jakob Elming Nizar Habash
</author>
<affiliation confidence="0.991477">
Languagelens Center for Computational Learning Systems
Copenhagen, Denmark Columbia University, New York, USA
</affiliation>
<email confidence="0.998195">
je@languagelens.com habash@ccls.columbia.edu
</email>
<sectionHeader confidence="0.994777" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999986454545455">
We investigate syntactic reordering within
an English to Arabic translation task. We
extend a pre-translation syntactic reorder-
ing approach developed on a close lan-
guage pair (English-Danish) to the dis-
tant language pair, English-Arabic. We
achieve significant improvements in trans-
lation quality over related approaches,
measured by manual as well as automatic
evaluations. These results prove the viabil-
ity of this approach for distant languages.
</bodyText>
<sectionHeader confidence="0.998428" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999984425531915">
The emergence of phrase-based statistical ma-
chine translation (PSMT) (Koehn et al., 2003a)
has been one of the major developments in statis-
tical approaches to translation. Allowing transla-
tion of word sequences (phrases) instead of single
words provides PSMT with a high degree of ro-
bustness in word selection and in local-word re-
ordering. Recent developments have shown that
improvements in PSMT quality are possible us-
ing syntax. One such development is the pre-
translation reordering approach, which adjusts the
source sentence to resemble target-language word
order prior to translation. This is typically done
using rules that are either manually created or
automatically learned from word-aligned parallel
corpora.
One particular variety of this approach, pro-
posed by Elming (2008), uses a large set of
linguistic features to automatically learn re-
ordering rules. The rules are applied non-
deterministically; however, phrase-internal word-
alignments are used to ensure that the intended re-
ordering does not come undone because of phrase
internal reordering (Elming, 2008). This approach
was shown to produce improved MT output on
English-Danish MT, a relatively closely-related
and similarly-structured language pair. In this
paper, we study whether this approach can be
extended to distant language pairs, specifically
English-to-Arabic. We achieve significant im-
provement in translation quality over related ap-
proaches, measured by manual as well as auto-
matic evaluations on this task. This proves the
viability of this approach on distant languages.
We also examined the effect of the alignment
method on learning reordering rules. Interestingly,
our experiments produced better translation using
rules learned from automatic alignments than us-
ing rules learned from manual alignments.
In the next section, we discuss and contrast re-
lated work. Section 3 describes aspects of English
and Arabic structure that are relevant to reorder-
ing. Section 4 describes the automatic induction
of reordering rules and its integration in PSMT. In
section 5, we describe the SMT system used in the
experiments. In section 6, we evaluate and discuss
the results of our English-Arabic MT system.
</bodyText>
<sectionHeader confidence="0.999856" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999711615384616">
Much work has been done in syntactic reorder-
ing for SMT, focusing on both source and target-
language syntax. In this paper, we adapt an ap-
proach that utilizes source-syntax information as
opposed to target-side syntax systems (Yamada
and Knight, 2001; Galley et al., 2004). This is
because we are translating from English to Arabic
and we are discouraged by recent results indicat-
ing Arabic parsing is not at a stage that makes it
usable in MT (Habash et al., 2006). While sev-
eral recent authors using a pre-translation (source-
side) reordering approach have achieved positive
results, it has been difficult to integrate syntactic
</bodyText>
<note confidence="0.9520465">
Proceedings of the EACL 2009 Workshop on Computational Approaches to Semitic Languages, pages 69–77,
Athens, Greece, 31 March, 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.999149">
69
</page>
<bodyText confidence="0.999802869565218">
information while retaining the strengths of the
statistical approach. In some studies, reordering
decisions are done “deterministically” by supply-
ing the decoder with a canonical word order (Xia
and McCord, 2004; Collins et al., 2005; Wang
et al., 2007; Habash, 2007). These reordering
rules are either manually specified or automati-
cally learned from alignments; and they are al-
ways placed outside the actual PSMT system. By
contrast, other studies (Crego and Mariño, 2007;
Zhang et al., 2007; Li et al., 2007; Elming, 2008)
are more in the spirit of PSMT, in that multi-
ple reorderings are presented to the PSMT sys-
tem as (possibly weighted) options that are al-
lowed to contribute alongside other parameters.
Specifically, we follow the pre-translation reorder-
ing approach of Elming (2008). This approach
has been proven to remedy shortcomings of other
pre-translation reordering approaches by reorder-
ing the input word sequence, but scoring the out-
put word sequence.
Elming (2008) only examined the approach
within English – Danish, a language pair that dis-
plays little reordering. By contrast, in this pa-
per, we target the more demanding reordering task
of translating between two distant languages, En-
glish and Arabic. While much work has been
done on Arabic to English MT (Habash and Sa-
dat, 2006; Lee, 2004) mostly focusing on ad-
dressing the problems caused by the rich mor-
phology of Arabic, we handle the less described
translation direction: English to Arabic. Recently,
there are some new publications on English to
Arabic MT. Sarikaya and Deng (2007) use joint
morphological-lexical language models to re-rank
the output of English dialectal-Arabic MT, and
Badr et al. (2008) report results on the value of
the morphological decomposition of Arabic dur-
ing training and describe different techniques for
re-composition of Arabic in the output. We differ
from the previous efforts targeting Arabic in that
(1) we do not address morphology issues through
segmentation (more on this in section 3) and (2)
we focus on utilizing syntactic knowledge to ad-
dress the reordering challenges of this translation
direction.
</bodyText>
<sectionHeader confidence="0.955228" genericHeader="method">
3 Arabic Syntactic Issues
</sectionHeader>
<bodyText confidence="0.999936923076923">
Arabic is a morphologically and syntactically
complex language with many differences from En-
glish. Arabic morphology has been well studied
in the context of MT. Previous results all sug-
gest that some degree of tokenization is helpful
when translating from Arabic (Habash and Sa-
dat, 2006; Lee, 2004). However, when trans-
lating into a morphologically rich language, tar-
get tokenization means that the translation process
is broken into multiple steps (Badr et al., 2008).
For our experiments, Arabic was not segmented
apart from simple punctuation tokenization. This
low level of segmentation was maintained in or-
der to agree with the segmentation provided in
the manually aligned corpus we used to learn our
rules (section 6.1). We found no simple means for
transferring the manual alignments to more seg-
mented language. We expect that better perfor-
mance would be achieved by introducing more
Arabic segmentation as reported by Badr et al.
(2008).1 As such, and unlike previous work in
PSMT translating into Arabic, we focus here on
syntax. We plan to investigate different tokeniza-
tion schemes for syntactic preprocessing in future
work. Next, we describe three prominent English-
Arabic syntactic phenomena that have motivated
some of our decisions in this paper.
First is verb-subject order. Arabic verb subjects
may be: (a.) pro-dropped (verb conjugated), (b.)
pre-verbal (SVO), or (c.) post-verbal (VSO). Al-
though the English SVO order is possible in Ara-
bic, it is not always preferred, especially when the
subject is particularly long. Unfortunately, this is
the harder case for PSMT to handle. For small
subject noun phrases (NP), PSMT might be able
to handle the reordering in the phrase table if the
verb and subject were seen in training. But this be-
comes much less likely with very long NPs that ex-
ceed the size of the phrases in a phrase table. The
example in Figure 1 illustrates this point. Bolding
and italics are used to mark the verb and subor-
dinating conjunction that surround the subject NP
(19 tokens) in English and what they map to in
Arabic, respectively.2
Secondly, Arabic adjectival modifiers typically
follow their nouns with the exception of some su-
perlative adjectives. However, English adjectival
modifiers can follow or precede their nouns de-
pending on the size of the adjectival phrase: single
word adjectives precede but multi-word adjectives
phrases follow (or precede while hyphenated). For
example, a tall man translates as J��gb J_, rjl
</bodyText>
<footnote confidence="0.99972175">
1Our results are not comparable to their results, since they
report on non-standard data sets.
2All Arabic transliterations in this paper are provided in
the Habash-Soudi-Buckwalter scheme (Habash et al., 2007).
</footnote>
<page confidence="0.997691">
70
</page>
<bodyText confidence="0.5866435">
[NP-SBJ The general coordinator of the railroad project among the countries of the Gulf Coopera-
tion Council, Hamid Khaja ,] [V announced] [SUB that ...]
</bodyText>
<figure confidence="0.9234192">
[ék. A g YÓAg ú
j. J�Ê�mÌ@ vðAªJ@ •Êm.× ÈðX YK�YmÌ @ aº‚Ë@ ¨ðQå�„ÖÏ ÐAªË@ 3‚:1ÖÏ@ NP-SBJ] [ �áÊ«@ V]
[... v@ SUB]
[V Aςln] [NP-SBJ Almnsq AlςAm lmšrwς Alskh AlHdyd byn dwl mjls AltςAwn Alxlyjy HAmd
xAjh] [SUB An ...]
</figure>
<figureCaption confidence="0.991226">
Figure 1: An example of long distance reordering of English SVO order to Arabic VSO order
</figureCaption>
<equation confidence="0.9955309">
� � � � � � �
■
� � � �
�
�
■
� � � � �
�
� � � � � � �
� � � � � � �
� � � � � � �
� � � � � � �
s1 s2 s3 s4 s5 sg s7
t7
tg
t5
t4
t3
t2
t1
</equation>
<figureCaption confidence="0.9660565">
Figure 2: Abstract alignment matrix example of
reordering.
</figureCaption>
<bodyText confidence="0.9976962">
Twyl ‘man tall’; however, the English phrase a
man tall of stature translates with no reordering as
a ÓA&amp;quot;®Ë@ ÉK�ñ£ Ég. P rjl Twyl AlqAmli ‘man tall the-
stature’. So does the superlative the tallest man
translating into Ég. P Èñ£@ ATwl rjl ‘tallest man.’
Finally, Arabic has one syntactic construction,
called Idafa, for indicating possession and com-
pounding, while English has three. The Idafa con-
struction typically consists of one or more indef-
inite nouns followed by a definite noun. For ex-
ample, the English phrases the car keys, the car’s
keys and the keys of the car all translate into the
Arabic oPAJ�‚Ë@ iJ��KA�®Ó mfAtyH AlsyArli ‘keys the-
car.’ Only one of the three English constructions
does not require content word reordering.
</bodyText>
<sectionHeader confidence="0.993597" genericHeader="method">
4 Reordering rules
</sectionHeader>
<subsectionHeader confidence="0.999906">
4.1 Definition of reordering
</subsectionHeader>
<bodyText confidence="0.982450411764706">
Following Elming (2008), we define reordering as
two word sequences, left sequence (LS) and right
sequence (RS), exchanging positions. These two
sequences are restricted by being parallel consecu-
tive, maximal and adjacent. The sequences are not
restricted in length, making both short and long
distance reordering possible. Furthermore, they
need not be phrases in the sense that they appear
as an entry in the phrase table.
Figure 2 illustrates reordering in a word align-
ment matrix. The matrix contains reorderings be-
tween the light grey sequences (s32 and s64)3 and
3Notation: s&apos;Y means the consecutive source sequence
the dark grey sequences (s5 5 and s66). On the other
hand, the sequences s3 3 and s54 are not considered
for reordering, since neither one is maximal, and
s5 4 is not consecutive on the target side.
</bodyText>
<subsectionHeader confidence="0.998525">
4.2 Learning rules
</subsectionHeader>
<bodyText confidence="0.998274027777778">
Table 1 contains an example of the features avail-
able to the algorithm learning reordering rules.
We include features for the candidate reorder-
ing sequences (LS and RS) and for their possi-
ble left (LC) and right (RC) contexts. In addi-
tion to words and parts-of-speech (POS), we pro-
vide phrase structure (PS) sequences and subordi-
nation information (SUBORD). The PS sequence
is made up of the highest level nodes in the syntax
tree that cover the words of the current sequence
and only these. Subordinate information can also
be extracted from the syntax tree. A subordinate
clause is defined as inside an SBAR constituent;
otherwise it is a main clause. Our intuition is that
all these features will allow us to learn the best
rules possible to address the phenomena discussed
in section 3 at the right level of generality.
In order to minimize the amount of training
data, word and POS sequences are annotated as
too long (T/L) if they are longer than 4 words,
and the same for phrase structure (PS) sequences
if they are longer than 3 units. A feature vector
is only used if at least one of these three levels is
not T/L for both LS and RS, and T/L contexts are
not included in the set. This does not constrain
the possible length of a reordering, since a PS se-
quence of length 1 can cover an entire sentence.
In the example in Table 1, LS and RS are single
words, but they are not restricted in length. The
span of the contexts varies from a single neighbor-
ing word to all the way to the sentence border. In
the example, LS and RS should be reordered, since
adjectives appear as post-modifiers in Arabic.
In order to learn rules from the annotated data,
we use a rule-based classifier, Ripper (Cohen,
covering word positions x to y.
</bodyText>
<page confidence="0.996742">
71
</page>
<table confidence="0.9713866">
Level LC LS RS RC
WORD &lt;s&gt; he bought  ||he bought  ||bought new books today  ||today.  ||today. &lt; /s&gt;
POS &lt;S&gt; NN VBD  ||NN VBD  ||VBD JJ NNS NN  ||NN .  ||NN . &lt; /S&gt;
PS &lt;S&gt; NP VBD  ||NP VBD  ||VBD JJ NNS NP  ||NP .  ||NP . &lt; /S&gt;
SUBORD MAIN MAIN MAIN MAIN
</table>
<tableCaption confidence="0.995982">
Table 1: Example of features for rule-learning. Possible contexts separated by ||.
</tableCaption>
<figureCaption confidence="0.924686">
Figure 3: Example word lattice.
</figureCaption>
<bodyText confidence="0.99995">
1996). The motivation for using Ripper is that it
allows features to be sets of strings, which fits well
with our representation of the context, and it pro-
duces easily readable rules that allow better under-
standing of the decisions being made. In section
6.3, extracted rules are exemplified and analyzed.
The probabilities of the rules are estimated us-
ing Maximum Likelihood Estimation based on
the information supplied by Ripper on the perfor-
mance of the individual rules on the training data.
These logarithmic probabilities are easily integrat-
able in the log-linear PSMT model as an additional
parameter by simple addition.
</bodyText>
<sectionHeader confidence="0.986501" genericHeader="method">
5 The PSMT system
</sectionHeader>
<bodyText confidence="0.999884171428572">
Our baseline is the PSMT system used for the
2006 NAACL SMT workshop (Koehn and Monz,
2006) with phrase length 3 and a trigram language
model (Stolcke, 2002). The decoder used for the
baseline system is Pharaoh (Koehn, 2004) with
its distance-penalizing reordering model. Since
Pharaoh does not support word lattice input, we
use our own decoder for the experiments. Ex-
cept for the reordering model, it uses the same
knowledge sources as Pharaoh, i.e. a bidirectional
phrase translation model, a lexical weight model,
phrase and word penalties, and a target language
model. Its behavior is comparable to Pharaoh
when doing monotone decoding.
The search algorithm of our decoder is similar
to the RG graph decoder of (Zens et al., 2002). It
expects a word lattice as input. Figure 3 shows
the word lattice for the example in table 2. In the
example used here, we choose to focus on the re-
ordering of adjective and noun. For readability,
we do not describe the possibility of reordering the
subject and verb. This will also be the case in later
use of the example.
Since the input format defines all possible word
orders allowed by the rule set, a simple monotone
search is sufficient. Using a language model of or-
der n, for each hypothesized target string ending
in the same n-1-gram, we only have to extend the
highest scoring hypothesis. None of the others can
possibly outperform this one later on. This is be-
cause the maximal context evaluating a phrase ex-
tending this hypothesis, is the history (n-1-gram)
of the first word of that phrase. The decoder is
not able to look any further back at the preceding
string.
</bodyText>
<subsectionHeader confidence="0.988887">
5.1 The reordering approach
</subsectionHeader>
<bodyText confidence="0.996326666666667">
Similar to Elming (2008), the integration of the
rule-based reordering in our PSMT system is car-
ried out in two separate stages:
</bodyText>
<listItem confidence="0.99674975">
1. Reordering the source sentence to assimilate
the word order of the target language.
2. Weighting of the target word order according
to the rules.
</listItem>
<bodyText confidence="0.9998305">
Stage (1) is done in a non-deterministic fashion
by generating a word lattice as input. This way, the
system has both the original word order, and the
reorderings predicted by the rule set. The different
paths of the word lattice are merely given as equal
suggestions to the decoder. They are in no way
individually weighted.
Separating stage (2) from stage (1) is motivated
by the fact that reordering can have two distinct
origins. They can occur because of stage (1), i.e.
the lattice reordering of the original English word
order (phrase external reordering), and they can
occur inside a single phrase (phrase internal re-
ordering). The focus of this approach lies in do-
ing phrase-independent word reordering. Rule-
predicted reorderings should be promoted regard-
less of whether they owe their existence to a syn-
tactic rule or a phrase table entry.
This is accomplished by letting the actual scor-
ing of the reordering focus on the target string.
</bodyText>
<page confidence="0.99142">
72
</page>
<table confidence="0.9997964">
Source: he1 bought2 new3 books4 today5
Rule: 3 4 —* 4 3
Hypothesis Target string Alignment
H1 Aštr jdydh ktbA 1+2 3 4
H2 Aštr ktbA jdydh 1+2 4 3
</table>
<tableCaption confidence="0.966626">
Table 2: Example of the scoring approach during
decoding at source word 4.
</tableCaption>
<bodyText confidence="0.999955591836735">
The decoder is informed of where a rule has pre-
dicted a reordering, how much it costs to do the
reordering, and how much it costs to avoid it. This
is then checked for each hypothesized target string
via a word alignment.
The word alignment keeps track of which
source position the word in each target position
originates from. In order to access this informa-
tion, each phrase table entry is annotated with its
internal word alignment, which is available as an
intermediate product from phrase table creation.
If a phrase pair has multiple word alignments, the
most frequent one is chosen.
Table 2 exemplifies the scoring approach, again
with focus on the adjective-noun reordering. The
source sentence is ‘he bought new books today’,
and a rule has predicted that source word 3 and
4 should change place. Due to the pro-drop na-
ture of Arabic, the first Arabic word is linked to
the two first English words (1+2). When the de-
coder has covered the first four input words, two
of the hypothesis target strings might be H1 and
H2. At this point, it becomes apparent that H2
contains the desired reordering (namely what cor-
responds to source word order ‘4 3’), and it get
assigned the reordering cost. H1 does not contain
the rule-suggested reordering (instead, the words
are in the original order ‘3 4’), and it gets the vi-
olation cost. Both these scorings are performed
in a phrase-independent manner. The decoder as-
signs the reordering cost to H2 without knowing
whether the reordering is internal (due to a phrase
table entry) or external (due to a syntactic rule).
Phrase internal reorderings at other points of the
sentence, i.e. points that are not covered by a rule,
are not judged by the reordering model. Our rule
extraction does not learn every possible reorder-
ing between the two languages, but only the most
general ones. If no rule has an opinion at a certain
point in a sentence, the decoder is free to choose
the phrase translation it prefers without reordering
cost.
Separating the scoring from the source lan-
guage reordering also has the advantage that the
approach in essence is compatible with other
approaches such as a traditional PSMT system
(Koehn et al., 2003b) or a hierarchical phrase sys-
tem (Chiang, 2005). We will, however, not exam-
ine this possibility further in the present paper.
</bodyText>
<sectionHeader confidence="0.998008" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.982607">
6.1 Data
</subsectionHeader>
<bodyText confidence="0.999882871794872">
We learn the reordering rules from the IBM
Arabic-English aligned corpus (IBMAC) (Itty-
cheriah and Roukos, 2005). Of its total 13.9K sen-
tence pairs, we only use 8.8K sentences because
the rest of the corpus uses different normalizations
for numerals that make the two sets incompatible.
6.6K of the sentences (179K English and 146K
Arabic words) are used to learn rule, while the rest
are used for development purposes. In addition to
the manual alignment supplied with these data, we
create an automatic word alignment for them using
GIZA++ (Och and Ney, 2003) and the grow-diag-
final (GDF) symmetrization algorithm (Koehn et
al., 2005). This was done together with the data
used to train the MT system. The English side
is parsed using a state-of-the-art statistical English
parser (Charniak, 2000). Two rule sets are learned
based on the manual alignments (MAN) and the
automatic alignments (GDF).
The MT system is trained on a corpus con-
sisting of 126K sentences with 4.2M English
and 3.3M Arabic words in simple tokeniza-
tion scheme. The domain is newswire (LDC-
NEWS) taken from Arabic News (LDC2004T17),
eTIRR (LDC2004E72), English translation of
Arabic Treebank (LDC2005E46), and Ummah
(LDC2004T18). Although there are additional
corpora available, we restricted ourselves to this
set to allow for a fast development cycle. We plan
to extend the data size in the future. The Ara-
bic language model is trained on the 5.4M sen-
tences (133M words) of newswire text in the 1994
to 1996 part of the Arabic Gigaword corpus. We
restricted ourselves to this part, since we are not
able to run Pharaoh with a larger language model.4
For test data, we used NIST MTEval test sets
from 2004 (MT04) and 2005 (MT05)5. Since
these data sets are created for Arabic-English eval-
uation with four English reference sentences for
</bodyText>
<footnote confidence="0.999919666666667">
4All of the training data we use is available from the Lin-
guistic Data Consortium (LDC): http://www.ldc.upenn.edu/.
5 http://www.nist.gov/speech/tests/mt/
</footnote>
<page confidence="0.997305">
73
</page>
<table confidence="0.9998782">
System Dev MT04 MT05
Pharaoh Free 28.37 23.53 24.79
Pharaoh DL4 29.52 24.72 25.88
Pharaoh Monotone 27.93 23.55 24.72
MAN NO weight 29.53 24.72 25.82
SO weight 29.43 24.74 25.82
TO weight 29.40 24.78 25.93
GDF NO weight 29.87 25.11 26.04
SO weight 29.84 25.06 26.01
TO weight 29.95 25.17 26.09
</table>
<tableCaption confidence="0.997787">
Table 3: Automatic evaluation scores for different
</tableCaption>
<bodyText confidence="0.991839384615385">
systems using rules extracted from manual align-
ments (MAN) and automatic alignments (GDF).
The TO system using GDF rules is significantly
better than the light grey cells at a 95% confidence
level (Zhang et al., 2004).
each Arabic sentence, we invert the sets by con-
catenating all English sentences to one file. This
means that the Arabic reference file contains four
duplicates of each sentence. Each duplicate is the
reference of a different English source sentence.
Following this merger, MT04 consists of 5.4K
sentences with 193K English and 144K Arabic
words, and MT05 consists of 4.2K sentences with
143K English and 114K Arabic words. MT04 is
a mix of domains containing speeches, editorials
and newswire texts. On the other hand, MT05 is
only newswire.
The NIST MTEval test set from 2002 (MT02)
is split into a tuning set for optimizing decoder pa-
rameter weights and a development set for ongo-
ing experimentation. The same merging procedure
as for MT04 and MT05 is employed. This results
in a tune set of 1.0K sentences with 34K English
and 26K Arabic words, and a development set of
3.1K sentences with 102K English and 79K Ara-
bic words.
</bodyText>
<subsectionHeader confidence="0.970238">
6.2 Results and discussion
</subsectionHeader>
<bodyText confidence="0.9998254">
The reordering approach is evaluated on the MT04
and MT05 test sets. Results are listed in table 3
along with results on the development set. We re-
port on (a) Pharaoh with no restriction on reorder-
ing (Pharaoh Free), (b) Pharaoh with distortion
limit 4 (Pharaoh DL4), (c) Pharaoh with monotone
decoding (Pharaoh Monotone), and (d) a system
provided with a rule reordered word lattice but no
(NO) weighting in the spirit of (Crego and Mariño,
2007), (e) the same system but with a source order
</bodyText>
<table confidence="0.9994865">
System MT04 MT05 Avr. human
Pharaoh Free 24.07 25.15 3.0 (2.80)
Pharaoh DL4 25.42 26.51 —
NO scoring 25.68 26.29 2.5 (2.43)
SO scoring 25.42 26.02 2.5 (2.64)
TO scoring 25.98 26.49 2.0 (2.08)
</table>
<tableCaption confidence="0.999052">
Table 4: Evaluation on the diff set. Average hu-
</tableCaption>
<bodyText confidence="0.919394272727273">
man ratings are medians with means in parenthe-
sis, lower scores are better, 1 is the best score.
(SO) weighting in the spirit of (Zhang et al., 2007;
Li et al., 2007), and finally (f) the same system but
with the target order (TO) weighting.
In addition to evaluating the reordering ap-
proaches, we also report on supplying them with
different reordering rule sets: a set that was
learned on manually aligned data (MAN), and a
set learned on the same data but with automatic
alignments (GDF).
</bodyText>
<subsectionHeader confidence="0.923942">
6.2.1 Overall Results
</subsectionHeader>
<bodyText confidence="0.999984388888889">
Pharaoh Monotone performs similarly to Pharaoh
Free. This shows that the question of improved
reordering is not about quantity, but rather qual-
ity: what constraints are optimal to generate the
best word order. The TO approach gets an increase
over Pharaoh Free of 1.3 and 1.6 %BLEU on the
test sets, and 0.2 and 0.5 %BLEU over Pharaoh
DL4.
Improvement is less noticeable over the other
pre-translation reordering approaches (NO and
SO). A possible explanation is that the rules do not
apply very often, in combination with the fact that
the approaches often behave alike. The difference
in SO and TO scoring only leads to a difference
in translation in ti14% of the sentences. This set,
the diff set, is interesting, since it provides a focus
on the difference between these approaches. In ta-
ble 4, we evaluate on this set.
</bodyText>
<subsectionHeader confidence="0.90596">
6.2.2 Diff Set
</subsectionHeader>
<bodyText confidence="0.9999266">
Overall the TO approach seems to be a superior
reordering method. To back this observation, 50
sentences of MT04 are manually evaluated by a
native speaker of Arabic. Callison-Burch et al.
(2007) show that ranking sentences gives higher
inter-annotator agreement than scoring adequacy
and fluency. We therefore employ this evaluation
method, asking the evaluator to rank sentences
from four of the systems given the input sentence.
Ties are allowed. Table 4 shows the average rat-
</bodyText>
<page confidence="0.997465">
74
</page>
<table confidence="0.999470285714286">
Decoder choice NO SO TO
MT04 Phrase internal 20.7 0.6 21.2
Phrase external 30.1 43.0 33.1
Reject 49.2 56.5 45.7
MT05 Phrase internal 21.3 0.7 21.6
Phrase external 29.5 42.9 31.8
Reject 49.2 56.4 46.5
</table>
<tableCaption confidence="0.977145">
Table 5: The reordering choices made based on
</tableCaption>
<bodyText confidence="0.987750714285714">
the three pre-translation reordering approaches for
the 20852 and 17195 reorderings proposed by the
rules for the MT04 and MT05 test sets. Measured
in %.
ings of the systems. This shows the TO scoring
to be significantly superior to the other methods
(p &lt; 0.01 using Wilcoxon signed-rank testing).
</bodyText>
<subsectionHeader confidence="0.916277">
6.2.3 MAN vs GDF
</subsectionHeader>
<bodyText confidence="0.999974260869565">
Another interesting observation is that reordering
rules learned from automatic alignments lead to
significantly better translation than rules learned
from manual alignment. Due to the much higher
quality of the manual alignment, the opposite
might be expected. However, this may be just
a variant on the observation that alignment im-
provements (measured against human references)
seldom lead to MT improvements (Lopez and
Resnik, 2006). The MAN alignments may in fact
be better than GDF, but they are most certainly
more different in nature from real alignment than
the GDF alignments are. As such, the MAN align-
ments are not as powerful as we would have liked
them to be. In our data sets, the GDF rules, seem
less specific, and they therefore apply more fre-
quently than the MAN rules. On average, this re-
sults in more than 7 times as many possible re-
ordering paths per sentence. This means that the
GDF rules supply the decoder with a larger search
space, which in turn means more proposed trans-
lation hypotheses. This may play a big part in the
effect of the rule sets.
</bodyText>
<subsectionHeader confidence="0.931663">
6.2.4 Reordering Choices
</subsectionHeader>
<bodyText confidence="0.999975266666667">
Table 5 shows the reordering choices made by the
approaches in decoding. Most noticeable is that
the SO approach is strongly biased against phrase
internal reorderings; TO uses more than 30 times
as many phrase internal reorderings as SO. In ad-
dition, TO is less likely to reject a rule proposed
reordering.
The 50 sentences from the manual evaluation
are also manually analyzed with regards to re-
ordering. For each reordering in these sentences,
the four systems are ranked according to how well
the area affected by the reordering is translated.
This indicates that the SO approach’s bias against
phrase internal reorderings may hurt performance.
25% of the time, when SO chooses an external re-
ordering, while the TO approach chooses an in-
ternal reordering, the TO approach gets a better
translation. Only in 7% of the cases is it the other
way around.
Another discovery from the analysis is when TO
chooses an internal reordering and NO rejects the
reordering. Here, TO leads to a better translation
45% of the time, while NO never outperforms TO.
In these cases, either approach has used a phrase
to cover the area, but via rule-based motivation,
TO has forced a less likely phrase with the correct
word order through. This clearly shows that lo-
cal reordering is not handled sufficiently by phrase
internal reordering alone. These need to be con-
trolled too.
</bodyText>
<subsectionHeader confidence="0.999965">
6.3 Rule analysis
</subsectionHeader>
<bodyText confidence="0.999954071428572">
The rule learning resulted in 61 rules based on
manual alignments and 39 based on automatic
alignments. Of these, the majority handled the
placement of adjectives, while only a few handled
the placement of the verb.
A few of the rules that were learned from the
manual alignment are shown in table 6. The first
two rules handle the placement of the finite verb
in Arabic. Rule 16 states that if a finite verb
appears in front of a subordinate clause, then it
should be moved to sentence initial position with
a probability of 68%. Due to the restrictions of
sequence lengths, it can only swap across maxi-
mally 4 words or a sequence of words that is de-
scribable by maximally 3 syntactic phrases. The
SBAR condition may help restrict the reordering
to finite verbs of the main clause. This rule and its
probability goes well with the description given in
sections 3, since VSO order is not obligatory. The
subject may be unexpressed, or it may appear in
front of the verb. This is even more obvious in
rule 27, which has a probability of only 43%.
Rules 11 and 1 deal with the inverse ordering of
adjectives and nouns. The first is general but un-
certain, the second is lexicalized and certain. The
reason for the low probability of rule 11 is primar-
ily that many proper names have been mis-tagged
by the parser as either JJ or NN, and to a lesser
</bodyText>
<page confidence="0.996518">
75
</page>
<note confidence="0.904114285714286">
No LC LS RS RC Prob.
16 WORD: &lt;s&gt; POS: FVF PS: SBAR 68%
27 WORD: &lt;s&gt; PS: NP POS: FVF 43%
11 POS: IN POS: JJ POS: NN 46%
1 ! POS: JJ POS: JJ WORD: president 90%
37 ! POS: NN POS: NN POS: NNS POS: IN 71%
! POS: JJ
</note>
<tableCaption confidence="0.997567">
Table 6: Example rules. ! specifies negative conditions.
</tableCaption>
<bodyText confidence="0.990919942857143">
extent that the rule should often not apply if the
right context is also an NN. Adding the latter re-
striction narrows the scope of the rule but would
have increased the probability to 54%.
Rule 1, on the other hand, has a high proba-
bility of 90%. It is only restricted by the con-
dition that the left context should not be an ad-
jective. In these cases, the adjectives should of-
ten be moved together, as is the case with ‘the
south african president’ —* ù� �®K�Q�¯@ H. ñ�Jm.Ì @ •��+QË@
Alryys Aljnwb Afryqy where ‘south african’ is
moved to the right of ‘president’.
Finally, rule 37 handles compound nouns. Here
a singular noun is moved to the right of a plural
noun, if the right context is a preposition, and the
left context is neither an adjective nor a singular
noun. This rule handles compound nouns, where
the modifying function of the first noun often is
hard to distinguish from that of an adjective. The
left context restrictions server the same purpose as
the left context in rule 1; these should often be
moved together with the singular noun. The func-
tion of the right context is harder to explain, but
without this restriction, the rule would have been
much less successful; dropping from a probability
of 71% to 51%.
An overall comparison of the rules produced
based on the manual and automatic alignments
shows no major difference in quality. This is espe-
cially interesting in light of the better translation
using the GDF rules. It is also very interesting
that it seems possible to get as good rules from the
GDF as from the MAN alignments. This is a new
result compared to Elming (2008), where results
on manual alignments only are reported.
</bodyText>
<sectionHeader confidence="0.993448" genericHeader="conclusions">
7 Conclusion and Future Plans
</sectionHeader>
<bodyText confidence="0.999944431818182">
We have explored the syntactic reordering ap-
proach previously presented in (Elming, 2008)
within a more distant language pair, English-
Arabic. A translation direction that is highly
under-represented in MT research, compared to
the opposite direction. We achieve significant im-
provement in translation quality over related ap-
proaches, measured by manual as well as auto-
matic evaluations on this task. Thus proving the
viability of the approach on distant languages.
We also examined the effect of the alignment
method on learning reordering rules. Interestingly,
our experiments produced better translation using
rules learned from automatic alignments than us-
ing rules learned from manual alignments. This is
an aspect we want to explore further in the future.
In future work, we would also like to address
the morphological complexity of Arabic together
with syntax. We plan to consider different seg-
mentations for Arabic and study their interaction
with translation and syntactic reordering.
An important aspect of the TO approach is that
it uses phrase internal alignments during transla-
tion. In the future, we wish to examine the effect
their quality has on translation. We are also inter-
ested in examining the approach within a standard
phrase-based decoder such as Moses (Koehn et al.,
2003b) or a hierarchical phrase system (Chiang,
2005).
The idea of training on reordered source lan-
guage is often connected with pre-translation re-
ordering. The present approach does not em-
ploy this strategy, since this is no trivial matter
in a non-deterministic, weighted approach. Zhang
et al. (2007) proposed an approach that builds
on unfolding alignments. This is not an opti-
mal solution, since this may not reflect their rules.
Training on both original and reordered data may
strengthen the approach, but it would not remedy
the problems of the SO approach, since it would
still be ignorant of the internal reorderings of a
phrase. Nevertheless, it may strengthen the TO
approach even further. We also wish to examine
this in future work.
</bodyText>
<page confidence="0.978747">
76
</page>
<sectionHeader confidence="0.983518" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999931991379311">
I. Badr, R. Zbib, and J. Glass. 2008. Segmentation for
English-to-Arabic statistical machine translation. In
Proceedings ofACL’08: HLT, Short Papers, Colum-
bus, OH, USA.
C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz, and
J. Schroeder. 2007. (Meta-) evaluation of machine
translation. In Proceedings of ACL’07 Workshop on
Statistical Machine Translation, Prague, Czech Re-
public.
E. Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL’00, Seattle, WA,
USA.
D. Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL’05, Ann Arbor, MI, USA.
W. Cohen. 1996. Learning trees and rules with set-
valued features. In Proceedings of AAAI, Portland,
OR, USA.
M. Collins, P. Koehn, and I. Kucerova. 2005. Clause
restructuring for statistical machine translation. In
Proceedings of ACL’05, Ann Arbor, MI, USA.
J. M. Crego and J. B. Mariño. 2007. Syntax-enhanced
n-gram-based smt. In Proceedings of the MT Sum-
mit, Copenhagen, Denmark.
J. Elming. 2008. Syntactic reordering integrated with
phrase-based smt. In Proceedings of the ACL Work-
shop on Syntax and Structure in Statistical Transla-
tion (SSST-2), Columbus, OH, USA.
M. Galley, M. Hopkins, K. Knight, and D. Marcu.
2004. What’s in a translation rule? In Proceedings
of HLT/NAACL’04, Boston, MA, USA.
N. Habash and F. Sadat. 2006. Arabic preprocessing
schemes for statistical machine translation. In Pro-
ceedings of HLT-NAACL’06, New York, NY, USA.
N. Habash, B. Dorr, and C. Monz. 2006. Challenges
in Building an Arabic-English GHMT System with
SMT Components. In Proceedings of AMTA’06,
Cambridge, MA, USA.
N. Habash, A. Soudi, and T. Buckwalter. 2007.
On Arabic Transliteration. In A. van den Bosch
and A. Soudi, editors, Arabic Computational Mor-
phology: Knowledge-based and Empirical Methods.
Springer.
N. Habash. 2007. Syntactic preprocessing for statisti-
cal machine translation. In Proceedings of the MT
Summit, Copenhagen, Denmark.
A. Ittycheriah and S. Roukos. 2005. A maximum
entropy word aligner for arabic-english machine
translation. In Proceedings of EMNLP, Vancouver,
Canada.
P. Koehn and C. Monz. 2006. Manual and auto-
matic evaluation of machine translation between Eu-
ropean languages. In Proceedings of the Workshop
on Statistical Machine Translation at NAACL’06,
New York, NY, USA.
P. Koehn, F. J. Och, and D. Marcu. 2003a. Statis-
tical phrase-based translation. In Proceedings of
NAACL’03, Edmonton, Canada.
P. Koehn, F. J. Och, and D. Marcu. 2003b. Statis-
tical Phrase-based Translation. In Proceedings of
NAACL’03, Edmonton, Canada.
P. Koehn, A. Axelrod, A. Birch Mayne, C. Callison-
Burch, M. Osborne, and D. Talbot. 2005. Ed-
inburgh system description for the 2005 IWSLT
speech translation evaluation. In International
Workshop on Spoken Language Translation 2005
(IWSLT’05), Pittsburgh, PA, USA.
P. Koehn. 2004. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models.
In Proceedings ofAMTA’04, Washington, DC, USA.
Y. Lee. 2004. Morphological Analysis for Statisti-
cal Machine Translation. In Proceedings of HLT-
NAACL’04, Boston, MA, USA.
C. Li, M. Li, D. Zhang, M. Li, M. Zhou, and Y. Guan.
2007. A probabilistic approach to syntax-based re-
ordering for statistical machine translation. In Pro-
ceedings of ACL’07, Prague, Czech Republic.
A. Lopez and P. Resnik. 2006. Word-based alignment,
phrase-based translation: what’s the link? In Pro-
ceedings of AMTA’06, Cambridge, MA, USA.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19–51.
R. Sarikaya and Y. Deng. 2007. Joint morphological-
lexical language modeling for machine translation.
In Proceedings of HLT-NAACL’07, Short Papers,
Rochester, NY, USA.
A. Stolcke. 2002. Srilm – an extensible language mod-
eling toolkit. In Proceedings of the International
Conference on Spoken Language Processing, Den-
ver, CO, USA.
C. Wang, M. Collins, and P. Koehn. 2007. Chinese
syntactic reordering for statistical machine transla-
tion. In Proceedings of EMNLP-CoNLL, Prague,
Czech Republic.
F. Xia and M. McCord. 2004. Improving a statis-
tical mt system with automatically learned rewrite
patterns. In Proceedings of COLING’04, Geneva,
Switzerland.
K. Yamada and K. Knight. 2001. A Syntax-Based
Statistical Translation Model. In Proceedings of
ACL’01, Toulouse, France.
R. Zens, F. J. Och, and H. Ney. 2002. Phrase-based sta-
tistical machine translation. In M. Jarke, J. Koehler,
and G. Lakemeyer, editors, KI - 2002: Advances in
Arti�cial Intelligence. 25. Annual German Confer-
ence on AI. Springer Verlag.
Y. Zhang, S. Vogel, and A. Waibel. 2004. Interpret-
ing bleu/nist scores: How much improvement do we
need to have a better system? In Proceedings of
the 4th International Conference on Language Re-
sources and Evaluation (LREC’04), Lisbon, Portu-
gal.
Y. Zhang, R. Zens, and H. Ney. 2007. Improved
chunk-level reordering for statistical machine trans-
lation. In Proceedings of the IWSLT, Trento, Italy.
</reference>
<page confidence="0.999128">
77
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.959416">
<title confidence="0.999621">Syntactic Reordering for English-Arabic Phrase-Based Machine Translation</title>
<author confidence="0.996817">Jakob Elming Nizar Habash</author>
<affiliation confidence="0.986134">Languagelens Center for Computational Learning Systems</affiliation>
<address confidence="0.989873">Copenhagen, Denmark Columbia University, New York, USA</address>
<email confidence="0.999751">je@languagelens.comhabash@ccls.columbia.edu</email>
<abstract confidence="0.998702583333333">We investigate syntactic reordering within an English to Arabic translation task. We extend a pre-translation syntactic reordering approach developed on a close language pair (English-Danish) to the distant language pair, English-Arabic. We achieve significant improvements in translation quality over related approaches, measured by manual as well as automatic evaluations. These results prove the viability of this approach for distant languages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>I Badr</author>
<author>R Zbib</author>
<author>J Glass</author>
</authors>
<title>Segmentation for English-to-Arabic statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL’08: HLT, Short Papers,</booktitle>
<location>Columbus, OH, USA.</location>
<contexts>
<context position="5427" citStr="Badr et al. (2008)" startWordPosition="823" endWordPosition="826">ittle reordering. By contrast, in this paper, we target the more demanding reordering task of translating between two distant languages, English and Arabic. While much work has been done on Arabic to English MT (Habash and Sadat, 2006; Lee, 2004) mostly focusing on addressing the problems caused by the rich morphology of Arabic, we handle the less described translation direction: English to Arabic. Recently, there are some new publications on English to Arabic MT. Sarikaya and Deng (2007) use joint morphological-lexical language models to re-rank the output of English dialectal-Arabic MT, and Badr et al. (2008) report results on the value of the morphological decomposition of Arabic during training and describe different techniques for re-composition of Arabic in the output. We differ from the previous efforts targeting Arabic in that (1) we do not address morphology issues through segmentation (more on this in section 3) and (2) we focus on utilizing syntactic knowledge to address the reordering challenges of this translation direction. 3 Arabic Syntactic Issues Arabic is a morphologically and syntactically complex language with many differences from English. Arabic morphology has been well studied</context>
<context position="6834" citStr="Badr et al. (2008)" startWordPosition="1046" endWordPosition="1049">nto a morphologically rich language, target tokenization means that the translation process is broken into multiple steps (Badr et al., 2008). For our experiments, Arabic was not segmented apart from simple punctuation tokenization. This low level of segmentation was maintained in order to agree with the segmentation provided in the manually aligned corpus we used to learn our rules (section 6.1). We found no simple means for transferring the manual alignments to more segmented language. We expect that better performance would be achieved by introducing more Arabic segmentation as reported by Badr et al. (2008).1 As such, and unlike previous work in PSMT translating into Arabic, we focus here on syntax. We plan to investigate different tokenization schemes for syntactic preprocessing in future work. Next, we describe three prominent EnglishArabic syntactic phenomena that have motivated some of our decisions in this paper. First is verb-subject order. Arabic verb subjects may be: (a.) pro-dropped (verb conjugated), (b.) pre-verbal (SVO), or (c.) post-verbal (VSO). Although the English SVO order is possible in Arabic, it is not always preferred, especially when the subject is particularly long. Unfort</context>
</contexts>
<marker>Badr, Zbib, Glass, 2008</marker>
<rawString>I. Badr, R. Zbib, and J. Glass. 2008. Segmentation for English-to-Arabic statistical machine translation. In Proceedings ofACL’08: HLT, Short Papers, Columbus, OH, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>C Fordyce</author>
<author>P Koehn</author>
<author>C Monz</author>
<author>J Schroeder</author>
</authors>
<title>(Meta-) evaluation of machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL’07 Workshop on Statistical Machine Translation,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="24794" citStr="Callison-Burch et al. (2007)" startWordPosition="4147" endWordPosition="4150">dering approaches (NO and SO). A possible explanation is that the rules do not apply very often, in combination with the fact that the approaches often behave alike. The difference in SO and TO scoring only leads to a difference in translation in ti14% of the sentences. This set, the diff set, is interesting, since it provides a focus on the difference between these approaches. In table 4, we evaluate on this set. 6.2.2 Diff Set Overall the TO approach seems to be a superior reordering method. To back this observation, 50 sentences of MT04 are manually evaluated by a native speaker of Arabic. Callison-Burch et al. (2007) show that ranking sentences gives higher inter-annotator agreement than scoring adequacy and fluency. We therefore employ this evaluation method, asking the evaluator to rank sentences from four of the systems given the input sentence. Ties are allowed. Table 4 shows the average rat74 Decoder choice NO SO TO MT04 Phrase internal 20.7 0.6 21.2 Phrase external 30.1 43.0 33.1 Reject 49.2 56.5 45.7 MT05 Phrase internal 21.3 0.7 21.6 Phrase external 29.5 42.9 31.8 Reject 49.2 56.4 46.5 Table 5: The reordering choices made based on the three pre-translation reordering approaches for the 20852 and 1</context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2007</marker>
<rawString>C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz, and J. Schroeder. 2007. (Meta-) evaluation of machine translation. In Proceedings of ACL’07 Workshop on Statistical Machine Translation, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL’00,</booktitle>
<location>Seattle, WA, USA.</location>
<contexts>
<context position="19825" citStr="Charniak, 2000" startWordPosition="3308" endWordPosition="3309">the rest of the corpus uses different normalizations for numerals that make the two sets incompatible. 6.6K of the sentences (179K English and 146K Arabic words) are used to learn rule, while the rest are used for development purposes. In addition to the manual alignment supplied with these data, we create an automatic word alignment for them using GIZA++ (Och and Ney, 2003) and the grow-diagfinal (GDF) symmetrization algorithm (Koehn et al., 2005). This was done together with the data used to train the MT system. The English side is parsed using a state-of-the-art statistical English parser (Charniak, 2000). Two rule sets are learned based on the manual alignments (MAN) and the automatic alignments (GDF). The MT system is trained on a corpus consisting of 126K sentences with 4.2M English and 3.3M Arabic words in simple tokenization scheme. The domain is newswire (LDCNEWS) taken from Arabic News (LDC2004T17), eTIRR (LDC2004E72), English translation of Arabic Treebank (LDC2005E46), and Ummah (LDC2004T18). Although there are additional corpora available, we restricted ourselves to this set to allow for a fast development cycle. We plan to extend the data size in the future. The Arabic language mode</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>E. Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of NAACL’00, Seattle, WA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL’05,</booktitle>
<location>Ann Arbor, MI, USA.</location>
<contexts>
<context position="18926" citStr="Chiang, 2005" startWordPosition="3161" endWordPosition="3162"> sentence, i.e. points that are not covered by a rule, are not judged by the reordering model. Our rule extraction does not learn every possible reordering between the two languages, but only the most general ones. If no rule has an opinion at a certain point in a sentence, the decoder is free to choose the phrase translation it prefers without reordering cost. Separating the scoring from the source language reordering also has the advantage that the approach in essence is compatible with other approaches such as a traditional PSMT system (Koehn et al., 2003b) or a hierarchical phrase system (Chiang, 2005). We will, however, not examine this possibility further in the present paper. 6 Evaluation 6.1 Data We learn the reordering rules from the IBM Arabic-English aligned corpus (IBMAC) (Ittycheriah and Roukos, 2005). Of its total 13.9K sentence pairs, we only use 8.8K sentences because the rest of the corpus uses different normalizations for numerals that make the two sets incompatible. 6.6K of the sentences (179K English and 146K Arabic words) are used to learn rule, while the rest are used for development purposes. In addition to the manual alignment supplied with these data, we create an autom</context>
<context position="32773" citStr="Chiang, 2005" startWordPosition="5528" endWordPosition="5529">plore further in the future. In future work, we would also like to address the morphological complexity of Arabic together with syntax. We plan to consider different segmentations for Arabic and study their interaction with translation and syntactic reordering. An important aspect of the TO approach is that it uses phrase internal alignments during translation. In the future, we wish to examine the effect their quality has on translation. We are also interested in examining the approach within a standard phrase-based decoder such as Moses (Koehn et al., 2003b) or a hierarchical phrase system (Chiang, 2005). The idea of training on reordered source language is often connected with pre-translation reordering. The present approach does not employ this strategy, since this is no trivial matter in a non-deterministic, weighted approach. Zhang et al. (2007) proposed an approach that builds on unfolding alignments. This is not an optimal solution, since this may not reflect their rules. Training on both original and reordered data may strengthen the approach, but it would not remedy the problems of the SO approach, since it would still be ignorant of the internal reorderings of a phrase. Nevertheless,</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>D. Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of ACL’05, Ann Arbor, MI, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Cohen</author>
</authors>
<title>Learning trees and rules with setvalued features.</title>
<date>1996</date>
<booktitle>In Proceedings of AAAI,</booktitle>
<location>Portland, OR, USA.</location>
<marker>Cohen, 1996</marker>
<rawString>W. Cohen. 1996. Learning trees and rules with setvalued features. In Proceedings of AAAI, Portland, OR, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>P Koehn</author>
<author>I Kucerova</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL’05,</booktitle>
<location>Ann Arbor, MI, USA.</location>
<contexts>
<context position="3976" citStr="Collins et al., 2005" startWordPosition="588" endWordPosition="591">t usable in MT (Habash et al., 2006). While several recent authors using a pre-translation (sourceside) reordering approach have achieved positive results, it has been difficult to integrate syntactic Proceedings of the EACL 2009 Workshop on Computational Approaches to Semitic Languages, pages 69–77, Athens, Greece, 31 March, 2009. c�2009 Association for Computational Linguistics 69 information while retaining the strengths of the statistical approach. In some studies, reordering decisions are done “deterministically” by supplying the decoder with a canonical word order (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007; Habash, 2007). These reordering rules are either manually specified or automatically learned from alignments; and they are always placed outside the actual PSMT system. By contrast, other studies (Crego and Mariño, 2007; Zhang et al., 2007; Li et al., 2007; Elming, 2008) are more in the spirit of PSMT, in that multiple reorderings are presented to the PSMT system as (possibly weighted) options that are allowed to contribute alongside other parameters. Specifically, we follow the pre-translation reordering approach of Elming (2008). This approach has been proven to remedy s</context>
</contexts>
<marker>Collins, Koehn, Kucerova, 2005</marker>
<rawString>M. Collins, P. Koehn, and I. Kucerova. 2005. Clause restructuring for statistical machine translation. In Proceedings of ACL’05, Ann Arbor, MI, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Crego</author>
<author>J B Mariño</author>
</authors>
<title>Syntax-enhanced n-gram-based smt.</title>
<date>2007</date>
<booktitle>In Proceedings of the MT</booktitle>
<location>Summit, Copenhagen, Denmark.</location>
<contexts>
<context position="4216" citStr="Crego and Mariño, 2007" startWordPosition="626" endWordPosition="629"> on Computational Approaches to Semitic Languages, pages 69–77, Athens, Greece, 31 March, 2009. c�2009 Association for Computational Linguistics 69 information while retaining the strengths of the statistical approach. In some studies, reordering decisions are done “deterministically” by supplying the decoder with a canonical word order (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007; Habash, 2007). These reordering rules are either manually specified or automatically learned from alignments; and they are always placed outside the actual PSMT system. By contrast, other studies (Crego and Mariño, 2007; Zhang et al., 2007; Li et al., 2007; Elming, 2008) are more in the spirit of PSMT, in that multiple reorderings are presented to the PSMT system as (possibly weighted) options that are allowed to contribute alongside other parameters. Specifically, we follow the pre-translation reordering approach of Elming (2008). This approach has been proven to remedy shortcomings of other pre-translation reordering approaches by reordering the input word sequence, but scoring the output word sequence. Elming (2008) only examined the approach within English – Danish, a language pair that displays little r</context>
<context position="22961" citStr="Crego and Mariño, 2007" startWordPosition="3829" endWordPosition="3832"> a tune set of 1.0K sentences with 34K English and 26K Arabic words, and a development set of 3.1K sentences with 102K English and 79K Arabic words. 6.2 Results and discussion The reordering approach is evaluated on the MT04 and MT05 test sets. Results are listed in table 3 along with results on the development set. We report on (a) Pharaoh with no restriction on reordering (Pharaoh Free), (b) Pharaoh with distortion limit 4 (Pharaoh DL4), (c) Pharaoh with monotone decoding (Pharaoh Monotone), and (d) a system provided with a rule reordered word lattice but no (NO) weighting in the spirit of (Crego and Mariño, 2007), (e) the same system but with a source order System MT04 MT05 Avr. human Pharaoh Free 24.07 25.15 3.0 (2.80) Pharaoh DL4 25.42 26.51 — NO scoring 25.68 26.29 2.5 (2.43) SO scoring 25.42 26.02 2.5 (2.64) TO scoring 25.98 26.49 2.0 (2.08) Table 4: Evaluation on the diff set. Average human ratings are medians with means in parenthesis, lower scores are better, 1 is the best score. (SO) weighting in the spirit of (Zhang et al., 2007; Li et al., 2007), and finally (f) the same system but with the target order (TO) weighting. In addition to evaluating the reordering approaches, we also report on su</context>
</contexts>
<marker>Crego, Mariño, 2007</marker>
<rawString>J. M. Crego and J. B. Mariño. 2007. Syntax-enhanced n-gram-based smt. In Proceedings of the MT Summit, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Elming</author>
</authors>
<title>Syntactic reordering integrated with phrase-based smt.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2),</booktitle>
<location>Columbus, OH, USA.</location>
<contexts>
<context position="1513" citStr="Elming (2008)" startWordPosition="211" endWordPosition="212">llowing translation of word sequences (phrases) instead of single words provides PSMT with a high degree of robustness in word selection and in local-word reordering. Recent developments have shown that improvements in PSMT quality are possible using syntax. One such development is the pretranslation reordering approach, which adjusts the source sentence to resemble target-language word order prior to translation. This is typically done using rules that are either manually created or automatically learned from word-aligned parallel corpora. One particular variety of this approach, proposed by Elming (2008), uses a large set of linguistic features to automatically learn reordering rules. The rules are applied nondeterministically; however, phrase-internal wordalignments are used to ensure that the intended reordering does not come undone because of phrase internal reordering (Elming, 2008). This approach was shown to produce improved MT output on English-Danish MT, a relatively closely-related and similarly-structured language pair. In this paper, we study whether this approach can be extended to distant language pairs, specifically English-to-Arabic. We achieve significant improvement in transl</context>
<context position="4268" citStr="Elming, 2008" startWordPosition="638" endWordPosition="639">7, Athens, Greece, 31 March, 2009. c�2009 Association for Computational Linguistics 69 information while retaining the strengths of the statistical approach. In some studies, reordering decisions are done “deterministically” by supplying the decoder with a canonical word order (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007; Habash, 2007). These reordering rules are either manually specified or automatically learned from alignments; and they are always placed outside the actual PSMT system. By contrast, other studies (Crego and Mariño, 2007; Zhang et al., 2007; Li et al., 2007; Elming, 2008) are more in the spirit of PSMT, in that multiple reorderings are presented to the PSMT system as (possibly weighted) options that are allowed to contribute alongside other parameters. Specifically, we follow the pre-translation reordering approach of Elming (2008). This approach has been proven to remedy shortcomings of other pre-translation reordering approaches by reordering the input word sequence, but scoring the output word sequence. Elming (2008) only examined the approach within English – Danish, a language pair that displays little reordering. By contrast, in this paper, we target the</context>
<context position="10042" citStr="Elming (2008)" startWordPosition="1615" endWordPosition="1616"> man translating into Ég. P Èñ£@ ATwl rjl ‘tallest man.’ Finally, Arabic has one syntactic construction, called Idafa, for indicating possession and compounding, while English has three. The Idafa construction typically consists of one or more indefinite nouns followed by a definite noun. For example, the English phrases the car keys, the car’s keys and the keys of the car all translate into the Arabic oPAJ�‚Ë@ iJ��KA�®Ó mfAtyH AlsyArli ‘keys thecar.’ Only one of the three English constructions does not require content word reordering. 4 Reordering rules 4.1 Definition of reordering Following Elming (2008), we define reordering as two word sequences, left sequence (LS) and right sequence (RS), exchanging positions. These two sequences are restricted by being parallel consecutive, maximal and adjacent. The sequences are not restricted in length, making both short and long distance reordering possible. Furthermore, they need not be phrases in the sense that they appear as an entry in the phrase table. Figure 2 illustrates reordering in a word alignment matrix. The matrix contains reorderings between the light grey sequences (s32 and s64)3 and 3Notation: s&apos;Y means the consecutive source sequence t</context>
<context position="15273" citStr="Elming (2008)" startWordPosition="2530" endWordPosition="2531">mple. Since the input format defines all possible word orders allowed by the rule set, a simple monotone search is sufficient. Using a language model of order n, for each hypothesized target string ending in the same n-1-gram, we only have to extend the highest scoring hypothesis. None of the others can possibly outperform this one later on. This is because the maximal context evaluating a phrase extending this hypothesis, is the history (n-1-gram) of the first word of that phrase. The decoder is not able to look any further back at the preceding string. 5.1 The reordering approach Similar to Elming (2008), the integration of the rule-based reordering in our PSMT system is carried out in two separate stages: 1. Reordering the source sentence to assimilate the word order of the target language. 2. Weighting of the target word order according to the rules. Stage (1) is done in a non-deterministic fashion by generating a word lattice as input. This way, the system has both the original word order, and the reorderings predicted by the rule set. The different paths of the word lattice are merely given as equal suggestions to the decoder. They are in no way individually weighted. Separating stage (2)</context>
<context position="31342" citStr="Elming (2008)" startWordPosition="5306" endWordPosition="5307">1; these should often be moved together with the singular noun. The function of the right context is harder to explain, but without this restriction, the rule would have been much less successful; dropping from a probability of 71% to 51%. An overall comparison of the rules produced based on the manual and automatic alignments shows no major difference in quality. This is especially interesting in light of the better translation using the GDF rules. It is also very interesting that it seems possible to get as good rules from the GDF as from the MAN alignments. This is a new result compared to Elming (2008), where results on manual alignments only are reported. 7 Conclusion and Future Plans We have explored the syntactic reordering approach previously presented in (Elming, 2008) within a more distant language pair, EnglishArabic. A translation direction that is highly under-represented in MT research, compared to the opposite direction. We achieve significant improvement in translation quality over related approaches, measured by manual as well as automatic evaluations on this task. Thus proving the viability of the approach on distant languages. We also examined the effect of the alignment meth</context>
</contexts>
<marker>Elming, 2008</marker>
<rawString>J. Elming. 2008. Syntactic reordering integrated with phrase-based smt. In Proceedings of the ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), Columbus, OH, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>M Hopkins</author>
<author>K Knight</author>
<author>D Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proceedings of HLT/NAACL’04,</booktitle>
<location>Boston, MA, USA.</location>
<contexts>
<context position="3199" citStr="Galley et al., 2004" startWordPosition="469" endWordPosition="472">ection 3 describes aspects of English and Arabic structure that are relevant to reordering. Section 4 describes the automatic induction of reordering rules and its integration in PSMT. In section 5, we describe the SMT system used in the experiments. In section 6, we evaluate and discuss the results of our English-Arabic MT system. 2 Related Work Much work has been done in syntactic reordering for SMT, focusing on both source and targetlanguage syntax. In this paper, we adapt an approach that utilizes source-syntax information as opposed to target-side syntax systems (Yamada and Knight, 2001; Galley et al., 2004). This is because we are translating from English to Arabic and we are discouraged by recent results indicating Arabic parsing is not at a stage that makes it usable in MT (Habash et al., 2006). While several recent authors using a pre-translation (sourceside) reordering approach have achieved positive results, it has been difficult to integrate syntactic Proceedings of the EACL 2009 Workshop on Computational Approaches to Semitic Languages, pages 69–77, Athens, Greece, 31 March, 2009. c�2009 Association for Computational Linguistics 69 information while retaining the strengths of the statisti</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>M. Galley, M. Hopkins, K. Knight, and D. Marcu. 2004. What’s in a translation rule? In Proceedings of HLT/NAACL’04, Boston, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Habash</author>
<author>F Sadat</author>
</authors>
<title>Arabic preprocessing schemes for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL’06,</booktitle>
<location>New York, NY, USA.</location>
<contexts>
<context position="5043" citStr="Habash and Sadat, 2006" startWordPosition="762" endWordPosition="766">ute alongside other parameters. Specifically, we follow the pre-translation reordering approach of Elming (2008). This approach has been proven to remedy shortcomings of other pre-translation reordering approaches by reordering the input word sequence, but scoring the output word sequence. Elming (2008) only examined the approach within English – Danish, a language pair that displays little reordering. By contrast, in this paper, we target the more demanding reordering task of translating between two distant languages, English and Arabic. While much work has been done on Arabic to English MT (Habash and Sadat, 2006; Lee, 2004) mostly focusing on addressing the problems caused by the rich morphology of Arabic, we handle the less described translation direction: English to Arabic. Recently, there are some new publications on English to Arabic MT. Sarikaya and Deng (2007) use joint morphological-lexical language models to re-rank the output of English dialectal-Arabic MT, and Badr et al. (2008) report results on the value of the morphological decomposition of Arabic during training and describe different techniques for re-composition of Arabic in the output. We differ from the previous efforts targeting Ar</context>
</contexts>
<marker>Habash, Sadat, 2006</marker>
<rawString>N. Habash and F. Sadat. 2006. Arabic preprocessing schemes for statistical machine translation. In Proceedings of HLT-NAACL’06, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Habash</author>
<author>B Dorr</author>
<author>C Monz</author>
</authors>
<date>2006</date>
<booktitle>Challenges in Building an Arabic-English GHMT System with SMT Components. In Proceedings of AMTA’06,</booktitle>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="3392" citStr="Habash et al., 2006" startWordPosition="505" endWordPosition="508">on 5, we describe the SMT system used in the experiments. In section 6, we evaluate and discuss the results of our English-Arabic MT system. 2 Related Work Much work has been done in syntactic reordering for SMT, focusing on both source and targetlanguage syntax. In this paper, we adapt an approach that utilizes source-syntax information as opposed to target-side syntax systems (Yamada and Knight, 2001; Galley et al., 2004). This is because we are translating from English to Arabic and we are discouraged by recent results indicating Arabic parsing is not at a stage that makes it usable in MT (Habash et al., 2006). While several recent authors using a pre-translation (sourceside) reordering approach have achieved positive results, it has been difficult to integrate syntactic Proceedings of the EACL 2009 Workshop on Computational Approaches to Semitic Languages, pages 69–77, Athens, Greece, 31 March, 2009. c�2009 Association for Computational Linguistics 69 information while retaining the strengths of the statistical approach. In some studies, reordering decisions are done “deterministically” by supplying the decoder with a canonical word order (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2</context>
</contexts>
<marker>Habash, Dorr, Monz, 2006</marker>
<rawString>N. Habash, B. Dorr, and C. Monz. 2006. Challenges in Building an Arabic-English GHMT System with SMT Components. In Proceedings of AMTA’06, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Habash</author>
<author>A Soudi</author>
<author>T Buckwalter</author>
</authors>
<title>On Arabic Transliteration.</title>
<date>2007</date>
<booktitle>Arabic Computational Morphology: Knowledge-based and Empirical Methods.</booktitle>
<editor>In A. van den Bosch and A. Soudi, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="8569" citStr="Habash et al., 2007" startWordPosition="1325" endWordPosition="1328">, respectively.2 Secondly, Arabic adjectival modifiers typically follow their nouns with the exception of some superlative adjectives. However, English adjectival modifiers can follow or precede their nouns depending on the size of the adjectival phrase: single word adjectives precede but multi-word adjectives phrases follow (or precede while hyphenated). For example, a tall man translates as J��gb J_, rjl 1Our results are not comparable to their results, since they report on non-standard data sets. 2All Arabic transliterations in this paper are provided in the Habash-Soudi-Buckwalter scheme (Habash et al., 2007). 70 [NP-SBJ The general coordinator of the railroad project among the countries of the Gulf Cooperation Council, Hamid Khaja ,] [V announced] [SUB that ...] [ék. A g YÓAg ú j. J�Ê�mÌ@ vðAªJ@ •Êm.× ÈðX YK�YmÌ @ aº‚Ë@ ¨ðQå�„ÖÏ ÐAªË@ 3‚:1ÖÏ@ NP-SBJ] [ �áÊ«@ V] [... v@ SUB] [V Aςln] [NP-SBJ Almnsq AlςAm lmšrwς Alskh AlHdyd byn dwl mjls AltςAwn Alxlyjy HAmd xAjh] [SUB An ...] Figure 1: An example of long distance reordering of English SVO order to Arabic VSO order � � � � � � � ■ � � � � � � ■ � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � s1 s2 s3 s4 s5 sg s7 t7 tg t5 t4 t3 t</context>
</contexts>
<marker>Habash, Soudi, Buckwalter, 2007</marker>
<rawString>N. Habash, A. Soudi, and T. Buckwalter. 2007. On Arabic Transliteration. In A. van den Bosch and A. Soudi, editors, Arabic Computational Morphology: Knowledge-based and Empirical Methods. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Habash</author>
</authors>
<title>Syntactic preprocessing for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the MT</booktitle>
<location>Summit, Copenhagen, Denmark.</location>
<contexts>
<context position="4010" citStr="Habash, 2007" startWordPosition="596" endWordPosition="597">le several recent authors using a pre-translation (sourceside) reordering approach have achieved positive results, it has been difficult to integrate syntactic Proceedings of the EACL 2009 Workshop on Computational Approaches to Semitic Languages, pages 69–77, Athens, Greece, 31 March, 2009. c�2009 Association for Computational Linguistics 69 information while retaining the strengths of the statistical approach. In some studies, reordering decisions are done “deterministically” by supplying the decoder with a canonical word order (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007; Habash, 2007). These reordering rules are either manually specified or automatically learned from alignments; and they are always placed outside the actual PSMT system. By contrast, other studies (Crego and Mariño, 2007; Zhang et al., 2007; Li et al., 2007; Elming, 2008) are more in the spirit of PSMT, in that multiple reorderings are presented to the PSMT system as (possibly weighted) options that are allowed to contribute alongside other parameters. Specifically, we follow the pre-translation reordering approach of Elming (2008). This approach has been proven to remedy shortcomings of other pre-translati</context>
</contexts>
<marker>Habash, 2007</marker>
<rawString>N. Habash. 2007. Syntactic preprocessing for statistical machine translation. In Proceedings of the MT Summit, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ittycheriah</author>
<author>S Roukos</author>
</authors>
<title>A maximum entropy word aligner for arabic-english machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<location>Vancouver, Canada.</location>
<contexts>
<context position="19138" citStr="Ittycheriah and Roukos, 2005" startWordPosition="3192" endWordPosition="3196">most general ones. If no rule has an opinion at a certain point in a sentence, the decoder is free to choose the phrase translation it prefers without reordering cost. Separating the scoring from the source language reordering also has the advantage that the approach in essence is compatible with other approaches such as a traditional PSMT system (Koehn et al., 2003b) or a hierarchical phrase system (Chiang, 2005). We will, however, not examine this possibility further in the present paper. 6 Evaluation 6.1 Data We learn the reordering rules from the IBM Arabic-English aligned corpus (IBMAC) (Ittycheriah and Roukos, 2005). Of its total 13.9K sentence pairs, we only use 8.8K sentences because the rest of the corpus uses different normalizations for numerals that make the two sets incompatible. 6.6K of the sentences (179K English and 146K Arabic words) are used to learn rule, while the rest are used for development purposes. In addition to the manual alignment supplied with these data, we create an automatic word alignment for them using GIZA++ (Och and Ney, 2003) and the grow-diagfinal (GDF) symmetrization algorithm (Koehn et al., 2005). This was done together with the data used to train the MT system. The Engl</context>
</contexts>
<marker>Ittycheriah, Roukos, 2005</marker>
<rawString>A. Ittycheriah and S. Roukos. 2005. A maximum entropy word aligner for arabic-english machine translation. In Proceedings of EMNLP, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>C Monz</author>
</authors>
<title>Manual and automatic evaluation of machine translation between European languages.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation at NAACL’06,</booktitle>
<location>New York, NY, USA.</location>
<contexts>
<context position="13694" citStr="Koehn and Monz, 2006" startWordPosition="2257" endWordPosition="2260">presentation of the context, and it produces easily readable rules that allow better understanding of the decisions being made. In section 6.3, extracted rules are exemplified and analyzed. The probabilities of the rules are estimated using Maximum Likelihood Estimation based on the information supplied by Ripper on the performance of the individual rules on the training data. These logarithmic probabilities are easily integratable in the log-linear PSMT model as an additional parameter by simple addition. 5 The PSMT system Our baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002). The decoder used for the baseline system is Pharaoh (Koehn, 2004) with its distance-penalizing reordering model. Since Pharaoh does not support word lattice input, we use our own decoder for the experiments. Except for the reordering model, it uses the same knowledge sources as Pharaoh, i.e. a bidirectional phrase translation model, a lexical weight model, phrase and word penalties, and a target language model. Its behavior is comparable to Pharaoh when doing monotone decoding. The search algorithm of our decoder is similar to</context>
</contexts>
<marker>Koehn, Monz, 2006</marker>
<rawString>P. Koehn and C. Monz. 2006. Manual and automatic evaluation of machine translation between European languages. In Proceedings of the Workshop on Statistical Machine Translation at NAACL’06, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL’03,</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="815" citStr="Koehn et al., 2003" startWordPosition="103" endWordPosition="106">w York, USA je@languagelens.com habash@ccls.columbia.edu Abstract We investigate syntactic reordering within an English to Arabic translation task. We extend a pre-translation syntactic reordering approach developed on a close language pair (English-Danish) to the distant language pair, English-Arabic. We achieve significant improvements in translation quality over related approaches, measured by manual as well as automatic evaluations. These results prove the viability of this approach for distant languages. 1 Introduction The emergence of phrase-based statistical machine translation (PSMT) (Koehn et al., 2003a) has been one of the major developments in statistical approaches to translation. Allowing translation of word sequences (phrases) instead of single words provides PSMT with a high degree of robustness in word selection and in local-word reordering. Recent developments have shown that improvements in PSMT quality are possible using syntax. One such development is the pretranslation reordering approach, which adjusts the source sentence to resemble target-language word order prior to translation. This is typically done using rules that are either manually created or automatically learned from</context>
<context position="18877" citStr="Koehn et al., 2003" startWordPosition="3151" endWordPosition="3154">e). Phrase internal reorderings at other points of the sentence, i.e. points that are not covered by a rule, are not judged by the reordering model. Our rule extraction does not learn every possible reordering between the two languages, but only the most general ones. If no rule has an opinion at a certain point in a sentence, the decoder is free to choose the phrase translation it prefers without reordering cost. Separating the scoring from the source language reordering also has the advantage that the approach in essence is compatible with other approaches such as a traditional PSMT system (Koehn et al., 2003b) or a hierarchical phrase system (Chiang, 2005). We will, however, not examine this possibility further in the present paper. 6 Evaluation 6.1 Data We learn the reordering rules from the IBM Arabic-English aligned corpus (IBMAC) (Ittycheriah and Roukos, 2005). Of its total 13.9K sentence pairs, we only use 8.8K sentences because the rest of the corpus uses different normalizations for numerals that make the two sets incompatible. 6.6K of the sentences (179K English and 146K Arabic words) are used to learn rule, while the rest are used for development purposes. In addition to the manual align</context>
<context position="32724" citStr="Koehn et al., 2003" startWordPosition="5519" endWordPosition="5522">rom manual alignments. This is an aspect we want to explore further in the future. In future work, we would also like to address the morphological complexity of Arabic together with syntax. We plan to consider different segmentations for Arabic and study their interaction with translation and syntactic reordering. An important aspect of the TO approach is that it uses phrase internal alignments during translation. In the future, we wish to examine the effect their quality has on translation. We are also interested in examining the approach within a standard phrase-based decoder such as Moses (Koehn et al., 2003b) or a hierarchical phrase system (Chiang, 2005). The idea of training on reordered source language is often connected with pre-translation reordering. The present approach does not employ this strategy, since this is no trivial matter in a non-deterministic, weighted approach. Zhang et al. (2007) proposed an approach that builds on unfolding alignments. This is not an optimal solution, since this may not reflect their rules. Training on both original and reordered data may strengthen the approach, but it would not remedy the problems of the SO approach, since it would still be ignorant of th</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. J. Och, and D. Marcu. 2003a. Statistical phrase-based translation. In Proceedings of NAACL’03, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical Phrase-based Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL’03,</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="815" citStr="Koehn et al., 2003" startWordPosition="103" endWordPosition="106">w York, USA je@languagelens.com habash@ccls.columbia.edu Abstract We investigate syntactic reordering within an English to Arabic translation task. We extend a pre-translation syntactic reordering approach developed on a close language pair (English-Danish) to the distant language pair, English-Arabic. We achieve significant improvements in translation quality over related approaches, measured by manual as well as automatic evaluations. These results prove the viability of this approach for distant languages. 1 Introduction The emergence of phrase-based statistical machine translation (PSMT) (Koehn et al., 2003a) has been one of the major developments in statistical approaches to translation. Allowing translation of word sequences (phrases) instead of single words provides PSMT with a high degree of robustness in word selection and in local-word reordering. Recent developments have shown that improvements in PSMT quality are possible using syntax. One such development is the pretranslation reordering approach, which adjusts the source sentence to resemble target-language word order prior to translation. This is typically done using rules that are either manually created or automatically learned from</context>
<context position="18877" citStr="Koehn et al., 2003" startWordPosition="3151" endWordPosition="3154">e). Phrase internal reorderings at other points of the sentence, i.e. points that are not covered by a rule, are not judged by the reordering model. Our rule extraction does not learn every possible reordering between the two languages, but only the most general ones. If no rule has an opinion at a certain point in a sentence, the decoder is free to choose the phrase translation it prefers without reordering cost. Separating the scoring from the source language reordering also has the advantage that the approach in essence is compatible with other approaches such as a traditional PSMT system (Koehn et al., 2003b) or a hierarchical phrase system (Chiang, 2005). We will, however, not examine this possibility further in the present paper. 6 Evaluation 6.1 Data We learn the reordering rules from the IBM Arabic-English aligned corpus (IBMAC) (Ittycheriah and Roukos, 2005). Of its total 13.9K sentence pairs, we only use 8.8K sentences because the rest of the corpus uses different normalizations for numerals that make the two sets incompatible. 6.6K of the sentences (179K English and 146K Arabic words) are used to learn rule, while the rest are used for development purposes. In addition to the manual align</context>
<context position="32724" citStr="Koehn et al., 2003" startWordPosition="5519" endWordPosition="5522">rom manual alignments. This is an aspect we want to explore further in the future. In future work, we would also like to address the morphological complexity of Arabic together with syntax. We plan to consider different segmentations for Arabic and study their interaction with translation and syntactic reordering. An important aspect of the TO approach is that it uses phrase internal alignments during translation. In the future, we wish to examine the effect their quality has on translation. We are also interested in examining the approach within a standard phrase-based decoder such as Moses (Koehn et al., 2003b) or a hierarchical phrase system (Chiang, 2005). The idea of training on reordered source language is often connected with pre-translation reordering. The present approach does not employ this strategy, since this is no trivial matter in a non-deterministic, weighted approach. Zhang et al. (2007) proposed an approach that builds on unfolding alignments. This is not an optimal solution, since this may not reflect their rules. Training on both original and reordered data may strengthen the approach, but it would not remedy the problems of the SO approach, since it would still be ignorant of th</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. J. Och, and D. Marcu. 2003b. Statistical Phrase-based Translation. In Proceedings of NAACL’03, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>A Axelrod</author>
<author>A Birch Mayne</author>
<author>C CallisonBurch</author>
<author>M Osborne</author>
<author>D Talbot</author>
</authors>
<title>Edinburgh system description for the 2005 IWSLT speech translation evaluation.</title>
<date>2005</date>
<booktitle>In International Workshop on Spoken Language Translation 2005 (IWSLT’05),</booktitle>
<location>Pittsburgh, PA, USA.</location>
<contexts>
<context position="19662" citStr="Koehn et al., 2005" startWordPosition="3280" endWordPosition="3283">rdering rules from the IBM Arabic-English aligned corpus (IBMAC) (Ittycheriah and Roukos, 2005). Of its total 13.9K sentence pairs, we only use 8.8K sentences because the rest of the corpus uses different normalizations for numerals that make the two sets incompatible. 6.6K of the sentences (179K English and 146K Arabic words) are used to learn rule, while the rest are used for development purposes. In addition to the manual alignment supplied with these data, we create an automatic word alignment for them using GIZA++ (Och and Ney, 2003) and the grow-diagfinal (GDF) symmetrization algorithm (Koehn et al., 2005). This was done together with the data used to train the MT system. The English side is parsed using a state-of-the-art statistical English parser (Charniak, 2000). Two rule sets are learned based on the manual alignments (MAN) and the automatic alignments (GDF). The MT system is trained on a corpus consisting of 126K sentences with 4.2M English and 3.3M Arabic words in simple tokenization scheme. The domain is newswire (LDCNEWS) taken from Arabic News (LDC2004T17), eTIRR (LDC2004E72), English translation of Arabic Treebank (LDC2005E46), and Ummah (LDC2004T18). Although there are additional co</context>
</contexts>
<marker>Koehn, Axelrod, Mayne, CallisonBurch, Osborne, Talbot, 2005</marker>
<rawString>P. Koehn, A. Axelrod, A. Birch Mayne, C. CallisonBurch, M. Osborne, and D. Talbot. 2005. Edinburgh system description for the 2005 IWSLT speech translation evaluation. In International Workshop on Spoken Language Translation 2005 (IWSLT’05), Pittsburgh, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Pharaoh: a beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In Proceedings ofAMTA’04,</booktitle>
<location>Washington, DC, USA.</location>
<contexts>
<context position="13827" citStr="Koehn, 2004" startWordPosition="2281" endWordPosition="2282">, extracted rules are exemplified and analyzed. The probabilities of the rules are estimated using Maximum Likelihood Estimation based on the information supplied by Ripper on the performance of the individual rules on the training data. These logarithmic probabilities are easily integratable in the log-linear PSMT model as an additional parameter by simple addition. 5 The PSMT system Our baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002). The decoder used for the baseline system is Pharaoh (Koehn, 2004) with its distance-penalizing reordering model. Since Pharaoh does not support word lattice input, we use our own decoder for the experiments. Except for the reordering model, it uses the same knowledge sources as Pharaoh, i.e. a bidirectional phrase translation model, a lexical weight model, phrase and word penalties, and a target language model. Its behavior is comparable to Pharaoh when doing monotone decoding. The search algorithm of our decoder is similar to the RG graph decoder of (Zens et al., 2002). It expects a word lattice as input. Figure 3 shows the word lattice for the example in </context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>P. Koehn. 2004. Pharaoh: a beam search decoder for phrase-based statistical machine translation models. In Proceedings ofAMTA’04, Washington, DC, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Lee</author>
</authors>
<title>Morphological Analysis for Statistical Machine Translation.</title>
<date>2004</date>
<booktitle>In Proceedings of HLTNAACL’04,</booktitle>
<location>Boston, MA, USA.</location>
<contexts>
<context position="5055" citStr="Lee, 2004" startWordPosition="767" endWordPosition="768">meters. Specifically, we follow the pre-translation reordering approach of Elming (2008). This approach has been proven to remedy shortcomings of other pre-translation reordering approaches by reordering the input word sequence, but scoring the output word sequence. Elming (2008) only examined the approach within English – Danish, a language pair that displays little reordering. By contrast, in this paper, we target the more demanding reordering task of translating between two distant languages, English and Arabic. While much work has been done on Arabic to English MT (Habash and Sadat, 2006; Lee, 2004) mostly focusing on addressing the problems caused by the rich morphology of Arabic, we handle the less described translation direction: English to Arabic. Recently, there are some new publications on English to Arabic MT. Sarikaya and Deng (2007) use joint morphological-lexical language models to re-rank the output of English dialectal-Arabic MT, and Badr et al. (2008) report results on the value of the morphological decomposition of Arabic during training and describe different techniques for re-composition of Arabic in the output. We differ from the previous efforts targeting Arabic in that</context>
</contexts>
<marker>Lee, 2004</marker>
<rawString>Y. Lee. 2004. Morphological Analysis for Statistical Machine Translation. In Proceedings of HLTNAACL’04, Boston, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Li</author>
<author>M Li</author>
<author>D Zhang</author>
<author>M Li</author>
<author>M Zhou</author>
<author>Y Guan</author>
</authors>
<title>A probabilistic approach to syntax-based reordering for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL’07,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="4253" citStr="Li et al., 2007" startWordPosition="634" endWordPosition="637">uages, pages 69–77, Athens, Greece, 31 March, 2009. c�2009 Association for Computational Linguistics 69 information while retaining the strengths of the statistical approach. In some studies, reordering decisions are done “deterministically” by supplying the decoder with a canonical word order (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007; Habash, 2007). These reordering rules are either manually specified or automatically learned from alignments; and they are always placed outside the actual PSMT system. By contrast, other studies (Crego and Mariño, 2007; Zhang et al., 2007; Li et al., 2007; Elming, 2008) are more in the spirit of PSMT, in that multiple reorderings are presented to the PSMT system as (possibly weighted) options that are allowed to contribute alongside other parameters. Specifically, we follow the pre-translation reordering approach of Elming (2008). This approach has been proven to remedy shortcomings of other pre-translation reordering approaches by reordering the input word sequence, but scoring the output word sequence. Elming (2008) only examined the approach within English – Danish, a language pair that displays little reordering. By contrast, in this paper</context>
<context position="23412" citStr="Li et al., 2007" startWordPosition="3913" endWordPosition="3916">th monotone decoding (Pharaoh Monotone), and (d) a system provided with a rule reordered word lattice but no (NO) weighting in the spirit of (Crego and Mariño, 2007), (e) the same system but with a source order System MT04 MT05 Avr. human Pharaoh Free 24.07 25.15 3.0 (2.80) Pharaoh DL4 25.42 26.51 — NO scoring 25.68 26.29 2.5 (2.43) SO scoring 25.42 26.02 2.5 (2.64) TO scoring 25.98 26.49 2.0 (2.08) Table 4: Evaluation on the diff set. Average human ratings are medians with means in parenthesis, lower scores are better, 1 is the best score. (SO) weighting in the spirit of (Zhang et al., 2007; Li et al., 2007), and finally (f) the same system but with the target order (TO) weighting. In addition to evaluating the reordering approaches, we also report on supplying them with different reordering rule sets: a set that was learned on manually aligned data (MAN), and a set learned on the same data but with automatic alignments (GDF). 6.2.1 Overall Results Pharaoh Monotone performs similarly to Pharaoh Free. This shows that the question of improved reordering is not about quantity, but rather quality: what constraints are optimal to generate the best word order. The TO approach gets an increase over Phar</context>
</contexts>
<marker>Li, Li, Zhang, Li, Zhou, Guan, 2007</marker>
<rawString>C. Li, M. Li, D. Zhang, M. Li, M. Zhou, and Y. Guan. 2007. A probabilistic approach to syntax-based reordering for statistical machine translation. In Proceedings of ACL’07, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lopez</author>
<author>P Resnik</author>
</authors>
<title>Word-based alignment, phrase-based translation: what’s the link?</title>
<date>2006</date>
<booktitle>In Proceedings of AMTA’06,</booktitle>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="26078" citStr="Lopez and Resnik, 2006" startWordPosition="4351" endWordPosition="4354">test sets. Measured in %. ings of the systems. This shows the TO scoring to be significantly superior to the other methods (p &lt; 0.01 using Wilcoxon signed-rank testing). 6.2.3 MAN vs GDF Another interesting observation is that reordering rules learned from automatic alignments lead to significantly better translation than rules learned from manual alignment. Due to the much higher quality of the manual alignment, the opposite might be expected. However, this may be just a variant on the observation that alignment improvements (measured against human references) seldom lead to MT improvements (Lopez and Resnik, 2006). The MAN alignments may in fact be better than GDF, but they are most certainly more different in nature from real alignment than the GDF alignments are. As such, the MAN alignments are not as powerful as we would have liked them to be. In our data sets, the GDF rules, seem less specific, and they therefore apply more frequently than the MAN rules. On average, this results in more than 7 times as many possible reordering paths per sentence. This means that the GDF rules supply the decoder with a larger search space, which in turn means more proposed translation hypotheses. This may play a big</context>
</contexts>
<marker>Lopez, Resnik, 2006</marker>
<rawString>A. Lopez and P. Resnik. 2006. Word-based alignment, phrase-based translation: what’s the link? In Proceedings of AMTA’06, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="19587" citStr="Och and Ney, 2003" startWordPosition="3269" endWordPosition="3272">ility further in the present paper. 6 Evaluation 6.1 Data We learn the reordering rules from the IBM Arabic-English aligned corpus (IBMAC) (Ittycheriah and Roukos, 2005). Of its total 13.9K sentence pairs, we only use 8.8K sentences because the rest of the corpus uses different normalizations for numerals that make the two sets incompatible. 6.6K of the sentences (179K English and 146K Arabic words) are used to learn rule, while the rest are used for development purposes. In addition to the manual alignment supplied with these data, we create an automatic word alignment for them using GIZA++ (Och and Ney, 2003) and the grow-diagfinal (GDF) symmetrization algorithm (Koehn et al., 2005). This was done together with the data used to train the MT system. The English side is parsed using a state-of-the-art statistical English parser (Charniak, 2000). Two rule sets are learned based on the manual alignments (MAN) and the automatic alignments (GDF). The MT system is trained on a corpus consisting of 126K sentences with 4.2M English and 3.3M Arabic words in simple tokenization scheme. The domain is newswire (LDCNEWS) taken from Arabic News (LDC2004T17), eTIRR (LDC2004E72), English translation of Arabic Tree</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sarikaya</author>
<author>Y Deng</author>
</authors>
<title>Joint morphologicallexical language modeling for machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of HLT-NAACL’07, Short Papers,</booktitle>
<location>Rochester, NY, USA.</location>
<contexts>
<context position="5302" citStr="Sarikaya and Deng (2007)" startWordPosition="805" endWordPosition="808">scoring the output word sequence. Elming (2008) only examined the approach within English – Danish, a language pair that displays little reordering. By contrast, in this paper, we target the more demanding reordering task of translating between two distant languages, English and Arabic. While much work has been done on Arabic to English MT (Habash and Sadat, 2006; Lee, 2004) mostly focusing on addressing the problems caused by the rich morphology of Arabic, we handle the less described translation direction: English to Arabic. Recently, there are some new publications on English to Arabic MT. Sarikaya and Deng (2007) use joint morphological-lexical language models to re-rank the output of English dialectal-Arabic MT, and Badr et al. (2008) report results on the value of the morphological decomposition of Arabic during training and describe different techniques for re-composition of Arabic in the output. We differ from the previous efforts targeting Arabic in that (1) we do not address morphology issues through segmentation (more on this in section 3) and (2) we focus on utilizing syntactic knowledge to address the reordering challenges of this translation direction. 3 Arabic Syntactic Issues Arabic is a m</context>
</contexts>
<marker>Sarikaya, Deng, 2007</marker>
<rawString>R. Sarikaya and Y. Deng. 2007. Joint morphologicallexical language modeling for machine translation. In Proceedings of HLT-NAACL’07, Short Papers, Rochester, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>Srilm – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<location>Denver, CO, USA.</location>
<contexts>
<context position="13760" citStr="Stolcke, 2002" startWordPosition="2270" endWordPosition="2271">llow better understanding of the decisions being made. In section 6.3, extracted rules are exemplified and analyzed. The probabilities of the rules are estimated using Maximum Likelihood Estimation based on the information supplied by Ripper on the performance of the individual rules on the training data. These logarithmic probabilities are easily integratable in the log-linear PSMT model as an additional parameter by simple addition. 5 The PSMT system Our baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002). The decoder used for the baseline system is Pharaoh (Koehn, 2004) with its distance-penalizing reordering model. Since Pharaoh does not support word lattice input, we use our own decoder for the experiments. Except for the reordering model, it uses the same knowledge sources as Pharaoh, i.e. a bidirectional phrase translation model, a lexical weight model, phrase and word penalties, and a target language model. Its behavior is comparable to Pharaoh when doing monotone decoding. The search algorithm of our decoder is similar to the RG graph decoder of (Zens et al., 2002). It expects a word la</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. Srilm – an extensible language modeling toolkit. In Proceedings of the International Conference on Spoken Language Processing, Denver, CO, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Wang</author>
<author>M Collins</author>
<author>P Koehn</author>
</authors>
<title>Chinese syntactic reordering for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3995" citStr="Wang et al., 2007" startWordPosition="592" endWordPosition="595"> et al., 2006). While several recent authors using a pre-translation (sourceside) reordering approach have achieved positive results, it has been difficult to integrate syntactic Proceedings of the EACL 2009 Workshop on Computational Approaches to Semitic Languages, pages 69–77, Athens, Greece, 31 March, 2009. c�2009 Association for Computational Linguistics 69 information while retaining the strengths of the statistical approach. In some studies, reordering decisions are done “deterministically” by supplying the decoder with a canonical word order (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007; Habash, 2007). These reordering rules are either manually specified or automatically learned from alignments; and they are always placed outside the actual PSMT system. By contrast, other studies (Crego and Mariño, 2007; Zhang et al., 2007; Li et al., 2007; Elming, 2008) are more in the spirit of PSMT, in that multiple reorderings are presented to the PSMT system as (possibly weighted) options that are allowed to contribute alongside other parameters. Specifically, we follow the pre-translation reordering approach of Elming (2008). This approach has been proven to remedy shortcomings of othe</context>
</contexts>
<marker>Wang, Collins, Koehn, 2007</marker>
<rawString>C. Wang, M. Collins, and P. Koehn. 2007. Chinese syntactic reordering for statistical machine translation. In Proceedings of EMNLP-CoNLL, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Xia</author>
<author>M McCord</author>
</authors>
<title>Improving a statistical mt system with automatically learned rewrite patterns.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING’04,</booktitle>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="3954" citStr="Xia and McCord, 2004" startWordPosition="584" endWordPosition="587">t a stage that makes it usable in MT (Habash et al., 2006). While several recent authors using a pre-translation (sourceside) reordering approach have achieved positive results, it has been difficult to integrate syntactic Proceedings of the EACL 2009 Workshop on Computational Approaches to Semitic Languages, pages 69–77, Athens, Greece, 31 March, 2009. c�2009 Association for Computational Linguistics 69 information while retaining the strengths of the statistical approach. In some studies, reordering decisions are done “deterministically” by supplying the decoder with a canonical word order (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007; Habash, 2007). These reordering rules are either manually specified or automatically learned from alignments; and they are always placed outside the actual PSMT system. By contrast, other studies (Crego and Mariño, 2007; Zhang et al., 2007; Li et al., 2007; Elming, 2008) are more in the spirit of PSMT, in that multiple reorderings are presented to the PSMT system as (possibly weighted) options that are allowed to contribute alongside other parameters. Specifically, we follow the pre-translation reordering approach of Elming (2008). This approach has b</context>
</contexts>
<marker>Xia, McCord, 2004</marker>
<rawString>F. Xia and M. McCord. 2004. Improving a statistical mt system with automatically learned rewrite patterns. In Proceedings of COLING’04, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yamada</author>
<author>K Knight</author>
</authors>
<title>A Syntax-Based Statistical Translation Model.</title>
<date>2001</date>
<booktitle>In Proceedings of ACL’01,</booktitle>
<location>Toulouse, France.</location>
<contexts>
<context position="3177" citStr="Yamada and Knight, 2001" startWordPosition="465" endWordPosition="468"> contrast related work. Section 3 describes aspects of English and Arabic structure that are relevant to reordering. Section 4 describes the automatic induction of reordering rules and its integration in PSMT. In section 5, we describe the SMT system used in the experiments. In section 6, we evaluate and discuss the results of our English-Arabic MT system. 2 Related Work Much work has been done in syntactic reordering for SMT, focusing on both source and targetlanguage syntax. In this paper, we adapt an approach that utilizes source-syntax information as opposed to target-side syntax systems (Yamada and Knight, 2001; Galley et al., 2004). This is because we are translating from English to Arabic and we are discouraged by recent results indicating Arabic parsing is not at a stage that makes it usable in MT (Habash et al., 2006). While several recent authors using a pre-translation (sourceside) reordering approach have achieved positive results, it has been difficult to integrate syntactic Proceedings of the EACL 2009 Workshop on Computational Approaches to Semitic Languages, pages 69–77, Athens, Greece, 31 March, 2009. c�2009 Association for Computational Linguistics 69 information while retaining the str</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>K. Yamada and K. Knight. 2001. A Syntax-Based Statistical Translation Model. In Proceedings of ACL’01, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zens</author>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Phrase-based statistical machine translation.</title>
<date>2002</date>
<booktitle>2002: Advances in Arti�cial Intelligence. 25. Annual German Conference on AI.</booktitle>
<editor>In M. Jarke, J. Koehler, and G. Lakemeyer, editors, KI</editor>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="14338" citStr="Zens et al., 2002" startWordPosition="2361" endWordPosition="2364">a trigram language model (Stolcke, 2002). The decoder used for the baseline system is Pharaoh (Koehn, 2004) with its distance-penalizing reordering model. Since Pharaoh does not support word lattice input, we use our own decoder for the experiments. Except for the reordering model, it uses the same knowledge sources as Pharaoh, i.e. a bidirectional phrase translation model, a lexical weight model, phrase and word penalties, and a target language model. Its behavior is comparable to Pharaoh when doing monotone decoding. The search algorithm of our decoder is similar to the RG graph decoder of (Zens et al., 2002). It expects a word lattice as input. Figure 3 shows the word lattice for the example in table 2. In the example used here, we choose to focus on the reordering of adjective and noun. For readability, we do not describe the possibility of reordering the subject and verb. This will also be the case in later use of the example. Since the input format defines all possible word orders allowed by the rule set, a simple monotone search is sufficient. Using a language model of order n, for each hypothesized target string ending in the same n-1-gram, we only have to extend the highest scoring hypothes</context>
</contexts>
<marker>Zens, Och, Ney, 2002</marker>
<rawString>R. Zens, F. J. Och, and H. Ney. 2002. Phrase-based statistical machine translation. In M. Jarke, J. Koehler, and G. Lakemeyer, editors, KI - 2002: Advances in Arti�cial Intelligence. 25. Annual German Conference on AI. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>S Vogel</author>
<author>A Waibel</author>
</authors>
<title>Interpreting bleu/nist scores: How much improvement do we need to have a better system?</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC’04),</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="21557" citStr="Zhang et al., 2004" startWordPosition="3590" endWordPosition="3593">ldc.upenn.edu/. 5 http://www.nist.gov/speech/tests/mt/ 73 System Dev MT04 MT05 Pharaoh Free 28.37 23.53 24.79 Pharaoh DL4 29.52 24.72 25.88 Pharaoh Monotone 27.93 23.55 24.72 MAN NO weight 29.53 24.72 25.82 SO weight 29.43 24.74 25.82 TO weight 29.40 24.78 25.93 GDF NO weight 29.87 25.11 26.04 SO weight 29.84 25.06 26.01 TO weight 29.95 25.17 26.09 Table 3: Automatic evaluation scores for different systems using rules extracted from manual alignments (MAN) and automatic alignments (GDF). The TO system using GDF rules is significantly better than the light grey cells at a 95% confidence level (Zhang et al., 2004). each Arabic sentence, we invert the sets by concatenating all English sentences to one file. This means that the Arabic reference file contains four duplicates of each sentence. Each duplicate is the reference of a different English source sentence. Following this merger, MT04 consists of 5.4K sentences with 193K English and 144K Arabic words, and MT05 consists of 4.2K sentences with 143K English and 114K Arabic words. MT04 is a mix of domains containing speeches, editorials and newswire texts. On the other hand, MT05 is only newswire. The NIST MTEval test set from 2002 (MT02) is split into </context>
</contexts>
<marker>Zhang, Vogel, Waibel, 2004</marker>
<rawString>Y. Zhang, S. Vogel, and A. Waibel. 2004. Interpreting bleu/nist scores: How much improvement do we need to have a better system? In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC’04), Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>Improved chunk-level reordering for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the IWSLT,</booktitle>
<location>Trento, Italy.</location>
<contexts>
<context position="4236" citStr="Zhang et al., 2007" startWordPosition="630" endWordPosition="633">ches to Semitic Languages, pages 69–77, Athens, Greece, 31 March, 2009. c�2009 Association for Computational Linguistics 69 information while retaining the strengths of the statistical approach. In some studies, reordering decisions are done “deterministically” by supplying the decoder with a canonical word order (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007; Habash, 2007). These reordering rules are either manually specified or automatically learned from alignments; and they are always placed outside the actual PSMT system. By contrast, other studies (Crego and Mariño, 2007; Zhang et al., 2007; Li et al., 2007; Elming, 2008) are more in the spirit of PSMT, in that multiple reorderings are presented to the PSMT system as (possibly weighted) options that are allowed to contribute alongside other parameters. Specifically, we follow the pre-translation reordering approach of Elming (2008). This approach has been proven to remedy shortcomings of other pre-translation reordering approaches by reordering the input word sequence, but scoring the output word sequence. Elming (2008) only examined the approach within English – Danish, a language pair that displays little reordering. By contra</context>
<context position="23394" citStr="Zhang et al., 2007" startWordPosition="3909" endWordPosition="3912">DL4), (c) Pharaoh with monotone decoding (Pharaoh Monotone), and (d) a system provided with a rule reordered word lattice but no (NO) weighting in the spirit of (Crego and Mariño, 2007), (e) the same system but with a source order System MT04 MT05 Avr. human Pharaoh Free 24.07 25.15 3.0 (2.80) Pharaoh DL4 25.42 26.51 — NO scoring 25.68 26.29 2.5 (2.43) SO scoring 25.42 26.02 2.5 (2.64) TO scoring 25.98 26.49 2.0 (2.08) Table 4: Evaluation on the diff set. Average human ratings are medians with means in parenthesis, lower scores are better, 1 is the best score. (SO) weighting in the spirit of (Zhang et al., 2007; Li et al., 2007), and finally (f) the same system but with the target order (TO) weighting. In addition to evaluating the reordering approaches, we also report on supplying them with different reordering rule sets: a set that was learned on manually aligned data (MAN), and a set learned on the same data but with automatic alignments (GDF). 6.2.1 Overall Results Pharaoh Monotone performs similarly to Pharaoh Free. This shows that the question of improved reordering is not about quantity, but rather quality: what constraints are optimal to generate the best word order. The TO approach gets an </context>
</contexts>
<marker>Zhang, Zens, Ney, 2007</marker>
<rawString>Y. Zhang, R. Zens, and H. Ney. 2007. Improved chunk-level reordering for statistical machine translation. In Proceedings of the IWSLT, Trento, Italy.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>