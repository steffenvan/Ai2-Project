<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000209">
<title confidence="0.991373">
A Fast Algorithm for Feature Selection in Conditional
Maximum Entropy Modeling
</title>
<author confidence="0.997408">
Yaqian Zhou
</author>
<affiliation confidence="0.9963845">
Computer Science Department
Fudan University
</affiliation>
<address confidence="0.883139">
Shanghai 200433, P.R. China
</address>
<email confidence="0.997872">
archzhou@yahoo.com
</email>
<author confidence="0.997405">
Lide Wu
</author>
<affiliation confidence="0.9984685">
Computer Science Department
Fudan University
</affiliation>
<address confidence="0.566052">
Shanghai 200433, P.R. China
</address>
<email confidence="0.964202">
ldwu@fudan.edu.cn
</email>
<author confidence="0.987816">
Fuliang Weng
</author>
<affiliation confidence="0.772724">
Research and Technology Center
Robert Bosch Corp.
</affiliation>
<address confidence="0.934909">
Palo Alto, CA 94304, USA
</address>
<email confidence="0.995371">
Fuliang.weng@rtc.bosch.com
</email>
<author confidence="0.994079">
Hauke Schmidt
</author>
<affiliation confidence="0.8883255">
Research and Technology Center
Robert Bosch Corp.
</affiliation>
<address confidence="0.755522">
Palo Alto, CA 94304, USA
</address>
<email confidence="0.988838">
hauke.schmidt@rtc.bosch.com
</email>
<sectionHeader confidence="0.99426" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999980454545455">
This paper describes a fast algorithm that se-
lects features for conditional maximum en-
tropy modeling. Berger et al. (1996) presents
an incremental feature selection (IFS) algo-
rithm, which computes the approximate gains
for all candidate features at each selection
stage, and is very time-consuming for any
problems with large feature spaces. In this
new algorithm, instead, we only compute the
approximate gains for the top-ranked features
based on the models obtained from previous
stages. Experiments on WSJ data in Penn
Treebank are conducted to show that the new
algorithm greatly speeds up the feature selec-
tion process while maintaining the same qual-
ity of selected features. One variant of this
new algorithm with look-ahead functionality
is also tested to further confirm the good
quality of the selected features. The new algo-
rithm is easy to implement, and given a fea-
ture space of size F, it only uses O(F) more
space than the original IFS algorithm.
</bodyText>
<sectionHeader confidence="0.999161" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996345686046512">
Maximum Entropy (ME) modeling has received
a lot of attention in language modeling and natural
language processing for the past few years (e.g.,
Rosenfeld, 1994; Berger et al 1996; Ratnaparkhi,
1998; Koeling, 2000). One of the main advantages
using ME modeling is the ability to incorporate
various features in the same framework with a
sound mathematical foundation. There are two
main tasks in ME modeling: the feature selection
process that chooses from a feature space a subset
of good features to be included in the model; and
the parameter estimation process that estimates the
weighting factors for each selected feature in the
exponential model. This paper is primarily con-
cerned with the feature selection process in ME
modeling.
While the majority of the work in ME modeling
has been focusing on parameter estimation, less
effort has been made in feature selection. This is
partly because feature selection may not be neces-
sary for certain tasks when parameter estimate al-
gorithms are fast. However, when a feature space
is large and complex, it is clearly advantageous to
perform feature selection, which not only speeds
up the probability computation and requires
smaller memory size during its application, but
also shortens the cycle of model selection during
the training.
Feature selection is a very difficult optimization
task when the feature space under investigation is
large. This is because we essentially try to find a
best subset from a collection of all the possible
feature subsets, which has a size of 2|W|, where |W|
is the size of the feature space.
In the past, most researchers resorted to a sim-
ple count cutoff technique for selecting features
(Rosenfeld, 1994; Ratnaparkhi, 1998; Reynar and
Ratnaparkhi, 1997; Koeling, 2000), where only the
features that occur in a corpus more than a pre-
defined cutoff threshold get selected. Chen and
Rosenfeld (1999) experimented on a feature selec-
tion technique that uses a [2 test to see whether a
feature should be included in the ME model, where
the [2 test is computed using the count from a prior
distribution and the count from the real training
data. It is a simple and probably effective tech-
nique for language modeling tasks. Since ME
models are optimized using their likelihood or
likelihood gains as the criterion, it is important to
establish the relationship between [2 test score and
�
the likelihood gain, which, however, is absent.
Berger et al. (1996) presented an incremental fea-
ture selection (IFS) algorithm where only one fea-
ture is added at each selection and the estimated
parameter values are kept for the features selected
in the previous stages. While this greedy search
assumption is reasonable, the speed of the IFS al-
gorithm is still an issue for complex tasks. For
better understanding its performance, we re-
implemented the algorithm. Given a task of
600,000 training instances, it takes nearly four
days to select 1000 features from a feature space
with a little more than 190,000 features. Berger
and Printz (1998) proposed an o-orthogonal condi-
tion for selecting k features at the same time with-
out affecting much the quality of the selected
features. While this technique is applicable for
certain feature sets, such as word link features re-
ported in their paper, the aorthogonal condition
usually does not hold if part-of-speech tags are
dominantly present in a feature subset. Past work,
including Ratnaparkhi (1998) and Zhou et al
(2003), has shown that the IFS algorithm utilizes
much fewer features than the count cutoff method,
while maintaining the similar precision and recall
on tasks, such as prepositional phrase attachment,
text categorization and base NP chunking. This
leads us to further explore the possible improve-
ment on the IFS algorithm.
In section 2, we briefly review the IFS algo-
rithm. Then, a fast feature selection algorithm is
described in section 3. Section 4 presents a number
of experiments, which show a massive speed-up
and quality feature selection of the new algorithm.
Finally, we conclude our discussion in section 5.
</bodyText>
<sectionHeader confidence="0.682411" genericHeader="method">
2 The Incremental Feature Selection Al-
gorithm
</sectionHeader>
<bodyText confidence="0.999913">
For better understanding of our new algorithm, we
start with briefly reviewing the IFS feature selec-
tion algorithm. Suppose the conditional ME model
takes the following form:
</bodyText>
<equation confidence="0.986775">
p(y  |x) = Z&apos;exp(å lj fj (x,y))
j
</equation>
<bodyText confidence="0.999304285714286">
where fj are the features, 0j are their corre-
sponding weights, and Z(x) is the normalization
factor.
The algorithm makes the approximation that the
addition of a feature f in an exponential model af-
fects only its associated weight 0, leaving un-
changed the 0-values associated with the other
features. Here we only present a sketch of the algo-
rithm in Figure 1. Please refer to the original paper
for the details.
In the algorithm, we use I for the number of
training instances, Y for the number of output
classes, and F for the number of candidate features
or the size of the candidate feature set.
</bodyText>
<figureCaption confidence="0.996856">
Figure 1: A Variant of the IFS Algorithm.
</figureCaption>
<equation confidence="0.391346615384615">
0. Initialize: S = 0, sum[1..I, 1..Y] = 1,
z[1..I] = Y
1. Gain computation:
MaxGain = 0
for f in feature space F do
aˆ = argmaxGSÈf(a
a
g =
ˆ mo GSor(a
if MaxGain &lt; gˆ then
MaxGain = gˆ
f* = f
0*=aˆ
</equation>
<listItem confidence="0.42623675">
2. Feature selection:
S = S O { f* }
3. if termination condition is met, then stop
4. Model adjustment:
</listItem>
<bodyText confidence="0.726889">
for instance i such that there is y
and f*(xi, y) = 1 do
</bodyText>
<equation confidence="0.823844333333333">
z[i] 0=sum[i, y]
sum[i, y] 0= exp(Lt)
z[i] += sum[i, y]
5. go to step 1.
)
)
</equation>
<bodyText confidence="0.999357428571429">
One difference here from the original IFS algo-
rithm is that we adopt a technique in (Goodman,
2002) for optimizing the parameters in the condi-
tional ME training. Specifically, we use array z to
store the normalizing factors, and array sum for all
the un-normalized conditional probabilities sum[i,
y]. Thus, one only needs to modify those sum[i, y]
that satisfy f*(xi, y)=1, and to make changes to their
�
corresponding normalizing factors z[i]. In contrast
to what is shown in Berger et al 1996’s paper, here
is how the different values in this variant of the IFS
algorithm are computed.
Let us denote
</bodyText>
<equation confidence="0.989896166666667">
sum(y  |x) exp( l j f j ( x , y ))
= å
j
Z(x) sum ( y  |x )
= å
y
</equation>
<bodyText confidence="0.9972375">
Then, the model can be represented by sum(y|x)
and Z(x) as follows:
</bodyText>
<equation confidence="0.88837">
p(y  |x) = sum(y  |x) / Z(x
</equation>
<bodyText confidence="0.998400875">
where sum(y|xi) and Z(xi) correspond to sum[i,y]
and z[i] in Figure 1, respectively.
Assume the selected feature set is S, and f is
currently being considered. The goal of each se-
lection stage is to select the feature f that maxi-
mizes the gain of the log likelihood, where the a
and gain of f are derived through following steps:
Let the log likelihood of the model be
</bodyText>
<equation confidence="0.962380666666667">
L(p) = -å p ~ x
( ,
x,y
y) log( (  |) / ( ))
sum y x Z x
x,y
</equation>
<bodyText confidence="0.981571">
and the empirical expectation of feature f be
</bodyText>
<equation confidence="0.921406">
Ep÷ ( f ) = p÷ (x,y) f (x,y)
å
x,y
</equation>
<bodyText confidence="0.9999545">
With the approximation assumption in Berger
et al (1996)’s paper, the un-normalized component
and the normalization factor of the model have the
following recursive forms:
</bodyText>
<equation confidence="0.897741">
sumSaÈ f (y  |x) = sumS (y  |x) • ea
Za
f (x) = ZS (x) - sumS (y  |x)
+ sumso f
</equation>
<bodyText confidence="0.90621">
The approximate gain of the log likelihood is
computed by
</bodyText>
<equation confidence="0.978295666666667">
GSo f (B) = L(pS� f
B ) [] L(pS)
= � ÷p (x)(logZS� f ,B(x)
� /ZS(x))
x
+BEp÷ ( f ) (1)
</equation>
<bodyText confidence="0.99864">
The maximum approximate gain and its corre-
sponding a are represented as:
</bodyText>
<figure confidence="0.665311285714286">
~ ( , ) max ( a
D L S f = GS È f
a
~O ( , ) arg
S f = max ( O
GS o f
O
</figure>
<sectionHeader confidence="0.928992" genericHeader="method">
3 A Fast Feature Selection Algorithm
</sectionHeader>
<bodyText confidence="0.999867083333333">
The inefficiency of the IFS algorithm is due to the
following reasons. The algorithm considers all the
candidate features before selecting one from them,
and it has to re-compute the gains for every feature
at each selection stage. In addition, to compute a
parameter using Newton’s method is not always
efficient. Therefore, the total computation for the
whole selection processing can be very expensive.
Let g(j, k) represent the gain due to the addition
of feature fj to the active model at stage k. In our
experiments, it is found even if D (i.e., the addi-
tional number of stages after stage k) is large, for
most j, g(j, k+D) - g(j, k) is a negative number or at
most a very small positive number. This leads us to
use the g(j, k) to approximate the upper bound of
g(j, k+D).
The intuition behind our new algorithm is that
when a new feature is added to a model, the gains
for the other features before the addition and after
the addition do not change much. When there are
changes, their actual amounts will mostly be within
a narrow range across different features from top
ranked ones to the bottom ranked ones. Therefore,
we only compute and compare the gains for the
features from the top-ranked downward until we
reach the one with the gain, based on the new
model, that is bigger than the gains of the remain-
ing features. With a few exceptions, the gains of
the majority of the remaining features were com-
puted based on the previous models.
As in the IFS algorithm, we assume that the ad-
dition of a feature f only affects its weighting fac-
tor a. Because a uniform distribution is assumed as
the prior in the initial stage, we may derive a
closed-form formula for a(j, 0) and g(j, 0) as fol-
lows.
</bodyText>
<equation confidence="0.989946111111111">
)
))
y ) log( ( |
p y
-
å ~ ( ,
p x
(  |)
y x
)
)
Let
Ed ( f ) =÷p (x)max
å { f (x,y)}
y
x
Re( f ) = Ep÷ ( f )/Ed ( f )
p 0 =1 /Y
Then
a ( 0) = log(Re(f) . 1-p0 )
l�� P0 1-Re(f)
g(j,0) = L(p!» f
a(i,0)) - L(po)
= Ed ( f )[Re( f )log Re(f)
p0
+ (1- Re( f ))log1-Re ( f )
1- p0 ]
</equation>
<bodyText confidence="0.999934384615385">
where 0 denotes an empty set, po is the uni-
form distribution. The other steps for computing
the gains and selecting the features are given in
Figure 2 as a pseudo code. Because we only com-
pute gains for a small number of top-ranked fea-
tures, we call this feature selection algorithm as
Selective Gain Computation (SGC) Algorithm.
In the algorithm, we use array g to keep the
sorted gains and their corresponding feature indi-
ces. In practice, we use a binary search tree to
maintain the order of the array.
The key difference between the IFS algorithm
and the SGC algorithm is that we do not evaluate
all the features for the active model at every stage
(one stage corresponds to the selection of a single
feature). Initially, the feature candidates are or-
dered based on their gains computed on the uni-
form distribution. The feature with the largest gain
gets selected, and it forms the model for the next
stage. In the next stage, the gain of the top feature
in the ordered list is computed based on the model
just formed in the previous stage. This gain is
compared with the gains of the rest features in the
list. If this newly computed gain is still the largest,
this feature is added to form the model at stage 3.
If the gain is not the largest, it is inserted in the
ordered list so that the order is maintained. In this
case, the gain of the next top-ranked feature in the
ordered list is re-computed using the model at the
current stage, i.e., stage 2.
This process continues until the gain of the top-
ranked feature computed under the current model
is still the largest gain in the ordered list. Then, the
model for the next stage is created with the addi-
tion of this newly selected feature. The whole fea-
ture selection process stops either when the
number of the selected features reaches a pre-
defined value in the input, or when the gains be-
come too small to be useful to the model.
</bodyText>
<figureCaption confidence="0.9130585">
Figure 2: Selective Gain Computation Algo-
rithm for Feature Selection
</figureCaption>
<bodyText confidence="0.999765111111111">
In addition to this basic version of the SGC al-
gorithm, at each stage, we may also re-compute
additional gains based on the current model for a
pre-defined number of features listed right after
feature f* (obtained in step 2) in the ordered list.
This is to make sure that the selected feature f* is
indeed the feature with the highest gain within the
pre-defined look-ahead distance. We call this vari-
ant the look-ahead version of the SGC algorithm.
</bodyText>
<figure confidence="0.998452815789474">
0. Initialize: S = 0, sum[1..I, 1..Y] = 1,
z[1..I] = Y, g[1..F] = {g(1,0),...,g(F,0)}
1. Gain computation:
MaxGain = 0
Loop
fj
Ig[
1, . . . ,
]}
F
if g[j] £ MaxGain then go to step 2
else
aˆ=argmaxGSÈf(a
a
g =
ˆ max ( a D
GS O f
a
g[j]= gˆ
if MaxGain &lt; gˆ then
MaxGain = gˆ
f* = fj
a*=aˆ
2. Feature selection:
S = S O { f* }
3. if termination condition is met, then stop
4. Model adjustment:
for instance i such that there is y
and f*(xi, y) = 1 do
z[i] -=sum[i, y]
sum[i, y] ´= exp(a*)
z[i] += sum[i, y]
5. go to step 1.
)
arg max
f
F
in
</figure>
<sectionHeader confidence="0.868912" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.997588770491803">
A number of experiments have been conducted to
verify the rationale behind the algorithm. In par-
ticular, we would like to have a good understand-
ing of the quality of the selected features using the
SGC algorithm, as well as the amount of speed-
ups, in comparison with the IFS algorithm.
The first sets of experiments use a dataset {(x,
y)}, derived from the Penn Treebank, where x is a
10 dimension vector including word, POS tag and
grammatical relation tag information from two ad-
jacent regions, and y is the grammatical relation
tag between the two regions. Examples of the
grammatical relation tags are subject and object
with either the right region or the left region as the
head. The total number of different grammatical
tags, i.e., the size of the output space, is 86. There
are a little more than 600,000 training instances
generated from section 02-22 of WSJ in Penn
Treebank, and the test corpus is generated from
section 23.
In our experiments, the feature space is parti-
tioned into sub-spaces, called feature templates,
where only certain dimensions are included. Con-
sidering all the possible combinations in the 10-
dimensional space would lead to 210 feature tem-
plates. To perform a feasible and fair comparison,
we use linguistic knowledge to filter out implausi-
ble subspaces so that only 24 feature templates are
actually used. With this amount of feature tem-
plates, we get more than 1,900,000 candidate fea-
tures from the training data. To speed up the
experiments, which is necessary for the IFS algo-
rithm, we use a cutoff of 5 to reduce the feature
space down to 191,098 features. On average, each
candidate feature covers about 485 instances,
which accounts for 0.083% over the whole training
instance set and is computed through:
The first experiment is to compare the speed of
the IFS algorithm with that of SGC algorithm.
Theoretically speaking, the IFS algorithm com-
putes the gains for all the features at every stage.
This means that it requires O(NF) time to select a
feature subset of size N from a candidate feature
set of size F. On the other hand, the SGC algorithm
considers much fewer features, only 24.1 features
on average at each stage, when selecting a feature
from the large feature space in this experiment.
Figure 3 shows the average number of features
computed at the selected points for the SGC algo-
rithm, SGC with 500 look-ahead, as well as the
IFS algorithm. The averaged number of features is
taken over an interval from the initial stage to the
current feature selection point, which is to smooth
out the fluctuation of the numbers of features each
selection stage considers. The second algorithm
looks at an additional fixed number of features,
500 in this experiment, beyond the ones considered
by the basic SGC algorithm. The last algorithm has
a linear decreasing number of features to select,
because the selected features will not be consid-
ered again. In Figure 3, the IFS algorithm stops
</bodyText>
<figureCaption confidence="0.829697428571429">
after 1000 features are selected. This is because it
takes too long for this algorithm to complete the
entire selection process. The same thing happens in
Figure 4, which is to be explained below.
Figure 3: The log number of features considered in
SGC algorithm, in comparison with the IFS algo-
rithm.
</figureCaption>
<bodyText confidence="0.999931">
To see the actual amount of time taken by the
SGC algorithms and the IFS algorithm with the
currently available computing power, we use a
Linux workstation with 1.6Ghz dual Xeon CPUs
and 1 GB memory to run the two experiments si-
multaneously. As it can be expected, excluding the
beginning common part of the code from the two
algorithms, the speedup from using the SGC algo-
rithm is many orders of magnitude, from more than
100 times to thousands, depending on the number
of features selected. The results are shown in Fig-
ure 4.
</bodyText>
<figure confidence="0.9977939375">
Average Considered Feature Number
log10(Y)
4
2
0
6
5
3
1
200 400 600 800 1000 2000 4000 6000 8000 10000
Berger SGC-0 SGC-500
Number of Selected Features
ac = 00 0
f j ( x , y ) / 1
j
j x,y
</figure>
<figureCaption confidence="0.996777">
Figure 4: The log time used by SGC algorithm, in
comparison with the IFS algorithm.
</figureCaption>
<bodyText confidence="0.999866">
To verify the quality of the selected features
using our SGC algorithm, we conduct four experi-
ments: one uses all the features to build a condi-
tional ME model, the second uses the IFS
algorithm to select 1,000 features, the third uses
our SGC algorithm, the fourth uses the SGC algo-
rithm with 500 look-ahead, and the fifth takes the
top n most frequent features in the training data.
The precisions are computed on section 23 of the
WSJ data set in Penn Treebank. The results are
listed in Figure 5. Three factors can be learned
from this figure. First, the three IFS and SGC algo-
rithms perform similarly. Second, 3000 seems to
</bodyText>
<figureCaption confidence="0.896101">
Figure 5: Precision results from models using the whole
feature set and the feature subsets through the IFS algo-
rithm, the SGC algorithm, the SGC algorithm with 500
look-ahead, and the count cutoff algorithm.
</figureCaption>
<bodyText confidence="0.999992818181818">
be a dividing line: when the models include fewer
than 3000 selected features, the IFS and SGC algo-
rithms do not perform as well as the model with all
the features; when the models include more than
3000 selected features, their performance signifi-
cantly surpass the model with all the features. The
inferior performance of the model with all the fea-
tures at the right side of the chart is likely due to
the data over-fitting problem. Third, the simple
count cutoff algorithm significantly under-
performs the other feature selection algorithms
when feature subsets with no more than 10,000
features are considered.
To further confirm the findings regarding preci-
sion, we conducted another experiment with Base
NP recognition as the task. The experiment uses
section 15-18 of WSJ as the training data, and sec-
tion 20 as the test data. When we select 1,160 fea-
tures from a simple feature space using our SGC
algorithm, we obtain a precision/recall of
92.75%/93.25%. The best reported ME work on
this task includes Koeling (2000) that has the pre-
cision/recall of 92.84%/93.18% with a cutoff of 5,
and Zhou et al. (2003) has reached the perform-
ance of 93.04%/93.31% with cutoff of 7 and
reached a performance of 92.46%/92.74% with
615 features using the IFS algorithm. While the
results are not directly comparable due to different
feature spaces used in the above experiments, our
result is competitive to these best numbers. This
shows that our new algorithm is both very effective
in selecting high quality features and very efficient
in performing the task.
</bodyText>
<sectionHeader confidence="0.985638" genericHeader="conclusions">
5 Comparison and Conclusion
</sectionHeader>
<bodyText confidence="0.999827285714286">
Feature selection has been an important topic in
both ME modeling and linear regression. In the
past, most researchers resorted to count cutoff
technique in selecting features for ME modeling
(Rosenfeld, 1994; Ratnaparkhi, 1998; Reynar and
Ratnaparkhi, 1997; Koeling, 2000). A more refined
algorithm, the incremental feature selection algo-
rithm by Berger et al (1996), allows one feature
being added at each selection and at the same time
keeps estimated parameter values for the features
selected in the previous stages. As discussed in
(Ratnaparkhi, 1998), the count cutoff technique
works very fast and is easy to implement, but has
the drawback of containing a large number of re-
</bodyText>
<figure confidence="0.996032161290323">
Average Time
for each selection step
(second)
log10(Y)
-1
-2
2
0
3
1
200 400 600 800 1000 2000 4000 6000 8000 10000
Number of Selected Features
Berger SGC-0
Precision (%)
94
92
90
78
76
74
72
70
88
86
84
82
80
200 400 600 800 1000 2000 4000 6000 8000 10000
Number of Selected Features
All (191098) IFS SGC-0
SGC-500 Count Cutoff
</figure>
<bodyText confidence="0.999778583333334">
dundant features. In contrast, the IFS removes the
redundancy in the selected feature set, but the
speed of the algorithm has been a big issue for
complex tasks. Having realized the drawback of
the IFS algorithm, Berger and Printz (1998) pro-
posed an f-orthogonal condition for selecting k
features at the same time without affecting much
the quality of the selected features. While this
technique is applicable for certain feature sets,
such as link features between words, the f-
orthogonal condition usually does not hold if part-
of-speech tags are dominantly present in a feature
subset.
Chen and Rosenfeld (1999) experimented on a
feature selection technique that uses a 02 test to see
whether a feature should be included in the ME
model, where the 02 test is computed using the
counts from a prior distribution and the counts
from the real training data. It is a simple and
probably effective technique for language model-
ing tasks. Since ME models are optimized using
their likelihood or likelihood gains as the criterion,
it is important to establish the relationship between
02 test score and the likelihood gain, which, how-
ever, is absent.
There is a large amount of literature on feature
selection in linear regression, where least mean
squared errors measure has been the primary opti-
mization criterion. Two issues need to be ad-
dressed in order to effectively use these techniques.
One is the scalability issue since most statistical
literature on feature selection only concerns with
dozens or hundreds of features, while our tasks
usually deal with feature sets with a million of
features. The other is the relationship between
mean squared errors and likelihood, similar to the
concern expressed in the previous paragraph.
These are important issues and require further in-
vestigation.
In summary, this paper presents our new im-
provement to the incremental feature selection al-
gorithm. The new algorithm runs hundreds to
thousands times faster than the original incre-
mental feature selection algorithm. In addition, the
new algorithm selects the features of a similar
quality as the original Berger et al algorithm,
which has also shown to be better than the simple
cutoff method in some cases.
</bodyText>
<sectionHeader confidence="0.96008" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9999753">
This work is done while the first author is visiting
the Center for Study of Language and Information
(CSLI) at Stanford University and the Research
and Technology Center of Robert Bosch Corpora-
tion. This project is sponsored by the Research and
Technology Center of Robert Bosch Corporation.
We are grateful to the kind support from Prof.
Stanley Peters of CSLI. We also thank the com-
ments from the three anonymous reviewers which
improve the quality of the paper.
</bodyText>
<sectionHeader confidence="0.999208" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999743088235294">
Adam L. Berger, Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A Maximum Entropy Approach
to Natural Language Processing. Computational Lin-
guistic, 22 (1): 39-71.
Adam L. Berger and Harry Printz. 1998. A Comparison
of Criteria for Maximum Entropy / Minimum Diver-
gence Feature Selection. Proceedings of the 3rd con-
ference on Empirical Methods in Natural Language
Processing. Granda, Spain.
Stanley Chen and Ronald Rosenfeld. 1999. Efficient
Sampling and Feature Selection in Whole Sentence
maximum Entropy Language Models. Proceedings of
ICASSP-1999, Phoenix, Arizona.
Joshua Goodman. 2002. Sequential Conditional Gener-
alized Iterative Scaling. Association for Computa-
tional Linguistics, Philadelphia, Pennsylvania.
Rob Koeling. 2000. Chunking with Maximum Entropy
Models. In: Proceedings of CoNLL-2000 and LLL-
2000, Lisbon, Portugal, 139-141.
Adwait Ratnaparkhi. 1998. Maximum Entropy Models
for Natural Language Ambiguity Resolution. Ph.D.
thesis, University of Pennsylvania.
Ronald Rosenfeld. 1994. Adaptive Statistical Language
Modeling: A Maximum Entropy Approach. Ph.D.
thesis, Carnegie Mellon University, April.
J. Reynar and A. Ratnaparkhi. 1997. A Maximum En-
tropy Approach to Identifying Sentence Boundaries.
In: Proceedings of the Fifth Conference on Applied
Natural Language Processing, Washington D.C., 16-
19.
Zhou Ya-qian, Guo Yi-kun, Huang Xuan-jing, and Wu
Li-de. 2003. Chinese and English BaseNP Recog-
nized by Maximum Entropy. Journal of Computer
Research and Development. 40(3):440-446, Beijin
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.236022">
<title confidence="0.9984665">A Fast Algorithm for Feature Selection in Conditional Maximum Entropy Modeling</title>
<author confidence="0.958618">Yaqian</author>
<affiliation confidence="0.9300565">Computer Science Fudan</affiliation>
<address confidence="0.978384">Shanghai 200433, P.R. China</address>
<email confidence="0.999751">archzhou@yahoo.com</email>
<author confidence="0.893517">Lide</author>
<affiliation confidence="0.881024">Computer Science Fudan</affiliation>
<address confidence="0.839769">Shanghai 200433, P.R.</address>
<email confidence="0.749496">ldwu@fudan.edu.cn</email>
<title confidence="0.926598">Fuliang Research and Technology</title>
<author confidence="0.99587">Robert Bosch</author>
<address confidence="0.988594">Palo Alto, CA 94304, USA</address>
<email confidence="0.959456">Fuliang.weng@rtc.bosch.com</email>
<title confidence="0.80589">Hauke Research and Technology</title>
<author confidence="0.974668">Robert Bosch</author>
<address confidence="0.913733">Palo Alto, CA 94304,</address>
<email confidence="0.999543">hauke.schmidt@rtc.bosch.com</email>
<abstract confidence="0.998594217391304">This paper describes a fast algorithm that selects features for conditional maximum entropy modeling. Berger et al. (1996) presents an incremental feature selection (IFS) algorithm, which computes the approximate gains for all candidate features at each selection stage, and is very time-consuming for any problems with large feature spaces. In this new algorithm, instead, we only compute the approximate gains for the top-ranked features based on the models obtained from previous stages. Experiments on WSJ data in Penn Treebank are conducted to show that the new algorithm greatly speeds up the feature selection process while maintaining the same quality of selected features. One variant of this new algorithm with look-ahead functionality is also tested to further confirm the good quality of the selected features. The new algorithm is easy to implement, and given a feaspace of size F, it only uses more space than the original IFS algorithm.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
</authors>
<title>A Maximum Entropy Approach to Natural Language Processing.</title>
<date>1996</date>
<journal>Computational Linguistic,</journal>
<volume>22</volume>
<issue>1</issue>
<pages>39--71</pages>
<contexts>
<context position="645" citStr="Berger et al. (1996)" startWordPosition="82" endWordPosition="85">ure Selection in Conditional Maximum Entropy Modeling Yaqian Zhou Computer Science Department Fudan University Shanghai 200433, P.R. China archzhou@yahoo.com Lide Wu Computer Science Department Fudan University Shanghai 200433, P.R. China ldwu@fudan.edu.cn Fuliang Weng Research and Technology Center Robert Bosch Corp. Palo Alto, CA 94304, USA Fuliang.weng@rtc.bosch.com Hauke Schmidt Research and Technology Center Robert Bosch Corp. Palo Alto, CA 94304, USA hauke.schmidt@rtc.bosch.com Abstract This paper describes a fast algorithm that selects features for conditional maximum entropy modeling. Berger et al. (1996) presents an incremental feature selection (IFS) algorithm, which computes the approximate gains for all candidate features at each selection stage, and is very time-consuming for any problems with large feature spaces. In this new algorithm, instead, we only compute the approximate gains for the top-ranked features based on the models obtained from previous stages. Experiments on WSJ data in Penn Treebank are conducted to show that the new algorithm greatly speeds up the feature selection process while maintaining the same quality of selected features. One variant of this new algorithm with l</context>
<context position="3938" citStr="Berger et al. (1996)" startWordPosition="622" endWordPosition="625">predefined cutoff threshold get selected. Chen and Rosenfeld (1999) experimented on a feature selection technique that uses a [2 test to see whether a feature should be included in the ME model, where the [2 test is computed using the count from a prior distribution and the count from the real training data. It is a simple and probably effective technique for language modeling tasks. Since ME models are optimized using their likelihood or likelihood gains as the criterion, it is important to establish the relationship between [2 test score and � the likelihood gain, which, however, is absent. Berger et al. (1996) presented an incremental feature selection (IFS) algorithm where only one feature is added at each selection and the estimated parameter values are kept for the features selected in the previous stages. While this greedy search assumption is reasonable, the speed of the IFS algorithm is still an issue for complex tasks. For better understanding its performance, we reimplemented the algorithm. Given a task of 600,000 training instances, it takes nearly four days to select 1000 features from a feature space with a little more than 190,000 features. Berger and Printz (1998) proposed an o-orthogo</context>
<context position="7384" citStr="Berger et al 1996" startWordPosition="1224" endWordPosition="1227"> i such that there is y and f*(xi, y) = 1 do z[i] 0=sum[i, y] sum[i, y] 0= exp(Lt) z[i] += sum[i, y] 5. go to step 1. ) ) One difference here from the original IFS algorithm is that we adopt a technique in (Goodman, 2002) for optimizing the parameters in the conditional ME training. Specifically, we use array z to store the normalizing factors, and array sum for all the un-normalized conditional probabilities sum[i, y]. Thus, one only needs to modify those sum[i, y] that satisfy f*(xi, y)=1, and to make changes to their � corresponding normalizing factors z[i]. In contrast to what is shown in Berger et al 1996’s paper, here is how the different values in this variant of the IFS algorithm are computed. Let us denote sum(y |x) exp( l j f j ( x , y )) = å j Z(x) sum ( y |x ) = å y Then, the model can be represented by sum(y|x) and Z(x) as follows: p(y |x) = sum(y |x) / Z(x where sum(y|xi) and Z(xi) correspond to sum[i,y] and z[i] in Figure 1, respectively. Assume the selected feature set is S, and f is currently being considered. The goal of each selection stage is to select the feature f that maximizes the gain of the log likelihood, where the a and gain of f are derived through following steps: Let </context>
<context position="20515" citStr="Berger et al (1996)" startWordPosition="3664" endWordPosition="3667">e spaces used in the above experiments, our result is competitive to these best numbers. This shows that our new algorithm is both very effective in selecting high quality features and very efficient in performing the task. 5 Comparison and Conclusion Feature selection has been an important topic in both ME modeling and linear regression. In the past, most researchers resorted to count cutoff technique in selecting features for ME modeling (Rosenfeld, 1994; Ratnaparkhi, 1998; Reynar and Ratnaparkhi, 1997; Koeling, 2000). A more refined algorithm, the incremental feature selection algorithm by Berger et al (1996), allows one feature being added at each selection and at the same time keeps estimated parameter values for the features selected in the previous stages. As discussed in (Ratnaparkhi, 1998), the count cutoff technique works very fast and is easy to implement, but has the drawback of containing a large number of reAverage Time for each selection step (second) log10(Y) -1 -2 2 0 3 1 200 400 600 800 1000 2000 4000 6000 8000 10000 Number of Selected Features Berger SGC-0 Precision (%) 94 92 90 78 76 74 72 70 88 86 84 82 80 200 400 600 800 1000 2000 4000 6000 8000 10000 Number of Selected Features</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della Pietra. 1996. A Maximum Entropy Approach to Natural Language Processing. Computational Linguistic, 22 (1): 39-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Harry Printz</author>
</authors>
<title>A Comparison of Criteria for Maximum Entropy / Minimum Divergence Feature Selection.</title>
<date>1998</date>
<booktitle>Proceedings of the 3rd conference on Empirical Methods in Natural Language Processing. Granda,</booktitle>
<contexts>
<context position="4516" citStr="Berger and Printz (1998)" startWordPosition="717" endWordPosition="720">hich, however, is absent. Berger et al. (1996) presented an incremental feature selection (IFS) algorithm where only one feature is added at each selection and the estimated parameter values are kept for the features selected in the previous stages. While this greedy search assumption is reasonable, the speed of the IFS algorithm is still an issue for complex tasks. For better understanding its performance, we reimplemented the algorithm. Given a task of 600,000 training instances, it takes nearly four days to select 1000 features from a feature space with a little more than 190,000 features. Berger and Printz (1998) proposed an o-orthogonal condition for selecting k features at the same time without affecting much the quality of the selected features. While this technique is applicable for certain feature sets, such as word link features reported in their paper, the aorthogonal condition usually does not hold if part-of-speech tags are dominantly present in a feature subset. Past work, including Ratnaparkhi (1998) and Zhou et al (2003), has shown that the IFS algorithm utilizes much fewer features than the count cutoff method, while maintaining the similar precision and recall on tasks, such as prepositi</context>
<context position="21397" citStr="Berger and Printz (1998)" startWordPosition="3823" endWordPosition="3826">t, but has the drawback of containing a large number of reAverage Time for each selection step (second) log10(Y) -1 -2 2 0 3 1 200 400 600 800 1000 2000 4000 6000 8000 10000 Number of Selected Features Berger SGC-0 Precision (%) 94 92 90 78 76 74 72 70 88 86 84 82 80 200 400 600 800 1000 2000 4000 6000 8000 10000 Number of Selected Features All (191098) IFS SGC-0 SGC-500 Count Cutoff dundant features. In contrast, the IFS removes the redundancy in the selected feature set, but the speed of the algorithm has been a big issue for complex tasks. Having realized the drawback of the IFS algorithm, Berger and Printz (1998) proposed an f-orthogonal condition for selecting k features at the same time without affecting much the quality of the selected features. While this technique is applicable for certain feature sets, such as link features between words, the forthogonal condition usually does not hold if partof-speech tags are dominantly present in a feature subset. Chen and Rosenfeld (1999) experimented on a feature selection technique that uses a 02 test to see whether a feature should be included in the ME model, where the 02 test is computed using the counts from a prior distribution and the counts from the</context>
</contexts>
<marker>Berger, Printz, 1998</marker>
<rawString>Adam L. Berger and Harry Printz. 1998. A Comparison of Criteria for Maximum Entropy / Minimum Divergence Feature Selection. Proceedings of the 3rd conference on Empirical Methods in Natural Language Processing. Granda, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Chen</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>Efficient Sampling and Feature Selection in Whole Sentence maximum Entropy Language Models.</title>
<date>1999</date>
<booktitle>Proceedings of ICASSP-1999,</booktitle>
<location>Phoenix, Arizona.</location>
<contexts>
<context position="3385" citStr="Chen and Rosenfeld (1999)" startWordPosition="526" endWordPosition="529">ection during the training. Feature selection is a very difficult optimization task when the feature space under investigation is large. This is because we essentially try to find a best subset from a collection of all the possible feature subsets, which has a size of 2|W|, where |W| is the size of the feature space. In the past, most researchers resorted to a simple count cutoff technique for selecting features (Rosenfeld, 1994; Ratnaparkhi, 1998; Reynar and Ratnaparkhi, 1997; Koeling, 2000), where only the features that occur in a corpus more than a predefined cutoff threshold get selected. Chen and Rosenfeld (1999) experimented on a feature selection technique that uses a [2 test to see whether a feature should be included in the ME model, where the [2 test is computed using the count from a prior distribution and the count from the real training data. It is a simple and probably effective technique for language modeling tasks. Since ME models are optimized using their likelihood or likelihood gains as the criterion, it is important to establish the relationship between [2 test score and � the likelihood gain, which, however, is absent. Berger et al. (1996) presented an incremental feature selection (IF</context>
<context position="21773" citStr="Chen and Rosenfeld (1999)" startWordPosition="3883" endWordPosition="3886">ount Cutoff dundant features. In contrast, the IFS removes the redundancy in the selected feature set, but the speed of the algorithm has been a big issue for complex tasks. Having realized the drawback of the IFS algorithm, Berger and Printz (1998) proposed an f-orthogonal condition for selecting k features at the same time without affecting much the quality of the selected features. While this technique is applicable for certain feature sets, such as link features between words, the forthogonal condition usually does not hold if partof-speech tags are dominantly present in a feature subset. Chen and Rosenfeld (1999) experimented on a feature selection technique that uses a 02 test to see whether a feature should be included in the ME model, where the 02 test is computed using the counts from a prior distribution and the counts from the real training data. It is a simple and probably effective technique for language modeling tasks. Since ME models are optimized using their likelihood or likelihood gains as the criterion, it is important to establish the relationship between 02 test score and the likelihood gain, which, however, is absent. There is a large amount of literature on feature selection in linea</context>
</contexts>
<marker>Chen, Rosenfeld, 1999</marker>
<rawString>Stanley Chen and Ronald Rosenfeld. 1999. Efficient Sampling and Feature Selection in Whole Sentence maximum Entropy Language Models. Proceedings of ICASSP-1999, Phoenix, Arizona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Sequential Conditional Generalized Iterative Scaling. Association for Computational Linguistics,</title>
<date>2002</date>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="6988" citStr="Goodman, 2002" startWordPosition="1160" endWordPosition="1161"> size of the candidate feature set. Figure 1: A Variant of the IFS Algorithm. 0. Initialize: S = 0, sum[1..I, 1..Y] = 1, z[1..I] = Y 1. Gain computation: MaxGain = 0 for f in feature space F do aˆ = argmaxGSÈf(a a g = ˆ mo GSor(a if MaxGain &lt; gˆ then MaxGain = gˆ f* = f 0*=aˆ 2. Feature selection: S = S O { f* } 3. if termination condition is met, then stop 4. Model adjustment: for instance i such that there is y and f*(xi, y) = 1 do z[i] 0=sum[i, y] sum[i, y] 0= exp(Lt) z[i] += sum[i, y] 5. go to step 1. ) ) One difference here from the original IFS algorithm is that we adopt a technique in (Goodman, 2002) for optimizing the parameters in the conditional ME training. Specifically, we use array z to store the normalizing factors, and array sum for all the un-normalized conditional probabilities sum[i, y]. Thus, one only needs to modify those sum[i, y] that satisfy f*(xi, y)=1, and to make changes to their � corresponding normalizing factors z[i]. In contrast to what is shown in Berger et al 1996’s paper, here is how the different values in this variant of the IFS algorithm are computed. Let us denote sum(y |x) exp( l j f j ( x , y )) = å j Z(x) sum ( y |x ) = å y Then, the model can be represent</context>
</contexts>
<marker>Goodman, 2002</marker>
<rawString>Joshua Goodman. 2002. Sequential Conditional Generalized Iterative Scaling. Association for Computational Linguistics, Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rob Koeling</author>
</authors>
<title>Chunking with Maximum Entropy Models. In:</title>
<date>2000</date>
<booktitle>Proceedings of CoNLL-2000 and LLL2000,</booktitle>
<pages>139--141</pages>
<location>Lisbon, Portugal,</location>
<contexts>
<context position="1714" citStr="Koeling, 2000" startWordPosition="256" endWordPosition="257">greatly speeds up the feature selection process while maintaining the same quality of selected features. One variant of this new algorithm with look-ahead functionality is also tested to further confirm the good quality of the selected features. The new algorithm is easy to implement, and given a feature space of size F, it only uses O(F) more space than the original IFS algorithm. 1 Introduction Maximum Entropy (ME) modeling has received a lot of attention in language modeling and natural language processing for the past few years (e.g., Rosenfeld, 1994; Berger et al 1996; Ratnaparkhi, 1998; Koeling, 2000). One of the main advantages using ME modeling is the ability to incorporate various features in the same framework with a sound mathematical foundation. There are two main tasks in ME modeling: the feature selection process that chooses from a feature space a subset of good features to be included in the model; and the parameter estimation process that estimates the weighting factors for each selected feature in the exponential model. This paper is primarily concerned with the feature selection process in ME modeling. While the majority of the work in ME modeling has been focusing on paramete</context>
<context position="3257" citStr="Koeling, 2000" startWordPosition="506" endWordPosition="507">ability computation and requires smaller memory size during its application, but also shortens the cycle of model selection during the training. Feature selection is a very difficult optimization task when the feature space under investigation is large. This is because we essentially try to find a best subset from a collection of all the possible feature subsets, which has a size of 2|W|, where |W| is the size of the feature space. In the past, most researchers resorted to a simple count cutoff technique for selecting features (Rosenfeld, 1994; Ratnaparkhi, 1998; Reynar and Ratnaparkhi, 1997; Koeling, 2000), where only the features that occur in a corpus more than a predefined cutoff threshold get selected. Chen and Rosenfeld (1999) experimented on a feature selection technique that uses a [2 test to see whether a feature should be included in the ME model, where the [2 test is computed using the count from a prior distribution and the count from the real training data. It is a simple and probably effective technique for language modeling tasks. Since ME models are optimized using their likelihood or likelihood gains as the criterion, it is important to establish the relationship between [2 test</context>
<context position="19588" citStr="Koeling (2000)" startWordPosition="3519" endWordPosition="3520">a over-fitting problem. Third, the simple count cutoff algorithm significantly underperforms the other feature selection algorithms when feature subsets with no more than 10,000 features are considered. To further confirm the findings regarding precision, we conducted another experiment with Base NP recognition as the task. The experiment uses section 15-18 of WSJ as the training data, and section 20 as the test data. When we select 1,160 features from a simple feature space using our SGC algorithm, we obtain a precision/recall of 92.75%/93.25%. The best reported ME work on this task includes Koeling (2000) that has the precision/recall of 92.84%/93.18% with a cutoff of 5, and Zhou et al. (2003) has reached the performance of 93.04%/93.31% with cutoff of 7 and reached a performance of 92.46%/92.74% with 615 features using the IFS algorithm. While the results are not directly comparable due to different feature spaces used in the above experiments, our result is competitive to these best numbers. This shows that our new algorithm is both very effective in selecting high quality features and very efficient in performing the task. 5 Comparison and Conclusion Feature selection has been an important </context>
</contexts>
<marker>Koeling, 2000</marker>
<rawString>Rob Koeling. 2000. Chunking with Maximum Entropy Models. In: Proceedings of CoNLL-2000 and LLL2000, Lisbon, Portugal, 139-141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Maximum Entropy Models for Natural Language Ambiguity Resolution.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="1698" citStr="Ratnaparkhi, 1998" startWordPosition="254" endWordPosition="255"> the new algorithm greatly speeds up the feature selection process while maintaining the same quality of selected features. One variant of this new algorithm with look-ahead functionality is also tested to further confirm the good quality of the selected features. The new algorithm is easy to implement, and given a feature space of size F, it only uses O(F) more space than the original IFS algorithm. 1 Introduction Maximum Entropy (ME) modeling has received a lot of attention in language modeling and natural language processing for the past few years (e.g., Rosenfeld, 1994; Berger et al 1996; Ratnaparkhi, 1998; Koeling, 2000). One of the main advantages using ME modeling is the ability to incorporate various features in the same framework with a sound mathematical foundation. There are two main tasks in ME modeling: the feature selection process that chooses from a feature space a subset of good features to be included in the model; and the parameter estimation process that estimates the weighting factors for each selected feature in the exponential model. This paper is primarily concerned with the feature selection process in ME modeling. While the majority of the work in ME modeling has been focu</context>
<context position="3211" citStr="Ratnaparkhi, 1998" startWordPosition="500" endWordPosition="501">ture selection, which not only speeds up the probability computation and requires smaller memory size during its application, but also shortens the cycle of model selection during the training. Feature selection is a very difficult optimization task when the feature space under investigation is large. This is because we essentially try to find a best subset from a collection of all the possible feature subsets, which has a size of 2|W|, where |W| is the size of the feature space. In the past, most researchers resorted to a simple count cutoff technique for selecting features (Rosenfeld, 1994; Ratnaparkhi, 1998; Reynar and Ratnaparkhi, 1997; Koeling, 2000), where only the features that occur in a corpus more than a predefined cutoff threshold get selected. Chen and Rosenfeld (1999) experimented on a feature selection technique that uses a [2 test to see whether a feature should be included in the ME model, where the [2 test is computed using the count from a prior distribution and the count from the real training data. It is a simple and probably effective technique for language modeling tasks. Since ME models are optimized using their likelihood or likelihood gains as the criterion, it is important</context>
<context position="4922" citStr="Ratnaparkhi (1998)" startWordPosition="783" endWordPosition="784">e reimplemented the algorithm. Given a task of 600,000 training instances, it takes nearly four days to select 1000 features from a feature space with a little more than 190,000 features. Berger and Printz (1998) proposed an o-orthogonal condition for selecting k features at the same time without affecting much the quality of the selected features. While this technique is applicable for certain feature sets, such as word link features reported in their paper, the aorthogonal condition usually does not hold if part-of-speech tags are dominantly present in a feature subset. Past work, including Ratnaparkhi (1998) and Zhou et al (2003), has shown that the IFS algorithm utilizes much fewer features than the count cutoff method, while maintaining the similar precision and recall on tasks, such as prepositional phrase attachment, text categorization and base NP chunking. This leads us to further explore the possible improvement on the IFS algorithm. In section 2, we briefly review the IFS algorithm. Then, a fast feature selection algorithm is described in section 3. Section 4 presents a number of experiments, which show a massive speed-up and quality feature selection of the new algorithm. Finally, we con</context>
<context position="20375" citStr="Ratnaparkhi, 1998" startWordPosition="3645" endWordPosition="3646">formance of 92.46%/92.74% with 615 features using the IFS algorithm. While the results are not directly comparable due to different feature spaces used in the above experiments, our result is competitive to these best numbers. This shows that our new algorithm is both very effective in selecting high quality features and very efficient in performing the task. 5 Comparison and Conclusion Feature selection has been an important topic in both ME modeling and linear regression. In the past, most researchers resorted to count cutoff technique in selecting features for ME modeling (Rosenfeld, 1994; Ratnaparkhi, 1998; Reynar and Ratnaparkhi, 1997; Koeling, 2000). A more refined algorithm, the incremental feature selection algorithm by Berger et al (1996), allows one feature being added at each selection and at the same time keeps estimated parameter values for the features selected in the previous stages. As discussed in (Ratnaparkhi, 1998), the count cutoff technique works very fast and is easy to implement, but has the drawback of containing a large number of reAverage Time for each selection step (second) log10(Y) -1 -2 2 0 3 1 200 400 600 800 1000 2000 4000 6000 8000 10000 Number of Selected Features </context>
</contexts>
<marker>Ratnaparkhi, 1998</marker>
<rawString>Adwait Ratnaparkhi. 1998. Maximum Entropy Models for Natural Language Ambiguity Resolution. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Rosenfeld</author>
</authors>
<title>Adaptive Statistical Language Modeling: A Maximum Entropy Approach.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>Carnegie Mellon University,</institution>
<contexts>
<context position="1660" citStr="Rosenfeld, 1994" startWordPosition="248" endWordPosition="249"> Treebank are conducted to show that the new algorithm greatly speeds up the feature selection process while maintaining the same quality of selected features. One variant of this new algorithm with look-ahead functionality is also tested to further confirm the good quality of the selected features. The new algorithm is easy to implement, and given a feature space of size F, it only uses O(F) more space than the original IFS algorithm. 1 Introduction Maximum Entropy (ME) modeling has received a lot of attention in language modeling and natural language processing for the past few years (e.g., Rosenfeld, 1994; Berger et al 1996; Ratnaparkhi, 1998; Koeling, 2000). One of the main advantages using ME modeling is the ability to incorporate various features in the same framework with a sound mathematical foundation. There are two main tasks in ME modeling: the feature selection process that chooses from a feature space a subset of good features to be included in the model; and the parameter estimation process that estimates the weighting factors for each selected feature in the exponential model. This paper is primarily concerned with the feature selection process in ME modeling. While the majority of</context>
<context position="3192" citStr="Rosenfeld, 1994" startWordPosition="498" endWordPosition="499">us to perform feature selection, which not only speeds up the probability computation and requires smaller memory size during its application, but also shortens the cycle of model selection during the training. Feature selection is a very difficult optimization task when the feature space under investigation is large. This is because we essentially try to find a best subset from a collection of all the possible feature subsets, which has a size of 2|W|, where |W| is the size of the feature space. In the past, most researchers resorted to a simple count cutoff technique for selecting features (Rosenfeld, 1994; Ratnaparkhi, 1998; Reynar and Ratnaparkhi, 1997; Koeling, 2000), where only the features that occur in a corpus more than a predefined cutoff threshold get selected. Chen and Rosenfeld (1999) experimented on a feature selection technique that uses a [2 test to see whether a feature should be included in the ME model, where the [2 test is computed using the count from a prior distribution and the count from the real training data. It is a simple and probably effective technique for language modeling tasks. Since ME models are optimized using their likelihood or likelihood gains as the criteri</context>
<context position="20356" citStr="Rosenfeld, 1994" startWordPosition="3643" endWordPosition="3644">and reached a performance of 92.46%/92.74% with 615 features using the IFS algorithm. While the results are not directly comparable due to different feature spaces used in the above experiments, our result is competitive to these best numbers. This shows that our new algorithm is both very effective in selecting high quality features and very efficient in performing the task. 5 Comparison and Conclusion Feature selection has been an important topic in both ME modeling and linear regression. In the past, most researchers resorted to count cutoff technique in selecting features for ME modeling (Rosenfeld, 1994; Ratnaparkhi, 1998; Reynar and Ratnaparkhi, 1997; Koeling, 2000). A more refined algorithm, the incremental feature selection algorithm by Berger et al (1996), allows one feature being added at each selection and at the same time keeps estimated parameter values for the features selected in the previous stages. As discussed in (Ratnaparkhi, 1998), the count cutoff technique works very fast and is easy to implement, but has the drawback of containing a large number of reAverage Time for each selection step (second) log10(Y) -1 -2 2 0 3 1 200 400 600 800 1000 2000 4000 6000 8000 10000 Number of</context>
</contexts>
<marker>Rosenfeld, 1994</marker>
<rawString>Ronald Rosenfeld. 1994. Adaptive Statistical Language Modeling: A Maximum Entropy Approach. Ph.D. thesis, Carnegie Mellon University, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Reynar</author>
<author>A Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Approach to Identifying Sentence Boundaries. In:</title>
<date>1997</date>
<booktitle>Proceedings of the Fifth Conference on Applied Natural Language Processing,</booktitle>
<pages>16--19</pages>
<location>Washington D.C.,</location>
<contexts>
<context position="3241" citStr="Reynar and Ratnaparkhi, 1997" startWordPosition="502" endWordPosition="505">ch not only speeds up the probability computation and requires smaller memory size during its application, but also shortens the cycle of model selection during the training. Feature selection is a very difficult optimization task when the feature space under investigation is large. This is because we essentially try to find a best subset from a collection of all the possible feature subsets, which has a size of 2|W|, where |W| is the size of the feature space. In the past, most researchers resorted to a simple count cutoff technique for selecting features (Rosenfeld, 1994; Ratnaparkhi, 1998; Reynar and Ratnaparkhi, 1997; Koeling, 2000), where only the features that occur in a corpus more than a predefined cutoff threshold get selected. Chen and Rosenfeld (1999) experimented on a feature selection technique that uses a [2 test to see whether a feature should be included in the ME model, where the [2 test is computed using the count from a prior distribution and the count from the real training data. It is a simple and probably effective technique for language modeling tasks. Since ME models are optimized using their likelihood or likelihood gains as the criterion, it is important to establish the relationship</context>
<context position="20405" citStr="Reynar and Ratnaparkhi, 1997" startWordPosition="3647" endWordPosition="3650">92.74% with 615 features using the IFS algorithm. While the results are not directly comparable due to different feature spaces used in the above experiments, our result is competitive to these best numbers. This shows that our new algorithm is both very effective in selecting high quality features and very efficient in performing the task. 5 Comparison and Conclusion Feature selection has been an important topic in both ME modeling and linear regression. In the past, most researchers resorted to count cutoff technique in selecting features for ME modeling (Rosenfeld, 1994; Ratnaparkhi, 1998; Reynar and Ratnaparkhi, 1997; Koeling, 2000). A more refined algorithm, the incremental feature selection algorithm by Berger et al (1996), allows one feature being added at each selection and at the same time keeps estimated parameter values for the features selected in the previous stages. As discussed in (Ratnaparkhi, 1998), the count cutoff technique works very fast and is easy to implement, but has the drawback of containing a large number of reAverage Time for each selection step (second) log10(Y) -1 -2 2 0 3 1 200 400 600 800 1000 2000 4000 6000 8000 10000 Number of Selected Features Berger SGC-0 Precision (%) 94 </context>
</contexts>
<marker>Reynar, Ratnaparkhi, 1997</marker>
<rawString>J. Reynar and A. Ratnaparkhi. 1997. A Maximum Entropy Approach to Identifying Sentence Boundaries. In: Proceedings of the Fifth Conference on Applied Natural Language Processing, Washington D.C., 16-19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhou Ya-qian</author>
<author>Guo Yi-kun</author>
<author>Huang Xuan-jing</author>
<author>Wu Li-de</author>
</authors>
<title>Chinese and English BaseNP Recognized by Maximum Entropy.</title>
<date>2003</date>
<journal>Journal of Computer Research and Development.</journal>
<pages>40--3</pages>
<location>Beijin</location>
<marker>Ya-qian, Yi-kun, Xuan-jing, Li-de, 2003</marker>
<rawString>Zhou Ya-qian, Guo Yi-kun, Huang Xuan-jing, and Wu Li-de. 2003. Chinese and English BaseNP Recognized by Maximum Entropy. Journal of Computer Research and Development. 40(3):440-446, Beijin</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>