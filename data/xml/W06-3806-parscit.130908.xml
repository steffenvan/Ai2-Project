<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000080">
<title confidence="0.999267">
Similarity between Pairs of Co-indexed Trees
for Textual Entailment Recognition
</title>
<author confidence="0.990318">
Fabio Massimo Zanzotto
</author>
<affiliation confidence="0.986491">
DISCo
University Of Milan-Bicocca
</affiliation>
<address confidence="0.539472">
Milano, Italy
</address>
<email confidence="0.993789">
zanzotto@disco.unimib.it
</email>
<author confidence="0.968809">
Alessandro Moschitti
</author>
<affiliation confidence="0.879543666666667">
DISP
University Of Rome ”Tor Vergata”
Roma, Italy
</affiliation>
<email confidence="0.994938">
moschitti@info.uniroma2.it
</email>
<sectionHeader confidence="0.995606" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9991815">
In this paper we present a novel similarity
between pairs of co-indexed trees to auto-
matically learn textual entailment classi-
fiers. We defined a kernel function based
on this similarity along with a more clas-
sical intra-pair similarity. Experiments
show an improvement of 4.4 absolute per-
cent points over state-of-the-art methods.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999883272727273">
Recently, a remarkable interest has been devoted to
textual entailment recognition (Dagan et al., 2005).
The task requires to determine whether or not a text
T entails a hypothesis H. As it is a binary classifica-
tion task, it could seem simple to use machine learn-
ing algorithms to learn an entailment classifier from
training examples. Unfortunately, this is not. The
learner should capture the similarities between dif-
ferent pairs, (T&apos;, H&apos;) and (T&apos;&apos;, H&apos;&apos;), taking into ac-
count the relations between sentences within a pair.
For example, having these two learning pairs:
</bodyText>
<equation confidence="0.549893">
T1 ⇒ H1
</equation>
<footnote confidence="0.802135454545455">
T1 “At the end of the year, all solid compa-
nies pay dividends”
H1 “At the end of the year, all solid
insurance companies pay dividends.”
T1 #&gt; H2
T1 “At the end of the year, all solid compa-
nies pay dividends”
H2 “At the end of the year, all solid compa-
nies pay cash dividends.”
determining whether or not the following implica-
tion holds:
</footnote>
<figure confidence="0.4668">
T3 ⇒ H3?
T3 “All wild animals eat plants that have
scientifically proven medicinal proper-
ties.”
H3 “All wild mountain animals eat plants
that have scientifically proven medici-
nal properties.”
requires to detect that:
</figure>
<listItem confidence="0.687453375">
1. T3 is structurally (and somehow lexically) sim-
ilar to T1 and H3 is more similar to H1 than to
H2;
2. relations between the sentences in the pairs
(T3, H3) (e.g., T3 and H3 have the same noun
governing the subject of the main sentence) are
similar to the relations between sentences in the
pairs (T1, H1) and (T1, H2).
</listItem>
<bodyText confidence="0.99541395">
Given this analysis we may derive that T3 ⇒ H3.
The example suggests that graph matching tec-
niques are not sufficient as these may only detect
the structural similarity between sentences of textual
entailment pairs. An extension is needed to consider
also if two pairs show compatible relations between
their sentences.
In this paper, we propose to observe textual entail-
ment pairs as pairs of syntactic trees with co-indexed
nodes. This shuold help to cosider both the struc-
tural similarity between syntactic tree pairs and the
similarity between relations among sentences within
a pair. Then, we use this cross-pair similarity with
more traditional intra-pair similarities (e.g., (Corley
and Mihalcea, 2005)) to define a novel kernel func-
tion. We experimented with such kernel using Sup-
port Vector Machines on the Recognizing Textual
Entailment (RTE) challenge test-beds. The compar-
ative results show that (a) we have designed an ef-
fective way to automatically learn entailment rules
</bodyText>
<page confidence="0.988943">
33
</page>
<bodyText confidence="0.561698">
Workshop on TextGraphs, at HLT-NAACL 2006, pages 33–36,
</bodyText>
<subsectionHeader confidence="0.608914">
New York City, June 2006. c�2006 Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.999836">
from examples and (b) our approach is highly accu-
rate and exceeds the accuracy of the current state-of-
the-art models.
In the remainder of this paper, Sec. 2 introduces
the cross-pair similarity and Sec. 3 shows the exper-
imental results.
</bodyText>
<sectionHeader confidence="0.945106" genericHeader="method">
2 Learning Textual Entailment from
examples
</sectionHeader>
<bodyText confidence="0.999883615384615">
To carry out automatic learning from exam-
ples, we need to define a cross-pair similarity
K((T&apos;, H&apos;), (T&apos;&apos;, H&apos;&apos;)). This function should con-
sider pairs similar when: (1) texts and hypotheses
are structurally and lexically similar (structural sim-
ilarity); (2) the relations between the sentences in
the pair (T&apos;, H&apos;) are compatible with the relations
in (T&apos;&apos;, H&apos;&apos;) (intra-pair word movement compatibil-
ity). We argue that such requirements could be met
by augmenting syntactic trees with placeholders that
co-index related words within pairs. We will then
define a cross-pair similarity over these pairs of co-
indexed trees.
</bodyText>
<subsectionHeader confidence="0.8954585">
2.1 Training examples as pairs of co-indexed
trees
</subsectionHeader>
<bodyText confidence="0.999985222222222">
Sentence pairs selected as possible sentences in en-
tailment are naturally co-indexed. Many words (or
expressions) wh in H have a referent wt in T. These
pairs (wt, wh) are called anchors. Possibly, it is
more important that the two words in an anchor are
related than the actual two words. The entailment
could hold even if the two words are substitued with
two other related words. To indicate this we co-
index words associating placeholders with anchors.
For example, in Fig. 1, 2” indicates the (compa-
nies,companies) anchor between T1 and H1. These
placeholders are then used to augment tree nodes. To
better take into account argument movements, place-
holders are propagated in the syntactic trees follow-
ing constituent heads (see Fig. 1).
In line with many other researches (e.g., (Cor-
ley and Mihalcea, 2005)), we determine these an-
chors using different similarity or relatedness dec-
tors: the exact matching between tokens or lemmas,
a similarity between tokens based on their edit dis-
tance, the derivationally related form relation and
the verb entailment relation in WordNet, and, fi-
nally, a WordNet-based similarity (Jiang and Con-
rath, 1997). Each of these detectors gives a different
weight to the anchor: the actual computed similarity
for the last and 1 for all the others. These weights
will be used in the final kernel.
</bodyText>
<subsectionHeader confidence="0.959322">
2.2 Similarity between pairs of co-indexed
trees
</subsectionHeader>
<bodyText confidence="0.999087268292683">
Pairs of syntactic trees where nodes are co-indexed
with placeholders allow the design a cross-pair simi-
larity that considers both the structural similarity and
the intra-pair word movement compatibility.
Syntactic trees of texts and hypotheses permit to
verify the structural similarity between pairs of sen-
tences. Texts should have similar structures as well
as hypotheses. In Fig. 1, the overlapping subtrees
are in bold. For example, T1 and T3 share the sub-
tree starting with S → NP VP. Although the lexicals
in T3 and H3 are quite different from those T1 and
H1, their bold subtrees are more similar to those of
T1 and H1 than to T1 and H2, respectively. H1 and
H3 share the production NP → DT JJ NN NNS while
H2 and H3 do not. To decide on the entailment for
(T3,H3), we can use the value of (T1, H1).
Anchors and placeholders are useful to verify if
two pairs can be aligned as showing compatible
intra-pair word movement. For example, (T1, H1)
and (T3, H3) show compatible constituent move-
ments given that the dashed lines connecting place-
holders of the two pairs indicates structurally equiv-
alent nodes both in the texts and the hypotheses. The
dashed line between 3 and b links the main verbs
both in the texts T1 and T3 and in the hypotheses H1
and H3. After substituting 3
and T3 share the subtree S → NP 2 VP 3. The same
subtree is shared between H1 and H3. This implies
that words in the pair (T1, H1) are correlated like
words in (T3, H3). Any different mapping between
the two anchor sets would not have this property.
Using the structural similarity, the placeholders,
and the connection between placeholders, the over-
all similarity is then defined as follows. Let A&apos; and
A&apos;&apos; be the placeholders of (T&apos;, H&apos;) and (T&apos;&apos;, H&apos;&apos;),
respectively. The similarity between two co-indexed
syntactic tree pairs Ks((T&apos;, H&apos;), (T&apos;&apos;, H&apos;&apos;)) is de-
fined using a classical similarity between two trees
KT(t1, t2) when the best alignment between the A&apos;
and A&apos;&apos; is given. Let C be the set of all bijective
to b and 2 to a, T1
</bodyText>
<page confidence="0.96017">
34
</page>
<figure confidence="0.999829712574851">
T1
T3
S
S
are collapsed to 2.
holders 2’
and 2”
PP
,
NP 2
VP 3
NP a
VP b
NP c
plants
... properties
c
IN
At
NP 0
NP 0
PP
, DT
all
JJ 2
solid
2’
companies
2”
NNS 2
VBP 3
pay
3
NP 4
NNS 4
DT
All
JJ a
wild
a’
NNS a
animals
a”
VBP b
eat
b
DT
the
NN 0
IN
NP 1
dividends
4
end
0
of
DT
NN 1
the
year
1
H1
H3
S
S
PP
NP 2
VP 3
NP a
VP b
,
IN
At
NP 0
, DT
JJ 2
NN
NNS 2
VBP 3
NP 4
NNS 4
DT
JJ a
NN
NNS a
VBP b
NP c
NP 0
PP
all
solid
2’
insurance
companies
2”
pay
3
All
wild
a’
mountain
animals
a”
eat
b
plants
c
... properties
DT
the
NN 0
NP 1
dividends
4
IN
end
0
of
DT
NN 1
the
year
1
H3
H2
S
S
VP b
NP c
plants
... properties
c
PP
At ... year
DT
all
JJ 2
solid
2’
NP 2
companies
2”
NNS 2
VP 3
VBP 3
pay
3
NN
NP 4
NNS 4
DT
All
JJ a
wild
a’
NP a
NN
mountain
NNS a
animals
a”
VBP b
eat
b
cash
dividends
4
</figure>
<figureCaption confidence="0.973016">
Figure 1: Relations between (T1, H1), (T1, H2), and (T3, H3).
mappings from a0 C_ A0 : |a0 |= |A00 |to A00, an
element c E C is a substitution function. The co-
indexed tree pair similarity is then defined as:
</figureCaption>
<bodyText confidence="0.883499285714286">
Ks((T&apos;, H&apos;), (T&apos;&apos; H&apos;&apos;)) =
maxcEC(KT (t(H&apos;, c), t(H&apos;&apos;, i)) + KT (t(T&apos;, c), t(T&apos;&apos;, i))
where (1) t(5, c) returns the syntactic tree of the
hypothesis (text) 5 with placeholders replaced by
means of the substitution c, (2) i is the identity sub-
stitution and (3) KT(t1, t2) is a function that mea-
sures the similarity between the two trees t1 and t2.
</bodyText>
<subsectionHeader confidence="0.993476">
2.3 Enhancing cross-pair syntactic similarity
</subsectionHeader>
<bodyText confidence="0.9981914">
As the computation cost of the similarity measure
depends on the number of the possible sets of corre-
spondences C and this depends on the size of the
anchor sets, we reduce the number of placehold-
ers used to represent the anchors. Placeholders will
</bodyText>
<page confidence="0.997144">
35
</page>
<bodyText confidence="0.9350445">
have the same name if these are in the same chunk
both in the text and the hypothesis, e.g., the place-
</bodyText>
<sectionHeader confidence="0.74239" genericHeader="method">
3 Experimental investigation
</sectionHeader>
<bodyText confidence="0.813354142857143">
The aim of the experiments is twofold: we show that
(a) entailments can be learned from examples and
(b) our kernel function over syntactic structures is
effective to derive syntactic properties. The above
goals can be achieved by comparing our cross-pair
similarity kernel against (and in combination with)
other methods.
</bodyText>
<subsectionHeader confidence="0.852638">
3.1 Experimented kernels
</subsectionHeader>
<bodyText confidence="0.9666625">
We compared three different kernels: (1) the ker-
nel Kl((T0, H0), (T00, H00)) based on the intra-pair
</bodyText>
<table confidence="0.997434625">
Datasets Kl Kl + Kt Kl + Kg
Train:D1 Test:T1 0.5888 0.6213 0.6300
Train:T1 Test:D1 0.5644 0.5732 0.5838
Train:D2(50%)0 Test:D2(50%)00 0.6083 0.6156 0.6350
Train:D2(50%)00 Test:D2(50%)0 0.6272 0.5861 0.6607
Train:D2 Test:T2 0.6038 0.6238 0.6388
Mean 0.5985 0.6040 0.6297
(± 0.0235 ) (± 0.0229 ) (± 0.0282 )
</table>
<tableCaption confidence="0.999535">
Table 1: Experimental results
</tableCaption>
<bodyText confidence="0.9704178">
lexical similarity siml(T, H) as defined in (Cor-
ley and Mihalcea, 2005). This kernel is de-
fined as Kl((T0, H0), (T00, H00)) = siml(T0, H0) ×
siml(T00, H00). (2) the kernel Kl+K3 that combines
our kernel with the lexical-similarity-based kernel;
(3) the kernel Kl + Kt that combines the lexical-
similarity-based kernel with a basic tree kernel.
This latter is defined as Kt((T0, H0), (T00, H00)) =
KT (T0, T00) + KT (H0, H00). We implemented these
kernels within SVM-light (Joachims, 1999).
</bodyText>
<subsectionHeader confidence="0.991755">
3.2 Experimental settings
</subsectionHeader>
<bodyText confidence="0.999836777777778">
For the experiments, we used the Recognizing Tex-
tual Entailment (RTE) Challenge data sets, which
we name as D1, T1 and D2, T2, are the develop-
ment and the test sets of the first and second RTE
challenges, respectively. D1 contains 567 examples
whereas T1, D2 and T2 have all the same size, i.e.
800 instances. The positive examples are the 50%
of the data. We produced also a random split of D2.
The two folds are D2(50%)0 and D2(50%)00.
We also used the following resources: the Char-
niak parser (Charniak, 2000) to carry out the syntac-
tic analysis; the wn::similarity package (Ped-
ersen et al., 2004) to compute the Jiang&amp;Conrath
(J&amp;C) distance (Jiang and Conrath, 1997) needed to
implement the lexical similarity siml(T, H) as de-
fined in (Corley and Mihalcea, 2005); SVM-light-
TK (Moschitti, 2004) to encode the basic tree kernel
function, KT, in SVM-light (Joachims, 1999).
</bodyText>
<subsectionHeader confidence="0.915494">
3.3 Results and analysis
</subsectionHeader>
<bodyText confidence="0.999977933333333">
Table 1 reports the accuracy of different similar-
ity kernels on the different training and test split de-
scribed in the previous section. The table shows
some important result.
First, as observed in (Corley and Mihalcea, 2005)
the lexical-based distance kernel Kl shows an accu-
racy significantly higher than the random baseline,
i.e. 50%. This accuracy (second line) is comparable
with the best systems in the first RTE challenge (Da-
gan et al., 2005). The accuracy reported for the best
systems, i.e. 58.6% (Glickman et al., 2005; Bayer
et al., 2005), is not significantly far from the result
obtained with Kl, i.e. 58.88%.
Second, our approach (last column) is signifi-
cantly better than all the other methods as it pro-
vides the best result for each combination of train-
ing and test sets. On the “Train:D1-Test:T 1” test-
bed, it exceeds the accuracy of the current state-of-
the-art models (Glickman et al., 2005; Bayer et al.,
2005) by about 4.4 absolute percent points (63% vs.
58.6%) and 4% over our best lexical similarity mea-
sure. By comparing the average on all datasets, our
system improves on all the methods by at least 3 ab-
solute percent points.
Finally, the accuracy produced by our kernel
based on co-indexed trees Kl + K3 is higher than
the one obtained with the plain syntactic tree ker-
nel Kl + Kt. Thus, the use of placeholders and co-
indexing is fundamental to automatically learn en-
tailments from examples.
</bodyText>
<sectionHeader confidence="0.999111" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999926419354839">
Samuel Bayer, John Burger, Lisa Ferro, John Henderson, and
Alexander Yeh. 2005. MITRE’s submissions to the eu pas-
cal rte challenge. In Proceedings of the 1st Pascal Challenge
Workshop, Southampton, UK.
Eugene Charniak. 2000. A maximum-entropy-inspired parser.
In Proc. of the 1st NAACL, pages 132–139, Seattle, Wash-
ington.
Courtney Corley and Rada Mihalcea. 2005. Measuring the se-
mantic similarity of texts. In Proc. of the ACL Workshop
on Empirical Modeling of Semantic Equivalence and Entail-
ment, pages 13–18, Ann Arbor, Michigan, June. Association
for Computational Linguistics.
Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The
PASCAL RTE challenge. In PASCAL Challenges Workshop,
Southampton, U.K.
Oren Glickman, Ido Dagan, and Moshe Koppel. 2005. Web
based probabilistic textual entailment. In Proceedings of the
1st Pascal Challenge Workshop, Southampton, UK.
Jay J. Jiang and David W. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In Proc. of
the 10th ROCLING, pages 132–139, Tapei, Taiwan.
Thorsten Joachims. 1999. Making large-scale svm learning
practical. In B. Schlkopf, C. Burges, and A. Smola, editors,
Advances in Kernel Methods-Support Vector Learning. MIT
Press.
Alessandro Moschitti. 2004. A study on convolution kernels
for shallow semantic parsing. In proceedings of the ACL,
Barcelona, Spain.
Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi.
2004. Wordnet::similarity - measuring the relatedness of
concepts. In Proc. of 5th NAACL, Boston, MA.
</reference>
<page confidence="0.998943">
36
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.568187">
<title confidence="0.998873">Similarity between Pairs of Co-indexed for Textual Entailment Recognition</title>
<author confidence="0.99997">Fabio Massimo</author>
<affiliation confidence="0.999982">University Of</affiliation>
<address confidence="0.769734">Milano,</address>
<email confidence="0.99787">zanzotto@disco.unimib.it</email>
<author confidence="0.945948">Alessandro</author>
<affiliation confidence="0.998065">University Of Rome ”Tor</affiliation>
<address confidence="0.792329">Roma,</address>
<email confidence="0.997378">moschitti@info.uniroma2.it</email>
<abstract confidence="0.998985">In this paper we present a novel similarity between pairs of co-indexed trees to automatically learn textual entailment classifiers. We defined a kernel function based on this similarity along with a more classical intra-pair similarity. Experiments show an improvement of 4.4 absolute percent points over state-of-the-art methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Samuel Bayer</author>
<author>John Burger</author>
<author>Lisa Ferro</author>
<author>John Henderson</author>
<author>Alexander Yeh</author>
</authors>
<title>MITRE’s submissions to the eu pascal rte challenge.</title>
<date>2005</date>
<booktitle>In Proceedings of the 1st Pascal Challenge Workshop,</booktitle>
<location>Southampton, UK.</location>
<contexts>
<context position="12087" citStr="Bayer et al., 2005" startWordPosition="2107" endWordPosition="2110">function, KT, in SVM-light (Joachims, 1999). 3.3 Results and analysis Table 1 reports the accuracy of different similarity kernels on the different training and test split described in the previous section. The table shows some important result. First, as observed in (Corley and Mihalcea, 2005) the lexical-based distance kernel Kl shows an accuracy significantly higher than the random baseline, i.e. 50%. This accuracy (second line) is comparable with the best systems in the first RTE challenge (Dagan et al., 2005). The accuracy reported for the best systems, i.e. 58.6% (Glickman et al., 2005; Bayer et al., 2005), is not significantly far from the result obtained with Kl, i.e. 58.88%. Second, our approach (last column) is significantly better than all the other methods as it provides the best result for each combination of training and test sets. On the “Train:D1-Test:T 1” testbed, it exceeds the accuracy of the current state-ofthe-art models (Glickman et al., 2005; Bayer et al., 2005) by about 4.4 absolute percent points (63% vs. 58.6%) and 4% over our best lexical similarity measure. By comparing the average on all datasets, our system improves on all the methods by at least 3 absolute percent point</context>
</contexts>
<marker>Bayer, Burger, Ferro, Henderson, Yeh, 2005</marker>
<rawString>Samuel Bayer, John Burger, Lisa Ferro, John Henderson, and Alexander Yeh. 2005. MITRE’s submissions to the eu pascal rte challenge. In Proceedings of the 1st Pascal Challenge Workshop, Southampton, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proc. of the 1st NAACL,</booktitle>
<pages>132--139</pages>
<location>Seattle, Washington.</location>
<contexts>
<context position="11150" citStr="Charniak, 2000" startWordPosition="1957" endWordPosition="1958">T (H0, H00). We implemented these kernels within SVM-light (Joachims, 1999). 3.2 Experimental settings For the experiments, we used the Recognizing Textual Entailment (RTE) Challenge data sets, which we name as D1, T1 and D2, T2, are the development and the test sets of the first and second RTE challenges, respectively. D1 contains 567 examples whereas T1, D2 and T2 have all the same size, i.e. 800 instances. The positive examples are the 50% of the data. We produced also a random split of D2. The two folds are D2(50%)0 and D2(50%)00. We also used the following resources: the Charniak parser (Charniak, 2000) to carry out the syntactic analysis; the wn::similarity package (Pedersen et al., 2004) to compute the Jiang&amp;Conrath (J&amp;C) distance (Jiang and Conrath, 1997) needed to implement the lexical similarity siml(T, H) as defined in (Corley and Mihalcea, 2005); SVM-lightTK (Moschitti, 2004) to encode the basic tree kernel function, KT, in SVM-light (Joachims, 1999). 3.3 Results and analysis Table 1 reports the accuracy of different similarity kernels on the different training and test split described in the previous section. The table shows some important result. First, as observed in (Corley and Mi</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proc. of the 1st NAACL, pages 132–139, Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Courtney Corley</author>
<author>Rada Mihalcea</author>
</authors>
<title>Measuring the semantic similarity of texts.</title>
<date>2005</date>
<booktitle>In Proc. of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment,</booktitle>
<pages>13--18</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="2795" citStr="Corley and Mihalcea, 2005" startWordPosition="447" endWordPosition="450">ching tecniques are not sufficient as these may only detect the structural similarity between sentences of textual entailment pairs. An extension is needed to consider also if two pairs show compatible relations between their sentences. In this paper, we propose to observe textual entailment pairs as pairs of syntactic trees with co-indexed nodes. This shuold help to cosider both the structural similarity between syntactic tree pairs and the similarity between relations among sentences within a pair. Then, we use this cross-pair similarity with more traditional intra-pair similarities (e.g., (Corley and Mihalcea, 2005)) to define a novel kernel function. We experimented with such kernel using Support Vector Machines on the Recognizing Textual Entailment (RTE) challenge test-beds. The comparative results show that (a) we have designed an effective way to automatically learn entailment rules 33 Workshop on TextGraphs, at HLT-NAACL 2006, pages 33–36, New York City, June 2006. c�2006 Association for Computational Linguistics from examples and (b) our approach is highly accurate and exceeds the accuracy of the current state-ofthe-art models. In the remainder of this paper, Sec. 2 introduces the cross-pair simila</context>
<context position="4969" citStr="Corley and Mihalcea, 2005" startWordPosition="796" endWordPosition="800">s. Possibly, it is more important that the two words in an anchor are related than the actual two words. The entailment could hold even if the two words are substitued with two other related words. To indicate this we coindex words associating placeholders with anchors. For example, in Fig. 1, 2” indicates the (companies,companies) anchor between T1 and H1. These placeholders are then used to augment tree nodes. To better take into account argument movements, placeholders are propagated in the syntactic trees following constituent heads (see Fig. 1). In line with many other researches (e.g., (Corley and Mihalcea, 2005)), we determine these anchors using different similarity or relatedness dectors: the exact matching between tokens or lemmas, a similarity between tokens based on their edit distance, the derivationally related form relation and the verb entailment relation in WordNet, and, finally, a WordNet-based similarity (Jiang and Conrath, 1997). Each of these detectors gives a different weight to the anchor: the actual computed similarity for the last and 1 for all the others. These weights will be used in the final kernel. 2.2 Similarity between pairs of co-indexed trees Pairs of syntactic trees where </context>
<context position="10194" citStr="Corley and Mihalcea, 2005" startWordPosition="1789" endWordPosition="1793"> comparing our cross-pair similarity kernel against (and in combination with) other methods. 3.1 Experimented kernels We compared three different kernels: (1) the kernel Kl((T0, H0), (T00, H00)) based on the intra-pair Datasets Kl Kl + Kt Kl + Kg Train:D1 Test:T1 0.5888 0.6213 0.6300 Train:T1 Test:D1 0.5644 0.5732 0.5838 Train:D2(50%)0 Test:D2(50%)00 0.6083 0.6156 0.6350 Train:D2(50%)00 Test:D2(50%)0 0.6272 0.5861 0.6607 Train:D2 Test:T2 0.6038 0.6238 0.6388 Mean 0.5985 0.6040 0.6297 (± 0.0235 ) (± 0.0229 ) (± 0.0282 ) Table 1: Experimental results lexical similarity siml(T, H) as defined in (Corley and Mihalcea, 2005). This kernel is defined as Kl((T0, H0), (T00, H00)) = siml(T0, H0) × siml(T00, H00). (2) the kernel Kl+K3 that combines our kernel with the lexical-similarity-based kernel; (3) the kernel Kl + Kt that combines the lexicalsimilarity-based kernel with a basic tree kernel. This latter is defined as Kt((T0, H0), (T00, H00)) = KT (T0, T00) + KT (H0, H00). We implemented these kernels within SVM-light (Joachims, 1999). 3.2 Experimental settings For the experiments, we used the Recognizing Textual Entailment (RTE) Challenge data sets, which we name as D1, T1 and D2, T2, are the development and the t</context>
<context position="11763" citStr="Corley and Mihalcea, 2005" startWordPosition="2053" endWordPosition="2056">arniak, 2000) to carry out the syntactic analysis; the wn::similarity package (Pedersen et al., 2004) to compute the Jiang&amp;Conrath (J&amp;C) distance (Jiang and Conrath, 1997) needed to implement the lexical similarity siml(T, H) as defined in (Corley and Mihalcea, 2005); SVM-lightTK (Moschitti, 2004) to encode the basic tree kernel function, KT, in SVM-light (Joachims, 1999). 3.3 Results and analysis Table 1 reports the accuracy of different similarity kernels on the different training and test split described in the previous section. The table shows some important result. First, as observed in (Corley and Mihalcea, 2005) the lexical-based distance kernel Kl shows an accuracy significantly higher than the random baseline, i.e. 50%. This accuracy (second line) is comparable with the best systems in the first RTE challenge (Dagan et al., 2005). The accuracy reported for the best systems, i.e. 58.6% (Glickman et al., 2005; Bayer et al., 2005), is not significantly far from the result obtained with Kl, i.e. 58.88%. Second, our approach (last column) is significantly better than all the other methods as it provides the best result for each combination of training and test sets. On the “Train:D1-Test:T 1” testbed, i</context>
</contexts>
<marker>Corley, Mihalcea, 2005</marker>
<rawString>Courtney Corley and Rada Mihalcea. 2005. Measuring the semantic similarity of texts. In Proc. of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 13–18, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<date>2005</date>
<booktitle>The PASCAL RTE challenge. In PASCAL Challenges Workshop,</booktitle>
<location>Southampton, U.K.</location>
<contexts>
<context position="733" citStr="Dagan et al., 2005" startWordPosition="96" endWordPosition="99">rsity Of Milan-Bicocca Milano, Italy zanzotto@disco.unimib.it Alessandro Moschitti DISP University Of Rome ”Tor Vergata” Roma, Italy moschitti@info.uniroma2.it Abstract In this paper we present a novel similarity between pairs of co-indexed trees to automatically learn textual entailment classifiers. We defined a kernel function based on this similarity along with a more classical intra-pair similarity. Experiments show an improvement of 4.4 absolute percent points over state-of-the-art methods. 1 Introduction Recently, a remarkable interest has been devoted to textual entailment recognition (Dagan et al., 2005). The task requires to determine whether or not a text T entails a hypothesis H. As it is a binary classification task, it could seem simple to use machine learning algorithms to learn an entailment classifier from training examples. Unfortunately, this is not. The learner should capture the similarities between different pairs, (T&apos;, H&apos;) and (T&apos;&apos;, H&apos;&apos;), taking into account the relations between sentences within a pair. For example, having these two learning pairs: T1 ⇒ H1 T1 “At the end of the year, all solid companies pay dividends” H1 “At the end of the year, all solid insurance companies pa</context>
<context position="11987" citStr="Dagan et al., 2005" startWordPosition="2089" endWordPosition="2093">fined in (Corley and Mihalcea, 2005); SVM-lightTK (Moschitti, 2004) to encode the basic tree kernel function, KT, in SVM-light (Joachims, 1999). 3.3 Results and analysis Table 1 reports the accuracy of different similarity kernels on the different training and test split described in the previous section. The table shows some important result. First, as observed in (Corley and Mihalcea, 2005) the lexical-based distance kernel Kl shows an accuracy significantly higher than the random baseline, i.e. 50%. This accuracy (second line) is comparable with the best systems in the first RTE challenge (Dagan et al., 2005). The accuracy reported for the best systems, i.e. 58.6% (Glickman et al., 2005; Bayer et al., 2005), is not significantly far from the result obtained with Kl, i.e. 58.88%. Second, our approach (last column) is significantly better than all the other methods as it provides the best result for each combination of training and test sets. On the “Train:D1-Test:T 1” testbed, it exceeds the accuracy of the current state-ofthe-art models (Glickman et al., 2005; Bayer et al., 2005) by about 4.4 absolute percent points (63% vs. 58.6%) and 4% over our best lexical similarity measure. By comparing the </context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2005</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The PASCAL RTE challenge. In PASCAL Challenges Workshop, Southampton, U.K.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Glickman</author>
<author>Ido Dagan</author>
<author>Moshe Koppel</author>
</authors>
<title>Web based probabilistic textual entailment.</title>
<date>2005</date>
<booktitle>In Proceedings of the 1st Pascal Challenge Workshop,</booktitle>
<location>Southampton, UK.</location>
<contexts>
<context position="12066" citStr="Glickman et al., 2005" startWordPosition="2103" endWordPosition="2106"> the basic tree kernel function, KT, in SVM-light (Joachims, 1999). 3.3 Results and analysis Table 1 reports the accuracy of different similarity kernels on the different training and test split described in the previous section. The table shows some important result. First, as observed in (Corley and Mihalcea, 2005) the lexical-based distance kernel Kl shows an accuracy significantly higher than the random baseline, i.e. 50%. This accuracy (second line) is comparable with the best systems in the first RTE challenge (Dagan et al., 2005). The accuracy reported for the best systems, i.e. 58.6% (Glickman et al., 2005; Bayer et al., 2005), is not significantly far from the result obtained with Kl, i.e. 58.88%. Second, our approach (last column) is significantly better than all the other methods as it provides the best result for each combination of training and test sets. On the “Train:D1-Test:T 1” testbed, it exceeds the accuracy of the current state-ofthe-art models (Glickman et al., 2005; Bayer et al., 2005) by about 4.4 absolute percent points (63% vs. 58.6%) and 4% over our best lexical similarity measure. By comparing the average on all datasets, our system improves on all the methods by at least 3 a</context>
</contexts>
<marker>Glickman, Dagan, Koppel, 2005</marker>
<rawString>Oren Glickman, Ido Dagan, and Moshe Koppel. 2005. Web based probabilistic textual entailment. In Proceedings of the 1st Pascal Challenge Workshop, Southampton, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay J Jiang</author>
<author>David W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proc. of the 10th ROCLING,</booktitle>
<pages>132--139</pages>
<location>Tapei, Taiwan.</location>
<contexts>
<context position="5305" citStr="Jiang and Conrath, 1997" startWordPosition="848" endWordPosition="852">or between T1 and H1. These placeholders are then used to augment tree nodes. To better take into account argument movements, placeholders are propagated in the syntactic trees following constituent heads (see Fig. 1). In line with many other researches (e.g., (Corley and Mihalcea, 2005)), we determine these anchors using different similarity or relatedness dectors: the exact matching between tokens or lemmas, a similarity between tokens based on their edit distance, the derivationally related form relation and the verb entailment relation in WordNet, and, finally, a WordNet-based similarity (Jiang and Conrath, 1997). Each of these detectors gives a different weight to the anchor: the actual computed similarity for the last and 1 for all the others. These weights will be used in the final kernel. 2.2 Similarity between pairs of co-indexed trees Pairs of syntactic trees where nodes are co-indexed with placeholders allow the design a cross-pair similarity that considers both the structural similarity and the intra-pair word movement compatibility. Syntactic trees of texts and hypotheses permit to verify the structural similarity between pairs of sentences. Texts should have similar structures as well as hyp</context>
<context position="11308" citStr="Jiang and Conrath, 1997" startWordPosition="1980" endWordPosition="1983">extual Entailment (RTE) Challenge data sets, which we name as D1, T1 and D2, T2, are the development and the test sets of the first and second RTE challenges, respectively. D1 contains 567 examples whereas T1, D2 and T2 have all the same size, i.e. 800 instances. The positive examples are the 50% of the data. We produced also a random split of D2. The two folds are D2(50%)0 and D2(50%)00. We also used the following resources: the Charniak parser (Charniak, 2000) to carry out the syntactic analysis; the wn::similarity package (Pedersen et al., 2004) to compute the Jiang&amp;Conrath (J&amp;C) distance (Jiang and Conrath, 1997) needed to implement the lexical similarity siml(T, H) as defined in (Corley and Mihalcea, 2005); SVM-lightTK (Moschitti, 2004) to encode the basic tree kernel function, KT, in SVM-light (Joachims, 1999). 3.3 Results and analysis Table 1 reports the accuracy of different similarity kernels on the different training and test split described in the previous section. The table shows some important result. First, as observed in (Corley and Mihalcea, 2005) the lexical-based distance kernel Kl shows an accuracy significantly higher than the random baseline, i.e. 50%. This accuracy (second line) is c</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jay J. Jiang and David W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proc. of the 10th ROCLING, pages 132–139, Tapei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale svm learning practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods-Support Vector Learning.</booktitle>
<editor>In B. Schlkopf, C. Burges, and A. Smola, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="10610" citStr="Joachims, 1999" startWordPosition="1862" endWordPosition="1863">ain:D2 Test:T2 0.6038 0.6238 0.6388 Mean 0.5985 0.6040 0.6297 (± 0.0235 ) (± 0.0229 ) (± 0.0282 ) Table 1: Experimental results lexical similarity siml(T, H) as defined in (Corley and Mihalcea, 2005). This kernel is defined as Kl((T0, H0), (T00, H00)) = siml(T0, H0) × siml(T00, H00). (2) the kernel Kl+K3 that combines our kernel with the lexical-similarity-based kernel; (3) the kernel Kl + Kt that combines the lexicalsimilarity-based kernel with a basic tree kernel. This latter is defined as Kt((T0, H0), (T00, H00)) = KT (T0, T00) + KT (H0, H00). We implemented these kernels within SVM-light (Joachims, 1999). 3.2 Experimental settings For the experiments, we used the Recognizing Textual Entailment (RTE) Challenge data sets, which we name as D1, T1 and D2, T2, are the development and the test sets of the first and second RTE challenges, respectively. D1 contains 567 examples whereas T1, D2 and T2 have all the same size, i.e. 800 instances. The positive examples are the 50% of the data. We produced also a random split of D2. The two folds are D2(50%)0 and D2(50%)00. We also used the following resources: the Charniak parser (Charniak, 2000) to carry out the syntactic analysis; the wn::similarity pac</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale svm learning practical. In B. Schlkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods-Support Vector Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>A study on convolution kernels for shallow semantic parsing.</title>
<date>2004</date>
<booktitle>In proceedings of the ACL,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="11435" citStr="Moschitti, 2004" startWordPosition="2002" endWordPosition="2003"> second RTE challenges, respectively. D1 contains 567 examples whereas T1, D2 and T2 have all the same size, i.e. 800 instances. The positive examples are the 50% of the data. We produced also a random split of D2. The two folds are D2(50%)0 and D2(50%)00. We also used the following resources: the Charniak parser (Charniak, 2000) to carry out the syntactic analysis; the wn::similarity package (Pedersen et al., 2004) to compute the Jiang&amp;Conrath (J&amp;C) distance (Jiang and Conrath, 1997) needed to implement the lexical similarity siml(T, H) as defined in (Corley and Mihalcea, 2005); SVM-lightTK (Moschitti, 2004) to encode the basic tree kernel function, KT, in SVM-light (Joachims, 1999). 3.3 Results and analysis Table 1 reports the accuracy of different similarity kernels on the different training and test split described in the previous section. The table shows some important result. First, as observed in (Corley and Mihalcea, 2005) the lexical-based distance kernel Kl shows an accuracy significantly higher than the random baseline, i.e. 50%. This accuracy (second line) is comparable with the best systems in the first RTE challenge (Dagan et al., 2005). The accuracy reported for the best systems, i.</context>
</contexts>
<marker>Moschitti, 2004</marker>
<rawString>Alessandro Moschitti. 2004. A study on convolution kernels for shallow semantic parsing. In proceedings of the ACL, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>Wordnet::similarity - measuring the relatedness of concepts.</title>
<date>2004</date>
<booktitle>In Proc. of 5th NAACL,</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="11238" citStr="Pedersen et al., 2004" startWordPosition="1969" endWordPosition="1973">Experimental settings For the experiments, we used the Recognizing Textual Entailment (RTE) Challenge data sets, which we name as D1, T1 and D2, T2, are the development and the test sets of the first and second RTE challenges, respectively. D1 contains 567 examples whereas T1, D2 and T2 have all the same size, i.e. 800 instances. The positive examples are the 50% of the data. We produced also a random split of D2. The two folds are D2(50%)0 and D2(50%)00. We also used the following resources: the Charniak parser (Charniak, 2000) to carry out the syntactic analysis; the wn::similarity package (Pedersen et al., 2004) to compute the Jiang&amp;Conrath (J&amp;C) distance (Jiang and Conrath, 1997) needed to implement the lexical similarity siml(T, H) as defined in (Corley and Mihalcea, 2005); SVM-lightTK (Moschitti, 2004) to encode the basic tree kernel function, KT, in SVM-light (Joachims, 1999). 3.3 Results and analysis Table 1 reports the accuracy of different similarity kernels on the different training and test split described in the previous section. The table shows some important result. First, as observed in (Corley and Mihalcea, 2005) the lexical-based distance kernel Kl shows an accuracy significantly highe</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. Wordnet::similarity - measuring the relatedness of concepts. In Proc. of 5th NAACL, Boston, MA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>