<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005110">
<title confidence="0.973102">
Two-dimensional Clustering for Text Categorization
</title>
<author confidence="0.967321">
Hiroya Takamura and Yuji Matsumoto
</author>
<affiliation confidence="0.857297333333333">
Department of Information Technology
Nara Institute of Science and Technology
8516-9, Takayama, Ikoma, Nara 630-0101, Japan
</affiliation>
<email confidence="0.996431">
fhiroya-t,matsul@is.aist-nara.ac.jp
</email>
<sectionHeader confidence="0.987593" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999863133333333">
We propose a new method to improve the
accuracy of Text Categorization using two-
dimensional clustering. In a number of previ-
ous probabilistic approaches, texts in the same
category are implicitly assumed to be gener-
ated from an identical distribution. We empiri-
cally show that this assumption is not accurate,
and propose a new framework based on two-
dimensional clustering to alleviate this problem.
In our method, training texts are clustered so
that the assumption is more likely to be true,
and at the same time, features are also clustered
in order to tackle the data sparseness problem.
We conduct some experiments to validate the
proposed two-dimensional clustering method.
</bodyText>
<sectionHeader confidence="0.995587" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999974">
Text Categorization is the task of classify-
ing texts into their most plausible category.
One problem in most previous probabilistic ap-
proaches to Text Categorization is that texts
in the same category are assumed to be gener-
ated by an identical distribution (we call it the
i.d. assumption, in this paper). However, cate-
gories are manually defined and there is no pre-
defined probabilistic structure behind them, as
discussed in the next section. Another problem
with Text Categorization is the data-sparseness
problem caused by the high dimensionality of
the feature space. The frequency of each word
is usually so small that it is difficult to estimate
reliable statistics.
In order to tackle these problems, we propose
a new framework based on two-dimensional
clustering. We first cluster training texts into
several clusters whose elements can be thought
as being generated from an identical distribu-
tion before estimating the probability model of
each category. The data-sparseness problem is
more critical, if the number of parameters is
larger as in the text clustering approach we are
adopt. So we alleviate this problem by cluster-
ing features (words). That is to say, we cluster
both texts and features simultaneously.
Through experiments, we show that our ap-
proach works well with probabilistic classifiers.
In Natural Language Processing, several clus-
tering applications have been proposed, for ex-
ample in (Brown, 1992; Li and Abe, 1998).
Among those applications, (Baker et al, 1998)
applied the class-distributional clustering to
Text Categorization. They theoretically proved
the optimality of their clustering method in
terms of Naive Bayes Score, and validated it
empirically. In class-distributional clustering,
occurrences of categories given a word are re-
garded as a probability distribution, and words
are clustered according to this distribution. In
(Slonim and Tishby, 2001), they use the Infor-
mation Bottleneck Method (Tishby et al, 1999)
for Text Categorization. Both (Baker et al,
1998) and (Slonim and Tishby, 2001), however,
deal only with word clustering. Unlike those
methods, our method adopts a two-dimensional
clustering of words and texts.
The idea of clustering words and texts simul-
taneously is also pursued in (Slonim and Tishby,
2000; Dhillon, 2001). However, those are con-
cerned exclusively with clustering and do not
propose any framework applicable to Text Cat-
egorization.
This paper is organized as follows. In Section
2, we investigate the i.d. assumption in Text
Categorization. In Section 3, we describe our
clustering methods. In Section 4, we explain our
categorization methods. Section 5 presents the
experiments and results with discussions. Fi-
nally, in Section 6, we summarize our research.
</bodyText>
<figure confidence="0.980184166666667">
1.6
1.4
Clusters in &amp;quot;earn&amp;quot;
Other Categories
1.2
KL-divergence
1
0.8
0.6
0.4
0.2
B C D Other Categories
</figure>
<bodyText confidence="0.964598">
Given a set of co-occurrence samples of words
and texts:
</bodyText>
<equation confidence="0.98802325">
S = {(w1, (w2 d2) , (Wm, dm)} , (2)
its log-likelihood is calculated as :
E(.,d)Es log P(w,
= E(.,d)Es log P(Cw, Cd)P(wIC.)P(drd).(3)
</equation>
<bodyText confidence="0.5225165">
The parameters in model (1) are estimated with
the Maximum Likelihood Estimation :
</bodyText>
<equation confidence="0.999565714285714">
N(Cw, Cd)
P(C,Cd) =
ISI
N(w)
P(wiCw) = N(C)&apos;
N(d)
P(drd) = N(C)&apos;
</equation>
<bodyText confidence="0.999355">
where N(x) denotes the frequency of x.
</bodyText>
<subsectionHeader confidence="0.999837">
3.2 Clustering Algorithms
</subsectionHeader>
<bodyText confidence="0.999351375">
In the algorithm described in (Li and Abe,
1998), given two positive integers k and 1, merg-
ing for the first dimension is performed k times,
followed by 1 merges for the second dimension.
We propose two different clustering algo-
rithms In both algorithms, a pair of words
or texts are chosen and merged at each step,
based on the model described in Section 3.1.
The difference is the way to choose the pair of
words or texts to be merged. One is what we
call text-first clustering, in which text clustering
is conducted first, followed by word clustering.
The other is greedy clustering, in which, at each
step, the pair with the least likelihood decrease
is searched from the word pairs and the text
pairs, and merged:
</bodyText>
<listItem confidence="0.99989">
• Text-first Clustering
1. Initialize
2. Merge two text clusters with the least
likelihood decrease repeatedly, while
the stopping criterion is not satisfied.
3. Merge two word clusters with the least
likelihood decrease repeatedly, while
the stopping criterion is not satisfied.
• Greedy Clustering
1. Initialize
</listItem>
<bodyText confidence="0.979599409090909">
2. Merge two text clusters or two word
clusters with the least likelihood de-
crease repeatedly, while the stopping
criterion is not satisfied.
We set the constraints that only texts in the
same category can be merged (we call the
category-constraint), and that only words with
the same part-of-speech can be merged (pos-
constraint). The category-constraint is indis-
pensable in our method, because of our catego-
rization method which is explained later. Both
of these constraints decrease the computational
time needed for clustering.
The text-first clustering has the advantage
that word clustering can be conducted using the
information given by class-distribution2. Class-
distributional clustering is a special case of text-
first clustering. If the stopping criterion of the
text clustering step is set as &amp;quot;no two clusters
can be merged without violating the category-
constraint&amp;quot;, then text-first clustering is identi-
cal to the class-distributional clustering.
</bodyText>
<subsectionHeader confidence="0.987227">
3.3 The Relation to Jensen-Shannon
Divergence
</subsectionHeader>
<bodyText confidence="0.999931555555555">
Here we show that using the criterion of the
least likelihood decrease is equivalent to select-
ing the closest pair of clusters in terms of a cer-
tain distance as a probability distribution. Let
AL denote the decrease of the log-likelihood (3)
caused by merging word-clusters i and j. Let ISI
denote the number of the whole training exam-
ples. Using P(C,i, Cd) = P(C„Cd)+P(Ci,Cd),
AL divided by I SI is transformed as:
</bodyText>
<equation confidence="0.984105">
AL
Isl
Ecd —P(Cii, Cd) log PP( ;)P(Cd
▪ Cd P (Ci, Cd) log piZii;C(C)d)
p(ci ,cd)
▪ Cd P(Ci, Cd) log P(C,i)P(Cd)
Cd P(Ci,Cc1){1°g ;DV d)
, p(cii,cd)
&apos;13g P(C)P(Cd)
P(Cj,Cd)
Cd P(Ci, Cd){1°g P(Ci)P(Cd)
</equation>
<bodyText confidence="0.6514695">
2More precisely, the information used in the text-
first clustering is different from the information given by
class-distribution, but as the clustering proceeds, these
two types of information become more similar.
</bodyText>
<equation confidence="0.996637857142857">
—log P1:76)
„ij11;(d)}
= P(Ci)ECd PP((CCddlICCiii))
P(Cdri) log
+P(Ci) Ecd P(Cd ICJ) log PP((ccddlIcc)
= P(C,)DKL(P(.ICz)IIP(&apos;ICzi))
+P(CADKL(P(&apos;ICAIIP(&apos;ICzi)), (7)
</equation>
<bodyText confidence="0.999797555555555">
where DKL(pliq) is the KL-divergence between
the probability distribution p and q. The last
line of (7) is the Jensen-Shannon divergence,
which is also known as &amp;quot;KL divergence to the
mean&amp;quot;. That is, in our method, the closest
pair of clusters in terms of the Jensen-Shannon
divergence is merged at each step. In other
words, the clustering method used in (Baker et
al, 1998) is valid in terms of the likelihood.
</bodyText>
<subsectionHeader confidence="0.995438">
3.4 AIC-based Stopping Criterion
</subsectionHeader>
<bodyText confidence="0.998944">
As the stopping criterion in the clustering algo-
rithm, we adopt AIC (Akaike Information Cri-
terion) (Akaike, 1974). In (Li and Abe, 1998),
they use MDL (Minimum Description Length)
Principle (Rissanen, 1987). We do not use MDL
Principle, because it tends to predict too small
numbers of clusters in preliminary experiments
(for text clustering, it predicted a smaller num-
ber of clusters than the number of categories,
which is not suitable for our method because of
the category-constraints).
AIC is realized as follows. The decrease of
the number of parameters caused by merging a
pair of clusters is :
</bodyText>
<equation confidence="0.9287578">
AN _ I I C(D ) I — 1, (word-merge)
P (IC(W)I —1, (text-merge) (8)
where,
IC(D)I = Number of clusters of words,
IC(W)I = Number of clusters of texts.
</equation>
<bodyText confidence="0.670006">
According to AIC, the stopping criterion should
be
</bodyText>
<equation confidence="0.530857">
(9)
</equation>
<bodyText confidence="0.995772">
The first term AL denotes the decrease of log-
likelihood (3) caused by merging.
Note that, in the text-first clustering, there
are two possible points where AIC is applied.
One is the point when the text clustering is fin-
ished, and the other is when the word clustering
is finished.
</bodyText>
<sectionHeader confidence="0.993275" genericHeader="introduction">
4 Categorization
</sectionHeader>
<bodyText confidence="0.999952433333333">
Probabilistic classifiers are expected to yield
good results combined with our cluster-
ing method, but the performance of non-
probabilistic classifiers with our method is un-
predictable. We evaluate our clustering method
using NB (Naive Bayes) classifiers (Mitchell,
1997), which is a probabilistic classifier, and
SVMs (Support Vector Machines) (Vapnik,
1995), which is a non-probabilistic classifier.
In our method, the texts are clustered before-
hand. So we first categorize the test texts and
predict which cluster each test text belongs to.
Then, we assign to each text the category that
the predicted cluster belongs to (in our cluster-
ing method, all the training texts in each cluster
are supposed to have the same category tag).
For the NB classifier, we use the Multinomial
Model (McCallum and Nigam, 1998), but ignore
the concern of document length.
SVM is a binary classifier based on Structural
Risk Minimization (Vapnik, 1995). It has a high
generalization performance and has been suc-
cessfully applied to Text Categorization, for ex-
ample in (Joachims, 1998). In order to apply
SVMs to multi-class classification, we use the
one-versus-rest method. However, when con-
structing a hyperplane for one cluster, the train-
ing texts belonging to the other clusters in the
same category are removed from the training
set.
</bodyText>
<sectionHeader confidence="0.999788" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998694">
5.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.9999146">
The data corpus used in this research is Reuters-
215783. We removed the texts whose body
was meaningless, after applying ModApte-split,
which is a standard way to split the corpus
into training texts and test texts. This proce-
dure yielded 8815 training texts, 3023 test texts
and 116 categories. We used as features only
nouns, verbs, proper nouns, adjectives and ad-
verbs that occur five times or more in the whole
training data. Stemming was also done using
TreeTagger (Schmid, 1994).
Some of the texts in Reuters-21578 have mul-
tiple category-tags. In the clustering phase, we
introduced multiple copies of those texts and la-
bel each text with one of its tags, so that every
</bodyText>
<figure confidence="0.959660551724138">
3 Available at http://www.research.att.comrlewis/
0.89
0.88
0.87
Accuracy
0.86
0.85
0.84
0.83
0.82
0 10 20 30 40 50 60 70 80 90 100
Compression Rate of Words (%)
0.9
0.89
0.88
0.87
Accuracy
0.86
0.85
0.84
0.83
0.82
Text-first Clustering
Class-Dist Clustering
0.81
0 10 20 30 40 50 60 70 80 90 100
Compression Rate of Words (%)
Text-first Clustering
Class-Dist Clustering
</figure>
<table confidence="0.987564">
Clustering Compress. Accuracy
Method Rate (%)
Class-Dist(AIC) 13.4 0.859
(Actual) 60 0.871
Text-first(AIC) 16.6 0.880
(Actual) 30 0.881
</table>
<tableCaption confidence="0.999802">
Table 1: Prediction of Number of Word-clusters
</tableCaption>
<bodyText confidence="0.9949416">
gether with the actual best compression rates
and their accuracies.
Table 2 shows the performance of NB classifi-
cation combined with greedy clustering. In the
case of greedy clustering, it is necessary to dis-
play both word compression rates and text com-
pression rates, so we didn&apos;t include the results
of the greedy clustering into Figure 2. In Table
2, the compression rates predicted by AIC, 9.7%
for words and 6.8% for texts, are also displayed.
</bodyText>
<subsectionHeader confidence="0.978219">
5.3 Discussion
</subsectionHeader>
<bodyText confidence="0.999973586206896">
At the point of 100% word compression rate (i.e.
no comression) in Figure 2, text-first clustering
performs better than class-distributional clus-
tering, although the difference is small (at this
point, texts have been clustered in the text-first
clustering). As the word compression rate de-
creases, the difference of the accuracy increases.
This means that the combination of text clus-
tering and word clustering works well.
Figure 3 shows that, also for SVMs, text-first
clustering outperforms class-distributional clus-
tering. However, the performance is worse for
smaller compression rates. This means, in terms
of accuracy, word-clustering is not effective for
SVMs. The clustering of texts is still effective.
Predicted compression rates in Table 1 are
not close to the actual best compression rates,
although the corresponding accuracy is not so
different for the text-first clustering. The dif-
ference of two AIC-predicted accuracies is sig-
nificant in the sign-test (with 1% significance-
level). The difference of the AIC-predicted ac-
curacy of our method and the accuracy without
clustering is also siginificant in the same test
with 5% siginificance-level.
Table 2 shows that the greedy-clustering does
not work well. The reason would be that word-
clustering in the early stage cannot use the in-
formation of class-distribution.
</bodyText>
<sectionHeader confidence="0.977931" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999985227272727">
We proposed a new method to improve the ac-
curacy of Text Categorization using the two-
dimensional clustering. In our method, both
training texts and features are clustered before
estimating the probability model.
Our approach is motivated by the fact that,
in most previous probabilistic approaches, one
category is assumed to have one identical proba-
bilistic distribution, but this assumption is not
always true, as discussed in this paper. Our
two-dimensional clustering approach alleviates
this problem, and at the same time, it can avoid
the data-sparseness problem.
Through experiments, we showed that two-
dimensional clustering worked well with Naive
Bayes Classifiers and that, for the SVMs,
two-dimensional clustering outperformed class-
distributional clustering.
Future work includes the following.
First, in this research, we conducted experi-
ments with only one data set. It would be de-
sirable to confirm our conclusions with further
experiments using different data sets. We used
AIC as a stopping criterion of the text clustering
step in the text-first clustering. But we haven&apos;t
investigated whether AIC was valid as the turn-
ing criterion, because it needs experiments over
two-dimensional parameter space. This point
has to be investigated. As a stopping criterion,
AIC does not always work well enough. Better
criteria should be pursued. In our framework,
AIC is actually targetting the joint probability
of words and texts. But, in order to obtain a
better stopping criterion, AIC should be incor-
porated in a more sophisticated way, such that
it aims at the categorization.
We used an agglomerative clustering, but a
divisive clustering method might be better in
terms of computational time.
One of the possible extensions of this model
is the soft version, as discussed in (Hofmann,
1998), in which the Expectation-Maximization
algorithm is used with the soft version of this
model.
</bodyText>
<sectionHeader confidence="0.994017" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.971834666666667">
Akaike, H. 1974. A New Look at the Statis-
tical Model Identification. IEEE Trans. Au-
torn. Control, vol. AC-19, pp. 716-723.
</reference>
<tableCaption confidence="0.982363">
Table 2: Categorization Accuracy (NB with Greedy Clustering)
</tableCaption>
<table confidence="0.999785166666667">
Word Compres.(%) 100.0 90.0 80.0 70.0 60.0 50.0 40.0 30.0 20.0
Text Compres.(%) 100.0 94.8 94.3 93.8 93.1 91.8 43.1 29.9 17.0
Acc. 0.717 0.718 0.719 0.722 0.731 0.739 0.807 0.834 0.836
10.0 9.7(AIC) 9.0 8.0 7.0 6.0 5.0 4.0 3.0 2.0
7.1 6.8 6.2 5.5 4.9 4.1 3.5 2.8 2.3 1.8
0.848 0.848 0.847 0.848 0.849 0.846 0.846 0.843 0.837 0.841
</table>
<reference confidence="0.998551461538462">
Baker, D. and McCallum, A. 1998. Distribu-
tional Clustering of Words for Text Classifica-
tion. Proceedings of SIGIR-98, 21st ACM In-
ternational Conference on Research and De-
velopment in Information Retrieval, pp. 96-
103.
Brown, P., Pietra, V.J., deSouza, P.V., Lai, J.C.
and Mercer, R.L. 1992. Class-based N-gram
Models of Natural Language. Computational
Linguistics, 18(4), pp. 467-479.
Church, K. and Hanks, P. 1990. Word Asso-
ciation Norms, Mutual Information and Lex-
icography. Computational Linguistics 16(1),
pp. 22-29.
Dhillon, I. 2001. Co-clustering Documents
and Words using Bipartite Spectral Graph
Partitioning. Technical Report 2001-05, UT
Austin CS Dept.
Hofmann, T. and Puzicha, J. 1998. Statistical
Models for Cooccurrence Data. AI-MEMO
1625, Artificial Intelligence Laboratory, Mas-
sachusetts Institute of Technology.
Joachims, T. 1998. Text Categorization with
Support Vector Machines: Learning with
Many Relevant Features. Proceedings of the
European Conference on Machine Learning.
Li, H. and Abe, N. 1998. Word Clustering
and Disambiguation Based on Co-occurrence
Data. Proceedings of COLING-ACL 98, pp.
749-755.
McCallum, A. and Nigam, K. 1998. A Com-
parison of Event Models for Naive Bayes
Text Classification. Proceedings of AAAI-98
Workshop on Learning for Text Categoriza-
tion, pp. 41-48.
Mitchell, T. 1997. Machine Learning, McGraw
Hill.
Nigam, K., McCallum, A., Thrun, S. and
Mitchell, T. 2000. Text Classification from
Labeled and Unlabeled Documents using EM.
Machine Learning, 39(2/3). pp. 103-134.
Rissanen, J. 1987. Stochastic Complexity.
Journal of Royal Statistical Society, Series B,
49(3), pp. 223-239.
Schmid, H. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In Proceedings
of International Conference on New Methods
in Language Processing, pp. 44-49, Manch-
ester.
Slonim, N. and Tishby, N. 2000. Document
Clustering using Word Clusters via the In-
formation Bottleneck Method. Research and
Development in Information Retrieval, pp.
208-215.
Slonim, N. and Tishby, N. 2001. The Power of
Word Clusters for Text Classification. 23rd
European Colloquium on Information Re-
trieval Research.
Tishby, N., Pereira, F. and Bialek, W. 1999.
The Information Bottleneck Method. Pro-
ceedings of the 37-th Annual Allerton Confer-
ence on Communication, Control and Com-
puting, pp. 368-377.
Vapnik, V. 1995. The Nature of Statistical
Learning Theory. Springer.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.901739">
<title confidence="0.999269">Two-dimensional Clustering for Text Categorization</title>
<author confidence="0.968364">Hiroya Takamura</author>
<author confidence="0.968364">Yuji</author>
<affiliation confidence="0.9998815">Department of Information Nara Institute of Science and</affiliation>
<address confidence="0.971956">8516-9, Takayama, Ikoma, Nara 630-0101,</address>
<email confidence="0.989921">fhiroya-t,matsul@is.aist-nara.ac.jp</email>
<abstract confidence="0.9975403125">propose a new method to the accuracy of Text Categorization using twodimensional clustering. In a number of previous probabilistic approaches, texts in the same category are implicitly assumed to be generated from an identical distribution. We empirically show that this assumption is not accurate, and propose a new framework based on twodimensional clustering to alleviate this problem. In our method, training texts are clustered so that the assumption is more likely to be true, and at the same time, features are also clustered in order to tackle the data sparseness problem. We conduct some experiments to validate the proposed two-dimensional clustering method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Akaike</author>
</authors>
<title>A New Look at the Statistical Model Identification.</title>
<date>1974</date>
<journal>IEEE Trans. Autorn. Control,</journal>
<volume>19</volume>
<pages>716--723</pages>
<contexts>
<context position="7784" citStr="Akaike, 1974" startWordPosition="1225" endWordPosition="1226">DKL(P(.ICz)IIP(&apos;ICzi)) +P(CADKL(P(&apos;ICAIIP(&apos;ICzi)), (7) where DKL(pliq) is the KL-divergence between the probability distribution p and q. The last line of (7) is the Jensen-Shannon divergence, which is also known as &amp;quot;KL divergence to the mean&amp;quot;. That is, in our method, the closest pair of clusters in terms of the Jensen-Shannon divergence is merged at each step. In other words, the clustering method used in (Baker et al, 1998) is valid in terms of the likelihood. 3.4 AIC-based Stopping Criterion As the stopping criterion in the clustering algorithm, we adopt AIC (Akaike Information Criterion) (Akaike, 1974). In (Li and Abe, 1998), they use MDL (Minimum Description Length) Principle (Rissanen, 1987). We do not use MDL Principle, because it tends to predict too small numbers of clusters in preliminary experiments (for text clustering, it predicted a smaller number of clusters than the number of categories, which is not suitable for our method because of the category-constraints). AIC is realized as follows. The decrease of the number of parameters caused by merging a pair of clusters is : AN _ I I C(D ) I — 1, (word-merge) P (IC(W)I —1, (text-merge) (8) where, IC(D)I = Number of clusters of words,</context>
</contexts>
<marker>Akaike, 1974</marker>
<rawString>Akaike, H. 1974. A New Look at the Statistical Model Identification. IEEE Trans. Autorn. Control, vol. AC-19, pp. 716-723.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Baker</author>
<author>A McCallum</author>
</authors>
<title>Distributional Clustering of Words for Text Classification.</title>
<date>1998</date>
<booktitle>Proceedings of SIGIR-98, 21st ACM International Conference on Research and Development in Information Retrieval,</booktitle>
<pages>96--103</pages>
<marker>Baker, McCallum, 1998</marker>
<rawString>Baker, D. and McCallum, A. 1998. Distributional Clustering of Words for Text Classification. Proceedings of SIGIR-98, 21st ACM International Conference on Research and Development in Information Retrieval, pp. 96-103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>V J Pietra</author>
<author>P V deSouza</author>
<author>J C Lai</author>
<author>R L Mercer</author>
</authors>
<date>1992</date>
<journal>Class-based N-gram Models of Natural Language. Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<pages>467--479</pages>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>Brown, P., Pietra, V.J., deSouza, P.V., Lai, J.C. and Mercer, R.L. 1992. Class-based N-gram Models of Natural Language. Computational Linguistics, 18(4), pp. 467-479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
<author>P Hanks</author>
</authors>
<date>1990</date>
<journal>Word Association Norms, Mutual Information and Lexicography. Computational Linguistics</journal>
<volume>16</volume>
<issue>1</issue>
<pages>22--29</pages>
<marker>Church, Hanks, 1990</marker>
<rawString>Church, K. and Hanks, P. 1990. Word Association Norms, Mutual Information and Lexicography. Computational Linguistics 16(1), pp. 22-29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dhillon</author>
</authors>
<title>Co-clustering Documents and Words using Bipartite Spectral Graph Partitioning.</title>
<date>2001</date>
<tech>Technical Report 2001-05,</tech>
<institution>UT Austin CS Dept.</institution>
<contexts>
<context position="3235" citStr="Dhillon, 2001" startWordPosition="490" endWordPosition="491">ated it empirically. In class-distributional clustering, occurrences of categories given a word are regarded as a probability distribution, and words are clustered according to this distribution. In (Slonim and Tishby, 2001), they use the Information Bottleneck Method (Tishby et al, 1999) for Text Categorization. Both (Baker et al, 1998) and (Slonim and Tishby, 2001), however, deal only with word clustering. Unlike those methods, our method adopts a two-dimensional clustering of words and texts. The idea of clustering words and texts simultaneously is also pursued in (Slonim and Tishby, 2000; Dhillon, 2001). However, those are concerned exclusively with clustering and do not propose any framework applicable to Text Categorization. This paper is organized as follows. In Section 2, we investigate the i.d. assumption in Text Categorization. In Section 3, we describe our clustering methods. In Section 4, we explain our categorization methods. Section 5 presents the experiments and results with discussions. Finally, in Section 6, we summarize our research. 1.6 1.4 Clusters in &amp;quot;earn&amp;quot; Other Categories 1.2 KL-divergence 1 0.8 0.6 0.4 0.2 B C D Other Categories Given a set of co-occurrence samples of wor</context>
</contexts>
<marker>Dhillon, 2001</marker>
<rawString>Dhillon, I. 2001. Co-clustering Documents and Words using Bipartite Spectral Graph Partitioning. Technical Report 2001-05, UT Austin CS Dept.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hofmann</author>
<author>J Puzicha</author>
</authors>
<title>Statistical Models for Cooccurrence Data.</title>
<date>1998</date>
<tech>AI-MEMO 1625,</tech>
<institution>Artificial Intelligence Laboratory, Massachusetts Institute of Technology.</institution>
<marker>Hofmann, Puzicha, 1998</marker>
<rawString>Hofmann, T. and Puzicha, J. 1998. Statistical Models for Cooccurrence Data. AI-MEMO 1625, Artificial Intelligence Laboratory, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Text Categorization with Support Vector Machines: Learning with Many Relevant Features.</title>
<date>1998</date>
<booktitle>Proceedings of the European Conference on Machine Learning.</booktitle>
<contexts>
<context position="9844" citStr="Joachims, 1998" startWordPosition="1565" endWordPosition="1566">o we first categorize the test texts and predict which cluster each test text belongs to. Then, we assign to each text the category that the predicted cluster belongs to (in our clustering method, all the training texts in each cluster are supposed to have the same category tag). For the NB classifier, we use the Multinomial Model (McCallum and Nigam, 1998), but ignore the concern of document length. SVM is a binary classifier based on Structural Risk Minimization (Vapnik, 1995). It has a high generalization performance and has been successfully applied to Text Categorization, for example in (Joachims, 1998). In order to apply SVMs to multi-class classification, we use the one-versus-rest method. However, when constructing a hyperplane for one cluster, the training texts belonging to the other clusters in the same category are removed from the training set. 5 Experiments 5.1 Experimental Settings The data corpus used in this research is Reuters215783. We removed the texts whose body was meaningless, after applying ModApte-split, which is a standard way to split the corpus into training texts and test texts. This procedure yielded 8815 training texts, 3023 test texts and 116 categories. We used as</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>Joachims, T. 1998. Text Categorization with Support Vector Machines: Learning with Many Relevant Features. Proceedings of the European Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Li</author>
<author>N Abe</author>
</authors>
<title>Word Clustering and Disambiguation Based on Co-occurrence Data.</title>
<date>1998</date>
<booktitle>Proceedings of COLING-ACL 98,</booktitle>
<pages>749--755</pages>
<contexts>
<context position="2397" citStr="Li and Abe, 1998" startWordPosition="364" endWordPosition="367">ose elements can be thought as being generated from an identical distribution before estimating the probability model of each category. The data-sparseness problem is more critical, if the number of parameters is larger as in the text clustering approach we are adopt. So we alleviate this problem by clustering features (words). That is to say, we cluster both texts and features simultaneously. Through experiments, we show that our approach works well with probabilistic classifiers. In Natural Language Processing, several clustering applications have been proposed, for example in (Brown, 1992; Li and Abe, 1998). Among those applications, (Baker et al, 1998) applied the class-distributional clustering to Text Categorization. They theoretically proved the optimality of their clustering method in terms of Naive Bayes Score, and validated it empirically. In class-distributional clustering, occurrences of categories given a word are regarded as a probability distribution, and words are clustered according to this distribution. In (Slonim and Tishby, 2001), they use the Information Bottleneck Method (Tishby et al, 1999) for Text Categorization. Both (Baker et al, 1998) and (Slonim and Tishby, 2001), howev</context>
<context position="4244" citStr="Li and Abe, 1998" startWordPosition="656" endWordPosition="659">iscussions. Finally, in Section 6, we summarize our research. 1.6 1.4 Clusters in &amp;quot;earn&amp;quot; Other Categories 1.2 KL-divergence 1 0.8 0.6 0.4 0.2 B C D Other Categories Given a set of co-occurrence samples of words and texts: S = {(w1, (w2 d2) , (Wm, dm)} , (2) its log-likelihood is calculated as : E(.,d)Es log P(w, = E(.,d)Es log P(Cw, Cd)P(wIC.)P(drd).(3) The parameters in model (1) are estimated with the Maximum Likelihood Estimation : N(Cw, Cd) P(C,Cd) = ISI N(w) P(wiCw) = N(C)&apos; N(d) P(drd) = N(C)&apos; where N(x) denotes the frequency of x. 3.2 Clustering Algorithms In the algorithm described in (Li and Abe, 1998), given two positive integers k and 1, merging for the first dimension is performed k times, followed by 1 merges for the second dimension. We propose two different clustering algorithms In both algorithms, a pair of words or texts are chosen and merged at each step, based on the model described in Section 3.1. The difference is the way to choose the pair of words or texts to be merged. One is what we call text-first clustering, in which text clustering is conducted first, followed by word clustering. The other is greedy clustering, in which, at each step, the pair with the least likelihood de</context>
<context position="7807" citStr="Li and Abe, 1998" startWordPosition="1228" endWordPosition="1231">i)) +P(CADKL(P(&apos;ICAIIP(&apos;ICzi)), (7) where DKL(pliq) is the KL-divergence between the probability distribution p and q. The last line of (7) is the Jensen-Shannon divergence, which is also known as &amp;quot;KL divergence to the mean&amp;quot;. That is, in our method, the closest pair of clusters in terms of the Jensen-Shannon divergence is merged at each step. In other words, the clustering method used in (Baker et al, 1998) is valid in terms of the likelihood. 3.4 AIC-based Stopping Criterion As the stopping criterion in the clustering algorithm, we adopt AIC (Akaike Information Criterion) (Akaike, 1974). In (Li and Abe, 1998), they use MDL (Minimum Description Length) Principle (Rissanen, 1987). We do not use MDL Principle, because it tends to predict too small numbers of clusters in preliminary experiments (for text clustering, it predicted a smaller number of clusters than the number of categories, which is not suitable for our method because of the category-constraints). AIC is realized as follows. The decrease of the number of parameters caused by merging a pair of clusters is : AN _ I I C(D ) I — 1, (word-merge) P (IC(W)I —1, (text-merge) (8) where, IC(D)I = Number of clusters of words, IC(W)I = Number of clu</context>
</contexts>
<marker>Li, Abe, 1998</marker>
<rawString>Li, H. and Abe, N. 1998. Word Clustering and Disambiguation Based on Co-occurrence Data. Proceedings of COLING-ACL 98, pp. 749-755.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>K Nigam</author>
</authors>
<title>A Comparison of Event Models for Naive Bayes Text Classification.</title>
<date>1998</date>
<booktitle>Proceedings of AAAI-98 Workshop on Learning for Text Categorization,</booktitle>
<pages>41--48</pages>
<contexts>
<context position="9588" citStr="McCallum and Nigam, 1998" startWordPosition="1523" endWordPosition="1526"> evaluate our clustering method using NB (Naive Bayes) classifiers (Mitchell, 1997), which is a probabilistic classifier, and SVMs (Support Vector Machines) (Vapnik, 1995), which is a non-probabilistic classifier. In our method, the texts are clustered beforehand. So we first categorize the test texts and predict which cluster each test text belongs to. Then, we assign to each text the category that the predicted cluster belongs to (in our clustering method, all the training texts in each cluster are supposed to have the same category tag). For the NB classifier, we use the Multinomial Model (McCallum and Nigam, 1998), but ignore the concern of document length. SVM is a binary classifier based on Structural Risk Minimization (Vapnik, 1995). It has a high generalization performance and has been successfully applied to Text Categorization, for example in (Joachims, 1998). In order to apply SVMs to multi-class classification, we use the one-versus-rest method. However, when constructing a hyperplane for one cluster, the training texts belonging to the other clusters in the same category are removed from the training set. 5 Experiments 5.1 Experimental Settings The data corpus used in this research is Reuters2</context>
</contexts>
<marker>McCallum, Nigam, 1998</marker>
<rawString>McCallum, A. and Nigam, K. 1998. A Comparison of Event Models for Naive Bayes Text Classification. Proceedings of AAAI-98 Workshop on Learning for Text Categorization, pp. 41-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mitchell</author>
</authors>
<date>1997</date>
<booktitle>Machine Learning,</booktitle>
<publisher>Hill.</publisher>
<location>McGraw</location>
<contexts>
<context position="9046" citStr="Mitchell, 1997" startWordPosition="1436" endWordPosition="1437">g to AIC, the stopping criterion should be (9) The first term AL denotes the decrease of loglikelihood (3) caused by merging. Note that, in the text-first clustering, there are two possible points where AIC is applied. One is the point when the text clustering is finished, and the other is when the word clustering is finished. 4 Categorization Probabilistic classifiers are expected to yield good results combined with our clustering method, but the performance of nonprobabilistic classifiers with our method is unpredictable. We evaluate our clustering method using NB (Naive Bayes) classifiers (Mitchell, 1997), which is a probabilistic classifier, and SVMs (Support Vector Machines) (Vapnik, 1995), which is a non-probabilistic classifier. In our method, the texts are clustered beforehand. So we first categorize the test texts and predict which cluster each test text belongs to. Then, we assign to each text the category that the predicted cluster belongs to (in our clustering method, all the training texts in each cluster are supposed to have the same category tag). For the NB classifier, we use the Multinomial Model (McCallum and Nigam, 1998), but ignore the concern of document length. SVM is a bina</context>
</contexts>
<marker>Mitchell, 1997</marker>
<rawString>Mitchell, T. 1997. Machine Learning, McGraw Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Nigam</author>
<author>A McCallum</author>
<author>S Thrun</author>
<author>T Mitchell</author>
</authors>
<title>Text Classification from Labeled and Unlabeled Documents using EM.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<pages>39--2</pages>
<marker>Nigam, McCallum, Thrun, Mitchell, 2000</marker>
<rawString>Nigam, K., McCallum, A., Thrun, S. and Mitchell, T. 2000. Text Classification from Labeled and Unlabeled Documents using EM. Machine Learning, 39(2/3). pp. 103-134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Rissanen</author>
</authors>
<title>Stochastic Complexity.</title>
<date>1987</date>
<journal>Journal of Royal Statistical Society, Series B,</journal>
<volume>49</volume>
<issue>3</issue>
<pages>223--239</pages>
<contexts>
<context position="7877" citStr="Rissanen, 1987" startWordPosition="1239" endWordPosition="1240"> between the probability distribution p and q. The last line of (7) is the Jensen-Shannon divergence, which is also known as &amp;quot;KL divergence to the mean&amp;quot;. That is, in our method, the closest pair of clusters in terms of the Jensen-Shannon divergence is merged at each step. In other words, the clustering method used in (Baker et al, 1998) is valid in terms of the likelihood. 3.4 AIC-based Stopping Criterion As the stopping criterion in the clustering algorithm, we adopt AIC (Akaike Information Criterion) (Akaike, 1974). In (Li and Abe, 1998), they use MDL (Minimum Description Length) Principle (Rissanen, 1987). We do not use MDL Principle, because it tends to predict too small numbers of clusters in preliminary experiments (for text clustering, it predicted a smaller number of clusters than the number of categories, which is not suitable for our method because of the category-constraints). AIC is realized as follows. The decrease of the number of parameters caused by merging a pair of clusters is : AN _ I I C(D ) I — 1, (word-merge) P (IC(W)I —1, (text-merge) (8) where, IC(D)I = Number of clusters of words, IC(W)I = Number of clusters of texts. According to AIC, the stopping criterion should be (9)</context>
</contexts>
<marker>Rissanen, 1987</marker>
<rawString>Rissanen, J. 1987. Stochastic Complexity. Journal of Royal Statistical Society, Series B, 49(3), pp. 223-239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Probabilistic Part-of-Speech Tagging Using Decision Trees.</title>
<date>1994</date>
<booktitle>In Proceedings of International Conference on New Methods in Language Processing,</booktitle>
<pages>44--49</pages>
<location>Manchester.</location>
<contexts>
<context position="10622" citStr="Schmid, 1994" startWordPosition="1692" endWordPosition="1693">s belonging to the other clusters in the same category are removed from the training set. 5 Experiments 5.1 Experimental Settings The data corpus used in this research is Reuters215783. We removed the texts whose body was meaningless, after applying ModApte-split, which is a standard way to split the corpus into training texts and test texts. This procedure yielded 8815 training texts, 3023 test texts and 116 categories. We used as features only nouns, verbs, proper nouns, adjectives and adverbs that occur five times or more in the whole training data. Stemming was also done using TreeTagger (Schmid, 1994). Some of the texts in Reuters-21578 have multiple category-tags. In the clustering phase, we introduced multiple copies of those texts and label each text with one of its tags, so that every 3 Available at http://www.research.att.comrlewis/ 0.89 0.88 0.87 Accuracy 0.86 0.85 0.84 0.83 0.82 0 10 20 30 40 50 60 70 80 90 100 Compression Rate of Words (%) 0.9 0.89 0.88 0.87 Accuracy 0.86 0.85 0.84 0.83 0.82 Text-first Clustering Class-Dist Clustering 0.81 0 10 20 30 40 50 60 70 80 90 100 Compression Rate of Words (%) Text-first Clustering Class-Dist Clustering Clustering Compress. Accuracy Method </context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Schmid, H. 1994. Probabilistic Part-of-Speech Tagging Using Decision Trees. In Proceedings of International Conference on New Methods in Language Processing, pp. 44-49, Manchester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Slonim</author>
<author>N Tishby</author>
</authors>
<title>Document Clustering using Word Clusters via the Information Bottleneck Method.</title>
<date>2000</date>
<booktitle>Research and Development in Information Retrieval,</booktitle>
<pages>208--215</pages>
<contexts>
<context position="3219" citStr="Slonim and Tishby, 2000" startWordPosition="486" endWordPosition="489">ve Bayes Score, and validated it empirically. In class-distributional clustering, occurrences of categories given a word are regarded as a probability distribution, and words are clustered according to this distribution. In (Slonim and Tishby, 2001), they use the Information Bottleneck Method (Tishby et al, 1999) for Text Categorization. Both (Baker et al, 1998) and (Slonim and Tishby, 2001), however, deal only with word clustering. Unlike those methods, our method adopts a two-dimensional clustering of words and texts. The idea of clustering words and texts simultaneously is also pursued in (Slonim and Tishby, 2000; Dhillon, 2001). However, those are concerned exclusively with clustering and do not propose any framework applicable to Text Categorization. This paper is organized as follows. In Section 2, we investigate the i.d. assumption in Text Categorization. In Section 3, we describe our clustering methods. In Section 4, we explain our categorization methods. Section 5 presents the experiments and results with discussions. Finally, in Section 6, we summarize our research. 1.6 1.4 Clusters in &amp;quot;earn&amp;quot; Other Categories 1.2 KL-divergence 1 0.8 0.6 0.4 0.2 B C D Other Categories Given a set of co-occurrenc</context>
</contexts>
<marker>Slonim, Tishby, 2000</marker>
<rawString>Slonim, N. and Tishby, N. 2000. Document Clustering using Word Clusters via the Information Bottleneck Method. Research and Development in Information Retrieval, pp. 208-215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Slonim</author>
<author>N Tishby</author>
</authors>
<title>The Power of Word Clusters for Text Classification.</title>
<date>2001</date>
<booktitle>23rd European Colloquium on Information Retrieval Research.</booktitle>
<contexts>
<context position="2845" citStr="Slonim and Tishby, 2001" startWordPosition="426" endWordPosition="429">ch works well with probabilistic classifiers. In Natural Language Processing, several clustering applications have been proposed, for example in (Brown, 1992; Li and Abe, 1998). Among those applications, (Baker et al, 1998) applied the class-distributional clustering to Text Categorization. They theoretically proved the optimality of their clustering method in terms of Naive Bayes Score, and validated it empirically. In class-distributional clustering, occurrences of categories given a word are regarded as a probability distribution, and words are clustered according to this distribution. In (Slonim and Tishby, 2001), they use the Information Bottleneck Method (Tishby et al, 1999) for Text Categorization. Both (Baker et al, 1998) and (Slonim and Tishby, 2001), however, deal only with word clustering. Unlike those methods, our method adopts a two-dimensional clustering of words and texts. The idea of clustering words and texts simultaneously is also pursued in (Slonim and Tishby, 2000; Dhillon, 2001). However, those are concerned exclusively with clustering and do not propose any framework applicable to Text Categorization. This paper is organized as follows. In Section 2, we investigate the i.d. assumptio</context>
</contexts>
<marker>Slonim, Tishby, 2001</marker>
<rawString>Slonim, N. and Tishby, N. 2001. The Power of Word Clusters for Text Classification. 23rd European Colloquium on Information Retrieval Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Tishby</author>
<author>F Pereira</author>
<author>W Bialek</author>
</authors>
<title>The Information Bottleneck Method.</title>
<date>1999</date>
<booktitle>Proceedings of the 37-th Annual Allerton Conference on Communication, Control and Computing,</booktitle>
<pages>368--377</pages>
<contexts>
<context position="2910" citStr="Tishby et al, 1999" startWordPosition="437" endWordPosition="440">essing, several clustering applications have been proposed, for example in (Brown, 1992; Li and Abe, 1998). Among those applications, (Baker et al, 1998) applied the class-distributional clustering to Text Categorization. They theoretically proved the optimality of their clustering method in terms of Naive Bayes Score, and validated it empirically. In class-distributional clustering, occurrences of categories given a word are regarded as a probability distribution, and words are clustered according to this distribution. In (Slonim and Tishby, 2001), they use the Information Bottleneck Method (Tishby et al, 1999) for Text Categorization. Both (Baker et al, 1998) and (Slonim and Tishby, 2001), however, deal only with word clustering. Unlike those methods, our method adopts a two-dimensional clustering of words and texts. The idea of clustering words and texts simultaneously is also pursued in (Slonim and Tishby, 2000; Dhillon, 2001). However, those are concerned exclusively with clustering and do not propose any framework applicable to Text Categorization. This paper is organized as follows. In Section 2, we investigate the i.d. assumption in Text Categorization. In Section 3, we describe our clusterin</context>
</contexts>
<marker>Tishby, Pereira, Bialek, 1999</marker>
<rawString>Tishby, N., Pereira, F. and Bialek, W. 1999. The Information Bottleneck Method. Proceedings of the 37-th Annual Allerton Conference on Communication, Control and Computing, pp. 368-377.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer.</publisher>
<contexts>
<context position="9134" citStr="Vapnik, 1995" startWordPosition="1448" endWordPosition="1449">oglikelihood (3) caused by merging. Note that, in the text-first clustering, there are two possible points where AIC is applied. One is the point when the text clustering is finished, and the other is when the word clustering is finished. 4 Categorization Probabilistic classifiers are expected to yield good results combined with our clustering method, but the performance of nonprobabilistic classifiers with our method is unpredictable. We evaluate our clustering method using NB (Naive Bayes) classifiers (Mitchell, 1997), which is a probabilistic classifier, and SVMs (Support Vector Machines) (Vapnik, 1995), which is a non-probabilistic classifier. In our method, the texts are clustered beforehand. So we first categorize the test texts and predict which cluster each test text belongs to. Then, we assign to each text the category that the predicted cluster belongs to (in our clustering method, all the training texts in each cluster are supposed to have the same category tag). For the NB classifier, we use the Multinomial Model (McCallum and Nigam, 1998), but ignore the concern of document length. SVM is a binary classifier based on Structural Risk Minimization (Vapnik, 1995). It has a high genera</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vapnik, V. 1995. The Nature of Statistical Learning Theory. Springer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>