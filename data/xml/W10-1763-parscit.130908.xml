<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005346">
<title confidence="0.961589">
Decision Trees for Lexical Smoothing in Statistical Machine
Translation
</title>
<author confidence="0.420694">
Rabih Zbib† and Spyros Matsoukas and Richard Schwartz and John Makhoul
BBN Technologies, 10 Moulton Street, Cambridge, MA 02138, USA
</author>
<affiliation confidence="0.472818">
† Massachusetts Institute of Technology, 77 Massachusetts Ave., Cambridge, MA 02139, USA
</affiliation>
<sectionHeader confidence="0.973846" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999829952380953">
We present a method for incorporat-
ing arbitrary context-informed word at-
tributes into statistical machine trans-
lation by clustering attribute-qualified
source words, and smoothing their
word translation probabilities using bi-
nary decision trees. We describe two
ways in which the decision trees are
used in machine translation: by us-
ing the attribute-qualified source word
clusters directly, or by using attribute-
dependent lexical translation probabil-
ities that are obtained from the trees,
as a lexical smoothing feature in the de-
coder model. We present experiments
using Arabic-to-English newswire data,
and using Arabic diacritics and part-of-
speech as source word attributes, and
show that the proposed method im-
proves on a state-of-the-art translation
system.
</bodyText>
<sectionHeader confidence="0.998522" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999962456140351">
Modern statistical machine translation (SMT)
models, such as phrase-based SMT or hierar-
chical SMT, implicitly incorporate source lan-
guage context. It has been shown, however,
that such systems can still benefit from the
explicit addition of lexical, syntactic or other
kinds of context-informed word features (Vick-
rey et al., 2005; Gimpel and Smith, 2008;
Brunning et al., 2009; Devlin, 2009). But the
benefit obtained from the addition of attribute
information is in general countered by the in-
crease in the model complexity, which in turn
results in a sparser translation model when es-
timated from the same corpus of data. The
increase in model sparsity usually results in a
deterioration of translation quality.
In this paper, we present a method for using
arbitrary types of source-side context-informed
word attributes, using binary decision trees to
deal with the sparsity side-effect. The deci-
sion trees cluster attribute-dependent source
words by reducing the entropy of the lexi-
cal translation probabilities. We also present
another method where, instead of clustering
the attribute-dependent source words, the de-
cision trees are used to interpolate attribute-
dependent lexical translation probability mod-
els, and use those probabilities to compute a
feature in the decoder log-linear model.
The experiments we present in this paper
were conducted on the translation of Arabic-
to-English newswire data using a hierarchical
system based on (Shen et al., 2008), and using
Arabic diacritics (see section 2.3) and part-of-
speech (POS) as source word attributes. Pre-
vious work that attempts to use Arabic dia-
critics in machine translation runs against the
sparsity problem, and appears to lose most of
the useful information contained in the dia-
critics when using partial diacritization (Diab
et al., 2007). Using the methods proposed
in this paper, we manage to obtain consistent
improvements from diacritics against a strong
baseline. The methods we propose, though,
are not restrictive to Arabic-to-English trans-
lation. The same techniques can also be used
with other language pairs and arbitrary word
attribute types. The attributes we use in the
described experiments are local; but long dis-
tance features can also be used.
In the next section, we review relevant pre-
vious work in three areas: Lexical smoothing
and lexical disambiguation techniques in ma-
chine translation; using decision trees in nat-
ural language processing, and especially ma-
chine translation; and Arabic diacritics. We
present a brief exposition of Arabic orthogra-
</bodyText>
<page confidence="0.974329">
428
</page>
<note confidence="0.9617925">
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 428–437,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999933875">
phy, and refer to previous work on automatic
diacritization of Arabic text. Section 3 de-
scribes the procedure for constructing the deci-
sion trees, and the two methods for using them
in machine translation. In section 4 we de-
scribe the experimental setup and present ex-
perimental results. Finally, section 5 concludes
the paper and discusses future directions.
</bodyText>
<sectionHeader confidence="0.995226" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<subsectionHeader confidence="0.996797">
2.1 Lexical Disambiguation and
Lexical Smoothing
</subsectionHeader>
<bodyText confidence="0.999970322033898">
Various ways have been proposed to improve
the lexical translation choices of SMT systems.
These approaches typically incorporate local
context information, either directly or indi-
rectly.
The use of Word Sense Disambiguation
(WSD) has been proposed to enhance ma-
chine translation by disambiguating the source
words (Cabezas and Resnick, 2005; Carpuat
and Wu, 2007; Chan et al., 2007). WSD
usually requires that the training data be la-
beled with senses, which might not be avail-
able for many languages. Also, WSD is tra-
ditionally formulated as a classification prob-
lem, and therefore does not naturally lend it-
self to be integrated into the generative frame-
work of machine translation. Carpuat and Wu
(2007) formulate the SMT lexical disambigua-
tion problem as a WSD task. Instead of learn-
ing from word sense corpora, they use the SMT
training data, and use local context features to
enhance the lexical disambiguation of phrase-
based SMT.
Sarikaya et al. (2007) incorporate context
more directly by using POS tags on the target
side to model word context. They augmented
the target words with POS tags of the word
itself and its surrounding words, and used the
augmented words in decoding and for language
model rescoring. They reported gains on Iraqi-
Arabic-to-English translation.
Finally, using word-to-word context-free lex-
ical translation probabilities has been shown
to improve the performance of machine trans-
lation systems, even those using much more
sophisticated models. This feature, usually
called lexical smoothing, has been used in
phrase-based systems (Koehn et al., 2003).
Och et al. (2004) also found that including
IBM Model 1 (Brown et al., 1993) word prob-
abilities in their log-linear model works better
than most other higher-level syntactic features
at improving the baseline. The incorporation
of context on the source or target side en-
hances the gain obtained from lexical smooth-
ing. Gimpel and Smith (2008) proposed us-
ing source-side lexical features in phrase-based
SMT by conditioning the phrase probabilities
on those features. They used word context,
syntactic features or positional features. The
features were added as components into the
log-linear decoder model, each with a tunable
weight. Devlin (2009) used context lexical fea-
tures in a hierarchical SMT system, interpolat-
ing lexical counts based on multiple contexts.
It also used target-side lexical features.
The work in the paper incorporates con-
text information based on the reduction of the
translation probability entropy.
</bodyText>
<subsectionHeader confidence="0.988673">
2.2 Decision Trees
</subsectionHeader>
<bodyText confidence="0.999922366666667">
Decision trees have been used extensively in
various areas of machine learning, typically
as a way to cluster patterns in order to im-
prove classification (Duda et al., 2000). They
have, for instance, been long used success-
fully in speech recognition to cluster context-
dependent phoneme model states (Young et
al., 1994).
Decision trees have also been used in ma-
chine translation, although to a lesser extent.
In this respect, our work is most similar to
(Brunning et al., 2009), where the authors ex-
tended word alignment models for IBM Model
1 and Hidden Markov Model (HMM) align-
ments. They used decision trees to cluster the
context-dependent source words. Contexts be-
longing to the same cluster were grouped to-
gether during Expectation Maximization (EM)
training, thus providing a more robust proba-
bility estimate. While Brunning et al. (2009)
used the source context clusters for word align-
ments, we use the attribute-dependent source
words directly in decoding. The approach we
propose can be readily used with any align-
ment model.
Stroppa et al. (2007) presented a general-
ization of phrase-based SMT (Koehn et al.,
2003) that also takes into account source-
side context information. They conditioned
the target phrase probability on the source
</bodyText>
<page confidence="0.998712">
429
</page>
<bodyText confidence="0.999048925925926">
phrase as well as source phrase context, such
as bordering words, or part-of-speech of bor-
dering words. They built a decision tree for
each source phrase extracted from the train-
ing data. The branching of the tree nodes
was based on the different context features,
branching on the most class-discriminative fea-
tures first. Each node is associated with the
set of aligned target phrases and correspond-
ing context-conditioned probabilities. The de-
cision tree thus smoothes the phrase probabil-
ities based on the different features, allowing
the model to back off to less context, or no
context at all depending on the presence of
that context-dependent source phrase in the
training data. The model, however, did not
provide for a back-off mechanism if the phrase
pair was not found in the extracted phrase ta-
ble. The method presented in this paper differs
in various aspects. We use context-dependent
information at the source word level, rather
than the phrase level, thus making it readily
applicable to any translation model and not
just phrase-based translation. By incorporat-
ing context at the word level, we can decode
directly with attribute-augmented source data
(see section 3.2).
</bodyText>
<subsectionHeader confidence="0.996026">
2.3 Arabic Diacritics
</subsectionHeader>
<bodyText confidence="0.999592471428572">
Since an important part of the experiments
described in this paper use diacritized Arabic
source, we present a brief description of Arabic
orthography, and specifically diacritics.
The Arabic script, like that of most other
Semitic languages, only represents consonants
and long vowels using letters 1. Short vowels
can be written as small marks written above
or below the preceding consonant, called di-
acritics. The diacritics are, however, omit-
ted from written text, except in special cases,
thus creating an additional level of lexical am-
biguity. Readers can usually guess the cor-
rect pronunciation of words in non-diacritized
text from the sentence and discourse context.
Grammatical case on nouns and adjectives are
also marked using diacritics at the end of
words. Arabic MT systems use undiacritized
text, since most available Arabic data is undi-
acritized.
✶Such writing systems are sometimes referred to as
Abjads (See Daniels, Peter T., et al. eds. The World&apos;s
Writing Systems Oxford. (1996), p.4.)
Automatic diacritization of Arabic has been
done with high accuracy, using various genera-
tive and discriminative modeling techniques.
For example, Ananthakrishnan et al. (2005)
used a generative model that incorporates
word level n-grams, sub-word level n-grams
and part-of-speech information to perform di-
acritization. Nelken and Shieber (2005) mod-
eled the generative process of dropping dia-
critics using weighted transducers, then used
Viterbi decoding to find the most likely gener-
ator. Zitouni et al. (2006) presented a method
based on maximum entropy classifiers, us-
ing features like character n-grams, word n-
grams, POS and morphological segmentation.
Habash and Rambow (2007) determined vari-
ous morpho-syntactic features of the word us-
ing SVM classifiers, then chose the correspond-
ing diacritization. The experiments in this
paper use the automatic diacritizer by Sakhr
Software. The diacritizer determines word di-
acritics through rule-based morphological and
syntactic analysis. It outputs a diacritization
for both the internal stem and case ending
markers of the word, with an accuracy of 97%
for stem diacritization and 91% for full dia-
critization (i.e., including case endings).
There has been work done on using dia-
critics in Automatic Speech Recognition, e.g.
(Vergyri and Kirchhoff, 2004). However, the
only previous work on using diacritization for
MT is (Diab et al., 2007), which used the di-
acritization system described in (Habash and
Rambow, 2007). It investigated the effect
of using full diacritization as well as partial
diacritization on MT results. The authors
found that using full diacritics deteriorates MT
performance. They used partial diacritiza-
tion schemes, such as diacritizing only passive
verbs, keeping the case endings diacritics, or
only gemination diacritics. They also saw no
gain in most configurations. The authors ar-
gued that the deterioration in performance is
caused by the increase in the size of the vo-
cabulary, which in turn makes the translation
model sparser; as well as by errors during the
automatic diacritization process.
</bodyText>
<page confidence="0.995278">
430
</page>
<sectionHeader confidence="0.9487615" genericHeader="method">
3 Decision Trees for Source Word
Attributes
</sectionHeader>
<subsectionHeader confidence="0.999385">
3.1 Growing the Decision Tree
</subsectionHeader>
<bodyText confidence="0.999988">
In this section, we describe the procedure
for growing the decision trees using context-
informed source word attributes.
The attribute-qualified source-side of the
parallel training data is first aligned to the
target-side data. If S is the set of attribute-
dependent forms of source word s, and tj is a
target word aligned to si E S, then we define:
</bodyText>
<equation confidence="0.9462975">
p(tj|si) _ count(si,tj)
— count(si)
</equation>
<bodyText confidence="0.999937363636364">
where count(si,tj) is the count of alignment
links between si and tj.
A separate binary decision tree is grown for
each source word. We start by including all the
attribute-dependent forms of the source word
at the root of the tree. We split the set of at-
tributes at each node into two child nodes, by
choosing the splitting that maximizes the re-
duction in weighted entropy of the probability
distribution in (1). In other words, at node n,
we choose the partition (S?1, S?2) such that:
</bodyText>
<equation confidence="0.50464425">
(S?1,S?2) _ {h(S) − (h(S1) + h(S2))} (2)
argmax
(S1,S2)
S1US2=S
</equation>
<bodyText confidence="0.999987482758621">
where h(S) is the entropy of the probabil-
ity distribution p(tj|si E S), weighted by the
number of samples in the training data of the
source words in S. We only split a node if the
entropy is reduced by more than a threshold
θh. This step is repeated recursively until the
tree cannot be grown anymore.
Weighting the entropy by the source word
counts gives more weight to the context-
dependent source words with a higher number
of samples in the training data, sine the lex-
ical translation probability estimates for fre-
quent words can be trusted better. The ratio-
nale behind the splitting criterion used is that
the split that reduces the entropy of the lexical
translation probability distribution the most
is also the split that best separates the list of
forms of the source word in terms of the target
word translation. For a source word that has
multiple meanings, depending on its context,
the decision tree will tend to implicitly sepa-
rate those meanings using the information in
the lexical translation probabilities.
Although we describe this method as grow-
ing one decision tree for each word, and using
one attribute type at a time, a decision tree
can clearly be constructed for multiple words,
and more than one attribute type can be used
in the same decision tree.
</bodyText>
<subsectionHeader confidence="0.940349">
3.2 Trees for Source Word Clustering
</subsectionHeader>
<bodyText confidence="0.999986951219512">
The source words could be augmented to ex-
plicitly incorporate the word attributes (dia-
critics or other attribute types). The aug-
mented source will be less ambiguous if the
attributes do in fact contain disambiguating
information. This, in principle, helps machine
translation performance. The flip side is that
the resulting increase in vocabulary size in-
creases the translation model sparsity, usually
with a detrimental effect on translation.
To mitigate the effect of the increase in vo-
cabulary, decision trees can be use to cluster
the attribute-augmented source words. More
specifically, a decision tree is grown for each
source word as described in the previous sec-
tion, using a predefined entropy threshold θh.
When the tree cannot be expanded anymore,
its leaf nodes will contain a multi-set parti-
tioning of the list of attribute-dependent forms
of that source word. Each of the clusters is
treated as an equivalence class, and all forms
in that class are mapped to a unique form (e.g.
an arbitrarily chosen member of the cluster).
The mappings are used to map the tokens in
the parallel training data before alignment is
run on the mapped data. The test data is
also mapped consistently. This clustering pro-
cedure will only keep the attribute-dependent
forms of the source words that decrease the un-
certainty in the translation probabilities, and
are thus useful for translation.
The experiments we report on use diacritics
as an attribute type. The various diacritized
forms of a source word are thus used to train
the decision trees. The resulting clusters are
used to map the data into a subset of the vo-
cabulary that is used in translation training
and decoding (see section 4.2 for results). Di-
acritics are obviously specific to Arabic. But
this method can be used with other attribute
types, by first appending the source words with
</bodyText>
<equation confidence="0.68823">
(1)
</equation>
<page confidence="0.817442">
431
</page>
<figure confidence="0.99768725">
sjn4{
sjn4{s
{sijona,sijnil
{sijonal {sijon
</figure>
<figureCaption confidence="0.992997">
Figure 1: Decision tree for source word sjn using
diacritics as an attribute.
</figureCaption>
<bodyText confidence="0.969191615384615">
their context (e.g. attach to each source word
its part-of-speech tag or context), and then
training decision trees and mapping the source
side of the data.
Figure 1 shows an example of a decision
tree for the Arabic word sjn2 using diacritics
as a source attribute. The root contains the
various diacritized forms (sijona `prison AC-
CUSATIVE&apos;, sijoni `prison DATIVE&apos;, sajona
`imprisonment ACCUSATIVE.&apos;, sajoni `im-
prisonment ACCUSATIVE.&apos;, sajana `he im-
prisoned&apos;). The leaf nodes contain the
attribute-dependent clusters.
</bodyText>
<subsectionHeader confidence="0.818566">
3.3 Trees for Lexical Smoothing
</subsectionHeader>
<bodyText confidence="0.999875">
As mentioned in section 2.1, lexical smoothing,
computed from word-to-word translation prob-
abilities, is a useful feature, even in SMT sys-
tems that use sophisticated translation mod-
els. This is likely due to the robustness of
context-free word-to-word translation proba-
bility estimates compared to the probabilities
of more complicated models. In those models,
the rules and probabilities are estimated from
much larger sample spaces.
In our system, the lexical smoothing feature
is computed as follows:
</bodyText>
<equation confidence="0.993303">
)(1−¯�(tj|si)) (3)
</equation>
<bodyText confidence="0.99980925">
where U is the modeling unit specific to the
translation model used. For a phrase-based
system, U is the phrase pair, and for a hierar-
chical system U is the translation rule. S (U)
</bodyText>
<footnote confidence="0.8162455">
2Examples are written using Buckwalter transliter-
ation.
</footnote>
<figureCaption confidence="0.9948395">
Figure 2: Decision tree for source word sjn grown
fully using diacritics.
</figureCaption>
<bodyText confidence="0.999785428571428">
is the set of terminals on the source side of U,
and T (U) is the set of terminals on its tar-
get. The NULL term in the equation above
accounts for unaligned target words, which we
found in our experiments to be beneficial. One
way of interpreting equation (3) is that f (U)
is the probability that for each target word tj
in U, tj is a likely translation of at least one
word si on the source side. The feature value
is then used as a component in the log-linear
model, with a tunable weight.
In this work, we generalize the lexical
smoothing feature to incorporate the source
word attributes. A tree is grown for each
source word as described in section 3.1, but
using an entropy threshold Oh = 0. In other
words, the tree is grown all the way until each
leaf node contains one attribute-dependent
form of the source word. Each node in the
tree contains a cluster of attribute-dependent
forms of the source word, and a corresponding
attribute-dependent lexical translation prob-
ability distribution. The lexical translation
probability models at the root nodes are those
of the regular attribute-independent lexical
translation probabilities. The models at the
leaf nodes are the most fine-grained, since they
are conditioned on only one attribute value.
Figure 2 shows a fully grown decision tree for
the same source word as the example in Figure
1.
The lexical probability distribution at the
leafs are from sparser data than the original
distributions, and are therefore less robust. To
address this, the attribute-dependent lexical
</bodyText>
<equation confidence="0.9909335">
f(U)= ri 1− �
tjET (U) siE{S(U)UNULL}
</equation>
<page confidence="0.992618">
432
</page>
<bodyText confidence="0.9999528">
smoothing feature is estimated by recursively
interpolating the lexical translation probabil-
ities up the tree. The probability distribu-
tion pn at each node n is interpolated with
the probability of its parent node as follows:
</bodyText>
<equation confidence="0.695405">
� pn if n is root,
pn = wnpn + (1 − wn)p:n otherwise
where m is the parent of n
(4)
</equation>
<bodyText confidence="0.998203357142857">
A fraction of the parent probability mass is
thus given to the probability of the child node.
If the probability estimate of an attribute-
dependent form of a source word with a cer-
tain target word t is not reliable, or if the
probability estimate is 0 (because the source
word in this context is not aligned with t),
then the model gracefully backs off by using
the probability estimates from other attribute-
dependent lexical translation probability mod-
els of the source word.
The interpolation weight is a logistic regres-
sion function of the source word count at a
node n:
</bodyText>
<equation confidence="0.998634333333333">
1
wn = (5)
1 + e−α−,3log(count(S.))
</equation>
<bodyText confidence="0.999875411764706">
The weight varies depending on the count
of the attribute-qualified source word in each
node, thus reflecting the confidence in the es-
timates of each node&apos;s distribution. The two
global parameters of the function, a bias α and
a scale Q are tuned to maximize the likelihood
of a set of alignment counts from a heldout
data set of 179K sentences. The tuning is done
using Powell&apos;s method (Brent, 1973).
During decoding, we use the probability dis-
tribution at the leaves to compute the feature
value f(R) for each hierarchical rule R. We
train and decode using the regular, attribute-
independent source. The source word at-
tributes are used in the decoder only to in-
dex the interpolated probability distribution
needed to compute f (R).
</bodyText>
<sectionHeader confidence="0.999602" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.988728">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9987345">
As mentioned before, the experiments we re-
port on use a string-to-dependency-tree hier-
archical translation system based on the model
described in (Shen et al., 2008). Forward and
</bodyText>
<table confidence="0.996464666666667">
Likelihood %
baseline -1.29 -
Diacs. -1.25 +2.98%
dec. trees
POS dec. -1.24 +3.41%
trees
</table>
<tableCaption confidence="0.984103">
Table 1: Normalized likelihood of the test set align-
ments without decision trees, then with decision trees
using diacritics and part-of-speech respectively.
</tableCaption>
<bodyText confidence="0.9999128">
backward context-free lexical smoothing are
used as decoder features in all the experiments.
Other features such as rule probabilities and
dependency tree language model (Shen et al.,
2008) are also used. We use GIZA++ (Och
and Ney, 2003) for word alignments. The de-
coder model parameters are tuned using Mini-
mum Error Rate training (Och, 2003) to max-
imize the IBM BLEU score (Papineni et al.,
2002).
For training the alignments, we use 27M
words from the Sakhr Arabic-English Paral-
lel Corpus (SSUSAC27). The language model
uses 7B words from the English Gigaword and
from data collected from the web. A 3-gram
language model is used during decoding. The
decoder produces an N-best list that is re-
ranked using a 5-gram language model.
We tune and test on two separate data sets
consisting of documents from the following col-
lections: the newswire portion of NIST MT04,
MT05, MT06, and MT08 evaluation sets, the
GALE Phase 1 (P1) and Phase 2 (P2) evalu-
ation sets, and the GALE P2 and P3 develop-
ment sets. The tuning set contains 1994 sen-
tences and the test set contains 3149 sentences.
The average length of sentences is 36 words.
Most of the documents in the two data sets
have 4 reference translations, but some have
only one. The average number of reference
translations per sentence is 3.94 for the tun-
ing set and 3.67 for the test set.
In the next section, we report on measure-
ments of the likelihood of test data, and de-
scribe the translation experiments in detail.
</bodyText>
<sectionHeader confidence="0.619661" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.998215666666667">
In order to assess whether the decision trees
are in fact helpful in decreasing the uncer-
tainty in the lexical translation probabilities
</bodyText>
<page confidence="0.999393">
433
</page>
<figureCaption confidence="0.9962265">
Figure 3: BLEU scores of the clustering experiments
as a function of the entropy threshold on tuning set.
</figureCaption>
<bodyText confidence="0.998899222222222">
on unseen data, we compute the likelihood
of the test data with respect to these prob-
abilities with and without the decision tree
splitting. We align the test set with its ref-
erence using GIZA++, and then obtain the
link count l_count(si, tj) for each alignment
link i = (si,ti) in the set of alignment links I.
We calculate the normalized likelihood of the
alignments:
</bodyText>
<equation confidence="0.988109333333333">
1 �
l_count(si, ti) log p� (ti I si) (6)
III iEI
</equation>
<bodyText confidence="0.9993745">
where p� (ti I si) is the probability for the word
pair (ti, si) in equation (4). If the same in-
stance of source word si is aligned to two tar-
get words ti and tj, then these two links are
counted separately. If a source in the test set
is out-of-vocabulary, or if a word pair (ti, si)
is aligned in the test alignment but not in the
training alignments (and thus has no probabil-
ity estimate), then it is ignored in the calcula-
tion of the log-likelihood.
Table 1 shows the likelihood for the baseline
case, where one lexical translation probability
distribution is used per source word. It also
shows the likelihoods calculated using the lex-
ical distributions in the leaf nodes of the de-
cision trees, when either diacritics or part-of-
speech are used as an attribute type. The table
shows an increase in the likelihood of 2.98% us-
ing diacritics, and 3.41% using part-of-speech.
The translation result tables present MT
scores in two different metrics: Translation
Edit Rate (Snover et al., 2006) and IBM
</bodyText>
<table confidence="0.99771">
TER BLEU
Test
baseline 40.14 52.05
full diacritics 40.31 52.39
+0.17 +0.34
dec. trees, diac (Oh = 50) 39.75 52.60
-0.39 +0.55
</table>
<tableCaption confidence="0.997439">
Table 2: Results of experiments using decision trees
to cluster source words.
</tableCaption>
<bodyText confidence="0.999987575">
BLEU. The reader is reminded that a higher
BLEU score and a lower TER are desired. The
tables also show the difference in scores be-
tween the baseline and each experiment. It is
worth noting that the gains reported are rela-
tive to a strong baseline that uses a state-of-
the-art system with many features, and a fairly
large training corpus.
The decision tree clustering experiment as
described in section 3.2 depends on a global
parameter, namely the threshold in entropy re-
duction Oh. We tune this parameter manually
on a tuning set. Figure 3 shows the BLEU
scores as a function of the threshold value, with
diacritics as an attribute type. The most gain
is obtained for an entropy threshold of 50.
The fully diacritized data has an average of
1.78 diacritized forms per source word. The av-
erage weighted by the number of occurrences is
6.28, which indicates that words with more di-
acritized forms tend to occur more frequently.
After clustering using a value of Oh = 50,
the average number of diacritized forms be-
comes 1.11, and the occurrence weighted av-
erage becomes 3.69. The clustering proce-
dure thus seems to eliminate most diacritized
forms, which likely do not contain helpful dis-
ambiguating information.
Table 2 lists the detailed results of experi-
ments using diacritics. In the first experiment,
we show that using full diacritization results in
a small gain on the BLEU score and no gain on
TER, which is somewhat consistent with the
result obtained by Diab et al. (2007). The next
experiment shows the results of clustering the
diacritized source words using decision trees
for the entropy threshold of 50. The TER loss
of the full diacritics becomes a gain, and the
BLEU gain increases. This confirms our spec-
ulation that the use of fully diacritized data in-
</bodyText>
<equation confidence="0.968578333333333">
� |I|
L = log rjp(ti I si)l_count(si,ti)
i
</equation>
<page confidence="0.997882">
434
</page>
<table confidence="0.998424666666667">
TER BLEU
Test
baseline 40.14 52.05
dec. trees, diacs 39.75 52.55
-0.39 +0.50
dec. trees, POS 40.05 52.40
-0.09 +0.35
dec. trees, diacs, no interpolation 39.98 52.09
-0.16 +0.04
</table>
<tableCaption confidence="0.999937">
Table 3: Results of experiments using the word attribute-dependent lexical smoothing feature.
</tableCaption>
<bodyText confidence="0.999937128205128">
creases the model sparsity, which undoes most
of the benefit obtained from the disambiguat-
ing information that the diacritics contain. Us-
ing the decision trees to cluster the diacritized
source data prunes diacritized forms that do
not decrease the entropy of the lexical trans-
lation probability distributions. It thus finds
a sweet-spot between the negative effect of in-
creasing the vocabulary size and the positive
effect of disambiguation.
In our experiments, using diacritics with
case endings gave consistently better score
than using diacritics with no case endings, de-
spite the fact that they result in a higher vo-
cabulary size. One possible explanation is that
diacritics not only help in lexical disambigua-
tion, but they might also be indirectly help-
ing in phrase reordering, since the diacritics on
the final letter indicate the word&apos;s grammatical
function.
The results from using decision trees to in-
terpolate attribute-dependent lexical smooth-
ing features are summarized in table 3. In
the first experiment, we show the results of
using diacritics to estimate the interpolated
lexical translation probabilities. The results
show a gain of +0.5 BLEU points and 0.39
TER points. The gain is statistically signifi-
cant with a 95% confidence level. Using part-
of-speech as an attribute gives a smaller, but
still statistically significant gain. We also ran
a control experiment, where we used diacritic-
dependent lexical translation probabilities ob-
tained from the decision trees, but did not per-
form the probability interpolation of equation
(4). The gains mostly disappear, especially on
BLEU, showing the importance of the inter-
polation step for the proper estimation of the
lexical smoothing feature.
</bodyText>
<sectionHeader confidence="0.971241" genericHeader="conclusions">
5 Conclusion and Future Directions
</sectionHeader>
<bodyText confidence="0.999978896551724">
We presented in this paper a new method for
incorporating explicit context-informed word
attributes into SMT using binary decision
trees. We reported on experiments on Arabic-
to-English translation using diacritized Ara-
bic and part-of-speech as word attributes, and
showed that the use of these attributes in-
creases the likelihood of source-target word
pairs of unseen data. We proposed two spe-
cific ways in which the results of the decision
tree training process are used in machine trans-
lation, and showed that they result in better
translation results.
For future work, we plan on using multi-
ple source-side attributes at the same time.
Different attributes could have different dis-
ambiguating information, which could pro-
vide more benefit than using any of the at-
tributes alone. We also plan on investigat-
ing the use of multi-word trees; trees for word
clusters can for instance be grown instead
of growing a separate tree for each source
word. Although the experiments presented
in this paper use local word attributes, noth-
ing in principle prevents this method from be-
ing used with long-distance sentence context,
or even with document-level or discourse-level
features. Our future plans include the investi-
gation of using such features as well.
</bodyText>
<sectionHeader confidence="0.964055" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.999865142857143">
This work was supported by DARPA/IPTO
Contract No. HR0011-06-C-0022 under the
GALE program.
The views, opinions, and/or findings con-
tained in this article are those of the author
and should not be interpreted as representing
the official views or policies, either expressed
</bodyText>
<page confidence="0.997738">
435
</page>
<bodyText confidence="0.99744575">
or implied, of the Defense Advanced Research
Projects Agency or the Department of Defense.
A pproved for Public Release, Distribution Un-
limited.
</bodyText>
<sectionHeader confidence="0.980178" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99968271">
S. Ananthakrishnan, S. Narayanan, and S. Ban-
galore. 2005. Automatic diacritization of ara-
bic transcripts for automatic speech recognition.
Kanpur, India.
R. Brent. 1973. Al,gorithms for Minimization
Without Derivatives. Prentice-Hall.
P. Brown, V. Della Pietra, S. Della Pietra, and
R. L. Mercer. 1993. The mathematics of sta-
tistical machine translation: Parameter estima-
tion. Computational, Linguistics, 19:263~311.
J. Brunning, A. de Gispert, and W. Byrne. 2009.
Context-dependent alignment models for statis-
tical machine translation. In NAACL &apos;09: Pro-
ceedings of the 2009 Human Language Technol,-
ogy Conference of the North American Chapter
of the Association for Computational, Linguis-
tics, pages 110~118.
C. Cabezas and P. Resnick. 2005. Using WSD
techniques for lexical selection in statistical ma-
chine translation. In Technical, report, Insti-
tute for Advanced Computer Studies (CS-TR-
4736, LAMP-TR-124, UMIACS-TR-2005-42),
College Park, MD.
M. Carpuat and D. Wu. 2007. Improving statis-
tical machine translation using word sense dis-
ambiguation. In EMNLP-CoNLL-2007: Pro-
ceedings of the 2007 Joint Conference on Em-
pirical, Methods in Natural, Language Processing
and Computational, Natural, Language Learning,
Prague, Czech Republic.
Y. Chan, H. Ng, and D. Chiang. 2007. Word
sense disambiguation improves statistical ma-
chine translation. In Proceedings of the 45th
Annual, Meeting of the Association for Compu-
tational, Linguistics (ACL).
J. Devlin. 2009. Lexical features for statistical
machine translation. Master&apos;s thesis, University
of Maryland, December 2009.
M. Diab, M. Ghoneim, and N. Habash. 2007. Ara-
bic diacritization in the context of statistical ma-
chine translation. In MT Summit XI, pages 143~
149, Copenhagen, Denmark.
R. O. Duda, P. E. Hart, and D. G. Stork. 2000.
Pattern Cl,assification. Wiley-Interscience Pub-
lication.
K. Gimpel and N. A. Smith. 2008. Rich source-
side context for statistical machine translation.
In StatMT &apos;08: Proceedings of the Third Work-
shop on Statistical, Machine Transl,ation, pages
9~17, Columbus, Ohio.
N. Habash and O. Rambow. 2007. Arabic diacriti-
zation through full morphological tagging. In
Proceedings of the 2007 Human Language Tech-
nol,ogy Conference of the North American Chap-
ter of the Association for Computational, Lin-
guistics, pages 53~56, Rochester, New York.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statis-
tical phrase-based translation. In Proceedings of
the 2003 Human Language Technol,ogy Confer-
ence of the North American Chapter of the As-
sociation for Computational, Linguistics, pages
48~54, Edmonton, Canada.
R. Nelken and S. M. Shieber. 2005. Arabic dia-
critization using weighted finite-state transduc-
ers. In Proceedings of the 2005 ACL Workshop
on Computational, Approaches to Semitic Lan-
guages, Ann Arbor, Michigan.
F. J. Och and H. Ney. 2003. A systematic com-
parison of various statistical alignment models.
Computational, Linguistics, 29(1):19~51.
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar,
K. Yamada, A. Fraser, S. Kumar, L. Shen,
D. Smith, K. Eng, V. Jain, Z. Jin, and D. R.
Radev. 2004. A smorgasbord of features for sta-
tistical machine translation. In HLT-NAACL,
pages 161~168.
F. J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings
of the 41st Annual, Meeting of the Association
for Computational, Linguistics (ACL), Sapporo,
Japan.
K. Papineni, S. Roukos, T. Ward, and W. Zhu.
2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of
the 40th Annual, Meeting of the Association for
Computational, Linguistics (ACL), Philadelphia,
PA.
Ruhi Sarikaya, Yonggang Deng, and Yuqing Gao.
2007. Context dependent word modeling for sta-
tistical machine translation using part-of-speech
tags. In Proceedings of INTERSPEECH 2007fs,
Antwerp, Belgium.
L. Shen, J. Xu, and R. Weischedel. 2008. A new
string-to-dependency machine translation algo-
rithm with a target dependency language model.
In Proceedings of the 46th Annual, Meeting of
the Association for Computational, Linguistics
(ACL), Columbus, Ohio.
M. Snover, B. Dorr, R. Schwartz, J. Makhoul, and
L. Micciulla. 2006. A study of translation error
</reference>
<page confidence="0.990337">
436
</page>
<reference confidence="0.998834882352941">
rate with targeted human annotation. In Pro-
ceedings of the 7th Conf. of the Association for
Machine Transl,ation in the Americas (AMTA
2006), pages 223~231, Cambridge, MA.
N. Stroppa, A. van den Bosch, and A Way.
2007. Exploiting source similarity for SMT us-
ing context-informed features. In Proceedings of
the 11th International, Conference on Theoreti-
cal, and Methodol,ogical, Issues in Machine Trans-
l,ation (TMI-07), pages 231~240.
D. Vergyri and K. Kirchhoff. 2004. Automatic
diacritization of arabic for acoustic modeling in
speech recognition. In Semitic &apos;04: Proceedings
of the Workshop on Computational, Approaches
to Arabic Script-based Languages, pages 66~73,
Geneva, Switzerland.
D. Vickrey, L. Biewald, M. Teyssier, and D. Koller.
2005. Word-sense disambiguation for machine
translation. In HLT &apos;05: Proceedings of the
conference on Human Language Technol,ogy and
Empirical, Methods in Natural, Language Pro-
cessing, Vancouser, BC, Canada.
S.J. Young, J.J. Odell, and P.C. Woodland. 1994.
Tree-based state tying for high accuracy acoustic
modelling. In HLT&apos;94: Proceedings of the Work-
shop on Human Language Technol,ogy, pages
307~312.
I. Zitouni, J. S. Sorensen, and Ruhi Sarikaya. 2006.
Maximum entropy based restoration of arabic
diacritics. In Proceedings of the 21st Interna-
tional, Conference on Computational, Linguistics
and the 44th annual, meeting of the Association
for Computational, Linguistics, pages 577~584,
Sydney, Australia.
</reference>
<page confidence="0.998632">
437
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.608279">
<title confidence="0.9954885">Decision Trees for Lexical Smoothing in Statistical Machine Translation</title>
<author confidence="0.735646">Spyros Matsoukas</author>
<author confidence="0.735646">Richard Schwartz</author>
<author confidence="0.735646">John</author>
<note confidence="0.8924655">BBN Technologies, 10 Moulton Street, Cambridge, MA 02138, USA Institute of Technology, 77 Massachusetts Ave., Cambridge, MA 02139, USA</note>
<abstract confidence="0.998946772727273">We present a method for incorporating arbitrary context-informed word attributes into statistical machine translation by clustering attribute-qualified source words, and smoothing their word translation probabilities using binary decision trees. We describe two ways in which the decision trees are used in machine translation: by using the attribute-qualified source word directly, or by using dependent lexical translation probabilities that are obtained from the trees, as a lexical smoothing feature in the decoder model. We present experiments using Arabic-to-English newswire data, and using Arabic diacritics and part-ofspeech as source word attributes, and show that the proposed method improves on a state-of-the-art translation system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Ananthakrishnan</author>
<author>S Narayanan</author>
<author>S Bangalore</author>
</authors>
<title>Automatic diacritization of arabic transcripts for automatic speech recognition.</title>
<date>2005</date>
<journal>Kanpur, India.</journal>
<contexts>
<context position="10418" citStr="Ananthakrishnan et al. (2005)" startWordPosition="1611" endWordPosition="1614">rs can usually guess the correct pronunciation of words in non-diacritized text from the sentence and discourse context. Grammatical case on nouns and adjectives are also marked using diacritics at the end of words. Arabic MT systems use undiacritized text, since most available Arabic data is undiacritized. ✶Such writing systems are sometimes referred to as Abjads (See Daniels, Peter T., et al. eds. The World&apos;s Writing Systems Oxford. (1996), p.4.) Automatic diacritization of Arabic has been done with high accuracy, using various generative and discriminative modeling techniques. For example, Ananthakrishnan et al. (2005) used a generative model that incorporates word level n-grams, sub-word level n-grams and part-of-speech information to perform diacritization. Nelken and Shieber (2005) modeled the generative process of dropping diacritics using weighted transducers, then used Viterbi decoding to find the most likely generator. Zitouni et al. (2006) presented a method based on maximum entropy classifiers, using features like character n-grams, word ngrams, POS and morphological segmentation. Habash and Rambow (2007) determined various morpho-syntactic features of the word using SVM classifiers, then chose the</context>
</contexts>
<marker>Ananthakrishnan, Narayanan, Bangalore, 2005</marker>
<rawString>S. Ananthakrishnan, S. Narayanan, and S. Bangalore. 2005. Automatic diacritization of arabic transcripts for automatic speech recognition. Kanpur, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Brent</author>
</authors>
<title>Al,gorithms for Minimization Without Derivatives.</title>
<date>1973</date>
<publisher>Prentice-Hall.</publisher>
<contexts>
<context position="20906" citStr="Brent, 1973" startWordPosition="3349" endWordPosition="3350"> attributedependent lexical translation probability models of the source word. The interpolation weight is a logistic regression function of the source word count at a node n: 1 wn = (5) 1 + e−α−,3log(count(S.)) The weight varies depending on the count of the attribute-qualified source word in each node, thus reflecting the confidence in the estimates of each node&apos;s distribution. The two global parameters of the function, a bias α and a scale Q are tuned to maximize the likelihood of a set of alignment counts from a heldout data set of 179K sentences. The tuning is done using Powell&apos;s method (Brent, 1973). During decoding, we use the probability distribution at the leaves to compute the feature value f(R) for each hierarchical rule R. We train and decode using the regular, attributeindependent source. The source word attributes are used in the decoder only to index the interpolated probability distribution needed to compute f (R). 4 Experiments 4.1 Experimental Setup As mentioned before, the experiments we report on use a string-to-dependency-tree hierarchical translation system based on the model described in (Shen et al., 2008). Forward and Likelihood % baseline -1.29 - Diacs. -1.25 +2.98% d</context>
</contexts>
<marker>Brent, 1973</marker>
<rawString>R. Brent. 1973. Al,gorithms for Minimization Without Derivatives. Prentice-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>V Della Pietra</author>
<author>S Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational, Linguistics,</journal>
<pages>19--263</pages>
<contexts>
<context position="5901" citStr="Brown et al., 1993" startWordPosition="902" endWordPosition="905">d context. They augmented the target words with POS tags of the word itself and its surrounding words, and used the augmented words in decoding and for language model rescoring. They reported gains on IraqiArabic-to-English translation. Finally, using word-to-word context-free lexical translation probabilities has been shown to improve the performance of machine translation systems, even those using much more sophisticated models. This feature, usually called lexical smoothing, has been used in phrase-based systems (Koehn et al., 2003). Och et al. (2004) also found that including IBM Model 1 (Brown et al., 1993) word probabilities in their log-linear model works better than most other higher-level syntactic features at improving the baseline. The incorporation of context on the source or target side enhances the gain obtained from lexical smoothing. Gimpel and Smith (2008) proposed using source-side lexical features in phrase-based SMT by conditioning the phrase probabilities on those features. They used word context, syntactic features or positional features. The features were added as components into the log-linear decoder model, each with a tunable weight. Devlin (2009) used context lexical featur</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. Brown, V. Della Pietra, S. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational, Linguistics, 19:263~311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Brunning</author>
<author>A de Gispert</author>
<author>W Byrne</author>
</authors>
<title>Context-dependent alignment models for statistical machine translation.</title>
<date>2009</date>
<booktitle>In NAACL &apos;09: Proceedings of the 2009 Human Language Technol,-ogy Conference of the North American Chapter of the Association for Computational, Linguistics,</booktitle>
<pages>110--118</pages>
<marker>Brunning, de Gispert, Byrne, 2009</marker>
<rawString>J. Brunning, A. de Gispert, and W. Byrne. 2009. Context-dependent alignment models for statistical machine translation. In NAACL &apos;09: Proceedings of the 2009 Human Language Technol,-ogy Conference of the North American Chapter of the Association for Computational, Linguistics, pages 110~118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cabezas</author>
<author>P Resnick</author>
</authors>
<title>Using WSD techniques for lexical selection in statistical machine translation.</title>
<date>2005</date>
<booktitle>In Technical, report, Institute for Advanced Computer Studies (CS-TR4736, LAMP-TR-124,</booktitle>
<pages>2005--42</pages>
<location>College Park, MD.</location>
<contexts>
<context position="4580" citStr="Cabezas and Resnick, 2005" startWordPosition="687" endWordPosition="690">rees, and the two methods for using them in machine translation. In section 4 we describe the experimental setup and present experimental results. Finally, section 5 concludes the paper and discusses future directions. 2 Previous Work 2.1 Lexical Disambiguation and Lexical Smoothing Various ways have been proposed to improve the lexical translation choices of SMT systems. These approaches typically incorporate local context information, either directly or indirectly. The use of Word Sense Disambiguation (WSD) has been proposed to enhance machine translation by disambiguating the source words (Cabezas and Resnick, 2005; Carpuat and Wu, 2007; Chan et al., 2007). WSD usually requires that the training data be labeled with senses, which might not be available for many languages. Also, WSD is traditionally formulated as a classification problem, and therefore does not naturally lend itself to be integrated into the generative framework of machine translation. Carpuat and Wu (2007) formulate the SMT lexical disambiguation problem as a WSD task. Instead of learning from word sense corpora, they use the SMT training data, and use local context features to enhance the lexical disambiguation of phrasebased SMT. Sari</context>
</contexts>
<marker>Cabezas, Resnick, 2005</marker>
<rawString>C. Cabezas and P. Resnick. 2005. Using WSD techniques for lexical selection in statistical machine translation. In Technical, report, Institute for Advanced Computer Studies (CS-TR4736, LAMP-TR-124, UMIACS-TR-2005-42), College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Carpuat</author>
<author>D Wu</author>
</authors>
<title>Improving statistical machine translation using word sense disambiguation.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL-2007: Proceedings of the 2007 Joint Conference on Empirical, Methods in Natural, Language Processing and Computational, Natural, Language Learning,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="4602" citStr="Carpuat and Wu, 2007" startWordPosition="691" endWordPosition="694">or using them in machine translation. In section 4 we describe the experimental setup and present experimental results. Finally, section 5 concludes the paper and discusses future directions. 2 Previous Work 2.1 Lexical Disambiguation and Lexical Smoothing Various ways have been proposed to improve the lexical translation choices of SMT systems. These approaches typically incorporate local context information, either directly or indirectly. The use of Word Sense Disambiguation (WSD) has been proposed to enhance machine translation by disambiguating the source words (Cabezas and Resnick, 2005; Carpuat and Wu, 2007; Chan et al., 2007). WSD usually requires that the training data be labeled with senses, which might not be available for many languages. Also, WSD is traditionally formulated as a classification problem, and therefore does not naturally lend itself to be integrated into the generative framework of machine translation. Carpuat and Wu (2007) formulate the SMT lexical disambiguation problem as a WSD task. Instead of learning from word sense corpora, they use the SMT training data, and use local context features to enhance the lexical disambiguation of phrasebased SMT. Sarikaya et al. (2007) inc</context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>M. Carpuat and D. Wu. 2007. Improving statistical machine translation using word sense disambiguation. In EMNLP-CoNLL-2007: Proceedings of the 2007 Joint Conference on Empirical, Methods in Natural, Language Processing and Computational, Natural, Language Learning, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Chan</author>
<author>H Ng</author>
<author>D Chiang</author>
</authors>
<title>Word sense disambiguation improves statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual, Meeting of the Association for Computational, Linguistics (ACL).</booktitle>
<contexts>
<context position="4622" citStr="Chan et al., 2007" startWordPosition="695" endWordPosition="698">ne translation. In section 4 we describe the experimental setup and present experimental results. Finally, section 5 concludes the paper and discusses future directions. 2 Previous Work 2.1 Lexical Disambiguation and Lexical Smoothing Various ways have been proposed to improve the lexical translation choices of SMT systems. These approaches typically incorporate local context information, either directly or indirectly. The use of Word Sense Disambiguation (WSD) has been proposed to enhance machine translation by disambiguating the source words (Cabezas and Resnick, 2005; Carpuat and Wu, 2007; Chan et al., 2007). WSD usually requires that the training data be labeled with senses, which might not be available for many languages. Also, WSD is traditionally formulated as a classification problem, and therefore does not naturally lend itself to be integrated into the generative framework of machine translation. Carpuat and Wu (2007) formulate the SMT lexical disambiguation problem as a WSD task. Instead of learning from word sense corpora, they use the SMT training data, and use local context features to enhance the lexical disambiguation of phrasebased SMT. Sarikaya et al. (2007) incorporate context mor</context>
</contexts>
<marker>Chan, Ng, Chiang, 2007</marker>
<rawString>Y. Chan, H. Ng, and D. Chiang. 2007. Word sense disambiguation improves statistical machine translation. In Proceedings of the 45th Annual, Meeting of the Association for Computational, Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Devlin</author>
</authors>
<title>Lexical features for statistical machine translation. Master&apos;s thesis,</title>
<date>2009</date>
<institution>University of Maryland,</institution>
<contexts>
<context position="1474" citStr="Devlin, 2009" startWordPosition="214" endWordPosition="215">eriments using Arabic-to-English newswire data, and using Arabic diacritics and part-ofspeech as source word attributes, and show that the proposed method improves on a state-of-the-art translation system. 1 Introduction Modern statistical machine translation (SMT) models, such as phrase-based SMT or hierarchical SMT, implicitly incorporate source language context. It has been shown, however, that such systems can still benefit from the explicit addition of lexical, syntactic or other kinds of context-informed word features (Vickrey et al., 2005; Gimpel and Smith, 2008; Brunning et al., 2009; Devlin, 2009). But the benefit obtained from the addition of attribute information is in general countered by the increase in the model complexity, which in turn results in a sparser translation model when estimated from the same corpus of data. The increase in model sparsity usually results in a deterioration of translation quality. In this paper, we present a method for using arbitrary types of source-side context-informed word attributes, using binary decision trees to deal with the sparsity side-effect. The decision trees cluster attribute-dependent source words by reducing the entropy of the lexical t</context>
<context position="6473" citStr="Devlin (2009)" startWordPosition="990" endWordPosition="991">ncluding IBM Model 1 (Brown et al., 1993) word probabilities in their log-linear model works better than most other higher-level syntactic features at improving the baseline. The incorporation of context on the source or target side enhances the gain obtained from lexical smoothing. Gimpel and Smith (2008) proposed using source-side lexical features in phrase-based SMT by conditioning the phrase probabilities on those features. They used word context, syntactic features or positional features. The features were added as components into the log-linear decoder model, each with a tunable weight. Devlin (2009) used context lexical features in a hierarchical SMT system, interpolating lexical counts based on multiple contexts. It also used target-side lexical features. The work in the paper incorporates context information based on the reduction of the translation probability entropy. 2.2 Decision Trees Decision trees have been used extensively in various areas of machine learning, typically as a way to cluster patterns in order to improve classification (Duda et al., 2000). They have, for instance, been long used successfully in speech recognition to cluster contextdependent phoneme model states (Yo</context>
</contexts>
<marker>Devlin, 2009</marker>
<rawString>J. Devlin. 2009. Lexical features for statistical machine translation. Master&apos;s thesis, University of Maryland, December 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Diab</author>
<author>M Ghoneim</author>
<author>N Habash</author>
</authors>
<title>Arabic diacritization in the context of statistical machine translation.</title>
<date>2007</date>
<booktitle>In MT Summit XI,</booktitle>
<pages>143--149</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="2889" citStr="Diab et al., 2007" startWordPosition="432" endWordPosition="435">nslation probability models, and use those probabilities to compute a feature in the decoder log-linear model. The experiments we present in this paper were conducted on the translation of Arabicto-English newswire data using a hierarchical system based on (Shen et al., 2008), and using Arabic diacritics (see section 2.3) and part-ofspeech (POS) as source word attributes. Previous work that attempts to use Arabic diacritics in machine translation runs against the sparsity problem, and appears to lose most of the useful information contained in the diacritics when using partial diacritization (Diab et al., 2007). Using the methods proposed in this paper, we manage to obtain consistent improvements from diacritics against a strong baseline. The methods we propose, though, are not restrictive to Arabic-to-English translation. The same techniques can also be used with other language pairs and arbitrary word attribute types. The attributes we use in the described experiments are local; but long distance features can also be used. In the next section, we review relevant previous work in three areas: Lexical smoothing and lexical disambiguation techniques in machine translation; using decision trees in nat</context>
<context position="11631" citStr="Diab et al., 2007" startWordPosition="1795" endWordPosition="1798">e the corresponding diacritization. The experiments in this paper use the automatic diacritizer by Sakhr Software. The diacritizer determines word diacritics through rule-based morphological and syntactic analysis. It outputs a diacritization for both the internal stem and case ending markers of the word, with an accuracy of 97% for stem diacritization and 91% for full diacritization (i.e., including case endings). There has been work done on using diacritics in Automatic Speech Recognition, e.g. (Vergyri and Kirchhoff, 2004). However, the only previous work on using diacritization for MT is (Diab et al., 2007), which used the diacritization system described in (Habash and Rambow, 2007). It investigated the effect of using full diacritization as well as partial diacritization on MT results. The authors found that using full diacritics deteriorates MT performance. They used partial diacritization schemes, such as diacritizing only passive verbs, keeping the case endings diacritics, or only gemination diacritics. They also saw no gain in most configurations. The authors argued that the deterioration in performance is caused by the increase in the size of the vocabulary, which in turn makes the transla</context>
<context position="26537" citStr="Diab et al. (2007)" startWordPosition="4317" endWordPosition="4320">words with more diacritized forms tend to occur more frequently. After clustering using a value of Oh = 50, the average number of diacritized forms becomes 1.11, and the occurrence weighted average becomes 3.69. The clustering procedure thus seems to eliminate most diacritized forms, which likely do not contain helpful disambiguating information. Table 2 lists the detailed results of experiments using diacritics. In the first experiment, we show that using full diacritization results in a small gain on the BLEU score and no gain on TER, which is somewhat consistent with the result obtained by Diab et al. (2007). The next experiment shows the results of clustering the diacritized source words using decision trees for the entropy threshold of 50. The TER loss of the full diacritics becomes a gain, and the BLEU gain increases. This confirms our speculation that the use of fully diacritized data in� |I| L = log rjp(ti I si)l_count(si,ti) i 434 TER BLEU Test baseline 40.14 52.05 dec. trees, diacs 39.75 52.55 -0.39 +0.50 dec. trees, POS 40.05 52.40 -0.09 +0.35 dec. trees, diacs, no interpolation 39.98 52.09 -0.16 +0.04 Table 3: Results of experiments using the word attribute-dependent lexical smoothing fe</context>
</contexts>
<marker>Diab, Ghoneim, Habash, 2007</marker>
<rawString>M. Diab, M. Ghoneim, and N. Habash. 2007. Arabic diacritization in the context of statistical machine translation. In MT Summit XI, pages 143~ 149, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R O Duda</author>
<author>P E Hart</author>
<author>D G Stork</author>
</authors>
<title>Pattern Cl,assification. Wiley-Interscience Publication.</title>
<date>2000</date>
<contexts>
<context position="6944" citStr="Duda et al., 2000" startWordPosition="1062" endWordPosition="1065">features or positional features. The features were added as components into the log-linear decoder model, each with a tunable weight. Devlin (2009) used context lexical features in a hierarchical SMT system, interpolating lexical counts based on multiple contexts. It also used target-side lexical features. The work in the paper incorporates context information based on the reduction of the translation probability entropy. 2.2 Decision Trees Decision trees have been used extensively in various areas of machine learning, typically as a way to cluster patterns in order to improve classification (Duda et al., 2000). They have, for instance, been long used successfully in speech recognition to cluster contextdependent phoneme model states (Young et al., 1994). Decision trees have also been used in machine translation, although to a lesser extent. In this respect, our work is most similar to (Brunning et al., 2009), where the authors extended word alignment models for IBM Model 1 and Hidden Markov Model (HMM) alignments. They used decision trees to cluster the context-dependent source words. Contexts belonging to the same cluster were grouped together during Expectation Maximization (EM) training, thus pr</context>
</contexts>
<marker>Duda, Hart, Stork, 2000</marker>
<rawString>R. O. Duda, P. E. Hart, and D. G. Stork. 2000. Pattern Cl,assification. Wiley-Interscience Publication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Gimpel</author>
<author>N A Smith</author>
</authors>
<title>Rich sourceside context for statistical machine translation.</title>
<date>2008</date>
<booktitle>In StatMT &apos;08: Proceedings of the Third Workshop on Statistical, Machine Transl,ation,</booktitle>
<pages>9--17</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="1436" citStr="Gimpel and Smith, 2008" startWordPosition="206" endWordPosition="209">ng feature in the decoder model. We present experiments using Arabic-to-English newswire data, and using Arabic diacritics and part-ofspeech as source word attributes, and show that the proposed method improves on a state-of-the-art translation system. 1 Introduction Modern statistical machine translation (SMT) models, such as phrase-based SMT or hierarchical SMT, implicitly incorporate source language context. It has been shown, however, that such systems can still benefit from the explicit addition of lexical, syntactic or other kinds of context-informed word features (Vickrey et al., 2005; Gimpel and Smith, 2008; Brunning et al., 2009; Devlin, 2009). But the benefit obtained from the addition of attribute information is in general countered by the increase in the model complexity, which in turn results in a sparser translation model when estimated from the same corpus of data. The increase in model sparsity usually results in a deterioration of translation quality. In this paper, we present a method for using arbitrary types of source-side context-informed word attributes, using binary decision trees to deal with the sparsity side-effect. The decision trees cluster attribute-dependent source words by</context>
<context position="6167" citStr="Gimpel and Smith (2008)" startWordPosition="944" endWordPosition="947">rd context-free lexical translation probabilities has been shown to improve the performance of machine translation systems, even those using much more sophisticated models. This feature, usually called lexical smoothing, has been used in phrase-based systems (Koehn et al., 2003). Och et al. (2004) also found that including IBM Model 1 (Brown et al., 1993) word probabilities in their log-linear model works better than most other higher-level syntactic features at improving the baseline. The incorporation of context on the source or target side enhances the gain obtained from lexical smoothing. Gimpel and Smith (2008) proposed using source-side lexical features in phrase-based SMT by conditioning the phrase probabilities on those features. They used word context, syntactic features or positional features. The features were added as components into the log-linear decoder model, each with a tunable weight. Devlin (2009) used context lexical features in a hierarchical SMT system, interpolating lexical counts based on multiple contexts. It also used target-side lexical features. The work in the paper incorporates context information based on the reduction of the translation probability entropy. 2.2 Decision Tr</context>
</contexts>
<marker>Gimpel, Smith, 2008</marker>
<rawString>K. Gimpel and N. A. Smith. 2008. Rich sourceside context for statistical machine translation. In StatMT &apos;08: Proceedings of the Third Workshop on Statistical, Machine Transl,ation, pages 9~17, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Habash</author>
<author>O Rambow</author>
</authors>
<title>Arabic diacritization through full morphological tagging.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Human Language Technol,ogy Conference of the North American Chapter of the Association for Computational, Linguistics,</booktitle>
<pages>53--56</pages>
<location>Rochester, New York.</location>
<contexts>
<context position="10923" citStr="Habash and Rambow (2007)" startWordPosition="1686" endWordPosition="1689">h accuracy, using various generative and discriminative modeling techniques. For example, Ananthakrishnan et al. (2005) used a generative model that incorporates word level n-grams, sub-word level n-grams and part-of-speech information to perform diacritization. Nelken and Shieber (2005) modeled the generative process of dropping diacritics using weighted transducers, then used Viterbi decoding to find the most likely generator. Zitouni et al. (2006) presented a method based on maximum entropy classifiers, using features like character n-grams, word ngrams, POS and morphological segmentation. Habash and Rambow (2007) determined various morpho-syntactic features of the word using SVM classifiers, then chose the corresponding diacritization. The experiments in this paper use the automatic diacritizer by Sakhr Software. The diacritizer determines word diacritics through rule-based morphological and syntactic analysis. It outputs a diacritization for both the internal stem and case ending markers of the word, with an accuracy of 97% for stem diacritization and 91% for full diacritization (i.e., including case endings). There has been work done on using diacritics in Automatic Speech Recognition, e.g. (Vergyri</context>
</contexts>
<marker>Habash, Rambow, 2007</marker>
<rawString>N. Habash and O. Rambow. 2007. Arabic diacritization through full morphological tagging. In Proceedings of the 2007 Human Language Technol,ogy Conference of the North American Chapter of the Association for Computational, Linguistics, pages 53~56, Rochester, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Human Language Technol,ogy Conference of the North American Chapter of the Association for Computational, Linguistics,</booktitle>
<pages>48--54</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="5823" citStr="Koehn et al., 2003" startWordPosition="887" endWordPosition="890">porate context more directly by using POS tags on the target side to model word context. They augmented the target words with POS tags of the word itself and its surrounding words, and used the augmented words in decoding and for language model rescoring. They reported gains on IraqiArabic-to-English translation. Finally, using word-to-word context-free lexical translation probabilities has been shown to improve the performance of machine translation systems, even those using much more sophisticated models. This feature, usually called lexical smoothing, has been used in phrase-based systems (Koehn et al., 2003). Och et al. (2004) also found that including IBM Model 1 (Brown et al., 1993) word probabilities in their log-linear model works better than most other higher-level syntactic features at improving the baseline. The incorporation of context on the source or target side enhances the gain obtained from lexical smoothing. Gimpel and Smith (2008) proposed using source-side lexical features in phrase-based SMT by conditioning the phrase probabilities on those features. They used word context, syntactic features or positional features. The features were added as components into the log-linear decode</context>
<context position="7896" citStr="Koehn et al., 2003" startWordPosition="1217" endWordPosition="1220">word alignment models for IBM Model 1 and Hidden Markov Model (HMM) alignments. They used decision trees to cluster the context-dependent source words. Contexts belonging to the same cluster were grouped together during Expectation Maximization (EM) training, thus providing a more robust probability estimate. While Brunning et al. (2009) used the source context clusters for word alignments, we use the attribute-dependent source words directly in decoding. The approach we propose can be readily used with any alignment model. Stroppa et al. (2007) presented a generalization of phrase-based SMT (Koehn et al., 2003) that also takes into account sourceside context information. They conditioned the target phrase probability on the source 429 phrase as well as source phrase context, such as bordering words, or part-of-speech of bordering words. They built a decision tree for each source phrase extracted from the training data. The branching of the tree nodes was based on the different context features, branching on the most class-discriminative features first. Each node is associated with the set of aligned target phrases and corresponding context-conditioned probabilities. The decision tree thus smoothes t</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Human Language Technol,ogy Conference of the North American Chapter of the Association for Computational, Linguistics, pages 48~54, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Nelken</author>
<author>S M Shieber</author>
</authors>
<title>Arabic diacritization using weighted finite-state transducers.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2005 ACL Workshop on Computational, Approaches to Semitic Languages,</booktitle>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="10587" citStr="Nelken and Shieber (2005)" startWordPosition="1634" endWordPosition="1637">rked using diacritics at the end of words. Arabic MT systems use undiacritized text, since most available Arabic data is undiacritized. ✶Such writing systems are sometimes referred to as Abjads (See Daniels, Peter T., et al. eds. The World&apos;s Writing Systems Oxford. (1996), p.4.) Automatic diacritization of Arabic has been done with high accuracy, using various generative and discriminative modeling techniques. For example, Ananthakrishnan et al. (2005) used a generative model that incorporates word level n-grams, sub-word level n-grams and part-of-speech information to perform diacritization. Nelken and Shieber (2005) modeled the generative process of dropping diacritics using weighted transducers, then used Viterbi decoding to find the most likely generator. Zitouni et al. (2006) presented a method based on maximum entropy classifiers, using features like character n-grams, word ngrams, POS and morphological segmentation. Habash and Rambow (2007) determined various morpho-syntactic features of the word using SVM classifiers, then chose the corresponding diacritization. The experiments in this paper use the automatic diacritizer by Sakhr Software. The diacritizer determines word diacritics through rule-bas</context>
</contexts>
<marker>Nelken, Shieber, 2005</marker>
<rawString>R. Nelken and S. M. Shieber. 2005. Arabic diacritization using weighted finite-state transducers. In Proceedings of the 2005 ACL Workshop on Computational, Approaches to Semitic Languages, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational, Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="21939" citStr="Och and Ney, 2003" startWordPosition="3510" endWordPosition="3513">t on use a string-to-dependency-tree hierarchical translation system based on the model described in (Shen et al., 2008). Forward and Likelihood % baseline -1.29 - Diacs. -1.25 +2.98% dec. trees POS dec. -1.24 +3.41% trees Table 1: Normalized likelihood of the test set alignments without decision trees, then with decision trees using diacritics and part-of-speech respectively. backward context-free lexical smoothing are used as decoder features in all the experiments. Other features such as rule probabilities and dependency tree language model (Shen et al., 2008) are also used. We use GIZA++ (Och and Ney, 2003) for word alignments. The decoder model parameters are tuned using Minimum Error Rate training (Och, 2003) to maximize the IBM BLEU score (Papineni et al., 2002). For training the alignments, we use 27M words from the Sakhr Arabic-English Parallel Corpus (SSUSAC27). The language model uses 7B words from the English Gigaword and from data collected from the web. A 3-gram language model is used during decoding. The decoder produces an N-best list that is reranked using a 5-gram language model. We tune and test on two separate data sets consisting of documents from the following collections: the </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational, Linguistics, 29(1):19~51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>D Gildea</author>
<author>S Khudanpur</author>
<author>A Sarkar</author>
<author>K Yamada</author>
<author>A Fraser</author>
<author>S Kumar</author>
<author>L Shen</author>
<author>D Smith</author>
<author>K Eng</author>
<author>V Jain</author>
<author>Z Jin</author>
<author>D R Radev</author>
</authors>
<title>A smorgasbord of features for statistical machine translation. In</title>
<date>2004</date>
<booktitle>HLT-NAACL,</booktitle>
<pages>161--168</pages>
<contexts>
<context position="5842" citStr="Och et al. (2004)" startWordPosition="891" endWordPosition="894">irectly by using POS tags on the target side to model word context. They augmented the target words with POS tags of the word itself and its surrounding words, and used the augmented words in decoding and for language model rescoring. They reported gains on IraqiArabic-to-English translation. Finally, using word-to-word context-free lexical translation probabilities has been shown to improve the performance of machine translation systems, even those using much more sophisticated models. This feature, usually called lexical smoothing, has been used in phrase-based systems (Koehn et al., 2003). Och et al. (2004) also found that including IBM Model 1 (Brown et al., 1993) word probabilities in their log-linear model works better than most other higher-level syntactic features at improving the baseline. The incorporation of context on the source or target side enhances the gain obtained from lexical smoothing. Gimpel and Smith (2008) proposed using source-side lexical features in phrase-based SMT by conditioning the phrase probabilities on those features. They used word context, syntactic features or positional features. The features were added as components into the log-linear decoder model, each with </context>
</contexts>
<marker>Och, Gildea, Khudanpur, Sarkar, Yamada, Fraser, Kumar, Shen, Smith, Eng, Jain, Jin, Radev, 2004</marker>
<rawString>F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng, V. Jain, Z. Jin, and D. R. Radev. 2004. A smorgasbord of features for statistical machine translation. In HLT-NAACL, pages 161~168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual, Meeting of the Association for Computational, Linguistics (ACL),</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="22045" citStr="Och, 2003" startWordPosition="3530" endWordPosition="3531"> 2008). Forward and Likelihood % baseline -1.29 - Diacs. -1.25 +2.98% dec. trees POS dec. -1.24 +3.41% trees Table 1: Normalized likelihood of the test set alignments without decision trees, then with decision trees using diacritics and part-of-speech respectively. backward context-free lexical smoothing are used as decoder features in all the experiments. Other features such as rule probabilities and dependency tree language model (Shen et al., 2008) are also used. We use GIZA++ (Och and Ney, 2003) for word alignments. The decoder model parameters are tuned using Minimum Error Rate training (Och, 2003) to maximize the IBM BLEU score (Papineni et al., 2002). For training the alignments, we use 27M words from the Sakhr Arabic-English Parallel Corpus (SSUSAC27). The language model uses 7B words from the English Gigaword and from data collected from the web. A 3-gram language model is used during decoding. The decoder produces an N-best list that is reranked using a 5-gram language model. We tune and test on two separate data sets consisting of documents from the following collections: the newswire portion of NIST MT04, MT05, MT06, and MT08 evaluation sets, the GALE Phase 1 (P1) and Phase 2 (P2</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. J. Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual, Meeting of the Association for Computational, Linguistics (ACL), Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual, Meeting of the Association for Computational,</booktitle>
<location>Linguistics (ACL), Philadelphia, PA.</location>
<contexts>
<context position="22100" citStr="Papineni et al., 2002" startWordPosition="3539" endWordPosition="3542">1.29 - Diacs. -1.25 +2.98% dec. trees POS dec. -1.24 +3.41% trees Table 1: Normalized likelihood of the test set alignments without decision trees, then with decision trees using diacritics and part-of-speech respectively. backward context-free lexical smoothing are used as decoder features in all the experiments. Other features such as rule probabilities and dependency tree language model (Shen et al., 2008) are also used. We use GIZA++ (Och and Ney, 2003) for word alignments. The decoder model parameters are tuned using Minimum Error Rate training (Och, 2003) to maximize the IBM BLEU score (Papineni et al., 2002). For training the alignments, we use 27M words from the Sakhr Arabic-English Parallel Corpus (SSUSAC27). The language model uses 7B words from the English Gigaword and from data collected from the web. A 3-gram language model is used during decoding. The decoder produces an N-best list that is reranked using a 5-gram language model. We tune and test on two separate data sets consisting of documents from the following collections: the newswire portion of NIST MT04, MT05, MT06, and MT08 evaluation sets, the GALE Phase 1 (P1) and Phase 2 (P2) evaluation sets, and the GALE P2 and P3 development s</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual, Meeting of the Association for Computational, Linguistics (ACL), Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruhi Sarikaya</author>
<author>Yonggang Deng</author>
<author>Yuqing Gao</author>
</authors>
<title>Context dependent word modeling for statistical machine translation using part-of-speech tags.</title>
<date>2007</date>
<booktitle>In Proceedings of INTERSPEECH 2007fs,</booktitle>
<location>Antwerp, Belgium.</location>
<contexts>
<context position="5198" citStr="Sarikaya et al. (2007)" startWordPosition="793" endWordPosition="796">2005; Carpuat and Wu, 2007; Chan et al., 2007). WSD usually requires that the training data be labeled with senses, which might not be available for many languages. Also, WSD is traditionally formulated as a classification problem, and therefore does not naturally lend itself to be integrated into the generative framework of machine translation. Carpuat and Wu (2007) formulate the SMT lexical disambiguation problem as a WSD task. Instead of learning from word sense corpora, they use the SMT training data, and use local context features to enhance the lexical disambiguation of phrasebased SMT. Sarikaya et al. (2007) incorporate context more directly by using POS tags on the target side to model word context. They augmented the target words with POS tags of the word itself and its surrounding words, and used the augmented words in decoding and for language model rescoring. They reported gains on IraqiArabic-to-English translation. Finally, using word-to-word context-free lexical translation probabilities has been shown to improve the performance of machine translation systems, even those using much more sophisticated models. This feature, usually called lexical smoothing, has been used in phrase-based sys</context>
</contexts>
<marker>Sarikaya, Deng, Gao, 2007</marker>
<rawString>Ruhi Sarikaya, Yonggang Deng, and Yuqing Gao. 2007. Context dependent word modeling for statistical machine translation using part-of-speech tags. In Proceedings of INTERSPEECH 2007fs, Antwerp, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Shen</author>
<author>J Xu</author>
<author>R Weischedel</author>
</authors>
<title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual, Meeting of the Association for Computational, Linguistics (ACL),</booktitle>
<location>Columbus, Ohio.</location>
<contexts>
<context position="2547" citStr="Shen et al., 2008" startWordPosition="377" endWordPosition="380">trees to deal with the sparsity side-effect. The decision trees cluster attribute-dependent source words by reducing the entropy of the lexical translation probabilities. We also present another method where, instead of clustering the attribute-dependent source words, the decision trees are used to interpolate attributedependent lexical translation probability models, and use those probabilities to compute a feature in the decoder log-linear model. The experiments we present in this paper were conducted on the translation of Arabicto-English newswire data using a hierarchical system based on (Shen et al., 2008), and using Arabic diacritics (see section 2.3) and part-ofspeech (POS) as source word attributes. Previous work that attempts to use Arabic diacritics in machine translation runs against the sparsity problem, and appears to lose most of the useful information contained in the diacritics when using partial diacritization (Diab et al., 2007). Using the methods proposed in this paper, we manage to obtain consistent improvements from diacritics against a strong baseline. The methods we propose, though, are not restrictive to Arabic-to-English translation. The same techniques can also be used with</context>
<context position="21441" citStr="Shen et al., 2008" startWordPosition="3433" endWordPosition="3436"> data set of 179K sentences. The tuning is done using Powell&apos;s method (Brent, 1973). During decoding, we use the probability distribution at the leaves to compute the feature value f(R) for each hierarchical rule R. We train and decode using the regular, attributeindependent source. The source word attributes are used in the decoder only to index the interpolated probability distribution needed to compute f (R). 4 Experiments 4.1 Experimental Setup As mentioned before, the experiments we report on use a string-to-dependency-tree hierarchical translation system based on the model described in (Shen et al., 2008). Forward and Likelihood % baseline -1.29 - Diacs. -1.25 +2.98% dec. trees POS dec. -1.24 +3.41% trees Table 1: Normalized likelihood of the test set alignments without decision trees, then with decision trees using diacritics and part-of-speech respectively. backward context-free lexical smoothing are used as decoder features in all the experiments. Other features such as rule probabilities and dependency tree language model (Shen et al., 2008) are also used. We use GIZA++ (Och and Ney, 2003) for word alignments. The decoder model parameters are tuned using Minimum Error Rate training (Och, 2</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>L. Shen, J. Xu, and R. Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proceedings of the 46th Annual, Meeting of the Association for Computational, Linguistics (ACL), Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>B Dorr</author>
<author>R Schwartz</author>
<author>J Makhoul</author>
<author>L Micciulla</author>
</authors>
<title>A study of translation error rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th Conf. of the Association for Machine Transl,ation in the Americas (AMTA</booktitle>
<pages>223--231</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="24843" citStr="Snover et al., 2006" startWordPosition="4025" endWordPosition="4028">bability estimate), then it is ignored in the calculation of the log-likelihood. Table 1 shows the likelihood for the baseline case, where one lexical translation probability distribution is used per source word. It also shows the likelihoods calculated using the lexical distributions in the leaf nodes of the decision trees, when either diacritics or part-ofspeech are used as an attribute type. The table shows an increase in the likelihood of 2.98% using diacritics, and 3.41% using part-of-speech. The translation result tables present MT scores in two different metrics: Translation Edit Rate (Snover et al., 2006) and IBM TER BLEU Test baseline 40.14 52.05 full diacritics 40.31 52.39 +0.17 +0.34 dec. trees, diac (Oh = 50) 39.75 52.60 -0.39 +0.55 Table 2: Results of experiments using decision trees to cluster source words. BLEU. The reader is reminded that a higher BLEU score and a lower TER are desired. The tables also show the difference in scores between the baseline and each experiment. It is worth noting that the gains reported are relative to a strong baseline that uses a state-ofthe-art system with many features, and a fairly large training corpus. The decision tree clustering experiment as descr</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Makhoul, Micciulla, 2006</marker>
<rawString>M. Snover, B. Dorr, R. Schwartz, J. Makhoul, and L. Micciulla. 2006. A study of translation error rate with targeted human annotation. In Proceedings of the 7th Conf. of the Association for Machine Transl,ation in the Americas (AMTA 2006), pages 223~231, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Stroppa</author>
<author>A van den Bosch</author>
<author>A Way</author>
</authors>
<title>Exploiting source similarity for SMT using context-informed features.</title>
<date>2007</date>
<booktitle>In Proceedings of the 11th International, Conference on Theoretical, and Methodol,ogical, Issues in Machine Transl,ation (TMI-07),</booktitle>
<pages>231--240</pages>
<marker>Stroppa, van den Bosch, Way, 2007</marker>
<rawString>N. Stroppa, A. van den Bosch, and A Way. 2007. Exploiting source similarity for SMT using context-informed features. In Proceedings of the 11th International, Conference on Theoretical, and Methodol,ogical, Issues in Machine Transl,ation (TMI-07), pages 231~240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Vergyri</author>
<author>K Kirchhoff</author>
</authors>
<title>Automatic diacritization of arabic for acoustic modeling in speech recognition.</title>
<date>2004</date>
<booktitle>In Semitic &apos;04: Proceedings of the Workshop on Computational, Approaches to Arabic Script-based Languages,</booktitle>
<pages>66--73</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="11544" citStr="Vergyri and Kirchhoff, 2004" startWordPosition="1780" endWordPosition="1783"> (2007) determined various morpho-syntactic features of the word using SVM classifiers, then chose the corresponding diacritization. The experiments in this paper use the automatic diacritizer by Sakhr Software. The diacritizer determines word diacritics through rule-based morphological and syntactic analysis. It outputs a diacritization for both the internal stem and case ending markers of the word, with an accuracy of 97% for stem diacritization and 91% for full diacritization (i.e., including case endings). There has been work done on using diacritics in Automatic Speech Recognition, e.g. (Vergyri and Kirchhoff, 2004). However, the only previous work on using diacritization for MT is (Diab et al., 2007), which used the diacritization system described in (Habash and Rambow, 2007). It investigated the effect of using full diacritization as well as partial diacritization on MT results. The authors found that using full diacritics deteriorates MT performance. They used partial diacritization schemes, such as diacritizing only passive verbs, keeping the case endings diacritics, or only gemination diacritics. They also saw no gain in most configurations. The authors argued that the deterioration in performance i</context>
</contexts>
<marker>Vergyri, Kirchhoff, 2004</marker>
<rawString>D. Vergyri and K. Kirchhoff. 2004. Automatic diacritization of arabic for acoustic modeling in speech recognition. In Semitic &apos;04: Proceedings of the Workshop on Computational, Approaches to Arabic Script-based Languages, pages 66~73, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Vickrey</author>
<author>L Biewald</author>
<author>M Teyssier</author>
<author>D Koller</author>
</authors>
<title>Word-sense disambiguation for machine translation.</title>
<date>2005</date>
<booktitle>In HLT &apos;05: Proceedings of the conference on Human Language Technol,ogy and Empirical, Methods in Natural, Language Processing,</booktitle>
<location>Vancouser, BC,</location>
<contexts>
<context position="1412" citStr="Vickrey et al., 2005" startWordPosition="201" endWordPosition="205">, as a lexical smoothing feature in the decoder model. We present experiments using Arabic-to-English newswire data, and using Arabic diacritics and part-ofspeech as source word attributes, and show that the proposed method improves on a state-of-the-art translation system. 1 Introduction Modern statistical machine translation (SMT) models, such as phrase-based SMT or hierarchical SMT, implicitly incorporate source language context. It has been shown, however, that such systems can still benefit from the explicit addition of lexical, syntactic or other kinds of context-informed word features (Vickrey et al., 2005; Gimpel and Smith, 2008; Brunning et al., 2009; Devlin, 2009). But the benefit obtained from the addition of attribute information is in general countered by the increase in the model complexity, which in turn results in a sparser translation model when estimated from the same corpus of data. The increase in model sparsity usually results in a deterioration of translation quality. In this paper, we present a method for using arbitrary types of source-side context-informed word attributes, using binary decision trees to deal with the sparsity side-effect. The decision trees cluster attribute-d</context>
</contexts>
<marker>Vickrey, Biewald, Teyssier, Koller, 2005</marker>
<rawString>D. Vickrey, L. Biewald, M. Teyssier, and D. Koller. 2005. Word-sense disambiguation for machine translation. In HLT &apos;05: Proceedings of the conference on Human Language Technol,ogy and Empirical, Methods in Natural, Language Processing, Vancouser, BC, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S J Young</author>
<author>J J Odell</author>
<author>P C Woodland</author>
</authors>
<title>Tree-based state tying for high accuracy acoustic modelling.</title>
<date>1994</date>
<booktitle>In HLT&apos;94: Proceedings of the Workshop on Human Language Technol,ogy,</booktitle>
<pages>307--312</pages>
<contexts>
<context position="7090" citStr="Young et al., 1994" startWordPosition="1085" endWordPosition="1088">9) used context lexical features in a hierarchical SMT system, interpolating lexical counts based on multiple contexts. It also used target-side lexical features. The work in the paper incorporates context information based on the reduction of the translation probability entropy. 2.2 Decision Trees Decision trees have been used extensively in various areas of machine learning, typically as a way to cluster patterns in order to improve classification (Duda et al., 2000). They have, for instance, been long used successfully in speech recognition to cluster contextdependent phoneme model states (Young et al., 1994). Decision trees have also been used in machine translation, although to a lesser extent. In this respect, our work is most similar to (Brunning et al., 2009), where the authors extended word alignment models for IBM Model 1 and Hidden Markov Model (HMM) alignments. They used decision trees to cluster the context-dependent source words. Contexts belonging to the same cluster were grouped together during Expectation Maximization (EM) training, thus providing a more robust probability estimate. While Brunning et al. (2009) used the source context clusters for word alignments, we use the attribut</context>
</contexts>
<marker>Young, Odell, Woodland, 1994</marker>
<rawString>S.J. Young, J.J. Odell, and P.C. Woodland. 1994. Tree-based state tying for high accuracy acoustic modelling. In HLT&apos;94: Proceedings of the Workshop on Human Language Technol,ogy, pages 307~312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Zitouni</author>
<author>J S Sorensen</author>
<author>Ruhi Sarikaya</author>
</authors>
<title>Maximum entropy based restoration of arabic diacritics.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International, Conference on Computational, Linguistics and the 44th annual, meeting of the Association for Computational, Linguistics,</booktitle>
<pages>577--584</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="10753" citStr="Zitouni et al. (2006)" startWordPosition="1661" endWordPosition="1664">s referred to as Abjads (See Daniels, Peter T., et al. eds. The World&apos;s Writing Systems Oxford. (1996), p.4.) Automatic diacritization of Arabic has been done with high accuracy, using various generative and discriminative modeling techniques. For example, Ananthakrishnan et al. (2005) used a generative model that incorporates word level n-grams, sub-word level n-grams and part-of-speech information to perform diacritization. Nelken and Shieber (2005) modeled the generative process of dropping diacritics using weighted transducers, then used Viterbi decoding to find the most likely generator. Zitouni et al. (2006) presented a method based on maximum entropy classifiers, using features like character n-grams, word ngrams, POS and morphological segmentation. Habash and Rambow (2007) determined various morpho-syntactic features of the word using SVM classifiers, then chose the corresponding diacritization. The experiments in this paper use the automatic diacritizer by Sakhr Software. The diacritizer determines word diacritics through rule-based morphological and syntactic analysis. It outputs a diacritization for both the internal stem and case ending markers of the word, with an accuracy of 97% for stem </context>
</contexts>
<marker>Zitouni, Sorensen, Sarikaya, 2006</marker>
<rawString>I. Zitouni, J. S. Sorensen, and Ruhi Sarikaya. 2006. Maximum entropy based restoration of arabic diacritics. In Proceedings of the 21st International, Conference on Computational, Linguistics and the 44th annual, meeting of the Association for Computational, Linguistics, pages 577~584, Sydney, Australia.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>