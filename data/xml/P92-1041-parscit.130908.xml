<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015893">
<sectionHeader confidence="0.699299" genericHeader="method">
INCREMENTAL DEPENDENCY PARSING
</sectionHeader>
<title confidence="0.529398333333333">
Vincenzo Lombardo
Dipartimento di Informatica - Universita&apos; di Torino
C.so Svizzera 185 - 10149 Torino - Italy
</title>
<email confidence="0.991271">
e-mail: vincenzo@di.unito.it
</email>
<sectionHeader confidence="0.997244" genericHeader="method">
Abstract
</sectionHeader>
<bodyText confidence="0.999792666666667">
The paper introduces a dependency-based grammar and
the associated parser and focusses on the problem of
determinism in parsing and recovery from errors.
First, it is shown how dependency-based parsing can
be afforded, by taking into account the suggestions
coming from other approaches, and the preference
criteria for parsing are briefly addressed. Second, the
issues of the interconnection between the syntactic
analysis and the semantic interpretation in
incremental processing are discussed and the adoption
of a TMS for the recovery of the processing errors is
suggested.
</bodyText>
<sectionHeader confidence="0.910315" genericHeader="method">
THE BASIC PARSING ALGORITHM
</sectionHeader>
<bodyText confidence="0.99516563888889">
The parser has been devised for a system that works
on the Italian language. The structure that results
from the parsing process is a dependency tree, that
exhibits syntactic and semantic information.
The dependency structure: The structure
combines the traditional view of dependency syntax
with the feature terms of the unification based
formalisms (Shieber 86): single attributes (like
number or tense) appear inside the nodes of the tree,
while complex attributes (like grammatical relations)
are realized as relations between nodes. The choice of
a dependency structure, which is very suitable for free
word order languages (Sgall et al. 86), reflects the
intuitive idea of a language with few constraints on
the order of legal constructions. Actually, the
flexibility of a partially configurational language like
Italian (that can be considered at an intermediate level
between the totally configurational languages like
English and the totally inflected free-ordered Slavonic
languages) can be accounted for with a relaxation of
the strong constraints posed by a constituency
grammar (Stock 1989) or by constraining to a certain
level a dependency grammar. Cases of topicalization,
like
un dolce di frutta ha ordinato il maestro
a cake with fruits has ordered the teacher
and in general all the five permutations of the &amp;quot;basic&amp;quot;
(i.e. more likely) SVO structure of the sentence are
so common in Italian, that it seems much more
economical to express the syntactic knowledge in
terms of dependency relations.
Every node in the structure is associated with a
word in the sentence, in such a way that the relation
between two nodes at any level is of a head&amp;modifier
type. The whole sentence has a head, namely the
verb, and its roles (the subj is included) are its
modifiers. Every modifier in turn has a head (a noun,
which can be a proper, common or pro-noun, for
participants not marked by a preposition, a
preposition, or a verb, in case of subordinate
sentences not preceded by a conjunction) and further
modifiers.
Hence the dependency tree gives an immediate
representation of the thematic structure of the
sentence, thus being very suitable for the semantic
interpretation. Such a structure also allows the
application of the rules, based on grammatical
relations, that govern complex syntactic phenomena,
as revealed by the extensive work on Relational
Grammar.
The dependency grammar is expressed declaratively
via two tables, that represent the relations of
immediate dominance and linear order for pairs of
categories. The constraints on the order between a
head and one of its modifiers and between two
modifiers of the same head are reflected by the nodes
in the dependency structure. The formation of the
complex structure that is associated with the nodes is
accomplished by means of unification: the basic
terms are originated by the lexicon and associated
with the nodes. There exist principles that govern the
propagation of the features in the dependency tree
expressed as analogous conventions to GPSG ones.
The incremental parser: In the system, the
semantic, as well as the contextual and the anaphoric
binding analysis, is interleaved with the syntactic
parsing. The analysis is incremental, in the sense that
it is carried out in a piecemeal strategy, by taking
care of partial results too.
In order to accomplish the incremental parsing and
to build a dependency representation of the sentence,
the linguistic knowledge of the two tables is
</bodyText>
<page confidence="0.984014">
291
</page>
<bodyText confidence="0.999625666666667">
compiled into more suitable data structures, called
diamonds. Diamonds represent a redundant version of
the linguistic knowledge of the tables: their graphical
representation (see the figure) gives an immediate idea
of how to employ them in an incremental parsing
with a dependency grammar.
</bodyText>
<figure confidence="0.926716470588235">
PREP
VERB
ead.tense---+
/J, VERB) &amp;
VERB
AD
at (DET, NOUN,
NOUN
cat (RELPRON) &amp;
....,*.rtense=+
1
ERB
..at (PREP)
17**•&amp;PREP
.c..a4.4(ADJ)
1
AD J
</figure>
<bodyText confidence="0.999939377777778">
The center of the diamond is instantiated as a node of
the category indicated during the course of the
analysis. The lower half of the diamond represents the
categories that can be seen as modifiers of the center
category. In particular, the categories on the left will
precede the head, while the categories on the right
will follow it (the number on the edges totally order
the modifiers on the same side of the head). The
upper half of the diamond represents the possible
heads of the center: the categories on the right will
follow it, while the categories on the left, that
precede it, indicate the type of node that will become
active when the current center has no more modifiers
in the sentence.
The (incremental) parsing algorithm is
straightforward: if the current node is of category X,
the correspondent diamond (which has X as the
center) individuates the possible alternatives in the
parsing. The next input word can be one of its
possible modifiers that follow it (right-low branch),
its head (right-up branch), another modifier of its
head, i.e. a sister (right-up branch and the following
left-down one in the diamond activated immediately
next), or a modifier of its head&apos;s head, an aunt (left-up
branch).
The edges are augmented with conditions on the
input word (cat is a predicate which tests its category
as belonging to a set of categories allowed to be the
left-corner of the subtree headed by a node of the
category that stands at the end of the edge).
Constraints on features are tested on the node itself or
stored for a subsequent verification.
Which edge to follow in the currently active
diamond is almost always a matter of a non
deterministic choice. Non determinism can be handled
via the interaction of many knowledge sources that
use the dependency tree as a shared information
structure, that represents the actual state of the
parsing. Such a structure does not contain only
syntactic, but also semantic information. For
example, every node associated with a non functional
word points to a concept in a terminological
knowledge base and the thematic structure of the verb
is explicitly represented by the edges of the
dependency tree.
</bodyText>
<sectionHeader confidence="0.867528" genericHeader="method">
PARSING PREFERENCES
</sectionHeader>
<bodyText confidence="0.951132951219512">
Many preference strategies have been proposed in the
literature for guiding parsers (Hobbs and Bear (1990)
present a review). There are some preferences of
syntactic (i.e. structural) nature, like the Right
Association and the Minimal Attachment, that were
among the first to be devised. Semantic preferences,
like the assignment of thematic roles to the elements
in the sentencel can contradict the expectations of the
syntactic preferences (Schubert 1984). Contextual
information (Crain, Steedman 1985) has also been
demonstrated to affect the parsing of sentences in a
series of psycholinguistic experiments. Lexical
preferencing (Stock 1989) (van der Linden 1991) is
particularly useful for the treatment of idiomatic
expressions.
Parsing preferences are integrated in the framework
described above, by making the syntactic parser
interact with condition-action rules, that implement
such preferences, at each step on the diamond
structure. This technique can be classified under the
weak integration strategy (Crain, Steedman 1985) at
the word level. The rules for the resolution of
ambiguities that belong to the various knowledge
sources analyze the state of the parsing on the
dependency structure and take into account the current
input word. For example, in the two sentences
a) Giorgio le diede con riluttanza una
ingente somma di denaro
Giorgio (to) her gave with reluctance a big amount of
money
b) Giorgio le diede con riluttanza a Pamela
Giorgio them gave with reluctance to Pamela
the pronoun &amp;quot;le&amp;quot; can be a plural accusative or a
singular dative case. In an incremental parser, when
we arrive to &amp;quot;le&amp;quot; we are faced with an ambiguity that
can be solved in a point which is arbitrarily ahead
(impossibility of using Marcus&apos; (1980) bounded
lAs we have noted in the beginning, this is not an easy
task to accomplish, since flexible languages like Italian
feature a hardly predictable behavior in ordering: such
assignments must sometimes be revised (see below).
</bodyText>
<figure confidence="0.998565727272727">
0
•gf. cat (ADJ,
i
NOUN)
OUN
DET
.b.6
cat
DET
cat (1117.
AD J
</figure>
<page confidence="0.995156">
292
</page>
<bodyText confidence="0.9998809375">
lookahead), when we find which grammatical relation
is needed to complete the subcategorintion frame of
the verb. Contextual information can help in solving
such an ambiguity, by binding the pronoun to a
referent, which can be singular or plural. Of course
there could be more than one possible referent for the
pronoun in the example above: in such a case there
exist a preference choice based on the meaning of the
verb and its selectional restrictions, and, in case of
further ambiguity, a default choice among the
possible referents. This choice must be stored as a
backtracking point (in JTMS style) or as being an
assumption of a context (in ATMS style), since it
can reveal to be wrong in the subsequent analysis.
The revision of the interpretation can be
accomplished via a reason maintenance system.
</bodyText>
<sectionHeader confidence="0.999292" genericHeader="method">
INTEGRATION WITH A REASON
MAINTENANCE SYSTEM
</sectionHeader>
<bodyText confidence="0.9999555">
Zemik and Brown (1988) have described a possible
integration of default reasoning in natural language
processing. Their use of a JTMS has been criticized
because of the impossibility to evaluate the best way
in presence of multiple contexts, that are available at
a certain point of the parsing process. This is the
reason why more recent works have focussed on
ATMS techniques (Charniak, Goldman 1988) and
their relations to chart parsing (Wiren 1990). ATMS
allows to continue the processing, by reactivating
interpretations, which have been previously discarded.
Currently, the integration with a reason
maintenance system (which can possibly be more
specialized for this particular task) is under study. The
dependency structure contains the short term
knowledge about the sentence at hand, with a
&amp;quot;dependency&amp;quot; (in the TMS terminology) net that
keeps the information on what relations have been
inferred from what choices. Once that new elements
contradict some previous conclusions, the dependency
net allows to individuate the choice points that are
meaningful for the current situation and to relabel,
according to the IN and OUT separation, the asserted
facts. In the example a) if we have disambiguated the
pronoun &amp;quot;le&amp;quot; as an object, such an interpretation
must be revised when we find the actual object (&amp;quot;a
big amount of money&amp;quot;). One of the reasons for
adopting truth maintenance techniques is that all the
facts that must be withdrawn and the starting of a
new analysis (in JTMS style) or to make relevant a
new context in place of an old one (in ATMS) must
take into account that partial analyses, not related to
the changes at hand (&amp;quot;with reluctance&amp;quot; in the
example), must be left unchanged. The specific
substructure A, affected by the value chosen for the
element B, and the element B are connected via a
(direct or indirect) link in the &amp;quot;dependency&amp;quot; net. A
change of value for B is propagated through the net
toward all the linked substructures and, particularly,
to A, which is to be revised. In the example a), once
detected that &amp;quot;le&amp;quot; is an indirect object, and then that
its referent must be female and singular, a new search
in the focus is attempted according to this new
setting. Hence, the revision process operates on both
the syntactic structure, with changes of category
and/or features values for the nodes involved (gender
and number for &amp;quot;le&amp;quot;) and of attachment points for
whole substructures, and the semantic representation
(from direct to indirect object relation), which has
been previously built.
</bodyText>
<sectionHeader confidence="0.994971" genericHeader="acknowledgments">
ACKNOWLEDGEMENTS
</sectionHeader>
<bodyText confidence="0.922974">
I thank prof. Leonardo Lesmo for his active and
precious support.
</bodyText>
<sectionHeader confidence="0.981338" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.999671484848485">
Charniak, E., Goldman, R. (1988). A Logic for
Semantic Interpretation. In Proceedings of the 26th
ACL (87-94).
Crain, S., Steedman, M. (1985). On not being led up
the Garden Path: The Use of Context by the
psychological Syntax Processor. In D. Dowty, L.
Karttunen and A. Zwicky (eds), Natural Language
Parsing. Psychological, Computational, and
Theoretical Perspectives, Cambridge University
Press, Cambridge, England (320-358).
Hobbs, J., Bear, J. (1990). Two Principles of Parse
Preference. In COL1NG 90(162-167).
van der Linden, E., J. (1991). Incremental Processing
and Hierarchical Lexicon. To appear.
Marcus, M. (1980). A Theory of Syntactic
Recognition for Natural Language. MIT Press,
Cambridge, Massachussets.
Schubert, L. (1984). On parsing preferences. In
COLING 84 (247-250).
Sgall, P., Haijcova, E. and Panevova, J. (1986). The
Meaning of the Sentence in its Semantic and
Pragmatic Aspects. D. Reidel Publishing Company.
Shieber, S., M. (1986). An Introduction to
Unification-Based Approach to Grammar. CSLI
Lecture Notes 4, CSLI, Stanford.
Stock, 0. (1989). Parsing with flexibility, dynamic
strategies and idioms in mind. In Computational
Linguistics 15 (1-19).
Wiren, M. (1990). Incremental Parsing and Reason
Maintenance. In COLING 90(287-292).
Zemik, U., Brown, A. (1988). Default Reasoning in
Natural Language Processing. In COL1NG 88 (801-
805).
</reference>
<page confidence="0.998966">
293
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.064945">
<title confidence="0.990175">INCREMENTAL DEPENDENCY PARSING</title>
<author confidence="0.997509">Vincenzo Lombardo</author>
<affiliation confidence="0.998822">Dipartimento di Informatica - Universita&apos; di Torino</affiliation>
<address confidence="0.994424">C.so Svizzera 185 - 10149 Torino - Italy</address>
<email confidence="0.999756">e-mail:vincenzo@di.unito.it</email>
<abstract confidence="0.998470472727272">The paper introduces a dependency-based grammar and the associated parser and focusses on the problem of determinism in parsing and recovery from errors. First, it is shown how dependency-based parsing can be afforded, by taking into account the suggestions coming from other approaches, and the preference criteria for parsing are briefly addressed. Second, the issues of the interconnection between the syntactic analysis and the semantic interpretation in incremental processing are discussed and the adoption of a TMS for the recovery of the processing errors is suggested. THE BASIC PARSING ALGORITHM The parser has been devised for a system that works on the Italian language. The structure that results from the parsing process is a dependency tree, that exhibits syntactic and semantic information. dependency structure: structure combines the traditional view of dependency syntax with the feature terms of the unification based formalisms (Shieber 86): single attributes (like number or tense) appear inside the nodes of the tree, while complex attributes (like grammatical relations) are realized as relations between nodes. The choice of a dependency structure, which is very suitable for free word order languages (Sgall et al. 86), reflects the intuitive idea of a language with few constraints on the order of legal constructions. Actually, the flexibility of a partially configurational language like Italian (that can be considered at an intermediate level between the totally configurational languages like English and the totally inflected free-ordered Slavonic languages) can be accounted for with a relaxation of the strong constraints posed by a constituency grammar (Stock 1989) or by constraining to a certain level a dependency grammar. Cases of topicalization, like un dolce di frutta ha ordinato il maestro a cake with fruits has ordered the teacher and in general all the five permutations of the &amp;quot;basic&amp;quot; (i.e. more likely) SVO structure of the sentence are so common in Italian, that it seems much more economical to express the syntactic knowledge in terms of dependency relations. Every node in the structure is associated with a word in the sentence, in such a way that the relation between two nodes at any level is of a head&amp;modifier type. The whole sentence has a head, namely the verb, and its roles (the subj is included) are its modifiers. Every modifier in turn has a head (a noun, which can be a proper, common or pro-noun, for participants not marked by a preposition, a preposition, or a verb, in case of subordinate sentences not preceded by a conjunction) and further modifiers. Hence the dependency tree gives an immediate representation of the thematic structure of the sentence, thus being very suitable for the semantic interpretation. Such a structure also allows the application of the rules, based on grammatical relations, that govern complex syntactic phenomena, as revealed by the extensive work on Relational Grammar. The dependency grammar is expressed declaratively via two tables, that represent the relations of immediate dominance and linear order for pairs of categories. The constraints on the order between a head and one of its modifiers and between two modifiers of the same head are reflected by the nodes in the dependency structure. The formation of the complex structure that is associated with the nodes is accomplished by means of unification: the basic terms are originated by the lexicon and associated with the nodes. There exist principles that govern the propagation of the features in the dependency tree expressed as analogous conventions to GPSG ones. incremental parser: the system, the semantic, as well as the contextual and the anaphoric binding analysis, is interleaved with the syntactic parsing. The analysis is incremental, in the sense that it is carried out in a piecemeal strategy, by taking care of partial results too. In order to accomplish the incremental parsing and to build a dependency representation of the sentence, the linguistic knowledge of the two tables is 291 compiled into more suitable data structures, called diamonds. Diamonds represent a redundant version of the linguistic knowledge of the tables: their graphical representation (see the figure) gives an immediate idea of how to employ them in an incremental parsing with a dependency grammar. PREP VERB ead.tense---+ VERB) &amp; VERB AD at (DET, NOUN, NOUN cat (RELPRON) &amp; 1 ERB ..at (PREP) 1 The center of the diamond is instantiated as a node of the category indicated during the course of the analysis. The lower half of the diamond represents the categories that can be seen as modifiers of the center category. In particular, the categories on the left will precede the head, while the categories on the right will follow it (the number on the edges totally order the modifiers on the same side of the head). The upper half of the diamond represents the possible heads of the center: the categories on the right will follow it, while the categories on the left, that precede it, indicate the type of node that will become active when the current center has no more modifiers in the sentence. The (incremental) parsing algorithm is straightforward: if the current node is of category X, the correspondent diamond (which has X as the center) individuates the possible alternatives in the parsing. The next input word can be one of its possible modifiers that follow it (right-low branch), its head (right-up branch), another modifier of its head, i.e. a sister (right-up branch and the following left-down one in the diamond activated immediately next), or a modifier of its head&apos;s head, an aunt (left-up branch). The edges are augmented with conditions on the word is predicate which tests its category as belonging to a set of categories allowed to be the left-corner of the subtree headed by a node of the category that stands at the end of the edge). Constraints on features are tested on the node itself or stored for a subsequent verification. Which edge to follow in the currently active diamond is almost always a matter of a non deterministic choice. Non determinism can be handled via the interaction of many knowledge sources that use the dependency tree as a shared information structure, that represents the actual state of the parsing. Such a structure does not contain only syntactic, but also semantic information. For example, every node associated with a non functional word points to a concept in a terminological knowledge base and the thematic structure of the verb is explicitly represented by the edges of the dependency tree. PARSING PREFERENCES Many preference strategies have been proposed in the literature for guiding parsers (Hobbs and Bear (1990) present a review). There are some preferences of syntactic (i.e. structural) nature, like the Right Association and the Minimal Attachment, that were among the first to be devised. Semantic preferences, like the assignment of thematic roles to the elements in the sentencel can contradict the expectations of the syntactic preferences (Schubert 1984). Contextual information (Crain, Steedman 1985) has also been demonstrated to affect the parsing of sentences in a series of psycholinguistic experiments. Lexical preferencing (Stock 1989) (van der Linden 1991) is particularly useful for the treatment of idiomatic expressions. Parsing preferences are integrated in the framework described above, by making the syntactic parser interact with condition-action rules, that implement such preferences, at each step on the diamond structure. This technique can be classified under the weak integration strategy (Crain, Steedman 1985) at the word level. The rules for the resolution of ambiguities that belong to the various knowledge sources analyze the state of the parsing on the dependency structure and take into account the current input word. For example, in the two sentences a) Giorgio le diede con riluttanza una ingente somma di denaro Giorgio (to) her gave with reluctance a big amount of money b) Giorgio le diede con riluttanza a Pamela Giorgio them gave with reluctance to Pamela the pronoun &amp;quot;le&amp;quot; can be a plural accusative or a singular dative case. In an incremental parser, when we arrive to &amp;quot;le&amp;quot; we are faced with an ambiguity that can be solved in a point which is arbitrarily ahead (impossibility of using Marcus&apos; (1980) bounded lAs we have noted in the beginning, this is not an easy task to accomplish, since flexible languages like Italian feature a hardly predictable behavior in ordering: such assignments must sometimes be revised (see below). 0 •cat (ADJ, i NOUN) OUN DET cat DET cat (1117. AD J 292 lookahead), when we find which grammatical relation is needed to complete the subcategorintion frame of the verb. Contextual information can help in solving such an ambiguity, by binding the pronoun to a referent, which can be singular or plural. Of course there could be more than one possible referent for the pronoun in the example above: in such a case there exist a preference choice based on the meaning of the verb and its selectional restrictions, and, in case of further ambiguity, a default choice among the possible referents. This choice must be stored as a backtracking point (in JTMS style) or as being an assumption of a context (in ATMS style), since it can reveal to be wrong in the subsequent analysis. The revision of the interpretation can be accomplished via a reason maintenance system. INTEGRATION WITH A REASON MAINTENANCE SYSTEM Zemik and Brown (1988) have described a possible integration of default reasoning in natural language processing. Their use of a JTMS has been criticized because of the impossibility to evaluate the best way in presence of multiple contexts, that are available at a certain point of the parsing process. This is the reason why more recent works have focussed on ATMS techniques (Charniak, Goldman 1988) and their relations to chart parsing (Wiren 1990). ATMS allows to continue the processing, by reactivating interpretations, which have been previously discarded. Currently, the integration with a reason maintenance system (which can possibly be more specialized for this particular task) is under study. The dependency structure contains the short term knowledge about the sentence at hand, with a &amp;quot;dependency&amp;quot; (in the TMS terminology) net that keeps the information on what relations have been inferred from what choices. Once that new elements contradict some previous conclusions, the dependency net allows to individuate the choice points that are meaningful for the current situation and to relabel, according to the IN and OUT separation, the asserted In the example we have disambiguated the pronoun &amp;quot;le&amp;quot; as an object, such an interpretation must be revised when we find the actual object (&amp;quot;a big amount of money&amp;quot;). One of the reasons for adopting truth maintenance techniques is that all the facts that must be withdrawn and the starting of a new analysis (in JTMS style) or to make relevant a new context in place of an old one (in ATMS) must take into account that partial analyses, not related to the changes at hand (&amp;quot;with reluctance&amp;quot; in the example), must be left unchanged. The specific substructure A, affected by the value chosen for the element B, and the element B are connected via a (direct or indirect) link in the &amp;quot;dependency&amp;quot; net. A change of value for B is propagated through the net toward all the linked substructures and, particularly, to A, which is to be revised. In the example a), once detected that &amp;quot;le&amp;quot; is an indirect object, and then that its referent must be female and singular, a new search in the focus is attempted according to this new setting. Hence, the revision process operates on both the syntactic structure, with changes of category and/or features values for the nodes involved (gender and number for &amp;quot;le&amp;quot;) and of attachment points for whole substructures, and the semantic representation (from direct to indirect object relation), which has been previously built. ACKNOWLEDGEMENTS I thank prof. Leonardo Lesmo for his active and precious support.</abstract>
<note confidence="0.664471909090909">REFERENCES Charniak, E., Goldman, R. (1988). A Logic for Interpretation. In of the 26th Crain, S., Steedman, M. (1985). On not being led up the Garden Path: The Use of Context by the psychological Syntax Processor. In D. Dowty, L. and A. Zwicky (eds), Language Parsing. Psychological, Computational, and Perspectives, University Press, Cambridge, England (320-358). Hobbs, J., Bear, J. (1990). Two Principles of Parse In van der Linden, E., J. (1991). Incremental Processing and Hierarchical Lexicon. To appear. M. (1980). A of Syntactic for Natural Language. Press, Cambridge, Massachussets. Schubert, L. (1984). On parsing preferences. In 84 P., Haijcova, E. and Panevova, J. (1986). Meaning of the Sentence in its Semantic and Aspects. Reidel Publishing Company.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>R Goldman</author>
</authors>
<title>A Logic for Semantic Interpretation.</title>
<date>1988</date>
<booktitle>In Proceedings of the 26th ACL</booktitle>
<pages>87--94</pages>
<marker>Charniak, Goldman, 1988</marker>
<rawString>Charniak, E., Goldman, R. (1988). A Logic for Semantic Interpretation. In Proceedings of the 26th ACL (87-94).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Crain</author>
<author>M Steedman</author>
</authors>
<title>On not being led up the Garden Path: The Use of Context by the psychological Syntax Processor. In</title>
<date>1985</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, England</location>
<marker>Crain, Steedman, 1985</marker>
<rawString>Crain, S., Steedman, M. (1985). On not being led up the Garden Path: The Use of Context by the psychological Syntax Processor. In D. Dowty, L. Karttunen and A. Zwicky (eds), Natural Language Parsing. Psychological, Computational, and Theoretical Perspectives, Cambridge University Press, Cambridge, England (320-358).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hobbs</author>
<author>J Bear</author>
</authors>
<title>Two Principles of Parse Preference.</title>
<date>1990</date>
<booktitle>In COL1NG</booktitle>
<pages>90--162</pages>
<contexts>
<context position="6997" citStr="Hobbs and Bear (1990)" startWordPosition="1116" endWordPosition="1119">ice. Non determinism can be handled via the interaction of many knowledge sources that use the dependency tree as a shared information structure, that represents the actual state of the parsing. Such a structure does not contain only syntactic, but also semantic information. For example, every node associated with a non functional word points to a concept in a terminological knowledge base and the thematic structure of the verb is explicitly represented by the edges of the dependency tree. PARSING PREFERENCES Many preference strategies have been proposed in the literature for guiding parsers (Hobbs and Bear (1990) present a review). There are some preferences of syntactic (i.e. structural) nature, like the Right Association and the Minimal Attachment, that were among the first to be devised. Semantic preferences, like the assignment of thematic roles to the elements in the sentencel can contradict the expectations of the syntactic preferences (Schubert 1984). Contextual information (Crain, Steedman 1985) has also been demonstrated to affect the parsing of sentences in a series of psycholinguistic experiments. Lexical preferencing (Stock 1989) (van der Linden 1991) is particularly useful for the treatme</context>
</contexts>
<marker>Hobbs, Bear, 1990</marker>
<rawString>Hobbs, J., Bear, J. (1990). Two Principles of Parse Preference. In COL1NG 90(162-167).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E van der Linden</author>
<author>J</author>
</authors>
<title>Incremental Processing and Hierarchical Lexicon.</title>
<date>1991</date>
<note>To appear.</note>
<marker>van der Linden, J, 1991</marker>
<rawString>van der Linden, E., J. (1991). Incremental Processing and Hierarchical Lexicon. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
</authors>
<title>A Theory of Syntactic Recognition for Natural Language.</title>
<date>1980</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachussets.</location>
<marker>Marcus, 1980</marker>
<rawString>Marcus, M. (1980). A Theory of Syntactic Recognition for Natural Language. MIT Press, Cambridge, Massachussets.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Schubert</author>
</authors>
<title>On parsing preferences.</title>
<date>1984</date>
<journal>In COLING</journal>
<volume>84</volume>
<pages>247--250</pages>
<contexts>
<context position="7348" citStr="Schubert 1984" startWordPosition="1170" endWordPosition="1171">in a terminological knowledge base and the thematic structure of the verb is explicitly represented by the edges of the dependency tree. PARSING PREFERENCES Many preference strategies have been proposed in the literature for guiding parsers (Hobbs and Bear (1990) present a review). There are some preferences of syntactic (i.e. structural) nature, like the Right Association and the Minimal Attachment, that were among the first to be devised. Semantic preferences, like the assignment of thematic roles to the elements in the sentencel can contradict the expectations of the syntactic preferences (Schubert 1984). Contextual information (Crain, Steedman 1985) has also been demonstrated to affect the parsing of sentences in a series of psycholinguistic experiments. Lexical preferencing (Stock 1989) (van der Linden 1991) is particularly useful for the treatment of idiomatic expressions. Parsing preferences are integrated in the framework described above, by making the syntactic parser interact with condition-action rules, that implement such preferences, at each step on the diamond structure. This technique can be classified under the weak integration strategy (Crain, Steedman 1985) at the word level. T</context>
</contexts>
<marker>Schubert, 1984</marker>
<rawString>Schubert, L. (1984). On parsing preferences. In COLING 84 (247-250).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Sgall</author>
<author>E Haijcova</author>
<author>J Panevova</author>
</authors>
<title>The Meaning of the Sentence in its Semantic and Pragmatic Aspects.</title>
<date>1986</date>
<journal>D. Reidel Publishing Company. Shieber, S., M.</journal>
<booktitle>CSLI Lecture Notes 4, CSLI,</booktitle>
<location>Stanford.</location>
<marker>Sgall, Haijcova, Panevova, 1986</marker>
<rawString>Sgall, P., Haijcova, E. and Panevova, J. (1986). The Meaning of the Sentence in its Semantic and Pragmatic Aspects. D. Reidel Publishing Company. Shieber, S., M. (1986). An Introduction to Unification-Based Approach to Grammar. CSLI Lecture Notes 4, CSLI, Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stock</author>
</authors>
<title>Parsing with flexibility, dynamic strategies and idioms in mind.</title>
<date>1989</date>
<journal>In Computational Linguistics</journal>
<volume>15</volume>
<pages>1--19</pages>
<contexts>
<context position="1889" citStr="Stock 1989" startWordPosition="276" endWordPosition="277">ons) are realized as relations between nodes. The choice of a dependency structure, which is very suitable for free word order languages (Sgall et al. 86), reflects the intuitive idea of a language with few constraints on the order of legal constructions. Actually, the flexibility of a partially configurational language like Italian (that can be considered at an intermediate level between the totally configurational languages like English and the totally inflected free-ordered Slavonic languages) can be accounted for with a relaxation of the strong constraints posed by a constituency grammar (Stock 1989) or by constraining to a certain level a dependency grammar. Cases of topicalization, like un dolce di frutta ha ordinato il maestro a cake with fruits has ordered the teacher and in general all the five permutations of the &amp;quot;basic&amp;quot; (i.e. more likely) SVO structure of the sentence are so common in Italian, that it seems much more economical to express the syntactic knowledge in terms of dependency relations. Every node in the structure is associated with a word in the sentence, in such a way that the relation between two nodes at any level is of a head&amp;modifier type. The whole sentence has a he</context>
<context position="7536" citStr="Stock 1989" startWordPosition="1195" endWordPosition="1196">en proposed in the literature for guiding parsers (Hobbs and Bear (1990) present a review). There are some preferences of syntactic (i.e. structural) nature, like the Right Association and the Minimal Attachment, that were among the first to be devised. Semantic preferences, like the assignment of thematic roles to the elements in the sentencel can contradict the expectations of the syntactic preferences (Schubert 1984). Contextual information (Crain, Steedman 1985) has also been demonstrated to affect the parsing of sentences in a series of psycholinguistic experiments. Lexical preferencing (Stock 1989) (van der Linden 1991) is particularly useful for the treatment of idiomatic expressions. Parsing preferences are integrated in the framework described above, by making the syntactic parser interact with condition-action rules, that implement such preferences, at each step on the diamond structure. This technique can be classified under the weak integration strategy (Crain, Steedman 1985) at the word level. The rules for the resolution of ambiguities that belong to the various knowledge sources analyze the state of the parsing on the dependency structure and take into account the current input</context>
</contexts>
<marker>Stock, 1989</marker>
<rawString>Stock, 0. (1989). Parsing with flexibility, dynamic strategies and idioms in mind. In Computational Linguistics 15 (1-19).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wiren</author>
</authors>
<title>Incremental Parsing and Reason Maintenance.</title>
<date>1990</date>
<booktitle>In COLING</booktitle>
<pages>90--287</pages>
<contexts>
<context position="10236" citStr="Wiren 1990" startWordPosition="1640" endWordPosition="1641"> the subsequent analysis. The revision of the interpretation can be accomplished via a reason maintenance system. INTEGRATION WITH A REASON MAINTENANCE SYSTEM Zemik and Brown (1988) have described a possible integration of default reasoning in natural language processing. Their use of a JTMS has been criticized because of the impossibility to evaluate the best way in presence of multiple contexts, that are available at a certain point of the parsing process. This is the reason why more recent works have focussed on ATMS techniques (Charniak, Goldman 1988) and their relations to chart parsing (Wiren 1990). ATMS allows to continue the processing, by reactivating interpretations, which have been previously discarded. Currently, the integration with a reason maintenance system (which can possibly be more specialized for this particular task) is under study. The dependency structure contains the short term knowledge about the sentence at hand, with a &amp;quot;dependency&amp;quot; (in the TMS terminology) net that keeps the information on what relations have been inferred from what choices. Once that new elements contradict some previous conclusions, the dependency net allows to individuate the choice points that a</context>
</contexts>
<marker>Wiren, 1990</marker>
<rawString>Wiren, M. (1990). Incremental Parsing and Reason Maintenance. In COLING 90(287-292).</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Zemik</author>
<author>A Brown</author>
</authors>
<title>Default Reasoning in Natural Language Processing.</title>
<date>1988</date>
<booktitle>In COL1NG</booktitle>
<volume>88</volume>
<pages>801--805</pages>
<contexts>
<context position="9806" citStr="Zemik and Brown (1988)" startWordPosition="1569" endWordPosition="1572">se there could be more than one possible referent for the pronoun in the example above: in such a case there exist a preference choice based on the meaning of the verb and its selectional restrictions, and, in case of further ambiguity, a default choice among the possible referents. This choice must be stored as a backtracking point (in JTMS style) or as being an assumption of a context (in ATMS style), since it can reveal to be wrong in the subsequent analysis. The revision of the interpretation can be accomplished via a reason maintenance system. INTEGRATION WITH A REASON MAINTENANCE SYSTEM Zemik and Brown (1988) have described a possible integration of default reasoning in natural language processing. Their use of a JTMS has been criticized because of the impossibility to evaluate the best way in presence of multiple contexts, that are available at a certain point of the parsing process. This is the reason why more recent works have focussed on ATMS techniques (Charniak, Goldman 1988) and their relations to chart parsing (Wiren 1990). ATMS allows to continue the processing, by reactivating interpretations, which have been previously discarded. Currently, the integration with a reason maintenance syst</context>
</contexts>
<marker>Zemik, Brown, 1988</marker>
<rawString>Zemik, U., Brown, A. (1988). Default Reasoning in Natural Language Processing. In COL1NG 88 (801-805).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>