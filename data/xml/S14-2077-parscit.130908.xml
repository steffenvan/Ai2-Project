<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006490">
<title confidence="0.9954535">
NRC-Canada-2014: Recent Improvements in
the Sentiment Analysis of Tweets
</title>
<author confidence="0.997647">
Xiaodan Zhu, Svetlana Kiritchenko, and Saif M. Mohammad
</author>
<affiliation confidence="0.815865">
National Research Council Canada
Ottawa, Ontario, Canada K1A 0R6
</affiliation>
<email confidence="0.926774">
{xiaodan.zhu,svetlana.kiritchenko,saif.mohammad}@nrc-cnrc.gc.ca
</email>
<sectionHeader confidence="0.996432" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999209">
This paper describes state-of-the-art statis-
tical systems for automatic sentiment anal-
ysis of tweets. In a Semeval-2014 shared
task (Task 9), our submissions obtained
highest scores in the term-level sentiment
classification subtask on both the 2013 and
2014 tweets test sets. In the message-level
sentiment classification task, our submis-
sions obtained highest scores on the Live-
Journal blog posts test set, sarcastic tweets
test set, and the 2013 SMS test set. These
systems build on our SemEval-2013 senti-
ment analysis systems (Mohammad et al.,
2013) which ranked first in both the term-
and message-level subtasks in 2013. Key
improvements over the 2013 systems are
in the handling of negation. We create
separate tweet-specific sentiment lexicons
for terms in affirmative contexts and in
negated contexts.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999555">
Automatically detecting sentiment of tweets (and
other microblog posts) has attracted extensive
interest from both the academia and industry.
The Conference on Semantic Evaluation Exercises
(SemEval) organizes a shared task on the senti-
ment analysis of tweets with two subtasks. In the
message-level task, the participating systems are
to identify whether a tweet as a whole expresses
positive, negative, or neutral sentiment. In the
term-level task, the objective is to determine the
sentiment of a marked target term (a single word
or a multi-word expression) within the tweet. Our
submissions stood first in both subtasks in 2013.
This paper describes improvements over that sys-
</bodyText>
<table confidence="0.942666">
Evaluation Set Term-level Task Message-level Task
Twt14 1 4
Twt13 1 2
Sarc14 3 1
LvJn14 2 1
SMS13 2 1
</table>
<tableCaption confidence="0.977773">
Table 1: Overall rank of NRC-Canada sentiment
</tableCaption>
<bodyText confidence="0.984923333333333">
analysis models in Semeval-2014 Task 9 under the
constrained condition. The rows are five evalua-
tion datasets and the columns are the two subtasks.
tem and the subsequent submissions to the 2014
shared task (Rosenthal et al., 2014).
The training data for the SemEval-2014 shared
task is same as that of SemEval-2013 (about
10,000 tweets). The 2014 test set has five sub-
categories: a tweet set provided newly in 2014
(Twt14), the tweet set used for testing in the 2013
shared task (Twt13), a set of tweets that are sarcas-
tic (Sarc14), a set of sentences from the blogging
website LiveJournal (LvJn14), and the set of SMS
messages used for testing in the 2013 shared task
(SMS13). Instances from these categories were in-
terspersed in the provided test set. The partici-
pants were not told about the source of the indi-
vidual messages. The objective was to determine
how well a system trained on tweets generalizes to
texts from other domains.
Our submissions to SemEval-2014 Task 9,
ranked first in five out of the ten subtask–dataset
combinations. In the other evaluation sets as well,
our submissions performed competitively. The
results are summarized in Table 1. As we will
show, automatically generated tweet-specific lexi-
cons were especially helpful in all subtask–dataset
combinations. The results also show that even
though our models are trained only on tweets, they
generalize well to data from other domains.
</bodyText>
<page confidence="0.990798">
443
</page>
<note confidence="0.730797">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 443–447,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.999839470588235">
Our systems are based on supervised SVMs and
a number of surface-form, semantic, and senti-
ment features. The major improvement in our
2014 system over the 2013 system is in the way it
handles negation. Morante and Sporleder (2012)
define negation to be “a grammatical category that
allows the changing of the truth value of a propo-
sition”. Negation is often expressed through the
use of negative signals or negators, words such as
isnt and never, and it can significantly affect the
sentiment of its scope. We create separate tweet-
specific sentiment lexicons for terms in affirmative
contexts and in negated contexts. That is, we au-
tomatically determine the average sentiment of a
term when occurring in an affirmative context, and
separately the average sentiment of a term when
occurring in a negated context.
</bodyText>
<sectionHeader confidence="0.977704" genericHeader="method">
2 Our Systems
</sectionHeader>
<bodyText confidence="0.999792833333333">
Our SemEval-2014 systems are based on our
SemEval-2013 systems (Mohammad et al., 2013).
For completeness, we briefly revisit our previ-
ous approach, which uses support vector machine
(SVM) as the classification algorithm and lever-
ages the following features.
Lexicon features These features are generated by
using three manually constructed sentiment lexi-
cons and two automatically constructed lexicons.
The manually constructed lexicons include the
NRC Emotion Lexicon (Mohammad and Turney,
2010; Mohammad and Yang, 2011), the MPQA
Lexicon (Wilson et al., 2005), and the Bing Liu
Lexicon (Hu and Liu, 2004). The two automati-
cally constructed lexicons, the Hashtag Sentiment
Lexicon and the Sentiment140 Lexicon, were cre-
ated specifically for tweets (Mohammad et al.,
2013).
The sentiment score of each term (e.g., a word
or bigram) in the automatically constructed lexi-
cons is computed by measuring the PMI (point-
wise mutual information) between the term and
the positive or negative category of tweets using
the formula:
</bodyText>
<equation confidence="0.788177">
SenScore (w) = PMI(w, pos) − PMI(w, neg)
(1)
</equation>
<bodyText confidence="0.999970232142857">
where w is a term in the lexicons. PMI(w, pos)
is the PMI score between w and the positive class,
and PMI(w, neg) is the PMI score between w
and the negative class. Therefore, a positive Sen-
Score (w) suggests a stronger association of word
w with positive sentiment and vice versa. The
magnitude indicates the strength of association.
Note that the sentiment class of the tweets used
to construct the lexicons was automatically iden-
tified either from hashtags or from emoticons as
described in (Mohammad et al., 2013).
With these lexicons available, the following fea-
tures were extracted for a text span. Here a text
span can be a target term, its context, or an en-
tire tweet, depending on the task. The lexicon
features include: (1) the number of sentiment to-
kens in a text span; sentiment tokens are word
tokens whose sentiment scores are not zero in a
lexicon; (2) the total sentiment score of the text
span: Ew∈textSpan SenScore (w); (3) the maxi-
mal score: maxw∈textSpanSenScore (w); (4) the
total positive and negative sentiment scores of the
text span; (5) the sentiment score of the last token
in the text span. Note that all these features are
generated, when applicable, by using each of the
sentiment lexicons mentioned above.
Ngrams We employed two types of ngram fea-
tures: word ngrams and character ngrams. The
former reflect the presence or absence of contigu-
ous or non-contiguous sequences of words, and
the latter are sequences of prefix/suffix characters
in each word. These features are same as in our
last year’s submission.
Negation The number of negated contexts. Our
definition of a negated context follows Pang et al.
(2002), which will be described in more details be-
low in Section 2.1.
POS The number of occurrences of each part-
of-speech tag. We tokenized and part-of-speech
tagged the tweets with the Carnegie Mellon Uni-
versity (CMU) Twitter NLP tool (Gimpel et al.,
2011).
Cluster features The CMU POS-tagging tool pro-
vides the token clusters produced with the Brown
clustering algorithm from 56 million English-
language tweets. These 1,000 clusters serve as an
alternative representation of tweet content, reduc-
ing the sparsity of the token space.
Encodings The encoding features are derived
from hashtags, punctuation marks, emoticons,
elongated words, and uppercased words.
For the term-level task, all the above features
are extracted for target terms and their context,
where a context is a window of words surround-
ing a target term. For the message-level task, the
features are extracted from the whole tweet.
</bodyText>
<page confidence="0.995826">
444
</page>
<bodyText confidence="0.999717">
In the term-level task, we used the LIB-
SVM (Chang and Lin, 2011) tool with the follow-
ing parameters: -t 0 -b 1 -m 1000. The total num-
ber of features is about 115,000. In the message-
level task, we used an in-house implementation of
SVM with a linear kernel. The parameter C was
set to 0.005. The total number of features was
about 1.5 million.
</bodyText>
<subsectionHeader confidence="0.9878755">
2.1 Improving Lexicons and Negation
Models
</subsectionHeader>
<bodyText confidence="0.999504833333333">
An important advantage of our SemEval-2013
systems comes from the use of the two high-
coverage tweet-specific sentiment lexicons. In
the SemEval-2014 submissions, we improve these
lexicons by incorporating negation modeling into
the lexicon generation process.
</bodyText>
<subsectionHeader confidence="0.604495">
2.1.1 Improving Sentiment Lexicons
</subsectionHeader>
<bodyText confidence="0.9997095625">
A word in a negated context has a different eval-
uative nature than the same word in an affirma-
tive (non-negated) context. We have proposed a
lexicon-based approach (Kiritchenko et al., 2014)
to determining the sentiment of words in these two
situations by automatically creating separate senti-
ment lexicons for the affirmative and negated con-
texts. In this way, we do not need to employ any
explicit assumptions to model negation.
To achieve this, a tweet corpus is split into two
parts: Affirmative Context Corpus and Negated
Context Corpus. Following the work of Pang et al.
(2002), we define a negated context as a segment
of a tweet that starts with a negation word (e.g., no,
shouldn’t) and ends with one of the punctuation
marks: ‘,’, ‘.’, ‘:’, ‘;’, ‘!’, ‘?’. The list of negation
words was adopted from Christopher Potts’ senti-
ment tutorial.1 Thus, part of a tweet that is marked
as negated is included into the negated context cor-
pus while the rest of the tweet becomes part of the
affirmative context corpus. The sentiment label
for the tweet is kept unchanged in both corpora.
Then, we generate an affirmative context lexicon
from the affirmative context corpus and a negated
context lexicon from the negated context corpus
using the technique described in (Kiritchenko et
al., 2014).
Furthermore, we refined the method of con-
structing the negated context lexicons by split-
ting a negated context into two parts: the imme-
diate context consisting of a single token that di-
rectly follows a negation word, and the distant
</bodyText>
<footnote confidence="0.898599">
1http://sentiment.christopherpotts.net/lingstruc.html
</footnote>
<bodyText confidence="0.9999375">
context consisting of the rest of the tokens in the
negated context. This has two benefits. Intu-
itively, negation affects words directly following
the negation words more strongly than more dis-
tant words. Second, immediate-context scores are
less noisy. Our simple negation scope identifica-
tion algorithm can at times fail and include parts
of a tweet that are not actually negated (e.g., if a
punctuation mark is missing). Overall, a sentiment
word can have up to three scores, one for affirma-
tive context, one for immediate negated context,
and one for distant negated context.
We reconstructed the Hashtag Sentiment Lexi-
con and the Sentiment140 Lexicon with this ap-
proach and used them in our SemEval-2014 sys-
tems.
</bodyText>
<subsectionHeader confidence="0.532135">
2.1.2 Discriminating Negation Words
</subsectionHeader>
<bodyText confidence="0.999734333333333">
Different negation words, e.g., never and didn’t,
can have different effects on sentiment (Zhu et al.,
2014; Taboada et al., 2011). In our SemEval-2014
submission, we discriminate negation words in the
term-level models. For example, the word accept-
able appearing in a sentence this is never accept-
able is marked as acceptable beNever, while in
the sentence this is not acceptable, it is marked
as acceptable beNot. In this way, different nega-
tors (e.g., be not and be never) are treated differ-
ently. Note that we do not differentiate the tense
and person of auxiliaries in order to reduce sparse-
ness (e.g., was not and am not are treated in the
same way). This new representation is used to ex-
tract ngrams and lexicon-based features.
</bodyText>
<sectionHeader confidence="0.99997" genericHeader="evaluation">
3 Results
</sectionHeader>
<bodyText confidence="0.999913352941177">
Overall performance The evaluation metric used
in the competition is the macro-averaged F-
measure calculated over the positive and negative
categories. Table 2 presents the overall perfor-
mance of our models. NRC13 and NRC14 are
the systems we submitted to SemEval-2013 and
SemEval-2014, respectively. The integers in the
brackets are our official ranks in SemEval-2014
under the constrained condition.
In the term-level task, our submission ranked
first on the two Tweet datasets among 14 teams.
The results show that we achieved significant im-
provements over our last year’s submission: the F-
score improves from 85.19 to 86.63 on the Twt14
data and from 89.10 to 90.14 on the Twt13 data.
More specifically, on the Twt14 data, the approach
described in Section 2.1.1 improved our F-score
</bodyText>
<page confidence="0.998049">
445
</page>
<table confidence="0.999330571428572">
Term-level Message-level
NRC13 NRC14 NRC13 NRC14
Twt14 85.19 86.63(1) 68.88 69.85(4)
Twt13 89.10 90.14(1) 69.02 70.75(2)
Sarc14 78.16 77.13(3) 47.64 58.16(1)
LvJn14 84.96 85.49(2) 74.01 74.84(1)
SMS13 88.34 88.03(2) 68.34 70.28(1)
</table>
<tableCaption confidence="0.941578">
Table 2: Overall performance of the NRC-Canada
sentiment analysis systems.
</tableCaption>
<bodyText confidence="0.99850915625">
from 85.19 to 86.37, and discriminating nega-
tion words (discussed in Section 2.1.2) further im-
proved the F-score from 86.37 to 86.63.
Our system ranked second on the LvJn14 and
SMS13 dataset. Note that the term-level system
that ranked first on LvJn14 performed worse than
our system on SMS13 and the system that ranked
first on SMS13 showed worse results than ours on
LvJn14, indicating that our term-level models in
general have good generalizability on these two
out-of-domain datasets.
On the message-level task, again the NRC14
system showed significant improvements over the
last year’s system on all five datasets. It achieved
the second best result on the Twt13 data and the
fourth result on the Twt14 data among 42 teams.
It was also the best system to predict sentiment in
sarcastic tweets (Sarc14). Furthermore, the system
proved to generalize well to other types of short
informal texts; it placed first on the two out-of-
domain datasets: SMS13 and LvJn14. We observe
a major improvement of our message-level model
on Sarc14 over our last year’s model, but as the
size of Sarc14 is small (86 tweets), more data and
analysis would be desirable to help better under-
stand this phenomenon.
Contribution of features Table 3 presents the re-
sults of ablation experiments on all five test sets for
the term-level task. The features derived from the
manual and automatic lexicons proved to be useful
on four datasets. The only exception is the Sarc14
data where removing lexicon features results in no
performance improvement. Considering that this
test set is very small (only about 100 test terms),
further investigation would be desirable if a larger
dataset becomes available. Also, in sarcasm the
real sentiment of a text span may be different from
its literal sentiment. In such a situation, a system
that correctly recognizes the literal sentiment may
actually make mistakes in capturing the real sen-
timent. The last two rows in Table 3 show the re-
sults obtained when the features are extracted only
from the target (and not from its context) and when
they are extracted only from the context of the tar-
get (and not from the target itself). Observe that
even though the context may influence the polar-
ity of the target, using target features alone is sub-
stantially more useful than using context features
alone. Nonetheless, adding context features im-
proves the F-scores in general.
On the message-level task (Table 4), the fea-
tures derived from the sentiment lexicons and, in
particular, from our large-coverage tweet-specific
lexicons turned out to be the most influential. The
use of the lexicons provided consistent gains of 9–
11 percentage points not only on tweet datasets,
but also on out-of-domain SMS and LiveJournal
data. Note that removing the features derived from
the manual lexicons as well as removing the ngram
features improves the performance on the Twt14
dataset. However, this effect is not observed on
the Twt13 and the out-of-domain test sets. The
possible explanation of this phenomenon is minor
overfitting on the tweet data.
</bodyText>
<sectionHeader confidence="0.997756" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999772875">
We presented supervised statistical systems for
message-level and term-level sentiment analysis
of tweets. They incorporate many surface-form,
semantic, and sentiment features. Among sub-
missions from over 40 teams in the Semeval-
2014 shared task “Sentiment Analysis in Twit-
ter”, our submissions ranked first in five out of
the ten subtask-dataset combinations. The sin-
gle most useful set of features are those obtained
from automatically generated tweet-specific lexi-
cons. We obtained significant improvements over
our previous system (which ranked first in the
2013 shared task) notably by estimating the senti-
ment of words in affirmative and negated contexts
separately. Also, since different negation words
impact sentiment differently, we modeled different
negation words separately in our term-level sys-
tem. This too led to an improvement in F-score.
The results on different kinds of evaluation sets
show that even though our systems are trained only
on tweets, they generalize well to text from other
domains such as blog posts and SMS messages.
Many of the resources we created and used are
made freely available.2
</bodyText>
<footnote confidence="0.951127">
2www.purl.com/net/sentimentoftweets
</footnote>
<page confidence="0.995824">
446
</page>
<table confidence="0.6963931875">
Experiment
all features
all - lexicons
all - manu. lex.
all - auto. lex.
all - ngrams
all - target
all - context
Twt14 Twt13 Sarc14 LvJn14 SMS13
86.63 90.14 77.13 85.49 88.03
81.98 86.25 80.74 80.00 83.91
86.08 89.25 75.32 84.13 87.69
86.05 88.32 80.38 83.96 86.18
83.31 86.67 72.95 81.58 82.41
72.93 74.19 63.09 72.21 69.34
84.40 88.83 77.22 82.99 87.97
</table>
<tableCaption confidence="0.977858">
Table 3: Term-level Task: The macro-averaged F-scores obtained on the 5 test sets with one of the feature
groups removed.
</tableCaption>
<table confidence="0.912742909090909">
Experiment Twt14 Twt13 Sarc14 LvJn14 SMS13
all features
all - lexicons
all - manu. lex.
all - auto. lex.
all - ngrams
69.85 70.75 58.16 74.84 70.28
60.59 60.04 47.17 65.80 60.56
71.84 69.84 53.34 73.41 66.60
63.40 65.08 47.57 71.76 66.94
70.02 67.90 44.58 74.43 68.45
</table>
<tableCaption confidence="0.9607425">
Table 4: Message-level Task: The macro-averaged F-scores obtained on the 5 test sets with one of the
feature groups removed.
</tableCaption>
<sectionHeader confidence="0.998271" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9965385">
We thank Colin Cherry for providing his SVM
code and for helpful discussions.
</bodyText>
<sectionHeader confidence="0.999447" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999834140350877">
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2(3):27:1–27:27.
Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for Twitter: Annotation, features, and experiments.
In Proceedings of ACL.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of KDD,
pages 168–177, New York, NY, USA. ACM.
Svetlana Kiritchenko, Xiaodan Zhu, and Saif Moham-
mad. 2014. Sentiment analysis of short informal
texts. (To appear) Journal of Artificial Intelligence
Research.
Saif M. Mohammad and Peter D. Turney. 2010. Emo-
tions evoked by common words and phrases: Using
Mechanical Turk to create an emotion lexicon. In
Proceedings of the NAACL-HLT Workshop on Com-
putational Approaches to Analysis and Generation
of Emotion in Text, LA, California.
Saif M. Mohammad and Tony (Wenda) Yang. 2011.
Tracking sentiment in mail: How genders differ on
emotional axes. In Proceedings of the ACL Work-
shop on Computational Approaches to Subjectivity
and Sentiment Analysis, Portland, OR, USA.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the state-of-the-
art in sentiment analysis of tweets. In Proceedings
of the International Workshop on Semantic Evalua-
tion, SemEval ’13, Atlanta, Georgia, USA, June.
Roser Morante and Caroline Sporleder. 2012. Modal-
ity and negation: An introduction to the special is-
sue. Computational linguistics, 38(2):223–260.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: Sentiment classification us-
ing machine learning techniques. In Proceedings of
EMNLP, pages 79–86, Philadelphia, PA.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. 2014. SemEval-2014 Task 9:
Sentiment Analysis in Twitter. In Proceedings of
SemEval-2014, Dublin, Ireland.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional Linguistics, 37(2):267–307.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of HLT-
EMNLP, HLT ’05, pages 347–354, Stroudsburg, PA,
USA.
Xiaodan Zhu, Hongyu Guo, Saif Mohammad, and
Svetlana Kiritchenko. 2014. An empirical study on
the effect of negation words on sentiment. In Pro-
ceedings of ACL, Baltimore, Maryland, USA, June.
</reference>
<page confidence="0.998614">
447
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.431857">
<title confidence="0.7710085">NRC-Canada-2014: Recent Improvements the Sentiment Analysis of Tweets</title>
<author confidence="0.578844">Xiaodan Zhu</author>
<author confidence="0.578844">Svetlana Kiritchenko</author>
<author confidence="0.578844">M Saif</author>
<affiliation confidence="0.660902">National Research Council</affiliation>
<address confidence="0.635819">Ottawa, Ontario, Canada K1A</address>
<abstract confidence="0.989572904761905">This paper describes state-of-the-art statistical systems for automatic sentiment analysis of tweets. In a Semeval-2014 shared task (Task 9), our submissions obtained highest scores in the term-level sentiment classification subtask on both the 2013 and 2014 tweets test sets. In the message-level sentiment classification task, our submissions obtained highest scores on the Live- Journal blog posts test set, sarcastic tweets test set, and the 2013 SMS test set. These systems build on our SemEval-2013 sentiment analysis systems (Mohammad et al., 2013) which ranked first in both the termand message-level subtasks in 2013. Key improvements over the 2013 systems are in the handling of negation. We create separate tweet-specific sentiment lexicons for terms in affirmative contexts and in negated contexts.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<volume>2</volume>
<issue>3</issue>
<contexts>
<context position="7979" citStr="Chang and Lin, 2011" startWordPosition="1271" endWordPosition="1274">own clustering algorithm from 56 million Englishlanguage tweets. These 1,000 clusters serve as an alternative representation of tweet content, reducing the sparsity of the token space. Encodings The encoding features are derived from hashtags, punctuation marks, emoticons, elongated words, and uppercased words. For the term-level task, all the above features are extracted for target terms and their context, where a context is a window of words surrounding a target term. For the message-level task, the features are extracted from the whole tweet. 444 In the term-level task, we used the LIBSVM (Chang and Lin, 2011) tool with the following parameters: -t 0 -b 1 -m 1000. The total number of features is about 115,000. In the messagelevel task, we used an in-house implementation of SVM with a linear kernel. The parameter C was set to 0.005. The total number of features was about 1.5 million. 2.1 Improving Lexicons and Negation Models An important advantage of our SemEval-2013 systems comes from the use of the two highcoverage tweet-specific sentiment lexicons. In the SemEval-2014 submissions, we improve these lexicons by incorporating negation modeling into the lexicon generation process. 2.1.1 Improving Se</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2(3):27:1–27:27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Dipanjan Das</author>
<author>Daniel Mills</author>
<author>Jacob Eisenstein</author>
<author>Michael Heilman</author>
<author>Dani Yogatama</author>
<author>Jeffrey Flanigan</author>
<author>Noah A Smith</author>
</authors>
<title>Part-of-speech tagging for Twitter: Annotation, features, and experiments.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, Smith, 2011</marker>
<rawString>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. 2011. Part-of-speech tagging for Twitter: Annotation, features, and experiments. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of KDD,</booktitle>
<pages>168--177</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="4909" citStr="Hu and Liu, 2004" startWordPosition="761" endWordPosition="764">ur Systems Our SemEval-2014 systems are based on our SemEval-2013 systems (Mohammad et al., 2013). For completeness, we briefly revisit our previous approach, which uses support vector machine (SVM) as the classification algorithm and leverages the following features. Lexicon features These features are generated by using three manually constructed sentiment lexicons and two automatically constructed lexicons. The manually constructed lexicons include the NRC Emotion Lexicon (Mohammad and Turney, 2010; Mohammad and Yang, 2011), the MPQA Lexicon (Wilson et al., 2005), and the Bing Liu Lexicon (Hu and Liu, 2004). The two automatically constructed lexicons, the Hashtag Sentiment Lexicon and the Sentiment140 Lexicon, were created specifically for tweets (Mohammad et al., 2013). The sentiment score of each term (e.g., a word or bigram) in the automatically constructed lexicons is computed by measuring the PMI (pointwise mutual information) between the term and the positive or negative category of tweets using the formula: SenScore (w) = PMI(w, pos) − PMI(w, neg) (1) where w is a term in the lexicons. PMI(w, pos) is the PMI score between w and the positive class, and PMI(w, neg) is the PMI score between </context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of KDD, pages 168–177, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
<author>Saif Mohammad</author>
</authors>
<title>Sentiment analysis of short informal texts. (To appear)</title>
<date>2014</date>
<journal>Journal of Artificial Intelligence Research.</journal>
<contexts>
<context position="8786" citStr="Kiritchenko et al., 2014" startWordPosition="1402" endWordPosition="1405">ar kernel. The parameter C was set to 0.005. The total number of features was about 1.5 million. 2.1 Improving Lexicons and Negation Models An important advantage of our SemEval-2013 systems comes from the use of the two highcoverage tweet-specific sentiment lexicons. In the SemEval-2014 submissions, we improve these lexicons by incorporating negation modeling into the lexicon generation process. 2.1.1 Improving Sentiment Lexicons A word in a negated context has a different evaluative nature than the same word in an affirmative (non-negated) context. We have proposed a lexicon-based approach (Kiritchenko et al., 2014) to determining the sentiment of words in these two situations by automatically creating separate sentiment lexicons for the affirmative and negated contexts. In this way, we do not need to employ any explicit assumptions to model negation. To achieve this, a tweet corpus is split into two parts: Affirmative Context Corpus and Negated Context Corpus. Following the work of Pang et al. (2002), we define a negated context as a segment of a tweet that starts with a negation word (e.g., no, shouldn’t) and ends with one of the punctuation marks: ‘,’, ‘.’, ‘:’, ‘;’, ‘!’, ‘?’. The list of negation wor</context>
</contexts>
<marker>Kiritchenko, Zhu, Mohammad, 2014</marker>
<rawString>Svetlana Kiritchenko, Xiaodan Zhu, and Saif Mohammad. 2014. Sentiment analysis of short informal texts. (To appear) Journal of Artificial Intelligence Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Peter D Turney</author>
</authors>
<title>Emotions evoked by common words and phrases: Using Mechanical Turk to create an emotion lexicon.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL-HLT Workshop on Computational Approaches to Analysis and Generation of Emotion in Text,</booktitle>
<location>LA, California.</location>
<contexts>
<context position="4798" citStr="Mohammad and Turney, 2010" startWordPosition="741" endWordPosition="744">ring in an affirmative context, and separately the average sentiment of a term when occurring in a negated context. 2 Our Systems Our SemEval-2014 systems are based on our SemEval-2013 systems (Mohammad et al., 2013). For completeness, we briefly revisit our previous approach, which uses support vector machine (SVM) as the classification algorithm and leverages the following features. Lexicon features These features are generated by using three manually constructed sentiment lexicons and two automatically constructed lexicons. The manually constructed lexicons include the NRC Emotion Lexicon (Mohammad and Turney, 2010; Mohammad and Yang, 2011), the MPQA Lexicon (Wilson et al., 2005), and the Bing Liu Lexicon (Hu and Liu, 2004). The two automatically constructed lexicons, the Hashtag Sentiment Lexicon and the Sentiment140 Lexicon, were created specifically for tweets (Mohammad et al., 2013). The sentiment score of each term (e.g., a word or bigram) in the automatically constructed lexicons is computed by measuring the PMI (pointwise mutual information) between the term and the positive or negative category of tweets using the formula: SenScore (w) = PMI(w, pos) − PMI(w, neg) (1) where w is a term in the lex</context>
</contexts>
<marker>Mohammad, Turney, 2010</marker>
<rawString>Saif M. Mohammad and Peter D. Turney. 2010. Emotions evoked by common words and phrases: Using Mechanical Turk to create an emotion lexicon. In Proceedings of the NAACL-HLT Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, LA, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Tony Yang</author>
</authors>
<title>Tracking sentiment in mail: How genders differ on emotional axes.</title>
<date>2011</date>
<booktitle>In Proceedings of the ACL Workshop on Computational Approaches to Subjectivity and Sentiment Analysis,</booktitle>
<location>Portland, OR, USA.</location>
<contexts>
<context position="4824" citStr="Mohammad and Yang, 2011" startWordPosition="745" endWordPosition="748">ext, and separately the average sentiment of a term when occurring in a negated context. 2 Our Systems Our SemEval-2014 systems are based on our SemEval-2013 systems (Mohammad et al., 2013). For completeness, we briefly revisit our previous approach, which uses support vector machine (SVM) as the classification algorithm and leverages the following features. Lexicon features These features are generated by using three manually constructed sentiment lexicons and two automatically constructed lexicons. The manually constructed lexicons include the NRC Emotion Lexicon (Mohammad and Turney, 2010; Mohammad and Yang, 2011), the MPQA Lexicon (Wilson et al., 2005), and the Bing Liu Lexicon (Hu and Liu, 2004). The two automatically constructed lexicons, the Hashtag Sentiment Lexicon and the Sentiment140 Lexicon, were created specifically for tweets (Mohammad et al., 2013). The sentiment score of each term (e.g., a word or bigram) in the automatically constructed lexicons is computed by measuring the PMI (pointwise mutual information) between the term and the positive or negative category of tweets using the formula: SenScore (w) = PMI(w, pos) − PMI(w, neg) (1) where w is a term in the lexicons. PMI(w, pos) is the </context>
</contexts>
<marker>Mohammad, Yang, 2011</marker>
<rawString>Saif M. Mohammad and Tony (Wenda) Yang. 2011. Tracking sentiment in mail: How genders differ on emotional axes. In Proceedings of the ACL Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, Portland, OR, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
</authors>
<title>NRC-Canada: Building the state-of-theart in sentiment analysis of tweets.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation, SemEval ’13,</booktitle>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="820" citStr="Mohammad et al., 2013" startWordPosition="109" endWordPosition="112">odan.zhu,svetlana.kiritchenko,saif.mohammad}@nrc-cnrc.gc.ca Abstract This paper describes state-of-the-art statistical systems for automatic sentiment analysis of tweets. In a Semeval-2014 shared task (Task 9), our submissions obtained highest scores in the term-level sentiment classification subtask on both the 2013 and 2014 tweets test sets. In the message-level sentiment classification task, our submissions obtained highest scores on the LiveJournal blog posts test set, sarcastic tweets test set, and the 2013 SMS test set. These systems build on our SemEval-2013 sentiment analysis systems (Mohammad et al., 2013) which ranked first in both the termand message-level subtasks in 2013. Key improvements over the 2013 systems are in the handling of negation. We create separate tweet-specific sentiment lexicons for terms in affirmative contexts and in negated contexts. 1 Introduction Automatically detecting sentiment of tweets (and other microblog posts) has attracted extensive interest from both the academia and industry. The Conference on Semantic Evaluation Exercises (SemEval) organizes a shared task on the sentiment analysis of tweets with two subtasks. In the message-level task, the participating syste</context>
<context position="4389" citStr="Mohammad et al., 2013" startWordPosition="684" endWordPosition="687">he changing of the truth value of a proposition”. Negation is often expressed through the use of negative signals or negators, words such as isnt and never, and it can significantly affect the sentiment of its scope. We create separate tweetspecific sentiment lexicons for terms in affirmative contexts and in negated contexts. That is, we automatically determine the average sentiment of a term when occurring in an affirmative context, and separately the average sentiment of a term when occurring in a negated context. 2 Our Systems Our SemEval-2014 systems are based on our SemEval-2013 systems (Mohammad et al., 2013). For completeness, we briefly revisit our previous approach, which uses support vector machine (SVM) as the classification algorithm and leverages the following features. Lexicon features These features are generated by using three manually constructed sentiment lexicons and two automatically constructed lexicons. The manually constructed lexicons include the NRC Emotion Lexicon (Mohammad and Turney, 2010; Mohammad and Yang, 2011), the MPQA Lexicon (Wilson et al., 2005), and the Bing Liu Lexicon (Hu and Liu, 2004). The two automatically constructed lexicons, the Hashtag Sentiment Lexicon and </context>
<context position="5887" citStr="Mohammad et al., 2013" startWordPosition="924" endWordPosition="927"> positive or negative category of tweets using the formula: SenScore (w) = PMI(w, pos) − PMI(w, neg) (1) where w is a term in the lexicons. PMI(w, pos) is the PMI score between w and the positive class, and PMI(w, neg) is the PMI score between w and the negative class. Therefore, a positive SenScore (w) suggests a stronger association of word w with positive sentiment and vice versa. The magnitude indicates the strength of association. Note that the sentiment class of the tweets used to construct the lexicons was automatically identified either from hashtags or from emoticons as described in (Mohammad et al., 2013). With these lexicons available, the following features were extracted for a text span. Here a text span can be a target term, its context, or an entire tweet, depending on the task. The lexicon features include: (1) the number of sentiment tokens in a text span; sentiment tokens are word tokens whose sentiment scores are not zero in a lexicon; (2) the total sentiment score of the text span: Ew∈textSpan SenScore (w); (3) the maximal score: maxw∈textSpanSenScore (w); (4) the total positive and negative sentiment scores of the text span; (5) the sentiment score of the last token in the text span</context>
</contexts>
<marker>Mohammad, Kiritchenko, Zhu, 2013</marker>
<rawString>Saif Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. NRC-Canada: Building the state-of-theart in sentiment analysis of tweets. In Proceedings of the International Workshop on Semantic Evaluation, SemEval ’13, Atlanta, Georgia, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Morante</author>
<author>Caroline Sporleder</author>
</authors>
<title>Modality and negation: An introduction to the special issue.</title>
<date>2012</date>
<journal>Computational linguistics,</journal>
<volume>38</volume>
<issue>2</issue>
<contexts>
<context position="3707" citStr="Morante and Sporleder (2012)" startWordPosition="573" endWordPosition="576"> 1. As we will show, automatically generated tweet-specific lexicons were especially helpful in all subtask–dataset combinations. The results also show that even though our models are trained only on tweets, they generalize well to data from other domains. 443 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 443–447, Dublin, Ireland, August 23-24, 2014. Our systems are based on supervised SVMs and a number of surface-form, semantic, and sentiment features. The major improvement in our 2014 system over the 2013 system is in the way it handles negation. Morante and Sporleder (2012) define negation to be “a grammatical category that allows the changing of the truth value of a proposition”. Negation is often expressed through the use of negative signals or negators, words such as isnt and never, and it can significantly affect the sentiment of its scope. We create separate tweetspecific sentiment lexicons for terms in affirmative contexts and in negated contexts. That is, we automatically determine the average sentiment of a term when occurring in an affirmative context, and separately the average sentiment of a term when occurring in a negated context. 2 Our Systems Our </context>
</contexts>
<marker>Morante, Sporleder, 2012</marker>
<rawString>Roser Morante and Caroline Sporleder. 2012. Modality and negation: An introduction to the special issue. Computational linguistics, 38(2):223–260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>79--86</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="7016" citStr="Pang et al. (2002)" startWordPosition="1115" endWordPosition="1118">iment scores of the text span; (5) the sentiment score of the last token in the text span. Note that all these features are generated, when applicable, by using each of the sentiment lexicons mentioned above. Ngrams We employed two types of ngram features: word ngrams and character ngrams. The former reflect the presence or absence of contiguous or non-contiguous sequences of words, and the latter are sequences of prefix/suffix characters in each word. These features are same as in our last year’s submission. Negation The number of negated contexts. Our definition of a negated context follows Pang et al. (2002), which will be described in more details below in Section 2.1. POS The number of occurrences of each partof-speech tag. We tokenized and part-of-speech tagged the tweets with the Carnegie Mellon University (CMU) Twitter NLP tool (Gimpel et al., 2011). Cluster features The CMU POS-tagging tool provides the token clusters produced with the Brown clustering algorithm from 56 million Englishlanguage tweets. These 1,000 clusters serve as an alternative representation of tweet content, reducing the sparsity of the token space. Encodings The encoding features are derived from hashtags, punctuation m</context>
<context position="9179" citStr="Pang et al. (2002)" startWordPosition="1467" endWordPosition="1470">2.1.1 Improving Sentiment Lexicons A word in a negated context has a different evaluative nature than the same word in an affirmative (non-negated) context. We have proposed a lexicon-based approach (Kiritchenko et al., 2014) to determining the sentiment of words in these two situations by automatically creating separate sentiment lexicons for the affirmative and negated contexts. In this way, we do not need to employ any explicit assumptions to model negation. To achieve this, a tweet corpus is split into two parts: Affirmative Context Corpus and Negated Context Corpus. Following the work of Pang et al. (2002), we define a negated context as a segment of a tweet that starts with a negation word (e.g., no, shouldn’t) and ends with one of the punctuation marks: ‘,’, ‘.’, ‘:’, ‘;’, ‘!’, ‘?’. The list of negation words was adopted from Christopher Potts’ sentiment tutorial.1 Thus, part of a tweet that is marked as negated is included into the negated context corpus while the rest of the tweet becomes part of the affirmative context corpus. The sentiment label for the tweet is kept unchanged in both corpora. Then, we generate an affirmative context lexicon from the affirmative context corpus and a negat</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: Sentiment classification using machine learning techniques. In Proceedings of EMNLP, pages 79–86, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>Preslav Nakov</author>
<author>Alan Ritter</author>
<author>Veselin Stoyanov</author>
</authors>
<title>SemEval-2014 Task 9: Sentiment Analysis in Twitter.</title>
<date>2014</date>
<booktitle>In Proceedings of SemEval-2014,</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="2151" citStr="Rosenthal et al., 2014" startWordPosition="320" endWordPosition="323">m-level task, the objective is to determine the sentiment of a marked target term (a single word or a multi-word expression) within the tweet. Our submissions stood first in both subtasks in 2013. This paper describes improvements over that sysEvaluation Set Term-level Task Message-level Task Twt14 1 4 Twt13 1 2 Sarc14 3 1 LvJn14 2 1 SMS13 2 1 Table 1: Overall rank of NRC-Canada sentiment analysis models in Semeval-2014 Task 9 under the constrained condition. The rows are five evaluation datasets and the columns are the two subtasks. tem and the subsequent submissions to the 2014 shared task (Rosenthal et al., 2014). The training data for the SemEval-2014 shared task is same as that of SemEval-2013 (about 10,000 tweets). The 2014 test set has five subcategories: a tweet set provided newly in 2014 (Twt14), the tweet set used for testing in the 2013 shared task (Twt13), a set of tweets that are sarcastic (Sarc14), a set of sentences from the blogging website LiveJournal (LvJn14), and the set of SMS messages used for testing in the 2013 shared task (SMS13). Instances from these categories were interspersed in the provided test set. The participants were not told about the source of the individual messages. </context>
</contexts>
<marker>Rosenthal, Nakov, Ritter, Stoyanov, 2014</marker>
<rawString>Sara Rosenthal, Preslav Nakov, Alan Ritter, and Veselin Stoyanov. 2014. SemEval-2014 Task 9: Sentiment Analysis in Twitter. In Proceedings of SemEval-2014, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maite Taboada</author>
<author>Julian Brooke</author>
<author>Milan Tofiloski</author>
<author>Kimberly Voll</author>
<author>Manfred Stede</author>
</authors>
<title>Lexiconbased methods for sentiment analysis.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>2</issue>
<contexts>
<context position="11062" citStr="Taboada et al., 2011" startWordPosition="1771" endWordPosition="1774">e negation scope identification algorithm can at times fail and include parts of a tweet that are not actually negated (e.g., if a punctuation mark is missing). Overall, a sentiment word can have up to three scores, one for affirmative context, one for immediate negated context, and one for distant negated context. We reconstructed the Hashtag Sentiment Lexicon and the Sentiment140 Lexicon with this approach and used them in our SemEval-2014 systems. 2.1.2 Discriminating Negation Words Different negation words, e.g., never and didn’t, can have different effects on sentiment (Zhu et al., 2014; Taboada et al., 2011). In our SemEval-2014 submission, we discriminate negation words in the term-level models. For example, the word acceptable appearing in a sentence this is never acceptable is marked as acceptable beNever, while in the sentence this is not acceptable, it is marked as acceptable beNot. In this way, different negators (e.g., be not and be never) are treated differently. Note that we do not differentiate the tense and person of auxiliaries in order to reduce sparseness (e.g., was not and am not are treated in the same way). This new representation is used to extract ngrams and lexicon-based featu</context>
</contexts>
<marker>Taboada, Brooke, Tofiloski, Voll, Stede, 2011</marker>
<rawString>Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly Voll, and Manfred Stede. 2011. Lexiconbased methods for sentiment analysis. Computational Linguistics, 37(2):267–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of HLTEMNLP, HLT ’05,</booktitle>
<pages>347--354</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4864" citStr="Wilson et al., 2005" startWordPosition="752" endWordPosition="755"> a term when occurring in a negated context. 2 Our Systems Our SemEval-2014 systems are based on our SemEval-2013 systems (Mohammad et al., 2013). For completeness, we briefly revisit our previous approach, which uses support vector machine (SVM) as the classification algorithm and leverages the following features. Lexicon features These features are generated by using three manually constructed sentiment lexicons and two automatically constructed lexicons. The manually constructed lexicons include the NRC Emotion Lexicon (Mohammad and Turney, 2010; Mohammad and Yang, 2011), the MPQA Lexicon (Wilson et al., 2005), and the Bing Liu Lexicon (Hu and Liu, 2004). The two automatically constructed lexicons, the Hashtag Sentiment Lexicon and the Sentiment140 Lexicon, were created specifically for tweets (Mohammad et al., 2013). The sentiment score of each term (e.g., a word or bigram) in the automatically constructed lexicons is computed by measuring the PMI (pointwise mutual information) between the term and the positive or negative category of tweets using the formula: SenScore (w) = PMI(w, pos) − PMI(w, neg) (1) where w is a term in the lexicons. PMI(w, pos) is the PMI score between w and the positive cla</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phraselevel sentiment analysis. In Proceedings of HLTEMNLP, HLT ’05, pages 347–354, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodan Zhu</author>
<author>Hongyu Guo</author>
<author>Saif Mohammad</author>
<author>Svetlana Kiritchenko</author>
</authors>
<title>An empirical study on the effect of negation words on sentiment.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Baltimore, Maryland, USA,</location>
<contexts>
<context position="11039" citStr="Zhu et al., 2014" startWordPosition="1767" endWordPosition="1770">s noisy. Our simple negation scope identification algorithm can at times fail and include parts of a tweet that are not actually negated (e.g., if a punctuation mark is missing). Overall, a sentiment word can have up to three scores, one for affirmative context, one for immediate negated context, and one for distant negated context. We reconstructed the Hashtag Sentiment Lexicon and the Sentiment140 Lexicon with this approach and used them in our SemEval-2014 systems. 2.1.2 Discriminating Negation Words Different negation words, e.g., never and didn’t, can have different effects on sentiment (Zhu et al., 2014; Taboada et al., 2011). In our SemEval-2014 submission, we discriminate negation words in the term-level models. For example, the word acceptable appearing in a sentence this is never acceptable is marked as acceptable beNever, while in the sentence this is not acceptable, it is marked as acceptable beNot. In this way, different negators (e.g., be not and be never) are treated differently. Note that we do not differentiate the tense and person of auxiliaries in order to reduce sparseness (e.g., was not and am not are treated in the same way). This new representation is used to extract ngrams </context>
</contexts>
<marker>Zhu, Guo, Mohammad, Kiritchenko, 2014</marker>
<rawString>Xiaodan Zhu, Hongyu Guo, Saif Mohammad, and Svetlana Kiritchenko. 2014. An empirical study on the effect of negation words on sentiment. In Proceedings of ACL, Baltimore, Maryland, USA, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>