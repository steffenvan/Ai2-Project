<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001018">
<title confidence="0.821537">
Generalized Stack Decoding Algorithms for Statistical Machine Translation∗
</title>
<note confidence="0.895518444444444">
Ismael Garcia Varea
Dpto. de Informatica
Univ. de Castilla-La Mancha
02071 Albacete, Spain
ivarea@info-ab.uclm.es
Daniel Ortiz Martinez
Inst. Tecnol´ogico de Inform´atica
Univ. Polit´ecnica de Valencia
46071 Valencia, Spain
</note>
<email confidence="0.988018">
dortiz@iti.upv.es
</email>
<sectionHeader confidence="0.997271" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9993532">
In this paper we propose a generalization
of the Stack-based decoding paradigm for
Statistical Machine Translation. The well
known single and multi-stack decoding
algorithms defined in the literature have
been integrated within a new formalism
which also defines a new family of stack-
based decoders. These decoders allows
a tradeoff to be made between the ad-
vantages of using only one or multiple
stacks. The key point of the new formal-
ism consists in parameterizeing the num-
ber of stacks to be used during the de-
coding process, and providing an efficient
method to decide in which stack each par-
tial hypothesis generated is to be inserted-
during the search process. Experimental
results are also reported for a search algo-
rithm for phrase-based statistical transla-
tion models.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99987525">
The translation process can be formulated from a
statistical point of view as follows: A source lan-
guage string fJ1 = f1 ... fJ is to be translated into
a target language string eI1 = e1 ... eI. Every tar-
get string is regarded as a possible translation for the
source language string with maximum a-posteriori
probability Pr(eI1|fJ1 ). According to Bayes’ theo-
rem, the target string eI1 that maximizes1 the product
</bodyText>
<footnote confidence="0.986726285714286">
This work has been partially supported by the Spanish
project TIC2003-08681-C02-02, the Agencia Valenciana de
Ciencia y Tecnologia under contract GRUPOS03/031, the Gen-
eralitat Valenciana, and the project HERMES (Vicerrectorado
de Investigaci´on - UCLM-05/06)
1Note that the expression should also be maximized by I;
however, for the sake of simplicity we suppose that it is known.
</footnote>
<note confidence="0.926429">
Francisco Casacuberta Nolla
Dpto. de Sist Inf. y Comp.
Univ. Polit´ec. de Valencia
46071 Valencia, Spain
</note>
<email confidence="0.96026">
fcn@dsic.upv.es
</email>
<bodyText confidence="0.995244333333333">
of both the target language model Pr(eI1) and the
string translation model Pr(fJ1 |eI1) must be chosen.
The equation that models this process is:
</bodyText>
<equation confidence="0.9588925">
eI1 = arg max{Pr(eI1) · Pr(fJ1 |eI1)} (1)
el
</equation>
<bodyText confidence="0.999668">
The search/decoding problem in SMT consists in
solving the maximization problem stated in Eq. (1).
In the literature, we can find different techniques to
deal with this problem, ranging from heuristic and
fast (as greedy decoders) to optimal and very slow
decoding algorithms (Germann et al., 2001). Also,
under certain circumstances, stack-based decoders
can obtain optimal solutions.
Many works (Berger et al., 1996; Wang and
Waibel, 1998; Germann et al., 2001; Och et al.,
2001; Ortiz et al., 2003) have adopted different types
of stack-based algorithms to solve the global search
optimization problem for statistical machine trans-
lation. All these works follow two main different
approaches according to the number of stacks used
in the design and implementation of the search algo-
rithm (the stacks are used to store partial hypotheses,
sorted according to their partial score/probability,
during the search process) :
</bodyText>
<listItem confidence="0.96544875">
• On the one hand, in (Wang and Waibel, 1998;
Och et al., 2001) a single stack is used. In
that case, in order to make the search feasible,
the pruning of the number of partial hypothe-
ses stored in the stack is needed. This causes
many search errors due to the fact that hy-
potheses covering a different number of source
(translated) words compete in the same condi-
tions. Therefore, the greater number of covered
words the higher possibility to be pruned.
• On the other hand (Berger et al., 1996; Ger-
mann et al., 2001) make use of multiple stacks
</listItem>
<page confidence="0.983322">
64
</page>
<subsectionHeader confidence="0.784312">
Proceedings of the Workshop on Statistical Machine Translation, pages 64–71,
New York City, June 2006. c�2006 Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.999893346153846">
(one for each set of source covered/translated
words in the partial hypothesis) in order to
solve the disadvantages of the single-stack ap-
proach. By contrast, the problem of finding
the best hypothesis to be expanded introduces
an exponential term in the computational com-
plexity of the algorithm.
In (Ortiz et al., 2003) the authors present an em-
pirical comparison (about efficiency and translation
quality) of the two approaches paying special atten-
tion to the advantages and disadvantages of the two
approaches.
In this paper we present a new formalism consist-
ing of a generalization of the classical stack-based
decoding paradigm for SMT. This new formalism
defines a new family of stack-based decoders, which
also integrates the well known stack-based decoding
algorithms proposed so far within the framework of
SMT, that is single and multi-stack decoders.
The rest of the paper is organized as follows: in
section 2 the phrase-based approach to SMT is de-
picted; in section 3 the main features of classical
stack-based decoders are presented; in section 4 the
new formalism is presented and in section 5 exper-
imental results are shown; finally some conclusions
are drawn in section 6.
</bodyText>
<sectionHeader confidence="0.980816" genericHeader="method">
2 Phrase Based Statistical Machine
Translation
</sectionHeader>
<bodyText confidence="0.999180636363636">
Different translation models (TMs) have been pro-
posed depending on how the relation between the
source and the target languages is structured; that is,
the way a target sentence is generated from a source
sentence. This relation is summarized using the con-
cept of alignment; that is, how the constituents (typ-
ically words or group-of-words) of a pair of sen-
tences are aligned to each other. The most widely
used single-word-based statistical alignment mod-
els (SAMs) have been proposed in (Brown et al.,
1993; Ney et al., 2000). On the other hand, models
that deal with structures or phrases instead of single
words have also been proposed: the syntax trans-
lation models are described in (Yamada and Knight,
2001) , alignment templates are used in (Och, 2002),
and the alignment template approach is re-framed
into the so-called phrase based translation (PBT)
in (Marcu and Wong, 2002; Zens et al., 2002; Koehn
et al., 2003; Tom´as and Casacuberta, 2003).
For the translation model (Pr(fJ1 |eI1)) in Eq. (1),
PBT can be explained from a generative point of
view as follows (Zens et al., 2002):
</bodyText>
<listItem confidence="0.987596333333333">
1. The target sentence eI1 is segmented into K
phrases (˜eK1 ).
2. Each target phrase ˜ek is translated into a source
phrase ˜f.
3. Finally, the source phrases are reordered in or-
der to compose the source sentence ˜fK1 = fJ1 .
</listItem>
<bodyText confidence="0.9962886">
In PBT, it is assumed that the relations between
the words of the source and target sentences can
be explained by means of the hidden variable ˜aK1 ,
which contains all the decisions made during the
generative story.
</bodyText>
<equation confidence="0.9934702">
�Pr(fJ1 |eI1) = Pr(, ˜fK 1 , ˜aK 1 |˜eK 1 )
K,˜aK
�= Pr(˜aK1 |˜eK1 )Pr(˜fK1 |˜aK1 , ˜eK1 )
K,˜aK
(2)
</equation>
<bodyText confidence="0.999028">
Different assumptions can be made from the pre-
vious equation. For example, in (Zens et al., 2002)
the following model is proposed:
</bodyText>
<equation confidence="0.877035">
�pθ(fJ1 |eI1) = α(eI1)
K,˜aK
</equation>
<bodyText confidence="0.969091">
where ˜ak notes the index of the source phrase e˜
which is aligned with the k-th target phrase ˜fk and
that all possible segmentations have the same proba-
bility. In (Tom´as and Casacuberta, 2001; Zens et al.,
2002), it also is assumed that the alignments must be
monotonic. This led us to the following equation:
</bodyText>
<equation confidence="0.8251825">
�pθ(fJ1 |eI1) = α(eI1)
K,˜aK
</equation>
<bodyText confidence="0.99985375">
In both cases the model parameters that have to be
estimated are the translation probabilities between
phrase pairs (θ = {p(˜f|˜e)}), which typically are es-
timated as follows:
</bodyText>
<equation confidence="0.956811363636364">
K
H p(˜fk|˜e˜ak) (3)
k=1
K
H p(˜fk|˜ek) (4)
k=1
˜f|˜e) = N(˜f, ˜e)
N(˜
e)
p(
(5)
</equation>
<page confidence="0.98676">
65
</page>
<bodyText confidence="0.99994">
where N( f|e) is the number of times that f have
been seen as a translation of a within the training
corpus.
</bodyText>
<sectionHeader confidence="0.987307" genericHeader="method">
3 Stack-Decoding Algorithms
</sectionHeader>
<bodyText confidence="0.999915090909091">
The stack decoding algorithm, also called A* algo-
rithm, was first introduced by F. Jelinek in (Jelinek,
1969). The stack decoding algorithm attempts to
generate partial solutions, called hypotheses, until a
complete translation is found2; these hypotheses are
stored in a stack and ordered by their score. Typi-
cally, this measure or score is the probability of the
product of the translation and the language models
introduced above. The A* decoder follows a se-
quence of steps for achieving a complete (and possi-
bly optimal) hypothesis:
</bodyText>
<listItem confidence="0.999768428571429">
1. Initialize the stack with an empty hypothesis.
2. Iterate
(a) Pop h (the best hypothesis) off the stack.
(b) If h is a complete sentence, output h and
terminate.
(c) Expand h.
(d) Go to step 2a.
</listItem>
<bodyText confidence="0.999922722222222">
The search is started from a null string and obtains
new hypotheses after an expansion process (step 2c)
which is executed at each iteration. The expansion
process consists of the application of a set of op-
erators over the best hypothesis in the stack, as it
is depicted in Figure 1. Thus, the design of stack
decoding algorithms involves defining a set of oper-
ators to be applied over every hypothesis as well as
the way in which they are combined in the expansion
process. Both the operators and the expansion algo-
rithm depend on the translation model that we use.
For the case of the phrase-based translation models
described in the previous section, the operator add is
defined, which adds a sequence of words to the tar-
get sentence, and aligns it with a sequence of words
of the source sentence.
The number of hypotheses to be stored during the
search can be huge. In order then to avoid mem-
</bodyText>
<footnote confidence="0.99250175">
2Each hypothesis has associated a coverage vector of length
J, which indicates the set of source words already cov-
ered/translated so far. In the following we will refer to this
simply as coverage.
</footnote>
<figureCaption confidence="0.952642">
Figure 1: Flow chart associated to the expansion of
a hypothesis when using an A⋆ algorithm.
</figureCaption>
<bodyText confidence="0.999501909090909">
ory overflow problems, the maximum number of hy-
potheses that a stack may store has to be limited. It
is important to note that for a hypothesis, the higher
the aligned source words, the worse the score. These
hypotheses will be discarded sooner when an A*
search algorithm is used due to the stack length lim-
itation. Because of this, the multi-stack algorithms
were introduced.
Multi-stack algorithms store those hypotheses
with different subsets of source aligned words in dif-
ferent stacks. That is to say, given an input sentence
fJ1 composed of J words, multi-stack algorithms
employes 2J stacks to translate it. Such an organi-
zation improves the pruning of the hypotheses when
the stack length limitation is exceeded, since only
hypotheses with the same number of covered posi-
tions can compete with each other.
All the search steps given for A* algorithm can
also be applied here, except step 2a. This is due
to the fact that multiple stacks are used instead of
only one. Figure 2 depicts the expansion process
that the multi-stack algorithms execute, which is
slightly different than the one presented in Figure 1.
Multi-stack algorithms have the negative property of
spending significant amounts of time in selecting the
hypotheses to be expanded, since at each iteration,
the best hypothesis in a set of 2J stacks must be
searched for (Ortiz et al., 2003). By contrast, for the
A* algorithm, it is not possible to reduce the length
of the stack in the same way as in the multi-stack
case without loss of translation quality.
Additionally, certain translation systems, e.g. the
Pharaoh decoder (Koehn, 2003) use an alternative
</bodyText>
<page confidence="0.896277">
66
</page>
<figureCaption confidence="0.948978">
Figure 2: Flow chart associated to the expansion of
a hypothesis when using a multi-stack algorithm.
</figureCaption>
<bodyText confidence="0.998917333333333">
approach which consists in assigning to the same
stack, those hypotheses with the same number of
source words covered.
</bodyText>
<sectionHeader confidence="0.996851" genericHeader="method">
4 Generalized Stack-Decoding Algorithms
</sectionHeader>
<bodyText confidence="0.9999387">
As was mentioned in the previous section, given a
sentence fJ1 to be translated, a single stack decod-
ing algorithm employs only one stack to perform the
translation process, while a multi-stack algorithm
employs 2J stacks. We propose a possible way to
make a tradeoff between the advantages of both al-
gorithms that introduces a new parameter which will
be referred to as the granularity of the algorithm.
The granularity parameter determines the number of
stacks used during the decoding process.
</bodyText>
<subsectionHeader confidence="0.997572">
4.1 Selecting the granularity of the algorithm
</subsectionHeader>
<bodyText confidence="0.998707142857143">
The granularity (G) of a generalized stack algorithm
is an integer which takes values between 1 and J,
where J is the number of words which compose the
sentence to translate.
Given a sentence fJ1 to be translated, a general-
ized stack algorithm with a granularity parameter
equal to g, will have the following features:
</bodyText>
<listItem confidence="0.971976714285714">
• The algorithm will use at most 29 stacks to per-
form the translation
• Each stack will contain hypotheses which have
2J−9 different coverages of fJ1
• If the algorithm can store at most 5 = s hy-
potheses, then, the maximum size of each stack
will be equal to &apos;
</listItem>
<page confidence="0.984809">
29
</page>
<subsectionHeader confidence="0.981168">
4.2 Mapping hypotheses to stacks
</subsectionHeader>
<bodyText confidence="0.999969727272727">
Generalized stack-decoding algorithms require a
mechanism to decide in which stack each hypothesis
is to be inserted. As stated in section 4.1, given an
input sentence fJ1 and a generalized stack-decoding
algorithm with G = g, the decoder will work with
29 stacks, and each one will contain 2J−9 different
coverages. Therefore, the above mentioned mecha-
nism can be expressed as a function which will be
referred to as the µ function. Given a hypothesis
coverage composed of J bits, the µ function return
a stack identifier composed of only g bits:
</bodyText>
<equation confidence="0.997394">
µ : ({0,1})J −→ ({0,1})9 (6)
</equation>
<bodyText confidence="0.976372307692308">
Generalized stack algorithms are strongly in-
spired by multi-stack algorithms; however, both
types of algorithms differ in the way the hypothesis
expansion is performed. Figure 3 shows the expan-
sion algorithm of a generalized stack decoder with
a granularity parameter equal to g and a function µ
which maps hypotheses coverages to stacks.
Figure 3: Flow chart associated to the expansion of
a hypothesis when using a generalized-stack algo-
rithm.
The function µ can be defined in many ways,
but there are two essential principles which must be
taken into account:
</bodyText>
<listItem confidence="0.9989446">
• The µ function must be efficiently calculated
• Hypotheses whose coverage have a similar
number of bits set to one must be assigned to
the same stack. This requirement allows the
pruning of the stacks to be improved, since the
</listItem>
<page confidence="0.997488">
67
</page>
<bodyText confidence="0.989590583333333">
hypotheses with a similar number of covered
words can compete fairly
A possible way to implement the µ function,
namely µ1, consists in simply shifting the coverage
vector J − g positions to the right, and then keeping
only the first g bits. Such a proposal is very easy
to calculate, however, it has a poor performance ac-
cording to the second principle explained above.
A better alternative to implement the µ function,
namely µ2, can be formulated as a composition of
two functions. A constructive definition of such a
implementation is detailed next:
</bodyText>
<listItem confidence="0.966069818181818">
1. Let us suppose that the source sentence is com-
posed by J words, we order the set of J bit
numbers as follows: first the numbers which do
not have any bit equal to one, next, the numbers
which have only one bit equal to one, and so on
2. Given the list of numbers described above, we
define a function which associates to each num-
ber of the list, the order of the number within
this list
3. Given the coverage of a partial hypothesis, x,
the stack on which this partial hypothesis is to
</listItem>
<bodyText confidence="0.988731555555555">
be inserted is obtained by a two step process:
First, we obtain the image of x returned by the
function described above. Next, the result is
shifted J − g positions to the right, keeping the
first g bits
Let Q be the function that shifts a bit vector J − g
positions to the right, keeping the first g bits; and let
α be the function that for each coverage returns its
order:
</bodyText>
<equation confidence="0.999550333333333">
α : ({0,1})J −→ ({0,1})J (7)
Then, µ2 is expressed as follows:
µ2(x) = Q o α(x) (8)
</equation>
<bodyText confidence="0.999415857142857">
Table 1 shows an example of the values which re-
turns the µ1 and the µ2 functions when the input sen-
tence has 4 words and the granularity of the decoder
is equal to 2. As it can be observed, µ2 function
performs better than µ1 function according to the
second principle described at the beginning of this
section.
</bodyText>
<table confidence="0.999357588235294">
x µ1(x) α(x) µ2(x)
0000 00 0000 00
0001 00 0001 00
0010 00 0010 00
0100 01 0011 00
1000 10 0100 01
0011 00 0101 01
0101 01 0110 01
0110 01 0111 01
1001 10 1000 10
1010 10 1001 10
1100 11 1010 10
0111 01 1011 10
1011 10 1100 11
1101 11 1101 11
1110 11 1110 11
1111 11 1111 11
</table>
<tableCaption confidence="0.8915315">
Table 1: Values returned by the µ1 and µ2 function
defined as a composition of the α and Q functions
</tableCaption>
<subsectionHeader confidence="0.9939">
4.3 Single and Multi Stack Algorithms
</subsectionHeader>
<bodyText confidence="0.996782571428571">
The classical single and multi-stack decoding al-
gorithms can be expressed/instantiated as particular
cases of the general formalism that have been pro-
posed.
Given the input sentence fJ1 , a generalized stack
decoding algorithm with G = 0 will have the fol-
lowing features:
</bodyText>
<listItem confidence="0.9274435">
• The algorithm works with 20 = 1 stacks.
• Such a stack may store hypotheses with 2J dif-
ferent coverages. That is to say, all possible
coverages.
• The mapping function returns the same stack
identifier for each coverage
</listItem>
<bodyText confidence="0.9991995">
The previously defined algorithm has the same
features as a single stack algorithm.
Let us now consider the features of a generalized
stack algorithm with a granularity value of J:
</bodyText>
<listItem confidence="0.9155644">
• The algorithm works with 2J stacks
• Each stack may store hypotheses with only
20 = 1 coverage.
• The mapping function returns a different stack
identifier for each coverage
</listItem>
<bodyText confidence="0.9980675">
The above mentioned features characterizes the
multi-stack algorithms described in the literature.
</bodyText>
<page confidence="0.998541">
68
</page>
<table confidence="0.999305928571429">
EUTRANS-I XEROX
Spanish English Spanish English
Training Sentences 10,000 55,761
Words
Vocabulary size
Average sentence leng.
97,131 99,292 753,607 665,400
686 513 11,051 7,957
9.7 9.9 13.5 11.9
Test Sentence 2,996 1,125
Words
Perplexity (Trigrams)
35,023 35,590 10,106 8,370
– 3.62 – 48.3
</table>
<tableCaption confidence="0.99789">
Table 2: EUTRANS-I and XEROX corpus statistics
</tableCaption>
<sectionHeader confidence="0.991455" genericHeader="evaluation">
5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999953515151515">
In this section, experimental results are presented for
two well-known tasks: the EUTRANS-I (Amengual
et al., 1996), a small size and easy translation task,
and the XEROX (Cubel et al., 2004), a medium size
and difficult translation task. The main statistics of
these corpora are shown in Table 2. The translation
results were obtained using a non-monotone gener-
alized stack algorithm. For both tasks, the training
of the different phrase models was carried out us-
ing the publicly available Thot toolkit (Ortiz et al.,
2005).
Different translation experiments have been car-
ried out, varying the value of G (ranging from 0 to
8) and the maximum number of hypothesis that the
algorithm is allow to store for all used stacks (5)
(ranging from 28 to 212). In these experiments the
following statistics are computed: the average score
(or logProb) that the phrase-based translation model
assigns to each hypothesis, the translation quality
(by means of WER and Bleu measures), and the av-
erage time (in secs.) per sentence3.
In Figures 4 and 5 two plots are shown: the av-
erage time per sentence (left) and the average score
(right), for EUTRANS and XEROX corpora respec-
tively. As can be seen in both figures, the bigger the
value of G the lower the average time per sentence.
This is true up to the value of G = 6. For higher
values of G (keeping fixed the value of 5) the aver-
age time per sentence increase slightly. This is due
to the fact that at this point the algorithm start to
spend more time to decide which hypothesis is to be
expanded. With respect to the average score similar
values are obtained up to the value of G = 4. Higher
</bodyText>
<footnote confidence="0.991008">
3All the experiments have been executed on a PC with a
2.60 Ghz Intel Pentium 4 processor with 2GB of memory. All
the times are given in seconds.
</footnote>
<bodyText confidence="0.9941935">
values of G slightly decreases the average score. In
this case, as G increases, the number of hypothe-
ses per stack decreases, taking into account that the
value of 5 is fixed, then the “optimal” hypothesis
can easily be pruned.
In tables 3 and 4 detailed experiments are shown
for a value of 5 = 212 and different values of G, for
EUTRANS and XEROX corpora respectively.
</bodyText>
<table confidence="0.999402714285714">
G WER Bleu secsXsent logprob
0 6.6 0.898 2.4 -18.88
1 6.6 0.898 1.9 -18.80
2 6.6 0.897 1.7 -18.81
4 6.6 0.898 1.3 -18.77
6 6.7 0.896 1.1 -18.83
8 6.7 0.896 1.5 -18.87
</table>
<tableCaption confidence="0.996865666666667">
Table 3: Translation experiments for EUTRANS cor-
pus using a generalized stack algorithm with differ-
ent values of G and a fixed value of 5 = 212
</tableCaption>
<table confidence="0.999907">
G WER Bleu secsXsent logProb
0 32.6 0.658 35.1 -33.92
1 32.8 0.657 20.4 -33.86
2 33.1 0.656 12.8 -33.79
4 32.9 0.657 7.0 -33.70
6 33.7 0.652 6.3 -33.69
8 36.3 0.634 13.7 -34.10
</table>
<tableCaption confidence="0.901767333333333">
Table 4: Translation experiments for XEROX cor-
pus using a generalized stack algorithm with differ-
ent values of G and a fixed value of 5 = 212
</tableCaption>
<bodyText confidence="0.9372195">
According to the experiments presented here we
can conclude that:
</bodyText>
<listItem confidence="0.9931646">
• The results correlates for the two considered
tasks: one small and easy, and other larger and
difficult.
• The proposed generalized stack decoding
paradigm can be used to make a tradeoff be-
</listItem>
<page confidence="0.98948">
69
</page>
<figure confidence="0.99788672">
Avg. Score
0 1 2 3 4 5 6 7 8
G
0 1 2 3 4 5 6 7 8
G
time
2.5
0.5
1.5
2
0
1
S=512
S=1024
S=2048
S=4096
-18
-18.5
-19
-19.5
-20
S=512
S=1024
S=2048
S=4096
</figure>
<figureCaption confidence="0.99866725">
Figure 4: Average time per sentence (in secs.) and average score per sentence. The results are shown for
different values of G and 5 for the EUTRANS corpus.
Figure 5: Average time per sentence (in secs.) and average score per sentence. The results are shown for
different values of G and 5 for the XEROX corpus.
</figureCaption>
<figure confidence="0.998532892857143">
S=512
S=1024
S=2048
S=4096
0 1 2 3 4 5 6 7 8
G
0 1 2 3 4 5 6 7 8
G
time 40
35
30
25
20
15
10
5
0
Avg. Score -31
-32
-33
-34
-35
-36
-37
S=512
S=1024
S=2048
S=4096
</figure>
<bodyText confidence="0.894612">
tween the advantages of classical single and
multi-stack decoding algorithms.
</bodyText>
<listItem confidence="0.579594333333333">
• As we expected, better results (regarding effi-
ciency and accuracy) are obtained when using
a value of G between 0 and J.
</listItem>
<sectionHeader confidence="0.99007" genericHeader="conclusions">
6 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.997506208333333">
In this paper, a generalization of the stack-decoding
paradigm has been proposed. This new formalism
includes the well known single and multi-stack de-
coding algorithms and a new family of stack-based
algorithms which have not been described yet in the
literature.
Essentially, generalized stack algorithms use a pa-
rameterized number of stacks during the decoding
process, and try to assign hypotheses to stacks such
that there is ”fair competition” within each stack,
i.e., brother hypotheses should cover roughly the
same number of input words (and the same words)
if possible.
The new family of stack-based algorithms allows
a tradeoff to be made between the classical single
and multi-stack decoding algorithms. For this pur-
pose, they employ a certain number of stacks be-
tween 1 (the number of stacks used by a single stack
algorithm) and 2j (the number of stacks used by a
multiple stack algorithm to translate a sentence with
J words.)
According to the experimental results, it has been
proved that an appropriate value of G yields in a
stack decoding algorithm that outperforms (in effi-
</bodyText>
<page confidence="0.992795">
70
</page>
<bodyText confidence="0.999901333333333">
ciency and acuraccy) the single and multi-stack al-
gorithms proposed so far.
As future work, we plan to extend the experimen-
tation framework presented here to larger and more
complex tasks as HANSARDS and EUROPARL cor-
pora.
</bodyText>
<sectionHeader confidence="0.99897" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999934255813953">
J.C. Amengual, J.M. Benedi, M.A. Castao, A. Marzal,
F. Prat, E. Vidal, J.M. Vilar, C. Delogu, A. di Carlo,
H. Ney, and S. Vogel. 1996. Definition of a ma-
chine translation task and generation of corpora. Tech-
nical report d4, Instituto Tecnol´ogico de Inform´atica,
September. ESPRIT, EuTrans IT-LTR-OS-20268.
Adam L. Berger, Peter F. Brown, Stephen A. Della Pietra,
Vincent J. Della Pietra, John R. Gillett, A. S. Kehler,
and R. L. Mercer. 1996. Language translation ap-
paratus and method of using context-based translation
models. United States Patent, No. 5510981, April.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and R. L. Mercer. 1993. The mathematics of
statistical machine translation: Parameter estimation.
Computational Linguistics, 19(2):263–311.
E. Cubel, J. Civera, J. M. Vilar, A. L. Lagarda,
E. Vidal, F. Casacuberta, D. Pic´o, J. Gonz´alez, and
L. Rodriguez. 2004. Finite-state models for computer
assisted translation. In Proceedings of the 16th Euro-
pean Conference on Artificial Intelligence (ECAI04),
pages 586–590, Valencia, Spain, August. IOS Press.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2001. Fast decoding and
optimal decoding for machine translation. In Proc.
of the 39th Annual Meeting of ACL, pages 228–235,
Toulouse, France, July.
F. Jelinek. 1969. A fast sequential decoding algorithm
using a stack. IBMJournal of Research and Develop-
ment, 13:675–685.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statisti-
cal phrase-based translation. In Proceedings of the
HLT/NAACL, Edmonton, Canada, May.
Phillip Koehn. 2003. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. User manual and description. Technical report,
USC Information Science Institute, December.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proceedings of the EMNLP Conference, pages
1408–1414, Philadelphia, USA, July.
Hermann Ney, Sonja Nießen, Franz J. Och, Hassan
Sawaf, Christoph Tillmann, and Stephan Vogel. 2000.
Algorithms for statistical translation of spoken lan-
guage. IEEE Trans. on Speech and Audio Processing,
8(1):24–36, January.
Franz J. Och, Nicola Ueffing, and Hermann Ney. 2001.
An efficient A* search algorithm for statistical ma-
chine translation. In Data-Driven Machine Transla-
tion Workshop, pages 55–62, Toulouse, France, July.
Franz Joseph Och. 2002. Statistical Machine Trans-
lation: From Single-Word Models to Alignment Tem-
plates. Ph.D. thesis, Computer Science Department,
RWTH Aachen, Germany, October.
D. Ortiz, Ismael Garcia-Varea, and Francisco Casacu-
berta. 2003. An empirical comparison of stack-based
decoding algorithms for statistical machine transla-
tion. In New Advance in Computer Vision, Lec-
ture Notes in Computer Science. Springer-Verlag. 1st
Iberian Conference on Pattern Recongnition and Im-
age Analysis -IbPRIA2003- Mallorca. Spain. June.
D. Ortiz, I. Garca-Varea, and F. Casacuberta. 2005. Thot:
a toolkit to train phrase-based statistical translation
models. In Tenth Machine Translation Summit, pages
141–148, Phuket, Thailand, September.
J. Tom´as and F. Casacuberta. 2001. Monotone statistical
translation using word groups. In Procs. of the Ma-
chine Translation Summit VIII, pages 357–361, Santi-
ago de Compostela, Spain.
J. Tom´as and F. Casacuberta. 2003. Combining phrase-
based and template-based models in statistical ma-
chine translation. In Pattern Recognition and Image
Analisys, volume 2652 of LNCS, pages 1021–1031.
Springer-Verlag. 1st bPRIA.
Ye-Yi Wang and Alex Waibel. 1998. Fast decoding
for statistical machine translation. In Proc. of the
Int. Conf. on Speech and Language Processing, pages
1357–1363, Sydney, Australia, November.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proc. of the 39th
Annual Meeting of ACL, pages 523–530, Toulouse,
France, July.
R. Zens, F.J. Och, and H. Ney. 2002. Phrase-based sta-
tistical machine translation. In Advances in artificial
intelligence. 25. Annual German Conference on AI,
volume 2479 of Lecture Notes in Computer Science,
pages 18–32. Springer Verlag, September.
</reference>
<page confidence="0.999142">
71
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.814959">
<title confidence="0.993244">Stack Decoding Algorithms for Statistical Machine</title>
<author confidence="0.991156">Ismael Garcia</author>
<affiliation confidence="0.9541805">Dpto. de Univ. de Castilla-La</affiliation>
<address confidence="0.998798">02071 Albacete,</address>
<email confidence="0.967677">ivarea@info-ab.uclm.es</email>
<author confidence="0.999619">Daniel Ortiz</author>
<affiliation confidence="0.983364">Inst. Tecnol´ogico de Univ. Polit´ecnica de</affiliation>
<address confidence="0.995555">46071 Valencia,</address>
<email confidence="0.995094">dortiz@iti.upv.es</email>
<abstract confidence="0.998684428571429">In this paper we propose a generalization of the Stack-based decoding paradigm for Statistical Machine Translation. The well known single and multi-stack decoding algorithms defined in the literature have been integrated within a new formalism which also defines a new family of stackbased decoders. These decoders allows a tradeoff to be made between the advantages of using only one or multiple stacks. The key point of the new formalism consists in parameterizeing the number of stacks to be used during the decoding process, and providing an efficient method to decide in which stack each partial hypothesis generated is to be insertedduring the search process. Experimental results are also reported for a search algorithm for phrase-based statistical translation models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J C Amengual</author>
<author>J M Benedi</author>
<author>M A Castao</author>
<author>A Marzal</author>
<author>F Prat</author>
<author>E Vidal</author>
<author>J M Vilar</author>
<author>C Delogu</author>
<author>A di Carlo</author>
<author>H Ney</author>
<author>S Vogel</author>
</authors>
<title>Definition of a machine translation task and generation of corpora.</title>
<date>1996</date>
<booktitle>Technical report d4, Instituto Tecnol´ogico de Inform´atica, September. ESPRIT, EuTrans IT-LTR-OS-20268.</booktitle>
<marker>Amengual, Benedi, Castao, Marzal, Prat, Vidal, Vilar, Delogu, di Carlo, Ney, Vogel, 1996</marker>
<rawString>J.C. Amengual, J.M. Benedi, M.A. Castao, A. Marzal, F. Prat, E. Vidal, J.M. Vilar, C. Delogu, A. di Carlo, H. Ney, and S. Vogel. 1996. Definition of a machine translation task and generation of corpora. Technical report d4, Instituto Tecnol´ogico de Inform´atica, September. ESPRIT, EuTrans IT-LTR-OS-20268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>John R Gillett</author>
<author>A S Kehler</author>
<author>R L Mercer</author>
</authors>
<title>Language translation apparatus and method of using context-based translation models.</title>
<date>1996</date>
<tech>United States Patent, No. 5510981,</tech>
<contexts>
<context position="2643" citStr="Berger et al., 1996" startWordPosition="404" endWordPosition="407">sic.upv.es of both the target language model Pr(eI1) and the string translation model Pr(fJ1 |eI1) must be chosen. The equation that models this process is: eI1 = arg max{Pr(eI1) · Pr(fJ1 |eI1)} (1) el The search/decoding problem in SMT consists in solving the maximization problem stated in Eq. (1). In the literature, we can find different techniques to deal with this problem, ranging from heuristic and fast (as greedy decoders) to optimal and very slow decoding algorithms (Germann et al., 2001). Also, under certain circumstances, stack-based decoders can obtain optimal solutions. Many works (Berger et al., 1996; Wang and Waibel, 1998; Germann et al., 2001; Och et al., 2001; Ortiz et al., 2003) have adopted different types of stack-based algorithms to solve the global search optimization problem for statistical machine translation. All these works follow two main different approaches according to the number of stacks used in the design and implementation of the search algorithm (the stacks are used to store partial hypotheses, sorted according to their partial score/probability, during the search process) : • On the one hand, in (Wang and Waibel, 1998; Och et al., 2001) a single stack is used. In tha</context>
</contexts>
<marker>Berger, Brown, Pietra, Pietra, Gillett, Kehler, Mercer, 1996</marker>
<rawString>Adam L. Berger, Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, John R. Gillett, A. S. Kehler, and R. L. Mercer. 1996. Language translation apparatus and method of using context-based translation models. United States Patent, No. 5510981, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="5596" citStr="Brown et al., 1993" startWordPosition="888" endWordPosition="891">lts are shown; finally some conclusions are drawn in section 6. 2 Phrase Based Statistical Machine Translation Different translation models (TMs) have been proposed depending on how the relation between the source and the target languages is structured; that is, the way a target sentence is generated from a source sentence. This relation is summarized using the concept of alignment; that is, how the constituents (typically words or group-of-words) of a pair of sentences are aligned to each other. The most widely used single-word-based statistical alignment models (SAMs) have been proposed in (Brown et al., 1993; Ney et al., 2000). On the other hand, models that deal with structures or phrases instead of single words have also been proposed: the syntax translation models are described in (Yamada and Knight, 2001) , alignment templates are used in (Och, 2002), and the alignment template approach is re-framed into the so-called phrase based translation (PBT) in (Marcu and Wong, 2002; Zens et al., 2002; Koehn et al., 2003; Tom´as and Casacuberta, 2003). For the translation model (Pr(fJ1 |eI1)) in Eq. (1), PBT can be explained from a generative point of view as follows (Zens et al., 2002): 1. The target </context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Cubel</author>
<author>J Civera</author>
<author>J M Vilar</author>
<author>A L Lagarda</author>
<author>E Vidal</author>
<author>F Casacuberta</author>
<author>D Pic´o</author>
<author>J Gonz´alez</author>
<author>L Rodriguez</author>
</authors>
<title>Finite-state models for computer assisted translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 16th European Conference on Artificial Intelligence (ECAI04),</booktitle>
<pages>586--590</pages>
<publisher>IOS Press.</publisher>
<location>Valencia, Spain,</location>
<marker>Cubel, Civera, Vilar, Lagarda, Vidal, Casacuberta, Pic´o, Gonz´alez, Rodriguez, 2004</marker>
<rawString>E. Cubel, J. Civera, J. M. Vilar, A. L. Lagarda, E. Vidal, F. Casacuberta, D. Pic´o, J. Gonz´alez, and L. Rodriguez. 2004. Finite-state models for computer assisted translation. In Proceedings of the 16th European Conference on Artificial Intelligence (ECAI04), pages 586–590, Valencia, Spain, August. IOS Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Germann</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Kenji Yamada</author>
</authors>
<title>Fast decoding and optimal decoding for machine translation.</title>
<date>2001</date>
<booktitle>In Proc. of the 39th Annual Meeting of ACL,</booktitle>
<pages>228--235</pages>
<location>Toulouse, France,</location>
<contexts>
<context position="2524" citStr="Germann et al., 2001" startWordPosition="388" endWordPosition="391"> is known. Francisco Casacuberta Nolla Dpto. de Sist Inf. y Comp. Univ. Polit´ec. de Valencia 46071 Valencia, Spain fcn@dsic.upv.es of both the target language model Pr(eI1) and the string translation model Pr(fJ1 |eI1) must be chosen. The equation that models this process is: eI1 = arg max{Pr(eI1) · Pr(fJ1 |eI1)} (1) el The search/decoding problem in SMT consists in solving the maximization problem stated in Eq. (1). In the literature, we can find different techniques to deal with this problem, ranging from heuristic and fast (as greedy decoders) to optimal and very slow decoding algorithms (Germann et al., 2001). Also, under certain circumstances, stack-based decoders can obtain optimal solutions. Many works (Berger et al., 1996; Wang and Waibel, 1998; Germann et al., 2001; Och et al., 2001; Ortiz et al., 2003) have adopted different types of stack-based algorithms to solve the global search optimization problem for statistical machine translation. All these works follow two main different approaches according to the number of stacks used in the design and implementation of the search algorithm (the stacks are used to store partial hypotheses, sorted according to their partial score/probability, duri</context>
</contexts>
<marker>Germann, Jahr, Knight, Marcu, Yamada, 2001</marker>
<rawString>Ulrich Germann, Michael Jahr, Kevin Knight, Daniel Marcu, and Kenji Yamada. 2001. Fast decoding and optimal decoding for machine translation. In Proc. of the 39th Annual Meeting of ACL, pages 228–235, Toulouse, France, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
</authors>
<title>A fast sequential decoding algorithm using a stack. IBMJournal of Research and Development,</title>
<date>1969</date>
<pages>13--675</pages>
<contexts>
<context position="7735" citStr="Jelinek, 1969" startWordPosition="1270" endWordPosition="1271"> that the alignments must be monotonic. This led us to the following equation: �pθ(fJ1 |eI1) = α(eI1) K,˜aK In both cases the model parameters that have to be estimated are the translation probabilities between phrase pairs (θ = {p(˜f|˜e)}), which typically are estimated as follows: K H p(˜fk|˜e˜ak) (3) k=1 K H p(˜fk|˜ek) (4) k=1 ˜f|˜e) = N(˜f, ˜e) N(˜ e) p( (5) 65 where N( f|e) is the number of times that f have been seen as a translation of a within the training corpus. 3 Stack-Decoding Algorithms The stack decoding algorithm, also called A* algorithm, was first introduced by F. Jelinek in (Jelinek, 1969). The stack decoding algorithm attempts to generate partial solutions, called hypotheses, until a complete translation is found2; these hypotheses are stored in a stack and ordered by their score. Typically, this measure or score is the probability of the product of the translation and the language models introduced above. The A* decoder follows a sequence of steps for achieving a complete (and possibly optimal) hypothesis: 1. Initialize the stack with an empty hypothesis. 2. Iterate (a) Pop h (the best hypothesis) off the stack. (b) If h is a complete sentence, output h and terminate. (c) Exp</context>
</contexts>
<marker>Jelinek, 1969</marker>
<rawString>F. Jelinek. 1969. A fast sequential decoding algorithm using a stack. IBMJournal of Research and Development, 13:675–685.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT/NAACL,</booktitle>
<location>Edmonton, Canada,</location>
<contexts>
<context position="6011" citStr="Koehn et al., 2003" startWordPosition="958" endWordPosition="961"> (typically words or group-of-words) of a pair of sentences are aligned to each other. The most widely used single-word-based statistical alignment models (SAMs) have been proposed in (Brown et al., 1993; Ney et al., 2000). On the other hand, models that deal with structures or phrases instead of single words have also been proposed: the syntax translation models are described in (Yamada and Knight, 2001) , alignment templates are used in (Och, 2002), and the alignment template approach is re-framed into the so-called phrase based translation (PBT) in (Marcu and Wong, 2002; Zens et al., 2002; Koehn et al., 2003; Tom´as and Casacuberta, 2003). For the translation model (Pr(fJ1 |eI1)) in Eq. (1), PBT can be explained from a generative point of view as follows (Zens et al., 2002): 1. The target sentence eI1 is segmented into K phrases (˜eK1 ). 2. Each target phrase ˜ek is translated into a source phrase ˜f. 3. Finally, the source phrases are reordered in order to compose the source sentence ˜fK1 = fJ1 . In PBT, it is assumed that the relations between the words of the source and target sentences can be explained by means of the hidden variable ˜aK1 , which contains all the decisions made during the gen</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-based translation. In Proceedings of the HLT/NAACL, Edmonton, Canada, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phillip Koehn</author>
</authors>
<title>Pharaoh: a beam search decoder for phrase-based statistical machine translation models. User manual and description.</title>
<date>2003</date>
<tech>Technical report,</tech>
<institution>USC Information Science Institute,</institution>
<contexts>
<context position="11160" citStr="Koehn, 2003" startWordPosition="1855" endWordPosition="1856">nsion process that the multi-stack algorithms execute, which is slightly different than the one presented in Figure 1. Multi-stack algorithms have the negative property of spending significant amounts of time in selecting the hypotheses to be expanded, since at each iteration, the best hypothesis in a set of 2J stacks must be searched for (Ortiz et al., 2003). By contrast, for the A* algorithm, it is not possible to reduce the length of the stack in the same way as in the multi-stack case without loss of translation quality. Additionally, certain translation systems, e.g. the Pharaoh decoder (Koehn, 2003) use an alternative 66 Figure 2: Flow chart associated to the expansion of a hypothesis when using a multi-stack algorithm. approach which consists in assigning to the same stack, those hypotheses with the same number of source words covered. 4 Generalized Stack-Decoding Algorithms As was mentioned in the previous section, given a sentence fJ1 to be translated, a single stack decoding algorithm employs only one stack to perform the translation process, while a multi-stack algorithm employs 2J stacks. We propose a possible way to make a tradeoff between the advantages of both algorithms that in</context>
</contexts>
<marker>Koehn, 2003</marker>
<rawString>Phillip Koehn. 2003. Pharaoh: a beam search decoder for phrase-based statistical machine translation models. User manual and description. Technical report, USC Information Science Institute, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>William Wong</author>
</authors>
<title>A phrase-based, joint probability model for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the EMNLP Conference,</booktitle>
<pages>1408--1414</pages>
<location>Philadelphia, USA,</location>
<contexts>
<context position="5972" citStr="Marcu and Wong, 2002" startWordPosition="950" endWordPosition="953"> alignment; that is, how the constituents (typically words or group-of-words) of a pair of sentences are aligned to each other. The most widely used single-word-based statistical alignment models (SAMs) have been proposed in (Brown et al., 1993; Ney et al., 2000). On the other hand, models that deal with structures or phrases instead of single words have also been proposed: the syntax translation models are described in (Yamada and Knight, 2001) , alignment templates are used in (Och, 2002), and the alignment template approach is re-framed into the so-called phrase based translation (PBT) in (Marcu and Wong, 2002; Zens et al., 2002; Koehn et al., 2003; Tom´as and Casacuberta, 2003). For the translation model (Pr(fJ1 |eI1)) in Eq. (1), PBT can be explained from a generative point of view as follows (Zens et al., 2002): 1. The target sentence eI1 is segmented into K phrases (˜eK1 ). 2. Each target phrase ˜ek is translated into a source phrase ˜f. 3. Finally, the source phrases are reordered in order to compose the source sentence ˜fK1 = fJ1 . In PBT, it is assumed that the relations between the words of the source and target sentences can be explained by means of the hidden variable ˜aK1 , which contain</context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>Daniel Marcu and William Wong. 2002. A phrase-based, joint probability model for statistical machine translation. In Proceedings of the EMNLP Conference, pages 1408–1414, Philadelphia, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hermann Ney</author>
<author>Sonja Nießen</author>
<author>Franz J Och</author>
<author>Hassan Sawaf</author>
<author>Christoph Tillmann</author>
<author>Stephan Vogel</author>
</authors>
<title>Algorithms for statistical translation of spoken language.</title>
<date>2000</date>
<journal>IEEE Trans. on Speech and Audio Processing,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="5615" citStr="Ney et al., 2000" startWordPosition="892" endWordPosition="895">ly some conclusions are drawn in section 6. 2 Phrase Based Statistical Machine Translation Different translation models (TMs) have been proposed depending on how the relation between the source and the target languages is structured; that is, the way a target sentence is generated from a source sentence. This relation is summarized using the concept of alignment; that is, how the constituents (typically words or group-of-words) of a pair of sentences are aligned to each other. The most widely used single-word-based statistical alignment models (SAMs) have been proposed in (Brown et al., 1993; Ney et al., 2000). On the other hand, models that deal with structures or phrases instead of single words have also been proposed: the syntax translation models are described in (Yamada and Knight, 2001) , alignment templates are used in (Och, 2002), and the alignment template approach is re-framed into the so-called phrase based translation (PBT) in (Marcu and Wong, 2002; Zens et al., 2002; Koehn et al., 2003; Tom´as and Casacuberta, 2003). For the translation model (Pr(fJ1 |eI1)) in Eq. (1), PBT can be explained from a generative point of view as follows (Zens et al., 2002): 1. The target sentence eI1 is seg</context>
</contexts>
<marker>Ney, Nießen, Och, Sawaf, Tillmann, Vogel, 2000</marker>
<rawString>Hermann Ney, Sonja Nießen, Franz J. Och, Hassan Sawaf, Christoph Tillmann, and Stephan Vogel. 2000. Algorithms for statistical translation of spoken language. IEEE Trans. on Speech and Audio Processing, 8(1):24–36, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>An efficient A* search algorithm for statistical machine translation.</title>
<date>2001</date>
<booktitle>In Data-Driven Machine Translation Workshop,</booktitle>
<pages>55--62</pages>
<location>Toulouse, France,</location>
<contexts>
<context position="2706" citStr="Och et al., 2001" startWordPosition="416" endWordPosition="419">g translation model Pr(fJ1 |eI1) must be chosen. The equation that models this process is: eI1 = arg max{Pr(eI1) · Pr(fJ1 |eI1)} (1) el The search/decoding problem in SMT consists in solving the maximization problem stated in Eq. (1). In the literature, we can find different techniques to deal with this problem, ranging from heuristic and fast (as greedy decoders) to optimal and very slow decoding algorithms (Germann et al., 2001). Also, under certain circumstances, stack-based decoders can obtain optimal solutions. Many works (Berger et al., 1996; Wang and Waibel, 1998; Germann et al., 2001; Och et al., 2001; Ortiz et al., 2003) have adopted different types of stack-based algorithms to solve the global search optimization problem for statistical machine translation. All these works follow two main different approaches according to the number of stacks used in the design and implementation of the search algorithm (the stacks are used to store partial hypotheses, sorted according to their partial score/probability, during the search process) : • On the one hand, in (Wang and Waibel, 1998; Och et al., 2001) a single stack is used. In that case, in order to make the search feasible, the pruning of th</context>
</contexts>
<marker>Och, Ueffing, Ney, 2001</marker>
<rawString>Franz J. Och, Nicola Ueffing, and Hermann Ney. 2001. An efficient A* search algorithm for statistical machine translation. In Data-Driven Machine Translation Workshop, pages 55–62, Toulouse, France, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Joseph Och</author>
</authors>
<title>Statistical Machine Translation: From Single-Word Models to Alignment Templates.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>Computer Science Department, RWTH</institution>
<location>Aachen, Germany,</location>
<contexts>
<context position="5847" citStr="Och, 2002" startWordPosition="933" endWordPosition="934">is, the way a target sentence is generated from a source sentence. This relation is summarized using the concept of alignment; that is, how the constituents (typically words or group-of-words) of a pair of sentences are aligned to each other. The most widely used single-word-based statistical alignment models (SAMs) have been proposed in (Brown et al., 1993; Ney et al., 2000). On the other hand, models that deal with structures or phrases instead of single words have also been proposed: the syntax translation models are described in (Yamada and Knight, 2001) , alignment templates are used in (Och, 2002), and the alignment template approach is re-framed into the so-called phrase based translation (PBT) in (Marcu and Wong, 2002; Zens et al., 2002; Koehn et al., 2003; Tom´as and Casacuberta, 2003). For the translation model (Pr(fJ1 |eI1)) in Eq. (1), PBT can be explained from a generative point of view as follows (Zens et al., 2002): 1. The target sentence eI1 is segmented into K phrases (˜eK1 ). 2. Each target phrase ˜ek is translated into a source phrase ˜f. 3. Finally, the source phrases are reordered in order to compose the source sentence ˜fK1 = fJ1 . In PBT, it is assumed that the relatio</context>
</contexts>
<marker>Och, 2002</marker>
<rawString>Franz Joseph Och. 2002. Statistical Machine Translation: From Single-Word Models to Alignment Templates. Ph.D. thesis, Computer Science Department, RWTH Aachen, Germany, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ortiz</author>
<author>Ismael Garcia-Varea</author>
<author>Francisco Casacuberta</author>
</authors>
<title>An empirical comparison of stack-based decoding algorithms for statistical machine translation.</title>
<date>2003</date>
<booktitle>In New Advance in Computer Vision, Lecture Notes in Computer Science. Springer-Verlag. 1st Iberian Conference on Pattern Recongnition and Image Analysis -IbPRIA2003- Mallorca.</booktitle>
<contexts>
<context position="2727" citStr="Ortiz et al., 2003" startWordPosition="420" endWordPosition="423">l Pr(fJ1 |eI1) must be chosen. The equation that models this process is: eI1 = arg max{Pr(eI1) · Pr(fJ1 |eI1)} (1) el The search/decoding problem in SMT consists in solving the maximization problem stated in Eq. (1). In the literature, we can find different techniques to deal with this problem, ranging from heuristic and fast (as greedy decoders) to optimal and very slow decoding algorithms (Germann et al., 2001). Also, under certain circumstances, stack-based decoders can obtain optimal solutions. Many works (Berger et al., 1996; Wang and Waibel, 1998; Germann et al., 2001; Och et al., 2001; Ortiz et al., 2003) have adopted different types of stack-based algorithms to solve the global search optimization problem for statistical machine translation. All these works follow two main different approaches according to the number of stacks used in the design and implementation of the search algorithm (the stacks are used to store partial hypotheses, sorted according to their partial score/probability, during the search process) : • On the one hand, in (Wang and Waibel, 1998; Och et al., 2001) a single stack is used. In that case, in order to make the search feasible, the pruning of the number of partial h</context>
<context position="4172" citStr="Ortiz et al., 2003" startWordPosition="658" endWordPosition="661">covered words the higher possibility to be pruned. • On the other hand (Berger et al., 1996; Germann et al., 2001) make use of multiple stacks 64 Proceedings of the Workshop on Statistical Machine Translation, pages 64–71, New York City, June 2006. c�2006 Association for Computational Linguistics (one for each set of source covered/translated words in the partial hypothesis) in order to solve the disadvantages of the single-stack approach. By contrast, the problem of finding the best hypothesis to be expanded introduces an exponential term in the computational complexity of the algorithm. In (Ortiz et al., 2003) the authors present an empirical comparison (about efficiency and translation quality) of the two approaches paying special attention to the advantages and disadvantages of the two approaches. In this paper we present a new formalism consisting of a generalization of the classical stack-based decoding paradigm for SMT. This new formalism defines a new family of stack-based decoders, which also integrates the well known stack-based decoding algorithms proposed so far within the framework of SMT, that is single and multi-stack decoders. The rest of the paper is organized as follows: in section </context>
<context position="10909" citStr="Ortiz et al., 2003" startWordPosition="1812" endWordPosition="1815"> with the same number of covered positions can compete with each other. All the search steps given for A* algorithm can also be applied here, except step 2a. This is due to the fact that multiple stacks are used instead of only one. Figure 2 depicts the expansion process that the multi-stack algorithms execute, which is slightly different than the one presented in Figure 1. Multi-stack algorithms have the negative property of spending significant amounts of time in selecting the hypotheses to be expanded, since at each iteration, the best hypothesis in a set of 2J stacks must be searched for (Ortiz et al., 2003). By contrast, for the A* algorithm, it is not possible to reduce the length of the stack in the same way as in the multi-stack case without loss of translation quality. Additionally, certain translation systems, e.g. the Pharaoh decoder (Koehn, 2003) use an alternative 66 Figure 2: Flow chart associated to the expansion of a hypothesis when using a multi-stack algorithm. approach which consists in assigning to the same stack, those hypotheses with the same number of source words covered. 4 Generalized Stack-Decoding Algorithms As was mentioned in the previous section, given a sentence fJ1 to </context>
</contexts>
<marker>Ortiz, Garcia-Varea, Casacuberta, 2003</marker>
<rawString>D. Ortiz, Ismael Garcia-Varea, and Francisco Casacuberta. 2003. An empirical comparison of stack-based decoding algorithms for statistical machine translation. In New Advance in Computer Vision, Lecture Notes in Computer Science. Springer-Verlag. 1st Iberian Conference on Pattern Recongnition and Image Analysis -IbPRIA2003- Mallorca. Spain. June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ortiz</author>
<author>I Garca-Varea</author>
<author>F Casacuberta</author>
</authors>
<title>Thot: a toolkit to train phrase-based statistical translation models.</title>
<date>2005</date>
<booktitle>In Tenth Machine Translation Summit,</booktitle>
<pages>141--148</pages>
<location>Phuket, Thailand,</location>
<contexts>
<context position="18044" citStr="Ortiz et al., 2005" startWordPosition="3061" endWordPosition="3064">6 8,370 – 3.62 – 48.3 Table 2: EUTRANS-I and XEROX corpus statistics 5 Experiments and Results In this section, experimental results are presented for two well-known tasks: the EUTRANS-I (Amengual et al., 1996), a small size and easy translation task, and the XEROX (Cubel et al., 2004), a medium size and difficult translation task. The main statistics of these corpora are shown in Table 2. The translation results were obtained using a non-monotone generalized stack algorithm. For both tasks, the training of the different phrase models was carried out using the publicly available Thot toolkit (Ortiz et al., 2005). Different translation experiments have been carried out, varying the value of G (ranging from 0 to 8) and the maximum number of hypothesis that the algorithm is allow to store for all used stacks (5) (ranging from 28 to 212). In these experiments the following statistics are computed: the average score (or logProb) that the phrase-based translation model assigns to each hypothesis, the translation quality (by means of WER and Bleu measures), and the average time (in secs.) per sentence3. In Figures 4 and 5 two plots are shown: the average time per sentence (left) and the average score (right</context>
</contexts>
<marker>Ortiz, Garca-Varea, Casacuberta, 2005</marker>
<rawString>D. Ortiz, I. Garca-Varea, and F. Casacuberta. 2005. Thot: a toolkit to train phrase-based statistical translation models. In Tenth Machine Translation Summit, pages 141–148, Phuket, Thailand, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tom´as</author>
<author>F Casacuberta</author>
</authors>
<title>Monotone statistical translation using word groups.</title>
<date>2001</date>
<booktitle>In Procs. of the Machine Translation Summit VIII,</booktitle>
<pages>357--361</pages>
<location>Santiago de Compostela,</location>
<marker>Tom´as, Casacuberta, 2001</marker>
<rawString>J. Tom´as and F. Casacuberta. 2001. Monotone statistical translation using word groups. In Procs. of the Machine Translation Summit VIII, pages 357–361, Santiago de Compostela, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tom´as</author>
<author>F Casacuberta</author>
</authors>
<title>Combining phrasebased and template-based models in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Pattern Recognition and Image Analisys,</booktitle>
<volume>2652</volume>
<pages>1021--1031</pages>
<note>Springer-Verlag. 1st bPRIA.</note>
<marker>Tom´as, Casacuberta, 2003</marker>
<rawString>J. Tom´as and F. Casacuberta. 2003. Combining phrasebased and template-based models in statistical machine translation. In Pattern Recognition and Image Analisys, volume 2652 of LNCS, pages 1021–1031. Springer-Verlag. 1st bPRIA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ye-Yi Wang</author>
<author>Alex Waibel</author>
</authors>
<title>Fast decoding for statistical machine translation.</title>
<date>1998</date>
<booktitle>In Proc. of the Int. Conf. on Speech and Language Processing,</booktitle>
<pages>1357--1363</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="2666" citStr="Wang and Waibel, 1998" startWordPosition="408" endWordPosition="411">e target language model Pr(eI1) and the string translation model Pr(fJ1 |eI1) must be chosen. The equation that models this process is: eI1 = arg max{Pr(eI1) · Pr(fJ1 |eI1)} (1) el The search/decoding problem in SMT consists in solving the maximization problem stated in Eq. (1). In the literature, we can find different techniques to deal with this problem, ranging from heuristic and fast (as greedy decoders) to optimal and very slow decoding algorithms (Germann et al., 2001). Also, under certain circumstances, stack-based decoders can obtain optimal solutions. Many works (Berger et al., 1996; Wang and Waibel, 1998; Germann et al., 2001; Och et al., 2001; Ortiz et al., 2003) have adopted different types of stack-based algorithms to solve the global search optimization problem for statistical machine translation. All these works follow two main different approaches according to the number of stacks used in the design and implementation of the search algorithm (the stacks are used to store partial hypotheses, sorted according to their partial score/probability, during the search process) : • On the one hand, in (Wang and Waibel, 1998; Och et al., 2001) a single stack is used. In that case, in order to mak</context>
</contexts>
<marker>Wang, Waibel, 1998</marker>
<rawString>Ye-Yi Wang and Alex Waibel. 1998. Fast decoding for statistical machine translation. In Proc. of the Int. Conf. on Speech and Language Processing, pages 1357–1363, Sydney, Australia, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntaxbased statistical translation model.</title>
<date>2001</date>
<booktitle>In Proc. of the 39th Annual Meeting of ACL,</booktitle>
<pages>523--530</pages>
<location>Toulouse, France,</location>
<contexts>
<context position="5801" citStr="Yamada and Knight, 2001" startWordPosition="923" endWordPosition="926">een the source and the target languages is structured; that is, the way a target sentence is generated from a source sentence. This relation is summarized using the concept of alignment; that is, how the constituents (typically words or group-of-words) of a pair of sentences are aligned to each other. The most widely used single-word-based statistical alignment models (SAMs) have been proposed in (Brown et al., 1993; Ney et al., 2000). On the other hand, models that deal with structures or phrases instead of single words have also been proposed: the syntax translation models are described in (Yamada and Knight, 2001) , alignment templates are used in (Och, 2002), and the alignment template approach is re-framed into the so-called phrase based translation (PBT) in (Marcu and Wong, 2002; Zens et al., 2002; Koehn et al., 2003; Tom´as and Casacuberta, 2003). For the translation model (Pr(fJ1 |eI1)) in Eq. (1), PBT can be explained from a generative point of view as follows (Zens et al., 2002): 1. The target sentence eI1 is segmented into K phrases (˜eK1 ). 2. Each target phrase ˜ek is translated into a source phrase ˜f. 3. Finally, the source phrases are reordered in order to compose the source sentence ˜fK1 </context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntaxbased statistical translation model. In Proc. of the 39th Annual Meeting of ACL, pages 523–530, Toulouse, France, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zens</author>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Phrase-based statistical machine translation.</title>
<date>2002</date>
<booktitle>In Advances in artificial intelligence. 25. Annual German Conference on AI,</booktitle>
<volume>2479</volume>
<pages>18--32</pages>
<publisher>Springer Verlag,</publisher>
<contexts>
<context position="5991" citStr="Zens et al., 2002" startWordPosition="954" endWordPosition="957">ow the constituents (typically words or group-of-words) of a pair of sentences are aligned to each other. The most widely used single-word-based statistical alignment models (SAMs) have been proposed in (Brown et al., 1993; Ney et al., 2000). On the other hand, models that deal with structures or phrases instead of single words have also been proposed: the syntax translation models are described in (Yamada and Knight, 2001) , alignment templates are used in (Och, 2002), and the alignment template approach is re-framed into the so-called phrase based translation (PBT) in (Marcu and Wong, 2002; Zens et al., 2002; Koehn et al., 2003; Tom´as and Casacuberta, 2003). For the translation model (Pr(fJ1 |eI1)) in Eq. (1), PBT can be explained from a generative point of view as follows (Zens et al., 2002): 1. The target sentence eI1 is segmented into K phrases (˜eK1 ). 2. Each target phrase ˜ek is translated into a source phrase ˜f. 3. Finally, the source phrases are reordered in order to compose the source sentence ˜fK1 = fJ1 . In PBT, it is assumed that the relations between the words of the source and target sentences can be explained by means of the hidden variable ˜aK1 , which contains all the decisions</context>
</contexts>
<marker>Zens, Och, Ney, 2002</marker>
<rawString>R. Zens, F.J. Och, and H. Ney. 2002. Phrase-based statistical machine translation. In Advances in artificial intelligence. 25. Annual German Conference on AI, volume 2479 of Lecture Notes in Computer Science, pages 18–32. Springer Verlag, September.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>