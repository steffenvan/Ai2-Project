<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014870">
<title confidence="0.98663">
Semi-Automated Named Entity Annotation
</title>
<author confidence="0.989664">
Kuzman Ganchev and Fernando Pereira Mark Mandel
</author>
<affiliation confidence="0.996853">
Computer and Information Science, Linguistic Data Consortium,
University of Pennsylvania, University of Pennsylvania, Philadelphia PA
</affiliation>
<title confidence="0.358818">
Philadelphia PA mamandel@ldc.upenn.edu
I kuzman and pereira } @cis.upenn.edu
</title>
<author confidence="0.998625">
Steven Carroll and Peter White
</author>
<affiliation confidence="0.998329">
Division of Oncology, Children’s Hospital of Philadelphia Philadelphia PA
</affiliation>
<email confidence="0.977078">
Icarroll and white }@genome.chop.edu
</email>
<sectionHeader confidence="0.995152" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999919272727273">
We investigate a way to partially automate
corpus annotation for named entity recogni-
tion, by requiring only binary decisions from
an annotator. Our approach is based on a lin-
ear sequence model trained using a k-best
MIRA learning algorithm. We ask an an-
notator to decide whether each mention pro-
duced by a high recall tagger is a true men-
tion or a false positive. We conclude that our
approach can reduce the effort of extending
a seed training corpus by up to 58%.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998798">
Semi-automated text annotation has been the subject
of several previous studies. Typically, a human an-
notator corrects the output of an automatic system.
The idea behind our approach is to start annota-
tion manually and to partially automate the process
in the later stages. We assume that some data has
already been manually tagged and use it to train a
tagger specifically for high recall. We then run this
tagger on the rest of our corpus and ask an annotator
to filter the list of suggested gene names.
The rest of this paper is organized as follows. Sec-
tion 2 describes the model and learning algorithm.
Section 3 relates our approach to previous work.
Section 4 describes our experiments and Section 5
concludes the paper.
</bodyText>
<sectionHeader confidence="0.99427" genericHeader="introduction">
2 Methods
</sectionHeader>
<bodyText confidence="0.860474">
Throughout this work, we use a linear sequence
model. This class of models includes popular tag-
ging models for named entities such as conditional
</bodyText>
<page confidence="0.982892">
53
</page>
<bodyText confidence="0.9997072">
random fields, maximum entropy Markov models
and max-margin Markov networks. Linear sequence
models score possible tag sequences for a given in-
put as the dot product between a learned weight vec-
tor and a feature vector derived from the input and
proposed tas sequence. Linear sequence models dif-
fer principally on how the weight vector is learned.
Our experiments use the MIRA algorithm (Cram-
mer et al., 2006; McDonald et al., 2005) to learn
the weight vector.
</bodyText>
<subsectionHeader confidence="0.949981">
2.1 Notation
</subsectionHeader>
<bodyText confidence="0.999653235294118">
In what follows, x denotes the generic input sen-
tence, Y (x) the set of possible labelings of x, and
Y +(x) the set of correct labelings of x. There is
also a distinguished “gold” labeling y(x) E Y +(x).
For each pair of a sentence x and labeling y E
Y (x), we compute a vector-valued feature represen-
tation f(x, y). Given a weight vector w, the score
w · f(x, y) ranks possible labelings of x, and we de-
note by Yk�,,,(x) the set of k top scoring labelings for
x.
We use the standard B,I,O encoding for named
entities (Ramshaw and Marcus, 1995). Thus Y (x)
for x of length n is the set of all sequences of length
n matching the regular expression (O|(BI*))*. In a
linear sequence model, for suitable feature functions
f, Yk�,,,(x) can be computed efficiently with Viterbi
decoding.
</bodyText>
<subsectionHeader confidence="0.999822">
2.2 k-best MIRA and Loss Functions
</subsectionHeader>
<bodyText confidence="0.994259153846154">
The learning portion of our method finds a weight
vector w that scores the correct labelings of the test
data higher than incorrect labelings. We used a k-
Proceedings of the Linguistic Annotation Workshop, pages 53–56,
Prague, June 2007. c�2007 Association for Computational Linguistics
best version of the MIRA algorithm (Crammer et
al., 2006; McDonald et al., 2005). This is an online
learning algorithm that starts with a zero weight vec-
tor and for each training sentence makes the small-
est possible update that would score the correct la-
bel higher than the old top k labels. That is, for each
training sentence x we update the weight vector w
according to the rule:
</bodyText>
<equation confidence="0.990786666666667">
wnew = arg minw kw − woldk
s. t. w · f(x, y(x)) − w · f(x, y) ≥ L(Y +(x), y)
∀y ∈ Yk,wold(x)
</equation>
<bodyText confidence="0.9922839">
where L(Y +(x), y) is the loss, which measures the
errors in labeling y relative to the set of correct la-
belings Y +(x).
An advantage of the MIRA algorithm (over many
other learning algorithms such as conditional ran-
dom fields) is that it allows the use of arbitrary loss
functions. For our experiments, the loss of a label-
ing is a weighted combination of the number of false
positive mentions and the number of false negative
mentions in that labeling.
</bodyText>
<subsectionHeader confidence="0.996198">
2.3 Semi-Automated Tagging
</subsectionHeader>
<bodyText confidence="0.999983923076923">
For our semi-automated annotation experiments, we
imagine the following scenario: We have already an-
notated half of our training corpus and want to anno-
tate the remaining half. The goal is to save annotator
effort by using a semi-automated approach instead
of annotating the rest entirely manually.
In particular we investigate the following method:
train a high-recall named entity tagger on the anno-
tated data and use that to tag the remaining corpus.
Now ask a human annotator to filter the resulting
mentions. The mentions rejected by the annotator
are simply dropped from the annotation, leaving the
remaining mentions.
</bodyText>
<sectionHeader confidence="0.971318" genericHeader="method">
3 Relation to Previous Work
</sectionHeader>
<bodyText confidence="0.99995">
This section relates our approach to previous work
on semi-automated approaches. First we discuss
how semi-automated annotation is different from ac-
tive learning and then discuss some previous semi-
automated annotation work.
</bodyText>
<subsectionHeader confidence="0.994772">
3.1 Semi-Automated versus Active Learning
</subsectionHeader>
<bodyText confidence="0.99984075">
It is important not to confuse semi-automated anno-
tation with active learning. While they both attempt
to alleviate the burden of creating an annotated cor-
pus, they do so in a completely orthogonal manner.
Active learning tries to select which instances should
be labeled in order to make the most impact on learn-
ing. Semi-automated annotation tries to make the
annotation of each instance faster or easier. In par-
ticular, it is possible to combine active learning and
semi-automated annotation by using an active learn-
ing method to select which sentences to label and
then using a semi-automated labeling method.
</bodyText>
<subsectionHeader confidence="0.5539215">
3.2 Previous work on semi-automated
annotation
</subsectionHeader>
<bodyText confidence="0.999984105263158">
The most common approach to semi-automatic an-
notation is to automatically tag an instance and then
ask an annotator to correct the results. We restrict
our discussion to this paradigm due to space con-
straints. Marcus et al. (1994), Chiou et al. (2001)
and Xue et al. (2002) apply this approach with some
minor modifications to part of speech tagging and
phrase structure parsing. The automatic system of
Marcus et al. only produces partial parses that are
then assembled by the annotators, while Chiou et al.
modified their automatic parser specifically for use
in annotation. Chou et al. (2006) use this tag and
correct approach to create a corpus of predicate ar-
gument structures in the biomedical domain. Culota
et al. (2006) use a refinement of the tag and correct
approach to extract addressbook information from e-
mail messages. They modify the system’s best guess
as the user makes corrections, resulting in less anno-
tation actions.
</bodyText>
<sectionHeader confidence="0.999743" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9999872">
We now evaluate to what extent our semi-automated
annotation framework can be useful, and how much
effort it requires. For both questions we compare
semi-automatic to fully manual annotation. In our
first set of experiments, we measured the usefulness
of semi-automatically annotated corpora for training
a gene mention tagger. In the second set of exper-
iments, we measured the annotation effort for gene
mentions with the standard fully manual method and
with the semi-automated methods.
</bodyText>
<subsectionHeader confidence="0.991405">
4.1 Measuring Effectiveness
</subsectionHeader>
<bodyText confidence="0.9974745">
The experiments in this section use the training data
from the the Biocreative II competition (Tanabe et
</bodyText>
<page confidence="0.997686">
54
</page>
<note confidence="0.9767875">
Sentence Expression of SREBP-1a stimulated StAR promoter activity in the context of COS-1 cells
gold label Expression of SREBP-1a stimulated StAR promoter activity in ...
alternative Expression of SREBP-1a stimulated StAR promoter activity in ...
alternative Expression of SREBP-1a stimulated StAR promoter activity in ...
</note>
<figureCaption confidence="0.750915">
Figure 1: An example sentence and its annotation in Biocreative II. The evaluation metric would give full
credit for guessing one of the alternative labels rather than the “gold” label.
al., 2005). The data is supplied as a set of sentences
chosen randomly from MEDLINE and annotated for
gene mentions.
</figureCaption>
<bodyText confidence="0.998953888888889">
Each sentence in the corpus is provided as a list of
“gold” gene mentions as well as a set of alternatives
for each mention. The alternatives are generated by
the annotators and count as true positives. Figure 1
shows an example sentence with its gold and alter-
native mentions. The evaluation metric for these ex-
periments is F-score augmented with the possibility
of alternatives (Yeh et al., 2005).
We used 5992 sentences as the data that has al-
ready been annotated manually (set Data-1), and
simulated different ways of annotating the remain-
ing 5982 sentences (set Data-2). We compare the
quality of annotation by testing taggers trained us-
ing these corpora on a 1493 sentence test set.
We trained a high-recall tagger (recall of 89.6%)
on Data-1, and ran it on Data-2. Since we have
labels available for Data-2, we simulated an anno-
tator filtering these proposed mentions by accepting
them only if they exactly match a “gold” or alterna-
tive mention. This gave us an F-score of 94.7% on
Data-2 and required 9981 binary decisions.
Figure 2 shows F1 score as a function of the num-
ber of extra sentences annotated. Without any ad-
ditional data, the F-measure of the tagger is 81.0%.
The two curves correspond to annotation with and
without alternatives. The horizontal line at 82.8%
shows the level achieved by the semi-automatic
method (when using all of Data-2).
From the figure, we can see that to get compa-
rable performance to the semi-automatic approach,
we need to fully manually annotate roughly a third
as much data with alternatives, or about two thirds as
much data without alternatives. The following sec-
tion examines what this means in terms of annotator
time by providing timing results for semi-automatic
and fully-manual annotation without alternatives.
</bodyText>
<figure confidence="0.9920535">
0 1000 2000 3000
Extra Annotated Sentences
</figure>
<figureCaption confidence="0.8132515">
Figure 2: Effect of the number of annotated in-
stances on F1 score. In all cases the original 5992
</figureCaption>
<bodyText confidence="0.963910666666667">
instances were used; the curves show manual an-
notation while the level line is the semi-automatic
method. The curves are averages over 3 trials.
</bodyText>
<subsectionHeader confidence="0.999361">
4.2 Measuring Effort
</subsectionHeader>
<bodyText confidence="0.999272944444444">
The second set of experiments compares annotator
effort between fully manual and semi-automatic an-
notation. Because we did not have access to an expe-
rienced annotator from the Biocreative project, and
gene mention annotations vary subtly among anno-
tation efforts, we evaluated annotator effort on on the
PennBioIE named entity corpus.1 Furthermore, we
have not yet annotated enough data locally to per-
form both effectiveness and effort experiments on
the local corpus alone. However, both corpora an-
notate gene mentions in MEDLINE abstracts, so we
expect that the timing results will not be significantly
different.
We asked an experienced annotator to tag 194
MEDLINE abstracts: 96 manually and 98 using the
semi-automated method. Manual annotation was
done using annotation software familiar to the an-
notator. Semi-automatic annotation was done with a
</bodyText>
<footnote confidence="0.981141">
1Available from http://bioie.ldc.upenn.edu/
</footnote>
<figure confidence="0.969144">
84.5
83.5
82.5
81.5
85
84
83
82
81
Manua
Manu
Semi-Automati
</figure>
<page confidence="0.99515">
55
</page>
<bodyText confidence="0.98935940625">
Web-based tool developed for the task. The new tool
highlights potential gene mentions in the text and al-
lows the annotator to filter them with a mouse click.
The annotator had been involved in the creation of
the local manually annotated corpus, and had a lot of
experience annotating named entities. The abstracts
for annotation were selected randomly so that they
did not contain any abstracts tagged earlier. There-
fore, we did not expect the annotator to have seen
any of them before the experiment.
To generate potential gene mentions for the semi-
automated annotation, we ran two taggers on the
data: a high recall tagger trained on the local corpus
and a high recall tagger trained on the Biocreative
corpus. At decode time, we took the gene mentions
from the top two predictions of each of these taggers
whenever there were any gene mentions predicted.
As a result, the annotator had to make more binary
decisions per sentence than they would have for ei-
ther training corpus alone. For the semi-automated
annotation, the annotator had to examine 682 sen-
tences and took on average 10 seconds per sentence.
For the fully-manual annotation, they examined 667
sentences and took 40 seconds per sentence on av-
erage. We did not ask the annotator to tag alterna-
tives because they did not have any experience with
tagging alternatives and we do not have a tool that
makes the annotation of alternatives easy. Conse-
quently, effort totals for annotation with alternatives
would have been skewed in our favor. The four-fold
speedup should be compared to the lower curve in
Figure 2.
</bodyText>
<sectionHeader confidence="0.994833" genericHeader="conclusions">
5 Discussion and Further Work
</sectionHeader>
<bodyText confidence="0.99999647826087">
We can use the effort results to estimate the relative
effort of annotating without alternatives and of semi-
automated annotation. To obtain the same improve-
ment in F-score, we need to semi-automatically an-
notate roughly a factor of 1.67 more data than using
the fully manual approach. Multiplying that by the
0.25 factor reduction in annotation time, we get that
the time required for a comparable improvement in
F-score is 0.42 times as long – a 58% reduction in
annotator time.
We do not have any experiments on annotating
alternatives, but the main difference between semi-
automated and fully-manual annotation is that the
former does not require the annotator to decide on
boundaries. Consequently, we expect that annota-
tion with alternatives will be considerably more ex-
pensive than without alternatives, since more bound-
aries have to be outlined.
In future work, it would be interesting to compare
this approach to the traditional approach of manually
correcting output of a system. Due to constraints
on annotator time, it was not possible to do these
experiments as part of the current work.
</bodyText>
<sectionHeader confidence="0.999178" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999873243243243">
Fu-Dong Chiou, David Chiang, and Martha Palmer.
2001. Facilitating treebank annotation using a statisti-
cal parser. In HLT ’01. ACL.
Wen-Chi Chou, Richard Tzong-Han Tsai, Ying-Shan Su,
Wei Ku, Ting-Yi Sung, and Wen-Lian Hsu. 2006.
A semi-automatic method for annotating a biomedical
proposition bank. In FLAC’06. ACL.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. JMLR, 7.
Aron Culota, Trausti Kristjansson, Andrew McCallum,
and Paul Viola. 2006. Corrective feedback and per-
sistent learning for information extraction. Arti�cial
Intelligence, 170:1101–1122.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics, 19(2):313–330.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In ACL’05. ACL.
Lance Ramshaw and Mitch Marcus. 1995. Text chunk-
ing using transformation-based learning. In David
Yarovsky and Kenneth Church, editors, Proceedings
of the Third Workshop on Very Large Corpora. ACL.
Lorraine Tanabe, Natalie Xie, Lynne H. Thom, Wayne
Matten, and W. John Wilbur. 2005. GENETAG: a
tagged corpus for gene/protein named entity recogni-
tion. BMC Bioinformatics, 6(Suppl. 1).
Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.
2002. Building a large-scale annotated chinese corpus.
In Proceedings of the 19th international conference on
Computational linguistics. ACL.
Alexander Yeh, Alexander Morgan, Marc Colosimo, and
Lynette Hirschman. 2005. BioCreAtIvE Task 1A:
gene mention finding evaluation. BMC Bioinformat-
ics, 6(Suppl. 1).
</reference>
<page confidence="0.998422">
56
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.377019">
<title confidence="0.9997">Semi-Automated Named Entity Annotation</title>
<author confidence="0.99891">Ganchev Pereira Mark Mandel</author>
<affiliation confidence="0.974877">Computer and Information Science, Linguistic Data Consortium, University of Pennsylvania, University of Pennsylvania, Philadelphia PA</affiliation>
<title confidence="0.447457">PA</title>
<author confidence="0.973798">Carroll White</author>
<affiliation confidence="0.989778">Division of Oncology, Children’s Hospital of Philadelphia Philadelphia</affiliation>
<abstract confidence="0.991493333333333">We investigate a way to partially automate corpus annotation for named entity recognition, by requiring only binary decisions from an annotator. Our approach is based on a linsequence model trained using a MIRA learning algorithm. We ask an annotator to decide whether each mention produced by a high recall tagger is a true mention or a false positive. We conclude that our approach can reduce the effort of extending a seed training corpus by up to 58%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Fu-Dong Chiou</author>
<author>David Chiang</author>
<author>Martha Palmer</author>
</authors>
<title>Facilitating treebank annotation using a statistical parser.</title>
<date>2001</date>
<booktitle>In HLT ’01.</booktitle>
<publisher>ACL.</publisher>
<contexts>
<context position="6183" citStr="Chiou et al. (2001)" startWordPosition="1032" endWordPosition="1035">most impact on learning. Semi-automated annotation tries to make the annotation of each instance faster or easier. In particular, it is possible to combine active learning and semi-automated annotation by using an active learning method to select which sentences to label and then using a semi-automated labeling method. 3.2 Previous work on semi-automated annotation The most common approach to semi-automatic annotation is to automatically tag an instance and then ask an annotator to correct the results. We restrict our discussion to this paradigm due to space constraints. Marcus et al. (1994), Chiou et al. (2001) and Xue et al. (2002) apply this approach with some minor modifications to part of speech tagging and phrase structure parsing. The automatic system of Marcus et al. only produces partial parses that are then assembled by the annotators, while Chiou et al. modified their automatic parser specifically for use in annotation. Chou et al. (2006) use this tag and correct approach to create a corpus of predicate argument structures in the biomedical domain. Culota et al. (2006) use a refinement of the tag and correct approach to extract addressbook information from email messages. They modify the s</context>
</contexts>
<marker>Chiou, Chiang, Palmer, 2001</marker>
<rawString>Fu-Dong Chiou, David Chiang, and Martha Palmer. 2001. Facilitating treebank annotation using a statistical parser. In HLT ’01. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-Chi Chou</author>
<author>Richard Tzong-Han Tsai</author>
<author>Ying-Shan Su</author>
<author>Wei Ku</author>
<author>Ting-Yi Sung</author>
<author>Wen-Lian Hsu</author>
</authors>
<title>A semi-automatic method for annotating a biomedical proposition bank.</title>
<date>2006</date>
<booktitle>In FLAC’06. ACL.</booktitle>
<contexts>
<context position="6527" citStr="Chou et al. (2006)" startWordPosition="1088" endWordPosition="1091">i-automated annotation The most common approach to semi-automatic annotation is to automatically tag an instance and then ask an annotator to correct the results. We restrict our discussion to this paradigm due to space constraints. Marcus et al. (1994), Chiou et al. (2001) and Xue et al. (2002) apply this approach with some minor modifications to part of speech tagging and phrase structure parsing. The automatic system of Marcus et al. only produces partial parses that are then assembled by the annotators, while Chiou et al. modified their automatic parser specifically for use in annotation. Chou et al. (2006) use this tag and correct approach to create a corpus of predicate argument structures in the biomedical domain. Culota et al. (2006) use a refinement of the tag and correct approach to extract addressbook information from email messages. They modify the system’s best guess as the user makes corrections, resulting in less annotation actions. 4 Experiments We now evaluate to what extent our semi-automated annotation framework can be useful, and how much effort it requires. For both questions we compare semi-automatic to fully manual annotation. In our first set of experiments, we measured the u</context>
</contexts>
<marker>Chou, Tsai, Su, Ku, Sung, Hsu, 2006</marker>
<rawString>Wen-Chi Chou, Richard Tzong-Han Tsai, Ying-Shan Su, Wei Ku, Ting-Yi Sung, and Wen-Lian Hsu. 2006. A semi-automatic method for annotating a biomedical proposition bank. In FLAC’06. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai ShalevShwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passiveaggressive algorithms.</title>
<date>2006</date>
<journal>JMLR,</journal>
<volume>7</volume>
<contexts>
<context position="2226" citStr="Crammer et al., 2006" startWordPosition="355" endWordPosition="359">s our experiments and Section 5 concludes the paper. 2 Methods Throughout this work, we use a linear sequence model. This class of models includes popular tagging models for named entities such as conditional 53 random fields, maximum entropy Markov models and max-margin Markov networks. Linear sequence models score possible tag sequences for a given input as the dot product between a learned weight vector and a feature vector derived from the input and proposed tas sequence. Linear sequence models differ principally on how the weight vector is learned. Our experiments use the MIRA algorithm (Crammer et al., 2006; McDonald et al., 2005) to learn the weight vector. 2.1 Notation In what follows, x denotes the generic input sentence, Y (x) the set of possible labelings of x, and Y +(x) the set of correct labelings of x. There is also a distinguished “gold” labeling y(x) E Y +(x). For each pair of a sentence x and labeling y E Y (x), we compute a vector-valued feature representation f(x, y). Given a weight vector w, the score w · f(x, y) ranks possible labelings of x, and we denote by Yk�,,,(x) the set of k top scoring labelings for x. We use the standard B,I,O encoding for named entities (Ramshaw and Mar</context>
<context position="3451" citStr="Crammer et al., 2006" startWordPosition="573" endWordPosition="576">1995). Thus Y (x) for x of length n is the set of all sequences of length n matching the regular expression (O|(BI*))*. In a linear sequence model, for suitable feature functions f, Yk�,,,(x) can be computed efficiently with Viterbi decoding. 2.2 k-best MIRA and Loss Functions The learning portion of our method finds a weight vector w that scores the correct labelings of the test data higher than incorrect labelings. We used a kProceedings of the Linguistic Annotation Workshop, pages 53–56, Prague, June 2007. c�2007 Association for Computational Linguistics best version of the MIRA algorithm (Crammer et al., 2006; McDonald et al., 2005). This is an online learning algorithm that starts with a zero weight vector and for each training sentence makes the smallest possible update that would score the correct label higher than the old top k labels. That is, for each training sentence x we update the weight vector w according to the rule: wnew = arg minw kw − woldk s. t. w · f(x, y(x)) − w · f(x, y) ≥ L(Y +(x), y) ∀y ∈ Yk,wold(x) where L(Y +(x), y) is the loss, which measures the errors in labeling y relative to the set of correct labelings Y +(x). An advantage of the MIRA algorithm (over many other learnin</context>
</contexts>
<marker>Crammer, Dekel, Keshet, ShalevShwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai ShalevShwartz, and Yoram Singer. 2006. Online passiveaggressive algorithms. JMLR, 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culota</author>
<author>Trausti Kristjansson</author>
<author>Andrew McCallum</author>
<author>Paul Viola</author>
</authors>
<title>Corrective feedback and persistent learning for information extraction.</title>
<date>2006</date>
<journal>Arti�cial Intelligence,</journal>
<pages>170--1101</pages>
<contexts>
<context position="6660" citStr="Culota et al. (2006)" startWordPosition="1111" endWordPosition="1114">notator to correct the results. We restrict our discussion to this paradigm due to space constraints. Marcus et al. (1994), Chiou et al. (2001) and Xue et al. (2002) apply this approach with some minor modifications to part of speech tagging and phrase structure parsing. The automatic system of Marcus et al. only produces partial parses that are then assembled by the annotators, while Chiou et al. modified their automatic parser specifically for use in annotation. Chou et al. (2006) use this tag and correct approach to create a corpus of predicate argument structures in the biomedical domain. Culota et al. (2006) use a refinement of the tag and correct approach to extract addressbook information from email messages. They modify the system’s best guess as the user makes corrections, resulting in less annotation actions. 4 Experiments We now evaluate to what extent our semi-automated annotation framework can be useful, and how much effort it requires. For both questions we compare semi-automatic to fully manual annotation. In our first set of experiments, we measured the usefulness of semi-automatically annotated corpora for training a gene mention tagger. In the second set of experiments, we measured t</context>
</contexts>
<marker>Culota, Kristjansson, McCallum, Viola, 2006</marker>
<rawString>Aron Culota, Trausti Kristjansson, Andrew McCallum, and Paul Viola. 2006. Corrective feedback and persistent learning for information extraction. Arti�cial Intelligence, 170:1101–1122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="6162" citStr="Marcus et al. (1994)" startWordPosition="1028" endWordPosition="1031"> in order to make the most impact on learning. Semi-automated annotation tries to make the annotation of each instance faster or easier. In particular, it is possible to combine active learning and semi-automated annotation by using an active learning method to select which sentences to label and then using a semi-automated labeling method. 3.2 Previous work on semi-automated annotation The most common approach to semi-automatic annotation is to automatically tag an instance and then ask an annotator to correct the results. We restrict our discussion to this paradigm due to space constraints. Marcus et al. (1994), Chiou et al. (2001) and Xue et al. (2002) apply this approach with some minor modifications to part of speech tagging and phrase structure parsing. The automatic system of Marcus et al. only produces partial parses that are then assembled by the annotators, while Chiou et al. modified their automatic parser specifically for use in annotation. Chou et al. (2006) use this tag and correct approach to create a corpus of predicate argument structures in the biomedical domain. Culota et al. (2006) use a refinement of the tag and correct approach to extract addressbook information from email messag</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1994. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In ACL’05. ACL.</booktitle>
<contexts>
<context position="2250" citStr="McDonald et al., 2005" startWordPosition="360" endWordPosition="363">Section 5 concludes the paper. 2 Methods Throughout this work, we use a linear sequence model. This class of models includes popular tagging models for named entities such as conditional 53 random fields, maximum entropy Markov models and max-margin Markov networks. Linear sequence models score possible tag sequences for a given input as the dot product between a learned weight vector and a feature vector derived from the input and proposed tas sequence. Linear sequence models differ principally on how the weight vector is learned. Our experiments use the MIRA algorithm (Crammer et al., 2006; McDonald et al., 2005) to learn the weight vector. 2.1 Notation In what follows, x denotes the generic input sentence, Y (x) the set of possible labelings of x, and Y +(x) the set of correct labelings of x. There is also a distinguished “gold” labeling y(x) E Y +(x). For each pair of a sentence x and labeling y E Y (x), we compute a vector-valued feature representation f(x, y). Given a weight vector w, the score w · f(x, y) ranks possible labelings of x, and we denote by Yk�,,,(x) the set of k top scoring labelings for x. We use the standard B,I,O encoding for named entities (Ramshaw and Marcus, 1995). Thus Y (x) f</context>
<context position="3475" citStr="McDonald et al., 2005" startWordPosition="577" endWordPosition="580">x of length n is the set of all sequences of length n matching the regular expression (O|(BI*))*. In a linear sequence model, for suitable feature functions f, Yk�,,,(x) can be computed efficiently with Viterbi decoding. 2.2 k-best MIRA and Loss Functions The learning portion of our method finds a weight vector w that scores the correct labelings of the test data higher than incorrect labelings. We used a kProceedings of the Linguistic Annotation Workshop, pages 53–56, Prague, June 2007. c�2007 Association for Computational Linguistics best version of the MIRA algorithm (Crammer et al., 2006; McDonald et al., 2005). This is an online learning algorithm that starts with a zero weight vector and for each training sentence makes the smallest possible update that would score the correct label higher than the old top k labels. That is, for each training sentence x we update the weight vector w according to the rule: wnew = arg minw kw − woldk s. t. w · f(x, y(x)) − w · f(x, y) ≥ L(Y +(x), y) ∀y ∈ Yk,wold(x) where L(Y +(x), y) is the loss, which measures the errors in labeling y relative to the set of correct labelings Y +(x). An advantage of the MIRA algorithm (over many other learning algorithms such as con</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In ACL’05. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lance Ramshaw</author>
<author>Mitch Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1995</date>
<booktitle>In David Yarovsky and Kenneth Church, editors, Proceedings of the Third Workshop on Very Large Corpora. ACL.</booktitle>
<contexts>
<context position="2836" citStr="Ramshaw and Marcus, 1995" startWordPosition="473" endWordPosition="476">er et al., 2006; McDonald et al., 2005) to learn the weight vector. 2.1 Notation In what follows, x denotes the generic input sentence, Y (x) the set of possible labelings of x, and Y +(x) the set of correct labelings of x. There is also a distinguished “gold” labeling y(x) E Y +(x). For each pair of a sentence x and labeling y E Y (x), we compute a vector-valued feature representation f(x, y). Given a weight vector w, the score w · f(x, y) ranks possible labelings of x, and we denote by Yk�,,,(x) the set of k top scoring labelings for x. We use the standard B,I,O encoding for named entities (Ramshaw and Marcus, 1995). Thus Y (x) for x of length n is the set of all sequences of length n matching the regular expression (O|(BI*))*. In a linear sequence model, for suitable feature functions f, Yk�,,,(x) can be computed efficiently with Viterbi decoding. 2.2 k-best MIRA and Loss Functions The learning portion of our method finds a weight vector w that scores the correct labelings of the test data higher than incorrect labelings. We used a kProceedings of the Linguistic Annotation Workshop, pages 53–56, Prague, June 2007. c�2007 Association for Computational Linguistics best version of the MIRA algorithm (Cramm</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>Lance Ramshaw and Mitch Marcus. 1995. Text chunking using transformation-based learning. In David Yarovsky and Kenneth Church, editors, Proceedings of the Third Workshop on Very Large Corpora. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lorraine Tanabe</author>
<author>Natalie Xie</author>
<author>Lynne H Thom</author>
<author>Wayne Matten</author>
<author>W John Wilbur</author>
</authors>
<title>GENETAG: a tagged corpus for gene/protein named entity recognition.</title>
<date>2005</date>
<journal>BMC Bioinformatics,</journal>
<volume>6</volume>
<marker>Tanabe, Xie, Thom, Matten, Wilbur, 2005</marker>
<rawString>Lorraine Tanabe, Natalie Xie, Lynne H. Thom, Wayne Matten, and W. John Wilbur. 2005. GENETAG: a tagged corpus for gene/protein named entity recognition. BMC Bioinformatics, 6(Suppl. 1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Fu-Dong Chiou</author>
<author>Martha Palmer</author>
</authors>
<title>Building a large-scale annotated chinese corpus.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th international conference on Computational linguistics. ACL.</booktitle>
<contexts>
<context position="6205" citStr="Xue et al. (2002)" startWordPosition="1037" endWordPosition="1040"> Semi-automated annotation tries to make the annotation of each instance faster or easier. In particular, it is possible to combine active learning and semi-automated annotation by using an active learning method to select which sentences to label and then using a semi-automated labeling method. 3.2 Previous work on semi-automated annotation The most common approach to semi-automatic annotation is to automatically tag an instance and then ask an annotator to correct the results. We restrict our discussion to this paradigm due to space constraints. Marcus et al. (1994), Chiou et al. (2001) and Xue et al. (2002) apply this approach with some minor modifications to part of speech tagging and phrase structure parsing. The automatic system of Marcus et al. only produces partial parses that are then assembled by the annotators, while Chiou et al. modified their automatic parser specifically for use in annotation. Chou et al. (2006) use this tag and correct approach to create a corpus of predicate argument structures in the biomedical domain. Culota et al. (2006) use a refinement of the tag and correct approach to extract addressbook information from email messages. They modify the system’s best guess as </context>
</contexts>
<marker>Xue, Chiou, Palmer, 2002</marker>
<rawString>Nianwen Xue, Fu-Dong Chiou, and Martha Palmer. 2002. Building a large-scale annotated chinese corpus. In Proceedings of the 19th international conference on Computational linguistics. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yeh</author>
<author>Alexander Morgan</author>
<author>Marc Colosimo</author>
<author>Lynette Hirschman</author>
</authors>
<title>BioCreAtIvE Task 1A: gene mention finding evaluation.</title>
<date>2005</date>
<journal>BMC Bioinformatics,</journal>
<volume>6</volume>
<contexts>
<context position="8534" citStr="Yeh et al., 2005" startWordPosition="1406" endWordPosition="1409">etric would give full credit for guessing one of the alternative labels rather than the “gold” label. al., 2005). The data is supplied as a set of sentences chosen randomly from MEDLINE and annotated for gene mentions. Each sentence in the corpus is provided as a list of “gold” gene mentions as well as a set of alternatives for each mention. The alternatives are generated by the annotators and count as true positives. Figure 1 shows an example sentence with its gold and alternative mentions. The evaluation metric for these experiments is F-score augmented with the possibility of alternatives (Yeh et al., 2005). We used 5992 sentences as the data that has already been annotated manually (set Data-1), and simulated different ways of annotating the remaining 5982 sentences (set Data-2). We compare the quality of annotation by testing taggers trained using these corpora on a 1493 sentence test set. We trained a high-recall tagger (recall of 89.6%) on Data-1, and ran it on Data-2. Since we have labels available for Data-2, we simulated an annotator filtering these proposed mentions by accepting them only if they exactly match a “gold” or alternative mention. This gave us an F-score of 94.7% on Data-2 an</context>
</contexts>
<marker>Yeh, Morgan, Colosimo, Hirschman, 2005</marker>
<rawString>Alexander Yeh, Alexander Morgan, Marc Colosimo, and Lynette Hirschman. 2005. BioCreAtIvE Task 1A: gene mention finding evaluation. BMC Bioinformatics, 6(Suppl. 1).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>