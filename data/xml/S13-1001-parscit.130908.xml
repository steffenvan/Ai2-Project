<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000009">
<title confidence="0.998685">
Towards a Formal Distributional Semantics:
Simulating Logical Calculi with Tensors
</title>
<author confidence="0.996823">
Edward Grefenstette
</author>
<affiliation confidence="0.9997935">
University of Oxford
Department of Computer Science
</affiliation>
<address confidence="0.9908105">
Wolfson Building, Parks Road
Oxford OX1 3QD, UK
</address>
<email confidence="0.999122">
edward.grefenstette@cs.ox.ac.uk
</email>
<sectionHeader confidence="0.998568" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998034869565217">
The development of compositional distribu-
tional models of semantics reconciling the em-
pirical aspects of distributional semantics with
the compositional aspects of formal seman-
tics is a popular topic in the contemporary lit-
erature. This paper seeks to bring this rec-
onciliation one step further by showing how
the mathematical constructs commonly used
in compositional distributional models, such
as tensors and matrices, can be used to sim-
ulate different aspects of predicate logic.
This paper discusses how the canonical iso-
morphism between tensors and multilinear
maps can be exploited to simulate a full-blown
quantifier-free predicate calculus using ten-
sors. It provides tensor interpretations of the
set of logical connectives required to model
propositional calculi. It suggests a variant
of these tensor calculi capable of modelling
quantifiers, using few non-linear operations.
It finally discusses the relation between these
variants, and how this relation should consti-
tute the subject of future work.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999970813953489">
The topic of compositional distributional semantics
has been growing in popularity over the past few
years. This emerging sub-field of natural language
semantic modelling seeks to combine two seemingly
orthogonal approaches to modelling the meaning of
words and sentences, namely formal semantics and
distributional semantics.
These approaches, summarised in Section 2, dif-
fer in that formal semantics, on the one hand, pro-
vides a neatly compositional picture of natural lan-
guage meaning, reducing sentences to logical rep-
resentations; one the other hand, distributional se-
mantics accounts for the ever-present ambiguity and
polysemy of words of natural language, and pro-
vides tractable ways of learning and comparing
word meanings based on corpus data.
Recent efforts, some of which are briefly re-
ported below, have been made to unify both of
these approaches to language modelling to pro-
duce compositional distributional models of seman-
tics, leveraging the learning mechanisms of distri-
butional semantics, and providing syntax-sensitive
operations for the production of representations of
sentence meaning obtained through combination of
corpus-inferred word meanings. These efforts have
been met with some success in evaluations such
as phrase similarity tasks (Mitchell and Lapata,
2008; Mitchell and Lapata, 2009; Grefenstette and
Sadrzadeh, 2011; Kartsaklis et al., 2012), sentiment
prediction (Socher et al., 2012), and paraphrase de-
tection (Blacoe and Lapata, 2012).
While these developments are promising with
regard to the goal of obtaining learnable-yet-
structured sentence-level representations of lan-
guage meaning, part of the motivation for unifying
formal and distributional models of semantics has
been lost. The compositional aspects of formal se-
mantics are combined with the corpus-based empir-
ical aspects of distributional semantics in such mod-
els, yet the logical aspects are not. But it is these
logical aspects which are so appealing in formal se-
mantic models, and therefore it would be desirable
to replicate the inferential powers of logic within
</bodyText>
<page confidence="0.784195">
1
</page>
<note confidence="0.9014845">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 1–10, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999400787878788">
compositional distributional models of semantics.
In this paper, I make steps towards addressing this
lost connection with logic in compositional distri-
butional semantics. In Section 2, I provide a brief
overview of formal and distributional semantic mod-
els of meaning. In Section 3, I give mathemati-
cal foundations for the rest of the paper by intro-
ducing tensors and tensor contraction as a way of
modelling multilinear functions. In Section 4, I dis-
cuss how predicates, relations, and logical atoms
of a quantifier-free predicate calculus can be mod-
elled with tensors. In Section 5, I present tenso-
rial representations of logical operations for a com-
plete propositional calculus. In Section 6, I discuss
a variant of the predicate calculus from Section 4
aimed at modelling quantifiers within such tensor-
based logics, and the limits of compositional for-
malisms based only on multilinear maps. I con-
clude, in Section 7, by suggesting directions for fur-
ther work based on the contents of this paper.
This paper does not seek to address the question
of how to determine how words should be trans-
lated into predicates and relations in the first place,
but rather shows how such predicates and relations
can be modelled using multilinear algebra. As such,
it can be seen as a general theoretical contribution
which is independent from the approaches to com-
positional distributional semantics it can be applied
to. It is directly compatible with the efforts of Co-
ecke et al. (2010) and Grefenstette et al. (2013), dis-
cussed below, but is also relevant to any other ap-
proach making use of tensors or matrices to encode
semantic relations.
</bodyText>
<sectionHeader confidence="0.999899" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999546928571429">
Formal semantics, from the Montagovian school of
thought (Montague,1974; Dowty et al., 1981), treats
natural languages as programming languages which
compile down to some formal language such as a
predicate calculus. The syntax of natural languages,
in the form of a grammar, is augmented by seman-
tic interpretations, in the form of expressions from
a higher order logic such as the lambda-beta calcu-
lus. The parse of a sentence then determines the
combinations of lambda-expressions, the reduction
of which yields a well-formed formula of a predi-
cate calculus, corresponding to the semantic repre-
sentation of the sentence. A simple formal semantic
model is illustrated in Figure 1.
</bodyText>
<subsectionHeader confidence="0.919174">
Syntactic Analysis Semantic Interpretation
</subsectionHeader>
<equation confidence="0.978197714285714">
S=*NPVP
NP =* cats, milk, etc.
VP =* Vt NP
Vt =* like, hug, etc.
[[likej([[catsj, [[milkj)
[[catsjdx.[likej(x, [[milkj)
dyx.[[likej(x,y) [[milkjFigure 1: A simple formal semantic model.
</equation>
<bodyText confidence="0.99978084375">
Formal semantic models are incredibly powerful,
in that the resulting logical representations of sen-
tences can be fed to automated theorem provers to
perform textual inference, consistency verification,
question answering, and a host of other tasks which
are well developed in the literature (e.g. see (Love-
land, 1978) and (Fitting, 1996)). However, the so-
phistication of such formal semantic models comes
at a cost: the complex set of rules allowing for
the logical interpretation of text must either be pro-
vided a priori, or learned. Learning such represen-
tations is a complex task, the difficulty of which is
compounded by issues of ambiguity and polysemy
which are pervasive in natural languages.
In contrast, distributional semantic models, best
summarised by the dictum of Firth (1957) that “You
shall know a word by the company it keeps,” pro-
vide an elegant and tractable way of learning se-
mantic representations of words from text. Word
meanings are modelled as high-dimensional vectors
in large semantic vector spaces, the basis elements
of which correspond to contextual features such as
other words from a lexicon. Semantic vectors for
words are built by counting how many time a target
word occurs within a context (e.g. within k words
of select words from the lexicon). These context
counts are then normalised by a term frequency-
inverse document frequency-like measure (e.g. TF-
IDF, pointwise mutual information, ratio of proba-
bilities), and are set as the basis weights of the vec-
tor representation of the word’s meaning. Word vec-
tors can then be compared using geometric distance
</bodyText>
<equation confidence="0.804521">
[[VPj([[NPj)
[[cats], [[milkj, ...
[[Vtj([[NPj)
dyx.[[likej(x,y), ...
</equation>
<page confidence="0.988369">
2
</page>
<figureCaption confidence="0.999764">
Figure 2: A simple distributional semantic model.
</figureCaption>
<bodyText confidence="0.999865025">
metrics such as cosine similarity, allowing us to de-
termine the similarity of words, cluster semantically
related words, and so on. Excellent overviews of dis-
tributional semantic models are provided by Curran
(2004) and Mitchell (2011). A simple distributional
semantic model showing the spacial representation
of words ‘dog’, ‘cat’ and ‘snake’ within the context
of feature words ‘pet’, ‘furry’, and ‘stroke’ is shown
in Figure 2.
Distributional semantic models have been suc-
cessfully applied to tasks such as word-sense
discrimination (Sch¨utze, 1998), thesaurus extrac-
tion (Grefenstette, 1994), and automated essay
marking (Landauer and Dumais, 1997). However,
while such models provide tractable ways of learn-
ing and comparing word meanings, they do not natu-
rally scale beyond word length. As recently pointed
out by Turney (2012), treating larger segments of
texts as lexical units and learning their representa-
tions distributionally (the ‘holistic approach’) vio-
lates the principle of linguistic creativity, according
to which we can formulate and understand phrases
which we’ve never observed before, provided we
know the meaning of their parts and how they are
combined. As such, distributional semantics makes
no effort to account for the compositional nature of
language like formal semantics does, and ignores is-
sues relating to syntactic and relational aspects of
language.
Several proposals have been put forth over the
last few years to provide vector composition func-
tions for distributional models in order to introduce
compositionality, thereby replicating some of the as-
pects of formal semantics while preserving learn-
ability. Simple operations such as vector addition
and multiplication, with or without scalar or matrix
weights (to take word order or basic relational as-
pects into account), have been suggested (Zanzotto
et al., 2010; Mitchell and Lapata, 2008; Mitchell and
Lapata, 2009).
Smolensky (1990) suggests using the tensor prod-
uct of word vectors to produce representations that
grow with sentence complexity. Clark and Pulman
(2006) extend this approach by including basis vec-
tors standing for dependency relations into tensor
product-based representations. Both of these ten-
sor product-based approaches run into dimensional-
ity problems as representations of sentence mean-
ing for sentences of different lengths or grammati-
cal structure do not live in the same space, and thus
cannot directly be compared. Coecke et al. (2010)
develop a framework using category theory, solving
this dimensionality problem of tensor-based models
by projecting tensored vectors for sentences into a
unique vector space for sentences, using functions
dynamically generated by the syntactic structure of
the sentences. In presenting their framework, which
partly inspired this paper, they describe how a verb
can be treated as a logical relation using tensors in
order to evaluate the truth value of a simple sentence,
as well as how negation can be modelled using ma-
trices.
A related approach, by Baroni and Zamparelli
(2010), represents unary relations such as adjectives
as matrices learned by linear regression from cor-
pus data, and models adjective-noun composition
as matrix-vector multiplication. Grefenstette et al.
(2013) generalise this approach to relations of any
arity and relate it to the framework of Coecke et al.
(2010) using a tensor-based approach to formal se-
mantic modelling similar to that presented in this pa-
per.
Finally, Socher et al. (2012) apply deep learning
techniques to model syntax-sensitive vector compo-
sition using non-linear operations, effectively turn-
ing parse trees into multi-stage neural networks.
Socher shows that the non-linear activation func-
tion used in such a neural network can be tailored to
replicate the behaviour of basic logical connectives
such as conjunction and negation.
</bodyText>
<figure confidence="0.933277666666667">
snake
stroke
cat
dog
pet
furry
</figure>
<sectionHeader confidence="0.67414" genericHeader="method">
3 Tensors and multilinear maps
</sectionHeader>
<bodyText confidence="0.999892444444444">
Tensors are the mathematical objects dealt with in
multilinear algebra just as vectors and matrices are
the objects dealt with in linear algebra. In fact, ten-
sors can be seen as generalisations of vectors and
matrices by introducing the notion of tensor rank.
Let the rank of a tensor be the number of indices re-
quired to describe a vector/matrix-like object in sum
notation. A vector v in a space V with basis {bVi }i can
be written as the weighted sum of the basis vectors:
</bodyText>
<equation confidence="0.983574">
Yv =
i
</equation>
<bodyText confidence="0.999521555555556">
where the cvi elements are the scalar basis weights
of the vector. Being fully described with one index,
vectors are rank 1 tensors. Similarly, a matrix M is
an element of a space V ® W with basis {(bVi , bWj )}i j
(such pairs of basis vectors of V and W are com-
monly written as {bVi ® bWj }i j in multilinear algebra).
Such matrices are rank 2 tensors, as they can be fully
described using two indices (one for rows, one for
columns):
</bodyText>
<equation confidence="0.9506575">
YM = cMij bVi ® bWj
ij
</equation>
<bodyText confidence="0.994278294117647">
where the scalar weights cMij are just the i jth ele-
ments of the matrix.
A tensor T of rank k is just a geometric object with
a higher rank. Let T be a member of V1®...®Vk; we
can express T as follows, using k indices α1 ... αk:
rank n − 1, corresponding to the sum of the ranks of
the input tensors minus 2. The tensors must satisfy
the following restriction: the left tensor must have
a rightmost index spanning the same number of di-
mensions as the leftmost index of the right tensor.
This is similar to the restriction that a m by n matrix
can only be multiplied with a p by q matrix if n = p,
i.e. if the index spanning the columns of the first ma-
trix covers the same number of columns as the index
spanning the rows of the second matrix covers rows.
Similarly to how the columns of one matrix ‘merge’
with the rows of another to produce a third matrix,
the part of the first tensor spanned by the index k
merges with the part of the second tensor spanned by
k by ‘summing through’ the shared basis elements
bVk
αk of each tensor. Each tensor therefore loses a
rank while being joined, explaining how the tensor
produced by TxU is of rank k+(n−k+1)−2 = n−1.
There exists an isomorphism between tensors and
multilinear maps (Bourbaki, 1989; Lee, 1997), such
that any curried multilinear map
f:V1--�...--�Vj--�Vk
can be represented as a tensor Tf E Vk ®Vj ®...®V1
(note the reversed order of the vector spaces), with
tensor contraction acting as function application.
This isomorphism guarantees that there exists such a
tensor Tf for every f, such that the following equal-
ity holds for any v1 E V1, ... , vj E Vj:
</bodyText>
<equation confidence="0.799833333333333">
cvi bVi
YT = T V vk fv1 ... vj = vk = Tf x v1 x ... x vj
α1...αk cα1 ... αkbl α1 ® ... ® bαk
</equation>
<bodyText confidence="0.9994514">
In this paper, we will be dealing with tensors of rank
1 (vectors), rank 2 (matrices) and rank 3, which can
be pictured as cuboids (or a matrix of matrices).
Tensor contraction is an operation which allows
us to take two tensors and produce a third. It is a
generalisation of inner products and matrix multipli-
cation to tensors of higher ranks. Let T be a tensor in
V1®...®Vj®Vk and U be a tensor in Vk®Vm®...®Vn.
The contraction of these tensors, written T x U, cor-
responds to the following calculation:
</bodyText>
<equation confidence="0.9777205">
TxU=
cT α1...αkcU αk...αnbV1
α1 ® ... ® bVj
αj ® bVmαm ® ... ® bVnαn
</equation>
<bodyText confidence="0.9978745">
Tensor contraction takes a tensor of rank k and a
tensor of rank n − k + 1 and produces a tensor of
</bodyText>
<sectionHeader confidence="0.97085" genericHeader="method">
4 Tensor-based predicate calculi
</sectionHeader>
<bodyText confidence="0.999974785714286">
In this section, I discuss how the isomorphism be-
tween multilinear maps and tensors described above
can be used to model predicates, relations, and log-
ical atoms of a predicate calculus. The four aspects
of a predicate calculus we must replicate here us-
ing tensors are as follows: truth values, the logical
domain and its elements (logical atoms), predicates,
and relations. I will discuss logical connectives in
the next section.
Both truth values and domain objects are the ba-
sic elements of a predicate calculus, and therefore
it makes sense to model them as vectors rather than
higher rank tensors, which I will reserve for rela-
tions. We first must consider the vector space used
</bodyText>
<equation confidence="0.454593">
Y
α1...αn
</equation>
<page confidence="0.951704">
4
</page>
<bodyText confidence="0.979737826086956">
to model the boolean truth values of B. Coecke et al.
(2010) suggest, as boolean vector space, the space B
with the basis {&gt;,⊥}, where &gt; = [1 0]&gt; is inter-
preted as ‘true’, and ⊥ = [0 1]&gt; as ‘false’.
I assign to the domain D, the set of objects in
our logic, a vector space D on R|D |with basis vec-
tors {di}i which are in bijective correspondence with
elements of D. An element of D is therefore rep-
resented as a one-hot vector in D, the single non-
null value of which is the weight for the basis vector
mapped to that element of D. Similarly, a subset of
D is a vector of D where those elements of D in the
subset have 1 as their corresponding basis weights in
the vector, and those not in the subset have 0. There-
fore there is a one-to-one correspondence between
the vectors in D and the elements of the power set
P(D), provided the basis weights of the vectors are
restricted to one of 0 or 1.
Each unary predicate P in the logic is represented
in the logical model as a set MP ⊆ D containing the
elements of the domain for which the predicate is
true. Predicates can be viewed as a unary function
fP : D → B where
</bodyText>
<equation confidence="0.97625">
( &gt; if x ∈ MP
fP(x) = ⊥ otherwise
</equation>
<bodyText confidence="0.999769">
These predicate functions can be modelled as rank 2
tensors in B ⊗ D, i.e. matrices. Such a matrix MP is
expressed in sum notation as follows:
</bodyText>
<equation confidence="0.99785075">
⎛⎜⎜⎜⎜⎜X ⎞⎟⎟⎟⎟⎟⎠ + ⎛⎜⎜⎜⎜⎜⎝ X ⎞⎟⎟⎟⎟⎟⎠
MP = cMP cMP
⎝ 1i &gt; ⊗ di 2i ⊥ ⊗ di
i i
</equation>
<bodyText confidence="0.974834590909091">
The basis weights are defined in terms of the set MP
as follows: cMP
1i = 1 if the logical atom xi associ-
ated with basis weight di is in MP, and 0 otherwise;
conversely, cMP
2i = 1 if the logical atom xi associated
with basis weight di is not in MP, and 0 otherwise.
To give a simple example, let’s consider a do-
main with three individuals, represented as the fol-
lowing one-hot vectors in D: john = [1 0 0]&gt;,
chris = [0 1 0]&gt;, and tom = [0 0 1]&gt;. Let’s
imagine that Chris and John are mathematicians, but
Tom is not. The predicate P for ‘is a mathemati-
cian’ therefore is represented model-theoretically as
the set MP = {chris, john}. Translating this into a
matrix gives the following tensor for P:
&amp;quot; 1 1 0 #
0 0 1
To compute the truth value of ‘John is a mathemati-
cian’, we perform predicate-argument application as
tensor contraction (matrix-vector multiplication, in
this case):
</bodyText>
<equation confidence="0.993446333333333">
&amp;quot; 1 1 0 ⎤⎥⎥⎥⎥⎥⎥⎥⎥⎦ =&amp;quot; 1 #
# ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎣ 0
MP × john = 1 = &gt;
0 0 1 0
0
Likewise for ‘Tom is a mathematician’:
&amp;quot; 1 1 0 ⎤⎥⎥⎥⎥⎥⎥⎥⎥⎦ = &amp;quot; 0 #
# ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎣ 0
MP × tom = 0 = ⊥
</equation>
<bodyText confidence="0.9172724">
0 0 1 1 1
Model theory for predicate calculus represents
any n-ary relation R, such as a verb, as the set MR
of n-tuples of elements from D for which R holds.
Therefore such relations can be viewed as functions
</bodyText>
<equation confidence="0.988363">
fR : Dn → B where:
lation R as a tensor TR in B ⊗ D ⊗ ... ⊗ D
|{z}
n
⎞⎟⎟⎟⎟⎟⎟
&gt; ⊗ dα1 ⊗ ... ⊗ dαn ⎠
TR
+ c2α1 ... αn⊥ ⊗ dα1 ⊗ ... ⊗ dαn
</equation>
<bodyText confidence="0.9931185">
As was the case for predicates, the weights for re-
lational tensors are defined in terms of the set mod-
elling the relation: cTR
1α1...αn is 1 if the tuple (x, ... , z)
associated with the basis vectors dαn ... dα1 (again,
note the reverse order) is in MR and 0 otherwise; and
</bodyText>
<equation confidence="0.4830955">
cTR
2α1...αn is 1 if the tuple (x, ... , z) associated with
</equation>
<bodyText confidence="0.999125428571429">
the basis vectors dαn ... dα1 is not in MR and 0 oth-
erwise.
To give an example involving relations, let our
domain be the individuals John (j) and Mary (m).
Mary loves John and herself, but John only loves
himself. The logical model for this scenario is as
follows:
</bodyText>
<equation confidence="0.923532">
D = { j, m} Mloves = {(j, j), (m, m), (m, j)}
</equation>
<bodyText confidence="0.999643333333333">
Distributionally speaking, the elements of the do-
main will be mapped to the following one-hot vec-
tors in some two-dimensional space D as follows:
</bodyText>
<equation confidence="0.83018">
fR(x1, ... , xn) =
We can represent the boolean function for such a re-
(&gt; if (x1, ... , xn) ∈ MR
⊥ otherwise
MP =
:
X cTR
1α1...αn
TR =
⎛⎜⎜⎜⎜⎜⎜
⎝
α1...αn
⎛⎜⎜⎜⎜⎜⎜X ⎝
α1...αn
</equation>
<page confidence="0.896542">
5
</page>
<bodyText confidence="0.9620742">
j = [1 0]&gt; and m = [0 1]&gt;. The tensor for ‘loves’ foll
can be written as follows, ignoring basis elements ows:
with null-valued basis weights, and using the dis-
tributivity of the tensor product over addition:
6 which allows us to express tensor contractions as
</bodyText>
<equation confidence="0.9889508">
&amp;quot; a1 b1 # &amp;quot; α #
T × v = c1 d1 a2 b2
c2 d2 β
Tloves = &gt; ⊗ ((d1 ⊗ d1) + (d2 ⊗ d2) + (d1 ⊗ d2))
+ (⊥ ⊗ d2 ⊗ d1)
</equation>
<bodyText confidence="0.9027965">
Computing “Mary loves John” would correspond to
the following calculation:
</bodyText>
<equation confidence="0.997821">
(Tloves × m) × j =
((&gt; ⊗ d2) + (&gt; ⊗ d1)) × j = &gt;
</equation>
<bodyText confidence="0.95565">
whereas “John loves Mary” would correspond to the
following calculation:
</bodyText>
<equation confidence="0.997349">
(Tloves × j) × m =
((&gt; ⊗ d1) + (⊥ ⊗ d2)) × m = ⊥
</equation>
<sectionHeader confidence="0.988085" genericHeader="method">
5 Logical connectives with tensors
</sectionHeader>
<bodyText confidence="0.998547333333333">
In this section, I discuss how the boolean connec-
tives of a propositional calculus can be modelled us-
ing tensors. Combined with the predicate and rela-
tion representations discussed above, these form a
complete quantifier-free predicate calculus based on
tensors and tensor contraction.
Negation has already been shown to be modelled
in the boolean space described earlier by Coecke et
al. (2010) as the swap matrix:
</bodyText>
<equation confidence="0.979970434782609">
&amp;quot; = 0 1
T¬ 1 0
&amp;quot; α · a1 + β · a2 α · b1 + β · b2 #
= α · c1 + β · c2 α · d1 + β · d2
or more concretely:
&amp;quot; a1 b1
T × &gt; = c1 d1
&amp;quot; a1 b1
T × ⊥ = c1 d1
c2 d2 0 c1 d1
#&amp;quot;0 # &amp;quot; a2 b2 #
a2 b2 =
c2 d2 1 c2 d2
owing operations:
&amp;quot; 1 1 #
(∨) 7→ T∨ = 1 0
0 0 0 1
&amp;quot; 1 0 #
(∧) 7→ T∧ = 0 0
0 1 1 1
&amp;quot; 1 0
(→) 7→ T→ =
0 1
</equation>
<bodyText confidence="0.860989214285714">
represents truth value vectors of the form
where
= 1. Applying the above logical op-
erations to such vectors produces vectors with the
same normalisation property. This is due to the fact
that the columns of the component matrices are all
normalised (i.e. each column sums to 1). To give
an example with conjunction, let v =
and
=
with
=
= 1. The con-
junction of these vectors is calculated as foll
</bodyText>
<equation confidence="0.992076391304348">
[αβ]&gt;
α+
[α1β1]&gt;
w
[α2β2]&gt;
α1+ β1
α2+ β2
ows:
(T∧ × v) × w
#&amp;quot; α1 # &amp;quot; α2 #
0 0
1 1 β1 β2
&amp;quot; α1 # &amp;quot; α2 #
0
= β1 α1 + β1 β2
This can easily be verified:
T¬ × &gt; =
&amp;quot; 0 1#&amp;quot; 1# &amp;quot; 0 #
= = ⊥
1 0 0 1
&amp;quot; 01 #&amp;quot; 0 # &amp;quot; 1 #
T¬ × ⊥ = = = &gt;
1 0 1 0
</equation>
<bodyText confidence="0.9673326">
All other logical operators are binary, and hence
modelled as rank 3 tensors. To make talking about
rank 3 tensors used to model binary operations eas-
ier, I will use the following block matrix notation for
2 × 2 × 2 rank 3 tensors T:
</bodyText>
<equation confidence="0.964881538461538">
&amp;quot; a1 b1
T = c1 d1
#
a2 b2
c2 d2
&amp;quot;=
# &amp;quot; 1 # &amp;quot; a1 b1 #
a2 b2 =
Using this notation, we can define tensors for the
foll
#
1 1
0 0
</equation>
<bodyText confidence="0.993658166666667">
I leave the trivial proof by exhaustion that these fit
the bill to the reader.
It is worth noting here that these tensors pre-
serve normalised probabilities of truth. Let us con-
sider amodel such at that described in Coecke et
al. (2010) which, in lieu of boolean truth values,
</bodyText>
<equation confidence="0.811721">
#
α1α2
β1α2 + (α1 + β1)β2
</equation>
<bodyText confidence="0.999791222222222">
tensor maps subsets of D (elements of D) to subsets
of D containing only those objects of the original
subset for which P holds (i.e. yielding another vector
in D).
To give an example: let us consider a domain with
two dogs (a and b) and a cat (c). One of the dogs (b)
is brown, as is the cat. Let S be the set of dogs, and P
the predicate “brown”. I represent these statements
in the model as follows:
</bodyText>
<figure confidence="0.533091285714286">
D = {a, b, c) S = {a, b) MP = {b, c)
The set of dogs is represented as a vector S =
[1 1 0]T and the predicate ‘brown’ as a tensor in
D ® D:
0 0 0 ����������
0 1 0
0 0 1
</figure>
<bodyText confidence="0.90442575">
The set of brown dogs is obtained by computing
fB(S ), which distributionally corresponds to apply-
ing the tensor TP to the vector representation of S
via tensor contraction, as follows:
</bodyText>
<equation confidence="0.77037875">
TPxS= i 0 0 0 ���������� i 1 ���������� ���������� 0
0 1 0 1 ���������� = b
0 0 1 0 1
0
</equation>
<bodyText confidence="0.999830333333333">
The result of this computation shows that the set of
brown dogs is the singleton set containing the only
brown dog, b. As for how logical connectives fit
into this picture, in both approaches discussed be-
low, conjunction and disjunction are modelled using
set-theoretic intersection and union, which are sim-
ply the component-wise min and max functions over
vectors, respectively.
Using this new way of modelling predicates as
tensors, I turn to the problem of modelling quantifi-
cation. We begin by putting all predicates in vector
form by replacing each instance of the bound vari-
able with a vector 1 filled with ones, which extracts
the diagonal from the predicate matrix.
An intuitive way of modelling universal quantifi-
cation is as follows: expressions of the form “All Xs
are Ys” are true if and only if MX = MX n MY, where
MX and MY are the set of Xs and the set of Ys, re-
spectively. Using this, we can define the map forall
for distributional universal quantification modelling
expressions of the form “All Xs are Ys” as follows:
</bodyText>
<equation confidence="0.976713666666667">
forall(X, Y) = �T if X = min(X, Y)
1 otherwise
TP = i
</equation>
<bodyText confidence="0.5744525">
To check that the probabilities are normalised we
calculate:
</bodyText>
<equation confidence="0.999788333333333">
α1α2 + fa1α2 + (α1 + fa1)fa2
= (α1 + fa1)α2 + (α1 + fa1)fa2
= (α1 + fa1)(α2 + fa2) = 1
</equation>
<bodyText confidence="0.9996435">
We can observe that the resulting probability distri-
bution for truth is still normalised. The same prop-
erty can be verified for the other connectives, which
I leave as an exercise for the reader.
</bodyText>
<sectionHeader confidence="0.880376" genericHeader="evaluation">
6 Quantifiers and non-linearity
</sectionHeader>
<bodyText confidence="0.999975807692308">
The predicate calculus described up until this point
has repeatedly been qualified as ‘quantifier-free’,
for the simple reason that quantification cannot be
modelled if each application of a predicate or rela-
tion immediately yields a truth value. In perform-
ing such reductions, we throw away the informa-
tion required for quantification, namely the infor-
mation which indicates which elements of a domain
the predicate holds true or false for. In this sec-
tion, I present a variant of the predicate calculus
developed earlier in this paper which allows us to
model simple quantification (i.e. excluding embed-
ded quantifiers) alongside a tensor-based approach
to predicates. However, I will prove that this ap-
proach to quantifier modelling relies on non-linear
functions, rendering them non-suitable for compo-
sitional distributional models relying solely on mul-
tilinear maps for composition (or alternatively, ren-
dering such models unsuitable for the modelling of
quantifiers by this method).
We saw, in Section 4, that vectors in the seman-
tic space D standing for the logical domain could
model logical atoms as well as sets of atoms. With
this in mind, instead of modelling a predicate P as
a truth-function, let us now view it as standing for
some function fP : P(D) --� P(D), defined as:
</bodyText>
<equation confidence="0.991401">
fP(X) = X n MP
</equation>
<bodyText confidence="0.956797125">
where X is a set of domain objects, and MP is the set
modelling the predicate. The tensor form of such a
function will be some TfPin D ® D. Let this square
matrix be a diagonal matrix such that basis weights
= 1 if the atom x corresponding to di is in MP
and 0 otherwise. Through tensor contraction, this
Tfp
cii
</bodyText>
<page confidence="0.990239">
7
</page>
<bodyText confidence="0.999660875">
To give a short example, the sentence ‘All Greeks are
human’ is verified by computing X = (Mgreek x 1),
Y = (Mhuman x 1), and verifying the equality X =
min(X, Y).
Existential statements of the form “There exists
X” can be modelled using the function exists, which
tests whether or not MX is empty, and is defined as
follows:
</bodyText>
<equation confidence="0.84898">
�T if |X |&gt; 0
l otherwise
</equation>
<bodyText confidence="0.999881727272727">
To give a short example, the sentence ‘there exists a
brown dog’ is verified by computing X = (Mbrown x
1) n (Mdog x 1) and verifying whether or not X is of
strictly positive length.
An important point to note here is that neither of
these quantification functions are multi-linear maps,
since a multilinear map must be linear in all argu-
ments. A counter example for forall is to consider
the case where MX and MY are empty, and multi-
ply their vector representations by non-zero scalar
weights α and fl.
</bodyText>
<equation confidence="0.995222">
αX = X
flY = Y
</equation>
<bodyText confidence="0.995088857142857">
forall(αX, flY) = forall(X, Y) = T
forall(αX, flY) # αflT
I observe that the equations above demonstrate that
forall is not a multilinear map.
The proof that exists is not a multilinear map is
equally trivial. Assume MX is an empty set and α is
a non-zero scalar weight:
</bodyText>
<equation confidence="0.999155666666667">
αX = X
exists(αX) = exists(X) = l
exists(αX) # αl
</equation>
<bodyText confidence="0.982555">
It follows that exists is not a multi-linear function.
</bodyText>
<sectionHeader confidence="0.996205" genericHeader="conclusions">
7 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999958555555556">
In this paper, I set out to demonstrate that it was
possible to replicate most aspects of predicate logic
using tensor-based models. I showed that tensors
can be constructed from logical models to represent
predicates and relations, with vectors encoding ele-
ments or sets of elements from the logical domain.
I discussed how tensor contraction allows for evalu-
ation of logical expressions encoded as tensors, and
that logical connectives can be defined as tensors to
form a full quantifier-free predicate calculus. I ex-
posed some of the limitations of this approach when
dealing with variables under the scope of quantifiers,
and proposed a variant for the tensor representation
of predicates which allows us to deal with quantifi-
cation. Further work on tensor-based modelling of
quantifiers should ideally seek to reconcile this work
with that of Barwise and Cooper (1981). In this sec-
tion, I discuss how both of these approaches to pred-
icate modelling can be put into relation, and suggest
further work that might be done on this topic, and on
the topic of integrating this work into compositional
distributional models of semantics.
The first approach to predicate modelling treats
predicates as truth functions represented as tensors,
while the second treats them as functions from sub-
sets of the domain to subsets of the domain. Yet both
representations of predicates contain the same infor-
mation. Let MP and M&apos;P be the tensor represen-
tations of a predicate P under the first and second
approach, respectively. The relation between these
representations lies in the equality diag(pMP) =
M&apos;P, where p is the covector [1 0] (and hence pMP
yields the first row of MP). The second row of MP
being defined in terms of the first, one can also re-
cover MP from the diagonal of M&apos;P.
Furthermore, both approaches deal with separate
aspects of predicate logic, namely applying predi-
cates to logical atoms, and applying them to bound
variables. With this in mind, it is possible to see how
both approaches can be used sequentially by noting
that tensor contraction allows for partial application
of relations to logical atoms. For example, apply-
ing a binary relation to its first argument under the
first tensor-based model yields a predicate. Translat-
ing this predicate into the second model’s form using
the equality defined above then permits us to use it
in quantified expressions. Using this, we can eval-
uate expressions of the form “There exists someone
who John loves”. Future work in this area should
therefore focus on developing a version of this ten-
sor calculus which permits seamless transition be-
tween both tensor formulations of logical predicates.
Finally, this paper aims to provide a starting point
for the integration of logical aspects into composi-
</bodyText>
<equation confidence="0.816088">
exists(X) =
</equation>
<page confidence="0.976989">
8
</page>
<bodyText confidence="0.999993333333333">
tional distributional semantic models. The work pre-
sented here serves to illustrate how tensors can sim-
ulate logical elements and operations, but does not
address (or seek to address) the fact that the vectors
and matrices in most compositional distributional
semantic models do not cleanly represent elements
of a logical domain. However, such distributional
representations can arguably be seen as represent-
ing the properties objects of a logical domain hold
in a corpus: for example the similar distributions of
‘car’ and ‘automobile’ could serve to indicate that
these concepts are co-extensive. This suggests two
directions research based on this paper could take.
One could use the hypothesis that similar vectors in-
dicate co-extensive concepts to infer a (probabilis-
tic) logical domain and set of predicates, and use the
methods described above without modification; al-
ternatively one could use the form of the logical op-
erations and predicate tensors described in this pa-
per as a basis for a higher-dimensional predicate cal-
culus, and investigate how such higher-dimensional
‘logical’ operations and elements could be defined
or learned. Either way, the problem of reconciling
the fuzzy ‘messiness’ of distributional models with
the sharp ‘cleanliness’ of logic is a difficult problem,
but I hope to have demonstrated in this paper that a
small step has been made in the right direction.
</bodyText>
<sectionHeader confidence="0.999197" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9993398">
Thanks to Ondˇrej Ryp´aˇcek, Nal Kalchbrenner
and Karl Moritz Hermann for their helpful com-
ments during discussions surrounding this pa-
per. This work is supported by EPSRC Project
EP/I03808X/1.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999901476923077">
M. Baroni and R. Zamparelli. Nouns are vectors, adjec-
tives are matrices: Representing adjective-noun con-
structions in semantic space. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 1183–1193. Association
for Computational Linguistics, 2010.
J. Barwise and R. Cooper Generalized quantifiers and
natural language. Linguistics and philosophy, pages
159–219. Springer, 1981.
W. Blacoe and M. Lapata. A comparison of vector-based
representations for semantic composition. Proceed-
ings of the 2012 Conference on Empirical Methods in
Natural Language Processing, 2012.
N. Bourbaki. Commutative Algebra: Chapters 1-7.
Springer-Verlag (Berlin and New York), 1989.
S. Clark and S. Pulman. Combining symbolic and distri-
butional models of meaning. In AAAI Spring Sympo-
sium on Quantum Interaction, 2006.
B. Coecke, M. Sadrzadeh, and S. Clark. Mathematical
Foundations for a Compositional Distributional Model
of Meaning. Linguistic Analysis, volume 36, pages
345–384. March 2010.
J. R. Curran. From distributional to semantic similarity.
PhD thesis, 2004.
D. R. Dowty, R. E. Wall, and S. Peters. Introduction to
Montague Semantics. Dordrecht, 1981.
J. R. Firth. A synopsis of linguistic theory 1930-1955.
Studies in linguistic analysis, 1957.
M. Fitting. First-order logic and automated theorem
proving. Springer Verlag, 1996.
E. Grefenstette, G. Dinu, Y. Zhang, M. Sadrzadeh, and
M. Baroni. Multi-step regression learning for com-
positional distributional semantics. In Proceedings of
the Tenth International Conference on Computational
Semantics. Association for Computational Linguistics,
2013.
E. Grefenstette and M. Sadrzadeh. Experimental support
for a categorical compositional distributional model of
meaning. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
2011.
G. Grefenstette. Explorations in automatic thesaurus dis-
covery. 1994.
D. Kartsaklis, and M. Sadrzadeh and S. Pulman. A
Unified Sentence Space for Categorical Distributional-
Compositional Semantics: Theory and Experiments.
In Proceedings of 24th International Conference on
Computational Linguistics (COLING 2012): Posters,
2012.
T. K. Landauer and S. T. Dumais. A solution to Plato’s
problem: The latent semantic analysis theory of ac-
quisition, induction, and representation of knowledge.
Psychological review, 1997.
J. Lee. Riemannian manifolds: An introduction to curva-
ture, volume 176. Springer Verlag, 1997.
D. W. Loveland. Automated theorem proving: A logical
basis. Elsevier North-Holland, 1978.
J. Mitchell and M. Lapata. Vector-based models of se-
mantic composition. In Proceedings of ACL, vol-
ume 8, 2008.
J. Mitchell and M. Lapata. Language models based on se-
mantic composition. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 1-Volume 1, pages 430–439. As-
sociation for Computational Linguistics, 2009.
</reference>
<page confidence="0.944628">
9
</page>
<reference confidence="0.999637208333333">
J. J. Mitchell. Composition in distributional models of
semantics. PhD thesis, 2011.
R. Montague. English as a Formal Language. Formal
Semantics: The Essential Readings, 1974.
H. Sch¨utze. Automatic word sense discrimination. Com-
putational linguistics, 24(1):97–123, 1998.
P. Smolensky. Tensor product variable binding and the
representation of symbolic structures in connection-
ist systems. Artificial intelligence, 46(1-2):159–216,
1990.
R. Socher, B. Huval, C.D. Manning, and A.Y Ng.
Semantic compositionality through recursive matrix-
vector spaces. Proceedings of the 2012 Conference on
Empirical Methods in Natural Language Processing,
pages 1201–1211, 2012.
P. D. Turney. Domain and function: A dual-space model
of semantic relations and compositions. Journal ofAr-
tificial Intelligence Research, 44:533–585, 2012.
F. M. Zanzotto, I. Korkontzelos, F. Fallucchi, and S. Man-
andhar. Estimating linear models for compositional
distributional semantics. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics, pages 1263–1271. Association for Computational
Linguistics, 2010.
</reference>
<page confidence="0.997783">
10
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.411001">
<title confidence="0.999679">Towards a Formal Distributional Simulating Logical Calculi with Tensors</title>
<author confidence="0.98667">Edward</author>
<affiliation confidence="0.9992105">University of Department of Computer</affiliation>
<address confidence="0.5826855">Wolfson Building, Parks Oxford OX1 3QD,</address>
<email confidence="0.998588">edward.grefenstette@cs.ox.ac.uk</email>
<abstract confidence="0.999488708333333">The development of compositional distributional models of semantics reconciling the empirical aspects of distributional semantics with the compositional aspects of formal semantics is a popular topic in the contemporary literature. This paper seeks to bring this reconciliation one step further by showing how the mathematical constructs commonly used in compositional distributional models, such as tensors and matrices, can be used to simaspects of predicate logic. This paper discusses how the canonical isomorphism between tensors and multilinear maps can be exploited to simulate a full-blown quantifier-free predicate calculus using tensors. It provides tensor interpretations of the set of logical connectives required to model propositional calculi. It suggests a variant of these tensor calculi capable of modelling quantifiers, using few non-linear operations. It finally discusses the relation between these variants, and how this relation should constitute the subject of future work.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>R Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1183--1193</pages>
<contexts>
<context position="10893" citStr="Baroni and Zamparelli (2010)" startWordPosition="1661" endWordPosition="1664">annot directly be compared. Coecke et al. (2010) develop a framework using category theory, solving this dimensionality problem of tensor-based models by projecting tensored vectors for sentences into a unique vector space for sentences, using functions dynamically generated by the syntactic structure of the sentences. In presenting their framework, which partly inspired this paper, they describe how a verb can be treated as a logical relation using tensors in order to evaluate the truth value of a simple sentence, as well as how negation can be modelled using matrices. A related approach, by Baroni and Zamparelli (2010), represents unary relations such as adjectives as matrices learned by linear regression from corpus data, and models adjective-noun composition as matrix-vector multiplication. Grefenstette et al. (2013) generalise this approach to relations of any arity and relate it to the framework of Coecke et al. (2010) using a tensor-based approach to formal semantic modelling similar to that presented in this paper. Finally, Socher et al. (2012) apply deep learning techniques to model syntax-sensitive vector composition using non-linear operations, effectively turning parse trees into multi-stage neura</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>M. Baroni and R. Zamparelli. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1183–1193. Association for Computational Linguistics, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Barwise</author>
<author>R Cooper</author>
</authors>
<title>Generalized quantifiers and natural language. Linguistics and philosophy,</title>
<date>1981</date>
<pages>159--219</pages>
<publisher>Springer,</publisher>
<contexts>
<context position="28284" citStr="Barwise and Cooper (1981)" startWordPosition="5061" endWordPosition="5064">coding elements or sets of elements from the logical domain. I discussed how tensor contraction allows for evaluation of logical expressions encoded as tensors, and that logical connectives can be defined as tensors to form a full quantifier-free predicate calculus. I exposed some of the limitations of this approach when dealing with variables under the scope of quantifiers, and proposed a variant for the tensor representation of predicates which allows us to deal with quantification. Further work on tensor-based modelling of quantifiers should ideally seek to reconcile this work with that of Barwise and Cooper (1981). In this section, I discuss how both of these approaches to predicate modelling can be put into relation, and suggest further work that might be done on this topic, and on the topic of integrating this work into compositional distributional models of semantics. The first approach to predicate modelling treats predicates as truth functions represented as tensors, while the second treats them as functions from subsets of the domain to subsets of the domain. Yet both representations of predicates contain the same information. Let MP and M&apos;P be the tensor representations of a predicate P under th</context>
</contexts>
<marker>Barwise, Cooper, 1981</marker>
<rawString>J. Barwise and R. Cooper Generalized quantifiers and natural language. Linguistics and philosophy, pages 159–219. Springer, 1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Blacoe</author>
<author>M Lapata</author>
</authors>
<title>A comparison of vector-based representations for semantic composition.</title>
<date>2012</date>
<booktitle>Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<contexts>
<context position="2745" citStr="Blacoe and Lapata, 2012" startWordPosition="392" endWordPosition="395"> approaches to language modelling to produce compositional distributional models of semantics, leveraging the learning mechanisms of distributional semantics, and providing syntax-sensitive operations for the production of representations of sentence meaning obtained through combination of corpus-inferred word meanings. These efforts have been met with some success in evaluations such as phrase similarity tasks (Mitchell and Lapata, 2008; Mitchell and Lapata, 2009; Grefenstette and Sadrzadeh, 2011; Kartsaklis et al., 2012), sentiment prediction (Socher et al., 2012), and paraphrase detection (Blacoe and Lapata, 2012). While these developments are promising with regard to the goal of obtaining learnable-yetstructured sentence-level representations of language meaning, part of the motivation for unifying formal and distributional models of semantics has been lost. The compositional aspects of formal semantics are combined with the corpus-based empirical aspects of distributional semantics in such models, yet the logical aspects are not. But it is these logical aspects which are so appealing in formal semantic models, and therefore it would be desirable to replicate the inferential powers of logic within 1 S</context>
</contexts>
<marker>Blacoe, Lapata, 2012</marker>
<rawString>W. Blacoe and M. Lapata. A comparison of vector-based representations for semantic composition. Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Bourbaki</author>
</authors>
<title>Commutative Algebra: Chapters 1-7. Springer-Verlag</title>
<date>1989</date>
<location>(Berlin and New York),</location>
<contexts>
<context position="13940" citStr="Bourbaki, 1989" startWordPosition="2214" endWordPosition="2215">columns of the first matrix covers the same number of columns as the index spanning the rows of the second matrix covers rows. Similarly to how the columns of one matrix ‘merge’ with the rows of another to produce a third matrix, the part of the first tensor spanned by the index k merges with the part of the second tensor spanned by k by ‘summing through’ the shared basis elements bVk αk of each tensor. Each tensor therefore loses a rank while being joined, explaining how the tensor produced by TxU is of rank k+(n−k+1)−2 = n−1. There exists an isomorphism between tensors and multilinear maps (Bourbaki, 1989; Lee, 1997), such that any curried multilinear map f:V1--�...--�Vj--�Vk can be represented as a tensor Tf E Vk ®Vj ®...®V1 (note the reversed order of the vector spaces), with tensor contraction acting as function application. This isomorphism guarantees that there exists such a tensor Tf for every f, such that the following equality holds for any v1 E V1, ... , vj E Vj: cvi bVi YT = T V vk fv1 ... vj = vk = Tf x v1 x ... x vj α1...αk cα1 ... αkbl α1 ® ... ® bαk In this paper, we will be dealing with tensors of rank 1 (vectors), rank 2 (matrices) and rank 3, which can be pictured as cuboids (</context>
</contexts>
<marker>Bourbaki, 1989</marker>
<rawString>N. Bourbaki. Commutative Algebra: Chapters 1-7. Springer-Verlag (Berlin and New York), 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>S Pulman</author>
</authors>
<title>Combining symbolic and distributional models of meaning.</title>
<date>2006</date>
<booktitle>In AAAI Spring Symposium on Quantum Interaction,</booktitle>
<contexts>
<context position="9920" citStr="Clark and Pulman (2006)" startWordPosition="1511" endWordPosition="1514">w years to provide vector composition functions for distributional models in order to introduce compositionality, thereby replicating some of the aspects of formal semantics while preserving learnability. Simple operations such as vector addition and multiplication, with or without scalar or matrix weights (to take word order or basic relational aspects into account), have been suggested (Zanzotto et al., 2010; Mitchell and Lapata, 2008; Mitchell and Lapata, 2009). Smolensky (1990) suggests using the tensor product of word vectors to produce representations that grow with sentence complexity. Clark and Pulman (2006) extend this approach by including basis vectors standing for dependency relations into tensor product-based representations. Both of these tensor product-based approaches run into dimensionality problems as representations of sentence meaning for sentences of different lengths or grammatical structure do not live in the same space, and thus cannot directly be compared. Coecke et al. (2010) develop a framework using category theory, solving this dimensionality problem of tensor-based models by projecting tensored vectors for sentences into a unique vector space for sentences, using functions d</context>
</contexts>
<marker>Clark, Pulman, 2006</marker>
<rawString>S. Clark and S. Pulman. Combining symbolic and distributional models of meaning. In AAAI Spring Symposium on Quantum Interaction, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Coecke</author>
<author>M Sadrzadeh</author>
<author>S Clark</author>
</authors>
<title>Mathematical Foundations for a Compositional Distributional Model of Meaning. Linguistic Analysis,</title>
<date>2010</date>
<volume>36</volume>
<pages>345--384</pages>
<contexts>
<context position="5056" citStr="Coecke et al. (2010)" startWordPosition="760" endWordPosition="764">lisms based only on multilinear maps. I conclude, in Section 7, by suggesting directions for further work based on the contents of this paper. This paper does not seek to address the question of how to determine how words should be translated into predicates and relations in the first place, but rather shows how such predicates and relations can be modelled using multilinear algebra. As such, it can be seen as a general theoretical contribution which is independent from the approaches to compositional distributional semantics it can be applied to. It is directly compatible with the efforts of Coecke et al. (2010) and Grefenstette et al. (2013), discussed below, but is also relevant to any other approach making use of tensors or matrices to encode semantic relations. 2 Related work Formal semantics, from the Montagovian school of thought (Montague,1974; Dowty et al., 1981), treats natural languages as programming languages which compile down to some formal language such as a predicate calculus. The syntax of natural languages, in the form of a grammar, is augmented by semantic interpretations, in the form of expressions from a higher order logic such as the lambda-beta calculus. The parse of a sentence</context>
<context position="10313" citStr="Coecke et al. (2010)" startWordPosition="1571" endWordPosition="1574">otto et al., 2010; Mitchell and Lapata, 2008; Mitchell and Lapata, 2009). Smolensky (1990) suggests using the tensor product of word vectors to produce representations that grow with sentence complexity. Clark and Pulman (2006) extend this approach by including basis vectors standing for dependency relations into tensor product-based representations. Both of these tensor product-based approaches run into dimensionality problems as representations of sentence meaning for sentences of different lengths or grammatical structure do not live in the same space, and thus cannot directly be compared. Coecke et al. (2010) develop a framework using category theory, solving this dimensionality problem of tensor-based models by projecting tensored vectors for sentences into a unique vector space for sentences, using functions dynamically generated by the syntactic structure of the sentences. In presenting their framework, which partly inspired this paper, they describe how a verb can be treated as a logical relation using tensors in order to evaluate the truth value of a simple sentence, as well as how negation can be modelled using matrices. A related approach, by Baroni and Zamparelli (2010), represents unary r</context>
<context position="15871" citStr="Coecke et al. (2010)" startWordPosition="2570" endWordPosition="2573">ations, and logical atoms of a predicate calculus. The four aspects of a predicate calculus we must replicate here using tensors are as follows: truth values, the logical domain and its elements (logical atoms), predicates, and relations. I will discuss logical connectives in the next section. Both truth values and domain objects are the basic elements of a predicate calculus, and therefore it makes sense to model them as vectors rather than higher rank tensors, which I will reserve for relations. We first must consider the vector space used Y α1...αn 4 to model the boolean truth values of B. Coecke et al. (2010) suggest, as boolean vector space, the space B with the basis {&gt;,⊥}, where &gt; = [1 0]&gt; is interpreted as ‘true’, and ⊥ = [0 1]&gt; as ‘false’. I assign to the domain D, the set of objects in our logic, a vector space D on R|D |with basis vectors {di}i which are in bijective correspondence with elements of D. An element of D is therefore represented as a one-hot vector in D, the single nonnull value of which is the weight for the basis vector mapped to that element of D. Similarly, a subset of D is a vector of D where those elements of D in the subset have 1 as their corresponding basis weights in </context>
<context position="20587" citStr="Coecke et al. (2010)" startWordPosition="3539" endWordPosition="3542">owing calculation: (Tloves × m) × j = ((&gt; ⊗ d2) + (&gt; ⊗ d1)) × j = &gt; whereas “John loves Mary” would correspond to the following calculation: (Tloves × j) × m = ((&gt; ⊗ d1) + (⊥ ⊗ d2)) × m = ⊥ 5 Logical connectives with tensors In this section, I discuss how the boolean connectives of a propositional calculus can be modelled using tensors. Combined with the predicate and relation representations discussed above, these form a complete quantifier-free predicate calculus based on tensors and tensor contraction. Negation has already been shown to be modelled in the boolean space described earlier by Coecke et al. (2010) as the swap matrix: &amp;quot; = 0 1 T¬ 1 0 &amp;quot; α · a1 + β · a2 α · b1 + β · b2 # = α · c1 + β · c2 α · d1 + β · d2 or more concretely: &amp;quot; a1 b1 T × &gt; = c1 d1 &amp;quot; a1 b1 T × ⊥ = c1 d1 c2 d2 0 c1 d1 #&amp;quot;0 # &amp;quot; a2 b2 # a2 b2 = c2 d2 1 c2 d2 owing operations: &amp;quot; 1 1 # (∨) 7→ T∨ = 1 0 0 0 0 1 &amp;quot; 1 0 # (∧) 7→ T∧ = 0 0 0 1 1 1 &amp;quot; 1 0 (→) 7→ T→ = 0 1 represents truth value vectors of the form where = 1. Applying the above logical operations to such vectors produces vectors with the same normalisation property. This is due to the fact that the columns of the component matrices are all normalised (i.e. each column sums to</context>
<context position="22136" citStr="Coecke et al. (2010)" startWordPosition="3942" endWordPosition="3945"> T¬ × ⊥ = = = &gt; 1 0 1 0 All other logical operators are binary, and hence modelled as rank 3 tensors. To make talking about rank 3 tensors used to model binary operations easier, I will use the following block matrix notation for 2 × 2 × 2 rank 3 tensors T: &amp;quot; a1 b1 T = c1 d1 # a2 b2 c2 d2 &amp;quot;= # &amp;quot; 1 # &amp;quot; a1 b1 # a2 b2 = Using this notation, we can define tensors for the foll # 1 1 0 0 I leave the trivial proof by exhaustion that these fit the bill to the reader. It is worth noting here that these tensors preserve normalised probabilities of truth. Let us consider amodel such at that described in Coecke et al. (2010) which, in lieu of boolean truth values, # α1α2 β1α2 + (α1 + β1)β2 tensor maps subsets of D (elements of D) to subsets of D containing only those objects of the original subset for which P holds (i.e. yielding another vector in D). To give an example: let us consider a domain with two dogs (a and b) and a cat (c). One of the dogs (b) is brown, as is the cat. Let S be the set of dogs, and P the predicate “brown”. I represent these statements in the model as follows: D = {a, b, c) S = {a, b) MP = {b, c) The set of dogs is represented as a vector S = [1 1 0]T and the predicate ‘brown’ as a tensor</context>
</contexts>
<marker>Coecke, Sadrzadeh, Clark, 2010</marker>
<rawString>B. Coecke, M. Sadrzadeh, and S. Clark. Mathematical Foundations for a Compositional Distributional Model of Meaning. Linguistic Analysis, volume 36, pages 345–384. March 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Curran</author>
</authors>
<title>From distributional to semantic similarity.</title>
<date>2004</date>
<tech>PhD thesis,</tech>
<contexts>
<context position="8072" citStr="Curran (2004)" startWordPosition="1236" endWordPosition="1237">ised by a term frequencyinverse document frequency-like measure (e.g. TFIDF, pointwise mutual information, ratio of probabilities), and are set as the basis weights of the vector representation of the word’s meaning. Word vectors can then be compared using geometric distance [[VPj([[NPj) [[cats], [[milkj, ... [[Vtj([[NPj) dyx.[[likej(x,y), ... 2 Figure 2: A simple distributional semantic model. metrics such as cosine similarity, allowing us to determine the similarity of words, cluster semantically related words, and so on. Excellent overviews of distributional semantic models are provided by Curran (2004) and Mitchell (2011). A simple distributional semantic model showing the spacial representation of words ‘dog’, ‘cat’ and ‘snake’ within the context of feature words ‘pet’, ‘furry’, and ‘stroke’ is shown in Figure 2. Distributional semantic models have been successfully applied to tasks such as word-sense discrimination (Sch¨utze, 1998), thesaurus extraction (Grefenstette, 1994), and automated essay marking (Landauer and Dumais, 1997). However, while such models provide tractable ways of learning and comparing word meanings, they do not naturally scale beyond word length. As recently pointed o</context>
</contexts>
<marker>Curran, 2004</marker>
<rawString>J. R. Curran. From distributional to semantic similarity. PhD thesis, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Dowty</author>
<author>R E Wall</author>
<author>S Peters</author>
</authors>
<title>Introduction to Montague Semantics.</title>
<date>1981</date>
<location>Dordrecht,</location>
<contexts>
<context position="5320" citStr="Dowty et al., 1981" startWordPosition="804" endWordPosition="807">lations in the first place, but rather shows how such predicates and relations can be modelled using multilinear algebra. As such, it can be seen as a general theoretical contribution which is independent from the approaches to compositional distributional semantics it can be applied to. It is directly compatible with the efforts of Coecke et al. (2010) and Grefenstette et al. (2013), discussed below, but is also relevant to any other approach making use of tensors or matrices to encode semantic relations. 2 Related work Formal semantics, from the Montagovian school of thought (Montague,1974; Dowty et al., 1981), treats natural languages as programming languages which compile down to some formal language such as a predicate calculus. The syntax of natural languages, in the form of a grammar, is augmented by semantic interpretations, in the form of expressions from a higher order logic such as the lambda-beta calculus. The parse of a sentence then determines the combinations of lambda-expressions, the reduction of which yields a well-formed formula of a predicate calculus, corresponding to the semantic representation of the sentence. A simple formal semantic model is illustrated in Figure 1. Syntactic</context>
</contexts>
<marker>Dowty, Wall, Peters, 1981</marker>
<rawString>D. R. Dowty, R. E. Wall, and S. Peters. Introduction to Montague Semantics. Dordrecht, 1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Firth</author>
</authors>
<title>A synopsis of linguistic theory 1930-1955. Studies in linguistic analysis,</title>
<date>1957</date>
<publisher>Springer Verlag,</publisher>
<contexts>
<context position="6931" citStr="Firth (1957)" startWordPosition="1055" endWordPosition="1056">nsistency verification, question answering, and a host of other tasks which are well developed in the literature (e.g. see (Loveland, 1978) and (Fitting, 1996)). However, the sophistication of such formal semantic models comes at a cost: the complex set of rules allowing for the logical interpretation of text must either be provided a priori, or learned. Learning such representations is a complex task, the difficulty of which is compounded by issues of ambiguity and polysemy which are pervasive in natural languages. In contrast, distributional semantic models, best summarised by the dictum of Firth (1957) that “You shall know a word by the company it keeps,” provide an elegant and tractable way of learning semantic representations of words from text. Word meanings are modelled as high-dimensional vectors in large semantic vector spaces, the basis elements of which correspond to contextual features such as other words from a lexicon. Semantic vectors for words are built by counting how many time a target word occurs within a context (e.g. within k words of select words from the lexicon). These context counts are then normalised by a term frequencyinverse document frequency-like measure (e.g. TF</context>
</contexts>
<marker>Firth, 1957</marker>
<rawString>J. R. Firth. A synopsis of linguistic theory 1930-1955. Studies in linguistic analysis, 1957. M. Fitting. First-order logic and automated theorem proving. Springer Verlag, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Grefenstette</author>
<author>G Dinu</author>
<author>Y Zhang</author>
<author>M Sadrzadeh</author>
<author>M Baroni</author>
</authors>
<title>Multi-step regression learning for compositional distributional semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of the Tenth International Conference on Computational Semantics. Association for Computational Linguistics,</booktitle>
<contexts>
<context position="5087" citStr="Grefenstette et al. (2013)" startWordPosition="766" endWordPosition="769">linear maps. I conclude, in Section 7, by suggesting directions for further work based on the contents of this paper. This paper does not seek to address the question of how to determine how words should be translated into predicates and relations in the first place, but rather shows how such predicates and relations can be modelled using multilinear algebra. As such, it can be seen as a general theoretical contribution which is independent from the approaches to compositional distributional semantics it can be applied to. It is directly compatible with the efforts of Coecke et al. (2010) and Grefenstette et al. (2013), discussed below, but is also relevant to any other approach making use of tensors or matrices to encode semantic relations. 2 Related work Formal semantics, from the Montagovian school of thought (Montague,1974; Dowty et al., 1981), treats natural languages as programming languages which compile down to some formal language such as a predicate calculus. The syntax of natural languages, in the form of a grammar, is augmented by semantic interpretations, in the form of expressions from a higher order logic such as the lambda-beta calculus. The parse of a sentence then determines the combinatio</context>
<context position="11097" citStr="Grefenstette et al. (2013)" startWordPosition="1688" endWordPosition="1691">que vector space for sentences, using functions dynamically generated by the syntactic structure of the sentences. In presenting their framework, which partly inspired this paper, they describe how a verb can be treated as a logical relation using tensors in order to evaluate the truth value of a simple sentence, as well as how negation can be modelled using matrices. A related approach, by Baroni and Zamparelli (2010), represents unary relations such as adjectives as matrices learned by linear regression from corpus data, and models adjective-noun composition as matrix-vector multiplication. Grefenstette et al. (2013) generalise this approach to relations of any arity and relate it to the framework of Coecke et al. (2010) using a tensor-based approach to formal semantic modelling similar to that presented in this paper. Finally, Socher et al. (2012) apply deep learning techniques to model syntax-sensitive vector composition using non-linear operations, effectively turning parse trees into multi-stage neural networks. Socher shows that the non-linear activation function used in such a neural network can be tailored to replicate the behaviour of basic logical connectives such as conjunction and negation. sna</context>
</contexts>
<marker>Grefenstette, Dinu, Zhang, Sadrzadeh, Baroni, 2013</marker>
<rawString>E. Grefenstette, G. Dinu, Y. Zhang, M. Sadrzadeh, and M. Baroni. Multi-step regression learning for compositional distributional semantics. In Proceedings of the Tenth International Conference on Computational Semantics. Association for Computational Linguistics, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Grefenstette</author>
<author>M Sadrzadeh</author>
</authors>
<title>Experimental support for a categorical compositional distributional model of meaning.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<contexts>
<context position="2623" citStr="Grefenstette and Sadrzadeh, 2011" startWordPosition="374" endWordPosition="377">ord meanings based on corpus data. Recent efforts, some of which are briefly reported below, have been made to unify both of these approaches to language modelling to produce compositional distributional models of semantics, leveraging the learning mechanisms of distributional semantics, and providing syntax-sensitive operations for the production of representations of sentence meaning obtained through combination of corpus-inferred word meanings. These efforts have been met with some success in evaluations such as phrase similarity tasks (Mitchell and Lapata, 2008; Mitchell and Lapata, 2009; Grefenstette and Sadrzadeh, 2011; Kartsaklis et al., 2012), sentiment prediction (Socher et al., 2012), and paraphrase detection (Blacoe and Lapata, 2012). While these developments are promising with regard to the goal of obtaining learnable-yetstructured sentence-level representations of language meaning, part of the motivation for unifying formal and distributional models of semantics has been lost. The compositional aspects of formal semantics are combined with the corpus-based empirical aspects of distributional semantics in such models, yet the logical aspects are not. But it is these logical aspects which are so appeal</context>
</contexts>
<marker>Grefenstette, Sadrzadeh, 2011</marker>
<rawString>E. Grefenstette and M. Sadrzadeh. Experimental support for a categorical compositional distributional model of meaning. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>Explorations in automatic thesaurus discovery.</title>
<date>1994</date>
<contexts>
<context position="8453" citStr="Grefenstette, 1994" startWordPosition="1290" endWordPosition="1291">tional semantic model. metrics such as cosine similarity, allowing us to determine the similarity of words, cluster semantically related words, and so on. Excellent overviews of distributional semantic models are provided by Curran (2004) and Mitchell (2011). A simple distributional semantic model showing the spacial representation of words ‘dog’, ‘cat’ and ‘snake’ within the context of feature words ‘pet’, ‘furry’, and ‘stroke’ is shown in Figure 2. Distributional semantic models have been successfully applied to tasks such as word-sense discrimination (Sch¨utze, 1998), thesaurus extraction (Grefenstette, 1994), and automated essay marking (Landauer and Dumais, 1997). However, while such models provide tractable ways of learning and comparing word meanings, they do not naturally scale beyond word length. As recently pointed out by Turney (2012), treating larger segments of texts as lexical units and learning their representations distributionally (the ‘holistic approach’) violates the principle of linguistic creativity, according to which we can formulate and understand phrases which we’ve never observed before, provided we know the meaning of their parts and how they are combined. As such, distribu</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>G. Grefenstette. Explorations in automatic thesaurus discovery. 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Kartsaklis</author>
<author>M Sadrzadeh</author>
<author>S Pulman</author>
</authors>
<title>A Unified Sentence Space for Categorical DistributionalCompositional Semantics: Theory and Experiments.</title>
<date>2012</date>
<booktitle>In Proceedings of 24th International Conference on Computational Linguistics (COLING 2012): Posters,</booktitle>
<contexts>
<context position="2649" citStr="Kartsaklis et al., 2012" startWordPosition="378" endWordPosition="381"> Recent efforts, some of which are briefly reported below, have been made to unify both of these approaches to language modelling to produce compositional distributional models of semantics, leveraging the learning mechanisms of distributional semantics, and providing syntax-sensitive operations for the production of representations of sentence meaning obtained through combination of corpus-inferred word meanings. These efforts have been met with some success in evaluations such as phrase similarity tasks (Mitchell and Lapata, 2008; Mitchell and Lapata, 2009; Grefenstette and Sadrzadeh, 2011; Kartsaklis et al., 2012), sentiment prediction (Socher et al., 2012), and paraphrase detection (Blacoe and Lapata, 2012). While these developments are promising with regard to the goal of obtaining learnable-yetstructured sentence-level representations of language meaning, part of the motivation for unifying formal and distributional models of semantics has been lost. The compositional aspects of formal semantics are combined with the corpus-based empirical aspects of distributional semantics in such models, yet the logical aspects are not. But it is these logical aspects which are so appealing in formal semantic mod</context>
</contexts>
<marker>Kartsaklis, Sadrzadeh, Pulman, 2012</marker>
<rawString>D. Kartsaklis, and M. Sadrzadeh and S. Pulman. A Unified Sentence Space for Categorical DistributionalCompositional Semantics: Theory and Experiments. In Proceedings of 24th International Conference on Computational Linguistics (COLING 2012): Posters, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>S T Dumais</author>
</authors>
<title>A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological review,</title>
<date>1997</date>
<contexts>
<context position="8510" citStr="Landauer and Dumais, 1997" startWordPosition="1296" endWordPosition="1299">larity, allowing us to determine the similarity of words, cluster semantically related words, and so on. Excellent overviews of distributional semantic models are provided by Curran (2004) and Mitchell (2011). A simple distributional semantic model showing the spacial representation of words ‘dog’, ‘cat’ and ‘snake’ within the context of feature words ‘pet’, ‘furry’, and ‘stroke’ is shown in Figure 2. Distributional semantic models have been successfully applied to tasks such as word-sense discrimination (Sch¨utze, 1998), thesaurus extraction (Grefenstette, 1994), and automated essay marking (Landauer and Dumais, 1997). However, while such models provide tractable ways of learning and comparing word meanings, they do not naturally scale beyond word length. As recently pointed out by Turney (2012), treating larger segments of texts as lexical units and learning their representations distributionally (the ‘holistic approach’) violates the principle of linguistic creativity, according to which we can formulate and understand phrases which we’ve never observed before, provided we know the meaning of their parts and how they are combined. As such, distributional semantics makes no effort to account for the compo</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>T. K. Landauer and S. T. Dumais. A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological review, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lee</author>
</authors>
<title>Riemannian manifolds: An introduction to curvature, volume 176.</title>
<date>1997</date>
<publisher>Springer Verlag,</publisher>
<contexts>
<context position="13952" citStr="Lee, 1997" startWordPosition="2216" endWordPosition="2217">irst matrix covers the same number of columns as the index spanning the rows of the second matrix covers rows. Similarly to how the columns of one matrix ‘merge’ with the rows of another to produce a third matrix, the part of the first tensor spanned by the index k merges with the part of the second tensor spanned by k by ‘summing through’ the shared basis elements bVk αk of each tensor. Each tensor therefore loses a rank while being joined, explaining how the tensor produced by TxU is of rank k+(n−k+1)−2 = n−1. There exists an isomorphism between tensors and multilinear maps (Bourbaki, 1989; Lee, 1997), such that any curried multilinear map f:V1--�...--�Vj--�Vk can be represented as a tensor Tf E Vk ®Vj ®...®V1 (note the reversed order of the vector spaces), with tensor contraction acting as function application. This isomorphism guarantees that there exists such a tensor Tf for every f, such that the following equality holds for any v1 E V1, ... , vj E Vj: cvi bVi YT = T V vk fv1 ... vj = vk = Tf x v1 x ... x vj α1...αk cα1 ... αkbl α1 ® ... ® bαk In this paper, we will be dealing with tensors of rank 1 (vectors), rank 2 (matrices) and rank 3, which can be pictured as cuboids (or a matrix </context>
</contexts>
<marker>Lee, 1997</marker>
<rawString>J. Lee. Riemannian manifolds: An introduction to curvature, volume 176. Springer Verlag, 1997. D. W. Loveland. Automated theorem proving: A logical basis. Elsevier North-Holland, 1978.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mitchell</author>
<author>M Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<volume>8</volume>
<contexts>
<context position="2562" citStr="Mitchell and Lapata, 2008" startWordPosition="366" endWordPosition="369">nd provides tractable ways of learning and comparing word meanings based on corpus data. Recent efforts, some of which are briefly reported below, have been made to unify both of these approaches to language modelling to produce compositional distributional models of semantics, leveraging the learning mechanisms of distributional semantics, and providing syntax-sensitive operations for the production of representations of sentence meaning obtained through combination of corpus-inferred word meanings. These efforts have been met with some success in evaluations such as phrase similarity tasks (Mitchell and Lapata, 2008; Mitchell and Lapata, 2009; Grefenstette and Sadrzadeh, 2011; Kartsaklis et al., 2012), sentiment prediction (Socher et al., 2012), and paraphrase detection (Blacoe and Lapata, 2012). While these developments are promising with regard to the goal of obtaining learnable-yetstructured sentence-level representations of language meaning, part of the motivation for unifying formal and distributional models of semantics has been lost. The compositional aspects of formal semantics are combined with the corpus-based empirical aspects of distributional semantics in such models, yet the logical aspects</context>
<context position="9737" citStr="Mitchell and Lapata, 2008" startWordPosition="1484" endWordPosition="1487">tional nature of language like formal semantics does, and ignores issues relating to syntactic and relational aspects of language. Several proposals have been put forth over the last few years to provide vector composition functions for distributional models in order to introduce compositionality, thereby replicating some of the aspects of formal semantics while preserving learnability. Simple operations such as vector addition and multiplication, with or without scalar or matrix weights (to take word order or basic relational aspects into account), have been suggested (Zanzotto et al., 2010; Mitchell and Lapata, 2008; Mitchell and Lapata, 2009). Smolensky (1990) suggests using the tensor product of word vectors to produce representations that grow with sentence complexity. Clark and Pulman (2006) extend this approach by including basis vectors standing for dependency relations into tensor product-based representations. Both of these tensor product-based approaches run into dimensionality problems as representations of sentence meaning for sentences of different lengths or grammatical structure do not live in the same space, and thus cannot directly be compared. Coecke et al. (2010) develop a framework usi</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>J. Mitchell and M. Lapata. Vector-based models of semantic composition. In Proceedings of ACL, volume 8, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mitchell</author>
<author>M Lapata</author>
</authors>
<title>Language models based on semantic composition.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>1</volume>
<pages>430--439</pages>
<contexts>
<context position="2589" citStr="Mitchell and Lapata, 2009" startWordPosition="370" endWordPosition="373">of learning and comparing word meanings based on corpus data. Recent efforts, some of which are briefly reported below, have been made to unify both of these approaches to language modelling to produce compositional distributional models of semantics, leveraging the learning mechanisms of distributional semantics, and providing syntax-sensitive operations for the production of representations of sentence meaning obtained through combination of corpus-inferred word meanings. These efforts have been met with some success in evaluations such as phrase similarity tasks (Mitchell and Lapata, 2008; Mitchell and Lapata, 2009; Grefenstette and Sadrzadeh, 2011; Kartsaklis et al., 2012), sentiment prediction (Socher et al., 2012), and paraphrase detection (Blacoe and Lapata, 2012). While these developments are promising with regard to the goal of obtaining learnable-yetstructured sentence-level representations of language meaning, part of the motivation for unifying formal and distributional models of semantics has been lost. The compositional aspects of formal semantics are combined with the corpus-based empirical aspects of distributional semantics in such models, yet the logical aspects are not. But it is these l</context>
<context position="9765" citStr="Mitchell and Lapata, 2009" startWordPosition="1488" endWordPosition="1491">ike formal semantics does, and ignores issues relating to syntactic and relational aspects of language. Several proposals have been put forth over the last few years to provide vector composition functions for distributional models in order to introduce compositionality, thereby replicating some of the aspects of formal semantics while preserving learnability. Simple operations such as vector addition and multiplication, with or without scalar or matrix weights (to take word order or basic relational aspects into account), have been suggested (Zanzotto et al., 2010; Mitchell and Lapata, 2008; Mitchell and Lapata, 2009). Smolensky (1990) suggests using the tensor product of word vectors to produce representations that grow with sentence complexity. Clark and Pulman (2006) extend this approach by including basis vectors standing for dependency relations into tensor product-based representations. Both of these tensor product-based approaches run into dimensionality problems as representations of sentence meaning for sentences of different lengths or grammatical structure do not live in the same space, and thus cannot directly be compared. Coecke et al. (2010) develop a framework using category theory, solving </context>
</contexts>
<marker>Mitchell, Lapata, 2009</marker>
<rawString>J. Mitchell and M. Lapata. Language models based on semantic composition. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1, pages 430–439. Association for Computational Linguistics, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Mitchell</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2011</date>
<tech>PhD thesis,</tech>
<contexts>
<context position="8092" citStr="Mitchell (2011)" startWordPosition="1239" endWordPosition="1240">quencyinverse document frequency-like measure (e.g. TFIDF, pointwise mutual information, ratio of probabilities), and are set as the basis weights of the vector representation of the word’s meaning. Word vectors can then be compared using geometric distance [[VPj([[NPj) [[cats], [[milkj, ... [[Vtj([[NPj) dyx.[[likej(x,y), ... 2 Figure 2: A simple distributional semantic model. metrics such as cosine similarity, allowing us to determine the similarity of words, cluster semantically related words, and so on. Excellent overviews of distributional semantic models are provided by Curran (2004) and Mitchell (2011). A simple distributional semantic model showing the spacial representation of words ‘dog’, ‘cat’ and ‘snake’ within the context of feature words ‘pet’, ‘furry’, and ‘stroke’ is shown in Figure 2. Distributional semantic models have been successfully applied to tasks such as word-sense discrimination (Sch¨utze, 1998), thesaurus extraction (Grefenstette, 1994), and automated essay marking (Landauer and Dumais, 1997). However, while such models provide tractable ways of learning and comparing word meanings, they do not naturally scale beyond word length. As recently pointed out by Turney (2012),</context>
</contexts>
<marker>Mitchell, 2011</marker>
<rawString>J. J. Mitchell. Composition in distributional models of semantics. PhD thesis, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Montague</author>
</authors>
<title>English as a Formal Language. Formal Semantics: The Essential Readings,</title>
<date>1974</date>
<marker>Montague, 1974</marker>
<rawString>R. Montague. English as a Formal Language. Formal Semantics: The Essential Readings, 1974.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<marker>Sch¨utze, 1998</marker>
<rawString>H. Sch¨utze. Automatic word sense discrimination. Computational linguistics, 24(1):97–123, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Smolensky</author>
</authors>
<title>Tensor product variable binding and the representation of symbolic structures in connectionist systems.</title>
<date>1990</date>
<journal>Artificial intelligence,</journal>
<pages>46--1</pages>
<contexts>
<context position="9783" citStr="Smolensky (1990)" startWordPosition="1492" endWordPosition="1493">nd ignores issues relating to syntactic and relational aspects of language. Several proposals have been put forth over the last few years to provide vector composition functions for distributional models in order to introduce compositionality, thereby replicating some of the aspects of formal semantics while preserving learnability. Simple operations such as vector addition and multiplication, with or without scalar or matrix weights (to take word order or basic relational aspects into account), have been suggested (Zanzotto et al., 2010; Mitchell and Lapata, 2008; Mitchell and Lapata, 2009). Smolensky (1990) suggests using the tensor product of word vectors to produce representations that grow with sentence complexity. Clark and Pulman (2006) extend this approach by including basis vectors standing for dependency relations into tensor product-based representations. Both of these tensor product-based approaches run into dimensionality problems as representations of sentence meaning for sentences of different lengths or grammatical structure do not live in the same space, and thus cannot directly be compared. Coecke et al. (2010) develop a framework using category theory, solving this dimensionalit</context>
</contexts>
<marker>Smolensky, 1990</marker>
<rawString>P. Smolensky. Tensor product variable binding and the representation of symbolic structures in connectionist systems. Artificial intelligence, 46(1-2):159–216, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>B Huval</author>
<author>C D Manning</author>
<author>A Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrixvector spaces.</title>
<date>2012</date>
<booktitle>Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1201--1211</pages>
<contexts>
<context position="2693" citStr="Socher et al., 2012" startWordPosition="384" endWordPosition="387">ted below, have been made to unify both of these approaches to language modelling to produce compositional distributional models of semantics, leveraging the learning mechanisms of distributional semantics, and providing syntax-sensitive operations for the production of representations of sentence meaning obtained through combination of corpus-inferred word meanings. These efforts have been met with some success in evaluations such as phrase similarity tasks (Mitchell and Lapata, 2008; Mitchell and Lapata, 2009; Grefenstette and Sadrzadeh, 2011; Kartsaklis et al., 2012), sentiment prediction (Socher et al., 2012), and paraphrase detection (Blacoe and Lapata, 2012). While these developments are promising with regard to the goal of obtaining learnable-yetstructured sentence-level representations of language meaning, part of the motivation for unifying formal and distributional models of semantics has been lost. The compositional aspects of formal semantics are combined with the corpus-based empirical aspects of distributional semantics in such models, yet the logical aspects are not. But it is these logical aspects which are so appealing in formal semantic models, and therefore it would be desirable to </context>
<context position="11333" citStr="Socher et al. (2012)" startWordPosition="1729" endWordPosition="1732">using tensors in order to evaluate the truth value of a simple sentence, as well as how negation can be modelled using matrices. A related approach, by Baroni and Zamparelli (2010), represents unary relations such as adjectives as matrices learned by linear regression from corpus data, and models adjective-noun composition as matrix-vector multiplication. Grefenstette et al. (2013) generalise this approach to relations of any arity and relate it to the framework of Coecke et al. (2010) using a tensor-based approach to formal semantic modelling similar to that presented in this paper. Finally, Socher et al. (2012) apply deep learning techniques to model syntax-sensitive vector composition using non-linear operations, effectively turning parse trees into multi-stage neural networks. Socher shows that the non-linear activation function used in such a neural network can be tailored to replicate the behaviour of basic logical connectives such as conjunction and negation. snake stroke cat dog pet furry 3 Tensors and multilinear maps Tensors are the mathematical objects dealt with in multilinear algebra just as vectors and matrices are the objects dealt with in linear algebra. In fact, tensors can be seen as</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>R. Socher, B. Huval, C.D. Manning, and A.Y Ng. Semantic compositionality through recursive matrixvector spaces. Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing, pages 1201–1211, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
</authors>
<title>Domain and function: A dual-space model of semantic relations and compositions.</title>
<date>2012</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<volume>44</volume>
<contexts>
<context position="8691" citStr="Turney (2012)" startWordPosition="1328" endWordPosition="1329">itchell (2011). A simple distributional semantic model showing the spacial representation of words ‘dog’, ‘cat’ and ‘snake’ within the context of feature words ‘pet’, ‘furry’, and ‘stroke’ is shown in Figure 2. Distributional semantic models have been successfully applied to tasks such as word-sense discrimination (Sch¨utze, 1998), thesaurus extraction (Grefenstette, 1994), and automated essay marking (Landauer and Dumais, 1997). However, while such models provide tractable ways of learning and comparing word meanings, they do not naturally scale beyond word length. As recently pointed out by Turney (2012), treating larger segments of texts as lexical units and learning their representations distributionally (the ‘holistic approach’) violates the principle of linguistic creativity, according to which we can formulate and understand phrases which we’ve never observed before, provided we know the meaning of their parts and how they are combined. As such, distributional semantics makes no effort to account for the compositional nature of language like formal semantics does, and ignores issues relating to syntactic and relational aspects of language. Several proposals have been put forth over the l</context>
</contexts>
<marker>Turney, 2012</marker>
<rawString>P. D. Turney. Domain and function: A dual-space model of semantic relations and compositions. Journal ofArtificial Intelligence Research, 44:533–585, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F M Zanzotto</author>
<author>I Korkontzelos</author>
<author>F Fallucchi</author>
<author>S Manandhar</author>
</authors>
<title>Estimating linear models for compositional distributional semantics.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>1263--1271</pages>
<contexts>
<context position="9710" citStr="Zanzotto et al., 2010" startWordPosition="1480" endWordPosition="1483">account for the compositional nature of language like formal semantics does, and ignores issues relating to syntactic and relational aspects of language. Several proposals have been put forth over the last few years to provide vector composition functions for distributional models in order to introduce compositionality, thereby replicating some of the aspects of formal semantics while preserving learnability. Simple operations such as vector addition and multiplication, with or without scalar or matrix weights (to take word order or basic relational aspects into account), have been suggested (Zanzotto et al., 2010; Mitchell and Lapata, 2008; Mitchell and Lapata, 2009). Smolensky (1990) suggests using the tensor product of word vectors to produce representations that grow with sentence complexity. Clark and Pulman (2006) extend this approach by including basis vectors standing for dependency relations into tensor product-based representations. Both of these tensor product-based approaches run into dimensionality problems as representations of sentence meaning for sentences of different lengths or grammatical structure do not live in the same space, and thus cannot directly be compared. Coecke et al. (20</context>
</contexts>
<marker>Zanzotto, Korkontzelos, Fallucchi, Manandhar, 2010</marker>
<rawString>F. M. Zanzotto, I. Korkontzelos, F. Fallucchi, and S. Manandhar. Estimating linear models for compositional distributional semantics. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 1263–1271. Association for Computational Linguistics, 2010.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>