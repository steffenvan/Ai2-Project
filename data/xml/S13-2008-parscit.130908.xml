<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.184626">
<title confidence="0.998609">
HsH: Estimating Semantic Similarity of Words and Short Phrases with
Frequency Normalized Distance Measures
</title>
<author confidence="0.998215">
Christian Wartena
</author>
<affiliation confidence="0.9951925">
Hochschule Hannover – University of Applied Sciences and Arts
Department of Information and Communication
</affiliation>
<address confidence="0.794857">
Expo Plaza 12, 30539 Hannover, Germany
</address>
<email confidence="0.770533">
Christian.Wartena@hs-hannover.de
</email>
<sectionHeader confidence="0.990799" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99997075">
This paper describes the approach of the
Hochschule Hannover to the SemEval 2013
Task Evaluating Phrasal Semantics. In or-
der to compare a single word with a two word
phrase we compute various distributional sim-
ilarities, among which a new similarity mea-
sure, based on Jensen-Shannon Divergence
with a correction for frequency effects. The
classification is done by a support vector ma-
chine that uses all similarities as features. The
approach turned out to be the most successful
one in the task.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999166789473684">
The task Evaluating Phrasal Semantics of the 2013
International Workshop on Semantic Evaluation
(Manandhar and Yuret, 2013) consists of two sub-
tasks. For the first subtask a list of pairs consisting
of a single word and a two word phrase are given.
For the English task a labeled list of 11,722 pairs
was provided for training and a test set with 3,906
unlabeled examples. For German the training set
contains 2,202 and the test set 732 pairs. The system
should be able to tell whether the two word phrase
is a definition of the single word or not. This task
is somewhat different from the usual perspective of
finding synonyms, since definitions are usually more
general than the words they define.
In distributional semantics words are represented
by context vectors and similarities of these con-
text vectors are assumed to reflect similarities of
the words they represent. We compute context vec-
tors for all words using the lemmatized version of
</bodyText>
<page confidence="0.988033">
48
</page>
<bodyText confidence="0.9998102">
the Wacky Corpora for English (UKWaC, approxi-
mately 2,2 billion words) and German (DeWaC, 1,7
billion words) (Baroni et al., 2009). For the phrases
we compute the context vectors as well directly on
the base of occurrences of that phrase, as well as
by construction from the context vectors of the two
components. For the similarities between the vec-
tors we use Jensen-Shannon divergence (JSD) and
cosine similarity. Since the JSD is extremely depen-
dent on the number of occurrences of the words, we
define a new similarity measure that corrects for this
dependency. Since none of the measures gives satis-
factory results, we use all measures to train a support
vector machine that classifies the pairs.
The remainder of this paper is organized as fol-
lows. We start with an overview of related work. In
section 3 we discuss the dependence of JSD on word
frequency and introduce a new similarity measure.
Section 4 then describes the system. The results are
given in section 5 and are discussed in section 6.
</bodyText>
<sectionHeader confidence="0.999766" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.997788276595745">
Though distributional similarity has widely been
studied and has become an established method to
find similar words, there is no consensus on the way
the context of a word has to be defined and on the
best way to compute the similarity between two con-
texts. In the most general definitions the context of
a word consists of a number of words and their re-
lation to the given word (Grefenstette, 1992; Curran
and Moens, 2002). In the following we will only
consider the simplest case in which there is only one
relation: the relation of being in the same sentence.
Each word can be represented by a so called con-
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 48–52, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
text vector in a high dimensional word space. Since
these vectors will be sparse, often dimensionality re-
duction techniques are applied. In the present pa-
per we use random indexing, introduced by Karlgren
and Sahlgren (2001) and Sahlgren (2005) to reduce
the size of the context vectors.
The way in which the context vectors are con-
structed also determines what similarity measures
are suited. For random indexing G¨ornerup and Karl-
gren (2010) found that best results are obtained us-
ing L1-norm or Jensen-Shannon divergence (JSD).
they also report that these measures highly correlate.
We could confirm this in a preliminary experiment
and therefore only use JSD in the following.
Recently, the question whether and how an ap-
propriate context vector for a phrase can be de-
rived from the context vectors of its components
has become a central issue in distributional seman-
tics (Clark and Pulman, 2007; Mitchell and Lap-
ata, 2008; Widdows, 2008; Clarke et al., 2008). It
is not yet clear which way of combining the vec-
tors of the components is best suited for what goals.
Giesbrecht (2010) and Mitchell and Lapata (2008)
e.g. find that for noun-noun compounds the prod-
uct of context vectors (corresponding to the intersec-
tion of contexts) and more complex tensor products
give best results, while Guevara (2011) obtains best
results for adjective-noun phrases with addition of
vectors (corresponding to union of contexts). Since
we do not (yet) have a single best similarity mea-
sure to distinguish definitions from non-definitions,
we use a combination of similarity measures to train
a model as e.g. also was done by B¨ar et al. (2012).
</bodyText>
<sectionHeader confidence="0.9732765" genericHeader="method">
3 Frequency Dependency Correction of
Jensen-Shannon Divergence
</sectionHeader>
<bodyText confidence="0.999988111111111">
Weeds et al. (2004) observed that in tasks in which
related words have to be found, some measures pre-
fer words with a frequency similar to that of the tar-
get word while others prefer high frequent words,
regardless of the frequency of the target word. Since
G¨ornerup and Karlgren (2010) found that L1-norm
and JSD give best results for similarity of random
index vectors, we are especially interested in JSD.
The JSD of two distributions p and q is given by
</bodyText>
<equation confidence="0.972246">
JSD(p,q) = 12D(p||12p+12q)+12D(q||12p+12q) (1)
where D(p||q) = ��p(i)lo� �(��
log �(�� is the Kullback-
</equation>
<bodyText confidence="0.999932176470588">
Leibler divergence. We will follow the usual termi-
nology of context vectors. However, we will always
normalize the vectors, such that they can be inter-
preted as probability mass distributions. According
to Weeds et al. (2004) the JSD belongs to the cat-
egory of distance measures that tends to give small
distances for highly frequent words. In Wartena et
al. (2010) we also made this observation and there-
fore we added an additional constraint on the selec-
tion of keywords that should avoid the selection of
too general words. In the present paper we try to ex-
plicitly model the dependency between the JSD and
the number of occurrences of the involved words.
We then use the difference between the JSD of the
co-occurrence vectors of two words and the JSD ex-
pected on the base of the frequency of these words
as a similarity measure. In the following we will use
the dependency between the JSD and the frequency
of the words directly. In (Wartena, 2013) we model
the JSD instead as a function of the number of non
zero values in the context vectors. The latter depen-
dency can be modeled by a simpler function, but did
not work as well with the SemEval data set.
Given two words w1 and w2 the JSD of their con-
text vectors can be modeled as a function of the min-
imum of the number of occurrences of w1 and w2.
Figure 3 shows the JSD of the context vectors of the
words of the training set and the context vector of
the definition phrase. In this figure the JSD of the
positive and the negative examples is marked with
different marks. The lower bound of the negative
examples is roughly marked by a (red) curve, that is
defined for context vectors c1 and c2 for words w1
and w2, respectively, by
</bodyText>
<equation confidence="0.997905">
1
JSDexp(c1, c2) = a + (2)
�nb + c
</equation>
<bodyText confidence="0.998983333333333">
where n� = min(n(w1), n(w2)) with n(w) the num-
ber of occurrences of w in the corpus and with a,
b and c constants that are estimated for each set of
word pairs. For the pairs from the English training
and test set the values are: a = 0.15, b = 0.3 and
c = 0.5. Experiments on the training data showed
that the final results are not very dependent on the
exact values of these constants.
Finally, our new measure is simply defined by
</bodyText>
<equation confidence="0.987923">
JSDnorm(p, q) =
JSD(p, q) − JSDexp(p, q). (3)
</equation>
<page confidence="0.997249">
49
</page>
<figureCaption confidence="0.990954666666667">
Figure 1: JSD (y-axis) of all pairs in the English training set versus the number of occurrences of the definition phrase
(x-axis) in the UkWaC-Corpus. The positives examples are marked by a +, the negative examples by a ×. Most
positive examples are hidden behind the negative ones. The solid (red) line gives the expected JSD.
</figureCaption>
<sectionHeader confidence="0.991217" genericHeader="method">
4 System Description
</sectionHeader>
<bodyText confidence="0.999992294117647">
The main assumption for our approach is, that a
word and its definition are distributionally more sim-
ilar than a word and an arbitrary definition. We use
random indexing to capture distributional properties
of words and phrases. Since similarity measures for
random index vectors have biases for frequent or in-
frequent pairs, we use a combination of different
measures. For the two-word definition phrases we
can either estimate the context vector on the base of
the two words that make up the phrase, or compute it
directly from occurrences of the whole phrase in the
corpus. The latter method has the advantage of being
independent of assumptions about semantic compo-
sition, but might have the problem that it is based
on a few examples only. Thus we use both distribu-
tions, and also include the similarities between the
single word and each of the words of the definition.
</bodyText>
<subsectionHeader confidence="0.974902">
4.1 Distributions
</subsectionHeader>
<bodyText confidence="0.980203333333334">
Consider a pair (w, d) with w a word and d a defi-
nition consisting of two words: d = (d1, d2). Now
for each of the words w, d1, d2 and the multiword
d we compute context vectors using the random in-
dexing technique. The context vectors are computed
over the complete Wacky corpus. The context used
for a word are all open-class words (i.e. Noun, Verb,
Adjective, Adverb, etc. but not Auxiliary, Pronoun,
etc.) in a sentence. Each word is represented by a
random index vector of 10 000 dimensions in which
8 random positions have a non-zero value. The ran-
dom vectors of all words in all contexts are summed
up to construct context vectors (with length 10 000),
denoted vw, vd, vd1, vd2. In many cases there are
only very few occurrences of d, making the context
vector vd very unreliable. Thus we also compute the
vectors vadd d= vd1 + vd2 and vmult
d = vd1 · vd2. Fi-
nally, we also compute the general context vector (or
background distribution) vgen which is the context
vector obtained by aggregating all used contexts.
</bodyText>
<subsectionHeader confidence="0.955458">
4.2 Similarities
</subsectionHeader>
<bodyText confidence="0.999951857142857">
Table 1 gives an overview of the similarities com-
puted for the context vector vw. In addition we also
compute D(vw||vgen), D(vd||vgen), D(vd1||vgen),
D(vd2||vgen). The original intuition was that the def-
inition of a word is usual given as a more general
term or hypernym. It turned out that this is not the
case. However, in combination with other features
these divergences proved to be useful for the ma-
chine learning algorithm. Finally, we also use the
direct (first-order) co-occurrence between w and d
by computing the ratio between the probability with
which we expect w and d to co-occur in one sentence
if they would be independent, and the real probabil-
ity of co-occurrence found in the corpus:
</bodyText>
<equation confidence="0.963157">
co-occurrence-ratio(w, d) = p(w, d)
p(w) · p(d) (4)
</equation>
<page confidence="0.995253">
50
</page>
<tableCaption confidence="0.992585">
Table 1: Similarity measures used to compute the simi-
larity of a context vector of some word to various context
vectors for a phrase d = (d1, d2).
</tableCaption>
<table confidence="0.998259">
vd vd1 vd2 vadd vmult d
d
jsd ✓ ✓ ✓ ✓
jsd-norm ✓ ✓ ✓ ✓
cossim ✓ ✓
</table>
<tableCaption confidence="0.9313245">
Table 2: Results for English and German (no names
dataset). Results on train sets are averaged results from
10-fold cross validation. Results on the test set are the
official task results.
</tableCaption>
<table confidence="0.9995504">
AUC Accuracy F-Measure
Train English 0.88 0.80 0.79
Test English - 0.80 0.79
Train German 0.90 0.83 0.82
Test German - 0.83 0.81
</table>
<bodyText confidence="0.993521764705882">
where p(w, d) is the probability that w and d are
found in the same sentence, and p(w), with w a word
or phrase, the probability that a sentence contains w.
For the computation of JSDnorm(vw, vadd
d ) we
need the number of occurrences on which vadd dis
based. As an estimate for this number we use
max(n(d1), n(d2)). The constants a, b and c in
equation 2 are set to the following values: for
all cases a = 0.15; for JSDnorm(vw, vd) we let
b = 0.3 and c = 0.5; for JSDnorm(vw, vd1) and
JSDnorm(vw, vd2) we let b = 0.35 and c = −0.1; for
JSDnorm(vw, vadd
d ) we let b = 0.4 and c = −0.1. For
the German subtask a = 0.28 and slightly different
values for b and c were used to account for slightly
different frequency dependencies.
</bodyText>
<subsectionHeader confidence="0.999908">
4.3 Combining Similarities
</subsectionHeader>
<bodyText confidence="0.9995472">
The 15 attributes for each pair obtained in this way
are used to train a support vector machine (SVM)
using LibSVM (Chang and Lin, 2011). Optimal pa-
rameters for the SVM were found by grid-search and
10-fold cross validation on the training data.
</bodyText>
<sectionHeader confidence="0.999874" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.9992172">
In Table 2 the results are summarized. Since the
task can also be seen as a ranking task, we include
the Area Under the ROC-Curve (AUC) as a classi-
cal measure for ranking quality. We can observe that
the results are highly stable between training set and
</bodyText>
<tableCaption confidence="0.9854405">
Table 3: Results for English train set (average from 10-
fold cross validation) using one feature
</tableCaption>
<table confidence="0.9931674375">
feature Accuracy AUC
jsd(vw,vd) 0.50 0.57
jsdnorm(vw, vd) 0.59 0.70
jsd(vw, vd1) 0.54 0.63
jsdnorm(vw, vd1) 0.61 0.69
jsd(vw, vd2) 0.57 0.65
jsdnorm(vw, vd2) 0.63 0.71
jsd(vw, vadd 0.59 0.67
d ) 0.66 0.74
jsdnorm(vw, vadd 0.69 0.76
d ) 0.62 0.71
cossim(vw, vadd 0.61 0.71
d )
cossim(vw, vmult
d )
co-occ-ratio(w, d)
</table>
<bodyText confidence="0.999622">
test set and across languages. Table 3 gives the re-
sults that are obtained on the training set using one
feature. We can observe that the normalized versions
of the JSD always perform better than the JSD itself.
Furthermore, we see that for the composed vectors
the cosine performs better than the normalized JSD,
while it performs worse than JSD for the other vec-
tors (not displayed in the table). This eventually can
be explained by the fact that we have to estimate the
number of contexts for the calculation of jsdexp.
</bodyText>
<sectionHeader confidence="0.996155" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9998559">
Though there are a number of ad-hoc decisions in
the system the approach was very successful and
performed best in the SemEval task on phrasal se-
mantics. The main insight from the development
of the system is, that there is not yet a single best
similarity measure to compare random index vec-
tors. The normalized JSD turns out to be a useful
improvement of the JSD but is problematic for con-
structed context vectors, the formula in equation (2)
is rather ad hoc and the constants are just rough esti-
mates. The formulation in (Wartena, 2013) might be
a step in the right direction, but also there we are still
far away from a unbiased similarity measure with a
well founded theoretical basis.
Finally, it is unclear, what is the best way to rep-
resent a phrase in distributional similarity. Here we
use three different vectors in parallel. It would be
more elegant if we had a way to merge context vec-
tors based on direct observations of the phrase with
a constructed context vector.
</bodyText>
<page confidence="0.997992">
51
</page>
<sectionHeader confidence="0.990291" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9995062875">
Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. Ukp: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. In Proceedings of the Sixth International Work-
shop on Semantic Evaluation (SemEval-2012), pages
435–440.
M. Baroni, S. Bernardini, A. Ferraresi, and E. Zanchetta.
2009. The wacky wide web: A collection of very large
linguistically processed web-crawled corpora. Lan-
guage Resources and Evaluation 43 (3): 209-226,
43(3):209–226.
C.-C. Chang and C.-J. Lin. 2011. Libsvm : a library
for support vector machines. ACM Transactions on
Intelligent Systems and Technology, 2(3):1–27.
Stephen Clark and Stephen Pulman. 2007. Combining
symbolic and distributional models of meaning. In
Proceedings of the AAAI Spring Symposium on Quan-
tum Interaction, pages 52–55.
Daoud Clarke, Rudi Lutz, and David Weir. 2008. Se-
mantic composition with quotient algebras. In Pro-
ceedings of the 9th International Conference on Com-
putational Semantics (IWCS 2011).
James R. Curran and Marc Moens. 2002. Improve-
ments in automatic thesaurus extraction. In Unsuper-
vised Lexical Acquisition: Proceedings of the Work-
shop of the ACL Special Interest Group on the Lexi-
con (SIGLAX)., pages 59–66. Association of Compu-
tational Linguistics.
Eugenie Giesbrecht. 2010. Towards a matrix-based dis-
tributional model of meaning. In Proceedings of the
NAACL HLT 2010 Student Research Workshop, pages
23–28, Los Angeles, California. ACL.
Olaf G¨ornerup and Jussi Karlgren. 2010. Cross-lingual
comparison between distributionally determined word
similarity networks. In Proceedings of the 2010 Work-
shop on Graph-based Methods for Natural Language
Processing, pages 48–54. ACL.
Gregory Grefenstette. 1992. Use of syntactic context to
produce term association lists for text retrieval. In SI-
GIR ’92: Proceedings of the 15th annual international
ACM SIGIR conference on Research and development
in information retrieval, pages 89–97. ACM.
Emiliano Guevara. 2011. Computing semantic compo-
sitionality in distributional semantics. In Proceedings
of the 9th International Conference on Computational
Semantics (IWCS 2011), pages 135–144.
Jussi Karlgren and Magnus Sahlgren. 2001. From words
to understanding. In Foundations of Real-World Intel-
ligence, pages 294–308. CSLI Publications, Stanford,
California.
Suresh Manandhar and Deniz Yuret, editors. 2013. Pro-
ceedings of the 7th International Workshop on Seman-
tic Evaluation (SemEval 2013), in conjunction with
the Second Joint Conference on Lexical and Compu-
tational Semantcis (*SEM 2013), Atlanta.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In ACL 2008, Pro-
ceedings of the 46th Annual Meeting of the Association
for Computational Linguistics, pages 236–244.
Magnus Sahlgren. 2005. An introduction to random
indexing. In Methods and Applications of Seman-
tic Indexing Workshop at the 7th International Con-
ference on Terminology and Knowledge Engineering,
TKE, volume 5.
Christian Wartena, Rogier Brussee, and Wouter
Slakhorst. 2010. Keyword extraction using word
co-occurrence. In Database and Expert Systems
Applications (DEXA), 2010 Workshop on, pages
54–58. IEEE.
Christian Wartena. 2013. Distributional similarity of
words with different frequencies. In Proceedings of
the Dutch-Belgian Information Retrieval Workshop,
Delft. To Appear.
Julie Weeds, David J. Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional simi-
larity. In COLING 2004, Proceedings of the 20th In-
ternational Conference on Computational Linguistics.
Dominic Widdows. 2008. Semantic vector products:
Some initial investigations. In Second Conference on
Quantum Interaction, Oxford, 26th 28th March 2008.
</reference>
<page confidence="0.998812">
52
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.256214">
<title confidence="0.9994565">HsH: Estimating Semantic Similarity of Words and Short Phrases with Frequency Normalized Distance Measures</title>
<author confidence="0.987279">Christian</author>
<affiliation confidence="0.710831">Hochschule Hannover – University of Applied Sciences and Department of Information and Expo Plaza 12, 30539 Hannover,</affiliation>
<email confidence="0.834299">Christian.Wartena@hs-hannover.de</email>
<abstract confidence="0.996986769230769">This paper describes the approach of the Hochschule Hannover to the SemEval 2013 Phrasal In order to compare a single word with a two word phrase we compute various distributional similarities, among which a new similarity measure, based on Jensen-Shannon Divergence with a correction for frequency effects. The classification is done by a support vector machine that uses all similarities as features. The approach turned out to be the most successful one in the task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel B¨ar</author>
<author>Chris Biemann</author>
<author>Iryna Gurevych</author>
<author>Torsten Zesch</author>
</authors>
<title>Ukp: Computing semantic textual similarity by combining multiple content similarity measures.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval-2012),</booktitle>
<pages>435--440</pages>
<marker>B¨ar, Biemann, Gurevych, Zesch, 2012</marker>
<rawString>Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten Zesch. 2012. Ukp: Computing semantic textual similarity by combining multiple content similarity measures. In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval-2012), pages 435–440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>S Bernardini</author>
<author>A Ferraresi</author>
<author>E Zanchetta</author>
</authors>
<title>The wacky wide web: A collection of very large linguistically processed web-crawled corpora.</title>
<date>2009</date>
<journal>Language Resources and Evaluation</journal>
<volume>43</volume>
<issue>3</issue>
<pages>209--226</pages>
<contexts>
<context position="1906" citStr="Baroni et al., 2009" startWordPosition="300" endWordPosition="303">be able to tell whether the two word phrase is a definition of the single word or not. This task is somewhat different from the usual perspective of finding synonyms, since definitions are usually more general than the words they define. In distributional semantics words are represented by context vectors and similarities of these context vectors are assumed to reflect similarities of the words they represent. We compute context vectors for all words using the lemmatized version of 48 the Wacky Corpora for English (UKWaC, approximately 2,2 billion words) and German (DeWaC, 1,7 billion words) (Baroni et al., 2009). For the phrases we compute the context vectors as well directly on the base of occurrences of that phrase, as well as by construction from the context vectors of the two components. For the similarities between the vectors we use Jensen-Shannon divergence (JSD) and cosine similarity. Since the JSD is extremely dependent on the number of occurrences of the words, we define a new similarity measure that corrects for this dependency. Since none of the measures gives satisfactory results, we use all measures to train a support vector machine that classifies the pairs. The remainder of this paper</context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>M. Baroni, S. Bernardini, A. Ferraresi, and E. Zanchetta. 2009. The wacky wide web: A collection of very large linguistically processed web-crawled corpora. Language Resources and Evaluation 43 (3): 209-226, 43(3):209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-C Chang</author>
<author>C-J Lin</author>
</authors>
<title>Libsvm : a library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<volume>2</volume>
<issue>3</issue>
<contexts>
<context position="12590" citStr="Chang and Lin, 2011" startWordPosition="2187" endWordPosition="2190">te for this number we use max(n(d1), n(d2)). The constants a, b and c in equation 2 are set to the following values: for all cases a = 0.15; for JSDnorm(vw, vd) we let b = 0.3 and c = 0.5; for JSDnorm(vw, vd1) and JSDnorm(vw, vd2) we let b = 0.35 and c = −0.1; for JSDnorm(vw, vadd d ) we let b = 0.4 and c = −0.1. For the German subtask a = 0.28 and slightly different values for b and c were used to account for slightly different frequency dependencies. 4.3 Combining Similarities The 15 attributes for each pair obtained in this way are used to train a support vector machine (SVM) using LibSVM (Chang and Lin, 2011). Optimal parameters for the SVM were found by grid-search and 10-fold cross validation on the training data. 5 Results In Table 2 the results are summarized. Since the task can also be seen as a ranking task, we include the Area Under the ROC-Curve (AUC) as a classical measure for ranking quality. We can observe that the results are highly stable between training set and Table 3: Results for English train set (average from 10- fold cross validation) using one feature feature Accuracy AUC jsd(vw,vd) 0.50 0.57 jsdnorm(vw, vd) 0.59 0.70 jsd(vw, vd1) 0.54 0.63 jsdnorm(vw, vd1) 0.61 0.69 jsd(vw, v</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>C.-C. Chang and C.-J. Lin. 2011. Libsvm : a library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2(3):1–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>Stephen Pulman</author>
</authors>
<title>Combining symbolic and distributional models of meaning.</title>
<date>2007</date>
<booktitle>In Proceedings of the AAAI Spring Symposium on Quantum Interaction,</booktitle>
<pages>52--55</pages>
<contexts>
<context position="4556" citStr="Clark and Pulman, 2007" startWordPosition="743" endWordPosition="746">t vectors. The way in which the context vectors are constructed also determines what similarity measures are suited. For random indexing G¨ornerup and Karlgren (2010) found that best results are obtained using L1-norm or Jensen-Shannon divergence (JSD). they also report that these measures highly correlate. We could confirm this in a preliminary experiment and therefore only use JSD in the following. Recently, the question whether and how an appropriate context vector for a phrase can be derived from the context vectors of its components has become a central issue in distributional semantics (Clark and Pulman, 2007; Mitchell and Lapata, 2008; Widdows, 2008; Clarke et al., 2008). It is not yet clear which way of combining the vectors of the components is best suited for what goals. Giesbrecht (2010) and Mitchell and Lapata (2008) e.g. find that for noun-noun compounds the product of context vectors (corresponding to the intersection of contexts) and more complex tensor products give best results, while Guevara (2011) obtains best results for adjective-noun phrases with addition of vectors (corresponding to union of contexts). Since we do not (yet) have a single best similarity measure to distinguish defi</context>
</contexts>
<marker>Clark, Pulman, 2007</marker>
<rawString>Stephen Clark and Stephen Pulman. 2007. Combining symbolic and distributional models of meaning. In Proceedings of the AAAI Spring Symposium on Quantum Interaction, pages 52–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daoud Clarke</author>
<author>Rudi Lutz</author>
<author>David Weir</author>
</authors>
<title>Semantic composition with quotient algebras.</title>
<date>2008</date>
<booktitle>In Proceedings of the 9th International Conference on Computational Semantics (IWCS</booktitle>
<contexts>
<context position="4620" citStr="Clarke et al., 2008" startWordPosition="754" endWordPosition="757">so determines what similarity measures are suited. For random indexing G¨ornerup and Karlgren (2010) found that best results are obtained using L1-norm or Jensen-Shannon divergence (JSD). they also report that these measures highly correlate. We could confirm this in a preliminary experiment and therefore only use JSD in the following. Recently, the question whether and how an appropriate context vector for a phrase can be derived from the context vectors of its components has become a central issue in distributional semantics (Clark and Pulman, 2007; Mitchell and Lapata, 2008; Widdows, 2008; Clarke et al., 2008). It is not yet clear which way of combining the vectors of the components is best suited for what goals. Giesbrecht (2010) and Mitchell and Lapata (2008) e.g. find that for noun-noun compounds the product of context vectors (corresponding to the intersection of contexts) and more complex tensor products give best results, while Guevara (2011) obtains best results for adjective-noun phrases with addition of vectors (corresponding to union of contexts). Since we do not (yet) have a single best similarity measure to distinguish definitions from non-definitions, we use a combination of similarity</context>
</contexts>
<marker>Clarke, Lutz, Weir, 2008</marker>
<rawString>Daoud Clarke, Rudi Lutz, and David Weir. 2008. Semantic composition with quotient algebras. In Proceedings of the 9th International Conference on Computational Semantics (IWCS 2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Marc Moens</author>
</authors>
<title>Improvements in automatic thesaurus extraction. In Unsupervised Lexical Acquisition:</title>
<date>2002</date>
<journal>Association of Computational Linguistics.</journal>
<booktitle>Proceedings of the Workshop of the ACL Special Interest Group on the Lexicon (SIGLAX).,</booktitle>
<pages>59--66</pages>
<contexts>
<context position="3221" citStr="Curran and Moens, 2002" startWordPosition="530" endWordPosition="533"> the dependence of JSD on word frequency and introduce a new similarity measure. Section 4 then describes the system. The results are given in section 5 and are discussed in section 6. 2 Related Work Though distributional similarity has widely been studied and has become an established method to find similar words, there is no consensus on the way the context of a word has to be defined and on the best way to compute the similarity between two contexts. In the most general definitions the context of a word consists of a number of words and their relation to the given word (Grefenstette, 1992; Curran and Moens, 2002). In the following we will only consider the simplest case in which there is only one relation: the relation of being in the same sentence. Each word can be represented by a so called conSecond Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 48–52, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics text vector in a high dimensional word space. Since these vectors will be sparse, often dimensionality reduction techniques are applied. In the present paper we use</context>
</contexts>
<marker>Curran, Moens, 2002</marker>
<rawString>James R. Curran and Marc Moens. 2002. Improvements in automatic thesaurus extraction. In Unsupervised Lexical Acquisition: Proceedings of the Workshop of the ACL Special Interest Group on the Lexicon (SIGLAX)., pages 59–66. Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugenie Giesbrecht</author>
</authors>
<title>Towards a matrix-based distributional model of meaning.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Student Research Workshop,</booktitle>
<pages>23--28</pages>
<publisher>ACL.</publisher>
<location>Los Angeles, California.</location>
<contexts>
<context position="4743" citStr="Giesbrecht (2010)" startWordPosition="779" endWordPosition="780">re obtained using L1-norm or Jensen-Shannon divergence (JSD). they also report that these measures highly correlate. We could confirm this in a preliminary experiment and therefore only use JSD in the following. Recently, the question whether and how an appropriate context vector for a phrase can be derived from the context vectors of its components has become a central issue in distributional semantics (Clark and Pulman, 2007; Mitchell and Lapata, 2008; Widdows, 2008; Clarke et al., 2008). It is not yet clear which way of combining the vectors of the components is best suited for what goals. Giesbrecht (2010) and Mitchell and Lapata (2008) e.g. find that for noun-noun compounds the product of context vectors (corresponding to the intersection of contexts) and more complex tensor products give best results, while Guevara (2011) obtains best results for adjective-noun phrases with addition of vectors (corresponding to union of contexts). Since we do not (yet) have a single best similarity measure to distinguish definitions from non-definitions, we use a combination of similarity measures to train a model as e.g. also was done by B¨ar et al. (2012). 3 Frequency Dependency Correction of Jensen-Shannon</context>
</contexts>
<marker>Giesbrecht, 2010</marker>
<rawString>Eugenie Giesbrecht. 2010. Towards a matrix-based distributional model of meaning. In Proceedings of the NAACL HLT 2010 Student Research Workshop, pages 23–28, Los Angeles, California. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olaf G¨ornerup</author>
<author>Jussi Karlgren</author>
</authors>
<title>Cross-lingual comparison between distributionally determined word similarity networks.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Workshop on Graph-based Methods for Natural Language Processing,</booktitle>
<pages>48--54</pages>
<publisher>ACL.</publisher>
<marker>G¨ornerup, Karlgren, 2010</marker>
<rawString>Olaf G¨ornerup and Jussi Karlgren. 2010. Cross-lingual comparison between distributionally determined word similarity networks. In Proceedings of the 2010 Workshop on Graph-based Methods for Natural Language Processing, pages 48–54. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Use of syntactic context to produce term association lists for text retrieval.</title>
<date>1992</date>
<booktitle>In SIGIR ’92: Proceedings of the 15th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>89--97</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="3196" citStr="Grefenstette, 1992" startWordPosition="528" endWordPosition="529">section 3 we discuss the dependence of JSD on word frequency and introduce a new similarity measure. Section 4 then describes the system. The results are given in section 5 and are discussed in section 6. 2 Related Work Though distributional similarity has widely been studied and has become an established method to find similar words, there is no consensus on the way the context of a word has to be defined and on the best way to compute the similarity between two contexts. In the most general definitions the context of a word consists of a number of words and their relation to the given word (Grefenstette, 1992; Curran and Moens, 2002). In the following we will only consider the simplest case in which there is only one relation: the relation of being in the same sentence. Each word can be represented by a so called conSecond Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 48–52, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics text vector in a high dimensional word space. Since these vectors will be sparse, often dimensionality reduction techniques are applied. In</context>
</contexts>
<marker>Grefenstette, 1992</marker>
<rawString>Gregory Grefenstette. 1992. Use of syntactic context to produce term association lists for text retrieval. In SIGIR ’92: Proceedings of the 15th annual international ACM SIGIR conference on Research and development in information retrieval, pages 89–97. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiliano Guevara</author>
</authors>
<title>Computing semantic compositionality in distributional semantics.</title>
<date>2011</date>
<booktitle>In Proceedings of the 9th International Conference on Computational Semantics (IWCS</booktitle>
<pages>135--144</pages>
<contexts>
<context position="4965" citStr="Guevara (2011)" startWordPosition="814" endWordPosition="815"> question whether and how an appropriate context vector for a phrase can be derived from the context vectors of its components has become a central issue in distributional semantics (Clark and Pulman, 2007; Mitchell and Lapata, 2008; Widdows, 2008; Clarke et al., 2008). It is not yet clear which way of combining the vectors of the components is best suited for what goals. Giesbrecht (2010) and Mitchell and Lapata (2008) e.g. find that for noun-noun compounds the product of context vectors (corresponding to the intersection of contexts) and more complex tensor products give best results, while Guevara (2011) obtains best results for adjective-noun phrases with addition of vectors (corresponding to union of contexts). Since we do not (yet) have a single best similarity measure to distinguish definitions from non-definitions, we use a combination of similarity measures to train a model as e.g. also was done by B¨ar et al. (2012). 3 Frequency Dependency Correction of Jensen-Shannon Divergence Weeds et al. (2004) observed that in tasks in which related words have to be found, some measures prefer words with a frequency similar to that of the target word while others prefer high frequent words, regard</context>
</contexts>
<marker>Guevara, 2011</marker>
<rawString>Emiliano Guevara. 2011. Computing semantic compositionality in distributional semantics. In Proceedings of the 9th International Conference on Computational Semantics (IWCS 2011), pages 135–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jussi Karlgren</author>
<author>Magnus Sahlgren</author>
</authors>
<title>From words to understanding.</title>
<date>2001</date>
<booktitle>In Foundations of Real-World Intelligence,</booktitle>
<pages>294--308</pages>
<publisher>CSLI Publications,</publisher>
<location>Stanford, California.</location>
<contexts>
<context position="3881" citStr="Karlgren and Sahlgren (2001)" startWordPosition="632" endWordPosition="635">consider the simplest case in which there is only one relation: the relation of being in the same sentence. Each word can be represented by a so called conSecond Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 48–52, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics text vector in a high dimensional word space. Since these vectors will be sparse, often dimensionality reduction techniques are applied. In the present paper we use random indexing, introduced by Karlgren and Sahlgren (2001) and Sahlgren (2005) to reduce the size of the context vectors. The way in which the context vectors are constructed also determines what similarity measures are suited. For random indexing G¨ornerup and Karlgren (2010) found that best results are obtained using L1-norm or Jensen-Shannon divergence (JSD). they also report that these measures highly correlate. We could confirm this in a preliminary experiment and therefore only use JSD in the following. Recently, the question whether and how an appropriate context vector for a phrase can be derived from the context vectors of its components has</context>
</contexts>
<marker>Karlgren, Sahlgren, 2001</marker>
<rawString>Jussi Karlgren and Magnus Sahlgren. 2001. From words to understanding. In Foundations of Real-World Intelligence, pages 294–308. CSLI Publications, Stanford, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suresh Manandhar</author>
<author>Deniz Yuret</author>
<author>editors</author>
</authors>
<date>2013</date>
<booktitle>Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013), in conjunction with the Second Joint Conference on Lexical and Computational Semantcis (*SEM 2013),</booktitle>
<location>Atlanta.</location>
<marker>Manandhar, Yuret, editors, 2013</marker>
<rawString>Suresh Manandhar and Deniz Yuret, editors. 2013. Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013), in conjunction with the Second Joint Conference on Lexical and Computational Semantcis (*SEM 2013), Atlanta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In ACL 2008, Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>236--244</pages>
<contexts>
<context position="4583" citStr="Mitchell and Lapata, 2008" startWordPosition="747" endWordPosition="751">ich the context vectors are constructed also determines what similarity measures are suited. For random indexing G¨ornerup and Karlgren (2010) found that best results are obtained using L1-norm or Jensen-Shannon divergence (JSD). they also report that these measures highly correlate. We could confirm this in a preliminary experiment and therefore only use JSD in the following. Recently, the question whether and how an appropriate context vector for a phrase can be derived from the context vectors of its components has become a central issue in distributional semantics (Clark and Pulman, 2007; Mitchell and Lapata, 2008; Widdows, 2008; Clarke et al., 2008). It is not yet clear which way of combining the vectors of the components is best suited for what goals. Giesbrecht (2010) and Mitchell and Lapata (2008) e.g. find that for noun-noun compounds the product of context vectors (corresponding to the intersection of contexts) and more complex tensor products give best results, while Guevara (2011) obtains best results for adjective-noun phrases with addition of vectors (corresponding to union of contexts). Since we do not (yet) have a single best similarity measure to distinguish definitions from non-definition</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In ACL 2008, Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics, pages 236–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Sahlgren</author>
</authors>
<title>An introduction to random indexing.</title>
<date>2005</date>
<booktitle>In Methods and Applications of Semantic Indexing Workshop at the 7th International Conference on Terminology and Knowledge Engineering, TKE,</booktitle>
<volume>5</volume>
<contexts>
<context position="3901" citStr="Sahlgren (2005)" startWordPosition="637" endWordPosition="638">ch there is only one relation: the relation of being in the same sentence. Each word can be represented by a so called conSecond Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 48–52, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics text vector in a high dimensional word space. Since these vectors will be sparse, often dimensionality reduction techniques are applied. In the present paper we use random indexing, introduced by Karlgren and Sahlgren (2001) and Sahlgren (2005) to reduce the size of the context vectors. The way in which the context vectors are constructed also determines what similarity measures are suited. For random indexing G¨ornerup and Karlgren (2010) found that best results are obtained using L1-norm or Jensen-Shannon divergence (JSD). they also report that these measures highly correlate. We could confirm this in a preliminary experiment and therefore only use JSD in the following. Recently, the question whether and how an appropriate context vector for a phrase can be derived from the context vectors of its components has become a central is</context>
</contexts>
<marker>Sahlgren, 2005</marker>
<rawString>Magnus Sahlgren. 2005. An introduction to random indexing. In Methods and Applications of Semantic Indexing Workshop at the 7th International Conference on Terminology and Knowledge Engineering, TKE, volume 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Wartena</author>
<author>Rogier Brussee</author>
<author>Wouter Slakhorst</author>
</authors>
<title>Keyword extraction using word co-occurrence.</title>
<date>2010</date>
<booktitle>In Database and Expert Systems Applications (DEXA), 2010 Workshop on,</booktitle>
<pages>54--58</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="6281" citStr="Wartena et al. (2010)" startWordPosition="1029" endWordPosition="1032">m and JSD give best results for similarity of random index vectors, we are especially interested in JSD. The JSD of two distributions p and q is given by JSD(p,q) = 12D(p||12p+12q)+12D(q||12p+12q) (1) where D(p||q) = ��p(i)lo� �(�� log �(�� is the KullbackLeibler divergence. We will follow the usual terminology of context vectors. However, we will always normalize the vectors, such that they can be interpreted as probability mass distributions. According to Weeds et al. (2004) the JSD belongs to the category of distance measures that tends to give small distances for highly frequent words. In Wartena et al. (2010) we also made this observation and therefore we added an additional constraint on the selection of keywords that should avoid the selection of too general words. In the present paper we try to explicitly model the dependency between the JSD and the number of occurrences of the involved words. We then use the difference between the JSD of the co-occurrence vectors of two words and the JSD expected on the base of the frequency of these words as a similarity measure. In the following we will use the dependency between the JSD and the frequency of the words directly. In (Wartena, 2013) we model th</context>
</contexts>
<marker>Wartena, Brussee, Slakhorst, 2010</marker>
<rawString>Christian Wartena, Rogier Brussee, and Wouter Slakhorst. 2010. Keyword extraction using word co-occurrence. In Database and Expert Systems Applications (DEXA), 2010 Workshop on, pages 54–58. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Wartena</author>
</authors>
<title>Distributional similarity of words with different frequencies.</title>
<date>2013</date>
<booktitle>In Proceedings of the Dutch-Belgian Information Retrieval Workshop,</booktitle>
<location>Delft.</location>
<note>To Appear.</note>
<contexts>
<context position="6869" citStr="Wartena, 2013" startWordPosition="1137" endWordPosition="1138">. In Wartena et al. (2010) we also made this observation and therefore we added an additional constraint on the selection of keywords that should avoid the selection of too general words. In the present paper we try to explicitly model the dependency between the JSD and the number of occurrences of the involved words. We then use the difference between the JSD of the co-occurrence vectors of two words and the JSD expected on the base of the frequency of these words as a similarity measure. In the following we will use the dependency between the JSD and the frequency of the words directly. In (Wartena, 2013) we model the JSD instead as a function of the number of non zero values in the context vectors. The latter dependency can be modeled by a simpler function, but did not work as well with the SemEval data set. Given two words w1 and w2 the JSD of their context vectors can be modeled as a function of the minimum of the number of occurrences of w1 and w2. Figure 3 shows the JSD of the context vectors of the words of the training set and the context vector of the definition phrase. In this figure the JSD of the positive and the negative examples is marked with different marks. The lower bound of t</context>
</contexts>
<marker>Wartena, 2013</marker>
<rawString>Christian Wartena. 2013. Distributional similarity of words with different frequencies. In Proceedings of the Dutch-Belgian Information Retrieval Workshop, Delft. To Appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David J Weir</author>
<author>Diana McCarthy</author>
</authors>
<title>Characterising measures of lexical distributional similarity.</title>
<date>2004</date>
<booktitle>In COLING 2004, Proceedings of the 20th International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="5374" citStr="Weeds et al. (2004)" startWordPosition="876" endWordPosition="879">l and Lapata (2008) e.g. find that for noun-noun compounds the product of context vectors (corresponding to the intersection of contexts) and more complex tensor products give best results, while Guevara (2011) obtains best results for adjective-noun phrases with addition of vectors (corresponding to union of contexts). Since we do not (yet) have a single best similarity measure to distinguish definitions from non-definitions, we use a combination of similarity measures to train a model as e.g. also was done by B¨ar et al. (2012). 3 Frequency Dependency Correction of Jensen-Shannon Divergence Weeds et al. (2004) observed that in tasks in which related words have to be found, some measures prefer words with a frequency similar to that of the target word while others prefer high frequent words, regardless of the frequency of the target word. Since G¨ornerup and Karlgren (2010) found that L1-norm and JSD give best results for similarity of random index vectors, we are especially interested in JSD. The JSD of two distributions p and q is given by JSD(p,q) = 12D(p||12p+12q)+12D(q||12p+12q) (1) where D(p||q) = ��p(i)lo� �(�� log �(�� is the KullbackLeibler divergence. We will follow the usual terminology o</context>
</contexts>
<marker>Weeds, Weir, McCarthy, 2004</marker>
<rawString>Julie Weeds, David J. Weir, and Diana McCarthy. 2004. Characterising measures of lexical distributional similarity. In COLING 2004, Proceedings of the 20th International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Widdows</author>
</authors>
<title>Semantic vector products: Some initial investigations.</title>
<date>2008</date>
<booktitle>In Second Conference on Quantum Interaction, Oxford, 26th 28th</booktitle>
<contexts>
<context position="4598" citStr="Widdows, 2008" startWordPosition="752" endWordPosition="753"> constructed also determines what similarity measures are suited. For random indexing G¨ornerup and Karlgren (2010) found that best results are obtained using L1-norm or Jensen-Shannon divergence (JSD). they also report that these measures highly correlate. We could confirm this in a preliminary experiment and therefore only use JSD in the following. Recently, the question whether and how an appropriate context vector for a phrase can be derived from the context vectors of its components has become a central issue in distributional semantics (Clark and Pulman, 2007; Mitchell and Lapata, 2008; Widdows, 2008; Clarke et al., 2008). It is not yet clear which way of combining the vectors of the components is best suited for what goals. Giesbrecht (2010) and Mitchell and Lapata (2008) e.g. find that for noun-noun compounds the product of context vectors (corresponding to the intersection of contexts) and more complex tensor products give best results, while Guevara (2011) obtains best results for adjective-noun phrases with addition of vectors (corresponding to union of contexts). Since we do not (yet) have a single best similarity measure to distinguish definitions from non-definitions, we use a com</context>
</contexts>
<marker>Widdows, 2008</marker>
<rawString>Dominic Widdows. 2008. Semantic vector products: Some initial investigations. In Second Conference on Quantum Interaction, Oxford, 26th 28th March 2008.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>