<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.8132725">
Measuring Non-native Speakers’ Proficiency of English by Using a Test
with Automatically-Generated Fill-in-the-Blank Questions
</title>
<note confidence="0.4773996">
Eiichiro SUMITA
Spoken Language Communication
Research Laboratories
ATR
Kyoto 619-0288 Japan
</note>
<email confidence="0.990255">
eiichiro.sumita@atr.jp
</email>
<note confidence="0.504329666666667">
Fumiaki SUGAYA
Text Information Processing Labo-
ratory
</note>
<author confidence="0.240747">
KDDI R&amp;D Laboratories Inc.
</author>
<affiliation confidence="0.258013">
Saitama 356-8502 Japan
</affiliation>
<email confidence="0.993351">
fsugaya@kddilabs.jp
</email>
<author confidence="0.932312">
Seiichi Yamamoto
</author>
<affiliation confidence="0.547692333333333">
Department of Information Systems
Design
Doshisha University
</affiliation>
<address confidence="0.563948">
Kyoto 610-0321 Japan
</address>
<email confidence="0.971688">
seyamamo@mail.doshisha.ac.jp
</email>
<author confidence="0.235946">
&amp;
</author>
<affiliation confidence="0.388608">
Spoken Language Communication
Research Laboratories
</affiliation>
<sectionHeader confidence="0.709155" genericHeader="abstract">
ATR
Abstract
</sectionHeader>
<bodyText confidence="0.998823">
This paper proposes the automatic generation
of Fill-in-the-Blank Questions (FBQs) together
with testing based on Item Response Theory
(IRT) to measure English proficiency. First, the
proposal generates an FBQ from a given sen-
tence in English. The position of a blank in the
sentence is determined, and the word at that
position is considered as the correct choice.
The candidates for incorrect choices for the
blank are hypothesized through a thesaurus.
Then, each of the candidates is verified by us-
ing the Web. Finally, the blanked sentence, the
correct choice and the incorrect choices surviv-
ing the verification are together laid out to
form the FBQ. Second, the proficiency of non-
native speakers who took the test consisting of
such FBQs is estimated through IRT.
Our experimental results suggest that:
</bodyText>
<listItem confidence="0.6777835">
(1) the generated questions plus IRT estimate
the non-native speakers’ English proficiency;
(2) while on the other hand, the test can be
completed almost perfectly by English native
speakers; and (3) the number of questions can
be reduced by using item information in IRT.
</listItem>
<bodyText confidence="0.999169">
The proposed method provides teach-
ers and testers with a tool that reduces time
and expenditure for testing English profi-
ciency.
</bodyText>
<sectionHeader confidence="0.999162" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99964727027027">
English has spread so widely that 1,500 million
people, about a quarter of the world’s population,
speak it, though at most about 400 million speak it
as their native language (Crystal, 2003). Thus,
English education for non-native speakers both
now and in the near future is of great importance.
The progress of computer technology is ad-
vancing an electronic tool for language learning
called Computer-Assisted Language Learning
(CALL) and for language testing called Computer-
Based Testing (CBT) or Computer-Adaptive Test-
ing (CAT). However, no computerized support for
producing a test, a collection of questions for
evaluating language proficiency, has emerged to
date. *
Fill-in-the-Blank Questions (FBQs) are widely
used from the classroom level to far larger scales
to measure peoples’ proficiency at English as a
second language. Examples of such tests include
TOEFL (Test Of English as a Foreign Language,
http://www.ets.org/toefl/) and TOEIC (Test Of
English for International Communication,
http://www.ets.org/toeic/).
A test comprising FBQs has merits in that (1) it
is easy for test-takers to input answers, (2) com-
puters can mark them, thus marking is invariable
and objective, and (3) they are suitable for the
modern testing theory, Item Response Theory
(IRT).
Because it is regarded that writing incorrect
choices that distract only the non-proficient test-
taker is a highly skilled business (Alderson, 1996),
FBQs have been written by human experts. Thus,
test construction is time-consuming and expensive.
As a result, utilizing up-to-date texts for question
writing is not practical, nor is tuning in to individ-
ual students.
</bodyText>
<note confidence="0.826211">
* See the detailed discussion in Section 6.
</note>
<page confidence="0.986422">
61
</page>
<note confidence="0.9771295">
Proceedings of the 2nd Workshop on Building Educational Applications Using NLP,
pages 61–68, Ann Arbor, June 2005. c�Association for Computational Linguistics, 2005
</note>
<bodyText confidence="0.994507444444444">
To solve the problems of time and expenditure,
this paper proposes a method for generating FBQs
using a corpus, a thesaurus, and the Web. Experi-
ments have shown that the proficiency estimated
through IRT with generated FBQs highly corre-
lates with non-native speakers’ real proficiency.
This system not only provides us with a quick and
inexpensive testing method, but it also features the
following advantages:
</bodyText>
<listItem confidence="0.992721727272727">
(I) It provides “anyone” individually with
up-to-date and interesting questions for
self-teaching. We have implemented a
program that downloads any Web page
such as a news site and generates ques-
tions from it.
(II) It also enables on-demand testing at
“anytime and anyplace.” We have im-
plemented a system that operates on a
mobile phone. Questions are generated
and pooled in the server, and upon a
</listItem>
<bodyText confidence="0.852358857142857">
user’s request, questions are
downloaded. CAT (Wainer, 2000) is
then conducted on the phone. The sys-
tem for mobile phone is scheduled to be
deployed in May of 2005 in Japan.
The remainder of this paper is organized as fol-
lows. Section 2 introduces a method for making
FBQ, Section 3 explains how to estimate test-
takers’ proficiency, and Section 4 presents the ex-
periments that demonstrate the effectiveness of the
proposal. Section 5 provides some discussion, and
Section 6 explains the differences between our
proposal and related work, followed by concluding
remarks.
</bodyText>
<sectionHeader confidence="0.875145" genericHeader="method">
2 Question Generation Method
</sectionHeader>
<bodyText confidence="0.9995755">
We will review an FBQ, and then explain our
method for producing it.
</bodyText>
<subsectionHeader confidence="0.993585">
2.1 Fill-in-the-Blank Question (FBQ)
</subsectionHeader>
<bodyText confidence="0.999392111111111">
FBQs are the one of the most popular types of
questions in testing. Figure 1 shows a typical sam-
ple consisting of a partially blanked English sen-
tence and four choices for filling the blank. The
tester ordinarily assumes that exactly one choice is
correct (in this case, b)) and the other three choices
are incorrect. The latter are often called distracters,
because they fulfill a role to distract the less profi-
cient test-takers.
</bodyText>
<figureCaption confidence="0.809055">
Figure 1: A sample Fill-in-the-Blank Question
(FBQ)
</figureCaption>
<subsectionHeader confidence="0.999856">
2.2 Flow of generation
</subsectionHeader>
<bodyText confidence="0.993618833333333">
Using question 1 above, the outline of generation
is presented below (Figure 2).
A seed sentence (in this case, “I only have to
keep my head above water one more week.”) is
input from the designated source, e.g., a corpus or
a Web page such as well-known news site. *
</bodyText>
<figureCaption confidence="0.608614">
Figure 2: Flow generating Fill-In-The-Blank Ques-
tion (FBQ)
</figureCaption>
<bodyText confidence="0.978099285714286">
[a] The seed sentence is a correct English sen-
tence that is decomposed into a sentence
with a blank (blanked sentence) and the
correct choice for the blank. After the seed
* Selection of the seed sentence (source text) is an important
open problem because the difficulty of the seed (text) should
influence the difficulty of the generated question. As for text
difficulty, several measures such as Lexile by MetaMetrics
(http://www.Lexile.com) have been proposed. They are known
as readability and are usually defined as a function of sentence
length and word frequency.
In this paper, we used corpora of business and travel con-
versations, because TOEIC itself is oriented toward business
and daily conversation.
</bodyText>
<figure confidence="0.994229066666667">
Question 1 (FBQ)
I only have to _______ my head above water one more
week.
a) reserve b) keep c) guarantee d) promise
N.B. the correct choice is b) keep.
Testing
knowledge
Lexicon
Corpus
[b] Generate distracter candidates
[a] Determine the blank position
[c] Verify the incorrectness
[d] Form the question
Seed Sentence
Question
</figure>
<page confidence="0.996238">
62
</page>
<bodyText confidence="0.986809102040816">
sentence is analyzed morphologically by a
computer, according to the testing knowl-
edge* the blank position of the sentence is
determined. In this paper’s experiment, the
verb of the seed is selected, and we obtain
the blanked sentence “I only have to
______ my head above water one more
week.” and the correct choice “keep.”
[b] To be a good distracter, the candidates must
maintain the grammatical characteristics of
the correct choice, and these should be
similar in meaning†. Using a thesaurus‡,
words similar to the correct choice are
listed up as candidates, e.g., “clear,” “guar-
antee,” “promise,” “reserve,” and “share”
for the above “keep.”
[c] Verify (see Section 2.3 for details) the in-
correctness of the sentence restored by each
candidate, and if it is not incorrect (in this
case, “clear” and “share”), the candidate is
given up.
[d] If a sufficient number (in this paper, three)
of candidates remain, form a question by
randomizing the order of all the choices
(“keep,” “guarantee,” “promise,” and “re-
‡ serve
* Testing knowledge tells us what part of the seed sentence
should be blanked. For example, we selected the verb of the
seed because it is one of the basic types of blanked words in
popular FBQs such as in TOEIC.
This can be a word of another POS (Part-Of-Speech). For
this, we can use knowledge in the field of second-language
education. Previous studies on errors in English usage by
Japanese native speakers such as (Izumi and Isahara, 2004)
unveiled patterns of errors specific to Japanese, e.g., (1) article
selection error, which results from the fact there are no articles
in Japanese; (2) preposition selection error, which results from
the fact some Japanese counterparts have broader meaning; (3)
adjective selection error, which results from mismatch of
meaning between Japanese words and their counterpart. Such
knowledge may generate questions harder for Japanese who
study English.
† There are various aspects other than meaning, for example,
spelling, pronunciation, and translation and so on. Depending
on the aspect, lexical information sources other than a thesau-
); otherwise, another seed sentence is
input and restart fr
”
om step [a].
</bodyText>
<subsectionHeader confidence="0.996472">
2.3 Incorrectness Verification
</subsectionHeader>
<bodyText confidence="0.9934748">
ition, (1) the blanked sentence
restored with the correct choice is correct, and (2)
the blanked sentence restored with the distracter
must be incorrect.
In order to generate an FBQ, the incorrectness
of the sentence restored by each distracter candi-
date must be verified and if the combination is not
incorrect, the can
fin
didate is rejected.
</bodyText>
<subsectionHeader confidence="0.554921">
Zero-Hit Sentence
</subsectionHeader>
<bodyText confidence="0.91113">
om the Web for the key y.
rus should be consulted. Figure 3: Incorrectness and Hits on the Web
</bodyText>
<figure confidence="0.6187833">
in
We used an
house English thesaurus whose hierarchyis
called Ruigo-Shin-Jiten (Ohno and Hamanishi, 1984).
the
above examples, the original word
expresses two dif-
ferent concepts:
which is shared by
and
</figure>
<figureCaption confidence="0.327479">
and (2) promise, which is
</figureCaption>
<bodyText confidence="0.874511833333333">
shared by the words
and
Since this depends on the thesaurus used, some may sense a
slight discomfort at these concepts. If a different thesaurus is
used, the distracter can
In
</bodyText>
<equation confidence="0.933931428571429">
“keep”
(1) possession-or-disposal,
“clear”
“share,”
“guarantee,”“promise,”
“reserve.”
didates may differ.
</equation>
<bodyText confidence="0.86349835">
In FBQs, by de
The Web includes all manners of language data
in vast quantities, which are for everyone easy to
access through a networked computer. Recently,
exploitation of the Web for various natural lan-
guage applications is rising (Grefenstette, 1999;
Turney, 2001; Kilgarriff and Grefenstette, 2003;
Tonoike et al., 2004).
We also propose aWeb-based approach. We
dare to assume that if there is a sentence on the
Web, that sentence is considered correct; other-
wise, the sentence is unlikely to be correct in that
there is no sentence written on the Web despite the
variety an
d quantity of data on it.
Figure 3 illustrates verification based on the re-
trieval from the Web. Here, s (x) is the blanked
sentence, s (w) denotes the sentence restored by the
word w, and hits (y) represents the number of
documents retrieved fr
</bodyText>
<equation confidence="0.8619476">
Blanked sentence:
s(
x
)
= “I only have to my head above water
</equation>
<bodyText confidence="0.7072415">
one more week. ”
Hits of incorrect choice candidates:
</bodyText>
<equation confidence="0.99681215">
hits (s (
“
clear
”)
) = 176 ;correct
hits (s
(“
guarantee
”)
) = 0 ;incorrect
hits (s (
“
promise
”)
) = 0 ;incorrect
hits (s (
“
reserv
e”)) = 0 ; incorrect
hits (s (“share”)) = 3 ; correct
</equation>
<bodyText confidence="0.997633444444444">
based on one of the off-the-shelf thesauruses for Japanese,
the words
If hits (s (w)), is small, then the sentence re-
stored with the word w is unlikely, thus the word w
should be a good distracter. If hits (s (w)), is large
then the sentence restored with the word w is likely,
then the word w is unlikely to be a good distracter
an
d is given up.
</bodyText>
<page confidence="0.997523">
63
</page>
<bodyText confidence="0.999956333333333">
We used the strongest condition. If hits (s (w))
is zero, then the sentence restored with the word w
is unlikely, thus the word w should be a good dis-
tracter. If hits (s (w)), is not zero, then the sentence
restored with the word w is likely, thus the word w
is unlikely to be a good distracter and is given up.
</bodyText>
<subsectionHeader confidence="0.634672">
Retrieval NOT By Sentence
</subsectionHeader>
<bodyText confidence="0.999271625">
It is often the case that retrieval by sentence does
not work. Instead of a sentence, a sequence of
words around a blank position, beginning with a
content word (or sentence head) and ending with a
content word (or sentence tail) is passed to a search
engine automatically. For the abovementioned
sample, the sequence of words passed to the engine
is “I only have to clear my head” and so on.
</bodyText>
<subsectionHeader confidence="0.884079">
Web Search
</subsectionHeader>
<bodyText confidence="0.999971548387097">
We can use any search engine, though we have
been using Google since February 2004. At that
point in time, Google covered an enormous four
billion pages.
The “correct” hits may come from non-native
speakers’ websites and contain invalid language
usage. To increase reliability, we could restrict
Google searches to Websites with URLs based in
English-speaking countries, although we have not
done so yet. There is another concern: even if
sentence fragments cannot be located on the Web,
it does not necessarily mean they are illegitimate.
Thus, the proposed verification based on the Web
is not perfect; the point, however, is that with such
limitations, the generated questions are useful for
estimating proficiency as demonstrated in a later
section.
Setting aside the convenience provided by the
off-the-shelf search engine, another search special-
ized for this application is possible, although the
current implementation is fast enough to automate
generation of FBQs, and the demand to accelerate
the search is not strong. Rather, the problem of
time needed for test construction has been reduced
by our proposal.
The throughput depends on the text from which
a seed sentence comes and the network traffic
when the Web is accessed. Empirically, one FBQ
is obtained in 20 seconds on average and the total
number of FBQs in a day adds up to over 4,000 on
a single computer.
</bodyText>
<sectionHeader confidence="0.995224" genericHeader="method">
3 Estimating Proficiency
</sectionHeader>
<subsectionHeader confidence="0.99807">
3.1 Item Response Theory (IRT)
</subsectionHeader>
<bodyText confidence="0.989238615384615">
Item Response Theory (IRT) is the basis of modern
language tests such as TOEIC, and enables Com-
puterized Adaptive Testing (CAT). Here, we
briefly introduce IRT. IRT, in which a question is
called an item, calculates the test-takers’ profi-
ciency based on the answers for items of the given
test (Embretson, 2000).
The basic idea is the item response function,
which relates the probability of test-takers answer-
ing particular items correctly to their proficiency.
The item response functions are modeled as logis-
tic curves making an S-shape, which take the form
(1) for item i.
</bodyText>
<equation confidence="0.998787">
1
P( )
θ = 1+exp(−ai(θ−bi)) (1)
</equation>
<bodyText confidence="0.945320714285714">
The test-taker parameter, θ, shows the profi-
ciency of the test-taker, with higher values indicat-
ing higher performance.
Each of the item parameters,
and
controls
the shape of the item response function.
pa-
rameter, called discrimination, indexes how
steeply the item response function rises. The b pa-
rameter is called difficulty. Difficult items feature
larger b values and the item response functions are
shifted to the right. These item parameters are usu-
ally estimated by a maximal likelihood method.
For computations including the estimation, there
are man
ai
bi,
The a
y commercial programs such as BILOG
(http://www.assess.com/) available.
</bodyText>
<subsectionHeader confidence="0.9581925">
3.2 Reducing test size by selection of effective
items
</subsectionHeader>
<bodyText confidence="0.9978525">
It is import
ant to estimate the proficiency of the
test-taker by using as few items as possible. For
this, we have proposed a method based on item
information.
Expression (2) is the item information of item i
at θj, the proficiency of the test-taker j, which indi-
cates how much measurement discrimination an
item provides.
The procedure is as follows.
</bodyText>
<listItem confidence="0.888907">
1. Initialize I by the set of all generated FBQs.
</listItem>
<page confidence="0.955511">
64
</page>
<listItem confidence="0.998476571428571">
2. According to Equation (3), we select the item
whose contribution to test information is
maximal.
3. We eliminate the selected item from I accord-
ing to Equation (4).
4. If I is empty, we obtain the ordered list of ef-
fective items; otherwise, go back to step 2.
</listItem>
<equation confidence="0.998539">
Ii θ j = a i P i θ j − P i θ j
( ) 2 ( )(1 ( )) (2)

iˆ arg
= max  ∑ ∑ I
i j i∈I
ˆ
I = I − i(4)
</equation>
<sectionHeader confidence="0.993618" genericHeader="method">
4 Experiment
</sectionHeader>
<bodyText confidence="0.9999695">
The FBQs for the experiment were generated in
February of 2004. Seed sentences were obtained
from ATR’s corpus (Kikui et al., 2003) of the
business and travel domains. The vocabulary of the
corpus comprises about 30,000 words. Sentences
are relatively short, with the average length being
6.47 words. For each domain 5,000 questions were
generated automatically and each question consists
of an English sentence with one blank and four
choices.
</bodyText>
<subsectionHeader confidence="0.9981">
4.1 Experiment with non-native speakers
</subsectionHeader>
<bodyText confidence="0.968601736842105">
We used the TOEIC score as the experiment’s pro-
ficiency measure, and collected 100 Japanese sub-
jects whose TOEIC scores were scattered from 400
to less than 900. The actual range for TOEIC
scores is 10 to 990. Our subjects covered the
dominant portion* of test-takers for TOEIC in Ja-
pan, excluding the highest and lowest extremes.†
We had the subjects answer 320 randomly se-
lected questions from the 10,000 mentioned above.
The raw marks were as follows: the average‡ mark
was 235.2 (73.5%); the highest mark was 290
(90.6%); and the lowest was 158 (49.4%).This
suggests that our FBQs are sensitive to test-takers’
proficiency. In Figure 4, the y-axis represents es-
timated proficiency according to IRT (Section 3.1)
*Over 70% of all test-takers are covered
and generated questions, while the x-axis is the
real TOEIC score of each subject.
As the graph illustrates, the IRT-estimated pro-
ficiency (θ) and real TOEIC scores of subjects cor-
relate highly with a co-efficiency of about 80%.
For comparison we refer to CASEC
(http://casec.evidus.com/), an off-the-shelf test
consisting of human-made questions and IRT. Its
co-efficiency with real TOEIC scores is reported to
be 86%.
This means the proposed automatically gener-
ated questions are promising for measuring English
proficiency, achieving a nearly competitive level
with human-made questions but with a few reser-
vations: (1) whether the difference of 6% is large
depends on the standpoint of possible users; (2) as
for the number of questions to be answered, our
proposal uses 320 questions in the experiments,
while TOEIC uses 200 questions and CASEC uses
only about 60 questions; (3) the proposed method
uses FBQs only whereas CASEC and TOEIC use
various types of questions.
</bodyText>
<figureCaption confidence="0.960826">
Figure 4: IRT-Estimated Proficiency (θ) vs. Real
TOEIC Score
</figureCaption>
<subsectionHeader confidence="0.997336">
4.2 Experiment with a native speaker
</subsectionHeader>
<bodyText confidence="0.999853666666667">
To examine the quality of the generated questions,
we asked a single subject§ who is a native speaker
of English to answer 4,000 questions (Table 1).
</bodyText>
<figure confidence="0.986056777777778">
3
2.5
2
1.5
θ 1
0.5
0
-0.5
-1400
50
600
TOEIC Score
700
80
900
(θj))
i
(3)
</figure>
<bodyText confidence="0.893783666666667">
We have covered only the range of TOEIC scores from 400
to 900 due to expense of the experiment. In this restricted
experiment, we do not claim that our proficiency estimation
method covers the full range of TOEIC scores. § Please note that the analysis is based on a single native-
‡ The standard deviation was 29.8 (9.3%). speaker, thus we need further analysis by multiple subjects.
(http://www.toeic.or.jp/toeic/data/data02.html).
The native speaker largely agreed with our gen-
†
eration, determining correct choices (type I). The
</bodyText>
<page confidence="0.892073">
65
</page>
<figure confidence="0.98540085">
R
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
50
Test Size in Items
100
150
200
250
300
350
</figure>
<bodyText confidence="0.999862666666667">
rate was 93.50%, better than 90.6%, the highest
mark among the non-native speakers.
We present the problematic cases here.
</bodyText>
<listItem confidence="0.995533692307692">
• Type II is caused by the seed sentence being
incorrect for the native speaker, and a distracter is
bad because it is correct. Or like type III, it con-
sists of ambiguous choices.
• Type III is caused by some generated distracters
being correct; therefore, the choices are ambiguous. Figure 5 Correlation coefficient and Test size
• Type IV is caused by the seed sentence being
incorrect and the generated distracters also being
incorrect; therefore, the question cannot be an-
swered.
• Type V is caused by the seed sentence being
nonsense to the native speaker; the question, there-
fore, cannot be answered.
</listItem>
<tableCaption confidence="0.998345">
Table 1 Responses of a Native speaker
</tableCaption>
<table confidence="0.99985">
Type Explanation Count %
I Single Match 3,740 93.50
Selection
II No match 55 1.38
III No Ambiguous 70 1.75
Selection Choices
IV No Correct 45 1.13
Choice
V Nonsense 90 2.25
</table>
<bodyText confidence="0.994000416666667">
Cases with bad seed sentences (portions of II,
IV, and V) require cleaning of the corpus by a na-
tive speaker, and cases with bad distracters (por-
tions of II and III) require refinement of the
proposed generation algorithm.
Since the questions produced by this method
can be flawed in ways which make them unan-
swerable even by native speakers (about 6.5% of
the time) due to the above-mentioned reasons, it is
difficult to use this method for high-stakes testing
applications although it is useful for estimating
proficiency as explained in the previous section.
</bodyText>
<subsectionHeader confidence="0.734655">
4.3 Proficiency θ estimated with the reduced
test and its relation to TOEIC Scores
</subsectionHeader>
<bodyText confidence="0.999885285714286">
Figure 5 shows the relationship between reduction
of the test size according to the method explained
in Section 3.2 and the estimated proficiency based
on the reduced test. The x-axis represents the size
of the reduced test in number of items, while the y-
axis represents the correlation coefficient (R) be-
tween estimated proficiency and real TOEIC score.
</bodyText>
<sectionHeader confidence="0.997517" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.99994575">
This section explains the on-demand generation of
FBQs according to individual preference, an im-
mediate extension and a limitation of our proposed
method, and finally touches on free-format Q&amp;A.
</bodyText>
<subsectionHeader confidence="0.995452">
5.1 Effects of Automatic FBQ Construction
</subsectionHeader>
<bodyText confidence="0.924724882352941">
The method provides teachers and testers with a
tool that reduces time and expenditure. Further-
more, the method can deal with any text. For ex-
ample, up-to-date and interesting materials such as
news articles of the day can be a source of seed
sentences (Figure 6 is a sample generated from an
article (http://www.japantimes.co.jp/) on an earth-
quake that occurred in Japan), which enables reali-
zation of a personalized learning environment.
Figure 6: On-demand construction – a sample
question from a Web news article in The Japan
Times on “an earthquake”
We have generated questions from over 100 docu-
ments on various genres such as novels, speeches,
academic papers and so on found in the enormous
collection of e-Books provided by Project Guten-
berg (http://www.gutenberg.org/).
</bodyText>
<subsectionHeader confidence="0.9610145">
5.2 A Variation of Fill-in-the-Blank Ques-
tions for Grammar Checking
</subsectionHeader>
<bodyText confidence="0.865874375">
In Section 2.2, we mentioned a constraint that a
good distracter should maintain the grammatical
characteristics of the correct choice originating in
N.B. The correct answer is c) originated.
Question 2 (FBQ)
The second quake 10 km below the seabed some
130 km east of Cape Shiono.
a) put b) came c) originated d) opened
</bodyText>
<page confidence="0.872347">
66
</page>
<bodyText confidence="0.993957222222222">
the seed sentence. The question checks not the
grammaticality but the semantic/pragmatic cor-
rectness.
We can generate another type of FBQ by
slightly modifying step [b] of the procedure in Sec-
tion 2.2 to retain the stem of the original word w
and vary the surface form of the word w. This
modified procedure generates a question that
checks the grammatical ability of the test takers.
</bodyText>
<figureCaption confidence="0.86090875">
Figure 7 shows a sample of this kind of question
taken from a TOEIC-test textbook (Educational
Testing Service, 2002).
Figure 7: A variation on fill-in-the-blank questions
</figureCaption>
<subsectionHeader confidence="0.903957">
5.3 Limitation of the Addressed FBQs
</subsectionHeader>
<bodyText confidence="0.999996428571428">
The questions dealt with in this paper concern test-
ing reading ability, but these questions are not suit-
able for testing listening ability because they are
presented visually and cannot be pronounced. To
test listening ability, like in TOIEC, other types of
questions should be used, and automated genera-
tion of them is yet to be developed.
</bodyText>
<subsectionHeader confidence="0.970485">
5.4 Free-Format Q&amp;A
</subsectionHeader>
<bodyText confidence="0.99815975">
Besides measuring one’s ability to receive infor-
mation in a foreign language, which has been ad-
dressed so far in this paper, it is important to
measure a person’s ability to transmit information
in a foreign language. For that purpose, tests for
translating, writing, or speaking in a free format
have been actively studied by many researchers
(Shermis, 2003; Yasuda, 2004).
</bodyText>
<sectionHeader confidence="0.999926" genericHeader="method">
6 Related Work*
</sectionHeader>
<bodyText confidence="0.998783333333333">
Here, we explain other studies on the generation of
multiple-choice questions for language learning.
There are a few previous studies on computer-
* There are many works on item generation theory (ITG) such
as Irvine and Kyllonen (2002), although we do not go any
further into the area. We focus only on multiple-choice ques-
tions for language learning in this paper.
based generation such as Mitkov (2003) and Wil-
son (1997).
</bodyText>
<subsectionHeader confidence="0.990986">
6.1 Cloze Test
</subsectionHeader>
<bodyText confidence="0.998895625">
A computer can generate questions by deleting
words or parts of words randomly or at every N-th
word from text. Test-takers are requested to restore
the word that has been deleted. This is called a
“cloze test.” The effectiveness of a “cloze test” or
its derivatives is a matter of controversy among
researchers of language testing such as Brown
(1993) and Alderson (1996).
</bodyText>
<subsectionHeader confidence="0.998416">
6.2 Tests on Facts
</subsectionHeader>
<bodyText confidence="0.999913777777778">
Mitkov (2003) proposed a computer-aided proce-
dure for generating multiple-choice questions from
textbooks. The differences from our proposal are
that (1) Mitkov’s method generates questions not
about language usage but about facts explicitly
stated in a text†; (2) Mitkov uses techniques such
as term extraction, parsing, transformation of trees,
which are different from our proposal; and (3) Mit-
kov does not use IRT while we use it.
</bodyText>
<sectionHeader confidence="0.998448" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.997106095238095">
This paper proposed the automatic construction of
Fill-in-the-Blank Questions (FBQs). The proposed
method generates FBQs using a corpus, a thesaurus,
and the Web. The generated questions and Item
Response Theory (IRT) then estimate second-
language proficiency.
Experiments have shown that the proposed
method is effective in that the estimated profi-
ciency highly correlates with non-native speakers’
real proficiency as represented by TOEIC scores;
native-speakers can achieve higher scores than
non-native speakers. It is possible to reduce the
size of the test by removing non-discriminative
questions with item information in IRT.
† Based on a fact stated in a textbook like, “A prepositional
phrase at the beginning of a sentence constitutes an introduc-
tory modifier,” Mitkov generates a question such as, “What
does a prepositional phrase at the beginning of a sentence
constitute? i. a modifier that accompanies a noun; ii. an asso-
ciated modifier; iii. an introductory modifier; iv. a misplaced
modifier.”
</bodyText>
<figure confidence="0.9266532">
N.B. The correct answer is c) care.
Question 3 (FBQ)
Because the equipment is very delicate, it must be han-
dled with ______.
a) caring b) careful c) care d) carefully
</figure>
<page confidence="0.9953">
67
</page>
<bodyText confidence="0.999917666666666">
The method provides teachers, testers, and test
takers with novel merits that enable low-cost test-
ing of second-language proficiency and provides
learners with up-to-date and interesting materials
suitable for individuals.
Further research should be done on (1) large-
scale evaluation of the proposal, (2) application to
different languages such as Chinese and Korean,
and (3) generation of different types of questions.
</bodyText>
<sectionHeader confidence="0.995722" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999732363636364">
The authors’ heartfelt thanks go to anonymous review-
ers for providing valuable suggestions and Kadokawa-
Shoten for providing the thesaurus named Ruigo-Shin-
Jiten. The research reported here was supported in part
by a contract with the NiCT entitled, “A study of speech
dialogue translation technology based on a large cor-
pus.” It was also supported in part by the Grants-in-Aid
for Scientific Research (KAKENHI), contract with
MEXT numbered 16300048. The study was conducted
in part as a cooperative research project by KDDI and
ATR.
</bodyText>
<sectionHeader confidence="0.998736" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999783">
Alderson, Charles. 1996. Do corpora have a role in
language assessment? Using Corpora for Language
Research, eds. Thomas, J. and Short, M., Longman:
248—259.
Brown, J. D. 1993. What are the characteristics of natu-
ral cloze tests? Language Testing 10: 93—116.
Crystal, David. 2003. English as a Global Language,
(Second Edition). Cambridge University Press: 212.
Educational Testing Service 2002. TOEIC koushiki
gaido &amp; mondaishu. IIBC: 249.
Embretson, Susan et al. 2000. Item Response Theory for
Psychologists. LEA: 371.
Grefenstette, G. 1999. The WWW as a resource for ex-
ample-based MT tasks. ASLIB “Translating and the
Computer” conference.
Irvine, H. S., and Kyllonen, P. C. (2002). Item genera-
tion for test development. LEA: 412.
Izumi, E., and Isahara, H. (2004). Investigation into
language learners&apos; acquisition order based on the er-
ror analysis of the learner corpus. In Proceedings of
Pacific-Asia Conference on Language, Information
and Computation (PACLIC) 18 Satellite Workshop
on E-Learning, Japan. (in printing)
Kikui, G., Sumita, E., Takaezawa, T. and Yamamoto, S.,
“Creating Corpora for Speech-to-Speech Transla-
tion,” Special Session “Multilingual Speech-to-
Speech Translation” of EuroSpeech, 2003.
Kilgarriff, A. and Grefenstette, G. 2003. Special Issue
on the WEB as Corpus. Computational Linguistics 29
(3): 333—502.
Mitkov, Ruslan and Ha, Le An. 2003. Computer-Aided
Generation of Multiple-Choice Tests. HLT-NAACL
2003 Workshop: Building Educational Applications
Using Natural Language Processing: 17—22.
Ohno, S. and Hamanishi, M. 1984. Ruigo-Shin-Jiten,
Kadokawa, Tokyo (in Japanese)
Shermis, M. D. and Burstein. J. C. 2003. Automated
Essay Scoring. LEA: 238.
Tonoike, M., Sato, S., and Utsuro, T. 2004. Answer
Validation by Keyword Association. IPSJ, SIGNL,
161: 53—60, (in Japanese).
Turney, P.D. 2001. Mining the Web for synonyms: PMI-
IR vs. LSA on TOEFL. ECML 2001: 491—502.
Wainer, Howard et al. 2000. Conputerized Adaptive
Testing: A Primer, (Second Edition). LEA: 335.
Wilson, E. 1997. The Automatic Generation of CALL
exercises from general corpora, in eds. Wichmann,
A., Fligelstone, S., McEnery, T., Knowles, G.,
Teaching and Language Corpora, Harlow: Long-
man:116-130.
Yasuda, K., Sugaya, F., Sumita, E., Takezawa, T., Kikui,
G. and Yamamoto, S. 2004. Automatic Measuring of
English Language Proficiency using MT Evaluation
Technology, COLING 2004 eLearning for Computa-
tional Linguistics and Computational Linguistics for
eLearning: 53-60.
</reference>
<page confidence="0.999445">
68
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.002701">
<title confidence="0.7200124">Measuring Non-native Speakers’ Proficiency of English by Using a Test with Automatically-Generated Fill-in-the-Blank Questions Eiichiro SUMITA Spoken Language Research</title>
<author confidence="0.272679">Kyoto</author>
<email confidence="0.833136">eiichiro.sumita@atr.jp</email>
<title confidence="0.720713333333333">Fumiaki SUGAYA Information Processing KDDI R&amp;D Laboratories</title>
<author confidence="0.424155">Saitama</author>
<email confidence="0.983309">fsugaya@kddilabs.jp</email>
<author confidence="0.998318">Seiichi Yamamoto</author>
<affiliation confidence="0.7616655">Department of Information Doshisha</affiliation>
<address confidence="0.565951">Kyoto 610-0321 Japan</address>
<email confidence="0.830481">seyamamo@mail.doshisha.ac.jp</email>
<affiliation confidence="0.251489333333333">Spoken Language Research ATR</affiliation>
<abstract confidence="0.99055924137931">paper proposes the Questions together testing based on Response Theory (IRT) to measure English proficiency. First, the proposal generates an FBQ from a given sentence in English. The position of a blank in the sentence is determined, and the word at that position is considered as the correct choice. The candidates for incorrect choices for the blank are hypothesized through a thesaurus. Then, each of the candidates is verified by using the Web. Finally, the blanked sentence, the correct choice and the incorrect choices surviving the verification are together laid out to form the FBQ. Second, the proficiency of nonnative speakers who took the test consisting of such FBQs is estimated through IRT. Our experimental results suggest that: (1) the generated questions plus IRT estimate the non-native speakers’ English proficiency; (2) while on the other hand, the test can be completed almost perfectly by English native speakers; and (3) the number of questions can reduced by using information IRT. The proposed method provides teachers and testers with a tool that reduces time and expenditure for testing English proficiency.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Charles Alderson</author>
</authors>
<title>Do corpora have a role in language assessment? Using Corpora for Language Research,</title>
<date>1996</date>
<pages>248--259</pages>
<editor>eds. Thomas, J. and Short, M., Longman:</editor>
<contexts>
<context position="3210" citStr="Alderson, 1996" startWordPosition="472" endWordPosition="473">nglish as a second language. Examples of such tests include TOEFL (Test Of English as a Foreign Language, http://www.ets.org/toefl/) and TOEIC (Test Of English for International Communication, http://www.ets.org/toeic/). A test comprising FBQs has merits in that (1) it is easy for test-takers to input answers, (2) computers can mark them, thus marking is invariable and objective, and (3) they are suitable for the modern testing theory, Item Response Theory (IRT). Because it is regarded that writing incorrect choices that distract only the non-proficient testtaker is a highly skilled business (Alderson, 1996), FBQs have been written by human experts. Thus, test construction is time-consuming and expensive. As a result, utilizing up-to-date texts for question writing is not practical, nor is tuning in to individual students. * See the detailed discussion in Section 6. 61 Proceedings of the 2nd Workshop on Building Educational Applications Using NLP, pages 61–68, Ann Arbor, June 2005. c�Association for Computational Linguistics, 2005 To solve the problems of time and expenditure, this paper proposes a method for generating FBQs using a corpus, a thesaurus, and the Web. Experiments have shown that th</context>
<context position="24648" citStr="Alderson (1996)" startWordPosition="4073" endWordPosition="4074">ITG) such as Irvine and Kyllonen (2002), although we do not go any further into the area. We focus only on multiple-choice questions for language learning in this paper. based generation such as Mitkov (2003) and Wilson (1997). 6.1 Cloze Test A computer can generate questions by deleting words or parts of words randomly or at every N-th word from text. Test-takers are requested to restore the word that has been deleted. This is called a “cloze test.” The effectiveness of a “cloze test” or its derivatives is a matter of controversy among researchers of language testing such as Brown (1993) and Alderson (1996). 6.2 Tests on Facts Mitkov (2003) proposed a computer-aided procedure for generating multiple-choice questions from textbooks. The differences from our proposal are that (1) Mitkov’s method generates questions not about language usage but about facts explicitly stated in a text†; (2) Mitkov uses techniques such as term extraction, parsing, transformation of trees, which are different from our proposal; and (3) Mitkov does not use IRT while we use it. 7 Conclusion This paper proposed the automatic construction of Fill-in-the-Blank Questions (FBQs). The proposed method generates FBQs using a co</context>
</contexts>
<marker>Alderson, 1996</marker>
<rawString>Alderson, Charles. 1996. Do corpora have a role in language assessment? Using Corpora for Language Research, eds. Thomas, J. and Short, M., Longman: 248—259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Brown</author>
</authors>
<title>What are the characteristics of natural cloze tests?</title>
<date>1993</date>
<journal>Language Testing</journal>
<volume>10</volume>
<pages>93--116</pages>
<contexts>
<context position="24628" citStr="Brown (1993)" startWordPosition="4070" endWordPosition="4071">neration theory (ITG) such as Irvine and Kyllonen (2002), although we do not go any further into the area. We focus only on multiple-choice questions for language learning in this paper. based generation such as Mitkov (2003) and Wilson (1997). 6.1 Cloze Test A computer can generate questions by deleting words or parts of words randomly or at every N-th word from text. Test-takers are requested to restore the word that has been deleted. This is called a “cloze test.” The effectiveness of a “cloze test” or its derivatives is a matter of controversy among researchers of language testing such as Brown (1993) and Alderson (1996). 6.2 Tests on Facts Mitkov (2003) proposed a computer-aided procedure for generating multiple-choice questions from textbooks. The differences from our proposal are that (1) Mitkov’s method generates questions not about language usage but about facts explicitly stated in a text†; (2) Mitkov uses techniques such as term extraction, parsing, transformation of trees, which are different from our proposal; and (3) Mitkov does not use IRT while we use it. 7 Conclusion This paper proposed the automatic construction of Fill-in-the-Blank Questions (FBQs). The proposed method gener</context>
</contexts>
<marker>Brown, 1993</marker>
<rawString>Brown, J. D. 1993. What are the characteristics of natural cloze tests? Language Testing 10: 93—116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Crystal</author>
</authors>
<title>English as a Global Language, (Second Edition).</title>
<date>2003</date>
<pages>212</pages>
<publisher>Cambridge University Press:</publisher>
<contexts>
<context position="1979" citStr="Crystal, 2003" startWordPosition="289" endWordPosition="290">that: (1) the generated questions plus IRT estimate the non-native speakers’ English proficiency; (2) while on the other hand, the test can be completed almost perfectly by English native speakers; and (3) the number of questions can be reduced by using item information in IRT. The proposed method provides teachers and testers with a tool that reduces time and expenditure for testing English proficiency. 1 Introduction English has spread so widely that 1,500 million people, about a quarter of the world’s population, speak it, though at most about 400 million speak it as their native language (Crystal, 2003). Thus, English education for non-native speakers both now and in the near future is of great importance. The progress of computer technology is advancing an electronic tool for language learning called Computer-Assisted Language Learning (CALL) and for language testing called ComputerBased Testing (CBT) or Computer-Adaptive Testing (CAT). However, no computerized support for producing a test, a collection of questions for evaluating language proficiency, has emerged to date. * Fill-in-the-Blank Questions (FBQs) are widely used from the classroom level to far larger scales to measure peoples’ </context>
</contexts>
<marker>Crystal, 2003</marker>
<rawString>Crystal, David. 2003. English as a Global Language, (Second Edition). Cambridge University Press: 212.</rawString>
</citation>
<citation valid="true">
<date>2002</date>
<booktitle>TOEIC koushiki gaido &amp; mondaishu. IIBC:</booktitle>
<pages>249</pages>
<institution>Educational Testing Service</institution>
<contexts>
<context position="24072" citStr="(2002)" startWordPosition="3975" endWordPosition="3975">ne’s ability to receive information in a foreign language, which has been addressed so far in this paper, it is important to measure a person’s ability to transmit information in a foreign language. For that purpose, tests for translating, writing, or speaking in a free format have been actively studied by many researchers (Shermis, 2003; Yasuda, 2004). 6 Related Work* Here, we explain other studies on the generation of multiple-choice questions for language learning. There are a few previous studies on computer* There are many works on item generation theory (ITG) such as Irvine and Kyllonen (2002), although we do not go any further into the area. We focus only on multiple-choice questions for language learning in this paper. based generation such as Mitkov (2003) and Wilson (1997). 6.1 Cloze Test A computer can generate questions by deleting words or parts of words randomly or at every N-th word from text. Test-takers are requested to restore the word that has been deleted. This is called a “cloze test.” The effectiveness of a “cloze test” or its derivatives is a matter of controversy among researchers of language testing such as Brown (1993) and Alderson (1996). 6.2 Tests on Facts Mit</context>
</contexts>
<marker>2002</marker>
<rawString>Educational Testing Service 2002. TOEIC koushiki gaido &amp; mondaishu. IIBC: 249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Embretson</author>
</authors>
<title>Item Response Theory for Psychologists.</title>
<date>2000</date>
<pages>371</pages>
<location>LEA:</location>
<contexts>
<context position="14187" citStr="Embretson, 2000" startWordPosition="2312" endWordPosition="2313">ghput depends on the text from which a seed sentence comes and the network traffic when the Web is accessed. Empirically, one FBQ is obtained in 20 seconds on average and the total number of FBQs in a day adds up to over 4,000 on a single computer. 3 Estimating Proficiency 3.1 Item Response Theory (IRT) Item Response Theory (IRT) is the basis of modern language tests such as TOEIC, and enables Computerized Adaptive Testing (CAT). Here, we briefly introduce IRT. IRT, in which a question is called an item, calculates the test-takers’ proficiency based on the answers for items of the given test (Embretson, 2000). The basic idea is the item response function, which relates the probability of test-takers answering particular items correctly to their proficiency. The item response functions are modeled as logistic curves making an S-shape, which take the form (1) for item i. 1 P( ) θ = 1+exp(−ai(θ−bi)) (1) The test-taker parameter, θ, shows the proficiency of the test-taker, with higher values indicating higher performance. Each of the item parameters, and controls the shape of the item response function. parameter, called discrimination, indexes how steeply the item response function rises. The b param</context>
</contexts>
<marker>Embretson, 2000</marker>
<rawString>Embretson, Susan et al. 2000. Item Response Theory for Psychologists. LEA: 371.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>The WWW as a resource for example-based MT tasks.</title>
<date>1999</date>
<booktitle>ASLIB “Translating and the Computer” conference.</booktitle>
<contexts>
<context position="10468" citStr="Grefenstette, 1999" startWordPosition="1648" endWordPosition="1649">xpresses two different concepts: which is shared by and and (2) promise, which is shared by the words and Since this depends on the thesaurus used, some may sense a slight discomfort at these concepts. If a different thesaurus is used, the distracter can In “keep” (1) possession-or-disposal, “clear” “share,” “guarantee,”“promise,” “reserve.” didates may differ. In FBQs, by de The Web includes all manners of language data in vast quantities, which are for everyone easy to access through a networked computer. Recently, exploitation of the Web for various natural language applications is rising (Grefenstette, 1999; Turney, 2001; Kilgarriff and Grefenstette, 2003; Tonoike et al., 2004). We also propose aWeb-based approach. We dare to assume that if there is a sentence on the Web, that sentence is considered correct; otherwise, the sentence is unlikely to be correct in that there is no sentence written on the Web despite the variety an d quantity of data on it. Figure 3 illustrates verification based on the retrieval from the Web. Here, s (x) is the blanked sentence, s (w) denotes the sentence restored by the word w, and hits (y) represents the number of documents retrieved fr Blanked sentence: s( x ) = </context>
</contexts>
<marker>Grefenstette, 1999</marker>
<rawString>Grefenstette, G. 1999. The WWW as a resource for example-based MT tasks. ASLIB “Translating and the Computer” conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H S Irvine</author>
<author>P C Kyllonen</author>
</authors>
<title>Item generation for test development.</title>
<date>2002</date>
<pages>412</pages>
<location>LEA:</location>
<contexts>
<context position="24072" citStr="Irvine and Kyllonen (2002)" startWordPosition="3972" endWordPosition="3975"> Besides measuring one’s ability to receive information in a foreign language, which has been addressed so far in this paper, it is important to measure a person’s ability to transmit information in a foreign language. For that purpose, tests for translating, writing, or speaking in a free format have been actively studied by many researchers (Shermis, 2003; Yasuda, 2004). 6 Related Work* Here, we explain other studies on the generation of multiple-choice questions for language learning. There are a few previous studies on computer* There are many works on item generation theory (ITG) such as Irvine and Kyllonen (2002), although we do not go any further into the area. We focus only on multiple-choice questions for language learning in this paper. based generation such as Mitkov (2003) and Wilson (1997). 6.1 Cloze Test A computer can generate questions by deleting words or parts of words randomly or at every N-th word from text. Test-takers are requested to restore the word that has been deleted. This is called a “cloze test.” The effectiveness of a “cloze test” or its derivatives is a matter of controversy among researchers of language testing such as Brown (1993) and Alderson (1996). 6.2 Tests on Facts Mit</context>
</contexts>
<marker>Irvine, Kyllonen, 2002</marker>
<rawString>Irvine, H. S., and Kyllonen, P. C. (2002). Item generation for test development. LEA: 412.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Izumi</author>
<author>H Isahara</author>
</authors>
<title>Investigation into language learners&apos; acquisition order based on the error analysis of the learner corpus.</title>
<date>2004</date>
<booktitle>In Proceedings of Pacific-Asia Conference on Language, Information and Computation (PACLIC) 18 Satellite Workshop on E-Learning,</booktitle>
<note>(in printing)</note>
<contexts>
<context position="8502" citStr="Izumi and Isahara, 2004" startWordPosition="1340" endWordPosition="1343">[d] If a sufficient number (in this paper, three) of candidates remain, form a question by randomizing the order of all the choices (“keep,” “guarantee,” “promise,” and “re‡ serve * Testing knowledge tells us what part of the seed sentence should be blanked. For example, we selected the verb of the seed because it is one of the basic types of blanked words in popular FBQs such as in TOEIC. This can be a word of another POS (Part-Of-Speech). For this, we can use knowledge in the field of second-language education. Previous studies on errors in English usage by Japanese native speakers such as (Izumi and Isahara, 2004) unveiled patterns of errors specific to Japanese, e.g., (1) article selection error, which results from the fact there are no articles in Japanese; (2) preposition selection error, which results from the fact some Japanese counterparts have broader meaning; (3) adjective selection error, which results from mismatch of meaning between Japanese words and their counterpart. Such knowledge may generate questions harder for Japanese who study English. † There are various aspects other than meaning, for example, spelling, pronunciation, and translation and so on. Depending on the aspect, lexical in</context>
</contexts>
<marker>Izumi, Isahara, 2004</marker>
<rawString>Izumi, E., and Isahara, H. (2004). Investigation into language learners&apos; acquisition order based on the error analysis of the learner corpus. In Proceedings of Pacific-Asia Conference on Language, Information and Computation (PACLIC) 18 Satellite Workshop on E-Learning, Japan. (in printing)</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Kikui</author>
<author>E Sumita</author>
<author>T Takaezawa</author>
<author>S Yamamoto</author>
</authors>
<title>Creating Corpora for Speech-to-Speech Translation,” Special Session “Multilingual Speech-toSpeech Translation” of EuroSpeech,</title>
<date>2003</date>
<contexts>
<context position="16097" citStr="Kikui et al., 2003" startWordPosition="2654" endWordPosition="2657">scrimination an item provides. The procedure is as follows. 1. Initialize I by the set of all generated FBQs. 64 2. According to Equation (3), we select the item whose contribution to test information is maximal. 3. We eliminate the selected item from I according to Equation (4). 4. If I is empty, we obtain the ordered list of effective items; otherwise, go back to step 2. Ii θ j = a i P i θ j − P i θ j ( ) 2 ( )(1 ( )) (2)  iˆ arg = max  ∑ ∑ I i j i∈I ˆ I = I − i(4) 4 Experiment The FBQs for the experiment were generated in February of 2004. Seed sentences were obtained from ATR’s corpus (Kikui et al., 2003) of the business and travel domains. The vocabulary of the corpus comprises about 30,000 words. Sentences are relatively short, with the average length being 6.47 words. For each domain 5,000 questions were generated automatically and each question consists of an English sentence with one blank and four choices. 4.1 Experiment with non-native speakers We used the TOEIC score as the experiment’s proficiency measure, and collected 100 Japanese subjects whose TOEIC scores were scattered from 400 to less than 900. The actual range for TOEIC scores is 10 to 990. Our subjects covered the dominant po</context>
</contexts>
<marker>Kikui, Sumita, Takaezawa, Yamamoto, 2003</marker>
<rawString>Kikui, G., Sumita, E., Takaezawa, T. and Yamamoto, S., “Creating Corpora for Speech-to-Speech Translation,” Special Session “Multilingual Speech-toSpeech Translation” of EuroSpeech, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
<author>G Grefenstette</author>
</authors>
<date>2003</date>
<journal>Special Issue on the WEB as Corpus. Computational Linguistics</journal>
<volume>29</volume>
<issue>3</issue>
<pages>333--502</pages>
<contexts>
<context position="10517" citStr="Kilgarriff and Grefenstette, 2003" startWordPosition="1652" endWordPosition="1655">hich is shared by and and (2) promise, which is shared by the words and Since this depends on the thesaurus used, some may sense a slight discomfort at these concepts. If a different thesaurus is used, the distracter can In “keep” (1) possession-or-disposal, “clear” “share,” “guarantee,”“promise,” “reserve.” didates may differ. In FBQs, by de The Web includes all manners of language data in vast quantities, which are for everyone easy to access through a networked computer. Recently, exploitation of the Web for various natural language applications is rising (Grefenstette, 1999; Turney, 2001; Kilgarriff and Grefenstette, 2003; Tonoike et al., 2004). We also propose aWeb-based approach. We dare to assume that if there is a sentence on the Web, that sentence is considered correct; otherwise, the sentence is unlikely to be correct in that there is no sentence written on the Web despite the variety an d quantity of data on it. Figure 3 illustrates verification based on the retrieval from the Web. Here, s (x) is the blanked sentence, s (w) denotes the sentence restored by the word w, and hits (y) represents the number of documents retrieved fr Blanked sentence: s( x ) = “I only have to my head above water one more week</context>
</contexts>
<marker>Kilgarriff, Grefenstette, 2003</marker>
<rawString>Kilgarriff, A. and Grefenstette, G. 2003. Special Issue on the WEB as Corpus. Computational Linguistics 29 (3): 333—502.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruslan Mitkov</author>
<author>Le An Ha</author>
</authors>
<title>Computer-Aided Generation of Multiple-Choice Tests.</title>
<date>2003</date>
<booktitle>HLT-NAACL 2003 Workshop: Building Educational Applications Using Natural Language Processing:</booktitle>
<pages>17--22</pages>
<marker>Mitkov, Ha, 2003</marker>
<rawString>Mitkov, Ruslan and Ha, Le An. 2003. Computer-Aided Generation of Multiple-Choice Tests. HLT-NAACL 2003 Workshop: Building Educational Applications Using Natural Language Processing: 17—22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ohno</author>
<author>M Hamanishi</author>
</authors>
<date>1984</date>
<location>Ruigo-Shin-Jiten, Kadokawa, Tokyo</location>
<note>(in Japanese)</note>
<contexts>
<context position="9809" citStr="Ohno and Hamanishi, 1984" startWordPosition="1543" endWordPosition="1546">and restart fr ” om step [a]. 2.3 Incorrectness Verification ition, (1) the blanked sentence restored with the correct choice is correct, and (2) the blanked sentence restored with the distracter must be incorrect. In order to generate an FBQ, the incorrectness of the sentence restored by each distracter candidate must be verified and if the combination is not incorrect, the can fin didate is rejected. Zero-Hit Sentence om the Web for the key y. rus should be consulted. Figure 3: Incorrectness and Hits on the Web in We used an house English thesaurus whose hierarchyis called Ruigo-Shin-Jiten (Ohno and Hamanishi, 1984). the above examples, the original word expresses two different concepts: which is shared by and and (2) promise, which is shared by the words and Since this depends on the thesaurus used, some may sense a slight discomfort at these concepts. If a different thesaurus is used, the distracter can In “keep” (1) possession-or-disposal, “clear” “share,” “guarantee,”“promise,” “reserve.” didates may differ. In FBQs, by de The Web includes all manners of language data in vast quantities, which are for everyone easy to access through a networked computer. Recently, exploitation of the Web for various </context>
</contexts>
<marker>Ohno, Hamanishi, 1984</marker>
<rawString>Ohno, S. and Hamanishi, M. 1984. Ruigo-Shin-Jiten, Kadokawa, Tokyo (in Japanese)</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C</author>
</authors>
<title>Automated Essay Scoring.</title>
<date>2003</date>
<pages>238</pages>
<location>LEA:</location>
<marker>C, 2003</marker>
<rawString>Shermis, M. D. and Burstein. J. C. 2003. Automated Essay Scoring. LEA: 238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tonoike</author>
<author>S Sato</author>
<author>T Utsuro</author>
</authors>
<title>Answer Validation by Keyword Association.</title>
<date>2004</date>
<journal>IPSJ, SIGNL,</journal>
<volume>161</volume>
<note>53—60, (in Japanese).</note>
<contexts>
<context position="10540" citStr="Tonoike et al., 2004" startWordPosition="1656" endWordPosition="1659">se, which is shared by the words and Since this depends on the thesaurus used, some may sense a slight discomfort at these concepts. If a different thesaurus is used, the distracter can In “keep” (1) possession-or-disposal, “clear” “share,” “guarantee,”“promise,” “reserve.” didates may differ. In FBQs, by de The Web includes all manners of language data in vast quantities, which are for everyone easy to access through a networked computer. Recently, exploitation of the Web for various natural language applications is rising (Grefenstette, 1999; Turney, 2001; Kilgarriff and Grefenstette, 2003; Tonoike et al., 2004). We also propose aWeb-based approach. We dare to assume that if there is a sentence on the Web, that sentence is considered correct; otherwise, the sentence is unlikely to be correct in that there is no sentence written on the Web despite the variety an d quantity of data on it. Figure 3 illustrates verification based on the retrieval from the Web. Here, s (x) is the blanked sentence, s (w) denotes the sentence restored by the word w, and hits (y) represents the number of documents retrieved fr Blanked sentence: s( x ) = “I only have to my head above water one more week. ” Hits of incorrect c</context>
</contexts>
<marker>Tonoike, Sato, Utsuro, 2004</marker>
<rawString>Tonoike, M., Sato, S., and Utsuro, T. 2004. Answer Validation by Keyword Association. IPSJ, SIGNL, 161: 53—60, (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
</authors>
<title>Mining the Web for synonyms: PMIIR vs. LSA on TOEFL. ECML</title>
<date>2001</date>
<pages>491--502</pages>
<contexts>
<context position="10482" citStr="Turney, 2001" startWordPosition="1650" endWordPosition="1651">nt concepts: which is shared by and and (2) promise, which is shared by the words and Since this depends on the thesaurus used, some may sense a slight discomfort at these concepts. If a different thesaurus is used, the distracter can In “keep” (1) possession-or-disposal, “clear” “share,” “guarantee,”“promise,” “reserve.” didates may differ. In FBQs, by de The Web includes all manners of language data in vast quantities, which are for everyone easy to access through a networked computer. Recently, exploitation of the Web for various natural language applications is rising (Grefenstette, 1999; Turney, 2001; Kilgarriff and Grefenstette, 2003; Tonoike et al., 2004). We also propose aWeb-based approach. We dare to assume that if there is a sentence on the Web, that sentence is considered correct; otherwise, the sentence is unlikely to be correct in that there is no sentence written on the Web despite the variety an d quantity of data on it. Figure 3 illustrates verification based on the retrieval from the Web. Here, s (x) is the blanked sentence, s (w) denotes the sentence restored by the word w, and hits (y) represents the number of documents retrieved fr Blanked sentence: s( x ) = “I only have t</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>Turney, P.D. 2001. Mining the Web for synonyms: PMIIR vs. LSA on TOEFL. ECML 2001: 491—502.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Howard Wainer</author>
</authors>
<title>Conputerized Adaptive Testing: A Primer,</title>
<date>2000</date>
<pages>335</pages>
<location>(Second Edition). LEA:</location>
<contexts>
<context position="4512" citStr="Wainer, 2000" startWordPosition="676" endWordPosition="677"> speakers’ real proficiency. This system not only provides us with a quick and inexpensive testing method, but it also features the following advantages: (I) It provides “anyone” individually with up-to-date and interesting questions for self-teaching. We have implemented a program that downloads any Web page such as a news site and generates questions from it. (II) It also enables on-demand testing at “anytime and anyplace.” We have implemented a system that operates on a mobile phone. Questions are generated and pooled in the server, and upon a user’s request, questions are downloaded. CAT (Wainer, 2000) is then conducted on the phone. The system for mobile phone is scheduled to be deployed in May of 2005 in Japan. The remainder of this paper is organized as follows. Section 2 introduces a method for making FBQ, Section 3 explains how to estimate testtakers’ proficiency, and Section 4 presents the experiments that demonstrate the effectiveness of the proposal. Section 5 provides some discussion, and Section 6 explains the differences between our proposal and related work, followed by concluding remarks. 2 Question Generation Method We will review an FBQ, and then explain our method for produc</context>
</contexts>
<marker>Wainer, 2000</marker>
<rawString>Wainer, Howard et al. 2000. Conputerized Adaptive Testing: A Primer, (Second Edition). LEA: 335.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Wilson</author>
</authors>
<title>The Automatic Generation of CALL exercises from general corpora,</title>
<date>1997</date>
<booktitle>Teaching and Language Corpora,</booktitle>
<pages>116--130</pages>
<editor>in eds. Wichmann, A., Fligelstone, S., McEnery, T., Knowles, G.,</editor>
<location>Harlow:</location>
<contexts>
<context position="24259" citStr="Wilson (1997)" startWordPosition="4006" endWordPosition="4008"> foreign language. For that purpose, tests for translating, writing, or speaking in a free format have been actively studied by many researchers (Shermis, 2003; Yasuda, 2004). 6 Related Work* Here, we explain other studies on the generation of multiple-choice questions for language learning. There are a few previous studies on computer* There are many works on item generation theory (ITG) such as Irvine and Kyllonen (2002), although we do not go any further into the area. We focus only on multiple-choice questions for language learning in this paper. based generation such as Mitkov (2003) and Wilson (1997). 6.1 Cloze Test A computer can generate questions by deleting words or parts of words randomly or at every N-th word from text. Test-takers are requested to restore the word that has been deleted. This is called a “cloze test.” The effectiveness of a “cloze test” or its derivatives is a matter of controversy among researchers of language testing such as Brown (1993) and Alderson (1996). 6.2 Tests on Facts Mitkov (2003) proposed a computer-aided procedure for generating multiple-choice questions from textbooks. The differences from our proposal are that (1) Mitkov’s method generates questions </context>
</contexts>
<marker>Wilson, 1997</marker>
<rawString>Wilson, E. 1997. The Automatic Generation of CALL exercises from general corpora, in eds. Wichmann, A., Fligelstone, S., McEnery, T., Knowles, G., Teaching and Language Corpora, Harlow: Longman:116-130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yasuda</author>
<author>F Sugaya</author>
<author>E Sumita</author>
<author>T Takezawa</author>
<author>G Kikui</author>
<author>S Yamamoto</author>
</authors>
<date>2004</date>
<booktitle>Automatic Measuring of English Language Proficiency using MT Evaluation Technology, COLING 2004 eLearning for Computational Linguistics and Computational Linguistics for eLearning:</booktitle>
<pages>53--60</pages>
<marker>Yasuda, Sugaya, Sumita, Takezawa, Kikui, Yamamoto, 2004</marker>
<rawString>Yasuda, K., Sugaya, F., Sumita, E., Takezawa, T., Kikui, G. and Yamamoto, S. 2004. Automatic Measuring of English Language Proficiency using MT Evaluation Technology, COLING 2004 eLearning for Computational Linguistics and Computational Linguistics for eLearning: 53-60.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>