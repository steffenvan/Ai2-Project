<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005537">
<title confidence="0.6374055">
Estimating probability of correctness for ASR N-Best lists
Jason D. Williams and Suhrid Balakrishnan
</title>
<author confidence="0.383973">
AT&amp;T Labs - Research, Shannon Laboratory, 180 Park Ave., Florham Park, NJ 07932, USA
</author>
<email confidence="0.977623">
{jdw,suhrid}@research.att.com
</email>
<sectionHeader confidence="0.993462" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99933475">
For a spoken dialog system to make good
use of a speech recognition N-Best list, it is
essential to know how much trust to place
in each entry. This paper presents a method
for assigning a probability of correctness to
each of the items on the N-Best list, and to
the hypothesis that the correct answer is not
on the list. We find that both multinomial lo-
gistic regression and support vector machine
models yields meaningful, useful probabili-
ties across different tasks and operating con-
ditions.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999962666666667">
For spoken dialog systems, speech recognition er-
rors are common, and so identifying and reducing
dialog understanding errors is an important problem.
One source of potentially useful information is the
N-Best list output by the automatic speech recog-
nition (ASR) engine. The N-Best list contains N
ranked hypotheses for the user’s speech, where the
top entry is the engine’s best hypothesis. When the
top entry is incorrect, the correct entry is often con-
tained lower down in the N-Best list. For a dialog
system to make use of the N-Best list, it is useful to
estimate the probability of correctness for each en-
try, and the probability that the correct entry is not
on the list. This paper describes a way of assigning
these probabilities.
</bodyText>
<sectionHeader confidence="0.678325" genericHeader="introduction">
2 Background and related work
</sectionHeader>
<bodyText confidence="0.999031444444445">
To begin, we formalize the problem. The user takes
a communicative action u, saying a phrase such as
“Coffee shops in Madison New Jersey”. Using a lan-
guage model g, the speech recognition engine pro-
cesses this audio and outputs an ordered list of N
hypotheses for u, u = {fi1,... �uN}, N &gt; 2. To
the N-Best list we add the entry fi*, where u = fi*
indicates that u does not appear on the N-Best list.
The ASR engine also generates a set of K recog-
nition features f = [f1, ... , fK]. These features
might include properties of the lattice, word confu-
sion network, garbage model, etc. The aim of this
paper is to estimate a model which accurately as-
signs the N + 1 probabilities P(u = fi,,,|u, f) for
n E {*,1, ... , N} given u and f. The model also
depends on the language model g, but we don’t in-
clude this conditioning in our notation for clarity.
In estimating these probabilities, we are most
concerned with the estimates being well-calibrated.
This means that the probability estimates we ob-
tain for events should accurately represent the em-
pirically observed proportions of those events. For
example, if 100 1-best recognitions are assigned a
probability of 60%, then approximately 60 of those
100 should in fact be the correct result.
Recent work proposed a generative model of the
N-Best list, P(u, f|u) (Williams, 2008). The main
motivation for computing a generative model is
that it is a component of the update equation used
by several statistical approaches to spoken dialog
(Williams and Young, 2007). However, the diffi-
culty with a generative model is that it must estimate
a joint probability over all the features, f; thus, mak-
ing use of many features becomes problematic. As
a result, discriminative approaches often yield bet-
ter results. In our work, we propose a discrimina-
tive approach and focus on estimating the probabil-
ities conditioned on the features. Additionally, un-
der some further fairly mild assumptions, by apply-
ing Bayes Rule our model can be shown equivalent
to the generative model required in the dialog state
update. This is a desirable property because dialog
systems using this re-statement have been shown to
work in practice (Young et al., 2009).
Much past work has assigned meaningful proba-
</bodyText>
<subsubsectionHeader confidence="0.640571">
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 132–135,
</subsubsectionHeader>
<affiliation confidence="0.938118">
Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics
</affiliation>
<page confidence="0.999462">
132
</page>
<bodyText confidence="0.999929142857143">
bilities to the top ASR hypothesis; the novelty here
is assigning probabilities to all the entries on the list.
Also, our task is different to N-Best list re-ranking,
which seeks to move more promising entries toward
the top of the list. Here we trust the ordering pro-
vided by the ASR engine, and only seek to assign
meaningful probabilities to the elements.
</bodyText>
<sectionHeader confidence="0.991747" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.999787294117647">
Our task is to estimate P(u = fin|u,f) for n ∈
{∗, 1, . . . , N}. Ideally we could view each element
on the N-Best list as its own class and train an
(N +1)-class regression model. However this is dif-
ficult for two reasons. First, the number of classes is
variable: ASR results can have different N-Best list
lengths for different utterances. Second, we found
that the distribution of items on the N-Best list has
a very long tail, so it would be difficult to obtain
enough data to accurately estimate late position class
probabilities.
As a result, we model the probability P in two
stages: first, we train a (discriminative) model Pa to
assign probabilities to just three classes: u = fi*,
u = �u1, and u ∈ �u2+, where u2+ = {�ut, ... , fiN}.
In the second stage, we use a separate probability
model Pb to distribute mass over the items in u2+:
</bodyText>
<equation confidence="0.99917025">
P(iin = u|�u, f) = (1)
{ Pa(u = fi1|f) if n = 1,
Pa(u ∈ u2+|f)Pb(u = fin|f) if n &gt; 1,
Pa(u = fi*|f) if n = ∗
</equation>
<bodyText confidence="0.999889866666667">
To model Pa, multinomial logistic regression
(MLR) is a natural choice as it yields a well-
calibrated estimator for multi-class problems. Stan-
dard MLR can over-fit when there are many features
in comparison to the number of training examples;
to address this we use ridge regularized MLR in our
experiments below (Genkin et al., 2005).
An alternative to MLR is support vector machines
(SVMs). SVMs are typically formulated including
regularization; however, their output scores are gen-
erally not interpretable as probabilities. Thus for Pa,
we use an extension which re-scales SVM scores to
yield well-calibrated probabilities (Platt, 1999).
Our second stage model Pb, distributes mass
over the items in the tail of the N-best list (n ∈
</bodyText>
<figure confidence="0.7235665">
0% 20% 40% 60% 80% 100%
Fractional position in N-Best list (n/N) of correct entry
</figure>
<figureCaption confidence="0.947661">
Figure 1: Empirical cumulative distribution of cor-
</figureCaption>
<bodyText confidence="0.940364470588235">
rect recognitions for N-Best lists, and the Beta dis-
tribution model for Pb on 1, 000 business search ut-
terances (Corpus 1 training set, from Section 4.)
{2,. .. , N}). In our exploratory analysis of N-Best
lists, we noticed a trend that facilitates modeling this
distribution. We observed that the distribution of the
fraction of the correction position n/N was rela-
tively invariant to N. For example, for both short
(N &lt; 100) and long (N ≥ 100) lists, the proba-
bility that the answer was in the top half of the list
was very similar (see Figure 1). Thus, we chose a
continuous distribution in terms of the fractional po-
sition n/N as the underlying distribution in our sec-
ond stage model. Given the domain of the fractional
position [0, 1], we chose a Beta distribution. Our fi-
nal second stage model is then an appropriately dis-
cretized version of the underlying Beta, namely, Pb:
</bodyText>
<equation confidence="0.998630666666667">
Pb(u = fin|f) = Pb(u = fin|N) =
n − 1 n − 2
Pbeta ( N − 1 α, β) − Pbeta ( N − 1; α, β)
</equation>
<bodyText confidence="0.998687333333333">
where Pbeta(x; α, β) is the standard Beta cumula-
tive distribution function parametrized by α and β.
Figure 1 shows an illustration. In summary, our
method requires training the three-class regression
model Pa, and estimating the Beta distribution pa-
rameters α and β.
</bodyText>
<sectionHeader confidence="0.984948" genericHeader="evaluation">
4 Data and experiments
</sectionHeader>
<bodyText confidence="0.999808666666667">
We tested the method by applying it to three cor-
pora of utterances from dialog systems in the busi-
ness search domain. All utterances were from
</bodyText>
<figure confidence="0.952914818181818">
Cumulative probability
100%
40%
20%
80%
60%
0%
N-Best lists with N &lt; 100 entries
N-Best lists with N &gt;= 100 entries
All N-Best lists
Model (Beta distribution)
</figure>
<page confidence="0.859716">
133
</page>
<table confidence="0.99409575">
Corpus WCN SVM MLR
1 -0.714 -0.697 -0.703
2 -0.251 -0.264 -0.222
3 -0.636 -0.605 -0.581
</table>
<tableCaption confidence="0.998388">
Table 1: Mean log-likelihoods on the portion of the
</tableCaption>
<bodyText confidence="0.999738289473684">
test set with the correct answer on the N-Best list.
None of the MLR nor SVM results differ signifi-
cantly from the WCN baseline at p &lt; 0.02.2
users with real information needs. Corpus 1 con-
tained 2, 000 high-quality-audio utterances spoken
by customers using the Speak4It application, a
business search application which operates on mo-
bile devices, supporting queries containing a listing
name and optionally a location.1 Corpus 2 and 3
contained telephone-quality-audio utterances from
14,000 calls to AT&amp;T’s “411” business directory
listing service. Corpus 2 contained locations (re-
sponses to “Say a city and state”); corpus 3 con-
tained listing names (responses to “OK what list-
ing?”). Corpus 1 was split in half for training and
testing; corpora 2 and 3 were split into 10, 000 train-
ing and 4, 000 testing utterances.
We performed recognition using the Watson
speech recognition engine (Goffin et al., 2005), in
two configurations. Configuration A uses a sta-
tistical language model trained to recognize busi-
ness listings and optionally locations, and acous-
tic models for high-quality audio. Configuration B
uses a rule-based language model consisting of all
city/state pairs in the USA, and acoustic models for
telephone-quality audio. Configuration A was ap-
plied to corpora 1 and 3, and Configuration B was
applied to corpus 2. This experimental design is in-
tended to test our method on both rule-based and
statistical language models, as well as matched and
mis-matched acoustic and language model condi-
tions.
We used the following recognition features in f:
f1 is the posterior probability from the best path
through the word confusion network, f2 is the num-
ber of segments in the word confusion network,
f3 is the length of the N-Best list, f4 is the aver-
age per-frame difference in likelihood between the
</bodyText>
<footnote confidence="0.727897">
1http://speak4it.com
22-tailed Wilcoxon Signed-Rank Test; 10-way partitioning.
</footnote>
<table confidence="0.99344675">
Corpus WCN SVM MLR
1 -1.12 -0.882 -0.890
2 -0.821 -0.753 -0.734
3 -1.00 -0.820 -0.824
</table>
<tableCaption confidence="0.99371">
Table 2: Mean log-likelihoods on the complete test
</tableCaption>
<bodyText confidence="0.977398390243903">
set. All MLR and SVM results are significantly bet-
ter than the WCN baseline (p &lt; 0.0054).2
highest-likelihood lattice path and a garbage model,
and f5 is the average per-frame difference in likeli-
hood between the highest-likelihood lattice path and
the maximum likelihood of that frame on any path
through the lattice. Features are standardized to the
range [−1, 1] and MLR and SVM hyperparameters
were fit by cross-validation on the training set. The
α and Q parameters were fit by maximum likelihood
on the training set.
We used the BMR toolkit for regularized multi-
nomial logistic regression (Genkin et al., 2005), and
the LIB-SVM toolkit for calibrated SVMs (Chang
and Lin, 2001).
We first measure average log-likelihood the mod-
els assign to the test sets. As a baseline, we use the
posterior probability estimated by the word confu-
sion network (WCN), which has been used in past
work for estimating likelihood of N-Best list entries
(Young et al., 2009). However, the WCN does not
assign probability to the u = fi* case – indeed, this
is a limitation of using WCN posteriors. So we re-
ported two sets of results. In Table 1, we report the
average log-likelihood given that the correct result
is on the N-Best list (higher values, i.e., closer to
zero are better). This table includes only the items
in the test set for which the correct result appeared
on the N-Best list (that is, excluding the u = fi*
cases). This table compares our models to WCNs
on the task for which the WCN is designed. On this
task, the MLR and SVM methods are competitive
with WCNs, but not significantly better.
In Table 2, we report average log-likelihood for
the entire test set. Here the WCNs use a fixed
prior for the u = fi* case, estimated on the training
sets (u = fi* class is always assigned 0.284; other
classes are assigned 1 − 0.284 = 0.716 times the
WCN posterior). This table compares our models
to WCNs on the task for which our model is de-
signed. Here, the MLR and SVM models yielded
</bodyText>
<page confidence="0.991131">
134
</page>
<figure confidence="0.999666731707317">
100%
90%
80%
70%
60%
50%
40%
30%
20% Perfect calibration (left axis)
MLR Calibration (left axis)
Number of entries (right axis)
0%
Fraction correct
10%
40
20
80
60
0
180
160
140
120
100
Number of entries
50%
40%
MLR-assigned probability
n=1 posterior from word confusion network (f )
1
Average delta to best frame in lattice (f )
5
Average delta to garbage model (f )
4
10%
0%
True Accepts
30%
20%
0.05 0.15 0.25 0.35 0.45 0.55 0.65 0.75 0.85 0.95
Regression-assigned probability
</figure>
<figureCaption confidence="0.9940195">
Figure 2: Calibration and histogram of probabilities
assigned by MLR on corpus 1 (test set).
</figureCaption>
<figure confidence="0.992029">
0% 10% 20% 30% 40%
False Accepts
</figure>
<figureCaption confidence="0.992161">
Figure 3: ROC curve for MLR and the 3 most infor-
mative input features on corpus 1 (test set).
</figureCaption>
<bodyText confidence="0.994845">
significantly better results than the WCN baseline.
We next investigated the calibration properties of
the models. Results for the MLR model on the
u = fi1 class from corpus 1 test set are shown in
Figure 2. This illustrates that the MLR model is rel-
atively well-calibrated and yields broadly distributed
probabilities. Results for the SVM were similar, and
are omitted for space.
Finally we investigated whether the models
yielded better accept/reject decisions than their in-
dividual features. Figure 3 shows the MLR model
a receiver operating characteristic (ROC) curve for
corpus 1 test set for the u = fi1 class. This con-
firms that the MLR model produces more accurate
accept/reject decisions than the individual features
alone. Results for the SVM were similar.
</bodyText>
<sectionHeader confidence="0.999678" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999977866666667">
This paper has presented a method for assigning
useful, meaningful probabilities to elements on an
ASR N-Best list. Multinomial logistic regression
(MLR) and support vector machines (SVMs) have
been tested, and both produce significantly better
models than a word confusion network baseline, as
measured by average log likelihood. Further, the
models appear to be well-calibrated and yield a bet-
ter indication of correctness than any of its input fea-
tures individually.
In dialog systems, we are often more interested in
the concepts than specific words, so in future work,
we hope to assign probabilities to concepts. In the
meantime, we are applying the method to our dialog
systems, to verify their usefulness in practice.
</bodyText>
<sectionHeader confidence="0.99909" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999552615384615">
CC Chang and CJ Lin, 2001. LIBSVM: a library for sup-
port vector machines. http://www.csie.ntu.
edu.tw/-cjlin/libsvm.
A Genkin, DD Lewis, and D Madigan, 2005.
BMR: Bayesian Multinomial Regression Soft-
ware. http://www.stat.rutgers.edu/
-madigan/BMR/.
V Goffin, C Allauzen, E Bocchieri, D Hakkani-Tur,
A Ljolje, S Parthasarathy, M Rahim, G Riccardi, and
M Saraclar. 2005. The AT&amp;T Watson speech recog-
nizer. In Proc ICASSP, Philadelphia.
JC Platt. 1999. Probabilistic outputs for support vector
machines and comparisons to regularized likelihood
methods. In Advances in Large Margin Classifiers,
pages 61–74. MIT Press.
JD Williams and SJ Young. 2007. Partially observable
Markov decision processes for spoken dialog systems.
Computer Speech and Language, 21(2):393–422.
JD Williams. 2008. Exploiting the ASR N-best by track-
ing multiple dialog state hypotheses. In Proc ICSLP,
Brisbane.
SJ Young, M Gaˇsi´c, S Keizer, F Mairesse, J Schatzmann,
B Thomson, and K Yu. 2009. The hidden information
state model: a practical framework for POMDP-based
spoken dialogue management. Computer Speech and
Language. To appear.
</reference>
<page confidence="0.998788">
135
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.367908">
<title confidence="0.999724">Estimating probability of correctness for ASR N-Best lists</title>
<author confidence="0.999728">Jason D Williams</author>
<author confidence="0.999728">Suhrid Balakrishnan</author>
<affiliation confidence="0.449814">AT&amp;T Labs - Research, Shannon Laboratory, 180 Park Ave., Florham Park, NJ 07932,</affiliation>
<abstract confidence="0.985574153846154">For a spoken dialog system to make good use of a speech recognition N-Best list, it is essential to know how much trust to place in each entry. This paper presents a method for assigning a probability of correctness to each of the items on the N-Best list, and to the hypothesis that the correct answer is not on the list. We find that both multinomial logistic regression and support vector machine models yields meaningful, useful probabilities across different tasks and operating conditions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>CC Chang</author>
<author>CJ Lin</author>
</authors>
<title>LIBSVM: a library for support vector machines.</title>
<date>2001</date>
<note>http://www.csie.ntu. edu.tw/-cjlin/libsvm.</note>
<contexts>
<context position="10572" citStr="Chang and Lin, 2001" startWordPosition="1809" endWordPosition="1812">WCN baseline (p &lt; 0.0054).2 highest-likelihood lattice path and a garbage model, and f5 is the average per-frame difference in likelihood between the highest-likelihood lattice path and the maximum likelihood of that frame on any path through the lattice. Features are standardized to the range [−1, 1] and MLR and SVM hyperparameters were fit by cross-validation on the training set. The α and Q parameters were fit by maximum likelihood on the training set. We used the BMR toolkit for regularized multinomial logistic regression (Genkin et al., 2005), and the LIB-SVM toolkit for calibrated SVMs (Chang and Lin, 2001). We first measure average log-likelihood the models assign to the test sets. As a baseline, we use the posterior probability estimated by the word confusion network (WCN), which has been used in past work for estimating likelihood of N-Best list entries (Young et al., 2009). However, the WCN does not assign probability to the u = fi* case – indeed, this is a limitation of using WCN posteriors. So we reported two sets of results. In Table 1, we report the average log-likelihood given that the correct result is on the N-Best list (higher values, i.e., closer to zero are better). This table incl</context>
</contexts>
<marker>Chang, Lin, 2001</marker>
<rawString>CC Chang and CJ Lin, 2001. LIBSVM: a library for support vector machines. http://www.csie.ntu. edu.tw/-cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Genkin</author>
<author>DD Lewis</author>
<author>D Madigan</author>
</authors>
<title>BMR: Bayesian Multinomial Regression Software.</title>
<date>2005</date>
<note>http://www.stat.rutgers.edu/ -madigan/BMR/.</note>
<contexts>
<context position="5621" citStr="Genkin et al., 2005" startWordPosition="972" endWordPosition="975">hree classes: u = fi*, u = �u1, and u ∈ �u2+, where u2+ = {�ut, ... , fiN}. In the second stage, we use a separate probability model Pb to distribute mass over the items in u2+: P(iin = u|�u, f) = (1) { Pa(u = fi1|f) if n = 1, Pa(u ∈ u2+|f)Pb(u = fin|f) if n &gt; 1, Pa(u = fi*|f) if n = ∗ To model Pa, multinomial logistic regression (MLR) is a natural choice as it yields a wellcalibrated estimator for multi-class problems. Standard MLR can over-fit when there are many features in comparison to the number of training examples; to address this we use ridge regularized MLR in our experiments below (Genkin et al., 2005). An alternative to MLR is support vector machines (SVMs). SVMs are typically formulated including regularization; however, their output scores are generally not interpretable as probabilities. Thus for Pa, we use an extension which re-scales SVM scores to yield well-calibrated probabilities (Platt, 1999). Our second stage model Pb, distributes mass over the items in the tail of the N-best list (n ∈ 0% 20% 40% 60% 80% 100% Fractional position in N-Best list (n/N) of correct entry Figure 1: Empirical cumulative distribution of correct recognitions for N-Best lists, and the Beta distribution mod</context>
<context position="10505" citStr="Genkin et al., 2005" startWordPosition="1798" endWordPosition="1801">est set. All MLR and SVM results are significantly better than the WCN baseline (p &lt; 0.0054).2 highest-likelihood lattice path and a garbage model, and f5 is the average per-frame difference in likelihood between the highest-likelihood lattice path and the maximum likelihood of that frame on any path through the lattice. Features are standardized to the range [−1, 1] and MLR and SVM hyperparameters were fit by cross-validation on the training set. The α and Q parameters were fit by maximum likelihood on the training set. We used the BMR toolkit for regularized multinomial logistic regression (Genkin et al., 2005), and the LIB-SVM toolkit for calibrated SVMs (Chang and Lin, 2001). We first measure average log-likelihood the models assign to the test sets. As a baseline, we use the posterior probability estimated by the word confusion network (WCN), which has been used in past work for estimating likelihood of N-Best list entries (Young et al., 2009). However, the WCN does not assign probability to the u = fi* case – indeed, this is a limitation of using WCN posteriors. So we reported two sets of results. In Table 1, we report the average log-likelihood given that the correct result is on the N-Best lis</context>
</contexts>
<marker>Genkin, Lewis, Madigan, 2005</marker>
<rawString>A Genkin, DD Lewis, and D Madigan, 2005. BMR: Bayesian Multinomial Regression Software. http://www.stat.rutgers.edu/ -madigan/BMR/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Goffin</author>
<author>C Allauzen</author>
<author>E Bocchieri</author>
<author>D Hakkani-Tur</author>
<author>A Ljolje</author>
<author>S Parthasarathy</author>
<author>M Rahim</author>
<author>G Riccardi</author>
<author>M Saraclar</author>
</authors>
<title>The AT&amp;T Watson speech recognizer.</title>
<date>2005</date>
<booktitle>In Proc ICASSP,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="8770" citStr="Goffin et al., 2005" startWordPosition="1517" endWordPosition="1520">iness search application which operates on mobile devices, supporting queries containing a listing name and optionally a location.1 Corpus 2 and 3 contained telephone-quality-audio utterances from 14,000 calls to AT&amp;T’s “411” business directory listing service. Corpus 2 contained locations (responses to “Say a city and state”); corpus 3 contained listing names (responses to “OK what listing?”). Corpus 1 was split in half for training and testing; corpora 2 and 3 were split into 10, 000 training and 4, 000 testing utterances. We performed recognition using the Watson speech recognition engine (Goffin et al., 2005), in two configurations. Configuration A uses a statistical language model trained to recognize business listings and optionally locations, and acoustic models for high-quality audio. Configuration B uses a rule-based language model consisting of all city/state pairs in the USA, and acoustic models for telephone-quality audio. Configuration A was applied to corpora 1 and 3, and Configuration B was applied to corpus 2. This experimental design is intended to test our method on both rule-based and statistical language models, as well as matched and mis-matched acoustic and language model conditi</context>
</contexts>
<marker>Goffin, Allauzen, Bocchieri, Hakkani-Tur, Ljolje, Parthasarathy, Rahim, Riccardi, Saraclar, 2005</marker>
<rawString>V Goffin, C Allauzen, E Bocchieri, D Hakkani-Tur, A Ljolje, S Parthasarathy, M Rahim, G Riccardi, and M Saraclar. 2005. The AT&amp;T Watson speech recognizer. In Proc ICASSP, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>JC Platt</author>
</authors>
<title>Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.</title>
<date>1999</date>
<booktitle>In Advances in Large Margin Classifiers,</booktitle>
<pages>61--74</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="5927" citStr="Platt, 1999" startWordPosition="1017" endWordPosition="1018">gistic regression (MLR) is a natural choice as it yields a wellcalibrated estimator for multi-class problems. Standard MLR can over-fit when there are many features in comparison to the number of training examples; to address this we use ridge regularized MLR in our experiments below (Genkin et al., 2005). An alternative to MLR is support vector machines (SVMs). SVMs are typically formulated including regularization; however, their output scores are generally not interpretable as probabilities. Thus for Pa, we use an extension which re-scales SVM scores to yield well-calibrated probabilities (Platt, 1999). Our second stage model Pb, distributes mass over the items in the tail of the N-best list (n ∈ 0% 20% 40% 60% 80% 100% Fractional position in N-Best list (n/N) of correct entry Figure 1: Empirical cumulative distribution of correct recognitions for N-Best lists, and the Beta distribution model for Pb on 1, 000 business search utterances (Corpus 1 training set, from Section 4.) {2,. .. , N}). In our exploratory analysis of N-Best lists, we noticed a trend that facilitates modeling this distribution. We observed that the distribution of the fraction of the correction position n/N was relativel</context>
</contexts>
<marker>Platt, 1999</marker>
<rawString>JC Platt. 1999. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. In Advances in Large Margin Classifiers, pages 61–74. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>JD Williams</author>
<author>SJ Young</author>
</authors>
<title>Partially observable Markov decision processes for spoken dialog systems.</title>
<date>2007</date>
<journal>Computer Speech and Language,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="3020" citStr="Williams and Young, 2007" startWordPosition="514" endWordPosition="517">re most concerned with the estimates being well-calibrated. This means that the probability estimates we obtain for events should accurately represent the empirically observed proportions of those events. For example, if 100 1-best recognitions are assigned a probability of 60%, then approximately 60 of those 100 should in fact be the correct result. Recent work proposed a generative model of the N-Best list, P(u, f|u) (Williams, 2008). The main motivation for computing a generative model is that it is a component of the update equation used by several statistical approaches to spoken dialog (Williams and Young, 2007). However, the difficulty with a generative model is that it must estimate a joint probability over all the features, f; thus, making use of many features becomes problematic. As a result, discriminative approaches often yield better results. In our work, we propose a discriminative approach and focus on estimating the probabilities conditioned on the features. Additionally, under some further fairly mild assumptions, by applying Bayes Rule our model can be shown equivalent to the generative model required in the dialog state update. This is a desirable property because dialog systems using th</context>
</contexts>
<marker>Williams, Young, 2007</marker>
<rawString>JD Williams and SJ Young. 2007. Partially observable Markov decision processes for spoken dialog systems. Computer Speech and Language, 21(2):393–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>JD Williams</author>
</authors>
<title>Exploiting the ASR N-best by tracking multiple dialog state hypotheses.</title>
<date>2008</date>
<booktitle>In Proc ICSLP,</booktitle>
<location>Brisbane.</location>
<contexts>
<context position="2834" citStr="Williams, 2008" startWordPosition="486" endWordPosition="487"> , N} given u and f. The model also depends on the language model g, but we don’t include this conditioning in our notation for clarity. In estimating these probabilities, we are most concerned with the estimates being well-calibrated. This means that the probability estimates we obtain for events should accurately represent the empirically observed proportions of those events. For example, if 100 1-best recognitions are assigned a probability of 60%, then approximately 60 of those 100 should in fact be the correct result. Recent work proposed a generative model of the N-Best list, P(u, f|u) (Williams, 2008). The main motivation for computing a generative model is that it is a component of the update equation used by several statistical approaches to spoken dialog (Williams and Young, 2007). However, the difficulty with a generative model is that it must estimate a joint probability over all the features, f; thus, making use of many features becomes problematic. As a result, discriminative approaches often yield better results. In our work, we propose a discriminative approach and focus on estimating the probabilities conditioned on the features. Additionally, under some further fairly mild assum</context>
</contexts>
<marker>Williams, 2008</marker>
<rawString>JD Williams. 2008. Exploiting the ASR N-best by tracking multiple dialog state hypotheses. In Proc ICSLP, Brisbane.</rawString>
</citation>
<citation valid="true">
<authors>
<author>SJ Young</author>
<author>M Gaˇsi´c</author>
<author>S Keizer</author>
<author>F Mairesse</author>
<author>J Schatzmann</author>
<author>B Thomson</author>
<author>K Yu</author>
</authors>
<title>The hidden information state model: a practical framework for POMDP-based spoken dialogue management. Computer Speech and Language.</title>
<date>2009</date>
<note>To appear.</note>
<marker>Young, Gaˇsi´c, Keizer, Mairesse, Schatzmann, Thomson, Yu, 2009</marker>
<rawString>SJ Young, M Gaˇsi´c, S Keizer, F Mairesse, J Schatzmann, B Thomson, and K Yu. 2009. The hidden information state model: a practical framework for POMDP-based spoken dialogue management. Computer Speech and Language. To appear.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>