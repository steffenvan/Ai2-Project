<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.283653">
<title confidence="0.982345">
Validation of a Dialog System for Language Learners
</title>
<author confidence="0.804924">
Alicia Sagae, W. Lewis Johnson, Stephen Bodnar
</author>
<affiliation confidence="0.699519">
Alelo, Inc.
</affiliation>
<address confidence="0.795538">
Los Angeles, CA
</address>
<email confidence="0.99878">
{asagae,ljohnson,sbodnar}@alelo.com
</email>
<sectionHeader confidence="0.994226" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.994566466666667">
In this paper we present experiments related to
the validation of spoken language understand-
ing capabilities in a language and culture train-
ing system. In this application, word-level
recognition rates are insufficient to character-
ize how well the system serves its users. We
present the results of an annotation exercise
that distinguishes instances of non-recognition
due to learner error from instances due to poor
system coverage. These statistics give a more
accurate and interesting description of system
performance, showing how the system could
be improved without sacrificing the instruc-
tional value of rejecting learner utterances
when they are poorly formed.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999930039215686">
Conversational practice in real-time dialogs with
virtual humans is a compelling element of train-
ing systems for communicative competency,
helping learners acquire procedural skills in addi-
tion to declarative knowledge (Johnson, Rickel et
al. 2000). Alelo&apos;s language and culture training
systems allow language learners to engage in
such dialogs in a serious game environment,
where they practice task-based missions in new
linguistic and cultural settings (Barrett and
Johnson 2010). To support this capability, Alelo
products apply a variety of spoken dialog tech-
nologies, including automatic speech recognition
(ASR) and agent-based models of dialog that
capture theories of politeness (Wang and
Johnson 2008), and cultural expectations (John-
son, 2010; (Sagae, Wetzel et al. 2009).
To properly assess these dialog systems, we
must take several issues into account. First, us-
ers who interact with these systems are language
learners, who can be expected occasionally to
produce invalid speech, and who may benefit
from the corrective signal of recognizer rejec-
tion. Second, word recognition is one step in a
social simulation pipeline that allows virtual hu-
mans to respond to learner input (Samtani,
Valente et al. 2008). Consequently, the system
goals extend beyond word-level decoding into
meaning interpretation and response planning.
As a result, Word Error Rate (WER) and re-
lated metrics, such as those described by Hunt
(1990) for evaluating ASR performance, are in-
sufficient to characterize how well the speech
understanding component of the dialog system
performs. We need a meaningful way to account
for the performance of the dialog system as a
whole, which can distinguish acceptable interpre-
tation failures from unacceptable ones.
We present a validation process for assessing
speech understanding in dialog systems for lan-
guage training applications. The process in-
volves annotation of historical user data acquired
from learner interaction with the Tactical Lan-
guage and Culture Training System (Johnson and
Valente 2009). The results indicate that learner
mistakes make up the majority of non-
recognitions, confirming the hypothesis that
“recognition failures” are a complex category of
events that are only partly explained by lack of
coverage in speech understanding components
such as ASR.
</bodyText>
<sectionHeader confidence="0.98613" genericHeader="method">
2 Metrics for Dialog System Assessment
</sectionHeader>
<bodyText confidence="0.9996428">
Speech recognition errors in the dialog system
result in at least two sub-types of error: non-
understandings, where the system cannot find an
interpretation for user input, and misunderstand-
ings, where the system finds an interpretation
that does not match the learner’s intent (McRoy
and Hirst 1995).
These classes generalize beyond speech rec-
ognition to speech understanding. This is shown
in Figure 1, where &amp;quot;act&amp;quot; refers to a message
</bodyText>
<subsubsectionHeader confidence="0.677338">
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 241–244,
</subsubsectionHeader>
<affiliation confidence="0.890867">
The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics
</affiliation>
<page confidence="0.999398">
241
</page>
<bodyText confidence="0.999770666666667">
modeled along the lines of Traum &amp; Hinkleman
(1992). In the context of speech-enabled dialog
systems, the understanding task is more critical,
since it more closely models the overall success
of the communication between the human user
and the virtual human interlocutor.
</bodyText>
<figureCaption confidence="0.979598">
Figure 1. Speech understanding pipeline.
</figureCaption>
<bodyText confidence="0.999991933333334">
As a result, a variety of metrics have been sug-
gested that assess performance at the level of
intent recognition, rather than word recognition.
Examples include PARADISE (Walker, Litman
et al. 1998) and the work of Suendermann, Lis-
combe, et al (2009).
We propose an assessment procedure that uses
expert annotation to compare speaker-intended
acts to the acts recognized by the speech-
understanding component of the dialog system.
Like the metrics mentioned above, it evaluates
the system&apos;s ability to recognize intent as well as
words. However we focus our attention on adap-
tations that characterize interactions with lan-
guage learners, who are a special type of user.
As a result, we can distinguish system non-
understandings and mis-understandings that are
due to system error from those that are caused by
learner mistakes.
Our goal is to use this information to reduce
mis-understandings due to system errors; such
mis-understandings can yield confusing dialog
behavior, causing learners to lose confidence in
the accuracy of the speech recognizer. Non-
understandings may be less serious, since they
occur in real life between learners and native
speakers. Non-understandings due to learner er-
ror may be beneficial if the additional practice
that results from non-understandings leads to an
increase in language accuracy.
</bodyText>
<sectionHeader confidence="0.995896" genericHeader="method">
3 Procedure
</sectionHeader>
<bodyText confidence="0.999859">
To assess performance, we recruited two annota-
tors to provide judgments on historical log data
regarding the accuracy of the system interpreta-
tions at multiple levels, including word-level
recognition and act recognition.
</bodyText>
<subsectionHeader confidence="0.999914">
3.1 Annotation team and data collection
</subsectionHeader>
<bodyText confidence="0.999952588235294">
The annotators are Alelo team members with
expertise in General Linguistics, French and
Spanish Linguistics, Translation, and Teaching
English as a Foreign Language (TEFL). Their
combined experience in content authoring for
Alelo courses covers more than 10 languages.
The data was collected in the fall of 2009 as
part of a field test for Alelo courses teaching Ira-
qi Arabic and Sub-Saharan French. Naval per-
sonnel at several sites around the United States
volunteered to complete the courses in self-
study. The training systems generated user logs,
capturing recordings of learner turns and system
recognition results for each turn. From these
logs, samples of beginner-level and intermediate-
level dialogs were selected and anonymized for
annotation.
</bodyText>
<subsectionHeader confidence="0.999321">
3.2 Speech understanding accuracy
</subsectionHeader>
<bodyText confidence="0.999955">
The point of this exercise is to explore how of-
ten the system fails to understand what a learner
is trying to say during spoken dialog.
Annotation was performed on a total of 345
learner turns. To determine the act-level accura-
cy of the speech understanding system, annota-
tors listened to the recording of each turn and
selected the act they heard from a drop-down list.
The results were compared with the system-
perceived act result recovered from the log.
Speech understanding rejections, where the sys-
tem determined that no meaningful act could be
perceived from the learner turn, were labeled
with the act name &amp;quot;garbage&amp;quot;. Human annotators
could also select the garbage act for recordings
where no meaningful interpretation could be
made.
</bodyText>
<sectionHeader confidence="0.999965" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.990849727272727">
To analyze the results, we measure system ac-
curacy at two levels. First, we determine accura-
cy on distinguishing meaningful utterances (ut-
terances that annotators labeled with an act) from
non-meaningful speech attempts (labeled as gar-
bage by annotators). The results are shown in
Table 1. Inter-annotator agreement as measured
by Cohen&apos;s Kappa on the first task is 0.8, indicat-
ing good agreement between our two experts.
Next, we examine the utterances classified as
meaningful by both the system and the annota-
</bodyText>
<page confidence="0.986785">
242
</page>
<bodyText confidence="0.9999415">
tors, to assess correctness at a finer level of gra-
nularity: given that the system identified the ut-
terance as meaningful, did the meaning that it
assigned match our annotators’ judgments? If
not, mis-understandings occur. These results are
shown in Table 2. System mis-understandings
over all meaningful utterances. Inter-annotator
agreement on the non-understanding classifica-
tion task was 0.73, suggesting that there is sub-
stantial agreement between our raters.
</bodyText>
<subsectionHeader confidence="0.97085">
4.1 Correct interpretations
</subsectionHeader>
<bodyText confidence="0.999986533333333">
Numbers in the bottom-right cells of Table 1
and the first row of Table 2 represent correct sys-
tem interpretations, according to an annotator. In
these instances, the annotator assigned an act to
the turn that matched the system interpretation
for that turn (in Table 2), or both the annotator
and the system assigned the label &amp;quot;garbage&amp;quot; (in
Table 1). On average these examples account for
62% of the total turns.
An important result from this procedure is that
it reveals the class of appropriate rejections by
the speech understanding component. These
&amp;quot;garbage-in, garbage-out&amp;quot; instances are instruc-
tive cases where the system indicates to the
learner that he or she should re-try the utterance.
</bodyText>
<subsectionHeader confidence="0.996128">
4.2 Mis-understandings
</subsectionHeader>
<bodyText confidence="0.999564583333333">
In Table 2, the row labeled &amp;quot;Incorrect&amp;quot; con-
tains mis-understandings, where the system made
an interpretation but failed to match the expert
annotation. Mis-understandings account for
around 3.5% of the turns in our data set, on aver-
age. The low rate of mis-understandings is an
encouraging result for the overall quality of the
understanding component. Prior to the introduc-
tion of the garbage model into the speech recog-
nizer the mis-understanding rate had been rela-
tively high, and these results indicate a signifi-
cant improvement.
</bodyText>
<table confidence="0.993642">
Annotator 1
Act Garbage
System Act 175 3
Garbage 94 73
Annotator 2
Act Garbage
System Act 176 2
Garbage 134 33
</table>
<tableCaption confidence="0.996601666666667">
Table 1. Distinguishing meaningful utterances
(corresponding to an Act) from non-meaningful
attempts (Garbage).
</tableCaption>
<table confidence="0.999673666666667">
System Annotator 1 Annotator 2
Correct 167 160
Incorrect 8 16
</table>
<tableCaption confidence="0.9970205">
Table 2. System mis-understandings over all mea-
ningful utterances.
</tableCaption>
<subsectionHeader confidence="0.980972">
4.3 Non-understandings
</subsectionHeader>
<bodyText confidence="0.999955125">
Instances from the data set where the annota-
tor was able to interpret an act, but the system
returned &amp;quot;garbage,&amp;quot; are shown in the lower-left
cells of Table 1. These are system non-
understandings, since the speech understanding
component was not able to map the learner input
to a meaningful act, even though the annotators
were. Non-understandings account for 33% of
turns in our data set, on average.
To understand the impact of these non-
understandings on dialog system quality, we
must consider the specialized case of language
learners. Several components of the speech un-
derstanding pipeline are tuned with language
learners in mind. For example, acoustic models
used in the automatic speech recognizer are
trained on a mixture of native and non-native
data. The goal is for the system to be as tolerant
as possible of pronunciation variability, while
still catching learner mistakes.
We expect learner speech attempts to occur
on a continuum, ranging from fully correct to
minor mistakes to unrecoverable errors. In the
first procedure, the annotators were instructed to
label a recording with a meaningful act in all
cases where they could do so, using garbage only
for unintelligible attempts. As a result, we con-
sciously placed the annotator tolerance at the far
end of this spectrum.
Since the system is less forgiving, we hypo-
thesize that the non-understandings we found
mask two different sub-classes: instances where
the system truly failed to interpret a well-formed
utterance, and instances where the system was
(perhaps appropriately) rejecting a learner mis-
take: an intelligible but malformed utterance.
In a follow-up procedure, the annotators revi-
sited instances labeled as non-understandings. In
this second round, they distinguished instances
where the learner successfully performed an act
that was simply outside the coverage of the
speech understanding system from instances
where they perceived a learner error, either in
pronunciation or grammar. The results are sum-
marized in Table 3.
We found that most of the cases of non-
recognition were actually due to learner error,
rather than system error.
</bodyText>
<page confidence="0.996753">
243
</page>
<table confidence="0.999814833333333">
Annotator 1
Error Type Count
Learner Grammar 0
Learner Pronunciation 58 (62%)
System Error 36
Total 94
Annotator 2
Error Type Count x
Learner Grammar 2 0
Learner Pronunciation 85 (63%) 0.65
System Error 47 0.65
Total 134 0.73
</table>
<tableCaption confidence="0.986752">
Table 3. Classification of non-understandings.
Inter-annotator agreement (x) is substantial
over all classes.
</tableCaption>
<sectionHeader confidence="0.996615" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999971117647059">
By applying a method for assessment that goes
beyond word recognition rate, we have produced
an analysis of the speech understanding compo-
nents in a dialog system for language learners.
Expert annotators found that most system-
understood speech attempts were interpreted cor-
rectly, with mis-understandings occurring only
3% of the time. While non-understandings oc-
curred much more frequently, a follow-up exer-
cise showed that learner pronunciation error was
the most frequent cause; these cases are legiti-
mate candidates for system rejection, leaving
12% of all instances as non-understandings
where the system was at fault. These instances
represent the most beneficial errors to correct
when making refinements to the speech under-
standing module.
In this exercise, one could interpret the hu-
man-assigned acts as a model of recognition by
an extremely sympathetic hearer. Although this
model may be too lenient to provide learners
with realistic communication practice, it could be
useful for the dialog engine to recognize some
poorly-formed utterances, for the purpose of
providing feedback. For example, a learner who
repeatedly attempts the same utterance with un-
acceptable but intelligible pronunciation could
trigger a tutoring-style intervention (“Are you
trying to say bonjour? Try it more like this...”).
The assessment methods and analysis pre-
sented in this paper are a first step toward this
type of system improvement, one that meets the
needs of language learners as a unique type of
dialog-system user.
</bodyText>
<sectionHeader confidence="0.994554" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999922">
The authors thank Rebecca Row and Mickey
Rosenberg for their contributions to the experi-
ments described here, and three anonymous re-
viewers for comments that improved the clarity
of the paper. This work was sponsored by PM
TRASYS, Voice of America, the Office of Naval
Research, and DARPA. Opinions expressed here
are those of the author and not of the sponsors or
the US Government.
</bodyText>
<sectionHeader confidence="0.999174" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999944243243243">
Barrett, K. A. and W. L. Johnson (2010). Developing
serious games for learning language-in-culture. Inter-
disciplinary Models and Tools for Serious Games:
Emerging Concepts and Future Directions. R. V. Eck.
Hershey, PA, IGI Global.
Hunt, M. J. (1990). &amp;quot;Figures of Merit for Assessing
Connected Word Recognisers.&amp;quot; Speech Communica-
tion 9: 239-336.
Johnson, W. L., J. Rickel, et al. (2000). &amp;quot;Animated
Pedagogical Agents: Face-to-Face Interaction in In-
teractive Learning Environments.&amp;quot; Journal of Artifi-
cial Intelligence in Education 11: 47--78.
Johnson, W. L. and A. Valente (2009). &amp;quot;Tactical Lan-
guage and Culture Training Systems: Using AI to
Teach Foreign Languages and Cultures.&amp;quot; AI Maga-
zine 30(2).
McRoy, S. W. and G. Hirst (1995). &amp;quot;The repair of
speech act misunderstandings by abductive infe-
rence.&amp;quot; Computational Linguistics 21(4): 435--478.
Sagae, A., B. Wetzel, et al. (2009). Culture-Driven
Response Strategies for Virtual Human Behavior in
Training Systems. SLaTE-2009, Warwickshire, Eng-
land.
Samtani, P., A. Valente, et al. (2008). Applying the
SAIBA framework to the Tactical Language and Cul-
ture Training System. AAMAS 2008 Workshop on
Functional Markup Language (FML).
Suendermann, D., J. Liscombe, et al. (2009). A hand-
some set of metrics to measure utterance classification
performance in spoken dialog systems. SigDial 2009.
Walker, M. A., D. J. Litman, et al. (1998). &amp;quot;Evaluat-
ing spoken dialogue agents with PARADISE: Two
case studies.&amp;quot; Computer Speech &amp; Language 12(4):
317-347.
Wang, N. and W. L. Johnson (2008). The Politeness
Effect in an Intelligent Foreign Language Tutoring
System. ITS 2008.
</reference>
<page confidence="0.998536">
244
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.974785">
<title confidence="0.999703">Validation of a Dialog System for Language Learners</title>
<author confidence="0.999371">Alicia Sagae</author>
<author confidence="0.999371">W Lewis Johnson</author>
<author confidence="0.999371">Stephen</author>
<affiliation confidence="0.999525">Alelo, Inc.</affiliation>
<address confidence="0.992846">Los Angeles, CA</address>
<email confidence="0.999429">asagae@alelo.com</email>
<email confidence="0.999429">ljohnson@alelo.com</email>
<email confidence="0.999429">sbodnar@alelo.com</email>
<abstract confidence="0.998955625">In this paper we present experiments related to the validation of spoken language understanding capabilities in a language and culture training system. In this application, word-level recognition rates are insufficient to characterize how well the system serves its users. We present the results of an annotation exercise that distinguishes instances of non-recognition due to learner error from instances due to poor system coverage. These statistics give a more accurate and interesting description of system performance, showing how the system could be improved without sacrificing the instructional value of rejecting learner utterances when they are poorly formed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K A Barrett</author>
<author>W L Johnson</author>
</authors>
<title>Developing serious games for learning language-in-culture. Interdisciplinary Models and Tools for Serious Games: Emerging Concepts and Future</title>
<date>2010</date>
<contexts>
<context position="1340" citStr="Barrett and Johnson 2010" startWordPosition="189" endWordPosition="192">tem could be improved without sacrificing the instructional value of rejecting learner utterances when they are poorly formed. 1 Introduction Conversational practice in real-time dialogs with virtual humans is a compelling element of training systems for communicative competency, helping learners acquire procedural skills in addition to declarative knowledge (Johnson, Rickel et al. 2000). Alelo&apos;s language and culture training systems allow language learners to engage in such dialogs in a serious game environment, where they practice task-based missions in new linguistic and cultural settings (Barrett and Johnson 2010). To support this capability, Alelo products apply a variety of spoken dialog technologies, including automatic speech recognition (ASR) and agent-based models of dialog that capture theories of politeness (Wang and Johnson 2008), and cultural expectations (Johnson, 2010; (Sagae, Wetzel et al. 2009). To properly assess these dialog systems, we must take several issues into account. First, users who interact with these systems are language learners, who can be expected occasionally to produce invalid speech, and who may benefit from the corrective signal of recognizer rejection. Second, word re</context>
</contexts>
<marker>Barrett, Johnson, 2010</marker>
<rawString>Barrett, K. A. and W. L. Johnson (2010). Developing serious games for learning language-in-culture. Interdisciplinary Models and Tools for Serious Games: Emerging Concepts and Future Directions. R. V. Eck. Hershey, PA, IGI Global.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Hunt</author>
</authors>
<title>Figures of Merit for Assessing Connected Word Recognisers.&amp;quot;</title>
<date>1990</date>
<journal>Speech Communication</journal>
<volume>9</volume>
<pages>239--336</pages>
<contexts>
<context position="2291" citStr="Hunt (1990)" startWordPosition="338" endWordPosition="339">s, we must take several issues into account. First, users who interact with these systems are language learners, who can be expected occasionally to produce invalid speech, and who may benefit from the corrective signal of recognizer rejection. Second, word recognition is one step in a social simulation pipeline that allows virtual humans to respond to learner input (Samtani, Valente et al. 2008). Consequently, the system goals extend beyond word-level decoding into meaning interpretation and response planning. As a result, Word Error Rate (WER) and related metrics, such as those described by Hunt (1990) for evaluating ASR performance, are insufficient to characterize how well the speech understanding component of the dialog system performs. We need a meaningful way to account for the performance of the dialog system as a whole, which can distinguish acceptable interpretation failures from unacceptable ones. We present a validation process for assessing speech understanding in dialog systems for language training applications. The process involves annotation of historical user data acquired from learner interaction with the Tactical Language and Culture Training System (Johnson and Valente 20</context>
</contexts>
<marker>Hunt, 1990</marker>
<rawString>Hunt, M. J. (1990). &amp;quot;Figures of Merit for Assessing Connected Word Recognisers.&amp;quot; Speech Communication 9: 239-336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W L Johnson</author>
<author>J Rickel</author>
</authors>
<title>Animated Pedagogical Agents: Face-to-Face Interaction in Interactive Learning Environments.&amp;quot;</title>
<date>2000</date>
<journal>Journal of Artificial Intelligence in Education</journal>
<volume>11</volume>
<pages>47--78</pages>
<marker>Johnson, Rickel, 2000</marker>
<rawString>Johnson, W. L., J. Rickel, et al. (2000). &amp;quot;Animated Pedagogical Agents: Face-to-Face Interaction in Interactive Learning Environments.&amp;quot; Journal of Artificial Intelligence in Education 11: 47--78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W L Johnson</author>
<author>A Valente</author>
</authors>
<title>Tactical Language and Culture Training Systems: Using AI to Teach Foreign Languages and Cultures.&amp;quot;</title>
<date>2009</date>
<journal>AI Magazine</journal>
<volume>30</volume>
<issue>2</issue>
<contexts>
<context position="2894" citStr="Johnson and Valente 2009" startWordPosition="426" endWordPosition="429">scribed by Hunt (1990) for evaluating ASR performance, are insufficient to characterize how well the speech understanding component of the dialog system performs. We need a meaningful way to account for the performance of the dialog system as a whole, which can distinguish acceptable interpretation failures from unacceptable ones. We present a validation process for assessing speech understanding in dialog systems for language training applications. The process involves annotation of historical user data acquired from learner interaction with the Tactical Language and Culture Training System (Johnson and Valente 2009). The results indicate that learner mistakes make up the majority of nonrecognitions, confirming the hypothesis that “recognition failures” are a complex category of events that are only partly explained by lack of coverage in speech understanding components such as ASR. 2 Metrics for Dialog System Assessment Speech recognition errors in the dialog system result in at least two sub-types of error: nonunderstandings, where the system cannot find an interpretation for user input, and misunderstandings, where the system finds an interpretation that does not match the learner’s intent (McRoy and H</context>
</contexts>
<marker>Johnson, Valente, 2009</marker>
<rawString>Johnson, W. L. and A. Valente (2009). &amp;quot;Tactical Language and Culture Training Systems: Using AI to Teach Foreign Languages and Cultures.&amp;quot; AI Magazine 30(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S W McRoy</author>
<author>G Hirst</author>
</authors>
<title>The repair of speech act misunderstandings by abductive inference.&amp;quot;</title>
<date>1995</date>
<journal>Computational Linguistics</journal>
<volume>21</volume>
<issue>4</issue>
<pages>435--478</pages>
<contexts>
<context position="3504" citStr="McRoy and Hirst 1995" startWordPosition="520" endWordPosition="523">lente 2009). The results indicate that learner mistakes make up the majority of nonrecognitions, confirming the hypothesis that “recognition failures” are a complex category of events that are only partly explained by lack of coverage in speech understanding components such as ASR. 2 Metrics for Dialog System Assessment Speech recognition errors in the dialog system result in at least two sub-types of error: nonunderstandings, where the system cannot find an interpretation for user input, and misunderstandings, where the system finds an interpretation that does not match the learner’s intent (McRoy and Hirst 1995). These classes generalize beyond speech recognition to speech understanding. This is shown in Figure 1, where &amp;quot;act&amp;quot; refers to a message Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 241–244, The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics 241 modeled along the lines of Traum &amp; Hinkleman (1992). In the context of speech-enabled dialog systems, the understanding task is more critical, since it more closely models the overall success of the communication between the human user a</context>
</contexts>
<marker>McRoy, Hirst, 1995</marker>
<rawString>McRoy, S. W. and G. Hirst (1995). &amp;quot;The repair of speech act misunderstandings by abductive inference.&amp;quot; Computational Linguistics 21(4): 435--478.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sagae</author>
<author>B Wetzel</author>
</authors>
<title>Culture-Driven Response Strategies for Virtual Human Behavior in Training Systems.</title>
<date>2009</date>
<location>SLaTE-2009, Warwickshire, England.</location>
<marker>Sagae, Wetzel, 2009</marker>
<rawString>Sagae, A., B. Wetzel, et al. (2009). Culture-Driven Response Strategies for Virtual Human Behavior in Training Systems. SLaTE-2009, Warwickshire, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Samtani</author>
<author>A Valente</author>
</authors>
<title>Applying the SAIBA framework to the Tactical Language and Culture Training System.</title>
<date>2008</date>
<booktitle>AAMAS 2008 Workshop on Functional Markup Language (FML).</booktitle>
<marker>Samtani, Valente, 2008</marker>
<rawString>Samtani, P., A. Valente, et al. (2008). Applying the SAIBA framework to the Tactical Language and Culture Training System. AAMAS 2008 Workshop on Functional Markup Language (FML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Suendermann</author>
<author>J Liscombe</author>
</authors>
<title>A handsome set of metrics to measure utterance classification performance in spoken dialog systems. SigDial</title>
<date>2009</date>
<marker>Suendermann, Liscombe, 2009</marker>
<rawString>Suendermann, D., J. Liscombe, et al. (2009). A handsome set of metrics to measure utterance classification performance in spoken dialog systems. SigDial 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Walker</author>
<author>D J Litman</author>
</authors>
<title>Evaluating spoken dialogue agents with PARADISE: Two case studies.&amp;quot;</title>
<date>1998</date>
<journal>Computer Speech &amp; Language</journal>
<volume>12</volume>
<issue>4</issue>
<pages>317--347</pages>
<marker>Walker, Litman, 1998</marker>
<rawString>Walker, M. A., D. J. Litman, et al. (1998). &amp;quot;Evaluating spoken dialogue agents with PARADISE: Two case studies.&amp;quot; Computer Speech &amp; Language 12(4): 317-347.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Wang</author>
<author>W L Johnson</author>
</authors>
<title>The Politeness Effect in an Intelligent Foreign Language Tutoring System. ITS</title>
<date>2008</date>
<contexts>
<context position="1569" citStr="Wang and Johnson 2008" startWordPosition="222" endWordPosition="225">f training systems for communicative competency, helping learners acquire procedural skills in addition to declarative knowledge (Johnson, Rickel et al. 2000). Alelo&apos;s language and culture training systems allow language learners to engage in such dialogs in a serious game environment, where they practice task-based missions in new linguistic and cultural settings (Barrett and Johnson 2010). To support this capability, Alelo products apply a variety of spoken dialog technologies, including automatic speech recognition (ASR) and agent-based models of dialog that capture theories of politeness (Wang and Johnson 2008), and cultural expectations (Johnson, 2010; (Sagae, Wetzel et al. 2009). To properly assess these dialog systems, we must take several issues into account. First, users who interact with these systems are language learners, who can be expected occasionally to produce invalid speech, and who may benefit from the corrective signal of recognizer rejection. Second, word recognition is one step in a social simulation pipeline that allows virtual humans to respond to learner input (Samtani, Valente et al. 2008). Consequently, the system goals extend beyond word-level decoding into meaning interpreta</context>
</contexts>
<marker>Wang, Johnson, 2008</marker>
<rawString>Wang, N. and W. L. Johnson (2008). The Politeness Effect in an Intelligent Foreign Language Tutoring System. ITS 2008.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>