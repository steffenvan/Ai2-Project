<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.076892">
<title confidence="0.9954045">
Text Classification from Positive and Unlabeled Data using Misclassified
Data Correction
</title>
<author confidence="0.987343">
Fumiyo Fukumoto and Yoshimi Suzuki and Suguru Matsuyoshi
</author>
<affiliation confidence="0.997208">
Interdisciplinary Graduate School of Medicine and Engineering
University of Yamanashi, Kofu, 400-8511, JAPAN
</affiliation>
<email confidence="0.997908">
{fukumoto,ysuzuki,sugurum}@yamanashi.ac.jp
</email>
<sectionHeader confidence="0.993869" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999955">
This paper addresses the problem of deal-
ing with a collection of labeled training
documents, especially annotating negative
training documents and presents a method
of text classification from positive and un-
labeled data. We applied an error detec-
tion and correction technique to the re-
sults of positive and negative documents
classified by the Support Vector Machines
(SVM). The results using Reuters docu-
ments showed that the method was compa-
rable to the current state-of-the-art biased-
SVM method as the F-score obtained by
our method was 0.627 and biased-SVM
was 0.614.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999982839285715">
Text classification using machine learning (ML)
techniques with a small number of labeled data has
become more important with the rapid increase in
volume of online documents. Quite a lot of learn-
ing techniques e.g., semi-supervised learning, self-
training, and active learning have been proposed.
Blum et al. proposed a semi-supervised learn-
ing approach called the Graph Mincut algorithm
which uses a small number of positive and nega-
tive examples and assigns values to unlabeled ex-
amples in a way that optimizes consistency in a
nearest-neighbor sense (Blum et al., 2001). Cabr-
era et al. described a method for self-training text
categorization using the Web as the corpus (Cabr-
era et al., 2009). The method extracts unlabeled
documents automatically from the Web and ap-
plies an enriched self-training for constructing the
classifier.
Several authors have attempted to improve clas-
sification accuracy using only positive and unla-
beled data (Yu et al., 2002; Ho et al., 2011). Liu
et al. proposed a method called biased-SVM that
uses soft-margin SVM as the underlying classi-
fiers (Liu et al., 2003). Elkan and Noto proposed
a theoretically justified method (Elkan and Noto,
2008). They showed that under the assumption
that the labeled documents are selected randomly
from the positive documents, a classifier trained on
positive and unlabeled documents predicts proba-
bilities that differ by only a constant factor from
the true conditional probabilities of being positive.
They reported that the results were comparable to
the current state-of-the-art biased SVM method.
The methods of Liu et al. and Elkan et al. model
a region containing most of the available positive
data. However, these methods are sensitive to the
parameter values, especially the small size of la-
beled data presents special difficulties in tuning
the parameters to produce optimal results.
In this paper, we propose a method for elimi-
nating the need for manually collecting training
documents, especially annotating negative train-
ing documents based on supervised ML tech-
niques. Our goal is to eliminate the need for manu-
ally collecting training documents, and hopefully
achieve classification accuracy from positive and
unlabeled data as high as that from labeled posi-
tive and labeled negative data. Like much previous
work on semi-supervised ML, we apply SVM to
the positive and unlabeled data, and add the classi-
fication results to the training data. The difference
is that before adding the classification results, we
applied the MisClassified data Detection and Cor-
rection (MCDC) technique to the results of SVM
learning in order to improve classification accu-
racy obtained by the final classifiers.
</bodyText>
<sectionHeader confidence="0.616296" genericHeader="method">
2 Framework of the System
</sectionHeader>
<bodyText confidence="0.9995922">
The MCDC method involves category error cor-
rection, i.e., correction of misclassified candidates,
while there are several strategies for automati-
cally detecting lexical/syntactic errors in corpora
(Abney et al., 1999; Eskin, 2000; Dickinson and
</bodyText>
<page confidence="0.986542">
474
</page>
<note confidence="0.461214">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 474–478,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<figure confidence="0.999626571428572">
training
training
training
PP1
MCDC
SVM
MCDC
SVM
N1RC2
MCDC
SVM
N1RC1
N1RC1
...
1N1
classification
CP1
selection
CN1
PCP
Final results
U\N1
N1CN
U
Training data D
Estimation of
error reduction
Extraction
of miss-
classified
candidates
SVM
SV label
䍴
NB label
classification
Error candidates
Loss function
test
NB
D \SV (Support vectors)
learning
D \Error candidates
D2
D1
Correction of
misclassified candidates
Judgment using loss values
Final results
</figure>
<figureCaption confidence="0.999995">
Figure 1: Overview of the system Figure 2: The MCDC procedure
</figureCaption>
<bodyText confidence="0.999891470588235">
Meurers., 2005; Boyd et al., 2008) or categorical
data errors (Akoglu et al., 2013). The method first
detects error candidates. As error candidates, we
focus on support vectors (SVs) extracted from the
training documents by SVM. Training by SVM is
performed to find the optimal hyperplane consist-
ing of SVs, and only the SVs affect the perfor-
mance. Thus, if some training document reduces
the overall performance of text classification be-
cause of an outlier, we can assume that the docu-
ment is a SV.
Figure 1 illustrates our system. First, we ran-
domly select documents from unlabeled data (U)
where the number of documents is equal to that of
the initial positive training documents (P1). We set
these selected documents to negative training doc-
uments (N1), and apply SVM to learn classifiers.
Next, we apply the MCDC technique to the re-
sults of SVM learning. For the result of correction
(RC1)1, we train SVM classifiers, and classify the
remaining unlabeled data (U \ N1). For the re-
sult of classification, we randomly select positive
(CP1) and negative (CN1) documents classified
by SVM and add to the SVM training data (RC1).
We re-train SVM classifiers with the training doc-
uments, and apply the MCDC. The procedure is
repeated until there are no unlabeled documents
judged to be either positive or negative. Finally,
the test data are classified using the final classi-
fiers. In the following subsections, we present the
MCDC procedure shown in Figure 2. It consists
of three steps: extraction of misclassified candi-
dates, estimation of error reduction, and correction
of misclassified candidates.
</bodyText>
<footnote confidence="0.9177165">
1The manually annotated positive examples are not cor-
rected.
</footnote>
<subsectionHeader confidence="0.997255">
2.1 Extraction of misclassified candidates
</subsectionHeader>
<bodyText confidence="0.9999799">
Let D be a set of training documents and xk E
{x1, x2, · · ·, xm} be a SV of negative or positive
documents obtained by SVM. We remove Umk=1xk
from the training documents D. The resulting
D \ Umk=1xk is used for training Naive Bayes
(NB) (McCallum, 2001), leading to a classifica-
tion model. This classification model is tested on
each xk, and assigns a positive or negative label.
If the label is different from that assigned to xk,
we declare xk an error candidate.
</bodyText>
<subsectionHeader confidence="0.999178">
2.2 Estimation of error reduction
</subsectionHeader>
<bodyText confidence="0.999961642857143">
We detect misclassified data from the extracted
candidates by estimating error reduction. The es-
timation of error reduction is often used in ac-
tive learning. The earliest work is the method of
Roy and McCallum (Roy and McCallum, 2001).
They proposed a method that directly optimizes
expected future error by log-loss or 0-1 loss, using
the entropy of the posterior class distribution on
a sample of unlabeled documents. We used their
method to detect misclassified data. Specifically,
we estimated future error rate by log-loss function.
It uses the entropy of the posterior class distribu-
tion on a sample of the unlabeled documents. A
loss function is defined by Eq (1).
</bodyText>
<equation confidence="0.999586666666667">
1 X  |X∈X ∈Y
E yE P(y |x)
�PD2∪(xk,yk)(y|x)). (1)
</equation>
<bodyText confidence="0.693186666666667">
Eq (1) denotes the expected error of the learner.
P(y  |x) denotes the true distribution of out-
put classes y E Y given inputs x. X denotes a
</bodyText>
<equation confidence="0.9958525">
EˆPD2∪(xk,yk) =
X log(
</equation>
<page confidence="0.988563">
475
</page>
<bodyText confidence="0.9998908125">
set of test documents. �PD2∪(xk,yk)(y  |x) shows
the learner’s prediction, and D2 denotes the train-
ing documents D except for the error candidates
∪lk=1xk. If the value of Eq (1) is sufficiently
small, the learner’s prediction is close to the true
output distribution.
We used bagging to reduce variance of P(y  |x)
as it is unknown for each test document x. More
precisely, from the training documents D, a dif-
ferent training set consisting of positive and nega-
tive documents is created2. The learner then cre-
ates a new classifier from the training documents.
The procedure is repeated m times3, and the final
class posterior for an instance is taken to be the un-
weighted average of the class posteriori for each of
the classifiers.
</bodyText>
<subsectionHeader confidence="0.99937">
2.3 Correction of misclassified candidates
</subsectionHeader>
<bodyText confidence="0.9988918">
For each error candidate xk, we calculated the ex-
pected error of the learner, EpD2∪(xk,yk old) and
E pD2∪(xk,yk new) by using Eq (1). Here, yk old
refers to the original label assigned to xk, and
yk new is the resulting category label estimated by
NB classifiers. If the value of the latter is smaller
than that of the former, we declare the document
xk to be misclassified, i.e., the label yk old is an
error, and its true label is yk new. Otherwise, the
label of xk is yk old.
</bodyText>
<sectionHeader confidence="0.999928" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.99845">
3.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999976733333333">
We chose the 1996 Reuters data (Reuters, 2000)
for evaluation. After eliminating unlabeled doc-
uments, we divided these into three. The data
(20,000 documents) extracted from 20 Aug to 19
Sept is used as training data indicating positive
and unlabeled documents. We set the range of S
from 0.1 to 0.9 to create a wide range of scenar-
ios, where S refers to the ratio of documents from
the positive class first selected from a fold as the
positive set. The rest of the positive and negative
documents are used as unlabeled data. We used
categories assigned to more than 100 documents
in the training data as it is necessary to examine
a wide range of S values. These categories are 88
in all. The data from 20 Sept to 19 Nov is used
</bodyText>
<footnote confidence="0.9980565">
2We set the number of negative documents extracted ran-
domly from the unlabeled documents to the same number of
positive training documents.
3We set the number of m to 100 in the experiments.
</footnote>
<bodyText confidence="0.99994135">
as a test set X, to estimate true output distribu-
tion. The remaining data consisting 607,259 from
20 Nov 1996 to 19 Aug 1997 is used as a test data
for text classification. We obtained a vocabulary
of 320,935 unique words after eliminating words
which occur only once, stemming by a part-of-
speech tagger (Schmid, 1995), and stop word re-
moval. The number of categories per documents is
3.21 on average. We used the SVM-Light package
(Joachims, 1998)4. We used a linear kernel and set
all parameters to their default values.
We compared our method, MCDC with three
baselines: (1) SVM, (2) Positive Example-Based
Learning (PEBL) proposed by (Yu et al., 2002),
and (3) biased-SVM (Liu et al., 2003). We chose
PEBL because the convergence procedure is very
similar to our framework. Biased-SVM is the
state-of-the-art SVM method, and often used for
comparison (Elkan and Noto, 2008). To make
comparisons fair, all methods were based on a lin-
ear kernel. We randomly selected 1,000 positive
and 1,000 negative documents classified by SVM
and added to the SVM training data in each itera-
tion5. For biased-SVM, we used training data and
classified test documents directly. We empirically
selected values of two parameters, “c” (trade-off
between training error and margin) and “j”, i.e.,
cost (cost-factor, by which training errors on posi-
tive examples) that optimized the F-score obtained
by classification of test documents.
The positive training data in SVM are assigned
to the target category. The negative training data
are the remaining data except for the documents
that were assigned to the target category, i.e., this
is the ideal method as we used all the training data
with positive/negative labeled documents. The
number of positive training data in other three
methods depends on the value of S, and the rest
of the positive and negative documents were used
as unlabeled data.
</bodyText>
<subsectionHeader confidence="0.999123">
3.2 Text classification
</subsectionHeader>
<bodyText confidence="0.999968285714286">
Classification results for 88 categories are shown
in Figure 3. Figure 3 shows micro-averaged F-
score against the S value. As expected, the re-
sults obtained by SVM were the best among all
S values. However, this is the ideal method
that requires 20,000 documents labeled posi-
tive/negative, while other methods including our
</bodyText>
<footnote confidence="0.999889">
4http://svmlight.joachims.org
5We set the number of documents up to 1,000.
</footnote>
<page confidence="0.99431">
476
</page>
<table confidence="0.974098153846154">
Level (# of Cat) SVM PEBL Biased-SVM MCDC
Cat F Cat F (Iter) Cat F (Iter) Cat F (Iter)
Top (22) Best GSPO .955 GSPO .802 (26) CCAT .939 GSPO .946 (9)
Worst GODD .099 GODD .079 (6) GODD .038 GODD .104 (4)
Avg .800 .475 (19) .593 .619 (8)
Second (32) Best M14 .870 E71 .848 (7) M14 .869 M14 .875 (9)
Worst C16 .297 E14 .161 (14) C16 .148 C16 .150 (3)
Avg .667 .383 (22) .588 .593 (7)
Third (33) Best M141 .878 C174 .792 (27) M141 .887 M141 .885 (8)
Worst G152 .102 C331 .179 (16) G155 .130 C331 .142 (6)
Avg .717 .313 (18) .518 .557 (8)
Fourth (1) – C1511 .738 C1511 .481 (16) C1511 .737 C1511 .719 (4)
Micro Avg F-score .718 .428 (19) .614 .627 (8)
</table>
<tableCaption confidence="0.999574">
Table 1: Classification performance (S = 0.7)
</tableCaption>
<figure confidence="0.8904445">
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Delta Value
</figure>
<figureCaption confidence="0.999296">
Figure 3: F-score against the value of S
</figureCaption>
<bodyText confidence="0.999663136363636">
method used only positive and unlabeled docu-
ments. Overall performance obtained by MCDC
was better for those obtained by PEBL and biased-
SVM methods in all S values, especially when the
positive set was small, e.g., S = 0.3, the improve-
ment of MCDC over biased-SVM and PEBL was
significant.
Table 1 shows the results obtained by each
method with a S value of 0.7. “Level” indi-
cates each level of the hierarchy and the numbers
in parentheses refer to the number of categories.
“Best” and “Worst” refer to the best and the low-
est F-scores in each level of a hierarchy, respec-
tively. “Iter” in PEBL indicates the number of it-
erations until the number of negative documents
is zero in the convergence procedure. Similarly,
“Iter” in the MCDC indicates the number of it-
erations until no unlabeled documents are judged
to be either positive or negative. As can be seen
clearly from Table 1, the results with MCDC were
better than those obtained by PEBL in each level
of the hierarchy. Similarly, the results were bet-
</bodyText>
<table confidence="0.99931725">
δ SV Ec Err Correct
Prec Rec F
0.3 227,547 54,943 79,329 .693 .649 .670
0.7 141,087 34,944 42,385 .712 .673 .692
</table>
<tableCaption confidence="0.999099">
Table 2: Miss-classified data correction results
</tableCaption>
<bodyText confidence="0.999377857142857">
ter than those of biased-SVM except for the fourth
level, “C1511”(Annual results). The average num-
bers of iterations with MCDC and PEBL were 8
and 19 times, respectively. In biased-SVM, it is
necessary to run SVM many times, as we searched
“c” and “j”. In contrast, MCDC does not require
such parameter tuning.
</bodyText>
<subsectionHeader confidence="0.999709">
3.3 Correction of misclassified candidates
</subsectionHeader>
<bodyText confidence="0.999990043478261">
Our goal is to achieve classification accuracy from
only positive documents and unlabeled data as
high as that from labeled positive and negative
data. We thus applied a miss-classified data de-
tection and correction technique for the classifica-
tion results obtained by SVM. Therefore, it is im-
portant to examine the accuracy of miss-classified
correction. Table 2 shows detection and correction
performance against all categories. “SV” shows
the total number of SVs in 88 categories in all iter-
ations. “Ec” refers to the total number of extracted
error candidates. “Err” denotes the number of doc-
uments classified incorrectly by SVM and added
to the training data, i.e., the number of documents
that should be assigned correctly by the correction
procedure. “Prec” and “Rec” show the precision
and recall of correction, respectively.
Table 2 shows that precision was better than re-
call with both S values, as the precision obtained
by -y value = 0.3 and 0.7 were 4.4% and 3.9%
improvement against recall values, respectively.
These observations indicated that the error candi-
dates extracted by our method were appropriately
</bodyText>
<figure confidence="0.998007166666667">
F-score
0.8
0.7
0.6
0.5
0.4
0.3
0.2
SVM
PEBL
Biased-SVM
MCDC
</figure>
<page confidence="0.995104">
477
</page>
<bodyText confidence="0.999847571428571">
corrected. In contrast, there were still other doc-
uments that were miss-classified but not extracted
as error candidates. We extracted error candidates
using the results of SVM and NB classifiers. En-
semble of other techniques such as boosting and
kNN for further efficacy gains seems promising to
try with our method.
</bodyText>
<sectionHeader confidence="0.999267" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999884538461538">
The research described in this paper involved text
classification using positive and unlabeled data.
Miss-classified data detection and correction tech-
nique was incorporated in the existing classifica-
tion technique. The results using the 1996 Reuters
corpora showed that the method was comparable
to the current state-of-the-art biased-SVM method
as the F-score obtained by our method was 0.627
and biased-SVM was 0.614. Future work will in-
clude feature reduction and investigation of other
classification algorithms to obtain further advan-
tages in efficiency and efficacy in manipulating
real-world large corpora.
</bodyText>
<sectionHeader confidence="0.998172" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999882949152543">
S. Abney, R. E. Schapire, and Y. Singer. 1999. Boost-
ing Applied to Tagging and PP Attachment. In Proc.
of the Joint SIGDAT Conference on EMNLP and
Very Large Corpora, pages 38–45.
L. Akoglu, H. Tong, J. Vreeken, and C. Faloutsos.
2013. Fast and Reliable Anomaly Detection in Cate-
gorical Data. In Proc. of the CIKM, pages 415–424.
A. Blum, J. Lafferty, M. Rwebangira, and R. Reddy.
2001. Learning from Labeled and Unlabeled Data
using Graph Mincuts. In Proc. of the 18th ICML,
pages 19–26.
A. Boyd, M. Dickinson, and D. Meurers. 2008. On
Detecting Errors in Dependency Treebanks. Re-
search on Language and Computation, 6(2):113–
137.
R. G. Cabrera, M. M. Gomez, P. Rosso, and L. V.
Pineda. 2009. Using the Web as Corpus for
Self-Training Text Categorization. Information Re-
trieval, 12(3):400–415.
M. Dickinson and W. D. Meurers. 2005. Detecting
Errors in Discontinuous Structural Annotation. In
Proc. of the ACL’05, pages 322–329.
C. Elkan and K. Noto. 2008. Learning Classifiers from
Only Positive and Unlabeled Data. In Proc. of the
KDD’08, pages 213–220.
E. Eskin. 2000. Detectiong Errors within a Corpus us-
ing Anomaly Detection. In Proc. of the 6th ANLP
Conference and the 1st Meeting of the NAACL,
pages 148–153.
C. H. Ho, M. H. Tsai, and C. J. Lin. 2011. Active
Learning and Experimental Design with SVMs. In
Proc. of the JMLR Workshop on Active Learning and
Experimental Design, pages 71–84.
T. Joachims. 1998. SVM Light Support Vector Ma-
chine. In Dept. of Computer Science Cornell Uni-
versity.
B. Liu, Y. Dai, X. Li, W. S. Lee, and P. S. Yu. 2003.
Building Text Classifiers using Positive and Unla-
beled Examples. In Proc. of the ICDM’03, pages
179–188.
A. K. McCallum. 2001. Multi-label Text Classifica-
tion with a Mixture Model Trained by EM. In Re-
vised Version of Paper Appearing in AAAI’99 Work-
shop on Text Learning, pages 135–168.
Reuters. 2000. Reuters Corpus Volume1 English Lan-
guage. 1996-08-20 to 1997-08-19 Release Date
2000-11-03 Format Version 1.
N. Roy and A. K. McCallum. 2001. Toward Optimal
Active Learning through Sampling Estimation ofEr-
ror Reduction. In Proc. of the 18th ICML, pages
441–448.
H. Schmid. 1995. Improvements in Part-of-Speech
Tagging with an Application to German. In Proc. of
the EACL SIGDAT Workshop, pages 47–50.
H. Yu, H. Han, and K. C-C. Chang. 2002. PEBL: Pos-
itive Example based Learning for Web Page Classi-
fication using SVM. In Proc. of the ACM Special
Interest Group on Knowledge Discovery and Data
Mining, pages 239–248.
</reference>
<page confidence="0.998019">
478
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.304680">
<title confidence="0.998638">Text Classification from Positive and Unlabeled Data using Data Correction</title>
<author confidence="0.998869">Fukumoto Suzuki</author>
<affiliation confidence="0.686906">Interdisciplinary Graduate School of Medicine and University of Yamanashi, Kofu, 400-8511,</affiliation>
<abstract confidence="0.9834211875">This paper addresses the problem of dealing with a collection of labeled training documents, especially annotating negative training documents and presents a method of text classification from positive and unlabeled data. We applied an error detection and correction technique to the results of positive and negative documents classified by the Support Vector Machines (SVM). The results using Reuters documents showed that the method was comparable to the current state-of-the-art biased- SVM method as the F-score obtained by our method was 0.627 and biased-SVM was 0.614.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
<author>R E Schapire</author>
<author>Y Singer</author>
</authors>
<title>Boosting Applied to Tagging and PP Attachment.</title>
<date>1999</date>
<booktitle>In Proc. of the Joint SIGDAT Conference on EMNLP and Very Large Corpora,</booktitle>
<pages>38--45</pages>
<contexts>
<context position="3817" citStr="Abney et al., 1999" startWordPosition="580" endWordPosition="583">emi-supervised ML, we apply SVM to the positive and unlabeled data, and add the classification results to the training data. The difference is that before adding the classification results, we applied the MisClassified data Detection and Correction (MCDC) technique to the results of SVM learning in order to improve classification accuracy obtained by the final classifiers. 2 Framework of the System The MCDC method involves category error correction, i.e., correction of misclassified candidates, while there are several strategies for automatically detecting lexical/syntactic errors in corpora (Abney et al., 1999; Eskin, 2000; Dickinson and 474 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 474–478, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics training training training PP1 MCDC SVM MCDC SVM N1RC2 MCDC SVM N1RC1 N1RC1 ... 1N1 classification CP1 selection CN1 PCP Final results U\N1 N1CN U Training data D Estimation of error reduction Extraction of missclassified candidates SVM SV label 䍴 NB label classification Error candidates Loss function test NB D \SV (Support vectors) learning D \Error candidates D2 D1 Correction</context>
</contexts>
<marker>Abney, Schapire, Singer, 1999</marker>
<rawString>S. Abney, R. E. Schapire, and Y. Singer. 1999. Boosting Applied to Tagging and PP Attachment. In Proc. of the Joint SIGDAT Conference on EMNLP and Very Large Corpora, pages 38–45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Akoglu</author>
<author>H Tong</author>
<author>J Vreeken</author>
<author>C Faloutsos</author>
</authors>
<title>Fast and Reliable Anomaly Detection in Categorical Data.</title>
<date>2013</date>
<booktitle>In Proc. of the CIKM,</booktitle>
<pages>415--424</pages>
<contexts>
<context position="4632" citStr="Akoglu et al., 2013" startWordPosition="704" endWordPosition="707">omputational Linguistics training training training PP1 MCDC SVM MCDC SVM N1RC2 MCDC SVM N1RC1 N1RC1 ... 1N1 classification CP1 selection CN1 PCP Final results U\N1 N1CN U Training data D Estimation of error reduction Extraction of missclassified candidates SVM SV label 䍴 NB label classification Error candidates Loss function test NB D \SV (Support vectors) learning D \Error candidates D2 D1 Correction of misclassified candidates Judgment using loss values Final results Figure 1: Overview of the system Figure 2: The MCDC procedure Meurers., 2005; Boyd et al., 2008) or categorical data errors (Akoglu et al., 2013). The method first detects error candidates. As error candidates, we focus on support vectors (SVs) extracted from the training documents by SVM. Training by SVM is performed to find the optimal hyperplane consisting of SVs, and only the SVs affect the performance. Thus, if some training document reduces the overall performance of text classification because of an outlier, we can assume that the document is a SV. Figure 1 illustrates our system. First, we randomly select documents from unlabeled data (U) where the number of documents is equal to that of the initial positive training documents </context>
</contexts>
<marker>Akoglu, Tong, Vreeken, Faloutsos, 2013</marker>
<rawString>L. Akoglu, H. Tong, J. Vreeken, and C. Faloutsos. 2013. Fast and Reliable Anomaly Detection in Categorical Data. In Proc. of the CIKM, pages 415–424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Blum</author>
<author>J Lafferty</author>
<author>M Rwebangira</author>
<author>R Reddy</author>
</authors>
<title>Learning from Labeled and Unlabeled Data using Graph Mincuts.</title>
<date>2001</date>
<booktitle>In Proc. of the 18th ICML,</booktitle>
<pages>pages</pages>
<contexts>
<context position="1467" citStr="Blum et al., 2001" startWordPosition="214" endWordPosition="217">.627 and biased-SVM was 0.614. 1 Introduction Text classification using machine learning (ML) techniques with a small number of labeled data has become more important with the rapid increase in volume of online documents. Quite a lot of learning techniques e.g., semi-supervised learning, selftraining, and active learning have been proposed. Blum et al. proposed a semi-supervised learning approach called the Graph Mincut algorithm which uses a small number of positive and negative examples and assigns values to unlabeled examples in a way that optimizes consistency in a nearest-neighbor sense (Blum et al., 2001). Cabrera et al. described a method for self-training text categorization using the Web as the corpus (Cabrera et al., 2009). The method extracts unlabeled documents automatically from the Web and applies an enriched self-training for constructing the classifier. Several authors have attempted to improve classification accuracy using only positive and unlabeled data (Yu et al., 2002; Ho et al., 2011). Liu et al. proposed a method called biased-SVM that uses soft-margin SVM as the underlying classifiers (Liu et al., 2003). Elkan and Noto proposed a theoretically justified method (Elkan and Noto</context>
</contexts>
<marker>Blum, Lafferty, Rwebangira, Reddy, 2001</marker>
<rawString>A. Blum, J. Lafferty, M. Rwebangira, and R. Reddy. 2001. Learning from Labeled and Unlabeled Data using Graph Mincuts. In Proc. of the 18th ICML, pages 19–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Boyd</author>
<author>M Dickinson</author>
<author>D Meurers</author>
</authors>
<title>On Detecting Errors in Dependency Treebanks.</title>
<date>2008</date>
<journal>Research on Language and Computation,</journal>
<volume>6</volume>
<issue>2</issue>
<pages>137</pages>
<contexts>
<context position="4583" citStr="Boyd et al., 2008" startWordPosition="696" endWordPosition="699">aria, August 4-9 2013. c�2013 Association for Computational Linguistics training training training PP1 MCDC SVM MCDC SVM N1RC2 MCDC SVM N1RC1 N1RC1 ... 1N1 classification CP1 selection CN1 PCP Final results U\N1 N1CN U Training data D Estimation of error reduction Extraction of missclassified candidates SVM SV label 䍴 NB label classification Error candidates Loss function test NB D \SV (Support vectors) learning D \Error candidates D2 D1 Correction of misclassified candidates Judgment using loss values Final results Figure 1: Overview of the system Figure 2: The MCDC procedure Meurers., 2005; Boyd et al., 2008) or categorical data errors (Akoglu et al., 2013). The method first detects error candidates. As error candidates, we focus on support vectors (SVs) extracted from the training documents by SVM. Training by SVM is performed to find the optimal hyperplane consisting of SVs, and only the SVs affect the performance. Thus, if some training document reduces the overall performance of text classification because of an outlier, we can assume that the document is a SV. Figure 1 illustrates our system. First, we randomly select documents from unlabeled data (U) where the number of documents is equal to</context>
</contexts>
<marker>Boyd, Dickinson, Meurers, 2008</marker>
<rawString>A. Boyd, M. Dickinson, and D. Meurers. 2008. On Detecting Errors in Dependency Treebanks. Research on Language and Computation, 6(2):113– 137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R G Cabrera</author>
<author>M M Gomez</author>
<author>P Rosso</author>
<author>L V Pineda</author>
</authors>
<title>Using the Web as Corpus for Self-Training Text Categorization.</title>
<date>2009</date>
<journal>Information Retrieval,</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="1591" citStr="Cabrera et al., 2009" startWordPosition="235" endWordPosition="239">ber of labeled data has become more important with the rapid increase in volume of online documents. Quite a lot of learning techniques e.g., semi-supervised learning, selftraining, and active learning have been proposed. Blum et al. proposed a semi-supervised learning approach called the Graph Mincut algorithm which uses a small number of positive and negative examples and assigns values to unlabeled examples in a way that optimizes consistency in a nearest-neighbor sense (Blum et al., 2001). Cabrera et al. described a method for self-training text categorization using the Web as the corpus (Cabrera et al., 2009). The method extracts unlabeled documents automatically from the Web and applies an enriched self-training for constructing the classifier. Several authors have attempted to improve classification accuracy using only positive and unlabeled data (Yu et al., 2002; Ho et al., 2011). Liu et al. proposed a method called biased-SVM that uses soft-margin SVM as the underlying classifiers (Liu et al., 2003). Elkan and Noto proposed a theoretically justified method (Elkan and Noto, 2008). They showed that under the assumption that the labeled documents are selected randomly from the positive documents,</context>
</contexts>
<marker>Cabrera, Gomez, Rosso, Pineda, 2009</marker>
<rawString>R. G. Cabrera, M. M. Gomez, P. Rosso, and L. V. Pineda. 2009. Using the Web as Corpus for Self-Training Text Categorization. Information Retrieval, 12(3):400–415.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dickinson</author>
<author>W D Meurers</author>
</authors>
<title>Detecting Errors in Discontinuous Structural Annotation.</title>
<date>2005</date>
<booktitle>In Proc. of the ACL’05,</booktitle>
<pages>322--329</pages>
<marker>Dickinson, Meurers, 2005</marker>
<rawString>M. Dickinson and W. D. Meurers. 2005. Detecting Errors in Discontinuous Structural Annotation. In Proc. of the ACL’05, pages 322–329.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Elkan</author>
<author>K Noto</author>
</authors>
<title>Learning Classifiers from Only Positive and Unlabeled Data.</title>
<date>2008</date>
<booktitle>In Proc. of the KDD’08,</booktitle>
<pages>213--220</pages>
<contexts>
<context position="2074" citStr="Elkan and Noto, 2008" startWordPosition="312" endWordPosition="315"> et al., 2001). Cabrera et al. described a method for self-training text categorization using the Web as the corpus (Cabrera et al., 2009). The method extracts unlabeled documents automatically from the Web and applies an enriched self-training for constructing the classifier. Several authors have attempted to improve classification accuracy using only positive and unlabeled data (Yu et al., 2002; Ho et al., 2011). Liu et al. proposed a method called biased-SVM that uses soft-margin SVM as the underlying classifiers (Liu et al., 2003). Elkan and Noto proposed a theoretically justified method (Elkan and Noto, 2008). They showed that under the assumption that the labeled documents are selected randomly from the positive documents, a classifier trained on positive and unlabeled documents predicts probabilities that differ by only a constant factor from the true conditional probabilities of being positive. They reported that the results were comparable to the current state-of-the-art biased SVM method. The methods of Liu et al. and Elkan et al. model a region containing most of the available positive data. However, these methods are sensitive to the parameter values, especially the small size of labeled da</context>
<context position="10734" citStr="Elkan and Noto, 2008" startWordPosition="1753" endWordPosition="1756">r only once, stemming by a part-ofspeech tagger (Schmid, 1995), and stop word removal. The number of categories per documents is 3.21 on average. We used the SVM-Light package (Joachims, 1998)4. We used a linear kernel and set all parameters to their default values. We compared our method, MCDC with three baselines: (1) SVM, (2) Positive Example-Based Learning (PEBL) proposed by (Yu et al., 2002), and (3) biased-SVM (Liu et al., 2003). We chose PEBL because the convergence procedure is very similar to our framework. Biased-SVM is the state-of-the-art SVM method, and often used for comparison (Elkan and Noto, 2008). To make comparisons fair, all methods were based on a linear kernel. We randomly selected 1,000 positive and 1,000 negative documents classified by SVM and added to the SVM training data in each iteration5. For biased-SVM, we used training data and classified test documents directly. We empirically selected values of two parameters, “c” (trade-off between training error and margin) and “j”, i.e., cost (cost-factor, by which training errors on positive examples) that optimized the F-score obtained by classification of test documents. The positive training data in SVM are assigned to the targe</context>
</contexts>
<marker>Elkan, Noto, 2008</marker>
<rawString>C. Elkan and K. Noto. 2008. Learning Classifiers from Only Positive and Unlabeled Data. In Proc. of the KDD’08, pages 213–220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Eskin</author>
</authors>
<title>Detectiong Errors within a Corpus using Anomaly Detection.</title>
<date>2000</date>
<booktitle>In Proc. of the 6th ANLP Conference and the 1st Meeting of the NAACL,</booktitle>
<pages>148--153</pages>
<contexts>
<context position="3830" citStr="Eskin, 2000" startWordPosition="584" endWordPosition="585">e apply SVM to the positive and unlabeled data, and add the classification results to the training data. The difference is that before adding the classification results, we applied the MisClassified data Detection and Correction (MCDC) technique to the results of SVM learning in order to improve classification accuracy obtained by the final classifiers. 2 Framework of the System The MCDC method involves category error correction, i.e., correction of misclassified candidates, while there are several strategies for automatically detecting lexical/syntactic errors in corpora (Abney et al., 1999; Eskin, 2000; Dickinson and 474 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 474–478, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics training training training PP1 MCDC SVM MCDC SVM N1RC2 MCDC SVM N1RC1 N1RC1 ... 1N1 classification CP1 selection CN1 PCP Final results U\N1 N1CN U Training data D Estimation of error reduction Extraction of missclassified candidates SVM SV label 䍴 NB label classification Error candidates Loss function test NB D \SV (Support vectors) learning D \Error candidates D2 D1 Correction of misclassi</context>
</contexts>
<marker>Eskin, 2000</marker>
<rawString>E. Eskin. 2000. Detectiong Errors within a Corpus using Anomaly Detection. In Proc. of the 6th ANLP Conference and the 1st Meeting of the NAACL, pages 148–153.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C H Ho</author>
<author>M H Tsai</author>
<author>C J Lin</author>
</authors>
<title>Active Learning and Experimental Design with SVMs.</title>
<date>2011</date>
<booktitle>In Proc. of the JMLR Workshop on Active Learning and Experimental Design,</booktitle>
<pages>71--84</pages>
<contexts>
<context position="1870" citStr="Ho et al., 2011" startWordPosition="279" endWordPosition="282">d the Graph Mincut algorithm which uses a small number of positive and negative examples and assigns values to unlabeled examples in a way that optimizes consistency in a nearest-neighbor sense (Blum et al., 2001). Cabrera et al. described a method for self-training text categorization using the Web as the corpus (Cabrera et al., 2009). The method extracts unlabeled documents automatically from the Web and applies an enriched self-training for constructing the classifier. Several authors have attempted to improve classification accuracy using only positive and unlabeled data (Yu et al., 2002; Ho et al., 2011). Liu et al. proposed a method called biased-SVM that uses soft-margin SVM as the underlying classifiers (Liu et al., 2003). Elkan and Noto proposed a theoretically justified method (Elkan and Noto, 2008). They showed that under the assumption that the labeled documents are selected randomly from the positive documents, a classifier trained on positive and unlabeled documents predicts probabilities that differ by only a constant factor from the true conditional probabilities of being positive. They reported that the results were comparable to the current state-of-the-art biased SVM method. The</context>
</contexts>
<marker>Ho, Tsai, Lin, 2011</marker>
<rawString>C. H. Ho, M. H. Tsai, and C. J. Lin. 2011. Active Learning and Experimental Design with SVMs. In Proc. of the JMLR Workshop on Active Learning and Experimental Design, pages 71–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>SVM Light Support Vector Machine. In</title>
<date>1998</date>
<institution>Dept. of Computer Science Cornell University.</institution>
<contexts>
<context position="10305" citStr="Joachims, 1998" startWordPosition="1686" endWordPosition="1687">gative documents extracted randomly from the unlabeled documents to the same number of positive training documents. 3We set the number of m to 100 in the experiments. as a test set X, to estimate true output distribution. The remaining data consisting 607,259 from 20 Nov 1996 to 19 Aug 1997 is used as a test data for text classification. We obtained a vocabulary of 320,935 unique words after eliminating words which occur only once, stemming by a part-ofspeech tagger (Schmid, 1995), and stop word removal. The number of categories per documents is 3.21 on average. We used the SVM-Light package (Joachims, 1998)4. We used a linear kernel and set all parameters to their default values. We compared our method, MCDC with three baselines: (1) SVM, (2) Positive Example-Based Learning (PEBL) proposed by (Yu et al., 2002), and (3) biased-SVM (Liu et al., 2003). We chose PEBL because the convergence procedure is very similar to our framework. Biased-SVM is the state-of-the-art SVM method, and often used for comparison (Elkan and Noto, 2008). To make comparisons fair, all methods were based on a linear kernel. We randomly selected 1,000 positive and 1,000 negative documents classified by SVM and added to the </context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>T. Joachims. 1998. SVM Light Support Vector Machine. In Dept. of Computer Science Cornell University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Liu</author>
<author>Y Dai</author>
<author>X Li</author>
<author>W S Lee</author>
<author>P S Yu</author>
</authors>
<title>Building Text Classifiers using Positive and Unlabeled Examples.</title>
<date>2003</date>
<booktitle>In Proc. of the ICDM’03,</booktitle>
<pages>179--188</pages>
<contexts>
<context position="1993" citStr="Liu et al., 2003" startWordPosition="300" endWordPosition="303">xamples in a way that optimizes consistency in a nearest-neighbor sense (Blum et al., 2001). Cabrera et al. described a method for self-training text categorization using the Web as the corpus (Cabrera et al., 2009). The method extracts unlabeled documents automatically from the Web and applies an enriched self-training for constructing the classifier. Several authors have attempted to improve classification accuracy using only positive and unlabeled data (Yu et al., 2002; Ho et al., 2011). Liu et al. proposed a method called biased-SVM that uses soft-margin SVM as the underlying classifiers (Liu et al., 2003). Elkan and Noto proposed a theoretically justified method (Elkan and Noto, 2008). They showed that under the assumption that the labeled documents are selected randomly from the positive documents, a classifier trained on positive and unlabeled documents predicts probabilities that differ by only a constant factor from the true conditional probabilities of being positive. They reported that the results were comparable to the current state-of-the-art biased SVM method. The methods of Liu et al. and Elkan et al. model a region containing most of the available positive data. However, these metho</context>
<context position="10551" citStr="Liu et al., 2003" startWordPosition="1725" endWordPosition="1728">sisting 607,259 from 20 Nov 1996 to 19 Aug 1997 is used as a test data for text classification. We obtained a vocabulary of 320,935 unique words after eliminating words which occur only once, stemming by a part-ofspeech tagger (Schmid, 1995), and stop word removal. The number of categories per documents is 3.21 on average. We used the SVM-Light package (Joachims, 1998)4. We used a linear kernel and set all parameters to their default values. We compared our method, MCDC with three baselines: (1) SVM, (2) Positive Example-Based Learning (PEBL) proposed by (Yu et al., 2002), and (3) biased-SVM (Liu et al., 2003). We chose PEBL because the convergence procedure is very similar to our framework. Biased-SVM is the state-of-the-art SVM method, and often used for comparison (Elkan and Noto, 2008). To make comparisons fair, all methods were based on a linear kernel. We randomly selected 1,000 positive and 1,000 negative documents classified by SVM and added to the SVM training data in each iteration5. For biased-SVM, we used training data and classified test documents directly. We empirically selected values of two parameters, “c” (trade-off between training error and margin) and “j”, i.e., cost (cost-fact</context>
</contexts>
<marker>Liu, Dai, Li, Lee, Yu, 2003</marker>
<rawString>B. Liu, Y. Dai, X. Li, W. S. Lee, and P. S. Yu. 2003. Building Text Classifiers using Positive and Unlabeled Examples. In Proc. of the ICDM’03, pages 179–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K McCallum</author>
</authors>
<title>Multi-label Text Classification with a Mixture Model Trained by EM.</title>
<date>2001</date>
<booktitle>In Revised Version of Paper Appearing in AAAI’99 Workshop on Text Learning,</booktitle>
<pages>135--168</pages>
<contexts>
<context position="6510" citStr="McCallum, 2001" startWordPosition="1020" endWordPosition="1021">ified using the final classifiers. In the following subsections, we present the MCDC procedure shown in Figure 2. It consists of three steps: extraction of misclassified candidates, estimation of error reduction, and correction of misclassified candidates. 1The manually annotated positive examples are not corrected. 2.1 Extraction of misclassified candidates Let D be a set of training documents and xk E {x1, x2, · · ·, xm} be a SV of negative or positive documents obtained by SVM. We remove Umk=1xk from the training documents D. The resulting D \ Umk=1xk is used for training Naive Bayes (NB) (McCallum, 2001), leading to a classification model. This classification model is tested on each xk, and assigns a positive or negative label. If the label is different from that assigned to xk, we declare xk an error candidate. 2.2 Estimation of error reduction We detect misclassified data from the extracted candidates by estimating error reduction. The estimation of error reduction is often used in active learning. The earliest work is the method of Roy and McCallum (Roy and McCallum, 2001). They proposed a method that directly optimizes expected future error by log-loss or 0-1 loss, using the entropy of th</context>
</contexts>
<marker>McCallum, 2001</marker>
<rawString>A. K. McCallum. 2001. Multi-label Text Classification with a Mixture Model Trained by EM. In Revised Version of Paper Appearing in AAAI’99 Workshop on Text Learning, pages 135–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reuters</author>
</authors>
<title>Reuters Corpus Volume1 English Language. 1996-08-20 to 1997-08-19 Release Date 2000-11-03 Format Version 1.</title>
<date>2000</date>
<contexts>
<context position="8982" citStr="Reuters, 2000" startWordPosition="1449" endWordPosition="1450">sifiers. 2.3 Correction of misclassified candidates For each error candidate xk, we calculated the expected error of the learner, EpD2∪(xk,yk old) and E pD2∪(xk,yk new) by using Eq (1). Here, yk old refers to the original label assigned to xk, and yk new is the resulting category label estimated by NB classifiers. If the value of the latter is smaller than that of the former, we declare the document xk to be misclassified, i.e., the label yk old is an error, and its true label is yk new. Otherwise, the label of xk is yk old. 3 Experiments 3.1 Experimental setup We chose the 1996 Reuters data (Reuters, 2000) for evaluation. After eliminating unlabeled documents, we divided these into three. The data (20,000 documents) extracted from 20 Aug to 19 Sept is used as training data indicating positive and unlabeled documents. We set the range of S from 0.1 to 0.9 to create a wide range of scenarios, where S refers to the ratio of documents from the positive class first selected from a fold as the positive set. The rest of the positive and negative documents are used as unlabeled data. We used categories assigned to more than 100 documents in the training data as it is necessary to examine a wide range o</context>
</contexts>
<marker>Reuters, 2000</marker>
<rawString>Reuters. 2000. Reuters Corpus Volume1 English Language. 1996-08-20 to 1997-08-19 Release Date 2000-11-03 Format Version 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Roy</author>
<author>A K McCallum</author>
</authors>
<title>Toward Optimal Active Learning through Sampling Estimation ofError Reduction.</title>
<date>2001</date>
<booktitle>In Proc. of the 18th ICML,</booktitle>
<pages>441--448</pages>
<contexts>
<context position="6991" citStr="Roy and McCallum, 2001" startWordPosition="1099" endWordPosition="1102">ined by SVM. We remove Umk=1xk from the training documents D. The resulting D \ Umk=1xk is used for training Naive Bayes (NB) (McCallum, 2001), leading to a classification model. This classification model is tested on each xk, and assigns a positive or negative label. If the label is different from that assigned to xk, we declare xk an error candidate. 2.2 Estimation of error reduction We detect misclassified data from the extracted candidates by estimating error reduction. The estimation of error reduction is often used in active learning. The earliest work is the method of Roy and McCallum (Roy and McCallum, 2001). They proposed a method that directly optimizes expected future error by log-loss or 0-1 loss, using the entropy of the posterior class distribution on a sample of unlabeled documents. We used their method to detect misclassified data. Specifically, we estimated future error rate by log-loss function. It uses the entropy of the posterior class distribution on a sample of the unlabeled documents. A loss function is defined by Eq (1). 1 X |X∈X ∈Y E yE P(y |x) �PD2∪(xk,yk)(y|x)). (1) Eq (1) denotes the expected error of the learner. P(y |x) denotes the true distribution of output classes y E Y g</context>
</contexts>
<marker>Roy, McCallum, 2001</marker>
<rawString>N. Roy and A. K. McCallum. 2001. Toward Optimal Active Learning through Sampling Estimation ofError Reduction. In Proc. of the 18th ICML, pages 441–448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Improvements in Part-of-Speech Tagging with an Application to German.</title>
<date>1995</date>
<booktitle>In Proc. of the EACL SIGDAT Workshop,</booktitle>
<pages>47--50</pages>
<contexts>
<context position="10175" citStr="Schmid, 1995" startWordPosition="1664" endWordPosition="1665">amine a wide range of S values. These categories are 88 in all. The data from 20 Sept to 19 Nov is used 2We set the number of negative documents extracted randomly from the unlabeled documents to the same number of positive training documents. 3We set the number of m to 100 in the experiments. as a test set X, to estimate true output distribution. The remaining data consisting 607,259 from 20 Nov 1996 to 19 Aug 1997 is used as a test data for text classification. We obtained a vocabulary of 320,935 unique words after eliminating words which occur only once, stemming by a part-ofspeech tagger (Schmid, 1995), and stop word removal. The number of categories per documents is 3.21 on average. We used the SVM-Light package (Joachims, 1998)4. We used a linear kernel and set all parameters to their default values. We compared our method, MCDC with three baselines: (1) SVM, (2) Positive Example-Based Learning (PEBL) proposed by (Yu et al., 2002), and (3) biased-SVM (Liu et al., 2003). We chose PEBL because the convergence procedure is very similar to our framework. Biased-SVM is the state-of-the-art SVM method, and often used for comparison (Elkan and Noto, 2008). To make comparisons fair, all methods w</context>
</contexts>
<marker>Schmid, 1995</marker>
<rawString>H. Schmid. 1995. Improvements in Part-of-Speech Tagging with an Application to German. In Proc. of the EACL SIGDAT Workshop, pages 47–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yu</author>
<author>H Han</author>
<author>K C-C Chang</author>
</authors>
<title>PEBL: Positive Example based Learning for Web Page Classification using SVM.</title>
<date>2002</date>
<booktitle>In Proc. of the ACM Special Interest Group on Knowledge Discovery and Data Mining,</booktitle>
<pages>239--248</pages>
<contexts>
<context position="1852" citStr="Yu et al., 2002" startWordPosition="275" endWordPosition="278">ng approach called the Graph Mincut algorithm which uses a small number of positive and negative examples and assigns values to unlabeled examples in a way that optimizes consistency in a nearest-neighbor sense (Blum et al., 2001). Cabrera et al. described a method for self-training text categorization using the Web as the corpus (Cabrera et al., 2009). The method extracts unlabeled documents automatically from the Web and applies an enriched self-training for constructing the classifier. Several authors have attempted to improve classification accuracy using only positive and unlabeled data (Yu et al., 2002; Ho et al., 2011). Liu et al. proposed a method called biased-SVM that uses soft-margin SVM as the underlying classifiers (Liu et al., 2003). Elkan and Noto proposed a theoretically justified method (Elkan and Noto, 2008). They showed that under the assumption that the labeled documents are selected randomly from the positive documents, a classifier trained on positive and unlabeled documents predicts probabilities that differ by only a constant factor from the true conditional probabilities of being positive. They reported that the results were comparable to the current state-of-the-art bias</context>
<context position="10512" citStr="Yu et al., 2002" startWordPosition="1718" endWordPosition="1721">t distribution. The remaining data consisting 607,259 from 20 Nov 1996 to 19 Aug 1997 is used as a test data for text classification. We obtained a vocabulary of 320,935 unique words after eliminating words which occur only once, stemming by a part-ofspeech tagger (Schmid, 1995), and stop word removal. The number of categories per documents is 3.21 on average. We used the SVM-Light package (Joachims, 1998)4. We used a linear kernel and set all parameters to their default values. We compared our method, MCDC with three baselines: (1) SVM, (2) Positive Example-Based Learning (PEBL) proposed by (Yu et al., 2002), and (3) biased-SVM (Liu et al., 2003). We chose PEBL because the convergence procedure is very similar to our framework. Biased-SVM is the state-of-the-art SVM method, and often used for comparison (Elkan and Noto, 2008). To make comparisons fair, all methods were based on a linear kernel. We randomly selected 1,000 positive and 1,000 negative documents classified by SVM and added to the SVM training data in each iteration5. For biased-SVM, we used training data and classified test documents directly. We empirically selected values of two parameters, “c” (trade-off between training error and</context>
</contexts>
<marker>Yu, Han, Chang, 2002</marker>
<rawString>H. Yu, H. Han, and K. C-C. Chang. 2002. PEBL: Positive Example based Learning for Web Page Classification using SVM. In Proc. of the ACM Special Interest Group on Knowledge Discovery and Data Mining, pages 239–248.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>