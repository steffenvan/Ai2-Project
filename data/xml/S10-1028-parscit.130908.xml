<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002709">
<title confidence="0.991467">
OWNS: Cross-lingual Word Sense Disambiguation Using Weighted
Overlap Counts and Wordnet Based Similarity Measures
</title>
<author confidence="0.990083">
Lipta Mahapatra Meera Mohan
</author>
<affiliation confidence="0.980901">
Dharmsinh Desai University
</affiliation>
<address confidence="0.48339">
Nadiad, India
</address>
<email confidence="0.9179855">
lipta.mahapatra89@gmail.com
mu.mohan@gmail.com
</email>
<author confidence="0.967158">
Mitesh M. Khapra Pushpak Bhattacharyya
</author>
<affiliation confidence="0.8022305">
Indian Institute of Technology Bombay
Powai, Mumbai 400076,India
</affiliation>
<email confidence="0.951404">
miteshk@cse.iitb.ac.in
pb@cse.iitb.ac.in
</email>
<sectionHeader confidence="0.993432" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999926789473684">
We report here our work on English
French Cross-lingual Word Sense Disam-
biguation where the task is to find the
best French translation for a target English
word depending on the context in which it
is used. Our approach relies on identifying
the nearest neighbors of the test sentence
from the training data using a pairwise
similarity measure. The proposed mea-
sure finds the affinity between two sen-
tences by calculating a weighted sum of
the word overlap and the semantic over-
lap between them. The semantic overlap
is calculated using standard Wordnet Sim-
ilarity measures. Once the nearest neigh-
bors have been identified, the best trans-
lation is found by taking a majority vote
over the French translations of the nearest
neighbors.
</bodyText>
<sectionHeader confidence="0.998975" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999577586206897">
Cross Language Word Sense Disambiguation
(CL-WSD) is the problem of finding the correct
target language translation of a word given the
context in which it appears in the source language.
In many cases a full disambiguation may not be
necessary as it is common for different meanings
of a word to have the same translation. This is es-
pecially true in cases where the sense distinction
is very fine and two or more senses of a word are
closely related. For example, the two senses of
the word letter, namely, “formal document’ and
“written/printed message” have the same French
translation “lettre”. The problem is thus reduced
to distinguishing between the coarser senses of
a word and ignoring the finer sense distinctions
which is known to be a common cause of errors
in conventional WSD. CL-WSD can thus be seen
as a slightly relaxed version of the conventional
WSD problem. However, CL-WSD has its own
set of challenges as described below.
The translations learnt from a parallel corpus
may contain a lot of errors. Such errors are hard
to avoid due to the inherent noise associated with
statistical alignment models. This problem can be
overcome if good bilingual dictionaries are avail-
able between the source and target language. Eu-
roWordNet1 can be used to construct such a bilin-
gual dictionary between English and French but it
is not freely available. Instead, in this work, we
use a noisy statistical dictionary learnt from the
Europarl parallel corpus (Koehn, 2005) which is
freely downloadable.
Another challenge arises in the form of match-
ing the lexical choice of a native speaker. For ex-
ample, the word coach (as in, vehicle) may get
translated differently as autocar, autobus or bus
even when it appears in very similar contexts.
Such decisions depend on the native speaker’s in-
tuition and are very difficult for a machine to repli-
cate due to their inconsistent usage in a parallel
training corpus.
The above challenges are indeed hard to over-
come, especially in an unsupervised setting, as ev-
idenced by the lower accuracies reported by all
systems participating in the SEMEVAL Shared
Task on Cross-lingual Word Sense Disambigua-
tion (Lefever and Hoste, 2010). Our system
ranked second in the English French task (in the
out-of-five evaluation). Even though its average
performance was lower than the baseline by 3%
it performed better than the baseline for 12 out of
the 20 target nouns.
Our approach identifies the top-five translations
of a word by taking a majority vote over the trans-
lations appearing in the nearest neighbors of the
test sentence as found in the training data. We
use a pairwise similarity measure which finds the
affinity between two sentences by calculating a
</bodyText>
<footnote confidence="0.990875">
1http://www.illc.uva.nl/EuroWordNet
</footnote>
<page confidence="0.90709">
138
</page>
<bodyText confidence="0.917145181818182">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 138–141,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
weighted sum of the word overlap and the seman-
tic overlap between them. The semantic overlap is
calculated using standard Wordnet Similarity mea-
sures.
The remainder of this paper is organized as fol-
lows. In section 2 we describe related work on
WSD. In section 3 we describe our approach. In
Section 4 we present the results followed by con-
clusion in section 5.
</bodyText>
<sectionHeader confidence="0.999407" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99999146875">
Knowledge based approaches to WSD such as
Lesk’s algorithm (Lesk, 1986), Walker’s algorithm
(Walker and Amsler, 1986), Conceptual Density
(Agirre and Rigau, 1996) and Random Walk Algo-
rithm (Mihalcea, 2005) are fundamentally overlap
based algorithms which suffer from data sparsity.
While these approaches do well in cases where
there is a surface match (i.e., exact word match)
between two occurrences of the target word (say,
training and test sentence) they fail in cases where
their is a semantic match between two occurrences
of the target word even though there is no surface
match between them. The main reason for this
failure is that these approaches do not take into
account semantic generalizations (e.g., train is-
a vehicle).
On the other hand, WSD approaches which use
Wordnet based semantic similarity measures (Pat-
wardhan et al., 2003) account for such seman-
tic generalizations and can be used in conjunc-
tion with overlap based approaches. We there-
fore propose a scoring function which combines
the strength of overlap based approaches – fre-
quently co-occurring words indeed provide strong
clues – with semantic generalizations using Word-
net based similarity measures. The disambigua-
tion is then done using k-NN (Ng and Lee, 1996)
where the k nearest neighbors of the test sentence
are identified using this scoring function. Once
the nearest neighbors have been identified, the best
translation is found by taking a majority vote over
the translations of these nearest neighbors.
</bodyText>
<sectionHeader confidence="0.96538" genericHeader="method">
3 Our approach
</sectionHeader>
<bodyText confidence="0.9997456">
In this section we explain our approach for Cross
Language Word Sense Disambiguation. The main
emphasis is on disambiguation i.e. finding English
sentences from the training data which are closely
related to the test sentence.
</bodyText>
<subsectionHeader confidence="0.99988">
3.1 Motivating Examples
</subsectionHeader>
<bodyText confidence="0.997821666666667">
To explain our approach we start with two moti-
vating examples. First, consider the following oc-
currences of the word coach:
</bodyText>
<listItem confidence="0.9998208">
• S1:...carriage of passengers by coach and
bus...
• S2:...occasional services by coach and bus
and the transit operations...
• S3:...the Gloucester coach saw the game...
</listItem>
<bodyText confidence="0.999703090909091">
In the first two cases, the word coach appears
in the sense of a vehicle and in both the cases the
word bus appears in the context. Hence, the sur-
face similarity (i.e., word-overlap count) of S1 and
S2 would be higher than that of S1 and S3 and
S2 and S3. This highlights the strength of overlap
based approaches – frequently co-occurring words
can provide strong clues for identifying similar us-
age patterns of a word.
Next, consider the following two occurrences of
the word coach:
</bodyText>
<listItem confidence="0.997766">
• S1:...I boarded the last coach of the train...
• S2:...I alighted from the first coach of the
bus...
</listItem>
<bodyText confidence="0.998086">
Here, the surface similarity (i.e., word-overlap
count) of S1 and S2 is zero even though in both
the cases the word coach appears in the sense of
vehicle. This problem can be overcome by us-
ing a suitable Wordnet based similarity measure
which can uncover the hidden semantic similarity
between these two sentences by identifying that
{bus, train} and {boarded, alighted} are closely
related words.
</bodyText>
<subsectionHeader confidence="0.999607">
3.2 Scoring function
</subsectionHeader>
<bodyText confidence="0.99977925">
Based on the above motivating examples, we pro-
pose a scoring function for calculating the simi-
larity between two sentences containing the target
word. Let S1 be the test sentence containing m
words and let S2 be a training sentence containing
n words. Further, let w1i be the i-th word of S1
and let w29 be the j-th word of S2. The similarity
between S1 and S2 is then given by,
</bodyText>
<equation confidence="0.998807333333333">
Sim(S1, S2) = A * Overlap(S1, S2)
+ (1 − A) * Semantic Sim(S1, S2)
(1)
</equation>
<bodyText confidence="0.849451">
where,
</bodyText>
<page confidence="0.868841">
139
</page>
<table confidence="0.45369025">
Overlap(S1, S2) =
where, lch(w1i, w2j)
Best Sim(w1i, S2) = max
w2j∈S2
</table>
<bodyText confidence="0.997096275862069">
We used the lch measure (Leacock and Chodorow,
1998) for calculating semantic similarity of two
words. The semantic similarity between S1 and
S2 is then calculated by simply summing over the
maximum semantic similarity of each constituent
word of S1 over all words of S2. Also note that
the overlap count is weighted according to the fre-
quency of the overlapping words. This frequency
is calculated from all the sentences in the train-
ing data containing the target word. The ratio-
nal behind using a frequency-weighted sum is that
more frequently appearing co-occurring words are
better indicators of the sense of the target word
(of course, stop words and function words are not
considered). For example, the word bus appeared
very frequently with coach in the training data
and was a strong indicator of the vehicle sense
of coach. The values of Overlap(S1, S2) and
Semantic Sim(S1, S2) are appropriately nor-
malized before summing them in Equation (1). To
prevent the semantic similarity measure from in-
troducing noise by over-generalizing we chose a
very high value of A. This effectively ensured
that the Semantic Sim(S1, S2) term in Equation
(1) became active only when the Overlap(S1, S2)
measure suffered data sparsity. In other words, we
placed a higher bet on Overlap(S1, S2) than on
Semantic Sim(S1, S2) as we found the former
to be more reliable.
</bodyText>
<subsectionHeader confidence="0.999201">
3.3 Finding translations of the target word
</subsectionHeader>
<bodyText confidence="0.9998275">
We used GIZA++2 (Och and Ney, 2003), a freely
available implementation of the IBM alignment
models (Brown et al., 1993) to get word level
alignments for the sentences in the English-French
</bodyText>
<footnote confidence="0.801521">
2http://sourceforge.net/projects/giza/
</footnote>
<bodyText confidence="0.999937916666667">
portion of the Europarl corpus. Under this align-
ment, each word in the source sentence is aligned
to zero or more words in the corresponding tar-
get sentence. Once the nearest neighbors for a test
sentence are identified using the similarity score
described earlier, we use the word alignment mod-
els to find the French translation of the target word
in the top-k nearest training sentences. These
translations are then ranked according to the num-
ber of times they appear in these top-k nearest
neighbors. The top-5 most frequent translations
are then returned as the output.
</bodyText>
<sectionHeader confidence="0.999945" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999991351351351">
We report results on the English-French Cross-
Lingual Word Sense Disambiguation task. The
test data contained 50 instances for 20 polysemous
nouns, namely, coach, education, execution, fig-
ure, job, letter, match, mission, mood, paper, post,
pot, range, rest, ring, scene, side, soil, strain and
test. We first extracted the sentences containing
these words from the English-French portion of
the Europarl corpus. These sentences served as the
training data to be compared with each test sen-
tence for identifying the nearest neighbors. The
appropriate translations for the target word in the
test sentence were then identified using the ap-
proach outlined in section 3.2 and 3.3. For the
best evaluation we submitted two runs: one con-
taining only the top-1 translation and another con-
taining top-2 translations. For the oof evaluation
we submitted one run containing the top-5 trans-
lations. The system was evaluated using Precision
and Recall measures as described in the task pa-
per (Lefever and Hoste, 2010). In the oof evalua-
tion our system gave the second best performance
among all the participants. However, the average
precision was 3% lower than the baseline calcu-
lated by simply identifying the five most frequent
translations of a word according to GIZA++ word
alignments. A detailed analysis showed that in the
oof evaluation we did better than the baseline for
12 out of the 20 nouns and in the best evaluation
we did better than the baseline for 5 out of the 20
nouns. Table 1 summarizes the performance of our
system in the best evaluation and Table 2 gives the
detailed performance of our system in the oof eval-
uation. In both the evaluations our system pro-
vided a translation for every word in the test data
and hence the precision was same as recall in all
cases. We refer to our system as OWNS (Overlap
</bodyText>
<equation confidence="0.94567">
1 �m n freq(w1i) ∗ 1{w1i=w2j}
i=1 j=1
m + n
and,
Semantic Sim(S1, S2) =
Best Sim(w1i, S2)
1
m
�m
i=1
</equation>
<page confidence="0.986666">
140
</page>
<table confidence="0.870266">
and WordNet Similarity).
System Precision Recall
OWNS 16.05 16.05
Baseline 20.71 20.71
</table>
<tableCaption confidence="0.993932">
Table 1: Performance of our system in best evalu-
</tableCaption>
<table confidence="0.983544416666667">
ation
Word OWNS Baseline
(Precision) (Precision)
coach 45.11 39.04
education 82.15 80.4
execution 59.22 39.63
figure 30.56 35.67
job 43.93 40.98
letter 46.01 42.34
match 31.01 15.73
mission 55.33 97.19
mood 35.22 64.81
paper 48.93 40.95
post 36.65 41.76
pot 26.8 65.23
range 16.28 17.02
rest 39.89 38.72
ring 39.74 33.74
scene 33.89 38.7
side 37.85 36.58
soil 67.79 59.9
strain 21.13 30.02
test 64.65 61.31
Average 43.11 45.99
</table>
<tableCaption confidence="0.629257">
Table 2: Performance of our system in oof evalua-
tion
</tableCaption>
<sectionHeader confidence="0.998355" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999966294117647">
We described our system for English French
Cross-Lingual Word Sense Disambiguation which
calculates the affinity between two sentences by
combining the weighted word overlap counts with
semantic similarity measures. This similarity
score is used to find the nearest neighbors of the
test sentence from the training data. Once the
nearest neighbors have been identified, the best
translation is found by taking a majority vote over
the translations of these nearest neighbors. Our
system gave the second best performance in the
oof evaluation among all the systems that partic-
ipated in the English French Cross-Lingual Word
Sense Disambiguation task. Even though the av-
erage performance of our system was less than the
baseline by around 3%, it outperformed the base-
line system for 12 out of the 20 nouns.
</bodyText>
<sectionHeader confidence="0.989395" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998308490196078">
Eneko Agirre and German Rigau. 1996. Word sense
disambiguation using conceptual density. In In Pro-
ceedings of the 16th International Conference on
Computational Linguistics (COLING).
Peter E Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
parameter estimation. Computational Linguistics,
19:263–311.
P. Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In In Proceedings of the
MT Summit.
C. Leacock and M. Chodorow, 1998. Combining lo-
cal context and WordNet similarity for word sense
identification, pages 305–332. In C. Fellbaum (Ed.),
MIT Press.
Els Lefever and Veronique Hoste. 2010. Semeval-
2010 task 3: Cross-lingual word sense disambigua-
tion. In Proceedings of the 5th International Work-
shop on Semantic Evaluations (SemEval-2010), As-
sociation for Computational Linguistics.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: how to tell a
pine cone from an ice cream cone. In In Proceed-
ings of the 5th annual international conference on
Systems documentation.
Rada Mihalcea. 2005. Large vocabulary unsupervised
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In In Proceed-
ings of the Joint Human Language Technology and
Empirical Methods in Natural Language Processing
Conference (HLT/EMNLP), pages 411–418.
Hwee Tou Ng and Hian Beng Lee. 1996. Integrating
multiple knowledge sources to disambiguate word
senses: An exemplar-based approach. In In Pro-
ceedings of the 34th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
40–47.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted
Pedersen. 2003. Using measures of semantic re-
latedness for word sense disambiguation. In In pro-
ceedings of the Fourth International Conference on
Intelligent Text Processing and Computation Lin-
guistics (CICLing.
D. Walker and R. Amsler. 1986. The use of machine
readable dictionaries in sublanguage analysis. In In
Analyzing Language in Restricted Domains, Grish-
man and Kittredge (eds), LEA Press, pages 69–83.
</reference>
<page confidence="0.998256">
141
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.285421">
<title confidence="0.9990175">Cross-lingual Word Sense Disambiguation Using Weighted Overlap Counts and Wordnet Based Similarity Measures</title>
<author confidence="0.998806">Lipta Mahapatra Meera Mohan</author>
<affiliation confidence="0.999225">Dharmsinh Desai University</affiliation>
<address confidence="0.91004">Nadiad, India</address>
<email confidence="0.999196">lipta.mahapatra89@gmail.commu.mohan@gmail.com</email>
<author confidence="0.99577">Mitesh M Khapra Pushpak Bhattacharyya</author>
<affiliation confidence="0.999109">Indian Institute of Technology Bombay</affiliation>
<address confidence="0.952332">Powai, Mumbai 400076,India</address>
<email confidence="0.5149905">miteshk@cse.iitb.ac.inpb@cse.iitb.ac.in</email>
<abstract confidence="0.99790365">We report here our work on English French Cross-lingual Word Sense Disambiguation where the task is to find the best French translation for a target English word depending on the context in which it is used. Our approach relies on identifying the nearest neighbors of the test sentence from the training data using a pairwise similarity measure. The proposed measure finds the affinity between two sentences by calculating a weighted sum of the word overlap and the semantic overlap between them. The semantic overlap is calculated using standard Wordnet Similarity measures. Once the nearest neighbors have been identified, the best translation is found by taking a majority vote over the French translations of the nearest neighbors.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>German Rigau</author>
</authors>
<title>Word sense disambiguation using conceptual density. In</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics (COLING).</booktitle>
<contexts>
<context position="4592" citStr="Agirre and Rigau, 1996" startWordPosition="728" endWordPosition="731"> Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics weighted sum of the word overlap and the semantic overlap between them. The semantic overlap is calculated using standard Wordnet Similarity measures. The remainder of this paper is organized as follows. In section 2 we describe related work on WSD. In section 3 we describe our approach. In Section 4 we present the results followed by conclusion in section 5. 2 Related Work Knowledge based approaches to WSD such as Lesk’s algorithm (Lesk, 1986), Walker’s algorithm (Walker and Amsler, 1986), Conceptual Density (Agirre and Rigau, 1996) and Random Walk Algorithm (Mihalcea, 2005) are fundamentally overlap based algorithms which suffer from data sparsity. While these approaches do well in cases where there is a surface match (i.e., exact word match) between two occurrences of the target word (say, training and test sentence) they fail in cases where their is a semantic match between two occurrences of the target word even though there is no surface match between them. The main reason for this failure is that these approaches do not take into account semantic generalizations (e.g., train isa vehicle). On the other hand, WSD app</context>
</contexts>
<marker>Agirre, Rigau, 1996</marker>
<rawString>Eneko Agirre and German Rigau. 1996. Word sense disambiguation using conceptual density. In In Proceedings of the 16th International Conference on Computational Linguistics (COLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter E Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="9542" citStr="Brown et al., 1993" startWordPosition="1558" endWordPosition="1561">tely normalized before summing them in Equation (1). To prevent the semantic similarity measure from introducing noise by over-generalizing we chose a very high value of A. This effectively ensured that the Semantic Sim(S1, S2) term in Equation (1) became active only when the Overlap(S1, S2) measure suffered data sparsity. In other words, we placed a higher bet on Overlap(S1, S2) than on Semantic Sim(S1, S2) as we found the former to be more reliable. 3.3 Finding translations of the target word We used GIZA++2 (Och and Ney, 2003), a freely available implementation of the IBM alignment models (Brown et al., 1993) to get word level alignments for the sentences in the English-French 2http://sourceforge.net/projects/giza/ portion of the Europarl corpus. Under this alignment, each word in the source sentence is aligned to zero or more words in the corresponding target sentence. Once the nearest neighbors for a test sentence are identified using the similarity score described earlier, we use the word alignment models to find the French translation of the target word in the top-k nearest training sentences. These translations are then ranked according to the number of times they appear in these top-k neares</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter E Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19:263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation. In</title>
<date>2005</date>
<booktitle>In Proceedings of the MT Summit.</booktitle>
<contexts>
<context position="2611" citStr="Koehn, 2005" startWordPosition="411" endWordPosition="412"> WSD problem. However, CL-WSD has its own set of challenges as described below. The translations learnt from a parallel corpus may contain a lot of errors. Such errors are hard to avoid due to the inherent noise associated with statistical alignment models. This problem can be overcome if good bilingual dictionaries are available between the source and target language. EuroWordNet1 can be used to construct such a bilingual dictionary between English and French but it is not freely available. Instead, in this work, we use a noisy statistical dictionary learnt from the Europarl parallel corpus (Koehn, 2005) which is freely downloadable. Another challenge arises in the form of matching the lexical choice of a native speaker. For example, the word coach (as in, vehicle) may get translated differently as autocar, autobus or bus even when it appears in very similar contexts. Such decisions depend on the native speaker’s intuition and are very difficult for a machine to replicate due to their inconsistent usage in a parallel training corpus. The above challenges are indeed hard to overcome, especially in an unsupervised setting, as evidenced by the lower accuracies reported by all systems participati</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>P. Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In In Proceedings of the MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>M Chodorow</author>
</authors>
<title>Combining local context and WordNet similarity for word sense identification,</title>
<date>1998</date>
<pages>305--332</pages>
<editor>In C. Fellbaum (Ed.),</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="8074" citStr="Leacock and Chodorow, 1998" startWordPosition="1314" endWordPosition="1317">closely related words. 3.2 Scoring function Based on the above motivating examples, we propose a scoring function for calculating the similarity between two sentences containing the target word. Let S1 be the test sentence containing m words and let S2 be a training sentence containing n words. Further, let w1i be the i-th word of S1 and let w29 be the j-th word of S2. The similarity between S1 and S2 is then given by, Sim(S1, S2) = A * Overlap(S1, S2) + (1 − A) * Semantic Sim(S1, S2) (1) where, 139 Overlap(S1, S2) = where, lch(w1i, w2j) Best Sim(w1i, S2) = max w2j∈S2 We used the lch measure (Leacock and Chodorow, 1998) for calculating semantic similarity of two words. The semantic similarity between S1 and S2 is then calculated by simply summing over the maximum semantic similarity of each constituent word of S1 over all words of S2. Also note that the overlap count is weighted according to the frequency of the overlapping words. This frequency is calculated from all the sentences in the training data containing the target word. The rational behind using a frequency-weighted sum is that more frequently appearing co-occurring words are better indicators of the sense of the target word (of course, stop words </context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>C. Leacock and M. Chodorow, 1998. Combining local context and WordNet similarity for word sense identification, pages 305–332. In C. Fellbaum (Ed.), MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Els Lefever</author>
<author>Veronique Hoste</author>
</authors>
<title>Semeval2010 task 3: Cross-lingual word sense disambiguation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluations (SemEval-2010), Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3309" citStr="Lefever and Hoste, 2010" startWordPosition="525" endWordPosition="528">ching the lexical choice of a native speaker. For example, the word coach (as in, vehicle) may get translated differently as autocar, autobus or bus even when it appears in very similar contexts. Such decisions depend on the native speaker’s intuition and are very difficult for a machine to replicate due to their inconsistent usage in a parallel training corpus. The above challenges are indeed hard to overcome, especially in an unsupervised setting, as evidenced by the lower accuracies reported by all systems participating in the SEMEVAL Shared Task on Cross-lingual Word Sense Disambiguation (Lefever and Hoste, 2010). Our system ranked second in the English French task (in the out-of-five evaluation). Even though its average performance was lower than the baseline by 3% it performed better than the baseline for 12 out of the 20 target nouns. Our approach identifies the top-five translations of a word by taking a majority vote over the translations appearing in the nearest neighbors of the test sentence as found in the training data. We use a pairwise similarity measure which finds the affinity between two sentences by calculating a 1http://www.illc.uva.nl/EuroWordNet 138 Proceedings of the 5th Internation</context>
<context position="11240" citStr="Lefever and Hoste, 2010" startWordPosition="1827" endWordPosition="1830">ortion of the Europarl corpus. These sentences served as the training data to be compared with each test sentence for identifying the nearest neighbors. The appropriate translations for the target word in the test sentence were then identified using the approach outlined in section 3.2 and 3.3. For the best evaluation we submitted two runs: one containing only the top-1 translation and another containing top-2 translations. For the oof evaluation we submitted one run containing the top-5 translations. The system was evaluated using Precision and Recall measures as described in the task paper (Lefever and Hoste, 2010). In the oof evaluation our system gave the second best performance among all the participants. However, the average precision was 3% lower than the baseline calculated by simply identifying the five most frequent translations of a word according to GIZA++ word alignments. A detailed analysis showed that in the oof evaluation we did better than the baseline for 12 out of the 20 nouns and in the best evaluation we did better than the baseline for 5 out of the 20 nouns. Table 1 summarizes the performance of our system in the best evaluation and Table 2 gives the detailed performance of our syste</context>
</contexts>
<marker>Lefever, Hoste, 2010</marker>
<rawString>Els Lefever and Veronique Hoste. 2010. Semeval2010 task 3: Cross-lingual word sense disambiguation. In Proceedings of the 5th International Workshop on Semantic Evaluations (SemEval-2010), Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone. In</title>
<date>1986</date>
<booktitle>In Proceedings of the 5th annual international conference on Systems documentation.</booktitle>
<contexts>
<context position="4501" citStr="Lesk, 1986" startWordPosition="718" endWordPosition="719">the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 138–141, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics weighted sum of the word overlap and the semantic overlap between them. The semantic overlap is calculated using standard Wordnet Similarity measures. The remainder of this paper is organized as follows. In section 2 we describe related work on WSD. In section 3 we describe our approach. In Section 4 we present the results followed by conclusion in section 5. 2 Related Work Knowledge based approaches to WSD such as Lesk’s algorithm (Lesk, 1986), Walker’s algorithm (Walker and Amsler, 1986), Conceptual Density (Agirre and Rigau, 1996) and Random Walk Algorithm (Mihalcea, 2005) are fundamentally overlap based algorithms which suffer from data sparsity. While these approaches do well in cases where there is a surface match (i.e., exact word match) between two occurrences of the target word (say, training and test sentence) they fail in cases where their is a semantic match between two occurrences of the target word even though there is no surface match between them. The main reason for this failure is that these approaches do not take </context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>Michael Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone. In In Proceedings of the 5th annual international conference on Systems documentation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
</authors>
<title>Large vocabulary unsupervised word sense disambiguation with graph-based algorithms for sequence data labeling. In</title>
<date>2005</date>
<booktitle>In Proceedings of the Joint Human Language Technology and Empirical Methods in Natural Language Processing Conference (HLT/EMNLP),</booktitle>
<pages>411--418</pages>
<contexts>
<context position="4635" citStr="Mihalcea, 2005" startWordPosition="737" endWordPosition="738">on for Computational Linguistics weighted sum of the word overlap and the semantic overlap between them. The semantic overlap is calculated using standard Wordnet Similarity measures. The remainder of this paper is organized as follows. In section 2 we describe related work on WSD. In section 3 we describe our approach. In Section 4 we present the results followed by conclusion in section 5. 2 Related Work Knowledge based approaches to WSD such as Lesk’s algorithm (Lesk, 1986), Walker’s algorithm (Walker and Amsler, 1986), Conceptual Density (Agirre and Rigau, 1996) and Random Walk Algorithm (Mihalcea, 2005) are fundamentally overlap based algorithms which suffer from data sparsity. While these approaches do well in cases where there is a surface match (i.e., exact word match) between two occurrences of the target word (say, training and test sentence) they fail in cases where their is a semantic match between two occurrences of the target word even though there is no surface match between them. The main reason for this failure is that these approaches do not take into account semantic generalizations (e.g., train isa vehicle). On the other hand, WSD approaches which use Wordnet based semantic si</context>
</contexts>
<marker>Mihalcea, 2005</marker>
<rawString>Rada Mihalcea. 2005. Large vocabulary unsupervised word sense disambiguation with graph-based algorithms for sequence data labeling. In In Proceedings of the Joint Human Language Technology and Empirical Methods in Natural Language Processing Conference (HLT/EMNLP), pages 411–418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Hian Beng Lee</author>
</authors>
<title>Integrating multiple knowledge sources to disambiguate word senses: An exemplar-based approach. In</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>40--47</pages>
<contexts>
<context position="5673" citStr="Ng and Lee, 1996" startWordPosition="903" endWordPosition="906">failure is that these approaches do not take into account semantic generalizations (e.g., train isa vehicle). On the other hand, WSD approaches which use Wordnet based semantic similarity measures (Patwardhan et al., 2003) account for such semantic generalizations and can be used in conjunction with overlap based approaches. We therefore propose a scoring function which combines the strength of overlap based approaches – frequently co-occurring words indeed provide strong clues – with semantic generalizations using Wordnet based similarity measures. The disambiguation is then done using k-NN (Ng and Lee, 1996) where the k nearest neighbors of the test sentence are identified using this scoring function. Once the nearest neighbors have been identified, the best translation is found by taking a majority vote over the translations of these nearest neighbors. 3 Our approach In this section we explain our approach for Cross Language Word Sense Disambiguation. The main emphasis is on disambiguation i.e. finding English sentences from the training data which are closely related to the test sentence. 3.1 Motivating Examples To explain our approach we start with two motivating examples. First, consider the </context>
</contexts>
<marker>Ng, Lee, 1996</marker>
<rawString>Hwee Tou Ng and Hian Beng Lee. 1996. Integrating multiple knowledge sources to disambiguate word senses: An exemplar-based approach. In In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL), pages 40–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="9458" citStr="Och and Ney, 2003" startWordPosition="1545" endWordPosition="1548">ense of coach. The values of Overlap(S1, S2) and Semantic Sim(S1, S2) are appropriately normalized before summing them in Equation (1). To prevent the semantic similarity measure from introducing noise by over-generalizing we chose a very high value of A. This effectively ensured that the Semantic Sim(S1, S2) term in Equation (1) became active only when the Overlap(S1, S2) measure suffered data sparsity. In other words, we placed a higher bet on Overlap(S1, S2) than on Semantic Sim(S1, S2) as we found the former to be more reliable. 3.3 Finding translations of the target word We used GIZA++2 (Och and Ney, 2003), a freely available implementation of the IBM alignment models (Brown et al., 1993) to get word level alignments for the sentences in the English-French 2http://sourceforge.net/projects/giza/ portion of the Europarl corpus. Under this alignment, each word in the source sentence is aligned to zero or more words in the corresponding target sentence. Once the nearest neighbors for a test sentence are identified using the similarity score described earlier, we use the word alignment models to find the French translation of the target word in the top-k nearest training sentences. These translation</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>Using measures of semantic relatedness for word sense disambiguation. In</title>
<date>2003</date>
<booktitle>In proceedings of the Fourth International Conference on Intelligent Text Processing and Computation Linguistics (CICLing.</booktitle>
<contexts>
<context position="5278" citStr="Patwardhan et al., 2003" startWordPosition="839" endWordPosition="843">overlap based algorithms which suffer from data sparsity. While these approaches do well in cases where there is a surface match (i.e., exact word match) between two occurrences of the target word (say, training and test sentence) they fail in cases where their is a semantic match between two occurrences of the target word even though there is no surface match between them. The main reason for this failure is that these approaches do not take into account semantic generalizations (e.g., train isa vehicle). On the other hand, WSD approaches which use Wordnet based semantic similarity measures (Patwardhan et al., 2003) account for such semantic generalizations and can be used in conjunction with overlap based approaches. We therefore propose a scoring function which combines the strength of overlap based approaches – frequently co-occurring words indeed provide strong clues – with semantic generalizations using Wordnet based similarity measures. The disambiguation is then done using k-NN (Ng and Lee, 1996) where the k nearest neighbors of the test sentence are identified using this scoring function. Once the nearest neighbors have been identified, the best translation is found by taking a majority vote over</context>
</contexts>
<marker>Patwardhan, Banerjee, Pedersen, 2003</marker>
<rawString>Siddharth Patwardhan, Satanjeev Banerjee, and Ted Pedersen. 2003. Using measures of semantic relatedness for word sense disambiguation. In In proceedings of the Fourth International Conference on Intelligent Text Processing and Computation Linguistics (CICLing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Walker</author>
<author>R Amsler</author>
</authors>
<title>The use of machine readable dictionaries in sublanguage analysis.</title>
<date>1986</date>
<booktitle>In In Analyzing Language in Restricted Domains, Grishman and Kittredge (eds),</booktitle>
<pages>69--83</pages>
<publisher>LEA Press,</publisher>
<contexts>
<context position="4547" citStr="Walker and Amsler, 1986" startWordPosition="722" endWordPosition="725"> Semantic Evaluation, ACL 2010, pages 138–141, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics weighted sum of the word overlap and the semantic overlap between them. The semantic overlap is calculated using standard Wordnet Similarity measures. The remainder of this paper is organized as follows. In section 2 we describe related work on WSD. In section 3 we describe our approach. In Section 4 we present the results followed by conclusion in section 5. 2 Related Work Knowledge based approaches to WSD such as Lesk’s algorithm (Lesk, 1986), Walker’s algorithm (Walker and Amsler, 1986), Conceptual Density (Agirre and Rigau, 1996) and Random Walk Algorithm (Mihalcea, 2005) are fundamentally overlap based algorithms which suffer from data sparsity. While these approaches do well in cases where there is a surface match (i.e., exact word match) between two occurrences of the target word (say, training and test sentence) they fail in cases where their is a semantic match between two occurrences of the target word even though there is no surface match between them. The main reason for this failure is that these approaches do not take into account semantic generalizations (e.g., t</context>
</contexts>
<marker>Walker, Amsler, 1986</marker>
<rawString>D. Walker and R. Amsler. 1986. The use of machine readable dictionaries in sublanguage analysis. In In Analyzing Language in Restricted Domains, Grishman and Kittredge (eds), LEA Press, pages 69–83.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>