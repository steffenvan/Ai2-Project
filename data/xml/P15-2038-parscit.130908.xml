<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004937">
<title confidence="0.995522">
Grounding Semantics in Olfactory Perception
</title>
<author confidence="0.993266">
Douwe Kiela, Luana Bulat and Stephen Clark
</author>
<affiliation confidence="0.992513">
Computer Laboratory
University of Cambridge
</affiliation>
<email confidence="0.985259">
douwe.kiela,ltf24,stephen.clark@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.993636" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999788307692308">
Multi-modal semantics has relied on fea-
ture norms or raw image data for per-
ceptual input. In this paper we examine
grounding semantic representations in ol-
factory (smell) data, through the construc-
tion of a novel bag of chemical compounds
model. We use standard evaluations for
multi-modal semantics, including measur-
ing conceptual similarity and cross-modal
zero-shot learning. To our knowledge, this
is the first work to evaluate semantic sim-
ilarity on representations grounded in ol-
factory data.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999879272727273">
Distributional semantics represents the meanings
of words as vectors in a “semantic space”, rely-
ing on the distributional hypothesis: the idea that
words that occur in similar contexts tend to have
similar meanings (Turney and Pantel, 2010; Clark,
2015). Although these models have been success-
ful, the fact that the meaning of a word is repre-
sented as a distribution over other words implies
they suffer from the grounding problem (Harnad,
1990); i.e. they do not account for the fact that
human semantic knowledge is grounded in phys-
ical reality and sensori-motor experience (Louw-
erse, 2008).
Multi-modal semantics attempts to address this
issue and there has been a surge of recent
work on perceptually grounded semantic models.
These models learn semantic representations from
both textual and perceptual input and outperform
language-only models on a range of tasks, includ-
ing modelling semantic similarity and relatedness,
and predicting compositionality (Silberer and La-
pata, 2012; Roller and Schulte im Walde, 2013;
Bruni et al., 2014). Perceptual information is ob-
tained from either feature norms (Silberer and La-
pata, 2012; Roller and Schulte im Walde, 2013;
Hill and Korhonen, 2014) or raw data sources such
as images (Feng and Lapata, 2010; Leong and Mi-
halcea, 2011; Bruni et al., 2014; Kiela and Bottou,
2014). The former are elicited from human anno-
tators and thus tend to be limited in scope and ex-
pensive to obtain. The latter approach has the ad-
vantage that images are widely available and easy
to obtain, which, combined with the ready avail-
ability of computer vision methods, has led to raw
visual information becoming the de-facto percep-
tual modality in multi-modal models.
However, if our objective is to ground seman-
tic representations in perceptual information, why
stop at image data? The meaning of lavender is
probably more grounded in its smell than in the
visual properties of the flower that produces it.
Olfactory (smell) perception is of particular in-
terest for grounded semantics because it is much
more primitive compared to the other perceptual
modalities (Carmichael et al., 1994; Krusemark et
al., 2013). As a result, natural language speak-
ers might take aspects of olfactory perception “for
granted”, which would imply that text is a rel-
atively poor source of such perceptual informa-
tion. A multi-modal approach would overcome
this problem, and might prove useful in, for ex-
ample, metaphor interpretation (the sweet smell of
success; rotten politics) and cognitive modelling,
as well as in real-world applications such as au-
tomatically retrieving smells or even producing
smell descriptions. Here, we explore grounding
semantic representations in olfactory perception.
We obtain olfactory representations by con-
structing a novel bag of chemical compounds
(BoCC) model. Following previous work in multi-
modal semantics, we evaluate on well known con-
ceptual similarity and relatedness tasks and on
zero-shot learning through induced cross-modal
mappings. To our knowledge this is the first
work to explore using olfactory perceptual data for
grounding linguistic semantic models.
</bodyText>
<page confidence="0.854869">
231
</page>
<footnote confidence="0.432880666666667">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 231–236,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</footnote>
<subsectionHeader confidence="0.632412">
Olfactory-Relevant Examples
</subsectionHeader>
<bodyText confidence="0.894019125">
MEN sim
steak meat 0.75
flower violet 0.70
tree maple 0.55
grass moss 0.50
beach sea 0.47
cereal wheat 0.38
bread flour 0.33
</bodyText>
<tableCaption confidence="0.758167">
Table 1: Examples of pairs in the evaluation
</tableCaption>
<bodyText confidence="0.8564175">
datasets where olfactory information is relevant,
together with the gold-standard similarity score.
</bodyText>
<sectionHeader confidence="0.98581" genericHeader="introduction">
2 Tasks
</sectionHeader>
<bodyText confidence="0.999472333333333">
Following previous work in grounded semantics,
we evaluate performance on two tasks: conceptual
similarity and cross-modal zero-shot learning.
</bodyText>
<subsectionHeader confidence="0.993016">
2.1 Conceptual similarity
</subsectionHeader>
<bodyText confidence="0.99999272972973">
We evaluate the performance of olfactory multi-
modal representations on two well-known similar-
ity datasets: SimLex-999 (Hill et al., 2014) and the
MEN test collection (Bruni et al., 2014). These
datasets consist of concept pairs together with a
human-annotated similarity score. Model perfor-
mance is evaluated using the Spearman ps corre-
lation between the ranking produced by the cosine
of the model-derived vectors and that produced by
the gold-standard similarity scores.
Evidence suggests that the inclusion of visual
representations only improves performance for
certain concepts, and that in some cases the in-
troduction of visual information is detrimental to
performance on similarity and relatedness tasks
(Kiela et al., 2014). The same is likely to be true
for other perceptual modalities: in the case of a
comparison such as lily-rose, the olfactory modal-
ity certainly is meaningful, while this is probably
not the case for skateboard-swimsuit. Some exam-
ples of relevant pairs can be found in Table 1.
Hence, we had two annotators rate the two
datasets according to whether smell is relevant to
the pairwise comparison. The annotation criterion
was as follows: if both concepts in a pairwise com-
parison have a distinctive associated smell, then
the comparison is relevant to the olfactory modal-
ity. Only if both annotators agree is the com-
parison deemed olfactory-relevant. This annota-
tion leads to a total of four evaluation sets: the
MEN test collection MEN (3000 pairs) and its
olfactory-relevant subset OMEN (311 pairs); and
the SimLex-999 dataset SLex (999 pairs) and its
olfactory-relevant subset OSLex (65 pairs). The
inter-annotator agreement on the olfactory rele-
vance judgments was high (n = 0.94 for the MEN
test collection and n = 0.96 for SimLex-999).1
</bodyText>
<subsectionHeader confidence="0.99845">
2.2 Cross-modal zero-shot learning
</subsectionHeader>
<bodyText confidence="0.999989648648649">
Cross-modal semantics, instead of being con-
cerned with improving semantic representations
through grounding, focuses on the problem of ref-
erence. Using, for instance, mappings between
visual and textual space, the objective is to learn
which words refer to which objects (Lazaridou et
al., 2014). This problem is very much related to
the object recognition task in computer vision, but
instead of using just visual data and labels, these
cross-modal models also utilize textual informa-
tion (Socher et al., 2014; Frome et al., 2013). This
approach allows for zero-shot learning, where the
model can predict how an object relates to other
concepts just from seeing an image of the object,
but without ever having seen the object previously
(Lazaridou et al., 2014).
We evaluate cross-modal zero-shot learning
performance through the average percentage cor-
rect at N (P@N), which measures how many of the
test instances were ranked within the top N high-
est ranked nearest neighbors. A chance baseline is
obtained by randomly ranking a concept’s nearest
neighbors. We use partial least squares regression
(PLSR) to induce cross-modal mappings from the
linguistic to the olfactory space and vice versa.2
Due to the nature of the olfactory data source
(see Section 3), it is not possible to build olfac-
tory representations for all concepts in the test sets.
However, cross-modal mappings yield an addi-
tional benefit: since linguistic representations have
full coverage over the datasets, we can project
from linguistic space to perceptual space to also
obtain full coverage for the perceptual modalities.
This technique has been used to increase coverage
for feature norms (Fagarasan et al., 2015). Con-
sequently, we are in a position to compare percep-
tual spaces directly to each other, and to linguistic
</bodyText>
<footnote confidence="0.9025295">
1To facilitate further work in multi-modal semantics be-
yond vision, our code and data have been made publicly
available at http://www.cl.cam.ac.uk/˜dk427/aroma.html.
2To avoid introducing another parameter, we set the num-
ber of latent variables in the cross-modal PLSR map to a third
of the number of dimensions of the perceptual representation.
</footnote>
<table confidence="0.9726425">
bakery bread 0.96
grass lawn 0.96
dog terrier 0.90
bacon meat 0.88
oak wood 0.84
daisy violet 0.76
daffodil rose 0.74
SimLex-999 sim
</table>
<page confidence="0.6413">
232
</page>
<figure confidence="0.724981833333333">
Chemical Compound
Melon ✓ ✓
Pineapple ✓ ✓
Licorice ✓
Anise ✓ ✓
Beer ✓ ✓
</figure>
<tableCaption confidence="0.871563">
Table 2: A BoCC model.
</tableCaption>
<bodyText confidence="0.9952775">
space, over the entire dataset, as well as on the rel-
evant olfactory subsets. When projecting into such
a space and reporting results, the model is prefixed
with an arrow (→) in the corresponding table.
</bodyText>
<sectionHeader confidence="0.995913" genericHeader="method">
3 Olfactory Perception
</sectionHeader>
<bodyText confidence="0.99987968">
The Sigma-Aldrich Fine Chemicals flavors and
fragrances catalog3 (henceforth SAFC) is one of
the largest publicly accessible databases of se-
mantic odor profiles that is used extensively in
fragrance research (Zarzo and Stanton, 2006).
It contains organoleptic labels and the chemical
compounds—or more accurately the perfume raw
materials (PRMs)—that produce them. By auto-
matically scraping the catalog we obtained a total
of 137 organoleptic smell labels from SAFC, with
a total of 11,152 associated PRMs. We also exper-
imented with Flavornet4 and the LRI and odour
database5, but found that the data from these were
more noisy and generally of lower quality.
For each of the smell labels in SAFC we count
the co-occurrences of associated chemical com-
pounds, yielding a bag of chemical compounds
(BoCC) model. Table 2 shows an example sub-
space of this model. Although the SAFC cata-
log is considered sufficiently comprehensive for
fragrance research (Zarzo and Stanton, 2006), the
fact that PRMs usually occur only once per smell
label means that the representations are rather
sparse. Hence, we apply dimensionality reduc-
tion to the original representation to get denser
</bodyText>
<footnote confidence="0.998902">
3http://www.sigmaaldrich.com/industries/flavors-and-
fragrances.html
4http://www.flavornet.org
5http://www.odour.org.uk
</footnote>
<figureCaption confidence="0.990653333333333">
Figure 1: Performance of olfactory representa-
tions when using SVD to reduce the number of
dimensions.
</figureCaption>
<table confidence="0.999371">
Dataset Linguistic BoCC-Raw BoCC-SVD
OMEN (35) 0.40 0.42 0.53
</table>
<tableCaption confidence="0.9336645">
Table 3: Comparison of olfactory representations
on the covered OMEN dataset.
</tableCaption>
<bodyText confidence="0.99990884">
vectors. We call the model without any dimen-
sionality reduction BOCC-RAW and use singu-
lar value decomposition (SVD) to create an ad-
ditional BOCC-SVD model with reduced dimen-
sionality. Positive pointwise mutual information
(PPMI) weighting is applied to the raw space be-
fore performing dimensionality reduction.
The number of dimensions in human olfactory
space is a hotly debated topic in the olfactory
chemical sciences (Buck and Axel, 1991; Zarzo
and Stanton, 2006). Recent studies involving
multi-dimensional scaling on the SAFC catalog
revealed approximately 32 dimensions in olfactory
perception space (Mamlouk et al., 2003; Mamlouk
and Martinetz, 2004). We examine this finding
by evaluating the Spearman ρs correlation on the
pairs of OMEN that occur in the SAFC database
(35 pairs). The coverage on SimLex was not suffi-
cient to also try that dataset (only 5 pairs). Figure
1 shows the results. It turns out that the best olfac-
tory representations are obtained with 30 dimen-
sions. In other words, our findings appear to cor-
roborate recent evidence suggesting that olfactory
space (at least when using SAFC as a data source)
is best modeled using around 30 dimensions.
</bodyText>
<subsectionHeader confidence="0.998241">
3.1 Linguistic representations
</subsectionHeader>
<bodyText confidence="0.991115833333333">
For the linguistic representations we use the con-
tinuous vector representations from the log-linear
skip-gram model of Mikolov et al. (2013), specif-
ically the 300-dimensional vector representations
trained on part of the Google News dataset (about
100 billion words) that have been released on the
</bodyText>
<figure confidence="0.9939925">
Phenethyl acetate
Isoamyl butyrate
Anisyl butyrate
Myrcene
Syringaldehyde
Smell label
</figure>
<page confidence="0.993063">
233
</page>
<table confidence="0.9991344">
MEN OMEN SLex OSLex
Linguistic 0.78 0.38 0.44 0.30
→BoCC-Raw 0.38 0.36 0.19 0.23
→BoCC-SVD 0.46 0.51 0.23 0.48
Multi-modal 0.69 0.53 0.40 0.49
</table>
<tableCaption confidence="0.9956265">
Table 4: Comparison of linguistic, olfactory and
multi-modal representations.
</tableCaption>
<table confidence="0.99983925">
Mapping P@1 P@5 P@20 P@50
Chance 0.0 3.76 13.53 36.09
Olfactory ⇒ Ling. 1.51 8.33 24.24 47.73
Ling. ⇒ Olfactory 4.55 15.15 43.18 67.42
</table>
<tableCaption confidence="0.960399">
Table 5: Zero-shot learning performance for
BoCC-SVD.
</tableCaption>
<bodyText confidence="0.48483">
Word2vec website.6
</bodyText>
<subsectionHeader confidence="0.998173">
3.2 Conceptual Similarity
</subsectionHeader>
<bodyText confidence="0.999802208333333">
Results on the 35 covered pairs of OMEN for the
two BoCC models are reported in Table 3. Ol-
factory representations outperform linguistic rep-
resentations on this subset. In fact, linguistic rep-
resentations perform poorly compared to their per-
formance on the whole of MEN. The SVD model
performs best, improving on the linguistic and raw
models with a 33% and 26% relative increase in
performance, respectively.
We use a cross-modal PLSR map, trained on
all available organoleptic labels in SAFC, to ex-
tend coverage and allow for a direct compari-
son between linguistic representations and cross-
modally projected olfactory representations on the
entire datasets and relevant subsets. The results
are shown in Table 4. As might be expected, lin-
guistic performs better than olfactory on the full
datasets. On the olfactory-relevant subsets, how-
ever, the projected BOCC-SVD model outper-
forms linguistic for both datasets. Performance in-
creases even further when the two representations
are combined into a multi-modal representation by
concatenating the L2-normalized linguistic and ol-
factory (→BOCC-SVD) vectors.
</bodyText>
<subsectionHeader confidence="0.997322">
3.3 Zero-shot learning
</subsectionHeader>
<bodyText confidence="0.99863675">
We learn a cross-modal mapping between the two
spaces and evaluate zero-shot learning. We use all
137 labels in the SAFC database that have corre-
sponding linguistic vectors for the training data.
</bodyText>
<footnote confidence="0.972967">
6https://code.google.com/p/word2vec/
</footnote>
<table confidence="0.996382833333333">
apple bacon brandy cashew
pear smoky rum hazelnut
banana roasted whiskey peanut
melon coffee wine-like almond
apricot mesquite grape hawthorne
pineapple mossy fleshy jam
chocolate lemon cheese caramel
cocoa citrus grassy nutty
sweet geranium butter roasted
coffee grapefruit oily maple
licorice tart creamy butterscotch
roasted floral coconut coffee
</table>
<tableCaption confidence="0.96717">
Table 6: Example nearest neighbors for BoCC-
SVD representations.
</tableCaption>
<bodyText confidence="0.998788384615385">
For each term, we train the map on all other la-
bels and measure whether the correct instance is
ranked within the top N neighbors. We use the
BOCC-SVD model for the olfactory space, since
it performed best on the conceptual similarity task.
Table 5 shows the results. It appears that mapping
linguistic to olfactory is easier than mapping olfac-
tory to linguistic, which may be explained by the
different number of dimensions in the two spaces.
One could say that it is easier to find the chemical
composition of a “smelly” word from its linguistic
representation, than it is to linguistically represent
or describe a chemical composition.
</bodyText>
<subsectionHeader confidence="0.999506">
3.4 Qualitative analysis
</subsectionHeader>
<bodyText confidence="0.999987625">
We also examined the BoCC representations qual-
itatively. As Table 6 shows, the nearest neigh-
bors are remarkably semantically coherent. The
nearest neighbors for bacon and cheese, for ex-
ample, accurately sum up how one might describe
those smells. The model also groups together nuts
and fruits, and expresses well what chocolate and
caramel smell (or taste) like.
</bodyText>
<sectionHeader confidence="0.997111" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.9999354">
We have studied grounding semantic representa-
tions in raw olfactory perceptual information. We
used a bag of chemical compounds model to ob-
tain olfactory representations and evaluated on
conceptual similarity and cross-modal zero-shot
learning, with good results. It is possible that the
olfactory modality is well-suited to other forms of
evaluation, but in this initial work we chose to fol-
low standard practice in multi-modal semantics to
allow for a direct comparison.
</bodyText>
<page confidence="0.991707">
234
</page>
<bodyText confidence="0.9999839">
This work opens up interesting possibilities in
analyzing smell and even taste. It could be applied
in a variety of settings beyond semantic similarity,
from chemical information retrieval to metaphor
interpretation to cognitive modelling. A specula-
tive blue-sky application based on this, and other
multi-modal models, would be an NLG applica-
tion describing a wine based on its chemical com-
position, and perhaps other information such as its
color and country of origin.
</bodyText>
<sectionHeader confidence="0.994733" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999832285714286">
DK is supported by EPSRC grant EP/I037512/1.
LB is supported by an EPSRC Doctoral Train-
ing Grant. SC is supported by ERC Start-
ing Grant DisCoTex (306920) and EPSRC grant
EP/I037512/1. We thank the anonymous review-
ers for their helpful comments and Flaviu Bulat
for providing useful feedback.
</bodyText>
<sectionHeader confidence="0.998152" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999710292682927">
Elia Bruni, Nam-Khanh Tran, and Marco Baroni.
2014. Multimodal distributional semantics. Journal
of Artifical Intelligence Research, 49:1–47.
Linda Buck and Richard Axel. 1991. A novel multi-
gene family may encode odorant receptors: a molec-
ular basis for odor recognition. Cell, 65(1):175–187.
S. Thomas Carmichael, M.-C. Clugnet, and Joseph L.
Price. 1994. Central olfactory connections in the
macaque monkey. Journal of Comparative Neurol-
ogy, 346(3):403–434.
Stephen Clark. 2015. Vector Space Models of Lexical
Meaning. In Shalom Lappin and Chris Fox, editors,
Handbook of Contemporary Semantics, chapter 16.
Wiley-Blackwell, Oxford.
Luana Fagarasan, Eva Maria Vecchi, and Stephen
Clark. 2015. From distributional semantics to fea-
ture norms: grounding semantic models in human
perceptual data. In Proceedings of the 11th Inter-
national Conference on Computational Semantics
(IWCS 2015), pages 52–57, London, UK.
Yansong Feng and Mirella Lapata. 2010. Visual infor-
mation in semantic representation. In Proceedings
of NAACL, pages 91–99.
Andrea Frome, Gregory S. Corrado, Jonathon Shlens,
Samy Bengio, Jeffrey Dean, Marc’Aurelio Ranzato,
and Tomas Mikolov. 2013. DeViSE: A Deep
Visual-Semantic Embedding Model. In Proceedings
of NIPS, pages 2121–2129.
Stevan Harnad. 1990. The symbol grounding problem.
Physica D, 42:335–346.
Felix Hill and Anna Korhonen. 2014. Learning ab-
stract concept embeddings from multi-modal data:
Since you probably can’t see what I mean. In Pro-
ceedings of EMNLP, pages 255–265.
Felix Hill, Roi Reichart, and Anna Korhonen.
2014. SimLex-999: Evaluating semantic mod-
els with (genuine) similarity estimation. CoRR,
abs/1408.3456.
Douwe Kiela and L´eon Bottou. 2014. Learning image
embeddings using convolutional neural networks for
improved multi-modal semantics. In Proceedings of
EMNLP, pages 36–45.
Douwe Kiela, Felix Hill, Anna Korhonen, and Stephen
Clark. 2014. Improving multi-modal representa-
tions using image dispersion: Why less is sometimes
more. In Proceedings of ACL, pages 835–841.
Elizabeth A Krusemark, Lucas R Novak, Darren R
Gitelman, and Wen Li. 2013. When the sense of
smell meets emotion: anxiety-state-dependent olfac-
tory processing and neural circuitry adaptation. The
Journal of Neuroscience, 33(39):15324–15332.
Angeliki Lazaridou, Elia Bruni, and Marco Baroni.
2014. Is this a wampimuk? Cross-modal map-
ping between distributional semantics and the visual
world. In Proceedings of ACL, pages 1403–1414.
Chee Wee Leong and Rada Mihalcea. 2011. Going
beyond text: A hybrid image-text approach for mea-
suring word relatedness. In Proceedings of IJCNLP,
pages 1403–1407.
Max M. Louwerse. 2008. Symbol interdependency in
symbolic and embodied cognition. Topics in Cogni-
tive Science, 59(1):617–645.
Amir Madany Mamlouk and Thomas Martinetz. 2004.
On the dimensions of the olfactory perception space.
Neurocomputing, 58:1019–1025.
Amir Madany Mamlouk, Christine Chee-Ruiter, Ul-
rich G Hofmann, and James M Bower. 2003. Quan-
tifying olfactory perception: Mapping olfactory per-
ception space by using multidimensional scaling and
self-organizing maps. Neurocomputing, 52:591–
597.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word repre-
sentations in vector space. In Proceedings of ICLR,
Scottsdale, Arizona, USA.
Stephen Roller and Sabine Schulte im Walde. 2013.
A multimodal LDA model integrating textual, cog-
nitive and visual modalities. In Proceedings of
EMNLP, pages 1146–1157.
Carina Silberer and Mirella Lapata. 2012. Grounded
models of semantic representation. In Proceedings
of EMNLP, pages 1423–1433.
</reference>
<page confidence="0.974889">
235
</page>
<reference confidence="0.999567769230769">
Richard Socher, Andrej Karpathy, Quoc V. Le, Christo-
pher D. Manning, and Andrew Y. Ng. 2014.
Grounded compositional semantics for finding and
describing images with sentences. Transactions of
ACL, 2:207–218.
Peter D. Turney and Patrick Pantel. 2010. From
Frequency to Meaning: vector space models of se-
mantics. Journal of Artifical Intelligence Research,
37(1):141–188, January.
Manuel Zarzo and David T. Stanton. 2006. Identi-
fication of latent variables in a semantic odor pro-
file database using principal component analysis.
Chemical Senses, 31(8):713–724.
</reference>
<page confidence="0.998548">
236
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.895723">
<title confidence="0.999936">Grounding Semantics in Olfactory Perception</title>
<author confidence="0.986483">Douwe Kiela</author>
<author confidence="0.986483">Luana Bulat</author>
<author confidence="0.986483">Stephen</author>
<affiliation confidence="0.9950215">Computer University of</affiliation>
<email confidence="0.976513">douwe.kiela,ltf24,stephen.clark@cl.cam.ac.uk</email>
<abstract confidence="0.995449785714286">Multi-modal semantics has relied on feature norms or raw image data for perceptual input. In this paper we examine grounding semantic representations in olfactory (smell) data, through the construction of a novel bag of chemical compounds model. We use standard evaluations for multi-modal semantics, including measuring conceptual similarity and cross-modal zero-shot learning. To our knowledge, this is the first work to evaluate semantic similarity on representations grounded in olfactory data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Nam-Khanh Tran</author>
<author>Marco Baroni</author>
</authors>
<title>Multimodal distributional semantics.</title>
<date>2014</date>
<journal>Journal of Artifical Intelligence Research,</journal>
<pages>49--1</pages>
<contexts>
<context position="1742" citStr="Bruni et al., 2014" startWordPosition="256" endWordPosition="259">problem (Harnad, 1990); i.e. they do not account for the fact that human semantic knowledge is grounded in physical reality and sensori-motor experience (Louwerse, 2008). Multi-modal semantics attempts to address this issue and there has been a surge of recent work on perceptually grounded semantic models. These models learn semantic representations from both textual and perceptual input and outperform language-only models on a range of tasks, including modelling semantic similarity and relatedness, and predicting compositionality (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; Bruni et al., 2014). Perceptual information is obtained from either feature norms (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or raw data sources such as images (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela and Bottou, 2014). The former are elicited from human annotators and thus tend to be limited in scope and expensive to obtain. The latter approach has the advantage that images are widely available and easy to obtain, which, combined with the ready availability of computer vision methods, has led to raw visual information becoming the d</context>
<context position="4754" citStr="Bruni et al., 2014" startWordPosition="716" endWordPosition="719">meat 0.75 flower violet 0.70 tree maple 0.55 grass moss 0.50 beach sea 0.47 cereal wheat 0.38 bread flour 0.33 Table 1: Examples of pairs in the evaluation datasets where olfactory information is relevant, together with the gold-standard similarity score. 2 Tasks Following previous work in grounded semantics, we evaluate performance on two tasks: conceptual similarity and cross-modal zero-shot learning. 2.1 Conceptual similarity We evaluate the performance of olfactory multimodal representations on two well-known similarity datasets: SimLex-999 (Hill et al., 2014) and the MEN test collection (Bruni et al., 2014). These datasets consist of concept pairs together with a human-annotated similarity score. Model performance is evaluated using the Spearman ps correlation between the ranking produced by the cosine of the model-derived vectors and that produced by the gold-standard similarity scores. Evidence suggests that the inclusion of visual representations only improves performance for certain concepts, and that in some cases the introduction of visual information is detrimental to performance on similarity and relatedness tasks (Kiela et al., 2014). The same is likely to be true for other perceptual m</context>
</contexts>
<marker>Bruni, Tran, Baroni, 2014</marker>
<rawString>Elia Bruni, Nam-Khanh Tran, and Marco Baroni. 2014. Multimodal distributional semantics. Journal of Artifical Intelligence Research, 49:1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linda Buck</author>
<author>Richard Axel</author>
</authors>
<title>A novel multigene family may encode odorant receptors: a molecular basis for odor recognition.</title>
<date>1991</date>
<journal>Cell,</journal>
<volume>65</volume>
<issue>1</issue>
<contexts>
<context position="10951" citStr="Buck and Axel, 1991" startWordPosition="1682" endWordPosition="1685">D to reduce the number of dimensions. Dataset Linguistic BoCC-Raw BoCC-SVD OMEN (35) 0.40 0.42 0.53 Table 3: Comparison of olfactory representations on the covered OMEN dataset. vectors. We call the model without any dimensionality reduction BOCC-RAW and use singular value decomposition (SVD) to create an additional BOCC-SVD model with reduced dimensionality. Positive pointwise mutual information (PPMI) weighting is applied to the raw space before performing dimensionality reduction. The number of dimensions in human olfactory space is a hotly debated topic in the olfactory chemical sciences (Buck and Axel, 1991; Zarzo and Stanton, 2006). Recent studies involving multi-dimensional scaling on the SAFC catalog revealed approximately 32 dimensions in olfactory perception space (Mamlouk et al., 2003; Mamlouk and Martinetz, 2004). We examine this finding by evaluating the Spearman ρs correlation on the pairs of OMEN that occur in the SAFC database (35 pairs). The coverage on SimLex was not sufficient to also try that dataset (only 5 pairs). Figure 1 shows the results. It turns out that the best olfactory representations are obtained with 30 dimensions. In other words, our findings appear to corroborate re</context>
</contexts>
<marker>Buck, Axel, 1991</marker>
<rawString>Linda Buck and Richard Axel. 1991. A novel multigene family may encode odorant receptors: a molecular basis for odor recognition. Cell, 65(1):175–187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Thomas Carmichael</author>
<author>M-C Clugnet</author>
<author>Joseph L Price</author>
</authors>
<title>Central olfactory connections in the macaque monkey.</title>
<date>1994</date>
<journal>Journal of Comparative Neurology,</journal>
<volume>346</volume>
<issue>3</issue>
<contexts>
<context position="2813" citStr="Carmichael et al., 1994" startWordPosition="434" endWordPosition="437">y available and easy to obtain, which, combined with the ready availability of computer vision methods, has led to raw visual information becoming the de-facto perceptual modality in multi-modal models. However, if our objective is to ground semantic representations in perceptual information, why stop at image data? The meaning of lavender is probably more grounded in its smell than in the visual properties of the flower that produces it. Olfactory (smell) perception is of particular interest for grounded semantics because it is much more primitive compared to the other perceptual modalities (Carmichael et al., 1994; Krusemark et al., 2013). As a result, natural language speakers might take aspects of olfactory perception “for granted”, which would imply that text is a relatively poor source of such perceptual information. A multi-modal approach would overcome this problem, and might prove useful in, for example, metaphor interpretation (the sweet smell of success; rotten politics) and cognitive modelling, as well as in real-world applications such as automatically retrieving smells or even producing smell descriptions. Here, we explore grounding semantic representations in olfactory perception. We obtai</context>
</contexts>
<marker>Carmichael, Clugnet, Price, 1994</marker>
<rawString>S. Thomas Carmichael, M.-C. Clugnet, and Joseph L. Price. 1994. Central olfactory connections in the macaque monkey. Journal of Comparative Neurology, 346(3):403–434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
</authors>
<title>Vector Space Models of Lexical Meaning.</title>
<date>2015</date>
<booktitle>Handbook of Contemporary Semantics, chapter 16. Wiley-Blackwell,</booktitle>
<editor>In Shalom Lappin and Chris Fox, editors,</editor>
<location>Oxford.</location>
<contexts>
<context position="952" citStr="Clark, 2015" startWordPosition="135" endWordPosition="136">factory (smell) data, through the construction of a novel bag of chemical compounds model. We use standard evaluations for multi-modal semantics, including measuring conceptual similarity and cross-modal zero-shot learning. To our knowledge, this is the first work to evaluate semantic similarity on representations grounded in olfactory data. 1 Introduction Distributional semantics represents the meanings of words as vectors in a “semantic space”, relying on the distributional hypothesis: the idea that words that occur in similar contexts tend to have similar meanings (Turney and Pantel, 2010; Clark, 2015). Although these models have been successful, the fact that the meaning of a word is represented as a distribution over other words implies they suffer from the grounding problem (Harnad, 1990); i.e. they do not account for the fact that human semantic knowledge is grounded in physical reality and sensori-motor experience (Louwerse, 2008). Multi-modal semantics attempts to address this issue and there has been a surge of recent work on perceptually grounded semantic models. These models learn semantic representations from both textual and perceptual input and outperform language-only models on</context>
</contexts>
<marker>Clark, 2015</marker>
<rawString>Stephen Clark. 2015. Vector Space Models of Lexical Meaning. In Shalom Lappin and Chris Fox, editors, Handbook of Contemporary Semantics, chapter 16. Wiley-Blackwell, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luana Fagarasan</author>
<author>Eva Maria Vecchi</author>
<author>Stephen Clark</author>
</authors>
<title>From distributional semantics to feature norms: grounding semantic models in human perceptual data.</title>
<date>2015</date>
<booktitle>In Proceedings of the 11th International Conference on Computational Semantics (IWCS 2015),</booktitle>
<pages>52--57</pages>
<location>London, UK.</location>
<contexts>
<context position="8067" citStr="Fagarasan et al., 2015" startWordPosition="1237" endWordPosition="1240">partial least squares regression (PLSR) to induce cross-modal mappings from the linguistic to the olfactory space and vice versa.2 Due to the nature of the olfactory data source (see Section 3), it is not possible to build olfactory representations for all concepts in the test sets. However, cross-modal mappings yield an additional benefit: since linguistic representations have full coverage over the datasets, we can project from linguistic space to perceptual space to also obtain full coverage for the perceptual modalities. This technique has been used to increase coverage for feature norms (Fagarasan et al., 2015). Consequently, we are in a position to compare perceptual spaces directly to each other, and to linguistic 1To facilitate further work in multi-modal semantics beyond vision, our code and data have been made publicly available at http://www.cl.cam.ac.uk/˜dk427/aroma.html. 2To avoid introducing another parameter, we set the number of latent variables in the cross-modal PLSR map to a third of the number of dimensions of the perceptual representation. bakery bread 0.96 grass lawn 0.96 dog terrier 0.90 bacon meat 0.88 oak wood 0.84 daisy violet 0.76 daffodil rose 0.74 SimLex-999 sim 232 Chemical </context>
</contexts>
<marker>Fagarasan, Vecchi, Clark, 2015</marker>
<rawString>Luana Fagarasan, Eva Maria Vecchi, and Stephen Clark. 2015. From distributional semantics to feature norms: grounding semantic models in human perceptual data. In Proceedings of the 11th International Conference on Computational Semantics (IWCS 2015), pages 52–57, London, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>Visual information in semantic representation.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>91--99</pages>
<contexts>
<context position="1950" citStr="Feng and Lapata, 2010" startWordPosition="291" endWordPosition="294">ddress this issue and there has been a surge of recent work on perceptually grounded semantic models. These models learn semantic representations from both textual and perceptual input and outperform language-only models on a range of tasks, including modelling semantic similarity and relatedness, and predicting compositionality (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; Bruni et al., 2014). Perceptual information is obtained from either feature norms (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or raw data sources such as images (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela and Bottou, 2014). The former are elicited from human annotators and thus tend to be limited in scope and expensive to obtain. The latter approach has the advantage that images are widely available and easy to obtain, which, combined with the ready availability of computer vision methods, has led to raw visual information becoming the de-facto perceptual modality in multi-modal models. However, if our objective is to ground semantic representations in perceptual information, why stop at image data? The meaning of lavender is probably more g</context>
</contexts>
<marker>Feng, Lapata, 2010</marker>
<rawString>Yansong Feng and Mirella Lapata. 2010. Visual information in semantic representation. In Proceedings of NAACL, pages 91–99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Frome</author>
<author>Gregory S Corrado</author>
<author>Jonathon Shlens</author>
<author>Samy Bengio</author>
<author>Jeffrey Dean</author>
<author>Marc’Aurelio Ranzato</author>
<author>Tomas Mikolov</author>
</authors>
<title>DeViSE: A Deep Visual-Semantic Embedding Model.</title>
<date>2013</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>2121--2129</pages>
<contexts>
<context position="6908" citStr="Frome et al., 2013" startWordPosition="1054" endWordPosition="1057">e MEN test collection and n = 0.96 for SimLex-999).1 2.2 Cross-modal zero-shot learning Cross-modal semantics, instead of being concerned with improving semantic representations through grounding, focuses on the problem of reference. Using, for instance, mappings between visual and textual space, the objective is to learn which words refer to which objects (Lazaridou et al., 2014). This problem is very much related to the object recognition task in computer vision, but instead of using just visual data and labels, these cross-modal models also utilize textual information (Socher et al., 2014; Frome et al., 2013). This approach allows for zero-shot learning, where the model can predict how an object relates to other concepts just from seeing an image of the object, but without ever having seen the object previously (Lazaridou et al., 2014). We evaluate cross-modal zero-shot learning performance through the average percentage correct at N (P@N), which measures how many of the test instances were ranked within the top N highest ranked nearest neighbors. A chance baseline is obtained by randomly ranking a concept’s nearest neighbors. We use partial least squares regression (PLSR) to induce cross-modal ma</context>
</contexts>
<marker>Frome, Corrado, Shlens, Bengio, Dean, Ranzato, Mikolov, 2013</marker>
<rawString>Andrea Frome, Gregory S. Corrado, Jonathon Shlens, Samy Bengio, Jeffrey Dean, Marc’Aurelio Ranzato, and Tomas Mikolov. 2013. DeViSE: A Deep Visual-Semantic Embedding Model. In Proceedings of NIPS, pages 2121–2129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stevan Harnad</author>
</authors>
<title>The symbol grounding problem.</title>
<date>1990</date>
<journal>Physica D,</journal>
<pages>42--335</pages>
<contexts>
<context position="1145" citStr="Harnad, 1990" startWordPosition="168" endWordPosition="169">cross-modal zero-shot learning. To our knowledge, this is the first work to evaluate semantic similarity on representations grounded in olfactory data. 1 Introduction Distributional semantics represents the meanings of words as vectors in a “semantic space”, relying on the distributional hypothesis: the idea that words that occur in similar contexts tend to have similar meanings (Turney and Pantel, 2010; Clark, 2015). Although these models have been successful, the fact that the meaning of a word is represented as a distribution over other words implies they suffer from the grounding problem (Harnad, 1990); i.e. they do not account for the fact that human semantic knowledge is grounded in physical reality and sensori-motor experience (Louwerse, 2008). Multi-modal semantics attempts to address this issue and there has been a surge of recent work on perceptually grounded semantic models. These models learn semantic representations from both textual and perceptual input and outperform language-only models on a range of tasks, including modelling semantic similarity and relatedness, and predicting compositionality (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; Bruni et al., 2014). P</context>
</contexts>
<marker>Harnad, 1990</marker>
<rawString>Stevan Harnad. 1990. The symbol grounding problem. Physica D, 42:335–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix Hill</author>
<author>Anna Korhonen</author>
</authors>
<title>Learning abstract concept embeddings from multi-modal data: Since you probably can’t see what I mean.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>255--265</pages>
<contexts>
<context position="1892" citStr="Hill and Korhonen, 2014" startWordPosition="280" endWordPosition="283">erience (Louwerse, 2008). Multi-modal semantics attempts to address this issue and there has been a surge of recent work on perceptually grounded semantic models. These models learn semantic representations from both textual and perceptual input and outperform language-only models on a range of tasks, including modelling semantic similarity and relatedness, and predicting compositionality (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; Bruni et al., 2014). Perceptual information is obtained from either feature norms (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or raw data sources such as images (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela and Bottou, 2014). The former are elicited from human annotators and thus tend to be limited in scope and expensive to obtain. The latter approach has the advantage that images are widely available and easy to obtain, which, combined with the ready availability of computer vision methods, has led to raw visual information becoming the de-facto perceptual modality in multi-modal models. However, if our objective is to ground semantic representations in perceptual information, why stop</context>
</contexts>
<marker>Hill, Korhonen, 2014</marker>
<rawString>Felix Hill and Anna Korhonen. 2014. Learning abstract concept embeddings from multi-modal data: Since you probably can’t see what I mean. In Proceedings of EMNLP, pages 255–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix Hill</author>
<author>Roi Reichart</author>
<author>Anna Korhonen</author>
</authors>
<title>SimLex-999: Evaluating semantic models with (genuine) similarity estimation.</title>
<date>2014</date>
<location>CoRR, abs/1408.3456.</location>
<contexts>
<context position="4705" citStr="Hill et al., 2014" startWordPosition="707" endWordPosition="710">stics Olfactory-Relevant Examples MEN sim steak meat 0.75 flower violet 0.70 tree maple 0.55 grass moss 0.50 beach sea 0.47 cereal wheat 0.38 bread flour 0.33 Table 1: Examples of pairs in the evaluation datasets where olfactory information is relevant, together with the gold-standard similarity score. 2 Tasks Following previous work in grounded semantics, we evaluate performance on two tasks: conceptual similarity and cross-modal zero-shot learning. 2.1 Conceptual similarity We evaluate the performance of olfactory multimodal representations on two well-known similarity datasets: SimLex-999 (Hill et al., 2014) and the MEN test collection (Bruni et al., 2014). These datasets consist of concept pairs together with a human-annotated similarity score. Model performance is evaluated using the Spearman ps correlation between the ranking produced by the cosine of the model-derived vectors and that produced by the gold-standard similarity scores. Evidence suggests that the inclusion of visual representations only improves performance for certain concepts, and that in some cases the introduction of visual information is detrimental to performance on similarity and relatedness tasks (Kiela et al., 2014). The</context>
</contexts>
<marker>Hill, Reichart, Korhonen, 2014</marker>
<rawString>Felix Hill, Roi Reichart, and Anna Korhonen. 2014. SimLex-999: Evaluating semantic models with (genuine) similarity estimation. CoRR, abs/1408.3456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douwe Kiela</author>
<author>L´eon Bottou</author>
</authors>
<title>Learning image embeddings using convolutional neural networks for improved multi-modal semantics.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>36--45</pages>
<contexts>
<context position="2021" citStr="Kiela and Bottou, 2014" startWordPosition="304" endWordPosition="307">tually grounded semantic models. These models learn semantic representations from both textual and perceptual input and outperform language-only models on a range of tasks, including modelling semantic similarity and relatedness, and predicting compositionality (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; Bruni et al., 2014). Perceptual information is obtained from either feature norms (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or raw data sources such as images (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela and Bottou, 2014). The former are elicited from human annotators and thus tend to be limited in scope and expensive to obtain. The latter approach has the advantage that images are widely available and easy to obtain, which, combined with the ready availability of computer vision methods, has led to raw visual information becoming the de-facto perceptual modality in multi-modal models. However, if our objective is to ground semantic representations in perceptual information, why stop at image data? The meaning of lavender is probably more grounded in its smell than in the visual properties of the flower that p</context>
</contexts>
<marker>Kiela, Bottou, 2014</marker>
<rawString>Douwe Kiela and L´eon Bottou. 2014. Learning image embeddings using convolutional neural networks for improved multi-modal semantics. In Proceedings of EMNLP, pages 36–45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douwe Kiela</author>
<author>Felix Hill</author>
<author>Anna Korhonen</author>
<author>Stephen Clark</author>
</authors>
<title>Improving multi-modal representations using image dispersion: Why less is sometimes more.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>835--841</pages>
<contexts>
<context position="5300" citStr="Kiela et al., 2014" startWordPosition="796" endWordPosition="799">-999 (Hill et al., 2014) and the MEN test collection (Bruni et al., 2014). These datasets consist of concept pairs together with a human-annotated similarity score. Model performance is evaluated using the Spearman ps correlation between the ranking produced by the cosine of the model-derived vectors and that produced by the gold-standard similarity scores. Evidence suggests that the inclusion of visual representations only improves performance for certain concepts, and that in some cases the introduction of visual information is detrimental to performance on similarity and relatedness tasks (Kiela et al., 2014). The same is likely to be true for other perceptual modalities: in the case of a comparison such as lily-rose, the olfactory modality certainly is meaningful, while this is probably not the case for skateboard-swimsuit. Some examples of relevant pairs can be found in Table 1. Hence, we had two annotators rate the two datasets according to whether smell is relevant to the pairwise comparison. The annotation criterion was as follows: if both concepts in a pairwise comparison have a distinctive associated smell, then the comparison is relevant to the olfactory modality. Only if both annotators a</context>
</contexts>
<marker>Kiela, Hill, Korhonen, Clark, 2014</marker>
<rawString>Douwe Kiela, Felix Hill, Anna Korhonen, and Stephen Clark. 2014. Improving multi-modal representations using image dispersion: Why less is sometimes more. In Proceedings of ACL, pages 835–841.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth A Krusemark</author>
<author>Lucas R Novak</author>
<author>Darren R Gitelman</author>
<author>Wen Li</author>
</authors>
<title>When the sense of smell meets emotion: anxiety-state-dependent olfactory processing and neural circuitry adaptation.</title>
<date>2013</date>
<journal>The Journal of Neuroscience,</journal>
<volume>33</volume>
<issue>39</issue>
<contexts>
<context position="2838" citStr="Krusemark et al., 2013" startWordPosition="438" endWordPosition="441">btain, which, combined with the ready availability of computer vision methods, has led to raw visual information becoming the de-facto perceptual modality in multi-modal models. However, if our objective is to ground semantic representations in perceptual information, why stop at image data? The meaning of lavender is probably more grounded in its smell than in the visual properties of the flower that produces it. Olfactory (smell) perception is of particular interest for grounded semantics because it is much more primitive compared to the other perceptual modalities (Carmichael et al., 1994; Krusemark et al., 2013). As a result, natural language speakers might take aspects of olfactory perception “for granted”, which would imply that text is a relatively poor source of such perceptual information. A multi-modal approach would overcome this problem, and might prove useful in, for example, metaphor interpretation (the sweet smell of success; rotten politics) and cognitive modelling, as well as in real-world applications such as automatically retrieving smells or even producing smell descriptions. Here, we explore grounding semantic representations in olfactory perception. We obtain olfactory representatio</context>
</contexts>
<marker>Krusemark, Novak, Gitelman, Li, 2013</marker>
<rawString>Elizabeth A Krusemark, Lucas R Novak, Darren R Gitelman, and Wen Li. 2013. When the sense of smell meets emotion: anxiety-state-dependent olfactory processing and neural circuitry adaptation. The Journal of Neuroscience, 33(39):15324–15332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angeliki Lazaridou</author>
<author>Elia Bruni</author>
<author>Marco Baroni</author>
</authors>
<title>Is this a wampimuk? Cross-modal mapping between distributional semantics and the visual world.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1403--1414</pages>
<contexts>
<context position="6672" citStr="Lazaridou et al., 2014" startWordPosition="1015" endWordPosition="1018">and its olfactory-relevant subset OMEN (311 pairs); and the SimLex-999 dataset SLex (999 pairs) and its olfactory-relevant subset OSLex (65 pairs). The inter-annotator agreement on the olfactory relevance judgments was high (n = 0.94 for the MEN test collection and n = 0.96 for SimLex-999).1 2.2 Cross-modal zero-shot learning Cross-modal semantics, instead of being concerned with improving semantic representations through grounding, focuses on the problem of reference. Using, for instance, mappings between visual and textual space, the objective is to learn which words refer to which objects (Lazaridou et al., 2014). This problem is very much related to the object recognition task in computer vision, but instead of using just visual data and labels, these cross-modal models also utilize textual information (Socher et al., 2014; Frome et al., 2013). This approach allows for zero-shot learning, where the model can predict how an object relates to other concepts just from seeing an image of the object, but without ever having seen the object previously (Lazaridou et al., 2014). We evaluate cross-modal zero-shot learning performance through the average percentage correct at N (P@N), which measures how many o</context>
</contexts>
<marker>Lazaridou, Bruni, Baroni, 2014</marker>
<rawString>Angeliki Lazaridou, Elia Bruni, and Marco Baroni. 2014. Is this a wampimuk? Cross-modal mapping between distributional semantics and the visual world. In Proceedings of ACL, pages 1403–1414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chee Wee Leong</author>
<author>Rada Mihalcea</author>
</authors>
<title>Going beyond text: A hybrid image-text approach for measuring word relatedness.</title>
<date>2011</date>
<booktitle>In Proceedings of IJCNLP,</booktitle>
<pages>1403--1407</pages>
<contexts>
<context position="1976" citStr="Leong and Mihalcea, 2011" startWordPosition="295" endWordPosition="299">here has been a surge of recent work on perceptually grounded semantic models. These models learn semantic representations from both textual and perceptual input and outperform language-only models on a range of tasks, including modelling semantic similarity and relatedness, and predicting compositionality (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; Bruni et al., 2014). Perceptual information is obtained from either feature norms (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or raw data sources such as images (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela and Bottou, 2014). The former are elicited from human annotators and thus tend to be limited in scope and expensive to obtain. The latter approach has the advantage that images are widely available and easy to obtain, which, combined with the ready availability of computer vision methods, has led to raw visual information becoming the de-facto perceptual modality in multi-modal models. However, if our objective is to ground semantic representations in perceptual information, why stop at image data? The meaning of lavender is probably more grounded in its smell than </context>
</contexts>
<marker>Leong, Mihalcea, 2011</marker>
<rawString>Chee Wee Leong and Rada Mihalcea. 2011. Going beyond text: A hybrid image-text approach for measuring word relatedness. In Proceedings of IJCNLP, pages 1403–1407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Max M Louwerse</author>
</authors>
<title>Symbol interdependency in symbolic and embodied cognition.</title>
<date>2008</date>
<journal>Topics in Cognitive Science,</journal>
<volume>59</volume>
<issue>1</issue>
<contexts>
<context position="1292" citStr="Louwerse, 2008" startWordPosition="191" endWordPosition="193"> data. 1 Introduction Distributional semantics represents the meanings of words as vectors in a “semantic space”, relying on the distributional hypothesis: the idea that words that occur in similar contexts tend to have similar meanings (Turney and Pantel, 2010; Clark, 2015). Although these models have been successful, the fact that the meaning of a word is represented as a distribution over other words implies they suffer from the grounding problem (Harnad, 1990); i.e. they do not account for the fact that human semantic knowledge is grounded in physical reality and sensori-motor experience (Louwerse, 2008). Multi-modal semantics attempts to address this issue and there has been a surge of recent work on perceptually grounded semantic models. These models learn semantic representations from both textual and perceptual input and outperform language-only models on a range of tasks, including modelling semantic similarity and relatedness, and predicting compositionality (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; Bruni et al., 2014). Perceptual information is obtained from either feature norms (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014)</context>
</contexts>
<marker>Louwerse, 2008</marker>
<rawString>Max M. Louwerse. 2008. Symbol interdependency in symbolic and embodied cognition. Topics in Cognitive Science, 59(1):617–645.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amir Madany Mamlouk</author>
<author>Thomas Martinetz</author>
</authors>
<title>On the dimensions of the olfactory perception space.</title>
<date>2004</date>
<journal>Neurocomputing,</journal>
<pages>58--1019</pages>
<contexts>
<context position="11168" citStr="Mamlouk and Martinetz, 2004" startWordPosition="1711" endWordPosition="1714">hout any dimensionality reduction BOCC-RAW and use singular value decomposition (SVD) to create an additional BOCC-SVD model with reduced dimensionality. Positive pointwise mutual information (PPMI) weighting is applied to the raw space before performing dimensionality reduction. The number of dimensions in human olfactory space is a hotly debated topic in the olfactory chemical sciences (Buck and Axel, 1991; Zarzo and Stanton, 2006). Recent studies involving multi-dimensional scaling on the SAFC catalog revealed approximately 32 dimensions in olfactory perception space (Mamlouk et al., 2003; Mamlouk and Martinetz, 2004). We examine this finding by evaluating the Spearman ρs correlation on the pairs of OMEN that occur in the SAFC database (35 pairs). The coverage on SimLex was not sufficient to also try that dataset (only 5 pairs). Figure 1 shows the results. It turns out that the best olfactory representations are obtained with 30 dimensions. In other words, our findings appear to corroborate recent evidence suggesting that olfactory space (at least when using SAFC as a data source) is best modeled using around 30 dimensions. 3.1 Linguistic representations For the linguistic representations we use the contin</context>
</contexts>
<marker>Mamlouk, Martinetz, 2004</marker>
<rawString>Amir Madany Mamlouk and Thomas Martinetz. 2004. On the dimensions of the olfactory perception space. Neurocomputing, 58:1019–1025.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amir Madany Mamlouk</author>
<author>Christine Chee-Ruiter</author>
<author>Ulrich G Hofmann</author>
<author>James M Bower</author>
</authors>
<title>Quantifying olfactory perception: Mapping olfactory perception space by using multidimensional scaling and self-organizing maps.</title>
<date>2003</date>
<journal>Neurocomputing,</journal>
<volume>52</volume>
<pages>597</pages>
<contexts>
<context position="11138" citStr="Mamlouk et al., 2003" startWordPosition="1707" endWordPosition="1710"> We call the model without any dimensionality reduction BOCC-RAW and use singular value decomposition (SVD) to create an additional BOCC-SVD model with reduced dimensionality. Positive pointwise mutual information (PPMI) weighting is applied to the raw space before performing dimensionality reduction. The number of dimensions in human olfactory space is a hotly debated topic in the olfactory chemical sciences (Buck and Axel, 1991; Zarzo and Stanton, 2006). Recent studies involving multi-dimensional scaling on the SAFC catalog revealed approximately 32 dimensions in olfactory perception space (Mamlouk et al., 2003; Mamlouk and Martinetz, 2004). We examine this finding by evaluating the Spearman ρs correlation on the pairs of OMEN that occur in the SAFC database (35 pairs). The coverage on SimLex was not sufficient to also try that dataset (only 5 pairs). Figure 1 shows the results. It turns out that the best olfactory representations are obtained with 30 dimensions. In other words, our findings appear to corroborate recent evidence suggesting that olfactory space (at least when using SAFC as a data source) is best modeled using around 30 dimensions. 3.1 Linguistic representations For the linguistic rep</context>
</contexts>
<marker>Mamlouk, Chee-Ruiter, Hofmann, Bower, 2003</marker>
<rawString>Amir Madany Mamlouk, Christine Chee-Ruiter, Ulrich G Hofmann, and James M Bower. 2003. Quantifying olfactory perception: Mapping olfactory perception space by using multidimensional scaling and self-organizing maps. Neurocomputing, 52:591– 597.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In Proceedings of ICLR,</booktitle>
<location>Scottsdale, Arizona, USA.</location>
<contexts>
<context position="11856" citStr="Mikolov et al. (2013)" startWordPosition="1825" endWordPosition="1828"> on the pairs of OMEN that occur in the SAFC database (35 pairs). The coverage on SimLex was not sufficient to also try that dataset (only 5 pairs). Figure 1 shows the results. It turns out that the best olfactory representations are obtained with 30 dimensions. In other words, our findings appear to corroborate recent evidence suggesting that olfactory space (at least when using SAFC as a data source) is best modeled using around 30 dimensions. 3.1 Linguistic representations For the linguistic representations we use the continuous vector representations from the log-linear skip-gram model of Mikolov et al. (2013), specifically the 300-dimensional vector representations trained on part of the Google News dataset (about 100 billion words) that have been released on the Phenethyl acetate Isoamyl butyrate Anisyl butyrate Myrcene Syringaldehyde Smell label 233 MEN OMEN SLex OSLex Linguistic 0.78 0.38 0.44 0.30 →BoCC-Raw 0.38 0.36 0.19 0.23 →BoCC-SVD 0.46 0.51 0.23 0.48 Multi-modal 0.69 0.53 0.40 0.49 Table 4: Comparison of linguistic, olfactory and multi-modal representations. Mapping P@1 P@5 P@20 P@50 Chance 0.0 3.76 13.53 36.09 Olfactory ⇒ Ling. 1.51 8.33 24.24 47.73 Ling. ⇒ Olfactory 4.55 15.15 43.18 67</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. In Proceedings of ICLR, Scottsdale, Arizona, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Roller</author>
<author>Sabine Schulte im Walde</author>
</authors>
<title>A multimodal LDA model integrating textual, cognitive and visual modalities.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1146--1157</pages>
<marker>Roller, Walde, 2013</marker>
<rawString>Stephen Roller and Sabine Schulte im Walde. 2013. A multimodal LDA model integrating textual, cognitive and visual modalities. In Proceedings of EMNLP, pages 1146–1157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carina Silberer</author>
<author>Mirella Lapata</author>
</authors>
<title>Grounded models of semantic representation.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1423--1433</pages>
<contexts>
<context position="1686" citStr="Silberer and Lapata, 2012" startWordPosition="245" endWordPosition="249">ution over other words implies they suffer from the grounding problem (Harnad, 1990); i.e. they do not account for the fact that human semantic knowledge is grounded in physical reality and sensori-motor experience (Louwerse, 2008). Multi-modal semantics attempts to address this issue and there has been a surge of recent work on perceptually grounded semantic models. These models learn semantic representations from both textual and perceptual input and outperform language-only models on a range of tasks, including modelling semantic similarity and relatedness, and predicting compositionality (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; Bruni et al., 2014). Perceptual information is obtained from either feature norms (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or raw data sources such as images (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela and Bottou, 2014). The former are elicited from human annotators and thus tend to be limited in scope and expensive to obtain. The latter approach has the advantage that images are widely available and easy to obtain, which, combined with the ready availability of computer vision m</context>
</contexts>
<marker>Silberer, Lapata, 2012</marker>
<rawString>Carina Silberer and Mirella Lapata. 2012. Grounded models of semantic representation. In Proceedings of EMNLP, pages 1423–1433.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Andrej Karpathy</author>
<author>Quoc V Le</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Grounded compositional semantics for finding and describing images with sentences.</title>
<date>2014</date>
<journal>Transactions of ACL,</journal>
<pages>2--207</pages>
<contexts>
<context position="6887" citStr="Socher et al., 2014" startWordPosition="1050" endWordPosition="1053">high (n = 0.94 for the MEN test collection and n = 0.96 for SimLex-999).1 2.2 Cross-modal zero-shot learning Cross-modal semantics, instead of being concerned with improving semantic representations through grounding, focuses on the problem of reference. Using, for instance, mappings between visual and textual space, the objective is to learn which words refer to which objects (Lazaridou et al., 2014). This problem is very much related to the object recognition task in computer vision, but instead of using just visual data and labels, these cross-modal models also utilize textual information (Socher et al., 2014; Frome et al., 2013). This approach allows for zero-shot learning, where the model can predict how an object relates to other concepts just from seeing an image of the object, but without ever having seen the object previously (Lazaridou et al., 2014). We evaluate cross-modal zero-shot learning performance through the average percentage correct at N (P@N), which measures how many of the test instances were ranked within the top N highest ranked nearest neighbors. A chance baseline is obtained by randomly ranking a concept’s nearest neighbors. We use partial least squares regression (PLSR) to </context>
</contexts>
<marker>Socher, Karpathy, Le, Manning, Ng, 2014</marker>
<rawString>Richard Socher, Andrej Karpathy, Quoc V. Le, Christopher D. Manning, and Andrew Y. Ng. 2014. Grounded compositional semantics for finding and describing images with sentences. Transactions of ACL, 2:207–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From Frequency to Meaning: vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artifical Intelligence Research,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="938" citStr="Turney and Pantel, 2010" startWordPosition="131" endWordPosition="134">tic representations in olfactory (smell) data, through the construction of a novel bag of chemical compounds model. We use standard evaluations for multi-modal semantics, including measuring conceptual similarity and cross-modal zero-shot learning. To our knowledge, this is the first work to evaluate semantic similarity on representations grounded in olfactory data. 1 Introduction Distributional semantics represents the meanings of words as vectors in a “semantic space”, relying on the distributional hypothesis: the idea that words that occur in similar contexts tend to have similar meanings (Turney and Pantel, 2010; Clark, 2015). Although these models have been successful, the fact that the meaning of a word is represented as a distribution over other words implies they suffer from the grounding problem (Harnad, 1990); i.e. they do not account for the fact that human semantic knowledge is grounded in physical reality and sensori-motor experience (Louwerse, 2008). Multi-modal semantics attempts to address this issue and there has been a surge of recent work on perceptually grounded semantic models. These models learn semantic representations from both textual and perceptual input and outperform language-</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From Frequency to Meaning: vector space models of semantics. Journal of Artifical Intelligence Research, 37(1):141–188, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manuel Zarzo</author>
<author>David T Stanton</author>
</authors>
<title>Identification of latent variables in a semantic odor profile database using principal component analysis.</title>
<date>2006</date>
<journal>Chemical Senses,</journal>
<volume>31</volume>
<issue>8</issue>
<contexts>
<context position="9212" citStr="Zarzo and Stanton, 2006" startWordPosition="1424" endWordPosition="1427">oak wood 0.84 daisy violet 0.76 daffodil rose 0.74 SimLex-999 sim 232 Chemical Compound Melon ✓ ✓ Pineapple ✓ ✓ Licorice ✓ Anise ✓ ✓ Beer ✓ ✓ Table 2: A BoCC model. space, over the entire dataset, as well as on the relevant olfactory subsets. When projecting into such a space and reporting results, the model is prefixed with an arrow (→) in the corresponding table. 3 Olfactory Perception The Sigma-Aldrich Fine Chemicals flavors and fragrances catalog3 (henceforth SAFC) is one of the largest publicly accessible databases of semantic odor profiles that is used extensively in fragrance research (Zarzo and Stanton, 2006). It contains organoleptic labels and the chemical compounds—or more accurately the perfume raw materials (PRMs)—that produce them. By automatically scraping the catalog we obtained a total of 137 organoleptic smell labels from SAFC, with a total of 11,152 associated PRMs. We also experimented with Flavornet4 and the LRI and odour database5, but found that the data from these were more noisy and generally of lower quality. For each of the smell labels in SAFC we count the co-occurrences of associated chemical compounds, yielding a bag of chemical compounds (BoCC) model. Table 2 shows an exampl</context>
<context position="10977" citStr="Zarzo and Stanton, 2006" startWordPosition="1686" endWordPosition="1689">r of dimensions. Dataset Linguistic BoCC-Raw BoCC-SVD OMEN (35) 0.40 0.42 0.53 Table 3: Comparison of olfactory representations on the covered OMEN dataset. vectors. We call the model without any dimensionality reduction BOCC-RAW and use singular value decomposition (SVD) to create an additional BOCC-SVD model with reduced dimensionality. Positive pointwise mutual information (PPMI) weighting is applied to the raw space before performing dimensionality reduction. The number of dimensions in human olfactory space is a hotly debated topic in the olfactory chemical sciences (Buck and Axel, 1991; Zarzo and Stanton, 2006). Recent studies involving multi-dimensional scaling on the SAFC catalog revealed approximately 32 dimensions in olfactory perception space (Mamlouk et al., 2003; Mamlouk and Martinetz, 2004). We examine this finding by evaluating the Spearman ρs correlation on the pairs of OMEN that occur in the SAFC database (35 pairs). The coverage on SimLex was not sufficient to also try that dataset (only 5 pairs). Figure 1 shows the results. It turns out that the best olfactory representations are obtained with 30 dimensions. In other words, our findings appear to corroborate recent evidence suggesting t</context>
</contexts>
<marker>Zarzo, Stanton, 2006</marker>
<rawString>Manuel Zarzo and David T. Stanton. 2006. Identification of latent variables in a semantic odor profile database using principal component analysis. Chemical Senses, 31(8):713–724.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>