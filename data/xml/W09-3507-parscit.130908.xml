<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014437">
<title confidence="0.999674">
A Language-Independent Transliteration Schema Using Character
Aligned Models At NEWS 2009
</title>
<author confidence="0.986222">
Praneeth Shishtla, Surya Ganesh V, Sethuramalingam Subramaniam, Vasudeva Varma
</author>
<affiliation confidence="0.837677">
Language Technologies Research Centre,
IIIT-Hyderabad, India
</affiliation>
<email confidence="0.973222">
praneethms@students.iiit.ac.in
{suryag,sethu}@research.iiit.ac.in, vv@iiit.ac.in
</email>
<sectionHeader confidence="0.993867" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999403666666667">
In this paper we present a statistical
transliteration technique that is language
independent. This technique uses statis-
tical alignment models and Conditional
Random Fields (CRF). Statistical align-
ment models maximizes the probability of
the observed (source, target) word pairs
using the expectation maximization algo-
rithm and then the character level align-
ments are set to maximum posterior pre-
dictions of the model. CRF has efficient
training and decoding processes which is
conditioned on both source and target lan-
guages and produces globally optimal so-
lution.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999769789473684">
A significant portion of out-of-vocabulary (OOV)
words in machine translation systems, information
extraction and cross language retrieval models are
named entities (NEs). If the languages are written
in different scripts, these named entities must be
transliterated. Transliteration is defined as the pro-
cess of obtaining the phonetic translation of names
across languages. A source language word can
have more than one valid transliteration in the tar-
get language. In areas like Cross Language Infor-
mation Retrieval (CLIR), it is important to gener-
ate all possible transliterations of a Named Entity.
Most current transliteration systems use a gen-
erative model for transliteration such as freely
available GIZA++1 (Och and Ney , 2000), an
implementation of the IBM alignment mod-
els (Brown et al., 1993) and HMM alignment
model. These systems use GIZA++ to get charac-
ter level alignments from word aligned data. The
</bodyText>
<footnote confidence="0.880073">
1http://www.fjoch.com/GIZA++.html
</footnote>
<bodyText confidence="0.999006909090909">
transliteration system (Nasreen and Larkey , 2003)
is built by counting up the alignments and convert-
ing the counts to conditional probabilities.
In this paper, we describe our participation
in NEWS 2009 Machine Transliteration Shared
Task (Li et al., 2009). We present a simple statis-
tical, language independent technique which uses
statistical alignment models and Conditional Ran-
dom Fields (CRFs) (Hanna , 2004). Using this
technique a desired number of transliterations are
generated for a given word.
</bodyText>
<sectionHeader confidence="0.992183" genericHeader="method">
2 Previous work
</sectionHeader>
<bodyText confidence="0.999756384615385">
One of the works on Transliteration is done by
Arababi et al. (Arababi et. al., 1994). They
model forward transliteration through a combina-
tion of neural net and expert systems. Work in
the field of Indian Language CLIR was done by
Jaleel and Larkey (Larkey et al., 2003). They
did this based on their work in English-Arabic
transliteration for CLIR (Nasreen and Larkey ,
2003). Their approach was based on HMM us-
ing GIZA++ (Och and Ney , 2000). Prior work in
Arabic-English transliteration for machine trans-
lation purpose was done by Arababi (Arbabi et al.,
1994). They developed a hybrid neural network
and knowledge-based system to generate multi-
ple English spellings for Arabic person names.
Knight and Graehl (Knight and Graehl , 1997) de-
veloped a five stage statistical model to do back
transliteration, that is, recover the original En-
glish name from its transliteration into Japanese
Katakana. Stalls and Knight (Stalls and Knight,
1998) adapted this approach for back translitera-
tion from Arabic to English of English names. Al-
Onaizan and Knight (Onaizan and Knight, 2002)
have produced a simpler Arabic/English translit-
erator and evaluates how well their system can
match a source spelling. Their work includes an
</bodyText>
<page confidence="0.987357">
40
</page>
<note confidence="0.9836765">
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 40–43,
Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999849333333333">
evaluation of the transliterations in terms of their
reasonableness according to human judges. None
of these studies measures their performance on a
retrieval task or on other NLP tasks. Fujii and
Ishikawa (Fujii and Ishikawa , 2001) describe a
transliteration system for English-Japanese CLIR
that requires some linguistic knowledge. They
evaluate the effectiveness of their system on an
English-Japanese CLIR task.
</bodyText>
<sectionHeader confidence="0.983989" genericHeader="method">
3 Problem Description
</sectionHeader>
<bodyText confidence="0.9999961">
The problem can be stated formally as a se-
quence labeling problem from one language al-
phabet to other. Consider a source language word
x1x2..xi..xN where each xi is treated as a word
in the observation sequence. Let the equivalent
target language orthography of the same word be
y1y2..yi..yN where each yi is treated as a label in
the label sequence. The task here is to generate a
valid target language word (label sequence) for the
source language word (observation sequence).
</bodyText>
<equation confidence="0.999567166666667">
x1 y1
x2 y2
. - .
. - .
. - .
xN yN
</equation>
<bodyText confidence="0.998881333333333">
Here the valid target language alphabet (yi) for a
source language alphabet (xi) in the input source
language word may depend on various factors like
</bodyText>
<listItem confidence="0.934103714285714">
1. The source language alphabet in the input
word.
2. The context (alphabets) surrounding source
language alphabet (xi) in the input word.
3. The context (alphabets) surrounding target
language alphabet (yi) in the desired output
word.
</listItem>
<sectionHeader confidence="0.738751" genericHeader="method">
4 Transliteration using alignment models
and CRF
</sectionHeader>
<bodyText confidence="0.999947333333333">
Our approach for transliteration is divided
into two phases. The first phase induces
character alignments over a word-aligned
bilingual corpus, and the second phase uses
some statistics over the alignments to translit-
erate the source language word and generate
the desired number of target language words.
The selected statistical model for transliteration
is based on a combination of statistical alignment
models and CRF. The alignment models maximize
the probability of the observed (source, target)
word pairs using the expectation maximization
algorithm. After the maximization process is
complete, the character level alignments are
set to maximum posterior predictions of the
model. This alignment is used to get character
level alignment of source and target language
words. From the character level alignment
obtained we compare each source language
character to a word and its corresponding tar-
get language character to a label. Conditional
random fields (CRFs) are a probabilistic frame-
work for labeling and segmenting sequential
data. We use CRF to generate target language
word (similar to label sequence) from source
language word (similar to observation sequence).
CRFs are undirected graphical models which
define a conditional distribution over a label se-
quence given an observation sequence. We define
CRFs as conditional probability distributions
P(Y |X) of target language words given source
language words. The probability of a particular
target language word Y given source language
</bodyText>
<equation confidence="0.7135815">
word X is the normalized product of potential
functions each of the form
(Ej λjtj(Yi−1,Yi,X,i))+(E k µksk(Yi,X,i))
e
</equation>
<bodyText confidence="0.99990325">
where tj(Yi−1, Yi, X, i) is a transition feature
function of the entire source language word and
the target language characters at positions i and
i − 1 in the target language word; sk(Yi, X, i) is a
state feature function of the target language word
at position i and the source language word; and λj
and µk are parameters to be estimated from train-
ing data.
</bodyText>
<equation confidence="0.987392333333333">
n
Fj(Y, X) _ fj(Yi−1, Yi, X, i)
i=1
</equation>
<bodyText confidence="0.99805">
where each fj(Yi−1, Yi, X, i) is either a state
function s(Yi−1, Yi, X, i) or a transition function
t(Yi−1, Yi, X, i). This allows the probability of a
target language word Y given a source language
word X to be written as
</bodyText>
<equation confidence="0.877765333333333">
P(Y |X,λ) _ ( 1
Z(X))e(EλjFj(Y,X))
Z(X) is a normalization factor.
</equation>
<page confidence="0.996616">
41
</page>
<sectionHeader confidence="0.987632" genericHeader="method">
5 Our Transliteration system
</sectionHeader>
<bodyText confidence="0.9996773125">
The whole model has three important phases. Two
of them are off-line processes and the other is a on-
line process. The two off-line phases are prepro-
cessing the parallel corpora and training the model
using CRF++2 (Lafferty et al., 2001). CRF++ is a
simple, customizable, and open source implemen-
tation of Conditional Random Fields (CRFs) for
segmenting/labeling sequential data. The on-line
phase involves generating desired number of target
language transliterations (UTF-8 encoded) for the
given English input word. In our case, the source
is always an English word. The same system is
used for every language pair which makes it a lan-
guage independent. The target languages consist
of Chinese, Hindi, Kannada Tamil and Russian
words.
</bodyText>
<subsectionHeader confidence="0.984555">
5.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.996106333333333">
The training file is converted into a format re-
quired by CRF++. The sequence of steps in pre-
processing are
</bodyText>
<listItem confidence="0.980014076923077">
1. Both source and target language words were
prefixed with a begin symbol B and suffixed
with an end symbol E which correspond to
start and end states. English words were con-
verted to lower case.
2. The training words were segmented in to
unigrams and the source-target word pairs
were aligned using GIZA++ (IBM model1,
HMM alignment model, IBM model3 and
IBM model4).
3. The alignment consist of NULLs on source
language i.e., a target language unigram is
aligned to NULL on the source language.
</listItem>
<bodyText confidence="0.762957333333333">
These NULLs are problematic during on-
line phase (as positions of NULLs are un-
known). So, these NULLs are removed by
appending the target language unigram to the
unigram of its previous alignment. For exam-
ple, the following alignment,
</bodyText>
<equation confidence="0.9126894">
k − K
NULL − A
transforms to -
k − KA
2http://crfpp.sourceforge.net/
</equation>
<bodyText confidence="0.838463875">
So, in the final alignment, the source side al-
ways contains unigrams and the target side
might contain ngrams which depends on al-
phabet size of the languages. These three
steps are performed to get the character level
alignment for each source and target lan-
guage training words.
4. This final alignment is transformed to train-
ing format as required by CRF++ to work.
In the training format, a source language un-
igram aligned to a target language ngram is
called a token. Each token must be repre-
sented in one line, with the columns sepa-
rated by white space (spaces or tabular char-
acters). Each token should have equal num-
ber of columns.
</bodyText>
<subsectionHeader confidence="0.999672">
5.2 Training Phase
</subsectionHeader>
<bodyText confidence="0.999990181818182">
The preprocessing phase converts the corpus into
CRF++ input file format. This file is used to
train the CRF model. The training requires a tem-
plate file which specifies the features to be selected
by the model. The training is done using Lim-
ited memory Broyden-Fletcher-Goldfarb-Shannon
method (L-BFGS) (Liu and Nocedal, 1989) which
uses quasi-newton algorithm for large scale nu-
merical optimization problem. We used English
characters as features for our model and a window
size of 5.
</bodyText>
<subsectionHeader confidence="0.99598">
5.3 Transliteration
</subsectionHeader>
<bodyText confidence="0.999977875">
For a language pair, the list of English words that
need to be transliterated is taken. These words are
converted into CRF++ test file format and translit-
erated using the trained model which gives the top
n probable English words. CRF++ uses forward
Viterbi and backward A* search whose combina-
tion produces the exact n-best results. This process
is repeated for all the five language pairs.
</bodyText>
<sectionHeader confidence="0.999847" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.9998631">
In this section, we present the results of our par-
ticipation in the NEWS-2009 shared task. We
conducted our experiments on five language pairs
namely English-Chinese (Li et al., 2004), English-
{Hindi, Kannada, Tamil, Russian} (Kumaran and
Kellner, 2007). As specified in NEWS 2009 Ma-
chine Transliteration Shared Task (Li et al., 2009),
we submitted our standard runs on all the five lan-
guage pairs. Table 1 shows the results of our sys-
tem.
</bodyText>
<page confidence="0.997531">
42
</page>
<table confidence="0.999299333333333">
Language Pair Accuracy in top-1 Mean F-score MRR MAP,�f MAPio MAPsys
English-Tamil 0.406 0.894 0.542 0.399 0.193 0.193
English-Hindi 0.407 0.877 0.544 0.402 0.195 0.195
English-Russian 0.548 0.916 0.640 0.548 0.210 0.210
English-Chinese 0.493 0.804 0.600 0.493 0.192 0.192
English-Kannada 0.350 0.864 0.482 0.344 0.175 0.175
</table>
<tableCaption confidence="0.999782">
Table 1: Transliteration results for the language pairs
</tableCaption>
<sectionHeader confidence="0.99448" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999897">
In this paper, we have described our translitera-
tion system build on a discriminative model using
CRF and statistical alignment models. As men-
tioned earlier, our system is language independent
and works on any language pair provided parallel
word lists are available for training in the particu-
lar language pair. The main advantage of our sys-
tem is that we use no language-specific heuristics
in any of our modules and hence it is extensible to
any language-pair with least effort.
</bodyText>
<sectionHeader confidence="0.997626" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.986829328571429">
A. Kumaran, Tobias Kellner. 2007. A generic frame-
work for machine transliteration, Proc. of the 30th
SIGIR.
A. L. Berger. 1997. The improved iterative scaling
algorithm: A gentle introduction.
Arbabi, M. and Fischthal, S. M. and Cheng, V. C. and
Bart, E. 1994. Algorithms for Arabic name translit-
eration, IBM Journal of Research And Development.
Al-Onaizan Y, Knight K. 2002. Machine translation of
names in Arabic text. Proceedings of the ACL con-
ference workshop on computational approaches to
Semitic languages.
Arababi Mansur, Scott M. Fischthal, Vincent C. Cheng,
and Elizabeth Bar. 1994. Algorithms for Arabic
name transliteration. IBM Journal of research and
Development.
D. C. Liu and J. Nocedal. 1989. On the limited memory
BFGS method for large-scale optimization, Math.
Programming 45 (1989), pp. 503–528.
Fujii Atsushi and Tetsuya Ishikawa. 2001.
Japanese/English Cross-Language Information
Retrieval: Exploration of Query Translation and
Transliteration. Computers and the Humanities,
Vol.35, No.4, pp.389-420.
H. M. Wallach. 2002. Efficient training of conditional
random fields. Masters thesis, University of Edin-
burgh.
Hanna M. Wallach. 2004. Conditional Random Fields:
An Introduction.
Haizhou Li, A Kumaran, Min Zhang, Vladimir Pervou-
chine. 2009. Whitepaper of NEWS 2009 Machine
Transliteration Shared Task. Proceedings of ACL-
IJCNLP 2009 Named Entities Workshop (NEWS
2009), Singapore.
Haizhou Li, A Kumaran, Vladimir Pervouchine, Min
Zhang. 2009. Report on NEWS 2009 Machine
Transliteration Shared Task. Proceedings of ACL-
IJCNLP 2009 Named Entities Workshop (NEWS
2009), Singapore.
Haizhou Li, Min Zhang, Jian Su. 2004. A joint source
channel model for machine transliteration. Proc. of
the 42nd ACL.
J. Darroch and D. Ratcliff. 1972. Generalized iterative
scaling for log-linear models. The Annals of Mathe-
matical Statistics, 43:14701480.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of
ICML, pp.282-289.
Knight Kevin and Graehl Jonathan. 1997. Machine
transliteration. In Proceedings of the 35th Annual
Meeting of the Association for Computational Lin-
guistics, pp. 128-135. Morgan Kaufmann.
Larkey, Connell,AbdulJaleel. 2003. Hindi CLIR in
Thirty Days.
Nasreen Abdul Jaleel and Leah S. Larkey. 2003.
Statistical Transliteration for English-Arabic Cross
Language Information Retrieval.
Och Franz Josef and Hermann Ney. 2000. Improved
Statistical Alignment Models. Proc. of the 38th An-
nual Meeting of the Association for Computational
Linguistics, pp. 440-447, Hong Kong, China.
P. F. Brown, S. A. Della Pietra, and R. L. Mercer.
1993. The mathematics of statistical machine trans-
lation: Parameter estimation. Computational Lin-
guistics, 19(2):263-311.
Phil Blunsom and Trevor Cohn. 2006. Discriminative
Word Alignment with Conditional Random Fields.
Stalls Bonnie Glover and Kevin Knight. 1998. Trans-
lating names and technical terms in Arabic text.
</reference>
<page confidence="0.999824">
43
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.071210">
<title confidence="0.6559315">A Language-Independent Transliteration Schema Using Aligned Models At NEWS 2009</title>
<author confidence="0.481325">Praneeth Shishtla</author>
<author confidence="0.481325">Surya Ganesh V</author>
<author confidence="0.481325">Sethuramalingam Subramaniam</author>
<author confidence="0.481325">Vasudeva</author>
<affiliation confidence="0.532974">Language Technologies Research</affiliation>
<email confidence="0.5474285">IIIT-Hyderabad,vv@iiit.ac.in</email>
<abstract confidence="0.984986875">In this paper we present a statistical transliteration technique that is language independent. This technique uses statistical alignment models and Conditional Random Fields (CRF). Statistical alignment models maximizes the probability of the observed (source, target) word pairs using the expectation maximization algorithm and then the character level alignments are set to maximum posterior predictions of the model. CRF has efficient training and decoding processes which is conditioned on both source and target languages and produces globally optimal solution.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Kumaran</author>
<author>Tobias Kellner</author>
</authors>
<title>A generic framework for machine transliteration,</title>
<date>2007</date>
<booktitle>Proc. of the 30th SIGIR.</booktitle>
<contexts>
<context position="10946" citStr="Kumaran and Kellner, 2007" startWordPosition="1742" endWordPosition="1745">t of English words that need to be transliterated is taken. These words are converted into CRF++ test file format and transliterated using the trained model which gives the top n probable English words. CRF++ uses forward Viterbi and backward A* search whose combination produces the exact n-best results. This process is repeated for all the five language pairs. 6 Results In this section, we present the results of our participation in the NEWS-2009 shared task. We conducted our experiments on five language pairs namely English-Chinese (Li et al., 2004), English{Hindi, Kannada, Tamil, Russian} (Kumaran and Kellner, 2007). As specified in NEWS 2009 Machine Transliteration Shared Task (Li et al., 2009), we submitted our standard runs on all the five language pairs. Table 1 shows the results of our system. 42 Language Pair Accuracy in top-1 Mean F-score MRR MAP,�f MAPio MAPsys English-Tamil 0.406 0.894 0.542 0.399 0.193 0.193 English-Hindi 0.407 0.877 0.544 0.402 0.195 0.195 English-Russian 0.548 0.916 0.640 0.548 0.210 0.210 English-Chinese 0.493 0.804 0.600 0.493 0.192 0.192 English-Kannada 0.350 0.864 0.482 0.344 0.175 0.175 Table 1: Transliteration results for the language pairs 7 Conclusion In this paper, w</context>
</contexts>
<marker>Kumaran, Kellner, 2007</marker>
<rawString>A. Kumaran, Tobias Kellner. 2007. A generic framework for machine transliteration, Proc. of the 30th SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Berger</author>
</authors>
<title>The improved iterative scaling algorithm: A gentle introduction.</title>
<date>1997</date>
<marker>Berger, 1997</marker>
<rawString>A. L. Berger. 1997. The improved iterative scaling algorithm: A gentle introduction.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Arbabi</author>
<author>S M Fischthal</author>
<author>V C Cheng</author>
<author>E Bart</author>
</authors>
<title>Algorithms for Arabic name transliteration,</title>
<date>1994</date>
<journal>IBM Journal of Research And Development.</journal>
<contexts>
<context position="2939" citStr="Arbabi et al., 1994" startWordPosition="433" endWordPosition="436">rations are generated for a given word. 2 Previous work One of the works on Transliteration is done by Arababi et al. (Arababi et. al., 1994). They model forward transliteration through a combination of neural net and expert systems. Work in the field of Indian Language CLIR was done by Jaleel and Larkey (Larkey et al., 2003). They did this based on their work in English-Arabic transliteration for CLIR (Nasreen and Larkey , 2003). Their approach was based on HMM using GIZA++ (Och and Ney , 2000). Prior work in Arabic-English transliteration for machine translation purpose was done by Arababi (Arbabi et al., 1994). They developed a hybrid neural network and knowledge-based system to generate multiple English spellings for Arabic person names. Knight and Graehl (Knight and Graehl , 1997) developed a five stage statistical model to do back transliteration, that is, recover the original English name from its transliteration into Japanese Katakana. Stalls and Knight (Stalls and Knight, 1998) adapted this approach for back transliteration from Arabic to English of English names. AlOnaizan and Knight (Onaizan and Knight, 2002) have produced a simpler Arabic/English transliterator and evaluates how well their</context>
</contexts>
<marker>Arbabi, Fischthal, Cheng, Bart, 1994</marker>
<rawString>Arbabi, M. and Fischthal, S. M. and Cheng, V. C. and Bart, E. 1994. Algorithms for Arabic name transliteration, IBM Journal of Research And Development.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Al-Onaizan</author>
<author>K Knight</author>
</authors>
<title>Machine translation of names in Arabic text.</title>
<date>2002</date>
<booktitle>Proceedings of the ACL conference workshop on computational approaches to Semitic languages.</booktitle>
<marker>Al-Onaizan, Knight, 2002</marker>
<rawString>Al-Onaizan Y, Knight K. 2002. Machine translation of names in Arabic text. Proceedings of the ACL conference workshop on computational approaches to Semitic languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arababi Mansur</author>
<author>Scott M Fischthal</author>
<author>Vincent C Cheng</author>
<author>Elizabeth Bar</author>
</authors>
<title>Algorithms for Arabic name transliteration.</title>
<date>1994</date>
<journal>IBM Journal of research and Development.</journal>
<marker>Mansur, Fischthal, Cheng, Bar, 1994</marker>
<rawString>Arababi Mansur, Scott M. Fischthal, Vincent C. Cheng, and Elizabeth Bar. 1994. Algorithms for Arabic name transliteration. IBM Journal of research and Development.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Liu</author>
<author>J Nocedal</author>
</authors>
<title>On the limited memory BFGS method for large-scale optimization,</title>
<date>1989</date>
<journal>Math. Programming</journal>
<volume>45</volume>
<pages>503--528</pages>
<contexts>
<context position="10112" citStr="Liu and Nocedal, 1989" startWordPosition="1607" endWordPosition="1610">work. In the training format, a source language unigram aligned to a target language ngram is called a token. Each token must be represented in one line, with the columns separated by white space (spaces or tabular characters). Each token should have equal number of columns. 5.2 Training Phase The preprocessing phase converts the corpus into CRF++ input file format. This file is used to train the CRF model. The training requires a template file which specifies the features to be selected by the model. The training is done using Limited memory Broyden-Fletcher-Goldfarb-Shannon method (L-BFGS) (Liu and Nocedal, 1989) which uses quasi-newton algorithm for large scale numerical optimization problem. We used English characters as features for our model and a window size of 5. 5.3 Transliteration For a language pair, the list of English words that need to be transliterated is taken. These words are converted into CRF++ test file format and transliterated using the trained model which gives the top n probable English words. CRF++ uses forward Viterbi and backward A* search whose combination produces the exact n-best results. This process is repeated for all the five language pairs. 6 Results In this section, w</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>D. C. Liu and J. Nocedal. 1989. On the limited memory BFGS method for large-scale optimization, Math. Programming 45 (1989), pp. 503–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fujii Atsushi</author>
<author>Tetsuya Ishikawa</author>
</authors>
<title>Japanese/English Cross-Language Information Retrieval: Exploration of Query Translation and Transliteration. Computers and the Humanities,</title>
<date>2001</date>
<pages>389--420</pages>
<location>Vol.35, No.4,</location>
<marker>Atsushi, Ishikawa, 2001</marker>
<rawString>Fujii Atsushi and Tetsuya Ishikawa. 2001. Japanese/English Cross-Language Information Retrieval: Exploration of Query Translation and Transliteration. Computers and the Humanities, Vol.35, No.4, pp.389-420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H M Wallach</author>
</authors>
<title>Efficient training of conditional random fields. Masters thesis,</title>
<date>2002</date>
<institution>University of Edinburgh.</institution>
<marker>Wallach, 2002</marker>
<rawString>H. M. Wallach. 2002. Efficient training of conditional random fields. Masters thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna M Wallach</author>
</authors>
<title>Conditional Random Fields: An Introduction.</title>
<date>2004</date>
<marker>Wallach, 2004</marker>
<rawString>Hanna M. Wallach. 2004. Conditional Random Fields: An Introduction.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>A Kumaran</author>
<author>Min Zhang</author>
<author>Vladimir Pervouchine</author>
</authors>
<date>2009</date>
<booktitle>Whitepaper of NEWS 2009 Machine Transliteration Shared Task. Proceedings of ACLIJCNLP 2009 Named Entities Workshop (NEWS</booktitle>
<contexts>
<context position="2110" citStr="Li et al., 2009" startWordPosition="296" endWordPosition="299">ed Entity. Most current transliteration systems use a generative model for transliteration such as freely available GIZA++1 (Och and Ney , 2000), an implementation of the IBM alignment models (Brown et al., 1993) and HMM alignment model. These systems use GIZA++ to get character level alignments from word aligned data. The 1http://www.fjoch.com/GIZA++.html transliteration system (Nasreen and Larkey , 2003) is built by counting up the alignments and converting the counts to conditional probabilities. In this paper, we describe our participation in NEWS 2009 Machine Transliteration Shared Task (Li et al., 2009). We present a simple statistical, language independent technique which uses statistical alignment models and Conditional Random Fields (CRFs) (Hanna , 2004). Using this technique a desired number of transliterations are generated for a given word. 2 Previous work One of the works on Transliteration is done by Arababi et al. (Arababi et. al., 1994). They model forward transliteration through a combination of neural net and expert systems. Work in the field of Indian Language CLIR was done by Jaleel and Larkey (Larkey et al., 2003). They did this based on their work in English-Arabic transliter</context>
<context position="11027" citStr="Li et al., 2009" startWordPosition="1756" endWordPosition="1759">CRF++ test file format and transliterated using the trained model which gives the top n probable English words. CRF++ uses forward Viterbi and backward A* search whose combination produces the exact n-best results. This process is repeated for all the five language pairs. 6 Results In this section, we present the results of our participation in the NEWS-2009 shared task. We conducted our experiments on five language pairs namely English-Chinese (Li et al., 2004), English{Hindi, Kannada, Tamil, Russian} (Kumaran and Kellner, 2007). As specified in NEWS 2009 Machine Transliteration Shared Task (Li et al., 2009), we submitted our standard runs on all the five language pairs. Table 1 shows the results of our system. 42 Language Pair Accuracy in top-1 Mean F-score MRR MAP,�f MAPio MAPsys English-Tamil 0.406 0.894 0.542 0.399 0.193 0.193 English-Hindi 0.407 0.877 0.544 0.402 0.195 0.195 English-Russian 0.548 0.916 0.640 0.548 0.210 0.210 English-Chinese 0.493 0.804 0.600 0.493 0.192 0.192 English-Kannada 0.350 0.864 0.482 0.344 0.175 0.175 Table 1: Transliteration results for the language pairs 7 Conclusion In this paper, we have described our transliteration system build on a discriminative model using</context>
</contexts>
<marker>Li, Kumaran, Zhang, Pervouchine, 2009</marker>
<rawString>Haizhou Li, A Kumaran, Min Zhang, Vladimir Pervouchine. 2009. Whitepaper of NEWS 2009 Machine Transliteration Shared Task. Proceedings of ACLIJCNLP 2009 Named Entities Workshop (NEWS 2009), Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>A Kumaran</author>
<author>Vladimir Pervouchine</author>
<author>Min Zhang</author>
</authors>
<date>2009</date>
<booktitle>Report on NEWS 2009 Machine Transliteration Shared Task. Proceedings of ACLIJCNLP 2009 Named Entities Workshop (NEWS</booktitle>
<contexts>
<context position="2110" citStr="Li et al., 2009" startWordPosition="296" endWordPosition="299">ed Entity. Most current transliteration systems use a generative model for transliteration such as freely available GIZA++1 (Och and Ney , 2000), an implementation of the IBM alignment models (Brown et al., 1993) and HMM alignment model. These systems use GIZA++ to get character level alignments from word aligned data. The 1http://www.fjoch.com/GIZA++.html transliteration system (Nasreen and Larkey , 2003) is built by counting up the alignments and converting the counts to conditional probabilities. In this paper, we describe our participation in NEWS 2009 Machine Transliteration Shared Task (Li et al., 2009). We present a simple statistical, language independent technique which uses statistical alignment models and Conditional Random Fields (CRFs) (Hanna , 2004). Using this technique a desired number of transliterations are generated for a given word. 2 Previous work One of the works on Transliteration is done by Arababi et al. (Arababi et. al., 1994). They model forward transliteration through a combination of neural net and expert systems. Work in the field of Indian Language CLIR was done by Jaleel and Larkey (Larkey et al., 2003). They did this based on their work in English-Arabic transliter</context>
<context position="11027" citStr="Li et al., 2009" startWordPosition="1756" endWordPosition="1759">CRF++ test file format and transliterated using the trained model which gives the top n probable English words. CRF++ uses forward Viterbi and backward A* search whose combination produces the exact n-best results. This process is repeated for all the five language pairs. 6 Results In this section, we present the results of our participation in the NEWS-2009 shared task. We conducted our experiments on five language pairs namely English-Chinese (Li et al., 2004), English{Hindi, Kannada, Tamil, Russian} (Kumaran and Kellner, 2007). As specified in NEWS 2009 Machine Transliteration Shared Task (Li et al., 2009), we submitted our standard runs on all the five language pairs. Table 1 shows the results of our system. 42 Language Pair Accuracy in top-1 Mean F-score MRR MAP,�f MAPio MAPsys English-Tamil 0.406 0.894 0.542 0.399 0.193 0.193 English-Hindi 0.407 0.877 0.544 0.402 0.195 0.195 English-Russian 0.548 0.916 0.640 0.548 0.210 0.210 English-Chinese 0.493 0.804 0.600 0.493 0.192 0.192 English-Kannada 0.350 0.864 0.482 0.344 0.175 0.175 Table 1: Transliteration results for the language pairs 7 Conclusion In this paper, we have described our transliteration system build on a discriminative model using</context>
</contexts>
<marker>Li, Kumaran, Pervouchine, Zhang, 2009</marker>
<rawString>Haizhou Li, A Kumaran, Vladimir Pervouchine, Min Zhang. 2009. Report on NEWS 2009 Machine Transliteration Shared Task. Proceedings of ACLIJCNLP 2009 Named Entities Workshop (NEWS 2009), Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>Min Zhang</author>
<author>Jian Su</author>
</authors>
<title>A joint source channel model for machine transliteration.</title>
<date>2004</date>
<booktitle>Proc. of the 42nd ACL.</booktitle>
<contexts>
<context position="10877" citStr="Li et al., 2004" startWordPosition="1733" endWordPosition="1736">size of 5. 5.3 Transliteration For a language pair, the list of English words that need to be transliterated is taken. These words are converted into CRF++ test file format and transliterated using the trained model which gives the top n probable English words. CRF++ uses forward Viterbi and backward A* search whose combination produces the exact n-best results. This process is repeated for all the five language pairs. 6 Results In this section, we present the results of our participation in the NEWS-2009 shared task. We conducted our experiments on five language pairs namely English-Chinese (Li et al., 2004), English{Hindi, Kannada, Tamil, Russian} (Kumaran and Kellner, 2007). As specified in NEWS 2009 Machine Transliteration Shared Task (Li et al., 2009), we submitted our standard runs on all the five language pairs. Table 1 shows the results of our system. 42 Language Pair Accuracy in top-1 Mean F-score MRR MAP,�f MAPio MAPsys English-Tamil 0.406 0.894 0.542 0.399 0.193 0.193 English-Hindi 0.407 0.877 0.544 0.402 0.195 0.195 English-Russian 0.548 0.916 0.640 0.548 0.210 0.210 English-Chinese 0.493 0.804 0.600 0.493 0.192 0.192 English-Kannada 0.350 0.864 0.482 0.344 0.175 0.175 Table 1: Transli</context>
</contexts>
<marker>Li, Zhang, Su, 2004</marker>
<rawString>Haizhou Li, Min Zhang, Jian Su. 2004. A joint source channel model for machine transliteration. Proc. of the 42nd ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Darroch</author>
<author>D Ratcliff</author>
</authors>
<title>Generalized iterative scaling for log-linear models.</title>
<date>1972</date>
<booktitle>The Annals of Mathematical Statistics,</booktitle>
<pages>43--14701480</pages>
<marker>Darroch, Ratcliff, 1972</marker>
<rawString>J. Darroch and D. Ratcliff. 1972. Generalized iterative scaling for log-linear models. The Annals of Mathematical Statistics, 43:14701480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proc. of ICML,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="7701" citStr="Lafferty et al., 2001" startWordPosition="1202" endWordPosition="1205">timated from training data. n Fj(Y, X) _ fj(Yi−1, Yi, X, i) i=1 where each fj(Yi−1, Yi, X, i) is either a state function s(Yi−1, Yi, X, i) or a transition function t(Yi−1, Yi, X, i). This allows the probability of a target language word Y given a source language word X to be written as P(Y |X,λ) _ ( 1 Z(X))e(EλjFj(Y,X)) Z(X) is a normalization factor. 41 5 Our Transliteration system The whole model has three important phases. Two of them are off-line processes and the other is a online process. The two off-line phases are preprocessing the parallel corpora and training the model using CRF++2 (Lafferty et al., 2001). CRF++ is a simple, customizable, and open source implementation of Conditional Random Fields (CRFs) for segmenting/labeling sequential data. The on-line phase involves generating desired number of target language transliterations (UTF-8 encoded) for the given English input word. In our case, the source is always an English word. The same system is used for every language pair which makes it a language independent. The target languages consist of Chinese, Hindi, Kannada Tamil and Russian words. 5.1 Preprocessing The training file is converted into a format required by CRF++. The sequence of s</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. of ICML, pp.282-289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Knight Kevin</author>
<author>Graehl Jonathan</author>
</authors>
<title>Machine transliteration.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>128--135</pages>
<publisher>Morgan Kaufmann.</publisher>
<marker>Kevin, Jonathan, 1997</marker>
<rawString>Knight Kevin and Graehl Jonathan. 1997. Machine transliteration. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, pp. 128-135. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Connell Larkey</author>
<author>AbdulJaleel</author>
</authors>
<date>2003</date>
<note>Hindi CLIR in Thirty Days.</note>
<marker>Larkey, AbdulJaleel, 2003</marker>
<rawString>Larkey, Connell,AbdulJaleel. 2003. Hindi CLIR in Thirty Days.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nasreen Abdul Jaleel</author>
<author>Leah S Larkey</author>
</authors>
<title>Statistical Transliteration for English-Arabic Cross Language Information Retrieval.</title>
<date>2003</date>
<marker>Jaleel, Larkey, 2003</marker>
<rawString>Nasreen Abdul Jaleel and Leah S. Larkey. 2003. Statistical Transliteration for English-Arabic Cross Language Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Och Franz Josef</author>
<author>Hermann Ney</author>
</authors>
<title>Improved Statistical Alignment Models.</title>
<date>2000</date>
<booktitle>Proc. of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>440--447</pages>
<location>Hong Kong, China.</location>
<marker>Josef, Ney, 2000</marker>
<rawString>Och Franz Josef and Hermann Ney. 2000. Improved Statistical Alignment Models. Proc. of the 38th Annual Meeting of the Association for Computational Linguistics, pp. 440-447, Hong Kong, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<contexts>
<context position="1706" citStr="Brown et al., 1993" startWordPosition="236" endWordPosition="239">ges are written in different scripts, these named entities must be transliterated. Transliteration is defined as the process of obtaining the phonetic translation of names across languages. A source language word can have more than one valid transliteration in the target language. In areas like Cross Language Information Retrieval (CLIR), it is important to generate all possible transliterations of a Named Entity. Most current transliteration systems use a generative model for transliteration such as freely available GIZA++1 (Och and Ney , 2000), an implementation of the IBM alignment models (Brown et al., 1993) and HMM alignment model. These systems use GIZA++ to get character level alignments from word aligned data. The 1http://www.fjoch.com/GIZA++.html transliteration system (Nasreen and Larkey , 2003) is built by counting up the alignments and converting the counts to conditional probabilities. In this paper, we describe our participation in NEWS 2009 Machine Transliteration Shared Task (Li et al., 2009). We present a simple statistical, language independent technique which uses statistical alignment models and Conditional Random Fields (CRFs) (Hanna , 2004). Using this technique a desired number</context>
</contexts>
<marker>Brown, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, S. A. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
</authors>
<title>Discriminative Word Alignment with Conditional Random Fields.</title>
<date>2006</date>
<marker>Blunsom, Cohn, 2006</marker>
<rawString>Phil Blunsom and Trevor Cohn. 2006. Discriminative Word Alignment with Conditional Random Fields.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stalls Bonnie Glover</author>
<author>Kevin Knight</author>
</authors>
<title>Translating names and technical terms in Arabic text.</title>
<date>1998</date>
<marker>Glover, Knight, 1998</marker>
<rawString>Stalls Bonnie Glover and Kevin Knight. 1998. Translating names and technical terms in Arabic text.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>