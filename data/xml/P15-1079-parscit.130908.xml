<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<note confidence="0.5881095">
String-to-Tree Multi Bottom-up Tree Transducers
Nina Seemann and Fabienne Braune and Andreas Maletti
Institute for Natural Language Processing, University of Stuttgart
Pfaffenwaldring 5b, 70569 Stuttgart, Germany
</note>
<email confidence="0.937447">
{seemanna,braunefe,maletti}@ims.uni-stuttgart.de
</email>
<sectionHeader confidence="0.993412" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999978066666667">
We achieve significant improvements in
several syntax-based machine translation
experiments using a string-to-tree vari-
ant of multi bottom-up tree transducers.
Our new parameterized rule extraction al-
gorithm extracts string-to-tree rules that
can be discontiguous and non-minimal
in contrast to existing algorithms for the
tree-to-tree setting. The obtained models
significantly outperform the string-to-tree
component of the Moses framework in a
large-scale empirical evaluation on several
known translation tasks. Our linguistic
analysis reveals the remarkable benefits of
discontiguous and non-minimal rules.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99993768627451">
We present an application of a variant of local
multi bottom-up tree transducers (eMBOTs) as
proposed in Maletti (2011) to statistical machine
translation. eMBOTs allow discontinuities on the
target language side since they have a sequence
of target tree fragments instead of a single tree
fragment in their rules. The original approach
makes use of syntactic information on both the
source and the target side (tree-to-tree) and a cor-
responding minimal rule extraction is presented
in (Maletti, 2011). Braune et al. (2013) imple-
mented it as well as a decoder inside the Moses
framework (Koehn et al., 2007) and demonstrated
that the resulting tree-to-tree eMBOT system sig-
nificantly improved over its tree-to-tree baseline
using minimal rules. We can see at least two draw-
backs in this approach. First, experiments investi-
gating the integration of syntactic information on
both sides generally report quality deterioration.
For example, Lavie et al. (2008), Liu et al. (2009),
and Chiang (2010) noted that translation quality
tends to decrease in tree-to-tree systems because
the rules become too restrictive. Second, minimal
rules (i.e., rules that cannot be obtained from other
extracted rules) typically consist of a few lexi-
cal items only and are thus not the most suitable
to translate idiomatic expressions and other fixed
phrases. To overcome these drawbacks, we abol-
ish the syntactic information for the source side
and develop a string-to-tree variant of eMBOTs.
In addition, we develop a new rule extraction algo-
rithm that can also extract non-minimal rules. In
general, the number of extractable rules explodes,
so our rule extraction places parameterized restric-
tions on the extracted rules in the same spirit as
in (Chiang, 2007). In this manner, we combine the
advantages of the hierarchical phrase-based ap-
proach on the source side and the tree-based ap-
proach with discontinuiety on the target side.
We evaluate our new system in 3 large-scale ex-
periments using translation tasks, in which we ex-
pect discontinuiety on the target. MBOTs are pow-
erful but asymmetric models since discontinuiety
is available only on the target. We chose to trans-
late from English to German, Arabic, and Chi-
nese. In all experiments our new system signifi-
cantly outperforms the string-to-tree syntax-based
component (Hoang et al., 2009) of Moses. The
(potentially) discontiguous rules of our model are
very useful in these setups, which we confirm in a
quantitative and qualitative analysis.
</bodyText>
<sectionHeader confidence="0.999791" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.9997763">
Modern statistical machine translation sys-
tems (Koehn, 2009) are based on different
translation models. Syntax-based systems have
become widely used because of their ability to
handle non-local reordering and other linguistic
phenomena better than phrase-based models (Och
and Ney, 2004). Synchronous tree substitution
grammars (STSGs) of Eisner (2003) use a single
source and target tree fragment per rule. In con-
trast, an eMBOT rule contains a single source tree
</bodyText>
<page confidence="0.980226">
815
</page>
<note confidence="0.984863333333333">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 815–824,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<figure confidence="0.991781636363636">
VAFIN VP
concludes X → ( , NP ,
ist PP geschlossen
PP
X on X → (NP ,
�
�
¨uber NN
NN NP
human rights → ( ) the X →
Menschenrechte die NN
</figure>
<figureCaption confidence="0.999977">
Figure 1: Several valid rules for our MBOT.
</figureCaption>
<bodyText confidence="0.98260075">
fragment and a sequence of target tree fragments.
eMBOTs can also be understood as a restriction of
the non-contiguous STSSGs of Sun et al. (2009),
which allow a sequence of source tree fragments
and a sequence of target tree fragments. eMBOT
rules require exactly one source tree fragment.
While the mentioned syntax-based models use
tree fragments for source and target (tree-to-tree),
Galley et al. (2004) and Galley et al. (2006) use
syntactic annotations only on the target language
side (string-to-tree). Further research by DeNeefe
et al. (2007) revealed that adding non-minimal
rules improves translation quality in this setting.
Here we improve statistical machine translation
in this setting even further using non-minimal
eMBOT rules.
</bodyText>
<sectionHeader confidence="0.999346" genericHeader="method">
3 Theoretical Model
</sectionHeader>
<bodyText confidence="0.999988090909091">
As our translation model, we use a string-to-tree
variant of the shallow local multi bottom-up tree
transducer of Braune et al. (2013). We will call
our variant MBOT for simplicity. Our MBOT is
a synchronous grammar (Chiang, 2006) similar to
a synchronous context-free grammar (SCFG), but
instead of a single source and target fragment per
rule, our rules are of the form s → (ti, ... , tr,,)
with a single source string s and potentially sev-
eral target tree fragments ti, ... , tr,,. Besides lex-
ical items the source string can contain (several
occurrences of) the placeholder X, which links to
non-lexical leaves in the target tree fragments. In
contrast to an SCFG each placeholder can have
several such links. However, each non-lexical leaf
in a target tree fragment has exactly one such link
to a placeholder X. An MBOT is simply a finite
collection of such rules. Several valid rules are
depicted in Figure 1.
The sentential forms of our MBOTs, which
occur during derivations, have exactly the same
shape as our rules and each rule is a sentential
</bodyText>
<figure confidence="0.530273">
Combined sentential form:
VP
</figure>
<figureCaption confidence="0.999658">
Figure 2: Substitution of sentential forms.
</figureCaption>
<bodyText confidence="0.99970625">
form. We can combine sentential forms with the
help of substitution (Chiang, 2006). Roughly
speaking, in a sentential form ξ we can replace
a placeholder X that is linked (left-to-right) to
non-lexical leaves Cl, ... , Ck in the target tree
fragments by the source string of any sentential
form ζ, whose roots of the target tree fragments
(left-to-right) read Cl, ... , Ck. The target tree
fragments of ζ will replace the respective linked
leaves in the target tree fragments of the sentential
form ξ. In other words, substitution has to respect
the symbols in the linked target tree fragments and
all linked leaves are replaced at the same time. We
illustrate substitution in Figure 2, where we re-
place the placeholder X in the source string, which
is linked to the underlined leaves NP and PP in the
target tree fragments. The rule below (also in Fig-
ure 1) is also a sentential form and matches since
its (underlined) root labels of the target tree frag-
ments read “NP PP”. Thus, we can substitute the
latter sentential form into the former and obtain
the sentential form shown at the bottom of Fig-
ure 2. Ideally, the substitution process is repeated
until the complete source sentence is derived.
</bodyText>
<sectionHeader confidence="0.970598" genericHeader="method">
4 Rule Extraction
</sectionHeader>
<bodyText confidence="0.973790909090909">
The rule extraction of Maletti (2011) extracts min-
imal tree-to-tree rules, which are rules containing
both source and target tree fragments, from sen-
tence pairs of a word-aligned and bi-parsed paral-
lel corpus. In particular, this requires parses for
both the source and the target language sentences
which adds a source for errors and specificity po-
tentially leading to lower translation performance
and lower coverage (Wellington et al., 2006). Chi-
ang (2010) showed that string-to-tree systems—
Matching sentential forms (underlining for emphasis):
</bodyText>
<equation confidence="0.961414533333333">
VAFIN VP
concludes X → (,
NP ,
ist PP geschlossen
PP
�X on X → NP ,
�
¨uber NN
�
VAFIN
concludes X on X → ( , NP ,
ist
�geschlossen
PP
¨uber NN
</equation>
<page confidence="0.963189">
816
</page>
<figure confidence="0.991876894736842">
PROAV[1,1]
damit1
that1 concludes2 the3 debate4 on5 human6 rights7
VAFIN[2,2]
ist2
ART[3,3]
die3
NP[3,4]
Aussprache4
TOP[1,7]
NN[4,4]
APPR[5,5]
¨uber5
PP[5,6]
Menschenrechte6
NN[6,6]
VP[5,7]
geschlossen7
VVPP[7,7]
</figure>
<figureCaption confidence="0.9849615">
Figure 3: Word-aligned sentence pair with target-
side parse.
</figureCaption>
<bodyText confidence="0.991886633333333">
which he calls fuzzy tree-to-tree-systems— gen-
erally yield higher translation quality compared to
corresponding tree-to-tree systems.
For efficiency reasons the rule extraction of
Maletti (2011) only extracts minimal rules, which
are the smallest tree fragments compatible with the
given word alignment and the parse trees. Simi-
larly, non-minimal rules are those that can be ob-
tained from minimal rules by substitution. In par-
ticular, each lexical item of a sentence pair oc-
curs in exactly one minimal rule extracted from
that sentence pair. However, minimal rules are
especially unsuitable for fixed phrases consisting
of rare words because minimal rules encourage
small fragments and thus word-by-word transla-
tion. Consequently, such fixed phrases will often
be assembled inconsistently by substitution from
small fragments. Non-minimal rules encourage a
consistent translation by covering larger parts of
the source sentence.
Here we want to develop an efficient rule ex-
traction procedure for our string-to-tree MBOTs
that avoids the mentioned drawbacks. Natu-
rally, we could substitute minimal rules into each
other to obtain non-minimal rules, but perform-
ing substitution for all combinations is clearly in-
tractable. Instead we essentially follow the ap-
proach of Koehn et al. (2003), Och and Ney
(2004), and Chiang (2007), which is based on con-
sistently aligned phrase pairs. Our training corpus
contains word-aligned sentence pairs (e, A,f),
which contain a source language sentence e, a
target language sentence f, and an alignment
A C_ [1,`e] x [1, `f], where `e and `f are the
lengths of the sentences e and f, respectively, and
[i, i&apos;] = {j E Z  |i &lt; j &lt; i&apos;} is the span (closed
interval of integers) from i to i&apos; for all positive in-
tegers i &lt; i&apos;. Rules are extracted for each pair
of the corpus, so in the following let (e, A, f) be
a word-aligned sentence pair. A source phrase
is simply a span [i, i&apos;] C_ [1, `e] and correspond-
ingly, a target phrase is a span [j, j&apos;] C_ [1, `f].
A rule span is a pair (p, ϕ) consisting of a source
phrase p and a sequence ϕ = p1 · · · pn of (non-
overlapping) target phrases p1, ... , pn. Spans
overlap if their intersection is non-empty. If n = 1
(i.e., there is exactly one target phrase in ϕ) then
(p, ϕ) is also a phrase pair (Koehn et al., 2003).
We want to emphasize that formally phrases are
spans and not the substrings occuring at that span.
Next, we lift the notion of consistently aligned
phrase pairs to our rule spans. Simply put, for
a consistently aligned rule span (p, p1 · · · pn) we
require that it respects the alignment A in the
sense that the origin i of an alignment (i, j) E A
is covered by p if and only if the destination j
is covered by p1, ... , pn. Formally, the rule
span (p, p1 · · · pn) is consistently aligned if for
every (i, j) E A we have i E p if and
only if j E Unk=1 pk. For example, given the
word-aligned sentence pair in Figure 3, the rule
span ([2, 4], [2, 4] [7, 7]) is consistently aligned,
whereas the phrase pair ([2, 4], [2, 7]) is not.
Our MBOTs use rules consisting of a source
string and a sequence of target tree fragments.
The target trees are provided by a parser for the
target language. For each word-aligned sentence
pair (e, A, f) we thus have a parse tree t for f. An
example is provided in Figure 3. We omit a for-
mal definition of trees, but recall that each node η
of the parse tree t governs a (unique) target phrase.
In Figure 3 we have indicated those target phrases
(spans) as subscript to the non-lexical node labels.
A consistently aligned rule span (p, p1 · · · pn) of
(e, A, f) is compatible with t if there exist nodes
η1, ... , ηn of t such that ηk governs pk for all
1 &lt; k &lt; n. For example, given the word-aligned
sentence pair and parse tree t in Figure 3, the con-
sistently aligned rule span ([2, 4], [2, 4] [7, 7]) is
not compatible with t because there is no node in t
that governs [2, 4]. However, for the same data, the
rule span ([2, 4], [2, 2] [3, 4] [7, 7]) is consistently
aligned and compatible with t. The required nodes
of t are labeled VAFIN, NP, VVPP.
Now we are ready to start the rule extrac-
tion. For each consistently aligned rule span
(p, p1 · · · pn) that is compatible with t and each se-
lection of nodes η1, ... , ηn of t such that nk gov-
erns pk for each 1 &lt; k &lt; n, we can extract the
rule e(p) -* (flat(tη1),. . . , flat(tηn)), where
</bodyText>
<page confidence="0.97164">
817
</page>
<figure confidence="0.993198411764706">
Initial rules for
rule span ([3, 3], [3, 3]): rule span ([4,4], [4,4]): rule span ([3, 4], [3, 4]):
�the - ART �die �debate - NN �Aussprache �the debate - NP �die Aussprache
rule span ([5, 7], [5,6]): rule span ([3, 7], [3, 4] [5, 6]):
�on human rights - PP �¨uber Menschenrechte �the debate on human rights - NP PP �
,
¨uber Menschenrechte
die Aussprache
rule span ([2, 2], [2, 2] [7, 7]):
VVPP �
geschlossen
rule span ([2, 4], [2, 2] [3, 4] [7, 7]):
NP
VAFIN
,
ist
die Aussprache
VVPP �
,
geschlossen
�concludes the debate -
VAFIN
�concludes - ist ,
rule span ([2, 7], [2,7]):
NP
VAFIN
,
,
�
ist
die Aussprache
�concludes the debate on human rights -
VP
¨uber Menschenrechte geschlossen
</figure>
<figureCaption confidence="0.999663">
Figure 4: Some initial rules extracted from the word-aligned sentence pair and parse of Figure 3.
</figureCaption>
<listItem confidence="0.997057166666667">
• e(p) is the substring of e at span p,1
• flat(u) removes all internal nodes from u (all
nodes except the root and the leaves), and
• tη is the subtree rooted in q for node q of t.
The rules obtained in this manner are called initial
rules for (e, A, f) and t. For example, for the rule
span ([2, 4], [2, 2] [3, 4] [7, 7]) we can extract only
one initial rule. More precisely, we have
• e([2, 4]) =concludes the debate
• tη1 = (VAFIN ist)
• tη2 = (NP (ART die) (NN Aussprache)),
• and tη3 = (VVPP geschlossen).
</listItem>
<bodyText confidence="0.999859166666667">
The function flat leaves tη1 and tη3 unchanged,
but flat(tη2) = (NP die Aussprache). Thus, we
obtain the boxed rule of Figure 4.
Clearly, the initial rules are just the start be-
cause they are completely lexical in the sense that
they never contain the placeholder X in the source
string nor a non-lexical leaf in any output tree frag-
ment. We introduce non-lexical rules using the
same approach as for the hierarchical rules of Chi-
ang (2007). Roughly speaking, we obtain a new
rule r&apos;&apos; by “excising” an initial rule r from another
rule r&apos; and replacing the removed part by
</bodyText>
<listItem confidence="0.9594822">
• the placeholder X in the source string,
• the root label of the removed tree fragment in
the target tree fragments, and
• linking the removed parts appropriately,
so that the flatted substitution of r into r&apos;&apos; can
</listItem>
<footnote confidence="0.399867">
1If p = [i, i�], then e(p) = e[i, i�] is the substring of e
ranging from the i-th token to the i�-th token.
</footnote>
<figure confidence="0.89296675">
Extractable rule obtained after excision:
NP
the debate X - ( , PP )
die Aussprache
</figure>
<figureCaption confidence="0.9495314">
Figure 5: Excision of the middle initial rule from
the topmost initial rule. Substituting the middle
rule into the result yields the topmost rule.
yield r&apos;. This “excision” process is illustrated in
Figure 5, where we remove the middle initial rule
</figureCaption>
<bodyText confidence="0.655411166666667">
from the topmost initial rule. The result is dis-
played at the bottom in Figure 5. Formally, the set
of extractable rules R for a given word-aligned
sentence pair (e, A, f) with parse tree t for f is
the smallest set subject to the following two con-
ditions:
</bodyText>
<listItem confidence="0.9954972">
• Each initial rule is in R and thus extractable.
• For every initial rule r and extractable rule
r&apos; E R, any flat rule r&apos;&apos;, into which we can
substitute r to obtain ρ with flat(ρ) = r&apos;, is
in R and thus extractable.2
</listItem>
<bodyText confidence="0.7804305">
For our running example depicted in Figure 3 we
display some extractable rules in Figure 6.
</bodyText>
<equation confidence="0.423307">
2A rule ρ = s → (t1, ... , t.) is flat if flat(ρ) = ρ, where
flat(ρ) = s → (flat(t1), ... , flat(t,)).
</equation>
<table confidence="0.712928869565217">
Extractable rule [top] and initial rule [bottom]:
NP PP
the debate on human rights - (
¨uber Menschenrechte
on human rights
PP
,
die Aussprache ¨uber Menschenrechte
1
818
�
Source string “the debate”:
VAFIN VP
concludes X on human rights —* ( , NP ,
ist ¨uber Menschenrechte geschlossen
Source string “on human rights”:
� VAFIN NP VP
concludes the debate X —* , ,
ist die Aussprache PP geschlossen
Source string “the debate on human rights”:
�VAFIN VP
concludes X —* ( , NP , )
ist PP geschlossen
</table>
<figureCaption confidence="0.996326">
Figure 6: Extractable rules obtained by excising various initial rules (see Figure 4) from the initial rule
displayed at the bottom of Figure 4.
</figureCaption>
<bodyText confidence="0.998251272727273">
Unfortunately, already Chiang (2007) points out
that the set of all extractable rules is generally
too large and keeping all extractable rules leads to
slow training, slow decoding, and spurious ambi-
guity. Our MBOT rules are restricted by the parse
tree for the target sentence, but the MBOT model
permits additional flexibility due to the presence
of multiple target tree fragments. Overall, we ex-
perience the same problems, and consequently, in
the experiments we use the following additional
constraints on rules s —* (ti, ... , tl):
</bodyText>
<listItem confidence="0.8211074">
(a) We only consider source phrases p of length at
most 10 (i.e., i&apos; — i &lt; 10 for p = [i, i&apos;]).3
(b) The source string s contains at most 5 occur-
rences of lexical items or X (i.e. fs &lt; 5).
(c) The source string s cannot have consecu-
tive Xs (i.e., XX is not a substring of s).
(d) The source string contains at least one lexical
item that was aligned in (e, A, f).
(e) The left-most token of the source string s can-
not be X (i.e., s[1,1] =� X).
</listItem>
<bodyText confidence="0.9903579">
Our implementation can easily be modified to han-
dle other constraints. Figure 7 shows extractable
rules violating those additional constraints.
Table 1 gives an overview on how many rules
are extracted. Our string-to-tree variant extracts
12–17 times more rules than the minimal tree-to-
tree rule extraction. For our experiments (see Sec-
tion 6), we filter all rule tables on the given input.
The decoding times for the minimal eMBOT and
our MBOT share the same order of magnitude.
</bodyText>
<sectionHeader confidence="0.986637" genericHeader="method">
5 Model Features
</sectionHeader>
<bodyText confidence="0.911257666666667">
For each source language sentence e, we want to
determine its most likely translation fˆ given by
fˆ = arg maxf p(f  |e) = arg maxf p(e  |f) p(f)
</bodyText>
<footnote confidence="0.542888">
3Note that this restricts the set of initial rules.
</footnote>
<bodyText confidence="0.999536761904762">
for some unknown probability distributions p. We
estimate p(e  |f) p(f) by a log-linear combination
of features hi( ) with weights λi scored on senten-
tial forms e —* (t) of our extracted MBOT M such
that the leaves of t read (left-to-right) f.
We use the decoder provided by MBOT-Moses
of Braune et al. (2013) and its standard features,
which includes all the common features (Koehn,
2009) and a gap penalty 1001−c, where c is the
number of target tree fragments that contributed
to t. This feature discourages rules with many tar-
get tree fragments. As usual, all features are ob-
tained as the product of the corresponding rule fea-
tures for the rules used to derive e —* (t) by means
of substitution. The rule weights for the transla-
tion weights are obtained as relative frequencies
normalized over all rules with the same right- and
left-hand side. Good-Turing smoothing (Good,
1953) is applied to all rules that were extracted at
most 10 times. The lexical translation weights are
obtained as usual.
</bodyText>
<sectionHeader confidence="0.997722" genericHeader="evaluation">
6 Experimental Results
</sectionHeader>
<bodyText confidence="0.999570833333333">
We considered three reasonable baselines: (i) min-
imal eMBOT, (ii) non-contiguous STSSG (Sun et
al., 2009), or (iii) a string-to-tree Moses system.
We decided against the minimal MMBOT as a base-
line since tree-to-tree systems generally get lower
BLEU scores than string-to-tree systems. We nev-
ertheless present its BLEU scores (see Table 3).
Unfortunately, we could not compare to Sun et
al. (2009) because their decoder and rule extrac-
tion algorithms are not publicly available. Fur-
thermore, we have the impression that their system
does not scale well:
</bodyText>
<listItem confidence="0.9982345">
• Only around 240,000 training sentences were
used. Our training data contains between
1.8M and 5.7M sentence pairs.
• The development and test set were length-
</listItem>
<page confidence="0.992702">
819
</page>
<figure confidence="0.998605333333333">
�
violates (b):
� PROAV VAFIN VP
that concludes X on human rights → ) ) NP )
damit ist ¨uber Menschenrechte geschlossen
violates (d):
� �
X → NP
�X on human rights → NP )
PP �¨uber Menschenrechte
violates (e):
violates (c):
�concludes X X →
VP �
PP geschlossen
VAFIN
) NP )
ist
</figure>
<figureCaption confidence="0.999903">
Figure 7: Showing extractable rules violating the restrictions.
</figureCaption>
<table confidence="0.984048">
System number of extracted rules
English-To-German English-To-Arabic English-To-Chinese
minimal tree-to-tree BMBOT 12,478,160 28,725,229 10,162,325
non-minimal string-to-tree MBOT 143,661,376 491,307,787 162,240,663
string-to-tree Moses 14,092,729 55,169,043 17,047,570
</table>
<tableCaption confidence="0.99968">
Table 1: Overview of numbers of extracted rules with respect to the different extraction algorithms.
</tableCaption>
<bodyText confidence="0.9762115">
ratio filtered to sentences up to 50 characters.
We do not modify those sets.
</bodyText>
<listItem confidence="0.9877535">
• Only rules with at most one gap were al-
lowed which would be equivalent to restrict
the number of target tree fragments to 2 in
our system.
</listItem>
<bodyText confidence="0.999564">
Hence we decided to use a string-to-tree Moses
system as baseline (see Section 6.1).
</bodyText>
<subsectionHeader confidence="0.995109">
6.1 Setup
</subsectionHeader>
<bodyText confidence="0.999973029411765">
As a baseline system for our experiments we use
the syntax-based component (Hoang et al., 2009)
of the Moses toolkit (Koehn et al., 2007). Our
system is the presented translation system based
on MBOTs. We use the MBOT-Moses decoder
(Braune et al., 2013) which – similar to the base-
line decoder – uses a CYK+ chart parsing algo-
rithm using a standard X-style parse tree which is
sped up by cube pruning (Chiang, 2007) with in-
tegrated language model scoring.
Our and the baseline system use linguistic syn-
tactic annotation (parses) only on the target side
(string-to-tree). During rule extraction we impose
the restrictions of Section 4. Additional glue-rules
that concatenate partial translations without per-
forming any reordering are used in all systems.
For all experiments (English-to-German,
English-to-Arabic, and English-to-Chinese), the
training data was length-ratio filtered. The word
alignments were generated by GIZA++ (Och
and Ney, 2003) with the grow-diag-final-and
heuristic (Koehn et al., 2005). The following
language-specific processing was performed. The
German text was true-cased and the functional
and morphological annotations were removed
from the parse. The Arabic text was tokenized
with MADA (Habash et al., 2009) and translit-
erated according to Buckwalter (2002). Finally,
the Chinese text was word-segmented using the
Stanford Word Segmenter (Chang et al., 2008).
In all experiments the feature weights Ai of the
log-linear model were trained using minimum er-
ror rate training (Och, 2003). The remaining infor-
mation for the experiments is presented in Table 2.
</bodyText>
<subsectionHeader confidence="0.999726">
6.2 Quantitative Analysis
</subsectionHeader>
<bodyText confidence="0.999878588235294">
The overall translation quality was measured with
4-gram BLEU (Papineni et al., 2002) on true-
cased data for German, on transliterated data for
Arabic, and on word-segmented data for Chinese.
Significance was computed with Gimpel’s imple-
mentation (Gimpel, 2011) of pairwise bootstrap
resampling with 1,000 samples. Table 3 lists the
evaluation results. In all three setups the MBOT
system significantly outperforms the baseline. For
German we obtain a BLEU score of 15.90 which
is a gain of 0.68 points. For Arabic we get an in-
crease of 0.78 points which results in 49.10 BLEU.
For Chinese we obtain a score of 18.35 BLEU
gaining 0.66 points.4 We also trained a vanilla
phrase-based system for each language pair on the
same data as described in Table 2.
To demonstrate the usefulness of the multiple
</bodyText>
<footnote confidence="0.994563">
4NIST-08 also shows BLEU for word-segmented output
(http://www.itl.nist.gov/iad/mig/tests/
mt/2008/doc/mt08_ official _ results _v0.
html). Best constrained system: 17.69 BLEU; best
unconstrained system: 19.63 BLEU.
</footnote>
<page confidence="0.989097">
820
</page>
<table confidence="0.986753909090909">
English to German English to Arabic English to Chinese
training data 7th EuroParl corpus (Koehn, 2005) MultiUN corpus (Eisele and Chen, 2010)
training data size ≈ 1.8M sentence pairs ≈ 5.7M sentence pairs ≈ 1.9M sentence pairs
target-side parser BitPar (Schmid, 2004) Berkeley parser (Petrov et al., 2006)
language model 5-gram SRILM (Stolcke, 2002)
add. LM data WMT 2013 Arabic in MultiUN Chinese in MultiUN
LM data size ≈ 57M sentences ≈ 9.7M sentences ≈ 9.5M sentences
tuning data WMT 2013 cut from MultiUN NIST 2002, 2003, 2005
tuning size 3,000 sentences 2,000 sentences 2,879 sentences
test data WMT 2013 (Bojar et al., 2013) cut from MultiUN NIST 2008 (NIST, 2010)
test size 3,000 sentences 1,000 sentences 1,859 sentences
</table>
<tableCaption confidence="0.979911">
Table 2: Summary of the performed experiments.
</tableCaption>
<table confidence="0.999328133333333">
Language pair System BLEU
Moses Baseline 15.22
MBOT ∗15.90
English-to-German
minimal $MBOT 14.09
Phrase-based Moses 16.73
Moses Baseline 48.32
MBOT ∗49.10
English-to-Arabic
minimal $MBOT 32.88
Phrase-based Moses 50.27
Moses Baseline 17.69
English-to-Chinese MBOT ∗18.35
minimal $MBOT 12.01
Phrase-based Moses 18.09
</table>
<tableCaption confidence="0.99917">
Table 3: Evaluation results. The starred results
</tableCaption>
<bodyText confidence="0.997132055555556">
are statistically significant improvements over the
baseline (at confidence p &lt; 1%).
target tree fragments of MBOTs, we analyzed the
MBOT rules that were used when decoding the
test set. We distinguish several types of rules. A
rule is contiguous if it has only 1 target tree frag-
ment. All other rules are (potentially) discontigu-
ous. Moreover, lexical rules are rules whose leaves
are exclusively lexical items. All other rules (i.e.,
those that contain at least one non-lexical leaf)
are structural. Table 4 reports how many rules of
each type are used during decoding for both our
MBOT system and the minimal $MBOT. Below,
we focus on analyzing our MBOT system. Out
of the rules used for German, 27% were (poten-
tially) discontiguous and 5% were structural. For
Arabic, we observe 67% discontiguous rules and
26% structural rules. For translating into Chinese
30% discontiguous rules were used and the struc-
tural rules account to 18%. These numbers show
that the usage of discontiguous rules tunes to the
specific language pair. For instance, Arabic uti-
lizes them more compared to German and Chi-
nese. Furthermore, German uses a lot of lexical
rules which is probably due to the fact that it is a
morphologically rich language. On the other hand,
Arabic and Chinese make good use of structural
rules. In addition, Table 4 presents a finer-grained
analysis based on the number of target tree frag-
ments. Only rules with at most 8 target tree frag-
ments were used. While German and Arabic seem
to require some rules with 6 target tree fragments,
Chinese probably does not. We conclude that the
number of target tree fragments can be restricted
to a language-pair specific number during rule ex-
traction.
</bodyText>
<subsectionHeader confidence="0.999617">
6.3 Qualitative Analysis
</subsectionHeader>
<bodyText confidence="0.9999909375">
In this section, we inspect some English-to-
German translations generated by the Moses base-
line and our MBOT system in order to provide
some evidence for linguistic constructions that our
system handles better. We identified (a) the real-
ization of reflexive pronouns, relative pronouns,
and particle verbs, (b) the realization of verbal
material, and (c) local and long distance reorder-
ing to be better throughout than in the baseline
system. All examples are (parts of) translations
of sentences from the test data. Ungrammatical
constructions are enclosed in brackets and marked
with a star. We focus on instances that seem rele-
vant to the new ability to use non-minimal rules.
We start with an example showing the realiza-
tion of a reflexive pronoun.
</bodyText>
<footnote confidence="0.7488044">
Source: Bitcoin differs from other types of virtual currency.
Reference: Bitcoin unterscheidet sich von anderen Arten
virtueller W¨ahrungen.
Baseline: Bitcoin [unterscheidet]&apos;` von anderen Arten [der
virtuellen W¨ahrung]*.
</footnote>
<page confidence="0.980283">
821
</page>
<table confidence="0.99990575">
Language pair System Type Lex Struct Total 2 Target tree fragments &gt; 6
3 4 5
English-to-German our cont. 27,351 635 27,986
MBOT
discont. 9,336 1,110 10,446 5,565 3,441 1,076 312 52
minimal cont. 55,910 4,492 60,402
`MBOT
discont. 2,167 7,386 9,553 6,458 2,589 471 34 1
English-to-Arabic our cont. 1,839 651 2,490
MBOT
discont. 3,670 1,324 4,994 3,008 1,269 528 153 36
minimal cont. 18,389 2,855 21,244
`MBOT
discont. 1,138 1,920 3,058 2,525 455 67 8 3
English-to-Chinese our cont. 17,135 1,585 18,720
MBOT
discont. 4,822 3,341 8,163 6,411 1,448 247 55 2
minimal cont. 34,275 8,820 43,095
`MBOT
discont. 516 4,292 4,808 3,816 900 82 6 4
</table>
<tableCaption confidence="0.880066">
Table 4: Number of rules per type used when decoding test (Lex = lexical rules; Struct = structural rules;
[dis]cont. = [dis]contiguous).
</tableCaption>
<bodyText confidence="0.9131166">
MBOT: Bitcoin unterscheidet sich von anderen Arten [der
virtuellen W¨ahrung]*.
Here the baseline drops the reflexive pronoun sich,
which is correctly realized by the MBOT system.
The rule used is displayed in Figure 8.
</bodyText>
<figureCaption confidence="0.995499">
Figure 8: Rule realizing the reflexive pronoun.
</figureCaption>
<bodyText confidence="0.98399">
Next, we show a translation in which our system
correctly generates a whole verbal segment.
</bodyText>
<table confidence="0.44629125">
Source: It turned out that not only ...
Reference: Es stellte sich heraus, dass nicht nur ...
Baseline: [Heraus,]* nicht nur ...
MBOT: Es stellte sich heraus, dass nicht nur ...
</table>
<bodyText confidence="0.9304154">
The baseline drops the verbal construction
whereas the large non-minimal rule of Figure 9 al-
lows our MBOT to avoid that drop. Again, the re-
quired reflexive pronoun sich is realized as well as
the necessary comma before the conjunction dass.
</bodyText>
<figureCaption confidence="0.994456">
Figure 9: MBOT rule for the verbal segment.
</figureCaption>
<bodyText confidence="0.9923202">
Another feature of MBOT is its power to per-
form long distance reordering with the help of sev-
eral discontiguous output fragments.
Source: ... weapons factories now, which do not endure
competition on the international market and ...
</bodyText>
<figure confidence="0.992887722222222">
sich
stellte
heraus
Es
,
VVFIN
,
PTKZU
,
KOUS �
dass
(It turned out that -
PPER
PRF
,
$,
,
,
(differs from other -
VVFIN
,
unterscheidet
PRF
,
sich
APPR
von
,
ADJA )
anderen &amp;quot;
Reference: ... R¨ustungsfabriken, die der internationalen
Konkurrenz nicht standhalten und ...
Baseline: ... [Waffen in den Fabriken nun]*, die nicht einem
Wettbewerb auf dem internationalen Markt []* und ...
MBOT: ... [Waffen Fabriken nun]*, die Konkurrenz auf dem
internationalen Markt nicht ertragen und ...
</figure>
<figureCaption confidence="0.973131333333333">
Figure 10 shows the rules which enable the
MBOT system to produce the correct reordering.
Figure 10: Long distance reordering.
</figureCaption>
<sectionHeader confidence="0.997631" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999794777777778">
We present an application of a string-to-tree vari-
ant of local multi bottom-up tree transducers,
which are tree-to-tree models, to statistical ma-
chine translation. Originally, only minimal rules
were extracted, but to overcome the typically
lower translation quality of tree-to-tree systems
and minimal rules, we abolish the syntactic an-
notation on the source side and develop a string-
to-tree variant. In addition, we present a new pa-
</bodyText>
<figure confidence="0.99610612">
PRELS
NP
,
PTKNEG
,
VP )
VP&amp;quot;
,
(which do not X -
NP
nicht
die
VP )
ertragen &amp;quot;
NP
(endure X -
,
NP
(competition X -
NP \J
Konkurrenz PP &amp;quot;
PP
(on the international market -
auf dem internationalen Markt
�
</figure>
<page confidence="0.992177">
822
</page>
<bodyText confidence="0.999351676470588">
rameterized rule extraction that can extract non-
minimal rules, which are particularly helpful for
translating fixed phrases. It would be interesting
to know how much can be gained when using only
one contribution at a time. Hence, we will explore
the impact of string-to-tree and non-minimal rules
in isolation.
We demonstrate that our new system signifi-
cantly outperforms the standard Moses string-to-
tree system on three different large-scale transla-
tion tasks (English-to-German, English-to-Arabic,
and English-to-Chinese) with a gain between 0.53
and 0.87 BLEU points. An analysis of the rules
used to decode the test sets suggests that the usage
of discontiguous rules is tuned to each language
pair. Furthermore, it shows that only discontigu-
ous rules with at most 8 target tree fragments are
used. Thus, further research could investigate a
hard limit on the number of target tree fragments
during rule extraction. We also perform a manual
inspection of the obtained translations and con-
firm that our string-to-tree MBOT rules can ade-
quately handle discontiguous phrases, which oc-
cur frequently in German, Arabic, and Chinese.
Other languages that exhibit such phenomena in-
clude Czech, Dutch, Russian, and Polish. Thus,
we hope that our approach can also be applied suc-
cessfully to other language pairs.
To support further experimentation by the
community, we publicly release our de-
veloped software and complete tool-chain
(http://www.ims.uni-stuttgart.de/
forschung/ressourcen/werkzeuge/
mbotmoses.html).
</bodyText>
<sectionHeader confidence="0.936699" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999774166666667">
The authors would like to express their gratitude
to the reviewers for their helpful comments and
Robin Kurtz for preparing the Arabic corpus.
All authors were financially supported by
the German Research Foundation (DFG) grant
MA 4959 / 1-1.
</bodyText>
<sectionHeader confidence="0.998472" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999655586206897">
Ondˇrej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Proc.
8th WMT, pages 1–44. Association for Computa-
tional Linguistics.
Fabienne Braune, Nina Seemann, Daniel Quernheim,
and Andreas Maletti. 2013. Shallow local multi
bottom-up tree transducers in statistical machine
translation. In Proc. 51st ACL, pages 811–821. As-
sociation for Computational Linguistics.
Timothy Buckwalter. 2002. Arabic translit-
eration. http://www.qamus.org/
transliteration.htm.
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese word segmen-
tation for machine translation performance. In Proc.
3rd WMT, pages 224–232. Association for Compu-
tational Linguistics.
David Chiang. 2006. An introduction to synchronous
grammars. In Proc. 44th ACL. Association for Com-
putational Linguistics. Part of a tutorial given with
Kevin Knight.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proc. 48th ACL, pages 1443–
1452. Association for Computational Linguistics.
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn
from phrase-based MT? In Proc. 2007 EMNLP,
pages 755–763. Association for Computational Lin-
guistics.
Andreas Eisele and Yu Chen. 2010. MultiUN: A mul-
tilingual corpus from United Nation documents. In
Proc. 7th LREC, pages 2868–2872. European Lan-
guage Resources Association.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proc. 41st
ACL, pages 205–208. Association for Computa-
tional Linguistics.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a translation rule?
In Proc. 2004 NAACL, pages 273–280. Association
for Computational Linguistics.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
44th ACL, pages 961–968. Association for Compu-
tational Linguistics.
Kevin Gimpel. 2011. Code for statistical significance
testing for MT evaluation metrics. http://www.
ark.cs.cmu.edu/MT/.
Irving J. Good. 1953. The population frequencies of
species and the estimation of population parameters.
Biometrika, 40(3–4):237–264.
</reference>
<page confidence="0.989823">
823
</page>
<reference confidence="0.999898593023256">
Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS
tagging, stemming and lemmatization. In Proc. 2nd
MEDAR, pages 102–109. Association for Computa-
tional Linguistics.
Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009.
A unified framework for phrase-based, hierarchical,
and syntax-based statistical machine translation. In
Proc. 6th IWSLT, pages 152–159.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
2003 NAACL, pages 48–54. Association for Compu-
tational Linguistics.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT Speech Translation Evaluation.
In Proc. 2nd IWSLT, pages 68–75.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proc. 45th ACL, pages 177–180. Associa-
tion for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proc. 10th
MT Summit, pages 79–86. Association for Machine
Translation in the Americas.
Philipp Koehn. 2009. Statistical Machine Translation.
Cambridge University Press.
Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008.
Syntax-driven learning of sub-sentential translation
equivalents and translation rules from parsed parallel
corpora. In Proc. 2nd SSST, pages 87–95. Associa-
tion for Computational Linguistics.
Yang Liu, Yajuan L¨u, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Proc.
47th ACL, pages 558–566. Association for Compu-
tational Linguistics.
Andreas Maletti. 2011. How to train your multi
bottom-up tree transducer. In Proc. 49th ACL, pages
825–834. Association for Computational Linguis-
tics.
NIST. 2010. NIST 2002 [2003, 2005, 2008] open ma-
chine translation evaluation. Linguistic Data Con-
sortium. LDC2010T10 [T11, T14, T21].
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19–51.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417–449.
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. 41st ACL,
pages 160–167. Association for Computational Lin-
guistics.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. In Proc.
40th ACL, pages 311–318. Association for Compu-
tational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. 44th ACL, pages
433–440. Association for Computational Linguis-
tics.
Helmut Schmid. 2004. Efficient parsing of highly
ambiguous context-free grammars with bit vectors.
In Proc. 20th COLING, pages 162–168. Association
for Computational Linguistics.
Andreas Stolcke. 2002. SRILM — an extensible
language modeling toolkit. In Proc. 7th INTER-
SPEECH, pages 257–286.
Jun Sun, Min Zhang, and Chew Lim Tan. 2009. A non-
contiguous tree sequence alignment-based model for
statistical machine translation. In Proc. 47th ACL,
pages 914–922. Association for Computational Lin-
guistics.
Benjamin Wellington, Sonjia Waxmonsky, and I. Dan
Melamed. 2006. Empirical lower bounds on the
complexity of translational equivalence. In Proc.
44th ACL, pages 977–984. Association for Compu-
tational Linguistics.
</reference>
<page confidence="0.99873">
824
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.441302">
<title confidence="0.999917">String-to-Tree Multi Bottom-up Tree Transducers</title>
<author confidence="0.996545">Seemann Braune</author>
<affiliation confidence="0.943449">Institute for Natural Language Processing, University of</affiliation>
<address confidence="0.445886">Pfaffenwaldring 5b, 70569 Stuttgart,</address>
<abstract confidence="0.999501125">We achieve significant improvements in several syntax-based machine translation experiments using a string-to-tree variant of multi bottom-up tree transducers. Our new parameterized rule extraction algorithm extracts string-to-tree rules that can be discontiguous and non-minimal in contrast to existing algorithms for the tree-to-tree setting. The obtained models significantly outperform the string-to-tree component of the Moses framework in a large-scale empirical evaluation on several known translation tasks. Our linguistic analysis reveals the remarkable benefits of discontiguous and non-minimal rules.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ondˇrej Bojar</author>
<author>Christian Buck</author>
<author>Chris Callison-Burch</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2013</date>
<booktitle>Findings of the 2013 Workshop on Statistical Machine Translation. In Proc. 8th WMT,</booktitle>
<pages>1--44</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="24310" citStr="Bojar et al., 2013" startWordPosition="4132" endWordPosition="4135">erman English to Arabic English to Chinese training data 7th EuroParl corpus (Koehn, 2005) MultiUN corpus (Eisele and Chen, 2010) training data size ≈ 1.8M sentence pairs ≈ 5.7M sentence pairs ≈ 1.9M sentence pairs target-side parser BitPar (Schmid, 2004) Berkeley parser (Petrov et al., 2006) language model 5-gram SRILM (Stolcke, 2002) add. LM data WMT 2013 Arabic in MultiUN Chinese in MultiUN LM data size ≈ 57M sentences ≈ 9.7M sentences ≈ 9.5M sentences tuning data WMT 2013 cut from MultiUN NIST 2002, 2003, 2005 tuning size 3,000 sentences 2,000 sentences 2,879 sentences test data WMT 2013 (Bojar et al., 2013) cut from MultiUN NIST 2008 (NIST, 2010) test size 3,000 sentences 1,000 sentences 1,859 sentences Table 2: Summary of the performed experiments. Language pair System BLEU Moses Baseline 15.22 MBOT ∗15.90 English-to-German minimal $MBOT 14.09 Phrase-based Moses 16.73 Moses Baseline 48.32 MBOT ∗49.10 English-to-Arabic minimal $MBOT 32.88 Phrase-based Moses 50.27 Moses Baseline 17.69 English-to-Chinese MBOT ∗18.35 minimal $MBOT 12.01 Phrase-based Moses 18.09 Table 3: Evaluation results. The starred results are statistically significant improvements over the baseline (at confidence p &lt; 1%). targe</context>
</contexts>
<marker>Bojar, Buck, Callison-Burch, Federmann, Haddow, Koehn, Monz, Post, Soricut, Specia, 2013</marker>
<rawString>Ondˇrej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine Translation. In Proc. 8th WMT, pages 1–44. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabienne Braune</author>
<author>Nina Seemann</author>
<author>Daniel Quernheim</author>
<author>Andreas Maletti</author>
</authors>
<title>Shallow local multi bottom-up tree transducers in statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proc. 51st ACL,</booktitle>
<pages>811--821</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1421" citStr="Braune et al. (2013)" startWordPosition="187" endWordPosition="190"> analysis reveals the remarkable benefits of discontiguous and non-minimal rules. 1 Introduction We present an application of a variant of local multi bottom-up tree transducers (eMBOTs) as proposed in Maletti (2011) to statistical machine translation. eMBOTs allow discontinuities on the target language side since they have a sequence of target tree fragments instead of a single tree fragment in their rules. The original approach makes use of syntactic information on both the source and the target side (tree-to-tree) and a corresponding minimal rule extraction is presented in (Maletti, 2011). Braune et al. (2013) implemented it as well as a decoder inside the Moses framework (Koehn et al., 2007) and demonstrated that the resulting tree-to-tree eMBOT system significantly improved over its tree-to-tree baseline using minimal rules. We can see at least two drawbacks in this approach. First, experiments investigating the integration of syntactic information on both sides generally report quality deterioration. For example, Lavie et al. (2008), Liu et al. (2009), and Chiang (2010) noted that translation quality tends to decrease in tree-to-tree systems because the rules become too restrictive. Second, mini</context>
<context position="5210" citStr="Braune et al. (2013)" startWordPosition="787" endWordPosition="790"> fragment. While the mentioned syntax-based models use tree fragments for source and target (tree-to-tree), Galley et al. (2004) and Galley et al. (2006) use syntactic annotations only on the target language side (string-to-tree). Further research by DeNeefe et al. (2007) revealed that adding non-minimal rules improves translation quality in this setting. Here we improve statistical machine translation in this setting even further using non-minimal eMBOT rules. 3 Theoretical Model As our translation model, we use a string-to-tree variant of the shallow local multi bottom-up tree transducer of Braune et al. (2013). We will call our variant MBOT for simplicity. Our MBOT is a synchronous grammar (Chiang, 2006) similar to a synchronous context-free grammar (SCFG), but instead of a single source and target fragment per rule, our rules are of the form s → (ti, ... , tr,,) with a single source string s and potentially several target tree fragments ti, ... , tr,,. Besides lexical items the source string can contain (several occurrences of) the placeholder X, which links to non-lexical leaves in the target tree fragments. In contrast to an SCFG each placeholder can have several such links. However, each non-le</context>
<context position="18587" citStr="Braune et al. (2013)" startWordPosition="3226" endWordPosition="3229">en input. The decoding times for the minimal eMBOT and our MBOT share the same order of magnitude. 5 Model Features For each source language sentence e, we want to determine its most likely translation fˆ given by fˆ = arg maxf p(f |e) = arg maxf p(e |f) p(f) 3Note that this restricts the set of initial rules. for some unknown probability distributions p. We estimate p(e |f) p(f) by a log-linear combination of features hi( ) with weights λi scored on sentential forms e —* (t) of our extracted MBOT M such that the leaves of t read (left-to-right) f. We use the decoder provided by MBOT-Moses of Braune et al. (2013) and its standard features, which includes all the common features (Koehn, 2009) and a gap penalty 1001−c, where c is the number of target tree fragments that contributed to t. This feature discourages rules with many target tree fragments. As usual, all features are obtained as the product of the corresponding rule features for the rules used to derive e —* (t) by means of substitution. The rule weights for the translation weights are obtained as relative frequencies normalized over all rules with the same right- and left-hand side. Good-Turing smoothing (Good, 1953) is applied to all rules t</context>
<context position="21299" citStr="Braune et al., 2013" startWordPosition="3667" endWordPosition="3670">with respect to the different extraction algorithms. ratio filtered to sentences up to 50 characters. We do not modify those sets. • Only rules with at most one gap were allowed which would be equivalent to restrict the number of target tree fragments to 2 in our system. Hence we decided to use a string-to-tree Moses system as baseline (see Section 6.1). 6.1 Setup As a baseline system for our experiments we use the syntax-based component (Hoang et al., 2009) of the Moses toolkit (Koehn et al., 2007). Our system is the presented translation system based on MBOTs. We use the MBOT-Moses decoder (Braune et al., 2013) which – similar to the baseline decoder – uses a CYK+ chart parsing algorithm using a standard X-style parse tree which is sped up by cube pruning (Chiang, 2007) with integrated language model scoring. Our and the baseline system use linguistic syntactic annotation (parses) only on the target side (string-to-tree). During rule extraction we impose the restrictions of Section 4. Additional glue-rules that concatenate partial translations without performing any reordering are used in all systems. For all experiments (English-to-German, English-to-Arabic, and English-to-Chinese), the training da</context>
</contexts>
<marker>Braune, Seemann, Quernheim, Maletti, 2013</marker>
<rawString>Fabienne Braune, Nina Seemann, Daniel Quernheim, and Andreas Maletti. 2013. Shallow local multi bottom-up tree transducers in statistical machine translation. In Proc. 51st ACL, pages 811–821. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Buckwalter</author>
</authors>
<title>Arabic transliteration.</title>
<date>2002</date>
<note>http://www.qamus.org/ transliteration.htm.</note>
<contexts>
<context position="22333" citStr="Buckwalter (2002)" startWordPosition="3821" endWordPosition="3822">ate partial translations without performing any reordering are used in all systems. For all experiments (English-to-German, English-to-Arabic, and English-to-Chinese), the training data was length-ratio filtered. The word alignments were generated by GIZA++ (Och and Ney, 2003) with the grow-diag-final-and heuristic (Koehn et al., 2005). The following language-specific processing was performed. The German text was true-cased and the functional and morphological annotations were removed from the parse. The Arabic text was tokenized with MADA (Habash et al., 2009) and transliterated according to Buckwalter (2002). Finally, the Chinese text was word-segmented using the Stanford Word Segmenter (Chang et al., 2008). In all experiments the feature weights Ai of the log-linear model were trained using minimum error rate training (Och, 2003). The remaining information for the experiments is presented in Table 2. 6.2 Quantitative Analysis The overall translation quality was measured with 4-gram BLEU (Papineni et al., 2002) on truecased data for German, on transliterated data for Arabic, and on word-segmented data for Chinese. Significance was computed with Gimpel’s implementation (Gimpel, 2011) of pairwise b</context>
</contexts>
<marker>Buckwalter, 2002</marker>
<rawString>Timothy Buckwalter. 2002. Arabic transliteration. http://www.qamus.org/ transliteration.htm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pi-Chuan Chang</author>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>Optimizing Chinese word segmentation for machine translation performance.</title>
<date>2008</date>
<booktitle>In Proc. 3rd WMT,</booktitle>
<pages>224--232</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="22434" citStr="Chang et al., 2008" startWordPosition="3834" endWordPosition="3837">ents (English-to-German, English-to-Arabic, and English-to-Chinese), the training data was length-ratio filtered. The word alignments were generated by GIZA++ (Och and Ney, 2003) with the grow-diag-final-and heuristic (Koehn et al., 2005). The following language-specific processing was performed. The German text was true-cased and the functional and morphological annotations were removed from the parse. The Arabic text was tokenized with MADA (Habash et al., 2009) and transliterated according to Buckwalter (2002). Finally, the Chinese text was word-segmented using the Stanford Word Segmenter (Chang et al., 2008). In all experiments the feature weights Ai of the log-linear model were trained using minimum error rate training (Och, 2003). The remaining information for the experiments is presented in Table 2. 6.2 Quantitative Analysis The overall translation quality was measured with 4-gram BLEU (Papineni et al., 2002) on truecased data for German, on transliterated data for Arabic, and on word-segmented data for Chinese. Significance was computed with Gimpel’s implementation (Gimpel, 2011) of pairwise bootstrap resampling with 1,000 samples. Table 3 lists the evaluation results. In all three setups the</context>
</contexts>
<marker>Chang, Galley, Manning, 2008</marker>
<rawString>Pi-Chuan Chang, Michel Galley, and Christopher D. Manning. 2008. Optimizing Chinese word segmentation for machine translation performance. In Proc. 3rd WMT, pages 224–232. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>An introduction to synchronous grammars.</title>
<date>2006</date>
<booktitle>In Proc. 44th ACL.</booktitle>
<publisher>Association</publisher>
<contexts>
<context position="5306" citStr="Chiang, 2006" startWordPosition="805" endWordPosition="806">ee), Galley et al. (2004) and Galley et al. (2006) use syntactic annotations only on the target language side (string-to-tree). Further research by DeNeefe et al. (2007) revealed that adding non-minimal rules improves translation quality in this setting. Here we improve statistical machine translation in this setting even further using non-minimal eMBOT rules. 3 Theoretical Model As our translation model, we use a string-to-tree variant of the shallow local multi bottom-up tree transducer of Braune et al. (2013). We will call our variant MBOT for simplicity. Our MBOT is a synchronous grammar (Chiang, 2006) similar to a synchronous context-free grammar (SCFG), but instead of a single source and target fragment per rule, our rules are of the form s → (ti, ... , tr,,) with a single source string s and potentially several target tree fragments ti, ... , tr,,. Besides lexical items the source string can contain (several occurrences of) the placeholder X, which links to non-lexical leaves in the target tree fragments. In contrast to an SCFG each placeholder can have several such links. However, each non-lexical leaf in a target tree fragment has exactly one such link to a placeholder X. An MBOT is si</context>
</contexts>
<marker>Chiang, 2006</marker>
<rawString>David Chiang. 2006. An introduction to synchronous grammars. In Proc. 44th ACL. Association for Computational Linguistics. Part of a tutorial given with Kevin Knight.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="2641" citStr="Chiang, 2007" startWordPosition="381" endWordPosition="382">es (i.e., rules that cannot be obtained from other extracted rules) typically consist of a few lexical items only and are thus not the most suitable to translate idiomatic expressions and other fixed phrases. To overcome these drawbacks, we abolish the syntactic information for the source side and develop a string-to-tree variant of eMBOTs. In addition, we develop a new rule extraction algorithm that can also extract non-minimal rules. In general, the number of extractable rules explodes, so our rule extraction places parameterized restrictions on the extracted rules in the same spirit as in (Chiang, 2007). In this manner, we combine the advantages of the hierarchical phrase-based approach on the source side and the tree-based approach with discontinuiety on the target side. We evaluate our new system in 3 large-scale experiments using translation tasks, in which we expect discontinuiety on the target. MBOTs are powerful but asymmetric models since discontinuiety is available only on the target. We chose to translate from English to German, Arabic, and Chinese. In all experiments our new system significantly outperforms the string-to-tree syntax-based component (Hoang et al., 2009) of Moses. Th</context>
<context position="9717" citStr="Chiang (2007)" startWordPosition="1524" endWordPosition="1525">ion. Consequently, such fixed phrases will often be assembled inconsistently by substitution from small fragments. Non-minimal rules encourage a consistent translation by covering larger parts of the source sentence. Here we want to develop an efficient rule extraction procedure for our string-to-tree MBOTs that avoids the mentioned drawbacks. Naturally, we could substitute minimal rules into each other to obtain non-minimal rules, but performing substitution for all combinations is clearly intractable. Instead we essentially follow the approach of Koehn et al. (2003), Och and Ney (2004), and Chiang (2007), which is based on consistently aligned phrase pairs. Our training corpus contains word-aligned sentence pairs (e, A,f), which contain a source language sentence e, a target language sentence f, and an alignment A C_ [1,`e] x [1, `f], where `e and `f are the lengths of the sentences e and f, respectively, and [i, i&apos;] = {j E Z |i &lt; j &lt; i&apos;} is the span (closed interval of integers) from i to i&apos; for all positive integers i &lt; i&apos;. Rules are extracted for each pair of the corpus, so in the following let (e, A, f) be a word-aligned sentence pair. A source phrase is simply a span [i, i&apos;] C_ [1, `e] a</context>
<context position="14504" citStr="Chiang (2007)" startWordPosition="2476" endWordPosition="2478">only one initial rule. More precisely, we have • e([2, 4]) =concludes the debate • tη1 = (VAFIN ist) • tη2 = (NP (ART die) (NN Aussprache)), • and tη3 = (VVPP geschlossen). The function flat leaves tη1 and tη3 unchanged, but flat(tη2) = (NP die Aussprache). Thus, we obtain the boxed rule of Figure 4. Clearly, the initial rules are just the start because they are completely lexical in the sense that they never contain the placeholder X in the source string nor a non-lexical leaf in any output tree fragment. We introduce non-lexical rules using the same approach as for the hierarchical rules of Chiang (2007). Roughly speaking, we obtain a new rule r&apos;&apos; by “excising” an initial rule r from another rule r&apos; and replacing the removed part by • the placeholder X in the source string, • the root label of the removed tree fragment in the target tree fragments, and • linking the removed parts appropriately, so that the flatted substitution of r into r&apos;&apos; can 1If p = [i, i�], then e(p) = e[i, i�] is the substring of e ranging from the i-th token to the i�-th token. Extractable rule obtained after excision: NP the debate X - ( , PP ) die Aussprache Figure 5: Excision of the middle initial rule from the topmo</context>
<context position="16641" citStr="Chiang (2007)" startWordPosition="2875" endWordPosition="2876">e on human rights - ( ¨uber Menschenrechte on human rights PP , die Aussprache ¨uber Menschenrechte 1 818 � Source string “the debate”: VAFIN VP concludes X on human rights —* ( , NP , ist ¨uber Menschenrechte geschlossen Source string “on human rights”: � VAFIN NP VP concludes the debate X —* , , ist die Aussprache PP geschlossen Source string “the debate on human rights”: �VAFIN VP concludes X —* ( , NP , ) ist PP geschlossen Figure 6: Extractable rules obtained by excising various initial rules (see Figure 4) from the initial rule displayed at the bottom of Figure 4. Unfortunately, already Chiang (2007) points out that the set of all extractable rules is generally too large and keeping all extractable rules leads to slow training, slow decoding, and spurious ambiguity. Our MBOT rules are restricted by the parse tree for the target sentence, but the MBOT model permits additional flexibility due to the presence of multiple target tree fragments. Overall, we experience the same problems, and consequently, in the experiments we use the following additional constraints on rules s —* (ti, ... , tl): (a) We only consider source phrases p of length at most 10 (i.e., i&apos; — i &lt; 10 for p = [i, i&apos;]).3 (b</context>
<context position="21461" citStr="Chiang, 2007" startWordPosition="3700" endWordPosition="3701"> allowed which would be equivalent to restrict the number of target tree fragments to 2 in our system. Hence we decided to use a string-to-tree Moses system as baseline (see Section 6.1). 6.1 Setup As a baseline system for our experiments we use the syntax-based component (Hoang et al., 2009) of the Moses toolkit (Koehn et al., 2007). Our system is the presented translation system based on MBOTs. We use the MBOT-Moses decoder (Braune et al., 2013) which – similar to the baseline decoder – uses a CYK+ chart parsing algorithm using a standard X-style parse tree which is sped up by cube pruning (Chiang, 2007) with integrated language model scoring. Our and the baseline system use linguistic syntactic annotation (parses) only on the target side (string-to-tree). During rule extraction we impose the restrictions of Section 4. Additional glue-rules that concatenate partial translations without performing any reordering are used in all systems. For all experiments (English-to-German, English-to-Arabic, and English-to-Chinese), the training data was length-ratio filtered. The word alignments were generated by GIZA++ (Och and Ney, 2003) with the grow-diag-final-and heuristic (Koehn et al., 2005). The fo</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Learning to translate with source and target syntax.</title>
<date>2010</date>
<booktitle>In Proc. 48th ACL,</booktitle>
<pages>1443--1452</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="1893" citStr="Chiang (2010)" startWordPosition="263" endWordPosition="264">e source and the target side (tree-to-tree) and a corresponding minimal rule extraction is presented in (Maletti, 2011). Braune et al. (2013) implemented it as well as a decoder inside the Moses framework (Koehn et al., 2007) and demonstrated that the resulting tree-to-tree eMBOT system significantly improved over its tree-to-tree baseline using minimal rules. We can see at least two drawbacks in this approach. First, experiments investigating the integration of syntactic information on both sides generally report quality deterioration. For example, Lavie et al. (2008), Liu et al. (2009), and Chiang (2010) noted that translation quality tends to decrease in tree-to-tree systems because the rules become too restrictive. Second, minimal rules (i.e., rules that cannot be obtained from other extracted rules) typically consist of a few lexical items only and are thus not the most suitable to translate idiomatic expressions and other fixed phrases. To overcome these drawbacks, we abolish the syntactic information for the source side and develop a string-to-tree variant of eMBOTs. In addition, we develop a new rule extraction algorithm that can also extract non-minimal rules. In general, the number of</context>
<context position="7879" citStr="Chiang (2010)" startWordPosition="1242" endWordPosition="1244">tial form shown at the bottom of Figure 2. Ideally, the substitution process is repeated until the complete source sentence is derived. 4 Rule Extraction The rule extraction of Maletti (2011) extracts minimal tree-to-tree rules, which are rules containing both source and target tree fragments, from sentence pairs of a word-aligned and bi-parsed parallel corpus. In particular, this requires parses for both the source and the target language sentences which adds a source for errors and specificity potentially leading to lower translation performance and lower coverage (Wellington et al., 2006). Chiang (2010) showed that string-to-tree systems— Matching sentential forms (underlining for emphasis): VAFIN VP concludes X → (, NP , ist PP geschlossen PP �X on X → NP , � ¨uber NN � VAFIN concludes X on X → ( , NP , ist �geschlossen PP ¨uber NN 816 PROAV[1,1] damit1 that1 concludes2 the3 debate4 on5 human6 rights7 VAFIN[2,2] ist2 ART[3,3] die3 NP[3,4] Aussprache4 TOP[1,7] NN[4,4] APPR[5,5] ¨uber5 PP[5,6] Menschenrechte6 NN[6,6] VP[5,7] geschlossen7 VVPP[7,7] Figure 3: Word-aligned sentence pair with targetside parse. which he calls fuzzy tree-to-tree-systems— generally yield higher translation quality c</context>
</contexts>
<marker>Chiang, 2010</marker>
<rawString>David Chiang. 2010. Learning to translate with source and target syntax. In Proc. 48th ACL, pages 1443– 1452. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve DeNeefe</author>
<author>Kevin Knight</author>
<author>Wei Wang</author>
<author>Daniel Marcu</author>
</authors>
<title>What can syntax-based MT learn from phrase-based MT?</title>
<date>2007</date>
<booktitle>In Proc. 2007 EMNLP,</booktitle>
<pages>755--763</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4862" citStr="DeNeefe et al. (2007)" startWordPosition="736" endWordPosition="739"> → Menschenrechte die NN Figure 1: Several valid rules for our MBOT. fragment and a sequence of target tree fragments. eMBOTs can also be understood as a restriction of the non-contiguous STSSGs of Sun et al. (2009), which allow a sequence of source tree fragments and a sequence of target tree fragments. eMBOT rules require exactly one source tree fragment. While the mentioned syntax-based models use tree fragments for source and target (tree-to-tree), Galley et al. (2004) and Galley et al. (2006) use syntactic annotations only on the target language side (string-to-tree). Further research by DeNeefe et al. (2007) revealed that adding non-minimal rules improves translation quality in this setting. Here we improve statistical machine translation in this setting even further using non-minimal eMBOT rules. 3 Theoretical Model As our translation model, we use a string-to-tree variant of the shallow local multi bottom-up tree transducer of Braune et al. (2013). We will call our variant MBOT for simplicity. Our MBOT is a synchronous grammar (Chiang, 2006) similar to a synchronous context-free grammar (SCFG), but instead of a single source and target fragment per rule, our rules are of the form s → (ti, ... ,</context>
</contexts>
<marker>DeNeefe, Knight, Wang, Marcu, 2007</marker>
<rawString>Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel Marcu. 2007. What can syntax-based MT learn from phrase-based MT? In Proc. 2007 EMNLP, pages 755–763. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Eisele</author>
<author>Yu Chen</author>
</authors>
<title>MultiUN: A multilingual corpus from United Nation documents.</title>
<date>2010</date>
<journal>European Language Resources Association.</journal>
<booktitle>In Proc. 7th LREC,</booktitle>
<pages>2868--2872</pages>
<contexts>
<context position="23820" citStr="Eisele and Chen, 2010" startWordPosition="4050" endWordPosition="4053">8 points which results in 49.10 BLEU. For Chinese we obtain a score of 18.35 BLEU gaining 0.66 points.4 We also trained a vanilla phrase-based system for each language pair on the same data as described in Table 2. To demonstrate the usefulness of the multiple 4NIST-08 also shows BLEU for word-segmented output (http://www.itl.nist.gov/iad/mig/tests/ mt/2008/doc/mt08_ official _ results _v0. html). Best constrained system: 17.69 BLEU; best unconstrained system: 19.63 BLEU. 820 English to German English to Arabic English to Chinese training data 7th EuroParl corpus (Koehn, 2005) MultiUN corpus (Eisele and Chen, 2010) training data size ≈ 1.8M sentence pairs ≈ 5.7M sentence pairs ≈ 1.9M sentence pairs target-side parser BitPar (Schmid, 2004) Berkeley parser (Petrov et al., 2006) language model 5-gram SRILM (Stolcke, 2002) add. LM data WMT 2013 Arabic in MultiUN Chinese in MultiUN LM data size ≈ 57M sentences ≈ 9.7M sentences ≈ 9.5M sentences tuning data WMT 2013 cut from MultiUN NIST 2002, 2003, 2005 tuning size 3,000 sentences 2,000 sentences 2,879 sentences test data WMT 2013 (Bojar et al., 2013) cut from MultiUN NIST 2008 (NIST, 2010) test size 3,000 sentences 1,000 sentences 1,859 sentences Table 2: Su</context>
</contexts>
<marker>Eisele, Chen, 2010</marker>
<rawString>Andreas Eisele and Yu Chen. 2010. MultiUN: A multilingual corpus from United Nation documents. In Proc. 7th LREC, pages 2868–2872. European Language Resources Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Learning non-isomorphic tree mappings for machine translation.</title>
<date>2003</date>
<booktitle>In Proc. 41st ACL,</booktitle>
<pages>205--208</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3751" citStr="Eisner (2003)" startWordPosition="551" endWordPosition="552">m significantly outperforms the string-to-tree syntax-based component (Hoang et al., 2009) of Moses. The (potentially) discontiguous rules of our model are very useful in these setups, which we confirm in a quantitative and qualitative analysis. 2 Related work Modern statistical machine translation systems (Koehn, 2009) are based on different translation models. Syntax-based systems have become widely used because of their ability to handle non-local reordering and other linguistic phenomena better than phrase-based models (Och and Ney, 2004). Synchronous tree substitution grammars (STSGs) of Eisner (2003) use a single source and target tree fragment per rule. In contrast, an eMBOT rule contains a single source tree 815 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 815–824, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics VAFIN VP concludes X → ( , NP , ist PP geschlossen PP X on X → (NP , � � ¨uber NN NN NP human rights → ( ) the X → Menschenrechte die NN Figure 1: Several valid rules for our MBOT. fragment and a sequence of target tree fr</context>
</contexts>
<marker>Eisner, 2003</marker>
<rawString>Jason Eisner. 2003. Learning non-isomorphic tree mappings for machine translation. In Proc. 41st ACL, pages 205–208. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proc. 2004 NAACL,</booktitle>
<pages>273--280</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4718" citStr="Galley et al. (2004)" startWordPosition="714" endWordPosition="717"> for Computational Linguistics VAFIN VP concludes X → ( , NP , ist PP geschlossen PP X on X → (NP , � � ¨uber NN NN NP human rights → ( ) the X → Menschenrechte die NN Figure 1: Several valid rules for our MBOT. fragment and a sequence of target tree fragments. eMBOTs can also be understood as a restriction of the non-contiguous STSSGs of Sun et al. (2009), which allow a sequence of source tree fragments and a sequence of target tree fragments. eMBOT rules require exactly one source tree fragment. While the mentioned syntax-based models use tree fragments for source and target (tree-to-tree), Galley et al. (2004) and Galley et al. (2006) use syntactic annotations only on the target language side (string-to-tree). Further research by DeNeefe et al. (2007) revealed that adding non-minimal rules improves translation quality in this setting. Here we improve statistical machine translation in this setting even further using non-minimal eMBOT rules. 3 Theoretical Model As our translation model, we use a string-to-tree variant of the shallow local multi bottom-up tree transducer of Braune et al. (2013). We will call our variant MBOT for simplicity. Our MBOT is a synchronous grammar (Chiang, 2006) similar to </context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proc. 2004 NAACL, pages 273–280. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proc. 44th ACL,</booktitle>
<pages>961--968</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4743" citStr="Galley et al. (2006)" startWordPosition="719" endWordPosition="722">stics VAFIN VP concludes X → ( , NP , ist PP geschlossen PP X on X → (NP , � � ¨uber NN NN NP human rights → ( ) the X → Menschenrechte die NN Figure 1: Several valid rules for our MBOT. fragment and a sequence of target tree fragments. eMBOTs can also be understood as a restriction of the non-contiguous STSSGs of Sun et al. (2009), which allow a sequence of source tree fragments and a sequence of target tree fragments. eMBOT rules require exactly one source tree fragment. While the mentioned syntax-based models use tree fragments for source and target (tree-to-tree), Galley et al. (2004) and Galley et al. (2006) use syntactic annotations only on the target language side (string-to-tree). Further research by DeNeefe et al. (2007) revealed that adding non-minimal rules improves translation quality in this setting. Here we improve statistical machine translation in this setting even further using non-minimal eMBOT rules. 3 Theoretical Model As our translation model, we use a string-to-tree variant of the shallow local multi bottom-up tree transducer of Braune et al. (2013). We will call our variant MBOT for simplicity. Our MBOT is a synchronous grammar (Chiang, 2006) similar to a synchronous context-fre</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proc. 44th ACL, pages 961–968. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
</authors>
<title>Code for statistical significance testing for MT evaluation metrics.</title>
<date>2011</date>
<note>http://www. ark.cs.cmu.edu/MT/.</note>
<contexts>
<context position="22919" citStr="Gimpel, 2011" startWordPosition="3911" endWordPosition="3912">ording to Buckwalter (2002). Finally, the Chinese text was word-segmented using the Stanford Word Segmenter (Chang et al., 2008). In all experiments the feature weights Ai of the log-linear model were trained using minimum error rate training (Och, 2003). The remaining information for the experiments is presented in Table 2. 6.2 Quantitative Analysis The overall translation quality was measured with 4-gram BLEU (Papineni et al., 2002) on truecased data for German, on transliterated data for Arabic, and on word-segmented data for Chinese. Significance was computed with Gimpel’s implementation (Gimpel, 2011) of pairwise bootstrap resampling with 1,000 samples. Table 3 lists the evaluation results. In all three setups the MBOT system significantly outperforms the baseline. For German we obtain a BLEU score of 15.90 which is a gain of 0.68 points. For Arabic we get an increase of 0.78 points which results in 49.10 BLEU. For Chinese we obtain a score of 18.35 BLEU gaining 0.66 points.4 We also trained a vanilla phrase-based system for each language pair on the same data as described in Table 2. To demonstrate the usefulness of the multiple 4NIST-08 also shows BLEU for word-segmented output (http://w</context>
</contexts>
<marker>Gimpel, 2011</marker>
<rawString>Kevin Gimpel. 2011. Code for statistical significance testing for MT evaluation metrics. http://www. ark.cs.cmu.edu/MT/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irving J Good</author>
</authors>
<title>The population frequencies of species and the estimation of population parameters.</title>
<date>1953</date>
<journal>Biometrika,</journal>
<pages>40--3</pages>
<contexts>
<context position="19161" citStr="Good, 1953" startWordPosition="3325" endWordPosition="3326">by MBOT-Moses of Braune et al. (2013) and its standard features, which includes all the common features (Koehn, 2009) and a gap penalty 1001−c, where c is the number of target tree fragments that contributed to t. This feature discourages rules with many target tree fragments. As usual, all features are obtained as the product of the corresponding rule features for the rules used to derive e —* (t) by means of substitution. The rule weights for the translation weights are obtained as relative frequencies normalized over all rules with the same right- and left-hand side. Good-Turing smoothing (Good, 1953) is applied to all rules that were extracted at most 10 times. The lexical translation weights are obtained as usual. 6 Experimental Results We considered three reasonable baselines: (i) minimal eMBOT, (ii) non-contiguous STSSG (Sun et al., 2009), or (iii) a string-to-tree Moses system. We decided against the minimal MMBOT as a baseline since tree-to-tree systems generally get lower BLEU scores than string-to-tree systems. We nevertheless present its BLEU scores (see Table 3). Unfortunately, we could not compare to Sun et al. (2009) because their decoder and rule extraction algorithms are not </context>
</contexts>
<marker>Good, 1953</marker>
<rawString>Irving J. Good. 1953. The population frequencies of species and the estimation of population parameters. Biometrika, 40(3–4):237–264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
<author>Ryan Roth</author>
</authors>
<title>MADA+TOKAN: A toolkit for Arabic tokenization, diacritization, morphological disambiguation, POS tagging, stemming and lemmatization.</title>
<date>2009</date>
<booktitle>In Proc. 2nd MEDAR,</booktitle>
<pages>102--109</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="22283" citStr="Habash et al., 2009" startWordPosition="3812" endWordPosition="3815">ons of Section 4. Additional glue-rules that concatenate partial translations without performing any reordering are used in all systems. For all experiments (English-to-German, English-to-Arabic, and English-to-Chinese), the training data was length-ratio filtered. The word alignments were generated by GIZA++ (Och and Ney, 2003) with the grow-diag-final-and heuristic (Koehn et al., 2005). The following language-specific processing was performed. The German text was true-cased and the functional and morphological annotations were removed from the parse. The Arabic text was tokenized with MADA (Habash et al., 2009) and transliterated according to Buckwalter (2002). Finally, the Chinese text was word-segmented using the Stanford Word Segmenter (Chang et al., 2008). In all experiments the feature weights Ai of the log-linear model were trained using minimum error rate training (Och, 2003). The remaining information for the experiments is presented in Table 2. 6.2 Quantitative Analysis The overall translation quality was measured with 4-gram BLEU (Papineni et al., 2002) on truecased data for German, on transliterated data for Arabic, and on word-segmented data for Chinese. Significance was computed with Gi</context>
</contexts>
<marker>Habash, Rambow, Roth, 2009</marker>
<rawString>Nizar Habash, Owen Rambow, and Ryan Roth. 2009. MADA+TOKAN: A toolkit for Arabic tokenization, diacritization, morphological disambiguation, POS tagging, stemming and lemmatization. In Proc. 2nd MEDAR, pages 102–109. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hieu Hoang</author>
<author>Philipp Koehn</author>
<author>Adam Lopez</author>
</authors>
<title>A unified framework for phrase-based, hierarchical, and syntax-based statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proc. 6th IWSLT,</booktitle>
<pages>152--159</pages>
<contexts>
<context position="3228" citStr="Hoang et al., 2009" startWordPosition="474" endWordPosition="477">same spirit as in (Chiang, 2007). In this manner, we combine the advantages of the hierarchical phrase-based approach on the source side and the tree-based approach with discontinuiety on the target side. We evaluate our new system in 3 large-scale experiments using translation tasks, in which we expect discontinuiety on the target. MBOTs are powerful but asymmetric models since discontinuiety is available only on the target. We chose to translate from English to German, Arabic, and Chinese. In all experiments our new system significantly outperforms the string-to-tree syntax-based component (Hoang et al., 2009) of Moses. The (potentially) discontiguous rules of our model are very useful in these setups, which we confirm in a quantitative and qualitative analysis. 2 Related work Modern statistical machine translation systems (Koehn, 2009) are based on different translation models. Syntax-based systems have become widely used because of their ability to handle non-local reordering and other linguistic phenomena better than phrase-based models (Och and Ney, 2004). Synchronous tree substitution grammars (STSGs) of Eisner (2003) use a single source and target tree fragment per rule. In contrast, an eMBOT</context>
<context position="21141" citStr="Hoang et al., 2009" startWordPosition="3640" endWordPosition="3643">tring-to-tree MBOT 143,661,376 491,307,787 162,240,663 string-to-tree Moses 14,092,729 55,169,043 17,047,570 Table 1: Overview of numbers of extracted rules with respect to the different extraction algorithms. ratio filtered to sentences up to 50 characters. We do not modify those sets. • Only rules with at most one gap were allowed which would be equivalent to restrict the number of target tree fragments to 2 in our system. Hence we decided to use a string-to-tree Moses system as baseline (see Section 6.1). 6.1 Setup As a baseline system for our experiments we use the syntax-based component (Hoang et al., 2009) of the Moses toolkit (Koehn et al., 2007). Our system is the presented translation system based on MBOTs. We use the MBOT-Moses decoder (Braune et al., 2013) which – similar to the baseline decoder – uses a CYK+ chart parsing algorithm using a standard X-style parse tree which is sped up by cube pruning (Chiang, 2007) with integrated language model scoring. Our and the baseline system use linguistic syntactic annotation (parses) only on the target side (string-to-tree). During rule extraction we impose the restrictions of Section 4. Additional glue-rules that concatenate partial translations </context>
</contexts>
<marker>Hoang, Koehn, Lopez, 2009</marker>
<rawString>Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009. A unified framework for phrase-based, hierarchical, and syntax-based statistical machine translation. In Proc. 6th IWSLT, pages 152–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. 2003 NAACL,</booktitle>
<pages>48--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9678" citStr="Koehn et al. (2003)" startWordPosition="1515" endWordPosition="1518">mall fragments and thus word-by-word translation. Consequently, such fixed phrases will often be assembled inconsistently by substitution from small fragments. Non-minimal rules encourage a consistent translation by covering larger parts of the source sentence. Here we want to develop an efficient rule extraction procedure for our string-to-tree MBOTs that avoids the mentioned drawbacks. Naturally, we could substitute minimal rules into each other to obtain non-minimal rules, but performing substitution for all combinations is clearly intractable. Instead we essentially follow the approach of Koehn et al. (2003), Och and Ney (2004), and Chiang (2007), which is based on consistently aligned phrase pairs. Our training corpus contains word-aligned sentence pairs (e, A,f), which contain a source language sentence e, a target language sentence f, and an alignment A C_ [1,`e] x [1, `f], where `e and `f are the lengths of the sentences e and f, respectively, and [i, i&apos;] = {j E Z |i &lt; j &lt; i&apos;} is the span (closed interval of integers) from i to i&apos; for all positive integers i &lt; i&apos;. Rules are extracted for each pair of the corpus, so in the following let (e, A, f) be a word-aligned sentence pair. A source phras</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. 2003 NAACL, pages 48–54. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Amittai Axelrod</author>
<author>Alexandra Birch Mayne</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>David Talbot</author>
</authors>
<title>Edinburgh system description for the 2005 IWSLT Speech Translation Evaluation.</title>
<date>2005</date>
<booktitle>In Proc. 2nd IWSLT,</booktitle>
<pages>68--75</pages>
<contexts>
<context position="22053" citStr="Koehn et al., 2005" startWordPosition="3779" endWordPosition="3782">cube pruning (Chiang, 2007) with integrated language model scoring. Our and the baseline system use linguistic syntactic annotation (parses) only on the target side (string-to-tree). During rule extraction we impose the restrictions of Section 4. Additional glue-rules that concatenate partial translations without performing any reordering are used in all systems. For all experiments (English-to-German, English-to-Arabic, and English-to-Chinese), the training data was length-ratio filtered. The word alignments were generated by GIZA++ (Och and Ney, 2003) with the grow-diag-final-and heuristic (Koehn et al., 2005). The following language-specific processing was performed. The German text was true-cased and the functional and morphological annotations were removed from the parse. The Arabic text was tokenized with MADA (Habash et al., 2009) and transliterated according to Buckwalter (2002). Finally, the Chinese text was word-segmented using the Stanford Word Segmenter (Chang et al., 2008). In all experiments the feature weights Ai of the log-linear model were trained using minimum error rate training (Och, 2003). The remaining information for the experiments is presented in Table 2. 6.2 Quantitative Ana</context>
</contexts>
<marker>Koehn, Axelrod, Mayne, Callison-Burch, Osborne, Talbot, 2005</marker>
<rawString>Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh system description for the 2005 IWSLT Speech Translation Evaluation. In Proc. 2nd IWSLT, pages 68–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. 45th ACL,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="1505" citStr="Koehn et al., 2007" startWordPosition="203" endWordPosition="206">ntroduction We present an application of a variant of local multi bottom-up tree transducers (eMBOTs) as proposed in Maletti (2011) to statistical machine translation. eMBOTs allow discontinuities on the target language side since they have a sequence of target tree fragments instead of a single tree fragment in their rules. The original approach makes use of syntactic information on both the source and the target side (tree-to-tree) and a corresponding minimal rule extraction is presented in (Maletti, 2011). Braune et al. (2013) implemented it as well as a decoder inside the Moses framework (Koehn et al., 2007) and demonstrated that the resulting tree-to-tree eMBOT system significantly improved over its tree-to-tree baseline using minimal rules. We can see at least two drawbacks in this approach. First, experiments investigating the integration of syntactic information on both sides generally report quality deterioration. For example, Lavie et al. (2008), Liu et al. (2009), and Chiang (2010) noted that translation quality tends to decrease in tree-to-tree systems because the rules become too restrictive. Second, minimal rules (i.e., rules that cannot be obtained from other extracted rules) typically</context>
<context position="21183" citStr="Koehn et al., 2007" startWordPosition="3648" endWordPosition="3651"> 162,240,663 string-to-tree Moses 14,092,729 55,169,043 17,047,570 Table 1: Overview of numbers of extracted rules with respect to the different extraction algorithms. ratio filtered to sentences up to 50 characters. We do not modify those sets. • Only rules with at most one gap were allowed which would be equivalent to restrict the number of target tree fragments to 2 in our system. Hence we decided to use a string-to-tree Moses system as baseline (see Section 6.1). 6.1 Setup As a baseline system for our experiments we use the syntax-based component (Hoang et al., 2009) of the Moses toolkit (Koehn et al., 2007). Our system is the presented translation system based on MBOTs. We use the MBOT-Moses decoder (Braune et al., 2013) which – similar to the baseline decoder – uses a CYK+ chart parsing algorithm using a standard X-style parse tree which is sped up by cube pruning (Chiang, 2007) with integrated language model scoring. Our and the baseline system use linguistic syntactic annotation (parses) only on the target side (string-to-tree). During rule extraction we impose the restrictions of Section 4. Additional glue-rules that concatenate partial translations without performing any reordering are used</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. 45th ACL, pages 177–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. 10th MT Summit,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="23781" citStr="Koehn, 2005" startWordPosition="4046" endWordPosition="4047">bic we get an increase of 0.78 points which results in 49.10 BLEU. For Chinese we obtain a score of 18.35 BLEU gaining 0.66 points.4 We also trained a vanilla phrase-based system for each language pair on the same data as described in Table 2. To demonstrate the usefulness of the multiple 4NIST-08 also shows BLEU for word-segmented output (http://www.itl.nist.gov/iad/mig/tests/ mt/2008/doc/mt08_ official _ results _v0. html). Best constrained system: 17.69 BLEU; best unconstrained system: 19.63 BLEU. 820 English to German English to Arabic English to Chinese training data 7th EuroParl corpus (Koehn, 2005) MultiUN corpus (Eisele and Chen, 2010) training data size ≈ 1.8M sentence pairs ≈ 5.7M sentence pairs ≈ 1.9M sentence pairs target-side parser BitPar (Schmid, 2004) Berkeley parser (Petrov et al., 2006) language model 5-gram SRILM (Stolcke, 2002) add. LM data WMT 2013 Arabic in MultiUN Chinese in MultiUN LM data size ≈ 57M sentences ≈ 9.7M sentences ≈ 9.5M sentences tuning data WMT 2013 cut from MultiUN NIST 2002, 2003, 2005 tuning size 3,000 sentences 2,000 sentences 2,879 sentences test data WMT 2013 (Bojar et al., 2013) cut from MultiUN NIST 2008 (NIST, 2010) test size 3,000 sentences 1,00</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proc. 10th MT Summit, pages 79–86. Association for Machine Translation in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical Machine Translation.</title>
<date>2009</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="3459" citStr="Koehn, 2009" startWordPosition="511" endWordPosition="512">arge-scale experiments using translation tasks, in which we expect discontinuiety on the target. MBOTs are powerful but asymmetric models since discontinuiety is available only on the target. We chose to translate from English to German, Arabic, and Chinese. In all experiments our new system significantly outperforms the string-to-tree syntax-based component (Hoang et al., 2009) of Moses. The (potentially) discontiguous rules of our model are very useful in these setups, which we confirm in a quantitative and qualitative analysis. 2 Related work Modern statistical machine translation systems (Koehn, 2009) are based on different translation models. Syntax-based systems have become widely used because of their ability to handle non-local reordering and other linguistic phenomena better than phrase-based models (Och and Ney, 2004). Synchronous tree substitution grammars (STSGs) of Eisner (2003) use a single source and target tree fragment per rule. In contrast, an eMBOT rule contains a single source tree 815 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 815–824, Beijing, Chin</context>
<context position="18667" citStr="Koehn, 2009" startWordPosition="3240" endWordPosition="3241">magnitude. 5 Model Features For each source language sentence e, we want to determine its most likely translation fˆ given by fˆ = arg maxf p(f |e) = arg maxf p(e |f) p(f) 3Note that this restricts the set of initial rules. for some unknown probability distributions p. We estimate p(e |f) p(f) by a log-linear combination of features hi( ) with weights λi scored on sentential forms e —* (t) of our extracted MBOT M such that the leaves of t read (left-to-right) f. We use the decoder provided by MBOT-Moses of Braune et al. (2013) and its standard features, which includes all the common features (Koehn, 2009) and a gap penalty 1001−c, where c is the number of target tree fragments that contributed to t. This feature discourages rules with many target tree fragments. As usual, all features are obtained as the product of the corresponding rule features for the rules used to derive e —* (t) by means of substitution. The rule weights for the translation weights are obtained as relative frequencies normalized over all rules with the same right- and left-hand side. Good-Turing smoothing (Good, 1953) is applied to all rules that were extracted at most 10 times. The lexical translation weights are obtaine</context>
</contexts>
<marker>Koehn, 2009</marker>
<rawString>Philipp Koehn. 2009. Statistical Machine Translation. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Alok Parlikar</author>
<author>Vamshi Ambati</author>
</authors>
<title>Syntax-driven learning of sub-sentential translation equivalents and translation rules from parsed parallel corpora.</title>
<date>2008</date>
<booktitle>In Proc. 2nd SSST,</booktitle>
<pages>87--95</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1855" citStr="Lavie et al. (2008)" startWordPosition="254" endWordPosition="257">akes use of syntactic information on both the source and the target side (tree-to-tree) and a corresponding minimal rule extraction is presented in (Maletti, 2011). Braune et al. (2013) implemented it as well as a decoder inside the Moses framework (Koehn et al., 2007) and demonstrated that the resulting tree-to-tree eMBOT system significantly improved over its tree-to-tree baseline using minimal rules. We can see at least two drawbacks in this approach. First, experiments investigating the integration of syntactic information on both sides generally report quality deterioration. For example, Lavie et al. (2008), Liu et al. (2009), and Chiang (2010) noted that translation quality tends to decrease in tree-to-tree systems because the rules become too restrictive. Second, minimal rules (i.e., rules that cannot be obtained from other extracted rules) typically consist of a few lexical items only and are thus not the most suitable to translate idiomatic expressions and other fixed phrases. To overcome these drawbacks, we abolish the syntactic information for the source side and develop a string-to-tree variant of eMBOTs. In addition, we develop a new rule extraction algorithm that can also extract non-mi</context>
</contexts>
<marker>Lavie, Parlikar, Ambati, 2008</marker>
<rawString>Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008. Syntax-driven learning of sub-sentential translation equivalents and translation rules from parsed parallel corpora. In Proc. 2nd SSST, pages 87–95. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Yajuan L¨u</author>
<author>Qun Liu</author>
</authors>
<title>Improving tree-to-tree translation with packed forests.</title>
<date>2009</date>
<booktitle>In Proc. 47th ACL,</booktitle>
<pages>558--566</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Liu, L¨u, Liu, 2009</marker>
<rawString>Yang Liu, Yajuan L¨u, and Qun Liu. 2009. Improving tree-to-tree translation with packed forests. In Proc. 47th ACL, pages 558–566. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Maletti</author>
</authors>
<title>How to train your multi bottom-up tree transducer.</title>
<date>2011</date>
<booktitle>In Proc. 49th ACL,</booktitle>
<pages>825--834</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1017" citStr="Maletti (2011)" startWordPosition="126" endWordPosition="127">bottom-up tree transducers. Our new parameterized rule extraction algorithm extracts string-to-tree rules that can be discontiguous and non-minimal in contrast to existing algorithms for the tree-to-tree setting. The obtained models significantly outperform the string-to-tree component of the Moses framework in a large-scale empirical evaluation on several known translation tasks. Our linguistic analysis reveals the remarkable benefits of discontiguous and non-minimal rules. 1 Introduction We present an application of a variant of local multi bottom-up tree transducers (eMBOTs) as proposed in Maletti (2011) to statistical machine translation. eMBOTs allow discontinuities on the target language side since they have a sequence of target tree fragments instead of a single tree fragment in their rules. The original approach makes use of syntactic information on both the source and the target side (tree-to-tree) and a corresponding minimal rule extraction is presented in (Maletti, 2011). Braune et al. (2013) implemented it as well as a decoder inside the Moses framework (Koehn et al., 2007) and demonstrated that the resulting tree-to-tree eMBOT system significantly improved over its tree-to-tree base</context>
<context position="7457" citStr="Maletti (2011)" startWordPosition="1177" endWordPosition="1178"> time. We illustrate substitution in Figure 2, where we replace the placeholder X in the source string, which is linked to the underlined leaves NP and PP in the target tree fragments. The rule below (also in Figure 1) is also a sentential form and matches since its (underlined) root labels of the target tree fragments read “NP PP”. Thus, we can substitute the latter sentential form into the former and obtain the sentential form shown at the bottom of Figure 2. Ideally, the substitution process is repeated until the complete source sentence is derived. 4 Rule Extraction The rule extraction of Maletti (2011) extracts minimal tree-to-tree rules, which are rules containing both source and target tree fragments, from sentence pairs of a word-aligned and bi-parsed parallel corpus. In particular, this requires parses for both the source and the target language sentences which adds a source for errors and specificity potentially leading to lower translation performance and lower coverage (Wellington et al., 2006). Chiang (2010) showed that string-to-tree systems— Matching sentential forms (underlining for emphasis): VAFIN VP concludes X → (, NP , ist PP geschlossen PP �X on X → NP , � ¨uber NN � VAFIN </context>
</contexts>
<marker>Maletti, 2011</marker>
<rawString>Andreas Maletti. 2011. How to train your multi bottom-up tree transducer. In Proc. 49th ACL, pages 825–834. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<title>open machine translation evaluation. Linguistic Data Consortium.</title>
<date>2010</date>
<note>LDC2010T10 [T11, T14, T21].</note>
<contexts>
<context position="24350" citStr="NIST, 2010" startWordPosition="4141" endWordPosition="4142">ing data 7th EuroParl corpus (Koehn, 2005) MultiUN corpus (Eisele and Chen, 2010) training data size ≈ 1.8M sentence pairs ≈ 5.7M sentence pairs ≈ 1.9M sentence pairs target-side parser BitPar (Schmid, 2004) Berkeley parser (Petrov et al., 2006) language model 5-gram SRILM (Stolcke, 2002) add. LM data WMT 2013 Arabic in MultiUN Chinese in MultiUN LM data size ≈ 57M sentences ≈ 9.7M sentences ≈ 9.5M sentences tuning data WMT 2013 cut from MultiUN NIST 2002, 2003, 2005 tuning size 3,000 sentences 2,000 sentences 2,879 sentences test data WMT 2013 (Bojar et al., 2013) cut from MultiUN NIST 2008 (NIST, 2010) test size 3,000 sentences 1,000 sentences 1,859 sentences Table 2: Summary of the performed experiments. Language pair System BLEU Moses Baseline 15.22 MBOT ∗15.90 English-to-German minimal $MBOT 14.09 Phrase-based Moses 16.73 Moses Baseline 48.32 MBOT ∗49.10 English-to-Arabic minimal $MBOT 32.88 Phrase-based Moses 50.27 Moses Baseline 17.69 English-to-Chinese MBOT ∗18.35 minimal $MBOT 12.01 Phrase-based Moses 18.09 Table 3: Evaluation results. The starred results are statistically significant improvements over the baseline (at confidence p &lt; 1%). target tree fragments of MBOTs, we analyzed t</context>
</contexts>
<marker>NIST, 2010</marker>
<rawString>NIST. 2010. NIST 2002 [2003, 2005, 2008] open machine translation evaluation. Linguistic Data Consortium. LDC2010T10 [T11, T14, T21].</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="21993" citStr="Och and Ney, 2003" startWordPosition="3771" endWordPosition="3774">hm using a standard X-style parse tree which is sped up by cube pruning (Chiang, 2007) with integrated language model scoring. Our and the baseline system use linguistic syntactic annotation (parses) only on the target side (string-to-tree). During rule extraction we impose the restrictions of Section 4. Additional glue-rules that concatenate partial translations without performing any reordering are used in all systems. For all experiments (English-to-German, English-to-Arabic, and English-to-Chinese), the training data was length-ratio filtered. The word alignments were generated by GIZA++ (Och and Ney, 2003) with the grow-diag-final-and heuristic (Koehn et al., 2005). The following language-specific processing was performed. The German text was true-cased and the functional and morphological annotations were removed from the parse. The Arabic text was tokenized with MADA (Habash et al., 2009) and transliterated according to Buckwalter (2002). Finally, the Chinese text was word-segmented using the Stanford Word Segmenter (Chang et al., 2008). In all experiments the feature weights Ai of the log-linear model were trained using minimum error rate training (Och, 2003). The remaining information for t</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz J. Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="3686" citStr="Och and Ney, 2004" startWordPosition="541" endWordPosition="544">glish to German, Arabic, and Chinese. In all experiments our new system significantly outperforms the string-to-tree syntax-based component (Hoang et al., 2009) of Moses. The (potentially) discontiguous rules of our model are very useful in these setups, which we confirm in a quantitative and qualitative analysis. 2 Related work Modern statistical machine translation systems (Koehn, 2009) are based on different translation models. Syntax-based systems have become widely used because of their ability to handle non-local reordering and other linguistic phenomena better than phrase-based models (Och and Ney, 2004). Synchronous tree substitution grammars (STSGs) of Eisner (2003) use a single source and target tree fragment per rule. In contrast, an eMBOT rule contains a single source tree 815 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 815–824, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics VAFIN VP concludes X → ( , NP , ist PP geschlossen PP X on X → (NP , � � ¨uber NN NN NP human rights → ( ) the X → Menschenrechte die NN Figure 1: Several va</context>
<context position="9698" citStr="Och and Ney (2004)" startWordPosition="1519" endWordPosition="1522">us word-by-word translation. Consequently, such fixed phrases will often be assembled inconsistently by substitution from small fragments. Non-minimal rules encourage a consistent translation by covering larger parts of the source sentence. Here we want to develop an efficient rule extraction procedure for our string-to-tree MBOTs that avoids the mentioned drawbacks. Naturally, we could substitute minimal rules into each other to obtain non-minimal rules, but performing substitution for all combinations is clearly intractable. Instead we essentially follow the approach of Koehn et al. (2003), Och and Ney (2004), and Chiang (2007), which is based on consistently aligned phrase pairs. Our training corpus contains word-aligned sentence pairs (e, A,f), which contain a source language sentence e, a target language sentence f, and an alignment A C_ [1,`e] x [1, `f], where `e and `f are the lengths of the sentences e and f, respectively, and [i, i&apos;] = {j E Z |i &lt; j &lt; i&apos;} is the span (closed interval of integers) from i to i&apos; for all positive integers i &lt; i&apos;. Rules are extracted for each pair of the corpus, so in the following let (e, A, f) be a word-aligned sentence pair. A source phrase is simply a span [</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz J. Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. 41st ACL,</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="22560" citStr="Och, 2003" startWordPosition="3857" endWordPosition="3858">ere generated by GIZA++ (Och and Ney, 2003) with the grow-diag-final-and heuristic (Koehn et al., 2005). The following language-specific processing was performed. The German text was true-cased and the functional and morphological annotations were removed from the parse. The Arabic text was tokenized with MADA (Habash et al., 2009) and transliterated according to Buckwalter (2002). Finally, the Chinese text was word-segmented using the Stanford Word Segmenter (Chang et al., 2008). In all experiments the feature weights Ai of the log-linear model were trained using minimum error rate training (Och, 2003). The remaining information for the experiments is presented in Table 2. 6.2 Quantitative Analysis The overall translation quality was measured with 4-gram BLEU (Papineni et al., 2002) on truecased data for German, on transliterated data for Arabic, and on word-segmented data for Chinese. Significance was computed with Gimpel’s implementation (Gimpel, 2011) of pairwise bootstrap resampling with 1,000 samples. Table 3 lists the evaluation results. In all three setups the MBOT system significantly outperforms the baseline. For German we obtain a BLEU score of 15.90 which is a gain of 0.68 points</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz J. Och. 2003. Minimum error rate training in statistical machine translation. In Proc. 41st ACL, pages 160–167. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei jing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. 40th ACL,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="22744" citStr="Papineni et al., 2002" startWordPosition="3883" endWordPosition="3886">n text was true-cased and the functional and morphological annotations were removed from the parse. The Arabic text was tokenized with MADA (Habash et al., 2009) and transliterated according to Buckwalter (2002). Finally, the Chinese text was word-segmented using the Stanford Word Segmenter (Chang et al., 2008). In all experiments the feature weights Ai of the log-linear model were trained using minimum error rate training (Och, 2003). The remaining information for the experiments is presented in Table 2. 6.2 Quantitative Analysis The overall translation quality was measured with 4-gram BLEU (Papineni et al., 2002) on truecased data for German, on transliterated data for Arabic, and on word-segmented data for Chinese. Significance was computed with Gimpel’s implementation (Gimpel, 2011) of pairwise bootstrap resampling with 1,000 samples. Table 3 lists the evaluation results. In all three setups the MBOT system significantly outperforms the baseline. For German we obtain a BLEU score of 15.90 which is a gain of 0.68 points. For Arabic we get an increase of 0.78 points which results in 49.10 BLEU. For Chinese we obtain a score of 18.35 BLEU gaining 0.66 points.4 We also trained a vanilla phrase-based sys</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. 40th ACL, pages 311–318. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proc. 44th ACL,</booktitle>
<pages>433--440</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="23984" citStr="Petrov et al., 2006" startWordPosition="4076" endWordPosition="4079">e pair on the same data as described in Table 2. To demonstrate the usefulness of the multiple 4NIST-08 also shows BLEU for word-segmented output (http://www.itl.nist.gov/iad/mig/tests/ mt/2008/doc/mt08_ official _ results _v0. html). Best constrained system: 17.69 BLEU; best unconstrained system: 19.63 BLEU. 820 English to German English to Arabic English to Chinese training data 7th EuroParl corpus (Koehn, 2005) MultiUN corpus (Eisele and Chen, 2010) training data size ≈ 1.8M sentence pairs ≈ 5.7M sentence pairs ≈ 1.9M sentence pairs target-side parser BitPar (Schmid, 2004) Berkeley parser (Petrov et al., 2006) language model 5-gram SRILM (Stolcke, 2002) add. LM data WMT 2013 Arabic in MultiUN Chinese in MultiUN LM data size ≈ 57M sentences ≈ 9.7M sentences ≈ 9.5M sentences tuning data WMT 2013 cut from MultiUN NIST 2002, 2003, 2005 tuning size 3,000 sentences 2,000 sentences 2,879 sentences test data WMT 2013 (Bojar et al., 2013) cut from MultiUN NIST 2008 (NIST, 2010) test size 3,000 sentences 1,000 sentences 1,859 sentences Table 2: Summary of the performed experiments. Language pair System BLEU Moses Baseline 15.22 MBOT ∗15.90 English-to-German minimal $MBOT 14.09 Phrase-based Moses 16.73 Moses </context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proc. 44th ACL, pages 433–440. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Efficient parsing of highly ambiguous context-free grammars with bit vectors.</title>
<date>2004</date>
<booktitle>In Proc. 20th COLING,</booktitle>
<pages>162--168</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="23946" citStr="Schmid, 2004" startWordPosition="4072" endWordPosition="4073">e-based system for each language pair on the same data as described in Table 2. To demonstrate the usefulness of the multiple 4NIST-08 also shows BLEU for word-segmented output (http://www.itl.nist.gov/iad/mig/tests/ mt/2008/doc/mt08_ official _ results _v0. html). Best constrained system: 17.69 BLEU; best unconstrained system: 19.63 BLEU. 820 English to German English to Arabic English to Chinese training data 7th EuroParl corpus (Koehn, 2005) MultiUN corpus (Eisele and Chen, 2010) training data size ≈ 1.8M sentence pairs ≈ 5.7M sentence pairs ≈ 1.9M sentence pairs target-side parser BitPar (Schmid, 2004) Berkeley parser (Petrov et al., 2006) language model 5-gram SRILM (Stolcke, 2002) add. LM data WMT 2013 Arabic in MultiUN Chinese in MultiUN LM data size ≈ 57M sentences ≈ 9.7M sentences ≈ 9.5M sentences tuning data WMT 2013 cut from MultiUN NIST 2002, 2003, 2005 tuning size 3,000 sentences 2,000 sentences 2,879 sentences test data WMT 2013 (Bojar et al., 2013) cut from MultiUN NIST 2008 (NIST, 2010) test size 3,000 sentences 1,000 sentences 1,859 sentences Table 2: Summary of the performed experiments. Language pair System BLEU Moses Baseline 15.22 MBOT ∗15.90 English-to-German minimal $MBOT</context>
</contexts>
<marker>Schmid, 2004</marker>
<rawString>Helmut Schmid. 2004. Efficient parsing of highly ambiguous context-free grammars with bit vectors. In Proc. 20th COLING, pages 162–168. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM — an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. 7th INTERSPEECH,</booktitle>
<pages>257--286</pages>
<contexts>
<context position="24028" citStr="Stolcke, 2002" startWordPosition="4084" endWordPosition="4085">o demonstrate the usefulness of the multiple 4NIST-08 also shows BLEU for word-segmented output (http://www.itl.nist.gov/iad/mig/tests/ mt/2008/doc/mt08_ official _ results _v0. html). Best constrained system: 17.69 BLEU; best unconstrained system: 19.63 BLEU. 820 English to German English to Arabic English to Chinese training data 7th EuroParl corpus (Koehn, 2005) MultiUN corpus (Eisele and Chen, 2010) training data size ≈ 1.8M sentence pairs ≈ 5.7M sentence pairs ≈ 1.9M sentence pairs target-side parser BitPar (Schmid, 2004) Berkeley parser (Petrov et al., 2006) language model 5-gram SRILM (Stolcke, 2002) add. LM data WMT 2013 Arabic in MultiUN Chinese in MultiUN LM data size ≈ 57M sentences ≈ 9.7M sentences ≈ 9.5M sentences tuning data WMT 2013 cut from MultiUN NIST 2002, 2003, 2005 tuning size 3,000 sentences 2,000 sentences 2,879 sentences test data WMT 2013 (Bojar et al., 2013) cut from MultiUN NIST 2008 (NIST, 2010) test size 3,000 sentences 1,000 sentences 1,859 sentences Table 2: Summary of the performed experiments. Language pair System BLEU Moses Baseline 15.22 MBOT ∗15.90 English-to-German minimal $MBOT 14.09 Phrase-based Moses 16.73 Moses Baseline 48.32 MBOT ∗49.10 English-to-Arabic</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM — an extensible language modeling toolkit. In Proc. 7th INTERSPEECH, pages 257–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Sun</author>
<author>Min Zhang</author>
<author>Chew Lim Tan</author>
</authors>
<title>A noncontiguous tree sequence alignment-based model for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proc. 47th ACL,</booktitle>
<pages>914--922</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4456" citStr="Sun et al. (2009)" startWordPosition="674" endWordPosition="677">ins a single source tree 815 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 815–824, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics VAFIN VP concludes X → ( , NP , ist PP geschlossen PP X on X → (NP , � � ¨uber NN NN NP human rights → ( ) the X → Menschenrechte die NN Figure 1: Several valid rules for our MBOT. fragment and a sequence of target tree fragments. eMBOTs can also be understood as a restriction of the non-contiguous STSSGs of Sun et al. (2009), which allow a sequence of source tree fragments and a sequence of target tree fragments. eMBOT rules require exactly one source tree fragment. While the mentioned syntax-based models use tree fragments for source and target (tree-to-tree), Galley et al. (2004) and Galley et al. (2006) use syntactic annotations only on the target language side (string-to-tree). Further research by DeNeefe et al. (2007) revealed that adding non-minimal rules improves translation quality in this setting. Here we improve statistical machine translation in this setting even further using non-minimal eMBOT rules. </context>
<context position="19407" citStr="Sun et al., 2009" startWordPosition="3362" endWordPosition="3365">ules with many target tree fragments. As usual, all features are obtained as the product of the corresponding rule features for the rules used to derive e —* (t) by means of substitution. The rule weights for the translation weights are obtained as relative frequencies normalized over all rules with the same right- and left-hand side. Good-Turing smoothing (Good, 1953) is applied to all rules that were extracted at most 10 times. The lexical translation weights are obtained as usual. 6 Experimental Results We considered three reasonable baselines: (i) minimal eMBOT, (ii) non-contiguous STSSG (Sun et al., 2009), or (iii) a string-to-tree Moses system. We decided against the minimal MMBOT as a baseline since tree-to-tree systems generally get lower BLEU scores than string-to-tree systems. We nevertheless present its BLEU scores (see Table 3). Unfortunately, we could not compare to Sun et al. (2009) because their decoder and rule extraction algorithms are not publicly available. Furthermore, we have the impression that their system does not scale well: • Only around 240,000 training sentences were used. Our training data contains between 1.8M and 5.7M sentence pairs. • The development and test set wer</context>
</contexts>
<marker>Sun, Zhang, Tan, 2009</marker>
<rawString>Jun Sun, Min Zhang, and Chew Lim Tan. 2009. A noncontiguous tree sequence alignment-based model for statistical machine translation. In Proc. 47th ACL, pages 914–922. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Wellington</author>
<author>Sonjia Waxmonsky</author>
<author>I Dan Melamed</author>
</authors>
<title>Empirical lower bounds on the complexity of translational equivalence.</title>
<date>2006</date>
<booktitle>In Proc. 44th ACL,</booktitle>
<pages>977--984</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7864" citStr="Wellington et al., 2006" startWordPosition="1238" endWordPosition="1241">rmer and obtain the sentential form shown at the bottom of Figure 2. Ideally, the substitution process is repeated until the complete source sentence is derived. 4 Rule Extraction The rule extraction of Maletti (2011) extracts minimal tree-to-tree rules, which are rules containing both source and target tree fragments, from sentence pairs of a word-aligned and bi-parsed parallel corpus. In particular, this requires parses for both the source and the target language sentences which adds a source for errors and specificity potentially leading to lower translation performance and lower coverage (Wellington et al., 2006). Chiang (2010) showed that string-to-tree systems— Matching sentential forms (underlining for emphasis): VAFIN VP concludes X → (, NP , ist PP geschlossen PP �X on X → NP , � ¨uber NN � VAFIN concludes X on X → ( , NP , ist �geschlossen PP ¨uber NN 816 PROAV[1,1] damit1 that1 concludes2 the3 debate4 on5 human6 rights7 VAFIN[2,2] ist2 ART[3,3] die3 NP[3,4] Aussprache4 TOP[1,7] NN[4,4] APPR[5,5] ¨uber5 PP[5,6] Menschenrechte6 NN[6,6] VP[5,7] geschlossen7 VVPP[7,7] Figure 3: Word-aligned sentence pair with targetside parse. which he calls fuzzy tree-to-tree-systems— generally yield higher transl</context>
</contexts>
<marker>Wellington, Waxmonsky, Melamed, 2006</marker>
<rawString>Benjamin Wellington, Sonjia Waxmonsky, and I. Dan Melamed. 2006. Empirical lower bounds on the complexity of translational equivalence. In Proc. 44th ACL, pages 977–984. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>