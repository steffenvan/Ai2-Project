<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012274">
<title confidence="0.891343">
Sense and Deduction: The Power of Peewees Applied
to the SENSEVAL-2 Swedish Lexical Sample Task
</title>
<author confidence="0.798172">
Torbjorn Lager t and Natalia Zinovjevat
</author>
<affiliation confidence="0.8627675">
1-Department of Linguistics, Uppsala University
tHapax Information Systems AB, Stockholm
</affiliation>
<sectionHeader confidence="0.966973" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999947666666667">
This paper describes our use of Prolog Word
Experts (PWEs) in the SENSEVAL-2 competi-
tion. We explain how we specify our PWEs as
sequences of transformation rules and how they
can be trained on sense tagged corpus data. We
give a semantics of PWEs by translating them
into first order predicate logic, and we describe
how PWEs can be compiled into Prolog pro-
cedures. We finally present our results for the
Swedish lexical sample task: 63% (fine-grained
score) for our best PWE, and a second place in
the ranking.
</bodyText>
<sectionHeader confidence="0.998516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.990521956521739">
Word experts are small expert system-like mod-
ules for processing a particular target word
based on neighboring words. Typically, a word
expert uses rules that test the identity and rela-
tive position of words in the context in order to
infer the role of the target word in the passage
(Berleant, 1995). In this paper, we describe the
development of various kinds of word experts in
a logic programming framework, dealing with
word sense disambiguation in the context of the
SENSEVAL-2 competition.
In a logic programming framework, the task
of engineering a word (sense) expert can be
specified as follows. Given a suitable represen-
tation of a text, we want to define a predicate
sense/2 such that sense(P,S) is true if the
word at position P in the text has the sense S. In
the remainder of the paper, we will refer to this
kind of word expert as a Prolog Word Expert (or
PWE for short — &amp;quot;Peewee&amp;quot; to its friends). This
is to distinguish it from other kinds of word ex-
perts, and to emphasize the fact that it is &apos;pro-
grammed in logic&apos;.
</bodyText>
<sectionHeader confidence="0.899541" genericHeader="method">
2 The Anatomy of a Peewee
</sectionHeader>
<subsectionHeader confidence="0.919994">
2.1 Peewee Specifications
</subsectionHeader>
<bodyText confidence="0.9966385">
In the present paper, a word expert&apos;s knowledge
will be expressed, not as Prolog clauses defining
sense/2 directly, but as a sequence of transfor-
mation rules. For example, here is how we spec-
ify a word expert which is able to disambiguate
occurrences of interest:1
</bodyText>
<equation confidence="0.9903164">
word_expert sense :=
sense: add 6 &lt;- word: interest@ [0] o
sense : 6&gt;1 &lt;- word: ilia) [1] o
sense : 1&gt;5 &lt;- word: &apos; O [-1] o
end.
</equation>
<bodyText confidence="0.978232904761905">
The first rule works as a default rule, which sim-
ply assigns the most frequent sense to the word
interest (6 in this case). If no other rules apply,
this is the tag that the word will eventually get.
The other rules dictate when — based on the
context — a word should have its tag changed.
The second rule is to be read &amp;quot;replace the tag
for sense 6 with the tag for sense 1, if the next
word is in&amp;quot;. The third rule says &amp;quot;replace the tag
for sense 1 with the tag for sense 5, if the pre-
vious &apos;word&apos; is &apos;W.&amp;quot; The o-symbol is a compo-
sition operator, and CR o Rs) basically means
that the output of applying the rule R forms the
input to the application of the rules Rs. Thus,
rules are strictly order-dependent. Note, for ex-
ample, that the third rule is applicable only if
the second rule is.
Needless to say, the above rules are not at
all sufficient for the task of disambiguating all
uses of interest. But the number of rules can
be increased, and typically a word expert will
</bodyText>
<footnote confidence="0.9454284">
&apos;This word was of course not used the Swedish task,
but is used here for expository reasons. The sense tags
are numbers: l=&amp;quot;readiness to give attention&amp;quot;, 5=&amp;quot;a
company share&amp;quot;, 6= &amp;quot;money paid for the use of money&amp;quot;,
etc.
</footnote>
<page confidence="0.999133">
99
</page>
<bodyText confidence="0.996293">
have access to anything between just a handful
of rules and several hundred ones.2
</bodyText>
<subsectionHeader confidence="0.99796">
2.2 Peewee Logic
</subsectionHeader>
<bodyText confidence="0.999988571428571">
Interestingly, a sequence of transformation rules
can be translated into a set of axioms, expressed
in first-order predicate logic, defining relation-
ships between positions in a text, word forms,
and senses (Lager, 2000; Lager &amp; Nivre, 2001).
For example, the meaning of the rules from the
previous section can be spelled out as follows:
</bodyText>
<equation confidence="0.9917005">
Vp[w (p,inter est) -+ (p,6)]
dpo ,P1[Si (Po ,6) A Pi =Po+1 A w (pi ,in) -4 S2 (PO )1)1
dpo,Pi,x[Si(po,x) A pi=po+1 A ,in) -4 S2 (po,x)}
Vpo ,Pi [S2 (po ,1) A pi=p0-1 A w (pi ,%) S3 (Po ,5)}
Vpo,p1,x[S2(po,x) A pi=p0-1 A ---,w(pi,%) ---+ S3 (po ,x)]
Vx,p{S3(p,x) S(p,x))
</equation>
<bodyText confidence="0.994655071428571">
The idea is that for each rule in the sequence a
new predicate Si is introduced, where the sub-
script indicates where in the sequence the rule
belongs. Semantically, Si relates a position to
a sense, and the formulas define this predicate
in terms of the predicate Si...4 plus a number
of other predicates. Each Si corresponding to a
replacement rule is defined by two sentences —
one stating the conditions under which a sense
tag is replaced with another sense tag, the other
one stating the conditions under which the old
sense tag is kept.
Given a suitable logical representation of a
text, such as
</bodyText>
<equation confidence="0.934887">
w(1, Sue) w(2, developed) w(3, an) w(4, interest)
w(5, in) w (6 , computers) w (7 , and) w(8, bought)
w(9, an) w(10, 11.5) w(11, %) w(12, interest)
w(13, in) w(14, Microsoft)
</equation>
<bodyText confidence="0.992246090909091">
and given a suitable constructive proof method,
the exact identity of the sense of an occurrence
of the word interest — say the word at position
12 — will follow as a logical consequence of the
theory formed by taking the union of the pre-
vious two sets of formulas. For example, the
formula 3x[S(12,x)] is a theorem, for which we
can construct (only) the example x 5, and
we have thus formally proved that this partic-
ular occurrence of interest means &amp;quot;a share in a
company&amp;quot; .3
</bodyText>
<footnote confidence="0.997513666666667">
2A demo of a more potent PWE is available at:
http://www.ling.gu.se/lager/Home/pwe_ui.html
3The theory can be used in other ways too. Searching
</footnote>
<bodyText confidence="0.999906">
What we have here is something that we like
to think of as word sense disambiguation as de-
duction, in analogy to the ideas of parsing as
deduction due to Pereira and Warren (1983).
</bodyText>
<subsectionHeader confidence="0.985366">
2.3 The Peewee Compiler
</subsectionHeader>
<bodyText confidence="0.9812088">
Since the above formulas have already logic pro-
gramming form, it is straightforward to trans-
late them into Prolog. For example, the second
and the third formulas can be translated as fol-
lows:4
</bodyText>
<equation confidence="0.9997895">
s2(P0,1) s1(P0,6), P1 is P0+1, w(P1,in).
s2(P0,X) sl(PO,X), P1 is P0+1, \+ w(P1,in).
</equation>
<bodyText confidence="0.999908428571429">
To write Prolog procedures such as these by
hand for many rules would be tedious and prone
to errors. Fortunately, since the formalism for
transformation rules is compositional, it was
straightforward to write a compiler5 that gener-
ates word expert procedures from word expert
specifications automatically.
</bodyText>
<subsectionHeader confidence="0.980099">
2.4 Peewee Training
</subsectionHeader>
<bodyText confidence="0.999911526315789">
There is an obvious choice of learning method
for training Prolog Word Experts, namely
Transformation-Based Learning (Brill, 1995).
Of course, the fact that transformation rules can
be learned from tagged corpora was a major rea-
son for using them in the first place. The p-TBL
system — described in detail in (Lager, 1999) —
uses the search and database capabilities of the
Prolog programming language to implement a
generalized form of transformation-based learn-
ing. Through its support of a compositional
rule/template formalism and `pluggable&apos; algo-
rithms, the p-TBL system can easily be tailored
to different learning tasks.6
Rules that can be learned in Transformation-
Based Learning are instances of rule templates.
For example, the second of the rules in our ex-
ample PWE specification is an instance of the
following template:
</bodyText>
<equation confidence="0.940099">
sense:A&gt;B &lt;- word:CO[1] .
</equation>
<bodyText confidence="0.533553">
for a word token with a particular sense (say 5) becomes
a matter of constructively proving 3p[S(p,5)].
</bodyText>
<footnote confidence="0.991542166666667">
4There are equivalent but more efficient ways to rep-
resent these clauses in Prolog (cf. Lager, 2000).
5Download the compiler from the PWE homepage at:
http://www.ling.gu.seks,lager/pwe.html
6The ii-TBL system is available from:
http: //www. . ling. gu. seRaager/mutbl . html
</footnote>
<page confidence="0.93199">
100
</page>
<bodyText confidence="0.999945777777778">
The template is to be read &amp;quot;replace the tag for
sense A with the tag for sense B if the word im-
mediately to the right is C&amp;quot;, where A, B and C are
variables. Learning is a matter of repeatedly in-
stantiating rule templates in training data, scor-
ing rules on the basis of counts of positive and
negative evidence of them, selecting the highest
scoring rule on the basis of this ranking, and
applying it to the training data.
</bodyText>
<sectionHeader confidence="0.765913" genericHeader="method">
3 Peewees at SENSEVAL-2
</sectionHeader>
<bodyText confidence="0.999554583333333">
The lexical sample task for Swedish in
SENSEVAL-2 involved 40 lemmas: 20 nouns, 15
verbs and 5 adjectives. Together they repre-
sented 145 senses and 304 sub-senses. 8,718
annotated instances were provided as training
material and 1,525 unannotated instances were
provided for testing. Furthermore, a lexicon
— the GLDB (Gothenburg Lexical Database) —
complete with morphological information, defi-
nitions, language examples, etc. was available.
Our team explored three approaches. For
each lemma, we trained:
</bodyText>
<listItem confidence="0.971268833333333">
• PWE-smpl: a simple PWE capable of ar-
riving at a single sense for each instance of
that lemma in the testing material.
• PWE-disj: a committee of PWEs (i.e. a
set of PWEs) capable of arriving at (pos-
sibly) multiple senses for each instance of
that lemma, by collecting the individual re-
sults into a set.
• PWE-vote: a committee of PWEs capable
of arriving at a single sense for each in-
stance of that lemma, by applying a simple
voting procedure.
</listItem>
<bodyText confidence="0.996693285714286">
As it turned out, the second of these approaches
produced a rather unimpressive result, and we
will therefore spend very little time discussing
it. Indeed, had we been able to run the scor-
ing software ourselves (which we were not), we
would have left them outside the competition
altogether.
</bodyText>
<subsectionHeader confidence="0.996098">
3.1 The Simple Peewees
</subsectionHeader>
<bodyText confidence="0.999843333333333">
For the training of our simplest form of sense
disambiguation expert, the following set of
seven templates was used:
</bodyText>
<equation confidence="0.999615285714286">
sense : A&gt;B &lt;- word:CO .
sense : A&gt;B &lt;- word:CO [-1 , -2] .
sense:A&gt;B &lt;- word:CO[1].
sense:A&gt;B &lt;- word:CO[1,2].
sense:A&gt;B &lt;- word:CO[1] &amp; word:DO[2].
sense:A&gt;B &lt;- word:CO[-1] &amp; word:DO[-2].
sense:A&gt;B &lt;- word:C0(-1] &amp; word:DO[1].
</equation>
<bodyText confidence="0.999978555555555">
The idea was to exploit a fact noted by many
researchers in the field: that the sense of an
occurrence of a word can fairly successfully be
determined from just looking at the two previ-
ous words and the two following words (cf. Ide
&amp; Veronis, 1998). The choice of the above set
of templates is based on a fairly thorough trail-
and-error process and works well for most words
that we have tried.
</bodyText>
<subsectionHeader confidence="0.999378">
3.2 The Peewee Committees
</subsectionHeader>
<bodyText confidence="0.999830636363637">
The idea here was to train five different PWEs
for each lemma, and then to use a simple vot-
ing mechanism to arrive at a final decision.
The PWEs were different only in that they
used different sets of templates during the train-
ing. Templates looking forwards only, templates
looking backwards only, and templates looking
both forwards and backwards. Furthermore,
one member in each committee was trained for
using a bag-of-words approach to disambigua-
tion, based on templates of the following form:
</bodyText>
<equation confidence="0.999784">
sense:A&gt;B &lt;- inBag:WO[0].
sense:A&gt;B &lt;- inBag:W10[0] &amp; inBag:W20[0].
</equation>
<bodyText confidence="0.999646666666667">
Finally, one PWE in each committee had access
to a list of words extracted from the language
examples provided by the GLDB.
</bodyText>
<subsectionHeader confidence="0.99487">
3.3 The Procedure
</subsectionHeader>
<bodyText confidence="0.999387333333333">
In this section we describe the actions that we
took in order to submit our entry in the compe-
tition.
</bodyText>
<listItem confidence="0.85173625">
• In a preparatory step, the XML formatted
training data was parsed and subsequently
converted into the format required by the
,u-TBL system.
</listItem>
<bodyText confidence="0.989811166666667">
.• The training was performed, and resulted
in one PWE specification per lemma.
Training took between 5 seconds and a cou-
ple of minutes per lemma, depending on the
amount of training data available for the
lemma in question.
</bodyText>
<listItem confidence="0.968520333333333">
• The PWE specifications were compiled into
a set of PWE procedures, by means of the
PWE compiler.
</listItem>
<page confidence="0.951046">
101
</page>
<listItem confidence="0.882243">
• Simple procedures were written to print the
</listItem>
<bodyText confidence="0.60949575">
results to a file in the prescribed format,
and the PWEs were then run on the test
data. This took only a couple of seconds
for the whole test corpus.
</bodyText>
<sectionHeader confidence="0.934812" genericHeader="evaluation">
3.4 Results
</sectionHeader>
<bodyText confidence="0.973275666666667">
In the following table we show the results of
our entry in the competition, copied from the
SENSEVAL-2 homepage.7
</bodyText>
<table confidence="0.9931386">
System Evaluation Accuracy (%).
PWE-smpl Fine 61.1
Mixed 66.8
PWE-vote Fine 63.0
Mixed 68.6
</table>
<bodyText confidence="0.9997173">
Five groups and altogether eight systems par-
ticipated in the Swedish lexical sample task. In
terms of ranking, our PWE-vote came in sec-
ond, after Yarowski&apos;s JHU system, and before
the GOteborg team&apos;s best entry. However, we
hasten to add that the step from Yarowski&apos;s
(nearly 70%, fine grained evaluation) to our re-
sults is a very significant 7%, and that the step
down to Goteborg&apos;s result is very small and
probably statistically insignificant. Our simple
Peewees shared the fourth place with Resnik et
al.&apos;s UMD-SST.
As can be seen from the table, the PWE com-
mittees did slightly better than a single simple
PWE. It is however dubious whether the small
difference was really worth the trouble. It is
quite possible that training a single PWE on
the combination of corpus data and the exam-
ples from the GLDB would have lead to a result
almost as good, and with less work.
</bodyText>
<sectionHeader confidence="0.998789" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.984432653846154">
It seems we can conclude that an ap-
proach to word sense disambiguation based
on Transformation-Based Learning is compet-
itive with approaches based on Memory-Based
Learning as used by the Goteborg team, and
support vector machine (SVM) learning, used
by the University of Maryland team. This is
7Note that the coarse-grained evaluation was not ap-
plicable to the Swedish task. Also, it should be noted
that our results in the first round of evaluation were
slightly worse than the results reported here. However,
this was due to a spelling error which could be corrected
by the conference organizers and thus did not involve
any resubmission of test results.
good news for those aiming at building NLP sys-
tems in which transformation rules play a major
role.
As we have seen, there is meaning in the life
of Peewees, and sound mathematical meaning
at that! Also, given the link between first order
logic and a logic programming language such
as Prolog, the implementation follows very di-
rectly from the specification. The existence of
a compiler from Peewee specifications into Pro-
log procedures makes Peewees very convenient
to work with in a Prolog environment.
</bodyText>
<sectionHeader confidence="0.998583" genericHeader="acknowledgments">
5 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999342">
We thank the SENSEVAL-2 organizers for mak-
ing all this possible, and in particular Jerker
Jarborg and Dimitrios Kokkinakis in Goteborg
for their work on preparing for the Swedish lex-
ical sample task.
</bodyText>
<sectionHeader confidence="0.997954" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998140928571429">
Berleant, D. (1995) Engineering &amp;quot;Word Ex-
perts&amp;quot; for Word Disambiguation. Natural
Language Engineering, 1(4).
Brill, E. (1995) Transformation-Based Error-
Driven Learning and Natural Language Pro-
cessing: A Case Study in Part of Speech Tag-
ging. Computational Linguistics 21.
Ide, N. and Veronis, J. (1998) Introduction to
the Special Issue on Word Sense Disambigua-
tion: The State of the Art. Computational
Linguistics 24(1).
Lager, T. (1999) The p-TBL System: Logic
Programming Tools for Transformation-
Based Learning. In Proceedings of CoNLL&apos;99,
Bergen, Norway.
Lager, T. (2000) A Logic Programming Ap-
proach to Word Expert Engineering. In Pro-
ceedings of ACIDCA 2000: Workshop on
Corpora and Natural Language Processing,
Monastir, Tunisia, March 22-24 2000.
Lager, T. and Nivre, J. (2001) Part of Speech
Tagging from a Logical Point of View. In
de Groote, P., Morrill, G., Retor, C. (eds.)
Logical Aspects of Computational Linguistics.
Springer-Verlag, LNAI. VOL. 2099.
Pereira, F. and Warren, D. H. D. (1983) Pars-
ing as Deduction, In Proceedings of the 21th
Meeting of the ACL.
</reference>
<page confidence="0.998621">
102
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.260221">
<title confidence="0.9837035">Sense and Deduction: The Power of Peewees the Lexical Sample Task</title>
<author confidence="0.970693">Torbjorn Lager t</author>
<author confidence="0.970693">Natalia</author>
<address confidence="0.661147">of Linguistics, Uppsala</address>
<email confidence="0.367405">tHapaxInformationSystemsAB,Stockholm</email>
<abstract confidence="0.997105461538461">This paper describes our use of Prolog Word (PWEs) in the competition. We explain how we specify our PWEs as sequences of transformation rules and how they can be trained on sense tagged corpus data. We give a semantics of PWEs by translating them into first order predicate logic, and we describe how PWEs can be compiled into Prolog procedures. We finally present our results for the lexical sample task: score) for our best PWE, and a second place in the ranking.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Berleant</author>
</authors>
<title>Engineering &amp;quot;Word Experts&amp;quot; for Word Disambiguation.</title>
<date>1995</date>
<journal>Natural Language Engineering,</journal>
<volume>1</volume>
<issue>4</issue>
<contexts>
<context position="1061" citStr="Berleant, 1995" startWordPosition="173" endWordPosition="174"> data. We give a semantics of PWEs by translating them into first order predicate logic, and we describe how PWEs can be compiled into Prolog procedures. We finally present our results for the Swedish lexical sample task: 63% (fine-grained score) for our best PWE, and a second place in the ranking. 1 Introduction Word experts are small expert system-like modules for processing a particular target word based on neighboring words. Typically, a word expert uses rules that test the identity and relative position of words in the context in order to infer the role of the target word in the passage (Berleant, 1995). In this paper, we describe the development of various kinds of word experts in a logic programming framework, dealing with word sense disambiguation in the context of the SENSEVAL-2 competition. In a logic programming framework, the task of engineering a word (sense) expert can be specified as follows. Given a suitable representation of a text, we want to define a predicate sense/2 such that sense(P,S) is true if the word at position P in the text has the sense S. In the remainder of the paper, we will refer to this kind of word expert as a Prolog Word Expert (or PWE for short — &amp;quot;Peewee&amp;quot; to </context>
</contexts>
<marker>Berleant, 1995</marker>
<rawString>Berleant, D. (1995) Engineering &amp;quot;Word Experts&amp;quot; for Word Disambiguation. Natural Language Engineering, 1(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Transformation-Based ErrorDriven Learning and Natural Language Processing: A Case Study in Part of Speech Tagging.</title>
<date>1995</date>
<journal>Computational Linguistics</journal>
<volume>21</volume>
<contexts>
<context position="6484" citStr="Brill, 1995" startWordPosition="1142" endWordPosition="1143">ple, the second and the third formulas can be translated as follows:4 s2(P0,1) s1(P0,6), P1 is P0+1, w(P1,in). s2(P0,X) sl(PO,X), P1 is P0+1, \+ w(P1,in). To write Prolog procedures such as these by hand for many rules would be tedious and prone to errors. Fortunately, since the formalism for transformation rules is compositional, it was straightforward to write a compiler5 that generates word expert procedures from word expert specifications automatically. 2.4 Peewee Training There is an obvious choice of learning method for training Prolog Word Experts, namely Transformation-Based Learning (Brill, 1995). Of course, the fact that transformation rules can be learned from tagged corpora was a major reason for using them in the first place. The p-TBL system — described in detail in (Lager, 1999) — uses the search and database capabilities of the Prolog programming language to implement a generalized form of transformation-based learning. Through its support of a compositional rule/template formalism and `pluggable&apos; algorithms, the p-TBL system can easily be tailored to different learning tasks.6 Rules that can be learned in TransformationBased Learning are instances of rule templates. For exampl</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Brill, E. (1995) Transformation-Based ErrorDriven Learning and Natural Language Processing: A Case Study in Part of Speech Tagging. Computational Linguistics 21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ide</author>
<author>J Veronis</author>
</authors>
<title>Introduction to the Special Issue on Word Sense Disambiguation: The State of the Art.</title>
<date>1998</date>
<journal>Computational Linguistics</journal>
<volume>24</volume>
<issue>1</issue>
<contexts>
<context position="9904" citStr="Ide &amp; Veronis, 1998" startWordPosition="1712" endWordPosition="1715">r. 3.1 The Simple Peewees For the training of our simplest form of sense disambiguation expert, the following set of seven templates was used: sense : A&gt;B &lt;- word:CO . sense : A&gt;B &lt;- word:CO [-1 , -2] . sense:A&gt;B &lt;- word:CO[1]. sense:A&gt;B &lt;- word:CO[1,2]. sense:A&gt;B &lt;- word:CO[1] &amp; word:DO[2]. sense:A&gt;B &lt;- word:CO[-1] &amp; word:DO[-2]. sense:A&gt;B &lt;- word:C0(-1] &amp; word:DO[1]. The idea was to exploit a fact noted by many researchers in the field: that the sense of an occurrence of a word can fairly successfully be determined from just looking at the two previous words and the two following words (cf. Ide &amp; Veronis, 1998). The choice of the above set of templates is based on a fairly thorough trailand-error process and works well for most words that we have tried. 3.2 The Peewee Committees The idea here was to train five different PWEs for each lemma, and then to use a simple voting mechanism to arrive at a final decision. The PWEs were different only in that they used different sets of templates during the training. Templates looking forwards only, templates looking backwards only, and templates looking both forwards and backwards. Furthermore, one member in each committee was trained for using a bag-of-words</context>
</contexts>
<marker>Ide, Veronis, 1998</marker>
<rawString>Ide, N. and Veronis, J. (1998) Introduction to the Special Issue on Word Sense Disambiguation: The State of the Art. Computational Linguistics 24(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Lager</author>
</authors>
<title>The p-TBL System: Logic Programming Tools for TransformationBased Learning.</title>
<date>1999</date>
<booktitle>In Proceedings of CoNLL&apos;99,</booktitle>
<location>Bergen,</location>
<contexts>
<context position="6676" citStr="Lager, 1999" startWordPosition="1177" endWordPosition="1178">ese by hand for many rules would be tedious and prone to errors. Fortunately, since the formalism for transformation rules is compositional, it was straightforward to write a compiler5 that generates word expert procedures from word expert specifications automatically. 2.4 Peewee Training There is an obvious choice of learning method for training Prolog Word Experts, namely Transformation-Based Learning (Brill, 1995). Of course, the fact that transformation rules can be learned from tagged corpora was a major reason for using them in the first place. The p-TBL system — described in detail in (Lager, 1999) — uses the search and database capabilities of the Prolog programming language to implement a generalized form of transformation-based learning. Through its support of a compositional rule/template formalism and `pluggable&apos; algorithms, the p-TBL system can easily be tailored to different learning tasks.6 Rules that can be learned in TransformationBased Learning are instances of rule templates. For example, the second of the rules in our example PWE specification is an instance of the following template: sense:A&gt;B &lt;- word:CO[1] . for a word token with a particular sense (say 5) becomes a matte</context>
</contexts>
<marker>Lager, 1999</marker>
<rawString>Lager, T. (1999) The p-TBL System: Logic Programming Tools for TransformationBased Learning. In Proceedings of CoNLL&apos;99, Bergen, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Lager</author>
</authors>
<title>A Logic Programming Approach to Word Expert Engineering.</title>
<date>2000</date>
<booktitle>In Proceedings of ACIDCA 2000: Workshop on Corpora and Natural Language Processing,</booktitle>
<location>Monastir, Tunisia,</location>
<contexts>
<context position="3757" citStr="Lager, 2000" startWordPosition="670" endWordPosition="671">the number of rules can be increased, and typically a word expert will &apos;This word was of course not used the Swedish task, but is used here for expository reasons. The sense tags are numbers: l=&amp;quot;readiness to give attention&amp;quot;, 5=&amp;quot;a company share&amp;quot;, 6= &amp;quot;money paid for the use of money&amp;quot;, etc. 99 have access to anything between just a handful of rules and several hundred ones.2 2.2 Peewee Logic Interestingly, a sequence of transformation rules can be translated into a set of axioms, expressed in first-order predicate logic, defining relationships between positions in a text, word forms, and senses (Lager, 2000; Lager &amp; Nivre, 2001). For example, the meaning of the rules from the previous section can be spelled out as follows: Vp[w (p,inter est) -+ (p,6)] dpo ,P1[Si (Po ,6) A Pi =Po+1 A w (pi ,in) -4 S2 (PO )1)1 dpo,Pi,x[Si(po,x) A pi=po+1 A ,in) -4 S2 (po,x)} Vpo ,Pi [S2 (po ,1) A pi=p0-1 A w (pi ,%) S3 (Po ,5)} Vpo,p1,x[S2(po,x) A pi=p0-1 A ---,w(pi,%) ---+ S3 (po ,x)] Vx,p{S3(p,x) S(p,x)) The idea is that for each rule in the sequence a new predicate Si is introduced, where the subscript indicates where in the sequence the rule belongs. Semantically, Si relates a position to a sense, and the form</context>
<context position="7416" citStr="Lager, 2000" startWordPosition="1293" endWordPosition="1294">n-based learning. Through its support of a compositional rule/template formalism and `pluggable&apos; algorithms, the p-TBL system can easily be tailored to different learning tasks.6 Rules that can be learned in TransformationBased Learning are instances of rule templates. For example, the second of the rules in our example PWE specification is an instance of the following template: sense:A&gt;B &lt;- word:CO[1] . for a word token with a particular sense (say 5) becomes a matter of constructively proving 3p[S(p,5)]. 4There are equivalent but more efficient ways to represent these clauses in Prolog (cf. Lager, 2000). 5Download the compiler from the PWE homepage at: http://www.ling.gu.seks,lager/pwe.html 6The ii-TBL system is available from: http: //www. . ling. gu. seRaager/mutbl . html 100 The template is to be read &amp;quot;replace the tag for sense A with the tag for sense B if the word immediately to the right is C&amp;quot;, where A, B and C are variables. Learning is a matter of repeatedly instantiating rule templates in training data, scoring rules on the basis of counts of positive and negative evidence of them, selecting the highest scoring rule on the basis of this ranking, and applying it to the training data.</context>
</contexts>
<marker>Lager, 2000</marker>
<rawString>Lager, T. (2000) A Logic Programming Approach to Word Expert Engineering. In Proceedings of ACIDCA 2000: Workshop on Corpora and Natural Language Processing, Monastir, Tunisia, March 22-24 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Lager</author>
<author>J Nivre</author>
</authors>
<title>Part of Speech Tagging from a Logical Point of View. In</title>
<date>2001</date>
<booktitle>Logical Aspects of Computational Linguistics. Springer-Verlag, LNAI. VOL.</booktitle>
<editor>de Groote, P., Morrill, G., Retor, C. (eds.)</editor>
<contexts>
<context position="3779" citStr="Lager &amp; Nivre, 2001" startWordPosition="672" endWordPosition="675"> rules can be increased, and typically a word expert will &apos;This word was of course not used the Swedish task, but is used here for expository reasons. The sense tags are numbers: l=&amp;quot;readiness to give attention&amp;quot;, 5=&amp;quot;a company share&amp;quot;, 6= &amp;quot;money paid for the use of money&amp;quot;, etc. 99 have access to anything between just a handful of rules and several hundred ones.2 2.2 Peewee Logic Interestingly, a sequence of transformation rules can be translated into a set of axioms, expressed in first-order predicate logic, defining relationships between positions in a text, word forms, and senses (Lager, 2000; Lager &amp; Nivre, 2001). For example, the meaning of the rules from the previous section can be spelled out as follows: Vp[w (p,inter est) -+ (p,6)] dpo ,P1[Si (Po ,6) A Pi =Po+1 A w (pi ,in) -4 S2 (PO )1)1 dpo,Pi,x[Si(po,x) A pi=po+1 A ,in) -4 S2 (po,x)} Vpo ,Pi [S2 (po ,1) A pi=p0-1 A w (pi ,%) S3 (Po ,5)} Vpo,p1,x[S2(po,x) A pi=p0-1 A ---,w(pi,%) ---+ S3 (po ,x)] Vx,p{S3(p,x) S(p,x)) The idea is that for each rule in the sequence a new predicate Si is introduced, where the subscript indicates where in the sequence the rule belongs. Semantically, Si relates a position to a sense, and the formulas define this predi</context>
</contexts>
<marker>Lager, Nivre, 2001</marker>
<rawString>Lager, T. and Nivre, J. (2001) Part of Speech Tagging from a Logical Point of View. In de Groote, P., Morrill, G., Retor, C. (eds.) Logical Aspects of Computational Linguistics. Springer-Verlag, LNAI. VOL. 2099.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>D H D Warren</author>
</authors>
<title>Parsing as Deduction,</title>
<date>1983</date>
<booktitle>In Proceedings of the 21th Meeting of the ACL.</booktitle>
<contexts>
<context position="5723" citStr="Pereira and Warren (1983)" startWordPosition="1023" endWordPosition="1026">consequence of the theory formed by taking the union of the previous two sets of formulas. For example, the formula 3x[S(12,x)] is a theorem, for which we can construct (only) the example x 5, and we have thus formally proved that this particular occurrence of interest means &amp;quot;a share in a company&amp;quot; .3 2A demo of a more potent PWE is available at: http://www.ling.gu.se/lager/Home/pwe_ui.html 3The theory can be used in other ways too. Searching What we have here is something that we like to think of as word sense disambiguation as deduction, in analogy to the ideas of parsing as deduction due to Pereira and Warren (1983). 2.3 The Peewee Compiler Since the above formulas have already logic programming form, it is straightforward to translate them into Prolog. For example, the second and the third formulas can be translated as follows:4 s2(P0,1) s1(P0,6), P1 is P0+1, w(P1,in). s2(P0,X) sl(PO,X), P1 is P0+1, \+ w(P1,in). To write Prolog procedures such as these by hand for many rules would be tedious and prone to errors. Fortunately, since the formalism for transformation rules is compositional, it was straightforward to write a compiler5 that generates word expert procedures from word expert specifications auto</context>
</contexts>
<marker>Pereira, Warren, 1983</marker>
<rawString>Pereira, F. and Warren, D. H. D. (1983) Parsing as Deduction, In Proceedings of the 21th Meeting of the ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>