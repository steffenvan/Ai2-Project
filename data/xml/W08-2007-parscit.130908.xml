<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012166">
<title confidence="0.985886">
Semantic structure from Correspondence Analysis
</title>
<author confidence="0.905202">
Barbara McGillivray Christer Johansson
</author>
<affiliation confidence="0.916521">
Dipartimento di Linguistica Dept. of Linguistics
Universit`a di Pisa University of Bergen
Pisa, Italy Bergen, Norway
</affiliation>
<email confidence="0.991051">
barbara.mcgillivray@aksis.uib.nochrister.johansson@uib.no
</email>
<author confidence="0.947235">
Daniel Apollon
</author>
<affiliation confidence="0.921681">
Text Technology Lab.
</affiliation>
<address confidence="0.804437">
Aksis, UNIFOB
Bergen, Norway
</address>
<email confidence="0.994876">
daniel.apollon@aksis.uib.no
</email>
<sectionHeader confidence="0.993805" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999958384615385">
A common problem for clustering tech-
niques is that clusters overlap, which
makes graphing the statistical structure in
the data difficult. A related problem is
that we often want to see the distribution
of factors (variables) as well as classes
(objects). Correspondence Analysis (CA)
offers a solution to both these problems.
The structure that CA discovers may be an
important step in representing similarity.
We have performed an analysis for Italian
verbs and nouns, and confirmed that simi-
lar structures are found for English.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99995385">
Over the past years, distributional methods have
been used to explore the semantic behaviour of
verbs, looking at their contexts in corpora (Lan-
dauer and Laham, 1998; Redington and Finch,
1998; Biemann, 2006, inter al.). We follow a gen-
eral approach suggested already by Firth (1957),
to associate distributional similarity with semantic
similarity.
One question concerns the syntax-semantics in-
terface. Results using distributions of verbs in con-
text had an impact on verb classification (Levin,
1993), automatic verb clustering (Schulte im
Walde, 2003), and selectional preference acquisi-
tion (Resnik, 1993; Li and Abe, 1995; McCarthy,
2001; Agirre and Martinez, 2001, inter al.).
In automatic verb clustering, verbs are repre-
sented by vectors of a multidimensional space
whose dimensions (variables) are identified by
some linguistic features, ranging, for example,
from subcategorization frames to participation in
</bodyText>
<footnote confidence="0.974742">
© 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
</footnote>
<bodyText confidence="0.999786195121951">
diathesis alternations and lexical selectional pref-
erences. The verbs cluster on co-occurrence with
the features chosen, and such information provide
a generalisation over the verbs with respect to the
variables.
In the case of selectional preference acquisition,
a verb (or a verb class) is associated to a class of
nouns that can be the lexical fillers of a case frame
slot for the verb. This allows us to calculate the
association strength between the verb and its filler
nouns. The generalisation step is performed for the
case frame instances (observations) and produces
more abstract noun classes that can be applied to
unseen cases. This often utilizes hierarchies of ex-
isting thesauri or wordnets.
We propose a method that uses Correspondence
Analysis (CA) to study the distribution (and asso-
ciated semantic behaviour) of a list of verbs with
nouns occurring in a particular syntactic relation,
for example their subjects. This is collected from
a corpus, and reflects usage in that corpus. Un-
like clustering methods, this technique does not
imply an exclusive choice between a) classifying
verbs on the basis of the noun fillers in their syn-
tactic frame, or b) associating noun classes to verbs
(sometimes mediated by a semantic hierarchy). In-
stead, this approach yields a geometric representa-
tion of the relationships between the nouns and the
verbs in a common dual space (biplot). CA aims
to find an overall structure (if any) of the data. The
method emphasizes unusual observations, as de-
viance from the expected is what creates the axes
of the analysis. CA generalizes over the actual oc-
currences of verb-noun pairs in the corpus, and vi-
sualizes the shape of the correspondence space.
When associating verbs with nouns, CA takes as
input a contingency table (here rows correspond to
the verbs, and columns correspond to their subject
fillers). Each verb is a row point in the multidimen-
sional noun space, and each noun is a column point
in the multidimensional verb space. The CA goals
</bodyText>
<page confidence="0.98481">
49
</page>
<note confidence="0.574842">
Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 49–52
</note>
<bodyText confidence="0.975200333333333">
Manchester, August 2008
are to reduce the dimension of the dual original
space, and to find an optimal subspace that is the
closest to this cloud of points in the x2-metric. The
best subspace is determined by finding the smallest
number of orthogonal axes that describe the most
variance from the original cloud.
Finally the coordinates of both row and col-
umn points of the x2 contingency table are pro-
jected onto this optimal subspace, simultaneously
displaying row and column points. If we consider
those points that are well represented, the closer
they are in this geometric representation, the more
similar their original distributions are. In this way,
we can detect not only that there is a relationship
between the verb (e.g. explode) and the noun (e.g.
bomb), but also how each word relates to each
other word.
</bodyText>
<sectionHeader confidence="0.944233" genericHeader="method">
2 Correspondence Analysis
</sectionHeader>
<bodyText confidence="0.999968023809524">
CA is a data analytic technique developed by
Benz´ecri in the 1960s, which has been widely used
in describing large contingency tables and binary
data. At the heart of CA is Singular Value Decom-
position (SVD), from which many other methods
were derived (Biplot, Classical Principal Compo-
nent Analysis, PCA and more).
Compared to usual clustering methods, CA
gives a more fine-grained view of the spread of
the input points. Benz´ecri (1973) points out that
CA is more efficient than clustering in terms of de-
composition of variance. Secondly, CA represents
possible regions in space with varying density, and
produces a flexible ”compound clustering” on both
objects and variables. Verb-nouns association pro-
files may not cluster in distinct space regions, but
may be evenly distributed, follow a gradient-like
distribution, or show overlapping clusters. In such
difficult cases for clustering, CA is able to offer
a representation of the geometry of the input pro-
files. Finally, CA offers the possibility of recon-
structing the original space from the output sub-
space.
Let us consider a data matrix M whose size is
(r, c), the (i, j)th entry of M containing the num-
ber of occurrences of verb j with noun i as its sub-
ject in a corpus. We calculate the relative frequen-
cies by dividing each entry M(i, j) by the sum of
row i, i. e. the frequency of noun i, to get the ma-
trix of row profiles R(i, j). Therefore, the more
similar two row profiles i1 and i2 are, the more
these two nouns can be considered as distributional
synonyms.
The next step implies comparing the row pro-
files with the average distribution where each entry
(i, j) is the product of the frequency of noun i by
the frequency of verb j divided by the grand total
N of the table. This comparison is calculated us-
ing the x2-distance (i.e. a weighted Euclidean dis-
tance), which eliminates effects of high frequency
alone. The next formula shows calculations for
rows. Calculations for columns are analogous.
</bodyText>
<equation confidence="0.9921695">
(R(i1, j) − R(i2, j))2
Eir=1 M(i, j)
</equation>
<bodyText confidence="0.998644">
The x2-distance between a profile point and the av-
erage profile (barycentre) is called inertia of the
profile point and the total inertia measures how
the individual profiles pi are spread around the
barycentre:
</bodyText>
<equation confidence="0.599739">
Inertia = N
</equation>
<bodyText confidence="0.99963884375">
CA then searches for the optimal subspace S that
minimises the distance from the profile points.
Once specified its dimension k &lt; min(r − 1, c −
1), S is found by applying the Singular Value De-
composition (SVD) to matrix R − 1¯p , which de-
composes it as the product N · D · M: where D is
a diagonal matrix with positive coefficients A1 &gt;
A2 &gt; ... &gt; Ak (singular values) and N and M are
orthonormal matrices (NTN = MTM = I). The
rows of M are the orthonormal basis vectors that
define S (called principal axes of inertia) and the
rows of matrix F = N · D are the projections of
the row profiles onto S. For k = 2, this allows us
to plot the new coordinates in a two-dimensional
space and get the correspondence analysis of the
row profiles.
The total inertia is decomposed into the direc-
tion of the principal axes of inertia. The first axis
represents the direction where the inertia of the
cloud is the maximum; the second axis maximises
the inertia among all the directions orthogonal to
the first axis, and so on.
The geometry of column profiles can be anal-
ysed similarly, because the two problems are di-
rectly linked and two transition formulae can be
used to pass from one coordinate system to the
other, explaining the French name ”analyse des
correspondances”.
As a result, both analyses decompose the same
inertia into the same system of principal axes. This
allows us to merge the two representations in one
single geometric display showing at the same time
</bodyText>
<equation confidence="0.998052285714286">
δ2(i1, i2) = Ec
j=1
M(i, j)δ2(pi,¯p)
r
i=1
Ec
j=1
</equation>
<page confidence="0.947346">
50
</page>
<bodyText confidence="0.999658">
the projections of row and column points in the
subspace.
In addition to this dual space representation, CA
gives a system of diagnostic measures for each of
the two dual spaces:
</bodyText>
<listItem confidence="0.99564775">
• contributions of the rows (and columns) to the
axes, i. e. the inertia of the points projected onto
the axes, which contributes to the principal inertia;
• contributions of the axes to the row (and col-
umn) points;
• quality of representation (cumulative sum of
contributions of the axes for each point); this high-
lights well represented points.
</listItem>
<sectionHeader confidence="0.998202" genericHeader="method">
3 Explorations
</sectionHeader>
<bodyText confidence="0.983770085714286">
We performed a CA using the Matlab Analytica
Toolbox developed by Daniel Apollon. We tested
this technique first on the Italian newspaper corpus
LA REPUBBLICA, which consists of 450 million
word tokens. This corpus was syntactically parsed
using the MALT dependency parser (Nivre, 2006).
A list of 196 verbs was compiled following the list
of German verbs contained in (Schulte im Walde,
2003) and adapting it to Italian. Looking at the
syntactic analyses of the corpus where the verbs of
the list showed a subcategorization frame contain-
ing a subject slot, their lexical subject fillers were
automatically extracted. The matrix M, whose
2553 row entries correspond to the nouns extracted
as subject fillers, was then used as input for the CA
(|M |= 196 x 2553 = 500388).
Starting from the quality of representation
scores of this analysis, we isolated a set of points
with increasingly good representation, ending with
an extremely faithful and low dimensional rep-
resentation. We called this method ”incremental
pruning”. Figure 1 shows the dual display of the
analysis for the Italian data in a two dimensional
space, after filtering out those points showing a
quality of representation below a threshold of 30%.
We can conceptualize the data set C after a CA
as the cumulative effect of three different underly-
ing phenomena: K, R and E.
K can be seen as a reduction of the latent struc-
ture of C; it contains its core structure as it has
been underlined by the analysis and left after prun-
ing.
R refers to the residual variance, not included in
the core analysis. It contains the most predictable
points1, which are plotted near to origin (barycen-
</bodyText>
<footnote confidence="0.941273">
1In our data: pronouns she, I, he, every-, no-, some-body,
</footnote>
<figureCaption confidence="0.999778">
Figure 1: Correspondence graph for Italian
</figureCaption>
<bodyText confidence="0.989115090909091">
tre of the data cloud). These points give a small
contribution to the inertia of the principal axes.
E contains the error in the representation, as
well as badly represented points.
Points far from the origin display strong struc-
ture; they may correspond to rare words used in
special contexts. Figure 1 shows that words related
to destruction2 are aligned in the same direction,
whereas the second vector is mainly constituted by
nouns and verbs that have to do with the political
and legal area3. The first principal axis accounts
for nearly 16% of the total inertia, whereas the
second axis accounts for 12%. The first six axes
accounts for over 70% of variation. Many words
were not well represented, but contribute to vari-
ance.
We confirmed our method on English, using the
British National Corpus4. A similar structure was
found. We restrict ourselves to reproduce the graph
for Italian.
who, nouns with partly pronominal qualities husband, wife,
friend, sir, son, father, mother, fact, event.
</bodyText>
<footnote confidence="0.427493">
2Along the y-axis from top down to the middle, we find the
</footnote>
<construct confidence="0.9619924">
nouns flame, extend, stick of dynamite, excavator, chemother-
apy, effusion, blaze, seism, demon, dynamite, fire, aviation,
earthquake, artificer, explosive device, insect, gas, landslide,
virus, rain, bulldozer, hurricane, wave, speculation, artillery,
remorse, bomb, missile, violence, revolution, etc.
</construct>
<footnote confidence="0.990915">
3Along the x-axis we find, from left to the middle,
the nouns order, regulations, norm, code, legislation, rules,
treaty, constitution, circular letter, system, directive, law, de-
cree, article, judgement, amendment, court, etc.
4via sketchengine http://www.sketchengine.co.uk
</footnote>
<page confidence="0.998476">
51
</page>
<sectionHeader confidence="0.998945" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999829611111111">
CA detects a structure for Italian verb-noun cor-
respondences in LA REPUBBLICA (∼ 450 mil-
lion words). A similar structure was confirmed us-
ing BNC for English. Both global and local struc-
tures are found, which gives possibilities to rep-
resent lexical units with reference to both princi-
pal axes and word similarity. The main dimen-
sions of the Italian corpus are topical (crime re-
lated vs. natural catastrophes, and laws vs. po-
litical institutions). Semantic relatedness were ob-
served in closely mapped words. Both global and
local structure is found, and we can speculate that
this helps representing lexical units in semantic la-
beling (Giuglea and Moschitti, 2006) for machine
learning tasks. We can conceptualize text graphs in
two distinct usages: knowledge re-presenting (e.g.
FrameNet) and visualizing relations in a data set.
Our method belongs in the second category.
</bodyText>
<sectionHeader confidence="0.996933" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999818">
The first author is a MULTILINGUA fellow at
Uni. Bergen, financially supported by a Marie
Curie action (European Commission). We wish to
thank the anonymous reviewers who gave impor-
tant leads to future research.
</bodyText>
<sectionHeader confidence="0.998569" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999769456521739">
Agirre, Eneko and David Martinez. 2001. Learning
class-to-class selectional preferences. In Proc. of
the ACL/EACL Workshop on Computational Natural
Language Learning, pages 1–8, Toulouse, France.
Benz´ecri, Jean-Paul. 1973. L’Analyse des Donn´ees,
volume 1. Dunod.
Biemann, Chris. 2006. Chinese Whispers – an
Efficient Graph Clustering Algorithm and its Ap-
plication to Natural Language Processing Prob-
lems. In Proc. of the HLT-NAACL-06 Workshop on
Textgraphs-06, pages 73–80, New York, USA.
Firth, John R., 1957. Studies in Linguistic Analysis,
chapter A synopsis of linguistic theory 1930-1955.
Philological Society.
Giuglea, Ana-Maria and Alessandro Moschitti. 2006.
Semantic Role Labeling via FrameNet, VerbNet and
PropBank. In Proc. of the 21st Int. Conf. on Com-
putational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics, pages
929–936, Sydney, Australia.
Landauer, Thomas K., Peter W. Foltz and Darrell La-
ham. 1998. Introduction to Latent Semantic Analy-
sis. Discourse Processes, 25:259–284.
Levin, Beth. 1993. English Verb Classes and Alterna-
tions. The University of Chicago Press.
Li, Hang and Naoki Abe. 1995. Generalizing case
frames using a thesaurus and the MDL principle. In
Proc. of RecentAdvances in Natural Language Tech-
nology, pages 239–248.
McCarthy, Diana. 2001. Lexical Acquisition at the
Syntax-Semantics Interface: Diathesis Alternations,
Subcategorization Frames and Selectional Prefer-
ences. Ph.D. thesis, University of Sussex.
Nivre, Joakim. 2006. Inductive Dependency Parsing.
Springer.
Redington, Martin, Nick Chater and Steven Finch.
1998. Distributional information: A powerful cue
for acquiring syntactic categories. Cognitive Sci-
ence, 22:425–469.
Resnik, Philip. 1993. Selection and Information:
A Class-Based Approach to Lexical Relationships.
Ph.D. thesis, University of Pennsylvania.
Schulte im Walde, Sabine. 2003. Experiments on
the Automatic Induction of German Semantic Verb
Classes. Ph.D. thesis, Institut f¨ur Maschinelle
Sprachverarbeitung, Universit¨at Stuttgart.
</reference>
<page confidence="0.998854">
52
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.575596">
<title confidence="0.998882">Semantic structure from Correspondence Analysis</title>
<author confidence="0.998841">Barbara McGillivray Christer Johansson</author>
<affiliation confidence="0.9999145">Dipartimento di Linguistica Dept. of Linguistics Universit`a di Pisa University of Bergen</affiliation>
<address confidence="0.983293">Pisa, Italy Bergen, Norway</address>
<email confidence="0.993476">barbara.mcgillivray@aksis.uib.nochrister.johansson@uib.no</email>
<author confidence="0.859263">Daniel</author>
<affiliation confidence="0.8832295">Text Technology Aksis,</affiliation>
<address confidence="0.999079">Bergen, Norway</address>
<email confidence="0.996155">daniel.apollon@aksis.uib.no</email>
<abstract confidence="0.992476428571429">A common problem for clustering techniques is that clusters overlap, which makes graphing the statistical structure in the data difficult. A related problem is that we often want to see the distribution of factors (variables) as well as classes (objects). Correspondence Analysis (CA) offers a solution to both these problems. The structure that CA discovers may be an important step in representing similarity. We have performed an analysis for Italian verbs and nouns, and confirmed that similar structures are found for English.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>David Martinez</author>
</authors>
<title>Learning class-to-class selectional preferences.</title>
<date>2001</date>
<booktitle>In Proc. of the ACL/EACL Workshop on Computational Natural Language Learning,</booktitle>
<pages>1--8</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="1580" citStr="Agirre and Martinez, 2001" startWordPosition="219" endWordPosition="222">have been used to explore the semantic behaviour of verbs, looking at their contexts in corpora (Landauer and Laham, 1998; Redington and Finch, 1998; Biemann, 2006, inter al.). We follow a general approach suggested already by Firth (1957), to associate distributional similarity with semantic similarity. One question concerns the syntax-semantics interface. Results using distributions of verbs in context had an impact on verb classification (Levin, 1993), automatic verb clustering (Schulte im Walde, 2003), and selectional preference acquisition (Resnik, 1993; Li and Abe, 1995; McCarthy, 2001; Agirre and Martinez, 2001, inter al.). In automatic verb clustering, verbs are represented by vectors of a multidimensional space whose dimensions (variables) are identified by some linguistic features, ranging, for example, from subcategorization frames to participation in © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. diathesis alternations and lexical selectional preferences. The verbs cluster on co-occurrence with the features chosen, and such information provide a generalisation over </context>
</contexts>
<marker>Agirre, Martinez, 2001</marker>
<rawString>Agirre, Eneko and David Martinez. 2001. Learning class-to-class selectional preferences. In Proc. of the ACL/EACL Workshop on Computational Natural Language Learning, pages 1–8, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-Paul Benz´ecri</author>
</authors>
<title>L’Analyse des Donn´ees,</title>
<date>1973</date>
<volume>1</volume>
<publisher>Dunod.</publisher>
<marker>Benz´ecri, 1973</marker>
<rawString>Benz´ecri, Jean-Paul. 1973. L’Analyse des Donn´ees, volume 1. Dunod.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
</authors>
<title>Chinese Whispers – an Efficient Graph Clustering Algorithm and its Application to Natural Language Processing Problems.</title>
<date>2006</date>
<booktitle>In Proc. of the HLT-NAACL-06 Workshop on Textgraphs-06,</booktitle>
<pages>73--80</pages>
<location>New York, USA.</location>
<contexts>
<context position="1118" citStr="Biemann, 2006" startWordPosition="154" endWordPosition="155">related problem is that we often want to see the distribution of factors (variables) as well as classes (objects). Correspondence Analysis (CA) offers a solution to both these problems. The structure that CA discovers may be an important step in representing similarity. We have performed an analysis for Italian verbs and nouns, and confirmed that similar structures are found for English. 1 Introduction Over the past years, distributional methods have been used to explore the semantic behaviour of verbs, looking at their contexts in corpora (Landauer and Laham, 1998; Redington and Finch, 1998; Biemann, 2006, inter al.). We follow a general approach suggested already by Firth (1957), to associate distributional similarity with semantic similarity. One question concerns the syntax-semantics interface. Results using distributions of verbs in context had an impact on verb classification (Levin, 1993), automatic verb clustering (Schulte im Walde, 2003), and selectional preference acquisition (Resnik, 1993; Li and Abe, 1995; McCarthy, 2001; Agirre and Martinez, 2001, inter al.). In automatic verb clustering, verbs are represented by vectors of a multidimensional space whose dimensions (variables) are </context>
</contexts>
<marker>Biemann, 2006</marker>
<rawString>Biemann, Chris. 2006. Chinese Whispers – an Efficient Graph Clustering Algorithm and its Application to Natural Language Processing Problems. In Proc. of the HLT-NAACL-06 Workshop on Textgraphs-06, pages 73–80, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John R Firth</author>
</authors>
<title>Studies in Linguistic Analysis, chapter A synopsis of linguistic theory 1930-1955.</title>
<date>1957</date>
<publisher>Philological Society.</publisher>
<contexts>
<context position="1194" citStr="Firth (1957)" startWordPosition="167" endWordPosition="168">iables) as well as classes (objects). Correspondence Analysis (CA) offers a solution to both these problems. The structure that CA discovers may be an important step in representing similarity. We have performed an analysis for Italian verbs and nouns, and confirmed that similar structures are found for English. 1 Introduction Over the past years, distributional methods have been used to explore the semantic behaviour of verbs, looking at their contexts in corpora (Landauer and Laham, 1998; Redington and Finch, 1998; Biemann, 2006, inter al.). We follow a general approach suggested already by Firth (1957), to associate distributional similarity with semantic similarity. One question concerns the syntax-semantics interface. Results using distributions of verbs in context had an impact on verb classification (Levin, 1993), automatic verb clustering (Schulte im Walde, 2003), and selectional preference acquisition (Resnik, 1993; Li and Abe, 1995; McCarthy, 2001; Agirre and Martinez, 2001, inter al.). In automatic verb clustering, verbs are represented by vectors of a multidimensional space whose dimensions (variables) are identified by some linguistic features, ranging, for example, from subcatego</context>
</contexts>
<marker>Firth, 1957</marker>
<rawString>Firth, John R., 1957. Studies in Linguistic Analysis, chapter A synopsis of linguistic theory 1930-1955. Philological Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Giuglea</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Semantic Role Labeling via FrameNet, VerbNet and PropBank.</title>
<date>2006</date>
<booktitle>In Proc. of the 21st Int. Conf. on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>929--936</pages>
<location>Sydney, Australia.</location>
<marker>Giuglea, Moschitti, 2006</marker>
<rawString>Giuglea, Ana-Maria and Alessandro Moschitti. 2006. Semantic Role Labeling via FrameNet, VerbNet and PropBank. In Proc. of the 21st Int. Conf. on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 929–936, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Peter W Foltz</author>
<author>Darrell Laham</author>
</authors>
<title>Introduction to Latent Semantic Analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<pages>25--259</pages>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>Landauer, Thomas K., Peter W. Foltz and Darrell Laham. 1998. Introduction to Latent Semantic Analysis. Discourse Processes, 25:259–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English Verb Classes and Alternations.</title>
<date>1993</date>
<publisher>The University of Chicago Press.</publisher>
<contexts>
<context position="1413" citStr="Levin, 1993" startWordPosition="197" endWordPosition="198">ysis for Italian verbs and nouns, and confirmed that similar structures are found for English. 1 Introduction Over the past years, distributional methods have been used to explore the semantic behaviour of verbs, looking at their contexts in corpora (Landauer and Laham, 1998; Redington and Finch, 1998; Biemann, 2006, inter al.). We follow a general approach suggested already by Firth (1957), to associate distributional similarity with semantic similarity. One question concerns the syntax-semantics interface. Results using distributions of verbs in context had an impact on verb classification (Levin, 1993), automatic verb clustering (Schulte im Walde, 2003), and selectional preference acquisition (Resnik, 1993; Li and Abe, 1995; McCarthy, 2001; Agirre and Martinez, 2001, inter al.). In automatic verb clustering, verbs are represented by vectors of a multidimensional space whose dimensions (variables) are identified by some linguistic features, ranging, for example, from subcategorization frames to participation in © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. diath</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Levin, Beth. 1993. English Verb Classes and Alternations. The University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Li</author>
<author>Naoki Abe</author>
</authors>
<title>Generalizing case frames using a thesaurus and the MDL principle.</title>
<date>1995</date>
<booktitle>In Proc. of RecentAdvances in Natural Language Technology,</booktitle>
<pages>239--248</pages>
<contexts>
<context position="1537" citStr="Li and Abe, 1995" startWordPosition="213" endWordPosition="216">ast years, distributional methods have been used to explore the semantic behaviour of verbs, looking at their contexts in corpora (Landauer and Laham, 1998; Redington and Finch, 1998; Biemann, 2006, inter al.). We follow a general approach suggested already by Firth (1957), to associate distributional similarity with semantic similarity. One question concerns the syntax-semantics interface. Results using distributions of verbs in context had an impact on verb classification (Levin, 1993), automatic verb clustering (Schulte im Walde, 2003), and selectional preference acquisition (Resnik, 1993; Li and Abe, 1995; McCarthy, 2001; Agirre and Martinez, 2001, inter al.). In automatic verb clustering, verbs are represented by vectors of a multidimensional space whose dimensions (variables) are identified by some linguistic features, ranging, for example, from subcategorization frames to participation in © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. diathesis alternations and lexical selectional preferences. The verbs cluster on co-occurrence with the features chosen, and such</context>
</contexts>
<marker>Li, Abe, 1995</marker>
<rawString>Li, Hang and Naoki Abe. 1995. Generalizing case frames using a thesaurus and the MDL principle. In Proc. of RecentAdvances in Natural Language Technology, pages 239–248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
</authors>
<title>Lexical Acquisition at the Syntax-Semantics Interface: Diathesis Alternations, Subcategorization Frames and Selectional Preferences.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Sussex.</institution>
<contexts>
<context position="1553" citStr="McCarthy, 2001" startWordPosition="217" endWordPosition="218">utional methods have been used to explore the semantic behaviour of verbs, looking at their contexts in corpora (Landauer and Laham, 1998; Redington and Finch, 1998; Biemann, 2006, inter al.). We follow a general approach suggested already by Firth (1957), to associate distributional similarity with semantic similarity. One question concerns the syntax-semantics interface. Results using distributions of verbs in context had an impact on verb classification (Levin, 1993), automatic verb clustering (Schulte im Walde, 2003), and selectional preference acquisition (Resnik, 1993; Li and Abe, 1995; McCarthy, 2001; Agirre and Martinez, 2001, inter al.). In automatic verb clustering, verbs are represented by vectors of a multidimensional space whose dimensions (variables) are identified by some linguistic features, ranging, for example, from subcategorization frames to participation in © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. diathesis alternations and lexical selectional preferences. The verbs cluster on co-occurrence with the features chosen, and such information pro</context>
</contexts>
<marker>McCarthy, 2001</marker>
<rawString>McCarthy, Diana. 2001. Lexical Acquisition at the Syntax-Semantics Interface: Diathesis Alternations, Subcategorization Frames and Selectional Preferences. Ph.D. thesis, University of Sussex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Inductive Dependency Parsing.</title>
<date>2006</date>
<publisher>Springer.</publisher>
<contexts>
<context position="9556" citStr="Nivre, 2006" startWordPosition="1563" endWordPosition="1564">umns) to the axes, i. e. the inertia of the points projected onto the axes, which contributes to the principal inertia; • contributions of the axes to the row (and column) points; • quality of representation (cumulative sum of contributions of the axes for each point); this highlights well represented points. 3 Explorations We performed a CA using the Matlab Analytica Toolbox developed by Daniel Apollon. We tested this technique first on the Italian newspaper corpus LA REPUBBLICA, which consists of 450 million word tokens. This corpus was syntactically parsed using the MALT dependency parser (Nivre, 2006). A list of 196 verbs was compiled following the list of German verbs contained in (Schulte im Walde, 2003) and adapting it to Italian. Looking at the syntactic analyses of the corpus where the verbs of the list showed a subcategorization frame containing a subject slot, their lexical subject fillers were automatically extracted. The matrix M, whose 2553 row entries correspond to the nouns extracted as subject fillers, was then used as input for the CA (|M |= 196 x 2553 = 500388). Starting from the quality of representation scores of this analysis, we isolated a set of points with increasingly</context>
</contexts>
<marker>Nivre, 2006</marker>
<rawString>Nivre, Joakim. 2006. Inductive Dependency Parsing. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Redington</author>
<author>Nick Chater</author>
<author>Steven Finch</author>
</authors>
<title>Distributional information: A powerful cue for acquiring syntactic categories.</title>
<date>1998</date>
<journal>Cognitive Science,</journal>
<pages>22--425</pages>
<marker>Redington, Chater, Finch, 1998</marker>
<rawString>Redington, Martin, Nick Chater and Steven Finch. 1998. Distributional information: A powerful cue for acquiring syntactic categories. Cognitive Science, 22:425–469.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Selection and Information: A Class-Based Approach to Lexical Relationships.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="1519" citStr="Resnik, 1993" startWordPosition="211" endWordPosition="212">ion Over the past years, distributional methods have been used to explore the semantic behaviour of verbs, looking at their contexts in corpora (Landauer and Laham, 1998; Redington and Finch, 1998; Biemann, 2006, inter al.). We follow a general approach suggested already by Firth (1957), to associate distributional similarity with semantic similarity. One question concerns the syntax-semantics interface. Results using distributions of verbs in context had an impact on verb classification (Levin, 1993), automatic verb clustering (Schulte im Walde, 2003), and selectional preference acquisition (Resnik, 1993; Li and Abe, 1995; McCarthy, 2001; Agirre and Martinez, 2001, inter al.). In automatic verb clustering, verbs are represented by vectors of a multidimensional space whose dimensions (variables) are identified by some linguistic features, ranging, for example, from subcategorization frames to participation in © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. diathesis alternations and lexical selectional preferences. The verbs cluster on co-occurrence with the feature</context>
</contexts>
<marker>Resnik, 1993</marker>
<rawString>Resnik, Philip. 1993. Selection and Information: A Class-Based Approach to Lexical Relationships. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Schulte im Walde</author>
<author>Sabine</author>
</authors>
<title>Experiments on the Automatic Induction of German Semantic Verb Classes.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>Institut f¨ur Maschinelle Sprachverarbeitung, Universit¨at Stuttgart.</institution>
<marker>Walde, Sabine, 2003</marker>
<rawString>Schulte im Walde, Sabine. 2003. Experiments on the Automatic Induction of German Semantic Verb Classes. Ph.D. thesis, Institut f¨ur Maschinelle Sprachverarbeitung, Universit¨at Stuttgart.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>