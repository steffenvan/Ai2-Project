<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000198">
<title confidence="0.9979385">
MiniExperts: An SVM Approach for Measuring
Semantic Textual Similarity
</title>
<author confidence="0.9110775">
Hanna B´echara*a, Hernani Costa*b, Shiva Taslimipoora, Rohit Guptaa,
Constantin Or˘asana, Gloria Corpas Pastorb and Ruslan Mitkova
</author>
<affiliation confidence="0.996717">
aRIILP, University of Wolverhampton, UK
bLEXYTRAD, University of Malaga, Spain
</affiliation>
<email confidence="0.47807">
{hanna.bechara,hercos,shiva.taslimi,r.gupta,
</email>
<bodyText confidence="0.805402">
c.orasan,gcorpas,r.mitkov}@{awlv.ac.uk,buma.es}
∗These two authors contributed equally to this work.
</bodyText>
<sectionHeader confidence="0.94962" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.801986">
This paper describes the system submitted
by the University of Wolverhampton and the
University of Malaga for SemEval-2015 Task
2: Semantic Textual Similarity. The system
uses a Supported Vector Machine approach
based on a number of linguistically motivated
features. Our system performed satisfactorily
for English and obtained a mean 0.7216
Pearson correlation. However, it performed
less adequately for Spanish, obtaining only a
mean 0.5158.
</bodyText>
<sectionHeader confidence="0.998793" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.984255428571429">
Similarity measures play an important role in
a wide variety of Natural Language Processing
(NLP) applications. Information Retrieval (IR),
for example, relies on semantic similarity in order
to determine the best result for a related query.
Semantic similarity also plays a crucial role in other
applications such as Paraphrasing and Translation
Memory (TM). However, computing semantic
similarity between sentences remains a complex and
difficult task. Over the years, SemEval’s shared
tasks worked to fine-tune and perfect these similarity
measures, and explore the nature of meaning in
language.
SemEval2015’s Task 2 involves computing
how similar two sentences are in both English
(Subtask 2a) and Spanish (Subtask 2b). In
this paper we detail our submission to SemEval
Task 2. We use an improved and revised
version of the system presented in our SemEval
2014 submission (Gupta et al., 2014). As
in Gupta et al., 2014, we employ a Machine
</bodyText>
<page confidence="0.890387">
96
</page>
<bodyText confidence="0.999648444444444">
Learning (ML) method which exploits available
NLP technology, adding features inspired by deep
semantics (such as parsing and paraphrasing)
with distributional Similarity Measures, Conceptual
Similarity Measures, Semantic Similarity Measures
and Corpus Pattern Analysis1 (CPA).
The remainder of the paper is structured as
follows. Section 2 describes our approach, i.e.
explains how the data was preprocessed and what
features were extracted. Section 3 is divided in two
section, the first one describes the ML algorithm and
how it was tuned for this task (section 3.1) and the
second one shows the obtained results along with
a descriptive analysis of the runs based on the test
and training data provided by the SemEval-2015
Task 2 (section 3.2). Finally, section 4 presents
the final remarks and highlights our future plans for
improving the system.
</bodyText>
<sectionHeader confidence="0.984023" genericHeader="introduction">
2 Approach
</sectionHeader>
<bodyText confidence="0.999698">
This section describes our approach to calculating
semantic relatedness. It covers all the required
preprocessing steps to extract the features
themselves.
</bodyText>
<subsectionHeader confidence="0.996716">
2.1 Data Preprocessing
</subsectionHeader>
<bodyText confidence="0.999949333333333">
This section presents all the tools, libraries and
frameworks used to preprocess not only the test
datasets but also the training datasets.
</bodyText>
<subsectionHeader confidence="0.884469">
2.1.1 POS-Tagger, Lemmatiser, Stemmer
</subsectionHeader>
<bodyText confidence="0.9624315">
The software we used for these specific NLP tasks
were: the Stanford CoreNLP2 (Toutanova et al.,
</bodyText>
<footnote confidence="0.994719">
1http://pdev.org.uk
2http://nlp.stanford.edu/software/
corenlp.shtml
</footnote>
<note confidence="0.663733">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 96–101,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.9997474">
2003) toolkit, which provides a lemmatiser, POS-
Tagger, NER, parsing, and coreference; the TT4J3
library, which is a Java wrapper around the popular
TreeTagger (Schmid, 1995); and the Porter stemmer
algorithm provided by the Snowball4 library.
</bodyText>
<subsectionHeader confidence="0.798858">
2.1.2 Named Entity Recogniser (NER)
</subsectionHeader>
<bodyText confidence="0.9999578">
The library used to identify named entities in
English and Spanish was the Apache OpenNLP
library5. For English, all the pre-trained NER
models made available by the Apache OpenNLP
library were used (i.e. we used models to identify
dates, locations, money, organisations, percentages,
persons and time). We also used all the pre-trained
NER models for Spanish (in this case, we used
models to identify persons, organisations, locations
and miscellanea).
</bodyText>
<sectionHeader confidence="0.342457" genericHeader="method">
2.1.3 Translation Model
</sectionHeader>
<bodyText confidence="0.999942133333333">
Since one of the features we implemented was
available only for English (i.e. the Semantic
Similarity Measures), we trained a Statistical
Machine Translation (SMT) system to translate our
Spanish dataset into English. For this purpose,
we used the PB-SMT system Moses (Koehn et al.,
2007), 5-gram language models with Kneser-Ney
smoothing trained with SRILM (Stolcke, 2002), the
GIZA++ implementation of IBM word alignment
model 4 (Och and Ney, 2003), with refinement and
phrase-extraction heuristics as described in Koehn et
al., 2003. We trained this system on the Europarl
Corpus (Koehn, 2005) and used Minimum Error
Rate Training (MERT) (Och, 2003) for tuning on the
development set.
</bodyText>
<subsectionHeader confidence="0.617443">
2.1.4 Resources
</subsectionHeader>
<bodyText confidence="0.999976666666667">
Given that a number of our features depends on
stopwords (see section 2.2), we compiled two lists
of stopwords, one for English and another one for
Spanish. Both are freely available to download6.
We also used two lists (English and Spanish) of
candidates for Multiword Expressions (MWEs) as a
resource for one of the features (see section 2.2.5).
These lists were extracted from the Europarl Corpus
(Koehn, 2005) using the collocation modules of the
</bodyText>
<footnote confidence="0.999818">
3https://code.google.com/p/tt4j
4http://snowball.tartarus.org
5http://opennlp.apache.org
6https://github.com/hpcosta/stopwords
</footnote>
<bodyText confidence="0.986566666666667">
NLTK package (Loper and Bird, 2002), and sorted
by the degree of likelihood association between their
components.
</bodyText>
<subsectionHeader confidence="0.998341">
2.2 Extracted Features
</subsectionHeader>
<bodyText confidence="0.9997215">
This section details the features that our system uses
to measure the semantic textual similarity between
two sentences. The system uses the same features
for both Subtask 2a and Subtask 2b. In addition
to the baseline features used in Gupta et al., 2014,
we introduced a set of Distributional, Semantic and
Conceptual Similarity Measures, as well as a feature
reflecting MWEs across sentences.
</bodyText>
<subsectionHeader confidence="0.510401">
2.2.1 Baseline Features
</subsectionHeader>
<bodyText confidence="0.9998896">
The system is built on the baseline system
developed for SemEval2014, which consists of 13
features explained in detail in Gupta et al., 2014.
The code which implements these features can be
found on GitHub7.
</bodyText>
<subsectionHeader confidence="0.912424">
2.2.2 Distributional Similarity Measures
</subsectionHeader>
<bodyText confidence="0.999972333333333">
Information Retrieval (IR) (Singhal, 2001) is
the task of locating specific information within a
collection of documents or other natural language
resources according to some request (Salton and
Buckley, 1988; Costa et al., 2010; Costa et al.,
2011). Among IR methods, we can find a large
number of statistical approaches based on the
occurrence of words in documents or sentences.
Following Harris’ distributional hypothesis (Harris,
1970), which assumes that similar words tend to
occur in similar contexts, these methods are suitable,
for instance, to find similar sentences based on the
words they contain or to compute the similarity
of words based on their co-occurrence. To that
end, we can assume that the amount of information
contained in a sentence could be evaluated by
summing the amount of information contained in
the sentence words. Moreover, the amount of
information conveyed by a word can be represented
by means of the weight assigned to it (Salton
and Buckley, 1988). Bearing this in mind, we
used two independent IR measures, the Spearman’s
Rank Correlation Coefficient (SCC) and the χ2
to compute the similarity between two sentences
</bodyText>
<footnote confidence="0.989487">
7https://github.com/rohitguptacs/
wlvsimilarity
</footnote>
<page confidence="0.999551">
97
</page>
<bodyText confidence="0.99998725">
written in the same language (cf. Kilgarriff, 2001).
Both measures are particularly useful for this task
because they are independent of text size (mostly
because both measures use a list of the common
entities), and they are language-independent. In
detail, for every pair of sentence (English and
Spanish), we used the lemmas to extract the list of
common terms to compute both measures.
</bodyText>
<subsectionHeader confidence="0.873795">
2.2.3 Conceptual Similarity Measures
</subsectionHeader>
<bodyText confidence="0.99966764">
This feature aims to find the conceptual similarity
between two sentences written in the same
language. In order to calculate the conceptual
similarity, we took advantage of the BabelNet8
(Navigli and Paolo Ponzetto, 2012) multilingual
semantic network. As BabelNet organises lexical
information in a semantic conceptual way, we
created a conceptual sentence for all input pair
of sentences (English and Spanish). More
precisely, for every pair of sentence we only
extracted lemmatised nouns, verbs, adjectives and
adverbs. Then, a conceptual term list was built
by extracting all the occurrences of the term in
the conceptual network (i.e. BabelNet). As a
result, we got a “conceptual representation” of
both sentences, each of them containing a set of
conceptual term lists. Next, for every term in the
“conceptual sentence 1”, we counted the number
of co-occurrences in the conceptual term lists in
the “conceptual sentence 2”. In other words, we
intersected the terms in sentence 1 with all the
conceptual term lists in sentence 2. After computing
all the co-occurrences, we used these values to
calculate the Jaccard’ (Jaccard, 1901), Lin’ (Lin,
1998) and PMI’ (Turney, 2001) scores.
</bodyText>
<subsectionHeader confidence="0.850891">
2.2.4 Semantic Similarity Measures
</subsectionHeader>
<bodyText confidence="0.9997945">
This feature takes advantage of the Align,
Disambiguate and Walk (ADW)9 library (Pilehvar et
al., 2013), a WordNet-based approach for measuring
semantic similarity of arbitrary pairs of lexical
items. It is important to mention that this feature
is the only one that only works for English, which
explains why we have a translation model (see
section 2.1.3). In other words, when we are dealing
</bodyText>
<footnote confidence="0.9985595">
8http://babelnet.org
9http://lcl.uniroma1.it/adw
</footnote>
<bodyText confidence="0.994721777777778">
with Spanish text, we use the trained model to
translate from Spanish to English.
As the ADW library permits us to measure
the semantic similarity between two raw English
sentences, either by using disambiguation or not, we
used both options to calculate all the comparison
methods made available by the library, i.e.
WeightedOverlap, Cosine, Jaccard, KLDivergence
and JensenShannon divergence.
</bodyText>
<subsectionHeader confidence="0.810462">
2.2.5 Multiword Expressions
</subsectionHeader>
<bodyText confidence="0.996863869565217">
Multiword Expressions (MWEs) are meaningful
lexical units whose distinct idiosyncratic properties
call for special treatment within a computational
system. Non-compositionality is one of the
properties of MWEs. The degree of association
between the components of a MWE has been
proved to be a promising approach to find out how
much they are non-compostional and therefore how
probable they are acceptable MWEs (Ramisch et
al., 2010). The more non-compositional a MWE
is, the more important is not to treat its components
separately for NLP purposes, including processing
semantic similarities.
For the purpose of our experiments, we focused
on two more common types of MWEs in English
and Spanish: verb noun combinations and
verb particle constructions. Whenever a
verb+noun or a verb+particle combination
occurs in our sentence pair, we search a prepared
list MWEs, sorted according to their likelihood
measures of association. The degree of association
of these combinations served as a feature in our ML
system.
</bodyText>
<sectionHeader confidence="0.894934" genericHeader="method">
3 Predicting Through Machine Learning
</sectionHeader>
<bodyText confidence="0.999980333333333">
In this section, we outline the ML model trained
on the extracted features to compute a relatedness
score between two sentences. It details the tools and
parameters used to build a support vector regressor,
which we used to predict a number between 0 and 5,
denoting a degree of semantic similarity.
</bodyText>
<subsectionHeader confidence="0.998898">
3.1 Model Description
</subsectionHeader>
<bodyText confidence="0.998712">
We used a Support Vector Machine (SVM) in order
to compute semantic relatedness for both subtasks.
</bodyText>
<page confidence="0.995245">
98
</page>
<bodyText confidence="0.999924217391304">
We used LibSVM10, a library for SVMs developed
by Chang and Lin, 2011.
We built a regression model which estimates
a continuous score between 0 and 5 for each
sentence pair. The values of C and γ have been
optimised through a grid-search which uses a 5-fold
cross-validation method, and all systems use an RBF
kernel.
The system for Subtask 2a (English) is trained
on a combination of training and trial data provided
by the 2012, 2013 and 2014 SemEval tasks. We
used these datasets to form a training set of 9750
sentence pairs combining the different domains
covered by the STS task: image description (image),
news headlines (headlines), student answers paired
with reference answers (answers-students), answers
to questions posted in stach exchange forums
(answers-forum), English discussion forum data
exhibiting committed belief (belief). However, the
training set for Subtask 2b (Spanish) was much
smaller, at only 804 sentence pairs collected by
combining previous datasets from the Newswire and
Wikipedia domains.
</bodyText>
<subsectionHeader confidence="0.973725">
3.2 Results and Analysis
</subsectionHeader>
<bodyText confidence="0.99998065">
The task required the submission of 3 different
runs for each task. The runs for the Subtask
2a (English) were identical except for some
parameter differences for the SVM training. Our
system performed adequately, with our primary run
achieving a mean Pearson Correlation of 0.7216.
However, the runs for Subtask 2b (Spanish) were
trained on different training sets. Run-1 and Run-2
are trained on the 804 Spanish sentence-pairs. The
Spanish set’s Run-3, however, is trained on the much
larger English training set. For this purpose, we
needed to translate the Spanish test set into English
in order to use the Semantic Similarity language-
dependent features (see sections 2.1.3 and 2.2.4).
This system did not outperform the basic Spanish
model used in Run-1 and Run-2, despite the much
larger training set. Our Spanish system did not yield
a satisfactory performance, achieving a Pearson
Correlation score of only 0.5158. This could be
part due to the smaller training set in Spanish,
</bodyText>
<footnote confidence="0.899323">
10http://www.csie.ntu.edu.tw/˜cjlin/
libsvm/
</footnote>
<bodyText confidence="0.96600075">
and the imperfect translations into English which
consequently influenced the performance of the
language-dependent features. The detailed results
for both tasks are given in Table 1 and 2.
</bodyText>
<table confidence="0.9995245">
Run-1 Run-2 Run-3
answers-forums 0.6781 0.6454 0.6179
answers-students 0.7304 0.7093 0.6977
belief 0.6294 0.5165 0.3236
headlines 0.6912 0.6084 0.5775
images 0.8109 0.7999 0.7954
mean 0.7216 0.6746 0.6353
rank (out of 74) 33 45 55
</table>
<tableCaption confidence="0.998117">
Table 1: Task 2a – Pearson Correlation for English.
</tableCaption>
<table confidence="0.9992222">
Run-1 Run-2 Run-3
wikipedia 0.5239 0.4671 0.4402
newswire 0.5076 0.5437 0.5524
mean 0.5158 0.5054 0.4963
rank (out of 17) 9 10 11
</table>
<tableCaption confidence="0.998714">
Table 2: Task 2b – Pearson Correlation for Spanish.
</tableCaption>
<sectionHeader confidence="0.963359" genericHeader="conclusions">
4 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999917761904762">
We have presented an efficient approach to calculate
semantic relatedness for both English and Spanish
sentence pairs. We used the same feature set for both
tasks, even though it meant translating the Spanish
sentences into English before extracting one of the
features (i.e. the Semantic Similarity). The system
did not performed well for Spanish as it ranked 9
out of 17, with a 0.5158 average Person correlation
over two test sets (0.1747 correlation points less
than the best submitted run). On the other hand, it
performed reasonably well for English, where the
system’s best result ranked 33 among 74 submitted
runs with 0.7216 Pearson correlation over five test
sets (only 0.0799 correlation points less than the best
submitted run).
In the future we plan to extract the conceptual
description provided by the BabelNet network in
order to match it with the conceptual terms. We have
not done that for now because we need to treat these
descriptions as sentences, which requires filtering
out the noise produced by them.
</bodyText>
<page confidence="0.998499">
99
</page>
<sectionHeader confidence="0.984246" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999564090909091">
Hanna B´echara, Hernani Costa and Rohit Gupta
are supported by the People Programme (Marie
Curie Actions) of the European Union’s Framework
Programme (FP7/2007-2013) under REA grant
agreement no 317471. Also, the research reported
in this work has been partially carried out in the
framework of the Educational Innovation Project
TRADICOR (PIE 13-054, 2014-2015); the R&amp;D
project INTELITERM (ref. no FFI2012-38881,
2012-2015); and the R&amp;D Project for Excellence
TERMITUR (ref. no HUM2754, 2014-2017).
</bodyText>
<sectionHeader confidence="0.995136" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.989494752808988">
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2:27.
Hernani Costa, Hugo Gonc¸alo Oliveira, and Paulo
Gomes. 2010. The Impact of Distributional
Metrics in the Quality of Relational Triples. In 19th
European Conf. on Artificial Intelligence, Workshop
on Language Technology for Cultural Heritage, Social
Sciences, and Humanities, ECAI’10, pages 23–29,
Lisbon, Portugal.
Hernani Costa, Hugo Gonc¸alo Oliveira, and Paulo
Gomes. 2011. Using the Web to Validate Lexico-
Semantic Relations. In 15th Portuguese Conf. on
Artificial Intelligence, volume 7026 of EPIA’11, pages
597–609, Lisbon, Portugal. Springer.
Rohit Gupta, Hanna Bechara, Ismail El Maarouf, and
Constantin Orasan. 2014. UoW: NLP techniques
developed at the University of Wolverhampton for
Semantic Similarity and Textual Entailment. In 8th
Int. Workshop on Semantic Evaluation (SemEval’14),
pages 785–789, Dublin, Ireland. ACL and Dublin City
University.
Zelig Harris. 1970. Distributional Structure. In Papers
in Structural and Transformational Linguistics, pages
775–794. D. Reidel Publishing Company, Dordrecht,
Holland.
Paul Jaccard. 1901.
florale dans une portion des alpes et des jura. Bulletin
del la Soci´et´e Vaudoise des Sciences Naturelles,
37:547–579.
Adam Kilgarriff. 2001. Comparing Corpora. Int.
Journal of Corpus Linguistics, 6(1):97–133.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Conf. of
the North American Chapter of the ACL on Human
Language Technology - Volume 1, NAACL’03, pages
48–54. ACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation. In
45th Annual Meeting of the ACL on Interactive Poster
and Demonstration Sessions, pages 177–180.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In MT Summit.
Dekang Lin. 1998. An Information-Theoretic Definition
of Similarity. In 15th Int. Conf. on Machine Learning,
ICML’98, pages 296–304, San Francisco, CA, USA.
Morgan Kaufmann.
Edward Loper and Steven Bird. 2002. NLTK: The
Natural Language Toolkit. In ACL-02 Workshop
on Effective Tools and Methodologies for Teaching
Natural Language Processing and Computational
Linguistics - Volume 1, ETMTNLP’02, pages 62–69.
ACL.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The Automatic Construction, Evaluation
and Application of a Wide-Coverage Multilingual
Semantic Network. Artificial Intelligence, 193:217–
250.
Franz Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19–51.
Franz Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In 41st Annual
Meeting on ACL - Volume 1, ACL’03, pages 160–167.
ACL.
Mohammad Taher Pilehvar, David Jurgens, and Roberto
Navigli. 2013. Align, Disambiguate and Walk: A
Unified Approach for Measuring Semantic Similarity.
In 51st Annual Meeting of the ACL - Volume 1, pages
1341–1351, Sofia, Bulgaria. ACL.
Carlos Ramisch, Aline Villavicencio, and Christian
Boitet. 2010. Multiword Expressions in the Wild?:
The Mwetoolkit Comes in Handy. In 23rd Int.
Conf. on Computational Linguistics: Demonstrations,
COLING’10, pages 57–60. ACL.
Gerard Salton and Christopher Buckley. 1988. Term-
Weighting Approaches in Automatic Text Retrieval.
Information Processing &amp; Management, 24(5):513–
523.
Helmut Schmid. 1995. Improvements In Part-of-Speech
Tagging With an Application To German. In ACL
SIGDAT-Workshop, pages 47–50, Dublin, Ireland.
Amit Singhal. 2001. Modern Information Retrieval:
A Brief Overview. Bulletin of the IEEE Computer
</reference>
<bodyText confidence="0.283159">
´Etude comparative de la distribution
</bodyText>
<page confidence="0.767111">
100
</page>
<reference confidence="0.992761733333333">
Society Technical Committee on Data Engineering,
24(4):35–42.
Andreas Stolcke. 2002. SRILM - an Extensible
Language Modeling Toolkit. In 71h Int. Conf. on
Spoken Language Processing, ICSLP’02, pages 901–
904.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Network.
In HLT-NAAC 2003, pages 252–259, Edmonton,
Canada. ACL.
Peter D. Turney. 2001. Mining the Web for Synonyms:
PMI-IR Versus LSA on TOEFL. In 121h European
Conf. on Machine Learning, EMCL’01, pages 491–
502, London, UK. Springer.
</reference>
<page confidence="0.998608">
101
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.386422">
<title confidence="0.9942365">MiniExperts: An SVM Approach for Semantic Textual Similarity</title>
<author confidence="0.8453195">Hernani Shiva Rohit Gloria Corpas</author>
<author confidence="0.8453195">Ruslan</author>
<affiliation confidence="0.9982285">University of Wolverhampton, University of Malaga,</affiliation>
<abstract confidence="0.963707538461538">two authors contributed equally to this work. Abstract This paper describes the system submitted by the University of Wolverhampton and the University of Malaga for SemEval-2015 Task Textual The system uses a Supported Vector Machine approach based on a number of linguistically motivated features. Our system performed satisfactorily for English and obtained a mean 0.7216 Pearson correlation. However, it performed less adequately for Spanish, obtaining only a mean 0.5158.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<pages>2--27</pages>
<contexts>
<context position="11531" citStr="Chang and Lin, 2011" startWordPosition="1720" endWordPosition="1723">. The degree of association of these combinations served as a feature in our ML system. 3 Predicting Through Machine Learning In this section, we outline the ML model trained on the extracted features to compute a relatedness score between two sentences. It details the tools and parameters used to build a support vector regressor, which we used to predict a number between 0 and 5, denoting a degree of semantic similarity. 3.1 Model Description We used a Support Vector Machine (SVM) in order to compute semantic relatedness for both subtasks. 98 We used LibSVM10, a library for SVMs developed by Chang and Lin, 2011. We built a regression model which estimates a continuous score between 0 and 5 for each sentence pair. The values of C and γ have been optimised through a grid-search which uses a 5-fold cross-validation method, and all systems use an RBF kernel. The system for Subtask 2a (English) is trained on a combination of training and trial data provided by the 2012, 2013 and 2014 SemEval tasks. We used these datasets to form a training set of 9750 sentence pairs combining the different domains covered by the STS task: image description (image), news headlines (headlines), student answers paired with </context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hernani Costa</author>
<author>Hugo Gonc¸alo Oliveira</author>
<author>Paulo Gomes</author>
</authors>
<title>The Impact of Distributional Metrics in the Quality of Relational Triples.</title>
<date>2010</date>
<booktitle>In 19th European Conf. on Artificial Intelligence, Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, ECAI’10,</booktitle>
<pages>23--29</pages>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="6486" citStr="Costa et al., 2010" startWordPosition="939" endWordPosition="942">Distributional, Semantic and Conceptual Similarity Measures, as well as a feature reflecting MWEs across sentences. 2.2.1 Baseline Features The system is built on the baseline system developed for SemEval2014, which consists of 13 features explained in detail in Gupta et al., 2014. The code which implements these features can be found on GitHub7. 2.2.2 Distributional Similarity Measures Information Retrieval (IR) (Singhal, 2001) is the task of locating specific information within a collection of documents or other natural language resources according to some request (Salton and Buckley, 1988; Costa et al., 2010; Costa et al., 2011). Among IR methods, we can find a large number of statistical approaches based on the occurrence of words in documents or sentences. Following Harris’ distributional hypothesis (Harris, 1970), which assumes that similar words tend to occur in similar contexts, these methods are suitable, for instance, to find similar sentences based on the words they contain or to compute the similarity of words based on their co-occurrence. To that end, we can assume that the amount of information contained in a sentence could be evaluated by summing the amount of information contained in</context>
</contexts>
<marker>Costa, Oliveira, Gomes, 2010</marker>
<rawString>Hernani Costa, Hugo Gonc¸alo Oliveira, and Paulo Gomes. 2010. The Impact of Distributional Metrics in the Quality of Relational Triples. In 19th European Conf. on Artificial Intelligence, Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, ECAI’10, pages 23–29, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hernani Costa</author>
<author>Hugo Gonc¸alo Oliveira</author>
<author>Paulo Gomes</author>
</authors>
<title>Using the Web to Validate LexicoSemantic Relations.</title>
<date>2011</date>
<booktitle>In 15th Portuguese Conf. on Artificial Intelligence,</booktitle>
<volume>7026</volume>
<pages>597--609</pages>
<publisher>Springer.</publisher>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="6507" citStr="Costa et al., 2011" startWordPosition="943" endWordPosition="946">ntic and Conceptual Similarity Measures, as well as a feature reflecting MWEs across sentences. 2.2.1 Baseline Features The system is built on the baseline system developed for SemEval2014, which consists of 13 features explained in detail in Gupta et al., 2014. The code which implements these features can be found on GitHub7. 2.2.2 Distributional Similarity Measures Information Retrieval (IR) (Singhal, 2001) is the task of locating specific information within a collection of documents or other natural language resources according to some request (Salton and Buckley, 1988; Costa et al., 2010; Costa et al., 2011). Among IR methods, we can find a large number of statistical approaches based on the occurrence of words in documents or sentences. Following Harris’ distributional hypothesis (Harris, 1970), which assumes that similar words tend to occur in similar contexts, these methods are suitable, for instance, to find similar sentences based on the words they contain or to compute the similarity of words based on their co-occurrence. To that end, we can assume that the amount of information contained in a sentence could be evaluated by summing the amount of information contained in the sentence words. </context>
</contexts>
<marker>Costa, Oliveira, Gomes, 2011</marker>
<rawString>Hernani Costa, Hugo Gonc¸alo Oliveira, and Paulo Gomes. 2011. Using the Web to Validate LexicoSemantic Relations. In 15th Portuguese Conf. on Artificial Intelligence, volume 7026 of EPIA’11, pages 597–609, Lisbon, Portugal. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit Gupta</author>
<author>Hanna Bechara</author>
<author>Ismail El Maarouf</author>
<author>Constantin Orasan</author>
</authors>
<title>UoW: NLP techniques developed at the University of Wolverhampton for Semantic Similarity and Textual Entailment.</title>
<date>2014</date>
<booktitle>In 8th Int. Workshop on Semantic Evaluation (SemEval’14),</booktitle>
<pages>785--789</pages>
<institution>ACL and Dublin City University.</institution>
<location>Dublin,</location>
<marker>Gupta, Bechara, El Maarouf, Orasan, 2014</marker>
<rawString>Rohit Gupta, Hanna Bechara, Ismail El Maarouf, and Constantin Orasan. 2014. UoW: NLP techniques developed at the University of Wolverhampton for Semantic Similarity and Textual Entailment. In 8th Int. Workshop on Semantic Evaluation (SemEval’14), pages 785–789, Dublin, Ireland. ACL and Dublin City University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zelig Harris</author>
</authors>
<title>Distributional Structure.</title>
<date>1970</date>
<booktitle>In Papers in Structural and Transformational Linguistics,</booktitle>
<pages>775--794</pages>
<publisher>D. Reidel Publishing Company,</publisher>
<location>Dordrecht, Holland.</location>
<contexts>
<context position="6698" citStr="Harris, 1970" startWordPosition="973" endWordPosition="974">onsists of 13 features explained in detail in Gupta et al., 2014. The code which implements these features can be found on GitHub7. 2.2.2 Distributional Similarity Measures Information Retrieval (IR) (Singhal, 2001) is the task of locating specific information within a collection of documents or other natural language resources according to some request (Salton and Buckley, 1988; Costa et al., 2010; Costa et al., 2011). Among IR methods, we can find a large number of statistical approaches based on the occurrence of words in documents or sentences. Following Harris’ distributional hypothesis (Harris, 1970), which assumes that similar words tend to occur in similar contexts, these methods are suitable, for instance, to find similar sentences based on the words they contain or to compute the similarity of words based on their co-occurrence. To that end, we can assume that the amount of information contained in a sentence could be evaluated by summing the amount of information contained in the sentence words. Moreover, the amount of information conveyed by a word can be represented by means of the weight assigned to it (Salton and Buckley, 1988). Bearing this in mind, we used two independent IR me</context>
</contexts>
<marker>Harris, 1970</marker>
<rawString>Zelig Harris. 1970. Distributional Structure. In Papers in Structural and Transformational Linguistics, pages 775–794. D. Reidel Publishing Company, Dordrecht, Holland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Jaccard</author>
</authors>
<title>florale dans une portion des alpes et des jura.</title>
<date>1901</date>
<booktitle>Bulletin del la Soci´et´e Vaudoise des Sciences Naturelles,</booktitle>
<pages>37--547</pages>
<contexts>
<context position="9031" citStr="Jaccard, 1901" startWordPosition="1338" endWordPosition="1339">Then, a conceptual term list was built by extracting all the occurrences of the term in the conceptual network (i.e. BabelNet). As a result, we got a “conceptual representation” of both sentences, each of them containing a set of conceptual term lists. Next, for every term in the “conceptual sentence 1”, we counted the number of co-occurrences in the conceptual term lists in the “conceptual sentence 2”. In other words, we intersected the terms in sentence 1 with all the conceptual term lists in sentence 2. After computing all the co-occurrences, we used these values to calculate the Jaccard’ (Jaccard, 1901), Lin’ (Lin, 1998) and PMI’ (Turney, 2001) scores. 2.2.4 Semantic Similarity Measures This feature takes advantage of the Align, Disambiguate and Walk (ADW)9 library (Pilehvar et al., 2013), a WordNet-based approach for measuring semantic similarity of arbitrary pairs of lexical items. It is important to mention that this feature is the only one that only works for English, which explains why we have a translation model (see section 2.1.3). In other words, when we are dealing 8http://babelnet.org 9http://lcl.uniroma1.it/adw with Spanish text, we use the trained model to translate from Spanish </context>
</contexts>
<marker>Jaccard, 1901</marker>
<rawString>Paul Jaccard. 1901. florale dans une portion des alpes et des jura. Bulletin del la Soci´et´e Vaudoise des Sciences Naturelles, 37:547–579.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
</authors>
<date>2001</date>
<journal>Comparing Corpora. Int. Journal of Corpus Linguistics,</journal>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="7517" citStr="Kilgarriff, 2001" startWordPosition="1101" endWordPosition="1102">words based on their co-occurrence. To that end, we can assume that the amount of information contained in a sentence could be evaluated by summing the amount of information contained in the sentence words. Moreover, the amount of information conveyed by a word can be represented by means of the weight assigned to it (Salton and Buckley, 1988). Bearing this in mind, we used two independent IR measures, the Spearman’s Rank Correlation Coefficient (SCC) and the χ2 to compute the similarity between two sentences 7https://github.com/rohitguptacs/ wlvsimilarity 97 written in the same language (cf. Kilgarriff, 2001). Both measures are particularly useful for this task because they are independent of text size (mostly because both measures use a list of the common entities), and they are language-independent. In detail, for every pair of sentence (English and Spanish), we used the lemmas to extract the list of common terms to compute both measures. 2.2.3 Conceptual Similarity Measures This feature aims to find the conceptual similarity between two sentences written in the same language. In order to calculate the conceptual similarity, we took advantage of the BabelNet8 (Navigli and Paolo Ponzetto, 2012) m</context>
</contexts>
<marker>Kilgarriff, 2001</marker>
<rawString>Adam Kilgarriff. 2001. Comparing Corpora. Int. Journal of Corpus Linguistics, 6(1):97–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In Conf. of the North American Chapter of the ACL on Human Language Technology -</booktitle>
<volume>1</volume>
<pages>48--54</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="4705" citStr="Koehn et al., 2003" startWordPosition="675" endWordPosition="678">entify persons, organisations, locations and miscellanea). 2.1.3 Translation Model Since one of the features we implemented was available only for English (i.e. the Semantic Similarity Measures), we trained a Statistical Machine Translation (SMT) system to translate our Spanish dataset into English. For this purpose, we used the PB-SMT system Moses (Koehn et al., 2007), 5-gram language models with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002), the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), with refinement and phrase-extraction heuristics as described in Koehn et al., 2003. We trained this system on the Europarl Corpus (Koehn, 2005) and used Minimum Error Rate Training (MERT) (Och, 2003) for tuning on the development set. 2.1.4 Resources Given that a number of our features depends on stopwords (see section 2.2), we compiled two lists of stopwords, one for English and another one for Spanish. Both are freely available to download6. We also used two lists (English and Spanish) of candidates for Multiword Expressions (MWEs) as a resource for one of the features (see section 2.2.5). These lists were extracted from the Europarl Corpus (Koehn, 2005) using the colloca</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In Conf. of the North American Chapter of the ACL on Human Language Technology - Volume 1, NAACL’03, pages 48–54. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,</booktitle>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra</location>
<contexts>
<context position="4458" citStr="Koehn et al., 2007" startWordPosition="639" endWordPosition="642">e available by the Apache OpenNLP library were used (i.e. we used models to identify dates, locations, money, organisations, percentages, persons and time). We also used all the pre-trained NER models for Spanish (in this case, we used models to identify persons, organisations, locations and miscellanea). 2.1.3 Translation Model Since one of the features we implemented was available only for English (i.e. the Semantic Similarity Measures), we trained a Statistical Machine Translation (SMT) system to translate our Spanish dataset into English. For this purpose, we used the PB-SMT system Moses (Koehn et al., 2007), 5-gram language models with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002), the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), with refinement and phrase-extraction heuristics as described in Koehn et al., 2003. We trained this system on the Europarl Corpus (Koehn, 2005) and used Minimum Error Rate Training (MERT) (Och, 2003) for tuning on the development set. 2.1.4 Resources Given that a number of our features depends on stopwords (see section 2.2), we compiled two lists of stopwords, one for English and another one for Spanish. Both are freely available t</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A Parallel Corpus for Statistical Machine Translation. In</title>
<date>2005</date>
<institution>MT Summit.</institution>
<contexts>
<context position="4766" citStr="Koehn, 2005" startWordPosition="687" endWordPosition="688">anslation Model Since one of the features we implemented was available only for English (i.e. the Semantic Similarity Measures), we trained a Statistical Machine Translation (SMT) system to translate our Spanish dataset into English. For this purpose, we used the PB-SMT system Moses (Koehn et al., 2007), 5-gram language models with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002), the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), with refinement and phrase-extraction heuristics as described in Koehn et al., 2003. We trained this system on the Europarl Corpus (Koehn, 2005) and used Minimum Error Rate Training (MERT) (Och, 2003) for tuning on the development set. 2.1.4 Resources Given that a number of our features depends on stopwords (see section 2.2), we compiled two lists of stopwords, one for English and another one for Spanish. Both are freely available to download6. We also used two lists (English and Spanish) of candidates for Multiword Expressions (MWEs) as a resource for one of the features (see section 2.2.5). These lists were extracted from the Europarl Corpus (Koehn, 2005) using the collocation modules of the 3https://code.google.com/p/tt4j 4http://s</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An Information-Theoretic Definition of Similarity.</title>
<date>1998</date>
<booktitle>In 15th Int. Conf. on Machine Learning, ICML’98,</booktitle>
<pages>296--304</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="9049" citStr="Lin, 1998" startWordPosition="1341" endWordPosition="1342">m list was built by extracting all the occurrences of the term in the conceptual network (i.e. BabelNet). As a result, we got a “conceptual representation” of both sentences, each of them containing a set of conceptual term lists. Next, for every term in the “conceptual sentence 1”, we counted the number of co-occurrences in the conceptual term lists in the “conceptual sentence 2”. In other words, we intersected the terms in sentence 1 with all the conceptual term lists in sentence 2. After computing all the co-occurrences, we used these values to calculate the Jaccard’ (Jaccard, 1901), Lin’ (Lin, 1998) and PMI’ (Turney, 2001) scores. 2.2.4 Semantic Similarity Measures This feature takes advantage of the Align, Disambiguate and Walk (ADW)9 library (Pilehvar et al., 2013), a WordNet-based approach for measuring semantic similarity of arbitrary pairs of lexical items. It is important to mention that this feature is the only one that only works for English, which explains why we have a translation model (see section 2.1.3). In other words, when we are dealing 8http://babelnet.org 9http://lcl.uniroma1.it/adw with Spanish text, we use the trained model to translate from Spanish to English. As the</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An Information-Theoretic Definition of Similarity. In 15th Int. Conf. on Machine Learning, ICML’98, pages 296–304, San Francisco, CA, USA. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Loper</author>
<author>Steven Bird</author>
</authors>
<title>NLTK: The Natural Language Toolkit.</title>
<date>2002</date>
<booktitle>In ACL-02 Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics -</booktitle>
<volume>1</volume>
<pages>62--69</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="5487" citStr="Loper and Bird, 2002" startWordPosition="786" endWordPosition="789">sources Given that a number of our features depends on stopwords (see section 2.2), we compiled two lists of stopwords, one for English and another one for Spanish. Both are freely available to download6. We also used two lists (English and Spanish) of candidates for Multiword Expressions (MWEs) as a resource for one of the features (see section 2.2.5). These lists were extracted from the Europarl Corpus (Koehn, 2005) using the collocation modules of the 3https://code.google.com/p/tt4j 4http://snowball.tartarus.org 5http://opennlp.apache.org 6https://github.com/hpcosta/stopwords NLTK package (Loper and Bird, 2002), and sorted by the degree of likelihood association between their components. 2.2 Extracted Features This section details the features that our system uses to measure the semantic textual similarity between two sentences. The system uses the same features for both Subtask 2a and Subtask 2b. In addition to the baseline features used in Gupta et al., 2014, we introduced a set of Distributional, Semantic and Conceptual Similarity Measures, as well as a feature reflecting MWEs across sentences. 2.2.1 Baseline Features The system is built on the baseline system developed for SemEval2014, which con</context>
</contexts>
<marker>Loper, Bird, 2002</marker>
<rawString>Edward Loper and Steven Bird. 2002. NLTK: The Natural Language Toolkit. In ACL-02 Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics - Volume 1, ETMTNLP’02, pages 62–69. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>BabelNet: The Automatic Construction, Evaluation and Application of a Wide-Coverage Multilingual Semantic Network.</title>
<date>2012</date>
<journal>Artificial Intelligence,</journal>
<volume>193</volume>
<pages>250</pages>
<marker>Navigli, Ponzetto, 2012</marker>
<rawString>Roberto Navigli and Simone Paolo Ponzetto. 2012. BabelNet: The Automatic Construction, Evaluation and Application of a Wide-Coverage Multilingual Semantic Network. Artificial Intelligence, 193:217– 250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="4620" citStr="Och and Ney, 2003" startWordPosition="663" endWordPosition="666">o used all the pre-trained NER models for Spanish (in this case, we used models to identify persons, organisations, locations and miscellanea). 2.1.3 Translation Model Since one of the features we implemented was available only for English (i.e. the Semantic Similarity Measures), we trained a Statistical Machine Translation (SMT) system to translate our Spanish dataset into English. For this purpose, we used the PB-SMT system Moses (Koehn et al., 2007), 5-gram language models with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002), the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), with refinement and phrase-extraction heuristics as described in Koehn et al., 2003. We trained this system on the Europarl Corpus (Koehn, 2005) and used Minimum Error Rate Training (MERT) (Och, 2003) for tuning on the development set. 2.1.4 Resources Given that a number of our features depends on stopwords (see section 2.2), we compiled two lists of stopwords, one for English and another one for Spanish. Both are freely available to download6. We also used two lists (English and Spanish) of candidates for Multiword Expressions (MWEs) as a resource for one of the features (see section 2.2.5)</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In 41st Annual Meeting on ACL - Volume 1, ACL’03,</booktitle>
<pages>160--167</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="4822" citStr="Och, 2003" startWordPosition="696" endWordPosition="697">as available only for English (i.e. the Semantic Similarity Measures), we trained a Statistical Machine Translation (SMT) system to translate our Spanish dataset into English. For this purpose, we used the PB-SMT system Moses (Koehn et al., 2007), 5-gram language models with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002), the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), with refinement and phrase-extraction heuristics as described in Koehn et al., 2003. We trained this system on the Europarl Corpus (Koehn, 2005) and used Minimum Error Rate Training (MERT) (Och, 2003) for tuning on the development set. 2.1.4 Resources Given that a number of our features depends on stopwords (see section 2.2), we compiled two lists of stopwords, one for English and another one for Spanish. Both are freely available to download6. We also used two lists (English and Spanish) of candidates for Multiword Expressions (MWEs) as a resource for one of the features (see section 2.2.5). These lists were extracted from the Europarl Corpus (Koehn, 2005) using the collocation modules of the 3https://code.google.com/p/tt4j 4http://snowball.tartarus.org 5http://opennlp.apache.org 6https:/</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In 41st Annual Meeting on ACL - Volume 1, ACL’03, pages 160–167. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammad Taher Pilehvar</author>
<author>David Jurgens</author>
<author>Roberto Navigli</author>
</authors>
<title>Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity.</title>
<date>2013</date>
<booktitle>In 51st Annual Meeting of the ACL -</booktitle>
<volume>1</volume>
<pages>1341--1351</pages>
<publisher>ACL.</publisher>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="9220" citStr="Pilehvar et al., 2013" startWordPosition="1364" endWordPosition="1367">both sentences, each of them containing a set of conceptual term lists. Next, for every term in the “conceptual sentence 1”, we counted the number of co-occurrences in the conceptual term lists in the “conceptual sentence 2”. In other words, we intersected the terms in sentence 1 with all the conceptual term lists in sentence 2. After computing all the co-occurrences, we used these values to calculate the Jaccard’ (Jaccard, 1901), Lin’ (Lin, 1998) and PMI’ (Turney, 2001) scores. 2.2.4 Semantic Similarity Measures This feature takes advantage of the Align, Disambiguate and Walk (ADW)9 library (Pilehvar et al., 2013), a WordNet-based approach for measuring semantic similarity of arbitrary pairs of lexical items. It is important to mention that this feature is the only one that only works for English, which explains why we have a translation model (see section 2.1.3). In other words, when we are dealing 8http://babelnet.org 9http://lcl.uniroma1.it/adw with Spanish text, we use the trained model to translate from Spanish to English. As the ADW library permits us to measure the semantic similarity between two raw English sentences, either by using disambiguation or not, we used both options to calculate all </context>
</contexts>
<marker>Pilehvar, Jurgens, Navigli, 2013</marker>
<rawString>Mohammad Taher Pilehvar, David Jurgens, and Roberto Navigli. 2013. Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity. In 51st Annual Meeting of the ACL - Volume 1, pages 1341–1351, Sofia, Bulgaria. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Ramisch</author>
<author>Aline Villavicencio</author>
<author>Christian Boitet</author>
</authors>
<title>Multiword Expressions in the Wild?: The Mwetoolkit Comes in Handy.</title>
<date>2010</date>
<booktitle>In 23rd Int. Conf. on Computational Linguistics: Demonstrations, COLING’10,</booktitle>
<pages>57--60</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="10417" citStr="Ramisch et al., 2010" startWordPosition="1541" endWordPosition="1544">ptions to calculate all the comparison methods made available by the library, i.e. WeightedOverlap, Cosine, Jaccard, KLDivergence and JensenShannon divergence. 2.2.5 Multiword Expressions Multiword Expressions (MWEs) are meaningful lexical units whose distinct idiosyncratic properties call for special treatment within a computational system. Non-compositionality is one of the properties of MWEs. The degree of association between the components of a MWE has been proved to be a promising approach to find out how much they are non-compostional and therefore how probable they are acceptable MWEs (Ramisch et al., 2010). The more non-compositional a MWE is, the more important is not to treat its components separately for NLP purposes, including processing semantic similarities. For the purpose of our experiments, we focused on two more common types of MWEs in English and Spanish: verb noun combinations and verb particle constructions. Whenever a verb+noun or a verb+particle combination occurs in our sentence pair, we search a prepared list MWEs, sorted according to their likelihood measures of association. The degree of association of these combinations served as a feature in our ML system. 3 Predicting Thro</context>
</contexts>
<marker>Ramisch, Villavicencio, Boitet, 2010</marker>
<rawString>Carlos Ramisch, Aline Villavicencio, and Christian Boitet. 2010. Multiword Expressions in the Wild?: The Mwetoolkit Comes in Handy. In 23rd Int. Conf. on Computational Linguistics: Demonstrations, COLING’10, pages 57–60. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Christopher Buckley</author>
</authors>
<title>TermWeighting Approaches in Automatic Text Retrieval.</title>
<date>1988</date>
<journal>Information Processing &amp; Management,</journal>
<volume>24</volume>
<issue>5</issue>
<pages>523</pages>
<contexts>
<context position="6466" citStr="Salton and Buckley, 1988" startWordPosition="935" endWordPosition="938">4, we introduced a set of Distributional, Semantic and Conceptual Similarity Measures, as well as a feature reflecting MWEs across sentences. 2.2.1 Baseline Features The system is built on the baseline system developed for SemEval2014, which consists of 13 features explained in detail in Gupta et al., 2014. The code which implements these features can be found on GitHub7. 2.2.2 Distributional Similarity Measures Information Retrieval (IR) (Singhal, 2001) is the task of locating specific information within a collection of documents or other natural language resources according to some request (Salton and Buckley, 1988; Costa et al., 2010; Costa et al., 2011). Among IR methods, we can find a large number of statistical approaches based on the occurrence of words in documents or sentences. Following Harris’ distributional hypothesis (Harris, 1970), which assumes that similar words tend to occur in similar contexts, these methods are suitable, for instance, to find similar sentences based on the words they contain or to compute the similarity of words based on their co-occurrence. To that end, we can assume that the amount of information contained in a sentence could be evaluated by summing the amount of info</context>
</contexts>
<marker>Salton, Buckley, 1988</marker>
<rawString>Gerard Salton and Christopher Buckley. 1988. TermWeighting Approaches in Automatic Text Retrieval. Information Processing &amp; Management, 24(5):513– 523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Improvements In Part-of-Speech Tagging With an Application To German.</title>
<date>1995</date>
<booktitle>In ACL SIGDAT-Workshop,</booktitle>
<pages>47--50</pages>
<location>Dublin, Ireland.</location>
<contexts>
<context position="3586" citStr="Schmid, 1995" startWordPosition="509" endWordPosition="510">nly the test datasets but also the training datasets. 2.1.1 POS-Tagger, Lemmatiser, Stemmer The software we used for these specific NLP tasks were: the Stanford CoreNLP2 (Toutanova et al., 1http://pdev.org.uk 2http://nlp.stanford.edu/software/ corenlp.shtml Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 96–101, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics 2003) toolkit, which provides a lemmatiser, POSTagger, NER, parsing, and coreference; the TT4J3 library, which is a Java wrapper around the popular TreeTagger (Schmid, 1995); and the Porter stemmer algorithm provided by the Snowball4 library. 2.1.2 Named Entity Recogniser (NER) The library used to identify named entities in English and Spanish was the Apache OpenNLP library5. For English, all the pre-trained NER models made available by the Apache OpenNLP library were used (i.e. we used models to identify dates, locations, money, organisations, percentages, persons and time). We also used all the pre-trained NER models for Spanish (in this case, we used models to identify persons, organisations, locations and miscellanea). 2.1.3 Translation Model Since one of the</context>
</contexts>
<marker>Schmid, 1995</marker>
<rawString>Helmut Schmid. 1995. Improvements In Part-of-Speech Tagging With an Application To German. In ACL SIGDAT-Workshop, pages 47–50, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Singhal</author>
</authors>
<title>Modern Information Retrieval: A Brief Overview.</title>
<date>2001</date>
<journal>Bulletin of the IEEE Computer Society Technical Committee on Data Engineering,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="6300" citStr="Singhal, 2001" startWordPosition="912" endWordPosition="913"> between two sentences. The system uses the same features for both Subtask 2a and Subtask 2b. In addition to the baseline features used in Gupta et al., 2014, we introduced a set of Distributional, Semantic and Conceptual Similarity Measures, as well as a feature reflecting MWEs across sentences. 2.2.1 Baseline Features The system is built on the baseline system developed for SemEval2014, which consists of 13 features explained in detail in Gupta et al., 2014. The code which implements these features can be found on GitHub7. 2.2.2 Distributional Similarity Measures Information Retrieval (IR) (Singhal, 2001) is the task of locating specific information within a collection of documents or other natural language resources according to some request (Salton and Buckley, 1988; Costa et al., 2010; Costa et al., 2011). Among IR methods, we can find a large number of statistical approaches based on the occurrence of words in documents or sentences. Following Harris’ distributional hypothesis (Harris, 1970), which assumes that similar words tend to occur in similar contexts, these methods are suitable, for instance, to find similar sentences based on the words they contain or to compute the similarity of </context>
</contexts>
<marker>Singhal, 2001</marker>
<rawString>Amit Singhal. 2001. Modern Information Retrieval: A Brief Overview. Bulletin of the IEEE Computer Society Technical Committee on Data Engineering, 24(4):35–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In 71h Int. Conf. on Spoken Language Processing, ICSLP’02,</booktitle>
<pages>901--904</pages>
<contexts>
<context position="4543" citStr="Stolcke, 2002" startWordPosition="652" endWordPosition="653">, locations, money, organisations, percentages, persons and time). We also used all the pre-trained NER models for Spanish (in this case, we used models to identify persons, organisations, locations and miscellanea). 2.1.3 Translation Model Since one of the features we implemented was available only for English (i.e. the Semantic Similarity Measures), we trained a Statistical Machine Translation (SMT) system to translate our Spanish dataset into English. For this purpose, we used the PB-SMT system Moses (Koehn et al., 2007), 5-gram language models with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002), the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), with refinement and phrase-extraction heuristics as described in Koehn et al., 2003. We trained this system on the Europarl Corpus (Koehn, 2005) and used Minimum Error Rate Training (MERT) (Och, 2003) for tuning on the development set. 2.1.4 Resources Given that a number of our features depends on stopwords (see section 2.2), we compiled two lists of stopwords, one for English and another one for Spanish. Both are freely available to download6. We also used two lists (English and Spanish) of candidates for Multiword</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an Extensible Language Modeling Toolkit. In 71h Int. Conf. on Spoken Language Processing, ICSLP’02, pages 901– 904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-Rich Part-ofSpeech Tagging with a Cyclic Dependency Network. In HLT-NAAC</title>
<date>2003</date>
<pages>252--259</pages>
<publisher>ACL.</publisher>
<location>Edmonton, Canada.</location>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-Rich Part-ofSpeech Tagging with a Cyclic Dependency Network. In HLT-NAAC 2003, pages 252–259, Edmonton, Canada. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Mining the Web for Synonyms: PMI-IR Versus LSA on TOEFL. In</title>
<date>2001</date>
<booktitle>121h European Conf. on Machine Learning, EMCL’01,</booktitle>
<pages>491--502</pages>
<publisher>UK. Springer.</publisher>
<location>London,</location>
<contexts>
<context position="9073" citStr="Turney, 2001" startWordPosition="1345" endWordPosition="1346">xtracting all the occurrences of the term in the conceptual network (i.e. BabelNet). As a result, we got a “conceptual representation” of both sentences, each of them containing a set of conceptual term lists. Next, for every term in the “conceptual sentence 1”, we counted the number of co-occurrences in the conceptual term lists in the “conceptual sentence 2”. In other words, we intersected the terms in sentence 1 with all the conceptual term lists in sentence 2. After computing all the co-occurrences, we used these values to calculate the Jaccard’ (Jaccard, 1901), Lin’ (Lin, 1998) and PMI’ (Turney, 2001) scores. 2.2.4 Semantic Similarity Measures This feature takes advantage of the Align, Disambiguate and Walk (ADW)9 library (Pilehvar et al., 2013), a WordNet-based approach for measuring semantic similarity of arbitrary pairs of lexical items. It is important to mention that this feature is the only one that only works for English, which explains why we have a translation model (see section 2.1.3). In other words, when we are dealing 8http://babelnet.org 9http://lcl.uniroma1.it/adw with Spanish text, we use the trained model to translate from Spanish to English. As the ADW library permits us </context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>Peter D. Turney. 2001. Mining the Web for Synonyms: PMI-IR Versus LSA on TOEFL. In 121h European Conf. on Machine Learning, EMCL’01, pages 491– 502, London, UK. Springer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>