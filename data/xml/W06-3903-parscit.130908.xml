<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.438789666666667">
How to change a person’s mind:
Understanding the difference between
the effects and consequences of speech acts
</title>
<author confidence="0.960769">
Debora Field and Allan Ramsay
</author>
<affiliation confidence="0.963455">
Computer Science, Univ. of Liverpool, L69 3BX, UK
Informatics, Univ. of Manchester, PO Box 88, M60 1QD, UK
</affiliation>
<note confidence="0.229972">
debora@ csc. liv. ac. uk,allan. ramsay@ manchester. ac. uk
</note>
<sectionHeader confidence="0.823273" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999516">
This paper discusses a planner of the semantics of utterances, whose essential
design is an epistemic theorem prover. The planner was designed for the purpose
of planning communicative actions, whose effects are famously unknowable and
unobservable by the doer/speaker, and depend on the beliefs of and inferences made
by the recipient/hearer. The fully implemented model can achieve goals that do
not match action effects, but that are rather entailed by them, which it does by
reasoning about how to act: state-space planning is interwoven with theorem proving
in such a way that a theorem prover uses the effects of actions as hypotheses. The
planner is able to model problematic conversational situations, including felicitous
and infelicitous instances of bluffing, lying, sarcasm, and stating the obvious.1
</bodyText>
<sectionHeader confidence="0.997336" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999879416666667">
The motivation for this research was the problem of planning the semantics
of communicative actions: given that I want you to believe P, how do I choose
what meaning to express to you? The well-documented, considerable difficul-
ties involved in this problem include this: a key player in the ensuing evolution
of the post-utterance environment is the hearer of the utterance.
First, consider an imaginary robot Rob, designed not for communication,
but for making tea. Whenever he is in use, Rob’s top-level goal is to attain a
state in which there is a certain configuration of cups, saucers, hot tea, cold
milk, etc. Rob’s plans for making tea are made on the strong assumption that
at plan execution time, the cups (and other items) will have no desires and
opinions of their own concerning which positions they should take up—Rob
expects to be the author of the effects of his actions. 2
</bodyText>
<footnote confidence="0.8525275">
1 Initially funded by the EPSRC. Recent partial funding under EU-grant FP6/IST No.
507019 (PIPS: Personalised Information Platform for Health and Life Services).
</footnote>
<page confidence="0.401357">
2 notwithstanding impeding concurrent events, sensor failures, motor failures, etc.
</page>
<bodyText confidence="0.9999605">
In contrast, consider the human John, designed for doing all sorts of
things besides making tea, including communicating messages to other hu-
mans. Imagine John’s current goal is to get human Sally to believe the propo-
sition John is kind. In some respects, John has a harder problem than Rob.
Unlike Rob, John has no direct access to the environment he wishes to affect—
he cannot simply implant John is kind into Sally’s belief state. John knows
that Sally has desires and opinions of her own, and that he will have to plan
something that he considers might well lead Sally to infer John is kind. This
means that when John is planning his action—whether to give her some choco-
late, pay her a compliment, tell her he is kind, lend her his credit card—he
has to consider the many different messages Sally might infer from the one
thing John chooses to say or do. Unfortunately, there is no STRIPS operator
[13] John can choose that will have his desired effect; he has to plan an action
that he expects will entail the state he desires.
We considered ‘reasoning-centred’ planning of actions that entailed goals
to be an approach that would enable this difficult predicament to be managed,
and implemented a model accordingly. Our planner is, in essence, an epistemic
theorem prover that hypothesises desirable actions, and is able to plan to
achieve goals that do not match action effects, but that are entailed by the
final state. Like John, the planner can have particular communicative goals in
mind, and knows that the execution of any single plan could have a myriad
different effects on H’s belief state, depending on what H chooses to infer.
</bodyText>
<subsectionHeader confidence="0.99457">
1.1 Bucking the trend
</subsectionHeader>
<bodyText confidence="0.999835266666667">
The main focus of current research in AI planning is on how to reduce the
search space required for making plans, and thus, for example, to get Rob the
tea-making robot to be able to make his plans fast enough to be of practical
use in the real world. Many planners use heuristics, either to constrain the
generation of a search space, or to prune and guide the search through the state
space for a solution, or both [4,5,18,25]. All such planners succeed by relying
on the static effects of actions—on the fact that you can tell by inspection
what the effects of an action will be in any situation—which limits their scope
in a particular way [4, p. 299]:
“... if one of the actions allows the planner to dig a hole of an arbitrary inte-
gral depth, then there are potentially infinitely many objects that can be cre-
ated... The effect of this action cannot be determined statically ... ”
The class of problems that these planners do not attempt to solve—the ability
to plan actions whose effects are not determined statically—was the class that
particularly interested us.
</bodyText>
<sectionHeader confidence="0.510039" genericHeader="method">
2 Planning the semantics of utterances
</sectionHeader>
<bodyText confidence="0.958267333333333">
With our attention firmly fixed on the myriad different effects a single commu-
nicative act can have on a hearer’s belief state, we concentrated on a (logically)
very simple utterance:
</bodyText>
<subsectionHeader confidence="0.532142">
“There’s a/an [some object]!”
</subsectionHeader>
<bodyText confidence="0.7953224">
We devised situations culminating in this utterance which illustrate sarcasm,
stating the obvious, bluffing, and lying, and developed a planner which
could use these tactics. Here is a much-shortened example of a scenario from
the model, which leads to the planning of an instance of sarcasm: 3 4
Initial state John has been bird-watching with Sally for hours, and so far,
they have only seen pigeons. John thinks Sally is feeling bored
and fed up. John has some chocolate in his bag. John thinks
Sally likes chocolate. John knows lots of rules about how con-
versation works, and what one can expect a hearer to infer
under given conditions.
</bodyText>
<subsectionHeader confidence="0.7271765">
Goal condition John wants to cheer Sally up.
Solutions John is just thinking about getting out some chocolate to give
</subsectionHeader>
<bodyText confidence="0.999739">
her, when yet another pigeon lands in a nearby tree. John sees
an opportunity to make Sally laugh by means of a bit of sar-
casm, and so plans to say to her,
</bodyText>
<subsectionHeader confidence="0.87346">
“There’s an albatross!”
</subsectionHeader>
<bodyText confidence="0.999949105263158">
John plans (the semantics of) his utterance, expecting that the utterance
will have particular ‘effects’ on Sally’s belief state; if John were to perform
the utterance, he would not be certain that it had achieved his intention, but
he would expect that it probably had. Whether John’s intention would be
achieved by this utterance depends on Sally having the ‘right’ set of beliefs
(the ones John thinks she has) and making the ‘right’ inferences (the ones
John expects her to make).
For example, if John’s utterance “There’s an albatross!” is to be felicitous,
the following must happen. Sally must first believe that John has said some-
thing that Sally thinks John and Sally mutually believe is false. From this, she
must infer that John has flouted a conversational maxim, and consequently
that John has attempted to implicate a meaning which is not expressed by the
semantics of “There’s an albatross!”. Sally must then infer that the implica-
ture John intends is of humour. Whether or not any of this happens depends
on Sally’s beliefs, which John cannot observe, but about which he has beliefs.
The formal version of this example contains all the necessary information
about the beliefs of John and Sally in this situation for the planner: (i) to be
able to plan John’s utterance; and (ii) to additionally deduce whether John’s
utterance would be felicitous or infelicitous, if he performed it.
</bodyText>
<footnote confidence="0.458823">
3 The example is an English paraphrase of a task, written in the model in Prolog code.
4 An albatross (Diomedea exulans) is a huge sea-faring bird, rarely seen from the land.
</footnote>
<subsectionHeader confidence="0.817364">
2.1 Linguistic motivations
</subsectionHeader>
<bodyText confidence="0.999972875">
Our approach to planning the semantics of utterances was to build on seminal
work in speech acts [3,28] and pragmatics [29,15,21]. In contrast to the ‘speech
acts with STRIPS’ approach [6,11,1,2], which is fraught with well-documented
difficulties [9,16,26,7,27], we aimed to develop a small set of linguistic acts that
were unambiguously identifiable purely by surface linguistic form (after [7]),
including ‘declare’, ‘request’, and perhaps others—a set of acts with negligible
effects (after [27]), and minimal preconditions. We in fact developed a single
linguistic act for all contexts.
</bodyText>
<subsubsectionHeader confidence="0.539065">
2.2 Planner design
</subsubsectionHeader>
<bodyText confidence="0.999413333333333">
The planner is essentially an epistemic theorem prover which employs some
planning search. The development process we undertook is helpful in under-
standing the planner’s design:
</bodyText>
<listItem confidence="0.990025705882353">
• A state-space search was implemented that searches backwards in hypothetical
time from the goal via STRIPS operators (based on foundational work in classical
planning [23,24,14,22,13]);
• A theorem prover for FOL was implemented that constructively proves conjunc-
tions, disjunctions, implications, and negations, and employs modus ponens and
unit resolution;
• State-space search and theorem proving were interwoven in such a way that:
· not only can disjunctions, implications and negations be proved true, they can
also be achieved;
· not only can a goal Q be proved true by proving (P ⇒ Q) n P, but Q can
also be achieved by proving P ⇒ Q and achieving P;
· a goal can be achieved by reasoning with recursive domain-specific rules—thus
the planner is able to plan to ‘dig holes of arbitrary depths’.
• The theorem prover was transformed into an epistemic theorem prover by incor-
porating a theory of knowledge and belief suitable for human reasoning about
action, so agents make plans according to their beliefs about the world, including
their beliefs about others’ beliefs.
</listItem>
<bodyText confidence="0.999977290322581">
A goal is proved by assuming the effect of some action is true, on the
grounds that the goal would be true in the situation that resulted from per-
forming that action. Hence, a set of actions is computed that might be useful
for achieving a goal by carrying out hypothetical proofs, where the hypotheses
are the actions whose effects have been exploited.
Here is a simple, non-dialogue example to aid explanation. Consider the
achievement of the goal above(e,f) and on(e,d), where above is the transitive
closure of on. First, it is not possible to judge whether the first goal above(e,f)
is true by inspecting the current state (which contains on( , ) facts but no
above(, ) facts), so reasoning is carried out to find out whether it is false.
Secondly, in order to achieve above(e,f), something different from an action
with an above(, ) expression in its add list is needed. Placing e onto f, for
example, will make above(e,f) proveable, but it will also make the achievement
of on(e,d) impossible. By reasoning with rules that describe the meaning of
above as the transitive closure of on, the planner hypothesises that on(d,f)
might enable the proof of above(e,f) to be completed, and also knows that
on(d,f) is an effect of action stack(d,f). A proof of the preconditions of action
stack(d,f) is carried out, and the process continues (with backtracking), until
a solution is found.
The preference for a backwards planning search was motivated by a defin-
ing quality of the communication problem, as epitomised by utterance plan-
ning: there are too many applicable actions to make a forwards search feasible.
People generally have the physical and mental capabilities to say whatever
they want at any moment. This means that the answer to the question ‘What
can I say in the current stater is something like ‘Anything, I just have to
decide what I want to say’. A backwards search is far more suitable than a
forwards search under conditions like these.
With this ‘reasoning-centred’ design, the planner is able to plan an utter-
ance to achieve a goal, ‘knowing’ that the utterance may or may not achieve
the desired effects on H, and that the same utterance can have many different
effects, depending on H’s belief state.
</bodyText>
<sectionHeader confidence="0.906424" genericHeader="method">
3 Modelling problematic conversations
</sectionHeader>
<bodyText confidence="0.998155">
In the model, utterances are planned according to Grice’s Cooperative Prin-
ciple [15]. Here is an extract from the CP (ibid p. 308):
</bodyText>
<figure confidence="0.381013857142857">
“[Quantity]
(i) Make your contribution as informative as is required (for the current purposes
of the exchange).
(ii) Do not make your contribution more informative than is required...
[Quality]
(i) Do not say what you believe to be false.
(ii) Do not say that for which you lack adequate evidence.”
</figure>
<bodyText confidence="0.999087666666666">
Grice’s maxims prescribe a standard for speaker behaviour which S can bla-
tantly contravene (‘flout’), thus signalling to H that there is an implicature
to be recovered. For instance, in our ‘sarcasm’ scenario, John’s utterance is
planned using the following maxim, derived from Grice’s first Quality maxim. 5
The first line means, ‘If S addresses H by putting Q into the conversational
minutes’:
</bodyText>
<footnote confidence="0.808216333333333">
(1) minute([S], [H], Q)
and believes(S, believes(H, mutuallybelieve(([H, S]), not(Q))))
==&gt; believes(S, believes(H, griceuncoop(S, [H], Q)))
</footnote>
<bodyText confidence="0.873131444444444">
5 The model embodies a ‘deduction’ model of belief [19], rather than a ‘possible worlds’
model [17,20]. Thus agents are not required to draw all logically possible inferences, and
are therefore not required to infer an infinite number of propositions from a mutual belief.
Using this maxim, John reasons that he can get Sally to realise he is flouting
a maxim in order to generate an implicature (that he is being ‘Grice uncoop-
erative with respect to Q’). But what is the nature of the implicature? This
is dealt with by two additional rules: (2), which describes what John thinks
Sally believes about the meaning of this kind of maxim-flouting; and (3), a
‘general knowledge’ rule:
</bodyText>
<equation confidence="0.889383333333333">
(2) believes(john,
believes(sally,
(griceuncoop(PERSON2, _PERSON1, Q)
and mutuallybelieve(([sally,john]), not(Q)))
==&gt; funny(PERSON2, re(Q))))
(3) believes(john,
believes(sally,
(funny(PERSON2, re(Q))
==&gt; happy(sally))))
</equation>
<bodyText confidence="0.9993115">
With these three rules, John can reason that saying something he thinks
he and Sally mutually disbelieve will make her laugh, and thus cheer her up,
thus achieving his goal. Here is a second maxim from the model, also derived
from Grice’s CP:
</bodyText>
<listItem confidence="0.63028">
(4) minute([S], [H], Q)
</listItem>
<bodyText confidence="0.9422865">
and believes(S, believes(H, mutuallybelieve(([H, S]), Q)))
==&gt; believes(S, believes(H, griceuncoop(S, [H], Q)))
Using this maxim, and some additional rules, John can plan to flout Quan-
tity maxim 2, and generate an implicature by ‘stating the obvious’.
</bodyText>
<subsectionHeader confidence="0.98975">
3.1 Modelling deception
</subsectionHeader>
<bodyText confidence="0.9999719">
Grice’s CP seems an excellent formalism for planning and understanding ut-
terances, so long as everyone is committed to obeying it. We know, however,
that people violate the CP maxims—S contravenes maxims without wanting
H to know. For example, lying violates Quality maxim (1) , bluffing violates
Quality maxim (2) , and being economical with the truth violates Quantity
maxim (1) . However, there is nothing in Grice’s maxims to help H deal with
the possibility that S may be trying to deceive her. Our solution is to give S
and H some further maxims which legislate for the fact that speakers do not
necessarily always adhere to the CP, and which enable S to plan to deceive,
and H to detect intended deceptions.
</bodyText>
<subsubsectionHeader confidence="0.993863">
3.1.1 Hearer violation maxims
</subsubsectionHeader>
<bodyText confidence="0.918504153846154">
Given that H admits the possibility that S might be trying to deceive her
with his utterance, we consider that there are three strong predictors of how
H’s belief state will change in response to S’s utterance of the proposition P:
(5) i What is H ’s view of the proposition P?
ii What is H ’s view concerning the goodwill of S?
iii What is H ’s view of the reliability of S’s testimony?
Consider, for example, an attempt at bluffing:6
Initial state John has gone bird-watching with Sally. John is wearing a warm
coat, and he thinks that Sally looks cold. John thinks Sally will be
impressed by a chivalrous gesture. John thinks Sally is new to bird-
watching, and that she is keen to learn about birds. John knows lots
of rules about how conversation works, and what one can expect a
hearer to infer under given conditions.
</bodyText>
<subsectionHeader confidence="0.5638145">
Goal condition John wants Sally to be impressed by him.
Solutions John is just thinking of offering Sally his coat to wear, when a huge
</subsectionHeader>
<bodyText confidence="0.793932">
bird lands in a nearby tree. John isn’t quite sure what species the
bird is, nevertheless, he decides to try and impress Sally with his
bird expertise, and plans to say to her,
</bodyText>
<subsectionHeader confidence="0.822619">
“There’s a dodo!”
</subsectionHeader>
<bodyText confidence="0.865130142857143">
Let us imagine that Sally’s answers to three above questions are as follows.
Before John performed his utterance:
(6) i Sally believed that the proposition P (“There’s a dodo!”) was false (because
she knew the bird was a buzzard).
Additionally, she did not believe that John thought that they mutually be-
lieved P was false.
ii She believed that John was well-disposed towards her.
iii She didn’t know whether John was a reliable source of information or not.
After John has said “There’s a dodo!”, Sally derives the following new set of
beliefs from the above set:
(7) i&apos; Sally still believes that the proposition P (“There’s a dodo!”) is false.
She now believes that John thinks that they mutually believe P is true.
ii&apos; She still believes that John is well-disposed towards her.
iii&apos; She now believes John is an unreliable source of information.
The mapping of belief set (6) into belief set (7) is determined in the model
by a ‘hearer violation (HV) maxim’. We call this maxim the ‘infelicitous bluff’
HV maxim. We have so far implemented eight HV maxims, however, there is
clearly scope for many more permutations of all the different possible answers
to (6). There are obvious additional refinements that should be made, for
example, people do not normally consider others to be reliable sources of
information on all subjects.
</bodyText>
<subsubsectionHeader confidence="0.960705">
3.1.2 Speaker violation maxims
</subsubsectionHeader>
<bodyText confidence="0.945445333333333">
If S is to succeed in his attempt to deceive H, he will have to take into account
how H is going to try and detect his deception. To represent this in the model,
S has his own ‘speaker violation (SV) maxims’, which concern the same issues
as the HV maxims, but from the other side of the table, as it were. What S
plans to say will depend on which answer he selects from each of these four
categories:
</bodyText>
<page confidence="0.685381">
6 A dodo is a large flightless bird that is famously extinct.
</page>
<equation confidence="0.6898865">
(8) i What is S’s view of H ’s view of various different propositions?
ii What is S’s own view of the same propositions?
iii What is S’s view of H ’s view of the goodwill of S?
iv What is S’s view of H ’s view of the reliability of S as a source?
</equation>
<bodyText confidence="0.6714259">
Here is an example of an SV maxim from the model:
(9) minute([S], [H], Q)
and believes(S, believes(H, reliable(S)))
and believes(S, believes(H, well_disposed_towards(S, [H])))
and believes(S, believes(H, Q or not(Q)))
==&gt; believes(S, believes(H, gricecoop(S, [H], Q)))
Using this maxim, John can reason that Sally will believe he is being Grice-
cooperative, which means Sally will believe that what he is saying is true, even
if John does not believe it himself. Thus John is able to plan to lie to Sally by
using tactics he hopes will prevent Sally from detecting his attempt to deceive.
</bodyText>
<sectionHeader confidence="0.876616" genericHeader="method">
4 Epistemic theorem prover
</sectionHeader>
<bodyText confidence="0.999941166666667">
The planner’s theorem prover embodies a constructive/intuitionist logic and
it proves theorems by natural deduction, chosen in preference to classical logic
and its inferencing methods. The way humans do every-day inferencing is, we
consider, quite different from the way inferencing is handled under classical
logic. In classical logic, for example, and using our general knowledge, we judge
the following formulae to be true:
</bodyText>
<listItem confidence="0.979079">
(10) Earth has one moon ⇒ Elvis is dead
(11) Earth has two moons ⇒ Elvis is alive
(12) Earth has two moons ⇒ Elvis is dead
</listItem>
<bodyText confidence="0.9501765">
(10) is true simply because antecedent and consequent are both true formulae.
We find this truth odd, however, because of the absence of any discernible
relationship between antecedent and consequent. (11) and (12) are true sim-
ply because the antecedent is false, which seems very counter-intuitive. Even
more peculiarly, the following formula is provable in classical logic in all cir-
cumstances:
(13) (Earth has one moon ⇒ Elvis is dead) or
(Elvis is dead ⇒ Earth has one moon)
but it feels very uncomfortable to say that it must be the case that one of
these implies the other.
In order to avoid having to admit proofs like this, and to be able to do
reasoning in a more human-like way, we opted for constructive logic and natu-
ral deduction. In order to prove P ⇒ Q by natural deduction, one must show
that Q is true when P is true; if P is not true, constructive logic does not
infer P ⇒ Q. This treatment of implication hints at a relationship between P
and Q which is absent from material implication.
</bodyText>
<subsectionHeader confidence="0.969304">
4.1 Constructive logic and belief
</subsectionHeader>
<bodyText confidence="0.999967942857143">
Taking a constructive view allows us to simplify our reasoning about when the
hearer believes something of the form P =* Q, and hence (because of the con-
structive interpretation of -,P as P =* 1) about whether she believes -,P. We
will assume that believes(H, P) means that H could infer P on the basis of
her belief set, not that she already does believe P, and we will examine the rela-
tionship between believes(H, P =* Q) and believes(H, P) =* believes(H, Q).
Consider first believes(H, P) =* believes(H, Q). Under what circumstances
could you convince yourself that this held?
For a constructive proof, you would have to assume that believes(H, P)
held, and try to prove believes(H, Q). So you would say to yourself ‘Suppose
I were H, and I believed P. Would I believe Q?’ The obvious way to answer
this would be to try to prove Q, using what you believe to be H’s rules of
inference. If you could do this, you could assume that H could construct a proof
of P =* Q, and hence it would be reasonable to conclude believes(H, P =* Q).
Suppose, on the other hand, that you believed believes(H, P =* Q), and
that you also believed believes(H, P). This would mean that you thought that
H had both P =* Q and P available to her. But if you had these two available
to you, you would be able to infer Q, so since H is very similar to you she
should also be able to infer Q. So from believes(H, P =* Q) and believes(H, P)
we can infer believes(H, Q), or in other words (believes(H, P =* Q)) =*
(believes(H, P) =* believes(H, Q)).
We thus see that if we take believes(H, P) to mean ‘If I were H I would
be able to prove P’, then (believes(H, P =* Q)) and (believes(H, P) =*
believes(H, Q)) are equivalent. This has considerable advantages in terms of
theorem proving, since it means that much of the time we can do our reasoning
by switching to the believer’s point of view and doing perfectly ordinary first-
order reasoning. If, in addition, we treat -,P as a shorthand for P =* 1, we
see that believes(H, -,P) is equivalent to believes(H, P) =* believes(H, 1). If
we take the further step of assuming that nobody believes 1, we can see
that believes(H, -,P) =* -,believes(H, P) (though not -,believes(H, P) =*
believes(H, -,P)). We cannot, however, always assume that everyone’s beliefs
are consistent, so we may not always want to take this further step (note
that in possible worlds treatments, we are forced to assume that everyone’s
beliefs are consistent), but it is useful to be able to use it as a default rule,
particularly once we understand the assumptions that lie behind it.
</bodyText>
<sectionHeader confidence="0.998467" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999938137254902">
[1] Allen, J. F. and C. R. Perrault, Analyzing intention in utterances (1980), AI
15: 143–78.
[2] Appelt, D. E., Planning English referring expressions (1985), AI 26: 1–33.
[3] Austin, J. L., How to do things with words (1962), Oxford: OUP, 2nd edition.
[4] Blum, A. L. and M. L. Furst, Fast planning through planning graph analysis
(1995), in Proc. 14th IJCAI, pp. 1636–1642.
[5] Bonet, B. and H. Geffner, Heuristic Search Planner (2000), AI Magazine 21(2).
[6] Bruce, B. C., Generation as a social action (1975), in B. L. Nash-Webber and
R. C. Schank (eds), Theoretical issues in natural language processing, pp. 74–7.
Cambridge, Massachusetts: ACL.
[7] Bunt, H., Dialogue pragmatics and context specification (2000), [8], pp. 81–150.
[8] Bunt, H. and W. Black, (eds), Abduction, belief and context in dialogue: studies
in computational pragmatics (2000), Philadelphia: John Benjamins.
[9] Cohen, P. R. and H. J. Levesque, Rational interaction as the basis for
communication (1990), [10], pp. 221–55.
[10] Cohen, P. R., J. Morgan and M. E. Pollack, (eds), Intentions in communication
(1990), Cambridge, Massachusetts: MIT.
[11] Cohen, P. R. and C. R. Perrault, Elements of a plan-based theory of speech
acts (1979), Cognitive Science 3: 177–212.
[12] Feigenbaum, E. A. and J. Feldman, Editors, Computers and thought (1995),
Cambridge, Massachusetts: MIT Press. First published 1963 by McGraw-Hill.
[13] Fikes, R. E. and N. J. Nilsson, STRIPS: A new approach to the application of
theorem proving to problem solving (1971), AI 2: 189–208.
[14] Green, C., Application of theorem proving to problem solving (1969), in Proc.
1st IJCAI, pp. 219–39.
[15] Grice, H. P., Logic and conversation (1975), in P. Cole and J. Morgan, (eds),
Syntax and semantics 3: Speech acts, pp. 41–58. New York: Academic Press.
[16] Grosz, B. J. and C. L. Sidner, Plans for discourse (1990), [10], pp. 416–44.
[17] Hintikka, J., Knowledge and belief: An introduction to the two notions (1962),
New York: Cornell University Press.
[18] Hoffmann, J. and B. Nebel, The FF planning system: Fast plan generation
through heuristic search (2001), Journal of AI Research 14: 253–302.
[19] Konolige, K., A deduction model of belief (1986), London: Pitman.
[20] Kripke, S., Semantical considerations on modal logic (1963), in Acta
Philosophica Fennica 16: 83–94.
[21] Lewis, D., Scorekeeping in a language game (1979), J. Phil. Logic 8: 339–59.
[22] McCarthy, J. and P. J. Hayes, Some philosophical problems from the standpoint
of artificial intelligence (1969), Machine Intelligence 4: 463–502.
[23] Newell, A., J. C. Shaw and H. A. Simon, Empirical explorations with the logic
theory machine (1957), Proc. Western Joint Computer Conference, 15: 218–239.
[24] Newell, A. and H. A. Simon, GPS, a program that simulates human thought
(1963), [12], pp. 279–93.
[25] Nguyen, X. and S. Kambhampati, Reviving partial order planning (2001), in
Proc. IJCAI, pp. 459–66.
[26] Pollack, M. E., Plans as complex mental attitudes (1990), [10], pp. 77–103.
[27] Ramsay, A., Speech act theory and epistemic planning (2000), [8], pp. 293–310.
[28] Searle, J. R., What is a speech act? (1965), in M. Black, (ed), Philosophy in
America, pp. 221–39. Allen and Unwin.
[29] Stalnaker, R., Pragmatics (1972), in D. Davidson and G. Harman, (eds),
Semantics of natural language (Synthese Library, Vol. 40), pp. 380–97.
Dordrecht, Holland: D. Reidel.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.409123">
<title confidence="0.843853">How to change a person’s mind: Understanding the difference the effects and consequences of speech acts</title>
<author confidence="0.996459">Debora Field</author>
<author confidence="0.996459">Allan Ramsay</author>
<affiliation confidence="0.945128">Computer Science, Univ. of Liverpool, L69 3BX,</affiliation>
<address confidence="0.700917">Informatics, Univ. of Manchester, PO Box 88, M60 1QD,</address>
<email confidence="0.715553">debora@csc.liv.ac.uk,allan.ramsay@manchester.ac.uk</email>
<abstract confidence="0.992109636363636">This paper discusses a planner of the semantics of utterances, whose essential design is an epistemic theorem prover. The planner was designed for the purpose of planning communicative actions, whose effects are famously unknowable and unobservable by the doer/speaker, and depend on the beliefs of and inferences made by the recipient/hearer. The fully implemented model can achieve goals that do not match action effects, but that are rather entailed by them, which it does by reasoning about how to act: state-space planning is interwoven with theorem proving in such a way that a theorem prover uses the effects of actions as hypotheses. The planner is able to model problematic conversational situations, including felicitous infelicitous instances of bluffing, lying, sarcasm, and stating the</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J F Allen</author>
<author>C R Perrault</author>
</authors>
<title>Analyzing intention in utterances</title>
<date>1980</date>
<journal>AI</journal>
<volume>15</volume>
<pages>143--78</pages>
<contexts>
<context position="8011" citStr="[6,11,1,2]" startWordPosition="1343" endWordPosition="1343">iefs of John and Sally in this situation for the planner: (i) to be able to plan John’s utterance; and (ii) to additionally deduce whether John’s utterance would be felicitous or infelicitous, if he performed it. 3 The example is an English paraphrase of a task, written in the model in Prolog code. 4 An albatross (Diomedea exulans) is a huge sea-faring bird, rarely seen from the land. 2.1 Linguistic motivations Our approach to planning the semantics of utterances was to build on seminal work in speech acts [3,28] and pragmatics [29,15,21]. In contrast to the ‘speech acts with STRIPS’ approach [6,11,1,2], which is fraught with well-documented difficulties [9,16,26,7,27], we aimed to develop a small set of linguistic acts that were unambiguously identifiable purely by surface linguistic form (after [7]), including ‘declare’, ‘request’, and perhaps others—a set of acts with negligible effects (after [27]), and minimal preconditions. We in fact developed a single linguistic act for all contexts. 2.2 Planner design The planner is essentially an epistemic theorem prover which employs some planning search. The development process we undertook is helpful in understanding the planner’s design: • A st</context>
</contexts>
<marker>[1]</marker>
<rawString>Allen, J. F. and C. R. Perrault, Analyzing intention in utterances (1980), AI 15: 143–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Appelt</author>
</authors>
<title>Planning English referring expressions</title>
<date>1985</date>
<journal>AI</journal>
<volume>26</volume>
<pages>1--33</pages>
<contexts>
<context position="8011" citStr="[6,11,1,2]" startWordPosition="1343" endWordPosition="1343">iefs of John and Sally in this situation for the planner: (i) to be able to plan John’s utterance; and (ii) to additionally deduce whether John’s utterance would be felicitous or infelicitous, if he performed it. 3 The example is an English paraphrase of a task, written in the model in Prolog code. 4 An albatross (Diomedea exulans) is a huge sea-faring bird, rarely seen from the land. 2.1 Linguistic motivations Our approach to planning the semantics of utterances was to build on seminal work in speech acts [3,28] and pragmatics [29,15,21]. In contrast to the ‘speech acts with STRIPS’ approach [6,11,1,2], which is fraught with well-documented difficulties [9,16,26,7,27], we aimed to develop a small set of linguistic acts that were unambiguously identifiable purely by surface linguistic form (after [7]), including ‘declare’, ‘request’, and perhaps others—a set of acts with negligible effects (after [27]), and minimal preconditions. We in fact developed a single linguistic act for all contexts. 2.2 Planner design The planner is essentially an epistemic theorem prover which employs some planning search. The development process we undertook is helpful in understanding the planner’s design: • A st</context>
</contexts>
<marker>[2]</marker>
<rawString>Appelt, D. E., Planning English referring expressions (1985), AI 26: 1–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Austin</author>
</authors>
<title>How to do things with words</title>
<date>1962</date>
<location>Oxford:</location>
<note>OUP, 2nd edition.</note>
<contexts>
<context position="7919" citStr="[3,28]" startWordPosition="1330" endWordPosition="1330"> The formal version of this example contains all the necessary information about the beliefs of John and Sally in this situation for the planner: (i) to be able to plan John’s utterance; and (ii) to additionally deduce whether John’s utterance would be felicitous or infelicitous, if he performed it. 3 The example is an English paraphrase of a task, written in the model in Prolog code. 4 An albatross (Diomedea exulans) is a huge sea-faring bird, rarely seen from the land. 2.1 Linguistic motivations Our approach to planning the semantics of utterances was to build on seminal work in speech acts [3,28] and pragmatics [29,15,21]. In contrast to the ‘speech acts with STRIPS’ approach [6,11,1,2], which is fraught with well-documented difficulties [9,16,26,7,27], we aimed to develop a small set of linguistic acts that were unambiguously identifiable purely by surface linguistic form (after [7]), including ‘declare’, ‘request’, and perhaps others—a set of acts with negligible effects (after [27]), and minimal preconditions. We in fact developed a single linguistic act for all contexts. 2.2 Planner design The planner is essentially an epistemic theorem prover which employs some planning search. T</context>
</contexts>
<marker>[3]</marker>
<rawString>Austin, J. L., How to do things with words (1962), Oxford: OUP, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Blum</author>
<author>M L Furst</author>
</authors>
<title>Fast planning through planning graph analysis</title>
<date>1995</date>
<booktitle>in Proc. 14th IJCAI,</booktitle>
<pages>1636--1642</pages>
<contexts>
<context position="4378" citStr="[4,5,18,25]" startWordPosition="728" endWordPosition="728"> goals in mind, and knows that the execution of any single plan could have a myriad different effects on H’s belief state, depending on what H chooses to infer. 1.1 Bucking the trend The main focus of current research in AI planning is on how to reduce the search space required for making plans, and thus, for example, to get Rob the tea-making robot to be able to make his plans fast enough to be of practical use in the real world. Many planners use heuristics, either to constrain the generation of a search space, or to prune and guide the search through the state space for a solution, or both [4,5,18,25]. All such planners succeed by relying on the static effects of actions—on the fact that you can tell by inspection what the effects of an action will be in any situation—which limits their scope in a particular way [4, p. 299]: “... if one of the actions allows the planner to dig a hole of an arbitrary integral depth, then there are potentially infinitely many objects that can be created... The effect of this action cannot be determined statically ... ” The class of problems that these planners do not attempt to solve—the ability to plan actions whose effects are not determined statically—was</context>
</contexts>
<marker>[4]</marker>
<rawString>Blum, A. L. and M. L. Furst, Fast planning through planning graph analysis (1995), in Proc. 14th IJCAI, pp. 1636–1642.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Bonet</author>
<author>H Geffner</author>
</authors>
<title>Heuristic Search Planner</title>
<date>2000</date>
<journal>AI Magazine</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="4378" citStr="[4,5,18,25]" startWordPosition="728" endWordPosition="728"> goals in mind, and knows that the execution of any single plan could have a myriad different effects on H’s belief state, depending on what H chooses to infer. 1.1 Bucking the trend The main focus of current research in AI planning is on how to reduce the search space required for making plans, and thus, for example, to get Rob the tea-making robot to be able to make his plans fast enough to be of practical use in the real world. Many planners use heuristics, either to constrain the generation of a search space, or to prune and guide the search through the state space for a solution, or both [4,5,18,25]. All such planners succeed by relying on the static effects of actions—on the fact that you can tell by inspection what the effects of an action will be in any situation—which limits their scope in a particular way [4, p. 299]: “... if one of the actions allows the planner to dig a hole of an arbitrary integral depth, then there are potentially infinitely many objects that can be created... The effect of this action cannot be determined statically ... ” The class of problems that these planners do not attempt to solve—the ability to plan actions whose effects are not determined statically—was</context>
</contexts>
<marker>[5]</marker>
<rawString>Bonet, B. and H. Geffner, Heuristic Search Planner (2000), AI Magazine 21(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B C Bruce</author>
</authors>
<title>Generation as a social action</title>
<date>1975</date>
<booktitle>Schank (eds), Theoretical issues in natural language processing,</booktitle>
<pages>74--7</pages>
<publisher>ACL.</publisher>
<location>Cambridge, Massachusetts:</location>
<note>in</note>
<contexts>
<context position="8011" citStr="[6,11,1,2]" startWordPosition="1343" endWordPosition="1343">iefs of John and Sally in this situation for the planner: (i) to be able to plan John’s utterance; and (ii) to additionally deduce whether John’s utterance would be felicitous or infelicitous, if he performed it. 3 The example is an English paraphrase of a task, written in the model in Prolog code. 4 An albatross (Diomedea exulans) is a huge sea-faring bird, rarely seen from the land. 2.1 Linguistic motivations Our approach to planning the semantics of utterances was to build on seminal work in speech acts [3,28] and pragmatics [29,15,21]. In contrast to the ‘speech acts with STRIPS’ approach [6,11,1,2], which is fraught with well-documented difficulties [9,16,26,7,27], we aimed to develop a small set of linguistic acts that were unambiguously identifiable purely by surface linguistic form (after [7]), including ‘declare’, ‘request’, and perhaps others—a set of acts with negligible effects (after [27]), and minimal preconditions. We in fact developed a single linguistic act for all contexts. 2.2 Planner design The planner is essentially an epistemic theorem prover which employs some planning search. The development process we undertook is helpful in understanding the planner’s design: • A st</context>
</contexts>
<marker>[6]</marker>
<rawString>Bruce, B. C., Generation as a social action (1975), in B. L. Nash-Webber and R. C. Schank (eds), Theoretical issues in natural language processing, pp. 74–7. Cambridge, Massachusetts: ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Bunt</author>
</authors>
<title>Dialogue pragmatics and context specification</title>
<date>2000</date>
<volume>8</volume>
<pages>81--150</pages>
<contexts>
<context position="8078" citStr="[9,16,26,7,27]" startWordPosition="1350" endWordPosition="1350">o be able to plan John’s utterance; and (ii) to additionally deduce whether John’s utterance would be felicitous or infelicitous, if he performed it. 3 The example is an English paraphrase of a task, written in the model in Prolog code. 4 An albatross (Diomedea exulans) is a huge sea-faring bird, rarely seen from the land. 2.1 Linguistic motivations Our approach to planning the semantics of utterances was to build on seminal work in speech acts [3,28] and pragmatics [29,15,21]. In contrast to the ‘speech acts with STRIPS’ approach [6,11,1,2], which is fraught with well-documented difficulties [9,16,26,7,27], we aimed to develop a small set of linguistic acts that were unambiguously identifiable purely by surface linguistic form (after [7]), including ‘declare’, ‘request’, and perhaps others—a set of acts with negligible effects (after [27]), and minimal preconditions. We in fact developed a single linguistic act for all contexts. 2.2 Planner design The planner is essentially an epistemic theorem prover which employs some planning search. The development process we undertook is helpful in understanding the planner’s design: • A state-space search was implemented that searches backwards in hypothe</context>
</contexts>
<marker>[7]</marker>
<rawString>Bunt, H., Dialogue pragmatics and context specification (2000), [8], pp. 81–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Bunt</author>
<author>W Black</author>
</authors>
<title>Abduction, belief and context in dialogue: studies in computational pragmatics</title>
<date>2000</date>
<location>Philadelphia: John Benjamins.</location>
<marker>[8]</marker>
<rawString>Bunt, H. and W. Black, (eds), Abduction, belief and context in dialogue: studies in computational pragmatics (2000), Philadelphia: John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P R Cohen</author>
<author>H J Levesque</author>
</authors>
<title>Rational interaction as the basis for communication</title>
<date>1990</date>
<volume>10</volume>
<pages>221--55</pages>
<contexts>
<context position="8078" citStr="[9,16,26,7,27]" startWordPosition="1350" endWordPosition="1350">o be able to plan John’s utterance; and (ii) to additionally deduce whether John’s utterance would be felicitous or infelicitous, if he performed it. 3 The example is an English paraphrase of a task, written in the model in Prolog code. 4 An albatross (Diomedea exulans) is a huge sea-faring bird, rarely seen from the land. 2.1 Linguistic motivations Our approach to planning the semantics of utterances was to build on seminal work in speech acts [3,28] and pragmatics [29,15,21]. In contrast to the ‘speech acts with STRIPS’ approach [6,11,1,2], which is fraught with well-documented difficulties [9,16,26,7,27], we aimed to develop a small set of linguistic acts that were unambiguously identifiable purely by surface linguistic form (after [7]), including ‘declare’, ‘request’, and perhaps others—a set of acts with negligible effects (after [27]), and minimal preconditions. We in fact developed a single linguistic act for all contexts. 2.2 Planner design The planner is essentially an epistemic theorem prover which employs some planning search. The development process we undertook is helpful in understanding the planner’s design: • A state-space search was implemented that searches backwards in hypothe</context>
</contexts>
<marker>[9]</marker>
<rawString>Cohen, P. R. and H. J. Levesque, Rational interaction as the basis for communication (1990), [10], pp. 221–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P R Cohen</author>
<author>J Morgan</author>
<author>M E Pollack</author>
</authors>
<date>1990</date>
<booktitle>Intentions in communication</booktitle>
<publisher>MIT.</publisher>
<location>Cambridge, Massachusetts:</location>
<marker>[10]</marker>
<rawString>Cohen, P. R., J. Morgan and M. E. Pollack, (eds), Intentions in communication (1990), Cambridge, Massachusetts: MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P R Cohen</author>
<author>C R Perrault</author>
</authors>
<title>Elements of a plan-based theory of speech acts</title>
<date>1979</date>
<journal>Cognitive Science</journal>
<volume>3</volume>
<pages>177--212</pages>
<contexts>
<context position="8011" citStr="[6,11,1,2]" startWordPosition="1343" endWordPosition="1343">iefs of John and Sally in this situation for the planner: (i) to be able to plan John’s utterance; and (ii) to additionally deduce whether John’s utterance would be felicitous or infelicitous, if he performed it. 3 The example is an English paraphrase of a task, written in the model in Prolog code. 4 An albatross (Diomedea exulans) is a huge sea-faring bird, rarely seen from the land. 2.1 Linguistic motivations Our approach to planning the semantics of utterances was to build on seminal work in speech acts [3,28] and pragmatics [29,15,21]. In contrast to the ‘speech acts with STRIPS’ approach [6,11,1,2], which is fraught with well-documented difficulties [9,16,26,7,27], we aimed to develop a small set of linguistic acts that were unambiguously identifiable purely by surface linguistic form (after [7]), including ‘declare’, ‘request’, and perhaps others—a set of acts with negligible effects (after [27]), and minimal preconditions. We in fact developed a single linguistic act for all contexts. 2.2 Planner design The planner is essentially an epistemic theorem prover which employs some planning search. The development process we undertook is helpful in understanding the planner’s design: • A st</context>
</contexts>
<marker>[11]</marker>
<rawString>Cohen, P. R. and C. R. Perrault, Elements of a plan-based theory of speech acts (1979), Cognitive Science 3: 177–212.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E A Feigenbaum</author>
<author>J Feldman</author>
</authors>
<title>Editors, Computers and thought</title>
<date>1995</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, Massachusetts:</location>
<note>First published 1963 by McGraw-Hill.</note>
<marker>[12]</marker>
<rawString>Feigenbaum, E. A. and J. Feldman, Editors, Computers and thought (1995), Cambridge, Massachusetts: MIT Press. First published 1963 by McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Fikes</author>
<author>N J Nilsson</author>
</authors>
<title>STRIPS: A new approach to the application of theorem proving to problem solving</title>
<date>1971</date>
<journal>AI</journal>
<volume>2</volume>
<pages>189--208</pages>
<contexts>
<context position="3189" citStr="[13]" startWordPosition="522" endWordPosition="522">John has no direct access to the environment he wishes to affect— he cannot simply implant John is kind into Sally’s belief state. John knows that Sally has desires and opinions of her own, and that he will have to plan something that he considers might well lead Sally to infer John is kind. This means that when John is planning his action—whether to give her some chocolate, pay her a compliment, tell her he is kind, lend her his credit card—he has to consider the many different messages Sally might infer from the one thing John chooses to say or do. Unfortunately, there is no STRIPS operator [13] John can choose that will have his desired effect; he has to plan an action that he expects will entail the state he desires. We considered ‘reasoning-centred’ planning of actions that entailed goals to be an approach that would enable this difficult predicament to be managed, and implemented a model accordingly. Our planner is, in essence, an epistemic theorem prover that hypothesises desirable actions, and is able to plan to achieve goals that do not match action effects, but that are entailed by the final state. Like John, the planner can have particular communicative goals in mind, and kn</context>
<context position="8790" citStr="[23,24,14,22,13]" startWordPosition="1454" endWordPosition="1454"> by surface linguistic form (after [7]), including ‘declare’, ‘request’, and perhaps others—a set of acts with negligible effects (after [27]), and minimal preconditions. We in fact developed a single linguistic act for all contexts. 2.2 Planner design The planner is essentially an epistemic theorem prover which employs some planning search. The development process we undertook is helpful in understanding the planner’s design: • A state-space search was implemented that searches backwards in hypothetical time from the goal via STRIPS operators (based on foundational work in classical planning [23,24,14,22,13]); • A theorem prover for FOL was implemented that constructively proves conjunctions, disjunctions, implications, and negations, and employs modus ponens and unit resolution; • State-space search and theorem proving were interwoven in such a way that: · not only can disjunctions, implications and negations be proved true, they can also be achieved; · not only can a goal Q be proved true by proving (P ⇒ Q) n P, but Q can also be achieved by proving P ⇒ Q and achieving P; · a goal can be achieved by reasoning with recursive domain-specific rules—thus the planner is able to plan to ‘dig holes of</context>
</contexts>
<marker>[13]</marker>
<rawString>Fikes, R. E. and N. J. Nilsson, STRIPS: A new approach to the application of theorem proving to problem solving (1971), AI 2: 189–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Green</author>
</authors>
<title>Application of theorem proving to problem solving</title>
<date>1969</date>
<booktitle>in Proc. 1st IJCAI,</booktitle>
<pages>219--39</pages>
<contexts>
<context position="8790" citStr="[23,24,14,22,13]" startWordPosition="1454" endWordPosition="1454"> by surface linguistic form (after [7]), including ‘declare’, ‘request’, and perhaps others—a set of acts with negligible effects (after [27]), and minimal preconditions. We in fact developed a single linguistic act for all contexts. 2.2 Planner design The planner is essentially an epistemic theorem prover which employs some planning search. The development process we undertook is helpful in understanding the planner’s design: • A state-space search was implemented that searches backwards in hypothetical time from the goal via STRIPS operators (based on foundational work in classical planning [23,24,14,22,13]); • A theorem prover for FOL was implemented that constructively proves conjunctions, disjunctions, implications, and negations, and employs modus ponens and unit resolution; • State-space search and theorem proving were interwoven in such a way that: · not only can disjunctions, implications and negations be proved true, they can also be achieved; · not only can a goal Q be proved true by proving (P ⇒ Q) n P, but Q can also be achieved by proving P ⇒ Q and achieving P; · a goal can be achieved by reasoning with recursive domain-specific rules—thus the planner is able to plan to ‘dig holes of</context>
</contexts>
<marker>[14]</marker>
<rawString>Green, C., Application of theorem proving to problem solving (1969), in Proc. 1st IJCAI, pp. 219–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Grice</author>
</authors>
<title>Logic and conversation</title>
<date>1975</date>
<pages>41--58</pages>
<publisher>Academic Press.</publisher>
<location>New York:</location>
<note>in</note>
<contexts>
<context position="7945" citStr="[29,15,21]" startWordPosition="1333" endWordPosition="1333"> this example contains all the necessary information about the beliefs of John and Sally in this situation for the planner: (i) to be able to plan John’s utterance; and (ii) to additionally deduce whether John’s utterance would be felicitous or infelicitous, if he performed it. 3 The example is an English paraphrase of a task, written in the model in Prolog code. 4 An albatross (Diomedea exulans) is a huge sea-faring bird, rarely seen from the land. 2.1 Linguistic motivations Our approach to planning the semantics of utterances was to build on seminal work in speech acts [3,28] and pragmatics [29,15,21]. In contrast to the ‘speech acts with STRIPS’ approach [6,11,1,2], which is fraught with well-documented difficulties [9,16,26,7,27], we aimed to develop a small set of linguistic acts that were unambiguously identifiable purely by surface linguistic form (after [7]), including ‘declare’, ‘request’, and perhaps others—a set of acts with negligible effects (after [27]), and minimal preconditions. We in fact developed a single linguistic act for all contexts. 2.2 Planner design The planner is essentially an epistemic theorem prover which employs some planning search. The development process we </context>
<context position="12032" citStr="[15]" startWordPosition="2001" endWordPosition="2001">he question ‘What can I say in the current stater is something like ‘Anything, I just have to decide what I want to say’. A backwards search is far more suitable than a forwards search under conditions like these. With this ‘reasoning-centred’ design, the planner is able to plan an utterance to achieve a goal, ‘knowing’ that the utterance may or may not achieve the desired effects on H, and that the same utterance can have many different effects, depending on H’s belief state. 3 Modelling problematic conversations In the model, utterances are planned according to Grice’s Cooperative Principle [15]. Here is an extract from the CP (ibid p. 308): “[Quantity] (i) Make your contribution as informative as is required (for the current purposes of the exchange). (ii) Do not make your contribution more informative than is required... [Quality] (i) Do not say what you believe to be false. (ii) Do not say that for which you lack adequate evidence.” Grice’s maxims prescribe a standard for speaker behaviour which S can blatantly contravene (‘flout’), thus signalling to H that there is an implicature to be recovered. For instance, in our ‘sarcasm’ scenario, John’s utterance is planned using the foll</context>
</contexts>
<marker>[15]</marker>
<rawString>Grice, H. P., Logic and conversation (1975), in P. Cole and J. Morgan, (eds), Syntax and semantics 3: Speech acts, pp. 41–58. New York: Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
<author>C L Sidner</author>
</authors>
<title>Plans for discourse</title>
<date>1990</date>
<pages>10--416</pages>
<contexts>
<context position="8078" citStr="[9,16,26,7,27]" startWordPosition="1350" endWordPosition="1350">o be able to plan John’s utterance; and (ii) to additionally deduce whether John’s utterance would be felicitous or infelicitous, if he performed it. 3 The example is an English paraphrase of a task, written in the model in Prolog code. 4 An albatross (Diomedea exulans) is a huge sea-faring bird, rarely seen from the land. 2.1 Linguistic motivations Our approach to planning the semantics of utterances was to build on seminal work in speech acts [3,28] and pragmatics [29,15,21]. In contrast to the ‘speech acts with STRIPS’ approach [6,11,1,2], which is fraught with well-documented difficulties [9,16,26,7,27], we aimed to develop a small set of linguistic acts that were unambiguously identifiable purely by surface linguistic form (after [7]), including ‘declare’, ‘request’, and perhaps others—a set of acts with negligible effects (after [27]), and minimal preconditions. We in fact developed a single linguistic act for all contexts. 2.2 Planner design The planner is essentially an epistemic theorem prover which employs some planning search. The development process we undertook is helpful in understanding the planner’s design: • A state-space search was implemented that searches backwards in hypothe</context>
</contexts>
<marker>[16]</marker>
<rawString>Grosz, B. J. and C. L. Sidner, Plans for discourse (1990), [10], pp. 416–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hintikka</author>
</authors>
<title>Knowledge and belief: An introduction to the two notions</title>
<date>1962</date>
<publisher>Cornell University Press.</publisher>
<location>New York:</location>
<contexts>
<context position="13019" citStr="[17,20]" startWordPosition="2157" endWordPosition="2157">ard for speaker behaviour which S can blatantly contravene (‘flout’), thus signalling to H that there is an implicature to be recovered. For instance, in our ‘sarcasm’ scenario, John’s utterance is planned using the following maxim, derived from Grice’s first Quality maxim. 5 The first line means, ‘If S addresses H by putting Q into the conversational minutes’: (1) minute([S], [H], Q) and believes(S, believes(H, mutuallybelieve(([H, S]), not(Q)))) ==&gt; believes(S, believes(H, griceuncoop(S, [H], Q))) 5 The model embodies a ‘deduction’ model of belief [19], rather than a ‘possible worlds’ model [17,20]. Thus agents are not required to draw all logically possible inferences, and are therefore not required to infer an infinite number of propositions from a mutual belief. Using this maxim, John reasons that he can get Sally to realise he is flouting a maxim in order to generate an implicature (that he is being ‘Grice uncooperative with respect to Q’). But what is the nature of the implicature? This is dealt with by two additional rules: (2), which describes what John thinks Sally believes about the meaning of this kind of maxim-flouting; and (3), a ‘general knowledge’ rule: (2) believes(john, </context>
</contexts>
<marker>[17]</marker>
<rawString>Hintikka, J., Knowledge and belief: An introduction to the two notions (1962), New York: Cornell University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hoffmann</author>
<author>B Nebel</author>
</authors>
<title>The FF planning system: Fast plan generation through heuristic search</title>
<date>2001</date>
<journal>Journal of AI Research</journal>
<volume>14</volume>
<pages>253--302</pages>
<contexts>
<context position="4378" citStr="[4,5,18,25]" startWordPosition="728" endWordPosition="728"> goals in mind, and knows that the execution of any single plan could have a myriad different effects on H’s belief state, depending on what H chooses to infer. 1.1 Bucking the trend The main focus of current research in AI planning is on how to reduce the search space required for making plans, and thus, for example, to get Rob the tea-making robot to be able to make his plans fast enough to be of practical use in the real world. Many planners use heuristics, either to constrain the generation of a search space, or to prune and guide the search through the state space for a solution, or both [4,5,18,25]. All such planners succeed by relying on the static effects of actions—on the fact that you can tell by inspection what the effects of an action will be in any situation—which limits their scope in a particular way [4, p. 299]: “... if one of the actions allows the planner to dig a hole of an arbitrary integral depth, then there are potentially infinitely many objects that can be created... The effect of this action cannot be determined statically ... ” The class of problems that these planners do not attempt to solve—the ability to plan actions whose effects are not determined statically—was</context>
</contexts>
<marker>[18]</marker>
<rawString>Hoffmann, J. and B. Nebel, The FF planning system: Fast plan generation through heuristic search (2001), Journal of AI Research 14: 253–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Konolige</author>
</authors>
<title>A deduction model of belief</title>
<date>1986</date>
<location>London: Pitman.</location>
<contexts>
<context position="12972" citStr="[19]" startWordPosition="2150" endWordPosition="2150"> evidence.” Grice’s maxims prescribe a standard for speaker behaviour which S can blatantly contravene (‘flout’), thus signalling to H that there is an implicature to be recovered. For instance, in our ‘sarcasm’ scenario, John’s utterance is planned using the following maxim, derived from Grice’s first Quality maxim. 5 The first line means, ‘If S addresses H by putting Q into the conversational minutes’: (1) minute([S], [H], Q) and believes(S, believes(H, mutuallybelieve(([H, S]), not(Q)))) ==&gt; believes(S, believes(H, griceuncoop(S, [H], Q))) 5 The model embodies a ‘deduction’ model of belief [19], rather than a ‘possible worlds’ model [17,20]. Thus agents are not required to draw all logically possible inferences, and are therefore not required to infer an infinite number of propositions from a mutual belief. Using this maxim, John reasons that he can get Sally to realise he is flouting a maxim in order to generate an implicature (that he is being ‘Grice uncooperative with respect to Q’). But what is the nature of the implicature? This is dealt with by two additional rules: (2), which describes what John thinks Sally believes about the meaning of this kind of maxim-flouting; and (3), </context>
</contexts>
<marker>[19]</marker>
<rawString>Konolige, K., A deduction model of belief (1986), London: Pitman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kripke</author>
</authors>
<title>Semantical considerations on modal logic</title>
<date>1963</date>
<journal>in Acta Philosophica Fennica</journal>
<volume>16</volume>
<pages>83--94</pages>
<contexts>
<context position="13019" citStr="[17,20]" startWordPosition="2157" endWordPosition="2157">ard for speaker behaviour which S can blatantly contravene (‘flout’), thus signalling to H that there is an implicature to be recovered. For instance, in our ‘sarcasm’ scenario, John’s utterance is planned using the following maxim, derived from Grice’s first Quality maxim. 5 The first line means, ‘If S addresses H by putting Q into the conversational minutes’: (1) minute([S], [H], Q) and believes(S, believes(H, mutuallybelieve(([H, S]), not(Q)))) ==&gt; believes(S, believes(H, griceuncoop(S, [H], Q))) 5 The model embodies a ‘deduction’ model of belief [19], rather than a ‘possible worlds’ model [17,20]. Thus agents are not required to draw all logically possible inferences, and are therefore not required to infer an infinite number of propositions from a mutual belief. Using this maxim, John reasons that he can get Sally to realise he is flouting a maxim in order to generate an implicature (that he is being ‘Grice uncooperative with respect to Q’). But what is the nature of the implicature? This is dealt with by two additional rules: (2), which describes what John thinks Sally believes about the meaning of this kind of maxim-flouting; and (3), a ‘general knowledge’ rule: (2) believes(john, </context>
</contexts>
<marker>[20]</marker>
<rawString>Kripke, S., Semantical considerations on modal logic (1963), in Acta Philosophica Fennica 16: 83–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lewis</author>
</authors>
<title>Scorekeeping in a language game</title>
<date>1979</date>
<journal>J. Phil. Logic</journal>
<volume>8</volume>
<pages>339--59</pages>
<contexts>
<context position="7945" citStr="[29,15,21]" startWordPosition="1333" endWordPosition="1333"> this example contains all the necessary information about the beliefs of John and Sally in this situation for the planner: (i) to be able to plan John’s utterance; and (ii) to additionally deduce whether John’s utterance would be felicitous or infelicitous, if he performed it. 3 The example is an English paraphrase of a task, written in the model in Prolog code. 4 An albatross (Diomedea exulans) is a huge sea-faring bird, rarely seen from the land. 2.1 Linguistic motivations Our approach to planning the semantics of utterances was to build on seminal work in speech acts [3,28] and pragmatics [29,15,21]. In contrast to the ‘speech acts with STRIPS’ approach [6,11,1,2], which is fraught with well-documented difficulties [9,16,26,7,27], we aimed to develop a small set of linguistic acts that were unambiguously identifiable purely by surface linguistic form (after [7]), including ‘declare’, ‘request’, and perhaps others—a set of acts with negligible effects (after [27]), and minimal preconditions. We in fact developed a single linguistic act for all contexts. 2.2 Planner design The planner is essentially an epistemic theorem prover which employs some planning search. The development process we </context>
</contexts>
<marker>[21]</marker>
<rawString>Lewis, D., Scorekeeping in a language game (1979), J. Phil. Logic 8: 339–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J McCarthy</author>
<author>P J Hayes</author>
</authors>
<title>Some philosophical problems from the standpoint of artificial intelligence</title>
<date>1969</date>
<journal>Machine Intelligence</journal>
<volume>4</volume>
<pages>463--502</pages>
<contexts>
<context position="8790" citStr="[23,24,14,22,13]" startWordPosition="1454" endWordPosition="1454"> by surface linguistic form (after [7]), including ‘declare’, ‘request’, and perhaps others—a set of acts with negligible effects (after [27]), and minimal preconditions. We in fact developed a single linguistic act for all contexts. 2.2 Planner design The planner is essentially an epistemic theorem prover which employs some planning search. The development process we undertook is helpful in understanding the planner’s design: • A state-space search was implemented that searches backwards in hypothetical time from the goal via STRIPS operators (based on foundational work in classical planning [23,24,14,22,13]); • A theorem prover for FOL was implemented that constructively proves conjunctions, disjunctions, implications, and negations, and employs modus ponens and unit resolution; • State-space search and theorem proving were interwoven in such a way that: · not only can disjunctions, implications and negations be proved true, they can also be achieved; · not only can a goal Q be proved true by proving (P ⇒ Q) n P, but Q can also be achieved by proving P ⇒ Q and achieving P; · a goal can be achieved by reasoning with recursive domain-specific rules—thus the planner is able to plan to ‘dig holes of</context>
</contexts>
<marker>[22]</marker>
<rawString>McCarthy, J. and P. J. Hayes, Some philosophical problems from the standpoint of artificial intelligence (1969), Machine Intelligence 4: 463–502.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Newell</author>
<author>J C Shaw</author>
<author>H A Simon</author>
</authors>
<title>Empirical explorations with the logic theory machine</title>
<date>1957</date>
<booktitle>Proc. Western Joint Computer Conference,</booktitle>
<volume>15</volume>
<pages>218--239</pages>
<contexts>
<context position="8790" citStr="[23,24,14,22,13]" startWordPosition="1454" endWordPosition="1454"> by surface linguistic form (after [7]), including ‘declare’, ‘request’, and perhaps others—a set of acts with negligible effects (after [27]), and minimal preconditions. We in fact developed a single linguistic act for all contexts. 2.2 Planner design The planner is essentially an epistemic theorem prover which employs some planning search. The development process we undertook is helpful in understanding the planner’s design: • A state-space search was implemented that searches backwards in hypothetical time from the goal via STRIPS operators (based on foundational work in classical planning [23,24,14,22,13]); • A theorem prover for FOL was implemented that constructively proves conjunctions, disjunctions, implications, and negations, and employs modus ponens and unit resolution; • State-space search and theorem proving were interwoven in such a way that: · not only can disjunctions, implications and negations be proved true, they can also be achieved; · not only can a goal Q be proved true by proving (P ⇒ Q) n P, but Q can also be achieved by proving P ⇒ Q and achieving P; · a goal can be achieved by reasoning with recursive domain-specific rules—thus the planner is able to plan to ‘dig holes of</context>
</contexts>
<marker>[23]</marker>
<rawString>Newell, A., J. C. Shaw and H. A. Simon, Empirical explorations with the logic theory machine (1957), Proc. Western Joint Computer Conference, 15: 218–239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Newell</author>
<author>H A Simon</author>
</authors>
<title>GPS, a program that simulates human thought</title>
<date>1963</date>
<volume>12</volume>
<pages>279--93</pages>
<contexts>
<context position="8790" citStr="[23,24,14,22,13]" startWordPosition="1454" endWordPosition="1454"> by surface linguistic form (after [7]), including ‘declare’, ‘request’, and perhaps others—a set of acts with negligible effects (after [27]), and minimal preconditions. We in fact developed a single linguistic act for all contexts. 2.2 Planner design The planner is essentially an epistemic theorem prover which employs some planning search. The development process we undertook is helpful in understanding the planner’s design: • A state-space search was implemented that searches backwards in hypothetical time from the goal via STRIPS operators (based on foundational work in classical planning [23,24,14,22,13]); • A theorem prover for FOL was implemented that constructively proves conjunctions, disjunctions, implications, and negations, and employs modus ponens and unit resolution; • State-space search and theorem proving were interwoven in such a way that: · not only can disjunctions, implications and negations be proved true, they can also be achieved; · not only can a goal Q be proved true by proving (P ⇒ Q) n P, but Q can also be achieved by proving P ⇒ Q and achieving P; · a goal can be achieved by reasoning with recursive domain-specific rules—thus the planner is able to plan to ‘dig holes of</context>
</contexts>
<marker>[24]</marker>
<rawString>Newell, A. and H. A. Simon, GPS, a program that simulates human thought (1963), [12], pp. 279–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Nguyen</author>
<author>S Kambhampati</author>
</authors>
<title>Reviving partial order planning</title>
<date>2001</date>
<booktitle>in Proc. IJCAI,</booktitle>
<pages>459--66</pages>
<contexts>
<context position="4378" citStr="[4,5,18,25]" startWordPosition="728" endWordPosition="728"> goals in mind, and knows that the execution of any single plan could have a myriad different effects on H’s belief state, depending on what H chooses to infer. 1.1 Bucking the trend The main focus of current research in AI planning is on how to reduce the search space required for making plans, and thus, for example, to get Rob the tea-making robot to be able to make his plans fast enough to be of practical use in the real world. Many planners use heuristics, either to constrain the generation of a search space, or to prune and guide the search through the state space for a solution, or both [4,5,18,25]. All such planners succeed by relying on the static effects of actions—on the fact that you can tell by inspection what the effects of an action will be in any situation—which limits their scope in a particular way [4, p. 299]: “... if one of the actions allows the planner to dig a hole of an arbitrary integral depth, then there are potentially infinitely many objects that can be created... The effect of this action cannot be determined statically ... ” The class of problems that these planners do not attempt to solve—the ability to plan actions whose effects are not determined statically—was</context>
</contexts>
<marker>[25]</marker>
<rawString>Nguyen, X. and S. Kambhampati, Reviving partial order planning (2001), in Proc. IJCAI, pp. 459–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Pollack</author>
</authors>
<title>Plans as complex mental attitudes</title>
<date>1990</date>
<pages>10--77</pages>
<contexts>
<context position="8078" citStr="[9,16,26,7,27]" startWordPosition="1350" endWordPosition="1350">o be able to plan John’s utterance; and (ii) to additionally deduce whether John’s utterance would be felicitous or infelicitous, if he performed it. 3 The example is an English paraphrase of a task, written in the model in Prolog code. 4 An albatross (Diomedea exulans) is a huge sea-faring bird, rarely seen from the land. 2.1 Linguistic motivations Our approach to planning the semantics of utterances was to build on seminal work in speech acts [3,28] and pragmatics [29,15,21]. In contrast to the ‘speech acts with STRIPS’ approach [6,11,1,2], which is fraught with well-documented difficulties [9,16,26,7,27], we aimed to develop a small set of linguistic acts that were unambiguously identifiable purely by surface linguistic form (after [7]), including ‘declare’, ‘request’, and perhaps others—a set of acts with negligible effects (after [27]), and minimal preconditions. We in fact developed a single linguistic act for all contexts. 2.2 Planner design The planner is essentially an epistemic theorem prover which employs some planning search. The development process we undertook is helpful in understanding the planner’s design: • A state-space search was implemented that searches backwards in hypothe</context>
</contexts>
<marker>[26]</marker>
<rawString>Pollack, M. E., Plans as complex mental attitudes (1990), [10], pp. 77–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ramsay</author>
</authors>
<title>Speech act theory and epistemic planning</title>
<date>2000</date>
<volume>8</volume>
<pages>293--310</pages>
<contexts>
<context position="8078" citStr="[9,16,26,7,27]" startWordPosition="1350" endWordPosition="1350">o be able to plan John’s utterance; and (ii) to additionally deduce whether John’s utterance would be felicitous or infelicitous, if he performed it. 3 The example is an English paraphrase of a task, written in the model in Prolog code. 4 An albatross (Diomedea exulans) is a huge sea-faring bird, rarely seen from the land. 2.1 Linguistic motivations Our approach to planning the semantics of utterances was to build on seminal work in speech acts [3,28] and pragmatics [29,15,21]. In contrast to the ‘speech acts with STRIPS’ approach [6,11,1,2], which is fraught with well-documented difficulties [9,16,26,7,27], we aimed to develop a small set of linguistic acts that were unambiguously identifiable purely by surface linguistic form (after [7]), including ‘declare’, ‘request’, and perhaps others—a set of acts with negligible effects (after [27]), and minimal preconditions. We in fact developed a single linguistic act for all contexts. 2.2 Planner design The planner is essentially an epistemic theorem prover which employs some planning search. The development process we undertook is helpful in understanding the planner’s design: • A state-space search was implemented that searches backwards in hypothe</context>
</contexts>
<marker>[27]</marker>
<rawString>Ramsay, A., Speech act theory and epistemic planning (2000), [8], pp. 293–310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Searle</author>
</authors>
<title>What is a speech act?</title>
<date>1965</date>
<booktitle>Philosophy in America,</booktitle>
<pages>221--39</pages>
<editor>in M. Black, (ed),</editor>
<publisher>Allen</publisher>
<contexts>
<context position="7919" citStr="[3,28]" startWordPosition="1330" endWordPosition="1330"> The formal version of this example contains all the necessary information about the beliefs of John and Sally in this situation for the planner: (i) to be able to plan John’s utterance; and (ii) to additionally deduce whether John’s utterance would be felicitous or infelicitous, if he performed it. 3 The example is an English paraphrase of a task, written in the model in Prolog code. 4 An albatross (Diomedea exulans) is a huge sea-faring bird, rarely seen from the land. 2.1 Linguistic motivations Our approach to planning the semantics of utterances was to build on seminal work in speech acts [3,28] and pragmatics [29,15,21]. In contrast to the ‘speech acts with STRIPS’ approach [6,11,1,2], which is fraught with well-documented difficulties [9,16,26,7,27], we aimed to develop a small set of linguistic acts that were unambiguously identifiable purely by surface linguistic form (after [7]), including ‘declare’, ‘request’, and perhaps others—a set of acts with negligible effects (after [27]), and minimal preconditions. We in fact developed a single linguistic act for all contexts. 2.2 Planner design The planner is essentially an epistemic theorem prover which employs some planning search. T</context>
</contexts>
<marker>[28]</marker>
<rawString>Searle, J. R., What is a speech act? (1965), in M. Black, (ed), Philosophy in America, pp. 221–39. Allen and Unwin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Stalnaker</author>
<author>Pragmatics</author>
</authors>
<date>1972</date>
<journal>Semantics of natural language (Synthese Library,</journal>
<volume>40</volume>
<pages>380--97</pages>
<location>Dordrecht, Holland: D. Reidel.</location>
<note>in</note>
<contexts>
<context position="7945" citStr="[29,15,21]" startWordPosition="1333" endWordPosition="1333"> this example contains all the necessary information about the beliefs of John and Sally in this situation for the planner: (i) to be able to plan John’s utterance; and (ii) to additionally deduce whether John’s utterance would be felicitous or infelicitous, if he performed it. 3 The example is an English paraphrase of a task, written in the model in Prolog code. 4 An albatross (Diomedea exulans) is a huge sea-faring bird, rarely seen from the land. 2.1 Linguistic motivations Our approach to planning the semantics of utterances was to build on seminal work in speech acts [3,28] and pragmatics [29,15,21]. In contrast to the ‘speech acts with STRIPS’ approach [6,11,1,2], which is fraught with well-documented difficulties [9,16,26,7,27], we aimed to develop a small set of linguistic acts that were unambiguously identifiable purely by surface linguistic form (after [7]), including ‘declare’, ‘request’, and perhaps others—a set of acts with negligible effects (after [27]), and minimal preconditions. We in fact developed a single linguistic act for all contexts. 2.2 Planner design The planner is essentially an epistemic theorem prover which employs some planning search. The development process we </context>
</contexts>
<marker>[29]</marker>
<rawString>Stalnaker, R., Pragmatics (1972), in D. Davidson and G. Harman, (eds), Semantics of natural language (Synthese Library, Vol. 40), pp. 380–97. Dordrecht, Holland: D. Reidel.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>