<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000280">
<title confidence="0.9994205">
An Extensible Probabilistic Transformation-based Approach
to the Third Recognizing Textual Entailment Challenge
</title>
<author confidence="0.99911">
Stefan Harmeling
</author>
<affiliation confidence="0.999254">
Institute of Adaptive and Neural Computation
School of Informatics, Edinburgh University, Scotland
</affiliation>
<email confidence="0.996962">
stefan.harmeling@ed.ac.uk
</email>
<sectionHeader confidence="0.995614" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999950428571429">
We introduce a system for textual entail-
ment that is based on a probabilistic model
of entailment. The model is defined using
some calculus of transformations on depen-
dency trees, which is characterized by the
fact that derivations in that calculus preserve
the truth only with a certain probability. We
also describe a possible set of transforma-
tions (and with it implicitly a calculus) that
was successfully applied to the RTE3 chal-
lenge data. However, our system can be im-
proved in many ways and we see it as the
starting point for a promising new approach
to textual entailment.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.993638821428571">
Textual entailment recognition asks the question
whether a piece of text like
The Cassini Spacecraft has taken images
from July 22, 2006 that show rivers and
lakes present on Saturn’s moon Titan.
implies a hypothesis like
The Cassini Spacecraft reached Titan.
There exists already many interesting approaches to
this problem, see (Dagan et al., 2005; Bar-Haim et
al., 2006) for various recent efforts and our paper
wont try to fully reinvent the wheel. Instead it will
present some work in progress that tries to model the
probability of entailment in terms of ideas motivated
by approaches like the edit-distance (Kouylekov and
Magnini, 2005; Kouylekov and Magnini, 2006; Tatu
et al., 2006; Adams, 2006). However, instead of
defining some distance based on edits, we will gen-
erate derivations in some calculus that is able to
transform dependency parse trees. The special prop-
erty of our calculus is that the truth is only preserved
with a certain probability along its derivations. This
might sound like a disadvantage. However, in com-
monsense reasoning there is usual a lot of uncer-
tainty due the fact that it is impossible to formal-
ize all world knowledge. We think that probabili-
ties might help us in such situations where it is im-
possible to include everything into the model, but in
which nonetheless we want to do reasoning.
</bodyText>
<sectionHeader confidence="0.912195" genericHeader="method">
2 Main idea
</sectionHeader>
<bodyText confidence="0.999496">
First of all, let us assume that the text and the hy-
pothesis of an textual entailment example are repre-
sented as dependency trees T and H. We would like
to formalize the probability that T entails H with
some model pθ(T �= H) parametrized by a vector
0. In order to define pθ(T �= H) we first introduce
the probability of preserving truth along syntactic
derivations in some calculus T �-τ H which we in-
formally introduce next.
Suppose we are given n transformations
TF1, ... , TFn that are designed to modify depen-
dency trees. For each such transformation TFj,
the probability of preserving truth is modelled as a
constant value Bj independent of the dependency
tree T it is applied to, i.e.
</bodyText>
<equation confidence="0.870163">
pθ(T �_TF;TFj(T)) = Bj for all T, (1)
</equation>
<bodyText confidence="0.913271">
with parameter 0 being the vector of all Bj. The idea
</bodyText>
<page confidence="0.966555">
137
</page>
<bodyText confidence="0.970690043478261">
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 137–142,
Prague, June 2007. c�2007 Association for Computational Linguistics
is that applying a transformation to T could also cre-
ate a dependency tree that is sometimes not entailed
by T anymore. Consider e.g. the transformation that
extracts an appositive and adds a new sentence for
it. Usually this is correct, but there are situations in
which the appositive appears inside a quote, where
it might lead to a wrong conclusion. Thus it makes
sense to consider probabilities to deal with imperfect
calculi.
We call an n-tuple of such transformations a
derivation, which we denote by τ with `(τ) = n.
Let τj count the number of times TFj appears in
τ. Furthermore, let τ(T) be the result of applying
the transformations in τ to some dependency tree T,
e.g. for τ = (TF3, TF3, TF17) with `(τ) = 3 we
have τ(T) = TF17(TF3(TF3(T))).
Suppose that a derivation τ = (t1, ... , t`(τ)) de-
rives H from T, i.e. τ(T) = H. Then we define the
probability of preserving the truth along the deriva-
tion τ as the product of the preservation probabilities
of the transformations involved1:
</bodyText>
<equation confidence="0.995306333333333">
`(τ)−1 n
� pθ(Ti �-tiTi+1) = H θτi j(2)
i=1 j=1
</equation>
<bodyText confidence="0.996479454545455">
with T1 = T, Ti+1 = ti(Ti) and T`(τ) = H. Note
that even though for a certain dependency tree T ap-
plying different derivations τ and σ can result in the
same tree, i.e. τ(T) = σ(T), their probabilities of
preserving truth can be different, since the probabil-
ities depend only the transformations applied.
In the previous paragraphs we have defined proba-
bilities of preserving truth for all finite length deriva-
tions in the calculus. This allows us now to define
the probability of T �= H to be the maximal proba-
bility over all possible derivations,
</bodyText>
<equation confidence="0.996208666666667">
pθ(T �=H) = max pθ(T SτH) = max
τ:τ(T)=H τ:τ(T)=H
(3)
</equation>
<bodyText confidence="0.9186145">
In the following we introduce a set of transforma-
tions that is able to transform any text into any hy-
pothesis and we will propose a heuristic that gener-
ates such derivations.
1Note, that this definition is similar to the idea of “transitive
chaining” introduced in (Dagan and Glickman, 2004).
</bodyText>
<sectionHeader confidence="0.998246" genericHeader="method">
3 Details
</sectionHeader>
<subsectionHeader confidence="0.999901">
3.1 Preprocessing and parsing
</subsectionHeader>
<bodyText confidence="0.999553">
For preprocessing we apply the following steps to
the text string and the hypothesis string:
</bodyText>
<listItem confidence="0.997432428571429">
(1) Remove white space, dots and quotations
marks at beginning and end.
(2) Remove trailing points of abbreviations.
(3) Remove space between names, e.g. ’Pat Smith’
becomes ’PatSmith’.
(4) Unify numbers, e.g. resolve ’million’.
(5) Unify dates, e.g. ’5 June’ becomes ’June 5’.
</listItem>
<bodyText confidence="0.998286">
We then split the text string into sentences simply by
splitting at all locations containing a dot followed by
a space. The resulting strings are fed to the Stanford
Parser (de Marneffe et al., 2006; Klein and Man-
ning, 2003) with its included pretrained model and
options ’-retainTmpSubcategories’ and ’-splitTMP
1’. This allows us to generate dependency trees the
nodes of which contain a single stemmed word, its
part-of-speech tag and its dependency tag (as pro-
duced using the parser’s output options ’wordsAnd-
Tags’ and ’typedDependencies’, see (de Marneffe et
al., 2006)). For the stemming we apply the function
’morphy’ of the NLTK-LITE toolbox (Bird, 2005).
If the text string contains more than one sentence,
they will be combined to a single dependency tree
with a common root node. Let us from now on refer
to the dependency trees of text and hypothesis by T
and H.
</bodyText>
<subsectionHeader confidence="0.999692">
3.2 Generating derivations
</subsectionHeader>
<bodyText confidence="0.999667">
The heuristic described in the following generates a
derivation that transforms T into H. For brevity we
will use in the text the abbreviations of the transfor-
mations as listed in Tab. 1.
</bodyText>
<listItem confidence="0.9940932">
(1) Resolve appositives and relative clauses. All
derivations for some T and H start by converting
any existing appositives and relative clauses in T to
new sentences that are added to T. For each sen-
tence that was added in this step, the applied trans-
formation, ATS or RTS, is appended to τ.
(2) Calculate how H can clamp to T. Often there
are several possibilities to assign all or some of the
words in H to words in T. For simplicity our system
currently ignores certain grammatical parts which
</listItem>
<equation confidence="0.893870571428572">
pθ(T SτH) =
n
ri
j=1
τ&apos;
.
θj&apos;
</equation>
<page confidence="0.989192">
138
</page>
<table confidence="0.885506416666667">
SS substitute synonym
SN substitute number
SNE substitute named entity
SI substitute identity
SHE substitute hypernym
SHO substitute hyponym
SC substitute currency
SP substitute pronoun
GTC grammar tag change
CP change prep
SA substitute antonym
DOS del other sents
RUP remove unclamped parts
RUN remove unclamped negs
RUNO remove unclamped negs oddity
MCU move clamped up
RRN restructure remove noun
RAN restructure add noun
RRV restructure remove verb
RAV restructure add verb
RPD restructure pos depth
RND restructure neg depth
RHNC restructure h neg count
RHNO restructure h neg oddity
</table>
<construct confidence="0.19553975">
ATP active to passive
PTA passive to active
ATS appos to sent
RTS rcmod to sent
</construct>
<tableCaption confidence="0.997443">
Table 1: Current transformations with their abbr.
</tableCaption>
<bodyText confidence="0.9997299">
are auxiliaries (’aux’), determiners (’det’), preposi-
tions (’prep’) and possessives (’poss’). Furthermore,
we currently ignore words of those parts of speech
(POS) that are not verbs (’VB’), nouns (’NN’), ad-
jectives (’JJ’), adverbs (’RB’), pronouns (’PR’), car-
dinals (’CD’) or dollar signs (’$’). For all other
words wH in H and words wT in T we calculate
whether wT can be substituted by wH. For this we
employ amongst simple heuristics also WordNet 2.1
(Fellbaum, 1998) as described next:
</bodyText>
<listItem confidence="0.999540896551724">
(1) Are the tokens and POS tags of wT and wH
identical? If yes, return (1, ’identity’).
(2) If the POS tags of wT and wH indicate that
both words appear in WordNet continue with
(3) otherwise with (8).
(3) Are they antonyms in WordNet? If yes, return
(2, ’antonym’).
(4) Are they synonyms in WordNet? If yes, return
(2, ’synonym’).
(5) Does wH appear in the hypernym hierarchy of
wT in WordNet? If yes, return (z, ’hyponym’)
with z being the distance, i.e. wT is a hyponym
of wH.
(6) Does wT appear in the hypernym hierarchy of
wH in WordNet? If yes, return (z, ’hypernym’)
with z being the distance, i.e. wT is a hypernym
of wH
(7) Are they named entities that share certain parts
of their strings? If yes, return (z, ’named en-
tity’) with z being larger dependent on how dif-
ferent they are.
(8) Is wT a pronoun and wH a noun? If yes, return
(2, ’pronoun’).
(9) Are wT and wH exactly matching cardinals? If
yes, return (1, ’number’).
(10) Are wT and wH identical currencies? If yes,
return (1, ’currency’).
(11) Are wT and wH both currencies? If yes, return
(2, ’currency’).
</listItem>
<bodyText confidence="0.999165633333333">
Note that along the hierarchy in WordNet we also
look one step along the “derived form” pointer to al-
low a noun like ’winner’ be substitutable by the verb
’win’. If a word wT is substitutable by a word wH,
we say that wT and wH are clamped. We call the
whole assignment that assigns some or all words of
H to words in T a clamp. Since usually a single
word wH is clamped to several words in T, we will
often have several different clamps. E.g. if H has
three words each of which is clamped to four words
in T there are sixty-four possible clamps in total,
i.e. sixty-four possible ways to clamp the words in
H to words in T.
Each of these different clamps gives rise to a dif-
ferent derivation. However, let us for simplicity con-
tinue to focus on a single clamp and see how to com-
plete a single derivation T.
(3) Substitute the clamped words. If wH and wT
are clamped, we know what their relationship is:
e.g. (3, hypernym) means that we have to go three
steps up wH’s hypernym-hierarchy in WordNet to
reach wT. Thus we have to apply three times the
transformation SHE to substitute wT by wH, which
we reflect in T by appending three times SHE to it.
Similarly, we add other transformations for other re-
lations. The substitution of wT with wH might also
trigger other transformations, such as PTA, ATP, CP
and GTC which try to adjust the surrounding gram-
matical structure. All applied transformations will
be appended to the derivation T.
</bodyText>
<page confidence="0.990997">
139
</page>
<listItem confidence="0.903859125">
(4) Pick the sentence with the most clamps. Af-
ter substituting all clamped words, we simply pick
the sentence in T with the most clamped words and
delete the others using DOS. E.g. if T consists of
three sentences, after this step T will only contain
the single sentence with the most clamps and DOS
will be appended twice to τ.
(5) Remove subtrees that do not contain clamped
nodes. After this step we add for each removed
node the transformation RUP to τ. Then we add
RUN for each removed negation modifier (’neg’)
and additionally RUNO if the number of removed
negation is odd. RUNO is a somewhat artificial
transformation and acts more like a flag. This might
be changed in future sets of transformation to better
comply with the transformation metaphor.
(6) Move the clamped nodes closer to the root
and remove unclamped subtree. Again we do
some counting before and after this step, which de-
termines the transformations to add to τ. In partic-
ular we count how many verbs are passed by mov-
ing clamped nodes up. For each passed verb we add
MCU to τ.
(7) Restructure and add the missing pieces. The
</listItem>
<bodyText confidence="0.978669882352941">
definition in Eq. (3) requires that any T can be trans-
formed into any H, otherwise the maximum is unde-
fined. In the last step we will thus remove all words
in T which are not needed for H and add all miss-
ing words to T and restructure until T becomes H.
For the bookkeeping we count the number of nouns,
verbs and negation modifier that have to be added
and removed. Furthermore, we count how many lev-
els up or down we need to move words in T such
that they match the structure in H. For all these
countings we add accordingly as many transforma-
tions RRN, RRV, RAN, RAV, RPD, RND, RHNC,
RHNO (see Tab. 1 for short explanations).
Finally, the completed derivation τ with τ(T) =
H is converted to a 28-dimensional feature vector
[τ1, ... , τ28]T using the notion of τj which has been
defined in Sec. 2.
</bodyText>
<subsectionHeader confidence="0.997704">
3.3 Estimating the parameters
</subsectionHeader>
<bodyText confidence="0.975044666666667">
Let Dtr = {(T1, H1, y1), ... , (T800, H800, y800)}
be the training examples with yi E {0, 1} indicat-
ing entailment. For brevity we define
</bodyText>
<equation confidence="0.998627">
fi(θ) = pθ(Ti �=Hi) (4)
</equation>
<bodyText confidence="0.994428">
to abbreviate the probability of entailment modelled
as outline in Sec. 2. Then the data likelihood can be
written as:
</bodyText>
<equation confidence="0.978194">
800
pθ(Dtr) = � fi(θ)yi(1−fi(θ))(1−yi) (5)
i=1
</equation>
<bodyText confidence="0.9945536">
We would like to maximize pθ(Dtr) in term of the
vector θ. However, the maxima in Eq. (3) make
this optimization difficult. For the submission to the
RTE3 challenge we choose the following way to ap-
proximate it:
</bodyText>
<listItem confidence="0.8559519375">
(1) Generate for each example pair several deriva-
tions (as described in the previous section) and
choose the eight shortest ones. If there are less
than eight derivations available, copy the short-
est ones to end up with eight (some of which
could be identical).
(2) There are now 8 · 800 derivations in total. We
denote the corresponding feature vectors by
x1, . . . , x6400. Note that xi is a vector contain-
ing the countings of the different transforma-
tions. E.g. if the corresponding derivation was
τ, then xij = τj.
(3) Similarly copy the training labels yi to match
those 6400 feature vectors, i.e. now our data
becomes Dtr = {(x1, y1), ... , (x6400, y6400)}.
(4) Replacing fi(θ) by
</listItem>
<equation confidence="0.929546363636364">
�
gi(θ) =
j
the data likelihood becomes:
6400
pθ(Dtr) = � gi(θ)yi(1−gi(θ))(1−yi) (7)
i=1
(5) Replace furthermore each θj by
1
σ(zj) = (8)
1 + exp(−zj)
</equation>
<bodyText confidence="0.998933333333333">
with σ being the sigmoidal function, which en-
sures that the values for θj stay between zero
and one.
</bodyText>
<listItem confidence="0.944106666666667">
(6) Maximize pz(Dtr) in terms of z =
[z1, ... , zn]T using gradient ascent.
(7) Calculate θj = σ(zj) for all j.
</listItem>
<figure confidence="0.763816">
θxij
(6)
j
</figure>
<page confidence="0.963647">
140
</page>
<subsectionHeader confidence="0.993186">
3.4 Classifying the test data
</subsectionHeader>
<bodyText confidence="0.999988181818182">
Having estimated the parameter vector θ we can ap-
ply the trained model to the test data Dte to infer
its unknown labels. Since we only generate some
derivations and can not try all possible—as would
be required by Eq. (3)—we again transform the test
data into 6400 feature vectors x1,... , x6400. Note
that x1, ... , x8 are the feature vectors belonging to
the first test example (T1, H1), and so forth. To ap-
proximate the probability of entailment we take the
maximum over the eight feature vectors assigned to
each test example, e.g. for (T1, H1),
</bodyText>
<equation confidence="0.978048666666667">
H1 max
pθ(T1
iE{1,...,8}
</equation>
<bodyText confidence="0.999976285714286">
and analogously for the other test examples. The
class label and herewith the answer to the ques-
tion whether Ti entails Hi is obtained be checking
whether pθ(Ti �= Hi) is above a certain threshold,
which we can determine using the training set. This
completes the description of the system behind our
first run of our RTE3 submission.
</bodyText>
<subsectionHeader confidence="0.975844">
3.5 Logistic Regression
</subsectionHeader>
<bodyText confidence="0.99992925">
The second run of our RTE3 submission is moti-
vated by the following observation: introducing a
weight vector with entries wj = log θj and using
logarithms, we can rewrite Eq. (3) as
</bodyText>
<equation confidence="0.8558855">
log pθ(T �=H) = max
τ:τ(T)=H
</equation>
<bodyText confidence="0.999984083333333">
The probability of entailment becomes the maxi-
mum of several linear expressions with the addi-
tional constraint wj &lt; 0 for all j which ensures
that θj is a probability. In order to compare with an-
other linear classifier we applied as the second run
logistic regression to the data. Again we used eight
derivations/feature vectors per training example to
estimate the parameters of the logistic regression.
Also with the test data we applied the weight vector
to eight derivations/feature vectors per test example
and choose the largest result which was then thresh-
old to obtain a label.
</bodyText>
<sectionHeader confidence="0.999851" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.9944355">
The first fact we see from the official RTE3 results
in Tab. 3 is that our system is better than random.
</bodyText>
<table confidence="0.9828778">
RTE 2 overall IE IR QA SUM
AccTr 0.5950 0.5700 0.5850 0.5500 0.6750
AccTe 0.5675 0.5000 0.5850 0.5600 0.6250
AccTr 0.6050 0.5700 0.5550 0.5800 0.7150
AccTe 0.5725 0.5000 0.5800 0.5800 0.6300
</table>
<tableCaption confidence="0.954066">
Table 2: Results for the RTE2 data. Shown are the
</tableCaption>
<bodyText confidence="0.76394275">
accuracies on the training and test sets. First two
lines for the first run (transformation-based model)
and the next two lines for the second run (logistic
regression).
</bodyText>
<table confidence="0.998097142857143">
RTE 3 overall IE IR QA SUM
AccTr 0.6475 0.5750 0.6350 0.7600 0.6200
AccTe 0.5600 0.4700 0.6250 0.6450 0.5000
PreTe 0.5813 0.5162 0.6214 0.6881 0.5607
AccTr 0.6550 0.5600 0.6300 0.7850 0.6450
AccTe 0.5775 0.5000 0.6300 0.6700 0.5100
PreTe 0.5952 0.5562 0.6172 0.7003 0.5693
</table>
<tableCaption confidence="0.987485">
Table 3: Official results on the RTE3 test data
</tableCaption>
<bodyText confidence="0.997385631578947">
and inofficial results on the corresponding training
data. Shown are the accuracies and average preci-
sion on the test data. First three lines for the first
run (transformation-based model) and the next three
lines for the second run (logistic regression).
However, with 56% and 57.75% it is not much bet-
ter. From the task specific data we see that it com-
pletely failed on the information extraction (IE) and
the summarization (SUM) data. On the other hand
it has reached good results well above 60% for the
information retrieval (IR) and question answering
(QA) data. From the accuracies of the training data
in Tab. 3 we see that there was some overfitting.
We also applied our system to the RTE2 challenge
data. The results are shown in Tab. 2 and show that
our system is not yet competitive with last year’s
best systems. It is curious that in the RTE2 data the
SUM task appears simpler than the other tasks while
in this year’s data IR and QA seem to be the easiest.
</bodyText>
<sectionHeader confidence="0.9982" genericHeader="method">
5 Future work and conclusion
</sectionHeader>
<bodyText confidence="0.999968625">
As already mentioned, this paper presents work in
progress and we hope to improve our system in
the near future. For the RTE3 challenge our main
goal was to get a system running before the dead-
line. However, we had to make a lot of compro-
mises/simplifications to achieve that goal.
Even though our current results suggest that right
now our system might not be able to compete with
</bodyText>
<figure confidence="0.92413025">
θx�&apos; (9)
j
28
H
j=1
n
τjwj. (10)
j=1
</figure>
<page confidence="0.990774">
141
</page>
<bodyText confidence="0.999979">
the better systems from last year’s challenge we see
the potential that our architecture provides a useful
platform on which one can test and evolve different
sets of transformations on dependency trees. Again,
we note that such a set of transformations can be
seen as a calculus that preserves truth only with a
certain probability, which is an interesting concept
to follow up on. Furthermore, this idea of a prob-
abilistic calculus is not limited to dependency trees
but could equally well applied to other representa-
tions of text.
Besides working on more powerful and faithful
transformations, our system might be improved also
simply by replacing our ad hoc solutions for the pre-
processing and sentence-splitting. We should also
try different parsers and see how they compare for
our purposes. Since our approach is based on a
probabilistic model, we could also try to incorporate
several optional parse trees (as a probabilistic parser
might be able to create) with their respective proba-
bilities and create a system that uses probabilities in
a consistent way all the way from tagging/parsing to
inferring entailment.
</bodyText>
<sectionHeader confidence="0.976719" genericHeader="conclusions">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999909">
The author is grateful for valuable discussions with
Chris Williams and Amos Storkey. This research
was supported by the EU-PASCAL network of ex-
cellence (IST- 2002-506778) and through a Euro-
pean Community Marie Curie Fellowship (MEIF-
CT-2005-025578). Furthermore, the author is thank-
ful to the organizers of the RTE challenges and to the
creators of WordNet, NLTK-LITE and the Stanford
Parser for sharing their software with the scientific
community.
</bodyText>
<sectionHeader confidence="0.999278" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999887146341463">
R. Adams. 2006. Textual entailment through extended
lexical overlap. In Proceedings of the Second PASCAL
Challenges Workshop on Recognising Textual Entail-
ment.
R. Bar-Haim, I. Dagan, B. Dolan, L. Ferro, D. Giampic-
colo, B. Magnini, and I. Szpektor, editors. 2006. Pro-
ceedings of the Second PASCAL Challenges Workshop
on Recognising Textual Entailment.
S. Bird. 2005. NLTK-Lite: Efficient scripting for natural
language processing. In 4th International Conference
on Natural Language Processing, pages 1–8.
I. Dagan and O. Glickman. 2004. Probabilistic tex-
tual entailment: Generic applied modeling of language
variability. In PASCAL Workshop on Learning Meth-
ods for Text Understanding and Mining, Grenoble.
I. Dagan, O. Glickman, and B. Magnini, editors. 2005.
Proceedings of the PASCAL Challenges Workshop on
Recognising Textual Entailment.
M.-C. de Marneffe, B. MacCartney, and C.D. Man-
ning. 2006. Generating typed dependency parses from
phrase structure parses. In International Conference
on Language Resources and Evaluation (LREC).
C. Fellbaum. 1998. WordNet: an electronic lexical
database. MIT Press.
D. Klein and C.D. Manning. 2003. Accurate unlexical-
ized parsing. In Proceedings of the 41st Meeting of the
Association for Computational Linguistics.
M. Kouylekov and B. Magnini. 2005. Recognizing tex-
tual entailment with tree edit distance algorithms. In
Proceedings of the PASCAL Challenges Workshop on
Recognising Textual Entailment.
M. Kouylekov and B. Magnini. 2006. Tree edit distance
for recognizing textual entailment: Estimating the cost
of insertion. In Proceedings of the Second PASCAL
Challenges Workshop on Recognising Textual Entail-
ment.
M. Tatu, B. Iles, J. Slavick, A. Novischi, and
D. Moldovan. 2006. COGEX at the second recog-
nizing textual entailment challenge. In Proceedings of
the Second PASCAL Challenges Workshop on Recog-
nising Textual Entailment.
</reference>
<page confidence="0.997728">
142
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.952757">
<title confidence="0.9993965">An Extensible Probabilistic Transformation-based to the Third Recognizing Textual Entailment Challenge</title>
<author confidence="0.996687">Stefan</author>
<affiliation confidence="0.9994045">Institute of Adaptive and Neural School of Informatics, Edinburgh University,</affiliation>
<email confidence="0.998339">stefan.harmeling@ed.ac.uk</email>
<abstract confidence="0.997229933333333">We introduce a system for textual entailment that is based on a probabilistic model of entailment. The model is defined using some calculus of transformations on dependency trees, which is characterized by the fact that derivations in that calculus preserve the truth only with a certain probability. We also describe a possible set of transformations (and with it implicitly a calculus) that was successfully applied to the RTE3 challenge data. However, our system can be improved in many ways and we see it as the starting point for a promising new approach to textual entailment.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Adams</author>
</authors>
<title>Textual entailment through extended lexical overlap.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment.</booktitle>
<contexts>
<context position="1565" citStr="Adams, 2006" startWordPosition="241" endWordPosition="242">acecraft has taken images from July 22, 2006 that show rivers and lakes present on Saturn’s moon Titan. implies a hypothesis like The Cassini Spacecraft reached Titan. There exists already many interesting approaches to this problem, see (Dagan et al., 2005; Bar-Haim et al., 2006) for various recent efforts and our paper wont try to fully reinvent the wheel. Instead it will present some work in progress that tries to model the probability of entailment in terms of ideas motivated by approaches like the edit-distance (Kouylekov and Magnini, 2005; Kouylekov and Magnini, 2006; Tatu et al., 2006; Adams, 2006). However, instead of defining some distance based on edits, we will generate derivations in some calculus that is able to transform dependency parse trees. The special property of our calculus is that the truth is only preserved with a certain probability along its derivations. This might sound like a disadvantage. However, in commonsense reasoning there is usual a lot of uncertainty due the fact that it is impossible to formalize all world knowledge. We think that probabilities might help us in such situations where it is impossible to include everything into the model, but in which nonethel</context>
</contexts>
<marker>Adams, 2006</marker>
<rawString>R. Adams. 2006. Textual entailment through extended lexical overlap. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bar-Haim</author>
<author>I Dagan</author>
<author>B Dolan</author>
<author>L Ferro</author>
</authors>
<date>2006</date>
<booktitle>Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment.</booktitle>
<editor>D. Giampiccolo, B. Magnini, and I. Szpektor, editors.</editor>
<contexts>
<context position="1234" citStr="Bar-Haim et al., 2006" startWordPosition="185" endWordPosition="188">ns (and with it implicitly a calculus) that was successfully applied to the RTE3 challenge data. However, our system can be improved in many ways and we see it as the starting point for a promising new approach to textual entailment. 1 Introduction Textual entailment recognition asks the question whether a piece of text like The Cassini Spacecraft has taken images from July 22, 2006 that show rivers and lakes present on Saturn’s moon Titan. implies a hypothesis like The Cassini Spacecraft reached Titan. There exists already many interesting approaches to this problem, see (Dagan et al., 2005; Bar-Haim et al., 2006) for various recent efforts and our paper wont try to fully reinvent the wheel. Instead it will present some work in progress that tries to model the probability of entailment in terms of ideas motivated by approaches like the edit-distance (Kouylekov and Magnini, 2005; Kouylekov and Magnini, 2006; Tatu et al., 2006; Adams, 2006). However, instead of defining some distance based on edits, we will generate derivations in some calculus that is able to transform dependency parse trees. The special property of our calculus is that the truth is only preserved with a certain probability along its de</context>
</contexts>
<marker>Bar-Haim, Dagan, Dolan, Ferro, 2006</marker>
<rawString>R. Bar-Haim, I. Dagan, B. Dolan, L. Ferro, D. Giampiccolo, B. Magnini, and I. Szpektor, editors. 2006. Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bird</author>
</authors>
<title>NLTK-Lite: Efficient scripting for natural language processing.</title>
<date>2005</date>
<booktitle>In 4th International Conference on Natural Language Processing,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="6159" citStr="Bird, 2005" startWordPosition="1036" endWordPosition="1037"> by splitting at all locations containing a dot followed by a space. The resulting strings are fed to the Stanford Parser (de Marneffe et al., 2006; Klein and Manning, 2003) with its included pretrained model and options ’-retainTmpSubcategories’ and ’-splitTMP 1’. This allows us to generate dependency trees the nodes of which contain a single stemmed word, its part-of-speech tag and its dependency tag (as produced using the parser’s output options ’wordsAndTags’ and ’typedDependencies’, see (de Marneffe et al., 2006)). For the stemming we apply the function ’morphy’ of the NLTK-LITE toolbox (Bird, 2005). If the text string contains more than one sentence, they will be combined to a single dependency tree with a common root node. Let us from now on refer to the dependency trees of text and hypothesis by T and H. 3.2 Generating derivations The heuristic described in the following generates a derivation that transforms T into H. For brevity we will use in the text the abbreviations of the transformations as listed in Tab. 1. (1) Resolve appositives and relative clauses. All derivations for some T and H start by converting any existing appositives and relative clauses in T to new sentences that </context>
</contexts>
<marker>Bird, 2005</marker>
<rawString>S. Bird. 2005. NLTK-Lite: Efficient scripting for natural language processing. In 4th International Conference on Natural Language Processing, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>O Glickman</author>
</authors>
<title>Probabilistic textual entailment: Generic applied modeling of language variability.</title>
<date>2004</date>
<booktitle>In PASCAL Workshop on Learning Methods for Text Understanding and Mining,</booktitle>
<location>Grenoble.</location>
<contexts>
<context position="5084" citStr="Dagan and Glickman, 2004" startWordPosition="864" endWordPosition="867">d only the transformations applied. In the previous paragraphs we have defined probabilities of preserving truth for all finite length derivations in the calculus. This allows us now to define the probability of T �= H to be the maximal probability over all possible derivations, pθ(T �=H) = max pθ(T SτH) = max τ:τ(T)=H τ:τ(T)=H (3) In the following we introduce a set of transformations that is able to transform any text into any hypothesis and we will propose a heuristic that generates such derivations. 1Note, that this definition is similar to the idea of “transitive chaining” introduced in (Dagan and Glickman, 2004). 3 Details 3.1 Preprocessing and parsing For preprocessing we apply the following steps to the text string and the hypothesis string: (1) Remove white space, dots and quotations marks at beginning and end. (2) Remove trailing points of abbreviations. (3) Remove space between names, e.g. ’Pat Smith’ becomes ’PatSmith’. (4) Unify numbers, e.g. resolve ’million’. (5) Unify dates, e.g. ’5 June’ becomes ’June 5’. We then split the text string into sentences simply by splitting at all locations containing a dot followed by a space. The resulting strings are fed to the Stanford Parser (de Marneffe e</context>
</contexts>
<marker>Dagan, Glickman, 2004</marker>
<rawString>I. Dagan and O. Glickman. 2004. Probabilistic textual entailment: Generic applied modeling of language variability. In PASCAL Workshop on Learning Methods for Text Understanding and Mining, Grenoble.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>O Glickman</author>
<author>B Magnini</author>
<author>editors</author>
</authors>
<date>2005</date>
<booktitle>Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment.</booktitle>
<contexts>
<context position="1210" citStr="Dagan et al., 2005" startWordPosition="181" endWordPosition="184">set of transformations (and with it implicitly a calculus) that was successfully applied to the RTE3 challenge data. However, our system can be improved in many ways and we see it as the starting point for a promising new approach to textual entailment. 1 Introduction Textual entailment recognition asks the question whether a piece of text like The Cassini Spacecraft has taken images from July 22, 2006 that show rivers and lakes present on Saturn’s moon Titan. implies a hypothesis like The Cassini Spacecraft reached Titan. There exists already many interesting approaches to this problem, see (Dagan et al., 2005; Bar-Haim et al., 2006) for various recent efforts and our paper wont try to fully reinvent the wheel. Instead it will present some work in progress that tries to model the probability of entailment in terms of ideas motivated by approaches like the edit-distance (Kouylekov and Magnini, 2005; Kouylekov and Magnini, 2006; Tatu et al., 2006; Adams, 2006). However, instead of defining some distance based on edits, we will generate derivations in some calculus that is able to transform dependency parse trees. The special property of our calculus is that the truth is only preserved with a certain </context>
</contexts>
<marker>Dagan, Glickman, Magnini, editors, 2005</marker>
<rawString>I. Dagan, O. Glickman, and B. Magnini, editors. 2005. Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-C de Marneffe</author>
<author>B MacCartney</author>
<author>C D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In International Conference on Language Resources and Evaluation (LREC).</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>M.-C. de Marneffe, B. MacCartney, and C.D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In International Conference on Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: an electronic lexical database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="8310" citStr="Fellbaum, 1998" startWordPosition="1402" endWordPosition="1403"> h neg oddity ATP active to passive PTA passive to active ATS appos to sent RTS rcmod to sent Table 1: Current transformations with their abbr. are auxiliaries (’aux’), determiners (’det’), prepositions (’prep’) and possessives (’poss’). Furthermore, we currently ignore words of those parts of speech (POS) that are not verbs (’VB’), nouns (’NN’), adjectives (’JJ’), adverbs (’RB’), pronouns (’PR’), cardinals (’CD’) or dollar signs (’$’). For all other words wH in H and words wT in T we calculate whether wT can be substituted by wH. For this we employ amongst simple heuristics also WordNet 2.1 (Fellbaum, 1998) as described next: (1) Are the tokens and POS tags of wT and wH identical? If yes, return (1, ’identity’). (2) If the POS tags of wT and wH indicate that both words appear in WordNet continue with (3) otherwise with (8). (3) Are they antonyms in WordNet? If yes, return (2, ’antonym’). (4) Are they synonyms in WordNet? If yes, return (2, ’synonym’). (5) Does wH appear in the hypernym hierarchy of wT in WordNet? If yes, return (z, ’hyponym’) with z being the distance, i.e. wT is a hyponym of wH. (6) Does wT appear in the hypernym hierarchy of wH in WordNet? If yes, return (z, ’hypernym’) with z</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. WordNet: an electronic lexical database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5721" citStr="Klein and Manning, 2003" startWordPosition="967" endWordPosition="971">.1 Preprocessing and parsing For preprocessing we apply the following steps to the text string and the hypothesis string: (1) Remove white space, dots and quotations marks at beginning and end. (2) Remove trailing points of abbreviations. (3) Remove space between names, e.g. ’Pat Smith’ becomes ’PatSmith’. (4) Unify numbers, e.g. resolve ’million’. (5) Unify dates, e.g. ’5 June’ becomes ’June 5’. We then split the text string into sentences simply by splitting at all locations containing a dot followed by a space. The resulting strings are fed to the Stanford Parser (de Marneffe et al., 2006; Klein and Manning, 2003) with its included pretrained model and options ’-retainTmpSubcategories’ and ’-splitTMP 1’. This allows us to generate dependency trees the nodes of which contain a single stemmed word, its part-of-speech tag and its dependency tag (as produced using the parser’s output options ’wordsAndTags’ and ’typedDependencies’, see (de Marneffe et al., 2006)). For the stemming we apply the function ’morphy’ of the NLTK-LITE toolbox (Bird, 2005). If the text string contains more than one sentence, they will be combined to a single dependency tree with a common root node. Let us from now on refer to the d</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C.D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kouylekov</author>
<author>B Magnini</author>
</authors>
<title>Recognizing textual entailment with tree edit distance algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment.</booktitle>
<contexts>
<context position="1503" citStr="Kouylekov and Magnini, 2005" startWordPosition="229" endWordPosition="232">ent recognition asks the question whether a piece of text like The Cassini Spacecraft has taken images from July 22, 2006 that show rivers and lakes present on Saturn’s moon Titan. implies a hypothesis like The Cassini Spacecraft reached Titan. There exists already many interesting approaches to this problem, see (Dagan et al., 2005; Bar-Haim et al., 2006) for various recent efforts and our paper wont try to fully reinvent the wheel. Instead it will present some work in progress that tries to model the probability of entailment in terms of ideas motivated by approaches like the edit-distance (Kouylekov and Magnini, 2005; Kouylekov and Magnini, 2006; Tatu et al., 2006; Adams, 2006). However, instead of defining some distance based on edits, we will generate derivations in some calculus that is able to transform dependency parse trees. The special property of our calculus is that the truth is only preserved with a certain probability along its derivations. This might sound like a disadvantage. However, in commonsense reasoning there is usual a lot of uncertainty due the fact that it is impossible to formalize all world knowledge. We think that probabilities might help us in such situations where it is impossib</context>
</contexts>
<marker>Kouylekov, Magnini, 2005</marker>
<rawString>M. Kouylekov and B. Magnini. 2005. Recognizing textual entailment with tree edit distance algorithms. In Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kouylekov</author>
<author>B Magnini</author>
</authors>
<title>Tree edit distance for recognizing textual entailment: Estimating the cost of insertion.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment.</booktitle>
<contexts>
<context position="1532" citStr="Kouylekov and Magnini, 2006" startWordPosition="233" endWordPosition="236">tion whether a piece of text like The Cassini Spacecraft has taken images from July 22, 2006 that show rivers and lakes present on Saturn’s moon Titan. implies a hypothesis like The Cassini Spacecraft reached Titan. There exists already many interesting approaches to this problem, see (Dagan et al., 2005; Bar-Haim et al., 2006) for various recent efforts and our paper wont try to fully reinvent the wheel. Instead it will present some work in progress that tries to model the probability of entailment in terms of ideas motivated by approaches like the edit-distance (Kouylekov and Magnini, 2005; Kouylekov and Magnini, 2006; Tatu et al., 2006; Adams, 2006). However, instead of defining some distance based on edits, we will generate derivations in some calculus that is able to transform dependency parse trees. The special property of our calculus is that the truth is only preserved with a certain probability along its derivations. This might sound like a disadvantage. However, in commonsense reasoning there is usual a lot of uncertainty due the fact that it is impossible to formalize all world knowledge. We think that probabilities might help us in such situations where it is impossible to include everything into</context>
</contexts>
<marker>Kouylekov, Magnini, 2006</marker>
<rawString>M. Kouylekov and B. Magnini. 2006. Tree edit distance for recognizing textual entailment: Estimating the cost of insertion. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tatu</author>
<author>B Iles</author>
<author>J Slavick</author>
<author>A Novischi</author>
<author>D Moldovan</author>
</authors>
<title>COGEX at the second recognizing textual entailment challenge.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment.</booktitle>
<contexts>
<context position="1551" citStr="Tatu et al., 2006" startWordPosition="237" endWordPosition="240">like The Cassini Spacecraft has taken images from July 22, 2006 that show rivers and lakes present on Saturn’s moon Titan. implies a hypothesis like The Cassini Spacecraft reached Titan. There exists already many interesting approaches to this problem, see (Dagan et al., 2005; Bar-Haim et al., 2006) for various recent efforts and our paper wont try to fully reinvent the wheel. Instead it will present some work in progress that tries to model the probability of entailment in terms of ideas motivated by approaches like the edit-distance (Kouylekov and Magnini, 2005; Kouylekov and Magnini, 2006; Tatu et al., 2006; Adams, 2006). However, instead of defining some distance based on edits, we will generate derivations in some calculus that is able to transform dependency parse trees. The special property of our calculus is that the truth is only preserved with a certain probability along its derivations. This might sound like a disadvantage. However, in commonsense reasoning there is usual a lot of uncertainty due the fact that it is impossible to formalize all world knowledge. We think that probabilities might help us in such situations where it is impossible to include everything into the model, but in </context>
</contexts>
<marker>Tatu, Iles, Slavick, Novischi, Moldovan, 2006</marker>
<rawString>M. Tatu, B. Iles, J. Slavick, A. Novischi, and D. Moldovan. 2006. COGEX at the second recognizing textual entailment challenge. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>