<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.044606">
<title confidence="0.988165">
Semantic Role Labeling using libSVM
</title>
<author confidence="0.975157">
Necati Ercan Ozgencil Nancy McCracken
</author>
<affiliation confidence="0.985449666666667">
Center for Natural Language Processing Center for Natural Language Processing
School of Engineering and Computer Science School of Information Studies
Syracuse University Syracuse University
</affiliation>
<email confidence="0.996941">
neozgenc@ecs.syr.edu njm@ecs.syr.edu
</email>
<sectionHeader confidence="0.995601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998355625">
We describe a system for the CoNLL-
2005 shared task of Semantic Role Label-
ing. The system implements a two-layer
architecture to first identify the arguments
and then to label them for each predicate.
The components are implemented as
SVM classifiers using libSVM. Features
were adapted and tuned for the system,
including a reduced set for the identifier
classifier. Experiments were conducted to
find kernel parameters for the Radial Ba-
sis Function (RBF) kernel. An algorithm
was defined to combine the results of the
argument labeling classifier according to
the constraints of the argument labeling
problem.
</bodyText>
<sectionHeader confidence="0.97842" genericHeader="categories and subject descriptors">
1 Introduction and Strategy
</sectionHeader>
<bodyText confidence="0.99927084375">
The Semantic Role Labeling (SRL) problem has
been the topic of the both the CoNLL-2004 and the
CoNLL-2005 Shared Tasks (Carreras and
Màrquez, 2005). The SRL system described here
depends on a full syntactic parse from the Charniak
parser, and investigates aspects of using Support
Vector Machines (SVMs) as the machine learning
technique for the SRL problem, using the libSVM
package.
In common with many other systems, this sys-
tem uses the two-level strategy of first identifying
which phrases can be arguments to predicates in
general, and then labeling the arguments according
to that predicate. The argument identification
phase is a binary classifier that decides whether
each constituent in the full syntax tree of the sen-
tence is a potential argument. These potential ar-
guments are passed into the argument labeling
classifier, which uses binary classifiers for each
label to decide if that label should be given to that
argument. A post-processing phase picks the best
labeling that satisfies the constraints of labeling the
predicate arguments.
For overall classification strategy and for
suggestions of features, we are indebted to the
work of Pradhan et al (2005) and to the work of
many authors in both the CoNLL-2004 shared task
and the similar semantic roles task of Senseval-3.
We used the results of their experiments with
features, and worked primarily on features for the
identifying classifier and with the constraint
satisfaction problem on the final argument output.
</bodyText>
<sectionHeader confidence="0.986816" genericHeader="method">
2 System Description
</sectionHeader>
<subsectionHeader confidence="0.868681">
2.1 Input Data
</subsectionHeader>
<bodyText confidence="0.999901857142857">
In this system, we chose to use full syntax trees
from the Charniak parser, as the constituents of
those trees more accurately represented argument
phrases in the training data at the time of the data
release. Within each sentence, we first map the
predicate to a constituent in the syntax tree. In the
cases that the predicate is not represented by a con-
stituent, we found that these were verb phrases of
length two or more, where the first word was the
main verb (carry out, gotten away, served up, etc.).
In these cases, we used the first word constituent as
the representation of the predicate, for purposes of
computing other features that depended on a rela-
tive position in the syntax tree.
</bodyText>
<page confidence="0.982872">
205
</page>
<note confidence="0.595535">
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 205–208, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<bodyText confidence="0.99990645">
We next identify every constituent in the tree as
a potential argument, and label the training data
accordingly. Although approximately 97% of the
arguments in the training data directly matched
constituents in the Charniak tree, only 91.3% of the
arguments in the development set match constitu-
ents. Examination of the sentences with incorrect
parses show that almost all of these are due to
some form of incorrect attachment, e.g. preposi-
tional attachment, of the parser. Heuristics can be
derived to correct constituents with quotes, but this
only affected a small fraction of a percent of the
incorrect arguments. Experiments with corrections
to the punctuation in the Collins parses were also
unsuccessful in identifying additional constituents.
Our recall results on the development directory are
bounded by the 91.3% alignment figure.
We also did not use the the partial syntax,
named entities or the verb senses in the
development data.
</bodyText>
<subsectionHeader confidence="0.999795">
2.2 Learning Components: SVM classifiers
</subsectionHeader>
<bodyText confidence="0.999742380952381">
For our system, we chose to use libSVM, an open
source SVM package (Chang and Lin, 2001).
In the SRL problem, the features are nominal,
and we followed the standard practice of represent-
ing a nominal feature with n discrete values as n
binary features. Many of the features in the SRL
problem can take on a large number of values, for
example, the head word of a constituent may take
on as many values as there are different words pre-
sent in the training set, and these large number of
features can cause substantial performance issues.
The libSVM package has several kernel func-
tions available, and we chose to use the radial basis
functions (RBF). For the argument labeling prob-
lem, we used the binary classifiers in libSVM, with
probability estimates of how well the label fits the
distribution. These are normally combined using
the “one-against-one” approach into a multi-class
classifier. Instead, we combined the binary classi-
fiers in our own post-processing phase to get a la-
beling satisfying the constraints of the problem.
</bodyText>
<subsectionHeader confidence="0.999425">
2.3 The Identifier Classifier Features
</subsectionHeader>
<bodyText confidence="0.999184129032258">
One aspect of our work was to use fewer features
for the identifier classifier than the basic feature set
from (Crildea and Jurafsky, 2002). The intuition
behind the reduction is that whether a constituent
in the tree is an argument depends primarily on the
structure and is independent of the lexical items of
the predicate and headword. This reduced feature
set is:
Phrase Type: The phrase label of the argument.
Position: Whether the phrase is before or after
the predicate.
Voice: Whether the predicate is in active or
passive voice. Passive voice is recognized if a past
participle verb is preceded by a form of the verb
“be” within 3 words.
Sub-categorization: The phrase labels of the
children of the predicate’s parent in the syntax tree.
Short Path: The path from the parent of the
argument position in the syntax tree to the parent
of the predicate.
The first four features are standard, and the short
path feature is defined as a shorter version of the
standard path feature that does not use the
argument phrase type on one end of the path, nor
the predicate type on the other end.
The use of this reduced set of features was
confirmed experimentally by comparing the effect
of this reduced feature set on the F-measure of the
identifier classifier, compared to feature sets that
also added the predicate, the head word and the
path features, as normally defined.
</bodyText>
<subsectionHeader confidence="0.99581">
2.4 Using the Identifier Classifier for Train-
ing and Testing
</subsectionHeader>
<bodyText confidence="0.999918625">
Theoretically, the input for training the identifier
classifier is that, for each predicate, all constituents
in the syntax tree are training instances, labeled
true if it is any argument of that predicate, and
false otherwise. However, this leads to too many
negative (false) instances for the training. To cor-
rect this, we experimented with two filters for
negative instances. The first filter is simply a ran-
dom filter; we randomly select a percentage of ar-
guments for each argument label. Experiments
with the percentage showed that 30% yielded the
best F-measure for the identifier classifier.
The second filter is based on phrase labels from
the syntax tree. The intent of this filter was to re-
move one word constituents of a phrase type that
was never used. We selected only those phrase
</bodyText>
<figure confidence="0.8538385">
Reduced + Pred + Head + Path
F-measure 81.51 81.31 72.60 81.19
</figure>
<tableCaption confidence="0.881669">
Table 1: Additional features reduce F-measure for the
identifier classifier.
</tableCaption>
<page confidence="0.994059">
206
</page>
<bodyText confidence="0.999942473684211">
labels whose frequency in the training was higher
than a threshold. Experiments showed that the best
threshold was 0.01, which resulted in approxi-
mately 86% negative training instances.
However, in the final experimentation, compari-
son of these two filters showed that the random
filter was best for F-measure results of the identi-
fier classifier.
The final set of experiments for the identifier
classifier was to fine tune the RBF kernel training
parameters, C and gamma. Although we followed
the standard grid strategy of finding the best pa-
rameters, unlike the built-in grid program of
libSVM with its accuracy measure, we judged the
results based on the more standard F-measure of
the classifier. The final values are that C = 2 and
gamma = 0.125.
The final result of the identifier classifier trained
on the first 10 directories of the training set is:
</bodyText>
<table confidence="0.58334075">
Precision: 78.27% Recall: 89.01%
(F-measure: 83.47)
Training on more directories did not substan-
tially improve these precision and recall figures.
</table>
<subsectionHeader confidence="0.995519">
2.5 Labeling Classifier Features
</subsectionHeader>
<bodyText confidence="0.999496235294118">
The following is a list of the features used in the
labeling classifiers.
Predicate: The predicate lemma from the
training file.
Path: The syntactic path through the parse tree
from the argument constituent to the predicate.
Head Word: The head word of the argument
constituent, calculated in the standard way, but
also stemmed. Applying stemming reduces the
number of unique values of this feature
substantially, 62% in one directory of training data.
Phrase Type, Position, Voice, and Sub-
categorization: as in the identifier classifier.
In addition, we experimented with the following
features, but did not find that they increased the
labeling classifier scores.
Head Word POS: the part of speech tag of the
head word of the argument constituent.
Temporal Cue Words: These words were
compiled by hand from ArgM-TMP phrases in the
training data.
Governing Category: The phrase label of the
parent of the argument.
Grammatical Rule: The generalization of the
subcategorization feature to show the phrase labels
of the children of the node that is the lowest parent
of all arguments of the predicate.
In the case of the temporal cue words, we
noticed that using our definition of this feature
increased the number of false positives for the
ARGM-TMP label; we guess that our temporal cue
words included too many words that occured in
other labels. Due to lack of time, we were not
able to more fully pursue these features.
</bodyText>
<subsectionHeader confidence="0.9209105">
2.6 Using the Labeling Classifier for Train-
ing and Testing
</subsectionHeader>
<bodyText confidence="0.999988272727273">
Our strategy for using the labeling classifier is
that in the testing, we pass only those arguments to
the labeling classifier that have been marked as
true by the identifier classifier. Therefore, for
training the labeling classifier, instances were con-
stituents that were given argument labels in the
training set, i.e. there were no “null” training ex-
amples.
For the labeling classifier, we also found the
best parameters for the RBF kernel of the classi-
fier. For this, we used the grid program of libSVM
that uses the multi-class classifier, using the accu-
racy measure to tune the parameters, since this
combines the precision of the binary classifiers for
each label. The final values are that C = 0.5 and
gamma = 0.5.
In order to show the contribution of the labeling
classifier to the entire system, a final test was done
on the development set, but passing it the correct
arguments. We tested this with a labeling classi-
fier trained on 10 directories and one trained on 20
directories, showing the final F-measure:
</bodyText>
<sectionHeader confidence="0.9780745" genericHeader="method">
10 directories: 83.27
20 directories: 84.51
</sectionHeader>
<subsectionHeader confidence="0.997635">
2.7 Post-processing the classifier labels
</subsectionHeader>
<bodyText confidence="0.9999141">
The final part of our system was to use the results
of the binary classifiers for each argument label to
produce a final labeling subject to the labeling con-
straints.
For each predicate, the constraints are: two con-
stituents cannot have the same argument label, a
constituent cannot have more than one label, if two
constituents have (different) labels, they cannot
have any overlap, and finally, no argument can
overlap the predicate.
</bodyText>
<page confidence="0.995197">
207
</page>
<table confidence="0.9999762">
Precision Recall F0=1
Development 73.57% 71.87% 72.71
Test WSJ 74.66% 74.21% 74.44
Test Brown 65.52% 62.93% 64.20
Test WSJ+Brown 73.48% 72.70% 73.09
</table>
<tableCaption confidence="0.920651">
Table 2: Overall results (top) and detailed results on the
WSJ test (bottom).
</tableCaption>
<bodyText confidence="0.999963466666667">
To achieve these constraints, we used the prob-
abilities produced by libSVM for each of the bi-
nary argument label classifiers. We produced a
constraint satisfaction module that uses a greedy
algorithm that uses probabilities from the matrix of
potential labeling for each constituent and label.
The algorithm iteratively chooses a label for a node
with the highest probability and removes any po-
tential labeling that would violate constraints with
that chosen label. It continues to choose labels for
nodes until all probabilities in the matrix are lower
than a threshold, determined by experiments to be
.3. In the future, it is our intent to replace this
greedy algorithm with a dynamic optimization al-
gorithm.
</bodyText>
<sectionHeader confidence="0.980126" genericHeader="conclusions">
3 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.999957">
3.1 Final System and Results
</subsectionHeader>
<bodyText confidence="0.999795454545454">
The final system used an identifier classifier
trained on (the first) 10 directories, in approxi-
mately 7 hours, and a labeling classifier trained on
20 directories, in approximately 23 hours. Testing
took approximately 3.3 seconds per sentence.
As a further test of the final system, we trained
both the identifier classifier and the labeling classi-
fier on the first 10 directories and used the second
10 directories as development tests. Here are some
of the results, showing the alignment and F-
measure on each directory, compared to 24.
</bodyText>
<table confidence="0.998336333333333">
Directory: 12 14 16 18 20 24
Alignment 95.7 96.1 95.9 96.5 95.9 91.3
F-measure 80.4 79.6 79.0 80.5 79.7 71.1
</table>
<tableCaption confidence="0.999901">
Table 3: Using additional directories for testing
</tableCaption>
<bodyText confidence="0.999897166666667">
Finally, we note that we did not correctly antici-
pate the final notation for the predicates in the test
set for two word verbs. Our system assumed that
two word verbs would be given a start and an end,
whereas the test set gives just the one word predi-
cate.
</bodyText>
<sectionHeader confidence="0.97154" genericHeader="references">
References
</sectionHeader>
<subsectionHeader confidence="0.9464265">
Xavier Carreras and Lluìs Màrquez, 2005. Introduction
to the CoNLL-2005 Shared Task: Semantic Role
Labeling, Proceedings of CoNLL-2005.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM :
</subsectionHeader>
<tableCaption confidence="0.723078">
a library for support vector machines. Software
available at http://www.csie.ntu.edu.tw/~cjlin/libsvm
</tableCaption>
<reference confidence="0.954131555555556">
Daniel Gildea and Daniel Jurafsky, 2002. Automatic
Labeling of Semantic Roles. Computational Linguis-
tics 28(3):245-288.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James H. Martin, and Daniel Jurafsky,
2005. Support Vector Learning for Semantic Argu-
ment Classification, To appear in Machine Learning
journal, Special issue on Speech and Natural Lan-
guage Processing.
</reference>
<figure confidence="0.999555848484848">
Precision
74.66%
83.59%
77.00%
66.97%
66.88%
77.66%
80.00%
55.13%
52.17%
27.43%
73.04%
57.69%
50.00%
54.00%
92.02%
96.05%
35.07%
50.00%
0.00%
68.69%
77.61%
71.95%
87.50%
0.00%
0.00%
0.00%
100.00%
0.00%
66.67%
8.33%
66.67%
Recall
74.21%
74.35%
71.57%
49.32%
72.81%
46.88%
49.59%
40.87%
75.64%
43.75%
66.85%
60.69%
50.99%
56.47%
54.94%
94.19%
95.22%
20.00%
63.57%
50.00%
97.32%
85.07%
80.00%
89.73%
85.71%
88.46%
16.67%
0.00%
0.00%
0.00%
0.00%
0.00%
F0=1
74.44
75.65
74.49
72.93
49.79
73.75
75.00
76.03
66.91
63.64
52.98
50.70
36.92
51.72
54.47
93.09
95.63
37.75
28.57
66.03
58.33
66.67
97.32
84.32
80.00
83.23
11.11
0.00
0.00
0.00
0.00
0.00
Test WSJ
Overall
A0
A1
A2
A3
A4
A5
AM-ADV
AM-CAU
AM-DIR
AM-DIS
AM-EXT
AM-LOC
AM-MNR
AM-MOD
AM-NEG
AM-PNC
AM-PRD
AM-REC
AM-TMP
R-A0
R-A1
R-A2
R-A3
R-A4
R-AM-ADV
R-AM-CAU
R-AM-EXT
R-AM-LOC
R-AM-MNR
R-AM-TMP
97.32%
V
</figure>
<page confidence="0.953933">
208
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.947735">
<title confidence="0.999493">Semantic Role Labeling using libSVM</title>
<author confidence="0.998726">Necati Ercan Ozgencil Nancy McCracken</author>
<affiliation confidence="0.998517333333333">Center for Natural Language Processing Center for Natural Language Processing School of Engineering and Computer Science School of Information Studies Syracuse University Syracuse University</affiliation>
<abstract confidence="0.997136705882353">We describe a system for the CoNLL- 2005 shared task of Semantic Role Labeling. The system implements a two-layer architecture to first identify the arguments and then to label them for each predicate. The components are implemented as SVM classifiers using libSVM. Features were adapted and tuned for the system, including a reduced set for the identifier classifier. Experiments were conducted to find kernel parameters for the Radial Basis Function (RBF) kernel. An algorithm was defined to combine the results of the argument labeling classifier according to the constraints of the argument labeling problem.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic Labeling of Semantic Roles.</title>
<date>2002</date>
<journal>Computational Linguistics</journal>
<pages>28--3</pages>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky, 2002. Automatic Labeling of Semantic Roles. Computational Linguistics 28(3):245-288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Kadri Hacioglu</author>
<author>Valerie Krugler</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Support Vector Learning for Semantic Argument Classification,</title>
<date>2005</date>
<booktitle>Machine Learning journal, Special issue on Speech and Natural Language Processing.</booktitle>
<note>To appear in</note>
<contexts>
<context position="2124" citStr="Pradhan et al (2005)" startWordPosition="319" endWordPosition="322">g the arguments according to that predicate. The argument identification phase is a binary classifier that decides whether each constituent in the full syntax tree of the sentence is a potential argument. These potential arguments are passed into the argument labeling classifier, which uses binary classifiers for each label to decide if that label should be given to that argument. A post-processing phase picks the best labeling that satisfies the constraints of labeling the predicate arguments. For overall classification strategy and for suggestions of features, we are indebted to the work of Pradhan et al (2005) and to the work of many authors in both the CoNLL-2004 shared task and the similar semantic roles task of Senseval-3. We used the results of their experiments with features, and worked primarily on features for the identifying classifier and with the constraint satisfaction problem on the final argument output. 2 System Description 2.1 Input Data In this system, we chose to use full syntax trees from the Charniak parser, as the constituents of those trees more accurately represented argument phrases in the training data at the time of the data release. Within each sentence, we first map the p</context>
</contexts>
<marker>Pradhan, Hacioglu, Krugler, Ward, Martin, Jurafsky, 2005</marker>
<rawString>Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, Wayne Ward, James H. Martin, and Daniel Jurafsky, 2005. Support Vector Learning for Semantic Argument Classification, To appear in Machine Learning journal, Special issue on Speech and Natural Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>