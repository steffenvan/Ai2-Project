<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000668">
<title confidence="0.976582">
Discriminative Learning of Syntactic and Semantic Dependencies
</title>
<author confidence="0.999033">
Lu Li1, Shixi Fan2, Xuan Wang1, Xiaolong Wang1
</author>
<affiliation confidence="0.992142">
Shenzhen Graduate School, Harbin Institute of Technology,
</affiliation>
<address confidence="0.842972">
Xili, Shenzhen 518055, China
</address>
<email confidence="0.9776175">
1{lli,wangxuan,wangxl}@insun.hit.edu.cn
2fanshixi@hit.edu.cn
</email>
<sectionHeader confidence="0.993659" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999606714285714">
A Maximum Entropy Model based system
for discriminative learning of syntactic and
semantic dependencies submitted to the
CoNLL-2008 shared task (Surdeanu, et al.,
2008) is presented in this paper. The sys-
tem converts the dependency learning task
to classification issues and reconstructs the
dependent relations based on classification
results. Finally F1 scores of 86.69, 69.95
and 78.35 are obtained for syntactic depen-
dencies, semantic dependencies and the
whole system respectively in closed chal-
lenge. For open challenge the correspond-
ing F1 scores are 86.69, 68.99 and 77.84.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999331470588235">
Given sentences and corresponding part-of-speech
of each word, the learning of syntactic and seman-
tic dependency contains two separable goals: (1)
building a dependency tree that defines the syn-
tactic dependency relationships between separated
words; (2) specifying predicates (no matter verbs
or nouns) of the sentences and labeling the seman-
tic dependents for each predicate.
In this paper a discriminative parser is pro-
posed to implement maximum entropy (ME) mod-
els (Berger, et al., 1996) to address the learning
task. The system is divided into two main subsys-
tems: syntactic dependency parsing and semantic
dependency labeling. The former is used to find a
well-formed syntactic dependency tree that occu-
pies all the words in the sentence. If edges are
added between any two words, a full-connected
</bodyText>
<footnote confidence="0.90317675">
© 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
</footnote>
<bodyText confidence="0.9991752">
graph is constructed and the dependency tree could
be found using a maximum spanning tree (MST)
algorithm (McDonald, et al., 2005). The latter fo-
cuses on separable predicates whose semantic de-
pendents could be determined using classification
tools, such as ME models 1 etc..
We participated in both closed and open chal-
lenge of the CoNLL-2008 shared task (Surdeanu,
et al., 2008). Results are reported on both develop-
ment and test sets in this paper.
</bodyText>
<sectionHeader confidence="0.963841" genericHeader="method">
2 System Description
</sectionHeader>
<subsectionHeader confidence="0.98661">
2.1 Syntactic Parsing
</subsectionHeader>
<bodyText confidence="0.999968705882353">
The goal of syntactic parsing is to create a la-
beled syntactic dependency parse y for input sen-
tence x including words and their parts of speech
(POS). Inspired by the parsing model that imple-
ments maximum spanning tree (MST) algorithm
to induce the dependency parsing tree (McDonald,
et al., 2005), the system employs the same frame-
work. The incorporated features are defined over
parts of speech of words occurring between and
around a possible head-dependent relation.
Suppose G = (V, E) is a directed graph, where
V is the set of vertices denoting the words in sen-
tence x and E is the set of directed edges between
any two vertices with some scores. The MST al-
gorithm is to find the most probable subgraph of G
that satisfies tree constraints over all vertices. The
score function of the parsing tree y is defined as
</bodyText>
<equation confidence="0.962873">
s(y) � � s(i, j) (1)
(i,j)Ey
</equation>
<bodyText confidence="0.99481">
where (i, j) E y indicates an edge in y from word
i to word j and s(i, j) denotes its score. Suppose Y
</bodyText>
<footnote confidence="0.44523375">
1http://homepages.inf.ed.ac.uk/s0450736/maxent.html
218
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 218–222
Manchester, August 2008
</footnote>
<table confidence="0.985641">
wi wj
pi pj
(wi,pi) (wj,pj)
(wi, wj) (pi, pj)
(wi, pj) (wj, pi)
(wi,wj,pi) (wi,wj,pj)
(pi, pj, wi) (pi, pj, wj)
(wi,wj,pi,pj) (pi, pk, pj),i &lt; k &lt; j
(pi,pi+1,pj−1,pj) (pi−1,pi,pj−1,pj)
(pi, pi+1, pj, pj+1) (pi−1, pi, pj, pj+1)
</table>
<tableCaption confidence="0.547119">
Table 1: Features for syntactic parsing.
</tableCaption>
<table confidence="0.9988241">
wi pi
pi−1 pi+1
(pi−1,pi) (pi, pi+1)
(pi−2, pi) (pi, pi+2)
(pi−3, pi) (pi,pi+3)
(pi−1,pi,pi+1) (wi,pi)
(wi, pi−1,pi) (wi, pi, pi+1)
(wi, pi−2, pi) (wi, pi, pi+2)
(wi, pi−3, pi) (wi, pi, pi+3)
(wi, pi−1, pi, pi+1)
</table>
<tableCaption confidence="0.998197">
Table 2: Features used for predicate tagging.
</tableCaption>
<bodyText confidence="0.9958915">
is the set of syntactic dependency labels, the score
function of edges is defined as
</bodyText>
<equation confidence="0.997039">
s(i,j) = max,∈YPr(l|x, i, j) (2)
</equation>
<bodyText confidence="0.998171285714286">
ME models are used to calculate the value of
Pr(l|x, i, j), where the features are extracted from
input sentence x. Given i and j as the subscripts
of words in the sentence and word i is the parent
of word j, the features can be illustrated in table
1. wi and pi are denoted as the ith word and the
ith part of speech respectively in the sentence. The
tuples define integrated features, such as (wi,pi)
indicates the feature combining the ith word and
ith part of speech. Besides these features, the dis-
tant between word i and word j in sentence x is
considered as a single feature. The distant is also
combined with features in table 1 to produce com-
plex features.
</bodyText>
<subsectionHeader confidence="0.999388">
2.2 Semantic Dependency Labeling
</subsectionHeader>
<bodyText confidence="0.999967555555556">
Semantic dependencies are always concerning
with specific predicates. Unlike syntactic depen-
dencies, semantic dependency relationships usu-
ally can not be represented as a tree. Thus, the
method used for semantic dependency labeling
is somewhat different from syntactic dependency
parsing. The work of semantic labeling can be di-
vided into two stages: predicate tagging and de-
pendents recognizing.
</bodyText>
<subsubsectionHeader confidence="0.600696">
2.2.1 Predicate Tagging
</subsubsectionHeader>
<bodyText confidence="0.999750666666667">
According to PropBank (Palmer, et al., 2005)
and NomBank (Meyers, et al., 2004), predicates
usually have several rolesets corresponding to dif-
ferent meanings. For example, the verb abandon
has three rolesets marked as ordinal numbers 01,
02 and 03 as described below.
</bodyText>
<equation confidence="0.753368066666667">
&lt;frameset&gt;
&lt;predicate lemma=“abandon”&gt;
&lt;roleset id=“abandon.01” name=“leave
behind” vncls=“51.2”&gt;
. . .
&lt;/roleset&gt;
&lt;roleset id=“abandon.02”
name=“exchange” vncls=“51.2”&gt;
id=“abandon.03”
name=“surrender, give over” vncls=“-
”&gt;
. . .
&lt;/roleset&gt;
&lt;/predicate&gt;
&lt;/frameset&gt;
</equation>
<bodyText confidence="0.99834547368421">
The goal of this part is to identify the predicates
in the sentences and to determine the roleset for
each of them. It should be cleared that the ordi-
nal numbers are only used to distinguish different
meanings of a predicate. However, if these num-
bers are treated as tags for predicates, some statisti-
cal properties will be obtained as illustrated in Fig-
ure 1. As can be seen, the distribution of the train
data would be quite informative for representing
the distribution of other three data sets. Based on
this idea, a classification framework is introduced
for predicate tagging.
Suppose the tag set is chosen to be T =
101, 02,..., 221 according to the horizontal axis of
Figure 1 and 00 is added to indicate that the ex-
amining word is not a predicate. Suppose ti is a
variable indicating the tag of word at position i in
sentences x. ME models are implemented to tag
the predicates.
</bodyText>
<equation confidence="0.67711175">
ti = argmaxt∈ TPr(tlx, i) (3)
. . .
&lt;/roleset&gt;
&lt;roleset
</equation>
<page confidence="0.953653">
219
</page>
<figureCaption confidence="0.798339">
Figure 1: Distribution of the ordinal numbers of
predicates on different data sets. 01 - 21 are at-
tached with the predicates in the corpus and 22
stands for ‘SU’.
</figureCaption>
<bodyText confidence="0.999713">
The features for predicate tagging are listed in ta-
ble 2, where the symbols share the same mean-
ing as in table 1. Experiments show that this pure
statistic processing method is effective for predi-
cate tagging.
</bodyText>
<subsubsectionHeader confidence="0.771228">
2.2.2 Dependents Recognizing
</subsubsectionHeader>
<bodyText confidence="0.999978666666667">
This subtask depends deeply on the results of
syntactic parsing and predicate tagging described
earlier in the system. Predicate tagging identifies
central words and syntactic parsing provides syn-
tactic features for its dependents identification and
classification.
Generally speaking, given a specific predicate in
a sentence, only a few of words are associated as its
semantic dependents. By statistical analysis a list
of part of speech tuples that are appearing to be se-
mantic dependency are collected. All other tuples
are filtered out to improve system performance.
Suppose (p, d) is a couple of predicate and one
of its possible dependents, T is the dependency
tree generated by syntactic parsing, L is the set of
semantic dependency labels. The dependents can
be recognized by using a classification model, ME
models are chosen as before.
</bodyText>
<equation confidence="0.998159">
l(p,d) = argmaxjcLPr(l|p, d, T) (4)
</equation>
<bodyText confidence="0.996784375">
Besides the semantic dependency labels, null is in-
cluded as a special tag to indicate that there is no
semantic dependency between p and d. As a result,
dependents identification (binary classification)
and dependents tagging (multi-classification) can
be solved together within one multi-classification
framework.
The selected features are listed below.
</bodyText>
<listItem confidence="0.999241783783784">
1. Predicate Features
• Lemma and POS of predicate, pred-
icate’s parent in syntactic dependency
tree.
• Voice active or passive.
• Syntactic dependency label of edge be-
tween predicate and its parent.
• POS framework POS list of predicate’s
siblings, POS list of predicate’s children.
• Syntactic dependency framework syn-
tactic dependency label list of the edges
between predicate’s parent and its sib-
lings.
• Parent framework syntactic depen-
dency label list of edges connecting to
predicate’s parent.
2. Dependent Features
• Lemma and POS of dependent, depen-
dent’s parent.
• POS framework POS list of depen-
dent’s siblings.
• Number of children of dependent’s par-
ent.
3. In Between Features
• Position of dependent according to
predicate: before or after.
• POS pair of predicate and dependent.
• Family relation between predicate and
dependent: ancestor or descendant.
• Path length between predicate and de-
pendent.
• Path POS POS list of all words appear-
ing on the path from predicate to depen-
dent.
• Path syntactic dependency label list of
dependency label of edges of path be-
tween predicate and dependent.
</listItem>
<sectionHeader confidence="0.991846" genericHeader="method">
3 Experiment results
</sectionHeader>
<bodyText confidence="0.9972624">
The classification models were trained using all the
training data. The detailed information are shown
in table 3. All experiments ran on 32-bit Intel(R)
Pentium(R) D CPU 3.00GHz processors with 2.0G
memory.
</bodyText>
<figure confidence="0.988740384615385">
12
train
devel
brown
wsj
8
6
4
2
0
0 5 10 15 20
Ordinal Numbers of Predicates
10
</figure>
<page confidence="0.983977">
220
</page>
<table confidence="0.82043825">
Feature Number Training Time
Syn. 7,488,533 30h
Prd. 1,484,398 8h
Sem. 3,588,514 12h
</table>
<tableCaption confidence="0.965406666666667">
Table 3: Details of ME models. Syn. is for syntac-
tic parsing, Prd. is for predicate tagging and Sem.
is for semantic dependents recognizing.
</tableCaption>
<table confidence="0.988663666666667">
Syntactic Semantic Overall
devel 85.29 69.60 77.49
brown 80.80 59.17 70.01
wsj 87.42 71.27 79.38
brown+wsj 86.69 69.95 78.35
(a) Closed Challenge
Syntactic Semantic Overall
devel 85.29 68.45 76.87
brown 80.80 58.22 69.51
wsj 87.42 70.32 78.87
brown+wsj 86.69 68.99 77.84
(b) Open Challenge
</table>
<tableCaption confidence="0.978059">
Table 4: Scores for joint learning of syntactic and
semantic dependencies.
</tableCaption>
<subsectionHeader confidence="0.999075">
3.1 Closed Challenge
</subsectionHeader>
<bodyText confidence="0.999916266666667">
The system for closed challenge is designed as a
two-stage parser: syntactic parsing and semantic
dependency labeling as described previously. Ta-
ble 4(a) shows the results on different corpus. As
shown in table 4(a), the scores of semantic depen-
dency labeling are quite low, that are influencing
the overall scores. The reason could be inferred
from the description in section 2.2.2 since seman-
tic dependent labeling inherits the errors from the
output of syntactic parsing and predicate tagging.
Following evaluates each part independently.
Besides the multiple classification model de-
scribed in table 3, a binary classification model
was built based on ME for predicate tagging. The
binary model can’t distinguish different rolesets of
predicate, but can identify which words are predi-
cates in sentences. The precision and recall for bi-
nary model are 90.80 and 88.87 respectively, while
for multiple model, the values are 84.60 and 85.60.
For semantic dependent labeling, experiments
were performed under conditions that the gold syn-
tactic dependency tree and predicates list were
given as input. The semantic scores became 80.09,
77.08 and 82.25 for devel, brown and wsj respec-
tively. This implies that the error of syntactic pars-
ing and predicate tagging could be probably aug-
mented in semantic dependent labeling. In order to
improve the performance of the whole system, the
deep dependence between the two stages should be
broken up in future research.
</bodyText>
<subsectionHeader confidence="0.99794">
3.2 Open Challenge
</subsectionHeader>
<bodyText confidence="0.9999794">
In open challenge, the same models are used for
syntactic parsing and predicate tagging as in closed
challenge and two other models are trained for se-
mantic dependent labeling. Suppose Mmst, Mmalt
and Mchunk are denoted as these three semantic
models, where Mmst is the model used in closed
challenge, Mmalt is trained on the syntactic de-
pendency tree provided by the open corpus with
the same feature set as Mmst, and Mchunk is
trained using features extracted from name entity
and wordnet super senses results provided by the
open corpus.
Considering a possible dependent given a spe-
cific predicate, the feature set used for Mchunk
contains only six elements:
</bodyText>
<listItem confidence="0.999886444444444">
• Whether the dependent is in name entity
chunk: True or False.
• Name entity label of the dependent.
• Whether the dependent is in BBN name entity
chunk: True or False.
• BBN name entity label of the dependent.
• Whether the dependent is in wordnet super
sense chunk: True or False.
• Wordnet super sense label of the dependent.
</listItem>
<bodyText confidence="0.999971625">
After implementing these three models on se-
mantic dependents recognizing, the results were
merged to generate the scores described in table
4(b).
The merging strategy is quite simple. Given a
couple of predicate and dependent (p, d), the sys-
tem produces three semantic dependency labels
denoting as lmst, lmalt and lchunk, the result la-
bel is chosen to be most frequent semantic label
among the three.
Comparing the scores of open challenge and
closed challenge, it can be found that the score of
the former is less than the latter, which is quite
strange since more resources were used in open
challenge. To examine the influences of differ-
ent semantic dependents recognizing models, each
</bodyText>
<page confidence="0.992525">
221
</page>
<table confidence="0.99632125">
Mmst Mmalt Mchunk
devel 69.60 64.48 41.72
brown 59.17 56.52 34.04
wsj 71.27 66.40 41.83
</table>
<tableCaption confidence="0.999451">
Table 5: Semantic scores of different models.
</tableCaption>
<bodyText confidence="0.999872833333333">
model was implemented in the closed challenge
and the results are shown in table 5. Specially,
model Mchunk generated too low scores and gave a
heavy negative influence on the final results. Find-
ing a good way to combine several results requires
further research.
</bodyText>
<sectionHeader confidence="0.999873" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999989615384615">
This paper have presented a simple discriminative
system submitted to the CoNLL-2008 shared task
to address the learning task of syntactic and se-
mantic dependencies. The system was divided into
syntactic parsing and semantic dependents label-
ing. Maximum spanning tree was used to find
a syntactic dependency tree in the full-connected
graph constructed over the words of a sentence.
Maximum entropy models were implemented to
classify syntactic dependency edges, predicates
and their semantic dependents. A brief analysis
has also been given on the results of both closed
challenge and open challenge.
</bodyText>
<sectionHeader confidence="0.97999" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9964088">
This research has been partially supported by the
National Natural Science Foundation of China
(No. 60435020 and No. 90612005), the Goal-
oriented Lessons from the National 863 Program
of China (No.2006AA01Z197) and Project of Mi-
crosoft Research Asia. We would like to thank
Zhixin Hao, Xiao Xin, Languang He and Tao Qian
for their wise suggestion and great help. Thanks
also to Muhammad Waqas Anwar for English im-
provement.
</bodyText>
<sectionHeader confidence="0.999346" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999682260869565">
Adam Berger, Stephen Della Pietra, Vincent Della
Pietra 1996. A Maximum Entropy Approach to Nat-
ural Language Processing. Computational Linguis-
tics, 22(1):39-71.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young
and Ralph Grishman 2004. The NomBank Project:
An Interim Report HLT-NAACL 2004 Workshop:
Frontiers in Corpus Annotation, 24-31.
Martha Palmer, Daniel Gildea, Paul Kingsbury 2005.
The Proposition Bank: An Annotated Corpus of Se-
mantic Roles Computational Linguistics, 31(1):71-
106.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Lluis M`arquez and Joakim Nivre 2008. The
CoNLL-2008 Shared Task on Joint Parsing of Syn-
tactic and Semantic Dependencies. Proceedings of
the 12th Conference on Computational Natural Lan-
guage Learning (CoNLL-2008)
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajiˇc 2005. Non-projective Dependency Pars-
ing using Spanning Tree Algorithms. Proceedings of
HLT/EMNLP, 523-530.
</reference>
<page confidence="0.997997">
222
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.635761">
<title confidence="0.99985">Discriminative Learning of Syntactic and Semantic Dependencies</title>
<author confidence="0.988673">Shixi Xuan Xiaolong</author>
<affiliation confidence="0.937058">Shenzhen Graduate School, Harbin Institute of</affiliation>
<address confidence="0.890468">Xili, Shenzhen 518055,</address>
<abstract confidence="0.981810466666667">A Maximum Entropy Model based system for discriminative learning of syntactic and semantic dependencies submitted to the CoNLL-2008 shared task (Surdeanu, et al., 2008) is presented in this paper. The system converts the dependency learning task to classification issues and reconstructs the dependent relations based on classification results. Finally F1 scores of 86.69, 69.95 and 78.35 are obtained for syntactic dependencies, semantic dependencies and the whole system respectively in closed challenge. For open challenge the corresponding F1 scores are 86.69, 68.99 and 77.84.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam Berger</author>
<author>Stephen Della Pietra</author>
</authors>
<title>A Maximum Entropy Approach to Natural Language Processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--1</pages>
<institution>Vincent Della Pietra</institution>
<marker>Berger, Pietra, 1996</marker>
<rawString>Adam Berger, Stephen Della Pietra, Vincent Della Pietra 1996. A Maximum Entropy Approach to Natural Language Processing. Computational Linguistics, 22(1):39-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Meyers</author>
<author>Ruth Reeves</author>
<author>Catherine Macleod</author>
<author>Rachel Szekely</author>
<author>Veronika Zielinska</author>
</authors>
<title>Brian Young and Ralph Grishman</title>
<date>2004</date>
<booktitle>The NomBank Project: An Interim Report HLT-NAACL 2004 Workshop: Frontiers in Corpus Annotation,</booktitle>
<pages>24--31</pages>
<contexts>
<context position="5322" citStr="Meyers, et al., 2004" startWordPosition="847" endWordPosition="850">ure. The distant is also combined with features in table 1 to produce complex features. 2.2 Semantic Dependency Labeling Semantic dependencies are always concerning with specific predicates. Unlike syntactic dependencies, semantic dependency relationships usually can not be represented as a tree. Thus, the method used for semantic dependency labeling is somewhat different from syntactic dependency parsing. The work of semantic labeling can be divided into two stages: predicate tagging and dependents recognizing. 2.2.1 Predicate Tagging According to PropBank (Palmer, et al., 2005) and NomBank (Meyers, et al., 2004), predicates usually have several rolesets corresponding to different meanings. For example, the verb abandon has three rolesets marked as ordinal numbers 01, 02 and 03 as described below. &lt;frameset&gt; &lt;predicate lemma=“abandon”&gt; &lt;roleset id=“abandon.01” name=“leave behind” vncls=“51.2”&gt; . . . &lt;/roleset&gt; &lt;roleset id=“abandon.02” name=“exchange” vncls=“51.2”&gt; id=“abandon.03” name=“surrender, give over” vncls=“- ”&gt; . . . &lt;/roleset&gt; &lt;/predicate&gt; &lt;/frameset&gt; The goal of this part is to identify the predicates in the sentences and to determine the roleset for each of them. It should be cleared that t</context>
</contexts>
<marker>Meyers, Reeves, Macleod, Szekely, Zielinska, 2004</marker>
<rawString>Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel Szekely, Veronika Zielinska, Brian Young and Ralph Grishman 2004. The NomBank Project: An Interim Report HLT-NAACL 2004 Workshop: Frontiers in Corpus Annotation, 24-31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The Proposition Bank: An Annotated Corpus of Semantic Roles Computational Linguistics,</title>
<date>2005</date>
<pages>31--1</pages>
<contexts>
<context position="5287" citStr="Palmer, et al., 2005" startWordPosition="841" endWordPosition="844">ce x is considered as a single feature. The distant is also combined with features in table 1 to produce complex features. 2.2 Semantic Dependency Labeling Semantic dependencies are always concerning with specific predicates. Unlike syntactic dependencies, semantic dependency relationships usually can not be represented as a tree. Thus, the method used for semantic dependency labeling is somewhat different from syntactic dependency parsing. The work of semantic labeling can be divided into two stages: predicate tagging and dependents recognizing. 2.2.1 Predicate Tagging According to PropBank (Palmer, et al., 2005) and NomBank (Meyers, et al., 2004), predicates usually have several rolesets corresponding to different meanings. For example, the verb abandon has three rolesets marked as ordinal numbers 01, 02 and 03 as described below. &lt;frameset&gt; &lt;predicate lemma=“abandon”&gt; &lt;roleset id=“abandon.01” name=“leave behind” vncls=“51.2”&gt; . . . &lt;/roleset&gt; &lt;roleset id=“abandon.02” name=“exchange” vncls=“51.2”&gt; id=“abandon.03” name=“surrender, give over” vncls=“- ”&gt; . . . &lt;/roleset&gt; &lt;/predicate&gt; &lt;/frameset&gt; The goal of this part is to identify the predicates in the sentences and to determine the roleset for each o</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, Paul Kingsbury 2005. The Proposition Bank: An Annotated Corpus of Semantic Roles Computational Linguistics, 31(1):71-106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
</authors>
<title>Lluis M`arquez and Joakim Nivre</title>
<date>2008</date>
<booktitle>The CoNLL-2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies. Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL-2008)</booktitle>
<contexts>
<context position="2225" citStr="Surdeanu, et al., 2008" startWordPosition="319" endWordPosition="322">tence. If edges are added between any two words, a full-connected © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. graph is constructed and the dependency tree could be found using a maximum spanning tree (MST) algorithm (McDonald, et al., 2005). The latter focuses on separable predicates whose semantic dependents could be determined using classification tools, such as ME models 1 etc.. We participated in both closed and open challenge of the CoNLL-2008 shared task (Surdeanu, et al., 2008). Results are reported on both development and test sets in this paper. 2 System Description 2.1 Syntactic Parsing The goal of syntactic parsing is to create a labeled syntactic dependency parse y for input sentence x including words and their parts of speech (POS). Inspired by the parsing model that implements maximum spanning tree (MST) algorithm to induce the dependency parsing tree (McDonald, et al., 2005), the system employs the same framework. The incorporated features are defined over parts of speech of words occurring between and around a possible head-dependent relation. Suppose G = (</context>
</contexts>
<marker>Surdeanu, Johansson, Meyers, 2008</marker>
<rawString>Mihai Surdeanu, Richard Johansson, Adam Meyers, Lluis M`arquez and Joakim Nivre 2008. The CoNLL-2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies. Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL-2008)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajiˇc</author>
</authors>
<title>Non-projective Dependency Parsing using Spanning Tree Algorithms.</title>
<date>2005</date>
<booktitle>Proceedings of HLT/EMNLP,</booktitle>
<pages>523--530</pages>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajiˇc 2005. Non-projective Dependency Parsing using Spanning Tree Algorithms. Proceedings of HLT/EMNLP, 523-530.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>