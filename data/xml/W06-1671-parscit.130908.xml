<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000916">
<title confidence="0.9986935">
Learning Field Compatibilities
to Extract Database Records from Unstructured Text
</title>
<author confidence="0.999432">
Michael Wick, Aron Culotta and Andrew McCallum
</author>
<affiliation confidence="0.9980725">
Department of Computer Science
University of Massachusetts
</affiliation>
<address confidence="0.652677">
Amherst, MA 01003
</address>
<email confidence="0.969345">
{mwick, culotta, mccallum}@cs.umass.edu
</email>
<sectionHeader confidence="0.982457" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999929368421053">
Named-entity recognition systems extract
entities such as people, organizations, and
locations from unstructured text. Rather
than extract these mentions in isolation,
this paper presents a record extraction sys-
tem that assembles mentions into records
(i.e. database tuples). We construct a
probabilistic model of the compatibility
between field values, then employ graph
partitioning algorithms to cluster fields
into cohesive records. We also investigate
compatibility functions over sets of fields,
rather than simply pairs of fields, to ex-
amine how higher representational power
can impact performance. We apply our
techniques to the task of extracting contact
records from faculty and student home-
pages, demonstrating a 53% error reduc-
tion over baseline approaches.
</bodyText>
<sectionHeader confidence="0.995153" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999938886792453">
Information extraction (IE) algorithms populate a
database with facts discovered from unstructured
text. This database is often used by higher-level
tasks such as question answering or knowledge
discovery. The richer the structure of the database,
the more useful it is to higher-level tasks.
A common IE task is named-entity recognition
(NER), the problem of locating mentions of en-
tities in text, such as people, places, and organi-
zations. NER techniques range from regular ex-
pressions to finite-state sequence models (Bikel et
al., 1999; Grishman, 1997; Sutton and McCallum,
2006). NER can be viewed as method of populat-
ing a database with single-tuple records, e.g. PER-
SON=Cecil Conner or ORGANIZATION= IBM.
We can add richer structure to these single-tuple
records by extracting the associations among en-
tities. For example, we can populate multi-field
records such as a contact record [PERSON=Steve
Jobs, JOBTITLE = CEO, COMPANY = Apple,
CITY = Cupertino, STATE = CA]. The relational
information in these types of records presents a
greater opportunity for text analysis.
The task of associating together entities is of-
ten framed as a binary relation extraction task:
Given a pair of entities, label the relation be-
tween them (e.g. Steve Jobs LOCATED-IN Cuper-
tino). Common approaches to relation extraction
include pattern matching (Brin, 1998; Agichtein
and Gravano, 2000) and classification (Zelenko et
al., 2003; Kambhatla, 2004).
However, binary relation extraction alone is not
well-suited for the contact record example above,
which requires associating together many fields
into one record. We refer to this task of piecing
together many fields into a single record as record
extraction.
Consider the task of extracting contact records
from personal homepages. An NER system may
label all mentions of cities, people, organizations,
phone numbers, job titles, etc. on a page, from
both semi-structured an unstructured text. Even
with a highly accurate NER system, it is not obvi-
ous which fields belong to the same record. For
example, a single document could contain five
names, three phone numbers and only one email.
Additionally, the layout of certain fields may be
convoluted or vary across documents.
Intuitively, we would like to learn the compat-
ibility among fields, for example the likelihood
that the organization University of North Dakota
is located in the state North Dakota, or that phone
numbers with area code 212 co-occur with the
</bodyText>
<page confidence="0.60554">
603
</page>
<note confidence="0.9661915">
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 603–611,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.9965265">
city New York. Additionally, the system should
take into account page layout information, so that
nearby fields are more likely to be grouped into the
same record.
In this paper, we describe a method to induce a
probabilistic compatibility function between sets
of fields. Embedding this compatibility func-
tion within a graph partitioning method, we de-
scribe how to cluster highly compatible fields into
records.
We evaluate our approach on personal home-
pages that have been manually annotated with
contact record information, and demonstrate a
53% error reduction over baseline methods.
</bodyText>
<sectionHeader confidence="0.999252" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999708618421053">
McDonald et al. (2005) present clustering tech-
niques to extract complex relations, i.e. relations
with more than two arguments. Record extraction
can be viewed as an instance of complex relation
extraction. We build upon this work in three ways:
(1) Our system learns the compatibility between
sets of fields, rather than just pairs of field; (2) our
system is not restricted to relations between en-
tities in the same sentence; and (3) our problem
domain has a varying number of fields per record,
as opposed to the fixed schema in McDonald et al.
(2005).
Bansal et al. (2004) present algorithms for the
related task of correlational clustering: finding an
optimal clustering from a matrix of pairwise com-
patibility scores. The correlational clustering ap-
proach does not handle compatibility scores calcu-
lated over sets of nodes, which we address in this
paper.
McCallum and Wellner (2005) discriminatively
train a model to learn binary coreference deci-
sions, then perform joint inference using graph
partitioning. This is analogous to our work, with
two distinctions. First, instead of binary coref-
erence decisions, our model makes binary com-
patibility decisions, reflecting whether a set of
fields belong together in the same record. Second,
whereas McCallum and Wellner (2005) factor the
coreference decisions into pairs of vertices, our
compatibility decisions are made between sets of
vertices. As we show in our experiments, factoring
decisions into sets of vertices enables more power-
ful features that can improve performance. These
higher-order features have also recently been in-
vestigated in other models of coreference, both
discriminative (Culotta and McCallum, 2006) and
generative (Milch et al., 2005).
Viola and Narasimhan (2005) present a prob-
abilistic grammar to parse contact information
blocks. While this model is capable of learn-
ing long-distance compatibilities (such as City and
State relations), features to enable this are not ex-
plored. Additionally, their work focuses on la-
beling fields in documents that have been pre-
segmented into records. This record segmentation
is precisely what we address in this paper.
Borkar et al. (2001) and Kristjannson et al.
(2004) also label contact address blocks, but ig-
nore the problem of clustering fields into records.
Also, Culotta et al. (2004) automatically extract
contact records from web pages, but use heuristics
to cluster fields into records.
Embley et al. (1999) provide heuristics to de-
tect record boundaries in highly structured web
documents, such as classified ads, and Embley
and Xu (2000) improve upon these heuristics for
slightly more ambiguous domains using a vector
space model. Both of these techniques apply to
data for which the records are highly contiguous
and have a distinctive separator between records.
These heuristic approaches are unlikely to be suc-
cessful in the unstructured text domain we address
in this paper.
Most other work on relation extraction focuses
only on binary relations (Zelenko et al., 2003;
Miller et al., 2000; Agichtein and Gravano, 2000;
Culotta and Sorensen, 2004). A serious difficulty
in applying binary relation extractors to the record
extraction task is that rather than enumerating over
all pairs of entities, the system must enumerate
over all subsets of entities, up to subsets of size
k, the maximum number of fields per record. We
address this difficulty by employing two sampling
methods: one that samples uniformly, and another
that samples on a focused subset of the combina-
torial space.
</bodyText>
<sectionHeader confidence="0.985867" genericHeader="method">
3 From Fields to Records
</sectionHeader>
<subsectionHeader confidence="0.997874">
3.1 Problem Definition
</subsectionHeader>
<bodyText confidence="0.957142142857143">
Let a field F be a pair (a, v), where a is an attribute
(column label) and v is a value, e.g. FZ = (CITY,
San Francisco). Let record R be a set of fields,
R = {F1 ... Fn,}. Note that R may contain mul-
tiple fields with the same attribute but different
values (e.g. a person may have multiple job ti-
tles). Assume we are given the output of a named-
</bodyText>
<page confidence="0.90155">
604
</page>
<bodyText confidence="0.994759">
entity recognizer, which labels tokens in a doc-
ument with their attribute type (e.g. NAME or
CITY). Thus, a document initially contains a set
of fields, {F1 ... F�1.
The task is to partition the fields in each anno-
tated document into a set of records {R1 ... Rk1
such that each record Ri contains exactly the set
of fields pertinent to that record. In this paper, we
assume each field belongs to exactly one record.
</bodyText>
<subsectionHeader confidence="0.999883">
3.2 Solution Overview
</subsectionHeader>
<bodyText confidence="0.999963230769231">
For each document, we construct a fully-
connected weighted graph G = (V, E), with ver-
tices V and weighted edges E. Each field in the
document is represented by a vertex in V , and the
edges are weighted by the compatibility of adja-
cent fields, i.e. a measure of how likely it is that
Fi and Fj belong to the same record.
Partitioning V into k disjoint clusters uniquely
maps the set of fields to a set of k records. Be-
low, we provide more detail on the two principal
steps in our solution: (1) estimating the compati-
bility function and (2) partitioning V into disjoint
clusters.
</bodyText>
<subsectionHeader confidence="0.999328">
3.3 Learning field compatibility
</subsectionHeader>
<bodyText confidence="0.9998589375">
Let F be a candidate cluster of fields forming a
partial record. We construct a compatibility func-
tion C that maps two sets of fields to a real value,
i.e. C : Fi x Fj —* R. We abbreviate the value
C(Fi, Fj) as Cij. The higher the value of Cij the
more likely it is that Fi and Fj belong to the same
record.
For example, in the contact record domain, Cij
can reflect whether a city and state should co-
occur, or how likely a company is to have a certain
job title.
We represent Cij by a maximum-entropy clas-
sifier over the binary variable Sij, which is true if
and only if field set Fi belongs to the same record
as field set Fj. Thus, we model the conditional
distribution
</bodyText>
<equation confidence="0.9911615">
PA(Sij|Fi,Fj) oC exp E
k
</equation>
<bodyText confidence="0.999948653061225">
where fk is a binary feature function that com-
putes attributes over the field sets, and A = {Ak1
is the set of real-valued weights that are the pa-
rameters of the maximum-entropy model. We set
Cij = PA(Sij =true|Fi,Fj). This approach can
be viewed as a logistic regression model for field
compatibility.
Examples of feature functions include format-
ting evidence (Fi appears at the top of the docu-
ment, Fj at the bottom), conflicting value infor-
mation (Fi and Fj contain conflicting values for
the state field), or other measures of compatibility
(a city value in Fi is known to exist in a state in
Fj). A feature may involve more than one field,
for example, if a name, title and university occurs
consecutively in some order. We give a more de-
tailed description of the feature functions in Sec-
tion 4.3.
We propose learning the A weights for each of
these features using supervised machine learning.
Given a set of documents D for which the true
mapping from fields to set of records is known,
we wish to estimate P(Sij|Fi,Fj) for all pairs of
field sets Fi, Fj.
Enumerating all positive and negative pairs of
field sets is computationally infeasible for large
datasets, so we instead propose two sampling
methods to generate training examples. The first
simply samples pairs of field sets uniformly from
the training data. For example, given a document
D containing true records {R1 ... Rk1, we sam-
ple positive and negative examples of field sets of
varying sizes from {Ri ... Rj1. The second sam-
pling method first trains the model using the exam-
ples generated by uniform sampling. This model
is then used to cluster the training data. Additional
training examples are created during the clustering
process and are used to retrain the model parame-
ters. This second sampling method is an attempt to
more closely align the characteristics of the train-
ing and testing examples.
Given a sample of labeled training data, we set
the parameters of the maximum-entropy classi-
fier in standard maximum-likelihood fashion, per-
forming gradient ascent on the log-likelihood of
the training data. The resulting weights indi-
cate how important each feature is in determin-
ing whether two sets of fields belong to the same
record.
</bodyText>
<subsectionHeader confidence="0.994509">
3.4 Partitioning Fields into Records
</subsectionHeader>
<bodyText confidence="0.999751142857143">
One could employ the estimated classifier to con-
vert fields into records as follows: Classify each
pair of fields as positive or negative, and perform
transitive closure to enforce transitivity of deci-
sions. That is, if the classifier determines that A
and B belong to the same record and that B and
C belong to the same record, then by transitivity
</bodyText>
<equation confidence="0.70737">
)�kfk(Sij, Fi, Fj)
605
</equation>
<bodyText confidence="0.999943878787879">
A and C must belong to the same record. The
drawback of this approach is that the compatibility
between A and C is ignored. In cases where the
classifier determines that A and C are highly in-
compatible, transitive closure can lead to poor pre-
cision. McCallum and Wellner (2005) explore this
issue in depth for the related task of noun corefer-
ence resolution.
With this in mind, we choose to avoid transitive
closure, and instead employ a graph partitioning
method to make record merging decisions jointly.
Given a document D with fields {F1 ... F�},
we construct a fully connected graph G = (V, E),
with edge weights determined by the learned com-
patibility function C. We wish to partition vertices
V into clusters with high intra-cluster compatibil-
ity.
One approach is to simply use greedy agglom-
erative clustering: initialize each vertex to its own
cluster, then iteratively merge clusters with the
highest inter-cluster edge weights. The compati-
bility between two clusters can be measured using
single-link or average-link clustering. The clus-
tering algorithm converges when the inter-cluster
edge weight between any pair of clusters is below
a specified threshold.
We propose a modification to this approach.
Since the compatibility function we have de-
scribed maps two sets of vertices to a real value,
we can use this directly to calculate the compati-
bility between two clusters, rather than performing
average or single link clustering.
We now describe the algorithm more concretely.
</bodyText>
<listItem confidence="0.978499">
• Input: (1) Graph G = (V, E), where each
vertex vi represents a field Fi. (2) A threshold
value T.
• Initialization: Place each vertex vi in its own
cluster Ri. (The hat notation indicates that
this cluster represents a possible record.)
• Iterate: Re-calculate the compatibility func-
tion Cij between each pair of clusters. Merge
the two most compatible clusters, Rz , R&apos;j.
• Termination: If there does not exist a pair of
clusters Ri, Rj such that Cij &gt; T, the algo-
rithm terminates and returns the current set of
clusters.
</listItem>
<bodyText confidence="0.999272">
A natural threshold value is T = 0.5, since this
is the point at which the binary compatibility clas-
sifier predicts that the fields belong to different
records. In Section 4.4, we examine how perfor-
mance varies with T.
</bodyText>
<subsectionHeader confidence="0.7659835">
3.5 Representational power of cluster
compatibility functions
</subsectionHeader>
<bodyText confidence="0.965732733333333">
Most previous work on inducing compatibility
functions learns the compatibility between pairs of
vertices, not clusters of vertices. In this section,
we provide intuition to explain why directly mod-
eling the compatibility of clusters of vertices may
be advantageous. We refer to the cluster compat-
ibility function as Cij, and the pairwise (binary)
compatibility function as Bij.
First, we note that Cij is a generalization of
single-link and average-link clustering methods
that use Bij, since the output of these methods
can simply be included as features in Cij. For ex-
ample, given two clusters Ri = {v1, v2, v3} and
Rj = {v4, v5, v6}, average-link clustering calcu-
lates the inter-cluster score between Ri and Rj as
</bodyText>
<equation confidence="0.85893">
E Bab
�Rj |aERi,bERj
</equation>
<bodyText confidence="0.999324666666667">
SAL(�Ri, Rj) can be included as a feature for
the compatibility function Cij, with an associated
weight estimated from training data.
Second, there may exist phenomena of the data
that can only be captured by a classifier that con-
siders “higher-order” features. Below we describe
two such cases.
In the first example, consider three vertices of
mild compatibility, as in Figure 1(a). (For these
examples, let Bij, Cij ∈ [0, 1].) Suppose that
these three phone numbers occur nearby in a doc-
ument. Since it is not uncommon for a person to
have two phone numbers with different area codes,
the pairwise compatibility function may score any
pair of nearby phone numbers as relatively com-
patible. However, since it is fairly uncommon for
a person to have three phone numbers with three
different area codes, we would not like all three
numbers to be merged into the same record.
Assume an average-link clustering algorithm.
After merging together the 333 and 444 numbers,
Bij will recompute the new inter-cluster compat-
ibility as 0.51, the average of the inter-cluster
edges. In contrast, the cluster compatibility func-
tion Cij can represent the fact that three numbers
with different area codes are to be merged, and can
penalize their compatibility accordingly. Thus, in
</bodyText>
<figure confidence="0.902235333333333">
SAL(�Ri, Rj) = 1
|�Ri||
606
</figure>
<figureCaption confidence="0.990182">
Figure 1: Two motivating examples illustrating why the cluster compatibility measure (C) may have
</figureCaption>
<bodyText confidence="0.4959622">
higher representational power than the pairwise compatibility measure (B). In (a), the pairwise measure
over-estimates the inter-cluster compatibility when there exist higher-order features such as A person
is unlikely to have phone numbers with three different area codes. In (b), the pairwise measure under-
estimates inter-cluster compatibility when weak features like string comparisons can be combined into a
more powerful feature by examining multiple field values.
</bodyText>
<figure confidence="0.999019689655173">
(a)
(b)
444-555-5555
.6 .49
333-555-5555
.53
666-555-5555
444-555-5555
.6 C = 0.1
B = 0.51
333-555-5555
666-555-5555
North
Dakota
.48 .49
Univ of North
Dakota,
Pleasantville
.9
Pleasantville
North
Dakota
Univ of North
Dakota,
Pleasantville
.9
C = 0.8
B = 0.485
Pleasantville
</figure>
<bodyText confidence="0.995697444444444">
this example, the pairwise compatibility function
over-estimates the true compatibility.
In the second example (Figure 1(b)), we con-
sider the opposite case. Consider three edges,
two of which have weak compatibility, and one of
which has high compatibility. For example, per-
haps the system has access to a list of city-state
pairs, and can reliably conclude that Pleasantville
is a city in the state North Dakota.
</bodyText>
<figure confidence="0.994681615384615">
FirstName MiddleName
LastName NickName
Suffix Title
JobTitle CompanyName
Department AddressLine
City1 City2
State Country
PostalCode HomePhone
Fax CompanyPhone
DirectCompanyPhone Mobile
Pager VoiceMail
URL Email
InstantMessage
</figure>
<tableCaption confidence="0.947855">
Table 1: The 25 fields annotated in the contact
record dataset.
</tableCaption>
<bodyText confidence="0.998963526315789">
Deciding that Univ of North Dakota, Pleas-
antville belongs in the same record as North
Dakota and Pleasantville is a bit more difficult.
Suppose a feature function measures the string
similarity between the city field Pleasantville and
the company field Univ of North Dakota, Pleas-
antville. Alone, this string similarity might not
be very strong, and so the pairwise compatibil-
ity is low. However, after Pleasantville and North
Dakota are merged together, the cluster compat-
ibility function can compute the string similarity
of the concatenation of the city and state fields,
resulting in a higher compatibility. In this ex-
ample, the pairwise compatibility function under-
estimates the true compatibility.
These two examples show that the cluster com-
patibility score can have more representational
power than the average of pairwise compatibility
scores.
</bodyText>
<sectionHeader confidence="0.998995" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.955054">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999985818181818">
We hand-labeled a subset of faculty and student
homepages from the WebKB dataset1. Each page
was labeled with the 25 fields listed in Table 1.
In addition, we labeled the records to which each
field belonged. For example, in Figure 2, we la-
beled the contact information for Professor Smith
into a separate record from that of her administra-
tive assistant. There are 252 labeled pages in total,
containing 8996 fields and 16679 word tokens. We
perform ten random samples of 70-30 splits of the
data for all experiments.
</bodyText>
<subsectionHeader confidence="0.968813">
4.2 Systems
</subsectionHeader>
<bodyText confidence="0.999681333333333">
We evaluate five different record extraction sys-
tems. With the exception of Transitive Closure,
all methods employ the agglomerative clustering
</bodyText>
<figure confidence="0.985561769230769">
1http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-
20/www/data/
607
Record 1
Record 2
Professor Smith is the
Administrative Assistant
Professor Jane Smith
Somesuch University
555 555 5555
555-367-7777
Mr. John Doe
Director of the Knowledge Lab ...
</figure>
<figureCaption confidence="0.996977">
Figure 2: A synthetic example representative of the labeled data. Note that Record 1 contains information
</figureCaption>
<bodyText confidence="0.95175675">
both from an address block and from free text, and that Record 2 must be separated from Record 1 even
though fields from each may be nearby in the text.
algorithm described previously. The difference is
in how the inter-cluster compatibility is calculated.
</bodyText>
<listItem confidence="0.9832292">
• Transitive Closure: The method described
in the beginning of Section 3.4, where hard
classification decisions are made, and transi-
tivity is enforced.
• Pairwise Compatibility: In this approach,
the compatibility function only estimates the
compatibility between pairs of fields, not sets
of fields. To compute inter-cluster compat-
ibility, the mean of the edges between the
clusters is calculated.
• McDonald: This method uses the pairwise
compatibility function, but instead of calcu-
lating the mean of inter-cluster edges, it cal-
culates the geometric mean of all pairs of
edges in the potential new cluster. That is,
to calculate the compatibility of records Ri
and Rj, we construct a new record Rij that
contains all fields of Ri and Rj, then calcu-
late the geometric mean of all pairs of fields
in Rij. This is analogous to the method used
in McDonald et al. (2005) for relation extrac-
tion.
• Cluster Compatibility (uniform): Inter-
cluster compatibility is calculated directly by
the cluster compatibility function. This is the
method we advocate in Section 3. Training
examples are sampled uniformly as described
in Section 3.3.
• Cluster Compatibility (iterative): Same as
above, but training examples are sampled us-
</listItem>
<bodyText confidence="0.9419545">
ing the iterative method described in Section
3.3.
</bodyText>
<subsectionHeader confidence="0.96476">
4.3 Features
</subsectionHeader>
<bodyText confidence="0.99998476">
For the pairwise compatibility classifier, we ex-
ploit various formatting as well as knowledge-
based features. Formatting features include the
number of hard returns between fields, whether
the fields occur on the same line, and whether the
fields occur consecutively. Knowledge-based fea-
tures include a mapping we compiled of cities and
states in the United States and Canada. Addition-
ally, we used compatibility features, such as which
fields are of the same type but have different val-
ues.
In building the cluster compatibility classifier,
we use many of the same features as in the bi-
nary classifier, but cast them as first-order existen-
tial features that are generated if the feature exists
between any pair of fields in the two clusters. Ad-
ditionally, we are able to exploit more powerful
compatibility and knowledge-base features. For
example, we examine if a title, a first name and a
last name occur consecutively (i.e., no other fields
occur in-between them). Also, we examine multi-
ple telephone numbers to ensure that they have the
same area codes. Additionally, we employ count
features that indicate if a certain field occurs more
than a given threshold.
</bodyText>
<subsectionHeader confidence="0.876149">
4.4 Results
</subsectionHeader>
<bodyText confidence="0.99998">
For these experiments, we compare performance
on the true record for each page. That is, we cal-
culate how often each system returns a complete
and accurate extraction of the contact record per-
taining to the owner of the webpage. We refer to
</bodyText>
<page confidence="0.854409">
608
</page>
<bodyText confidence="0.999970333333333">
this record as the canonical record and measure
performance in terms of precision, recall and F1
for each field in the canonical record.
Table 2 compares precision, recall and F1 across
the various systems. The cluster compatibility
method with iterative sampling has the highest F1,
demonstrating a 14% error reduction over the next
best method and a 53% error reduction over the
transitive closure baseline.
Transitive closure has the highest recall, but it
comes at the expense of precision, and hence ob-
tains lower F1 scores than more conservative com-
patibility methods. The McDonald method also
has high recall, but drastically improves precision
over the transitivity method by taking into consid-
eration all edge weights.
The pairwise measure yields a slightly higher
F1 score than McDonald mostly due to precision
improvements. Because the McDonald method
calculates the mean of all edge weights rather
than just the inter-cluster edge weights, inter-
cluster weights are often outweighed by intra-
cluster weights. This can cause two densely-
connected clusters to be merged despite low inter-
cluster edge weights.
To further investigate performance differences,
we perform three additional experiments. The first
measures how sensitive the algorithms are to the
threshold value T. Figure 3 plots the precision-
recall curve obtained by varying T from 1.0 to 0.1.
As expected, high values of T result in low recall
but high precision, since the algorithms halt with
a large number of small clusters. The highlighted
points correspond to T = 0.5. These results indi-
cate that setting T to 0.5 is near optimal, and that
the cluster compatibility method outperforms the
pairwise across a wide range of values for T.
In the second experiment, we plot F1 versus
the size of the canonical record. Figure 4 indi-
cates that most of the performance gain occurs
in smaller canonical records (containing between
6 and 12 fields). Small canonical records are
most susceptible to precision errors simply be-
cause there are more extraneous fields that may
be incorrectly assigned to them. These precision
errors are often addressed by the cluster compati-
bility method, as shown in Table 2.
In the final experiment, we plot F1 versus the
total number of fields on the page. Figure 5 indi-
cates that the cluster compatibility method is best
at handling documents with large number of fields.
</bodyText>
<table confidence="0.998844666666667">
F1 Precision Recall
Cluster (I) 91.81 (.013) 92.87 (.005) 90.78 (.007)
Cluster (U) 90.02 93.56 (.007) 86.74 (.011)
Pairwise 90.51 91.07 (.004) 89.95 (.006)
McDonald 88.36 (.012) 83.55 (.004) 93.75 (.005)
Trans Clos 82.37 (.002) 70.75 (.009) 98.56 (.020)
</table>
<tableCaption confidence="0.797205333333333">
Table 2: Precision, recall, and F1 performance for
the record extraction task. The standard error is
calculated over 10 cross-validation trials.
</tableCaption>
<figureCaption confidence="0.671226">
Figure 3: Precision-recall curve for cluster, pair-
</figureCaption>
<bodyText confidence="0.995900961538461">
wise, and mcdonald. The graph is obtained by
varying the stopping threshold T from 1.0 to 0.1.
The highlighted points correspond to T = 0.5.
When there are over 80 fields in the document, the
performance of the pairwise method drops dramat-
ically, while cluster compatibility only declines
slightly. We believe the improved precision of the
cluster compatibility method explains this trend as
well.
We also examine documents where cluster com-
patibility outperforms the pairwise methods. Typ-
ically, these documents contain interleaving con-
tact records. Often, it is the case that a single pair
of fields is sufficient to determine whether a clus-
ter should not be merged. For example, the cluster
classifier can directly model the fact that a con-
tact record should not have multiple first or last
names. It can also associate a weight with the fact
that several fields overlap (e.g., the chances that
a cluster has two first names, two last names and
two cities). In contrast, the binary classifier only
examines pairs of fields in isolation and averages
these probabilities with other edges. This averag-
ing can dilute the evidence from a single pair of
fields. Embarrassing errors may result, such as
a contact record with two first names or two last
</bodyText>
<page confidence="0.718304">
1
</page>
<figure confidence="0.99824225">
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6
0.55
0 0.1 0.2 0.3 0.4
cluster
pairwise
609
F1
6-9 9-12 12+
number fields per record
</figure>
<figureCaption confidence="0.97664225">
Figure 4: Field F1 as the size of the canonical
record increases. This figure suggests that clus-
ter compatibility is most helpful for small records.
Figure 5: Field F1 as the number of fields in
</figureCaption>
<bodyText confidence="0.955766166666667">
the document increases. This figure suggests that
cluster compatibility is most helpful when the doc-
ument has more than 80 fields.
names. These errors are particularly prevalent in
interleaving contact records since adjacent fields
often belong to the same record.
</bodyText>
<sectionHeader confidence="0.995158" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999995173913044">
We have investigated graph partitioning methods
for discovering database records from fields anno-
tated in text. We have proposed a cluster compat-
ibility function that measures how likely it is that
two sets of fields belong to the same cluster. We
argue that this enhancement to existing techniques
provides more representational power.
We have evaluated these methods on a set of
hand-annotated data and concluded that (1) graph
partitioning techniques are more accurate than per-
forming transitive closure, and (2) cluster compat-
ibility methods can avoid common mistakes made
by pairwise compatibility methods.
As information extraction systems become
more reliable, it will become increasingly impor-
tant to develop accurate ways of associating dis-
parate fields into cohesive records. This will en-
able more complex reasoning over text.
One shortcoming of this approach is that fields
are not allowed to belong to multiple records,
because the partitioning algorithm returns non-
overlapping clusters. Exploring overlapping clus-
tering techniques is an area of future work.
Another avenue of future research is to consider
syntactic information in the compatibility func-
tion. While performance on contact record extrac-
tion is highly influenced by formatting features,
many fields occur within sentences, and syntactic
information (such as dependency trees or phrase-
structure trees) may improve performance.
Overall performance can also be improved by
increasing the sophistication of the partitioning
method. For example, we can examine “block
moves” to swap multiple fields between clusters
in unison, possibly avoiding local minima of the
greedy method (Kanani et al., 2006). This can be
especially helpful because many mistakes may be
made at the start of clustering, before clusters are
large enough to reflect true records.
Additionally, many personal web pages con-
tain a time-line of information that describe a per-
son’s educational and professional history. Learn-
ing to associate time information with each con-
tact record enables career path modeling, which
presents interesting opportunities for knowledge
discovery techniques, a subject of ongoing work.
</bodyText>
<sectionHeader confidence="0.995491" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999752083333333">
We thank the anonymous reviewers for helpful
suggestions. This work was supported in part by
the Center for Intelligent Information Retrieval, in
part by U.S. Government contract #NBCH040171
through a subcontract with BBNT Solutions LLC,
in part by The Central Intelligence Agency, the
National Security Agency and National Science
Foundation under NSF grant #IIS-0326249, and in
part by the Defense Advanced Research Projects
Agency (DARPA), through the Department of the
Interior, NBC, Acquisition Services Division, un-
der contract number NBCHD030010. Any opin-
</bodyText>
<figure confidence="0.991715290322581">
0.96
0.94
0.92
0.9
0.88
0.86
0.84
0.82
0.8
0.78
0.76
0.74
pairwise
mcdonald
cluster
0-20 20-40 40-60 60-80 80+
number fields per document
F1
0.96
0.94
0.92
0.88
0.86
0.84
0.82
0.9
0.8
pairwise
mcdonald
cluster
610
</figure>
<bodyText confidence="0.998405">
ions, findings and conclusions or recommenda-
tions expressed in this material are the author(s)
and do not necessarily reflect those of the sponsor.
</bodyText>
<sectionHeader confidence="0.996299" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999833682926829">
Eugene Agichtein and Luis Gravano. 2000. Snow-
ball: Extracting relations from large plain-text col-
lections. In Proceedings of the Fifth ACM Interna-
tional Conference on Digital Libraries.
Nikhil Bansal, Avrim Blum, and Shuchi Chawla. 2004.
Correlation clustering. Machine Learining, 56:89–
113.
Daniel M. Bikel, Richard Schwartz, and Ralph M.
Weischedel. 1999. An algorithm that learns what’s
in a name. Machine Learning, 34:211–231.
Vinayak R. Borkar, Kaustubh Deshmukh, and Sunita
Sarawagi. 2001. Automatic segmentation of text
into structured records. In SIGMOD Conference.
Sergey Brin. 1998. Extracting patterns and rela-
tions from the world wide web. In WebDB Work-
shop at 6th International Conference on Extending
Database Technology.
Aron Culotta and Andrew McCallum. 2006. Practical
Markov logic containing first-order quantifiers with
application to identity uncertainty. In HLT Work-
shop on Computationally Hard Problems and Joint
Inference in Speech and Language Processing, June.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In ACL.
Aron Culotta, Ron Bekkerman, and Andrew McCal-
lum. 2004. Extracting social networks and contact
information from email and the web. In First Con-
ference on Email and Anti-Spam (CEAS), Mountain
View, CA.
David W. Embley and Lin Xu. 2000. Record location
and reconfiguration in unstructured multiple-record
web documents. In WebDB, pages 123–128.
David W. Embley, Xiaoyi Jiang, and Yiu-Kai Ng.
1999. Record-boundary discovery in web docu-
ments. In SIGMOD Conference, pages 467–478.
Ralph Grishman. 1997. Information extraction: Tech-
niques and challenges. In SCIE, pages 10–27.
Nanda Kambhatla. 2004. Combining lexical, syntac-
tic, and semantic features with maximum entropy
models for extracting relations. In ACL.
Pallika Kanani, Andrew McCallum, and Chris Pal.
2006. Improving author coreference by resource-
bounded information gathering from the web. Tech-
nical note.
Trausti Kristjannson, Aron Culotta, Paul Viola, and
Andrew McCallum. 2004. Interactive information
extraction with conditional random fields. Nine-
teenth National Conference on Artificial Intelligence
(AAAI 2004).
Andrew McCallum and Ben Wellner. 2005. Condi-
tional models of identity uncertainty with applica-
tion to noun coreference. In Lawrence K. Saul, Yair
Weiss, and L´eon Bottou, editors, Advances in Neu-
ral Information Processing Systems 17. MIT Press,
Cambridge, MA.
Ryan McDonald, Fernando Pereira, Seth Kulick, Scott
Winters, Yang Jin, and Pete White. 2005. Simple
algorithms for complex relation extraction with ap-
plications to biomedical ie. In 43rd Annual Meeting
of the Association for Computational Linguistics.
Brian Milch, Bhaskara Marthi, and Stuart Russell.
2005. BLOG: Probabilistic models with unknown
objects. In IJCAI.
Scott Miller, Heidi Fox, Lance A. Ramshaw, and Ralph
Weischedel. 2000. A novel use of statistical parsing
to extract information from text. In ANLP.
Charles Sutton and Andrew McCallum. 2006. An in-
troduction to conditional random fields for relational
learning. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning. MIT
Press. To appear.
Paul Viola and Mukund Narasimhan. 2005. Learning
to extract information from semi-structured text us-
ing a discriminative context free grammar. In SIGIR
’05: Proceedings of the 28th annual international
ACM SIGIR conference on Research and develop-
ment in information retrieval, pages 330–337, New
York, NY, USA. ACM Press.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation ex-
traction. Journal of Machine Learning Research,
3:1083–1106.
</reference>
<page confidence="0.920381">
611
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.607504">
<title confidence="0.890497">Learning Field to Extract Database Records from Unstructured Text</title>
<author confidence="0.994815">Aron Culotta Wick</author>
<affiliation confidence="0.9998345">Department of Computer University of</affiliation>
<address confidence="0.990178">Amherst, MA</address>
<email confidence="0.809394">culotta,</email>
<abstract confidence="0.99811535">Named-entity recognition systems extract entities such as people, organizations, and locations from unstructured text. Rather than extract these mentions in isolation, paper presents a extraction system that assembles mentions into records (i.e. database tuples). We construct a probabilistic model of the compatibility between field values, then employ graph partitioning algorithms to cluster fields into cohesive records. We also investigate functions over fields, rather than simply pairs of fields, to examine how higher representational power can impact performance. We apply our techniques to the task of extracting contact records from faculty and student homepages, demonstrating a 53% error reduction over baseline approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Agichtein</author>
<author>Luis Gravano</author>
</authors>
<title>Snowball: Extracting relations from large plain-text collections.</title>
<date>2000</date>
<booktitle>In Proceedings of the Fifth ACM International Conference on Digital Libraries.</booktitle>
<contexts>
<context position="2420" citStr="Agichtein and Gravano, 2000" startWordPosition="353" endWordPosition="356">ngle-tuple records by extracting the associations among entities. For example, we can populate multi-field records such as a contact record [PERSON=Steve Jobs, JOBTITLE = CEO, COMPANY = Apple, CITY = Cupertino, STATE = CA]. The relational information in these types of records presents a greater opportunity for text analysis. The task of associating together entities is often framed as a binary relation extraction task: Given a pair of entities, label the relation between them (e.g. Steve Jobs LOCATED-IN Cupertino). Common approaches to relation extraction include pattern matching (Brin, 1998; Agichtein and Gravano, 2000) and classification (Zelenko et al., 2003; Kambhatla, 2004). However, binary relation extraction alone is not well-suited for the contact record example above, which requires associating together many fields into one record. We refer to this task of piecing together many fields into a single record as record extraction. Consider the task of extracting contact records from personal homepages. An NER system may label all mentions of cities, people, organizations, phone numbers, job titles, etc. on a page, from both semi-structured an unstructured text. Even with a highly accurate NER system, it </context>
<context position="7345" citStr="Agichtein and Gravano, 2000" startWordPosition="1122" endWordPosition="1125">l. (1999) provide heuristics to detect record boundaries in highly structured web documents, such as classified ads, and Embley and Xu (2000) improve upon these heuristics for slightly more ambiguous domains using a vector space model. Both of these techniques apply to data for which the records are highly contiguous and have a distinctive separator between records. These heuristic approaches are unlikely to be successful in the unstructured text domain we address in this paper. Most other work on relation extraction focuses only on binary relations (Zelenko et al., 2003; Miller et al., 2000; Agichtein and Gravano, 2000; Culotta and Sorensen, 2004). A serious difficulty in applying binary relation extractors to the record extraction task is that rather than enumerating over all pairs of entities, the system must enumerate over all subsets of entities, up to subsets of size k, the maximum number of fields per record. We address this difficulty by employing two sampling methods: one that samples uniformly, and another that samples on a focused subset of the combinatorial space. 3 From Fields to Records 3.1 Problem Definition Let a field F be a pair (a, v), where a is an attribute (column label) and v is a valu</context>
</contexts>
<marker>Agichtein, Gravano, 2000</marker>
<rawString>Eugene Agichtein and Luis Gravano. 2000. Snowball: Extracting relations from large plain-text collections. In Proceedings of the Fifth ACM International Conference on Digital Libraries.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikhil Bansal</author>
<author>Avrim Blum</author>
<author>Shuchi Chawla</author>
</authors>
<date>2004</date>
<booktitle>Correlation clustering. Machine Learining,</booktitle>
<pages>56--89</pages>
<contexts>
<context position="4867" citStr="Bansal et al. (2004)" startWordPosition="741" endWordPosition="744">ction over baseline methods. 2 Related Work McDonald et al. (2005) present clustering techniques to extract complex relations, i.e. relations with more than two arguments. Record extraction can be viewed as an instance of complex relation extraction. We build upon this work in three ways: (1) Our system learns the compatibility between sets of fields, rather than just pairs of field; (2) our system is not restricted to relations between entities in the same sentence; and (3) our problem domain has a varying number of fields per record, as opposed to the fixed schema in McDonald et al. (2005). Bansal et al. (2004) present algorithms for the related task of correlational clustering: finding an optimal clustering from a matrix of pairwise compatibility scores. The correlational clustering approach does not handle compatibility scores calculated over sets of nodes, which we address in this paper. McCallum and Wellner (2005) discriminatively train a model to learn binary coreference decisions, then perform joint inference using graph partitioning. This is analogous to our work, with two distinctions. First, instead of binary coreference decisions, our model makes binary compatibility decisions, reflecting </context>
</contexts>
<marker>Bansal, Blum, Chawla, 2004</marker>
<rawString>Nikhil Bansal, Avrim Blum, and Shuchi Chawla. 2004. Correlation clustering. Machine Learining, 56:89– 113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
<author>Richard Schwartz</author>
<author>Ralph M Weischedel</author>
</authors>
<title>An algorithm that learns what’s in a name.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--211</pages>
<contexts>
<context position="1580" citStr="Bikel et al., 1999" startWordPosition="222" endWordPosition="225">, demonstrating a 53% error reduction over baseline approaches. 1 Introduction Information extraction (IE) algorithms populate a database with facts discovered from unstructured text. This database is often used by higher-level tasks such as question answering or knowledge discovery. The richer the structure of the database, the more useful it is to higher-level tasks. A common IE task is named-entity recognition (NER), the problem of locating mentions of entities in text, such as people, places, and organizations. NER techniques range from regular expressions to finite-state sequence models (Bikel et al., 1999; Grishman, 1997; Sutton and McCallum, 2006). NER can be viewed as method of populating a database with single-tuple records, e.g. PERSON=Cecil Conner or ORGANIZATION= IBM. We can add richer structure to these single-tuple records by extracting the associations among entities. For example, we can populate multi-field records such as a contact record [PERSON=Steve Jobs, JOBTITLE = CEO, COMPANY = Apple, CITY = Cupertino, STATE = CA]. The relational information in these types of records presents a greater opportunity for text analysis. The task of associating together entities is often framed as </context>
</contexts>
<marker>Bikel, Schwartz, Weischedel, 1999</marker>
<rawString>Daniel M. Bikel, Richard Schwartz, and Ralph M. Weischedel. 1999. An algorithm that learns what’s in a name. Machine Learning, 34:211–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vinayak R Borkar</author>
<author>Kaustubh Deshmukh</author>
<author>Sunita Sarawagi</author>
</authors>
<title>Automatic segmentation of text into structured records.</title>
<date>2001</date>
<booktitle>In SIGMOD Conference.</booktitle>
<contexts>
<context position="6449" citStr="Borkar et al. (2001)" startWordPosition="980" endWordPosition="983">e higher-order features have also recently been investigated in other models of coreference, both discriminative (Culotta and McCallum, 2006) and generative (Milch et al., 2005). Viola and Narasimhan (2005) present a probabilistic grammar to parse contact information blocks. While this model is capable of learning long-distance compatibilities (such as City and State relations), features to enable this are not explored. Additionally, their work focuses on labeling fields in documents that have been presegmented into records. This record segmentation is precisely what we address in this paper. Borkar et al. (2001) and Kristjannson et al. (2004) also label contact address blocks, but ignore the problem of clustering fields into records. Also, Culotta et al. (2004) automatically extract contact records from web pages, but use heuristics to cluster fields into records. Embley et al. (1999) provide heuristics to detect record boundaries in highly structured web documents, such as classified ads, and Embley and Xu (2000) improve upon these heuristics for slightly more ambiguous domains using a vector space model. Both of these techniques apply to data for which the records are highly contiguous and have a d</context>
</contexts>
<marker>Borkar, Deshmukh, Sarawagi, 2001</marker>
<rawString>Vinayak R. Borkar, Kaustubh Deshmukh, and Sunita Sarawagi. 2001. Automatic segmentation of text into structured records. In SIGMOD Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
</authors>
<title>Extracting patterns and relations from the world wide web.</title>
<date>1998</date>
<booktitle>In WebDB Workshop at 6th International Conference on Extending Database Technology.</booktitle>
<contexts>
<context position="2390" citStr="Brin, 1998" startWordPosition="351" endWordPosition="352"> to these single-tuple records by extracting the associations among entities. For example, we can populate multi-field records such as a contact record [PERSON=Steve Jobs, JOBTITLE = CEO, COMPANY = Apple, CITY = Cupertino, STATE = CA]. The relational information in these types of records presents a greater opportunity for text analysis. The task of associating together entities is often framed as a binary relation extraction task: Given a pair of entities, label the relation between them (e.g. Steve Jobs LOCATED-IN Cupertino). Common approaches to relation extraction include pattern matching (Brin, 1998; Agichtein and Gravano, 2000) and classification (Zelenko et al., 2003; Kambhatla, 2004). However, binary relation extraction alone is not well-suited for the contact record example above, which requires associating together many fields into one record. We refer to this task of piecing together many fields into a single record as record extraction. Consider the task of extracting contact records from personal homepages. An NER system may label all mentions of cities, people, organizations, phone numbers, job titles, etc. on a page, from both semi-structured an unstructured text. Even with a h</context>
</contexts>
<marker>Brin, 1998</marker>
<rawString>Sergey Brin. 1998. Extracting patterns and relations from the world wide web. In WebDB Workshop at 6th International Conference on Extending Database Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Andrew McCallum</author>
</authors>
<title>Practical Markov logic containing first-order quantifiers with application to identity uncertainty.</title>
<date>2006</date>
<booktitle>In HLT Workshop on Computationally Hard Problems and Joint Inference in Speech and Language Processing,</booktitle>
<contexts>
<context position="5970" citStr="Culotta and McCallum, 2006" startWordPosition="904" endWordPosition="907">wo distinctions. First, instead of binary coreference decisions, our model makes binary compatibility decisions, reflecting whether a set of fields belong together in the same record. Second, whereas McCallum and Wellner (2005) factor the coreference decisions into pairs of vertices, our compatibility decisions are made between sets of vertices. As we show in our experiments, factoring decisions into sets of vertices enables more powerful features that can improve performance. These higher-order features have also recently been investigated in other models of coreference, both discriminative (Culotta and McCallum, 2006) and generative (Milch et al., 2005). Viola and Narasimhan (2005) present a probabilistic grammar to parse contact information blocks. While this model is capable of learning long-distance compatibilities (such as City and State relations), features to enable this are not explored. Additionally, their work focuses on labeling fields in documents that have been presegmented into records. This record segmentation is precisely what we address in this paper. Borkar et al. (2001) and Kristjannson et al. (2004) also label contact address blocks, but ignore the problem of clustering fields into recor</context>
</contexts>
<marker>Culotta, McCallum, 2006</marker>
<rawString>Aron Culotta and Andrew McCallum. 2006. Practical Markov logic containing first-order quantifiers with application to identity uncertainty. In HLT Workshop on Computationally Hard Problems and Joint Inference in Speech and Language Processing, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Jeffrey Sorensen</author>
</authors>
<title>Dependency tree kernels for relation extraction.</title>
<date>2004</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="7374" citStr="Culotta and Sorensen, 2004" startWordPosition="1126" endWordPosition="1129">to detect record boundaries in highly structured web documents, such as classified ads, and Embley and Xu (2000) improve upon these heuristics for slightly more ambiguous domains using a vector space model. Both of these techniques apply to data for which the records are highly contiguous and have a distinctive separator between records. These heuristic approaches are unlikely to be successful in the unstructured text domain we address in this paper. Most other work on relation extraction focuses only on binary relations (Zelenko et al., 2003; Miller et al., 2000; Agichtein and Gravano, 2000; Culotta and Sorensen, 2004). A serious difficulty in applying binary relation extractors to the record extraction task is that rather than enumerating over all pairs of entities, the system must enumerate over all subsets of entities, up to subsets of size k, the maximum number of fields per record. We address this difficulty by employing two sampling methods: one that samples uniformly, and another that samples on a focused subset of the combinatorial space. 3 From Fields to Records 3.1 Problem Definition Let a field F be a pair (a, v), where a is an attribute (column label) and v is a value, e.g. FZ = (CITY, San Franc</context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree kernels for relation extraction. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Ron Bekkerman</author>
<author>Andrew McCallum</author>
</authors>
<title>Extracting social networks and contact information from email and the web.</title>
<date>2004</date>
<booktitle>In First Conference on Email and Anti-Spam (CEAS),</booktitle>
<location>Mountain View, CA.</location>
<contexts>
<context position="6601" citStr="Culotta et al. (2004)" startWordPosition="1005" endWordPosition="1008">ative (Milch et al., 2005). Viola and Narasimhan (2005) present a probabilistic grammar to parse contact information blocks. While this model is capable of learning long-distance compatibilities (such as City and State relations), features to enable this are not explored. Additionally, their work focuses on labeling fields in documents that have been presegmented into records. This record segmentation is precisely what we address in this paper. Borkar et al. (2001) and Kristjannson et al. (2004) also label contact address blocks, but ignore the problem of clustering fields into records. Also, Culotta et al. (2004) automatically extract contact records from web pages, but use heuristics to cluster fields into records. Embley et al. (1999) provide heuristics to detect record boundaries in highly structured web documents, such as classified ads, and Embley and Xu (2000) improve upon these heuristics for slightly more ambiguous domains using a vector space model. Both of these techniques apply to data for which the records are highly contiguous and have a distinctive separator between records. These heuristic approaches are unlikely to be successful in the unstructured text domain we address in this paper.</context>
</contexts>
<marker>Culotta, Bekkerman, McCallum, 2004</marker>
<rawString>Aron Culotta, Ron Bekkerman, and Andrew McCallum. 2004. Extracting social networks and contact information from email and the web. In First Conference on Email and Anti-Spam (CEAS), Mountain View, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David W Embley</author>
<author>Lin Xu</author>
</authors>
<title>Record location and reconfiguration in unstructured multiple-record web documents. In WebDB,</title>
<date>2000</date>
<pages>123--128</pages>
<contexts>
<context position="6859" citStr="Embley and Xu (2000)" startWordPosition="1045" endWordPosition="1048">not explored. Additionally, their work focuses on labeling fields in documents that have been presegmented into records. This record segmentation is precisely what we address in this paper. Borkar et al. (2001) and Kristjannson et al. (2004) also label contact address blocks, but ignore the problem of clustering fields into records. Also, Culotta et al. (2004) automatically extract contact records from web pages, but use heuristics to cluster fields into records. Embley et al. (1999) provide heuristics to detect record boundaries in highly structured web documents, such as classified ads, and Embley and Xu (2000) improve upon these heuristics for slightly more ambiguous domains using a vector space model. Both of these techniques apply to data for which the records are highly contiguous and have a distinctive separator between records. These heuristic approaches are unlikely to be successful in the unstructured text domain we address in this paper. Most other work on relation extraction focuses only on binary relations (Zelenko et al., 2003; Miller et al., 2000; Agichtein and Gravano, 2000; Culotta and Sorensen, 2004). A serious difficulty in applying binary relation extractors to the record extractio</context>
</contexts>
<marker>Embley, Xu, 2000</marker>
<rawString>David W. Embley and Lin Xu. 2000. Record location and reconfiguration in unstructured multiple-record web documents. In WebDB, pages 123–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David W Embley</author>
<author>Xiaoyi Jiang</author>
<author>Yiu-Kai Ng</author>
</authors>
<title>Record-boundary discovery in web documents.</title>
<date>1999</date>
<booktitle>In SIGMOD Conference,</booktitle>
<pages>467--478</pages>
<contexts>
<context position="6727" citStr="Embley et al. (1999)" startWordPosition="1024" endWordPosition="1027">ile this model is capable of learning long-distance compatibilities (such as City and State relations), features to enable this are not explored. Additionally, their work focuses on labeling fields in documents that have been presegmented into records. This record segmentation is precisely what we address in this paper. Borkar et al. (2001) and Kristjannson et al. (2004) also label contact address blocks, but ignore the problem of clustering fields into records. Also, Culotta et al. (2004) automatically extract contact records from web pages, but use heuristics to cluster fields into records. Embley et al. (1999) provide heuristics to detect record boundaries in highly structured web documents, such as classified ads, and Embley and Xu (2000) improve upon these heuristics for slightly more ambiguous domains using a vector space model. Both of these techniques apply to data for which the records are highly contiguous and have a distinctive separator between records. These heuristic approaches are unlikely to be successful in the unstructured text domain we address in this paper. Most other work on relation extraction focuses only on binary relations (Zelenko et al., 2003; Miller et al., 2000; Agichtein</context>
</contexts>
<marker>Embley, Jiang, Ng, 1999</marker>
<rawString>David W. Embley, Xiaoyi Jiang, and Yiu-Kai Ng. 1999. Record-boundary discovery in web documents. In SIGMOD Conference, pages 467–478.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
</authors>
<title>Information extraction: Techniques and challenges.</title>
<date>1997</date>
<booktitle>In SCIE,</booktitle>
<pages>10--27</pages>
<contexts>
<context position="1596" citStr="Grishman, 1997" startWordPosition="226" endWordPosition="227">% error reduction over baseline approaches. 1 Introduction Information extraction (IE) algorithms populate a database with facts discovered from unstructured text. This database is often used by higher-level tasks such as question answering or knowledge discovery. The richer the structure of the database, the more useful it is to higher-level tasks. A common IE task is named-entity recognition (NER), the problem of locating mentions of entities in text, such as people, places, and organizations. NER techniques range from regular expressions to finite-state sequence models (Bikel et al., 1999; Grishman, 1997; Sutton and McCallum, 2006). NER can be viewed as method of populating a database with single-tuple records, e.g. PERSON=Cecil Conner or ORGANIZATION= IBM. We can add richer structure to these single-tuple records by extracting the associations among entities. For example, we can populate multi-field records such as a contact record [PERSON=Steve Jobs, JOBTITLE = CEO, COMPANY = Apple, CITY = Cupertino, STATE = CA]. The relational information in these types of records presents a greater opportunity for text analysis. The task of associating together entities is often framed as a binary relatio</context>
</contexts>
<marker>Grishman, 1997</marker>
<rawString>Ralph Grishman. 1997. Information extraction: Techniques and challenges. In SCIE, pages 10–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nanda Kambhatla</author>
</authors>
<title>Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations.</title>
<date>2004</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2479" citStr="Kambhatla, 2004" startWordPosition="363" endWordPosition="364">xample, we can populate multi-field records such as a contact record [PERSON=Steve Jobs, JOBTITLE = CEO, COMPANY = Apple, CITY = Cupertino, STATE = CA]. The relational information in these types of records presents a greater opportunity for text analysis. The task of associating together entities is often framed as a binary relation extraction task: Given a pair of entities, label the relation between them (e.g. Steve Jobs LOCATED-IN Cupertino). Common approaches to relation extraction include pattern matching (Brin, 1998; Agichtein and Gravano, 2000) and classification (Zelenko et al., 2003; Kambhatla, 2004). However, binary relation extraction alone is not well-suited for the contact record example above, which requires associating together many fields into one record. We refer to this task of piecing together many fields into a single record as record extraction. Consider the task of extracting contact records from personal homepages. An NER system may label all mentions of cities, people, organizations, phone numbers, job titles, etc. on a page, from both semi-structured an unstructured text. Even with a highly accurate NER system, it is not obvious which fields belong to the same record. For </context>
</contexts>
<marker>Kambhatla, 2004</marker>
<rawString>Nanda Kambhatla. 2004. Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pallika Kanani</author>
<author>Andrew McCallum</author>
<author>Chris Pal</author>
</authors>
<title>Improving author coreference by resourcebounded information gathering from the web. Technical note.</title>
<date>2006</date>
<contexts>
<context position="29680" citStr="Kanani et al., 2006" startWordPosition="4794" endWordPosition="4797">a of future work. Another avenue of future research is to consider syntactic information in the compatibility function. While performance on contact record extraction is highly influenced by formatting features, many fields occur within sentences, and syntactic information (such as dependency trees or phrasestructure trees) may improve performance. Overall performance can also be improved by increasing the sophistication of the partitioning method. For example, we can examine “block moves” to swap multiple fields between clusters in unison, possibly avoiding local minima of the greedy method (Kanani et al., 2006). This can be especially helpful because many mistakes may be made at the start of clustering, before clusters are large enough to reflect true records. Additionally, many personal web pages contain a time-line of information that describe a person’s educational and professional history. Learning to associate time information with each contact record enables career path modeling, which presents interesting opportunities for knowledge discovery techniques, a subject of ongoing work. Acknowledgments We thank the anonymous reviewers for helpful suggestions. This work was supported in part by the </context>
</contexts>
<marker>Kanani, McCallum, Pal, 2006</marker>
<rawString>Pallika Kanani, Andrew McCallum, and Chris Pal. 2006. Improving author coreference by resourcebounded information gathering from the web. Technical note.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trausti Kristjannson</author>
<author>Aron Culotta</author>
<author>Paul Viola</author>
<author>Andrew McCallum</author>
</authors>
<title>Interactive information extraction with conditional random fields.</title>
<date>2004</date>
<booktitle>Nineteenth National Conference on Artificial Intelligence (AAAI</booktitle>
<contexts>
<context position="6480" citStr="Kristjannson et al. (2004)" startWordPosition="985" endWordPosition="988">ave also recently been investigated in other models of coreference, both discriminative (Culotta and McCallum, 2006) and generative (Milch et al., 2005). Viola and Narasimhan (2005) present a probabilistic grammar to parse contact information blocks. While this model is capable of learning long-distance compatibilities (such as City and State relations), features to enable this are not explored. Additionally, their work focuses on labeling fields in documents that have been presegmented into records. This record segmentation is precisely what we address in this paper. Borkar et al. (2001) and Kristjannson et al. (2004) also label contact address blocks, but ignore the problem of clustering fields into records. Also, Culotta et al. (2004) automatically extract contact records from web pages, but use heuristics to cluster fields into records. Embley et al. (1999) provide heuristics to detect record boundaries in highly structured web documents, such as classified ads, and Embley and Xu (2000) improve upon these heuristics for slightly more ambiguous domains using a vector space model. Both of these techniques apply to data for which the records are highly contiguous and have a distinctive separator between re</context>
</contexts>
<marker>Kristjannson, Culotta, Viola, McCallum, 2004</marker>
<rawString>Trausti Kristjannson, Aron Culotta, Paul Viola, and Andrew McCallum. 2004. Interactive information extraction with conditional random fields. Nineteenth National Conference on Artificial Intelligence (AAAI 2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Ben Wellner</author>
</authors>
<title>Conditional models of identity uncertainty with application to noun coreference. In</title>
<date>2005</date>
<booktitle>Advances in Neural Information Processing Systems 17.</booktitle>
<editor>Lawrence K. Saul, Yair Weiss, and L´eon Bottou, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="5180" citStr="McCallum and Wellner (2005)" startWordPosition="788" endWordPosition="791">earns the compatibility between sets of fields, rather than just pairs of field; (2) our system is not restricted to relations between entities in the same sentence; and (3) our problem domain has a varying number of fields per record, as opposed to the fixed schema in McDonald et al. (2005). Bansal et al. (2004) present algorithms for the related task of correlational clustering: finding an optimal clustering from a matrix of pairwise compatibility scores. The correlational clustering approach does not handle compatibility scores calculated over sets of nodes, which we address in this paper. McCallum and Wellner (2005) discriminatively train a model to learn binary coreference decisions, then perform joint inference using graph partitioning. This is analogous to our work, with two distinctions. First, instead of binary coreference decisions, our model makes binary compatibility decisions, reflecting whether a set of fields belong together in the same record. Second, whereas McCallum and Wellner (2005) factor the coreference decisions into pairs of vertices, our compatibility decisions are made between sets of vertices. As we show in our experiments, factoring decisions into sets of vertices enables more pow</context>
<context position="12855" citStr="McCallum and Wellner (2005)" startWordPosition="2106" endWordPosition="2109">timated classifier to convert fields into records as follows: Classify each pair of fields as positive or negative, and perform transitive closure to enforce transitivity of decisions. That is, if the classifier determines that A and B belong to the same record and that B and C belong to the same record, then by transitivity )�kfk(Sij, Fi, Fj) 605 A and C must belong to the same record. The drawback of this approach is that the compatibility between A and C is ignored. In cases where the classifier determines that A and C are highly incompatible, transitive closure can lead to poor precision. McCallum and Wellner (2005) explore this issue in depth for the related task of noun coreference resolution. With this in mind, we choose to avoid transitive closure, and instead employ a graph partitioning method to make record merging decisions jointly. Given a document D with fields {F1 ... F�}, we construct a fully connected graph G = (V, E), with edge weights determined by the learned compatibility function C. We wish to partition vertices V into clusters with high intra-cluster compatibility. One approach is to simply use greedy agglomerative clustering: initialize each vertex to its own cluster, then iteratively </context>
</contexts>
<marker>McCallum, Wellner, 2005</marker>
<rawString>Andrew McCallum and Ben Wellner. 2005. Conditional models of identity uncertainty with application to noun coreference. In Lawrence K. Saul, Yair Weiss, and L´eon Bottou, editors, Advances in Neural Information Processing Systems 17. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Seth Kulick</author>
<author>Scott Winters</author>
<author>Yang Jin</author>
<author>Pete White</author>
</authors>
<title>Simple algorithms for complex relation extraction with applications to biomedical ie.</title>
<date>2005</date>
<booktitle>In 43rd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="4313" citStr="McDonald et al. (2005)" startWordPosition="647" endWordPosition="650">y New York. Additionally, the system should take into account page layout information, so that nearby fields are more likely to be grouped into the same record. In this paper, we describe a method to induce a probabilistic compatibility function between sets of fields. Embedding this compatibility function within a graph partitioning method, we describe how to cluster highly compatible fields into records. We evaluate our approach on personal homepages that have been manually annotated with contact record information, and demonstrate a 53% error reduction over baseline methods. 2 Related Work McDonald et al. (2005) present clustering techniques to extract complex relations, i.e. relations with more than two arguments. Record extraction can be viewed as an instance of complex relation extraction. We build upon this work in three ways: (1) Our system learns the compatibility between sets of fields, rather than just pairs of field; (2) our system is not restricted to relations between entities in the same sentence; and (3) our problem domain has a varying number of fields per record, as opposed to the fixed schema in McDonald et al. (2005). Bansal et al. (2004) present algorithms for the related task of co</context>
<context position="21486" citStr="McDonald et al. (2005)" startWordPosition="3481" endWordPosition="3484">atibility between pairs of fields, not sets of fields. To compute inter-cluster compatibility, the mean of the edges between the clusters is calculated. • McDonald: This method uses the pairwise compatibility function, but instead of calculating the mean of inter-cluster edges, it calculates the geometric mean of all pairs of edges in the potential new cluster. That is, to calculate the compatibility of records Ri and Rj, we construct a new record Rij that contains all fields of Ri and Rj, then calculate the geometric mean of all pairs of fields in Rij. This is analogous to the method used in McDonald et al. (2005) for relation extraction. • Cluster Compatibility (uniform): Intercluster compatibility is calculated directly by the cluster compatibility function. This is the method we advocate in Section 3. Training examples are sampled uniformly as described in Section 3.3. • Cluster Compatibility (iterative): Same as above, but training examples are sampled using the iterative method described in Section 3.3. 4.3 Features For the pairwise compatibility classifier, we exploit various formatting as well as knowledgebased features. Formatting features include the number of hard returns between fields, whet</context>
</contexts>
<marker>McDonald, Pereira, Kulick, Winters, Jin, White, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Seth Kulick, Scott Winters, Yang Jin, and Pete White. 2005. Simple algorithms for complex relation extraction with applications to biomedical ie. In 43rd Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Milch</author>
<author>Bhaskara Marthi</author>
<author>Stuart Russell</author>
</authors>
<title>BLOG: Probabilistic models with unknown objects.</title>
<date>2005</date>
<booktitle>In IJCAI.</booktitle>
<contexts>
<context position="6006" citStr="Milch et al., 2005" startWordPosition="910" endWordPosition="913">reference decisions, our model makes binary compatibility decisions, reflecting whether a set of fields belong together in the same record. Second, whereas McCallum and Wellner (2005) factor the coreference decisions into pairs of vertices, our compatibility decisions are made between sets of vertices. As we show in our experiments, factoring decisions into sets of vertices enables more powerful features that can improve performance. These higher-order features have also recently been investigated in other models of coreference, both discriminative (Culotta and McCallum, 2006) and generative (Milch et al., 2005). Viola and Narasimhan (2005) present a probabilistic grammar to parse contact information blocks. While this model is capable of learning long-distance compatibilities (such as City and State relations), features to enable this are not explored. Additionally, their work focuses on labeling fields in documents that have been presegmented into records. This record segmentation is precisely what we address in this paper. Borkar et al. (2001) and Kristjannson et al. (2004) also label contact address blocks, but ignore the problem of clustering fields into records. Also, Culotta et al. (2004) auto</context>
</contexts>
<marker>Milch, Marthi, Russell, 2005</marker>
<rawString>Brian Milch, Bhaskara Marthi, and Stuart Russell. 2005. BLOG: Probabilistic models with unknown objects. In IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Miller</author>
<author>Heidi Fox</author>
<author>Lance A Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>A novel use of statistical parsing to extract information from text.</title>
<date>2000</date>
<booktitle>In ANLP.</booktitle>
<contexts>
<context position="7316" citStr="Miller et al., 2000" startWordPosition="1118" endWordPosition="1121"> records. Embley et al. (1999) provide heuristics to detect record boundaries in highly structured web documents, such as classified ads, and Embley and Xu (2000) improve upon these heuristics for slightly more ambiguous domains using a vector space model. Both of these techniques apply to data for which the records are highly contiguous and have a distinctive separator between records. These heuristic approaches are unlikely to be successful in the unstructured text domain we address in this paper. Most other work on relation extraction focuses only on binary relations (Zelenko et al., 2003; Miller et al., 2000; Agichtein and Gravano, 2000; Culotta and Sorensen, 2004). A serious difficulty in applying binary relation extractors to the record extraction task is that rather than enumerating over all pairs of entities, the system must enumerate over all subsets of entities, up to subsets of size k, the maximum number of fields per record. We address this difficulty by employing two sampling methods: one that samples uniformly, and another that samples on a focused subset of the combinatorial space. 3 From Fields to Records 3.1 Problem Definition Let a field F be a pair (a, v), where a is an attribute (</context>
</contexts>
<marker>Miller, Fox, Ramshaw, Weischedel, 2000</marker>
<rawString>Scott Miller, Heidi Fox, Lance A. Ramshaw, and Ralph Weischedel. 2000. A novel use of statistical parsing to extract information from text. In ANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Andrew McCallum</author>
</authors>
<title>An introduction to conditional random fields for relational learning.</title>
<date>2006</date>
<booktitle>In Lise Getoor and Ben Taskar, editors, Introduction to Statistical Relational Learning.</booktitle>
<publisher>MIT Press.</publisher>
<note>To appear.</note>
<contexts>
<context position="1624" citStr="Sutton and McCallum, 2006" startWordPosition="228" endWordPosition="231">n over baseline approaches. 1 Introduction Information extraction (IE) algorithms populate a database with facts discovered from unstructured text. This database is often used by higher-level tasks such as question answering or knowledge discovery. The richer the structure of the database, the more useful it is to higher-level tasks. A common IE task is named-entity recognition (NER), the problem of locating mentions of entities in text, such as people, places, and organizations. NER techniques range from regular expressions to finite-state sequence models (Bikel et al., 1999; Grishman, 1997; Sutton and McCallum, 2006). NER can be viewed as method of populating a database with single-tuple records, e.g. PERSON=Cecil Conner or ORGANIZATION= IBM. We can add richer structure to these single-tuple records by extracting the associations among entities. For example, we can populate multi-field records such as a contact record [PERSON=Steve Jobs, JOBTITLE = CEO, COMPANY = Apple, CITY = Cupertino, STATE = CA]. The relational information in these types of records presents a greater opportunity for text analysis. The task of associating together entities is often framed as a binary relation extraction task: Given a p</context>
</contexts>
<marker>Sutton, McCallum, 2006</marker>
<rawString>Charles Sutton and Andrew McCallum. 2006. An introduction to conditional random fields for relational learning. In Lise Getoor and Ben Taskar, editors, Introduction to Statistical Relational Learning. MIT Press. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Viola</author>
<author>Mukund Narasimhan</author>
</authors>
<title>Learning to extract information from semi-structured text using a discriminative context free grammar.</title>
<date>2005</date>
<booktitle>In SIGIR ’05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>330--337</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="6035" citStr="Viola and Narasimhan (2005)" startWordPosition="914" endWordPosition="917">our model makes binary compatibility decisions, reflecting whether a set of fields belong together in the same record. Second, whereas McCallum and Wellner (2005) factor the coreference decisions into pairs of vertices, our compatibility decisions are made between sets of vertices. As we show in our experiments, factoring decisions into sets of vertices enables more powerful features that can improve performance. These higher-order features have also recently been investigated in other models of coreference, both discriminative (Culotta and McCallum, 2006) and generative (Milch et al., 2005). Viola and Narasimhan (2005) present a probabilistic grammar to parse contact information blocks. While this model is capable of learning long-distance compatibilities (such as City and State relations), features to enable this are not explored. Additionally, their work focuses on labeling fields in documents that have been presegmented into records. This record segmentation is precisely what we address in this paper. Borkar et al. (2001) and Kristjannson et al. (2004) also label contact address blocks, but ignore the problem of clustering fields into records. Also, Culotta et al. (2004) automatically extract contact rec</context>
</contexts>
<marker>Viola, Narasimhan, 2005</marker>
<rawString>Paul Viola and Mukund Narasimhan. 2005. Learning to extract information from semi-structured text using a discriminative context free grammar. In SIGIR ’05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 330–337, New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Zelenko</author>
<author>Chinatsu Aone</author>
<author>Anthony Richardella</author>
</authors>
<title>Kernel methods for relation extraction.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1083</pages>
<contexts>
<context position="2461" citStr="Zelenko et al., 2003" startWordPosition="359" endWordPosition="362"> among entities. For example, we can populate multi-field records such as a contact record [PERSON=Steve Jobs, JOBTITLE = CEO, COMPANY = Apple, CITY = Cupertino, STATE = CA]. The relational information in these types of records presents a greater opportunity for text analysis. The task of associating together entities is often framed as a binary relation extraction task: Given a pair of entities, label the relation between them (e.g. Steve Jobs LOCATED-IN Cupertino). Common approaches to relation extraction include pattern matching (Brin, 1998; Agichtein and Gravano, 2000) and classification (Zelenko et al., 2003; Kambhatla, 2004). However, binary relation extraction alone is not well-suited for the contact record example above, which requires associating together many fields into one record. We refer to this task of piecing together many fields into a single record as record extraction. Consider the task of extracting contact records from personal homepages. An NER system may label all mentions of cities, people, organizations, phone numbers, job titles, etc. on a page, from both semi-structured an unstructured text. Even with a highly accurate NER system, it is not obvious which fields belong to the</context>
<context position="7295" citStr="Zelenko et al., 2003" startWordPosition="1114" endWordPosition="1117">to cluster fields into records. Embley et al. (1999) provide heuristics to detect record boundaries in highly structured web documents, such as classified ads, and Embley and Xu (2000) improve upon these heuristics for slightly more ambiguous domains using a vector space model. Both of these techniques apply to data for which the records are highly contiguous and have a distinctive separator between records. These heuristic approaches are unlikely to be successful in the unstructured text domain we address in this paper. Most other work on relation extraction focuses only on binary relations (Zelenko et al., 2003; Miller et al., 2000; Agichtein and Gravano, 2000; Culotta and Sorensen, 2004). A serious difficulty in applying binary relation extractors to the record extraction task is that rather than enumerating over all pairs of entities, the system must enumerate over all subsets of entities, up to subsets of size k, the maximum number of fields per record. We address this difficulty by employing two sampling methods: one that samples uniformly, and another that samples on a focused subset of the combinatorial space. 3 From Fields to Records 3.1 Problem Definition Let a field F be a pair (a, v), wher</context>
</contexts>
<marker>Zelenko, Aone, Richardella, 2003</marker>
<rawString>Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. 2003. Kernel methods for relation extraction. Journal of Machine Learning Research, 3:1083–1106.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>