<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.984626">
WSL: Sentence Similarity Using Semantic Distance Between Words
</title>
<author confidence="0.970849">
Naoko Miura Tomohiro Takagi
</author>
<affiliation confidence="0.917239">
Meiji University, Japan
</affiliation>
<address confidence="0.973899">
1-1-1 Higashi-Mita, Tama-ku, Kawasaki-shi,Kanagawa 214–8571
</address>
<email confidence="0.999335">
E-mail:{n_miura,takagi}@cs.meiji.ac.jp
</email>
<sectionHeader confidence="0.982896" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999523111111111">
A typical social networking service contains
huge amounts of data, and analyzing this data
at the level of the sentence is important. In
this paper, we describe our system for a
SemEval2015 semantic textual similarity task
(task2). We present our approach, which uses
edit distance to consider word order, and in-
troduce word appearance in context. We re-
port the results from SemEval2015.
</bodyText>
<sectionHeader confidence="0.995112" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99994556">
The Internet, particularly sites related to social
networking services (SNS), contains a vast array of
information used for a variety of purposes. The
vector space model is conventionally used for nat-
ural language processing. This model creates vec-
tors on the basis of frequency of word appearance
and co-occurring words, without taking word order
into account. When it comes to short texts, word
co-occurrence is rare (or even non-existent), and
the number of words is often less than in a typical
newspaper article. Because the average SNS con-
tains data consisting mostly of short sentences, the
vector space model is not the best choice.
In this work, we describe a system we developed
and submitted to SemEval2015. In the proposed
system, we compute sentence similarity using edit
distance to consider word order along with the se-
mantic distance between words. We also introduce
word appearance in context.
The rest of this paper is organized as follows.
Section 2 reviews related work and in Section 3 we
present the three systems we submitted for
SemEval2015. In Section 4, we discuss the results
of our evaluation at SemEval2015.We conclude in
Section 5 with a brief summary.
</bodyText>
<note confidence="0.3904375">
Fig. 1. Hierarchical semantic knowledge base (Li et al.,
2006).
</note>
<sectionHeader confidence="0.998012" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999949647058823">
Recent research has introduced the lexical database
as a dictionary to analyze short texts(Aziz et
al.,2010). Aziz uses a set of similar noun phrases
and similar verb phrases and common words to
compute sentence similarity. Li combines semantic
similarity between words into a hierarchical se-
mantic knowledge base and word order(Li et
al.,2006). There are currently a few hierarchical
semantic knowledge bases available, one of which
is WordNet(Miller,1995). WordNet contains
155,287 words and 117,659 synsets that were
stored in 2012 into the lexical categories of nouns,
verbs, adjectives, and adverbs(WordNet Statistics,
2014). All synsets have semantic relation to other
synsets. An example in the case of using nouns is
shown in Fig.1. Li proposed a formula to measure
the similarity s(w1,w2) between words w1 and w2 as
</bodyText>
<equation confidence="0.9489492">
eh  eh
s (w1 ,w2)  e e&amp;quot;h  eh
-al
, (1)
128
</equation>
<bodyText confidence="0.952368045454545">
Proceedings of the 9th Intern ational Workshop on Semantic Evaluation (SemEval 2015), pages 128–131,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
where l is the shortest path length between w1 and
w2 and h is the depth of the subsumer of w1 and w2
in WordNet. For example, we describe the path
between “boy” and “girl” in Fig. 1. The shortest
path is boy-male-person-female-girl, which is 4, so
l = 4. The subsumer of “boy” and “girl” is “per-
son, human...”, so the depth of this synset is h. In
hierarchical semantic nets, words at the upper lay-
ers have a general meaning and less similarity than
words at the lower layers. Li sets a = 0.2 and =
0.45.
Not only the similarity between words but also
word order is important. For example, the two sen-
tences “a dog bites Mike” and “Mike bites a dog”
consist of the same words, but the meanings are
very different. In this case, we use vectors such
that when each vector completely matches, the sen-
tence similarity is high. Our approach is based on
edit distance to take into account word order and
combined semantic similarity between words.
</bodyText>
<sectionHeader confidence="0.983848" genericHeader="method">
3 System Details
</sectionHeader>
<bodyText confidence="0.9998953">
The proposed system uses edit distance to take
word order into account. It also uses the impact of
word appearance in each context.
In this paper, we describe sentence S1 as
S1={a1,a2, ... ,an} and sentence S2 as S2 ={b1,b2,
...,bm}. S1 consists of n words and S2 consists of m
words. ai is the i th word of S1 and bj is the j th
word of S2. We describe the similarity Sim(S1,,S2)
between S1 and S2 within the range of 0 (no rela-
tion) to 1 (semantic equivalence).
</bodyText>
<subsectionHeader confidence="0.998296">
3.1 Edit Distance
</subsectionHeader>
<bodyText confidence="0.999978857142857">
Edit distance is a way of computing the dissimilari-
ty between two strings. Conventionally, the dis-
tance is computed for a set of characters with three
kinds of operations (substitution, insertion, dele-
tion). However, our approaches are for word sets.
Here, we describe the two kinds of edit distance
extended in our system.
</bodyText>
<subsectionHeader confidence="0.815217">
3.1.1 Levenshtein Distance
</subsectionHeader>
<bodyText confidence="0.9571435">
The Levenshtein distance between S1 and S2
(|S1|=n,|S2|=m) is L(n,m), where
</bodyText>
<equation confidence="0.959322">
0≦i≦n, 0≦j≦m
L(i, j)  max(i, j) if min(i,j)=0,
otherwise. (2)
The indicator function c1(ai,bj) is defined as
(q 
dj 1  qqqt  ( 0)
q 
</equation>
<figure confidence="0.7275362">
3  n m q 
(3)
3.1.2 Jaro-Winkler Distance
The Jaro distance between S1 and S2 (|S1|=n,|S2|=m)
is dj:
0

,
d if d 0.7
Jj j
dj+(k*p* (1
— d )) otherwise
j .
S1
S2.
</figure>
<bodyText confidence="0.9550862">
and
We consider two words as matching
when they are the same and not father than
ce is dw:
dw � , (5)
of the sentence. p is constant an
d usually set to p =
0.1.
between words from Li (Li et
(1)). It
can be used for both nouns and verbs because both
are organized into hierarchies. However, it is not
available for adjectives and adverbs, which are not
organized into hierarchies. Therefore, in addition
to Eq. (1), when
</bodyText>
<equation confidence="0.807020777777778">
we
define semantic similarity between words if they
al.,2006)(Eq.
w1∈synsetA,w2∈synsetB,
d adjectives as
synsetB
 )
1 (synsetA =synsetB) (6)
0 (synsetA
</equation>
<bodyText confidence="0.90056475">
t is half the number of transpositions.
The Jaro-Winkler distan
where q is the number of matching words between
where k is the length of common words at the start
</bodyText>
<subsectionHeader confidence="0.941581">
3.2 Semantic Distance
</subsectionHeader>
<bodyText confidence="0.974024428571428">
We borrow our approach to compute similarity
are adverbs an
is 1 if
and
are in the same synset.
Conventionally, we calculated edit distance on
the basis of match or mismatch between words an
</bodyText>
<figure confidence="0.957935923076923">
s(w1,w2)
w1
w2
d
0)
(4)


s w w
( 1 , 2 ) =


129
</figure>
<bodyText confidence="0.9837005">
ignored how similar two words are. However, with
this approach, if two words have the same meaning
although they are different words (e.g., “fall” and
“autumn”), edit distance defines them as a mis-
match. We address this issue by introducing se-
mantic similarity between words as distance.
(a) Levenshtein distance
We rewrite Eq. (3) as
We propose a measure for the sentence simi-
larity of S1 and S2 Sim(S1,,S2) as
</bodyText>
<listItem confidence="0.427217">
(b) Jaro-Winkler distance
</listItem>
<bodyText confidence="0.952832">
We rewrite Jaro-distance dj defined by Eq. (4)
as
</bodyText>
<equation confidence="0.998695">
�10 (q 0)
dj — { 1 q&apos; + q&apos; + q&apos;—t (q&apos;# 0) (9)
1l 3(n m q&apos;
1≦i≦n,1≦j≦m
Sim (S1, S2)  dw (12)
(10)
</equation>
<bodyText confidence="0.85186575862069">
There is one issue when we compute
as
follows. Let us consider two sentences:
ate an
and
hate an
These sentences indi-
cate opposite meanings. However, except for
and
both sentences consist of the same
words and have the same word order. Therefore,
the method we mentioned above (Eq. (8)) com-
putes the
as high. However, we decide
that the similarity between these sentences have
opposite meanings because of
and
For this reason, we introduce conditional probabil-
ity to estimate word appearance for each context
and extract the probabilities from a corpus as train-
ing data. Further, we give this word appearance for
semantic similarity (Eq. (1)) as a weight.
Let us show an example. P(I
and
are words of
appear-
ance in context
We define
as the set of nouns,
</bodyText>
<equation confidence="0.787924608695652">
verbs, adjectives, an
Sim(S1,,S2),
“I
apple”
“I
apple”.
“ate”
“hate”,
Sim(S1,,S2)
“ate”
“hate”.
|S2),P(ate|S2),P(an|S2),
P(apple|S2)
S1
S2.
S*
d adverbs (e.g., when sentence
S is “It is a dog”, S* is {“is”, “dog”}).
We measure each word appearance weight(w) in
context S as:
* P ( w  |S )
(1  e r ) ,
weight(w) (14)
</equation>
<figure confidence="0.9457039375">
c1 (ai , bj)  (1- s(ai , bj )) * weight(ai)-1 * weight(bj )- 1 (15)
(a
,b;)
� 0 (a
b
) (11)
s(a
,b;) (a

2
i

i
;
i
ib;)
</figure>
<bodyText confidence="0.777692222222222">
ty of S1 and S2 Sim(S1,, S2) as
We define q’ in Eq. (10). q’ indicates the sum
of all semantic similarity between words in S1
and S2 (1≦i≦n,1≦j≦m , SUM(c2(ai,bj)) ). Further,
originally, we calculated t only if two words
are matching (ai=bj); however, in our proposed
methods we change to s(ai,bj)&gt;0.5 to take into
account of the semantic similarity of words.
in Eq. (10) is defined by Eq. (11). It means
</bodyText>
<figure confidence="0.971243657894737">
the semantic similari
C2
ty of words.
c
We propose a measure for the sentence simi-
lari
3.3 The Impact of Word Appearance in Con-
text
&apos;
P(w
 |S)  docw,s (13)
docS&apos;
where
is the number of documents that con-
tains both w and
and
is the number of doc-
uments that contains
We sety = 5.0 .
We take into account the impact of words in
context and apply it to Levenshtein distan
docw,S*
S*
docS*
S*.
ce, re-
writing Eq. (7) as
When a word in one sentence co-occurs with
words in the other sentence frequently, the impact
is low, and when it co-occurs less frequently, the
impact is high. We use Eq.(15) when
and
are
nouns or verbs an
ai
bj
d s(ai,bj) &lt; 0.7 .
130
</figure>
<sectionHeader confidence="0.996442" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999930285714286">
STS systems at SemEval 2015 were evaluated on
five data sets. Each data set contained a number of
sentence pairs that have a gold-standard score in
the range of 0–5 as correct answers. The STS sys-
tems were evaluated by Pearson correlation be-
tween the system output and the gold-standard
score. We used the Reuters Corpus as training data.
</bodyText>
<subsectionHeader confidence="0.987957">
4.1 Submissions
</subsectionHeader>
<bodyText confidence="0.8646579375">
We submitted the outputs of three of our system
runs. In the STS task, the similarity between the
score(S1,S2) of two sentences needed to be in the
range of 0–5. Accordingly, we set score(S1,S2) as
score(S1,S2) =5* Sim(S1,S2). For pre-processing, we
use Stanford-NLP tools for tokenization and POS-
tagging. We also remove punctuation marks.
And we use JWNL to measure the similarity be-
tween words. (Eq.(1))
-run1
Levenshtein distance approach (Eq. (8))
-run2
Jaro-Winkler distance approach (Eq. (12))
-run3
Using run1 (Eq. (8)) in conjunction with word
appearance in context (Eq. (15))
</bodyText>
<subsectionHeader confidence="0.965623">
4.2 Evaluation on STS 2015 Data
</subsectionHeader>
<bodyText confidence="0.899525421052632">
Table 1 shows the results (Pearson correlation) of
each of our three runs evaluated on five data sets.
Our best system was run3. It was ranked 64 out of
74 systems.
The weighted-mean scores of run1 and run2
were almost the same. When we compare the
scores of run1 and run3, run3 performed better
on four datasets (the exception was “answers-
forums”). Overall, the best performance in terms of
weighted-mean score was by run3.
Data Set run1 run2 run3
answers-forums 0.3759 0.4287 0.3709
answers-students 0.5269 0.6028 0.5437
belief 0.6387 0.5231 0.6478
headlines 0.5462 0.6029 0.5752
images 0.5710 0.4879 0.6407
Weighted-Mean 0.5379 0.5424 0.5672
Table 1 . Results of evaluation on SemEval2015 STS
task.
</bodyText>
<sectionHeader confidence="0.997064" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999760142857143">
In this paper, we proposed methods for deter-
mining sentence similarity. We adopted the seman-
tic distance of word on edit distance along with
word appearance in context. Evaluation results
suggest that using word appearance in context is an
effective element for determining sentence similar-
ity.
</bodyText>
<sectionHeader confidence="0.996415" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.935489">
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Iiiigo Lopez-Gazpio, Montse Maritxalar, Rada
Mihalcea, German Rigau, Larraitz Uria, Janyce
Wiebe. SemEval-2015 Task 2: Semantic Textual
Similarity, English, Spanish and Pilot on Interpreta-
bility. In Proceedings of the 9th International Work-
shop on Semantic Evaluation (SemEval 2015)
June,2015, Denver,CO, Association for Computa-
tional Linguistics.
Yuhua Li, David McLean, Zuhair A. Bander, James D.
O’Shea, and Keeley Crockett (2006). Sentence Simi-
larity Based on Semantic Nets and Corpus Statistics.
IEEE Transactions on Knowledge and Data Engi-
neering Vol. 18 No. 8 August 2006.
Mehwish Aziz and Muhammad Rafi (2010). Sentence
based semantic similarity measure for blog-posts.
Digital Content, Multimedia Technology and its Ap-
plications (IDC). 2010 6th International Conference.
G.A. Miller, “WordNet: A Lexical Database for Eng-
lish”. (1995) Communications of the ACM, Vol. 38,
Issue 11 Nov. 1995.
WordNet Statistics WordNet.princeton.edu. Re-
trieved2014-03-11
http://wordnet.princeton.edu/wordnet/man/wnstats.7
WN.html
Reuters Corpus English Language, 1996/08/20-
1997/08/19.
http://about.reuters.com/researchandstandards/corpus/
JWNL (Java WordNet Library)
http://sourceforge.net/projects/jwordnet/
</reference>
<page confidence="0.456584">
131
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.388022">
<title confidence="0.999217">WSL: Sentence Similarity Using Semantic Distance Between Words</title>
<author confidence="0.944835">Naoko Miura Tomohiro</author>
<affiliation confidence="0.990772">Meiji University, Japan</affiliation>
<address confidence="0.798611">Higashi-Mita, Tama-ku, Kawasaki-shi,Kanagawa</address>
<email confidence="0.942629">E-mail:{n_miura,takagi}@cs.meiji.ac.jp</email>
<abstract confidence="0.994427444444444">A typical social networking service contains huge amounts of data, and analyzing this data at the level of the sentence is important. In this paper, we describe our system for a SemEval2015 semantic textual similarity task (task2). We present our approach, which uses edit distance to consider word order, and introduce word appearance in context. We re-</abstract>
<note confidence="0.581674">port the results from SemEval2015.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Eneko Agirre</author>
<author>Carmen Banea</author>
<author>Claire Cardie</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
</authors>
<title>Aitor Gonzalez-Agirre, Weiwei Guo, Iiiigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, Janyce Wiebe.</title>
<booktitle>SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015) June,2015, Denver,CO, Association for Computational Linguistics.</booktitle>
<marker>Agirre, Banea, Cardie, Cer, Diab, </marker>
<rawString>Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Iiiigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, Janyce Wiebe. SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015) June,2015, Denver,CO, Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuhua Li</author>
<author>David McLean</author>
<author>Zuhair A Bander</author>
<author>James D O’Shea</author>
<author>Keeley Crockett</author>
</authors>
<title>Sentence Similarity Based on Semantic Nets and Corpus Statistics.</title>
<date>2006</date>
<journal>IEEE Transactions on Knowledge and Data Engineering</journal>
<volume>18</volume>
<marker>Li, McLean, Bander, O’Shea, Crockett, 2006</marker>
<rawString>Yuhua Li, David McLean, Zuhair A. Bander, James D. O’Shea, and Keeley Crockett (2006). Sentence Similarity Based on Semantic Nets and Corpus Statistics. IEEE Transactions on Knowledge and Data Engineering Vol. 18 No. 8 August 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehwish Aziz</author>
<author>Muhammad Rafi</author>
</authors>
<title>Sentence based semantic similarity measure for blog-posts.</title>
<date>2010</date>
<booktitle>Digital Content, Multimedia Technology and its Applications (IDC). 2010 6th International Conference.</booktitle>
<marker>Aziz, Rafi, 2010</marker>
<rawString>Mehwish Aziz and Muhammad Rafi (2010). Sentence based semantic similarity measure for blog-posts. Digital Content, Multimedia Technology and its Applications (IDC). 2010 6th International Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
</authors>
<title>WordNet: A Lexical Database for English”.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<marker>Miller, 1995</marker>
<rawString>G.A. Miller, “WordNet: A Lexical Database for English”. (1995) Communications of the ACM, Vol. 38, Issue 11 Nov. 1995.</rawString>
</citation>
<citation valid="false">
<authors>
<author>WordNet Statistics WordNet princeton edu</author>
</authors>
<pages>2014--03</pages>
<note>http://wordnet.princeton.edu/wordnet/man/wnstats.7 WN.html</note>
<marker>edu, </marker>
<rawString>WordNet Statistics WordNet.princeton.edu. Retrieved2014-03-11 http://wordnet.princeton.edu/wordnet/man/wnstats.7 WN.html</rawString>
</citation>
<citation valid="false">
<date>1996</date>
<institution>Reuters Corpus English Language,</institution>
<marker>1996</marker>
<rawString>Reuters Corpus English Language, 1996/08/20-1997/08/19.</rawString>
</citation>
<citation valid="false">
<note>http://about.reuters.com/researchandstandards/corpus/ JWNL (Java WordNet Library) http://sourceforge.net/projects/jwordnet/</note>
<marker></marker>
<rawString>http://about.reuters.com/researchandstandards/corpus/ JWNL (Java WordNet Library) http://sourceforge.net/projects/jwordnet/</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>