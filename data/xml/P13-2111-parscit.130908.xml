<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000044">
<title confidence="0.968001">
Efficient Implementation of Beam-Search Incremental Parsers∗
</title>
<author confidence="0.9971">
Yoav Goldberg
</author>
<affiliation confidence="0.992653">
Dept. of Computer Science
Bar-Ilan University
</affiliation>
<address confidence="0.551341">
Ramat Gan, Tel Aviv, 5290002 Israel
</address>
<email confidence="0.98451">
yoav.goldberg@gmail.com
</email>
<author confidence="0.997716">
Kai Zhao Liang Huang
</author>
<affiliation confidence="0.992843">
Graduate Center and Queens College
City University of New York
</affiliation>
<email confidence="0.7295175">
{kzhao@gc, lhuang@cs.qc}.cuny.edu
{kzhao.hf,liang.huang.sh}.gmail.com
</email>
<sectionHeader confidence="0.981277" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999963842105263">
Beam search incremental parsers are ac-
curate, but not as fast as they could be.
We demonstrate that, contrary to popu-
lar belief, most current implementations
of beam parsers in fact run in O(n2),
rather than linear time, because each state-
transition is actually implemented as an
O(n) operation. We present an improved
implementation, based on Tree Structured
Stack (TSS), in which a transition is per-
formed in O(1), resulting in a real linear-
time algorithm, which is verified empiri-
cally. We further improve parsing speed
by sharing feature-extraction and dot-
product across beam items. Practically,
our methods combined offer a speedup of
∼2x over strong baselines on Penn Tree-
bank sentences, and are orders of magni-
tude faster on much longer sentences.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.994572224137931">
Beam search incremental parsers (Roark, 2001;
Collins and Roark, 2004; Zhang and Clark, 2008;
Huang et al., 2009; Huang and Sagae, 2010;
Zhang and Nivre, 2011; Zhang and Clark, 2011)
provide very competitive parsing accuracies for
various grammar formalisms (CFG, CCG, and de-
pendency grammars). In terms of purning strate-
gies, they can be broadly divided into two cat-
egories: the first group (Roark, 2001; Collins
and Roark, 2004) uses soft (aka probabilistic)
beams borrowed from bottom-up parsers (Char-
niak, 2000; Collins, 1999) which has no control
of complexity, while the second group (the rest
and many more recent ones) employs hard beams
borrowed from machine translation (Koehn, 2004)
which guarantee (as they claim) a linear runtime
O(kn) where k is the beam width. However, we
will demonstrate below that, contrary to popular
∗Supported in part by DARPA FA8750-13-2-0041 (DEFT).
belief, in most standard implementations their ac-
tual runtime is in fact O(kn2) rather than linear.
Although this argument in general also applies to
dynamic programming (DP) parsers,1 in this pa-
per we only focus on the standard, non-dynamic
programming approach since it is arguably still the
dominant practice (e.g. it is easier with the popular
arc-eager parser with a rich feature set (Kuhlmann
et al., 2011; Zhang and Nivre, 2011)) and it bene-
fits more from our improved algorithms.
The dependence on the beam-size k is because
one needs to do k-times the number of basic opera-
tions (feature-extractions, dot-products, and state-
transitions) relative to a greedy parser (Nivre and
Scholz, 2004; Goldberg and Elhadad, 2010). Note
that in a beam setting, the same state can expand
to several new states in the next step, which is usu-
ally achieved by copying the state prior to making
a transition, whereas greedy search only stores one
state which is modified in-place.
Copying amounts to a large fraction of the
slowdown of beam-based with respect to greedy
parsers. Copying is expensive, because the state
keeps track of (a) a stack and (b) the set of
dependency-arcs added so far. Both the arc-set and
the stack can grow to O(n) size in the worst-case,
making the state-copy (and hence state-transition)
an O(n) operation. Thus, beam search imple-
mentations that copy the entire state are in fact
quadratic O(kn2) and not linear, with a slowdown
factor of O(kn) with respect to greedy parsers,
which is confirmed empirically in Figure 4.
We present a way of decreasing the O(n) tran-
sition cost to O(1) achieving strictly linear-time
parsing, using a data structure of Tree-Structured
Stack (TSS) that is inspired by but simpler than
the graph-structured stack (GSS) of Tomita (1985)
used in dynamic programming (Huang and Sagae,
2010).2 On average Treebank sentences, the TSS
</bodyText>
<footnote confidence="0.99930825">
1The Huang-Sagae DP parser (http://acl.cs.qc.edu)
does run in O(kn), which inspired this paper when we ex-
perimented with simulating non-DP beam search using GSS.
2Our notion of TSS is crucially different from the data
</footnote>
<page confidence="0.852987">
628
</page>
<note confidence="0.373936">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 628–633,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<equation confidence="0.934622428571429">
input: w0 ... wn−1
axiom 0 : (0, E): 0
E : (j, S|s1|s0) : A
E + 1 : (j, S|s0) : A U {s1&amp;quot;s0}
E : (j, S|s1|s0) : A
E + 1 : (j, S|s1) : A U {s1&amp;quot;s0}
goal 2n — 1 : (n, s0): A
</equation>
<figureCaption confidence="0.967905">
Figure 1: An abstraction of the arc-standard de-
</figureCaption>
<bodyText confidence="0.963878388888889">
ductive system Nivre (2008). The stack S is a list
of heads, j is the index of the token at the front of
the buffer, and E is the step number (beam index).
A is the arc-set of dependency arcs accumulated
so far, which we will get rid of in Section 4.1.
version, being linear time, leads to a speedup of
2x-2.7x over the naive implementation, and about
1.3x-1.7x over the optimized baseline presented
in Section 5.
Having achieved efficient state-transitions, we
turn to feature extraction and dot products (Sec-
tion 6). We present a simple scheme of sharing
repeated scoring operations across different beam
items, resulting in an additional 7 to 25% speed in-
crease. On Treebank sentences, the methods com-
bined lead to a speedup of —2x over strong base-
lines (—10x over naive ones), and on longer sen-
tences they are orders of magnitude faster.
</bodyText>
<sectionHeader confidence="0.968522" genericHeader="method">
2 Beam Search Incremental Parsing
</sectionHeader>
<bodyText confidence="0.9981037">
We assume familiarity with transition-based de-
pendency parsing. The unfamiliar reader is re-
ferred to Nivre (2008). We briefly describe a
standard shift-reduce dependency parser (which is
called “arc-standard” by Nivre) to establish nota-
tion. Parser states (sometimes called configura-
tions) are composed of a stack, a buffer, and an
arc-set. Parsing transitions are applied to states,
and result in new states. The arc-standard system
has three kinds of transitions: SHIFT, REDUCEL,
</bodyText>
<footnote confidence="0.689020857142857">
structure with the same name in an earlier work of Tomita
(1985). In fact, Tomita’s TSS merges the top portion of the
stacks (more like GSS) while ours merges the bottom por-
tion. We thank Yue Zhang for informing us that TSS was
already implemented for the CCG parser in zpar (http://
sourceforge.net/projects/zpar/) though it was not men-
tioned in his paper (Zhang and Clark, 2011).
</footnote>
<bodyText confidence="0.999898333333333">
and REDUCER, which are summarized in the de-
ductive system in Figure 1. The SHIFT transition
removes the first word from the buffer and pushes
it to the stack, and the REDUCEL and REDUCER
actions each add a dependency relation between
the two words on the top of the stack (which is
achieved by adding the arc s1&amp;quot;s0 or s1&amp;quot;s0 to the
arc-set A), and pops the new dependent from the
stack. When reaching the goal state the parser re-
turns a tree composed of the arcs in the arc-set.
At parsing time, transitions are chosen based on
a trained scoring model which looks at features
of the state. In a beam parser, k items (hypothe-
ses) are maintained. Items are composed of a state
and a score. At step i, each of the k items is ex-
tended by applying all possible transitions to the
given state, resulting in k x a items, a being the
number of possible transitions. Of these, the top
scoring k items are kept and used in step i + 1. Fi-
nally, the tree associated with the highest-scoring
item is returned.
</bodyText>
<sectionHeader confidence="0.982714" genericHeader="method">
3 The Common Implementation of State
</sectionHeader>
<bodyText confidence="0.908147741935484">
The stack is usually represented as a list or an array
of token indices, and the arc-set as an array heads
of length n mapping the word at position m to the
index of its parent. In order to allow for fast fea-
ture extraction, additional arrays are used to map
each token to its left-most and right-most modi-
fier, which are used in most incremental parsers,
e.g. (Huang and Sagae, 2010; Zhang and Nivre,
2011). The buffer is usually implemented as a
pointer to a shared sentence object, and an index j
to the current front of the buffer. Finally, it is com-
mon to keep an additional array holding the tran-
sition sequence leading to the current state, which
can be represented compactly as a pointer to the
previous state and the current action. The state
structure is summarized below:
class state
stack[n] of token_ids
array[n] heads
array[n] leftmost_modifiers
array[n] rightmost_modifiers
int j
int last_action
state previous
In a greedy parser, state transition is performed in-
place. However, in a beam parser the states cannot
be modified in place, and a state transition oper-
ation needs to result in a new, independent state
object. The common practice is to copy the cur-
rent state, and then update the needed fields in the
copy. Copying a stack and arrays of size n is an
</bodyText>
<equation confidence="0.940168333333333">
j &lt; n
E + 1 : (j + 1, S|wj) : A
SHIFT
E : (j, S) : A
REDUCEL
REDUCER
</equation>
<page confidence="0.973327">
629
</page>
<bodyText confidence="0.932034">
O(n) operation. In what follows, we present a way
to perform transitions in O(1).
</bodyText>
<sectionHeader confidence="0.993317" genericHeader="method">
4 Efficient State Transitions
</sectionHeader>
<subsectionHeader confidence="0.998373">
4.1 Distributed Representation of Trees
</subsectionHeader>
<bodyText confidence="0.9989895">
The state needs to keep track of the set of arcs
added to the tree so far for two reasons:
</bodyText>
<listItem confidence="0.886055">
(a) In order to return the complete tree at the end.
(b) In order to compute features when parsing.
</listItem>
<bodyText confidence="0.964330714285714">
Observe that we do not in fact need to store any
arc in order to achieve (a) – we could reconstruct
the entire set by backtracking once we reach the
final configuration. Hence, the arc-set in Figure 1
is only needed for computing features. Instead of
storing the entire arc-set, we could keep only the
information needed for feature computation. In
the feature set we use (Huang and Sagae, 2010),
we need access to (1) items on the buffer, (2)
the 3 top-most elements of the stack, and (3) the
current left-most and right-most modifiers of the
two topmost stack elements. The left-most and
right-most modifiers are already kept in the state
representation, but store more information than
needed: we only need to keep track of the mod-
ifiers of current stack items. Once a token is re-
moved from the stack it will never return, and we
will not need access to its modifiers again. We
can therefore remove the left/rightmost modifier
arrays, and instead have the stack store triplets
(token, leftmost—mod, rightmost—mod). The
heads array is no longer needed. Our new state
representation becomes:
class state
stack[n] of (tok, left, right)
int j
int last—action
state previous
</bodyText>
<subsectionHeader confidence="0.99094">
4.2 Tree Structured Stack: TSS
</subsectionHeader>
<bodyText confidence="0.9950538">
We now turn to handle the stack. Notice that the
buffer, which is also of size O(n), is represented
as a pointer to an immutable shared object, and is
therefore very efficient to copy. We would like to
treat the stack in a similar fashion.
An immutable stack can be implemented func-
tionally as a cons list, where the head is the top
of the stack and the tail is the rest of the stack.
Pushing an item to the stack amounts to adding a
new head link to the list and returning it. Popping
an item from the stack amounts to returning the
tail of the list. Notice that, crucially, a pop opera-
tion does not change the underlying list at all, and
a push operation only adds to the front of a list.
Thus, the stack operations are non-destructive, in
the sense that once you hold a reference to a stack,
the view of the stack through this reference does
not change regardless of future operations that are
applied to the stack. Moreover, push and pop op-
erations are very efficient. This stack implementa-
tion is an example of a persistent data structure – a
data structure inspired by functional programming
which keeps the old versions of itself intact when
modified (Okasaki, 1999).
While each client sees the stack as a list, the un-
derlying representation is a tree, and clients hold
pointers to nodes in the tree. A push operation
adds a branch to the tree and returns the new
pointer, while a pop operation returns the pointer
of the parent, see Figure 3 for an example. We call
this representation a tree-structured stack (TSS).
Using this stack representation, we can replace
the O(n) stack by an integer holding the item at
the top of the stack (s0), and a pointer to the tail of
the stack (tail). As discussed above, in addition
to the top of the stack we also keep its leftmost and
rightmost modifiers s0L and s0R. The simplified
state representation becomes:
class state
int s0, s0L, s0R
state tail
int j
int last—action
state previous
State is now reduced to seven integers, and the
transitions can be implemented very efficiently as
we show in Figure 2. The parser state is trans-
formed into a compact object, and state transitions
are O(1) operations involving only a few pointer
lookups and integer assignments.
</bodyText>
<subsectionHeader confidence="0.966899">
4.3 TSS vs. GSS; Space Complexity
</subsectionHeader>
<bodyText confidence="0.999923428571429">
TSS is inspired by the graph-structured stack
(GSS) used in the dynamic-programming parser of
Huang and Sagae (2010), but without reentrancy
(see also Footnote 2). More importantly, the state
signature in TSS is much slimmer than that in
GSS. Using the notation of Huang and Sagae, in-
stead of maintaining the full DP signature of
</bodyText>
<equation confidence="0.852433">
�fDP(j, S) = (j, fd(sd), ... , f0(s0))
</equation>
<bodyText confidence="0.999973">
where sd denotes the dth tree on stack, in non-DP
TSS we only need to store the features f0(s0) for
the final tree on the stack,
</bodyText>
<equation confidence="0.985763173913043">
�fnoDP(j, S) = (j, f0(s0)),
630
def Shift(state)
newstate.s0 = state.j
newstate.s0L = None
newstate.s0R = None
newstate.tail = state
newstate.j = state.j + 1
return newstate
def ReduceL(state)
newstate.s0 = state.s0
newstate.s0L = state.tail.s0
newstate.s0R = state.s0R
newstate.tail = state.tail.tail
newstate.j = j
return newstate
def ReduceR(state)
newstate.s0 = state.tail.s0
newstate.s0L = state.tail.s0L
newstate.s0R = state.s0
newstate.tail = state.tail.tail
newstate.j = j
return newstate
</equation>
<figureCaption confidence="0.99833725">
Figure 2: State transitions implementation in the TSS representation (see Fig. 3 for the tail pointers).
The two lines on s0L and s0R are specific to feature set design, and can be expanded for richer feature
sets. To conserve space, we do not show the obvious assignments to last_action and previous.
Figure 3: Example of tree-structured stack. The
</figureCaption>
<bodyText confidence="0.930387157894737">
forward arrows denote state transitions, and the
dotted backward arrows are the tail pointers to
the stack tail. The boxes denote the top-of-stack at
each state. Notice that for b = shift(a) we perform
a single push operation getting b.tail = a, while
for b = reduce(a) transition we perform two pops
and a push, resulting in b.tail = a.tail.tail.
thanks to the uniqueness of tail pointers (“left-
pointers” in Huang and Sagae).
In terms of space complexity, each state is re-
duced from O(n) in size to O(d) with GSS and
to O(1) with TSS,3 making it possible to store the
entire beam in O(kn) space. Moreover, the con-
stant state-size makes memory management easier
and reduces fragmentation, by making it possible
to pre-allocate the entire beam upfront. We did
not explore its empirical implications in this work,
as our implementation language, Python, does not
support low-level memory management.
</bodyText>
<subsectionHeader confidence="0.998255">
4.4 Generality of the Approach
</subsectionHeader>
<bodyText confidence="0.9998515">
We presented a concrete implementation for the
arc-standard system with a relatively simple (yet
state-of-the-art) feature set. As in Kuhlmann et
al. (2011), our approach is also applicable to
other transitions systems and richer feature-sets
with some additional book-keeping. A well-
</bodyText>
<footnote confidence="0.9808635">
3For example, a GSS state in Huang and Sagae’s experi-
ments also stores s1, s1L, s1R, s2 besides the fo(so) fea-
tures (s0, s0L, s0R) needed by TSS. d is treated as a con-
stant by Huang and Sagae but actually it could be a variable.
</footnote>
<bodyText confidence="0.96902875">
documented Python implementation for the la-
beled arc-eager system with the rich feature set
of Zhang and Nivre (2011) is available on the first
author’s homepage.
</bodyText>
<sectionHeader confidence="0.990319" genericHeader="method">
5 Fewer Transitions: Lazy Expansion
</sectionHeader>
<bodyText confidence="0.9997963125">
Another way of decreasing state-transition costs
is making less transitions to begin with: instead
of performing all possible transitions from each
beam item and then keeping only k of the re-
sulting states, we could perform only transitions
that are sure to end up on the next step in the
beam. This is done by first computing transition
scores from each beam item, then keeping the top
k highest scoring (state, action) pairs, perform-
ing only those k transitions. This technique is
especially important when the number of possi-
ble transitions is large, such as in labeled parsing.
The technique, though never mentioned in the lit-
erature, was employed in some implementations
(e.g., Yue Zhang’s zpar). We mention it here for
completeness since it’s not well-known yet.
</bodyText>
<sectionHeader confidence="0.967308" genericHeader="method">
6 (Partial) Feature Sharing
</sectionHeader>
<bodyText confidence="0.999847055555556">
After making the state-transition efficient, we turn
to deal with the other major expensive operation:
feature-extractions and dot-products. While we
can’t speed up the process, we observe that some
computations are repeated in different parts of the
beam, and propose to share these computations.
Notice that relatively few token indices from a
state can determine the values of many features.
For example, knowing the buffer index j deter-
mines the words and tags of items after location
j on the buffer, as well as features composed of
combinations of these values.
Based on this observation we propose the no-
tion of a state signature, which is a set of token
indices. An example of a state signature would
be sig(state) = (s0, s0L, s1, s1L), indicating the
indices of the two tokens at the top of the stack to-
gether with their leftmost modifiers. Given a sig-
</bodyText>
<figure confidence="0.989129833333333">
3
4
b
sh
c
c
0
L
L
sh sh sh sh
a b c d
R
R
b
sh
a
c
1 2
</figure>
<page confidence="0.830794">
631
</page>
<figureCaption confidence="0.956837">
Figure 4: Non-linearity of the standard beam
</figureCaption>
<bodyText confidence="0.969849956521739">
search compared to the linearity of our TSS beam
search for labeled arc-eager and unlabeled arc-
standard parsers on long sentences (running times
vs. sentence length). All parsers use beam size 8.
nature, we decompose the feature function φ(x)
into two parts φ(x) = φs(sig(x)) + φo(x), where
φs(sig(x)) extracts all features that depend exclu-
sively on signature items, and φo(x) extracts all
other features.4 The scoring function w · φ(x) de-
composes into w · φs(sig(x)) + w · φo(x). Dur-
ing beam decoding, we maintain a cache map-
ping seen signatures sig(state) to (partial) tran-
sition scores w · φs(sig(state)). We now need
to calculate w · φo(x) for each beam item, but
w · φs(sig(x)) only for one of the items sharing
the signature. Defining the signature involves a
natural balance between signatures that repeat of-
ten and signatures that cover many features. In the
experiments in this paper, we chose the signature
function for the arc-standard parser to contain all
core elements participating in feature extraction5,
and for the arc-eager parser a signature containing
only a partial subset.6
</bodyText>
<sectionHeader confidence="0.999638" genericHeader="evaluation">
7 Experiments
</sectionHeader>
<bodyText confidence="0.999374166666667">
We implemented beam-based parsers using the
traditional approach as well as with our proposed
extension and compared their runtime.
The first experiment highlights the non-linear
behavior of the standard implementation, com-
pared to the linear behavior of the TSS method.
</bodyText>
<footnote confidence="0.8645135">
4One could extend the approach further to use several sig-
natures and further decompose the feature function. We did
not pursue this idea in this work.
5s0,s0L,s0R,s1,s1L,s1R,s2,j.
6s0, s0L, s0R,s0h,b0L,j, where s0h is the parent of
s0, and b0L is the leftmost modifier of j.
</footnote>
<table confidence="0.997440833333333">
system plain plain plain plain +TSS+lazy
(sec 3) +TSS +lazy +TSS +feat-share
(sec 4) (sec 5) +lazy (sec 6)
ArcS-U 20.8 38.6 24.3 41.1 47.4
ArcE-U 25.4 48.3 38.2 58.2 72.3
ArcE-L 1.8 4.9 11.1 14.5 17.3
</table>
<tableCaption confidence="0.826522">
Table 1: Parsing speeds for the different tech-
niques measured in sentences/sec (beam size 8).
</tableCaption>
<bodyText confidence="0.989964483870968">
All parsers are implemented in Python, with dot-
products in C. ArcS/ArcE denotes arc-standard
vs. arc-eager, L/U labeled (stanford deps, 49 la-
bels) vs. unlabeled parsing. ArcS use feature set
of Huang and Sagae (2010) (50 templates), and ArcE
that of Zhang and Nivre (2011) (72 templates).
As parsing time is dominated by score computa-
tion, the effect is too small to be measured on
natural language sentences, but it is noticeable
for longer sentences. Figure 4 plots the runtime
for synthetic examples with lengths ranging from
50 to 1000 tokens, which are generated by con-
catenating sentences from Sections 22–24 of Penn
Treebank (PTB), and demonstrates the non-linear
behavior (dataset included). We argue parsing
longer sentences is by itself an interesting and
potentially important problem (e.g. for other lan-
guages such as Arabic and Chinese where word
or sentence boundaries are vague, and for pars-
ing beyond sentence-level, e.g. discourse parsing
or parsing with inter-sentence dependencies).
Our next set of experiments compares the actual
speedup observed on English sentences. Table 1
shows the speed of the parsers (sentences/sec-
ond) with the various proposed optimization tech-
niques. We first train our parsers on Sections 02–
21 of PTB, using Section 22 as the test set. The
accuracies of all our parsers are at the state-of-
the-art level. The final speedups are up to 10x
against naive baselines and ∼2x against the lazy-
transitions baselines.
</bodyText>
<sectionHeader confidence="0.999352" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999977">
We demonstrated in both theory and experiments
that the standard implementation of beam search
parsers run in O(n2) time, and have presented im-
proved algorithms which run in O(n) time. Com-
bined with other techniques, our method offers
significant speedups (∼2x) over strong baselines,
or 10x over naive ones, and is orders of magnitude
faster on much longer sentences. We have demon-
strated that our approach is general and we believe
it will benefit many other incremental parsers.
</bodyText>
<page confidence="0.997503">
632
</page>
<sectionHeader confidence="0.995883" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99991118">
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of NAACL.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of ACL.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Yoav Goldberg and Michael Elhadad. 2010. An ef-
ficient algorithm for easy-first non-directional de-
pendency parsing. In Proceedings of HLT-NAACL,
pages 742–750.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of ACL 2010.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of EMNLP.
Philipp Koehn. 2004. Pharaoh: a beam search de-
coder for phrase-based statistical machine transla-
tion models. In Proceedings of AMTA, pages 115–
124.
Marco Kuhlmann, Carlos Gmez-Rodrguez, and Gior-
gio Satta. 2011. Dynamic programming algorithms
for transition-based dependency parsers. In Pro-
ceedings of ACL.
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of english text. In Proceedings
of COLING, Geneva.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34(4):513–553.
Chris Okasaki. 1999. Purely functional data struc-
tures. Cambridge University Press.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249–276.
Masaru Tomita. 1985. An efficient context-free pars-
ing algorithm for natural languages. In Proceedings
of the 9th international joint conference on Artificial
intelligence - Volume 2, pages 756–764.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In Proceedings of EMNLP.
Yue Zhang and Stephen Clark. 2011. Shift-reduce ccg
parsing. In Proceedings of ACL.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL, pages 188–193.
</reference>
<page confidence="0.999145">
633
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.542115">
<title confidence="0.920878">Implementation of Beam-Search Incremental</title>
<author confidence="0.741327">Yoav</author>
<affiliation confidence="0.8662335">Dept. of Computer Bar-Ilan</affiliation>
<address confidence="0.987819">Ramat Gan, Tel Aviv, 5290002</address>
<email confidence="0.998778">yoav.goldberg@gmail.com</email>
<author confidence="0.999761">Kai Zhao Liang Huang</author>
<affiliation confidence="0.9885965">Graduate Center and Queens City University of New</affiliation>
<abstract confidence="0.9991282">Beam search incremental parsers are accurate, but not as fast as they could be. We demonstrate that, contrary to popular belief, most current implementations beam parsers in fact run in rather than linear time, because each statetransition is actually implemented as an We present an improved based on Structured in which a transition is perin resulting in a real lineartime algorithm, which is verified empirically. We further improve parsing speed by sharing feature-extraction and dotproduct across beam items. Practically, our methods combined offer a speedup of over strong baselines on Penn Treebank sentences, and are orders of magnitude faster on much longer sentences.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="1628" citStr="Charniak, 2000" startWordPosition="242" endWordPosition="244">enn Treebank sentences, and are orders of magnitude faster on much longer sentences. 1 Introduction Beam search incremental parsers (Roark, 2001; Collins and Roark, 2004; Zhang and Clark, 2008; Huang et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Zhang and Clark, 2011) provide very competitive parsing accuracies for various grammar formalisms (CFG, CCG, and dependency grammars). In terms of purning strategies, they can be broadly divided into two categories: the first group (Roark, 2001; Collins and Roark, 2004) uses soft (aka probabilistic) beams borrowed from bottom-up parsers (Charniak, 2000; Collins, 1999) which has no control of complexity, while the second group (the rest and many more recent ones) employs hard beams borrowed from machine translation (Koehn, 2004) which guarantee (as they claim) a linear runtime O(kn) where k is the beam width. However, we will demonstrate below that, contrary to popular ∗Supported in part by DARPA FA8750-13-2-0041 (DEFT). belief, in most standard implementations their actual runtime is in fact O(kn2) rather than linear. Although this argument in general also applies to dynamic programming (DP) parsers,1 in this paper we only focus on the stan</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropyinspired parser. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1183" citStr="Collins and Roark, 2004" startWordPosition="170" endWordPosition="173">ime, because each statetransition is actually implemented as an O(n) operation. We present an improved implementation, based on Tree Structured Stack (TSS), in which a transition is performed in O(1), resulting in a real lineartime algorithm, which is verified empirically. We further improve parsing speed by sharing feature-extraction and dotproduct across beam items. Practically, our methods combined offer a speedup of ∼2x over strong baselines on Penn Treebank sentences, and are orders of magnitude faster on much longer sentences. 1 Introduction Beam search incremental parsers (Roark, 2001; Collins and Roark, 2004; Zhang and Clark, 2008; Huang et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Zhang and Clark, 2011) provide very competitive parsing accuracies for various grammar formalisms (CFG, CCG, and dependency grammars). In terms of purning strategies, they can be broadly divided into two categories: the first group (Roark, 2001; Collins and Roark, 2004) uses soft (aka probabilistic) beams borrowed from bottom-up parsers (Charniak, 2000; Collins, 1999) which has no control of complexity, while the second group (the rest and many more recent ones) employs hard beams borrowed from machine t</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="1644" citStr="Collins, 1999" startWordPosition="245" endWordPosition="246">tences, and are orders of magnitude faster on much longer sentences. 1 Introduction Beam search incremental parsers (Roark, 2001; Collins and Roark, 2004; Zhang and Clark, 2008; Huang et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Zhang and Clark, 2011) provide very competitive parsing accuracies for various grammar formalisms (CFG, CCG, and dependency grammars). In terms of purning strategies, they can be broadly divided into two categories: the first group (Roark, 2001; Collins and Roark, 2004) uses soft (aka probabilistic) beams borrowed from bottom-up parsers (Charniak, 2000; Collins, 1999) which has no control of complexity, while the second group (the rest and many more recent ones) employs hard beams borrowed from machine translation (Koehn, 2004) which guarantee (as they claim) a linear runtime O(kn) where k is the beam width. However, we will demonstrate below that, contrary to popular ∗Supported in part by DARPA FA8750-13-2-0041 (DEFT). belief, in most standard implementations their actual runtime is in fact O(kn2) rather than linear. Although this argument in general also applies to dynamic programming (DP) parsers,1 in this paper we only focus on the standard, non-dynami</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Michael Elhadad</author>
</authors>
<title>An efficient algorithm for easy-first non-directional dependency parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>742--750</pages>
<contexts>
<context position="2730" citStr="Goldberg and Elhadad, 2010" startWordPosition="417" endWordPosition="420"> Although this argument in general also applies to dynamic programming (DP) parsers,1 in this paper we only focus on the standard, non-dynamic programming approach since it is arguably still the dominant practice (e.g. it is easier with the popular arc-eager parser with a rich feature set (Kuhlmann et al., 2011; Zhang and Nivre, 2011)) and it benefits more from our improved algorithms. The dependence on the beam-size k is because one needs to do k-times the number of basic operations (feature-extractions, dot-products, and statetransitions) relative to a greedy parser (Nivre and Scholz, 2004; Goldberg and Elhadad, 2010). Note that in a beam setting, the same state can expand to several new states in the next step, which is usually achieved by copying the state prior to making a transition, whereas greedy search only stores one state which is modified in-place. Copying amounts to a large fraction of the slowdown of beam-based with respect to greedy parsers. Copying is expensive, because the state keeps track of (a) a stack and (b) the set of dependency-arcs added so far. Both the arc-set and the stack can grow to O(n) size in the worst-case, making the state-copy (and hence state-transition) an O(n) operation</context>
</contexts>
<marker>Goldberg, Elhadad, 2010</marker>
<rawString>Yoav Goldberg and Michael Elhadad. 2010. An efficient algorithm for easy-first non-directional dependency parsing. In Proceedings of HLT-NAACL, pages 742–750.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kenji Sagae</author>
</authors>
<title>Dynamic programming for linear-time incremental parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="1249" citStr="Huang and Sagae, 2010" startWordPosition="182" endWordPosition="185"> operation. We present an improved implementation, based on Tree Structured Stack (TSS), in which a transition is performed in O(1), resulting in a real lineartime algorithm, which is verified empirically. We further improve parsing speed by sharing feature-extraction and dotproduct across beam items. Practically, our methods combined offer a speedup of ∼2x over strong baselines on Penn Treebank sentences, and are orders of magnitude faster on much longer sentences. 1 Introduction Beam search incremental parsers (Roark, 2001; Collins and Roark, 2004; Zhang and Clark, 2008; Huang et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Zhang and Clark, 2011) provide very competitive parsing accuracies for various grammar formalisms (CFG, CCG, and dependency grammars). In terms of purning strategies, they can be broadly divided into two categories: the first group (Roark, 2001; Collins and Roark, 2004) uses soft (aka probabilistic) beams borrowed from bottom-up parsers (Charniak, 2000; Collins, 1999) which has no control of complexity, while the second group (the rest and many more recent ones) employs hard beams borrowed from machine translation (Koehn, 2004) which guarantee (as they claim) a linear </context>
<context position="3842" citStr="Huang and Sagae, 2010" startWordPosition="604" endWordPosition="607"> can grow to O(n) size in the worst-case, making the state-copy (and hence state-transition) an O(n) operation. Thus, beam search implementations that copy the entire state are in fact quadratic O(kn2) and not linear, with a slowdown factor of O(kn) with respect to greedy parsers, which is confirmed empirically in Figure 4. We present a way of decreasing the O(n) transition cost to O(1) achieving strictly linear-time parsing, using a data structure of Tree-Structured Stack (TSS) that is inspired by but simpler than the graph-structured stack (GSS) of Tomita (1985) used in dynamic programming (Huang and Sagae, 2010).2 On average Treebank sentences, the TSS 1The Huang-Sagae DP parser (http://acl.cs.qc.edu) does run in O(kn), which inspired this paper when we experimented with simulating non-DP beam search using GSS. 2Our notion of TSS is crucially different from the data 628 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 628–633, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics input: w0 ... wn−1 axiom 0 : (0, E): 0 E : (j, S|s1|s0) : A E + 1 : (j, S|s0) : A U {s1&amp;quot;s0} E : (j, S|s1|s0) : A E + 1 : (j, S|s1) : A U {s1&amp;quot;s0} goal</context>
<context position="7666" citStr="Huang and Sagae, 2010" startWordPosition="1284" endWordPosition="1287">g in k x a items, a being the number of possible transitions. Of these, the top scoring k items are kept and used in step i + 1. Finally, the tree associated with the highest-scoring item is returned. 3 The Common Implementation of State The stack is usually represented as a list or an array of token indices, and the arc-set as an array heads of length n mapping the word at position m to the index of its parent. In order to allow for fast feature extraction, additional arrays are used to map each token to its left-most and right-most modifier, which are used in most incremental parsers, e.g. (Huang and Sagae, 2010; Zhang and Nivre, 2011). The buffer is usually implemented as a pointer to a shared sentence object, and an index j to the current front of the buffer. Finally, it is common to keep an additional array holding the transition sequence leading to the current state, which can be represented compactly as a pointer to the previous state and the current action. The state structure is summarized below: class state stack[n] of token_ids array[n] heads array[n] leftmost_modifiers array[n] rightmost_modifiers int j int last_action state previous In a greedy parser, state transition is performed inplace</context>
<context position="9370" citStr="Huang and Sagae, 2010" startWordPosition="1592" endWordPosition="1595">Distributed Representation of Trees The state needs to keep track of the set of arcs added to the tree so far for two reasons: (a) In order to return the complete tree at the end. (b) In order to compute features when parsing. Observe that we do not in fact need to store any arc in order to achieve (a) – we could reconstruct the entire set by backtracking once we reach the final configuration. Hence, the arc-set in Figure 1 is only needed for computing features. Instead of storing the entire arc-set, we could keep only the information needed for feature computation. In the feature set we use (Huang and Sagae, 2010), we need access to (1) items on the buffer, (2) the 3 top-most elements of the stack, and (3) the current left-most and right-most modifiers of the two topmost stack elements. The left-most and right-most modifiers are already kept in the state representation, but store more information than needed: we only need to keep track of the modifiers of current stack items. Once a token is removed from the stack it will never return, and we will not need access to its modifiers again. We can therefore remove the left/rightmost modifier arrays, and instead have the stack store triplets (token, leftmos</context>
<context position="12545" citStr="Huang and Sagae (2010)" startWordPosition="2148" endWordPosition="2151">e stack we also keep its leftmost and rightmost modifiers s0L and s0R. The simplified state representation becomes: class state int s0, s0L, s0R state tail int j int last—action state previous State is now reduced to seven integers, and the transitions can be implemented very efficiently as we show in Figure 2. The parser state is transformed into a compact object, and state transitions are O(1) operations involving only a few pointer lookups and integer assignments. 4.3 TSS vs. GSS; Space Complexity TSS is inspired by the graph-structured stack (GSS) used in the dynamic-programming parser of Huang and Sagae (2010), but without reentrancy (see also Footnote 2). More importantly, the state signature in TSS is much slimmer than that in GSS. Using the notation of Huang and Sagae, instead of maintaining the full DP signature of �fDP(j, S) = (j, fd(sd), ... , f0(s0)) where sd denotes the dth tree on stack, in non-DP TSS we only need to store the features f0(s0) for the final tree on the stack, �fnoDP(j, S) = (j, f0(s0)), 630 def Shift(state) newstate.s0 = state.j newstate.s0L = None newstate.s0R = None newstate.tail = state newstate.j = state.j + 1 return newstate def ReduceL(state) newstate.s0 = state.s0 ne</context>
<context position="19347" citStr="Huang and Sagae (2010)" startWordPosition="3282" endWordPosition="3285">1,s1L,s1R,s2,j. 6s0, s0L, s0R,s0h,b0L,j, where s0h is the parent of s0, and b0L is the leftmost modifier of j. system plain plain plain plain +TSS+lazy (sec 3) +TSS +lazy +TSS +feat-share (sec 4) (sec 5) +lazy (sec 6) ArcS-U 20.8 38.6 24.3 41.1 47.4 ArcE-U 25.4 48.3 38.2 58.2 72.3 ArcE-L 1.8 4.9 11.1 14.5 17.3 Table 1: Parsing speeds for the different techniques measured in sentences/sec (beam size 8). All parsers are implemented in Python, with dotproducts in C. ArcS/ArcE denotes arc-standard vs. arc-eager, L/U labeled (stanford deps, 49 labels) vs. unlabeled parsing. ArcS use feature set of Huang and Sagae (2010) (50 templates), and ArcE that of Zhang and Nivre (2011) (72 templates). As parsing time is dominated by score computation, the effect is too small to be measured on natural language sentences, but it is noticeable for longer sentences. Figure 4 plots the runtime for synthetic examples with lengths ranging from 50 to 1000 tokens, which are generated by concatenating sentences from Sections 22–24 of Penn Treebank (PTB), and demonstrates the non-linear behavior (dataset included). We argue parsing longer sentences is by itself an interesting and potentially important problem (e.g. for other lang</context>
</contexts>
<marker>Huang, Sagae, 2010</marker>
<rawString>Liang Huang and Kenji Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Proceedings of ACL 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Wenbin Jiang</author>
<author>Qun Liu</author>
</authors>
<title>Bilingually-constrained (monolingual) shift-reduce parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1226" citStr="Huang et al., 2009" startWordPosition="178" endWordPosition="181">plemented as an O(n) operation. We present an improved implementation, based on Tree Structured Stack (TSS), in which a transition is performed in O(1), resulting in a real lineartime algorithm, which is verified empirically. We further improve parsing speed by sharing feature-extraction and dotproduct across beam items. Practically, our methods combined offer a speedup of ∼2x over strong baselines on Penn Treebank sentences, and are orders of magnitude faster on much longer sentences. 1 Introduction Beam search incremental parsers (Roark, 2001; Collins and Roark, 2004; Zhang and Clark, 2008; Huang et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Zhang and Clark, 2011) provide very competitive parsing accuracies for various grammar formalisms (CFG, CCG, and dependency grammars). In terms of purning strategies, they can be broadly divided into two categories: the first group (Roark, 2001; Collins and Roark, 2004) uses soft (aka probabilistic) beams borrowed from bottom-up parsers (Charniak, 2000; Collins, 1999) which has no control of complexity, while the second group (the rest and many more recent ones) employs hard beams borrowed from machine translation (Koehn, 2004) which guarantee (a</context>
</contexts>
<marker>Huang, Jiang, Liu, 2009</marker>
<rawString>Liang Huang, Wenbin Jiang, and Qun Liu. 2009. Bilingually-constrained (monolingual) shift-reduce parsing. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: a beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In Proceedings of AMTA,</booktitle>
<pages>115--124</pages>
<contexts>
<context position="1807" citStr="Koehn, 2004" startWordPosition="271" endWordPosition="272">Clark, 2008; Huang et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Zhang and Clark, 2011) provide very competitive parsing accuracies for various grammar formalisms (CFG, CCG, and dependency grammars). In terms of purning strategies, they can be broadly divided into two categories: the first group (Roark, 2001; Collins and Roark, 2004) uses soft (aka probabilistic) beams borrowed from bottom-up parsers (Charniak, 2000; Collins, 1999) which has no control of complexity, while the second group (the rest and many more recent ones) employs hard beams borrowed from machine translation (Koehn, 2004) which guarantee (as they claim) a linear runtime O(kn) where k is the beam width. However, we will demonstrate below that, contrary to popular ∗Supported in part by DARPA FA8750-13-2-0041 (DEFT). belief, in most standard implementations their actual runtime is in fact O(kn2) rather than linear. Although this argument in general also applies to dynamic programming (DP) parsers,1 in this paper we only focus on the standard, non-dynamic programming approach since it is arguably still the dominant practice (e.g. it is easier with the popular arc-eager parser with a rich feature set (Kuhlmann et a</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: a beam search decoder for phrase-based statistical machine translation models. In Proceedings of AMTA, pages 115– 124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Kuhlmann</author>
<author>Carlos Gmez-Rodrguez</author>
<author>Giorgio Satta</author>
</authors>
<title>Dynamic programming algorithms for transition-based dependency parsers.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2415" citStr="Kuhlmann et al., 2011" startWordPosition="367" endWordPosition="370">(Koehn, 2004) which guarantee (as they claim) a linear runtime O(kn) where k is the beam width. However, we will demonstrate below that, contrary to popular ∗Supported in part by DARPA FA8750-13-2-0041 (DEFT). belief, in most standard implementations their actual runtime is in fact O(kn2) rather than linear. Although this argument in general also applies to dynamic programming (DP) parsers,1 in this paper we only focus on the standard, non-dynamic programming approach since it is arguably still the dominant practice (e.g. it is easier with the popular arc-eager parser with a rich feature set (Kuhlmann et al., 2011; Zhang and Nivre, 2011)) and it benefits more from our improved algorithms. The dependence on the beam-size k is because one needs to do k-times the number of basic operations (feature-extractions, dot-products, and statetransitions) relative to a greedy parser (Nivre and Scholz, 2004; Goldberg and Elhadad, 2010). Note that in a beam setting, the same state can expand to several new states in the next step, which is usually achieved by copying the state prior to making a transition, whereas greedy search only stores one state which is modified in-place. Copying amounts to a large fraction of </context>
<context position="14859" citStr="Kuhlmann et al. (2011)" startWordPosition="2520" endWordPosition="2523">tate is reduced from O(n) in size to O(d) with GSS and to O(1) with TSS,3 making it possible to store the entire beam in O(kn) space. Moreover, the constant state-size makes memory management easier and reduces fragmentation, by making it possible to pre-allocate the entire beam upfront. We did not explore its empirical implications in this work, as our implementation language, Python, does not support low-level memory management. 4.4 Generality of the Approach We presented a concrete implementation for the arc-standard system with a relatively simple (yet state-of-the-art) feature set. As in Kuhlmann et al. (2011), our approach is also applicable to other transitions systems and richer feature-sets with some additional book-keeping. A well3For example, a GSS state in Huang and Sagae’s experiments also stores s1, s1L, s1R, s2 besides the fo(so) features (s0, s0L, s0R) needed by TSS. d is treated as a constant by Huang and Sagae but actually it could be a variable. documented Python implementation for the labeled arc-eager system with the rich feature set of Zhang and Nivre (2011) is available on the first author’s homepage. 5 Fewer Transitions: Lazy Expansion Another way of decreasing state-transition c</context>
</contexts>
<marker>Kuhlmann, Gmez-Rodrguez, Satta, 2011</marker>
<rawString>Marco Kuhlmann, Carlos Gmez-Rodrguez, and Giorgio Satta. 2011. Dynamic programming algorithms for transition-based dependency parsers. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Mario Scholz</author>
</authors>
<title>Deterministic dependency parsing of english text.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING,</booktitle>
<location>Geneva.</location>
<contexts>
<context position="2701" citStr="Nivre and Scholz, 2004" startWordPosition="413" endWordPosition="416">kn2) rather than linear. Although this argument in general also applies to dynamic programming (DP) parsers,1 in this paper we only focus on the standard, non-dynamic programming approach since it is arguably still the dominant practice (e.g. it is easier with the popular arc-eager parser with a rich feature set (Kuhlmann et al., 2011; Zhang and Nivre, 2011)) and it benefits more from our improved algorithms. The dependence on the beam-size k is because one needs to do k-times the number of basic operations (feature-extractions, dot-products, and statetransitions) relative to a greedy parser (Nivre and Scholz, 2004; Goldberg and Elhadad, 2010). Note that in a beam setting, the same state can expand to several new states in the next step, which is usually achieved by copying the state prior to making a transition, whereas greedy search only stores one state which is modified in-place. Copying amounts to a large fraction of the slowdown of beam-based with respect to greedy parsers. Copying is expensive, because the state keeps track of (a) a stack and (b) the set of dependency-arcs added so far. Both the arc-set and the stack can grow to O(n) size in the worst-case, making the state-copy (and hence state-</context>
</contexts>
<marker>Nivre, Scholz, 2004</marker>
<rawString>Joakim Nivre and Mario Scholz. 2004. Deterministic dependency parsing of english text. In Proceedings of COLING, Geneva.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Algorithms for deterministic incremental dependency parsing.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="4537" citStr="Nivre (2008)" startWordPosition="733" endWordPosition="734">.qc.edu) does run in O(kn), which inspired this paper when we experimented with simulating non-DP beam search using GSS. 2Our notion of TSS is crucially different from the data 628 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 628–633, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics input: w0 ... wn−1 axiom 0 : (0, E): 0 E : (j, S|s1|s0) : A E + 1 : (j, S|s0) : A U {s1&amp;quot;s0} E : (j, S|s1|s0) : A E + 1 : (j, S|s1) : A U {s1&amp;quot;s0} goal 2n — 1 : (n, s0): A Figure 1: An abstraction of the arc-standard deductive system Nivre (2008). The stack S is a list of heads, j is the index of the token at the front of the buffer, and E is the step number (beam index). A is the arc-set of dependency arcs accumulated so far, which we will get rid of in Section 4.1. version, being linear time, leads to a speedup of 2x-2.7x over the naive implementation, and about 1.3x-1.7x over the optimized baseline presented in Section 5. Having achieved efficient state-transitions, we turn to feature extraction and dot products (Section 6). We present a simple scheme of sharing repeated scoring operations across different beam items, resulting in </context>
</contexts>
<marker>Nivre, 2008</marker>
<rawString>Joakim Nivre. 2008. Algorithms for deterministic incremental dependency parsing. Computational Linguistics, 34(4):513–553.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Okasaki</author>
</authors>
<title>Purely functional data structures.</title>
<date>1999</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="11352" citStr="Okasaki, 1999" startWordPosition="1943" endWordPosition="1944">that, crucially, a pop operation does not change the underlying list at all, and a push operation only adds to the front of a list. Thus, the stack operations are non-destructive, in the sense that once you hold a reference to a stack, the view of the stack through this reference does not change regardless of future operations that are applied to the stack. Moreover, push and pop operations are very efficient. This stack implementation is an example of a persistent data structure – a data structure inspired by functional programming which keeps the old versions of itself intact when modified (Okasaki, 1999). While each client sees the stack as a list, the underlying representation is a tree, and clients hold pointers to nodes in the tree. A push operation adds a branch to the tree and returns the new pointer, while a pop operation returns the pointer of the parent, see Figure 3 for an example. We call this representation a tree-structured stack (TSS). Using this stack representation, we can replace the O(n) stack by an integer holding the item at the top of the stack (s0), and a pointer to the tail of the stack (tail). As discussed above, in addition to the top of the stack we also keep its left</context>
</contexts>
<marker>Okasaki, 1999</marker>
<rawString>Chris Okasaki. 1999. Purely functional data structures. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
</authors>
<title>Probabilistic top-down parsing and language modeling.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="1158" citStr="Roark, 2001" startWordPosition="168" endWordPosition="169">than linear time, because each statetransition is actually implemented as an O(n) operation. We present an improved implementation, based on Tree Structured Stack (TSS), in which a transition is performed in O(1), resulting in a real lineartime algorithm, which is verified empirically. We further improve parsing speed by sharing feature-extraction and dotproduct across beam items. Practically, our methods combined offer a speedup of ∼2x over strong baselines on Penn Treebank sentences, and are orders of magnitude faster on much longer sentences. 1 Introduction Beam search incremental parsers (Roark, 2001; Collins and Roark, 2004; Zhang and Clark, 2008; Huang et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Zhang and Clark, 2011) provide very competitive parsing accuracies for various grammar formalisms (CFG, CCG, and dependency grammars). In terms of purning strategies, they can be broadly divided into two categories: the first group (Roark, 2001; Collins and Roark, 2004) uses soft (aka probabilistic) beams borrowed from bottom-up parsers (Charniak, 2000; Collins, 1999) which has no control of complexity, while the second group (the rest and many more recent ones) employs hard beam</context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>Brian Roark. 2001. Probabilistic top-down parsing and language modeling. Computational Linguistics, 27(2):249–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaru Tomita</author>
</authors>
<title>An efficient context-free parsing algorithm for natural languages.</title>
<date>1985</date>
<booktitle>In Proceedings of the 9th international joint conference on Artificial intelligence -</booktitle>
<volume>2</volume>
<pages>756--764</pages>
<contexts>
<context position="3790" citStr="Tomita (1985)" startWordPosition="598" endWordPosition="599">dded so far. Both the arc-set and the stack can grow to O(n) size in the worst-case, making the state-copy (and hence state-transition) an O(n) operation. Thus, beam search implementations that copy the entire state are in fact quadratic O(kn2) and not linear, with a slowdown factor of O(kn) with respect to greedy parsers, which is confirmed empirically in Figure 4. We present a way of decreasing the O(n) transition cost to O(1) achieving strictly linear-time parsing, using a data structure of Tree-Structured Stack (TSS) that is inspired by but simpler than the graph-structured stack (GSS) of Tomita (1985) used in dynamic programming (Huang and Sagae, 2010).2 On average Treebank sentences, the TSS 1The Huang-Sagae DP parser (http://acl.cs.qc.edu) does run in O(kn), which inspired this paper when we experimented with simulating non-DP beam search using GSS. 2Our notion of TSS is crucially different from the data 628 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 628–633, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics input: w0 ... wn−1 axiom 0 : (0, E): 0 E : (j, S|s1|s0) : A E + 1 : (j, S|s0) : A U {s1&amp;quot;s0} E : (</context>
<context position="5932" citStr="Tomita (1985)" startWordPosition="966" endWordPosition="967"> are orders of magnitude faster. 2 Beam Search Incremental Parsing We assume familiarity with transition-based dependency parsing. The unfamiliar reader is referred to Nivre (2008). We briefly describe a standard shift-reduce dependency parser (which is called “arc-standard” by Nivre) to establish notation. Parser states (sometimes called configurations) are composed of a stack, a buffer, and an arc-set. Parsing transitions are applied to states, and result in new states. The arc-standard system has three kinds of transitions: SHIFT, REDUCEL, structure with the same name in an earlier work of Tomita (1985). In fact, Tomita’s TSS merges the top portion of the stacks (more like GSS) while ours merges the bottom portion. We thank Yue Zhang for informing us that TSS was already implemented for the CCG parser in zpar (http:// sourceforge.net/projects/zpar/) though it was not mentioned in his paper (Zhang and Clark, 2011). and REDUCER, which are summarized in the deductive system in Figure 1. The SHIFT transition removes the first word from the buffer and pushes it to the stack, and the REDUCEL and REDUCER actions each add a dependency relation between the two words on the top of the stack (which is </context>
</contexts>
<marker>Tomita, 1985</marker>
<rawString>Masaru Tomita. 1985. An efficient context-free parsing algorithm for natural languages. In Proceedings of the 9th international joint conference on Artificial intelligence - Volume 2, pages 756–764.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A tale of two parsers: investigating and combining graphbased and transition-based dependency parsing using beam-search.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1206" citStr="Zhang and Clark, 2008" startWordPosition="174" endWordPosition="177">ansition is actually implemented as an O(n) operation. We present an improved implementation, based on Tree Structured Stack (TSS), in which a transition is performed in O(1), resulting in a real lineartime algorithm, which is verified empirically. We further improve parsing speed by sharing feature-extraction and dotproduct across beam items. Practically, our methods combined offer a speedup of ∼2x over strong baselines on Penn Treebank sentences, and are orders of magnitude faster on much longer sentences. 1 Introduction Beam search incremental parsers (Roark, 2001; Collins and Roark, 2004; Zhang and Clark, 2008; Huang et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Zhang and Clark, 2011) provide very competitive parsing accuracies for various grammar formalisms (CFG, CCG, and dependency grammars). In terms of purning strategies, they can be broadly divided into two categories: the first group (Roark, 2001; Collins and Roark, 2004) uses soft (aka probabilistic) beams borrowed from bottom-up parsers (Charniak, 2000; Collins, 1999) which has no control of complexity, while the second group (the rest and many more recent ones) employs hard beams borrowed from machine translation (Koehn, 2004</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. A tale of two parsers: investigating and combining graphbased and transition-based dependency parsing using beam-search. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Shift-reduce ccg parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1296" citStr="Zhang and Clark, 2011" startWordPosition="190" endWordPosition="193">ion, based on Tree Structured Stack (TSS), in which a transition is performed in O(1), resulting in a real lineartime algorithm, which is verified empirically. We further improve parsing speed by sharing feature-extraction and dotproduct across beam items. Practically, our methods combined offer a speedup of ∼2x over strong baselines on Penn Treebank sentences, and are orders of magnitude faster on much longer sentences. 1 Introduction Beam search incremental parsers (Roark, 2001; Collins and Roark, 2004; Zhang and Clark, 2008; Huang et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Zhang and Clark, 2011) provide very competitive parsing accuracies for various grammar formalisms (CFG, CCG, and dependency grammars). In terms of purning strategies, they can be broadly divided into two categories: the first group (Roark, 2001; Collins and Roark, 2004) uses soft (aka probabilistic) beams borrowed from bottom-up parsers (Charniak, 2000; Collins, 1999) which has no control of complexity, while the second group (the rest and many more recent ones) employs hard beams borrowed from machine translation (Koehn, 2004) which guarantee (as they claim) a linear runtime O(kn) where k is the beam width. Howeve</context>
<context position="6248" citStr="Zhang and Clark, 2011" startWordPosition="1018" endWordPosition="1021">. Parser states (sometimes called configurations) are composed of a stack, a buffer, and an arc-set. Parsing transitions are applied to states, and result in new states. The arc-standard system has three kinds of transitions: SHIFT, REDUCEL, structure with the same name in an earlier work of Tomita (1985). In fact, Tomita’s TSS merges the top portion of the stacks (more like GSS) while ours merges the bottom portion. We thank Yue Zhang for informing us that TSS was already implemented for the CCG parser in zpar (http:// sourceforge.net/projects/zpar/) though it was not mentioned in his paper (Zhang and Clark, 2011). and REDUCER, which are summarized in the deductive system in Figure 1. The SHIFT transition removes the first word from the buffer and pushes it to the stack, and the REDUCEL and REDUCER actions each add a dependency relation between the two words on the top of the stack (which is achieved by adding the arc s1&amp;quot;s0 or s1&amp;quot;s0 to the arc-set A), and pops the new dependent from the stack. When reaching the goal state the parser returns a tree composed of the arcs in the arc-set. At parsing time, transitions are chosen based on a trained scoring model which looks at features of the state. In a beam</context>
</contexts>
<marker>Zhang, Clark, 2011</marker>
<rawString>Yue Zhang and Stephen Clark. 2011. Shift-reduce ccg parsing. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Joakim Nivre</author>
</authors>
<title>Transition-based dependency parsing with rich non-local features.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>188--193</pages>
<contexts>
<context position="1272" citStr="Zhang and Nivre, 2011" startWordPosition="186" endWordPosition="189">an improved implementation, based on Tree Structured Stack (TSS), in which a transition is performed in O(1), resulting in a real lineartime algorithm, which is verified empirically. We further improve parsing speed by sharing feature-extraction and dotproduct across beam items. Practically, our methods combined offer a speedup of ∼2x over strong baselines on Penn Treebank sentences, and are orders of magnitude faster on much longer sentences. 1 Introduction Beam search incremental parsers (Roark, 2001; Collins and Roark, 2004; Zhang and Clark, 2008; Huang et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Zhang and Clark, 2011) provide very competitive parsing accuracies for various grammar formalisms (CFG, CCG, and dependency grammars). In terms of purning strategies, they can be broadly divided into two categories: the first group (Roark, 2001; Collins and Roark, 2004) uses soft (aka probabilistic) beams borrowed from bottom-up parsers (Charniak, 2000; Collins, 1999) which has no control of complexity, while the second group (the rest and many more recent ones) employs hard beams borrowed from machine translation (Koehn, 2004) which guarantee (as they claim) a linear runtime O(kn) where k i</context>
<context position="7690" citStr="Zhang and Nivre, 2011" startWordPosition="1288" endWordPosition="1291">ng the number of possible transitions. Of these, the top scoring k items are kept and used in step i + 1. Finally, the tree associated with the highest-scoring item is returned. 3 The Common Implementation of State The stack is usually represented as a list or an array of token indices, and the arc-set as an array heads of length n mapping the word at position m to the index of its parent. In order to allow for fast feature extraction, additional arrays are used to map each token to its left-most and right-most modifier, which are used in most incremental parsers, e.g. (Huang and Sagae, 2010; Zhang and Nivre, 2011). The buffer is usually implemented as a pointer to a shared sentence object, and an index j to the current front of the buffer. Finally, it is common to keep an additional array holding the transition sequence leading to the current state, which can be represented compactly as a pointer to the previous state and the current action. The state structure is summarized below: class state stack[n] of token_ids array[n] heads array[n] leftmost_modifiers array[n] rightmost_modifiers int j int last_action state previous In a greedy parser, state transition is performed inplace. However, in a beam par</context>
<context position="15333" citStr="Zhang and Nivre (2011)" startWordPosition="2603" endWordPosition="2606">nted a concrete implementation for the arc-standard system with a relatively simple (yet state-of-the-art) feature set. As in Kuhlmann et al. (2011), our approach is also applicable to other transitions systems and richer feature-sets with some additional book-keeping. A well3For example, a GSS state in Huang and Sagae’s experiments also stores s1, s1L, s1R, s2 besides the fo(so) features (s0, s0L, s0R) needed by TSS. d is treated as a constant by Huang and Sagae but actually it could be a variable. documented Python implementation for the labeled arc-eager system with the rich feature set of Zhang and Nivre (2011) is available on the first author’s homepage. 5 Fewer Transitions: Lazy Expansion Another way of decreasing state-transition costs is making less transitions to begin with: instead of performing all possible transitions from each beam item and then keeping only k of the resulting states, we could perform only transitions that are sure to end up on the next step in the beam. This is done by first computing transition scores from each beam item, then keeping the top k highest scoring (state, action) pairs, performing only those k transitions. This technique is especially important when the numbe</context>
<context position="19403" citStr="Zhang and Nivre (2011)" startWordPosition="3292" endWordPosition="3295">e parent of s0, and b0L is the leftmost modifier of j. system plain plain plain plain +TSS+lazy (sec 3) +TSS +lazy +TSS +feat-share (sec 4) (sec 5) +lazy (sec 6) ArcS-U 20.8 38.6 24.3 41.1 47.4 ArcE-U 25.4 48.3 38.2 58.2 72.3 ArcE-L 1.8 4.9 11.1 14.5 17.3 Table 1: Parsing speeds for the different techniques measured in sentences/sec (beam size 8). All parsers are implemented in Python, with dotproducts in C. ArcS/ArcE denotes arc-standard vs. arc-eager, L/U labeled (stanford deps, 49 labels) vs. unlabeled parsing. ArcS use feature set of Huang and Sagae (2010) (50 templates), and ArcE that of Zhang and Nivre (2011) (72 templates). As parsing time is dominated by score computation, the effect is too small to be measured on natural language sentences, but it is noticeable for longer sentences. Figure 4 plots the runtime for synthetic examples with lengths ranging from 50 to 1000 tokens, which are generated by concatenating sentences from Sections 22–24 of Penn Treebank (PTB), and demonstrates the non-linear behavior (dataset included). We argue parsing longer sentences is by itself an interesting and potentially important problem (e.g. for other languages such as Arabic and Chinese where word or sentence </context>
</contexts>
<marker>Zhang, Nivre, 2011</marker>
<rawString>Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Proceedings of ACL, pages 188–193.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>