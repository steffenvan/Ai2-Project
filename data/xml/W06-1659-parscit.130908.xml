<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000032">
<title confidence="0.9944015">
Unsupervised Information Extraction Approach Using Graph Mutual
Reinforcement
</title>
<author confidence="0.964392">
Hany Hassan Ahmed Hassan Ossama Emam
</author>
<affiliation confidence="0.947086">
IBM Cairo Technology Development Center
</affiliation>
<address confidence="0.9377065">
Giza, Egypt
P.O. Box 166 Al-Ahram
</address>
<email confidence="0.999127">
hanyh@eg.ibm.com hasanah@eg.ibm.com emam@eg.ibm.com
</email>
<sectionHeader confidence="0.996662" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999809863636364">
Information Extraction (IE) is the task of
extracting knowledge from unstructured
text. We present a novel unsupervised
approach for information extraction
based on graph mutual reinforcement.
The proposed approach does not require
any seed patterns or examples. Instead, it
depends on redundancy in large data sets
and graph based mutual reinforcement to
induce generalized “extraction patterns”.
The proposed approach has been used to
acquire extraction patterns for the ACE
(Automatic Content Extraction) Relation
Detection and Characterization (RDC)
task. ACE RDC is considered a hard task
in information extraction due to the ab-
sence of large amounts of training data
and inconsistencies in the available data.
The proposed approach achieves superior
performance which could be compared to
supervised techniques with reasonable
training data.
</bodyText>
<sectionHeader confidence="0.998889" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999787638297873">
In this paper we propose a novel, and completely
unsupervised approach for information extrac-
tion. We present a general technique; however
we focus on relation extraction as an important
task of Information Extraction. The approach
depends on constructing generalized extraction
patterns, which could match many instances, and
deploys graph based mutual reinforcement to
weight the importance of these patterns. The mu-
tual reinforcement is used to automatically iden-
tify the most informative patterns, where patterns
that match many instances tend to be correct.
Similarly, instances matched by many patterns
tend to be correct. The intuition is that large un-
supervised data is redundant, i.e. different in-
stances of information could be found many
times in different contexts and by different repre-
sentation. The problem can therefore be seen as
hubs (instances) and authorities (patterns) prob-
lem which can be solved using the Hypertext
Induced Topic Selection (HITS) algorithm
(Kleinberg, 1998).
HITS is an algorithmic formulation of the no-
tion of authority in web pages link analysis,
based on a relationship between a set of relevant
“authoritative pages” and a set of “hub pages”.
The HITS algorithm benefits from the following
observation: when a page (hub) links to another
page (authority), the former confers authority
over the latter.
By analogy to the authoritative web pages
problem, we could represent the patterns as au-
thorities and instances as hubs, and use mutual
reinforcement between patterns and instances to
weight the most authoritative patterns. Highly
weighted patterns are then used in extracting in-
formation.
The proposed approach does not need any
seeds or examples. Human involvement is only
needed in determining the entities of interest; the
entities among which we are seeking relations.
The paper proceeds as follows: in Section 2
we discuss previous work followed by a brief
definition of our general notation in Section 3. A
detailed description of the proposed approach
then follows in Section 4. Section 5 discusses the
application of the proposed approach to the prob-
</bodyText>
<page confidence="0.965541">
501
</page>
<note confidence="0.8535585">
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 501–508,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999691333333333">
lem of detecting semantic relations from text.
Section 6 discusses experimental results while
the conclusion is presented in Section 7.
</bodyText>
<sectionHeader confidence="0.989345" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999584102564103">
Most of the previous work on Information Ex-
traction (IE) focused on supervised learning. Re-
lation Detection and Characterization (RDC) was
introduced in the Automatic Content Extraction
Program (ACE) (ACE, 2004). The approaches
proposed to the ACE RDC task such as kernel
methods (Zelenko et al., 2002) and Maximum
Entropy methods (Kambhatla, 2004) required the
availability of large set of human annotated cor-
pora which are tagged with relation instances.
However human annotated instances are limited,
expensive, and time consuming to obtain, due to
the lack of experienced human annotators and the
low inter-annotator agreements.
Some previous work adopted weakly super-
vised or unsupervised learning approaches.
These approaches have the advantage of not
needing large tagged corpora but need seed ex-
amples or seed extraction patterns. The major
drawback of these approaches is their depend-
ency on seed examples or seed patterns which
may lead to limited generalization due to de-
pendency on handcrafted examples. Some of
these approaches are briefed here:
(Brin,98) presented an approach for extracting
the authorship information as found in books de-
scription on the World Wide Web. This tech-
nique is based on dual iterative pattern relation
extraction wherein a relation and pattern set is
iteratively constructed. This approach has two
major drawbacks: the use of handcrafted seed
examples to extract more examples similar to
these handcrafted seed examples and the use of a
lexicon as the main source for extracting infor-
mation.
(Blum and Mitchell, 1998) proposed an ap-
proach based on co-training that uses unlabeled
data in a particular setting. They exploit the fact
that, for some problems, each example can be
described by multiple representations.
(Riloff &amp; Jones, 1999) presented the Meta-
Bootstrapping algorithm that uses an un-
annotated training data set and a set of seeds to
learn a dictionary of extraction patterns and a
domain specific semantic lexicon. Other works
tried to exploit the duality of patterns and their
extractions for the purpose of inferring the se-
mantic class of words like (Thelen &amp; Riloff,
2002) and (Lin et al, 2003).
(Muslea et al., 1999) introduced an inductive
algorithm to generate extraction rules based on
user labeled training examples. This approach
suffers from the labeled data bottleneck.
(Agichtein et. al, 2000) presented an approach
using seed examples to generate initial patterns
and to iteratively obtain further patterns. Then
ad-hoc measures were deployed to estimate the
relevancy of the patterns that have been newly
obtained. The major drawbacks of this approach
are: its dependency on seed examples leads to
limited capability of generalization, and the esti-
mation of patterns relevancy requires the de-
ployment of ad-hoc measures.
(Hasegawa et. al. 2004) introduced unsuper-
vised approach for relation extraction depending
on clustering context words between named enti-
ties; this approach depends on ad-hoc context
similarity between phrases in the context and
focused on certain types of relations.
(Etzioni et al, 2005) proposed a system for
building lists of named entities found on the web.
Their system uses a set of eight domain-
independent extraction patterns to generate can-
didate facts.
All approaches, proposed so far, suffer from
either requiring large amount of labeled data or
the dependency on seed patterns (or examples)
that result in limited generalization.
</bodyText>
<sectionHeader confidence="0.996627" genericHeader="method">
3 General Notation
</sectionHeader>
<bodyText confidence="0.999556681818182">
In graph theory, a graph is a set of objects called
vertices joined by links called edges. A bipartite
graph, also called a bigraph, is a special graph
where the set of vertices can be divided into two
disjoint sets with no two vertices of the same set
sharing an edge.
The Hypertext Induced Topic Selection
(HITS) algorithm is an algorithm for rating, and
therefore ranking, web pages. The HITS algo-
rithm makes use of the following observation:
when a page (hub) links to another page (author-
ity), the former confers authority over the latter.
HITS uses two values for each page, the &amp;quot;author-
ity value&amp;quot; and the &amp;quot;hub value&amp;quot;. &amp;quot;Authority value&amp;quot;
and &amp;quot;hub value&amp;quot; are defined in terms of one an-
other in a mutual recursion. An authority value is
computed as the sum of the scaled hub values
that point to that authority. A hub value is the
sum of the scaled authority values of the authori-
ties it points to.
A template, as we define for this work, is a se-
quence of generic forms that could generalize
</bodyText>
<page confidence="0.995501">
502
</page>
<bodyText confidence="0.9924355">
over the given instances. An example template
is:
</bodyText>
<table confidence="0.713973">
GPE POS (PERSON)+
GPE: Geographical Political En-
tity
POS: possessive ending
PERSON: PERSON Entity
</table>
<bodyText confidence="0.997306166666667">
This template could match the sentence:
“France’s President Jacque Chirac...”. This tem-
plate is derived from the representation of the
Named Entity tags, Part-of-Speech (POS) tags
and semantic tags. The choice of the template
representation here is for illustration purpose
only; any combination of tags, representations
and tagging styles might be used.
A pattern is more specific than a template. A
pattern specifies the role played by the tags (first
entity, second entity, or relation). An example of
a pattern is:
</bodyText>
<equation confidence="0.941887">
GPE(E2) POS (PERSON)+(E1)
</equation>
<bodyText confidence="0.998286583333334">
This pattern indicates that the word(s) with the
tag GPE in the sentence represents the second
en-tity (Entity 2) in the relation, while the
word(s) tagged PERSON represents the first en-
tity (Entity 1) in this relation, the “+” symbol
means that the (PERSON) entity is repetitive (i.e.
may consist of several tokens).
A tuple, in our notation during this paper, is
the result of the application of a pattern to un-
structured text. In the above example, one result
of applying the pattern to some raw text is the
following tuple:
</bodyText>
<table confidence="0.387586666666667">
Entity 1: Jacque Chirac
Entity 2: France
Relation: EMP-Executive
</table>
<sectionHeader confidence="0.925043" genericHeader="method">
4 The Approach
</sectionHeader>
<bodyText confidence="0.9994095">
The unsupervised graph-based mutual rein-
forcement approach, we propose, depends on the
construction of generalized “extraction patterns”
that could match many instances. The patterns
are then weighted according to their importance
by deploying graph based mutual reinforcement
techniques. This duality in patterns and extracted
information (tuples) could be stated that patterns
could match different tuples, and tuples in turn
could be matched by different patterns. The pro-
posed approach is composed of two main steps
namely, initial patterns construction and pattern
weighting or induction. Both steps are detailed in
the next sub-sections.
</bodyText>
<subsectionHeader confidence="0.980111">
4.1 Initial Patterns Construction
</subsectionHeader>
<bodyText confidence="0.944262333333333">
As shown in Figure 1, several syntactic, lexical,
and semantic analyzers could be applied to the
unstructured text. The resulting analyses could be
employed in the construction of extraction pat-
terns. It is worth mentioning that the proposed
approach is general enough to accommodate any
pattern design; the introduced pattern design is
for illustration purposes only.
American vice President Al Gore said today...
</bodyText>
<figureCaption confidence="0.843831">
Figure 1: An example of the output of analys-
ers applied to the unstructured text
</figureCaption>
<bodyText confidence="0.998829">
Initially, we need to start with some templates
and patterns to proceed with the induction proc-
ess. Relatively large amount of text data is
tagged with different taggers to produce the pre-
viously mentioned patterns styles. An n-gram
language model is built on this data and used to
construct weighted finite state machines.
Paths with low cost (high language model
probabilities) are chosen to construct the initial
set of templates; the intuition is that paths with
low cost (high probability) are frequent and
could represent potential candidate patterns.
The resulting initial set of templates is applied
to a very large text data to produce all possible
patterns. The number of candidate initial patterns
could be reduced significantly by specifying the
candidate types of entities; for example we might
specify that the first entity could be PEROSN or
PEOPLE while the second entity could be OR-
GANIZATION, LOCATION, COUNTRY and
etc...
The candidate patterns are then applied to the
tagged stream and the unstructured text to collect
a set of patterns and matched tuples pairs.
The following procedure briefs the Initial Pat-
tern Construction Step:
</bodyText>
<listItem confidence="0.967154">
• Select a random set of text data.
</listItem>
<figure confidence="0.9585202">
Entities PEOPLE O O PERSON O O...
POS ADJ NOUN_PHRASE NNP VBD CD...
Tagged
Stream
PEOPLE NOUN_PHRASE PERSON VBD CD...
</figure>
<page confidence="0.983816">
503
</page>
<listItem confidence="0.999477583333333">
• Apply various taggers on text data and con-
struct templates style.
• Build n-gram language model on template
style data.
• Construct weighted finite state machines
from the n-gram language model.
• Choose n-best paths in the finite state ma-
chines.
• Use best paths as initial templates.
• Apply initial templates on large text data.
• Construct initial patterns and associated tu-
ples sets.
</listItem>
<subsectionHeader confidence="0.94609">
4.2 Pattern Induction
</subsectionHeader>
<bodyText confidence="0.999945833333333">
The inherent duality in the patterns and tuples
relation suggests that the problem could be inter-
preted as a hub authority problem. This problem
could be solved by applying the HITS algorithm
to iteratively assign authority and hub scores to
patterns and tuples respectively.
</bodyText>
<subsectionHeader confidence="0.951501">
Patterns Tuples
</subsectionHeader>
<figureCaption confidence="0.987054">
Figure 2: A bipartite graph represent-
ing patterns and tuples
</figureCaption>
<bodyText confidence="0.9975035">
Patterns and tuples are represented by a bipar-
tite graph as illustrated in figure 2. Each pattern
or tuple is represented by a node in the graph.
Edges represent matching between patterns and
tuples. The pattern induction problem can be
formulated as follows: Given a very large set of
data D containing a large set of patterns P which
match a large set of tuples T, the problem is to
~
identify P , the set of patterns that match the set
of the most correct tuples T . The intuition is
that the tuples matched by many different pat-
terns tend to be correct and the patterns matching
many different tuples tend to be good patterns. In
other words; we want to choose, among the large
space of patterns in the data, the most informa-
tive, highest confidence patterns that could iden-
tify correct tuples; i.e. choosing the most “au-
thoritative” patterns in analogy with the hub au-
thority problem. However, both P ~ and T~ are un-
known. The induction process proceeds as fol-
lows: each pattern p in P is associated with a
numerical authority weight av which expresses
how many tuples match that pattern. Similarly,
each tuple t in T has a numerical hub weight ht
which expresses how many patterns were
matched by this tuple. The weights are calculated
iteratively as follows:
</bodyText>
<equation confidence="0.997745470588235">
( 1) ( ) ( ) ( )
h u
( )
i + i
= � T = p
a p (1)
u 1 i
H ( )
( ) ( ) ( )
a u
( 1 )
i ( )
i
+ P t
h t = (2)
� =
u 1 i A ( )
</equation>
<bodyText confidence="0.999651333333333">
where T(p) is the set of tuples matched by p, P(t)
is the set of patterns matching t, a(i+1)( p) is the
authoritative weight of pattern p at iteration
(i + 1), and h(i+1)( t) is the hub weight of tuple t
at iteration (i + 1) . H(i) and A(i) are normaliza-
tion factors defined as:
</bodyText>
<equation confidence="0.9945295">
( ) =  ||
P T p
( )
H i h i u
( )
� = � = ( ) (3)
p 1 u 1
( ) =  ||
T P t
( )
A i a i u
( )
� = � = ( ) (4)
v 1 u 1
</equation>
<bodyText confidence="0.9954725">
Highly weighted patterns are identified and used
for extracting relations.
</bodyText>
<subsectionHeader confidence="0.998322">
4.3 Tuple Clustering
</subsectionHeader>
<bodyText confidence="0.9999386">
The tuple space should be reduced to allow more
matching between pattern-tuple pairs. This space
reduction could be accomplished by seeking a
tuple similarity measure, and constructing a
weighted undirected graph of tuples. Two tuples
are linked with an edge if their similarity meas-
ure exceeds a certain threshold. Graph clustering
algorithms could be deployed to partition the
graph into a set of homogeneous communities or
clusters. To reduce the space of tuples, we seek a
matching criterion that group similar tuples to-
gether. Using WordNet, we can measure the se-
mantic similarity or relatedness between a pair of
concepts (or word senses), and by extension, be-
tween a pair of sentences. We use the similarity
</bodyText>
<equation confidence="0.996489214285714">
P
P
P
P
T
T
T
T
P
T
P
T
P
T
</equation>
<page confidence="0.984645">
504
</page>
<bodyText confidence="0.999974285714286">
measure described in (Wu and Palmer, 1994)
which finds the path length to the root node
from the least common subsumer (LCS) of the
two word senses which is the most specific word
sense they share as an ancestor. The similarity
score of two tuples, ST, is calculated as follows:
where SE1, and SE2 are the similarity scores of the
first entities in the two tuples, and their second
entitles respectively.
The tuple matching procedure assigns a simi-
larity measure to each pair of tuples in the data-
set. Using this measure we can construct an undi-
rected graph G. The vertices of G are the tuples.
Two vertices are connected with an edge if the
similarity measure between their underlying tu-
ples exceeds a certain threshold. It was noticed
that the constructed graph consists of a set of
semi isolated groups as shown in figure 3. Those
groups have a very large number of inter-group
edges and meanwhile a rather small number of
intra-group edges. This implies that using a
graph clustering algorithm would eliminate those
weak intra-group edges and produce separate
groups or clusters representing similar tuples. We
used Markov Cluster Algorithm (MCL) for graph
clustering (Dongen, 2000). MCL is a fast and
scalable unsupervised clustering algorithm for
graphs based on simulation of stochastic flow.
</bodyText>
<subsectionHeader confidence="0.954698">
Before Clustering After Clustering
</subsectionHeader>
<figureCaption confidence="0.933259">
Figure 3: Applying Clustering Algorithms to Tu-
ple graph
</figureCaption>
<bodyText confidence="0.7892062">
An example of a couple of tuples that could be
matched by this technique is:
United Stated(E2) presi-
dent(E1)
US(E2) leader(E1)
A bipartite graph of patterns and tuple clusters
is constructed. Weights are assigned to patterns
and tuple clusters by iteratively applying the
HITS algorithm and the highly ranked patterns
are then used for relation extraction.
</bodyText>
<sectionHeader confidence="0.998425" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.810246">
5.1 ACE Relation Detection and Charac-
terization
</subsectionHeader>
<bodyText confidence="0.999350444444444">
In this section, we describe Automatic Content
Extraction (ACE). ACE is an evaluation con-
ducted by NIST to measure Entity Detection and
Tracking (EDT) and Relation Detection and
Characterization (RDC). The EDT task is con-
cerned with the detection of mentions of entities,
and grouping them together by identifying their
coreference. The RDC task detects relations be-
tween entities identified by the EDT task. We
choose the RDC task to show the performance of
the graph based unsupervised approach we pro-
pose. To this end we need to introduce the notion
of mentions and entities. Mentions are any in-
stances of textual references to objects like peo-
ple, organizations, geopolitical entities (countries,
cities ...etc), locations, or facilities. On the other
hand, entities are objects containing all mentions
to the same object. Here, we present some exam-
ples of ACE entities and relations:
Spain’s Interior Minister
announced this evening the
arrest of separatist organi-
zation Eta’s presumed leader
Ignacio Garcia Arregui. Ar-
regui, who is considered to
be the Eta organization’s
top man, was arrested at
17h45 Greenwich. The Spanish
judiciary suspects Arregui
of ordering a failed attack
on King Juan Carlos in 1995.
In this fragment, all the underlined phrases are
mentions to “Eta” organization, or to “Garcia
Arregui”. There is a management relation be-
tween “leader” which references to “Gar-
cia Arregui” and “Eta”.
</bodyText>
<subsectionHeader confidence="0.999507">
5.2 Patterns Construction and Induction
</subsectionHeader>
<bodyText confidence="0.99964325">
We used the LDC English Gigaword Corpus,
AFE source from January to August 1996 as a
source for unstructured text. This provides a total
of 99475 documents containing 36 M words. In
the performed experiments, we focus on two
types of relations EMP-ORG relations and GPE-
AFF relations which represent almost 50% of all
relations in RDC – ACE task.
</bodyText>
<equation confidence="0.988653035714286">
T
T T
T
T
T
T
T
T
T
T
T
T
T
T T
T
T T
T
T
T
T
T
T
T
T
T
ST = S E1 + SE
2 2 (5)
2
</equation>
<page confidence="0.994383">
505
</page>
<bodyText confidence="0.937323117647059">
POS (part of speech) tagger and mention tagger
were applied to the data, the used pattern design
consists of a mix between the part of speech
(POS) tags and the mention tags for the words in
the unsupervised data. We use the mention tag, if
it exists; otherwise we use the part of speech tag.
An example of the analyzed text and the pre-
sumed associated pattern is shown:
Text: Eta’s presumed leader
Arregui ...
Pos: NNP POS JJ NN NNP
Mention: ORG 0 0 0 PERSON
Pattern: ORG(E2) POS JJ
NN(R) PERSON(E1)
An n-gram language model, 5-gram model and
back off to lower order n-grams, was built on the
data tagged with the described patterns’ style.
Weighted finite states machines were constructed
with the language model probabilities. The n-best
paths, 20 k paths, were identified and deployed
as the initial template set. Sequences that do not
contain the entities of interest, and hence cannot
represent relations, were automatically filtered
out. This resulted in an initial templates set of
around 3000 element. This initial templates set
was applied on the text data to establish initial
patterns and tuples pairs. Graph based mutual
reinforcement technique was deployed with 10
iterations on the patterns and tuples pairs to
weight the patterns.
We conducted two groups of experiments, the
first with simple syntactic tuple matching, and
the second with semantic tuple clustering as de-
scribed in section 4.3
</bodyText>
<sectionHeader confidence="0.998858" genericHeader="evaluation">
6 Results and Discussion
</sectionHeader>
<bodyText confidence="0.99969545">
We compare our results to a state-of-the-art su-
pervised system similar to the system described
in (Kambhatla, 2004). Although it is unfair to
make a comparison between a supervised system
and a completely unsupervised system, we chose
to make this comparison to test the performance
of the proposed unsupervised approach on a real
task with defined test set and state-of-the-art per-
formance. The supervised system was trained on
145 K words which contain 2368 instances of the
two relation types we are considering.
The system performance is measured using
precision, recall and F-Measure with various
amounts of induced patterns. Table 1 presents the
precision, recall and F-measure for the two rela-
tions using the presented approach with the utili-
zation of different amount of highly weighted
patterns. Table 2 presents the same results using
semantic tuple matching and clustering, as de-
scribed in section 4.3.
</bodyText>
<table confidence="0.9997035">
No. of Precision Recall F-Measure
Patterns
1500 35.9 66.3 46.58
1000 41.2 59.7 48.75
700 43.1 58.1 49.49
500 46 56.5 50.71
400 46.9 52.9 49.72
200 50.1 44.9 47.36
</table>
<tableCaption confidence="0.939701333333333">
Table 1: The effect of varying the number of
induced patterns on the system performance
(syntactic tuple matching)
</tableCaption>
<table confidence="0.999852125">
No. of Precision Recall F-Measure
Patterns
1500 36.1 67.2 46.97
1000 43.7 59.6 50.43
700 44.1 59.3 50.58
500 46.3 57.2 51.18
400 47.3 57.6 51.94
200 48.1 45.9 46.97
</table>
<tableCaption confidence="0.95691">
Table 2: The effect of varying the number of
</tableCaption>
<figureCaption confidence="0.666851">
induced patterns on the system performance (se-
mantic tuple matching)
Figure 4: A comparison between the supervised
system (Sup), the unsupervised system with syn-
tactic tuple matching (Unsup-Syn), and with se-
mantic tuple matching (Unsup-Sem)
</figureCaption>
<bodyText confidence="0.923598166666667">
Best F-Measure is achieved using relatively
small number of induced patterns (400 and 500
patterns) while using more patterns increases the
recall but degrades the precision.
Table 2 indicates that the semantic clustering
of tuples did not provide significant improve-
</bodyText>
<figure confidence="0.97870175">
Sup
Unsup-Syn
Unsup-Sem
60
40
20
80
70
50
30
10
0
Precision
67.1
47.3
46
Recall
54.2
56.5
57.6
F Measure
59.96
50.71
51.94
</figure>
<page confidence="0.996455">
506
</page>
<bodyText confidence="0.993022333333333">
ment; although better performance was achieved
with less number of patterns (400 patterns). We
think that the deployed similarity measure and it
needs further investigation to figure out the rea-
son for that.
Figure 4 presents the comparison between the
proposed unsupervised systems and the reference
supervised system. The unsupervised systems
achieves good results even in comparison to a
state-of-the-art supervised system.
Sample patterns and corresponding matching
text are introduced in Table 3 and Table 4. Table
</bodyText>
<table confidence="0.946195545454546">
3 shows some highly ranked patterns while Table
4 shows examples of low ranked patterns.
Pattern Matches
GPE (PERSON)+ Peruvian President Alberto Fu-
jimori
GPE (PERSON)+ Zimbabwean President Robert
Mugabe
GPE (PERSON)+ PLO leader Yasser Arafat
GPE POS (PERSON)+ Zimbabwe &apos;s President Robert
Mugabe
GPE JJ PERSON American clinical neuropsy-
chologist
GPE JJ PERSON American diplomatic personnel
PERSON IN JJ GPE candidates for local government
ORGANIZATION PER- Airways spokesman
SON
ORGANIZATION PER- Ajax players
SON
PERSON IN DT (OR- chairman of the opposition par-
GANIZATION)+ ties
(ORGANIZATION)+ opposition parties chairmans
PERSON
</table>
<tableCaption confidence="0.822681">
Table3: Examples of patterns with high weights
</tableCaption>
<table confidence="0.9997865">
Pattern Matches
GPE CC (PERSON)+ Barcelona and Johan
Cruyff
GPE , CC PERSON Paris , but Riccardi
GPE VBZ VBN PERSON Pyongyang has accepted
Gallucci
GPE VBZ VBN PERSON Russia has abandoned us
GPE VBZ VBN P PER- Rwanda &apos;s defeated Hutu
SON
GPE VBZ VBN PERSON state has pressed Arafat
GPE VBZ VBN TO VB Taiwan has tried to keep
PERSON Lee
(PERSON)+ VBD GPE Alfred Streim told Ger-
ORGANIZATION man radio
(PERSON)+ VBD GPE Dennis Ross met Syrian
ORGANIZATION army
(PERSON)+ VBD GPE Van Miert told EU indus-
ORGANIZATION try
</table>
<tableCaption confidence="0.948629">
Table4: Examples of patterns with low weights
</tableCaption>
<sectionHeader confidence="0.901976" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999989185185185">
In this work, a general framework for unsuper-
vised information extraction based on mutual
reinforcement in graphs has been introduced. We
construct generalized extraction patterns and de-
ploy graph based mutual reinforcement to auto-
matically identify the most informative patterns.
We provide motivation for our approach from a
graph theory and graph link analysis perspective.
Experimental results have been presented sup-
porting the applicability of the proposed ap-
proach to ACE Relation Detection and Charac-
terization (RDC) task, demonstrating its applica-
bility to hard information extraction problems.
The proposed approach achieves remarkable re-
sults comparable to a state-of-the-art supervised
system, achieving 51.94 F-measure compared to
59.96 F-measure of the state-of-the-art super-
vised system which requires huge amount of hu-
man annotated data. The proposed approach
represents a powerful unsupervised technique for
information extraction in general and particularly
for relations extraction that requires no seed pat-
terns or examples and achieves significant per-
formance.
In our future work, we plan to focus on general-
izing the approach for targeting more NLP prob-
lems.
</bodyText>
<sectionHeader confidence="0.997636" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999927571428571">
We would like to thank Salim Roukos for his
invaluable suggestions and support. We would
also like to thank Hala Mostafa for helping with
the early investigation of this work. Finally we
would like to thank the anonymous reviewers for
their constructive criticism and helpful com-
ments.
</bodyText>
<sectionHeader confidence="0.997551" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9980405">
ACE. 2004. The NIST ACE evaluation website.
http://www.nist.gov/speech/tests/ace/
Eugene Agichtein and Luis Gravano. 2000. Snow-
ball: Extracting Relations from Large Plain-Text
Collections. Proceedings of the 5th ACM Confer-
ence on Digital Libraries (DL 2000).
Sergy Brin. 1998. Extracting Patterns and Relations
from the World Wide Web. Proceedings of the 1998
International Workshop on the Web and Data-
bases”
Stijn van Dongen. 2000. A Cluster Algorithm for
Graphs. Technical Report INS-R0010, National
Research Institute for Mathematics and Computer
Science in the Netherlands.
</reference>
<page confidence="0.969351">
507
</page>
<reference confidence="0.999879747368421">
Stijn van Dongen. 2000. Graph Clustering by Flow
Simulation. PhD thesis, University of Utrecht
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2004. Web-
scale information extraction in KnowItAll (prelimi-
nary results). In Proceedings of the 13th World
Wide Web Conference, pages 100-109.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsu-
pervised Named-Entity Extraction from the Web:
An Experimental Study. Artificial Intelligence,
2005.
Radu Florian, Hany Hassan, Hongyan Jing, Nanda
Kambhatla, Xiaqiang Luo, Nicolas Nicolov, and
Salim Roukos. 2004. A Statistical Model for multi-
lingual entity detection and tracking. Proceedings
of the Human Language Technologies Conference
(HLT-NAACL 2004).
Dayne Freitag, and Nicholas Kushmerick. 2000.
Boosted wrapper induction. The 14th European
Conference on Artificial Intelligence Workshop on
Machine Learning for Information Extraction
Rayid Ghani and Rosie Jones. 2002. A Comparison of
Efficacy and Assumptions of Bootstrapping Algo-
rithms for Training Information Extraction Sys-
tems. Workshop on Linguistic Knowledge Acquisi-
tion and Representation: Bootstrapping Annotated
Data at the Linguistic Resources and Evaluation
Conference (LREC 2002).
Takaaki Hasegawa, Satoshi Sekine, Ralph Grishman.
2004. Discovering Relations among Named Enti-
ties from Large Corpora. Proceedings of The 42nd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL 2004).
Taher Haveliwala. 2002. Topic-sensitive PageRank.
Proceedings of the 11th International World Wide
Web Conference
Thorsten Joachims. 2003. Transductive Learning via
Spectral Graph Partitioning. Proceedings of the In-
ternational Conference on Machine Learning
(ICML 2003).
Nanda Kambhatla. 2004. Combining Lexical, Syntac-
tic, and Semantic Features with Maximum Entropy
Models for Information Extraction. Proceedings of
The 42nd Annual Meeting of the Association for
Computational Linguistics (ACL 2004).
John Kleinberg. 1998. Authoritative Sources in a Hy-
perlinked Environment. Proceedings of the 9th
ACM-SIAM Symposium on Discrete Algorithms.
N. Kushmerick, D.S. Weld, R.B. Doorenbos. 1997.
Wrapper Induction for Information Extraction.
Proceedings of the International Joint Conference
on Artificial Intelligence.
Winston Lin, Roman Yangarber, Ralph Grishman.
2003. Bootstrapped Learning of Semantic Classes
from Positive and Negative Examples. Proceedings
of the 20th International Conference on Machine
Learning (ICML 2003) Workshop on The Contin-
uum from Labeled to Unlabeled Data in Machine
Learning and Data Mining.
Ion Muslea, Steven Minton, and Craig
Knoblock.1999. A hierarchical approach to wrap-
per induction. Proceedings of the Third Interna-
tional Conference on Autonomous Agents.
Ted Pedersen, Siddharth Patwardhan, and Jason
Michelizzi. 2004, WordNet::Similarity - Measuring
the Relatedness of Concepts. Proceedings of Fifth
Annual Meeting of the North American Chapter of
the Association for Computational Linguistics
(NAACL 2004)
Ellen Riloff and Rosie Jones. 2003. Learning diction-
aries for information extraction by multilevel boot-
strapping. Proceedings of the Sixteenth national
Conference on Artificial Intelligence (AAAI 1999).
Michael Thelen and Ellen Riloff. 2002. A Bootstrap-
ping Method for Learning Semantic Lexicons using
Extraction Pattern Contexts. Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2002).
Scott White, and Padhraic Smyth. 2003. Algorithms
for Discoveing Relative Importance in Graphs.
Proceedings of Ninth ACM SIGKDD International
Conference on Knowledge Discovery and Data
Mining.
Zhibiao Wu, and Martha Palmer. 1994. Verb seman-
tics and lexical selection. Proceedings of the 32nd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL 1994).
Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty.
2003. Semi-supervised Learning using Gaussian
Fields and Harmonic Functions. Proceedings of
the 20th International Conference on Machine
Learning (ICML 2003).
</reference>
<page confidence="0.996486">
508
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.821460">
<title confidence="0.9889415">Unsupervised Information Extraction Approach Using Graph Mutual Reinforcement</title>
<author confidence="0.936716">Hany Hassan Ahmed Hassan Ossama Emam</author>
<affiliation confidence="0.990685">IBM Cairo Technology Development</affiliation>
<address confidence="0.965595">Giza, Egypt P.O. Box 166 Al-Ahram</address>
<email confidence="0.995774">hanyh@eg.ibm.comhasanah@eg.ibm.comemam@eg.ibm.com</email>
<abstract confidence="0.996957739130435">Information Extraction (IE) is the task of extracting knowledge from unstructured text. We present a novel unsupervised approach for information extraction based on graph mutual reinforcement. The proposed approach does not require any seed patterns or examples. Instead, it depends on redundancy in large data sets and graph based mutual reinforcement to induce generalized “extraction patterns”. The proposed approach has been used to acquire extraction patterns for the ACE (Automatic Content Extraction) Relation Detection and Characterization (RDC) task. ACE RDC is considered a hard task in information extraction due to the absence of large amounts of training data and inconsistencies in the available data. The proposed approach achieves superior performance which could be compared to supervised techniques with reasonable training data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>ACE</author>
</authors>
<title>The NIST ACE evaluation website. http://www.nist.gov/speech/tests/ace/</title>
<date>2004</date>
<contexts>
<context position="3769" citStr="ACE, 2004" startWordPosition="558" endWordPosition="559">cusses the application of the proposed approach to the prob501 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 501–508, Sydney, July 2006. c�2006 Association for Computational Linguistics lem of detecting semantic relations from text. Section 6 discusses experimental results while the conclusion is presented in Section 7. 2 Previous Work Most of the previous work on Information Extraction (IE) focused on supervised learning. Relation Detection and Characterization (RDC) was introduced in the Automatic Content Extraction Program (ACE) (ACE, 2004). The approaches proposed to the ACE RDC task such as kernel methods (Zelenko et al., 2002) and Maximum Entropy methods (Kambhatla, 2004) required the availability of large set of human annotated corpora which are tagged with relation instances. However human annotated instances are limited, expensive, and time consuming to obtain, due to the lack of experienced human annotators and the low inter-annotator agreements. Some previous work adopted weakly supervised or unsupervised learning approaches. These approaches have the advantage of not needing large tagged corpora but need seed examples o</context>
</contexts>
<marker>ACE, 2004</marker>
<rawString>ACE. 2004. The NIST ACE evaluation website. http://www.nist.gov/speech/tests/ace/</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Agichtein</author>
<author>Luis Gravano</author>
</authors>
<title>Snowball: Extracting Relations from Large Plain-Text Collections.</title>
<date>2000</date>
<booktitle>Proceedings of the 5th ACM Conference on Digital Libraries (DL</booktitle>
<marker>Agichtein, Gravano, 2000</marker>
<rawString>Eugene Agichtein and Luis Gravano. 2000. Snowball: Extracting Relations from Large Plain-Text Collections. Proceedings of the 5th ACM Conference on Digital Libraries (DL 2000).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergy Brin</author>
</authors>
<title>Extracting Patterns and Relations from the World Wide Web.</title>
<date>1998</date>
<booktitle>Proceedings of the 1998 International Workshop on the Web and Databases”</booktitle>
<marker>Brin, 1998</marker>
<rawString>Sergy Brin. 1998. Extracting Patterns and Relations from the World Wide Web. Proceedings of the 1998 International Workshop on the Web and Databases”</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stijn van Dongen</author>
</authors>
<title>A Cluster Algorithm for Graphs.</title>
<date>2000</date>
<tech>Technical Report INS-R0010,</tech>
<institution>National Research Institute for Mathematics and Computer Science</institution>
<note>in the Netherlands.</note>
<marker>van Dongen, 2000</marker>
<rawString>Stijn van Dongen. 2000. A Cluster Algorithm for Graphs. Technical Report INS-R0010, National Research Institute for Mathematics and Computer Science in the Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stijn van Dongen</author>
</authors>
<title>Graph Clustering by Flow Simulation.</title>
<date>2000</date>
<tech>PhD thesis,</tech>
<institution>University of Utrecht</institution>
<marker>van Dongen, 2000</marker>
<rawString>Stijn van Dongen. 2000. Graph Clustering by Flow Simulation. PhD thesis, University of Utrecht</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michael Cafarella</author>
<author>Doug Downey</author>
<author>AnaMaria Popescu</author>
<author>Tal Shaked</author>
<author>Stephen Soderland</author>
<author>Daniel S Weld</author>
<author>Alexander Yates</author>
</authors>
<title>Webscale information extraction in KnowItAll (preliminary results).</title>
<date>2004</date>
<booktitle>In Proceedings of the 13th World Wide Web Conference,</booktitle>
<pages>100--109</pages>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderland, Weld, Yates, 2004</marker>
<rawString>Oren Etzioni, Michael Cafarella, Doug Downey, AnaMaria Popescu, Tal Shaked, Stephen Soderland, Daniel S. Weld, and Alexander Yates. 2004. Webscale information extraction in KnowItAll (preliminary results). In Proceedings of the 13th World Wide Web Conference, pages 100-109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michael Cafarella</author>
<author>Doug Downey</author>
<author>AnaMaria Popescu</author>
<author>Tal Shaked</author>
<author>Stephen Soderland</author>
<author>Daniel S Weld</author>
<author>Alexander Yates</author>
</authors>
<title>Unsupervised Named-Entity Extraction from the Web: An Experimental Study. Artificial Intelligence,</title>
<date>2005</date>
<contexts>
<context position="6638" citStr="Etzioni et al, 2005" startWordPosition="1004" endWordPosition="1007">urther patterns. Then ad-hoc measures were deployed to estimate the relevancy of the patterns that have been newly obtained. The major drawbacks of this approach are: its dependency on seed examples leads to limited capability of generalization, and the estimation of patterns relevancy requires the deployment of ad-hoc measures. (Hasegawa et. al. 2004) introduced unsupervised approach for relation extraction depending on clustering context words between named entities; this approach depends on ad-hoc context similarity between phrases in the context and focused on certain types of relations. (Etzioni et al, 2005) proposed a system for building lists of named entities found on the web. Their system uses a set of eight domainindependent extraction patterns to generate candidate facts. All approaches, proposed so far, suffer from either requiring large amount of labeled data or the dependency on seed patterns (or examples) that result in limited generalization. 3 General Notation In graph theory, a graph is a set of objects called vertices joined by links called edges. A bipartite graph, also called a bigraph, is a special graph where the set of vertices can be divided into two disjoint sets with no two </context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderland, Weld, Yates, 2005</marker>
<rawString>Oren Etzioni, Michael Cafarella, Doug Downey, AnaMaria Popescu, Tal Shaked, Stephen Soderland, Daniel S. Weld, and Alexander Yates. 2005. Unsupervised Named-Entity Extraction from the Web: An Experimental Study. Artificial Intelligence, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Florian</author>
<author>Hany Hassan</author>
<author>Hongyan Jing</author>
<author>Nanda Kambhatla</author>
<author>Xiaqiang Luo</author>
<author>Nicolas Nicolov</author>
<author>Salim Roukos</author>
</authors>
<title>A Statistical Model for multilingual entity detection and tracking.</title>
<date>2004</date>
<booktitle>Proceedings of the Human Language Technologies Conference (HLT-NAACL</booktitle>
<marker>Florian, Hassan, Jing, Kambhatla, Luo, Nicolov, Roukos, 2004</marker>
<rawString>Radu Florian, Hany Hassan, Hongyan Jing, Nanda Kambhatla, Xiaqiang Luo, Nicolas Nicolov, and Salim Roukos. 2004. A Statistical Model for multilingual entity detection and tracking. Proceedings of the Human Language Technologies Conference (HLT-NAACL 2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dayne Freitag</author>
<author>Nicholas Kushmerick</author>
</authors>
<title>Boosted wrapper induction.</title>
<date>2000</date>
<booktitle>The 14th European Conference on Artificial Intelligence Workshop on Machine Learning for Information Extraction</booktitle>
<marker>Freitag, Kushmerick, 2000</marker>
<rawString>Dayne Freitag, and Nicholas Kushmerick. 2000. Boosted wrapper induction. The 14th European Conference on Artificial Intelligence Workshop on Machine Learning for Information Extraction</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rayid Ghani</author>
<author>Rosie Jones</author>
</authors>
<title>A Comparison of Efficacy and Assumptions of Bootstrapping Algorithms for Training Information Extraction Systems.</title>
<date>2002</date>
<booktitle>Workshop on Linguistic Knowledge Acquisition and Representation: Bootstrapping Annotated Data at the Linguistic Resources and Evaluation Conference (LREC</booktitle>
<marker>Ghani, Jones, 2002</marker>
<rawString>Rayid Ghani and Rosie Jones. 2002. A Comparison of Efficacy and Assumptions of Bootstrapping Algorithms for Training Information Extraction Systems. Workshop on Linguistic Knowledge Acquisition and Representation: Bootstrapping Annotated Data at the Linguistic Resources and Evaluation Conference (LREC 2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takaaki Hasegawa</author>
<author>Satoshi Sekine</author>
<author>Ralph Grishman</author>
</authors>
<title>Discovering Relations among Named Entities from Large Corpora.</title>
<date>2004</date>
<booktitle>Proceedings of The 42nd Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<marker>Hasegawa, Sekine, Grishman, 2004</marker>
<rawString>Takaaki Hasegawa, Satoshi Sekine, Ralph Grishman. 2004. Discovering Relations among Named Entities from Large Corpora. Proceedings of The 42nd Annual Meeting of the Association for Computational Linguistics (ACL 2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taher Haveliwala</author>
</authors>
<title>Topic-sensitive PageRank.</title>
<date>2002</date>
<booktitle>Proceedings of the 11th International World Wide Web Conference</booktitle>
<marker>Haveliwala, 2002</marker>
<rawString>Taher Haveliwala. 2002. Topic-sensitive PageRank. Proceedings of the 11th International World Wide Web Conference</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Transductive Learning via Spectral Graph Partitioning.</title>
<date>2003</date>
<booktitle>Proceedings of the International Conference on Machine Learning (ICML</booktitle>
<marker>Joachims, 2003</marker>
<rawString>Thorsten Joachims. 2003. Transductive Learning via Spectral Graph Partitioning. Proceedings of the International Conference on Machine Learning (ICML 2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nanda Kambhatla</author>
</authors>
<title>Combining Lexical, Syntactic, and Semantic Features with Maximum Entropy Models for Information Extraction.</title>
<date>2004</date>
<booktitle>Proceedings of The 42nd Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="3906" citStr="Kambhatla, 2004" startWordPosition="580" endWordPosition="581">anguage Processing (EMNLP 2006), pages 501–508, Sydney, July 2006. c�2006 Association for Computational Linguistics lem of detecting semantic relations from text. Section 6 discusses experimental results while the conclusion is presented in Section 7. 2 Previous Work Most of the previous work on Information Extraction (IE) focused on supervised learning. Relation Detection and Characterization (RDC) was introduced in the Automatic Content Extraction Program (ACE) (ACE, 2004). The approaches proposed to the ACE RDC task such as kernel methods (Zelenko et al., 2002) and Maximum Entropy methods (Kambhatla, 2004) required the availability of large set of human annotated corpora which are tagged with relation instances. However human annotated instances are limited, expensive, and time consuming to obtain, due to the lack of experienced human annotators and the low inter-annotator agreements. Some previous work adopted weakly supervised or unsupervised learning approaches. These approaches have the advantage of not needing large tagged corpora but need seed examples or seed extraction patterns. The major drawback of these approaches is their dependency on seed examples or seed patterns which may lead t</context>
<context position="20438" citStr="Kambhatla, 2004" startWordPosition="3392" endWordPosition="3393">ltered out. This resulted in an initial templates set of around 3000 element. This initial templates set was applied on the text data to establish initial patterns and tuples pairs. Graph based mutual reinforcement technique was deployed with 10 iterations on the patterns and tuples pairs to weight the patterns. We conducted two groups of experiments, the first with simple syntactic tuple matching, and the second with semantic tuple clustering as described in section 4.3 6 Results and Discussion We compare our results to a state-of-the-art supervised system similar to the system described in (Kambhatla, 2004). Although it is unfair to make a comparison between a supervised system and a completely unsupervised system, we chose to make this comparison to test the performance of the proposed unsupervised approach on a real task with defined test set and state-of-the-art performance. The supervised system was trained on 145 K words which contain 2368 instances of the two relation types we are considering. The system performance is measured using precision, recall and F-Measure with various amounts of induced patterns. Table 1 presents the precision, recall and F-measure for the two relations using the</context>
</contexts>
<marker>Kambhatla, 2004</marker>
<rawString>Nanda Kambhatla. 2004. Combining Lexical, Syntactic, and Semantic Features with Maximum Entropy Models for Information Extraction. Proceedings of The 42nd Annual Meeting of the Association for Computational Linguistics (ACL 2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Kleinberg</author>
</authors>
<title>Authoritative Sources in a Hyperlinked Environment.</title>
<date>1998</date>
<booktitle>Proceedings of the 9th ACM-SIAM Symposium on Discrete Algorithms.</booktitle>
<contexts>
<context position="2110" citStr="Kleinberg, 1998" startWordPosition="301" endWordPosition="302"> importance of these patterns. The mutual reinforcement is used to automatically identify the most informative patterns, where patterns that match many instances tend to be correct. Similarly, instances matched by many patterns tend to be correct. The intuition is that large unsupervised data is redundant, i.e. different instances of information could be found many times in different contexts and by different representation. The problem can therefore be seen as hubs (instances) and authorities (patterns) problem which can be solved using the Hypertext Induced Topic Selection (HITS) algorithm (Kleinberg, 1998). HITS is an algorithmic formulation of the notion of authority in web pages link analysis, based on a relationship between a set of relevant “authoritative pages” and a set of “hub pages”. The HITS algorithm benefits from the following observation: when a page (hub) links to another page (authority), the former confers authority over the latter. By analogy to the authoritative web pages problem, we could represent the patterns as authorities and instances as hubs, and use mutual reinforcement between patterns and instances to weight the most authoritative patterns. Highly weighted patterns ar</context>
</contexts>
<marker>Kleinberg, 1998</marker>
<rawString>John Kleinberg. 1998. Authoritative Sources in a Hyperlinked Environment. Proceedings of the 9th ACM-SIAM Symposium on Discrete Algorithms.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kushmerick</author>
<author>D S Weld</author>
<author>R B Doorenbos</author>
</authors>
<title>Wrapper Induction for Information Extraction.</title>
<date>1997</date>
<booktitle>Proceedings of the International Joint Conference on Artificial Intelligence.</booktitle>
<marker>Kushmerick, Weld, Doorenbos, 1997</marker>
<rawString>N. Kushmerick, D.S. Weld, R.B. Doorenbos. 1997. Wrapper Induction for Information Extraction. Proceedings of the International Joint Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Winston Lin</author>
<author>Roman Yangarber</author>
<author>Ralph Grishman</author>
</authors>
<title>Bootstrapped Learning of Semantic Classes from Positive and Negative Examples.</title>
<date>2003</date>
<booktitle>Proceedings of the 20th International Conference on Machine Learning (ICML 2003) Workshop on The Continuum from Labeled to Unlabeled Data in Machine Learning and Data Mining.</booktitle>
<contexts>
<context position="5711" citStr="Lin et al, 2003" startWordPosition="866" endWordPosition="869">lum and Mitchell, 1998) proposed an approach based on co-training that uses unlabeled data in a particular setting. They exploit the fact that, for some problems, each example can be described by multiple representations. (Riloff &amp; Jones, 1999) presented the MetaBootstrapping algorithm that uses an unannotated training data set and a set of seeds to learn a dictionary of extraction patterns and a domain specific semantic lexicon. Other works tried to exploit the duality of patterns and their extractions for the purpose of inferring the semantic class of words like (Thelen &amp; Riloff, 2002) and (Lin et al, 2003). (Muslea et al., 1999) introduced an inductive algorithm to generate extraction rules based on user labeled training examples. This approach suffers from the labeled data bottleneck. (Agichtein et. al, 2000) presented an approach using seed examples to generate initial patterns and to iteratively obtain further patterns. Then ad-hoc measures were deployed to estimate the relevancy of the patterns that have been newly obtained. The major drawbacks of this approach are: its dependency on seed examples leads to limited capability of generalization, and the estimation of patterns relevancy requir</context>
</contexts>
<marker>Lin, Yangarber, Grishman, 2003</marker>
<rawString>Winston Lin, Roman Yangarber, Ralph Grishman. 2003. Bootstrapped Learning of Semantic Classes from Positive and Negative Examples. Proceedings of the 20th International Conference on Machine Learning (ICML 2003) Workshop on The Continuum from Labeled to Unlabeled Data in Machine Learning and Data Mining.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ion Muslea</author>
<author>Steven Minton</author>
<author>Craig Knoblock 1999</author>
</authors>
<title>A hierarchical approach to wrapper induction.</title>
<booktitle>Proceedings of the Third International Conference on Autonomous Agents.</booktitle>
<marker>Muslea, Minton, 1999, </marker>
<rawString>Ion Muslea, Steven Minton, and Craig Knoblock.1999. A hierarchical approach to wrapper induction. Proceedings of the Third International Conference on Autonomous Agents.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>WordNet::Similarity - Measuring the Relatedness of Concepts.</title>
<date>2004</date>
<booktitle>Proceedings of Fifth Annual Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL</booktitle>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004, WordNet::Similarity - Measuring the Relatedness of Concepts. Proceedings of Fifth Annual Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL 2004)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Rosie Jones</author>
</authors>
<title>Learning dictionaries for information extraction by multilevel bootstrapping.</title>
<date>2003</date>
<booktitle>Proceedings of the Sixteenth national Conference on Artificial Intelligence (AAAI</booktitle>
<marker>Riloff, Jones, 2003</marker>
<rawString>Ellen Riloff and Rosie Jones. 2003. Learning dictionaries for information extraction by multilevel bootstrapping. Proceedings of the Sixteenth national Conference on Artificial Intelligence (AAAI 1999).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Thelen</author>
<author>Ellen Riloff</author>
</authors>
<title>A Bootstrapping Method for Learning Semantic Lexicons using Extraction Pattern Contexts.</title>
<date>2002</date>
<booktitle>Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<contexts>
<context position="5689" citStr="Thelen &amp; Riloff, 2002" startWordPosition="861" endWordPosition="864">r extracting information. (Blum and Mitchell, 1998) proposed an approach based on co-training that uses unlabeled data in a particular setting. They exploit the fact that, for some problems, each example can be described by multiple representations. (Riloff &amp; Jones, 1999) presented the MetaBootstrapping algorithm that uses an unannotated training data set and a set of seeds to learn a dictionary of extraction patterns and a domain specific semantic lexicon. Other works tried to exploit the duality of patterns and their extractions for the purpose of inferring the semantic class of words like (Thelen &amp; Riloff, 2002) and (Lin et al, 2003). (Muslea et al., 1999) introduced an inductive algorithm to generate extraction rules based on user labeled training examples. This approach suffers from the labeled data bottleneck. (Agichtein et. al, 2000) presented an approach using seed examples to generate initial patterns and to iteratively obtain further patterns. Then ad-hoc measures were deployed to estimate the relevancy of the patterns that have been newly obtained. The major drawbacks of this approach are: its dependency on seed examples leads to limited capability of generalization, and the estimation of pat</context>
</contexts>
<marker>Thelen, Riloff, 2002</marker>
<rawString>Michael Thelen and Ellen Riloff. 2002. A Bootstrapping Method for Learning Semantic Lexicons using Extraction Pattern Contexts. Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP 2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott White</author>
<author>Padhraic Smyth</author>
</authors>
<title>Algorithms for Discoveing Relative Importance in Graphs.</title>
<date>2003</date>
<booktitle>Proceedings of Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.</booktitle>
<marker>White, Smyth, 2003</marker>
<rawString>Scott White, and Padhraic Smyth. 2003. Algorithms for Discoveing Relative Importance in Graphs. Proceedings of Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhibiao Wu</author>
<author>Martha Palmer</author>
</authors>
<title>Verb semantics and lexical selection.</title>
<date>1994</date>
<booktitle>Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="15226" citStr="Wu and Palmer, 1994" startWordPosition="2515" endWordPosition="2518">nd constructing a weighted undirected graph of tuples. Two tuples are linked with an edge if their similarity measure exceeds a certain threshold. Graph clustering algorithms could be deployed to partition the graph into a set of homogeneous communities or clusters. To reduce the space of tuples, we seek a matching criterion that group similar tuples together. Using WordNet, we can measure the semantic similarity or relatedness between a pair of concepts (or word senses), and by extension, between a pair of sentences. We use the similarity P P P P T T T T P T P T P T 504 measure described in (Wu and Palmer, 1994) which finds the path length to the root node from the least common subsumer (LCS) of the two word senses which is the most specific word sense they share as an ancestor. The similarity score of two tuples, ST, is calculated as follows: where SE1, and SE2 are the similarity scores of the first entities in the two tuples, and their second entitles respectively. The tuple matching procedure assigns a similarity measure to each pair of tuples in the dataset. Using this measure we can construct an undirected graph G. The vertices of G are the tuples. Two vertices are connected with an edge if the </context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Zhibiao Wu, and Martha Palmer. 1994. Verb semantics and lexical selection. Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics (ACL 1994).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>Zoubin Ghahramani</author>
<author>John Lafferty</author>
</authors>
<title>Semi-supervised Learning using Gaussian Fields and Harmonic Functions.</title>
<date>2003</date>
<booktitle>Proceedings of the 20th International Conference on Machine Learning (ICML</booktitle>
<marker>Zhu, Ghahramani, Lafferty, 2003</marker>
<rawString>Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty. 2003. Semi-supervised Learning using Gaussian Fields and Harmonic Functions. Proceedings of the 20th International Conference on Machine Learning (ICML 2003).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>