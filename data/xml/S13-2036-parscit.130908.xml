<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.378080">
<title confidence="0.978985">
Duluth : Word Sense Induction Applied to Web Page Clustering
</title>
<author confidence="0.998894">
Ted Pedersen
</author>
<affiliation confidence="0.877697333333333">
Department of Computer Science
University of Minnesota
Duluth, MN 55812 USA
</affiliation>
<email confidence="0.927328">
tpederse@d.umn.edu
http://senseclusters.sourceforge.net
</email>
<sectionHeader confidence="0.992547" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99871975">
The Duluth systems that participated in task
11 of SemEval–2013 carried out word sense
induction (WSI) in order to cluster Web search
results. They relied on an approach that repre-
sented Web snippets using second–order co–
occurrences. These systems were all imple-
mented using SenseClusters, a freely available
open source software package.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.989518782608696">
The goal of task 11 of SemEval–2013 was to clus-
ter Web search results (Navigli and Vannella, 2013).
The test data consisted of the top 64 Google results
for each of 100 potentially ambiguous queries, for a
total of 6,400 test instances. The Web snippets re-
turned for each query were clustered and evaluated
separately, with an overall evaluation score provided
for each system.
The problem of Web page clustering is one of
the use cases envisioned for SenseClusters (Peder-
sen and Kulkarni, 2007; Pedersen, 2010a), a freely
available open source software package developed
at the University of Minnesota, Duluth starting in
2002. It supports first and second–order clustering
of contexts using both co–occurrence matrices (Pu-
randare and Pedersen, 2004; Kulkarni and Pedersen,
2005) and Latent Semantic Analysis (Landauer and
Dumais, 1997).
SenseClusters has participated in various forms at
different SenseEval and SemEval shared tasks, in-
cluding SemEval-2007 (Pedersen, 2007), SemEval-
2010 (Pedersen, 2010b) and also in an i2b2 clinical
medicine task (Pedersen, 2006).
</bodyText>
<sectionHeader confidence="0.984913" genericHeader="method">
2 Duluth System
</sectionHeader>
<bodyText confidence="0.99984">
While we refer to three Duluth systems (sys1, sys7,
and sys9), in reality these are all variations of the
same overall system. All three are based on second–
order context clustering as provided in SenseClus-
ters. The query terms are treated exactly like any
other word in the snippets, which is called headless
clustering in SenseClusters.
</bodyText>
<subsectionHeader confidence="0.957531">
2.1 Common aspects to all systems
</subsectionHeader>
<bodyText confidence="0.99995675">
The input to sys1, sys7, and sys9 consists of 64
Web search snippets, each approximately 25 words
in length. All text was converted to upper case prior
to processing. The goal was to group the 64 snip-
pets for each query into k distinct clusters, where k
was automatically determined by the PK2 method of
SenseClusters (Pedersen and Kulkarni, 2006a; Ped-
ersen and Kulkarni, 2006b). Each discovered clus-
ter represents a different underlying meaning of the
given query term that resulted in those snippets be-
ing returned. Word sense induction was carried out
separately on the Web snippets associated with each
query term, meaning that the algorithm was run 100
times and clustered 64 Web page snippets each time.
In second–order context clustering, the words in
a context (i.e., Web snippet) to be clustered are re-
placed by vectors that are derived from some cor-
pus of text. The corpora used are among the main
differences in the Duluth systems. Once the words
in a context are replaced by vectors, those vectors
are averaged together to create a new representa-
tion of the context. That representation is said to
be second–order because each word is represented
by its direct or first order co–occurrences, and simi-
</bodyText>
<page confidence="0.968887">
202
</page>
<bodyText confidence="0.996601105263158">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 202–206, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
larities between words in the same Web snippet are
captured by the set of words that mutually co–occur
with them.
If car is represented by the vector [motor, mag-
azine, insurance], and if life is represented by the
vector [sentence, force, insurance], then car and life
are said to be second–order co-occurrences because
they both occur with insurance. A second–order co-
occurrence can capture more indirect relationships
between words, and so these second–order connec-
tions tend to be more numerous and more subtle
than first–order co–occurrences (which would re-
quire that car and life co–occur near or adjacent to
each other in a Web snippet to establish a relation-
ship).
The co-occurrence matrix is created by finding bi-
grams that occur more than a given number of times
(this varies per system) and have a log-likelihood ra-
tio greater than 3.84.1 Then, the first word in a bi-
gram is represented in the rows of the matrix, the
second word is represented in the columns. The
value in the corresponding cell is the log-likelihood
score. This matrix is therefore not symmetric, and
has different entries for old age and age old. Also,
any bigram that includes one or two stop words (e.g.,
to fire, running to, for the) will be excluded and not
included in the co-occurrence matrix and will not be
included in the overall sample count used for com-
puting the log–likelihood ratio. To summarize then,
words in a Web snippet are represented by the words
with which they occur in bigrams, where the context
word is the first word in the bigram, and the vector
is the set of words that follow it in bigrams.
Once the co–occurrence matrix is created, it may
be optionally reduced by Singular Value Decompo-
sition. The result of this will be a matrix with the
same number of rows prior to SVD, but a reduced
number of columns. The goal of SVD is to com-
press together columns of words with similar co–
occurrence patterns, and thereby reduce the size and
noisiness of the data. Whether the matrix is reduced
or not, then each word in each snippet to be clustered
is replaced by a vector from that matrix. A word is
1This value corresponds with a p-value of 0.05 when testing
for significance, meaning that bigrams with log-likelihood at
least equal to 3.84 have at least a 95% chance of having been
drawn from a population where their co-occurrence is not by
chance.
replaced by the row in the co-occurrence matrix to
which it corresponds. Any words that do not have
an entry in the co-occurrence matrix will not be rep-
resented. Then, the contexts are clustered using the
method of repeated bisections (Zhao and Karypis,
2004), where the number of clusters is automatically
discovered using the PK2 method.
</bodyText>
<subsectionHeader confidence="0.998896">
2.2 Differences among systems
</subsectionHeader>
<bodyText confidence="0.999860307692308">
The main difference among the systems was the cor-
pora used to create their co-occurrence matrices.
The smallest corpus was used by sys7, which sim-
ply treated the 64 snippets returned by each query
as the corpus for creating a co–occurrence matrix.
Thus, each query term had a unique co-occurrence
matrix that was created from the Web snippets re-
turned by that query. This results in a very small
amount of data per query (approx. 25 words/snip-
pet * 64 snippets = 1600 words), and so bigrams
were allowed to have up to three intervening words
that were skipped (in order to increase the number
of bigrams used to create the co–occurrence ma-
trix). Bigrams were excluded if they only occurred 1
time, had a log–likelihood ratio of less than 3.84, or
were made up of one or two stop words. Even with
this more flexible definition of bigram, the resulting
co–occurrence matrices were still quite small. The
largest resulting co–occurrence matrix for any query
was 221 x 222, with 602 non–zero values (meaning
there were 602 different bigrams used as features).
The smallest of the co-occurrence matrices was 102
x 113 with 242 non–zero values. Given these small
sizes, SVD was not employed in sys7.
sys1 and sys9 used larger corpora, and therefore
required bigrams to be made up of adjacent words
that occurred 5 or more times, had log–likelihood
ratio scores of 3.84 or above, and contained no stop
words. Rather than having a different co–occurrence
matrix for each query, sys1 and sys9 created a single
co-occurrence matrix for all queries.
In sys1, all the Web snippet results for all 100
queries were combined into a single corpus. Thus,
the co–occurrence matrix was based on bigram fea-
tures found in a corpus of 6,400 Web snippets that
consisted of approximately 160,000 words. This
resulted in a co–occurrence matrix of size 771 x
952 with 1,558 non–zero values prior to SVD. After
SVD the matrix was 771 x 90, and all cells had non-
</bodyText>
<page confidence="0.994918">
203
</page>
<bodyText confidence="0.998936933333333">
zero values (as a result of SVD). Note that if there
are less than 3,000 columns in a co-occurrence ma-
trix, the columns are reduced down to 10% of their
original size. If there are more than 3,000 columns
then it is reduced to 300 dimensions. This follows
recommendations for SVD given for Latent Seman-
tic Analysis (Landauer and Dumais, 1997).
Rather than using task data, sys9 uses the first
10,000 paragraphs of Associated Press newswire
(APW) that appear in the English Gigaword corpus
(1st edition) (Graff and Cieri, 2003). This created
a corpus of approximately 3.6 million words which
resulted in a co-occurrence matrix prior to SVD of
9,853 x 10,995 with 43,199 non-zero values. After
SVD the co–occurrence matrix was 9,853 by 300.
</bodyText>
<sectionHeader confidence="0.999973" genericHeader="evaluation">
3 Results
</sectionHeader>
<bodyText confidence="0.998741315789474">
Various measures were reported by the task organiz-
ers, including F1 (F1-13), the Rand Index (RI), the
Adjusted Rand Index (ARI), and the Jaccard Coeffi-
cient. More details can be found in (Di Marco and
Navigli, 2013).
In addition we computed the paired F-Score (F-
10) (Artiles et al., 2009) as used in the 2010 Se-
mEval word sense induction task (Manandhar et al.,
2010) and the F-Measure (F-SC), which is provided
by SenseClusters. This allows for the comparison of
results from this task with the 2010 task and various
results from SenseClusters.
The organizers also provided scores for S-recall
and S-precision (Zhai et al., 2003), however for
these to be meaningful the results for each cluster
must be output in ranked order. The Duluth sys-
tems did not make a ranking distinction among the
instances in each cluster, and so these scores are not
particularly meaningful for the Duluth systems.
</bodyText>
<subsectionHeader confidence="0.999656">
3.1 Comparisons to Baselines
</subsectionHeader>
<bodyText confidence="0.999979315789474">
Table 1 includes the results of the three submitted
Duluth systems, plus numerous baselines. RandX
designates a random baseline where senses were as-
signed by randomly assigning a value between 1 and
X. In word sense induction, the labels assigned to
discovered clusters are arbitrary, so a random base-
line is a convenient sanity check. MFS replicates the
most frequent sense baseline from supervised learn-
ing by simply assigning all instances for a word to
a single cluster. This is sometimes also known as
the “all–in–one” baseline. Gold are the evaluation
results when the gold standard data is provided as
input (and compared to itself).
The various baselines give us a sense of the char-
acteristics of the different evaluation measures, and
a few points emerge. We have argued previously
(Pedersen, 2010a) that any evaluation measure used
for word sense induction needs to be able to ex-
pose random baselines and distinguish them from
more systematic results. By this standard a number
of measures are found to be lacking. In SemEval–
2010 we demonstrated that the V-Measure (Rosen-
berg and Hirschberg, 2007) had an overwhelming
bias towards systems that produce larger numbers of
clusters – as a result it scored random baselines that
generated larger number of clusters (like Rand25
and Rand50) very highly.
The Rand Index (RI), which does not correct
for chance agreement, also scores random baselines
higher than both non–random systems and MFS.
The Adjusted Rand Index (ARI) corrects for chance
and scores random systems near 0, but it also scores
MFS near 0. According to ARI, random systems
and MFS perform at essentially the same level. This
is a troublesome tendency when evaluating word
sense induction systems, since MFS is often consid-
ered a reasonable baseline that provides useful re-
sults. Many words have relatively skewed distribu-
tions where they are mostly used in one sense, and
this is exactly what is approximated by MFS.
Of the measures included in Table 1, the paired
FScore (F-10), the F-Measure (F-SC), and the Jac-
card Coefficient provide results that seem most ap-
propriate for word sense induction. This is because
these measures score random baselines lower than
MFS, and that RandX scores lower than RandY,
when (X &gt; Y). The paired FScore (F-10) and the
Jaccard Coefficient arrived at similar results, where
Rand50 received an extremely low score, and MFS
scored the highest. The F-measure (F-SC) had a
similar profile, except that the decline in evaluation
scores as X grows in RandX is somewhat less.
The paired F-Score (F-10), the F-Measure (F-SC),
and F1 (F1-13) all score MFS at approximately 54%,
which is intuitively appealing since that is the per-
centage of instances correctly clustered if all in-
stances are placed into a single cluster. However, in
</bodyText>
<page confidence="0.999494">
204
</page>
<tableCaption confidence="0.999624">
Table 1: Experimental Results
</tableCaption>
<table confidence="0.999414090909091">
System F-10 F-SC Jaccard F1-13 RI ARI clusters size
sys1 46.53 46.90 31.79 56.83 52.18 5.75 2.5 26.5
sys7 45.89 44.03 31.03 58.78 52.04 6.78 3.0 25.2
sys9 35.56 37.21 22.24 57.02 54.63 2.59 3.3 19.8
Rand2 41.49 42.86 26.99 54.89 50.06 -0.04 2.0 32.0
Rand5 25.17 31.28 14.52 56.73 56.13 0.12 5.0 12.8
Rand10 15.05 28.71 8.18 59.67 58.10 0.02 10.0 6.4
Rand25 7.01 26.78 3.63 66.89 59.24 -0.15 23.2 2.8
Rand50 4.07 25.97 2.00 76.19 59.73 0.10 35.9 1.8
MFS 54.06 54.42 39.90 54.42 39.90 0.0 1.0 64.0
Gold 100.00 100.00 100.00 100.00 100.00 99.0 7.7 11.6
</table>
<bodyText confidence="0.999400722222222">
other cases these measures begin to diverge. F1 (F1-
13) tends to score random baselines even higher than
MFS, and Rand50 gets a higher score than Rand2,
which is somewhat counter intuitive. In fact accord-
ing to F1 (F1-13), Rand50 would have been the top
ranked system in task 11. It appears that F1 (F1-
13) is strongly influenced by cluster purity, but does
not penalize a system for creating too many clusters.
Thus, as the number of clusters increases, F1 (F1-
13) will consistently improve since smaller clusters
are nearly always more pure than larger ones.
Interestingly enough, the Rand Index (RI) and the
Jaccard Coefficient both score MFS at 39%. This
number does not have an intuitively appealing inter-
pretation, and thereafter RI and Jaccard diverge. RI
scores random baselines higher than MFS, whereas
the Jaccard Coefficient takes the more reasonable
path of scoring random baselines well below MFS.
</bodyText>
<subsectionHeader confidence="0.999834">
3.2 Duluth Systems Evaluation
</subsectionHeader>
<bodyText confidence="0.999830214285714">
The FScore (F-10), F-Measure (F-SC), and Jaccard
Coefficient result in a comparable and consistent
view of the system results. sys1 was found to be the
most accurate, followed closely by sys7. All three
measures showed that sys9 lagged considerably.
While all three systems relied on second–order
co-occurrences, sys7 used the least amount of data,
while sys9 used the most. This shows that better re-
sults can be obtained using the Web snippets to be
clustered as the source of the co–occurrence data (as
sys1 and sys7 did) rather than larger amounts of pos-
sibly less relevant text (as sys9 did).
Each of these systems created a roughly compara-
ble number of clusters (on average, per query term,
shown in the column labeled clusters). sys7 created
2.53, while sys9 created 3.01, and sys1 found 3.32.
The average number of web snippets in the discov-
ered clusters (shown in the column labeled size) are
likewise somewhat consistent: sys1 was the largest
at 26.5, sys7 had 25.2, and sys9 was the smallest
with 19.8. The gold standard found an average of
7.7 queries per cluster and 11.6 snippets per cluster.
After the competition sys1 and sys9 were run
without SVD. There was no significant difference in
results with or without SVD. This is consistent with
previous work that found SVD had relatively little
impact in name discrimination experiments (Peder-
sen et al., 2005).
</bodyText>
<sectionHeader confidence="0.997903" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999982">
sys7 achieved the best results by using very small
co-occurrence matrices of approximately one to two
hundred rows and columns. While small, this data
was most relevant to the task since it was made up of
the Web snippets to be clustered. sys1 increased the
size of the co–occurrence matrix to 771 x 96 by us-
ing all of the test data, but saw no increase in perfor-
mance. sys9 used the largest corpus, which resulted
in a co–occurrence matrix of 9,853 x 300, and had
the poorest results of the Duluth systems.
Sixty–four instances is a small amount of data for
clustering. In future we will augment each query
with additional unannotated web snippets that will
be discarded after clustering. Hopefully the core 64
instances that remain will be clustered more effec-
tively given the cushion provided by the extra data.
</bodyText>
<page confidence="0.998115">
205
</page>
<sectionHeader confidence="0.987819" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999551600000001">
J. Artiles, E. Amig´o, and J. Gonzalo. 2009. The role of
named entities in Web People Search. In Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 534–542, Singapore,
August.
A. Di Marco and R. Navigli. 2013. Clustering and di-
versifying web search results with graph-based word
sense induction. Computational Linguistics, 39(4):1–
46.
D. Graff and C. Cieri. 2003. English Gigaword. Lin-
guistic Data Consortium, Philadelphia.
A. Kulkarni and T Pedersen. 2005. SenseClusters: Un-
supervised discrimination and labeling of similar con-
texts. In Proceedings of the Demonstration and Inter-
active Poster Session of the 43rd Annual Meeting of
the Association for Computational Linguistics, pages
105–108, Ann Arbor, MI, June.
T. Landauer and S. Dumais. 1997. A solution to Plato’s
problem: The Latent Semantic Analysis theory of ac-
quisition, induction and representation of knowledge.
Psychological Review, 104:211–240.
S. Manandhar, I. Klapaftis, D. Dligach, and S. Pradhan.
2010. SemEval-2010 Task 14: Word sense induction
and disambiguation. In Proceedings of the SemEval
2010 Workshop : the 5th International Workshop on
Semantic Evaluations, Uppsala, Sweden, July.
R. Navigli and D. Vannella. 2013. Semeval-2013 task
11: Evaluating word sense induction and disambigua-
tion within an end-user application. In Proceedings
of the 7th International Workshop on Semantic Eval-
uation (SemEval 2013), in conjunction with the Sec-
ond Joint Conference on Lexical and Computational
Semantics (*SEM-2013), Atlanta, June.
T. Pedersen and A. Kulkarni. 2006a. Automatic clus-
ter stopping with criterion functions and the gap statis-
tic. In Proceedings of the Demonstration Session of
the Human Language Technology Conference and the
Sixth Annual Meeting of the North American Chap-
ter of the Association for Computational Linguistics,
pages 276–279, New York City, June.
T. Pedersen and A. Kulkarni. 2006b. Selecting the
right number of senses based on clustering criterion
functions. In Proceedings of the Posters and Demo
Program of the Eleventh Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 111–114, Trento, Italy, April.
T. Pedersen and A. Kulkarni. 2007. Discovering identi-
ties in web contexts with unsupervised clustering. In
Proceedings of the IJCAI-2007 Workshop on Analytics
for Noisy Unstructured Text Data, pages 23–30, Hy-
derabad, India, January.
T. Pedersen, A. Purandare, and A. Kulkarni. 2005. Name
discrimination by clustering similar contexts. In Pro-
ceedings of the Sixth International Conference on In-
telligent Text Processing and Computational Linguis-
tics, pages 220–231, Mexico City, February.
T. Pedersen. 2006. Determining smoker status using su-
pervised and unsupervised learning with lexical fea-
tures. In Working Notes of the i2b2 Workshop on Chal-
lenges in Natural Language Processing for Clinical
Data, Washington, DC, November.
T. Pedersen. 2007. UMND2 : SenseClusters applied
to the sense induction task of Senseval-4. In Proceed-
ings of the Fourth International Workshop on Semantic
Evaluations (SemEval-2007), pages 394–397, Prague,
Czech Republic, June.
T. Pedersen. 2010a. Computational approaches to mea-
suring the similarity of short contexts. Technical re-
port, University of Minnesota Supercomputing Insti-
tute Research Report UMSI 2010/118, October.
T. Pedersen. 2010b. Duluth-WSI: SenseClusters applied
to the sense induction task of semEval-2. In Proceed-
ings of the SemEval 2010 Workshop : the 5th Interna-
tional Workshop on Semantic Evaluations, pages 363–
366, Uppsala, July.
A. Purandare and T. Pedersen. 2004. Word sense
discrimination by clustering contexts in vector and
similarity spaces. In Proceedings of the Conference
on Computational Natural Language Learning, pages
41–48, Boston, MA.
A. Rosenberg and J. Hirschberg. 2007. V-measure: A
conditional entropy-based external cluster evaluation
measure. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 410–420, Prague, Czech Republic, June.
C. X. Zhai, W. Cohen, and J. Lafferty. 2003. Be-
yond independent relevance: methods and evaluation
metrics for subtopic retrieval. In Proceedings of the
26th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 10–17. ACM.
Y. Zhao and G. Karypis. 2004. Empirical and theoretical
comparisons of selected criterion functions for docu-
ment clustering. Machine Learning, 55:311–331.
</reference>
<page confidence="0.998891">
206
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.934250">
<title confidence="0.982105">Duluth : Word Sense Induction Applied to Web Page Clustering</title>
<author confidence="0.983473">Ted</author>
<affiliation confidence="0.99983">Department of Computer University of</affiliation>
<address confidence="0.992434">Duluth, MN 55812</address>
<web confidence="0.973236">http://senseclusters.sourceforge.net</web>
<abstract confidence="0.999315">The Duluth systems that participated in task 11 of SemEval–2013 carried out word sense induction (WSI) in order to cluster Web search results. They relied on an approach that represented Web snippets using second–order co– occurrences. These systems were all implemented using SenseClusters, a freely available open source software package.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Artiles</author>
<author>E Amig´o</author>
<author>J Gonzalo</author>
</authors>
<title>The role of named entities in Web People Search.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>534--542</pages>
<location>Singapore,</location>
<marker>Artiles, Amig´o, Gonzalo, 2009</marker>
<rawString>J. Artiles, E. Amig´o, and J. Gonzalo. 2009. The role of named entities in Web People Search. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 534–542, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Di Marco</author>
<author>R Navigli</author>
</authors>
<title>Clustering and diversifying web search results with graph-based word sense induction.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>4</issue>
<pages>46</pages>
<marker>Di Marco, Navigli, 2013</marker>
<rawString>A. Di Marco and R. Navigli. 2013. Clustering and diversifying web search results with graph-based word sense induction. Computational Linguistics, 39(4):1– 46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Graff</author>
<author>C Cieri</author>
</authors>
<date>2003</date>
<booktitle>English Gigaword. Linguistic Data Consortium,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="8685" citStr="Graff and Cieri, 2003" startWordPosition="1441" endWordPosition="1444">1,558 non–zero values prior to SVD. After SVD the matrix was 771 x 90, and all cells had non203 zero values (as a result of SVD). Note that if there are less than 3,000 columns in a co-occurrence matrix, the columns are reduced down to 10% of their original size. If there are more than 3,000 columns then it is reduced to 300 dimensions. This follows recommendations for SVD given for Latent Semantic Analysis (Landauer and Dumais, 1997). Rather than using task data, sys9 uses the first 10,000 paragraphs of Associated Press newswire (APW) that appear in the English Gigaword corpus (1st edition) (Graff and Cieri, 2003). This created a corpus of approximately 3.6 million words which resulted in a co-occurrence matrix prior to SVD of 9,853 x 10,995 with 43,199 non-zero values. After SVD the co–occurrence matrix was 9,853 by 300. 3 Results Various measures were reported by the task organizers, including F1 (F1-13), the Rand Index (RI), the Adjusted Rand Index (ARI), and the Jaccard Coefficient. More details can be found in (Di Marco and Navigli, 2013). In addition we computed the paired F-Score (F10) (Artiles et al., 2009) as used in the 2010 SemEval word sense induction task (Manandhar et al., 2010) and the F</context>
</contexts>
<marker>Graff, Cieri, 2003</marker>
<rawString>D. Graff and C. Cieri. 2003. English Gigaword. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kulkarni</author>
<author>T Pedersen</author>
</authors>
<title>SenseClusters: Unsupervised discrimination and labeling of similar contexts.</title>
<date>2005</date>
<booktitle>In Proceedings of the Demonstration and Interactive Poster Session of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>105--108</pages>
<location>Ann Arbor, MI,</location>
<contexts>
<context position="1351" citStr="Kulkarni and Pedersen, 2005" startWordPosition="200" endWordPosition="203"> for each of 100 potentially ambiguous queries, for a total of 6,400 test instances. The Web snippets returned for each query were clustered and evaluated separately, with an overall evaluation score provided for each system. The problem of Web page clustering is one of the use cases envisioned for SenseClusters (Pedersen and Kulkarni, 2007; Pedersen, 2010a), a freely available open source software package developed at the University of Minnesota, Duluth starting in 2002. It supports first and second–order clustering of contexts using both co–occurrence matrices (Purandare and Pedersen, 2004; Kulkarni and Pedersen, 2005) and Latent Semantic Analysis (Landauer and Dumais, 1997). SenseClusters has participated in various forms at different SenseEval and SemEval shared tasks, including SemEval-2007 (Pedersen, 2007), SemEval2010 (Pedersen, 2010b) and also in an i2b2 clinical medicine task (Pedersen, 2006). 2 Duluth System While we refer to three Duluth systems (sys1, sys7, and sys9), in reality these are all variations of the same overall system. All three are based on second– order context clustering as provided in SenseClusters. The query terms are treated exactly like any other word in the snippets, which is c</context>
</contexts>
<marker>Kulkarni, Pedersen, 2005</marker>
<rawString>A. Kulkarni and T Pedersen. 2005. SenseClusters: Unsupervised discrimination and labeling of similar contexts. In Proceedings of the Demonstration and Interactive Poster Session of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 105–108, Ann Arbor, MI, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Landauer</author>
<author>S Dumais</author>
</authors>
<title>A solution to Plato’s problem: The Latent Semantic Analysis theory of acquisition, induction and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<pages>104--211</pages>
<contexts>
<context position="1408" citStr="Landauer and Dumais, 1997" startWordPosition="208" endWordPosition="211"> of 6,400 test instances. The Web snippets returned for each query were clustered and evaluated separately, with an overall evaluation score provided for each system. The problem of Web page clustering is one of the use cases envisioned for SenseClusters (Pedersen and Kulkarni, 2007; Pedersen, 2010a), a freely available open source software package developed at the University of Minnesota, Duluth starting in 2002. It supports first and second–order clustering of contexts using both co–occurrence matrices (Purandare and Pedersen, 2004; Kulkarni and Pedersen, 2005) and Latent Semantic Analysis (Landauer and Dumais, 1997). SenseClusters has participated in various forms at different SenseEval and SemEval shared tasks, including SemEval-2007 (Pedersen, 2007), SemEval2010 (Pedersen, 2010b) and also in an i2b2 clinical medicine task (Pedersen, 2006). 2 Duluth System While we refer to three Duluth systems (sys1, sys7, and sys9), in reality these are all variations of the same overall system. All three are based on second– order context clustering as provided in SenseClusters. The query terms are treated exactly like any other word in the snippets, which is called headless clustering in SenseClusters. 2.1 Common as</context>
<context position="8501" citStr="Landauer and Dumais, 1997" startWordPosition="1412" endWordPosition="1415">e matrix was based on bigram features found in a corpus of 6,400 Web snippets that consisted of approximately 160,000 words. This resulted in a co–occurrence matrix of size 771 x 952 with 1,558 non–zero values prior to SVD. After SVD the matrix was 771 x 90, and all cells had non203 zero values (as a result of SVD). Note that if there are less than 3,000 columns in a co-occurrence matrix, the columns are reduced down to 10% of their original size. If there are more than 3,000 columns then it is reduced to 300 dimensions. This follows recommendations for SVD given for Latent Semantic Analysis (Landauer and Dumais, 1997). Rather than using task data, sys9 uses the first 10,000 paragraphs of Associated Press newswire (APW) that appear in the English Gigaword corpus (1st edition) (Graff and Cieri, 2003). This created a corpus of approximately 3.6 million words which resulted in a co-occurrence matrix prior to SVD of 9,853 x 10,995 with 43,199 non-zero values. After SVD the co–occurrence matrix was 9,853 by 300. 3 Results Various measures were reported by the task organizers, including F1 (F1-13), the Rand Index (RI), the Adjusted Rand Index (ARI), and the Jaccard Coefficient. More details can be found in (Di Ma</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>T. Landauer and S. Dumais. 1997. A solution to Plato’s problem: The Latent Semantic Analysis theory of acquisition, induction and representation of knowledge. Psychological Review, 104:211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Manandhar</author>
<author>I Klapaftis</author>
<author>D Dligach</author>
<author>S Pradhan</author>
</authors>
<title>SemEval-2010 Task 14: Word sense induction and disambiguation.</title>
<date>2010</date>
<booktitle>In Proceedings of the SemEval 2010 Workshop : the 5th International Workshop on Semantic Evaluations,</booktitle>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="9275" citStr="Manandhar et al., 2010" startWordPosition="1543" endWordPosition="1546"> edition) (Graff and Cieri, 2003). This created a corpus of approximately 3.6 million words which resulted in a co-occurrence matrix prior to SVD of 9,853 x 10,995 with 43,199 non-zero values. After SVD the co–occurrence matrix was 9,853 by 300. 3 Results Various measures were reported by the task organizers, including F1 (F1-13), the Rand Index (RI), the Adjusted Rand Index (ARI), and the Jaccard Coefficient. More details can be found in (Di Marco and Navigli, 2013). In addition we computed the paired F-Score (F10) (Artiles et al., 2009) as used in the 2010 SemEval word sense induction task (Manandhar et al., 2010) and the F-Measure (F-SC), which is provided by SenseClusters. This allows for the comparison of results from this task with the 2010 task and various results from SenseClusters. The organizers also provided scores for S-recall and S-precision (Zhai et al., 2003), however for these to be meaningful the results for each cluster must be output in ranked order. The Duluth systems did not make a ranking distinction among the instances in each cluster, and so these scores are not particularly meaningful for the Duluth systems. 3.1 Comparisons to Baselines Table 1 includes the results of the three s</context>
</contexts>
<marker>Manandhar, Klapaftis, Dligach, Pradhan, 2010</marker>
<rawString>S. Manandhar, I. Klapaftis, D. Dligach, and S. Pradhan. 2010. SemEval-2010 Task 14: Word sense induction and disambiguation. In Proceedings of the SemEval 2010 Workshop : the 5th International Workshop on Semantic Evaluations, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Navigli</author>
<author>D Vannella</author>
</authors>
<title>Semeval-2013 task 11: Evaluating word sense induction and disambiguation within an end-user application.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013), in conjunction with the Second Joint Conference on Lexical and Computational Semantics (*SEM-2013),</booktitle>
<location>Atlanta,</location>
<contexts>
<context position="669" citStr="Navigli and Vannella, 2013" startWordPosition="94" endWordPosition="97">eb Page Clustering Ted Pedersen Department of Computer Science University of Minnesota Duluth, MN 55812 USA tpederse@d.umn.edu http://senseclusters.sourceforge.net Abstract The Duluth systems that participated in task 11 of SemEval–2013 carried out word sense induction (WSI) in order to cluster Web search results. They relied on an approach that represented Web snippets using second–order co– occurrences. These systems were all implemented using SenseClusters, a freely available open source software package. 1 Introduction The goal of task 11 of SemEval–2013 was to cluster Web search results (Navigli and Vannella, 2013). The test data consisted of the top 64 Google results for each of 100 potentially ambiguous queries, for a total of 6,400 test instances. The Web snippets returned for each query were clustered and evaluated separately, with an overall evaluation score provided for each system. The problem of Web page clustering is one of the use cases envisioned for SenseClusters (Pedersen and Kulkarni, 2007; Pedersen, 2010a), a freely available open source software package developed at the University of Minnesota, Duluth starting in 2002. It supports first and second–order clustering of contexts using both </context>
</contexts>
<marker>Navigli, Vannella, 2013</marker>
<rawString>R. Navigli and D. Vannella. 2013. Semeval-2013 task 11: Evaluating word sense induction and disambiguation within an end-user application. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013), in conjunction with the Second Joint Conference on Lexical and Computational Semantics (*SEM-2013), Atlanta, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
<author>A Kulkarni</author>
</authors>
<title>Automatic cluster stopping with criterion functions and the gap statistic.</title>
<date>2006</date>
<booktitle>In Proceedings of the Demonstration Session of the Human Language Technology Conference and the Sixth Annual Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>276--279</pages>
<location>New York City,</location>
<contexts>
<context position="2375" citStr="Pedersen and Kulkarni, 2006" startWordPosition="365" endWordPosition="368">tions of the same overall system. All three are based on second– order context clustering as provided in SenseClusters. The query terms are treated exactly like any other word in the snippets, which is called headless clustering in SenseClusters. 2.1 Common aspects to all systems The input to sys1, sys7, and sys9 consists of 64 Web search snippets, each approximately 25 words in length. All text was converted to upper case prior to processing. The goal was to group the 64 snippets for each query into k distinct clusters, where k was automatically determined by the PK2 method of SenseClusters (Pedersen and Kulkarni, 2006a; Pedersen and Kulkarni, 2006b). Each discovered cluster represents a different underlying meaning of the given query term that resulted in those snippets being returned. Word sense induction was carried out separately on the Web snippets associated with each query term, meaning that the algorithm was run 100 times and clustered 64 Web page snippets each time. In second–order context clustering, the words in a context (i.e., Web snippet) to be clustered are replaced by vectors that are derived from some corpus of text. The corpora used are among the main differences in the Duluth systems. Onc</context>
</contexts>
<marker>Pedersen, Kulkarni, 2006</marker>
<rawString>T. Pedersen and A. Kulkarni. 2006a. Automatic cluster stopping with criterion functions and the gap statistic. In Proceedings of the Demonstration Session of the Human Language Technology Conference and the Sixth Annual Meeting of the North American Chapter of the Association for Computational Linguistics, pages 276–279, New York City, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
<author>A Kulkarni</author>
</authors>
<title>Selecting the right number of senses based on clustering criterion functions.</title>
<date>2006</date>
<booktitle>In Proceedings of the Posters and Demo Program of the Eleventh Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>111--114</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="2375" citStr="Pedersen and Kulkarni, 2006" startWordPosition="365" endWordPosition="368">tions of the same overall system. All three are based on second– order context clustering as provided in SenseClusters. The query terms are treated exactly like any other word in the snippets, which is called headless clustering in SenseClusters. 2.1 Common aspects to all systems The input to sys1, sys7, and sys9 consists of 64 Web search snippets, each approximately 25 words in length. All text was converted to upper case prior to processing. The goal was to group the 64 snippets for each query into k distinct clusters, where k was automatically determined by the PK2 method of SenseClusters (Pedersen and Kulkarni, 2006a; Pedersen and Kulkarni, 2006b). Each discovered cluster represents a different underlying meaning of the given query term that resulted in those snippets being returned. Word sense induction was carried out separately on the Web snippets associated with each query term, meaning that the algorithm was run 100 times and clustered 64 Web page snippets each time. In second–order context clustering, the words in a context (i.e., Web snippet) to be clustered are replaced by vectors that are derived from some corpus of text. The corpora used are among the main differences in the Duluth systems. Onc</context>
</contexts>
<marker>Pedersen, Kulkarni, 2006</marker>
<rawString>T. Pedersen and A. Kulkarni. 2006b. Selecting the right number of senses based on clustering criterion functions. In Proceedings of the Posters and Demo Program of the Eleventh Conference of the European Chapter of the Association for Computational Linguistics, pages 111–114, Trento, Italy, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
<author>A Kulkarni</author>
</authors>
<title>Discovering identities in web contexts with unsupervised clustering.</title>
<date>2007</date>
<booktitle>In Proceedings of the IJCAI-2007 Workshop on Analytics for Noisy Unstructured Text Data,</booktitle>
<pages>23--30</pages>
<location>Hyderabad, India,</location>
<contexts>
<context position="1065" citStr="Pedersen and Kulkarni, 2007" startWordPosition="159" endWordPosition="163">occurrences. These systems were all implemented using SenseClusters, a freely available open source software package. 1 Introduction The goal of task 11 of SemEval–2013 was to cluster Web search results (Navigli and Vannella, 2013). The test data consisted of the top 64 Google results for each of 100 potentially ambiguous queries, for a total of 6,400 test instances. The Web snippets returned for each query were clustered and evaluated separately, with an overall evaluation score provided for each system. The problem of Web page clustering is one of the use cases envisioned for SenseClusters (Pedersen and Kulkarni, 2007; Pedersen, 2010a), a freely available open source software package developed at the University of Minnesota, Duluth starting in 2002. It supports first and second–order clustering of contexts using both co–occurrence matrices (Purandare and Pedersen, 2004; Kulkarni and Pedersen, 2005) and Latent Semantic Analysis (Landauer and Dumais, 1997). SenseClusters has participated in various forms at different SenseEval and SemEval shared tasks, including SemEval-2007 (Pedersen, 2007), SemEval2010 (Pedersen, 2010b) and also in an i2b2 clinical medicine task (Pedersen, 2006). 2 Duluth System While we r</context>
</contexts>
<marker>Pedersen, Kulkarni, 2007</marker>
<rawString>T. Pedersen and A. Kulkarni. 2007. Discovering identities in web contexts with unsupervised clustering. In Proceedings of the IJCAI-2007 Workshop on Analytics for Noisy Unstructured Text Data, pages 23–30, Hyderabad, India, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
<author>A Purandare</author>
<author>A Kulkarni</author>
</authors>
<title>Name discrimination by clustering similar contexts.</title>
<date>2005</date>
<booktitle>In Proceedings of the Sixth International Conference on Intelligent Text Processing and Computational Linguistics,</booktitle>
<pages>220--231</pages>
<location>Mexico City,</location>
<contexts>
<context position="15519" citStr="Pedersen et al., 2005" startWordPosition="2580" endWordPosition="2584">53, while sys9 created 3.01, and sys1 found 3.32. The average number of web snippets in the discovered clusters (shown in the column labeled size) are likewise somewhat consistent: sys1 was the largest at 26.5, sys7 had 25.2, and sys9 was the smallest with 19.8. The gold standard found an average of 7.7 queries per cluster and 11.6 snippets per cluster. After the competition sys1 and sys9 were run without SVD. There was no significant difference in results with or without SVD. This is consistent with previous work that found SVD had relatively little impact in name discrimination experiments (Pedersen et al., 2005). 4 Conclusions sys7 achieved the best results by using very small co-occurrence matrices of approximately one to two hundred rows and columns. While small, this data was most relevant to the task since it was made up of the Web snippets to be clustered. sys1 increased the size of the co–occurrence matrix to 771 x 96 by using all of the test data, but saw no increase in performance. sys9 used the largest corpus, which resulted in a co–occurrence matrix of 9,853 x 300, and had the poorest results of the Duluth systems. Sixty–four instances is a small amount of data for clustering. In future we </context>
</contexts>
<marker>Pedersen, Purandare, Kulkarni, 2005</marker>
<rawString>T. Pedersen, A. Purandare, and A. Kulkarni. 2005. Name discrimination by clustering similar contexts. In Proceedings of the Sixth International Conference on Intelligent Text Processing and Computational Linguistics, pages 220–231, Mexico City, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
</authors>
<title>Determining smoker status using supervised and unsupervised learning with lexical features.</title>
<date>2006</date>
<booktitle>In Working Notes of the i2b2 Workshop on Challenges in Natural Language Processing for Clinical Data,</booktitle>
<location>Washington, DC,</location>
<contexts>
<context position="1637" citStr="Pedersen, 2006" startWordPosition="242" endWordPosition="243"> SenseClusters (Pedersen and Kulkarni, 2007; Pedersen, 2010a), a freely available open source software package developed at the University of Minnesota, Duluth starting in 2002. It supports first and second–order clustering of contexts using both co–occurrence matrices (Purandare and Pedersen, 2004; Kulkarni and Pedersen, 2005) and Latent Semantic Analysis (Landauer and Dumais, 1997). SenseClusters has participated in various forms at different SenseEval and SemEval shared tasks, including SemEval-2007 (Pedersen, 2007), SemEval2010 (Pedersen, 2010b) and also in an i2b2 clinical medicine task (Pedersen, 2006). 2 Duluth System While we refer to three Duluth systems (sys1, sys7, and sys9), in reality these are all variations of the same overall system. All three are based on second– order context clustering as provided in SenseClusters. The query terms are treated exactly like any other word in the snippets, which is called headless clustering in SenseClusters. 2.1 Common aspects to all systems The input to sys1, sys7, and sys9 consists of 64 Web search snippets, each approximately 25 words in length. All text was converted to upper case prior to processing. The goal was to group the 64 snippets for</context>
</contexts>
<marker>Pedersen, 2006</marker>
<rawString>T. Pedersen. 2006. Determining smoker status using supervised and unsupervised learning with lexical features. In Working Notes of the i2b2 Workshop on Challenges in Natural Language Processing for Clinical Data, Washington, DC, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
</authors>
<title>UMND2 : SenseClusters applied to the sense induction task of Senseval-4.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>394--397</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1546" citStr="Pedersen, 2007" startWordPosition="228" endWordPosition="229"> for each system. The problem of Web page clustering is one of the use cases envisioned for SenseClusters (Pedersen and Kulkarni, 2007; Pedersen, 2010a), a freely available open source software package developed at the University of Minnesota, Duluth starting in 2002. It supports first and second–order clustering of contexts using both co–occurrence matrices (Purandare and Pedersen, 2004; Kulkarni and Pedersen, 2005) and Latent Semantic Analysis (Landauer and Dumais, 1997). SenseClusters has participated in various forms at different SenseEval and SemEval shared tasks, including SemEval-2007 (Pedersen, 2007), SemEval2010 (Pedersen, 2010b) and also in an i2b2 clinical medicine task (Pedersen, 2006). 2 Duluth System While we refer to three Duluth systems (sys1, sys7, and sys9), in reality these are all variations of the same overall system. All three are based on second– order context clustering as provided in SenseClusters. The query terms are treated exactly like any other word in the snippets, which is called headless clustering in SenseClusters. 2.1 Common aspects to all systems The input to sys1, sys7, and sys9 consists of 64 Web search snippets, each approximately 25 words in length. All text</context>
</contexts>
<marker>Pedersen, 2007</marker>
<rawString>T. Pedersen. 2007. UMND2 : SenseClusters applied to the sense induction task of Senseval-4. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 394–397, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
</authors>
<title>Computational approaches to measuring the similarity of short contexts.</title>
<date>2010</date>
<tech>Technical report,</tech>
<institution>University of Minnesota Supercomputing Institute Research</institution>
<contexts>
<context position="1081" citStr="Pedersen, 2010" startWordPosition="164" endWordPosition="165">re all implemented using SenseClusters, a freely available open source software package. 1 Introduction The goal of task 11 of SemEval–2013 was to cluster Web search results (Navigli and Vannella, 2013). The test data consisted of the top 64 Google results for each of 100 potentially ambiguous queries, for a total of 6,400 test instances. The Web snippets returned for each query were clustered and evaluated separately, with an overall evaluation score provided for each system. The problem of Web page clustering is one of the use cases envisioned for SenseClusters (Pedersen and Kulkarni, 2007; Pedersen, 2010a), a freely available open source software package developed at the University of Minnesota, Duluth starting in 2002. It supports first and second–order clustering of contexts using both co–occurrence matrices (Purandare and Pedersen, 2004; Kulkarni and Pedersen, 2005) and Latent Semantic Analysis (Landauer and Dumais, 1997). SenseClusters has participated in various forms at different SenseEval and SemEval shared tasks, including SemEval-2007 (Pedersen, 2007), SemEval2010 (Pedersen, 2010b) and also in an i2b2 clinical medicine task (Pedersen, 2006). 2 Duluth System While we refer to three Du</context>
<context position="10638" citStr="Pedersen, 2010" startWordPosition="1767" endWordPosition="1768"> 1 and X. In word sense induction, the labels assigned to discovered clusters are arbitrary, so a random baseline is a convenient sanity check. MFS replicates the most frequent sense baseline from supervised learning by simply assigning all instances for a word to a single cluster. This is sometimes also known as the “all–in–one” baseline. Gold are the evaluation results when the gold standard data is provided as input (and compared to itself). The various baselines give us a sense of the characteristics of the different evaluation measures, and a few points emerge. We have argued previously (Pedersen, 2010a) that any evaluation measure used for word sense induction needs to be able to expose random baselines and distinguish them from more systematic results. By this standard a number of measures are found to be lacking. In SemEval– 2010 we demonstrated that the V-Measure (Rosenberg and Hirschberg, 2007) had an overwhelming bias towards systems that produce larger numbers of clusters – as a result it scored random baselines that generated larger number of clusters (like Rand25 and Rand50) very highly. The Rand Index (RI), which does not correct for chance agreement, also scores random baselines </context>
</contexts>
<marker>Pedersen, 2010</marker>
<rawString>T. Pedersen. 2010a. Computational approaches to measuring the similarity of short contexts. Technical report, University of Minnesota Supercomputing Institute Research Report UMSI 2010/118, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
</authors>
<title>Duluth-WSI: SenseClusters applied to the sense induction task of semEval-2.</title>
<date>2010</date>
<booktitle>In Proceedings of the SemEval 2010 Workshop : the 5th International Workshop on Semantic Evaluations,</booktitle>
<pages>363--366</pages>
<location>Uppsala,</location>
<contexts>
<context position="1081" citStr="Pedersen, 2010" startWordPosition="164" endWordPosition="165">re all implemented using SenseClusters, a freely available open source software package. 1 Introduction The goal of task 11 of SemEval–2013 was to cluster Web search results (Navigli and Vannella, 2013). The test data consisted of the top 64 Google results for each of 100 potentially ambiguous queries, for a total of 6,400 test instances. The Web snippets returned for each query were clustered and evaluated separately, with an overall evaluation score provided for each system. The problem of Web page clustering is one of the use cases envisioned for SenseClusters (Pedersen and Kulkarni, 2007; Pedersen, 2010a), a freely available open source software package developed at the University of Minnesota, Duluth starting in 2002. It supports first and second–order clustering of contexts using both co–occurrence matrices (Purandare and Pedersen, 2004; Kulkarni and Pedersen, 2005) and Latent Semantic Analysis (Landauer and Dumais, 1997). SenseClusters has participated in various forms at different SenseEval and SemEval shared tasks, including SemEval-2007 (Pedersen, 2007), SemEval2010 (Pedersen, 2010b) and also in an i2b2 clinical medicine task (Pedersen, 2006). 2 Duluth System While we refer to three Du</context>
<context position="10638" citStr="Pedersen, 2010" startWordPosition="1767" endWordPosition="1768"> 1 and X. In word sense induction, the labels assigned to discovered clusters are arbitrary, so a random baseline is a convenient sanity check. MFS replicates the most frequent sense baseline from supervised learning by simply assigning all instances for a word to a single cluster. This is sometimes also known as the “all–in–one” baseline. Gold are the evaluation results when the gold standard data is provided as input (and compared to itself). The various baselines give us a sense of the characteristics of the different evaluation measures, and a few points emerge. We have argued previously (Pedersen, 2010a) that any evaluation measure used for word sense induction needs to be able to expose random baselines and distinguish them from more systematic results. By this standard a number of measures are found to be lacking. In SemEval– 2010 we demonstrated that the V-Measure (Rosenberg and Hirschberg, 2007) had an overwhelming bias towards systems that produce larger numbers of clusters – as a result it scored random baselines that generated larger number of clusters (like Rand25 and Rand50) very highly. The Rand Index (RI), which does not correct for chance agreement, also scores random baselines </context>
</contexts>
<marker>Pedersen, 2010</marker>
<rawString>T. Pedersen. 2010b. Duluth-WSI: SenseClusters applied to the sense induction task of semEval-2. In Proceedings of the SemEval 2010 Workshop : the 5th International Workshop on Semantic Evaluations, pages 363– 366, Uppsala, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Purandare</author>
<author>T Pedersen</author>
</authors>
<title>Word sense discrimination by clustering contexts in vector and similarity spaces.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Computational Natural Language Learning,</booktitle>
<pages>41--48</pages>
<location>Boston, MA.</location>
<contexts>
<context position="1321" citStr="Purandare and Pedersen, 2004" startWordPosition="195" endWordPosition="199">d of the top 64 Google results for each of 100 potentially ambiguous queries, for a total of 6,400 test instances. The Web snippets returned for each query were clustered and evaluated separately, with an overall evaluation score provided for each system. The problem of Web page clustering is one of the use cases envisioned for SenseClusters (Pedersen and Kulkarni, 2007; Pedersen, 2010a), a freely available open source software package developed at the University of Minnesota, Duluth starting in 2002. It supports first and second–order clustering of contexts using both co–occurrence matrices (Purandare and Pedersen, 2004; Kulkarni and Pedersen, 2005) and Latent Semantic Analysis (Landauer and Dumais, 1997). SenseClusters has participated in various forms at different SenseEval and SemEval shared tasks, including SemEval-2007 (Pedersen, 2007), SemEval2010 (Pedersen, 2010b) and also in an i2b2 clinical medicine task (Pedersen, 2006). 2 Duluth System While we refer to three Duluth systems (sys1, sys7, and sys9), in reality these are all variations of the same overall system. All three are based on second– order context clustering as provided in SenseClusters. The query terms are treated exactly like any other wo</context>
</contexts>
<marker>Purandare, Pedersen, 2004</marker>
<rawString>A. Purandare and T. Pedersen. 2004. Word sense discrimination by clustering contexts in vector and similarity spaces. In Proceedings of the Conference on Computational Natural Language Learning, pages 41–48, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rosenberg</author>
<author>J Hirschberg</author>
</authors>
<title>V-measure: A conditional entropy-based external cluster evaluation measure.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>410--420</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="10941" citStr="Rosenberg and Hirschberg, 2007" startWordPosition="1814" endWordPosition="1818">s is sometimes also known as the “all–in–one” baseline. Gold are the evaluation results when the gold standard data is provided as input (and compared to itself). The various baselines give us a sense of the characteristics of the different evaluation measures, and a few points emerge. We have argued previously (Pedersen, 2010a) that any evaluation measure used for word sense induction needs to be able to expose random baselines and distinguish them from more systematic results. By this standard a number of measures are found to be lacking. In SemEval– 2010 we demonstrated that the V-Measure (Rosenberg and Hirschberg, 2007) had an overwhelming bias towards systems that produce larger numbers of clusters – as a result it scored random baselines that generated larger number of clusters (like Rand25 and Rand50) very highly. The Rand Index (RI), which does not correct for chance agreement, also scores random baselines higher than both non–random systems and MFS. The Adjusted Rand Index (ARI) corrects for chance and scores random systems near 0, but it also scores MFS near 0. According to ARI, random systems and MFS perform at essentially the same level. This is a troublesome tendency when evaluating word sense induc</context>
</contexts>
<marker>Rosenberg, Hirschberg, 2007</marker>
<rawString>A. Rosenberg and J. Hirschberg. 2007. V-measure: A conditional entropy-based external cluster evaluation measure. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 410–420, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C X Zhai</author>
<author>W Cohen</author>
<author>J Lafferty</author>
</authors>
<title>Beyond independent relevance: methods and evaluation metrics for subtopic retrieval.</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>10--17</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="9538" citStr="Zhai et al., 2003" startWordPosition="1584" endWordPosition="1587">ures were reported by the task organizers, including F1 (F1-13), the Rand Index (RI), the Adjusted Rand Index (ARI), and the Jaccard Coefficient. More details can be found in (Di Marco and Navigli, 2013). In addition we computed the paired F-Score (F10) (Artiles et al., 2009) as used in the 2010 SemEval word sense induction task (Manandhar et al., 2010) and the F-Measure (F-SC), which is provided by SenseClusters. This allows for the comparison of results from this task with the 2010 task and various results from SenseClusters. The organizers also provided scores for S-recall and S-precision (Zhai et al., 2003), however for these to be meaningful the results for each cluster must be output in ranked order. The Duluth systems did not make a ranking distinction among the instances in each cluster, and so these scores are not particularly meaningful for the Duluth systems. 3.1 Comparisons to Baselines Table 1 includes the results of the three submitted Duluth systems, plus numerous baselines. RandX designates a random baseline where senses were assigned by randomly assigning a value between 1 and X. In word sense induction, the labels assigned to discovered clusters are arbitrary, so a random baseline </context>
</contexts>
<marker>Zhai, Cohen, Lafferty, 2003</marker>
<rawString>C. X. Zhai, W. Cohen, and J. Lafferty. 2003. Beyond independent relevance: methods and evaluation metrics for subtopic retrieval. In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 10–17. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhao</author>
<author>G Karypis</author>
</authors>
<title>Empirical and theoretical comparisons of selected criterion functions for document clustering.</title>
<date>2004</date>
<booktitle>Machine Learning,</booktitle>
<pages>55--311</pages>
<contexts>
<context position="6110" citStr="Zhao and Karypis, 2004" startWordPosition="999" endWordPosition="1002">uced or not, then each word in each snippet to be clustered is replaced by a vector from that matrix. A word is 1This value corresponds with a p-value of 0.05 when testing for significance, meaning that bigrams with log-likelihood at least equal to 3.84 have at least a 95% chance of having been drawn from a population where their co-occurrence is not by chance. replaced by the row in the co-occurrence matrix to which it corresponds. Any words that do not have an entry in the co-occurrence matrix will not be represented. Then, the contexts are clustered using the method of repeated bisections (Zhao and Karypis, 2004), where the number of clusters is automatically discovered using the PK2 method. 2.2 Differences among systems The main difference among the systems was the corpora used to create their co-occurrence matrices. The smallest corpus was used by sys7, which simply treated the 64 snippets returned by each query as the corpus for creating a co–occurrence matrix. Thus, each query term had a unique co-occurrence matrix that was created from the Web snippets returned by that query. This results in a very small amount of data per query (approx. 25 words/snippet * 64 snippets = 1600 words), and so bigram</context>
</contexts>
<marker>Zhao, Karypis, 2004</marker>
<rawString>Y. Zhao and G. Karypis. 2004. Empirical and theoretical comparisons of selected criterion functions for document clustering. Machine Learning, 55:311–331.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>