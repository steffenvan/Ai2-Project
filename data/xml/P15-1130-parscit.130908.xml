<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9980805">
Predicting Polarities of Tweets by Composing Word Embeddings with
Long Short-Term Memory
</title>
<author confidence="0.99981">
Xin Wang1, Yuanchao Liu1, Chengjie Sun1, Baoxun Wang2 and Xiaolong Wang1
</author>
<affiliation confidence="0.836365666666667">
1School of Computer Science and Technology,
Harbin Institute of Technology, Harbin, China
2Application and Service Group, Microsoft, Beijing, China
</affiliation>
<email confidence="0.897583">
1{ xwang,lyc,cjsun,wangxl}@insun.hit.edu.cn
2baoxwang@microsoft.com
</email>
<sectionHeader confidence="0.997248" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999879625">
In this paper, we introduce Long Short-
Term Memory (LSTM) recurrent network
for twitter sentiment prediction. With the
help of gates and constant error carousels
in the memory block structure, the model
could handle interactions between words
through a flexible compositional function.
Experiments on a public noisy labelled
data show that our model outperforms sev-
eral feature-engineering approaches, with
the result comparable to the current best
data-driven technique. According to the
evaluation on a generated negation phrase
test set, the proposed architecture dou-
bles the performance of non-neural model
based on bag-of-word features. Further-
more, words with special functions (such
as negation and transition) are distin-
guished and the dissimilarities of words
with opposite sentiment are magnified. An
interesting case study on negation expres-
sion processing shows a promising poten-
tial of the architecture dealing with com-
plex sentiment phrases.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999606240740741">
Twitter and other similar microblogs are rich re-
sources for opinions on various kinds of products
and events. Detecting sentiment in microblogs is
a challenging task that has attracted increasing re-
search interest in recent years (Hu et al., 2013b;
Volkova et al., 2013).
Go et al. (2009) carried out the pioneer work
of predicting sentiment in tweets using machine
learning technology. They conducted comprehen-
sive experiments on multiple classifiers based on
bag-of-words feature. Such feature is widely used
because it’s simple and surprisingly efficient in
many tasks. However, there are also disadvan-
tages of bag-of-words features represented by one-
hot vectors. Firstly, it bears a data sparsity is-
sue (Saif et al., 2012a). In tweets, irregulari-
ties and 140-character limitation exacerbate the
sparseness. Secondly, losing sequence informa-
tion makes it difficult to figure out the polarity
properly (Pang et al., 2002). A typical case is that
the sentiment word in a negation phrase tends to
express opposite sentiment to that of the context.
Distributed representations of words can ease
the sparseness, but there are limitations to the
unsupervised-learned ones. Words with special
functions in specific tasks are not distinguished.
Such as negation words, which play a special
role in polarity classification, are represented sim-
ilarly with other adverbs. Such similarities will
limit the compositional models’ abilities of de-
scribing a sentiment-specific interaction between
words. Moreover, word vectors trained by co-
occurrence statistics in a small window of con-
text effectively represent the syntactic and seman-
tic similarity. Thus, words like good and bad have
very similar representations (Socher et al., 2011).
It’s problematic for sentiment classifiers.
Sentiment is expressed by phrases rather than
by words (Socher et al., 2013). Seizing such se-
quence information would help to analyze com-
plex sentiment expressions. One possible method
to leverage context is connecting embeddings of
words in a window and compose them to a fix-
length vector (Collobert et al., 2011). However,
window-based methods may have difficulty reach-
ing long-distance words and simply connected
vectors do not always represent the interactions of
context properly.
Theoretically, a recurrent neural network could
process the whole sentence of arbitrary length by
encoding the context cyclically. However, the
length of reachable context is often limited when
using stochastic gradient descent (Bengio et al.,
1994; Pascanu et al., 2013). Besides that, a
</bodyText>
<page confidence="0.861987">
1343
</page>
<note confidence="0.973073666666667">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1343–1353,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999944833333333">
traditional recurrent architecture is not powerful
enough to deal with the complex sentiment expres-
sions. Fixed input limits the network’s ability of
learning task-specific representations and simple
additive combination of hidden activations and in-
put activations has difficulty capturing more com-
plex linguistic phenomena.
In this paper, we introduce the Long Short-
Term Memory (LSTM) recurrent neural network
for twitter sentiment classification by means of
simulating the interactions of words during the
compositional process. Multiplicative operations
between word embeddings through gate structures
provide more flexibility and lead to better com-
positional results compare to the additive ones
in simple recurrent neural network. Experimen-
tally, the proposed architecture outperforms vari-
ous classifiers and feature engineering approaches,
matching the performance of the current best data-
driven approach. Vectors of task-distinctive words
(such as not) are distinguished after tuning and
representations of opposite-polarity words are sep-
arated. Moreover, predicting result on negation
test set shows our model is effective in dealing
with negation phrases (a typical case of sentiment
expressed by sequence). We study the process of
the network handling the negation expressions and
show the promising potential of our model sim-
ulating complex linguistic phenomena with gates
and constant error carousels in the LSTM blocks.
</bodyText>
<sectionHeader confidence="0.99997" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.99512">
2.1 Microblogs Sentiment Analysis
</subsectionHeader>
<bodyText confidence="0.99976285">
There have been a large amount of works on sen-
timent analysis over tweets. Some research makes
use of social network information (Tan et al.,
2011; Calais Guerra et al., 2011). These works re-
veal that social network relations of opinion hold-
ers could bring an influential bias to the textual
models. While some other works utilize the mi-
croblogging features uncommon in the formal lit-
erature, such as hashtags, emoticons (Hu et al.,
2013a; Liu et al., 2012). Speriosu et al. (2011) pro-
pose a unified graph propagation model to lever-
age textual features (such as emoticons) as well as
social information.
Semantic concept or entity based approaches
lead another research direction. Saif et al. (2012a;
2012b) make use of sentiment-topic features and
entities extracted by a third-party service to ease
data sparsity. Aspect-based models are also ex-
ploited to improve the tweet-level classifier (Lek
and Poo, 2013).
</bodyText>
<subsectionHeader confidence="0.997614">
2.2 Representation Learning and Deep
Models
</subsectionHeader>
<bodyText confidence="0.999408257142857">
Bengio et al. (2003) use distributed representa-
tions for words to fight the curse of dimension-
ality when training a neural probabilistic language
model. Such word vectors ease the syntactic and
semantic sparsity of bag-of-words representations.
Much recent research has explored such represen-
tations (Turian et al., 2010; Huang et al., 2012).
Recent works reveal that modifying word vec-
tors during training could capture polarity infor-
mation for the sentiment words effectively (Socher
et al., 2011; Tang et al., 2014). It would be also
insightful to analyze the embeddings that changed
the most during training. We conduct a compar-
ison between initial and tuned vectors and show
how the tuned vectors of task-distinctive function
words cooperate with the proposed architecture to
capture sequence information.
Distributed word vectors help in various NLP
tasks when using in neural models (Collobert et
al., 2011; Kalchbrenner et al., 2014). Com-
posing these representations to fix-length vectors
that contain phrase or sentence level information
also improves performance of sentiment analy-
sis (Yessenalina and Cardie, 2011). Recursive
neural networks model contextual interaction in
binary trees (Socher et al., 2011; Socher et al.,
2013). Words in the complex utterances are con-
sidered as leaf nodes and composed in a bottom-
up fashion. However, it’s difficult to get a binary
tree structure from the irregular short comments
like tweets. Not requiring structure information
or parser, long short-term memory models encode
the context in a chain and accommodate complex
linguistic phenomena with structure of gates and
constant error carousels.
</bodyText>
<sectionHeader confidence="0.914459" genericHeader="method">
3 Recurrent Neural Networks for
</sectionHeader>
<subsectionHeader confidence="0.959033">
Sentiment Analysis
</subsectionHeader>
<bodyText confidence="0.999550375">
Recurrent Neural Networks (RNN) have gained
attention in NLP field since Mikolov et al. (2010)
developed a statistical language model based on
a simple form known as Elman network (El-
man, 1990). Recent works used RNNs to pre-
dict words or characters in a sequence (Chrupała,
2014; Zhang and Lapata, 2014). Treating opin-
ion expression extraction as a sequence labelling
</bodyText>
<page confidence="0.990318">
1344
</page>
<figure confidence="0.994144">
output
hidden
input
t-1 t t+1
</figure>
<figureCaption confidence="0.9502695">
Figure 1: Illustration of simple recurrent neural
network. The input of the hidden layer comes
from both input layer and the hidden layer acti-
vations of previous time step.
</figureCaption>
<bodyText confidence="0.999790172413793">
problem, Irsoy and Cardie (2014) leverage deep
RNN models and achieve new state-of-the-art re-
sults for fine-grained extraction task. The lastest
work propose a tree-structured LSTM and conduct
a comprehensive study on using LSTM in predict-
ing the semantic relatedness of two sentences and
sentiment classification (Tai et al., 2015).
Fig.1 shows the illustration of a recurrent net-
work. By using self-connected layers, RNNs al-
low information cyclically encoded inside the net-
works. Such structures make it possible to get a
fix-length representation of a whole tweet by tem-
porally composing word vectors.
The recurrent architecture we used in this work
is shown in Fig.2. Each word is mapped to a vec-
tor through a Lookup-Table (LT) layer. The in-
put of the hidden layer comes from both the cur-
rent lookup-table layer activations and the hidden
layer’s activations one step back in time. In this
way, hidden layer encodes the past and current in-
formation. The hidden activations of the last time
step could be considered as the representation of
the whole sentence and used as input to classifica-
tion layer. By storing the word vectors in LT layer,
the model has reading and tuning access to word
representations.
Based on such recurrent architecture, we can
capture sequence information in the context and
identify polarities of the tweets.
</bodyText>
<subsectionHeader confidence="0.9898175">
3.1 Elman Network With Fixed
Lookup-Table
</subsectionHeader>
<bodyText confidence="0.997120333333333">
RNN-FLT: A simple implementation of the recur-
rent sentiment classifier is an Elman network (also
known as simple RNN) with Fixed Lookup-Table
(FLT). In such model, unsupervised pre-trained
word vectors in LT layer are constant during the
whole training process. The hidden layer activa-
</bodyText>
<figure confidence="0.63812025">
h
h
h
h Y
</figure>
<figureCaption confidence="0.69249">
Figure 2: Illustration of the general recurrent ar-
chitecture unfolded as a deep feedforward net-
work.
</figureCaption>
<bodyText confidence="0.516504">
tion of position h at time t is:
</bodyText>
<equation confidence="0.992995">
bth = f (ath) (1)
wh′hbt−1 (2)
h′
</equation>
<bodyText confidence="0.999992909090909">
where et represents the E-length embedding of
the tth word of the sentence, which stored in LT
layer. wih is the weight of connection between in-
put and hidden layer, while wh′h is the weights
of recurrent connection (self-connection of hidden
layer). f represents the sigmoid function. The
binary classification loss function O is computed
via cross entropy (CE) criterion and the network is
trained by stochastic gradient descent using back-
propagation through time (BPTT) (Werbos, 1990).
Here, we introduce the notation:
</bodyText>
<equation confidence="0.979706">
δti = ∂O (3)
∂at
i
</equation>
<bodyText confidence="0.99975875">
Firstly, the error propagate from output layer to
hidden layer of last time step T. The derivatives
with respect to the hidden activation of position i
at the last time step T are computed as follow:
</bodyText>
<equation confidence="0.909996333333333">
δTi = f′ (aT) ∂y
vi (4)
y
</equation>
<bodyText confidence="0.999663666666667">
where vi represents the weights of hidden-output
connection and the activation of the output layer y
is used to estimate probability of the tweet bearing
</bodyText>
<equation confidence="0.932494363636364">
EE
i
wiheti +
t
ah =
H
E
h′
1345
a particular polarity.
IbT i vi (5)
</equation>
<bodyText confidence="0.9979405">
Then the gradients of hidden layer of previous
time steps can be recursively computed as:
</bodyText>
<equation confidence="0.921519">
δt+1
h′ whh′ (6)
</equation>
<subsectionHeader confidence="0.93583">
3.2 Elman Network with Trainable
Lookup-Table
</subsectionHeader>
<bodyText confidence="0.999970777777778">
Unsupervised trained word embeddings represent
the syntactic and semantic similarity. However,
in specific tasks, the importance and functions of
different words vary. Negation words have simi-
lar unsupervised trained representations with other
adverbs, but they make distinctive contributions
in sentiment expressions. Besides the function
words, tuning word vectors of sentiment words
into polarity-representable ones turns out to be an
effective way to improve the performance of sen-
timent classifiers. (Maas et al., 2011; Labutov and
Lipson, 2013). Such tuned vectors work together
with the deep models, gaining the ability to de-
scribe complex linguistic phenomena.
RNN-TLT: To this end, we modify the word
vectors in the Trainable Lookup-Table (TLT) via
back propagation to get a better embedding of
words. The gradient of lookup-table layer is:
</bodyText>
<equation confidence="0.995293666666667">
H H
)δt i = g′ (at i E δthwih = E δthwih (7)
h=1 h=1
</equation>
<bodyText confidence="0.9999895">
where identity function g (x) = x is considered as
the activation function of lookup-table layer.
</bodyText>
<subsectionHeader confidence="0.999931">
3.3 Long Short-Term Memory
</subsectionHeader>
<bodyText confidence="0.968938304347826">
The simple RNN has the ability to capture con-
text information. However, the length of reach-
able context is often limited. The gradient tends
to vanish or blow up during the back propaga-
tion (Bengio et al., 1994; Pascanu et al., 2013).
Moreover, Elman network simply combines pre-
vious hidden activations with the current inputs
through addictive function. Such combination is
not powerful enough to describe a complex inter-
actions of words.
An effective solution for these problems is
the Long Short-Term Memory (LSTM) architec-
ture (Hochreiter and Schmidhuber, 1997; Gers,
Figure 3: Illustration of LSTM memory block
with one cell. Constant Error Carousel (CEC)
maintains the internal activation (called state) with
a recurrent connection of fixed weight 1.0, which
may be reset by the forget gate. The input and
output gates scale the input and output respec-
tively. All the gates are controlled by the main-
tained state, network input and hidden activation
of previous time step.
2001). Such architecture consists of a set of re-
currently connected subnets, known as memory
blocks. Each block contains one or more self-
connected memory cells and the input, output and
forget gates. Fig.3 gives an illustration of an
LSTM block. Once an error signal arrives Con-
stant Error Carousel (CEC), it remains constant,
neither growing nor decaying unless the forget
gate squashes it. In this way, it solves the vanish-
ing gradient problem and learns more appropriate
parameters during training.
Moreover, based on this structure, the input,
output and stored information can be partial ad-
justed by the gates, which enhances the flexibil-
ity of the model. The activations of hidden layer
rely on the current/previous state, previous hidden
activation and current input. These activations in-
teract to make up the final hidden outputs through
not only additive but also element-wise multiplica-
tive functions. Such structures are more capable to
learn a complex composition of word vectors than
simple RNNs.
These gates are controlled by current input, pre-
vious hidden activation and cell state in CEC unit:
</bodyText>
<figure confidence="0.835029272727273">
GtI = f (UIxt + VIht−1 + WIst−1) (8)
RXWSXW
&amp;(&amp;
LQSXW
i.0
RXWSXW
JDWH
IRUJHW
JDWH
LQSXW
JDWH
</figure>
<equation confidence="0.802314375">
y = f EH
i
H
)δt h = f′ (at h E
h′
1346
GtF = f (UFxt + VFht−1 + WFst−1) (9)
Gt = f (UOxt + VO ht− 1 + WO st) (10)
</equation>
<bodyText confidence="0.999581444444444">
where Gt indicates the gate activation at time t,
xt, ht and st is input, hidden activation and state in
CEC unit at time t respectively, while U, V and W
represent the corresponding weight matrices con-
nect them to the gates. Subscript I, F and O in-
dicate input, forget and output respectively. The
CEC state and block output are computed by the
functions with element-wise multiplicative opera-
tion:
</bodyText>
<equation confidence="0.998885">
� USxt + VSht−1�
st = Gt Fst−1 + Gt If (11)
at = Gt Ost (12)
</equation>
<bodyText confidence="0.990597">
where US indicates connection weight between in-
put and state, while VS represents the weight ma-
trix connecting hidden layer to state.
LSTM-TLT: By replacing the conventional
neural units in RNN-TLT with LSTM blocks,
we can get the LSTM network with Trainable
Lookup-Table. Such model achieves a flexible
compositional structure where the activations in-
teract in a multiplicative function. It provides
the capacity of describing diverse linguistic phe-
nomenon by learning complex compositions of
word embeddings.
</bodyText>
<sectionHeader confidence="0.999761" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.986134">
4.1 Data Set
</subsectionHeader>
<bodyText confidence="0.999982555555555">
We conduct experiments on the Stanford Twit-
ter Sentiment corpus (STS)1. The noisy-labelled
dataset is collected using emoticons as queries in
Twitter API (Go et al., 2009). 800,000 tweets con-
taining positive emoticons are extracted and la-
belled as positive, while 800,000 negative tweets
are extracted based on negative emoticons. The
manually labelled test set consists of 177 negative
and 182 positive tweets.
</bodyText>
<subsectionHeader confidence="0.950009">
4.2 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.9079795">
Recurrent Neural Network: We implement
the recurrent architecture with trainable lookup-
table layer by modifying RNNLIB (Graves, 2010)
toolkit.
Early Stopping: From the noisy labelled data,
we randomly selected 20,000 negative and 20,000
</bodyText>
<footnote confidence="0.954944">
1http://twittersentiment.appspot.com/
</footnote>
<bodyText confidence="0.9915245">
positive tweets as validation set for early stopping.
The rest 1,560,000 tweets are used as training set.
Parameter Setting: Tuned on the validation set,
the size of the hidden layer is set to 60.
Word Embeddings: We run word2vec on the
training set of 1.56M tweets (without labels) to get
domain-specific representations and use them as
initial input of the model. Limited to the input for-
mat of the toolkit, we learned 25-dimensional (rel-
atively small) vectors. Skip-gram architecture and
hierarchical softmax algorithm are chosen during
training.
</bodyText>
<table confidence="0.996005230769231">
4.3 Comparison with Data Driven
Approaches
Classifier Accuracy(%)
SVM 81.6
MNB 82.7
MAXENT 83.0
MAX-TDNN 78.8
NBoW 80.9
DCNN 87.4
RAE 77.6
RNN-FLT 80.2
RNN-TLT 86.4
LSTM-TLT 87.2
</table>
<tableCaption confidence="0.999912">
Table 1: Accuracies of different classifiers.
</tableCaption>
<bodyText confidence="0.993281863636364">
Naive Bayes, Maximum Entropy and SVM are
widely used classifiers. Go et al. (2009) presented
the results of three non-neural models using uni-
gram and bigram features.
Dynamic Convolutional Neural Network
(DCNN) (Kalchbrenner et al., 2014) is a general-
ization of MAX-TDNN (Collobert et al., 2011).
It has a clear hierarchy and is able to capture
long-range semantic relations. While the Neural
Bag-of-Words (NBoW) takes the summation of
word vectors as the input of a classification layer.
Kalchbrenner et al. (2014) reported performances
of the above three neural classifiers.
Recursive Autoencoder (RAE) has proven to be
an effective model to compose words vectors in
sentiment classification tasks (Socher et al., 2011).
We run RAE with randomly initialized word em-
beddings. We do not compare with RNTN (Socher
et al., 2013) for lack of phrase-level sentiment la-
bels and accurate parsing results.
Table 1 shows the accuracies of different clas-
sifiers. Notably, RNN-TLT and LSTM-TLT out-
</bodyText>
<page confidence="0.973875">
1347
</page>
<bodyText confidence="0.99996548">
perform the three non-neural classifiers. Trained
on the considerable data, these classifiers pro-
vide strong baselines. However, bag-of-words rep-
resentations are not powerful enough. Sparsity
and losing sequence information hurt the perfor-
mance of classifiers. Neural models overcome
these problems by using distributed representa-
tions and temporally encoding the contextual in-
teraction.
We notice a considerable increase in the perfor-
mance of the RNN-TLT with respect to the NBoW,
whose embeddings are also tuned during super-
vised training. It suggests that recurrent models
could generate better tweet-level representations
for the task by composing the word embeddings
in a temporal manner and capturing the sequential
information of the context.
Convolutional neural networks have outstand-
ing abilities of feature extraction, while LSTM-
TLT achieves a comparable performance. It sug-
gests that LSTM model is effective in learning
sentence-level representations with a flexible com-
positional structure.
RAE provides more general representations of
phrases by learning to reconstruct the word vec-
tors. Recurrent models outperform RAE indi-
cates that task-specific composing and representa-
tion learning with less syntactic information lead
to a better result.
Comparing RNN-FLT with RNN-TLT, we can
easily figure out that the model with trainable
lookup-table achieves better performance. This
is due to the fact that tuned embeddings capture
the sentiment information of text by distinguish-
ing words with opposite sentiment polarities and
providing more flexibility for composing. LSTM-
TLT does not outperform RNN-TLT significantly.
And the situations are almost the same on short-
sentence (less than 25 words) and long-sentence
(not less than 25 words) test set. Such results in-
dicate that the ability of LSTM getting access to
longer-distance context is not the determinant of
improvement, while the capacity of LSTM han-
dling complex expressions plays a more important
role. Such capacity will be further discussed in
subsection 4.7.
Since the training set is large enough, we have
not observed strong overfitting during the training
process. Therefore, no regularization technology
is employed in the experiments.
</bodyText>
<subsectionHeader confidence="0.726662">
4.4 Comparison with Feature Engineering
</subsectionHeader>
<table confidence="0.912479222222222">
Approaches
Method Craft feature Accuracy(%)
Speriosu et emoticon 84.7
al. (2011) hashtag
Saif et sentiment-topic 86.3
al. (2012a) semantic 84.1
Lek and aspect-based 88.3
Poo (2013)
This work 87.2
</table>
<tableCaption confidence="0.9720525">
Table 2: Comparison with different feature engi-
neering methods.
</tableCaption>
<bodyText confidence="0.989788545454546">
Table 2 shows the comparison with different
feature engineering methods. In Speriosu et al.
(2011)’s work, sentiment labels propagated in a
graph constructed on the basis of contextual re-
lations (e.g. word presence in a tweet) as well
as social relations. Saif et al. (2012a) eased the
data sparsity by adding sentiment-topic features
that extracted using traditional lexicon. While Lek
and Poo (2013) extracted tuple of [aspect, word,
sentiment] with hand-crafted templates. With the
help of opinion lexicon and POS tagger especially
designed for twitter data, their approach achieved
a state-of-the-art result.
Even though these methods rely on lexicons and
extracted entities, our data-driven model outper-
forms most of them, except the aspect-based one
that introduced twitter-specific resources. This
is due to the fact that traditional lexicons, even
emoticons added, are not able to cover the diver-
sification of twitter sentiment expressions, while
LSTM learns appropriate representations of senti-
ment information through compositional manner.
</bodyText>
<subsectionHeader confidence="0.98701">
4.5 Experiments on Manually Labelled Data
</subsectionHeader>
<bodyText confidence="0.999968769230769">
Different from STS dataset deciding the polar-
ity based on emoticons, the benchmark dataset
in SemEval 2013 (Nakov et al., 2013) is labelled
by human annotators. In this work we focus on
the binary polarity classification and abandon the
neutral tweets. There are 4099/735/1742 avail-
able tweets in the training/dev/test set respectively.
Since the training set is relatively small, we don’t
apply fine tuning on word vectors. Namely we
use fixed lookup-table for both RNN and LSTM.
300-dimensional vectors are learned on the 1.56M
tweets of STS dataset using word2vec. Other set-
tings stay the same as previous experiments.
</bodyText>
<page confidence="0.955614">
1348
</page>
<table confidence="0.5043164">
Method Accuracy(%)
SVM 74.5
RAE 75.4
RNN-FLT 83.0
LSTM-FLT 84.0
</table>
<tableCaption confidence="0.986844">
Table 3: Accuracies of different methods on Se-
mEval 2013
</tableCaption>
<bodyText confidence="0.7384514">
Table 3 shows our work compared to SVM
and Recursive Autoencoder. From the result, we
can see that the recurrent models outperforms the
baselines by exploiting more context information
of word interactions.
</bodyText>
<subsectionHeader confidence="0.98712">
4.6 Representation Learning
</subsectionHeader>
<bodyText confidence="0.996239">
Recent works reveal that modifying word vec-
tors during training could capture polarity infor-
mation for the sentiment words effectively (Socher
et al., 2011; Tang et al., 2014). However, it would
be also helpful to analyse the embeddings that
changed the most.
Function words: We choose 1000 most fre-
quent words. For each word, we compute the dis-
tance between unsupervised vector and tuned vec-
tor. 20 words that change most are shown in Fig.4.
It’s noteworthy that there are five negation
words (not, no, n’t, never and Not) in the notably-
change group. The representations of negation
words are quite similar with other adverbs in un-
supervised learned embeddings, while the pro-
posed model distinguishes them. This indicate that
our polarity-supervised models identify negation
words as distinctive symbols in sentiment classifi-
cation task, while unsupervised learned vectors do
not contain such information.
Besides the negation words and sentiment
words, there are also other prepositions, pronouns
and conjunctions change dramatically (e.g. and
and but). Such function words also play a special
role in sentiment expressions (Socher et al., 2013)
and the model in this paper distinguishes them.
However, the contributions of these words to the
task are not that explainable as negation words (at
least without sentiment strength information).
To further explain how the tuned vectors work
together with the network and describe interac-
tions between words, we study the process of the
model classifying negation phrases in the follow-
ing subsection.
Sentiment words: In order to study the em-
</bodyText>
<figure confidence="0.489656">
0 0.1 0.2 0.3 0.4 0.5
</figure>
<figureCaption confidence="0.701907">
Figure 4: Word change scale to [0,1]. Distances
</figureCaption>
<bodyText confidence="0.994153961538461">
are computed by reversing cosine similarity.
bedding change of sentiment words, we choose
the most frequent sentiment words in our train-
ing data, 20 positive and 20 negative, and ob-
serve the dissimilarity of the vectors in a two-
dimensional space. An alternative least-square
scaling is implemented based on Euclidean dis-
tance between word vectors. Figure 5 shows
sentiment-specific tuning reduces the overlap of
opposite polarities. Polarities of words are identi-
fied based on a widely-used sentiment lexicon (Hu
and Liu, 2004).
To explicitly evaluate it, we selected embed-
dings of 2000 most frequent sentiment words
(1000 each polarity) and compute the centers of
both classes. If an embedding is closer to the op-
posite polarity center, we consider it as an over-
lap. Experimentally, the proportion of overlap of
unsupervised learned vectors is 19.55%, while the
one of tuned vectors is 11.4%. Namely the over-
lap ratio is reduced by 41.7%. Experimentally,
such polarity separating relies on tuning through
lookup-table layer rather than LSTM structure.
With the decrease of overlap of polarities, senti-
ment of word turns more distinguishable, which is
helpful for polarity prediction.
</bodyText>
<subsectionHeader confidence="0.999041">
4.7 Case Study: Negation
</subsectionHeader>
<bodyText confidence="0.9985925">
Negation phrases are typical cases where senti-
ment is expressed by sequence rather than words.
To evaluate the ability of the model dealing with
such cases, we select most frequent 1000 negative
and 1000 positive words in the training data and
generate the corresponding negation phrases (such
</bodyText>
<figure confidence="0.9855026">
I
not
And
wait
this
for
be
from
no
bad
Now
n&apos;t
better
But
finally
never
Not
but
me
good
</figure>
<page confidence="0.823088">
1349
</page>
<figureCaption confidence="0.9714845">
Figure 5: Distance of word vectors shown in two-
dimensional space. The above figure shows the
</figureCaption>
<bodyText confidence="0.5587324">
distribution of unsupervised learning vectors and
the below figure indicates the tuned one. The solid
and hollow points represent the positive and nega-
tive words respectively.
as not good).
</bodyText>
<figure confidence="0.6421">
Classifier Accuracy(%)
MNB+unigram+bigram 32.98
RNN-TLT 52.00
LSTM-TLT 64.85
</figure>
<tableCaption confidence="0.8471945">
Table 4: Accuracy on generated negation phrases
test set.
</tableCaption>
<bodyText confidence="0.999213210526316">
Statistical result shows that only 37.6% of the
negation phrases appeared in the training text. It
sets a theoretical upper bound to the classifiers
based on the unigram and bigram features. Ex-
perimental result shown in Table 4 indicates that
LSTM model effectively handles the sequential
expressions of negation. By composing word vec-
tors, recurrent models ease the sparsity of bag-of-
word features and achieve a significant improve
than MNB using unigram and bigram features.
LSTM outperform RNN by 12.85%, such result
suggests the element-wise multiplicative composi-
tional function of LSTM provides more flexibility
to simulate interactions between word vectors. A
clear process of LSTM handling negation phrases
is observed, which is described in the rest of the
subsection, while the one of RNN is not that obvi-
ous.
As mentioned in 4.6, the task-distinctive func-
</bodyText>
<figure confidence="0.9845005">
&lt;s&gt;—bad &lt;s&gt;
hyperplane
not—good
not—bad
not &lt;s&gt;_good
-.75 -.50 -.25 .00 .25
</figure>
<figureCaption confidence="0.984273">
Figure 6: Hidden activations of negation phrases.
</figureCaption>
<bodyText confidence="0.971019861111111">
&lt;s&gt; represent the beginning of sentences. not bad
and good lead to positive outputs, while not good
and bad result in negative values. The dotted line
indicates the classification hyperplane. The solid
arrows represent the hidden vector changes when
the network take the word good as input, while the
dotted arrows indicate the changes when the word
bad is input. The sentiment words are input in two
situations (as initial input or after negation word),
while the changes of hidden vectors of same word
are opposite in the two situations.
tion words are distinguished. It would be insight-
ful to show how it works together with the LSTM
structure.
We train the network on STS dataset and test it
on few words and phrases (good, bad, not good
and not bad). For the convenience of analysis the
activation within the network, we set the size of
hidden layer to 2. Such setting reduces the perfor-
mance by about 7% on the public test set, but the
trained model still work effectively. Fig.6 shows
the activations of LSTM hidden layers. Both sen-
timent words and negation phrases are classified
into correct categories. Furthermore, when senti-
ment words like good (i) input as the first word
of sentence and (ii) input after negation word, it
cause opposite change in hidden layer. These be-
haviours simulate the change of sentiment in the
negation expressions.
As mentioned in 3.3, gates’ activations are con-
trolled by current input, state in CEC unit and out-
put of hidden layer of previous time step. They
are many possible ways for the model to simulat-
ing the sentiment change. In the experiment, the
observed situation is shown in Fig.7:
Negation word contains both polarities. The
</bodyText>
<figure confidence="0.979975884615385">
5
4
3
2
1
.2
.3
.5 .4 .3 .2 1 2 3
3
.2
.3
.4
3
1
2
1
.5
.3
.00
-.10
-.20
-.30
-.40
-.50
-.60
1350
</figure>
<figureCaption confidence="0.878185666666667">
Figure 7: Observed process of LSTM block han-
dling negation phrase not good. Some less impor-
tant connections are omitted in this figure.
</figureCaption>
<bodyText confidence="0.99871296">
positive-axle and negative-axle are almost orthog-
onal. Negation word has large components on
both axles.
not make input gate close. Experiments show
recurrent activations make the input gate close,
namely previous word not squashes the input (both
current and recurrent input) to a very small value.
Choose a polarity to forget. The combination
of the recurrent input not and current input good
make the CEC unit forget the positive informa-
tion, namely they make forget gate reduce state’s
component on positive-axle while leaving a large
projection on negative-axle. A significant dissim-
ilarity of forget gate activations between positive
and negative words is observed in the experiment,
when they are input after not.
In this way, the temporally-input phrase not
good shows a negative polarity. Correspondingly,
phrase not bad turns positive after reducing the
negative components of the negation word. Such
case shows the process of the gates and CEC unit
cooperating in the LSTM structure. Together with
tuned vectors, the architecture has a promising po-
tential of capture sequence information by simu-
lating complex interactions between words.
</bodyText>
<sectionHeader confidence="0.999546" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99948515">
In this paper we have explored to capture twit-
ter sentiment expressed by interactions of words.
The contributions of this paper can be summarized
as follows: (i) We have described long short-term
memory based model to compose word represen-
tations through a flexible compositional function.
Tested on a public dataset, the proposed architec-
ture achieves result comparable to the current best
data-driven model. The experiment on negation
test set shows the ability of the model capturing
sequential information. (ii) Beyond tuning vectors
of sentiment words, we put forward a perspective
of distinguishing task-distinctive function words
only relying on the label of the whole sequence.
(iii) We conduct an interesting case study on the
process of task-distinctive word vectors working
together with deep model, which is usually con-
sidered as a black-box in other neural networks,
indicating the promising potential of the architec-
ture simulating complex linguistic phenomena.
</bodyText>
<sectionHeader confidence="0.999157" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999043818181818">
We thank Deyuan Zhang, Lei Cui, Feng Liu and
Ming Liu for their great help. We thank the
anonymous reviewers for their insightful feed-
backs on this work. This research is sup-
ported by National Natural Science Foundation
of China (No.613400114), Specialized Research
Fund for the Doctoral Program of Higher Educa-
tion (No.20132302120047), the Special Financial
Grant from the China Postdoctoral Science Foun-
dation (No.2014T70340), China Postdoctoral Sci-
ence Foundation (No.2013M530156)
</bodyText>
<sectionHeader confidence="0.999098" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997573458333334">
Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gra-
dient descent is difficult. Neural Networks, IEEE
Transactions on, 5(2):157–166.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137–1155.
Pedro Henrique Calais Guerra, Adriano Veloso, Wag-
ner Meira Jr, and Virg´ılio Almeida. 2011. From
bias to opinion: a transfer-learning approach to real-
time sentiment analysis. In Proceedings of the 17th
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 150–158.
ACM.
Grzegorz Chrupała. 2014. Normalizing tweets with
edit scripts and recurrent neural embeddings. In
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), pages 680–686, Baltimore, Mary-
land, June. Association for Computational Linguis-
tics.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
</reference>
<figure confidence="0.997307846153846">
QHJ
SRV
RSHQ
QRW
pN.�
QRW JRRG
�V!
QHJ
SRV
QHJDWLYH
FORVH
JRRG
QRW
</figure>
<page confidence="0.946703">
1351
</page>
<reference confidence="0.999170183486239">
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
Jeffrey L Elman. 1990. Finding structure in time.
Cognitive science, 14(2):179–211.
Felix Gers. 2001. Long Short-Term Memory in Recur-
rent Neural Networks. Ph.D. thesis, Ph. D. thesis,
Ecole Polytechnique Federale de Lausanne.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford, pages 1–12.
Alex Graves. 2010. Rnnlib: A recurrent neural
network library for sequence learning problems.
http://sourceforge.net/projects/
rnnl.
Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ’04, pages
168–177, New York, NY, USA. ACM.
Xia Hu, Jiliang Tang, Huiji Gao, and Huan Liu.
2013a. Unsupervised sentiment analysis with emo-
tional signals. In Proceedings of the 22nd interna-
tional conference on World Wide Web, pages 607–
618. International World Wide Web Conferences
Steering Committee.
Xia Hu, Lei Tang, Jiliang Tang, and Huan Liu. 2013b.
Exploiting social relations for sentiment analysis in
microblogging. In Proceedings of the sixth ACM in-
ternational conference on Web search and data min-
ing, pages 537–546. ACM.
Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 873–882. Asso-
ciation for Computational Linguistics.
Ozan Irsoy and Claire Cardie. 2014. Opinion mining
with deep recurrent neural networks. In Proceed-
ings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
720–728.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics, June.
Igor Labutov and Hod Lipson. 2013. Re-embedding
words. In Proceedings of the 51th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 489–493. Association for Computational
Linguistics.
Hsiang Hui Lek and Danny CC Poo. 2013. Aspect-
based twitter sentiment classification. In Tools with
Artificial Intelligence (ICTAI), 2013 IEEE 25th In-
ternational Conference on, pages 366–373. IEEE.
Kun-Lin Liu, Wu-Jun Li, and Minyi Guo. 2012.
Emoticon smoothed language models for twitter
sentiment analysis. In AAAI.
Andrew L Maas, Raymond E Daly, Peter T Pham, Dan
Huang, Andrew Y Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 142–150.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH, pages 1045–1048.
Preslav Nakov, Zornitsa Kozareva, Alan Ritter, Sara
Rosenthal, Veselin Stoyanov, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter. In Proceedings of the International Work-
shop on Semantic Evaluation, SemEval, volume 13.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 conference on Empirical methods in natural
language processing-Volume 10, pages 79–86. As-
sociation for Computational Linguistics.
Razvan Pascanu, Tomas Mikolov, and Yoshua Ben-
gio. 2013. On the difficulty of training recurrent
neural networks. In Proceedings of The 30th In-
ternational Conference on Machine Learning, pages
1310–1318.
Hassan Saif, Yulan He, and Harith Alani. 2012a. Al-
leviating data sparsity for twitter sentiment analysis.
Making Sense of Microposts (# MSM2012).
Hassan Saif, Yulan He, and Harith Alani. 2012b. Se-
mantic sentiment analysis of twitter. In The Seman-
tic Web–ISWC 2012, pages 508–524. Springer.
Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151–161. Association for
Computational Linguistics.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1631–1642.
</reference>
<page confidence="0.859929">
1352
</page>
<reference confidence="0.999827638297872">
Michael Speriosu, Nikita Sudan, Sid Upadhyay, and
Jason Baldridge. 2011. Twitter polarity classifica-
tion with label propagation over lexical links and the
follower graph. In Proceedings of the First work-
shop on Unsupervised Learning in NLP, pages 53–
63. Association for Computational Linguistics.
Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved semantic representa-
tions from tree-structured long short-term memory
networks. arXiv preprint arXiv:1503.00075.
Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, Ming
Zhou, and Ping Li. 2011. User-level sentiment
analysis incorporating social networks. In Proceed-
ings of the 17th ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
pages 1397–1405. ACM.
Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting
Liu, and Bing Qin. 2014. Learning sentiment-
specific word embedding for twitter sentiment clas-
sification. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1555–1565.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384–394. Association for
Computational Linguistics.
Svitlana Volkova, Theresa Wilson, and David
Yarowsky. 2013. Exploring sentiment in social
media: Bootstrapping subjectivity clues from
multilingual twitter streams. In Association for
Computational Linguistics (ACL).
Paul J Werbos. 1990. Backpropagation through time:
what it does and how to do it. Proceedings of the
IEEE, 78(10):1550–1560.
Ainur Yessenalina and Claire Cardie. 2011. Com-
positional matrix-space models for sentiment anal-
ysis. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
172–182. Association for Computational Linguis-
tics.
Xingxing Zhang and Mirella Lapata. 2014. Chi-
nese poetry generation with recurrent neural net-
works. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 670–680.
</reference>
<page confidence="0.978935">
1353
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.416577">
<title confidence="0.999552">Predicting Polarities of Tweets by Composing Word Embeddings</title>
<author confidence="0.742085">Long Short-Term Memory Yuanchao Chengjie Baoxun</author>
<author confidence="0.742085">Xiaolong</author>
<affiliation confidence="0.947504">of Computer Science and Harbin Institute of Technology, Harbin, and Service Group, Microsoft, Beijing,</affiliation>
<abstract confidence="0.99931144">In this paper, we introduce Long Short- Term Memory (LSTM) recurrent network for twitter sentiment prediction. With the help of gates and constant error carousels in the memory block structure, the model could handle interactions between words through a flexible compositional function. Experiments on a public noisy labelled data show that our model outperforms several feature-engineering approaches, with the result comparable to the current best data-driven technique. According to the evaluation on a generated negation phrase test set, the proposed architecture doubles the performance of non-neural model based on bag-of-word features. Furthermore, words with special functions (such as negation and transition) are distinguished and the dissimilarities of words with opposite sentiment are magnified. An interesting case study on negation expression processing shows a promising potential of the architecture dealing with complex sentiment phrases.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Patrice Simard</author>
<author>Paolo Frasconi</author>
</authors>
<title>Learning long-term dependencies with gradient descent is difficult. Neural Networks,</title>
<date>1994</date>
<journal>IEEE Transactions on,</journal>
<volume>5</volume>
<issue>2</issue>
<contexts>
<context position="3861" citStr="Bengio et al., 1994" startWordPosition="562" endWordPosition="565">ld help to analyze complex sentiment expressions. One possible method to leverage context is connecting embeddings of words in a window and compose them to a fixlength vector (Collobert et al., 2011). However, window-based methods may have difficulty reaching long-distance words and simply connected vectors do not always represent the interactions of context properly. Theoretically, a recurrent neural network could process the whole sentence of arbitrary length by encoding the context cyclically. However, the length of reachable context is often limited when using stochastic gradient descent (Bengio et al., 1994; Pascanu et al., 2013). Besides that, a 1343 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1343–1353, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics traditional recurrent architecture is not powerful enough to deal with the complex sentiment expressions. Fixed input limits the network’s ability of learning task-specific representations and simple additive combination of hidden activations and input activations has difficulty capturing m</context>
<context position="13110" citStr="Bengio et al., 1994" startWordPosition="2027" endWordPosition="2030">ability to describe complex linguistic phenomena. RNN-TLT: To this end, we modify the word vectors in the Trainable Lookup-Table (TLT) via back propagation to get a better embedding of words. The gradient of lookup-table layer is: H H )δt i = g′ (at i E δthwih = E δthwih (7) h=1 h=1 where identity function g (x) = x is considered as the activation function of lookup-table layer. 3.3 Long Short-Term Memory The simple RNN has the ability to capture context information. However, the length of reachable context is often limited. The gradient tends to vanish or blow up during the back propagation (Bengio et al., 1994; Pascanu et al., 2013). Moreover, Elman network simply combines previous hidden activations with the current inputs through addictive function. Such combination is not powerful enough to describe a complex interactions of words. An effective solution for these problems is the Long Short-Term Memory (LSTM) architecture (Hochreiter and Schmidhuber, 1997; Gers, Figure 3: Illustration of LSTM memory block with one cell. Constant Error Carousel (CEC) maintains the internal activation (called state) with a recurrent connection of fixed weight 1.0, which may be reset by the forget gate. The input an</context>
</contexts>
<marker>Bengio, Simard, Frasconi, 1994</marker>
<rawString>Yoshua Bengio, Patrice Simard, and Paolo Frasconi. 1994. Learning long-term dependencies with gradient descent is difficult. Neural Networks, IEEE Transactions on, 5(2):157–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Jauvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="6630" citStr="Bengio et al. (2003)" startWordPosition="973" endWordPosition="976">uncommon in the formal literature, such as hashtags, emoticons (Hu et al., 2013a; Liu et al., 2012). Speriosu et al. (2011) propose a unified graph propagation model to leverage textual features (such as emoticons) as well as social information. Semantic concept or entity based approaches lead another research direction. Saif et al. (2012a; 2012b) make use of sentiment-topic features and entities extracted by a third-party service to ease data sparsity. Aspect-based models are also exploited to improve the tweet-level classifier (Lek and Poo, 2013). 2.2 Representation Learning and Deep Models Bengio et al. (2003) use distributed representations for words to fight the curse of dimensionality when training a neural probabilistic language model. Such word vectors ease the syntactic and semantic sparsity of bag-of-words representations. Much recent research has explored such representations (Turian et al., 2010; Huang et al., 2012). Recent works reveal that modifying word vectors during training could capture polarity information for the sentiment words effectively (Socher et al., 2011; Tang et al., 2014). It would be also insightful to analyze the embeddings that changed the most during training. We cond</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro Henrique Calais Guerra</author>
<author>Adriano Veloso</author>
<author>Wagner Meira Jr</author>
<author>Virg´ılio Almeida</author>
</authors>
<title>From bias to opinion: a transfer-learning approach to realtime sentiment analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>150--158</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5827" citStr="Guerra et al., 2011" startWordPosition="845" endWordPosition="848"> are separated. Moreover, predicting result on negation test set shows our model is effective in dealing with negation phrases (a typical case of sentiment expressed by sequence). We study the process of the network handling the negation expressions and show the promising potential of our model simulating complex linguistic phenomena with gates and constant error carousels in the LSTM blocks. 2 Related Work 2.1 Microblogs Sentiment Analysis There have been a large amount of works on sentiment analysis over tweets. Some research makes use of social network information (Tan et al., 2011; Calais Guerra et al., 2011). These works reveal that social network relations of opinion holders could bring an influential bias to the textual models. While some other works utilize the microblogging features uncommon in the formal literature, such as hashtags, emoticons (Hu et al., 2013a; Liu et al., 2012). Speriosu et al. (2011) propose a unified graph propagation model to leverage textual features (such as emoticons) as well as social information. Semantic concept or entity based approaches lead another research direction. Saif et al. (2012a; 2012b) make use of sentiment-topic features and entities extracted by a th</context>
</contexts>
<marker>Guerra, Veloso, Jr, Almeida, 2011</marker>
<rawString>Pedro Henrique Calais Guerra, Adriano Veloso, Wagner Meira Jr, and Virg´ılio Almeida. 2011. From bias to opinion: a transfer-learning approach to realtime sentiment analysis. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 150–158. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Chrupała</author>
</authors>
<title>Normalizing tweets with edit scripts and recurrent neural embeddings.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>680--686</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="8586" citStr="Chrupała, 2014" startWordPosition="1275" endWordPosition="1276"> to get a binary tree structure from the irregular short comments like tweets. Not requiring structure information or parser, long short-term memory models encode the context in a chain and accommodate complex linguistic phenomena with structure of gates and constant error carousels. 3 Recurrent Neural Networks for Sentiment Analysis Recurrent Neural Networks (RNN) have gained attention in NLP field since Mikolov et al. (2010) developed a statistical language model based on a simple form known as Elman network (Elman, 1990). Recent works used RNNs to predict words or characters in a sequence (Chrupała, 2014; Zhang and Lapata, 2014). Treating opinion expression extraction as a sequence labelling 1344 output hidden input t-1 t t+1 Figure 1: Illustration of simple recurrent neural network. The input of the hidden layer comes from both input layer and the hidden layer activations of previous time step. problem, Irsoy and Cardie (2014) leverage deep RNN models and achieve new state-of-the-art results for fine-grained extraction task. The lastest work propose a tree-structured LSTM and conduct a comprehensive study on using LSTM in predicting the semantic relatedness of two sentences and sentiment cla</context>
</contexts>
<marker>Chrupała, 2014</marker>
<rawString>Grzegorz Chrupała. 2014. Normalizing tweets with edit scripts and recurrent neural embeddings. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 680–686, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, </marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa.</rawString>
</citation>
<citation valid="true">
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="6133" citStr="(2011)" startWordPosition="900" endWordPosition="900">istic phenomena with gates and constant error carousels in the LSTM blocks. 2 Related Work 2.1 Microblogs Sentiment Analysis There have been a large amount of works on sentiment analysis over tweets. Some research makes use of social network information (Tan et al., 2011; Calais Guerra et al., 2011). These works reveal that social network relations of opinion holders could bring an influential bias to the textual models. While some other works utilize the microblogging features uncommon in the formal literature, such as hashtags, emoticons (Hu et al., 2013a; Liu et al., 2012). Speriosu et al. (2011) propose a unified graph propagation model to leverage textual features (such as emoticons) as well as social information. Semantic concept or entity based approaches lead another research direction. Saif et al. (2012a; 2012b) make use of sentiment-topic features and entities extracted by a third-party service to ease data sparsity. Aspect-based models are also exploited to improve the tweet-level classifier (Lek and Poo, 2013). 2.2 Representation Learning and Deep Models Bengio et al. (2003) use distributed representations for words to fight the curse of dimensionality when training a neural </context>
<context position="21010" citStr="(2011)" startWordPosition="3270" endWordPosition="3270">ot less than 25 words) test set. Such results indicate that the ability of LSTM getting access to longer-distance context is not the determinant of improvement, while the capacity of LSTM handling complex expressions plays a more important role. Such capacity will be further discussed in subsection 4.7. Since the training set is large enough, we have not observed strong overfitting during the training process. Therefore, no regularization technology is employed in the experiments. 4.4 Comparison with Feature Engineering Approaches Method Craft feature Accuracy(%) Speriosu et emoticon 84.7 al. (2011) hashtag Saif et sentiment-topic 86.3 al. (2012a) semantic 84.1 Lek and aspect-based 88.3 Poo (2013) This work 87.2 Table 2: Comparison with different feature engineering methods. Table 2 shows the comparison with different feature engineering methods. In Speriosu et al. (2011)’s work, sentiment labels propagated in a graph constructed on the basis of contextual relations (e.g. word presence in a tweet) as well as social relations. Saif et al. (2012a) eased the data sparsity by adding sentiment-topic features that extracted using traditional lexicon. While Lek and Poo (2013) extracted tuple of</context>
</contexts>
<marker>2011</marker>
<rawString>2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey L Elman</author>
</authors>
<title>Finding structure in time.</title>
<date>1990</date>
<journal>Cognitive science,</journal>
<volume>14</volume>
<issue>2</issue>
<contexts>
<context position="8501" citStr="Elman, 1990" startWordPosition="1259" endWordPosition="1261">onsidered as leaf nodes and composed in a bottomup fashion. However, it’s difficult to get a binary tree structure from the irregular short comments like tweets. Not requiring structure information or parser, long short-term memory models encode the context in a chain and accommodate complex linguistic phenomena with structure of gates and constant error carousels. 3 Recurrent Neural Networks for Sentiment Analysis Recurrent Neural Networks (RNN) have gained attention in NLP field since Mikolov et al. (2010) developed a statistical language model based on a simple form known as Elman network (Elman, 1990). Recent works used RNNs to predict words or characters in a sequence (Chrupała, 2014; Zhang and Lapata, 2014). Treating opinion expression extraction as a sequence labelling 1344 output hidden input t-1 t t+1 Figure 1: Illustration of simple recurrent neural network. The input of the hidden layer comes from both input layer and the hidden layer activations of previous time step. problem, Irsoy and Cardie (2014) leverage deep RNN models and achieve new state-of-the-art results for fine-grained extraction task. The lastest work propose a tree-structured LSTM and conduct a comprehensive study on</context>
</contexts>
<marker>Elman, 1990</marker>
<rawString>Jeffrey L Elman. 1990. Finding structure in time. Cognitive science, 14(2):179–211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix Gers</author>
</authors>
<title>Long Short-Term Memory in Recurrent Neural Networks.</title>
<date>2001</date>
<tech>Ph.D. thesis, Ph. D. thesis,</tech>
<institution>Ecole Polytechnique Federale de Lausanne.</institution>
<marker>Gers, 2001</marker>
<rawString>Felix Gers. 2001. Long Short-Term Memory in Recurrent Neural Networks. Ph.D. thesis, Ph. D. thesis, Ecole Polytechnique Federale de Lausanne.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alec Go</author>
<author>Richa Bhayani</author>
<author>Lei Huang</author>
</authors>
<title>Twitter sentiment classification using distant supervision. CS224N Project Report,</title>
<date>2009</date>
<pages>1--12</pages>
<location>Stanford,</location>
<contexts>
<context position="1645" citStr="Go et al. (2009)" startWordPosition="232" endWordPosition="235">urthermore, words with special functions (such as negation and transition) are distinguished and the dissimilarities of words with opposite sentiment are magnified. An interesting case study on negation expression processing shows a promising potential of the architecture dealing with complex sentiment phrases. 1 Introduction Twitter and other similar microblogs are rich resources for opinions on various kinds of products and events. Detecting sentiment in microblogs is a challenging task that has attracted increasing research interest in recent years (Hu et al., 2013b; Volkova et al., 2013). Go et al. (2009) carried out the pioneer work of predicting sentiment in tweets using machine learning technology. They conducted comprehensive experiments on multiple classifiers based on bag-of-words feature. Such feature is widely used because it’s simple and surprisingly efficient in many tasks. However, there are also disadvantages of bag-of-words features represented by onehot vectors. Firstly, it bears a data sparsity issue (Saif et al., 2012a). In tweets, irregularities and 140-character limitation exacerbate the sparseness. Secondly, losing sequence information makes it difficult to figure out the po</context>
<context position="16367" citStr="Go et al., 2009" startWordPosition="2575" endWordPosition="2578">matrix connecting hidden layer to state. LSTM-TLT: By replacing the conventional neural units in RNN-TLT with LSTM blocks, we can get the LSTM network with Trainable Lookup-Table. Such model achieves a flexible compositional structure where the activations interact in a multiplicative function. It provides the capacity of describing diverse linguistic phenomenon by learning complex compositions of word embeddings. 4 Experiments 4.1 Data Set We conduct experiments on the Stanford Twitter Sentiment corpus (STS)1. The noisy-labelled dataset is collected using emoticons as queries in Twitter API (Go et al., 2009). 800,000 tweets containing positive emoticons are extracted and labelled as positive, while 800,000 negative tweets are extracted based on negative emoticons. The manually labelled test set consists of 177 negative and 182 positive tweets. 4.2 Experimental Settings Recurrent Neural Network: We implement the recurrent architecture with trainable lookuptable layer by modifying RNNLIB (Graves, 2010) toolkit. Early Stopping: From the noisy labelled data, we randomly selected 20,000 negative and 20,000 1http://twittersentiment.appspot.com/ positive tweets as validation set for early stopping. The </context>
<context position="17766" citStr="Go et al. (2009)" startWordPosition="2783" endWordPosition="2786">ning set of 1.56M tweets (without labels) to get domain-specific representations and use them as initial input of the model. Limited to the input format of the toolkit, we learned 25-dimensional (relatively small) vectors. Skip-gram architecture and hierarchical softmax algorithm are chosen during training. 4.3 Comparison with Data Driven Approaches Classifier Accuracy(%) SVM 81.6 MNB 82.7 MAXENT 83.0 MAX-TDNN 78.8 NBoW 80.9 DCNN 87.4 RAE 77.6 RNN-FLT 80.2 RNN-TLT 86.4 LSTM-TLT 87.2 Table 1: Accuracies of different classifiers. Naive Bayes, Maximum Entropy and SVM are widely used classifiers. Go et al. (2009) presented the results of three non-neural models using unigram and bigram features. Dynamic Convolutional Neural Network (DCNN) (Kalchbrenner et al., 2014) is a generalization of MAX-TDNN (Collobert et al., 2011). It has a clear hierarchy and is able to capture long-range semantic relations. While the Neural Bag-of-Words (NBoW) takes the summation of word vectors as the input of a classification layer. Kalchbrenner et al. (2014) reported performances of the above three neural classifiers. Recursive Autoencoder (RAE) has proven to be an effective model to compose words vectors in sentiment cla</context>
</contexts>
<marker>Go, Bhayani, Huang, 2009</marker>
<rawString>Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter sentiment classification using distant supervision. CS224N Project Report, Stanford, pages 1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Graves</author>
</authors>
<title>Rnnlib: A recurrent neural network library for sequence learning problems.</title>
<date>2010</date>
<note>http://sourceforge.net/projects/ rnnl.</note>
<contexts>
<context position="16767" citStr="Graves, 2010" startWordPosition="2634" endWordPosition="2635">rd embeddings. 4 Experiments 4.1 Data Set We conduct experiments on the Stanford Twitter Sentiment corpus (STS)1. The noisy-labelled dataset is collected using emoticons as queries in Twitter API (Go et al., 2009). 800,000 tweets containing positive emoticons are extracted and labelled as positive, while 800,000 negative tweets are extracted based on negative emoticons. The manually labelled test set consists of 177 negative and 182 positive tweets. 4.2 Experimental Settings Recurrent Neural Network: We implement the recurrent architecture with trainable lookuptable layer by modifying RNNLIB (Graves, 2010) toolkit. Early Stopping: From the noisy labelled data, we randomly selected 20,000 negative and 20,000 1http://twittersentiment.appspot.com/ positive tweets as validation set for early stopping. The rest 1,560,000 tweets are used as training set. Parameter Setting: Tuned on the validation set, the size of the hidden layer is set to 60. Word Embeddings: We run word2vec on the training set of 1.56M tweets (without labels) to get domain-specific representations and use them as initial input of the model. Limited to the input format of the toolkit, we learned 25-dimensional (relatively small) vec</context>
</contexts>
<marker>Graves, 2010</marker>
<rawString>Alex Graves. 2010. Rnnlib: A recurrent neural network library for sequence learning problems. http://sourceforge.net/projects/ rnnl.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sepp Hochreiter</author>
<author>J¨urgen Schmidhuber</author>
</authors>
<title>Long short-term memory.</title>
<date>1997</date>
<booktitle>Neural computation,</booktitle>
<pages>9--8</pages>
<contexts>
<context position="13464" citStr="Hochreiter and Schmidhuber, 1997" startWordPosition="2079" endWordPosition="2082">tivation function of lookup-table layer. 3.3 Long Short-Term Memory The simple RNN has the ability to capture context information. However, the length of reachable context is often limited. The gradient tends to vanish or blow up during the back propagation (Bengio et al., 1994; Pascanu et al., 2013). Moreover, Elman network simply combines previous hidden activations with the current inputs through addictive function. Such combination is not powerful enough to describe a complex interactions of words. An effective solution for these problems is the Long Short-Term Memory (LSTM) architecture (Hochreiter and Schmidhuber, 1997; Gers, Figure 3: Illustration of LSTM memory block with one cell. Constant Error Carousel (CEC) maintains the internal activation (called state) with a recurrent connection of fixed weight 1.0, which may be reset by the forget gate. The input and output gates scale the input and output respectively. All the gates are controlled by the maintained state, network input and hidden activation of previous time step. 2001). Such architecture consists of a set of recurrently connected subnets, known as memory blocks. Each block contains one or more selfconnected memory cells and the input, output and</context>
</contexts>
<marker>Hochreiter, Schmidhuber, 1997</marker>
<rawString>Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735–1780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’04,</booktitle>
<pages>168--177</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="25452" citStr="Hu and Liu, 2004" startWordPosition="3955" endWordPosition="3958">rder to study the em0 0.1 0.2 0.3 0.4 0.5 Figure 4: Word change scale to [0,1]. Distances are computed by reversing cosine similarity. bedding change of sentiment words, we choose the most frequent sentiment words in our training data, 20 positive and 20 negative, and observe the dissimilarity of the vectors in a twodimensional space. An alternative least-square scaling is implemented based on Euclidean distance between word vectors. Figure 5 shows sentiment-specific tuning reduces the overlap of opposite polarities. Polarities of words are identified based on a widely-used sentiment lexicon (Hu and Liu, 2004). To explicitly evaluate it, we selected embeddings of 2000 most frequent sentiment words (1000 each polarity) and compute the centers of both classes. If an embedding is closer to the opposite polarity center, we consider it as an overlap. Experimentally, the proportion of overlap of unsupervised learned vectors is 19.55%, while the one of tuned vectors is 11.4%. Namely the overlap ratio is reduced by 41.7%. Experimentally, such polarity separating relies on tuning through lookup-table layer rather than LSTM structure. With the decrease of overlap of polarities, sentiment of word turns more d</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’04, pages 168–177, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xia Hu</author>
<author>Jiliang Tang</author>
<author>Huiji Gao</author>
<author>Huan Liu</author>
</authors>
<title>Unsupervised sentiment analysis with emotional signals.</title>
<date>2013</date>
<booktitle>In Proceedings of the 22nd international conference on World Wide Web,</booktitle>
<pages>607--618</pages>
<institution>International World Wide Web Conferences Steering Committee.</institution>
<contexts>
<context position="1603" citStr="Hu et al., 2013" startWordPosition="224" endWordPosition="227">al model based on bag-of-word features. Furthermore, words with special functions (such as negation and transition) are distinguished and the dissimilarities of words with opposite sentiment are magnified. An interesting case study on negation expression processing shows a promising potential of the architecture dealing with complex sentiment phrases. 1 Introduction Twitter and other similar microblogs are rich resources for opinions on various kinds of products and events. Detecting sentiment in microblogs is a challenging task that has attracted increasing research interest in recent years (Hu et al., 2013b; Volkova et al., 2013). Go et al. (2009) carried out the pioneer work of predicting sentiment in tweets using machine learning technology. They conducted comprehensive experiments on multiple classifiers based on bag-of-words feature. Such feature is widely used because it’s simple and surprisingly efficient in many tasks. However, there are also disadvantages of bag-of-words features represented by onehot vectors. Firstly, it bears a data sparsity issue (Saif et al., 2012a). In tweets, irregularities and 140-character limitation exacerbate the sparseness. Secondly, losing sequence informati</context>
<context position="6089" citStr="Hu et al., 2013" startWordPosition="889" endWordPosition="892">ising potential of our model simulating complex linguistic phenomena with gates and constant error carousels in the LSTM blocks. 2 Related Work 2.1 Microblogs Sentiment Analysis There have been a large amount of works on sentiment analysis over tweets. Some research makes use of social network information (Tan et al., 2011; Calais Guerra et al., 2011). These works reveal that social network relations of opinion holders could bring an influential bias to the textual models. While some other works utilize the microblogging features uncommon in the formal literature, such as hashtags, emoticons (Hu et al., 2013a; Liu et al., 2012). Speriosu et al. (2011) propose a unified graph propagation model to leverage textual features (such as emoticons) as well as social information. Semantic concept or entity based approaches lead another research direction. Saif et al. (2012a; 2012b) make use of sentiment-topic features and entities extracted by a third-party service to ease data sparsity. Aspect-based models are also exploited to improve the tweet-level classifier (Lek and Poo, 2013). 2.2 Representation Learning and Deep Models Bengio et al. (2003) use distributed representations for words to fight the cur</context>
</contexts>
<marker>Hu, Tang, Gao, Liu, 2013</marker>
<rawString>Xia Hu, Jiliang Tang, Huiji Gao, and Huan Liu. 2013a. Unsupervised sentiment analysis with emotional signals. In Proceedings of the 22nd international conference on World Wide Web, pages 607– 618. International World Wide Web Conferences Steering Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xia Hu</author>
<author>Lei Tang</author>
<author>Jiliang Tang</author>
<author>Huan Liu</author>
</authors>
<title>Exploiting social relations for sentiment analysis in microblogging.</title>
<date>2013</date>
<booktitle>In Proceedings of the sixth ACM international conference on Web search and data mining,</booktitle>
<pages>537--546</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1603" citStr="Hu et al., 2013" startWordPosition="224" endWordPosition="227">al model based on bag-of-word features. Furthermore, words with special functions (such as negation and transition) are distinguished and the dissimilarities of words with opposite sentiment are magnified. An interesting case study on negation expression processing shows a promising potential of the architecture dealing with complex sentiment phrases. 1 Introduction Twitter and other similar microblogs are rich resources for opinions on various kinds of products and events. Detecting sentiment in microblogs is a challenging task that has attracted increasing research interest in recent years (Hu et al., 2013b; Volkova et al., 2013). Go et al. (2009) carried out the pioneer work of predicting sentiment in tweets using machine learning technology. They conducted comprehensive experiments on multiple classifiers based on bag-of-words feature. Such feature is widely used because it’s simple and surprisingly efficient in many tasks. However, there are also disadvantages of bag-of-words features represented by onehot vectors. Firstly, it bears a data sparsity issue (Saif et al., 2012a). In tweets, irregularities and 140-character limitation exacerbate the sparseness. Secondly, losing sequence informati</context>
<context position="6089" citStr="Hu et al., 2013" startWordPosition="889" endWordPosition="892">ising potential of our model simulating complex linguistic phenomena with gates and constant error carousels in the LSTM blocks. 2 Related Work 2.1 Microblogs Sentiment Analysis There have been a large amount of works on sentiment analysis over tweets. Some research makes use of social network information (Tan et al., 2011; Calais Guerra et al., 2011). These works reveal that social network relations of opinion holders could bring an influential bias to the textual models. While some other works utilize the microblogging features uncommon in the formal literature, such as hashtags, emoticons (Hu et al., 2013a; Liu et al., 2012). Speriosu et al. (2011) propose a unified graph propagation model to leverage textual features (such as emoticons) as well as social information. Semantic concept or entity based approaches lead another research direction. Saif et al. (2012a; 2012b) make use of sentiment-topic features and entities extracted by a third-party service to ease data sparsity. Aspect-based models are also exploited to improve the tweet-level classifier (Lek and Poo, 2013). 2.2 Representation Learning and Deep Models Bengio et al. (2003) use distributed representations for words to fight the cur</context>
</contexts>
<marker>Hu, Tang, Tang, Liu, 2013</marker>
<rawString>Xia Hu, Lei Tang, Jiliang Tang, and Huan Liu. 2013b. Exploiting social relations for sentiment analysis in microblogging. In Proceedings of the sixth ACM international conference on Web search and data mining, pages 537–546. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric H Huang</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Improving word representations via global context and multiple word prototypes.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>873--882</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6951" citStr="Huang et al., 2012" startWordPosition="1021" endWordPosition="1024"> Saif et al. (2012a; 2012b) make use of sentiment-topic features and entities extracted by a third-party service to ease data sparsity. Aspect-based models are also exploited to improve the tweet-level classifier (Lek and Poo, 2013). 2.2 Representation Learning and Deep Models Bengio et al. (2003) use distributed representations for words to fight the curse of dimensionality when training a neural probabilistic language model. Such word vectors ease the syntactic and semantic sparsity of bag-of-words representations. Much recent research has explored such representations (Turian et al., 2010; Huang et al., 2012). Recent works reveal that modifying word vectors during training could capture polarity information for the sentiment words effectively (Socher et al., 2011; Tang et al., 2014). It would be also insightful to analyze the embeddings that changed the most during training. We conduct a comparison between initial and tuned vectors and show how the tuned vectors of task-distinctive function words cooperate with the proposed architecture to capture sequence information. Distributed word vectors help in various NLP tasks when using in neural models (Collobert et al., 2011; Kalchbrenner et al., 2014)</context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. 2012. Improving word representations via global context and multiple word prototypes. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 873–882. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ozan Irsoy</author>
<author>Claire Cardie</author>
</authors>
<title>Opinion mining with deep recurrent neural networks.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>720--728</pages>
<contexts>
<context position="8916" citStr="Irsoy and Cardie (2014)" startWordPosition="1327" endWordPosition="1330">iment Analysis Recurrent Neural Networks (RNN) have gained attention in NLP field since Mikolov et al. (2010) developed a statistical language model based on a simple form known as Elman network (Elman, 1990). Recent works used RNNs to predict words or characters in a sequence (Chrupała, 2014; Zhang and Lapata, 2014). Treating opinion expression extraction as a sequence labelling 1344 output hidden input t-1 t t+1 Figure 1: Illustration of simple recurrent neural network. The input of the hidden layer comes from both input layer and the hidden layer activations of previous time step. problem, Irsoy and Cardie (2014) leverage deep RNN models and achieve new state-of-the-art results for fine-grained extraction task. The lastest work propose a tree-structured LSTM and conduct a comprehensive study on using LSTM in predicting the semantic relatedness of two sentences and sentiment classification (Tai et al., 2015). Fig.1 shows the illustration of a recurrent network. By using self-connected layers, RNNs allow information cyclically encoded inside the networks. Such structures make it possible to get a fix-length representation of a whole tweet by temporally composing word vectors. The recurrent architecture </context>
</contexts>
<marker>Irsoy, Cardie, 2014</marker>
<rawString>Ozan Irsoy and Claire Cardie. 2014. Opinion mining with deep recurrent neural networks. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 720–728.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Edward Grefenstette</author>
<author>Phil Blunsom</author>
</authors>
<title>A convolutional neural network for modelling sentences.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<contexts>
<context position="7551" citStr="Kalchbrenner et al., 2014" startWordPosition="1114" endWordPosition="1117"> 2010; Huang et al., 2012). Recent works reveal that modifying word vectors during training could capture polarity information for the sentiment words effectively (Socher et al., 2011; Tang et al., 2014). It would be also insightful to analyze the embeddings that changed the most during training. We conduct a comparison between initial and tuned vectors and show how the tuned vectors of task-distinctive function words cooperate with the proposed architecture to capture sequence information. Distributed word vectors help in various NLP tasks when using in neural models (Collobert et al., 2011; Kalchbrenner et al., 2014). Composing these representations to fix-length vectors that contain phrase or sentence level information also improves performance of sentiment analysis (Yessenalina and Cardie, 2011). Recursive neural networks model contextual interaction in binary trees (Socher et al., 2011; Socher et al., 2013). Words in the complex utterances are considered as leaf nodes and composed in a bottomup fashion. However, it’s difficult to get a binary tree structure from the irregular short comments like tweets. Not requiring structure information or parser, long short-term memory models encode the context in a</context>
<context position="17922" citStr="Kalchbrenner et al., 2014" startWordPosition="2805" endWordPosition="2808">format of the toolkit, we learned 25-dimensional (relatively small) vectors. Skip-gram architecture and hierarchical softmax algorithm are chosen during training. 4.3 Comparison with Data Driven Approaches Classifier Accuracy(%) SVM 81.6 MNB 82.7 MAXENT 83.0 MAX-TDNN 78.8 NBoW 80.9 DCNN 87.4 RAE 77.6 RNN-FLT 80.2 RNN-TLT 86.4 LSTM-TLT 87.2 Table 1: Accuracies of different classifiers. Naive Bayes, Maximum Entropy and SVM are widely used classifiers. Go et al. (2009) presented the results of three non-neural models using unigram and bigram features. Dynamic Convolutional Neural Network (DCNN) (Kalchbrenner et al., 2014) is a generalization of MAX-TDNN (Collobert et al., 2011). It has a clear hierarchy and is able to capture long-range semantic relations. While the Neural Bag-of-Words (NBoW) takes the summation of word vectors as the input of a classification layer. Kalchbrenner et al. (2014) reported performances of the above three neural classifiers. Recursive Autoencoder (RAE) has proven to be an effective model to compose words vectors in sentiment classification tasks (Socher et al., 2011). We run RAE with randomly initialized word embeddings. We do not compare with RNTN (Socher et al., 2013) for lack of</context>
</contexts>
<marker>Kalchbrenner, Grefenstette, Blunsom, 2014</marker>
<rawString>Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. 2014. A convolutional neural network for modelling sentences. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Labutov</author>
<author>Hod Lipson</author>
</authors>
<title>Re-embedding words.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>489--493</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="12422" citStr="Labutov and Lipson, 2013" startWordPosition="1905" endWordPosition="1908">omputed as: δt+1 h′ whh′ (6) 3.2 Elman Network with Trainable Lookup-Table Unsupervised trained word embeddings represent the syntactic and semantic similarity. However, in specific tasks, the importance and functions of different words vary. Negation words have similar unsupervised trained representations with other adverbs, but they make distinctive contributions in sentiment expressions. Besides the function words, tuning word vectors of sentiment words into polarity-representable ones turns out to be an effective way to improve the performance of sentiment classifiers. (Maas et al., 2011; Labutov and Lipson, 2013). Such tuned vectors work together with the deep models, gaining the ability to describe complex linguistic phenomena. RNN-TLT: To this end, we modify the word vectors in the Trainable Lookup-Table (TLT) via back propagation to get a better embedding of words. The gradient of lookup-table layer is: H H )δt i = g′ (at i E δthwih = E δthwih (7) h=1 h=1 where identity function g (x) = x is considered as the activation function of lookup-table layer. 3.3 Long Short-Term Memory The simple RNN has the ability to capture context information. However, the length of reachable context is often limited. </context>
</contexts>
<marker>Labutov, Lipson, 2013</marker>
<rawString>Igor Labutov and Hod Lipson. 2013. Re-embedding words. In Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics, pages 489–493. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hsiang Hui Lek</author>
<author>Danny CC Poo</author>
</authors>
<title>Aspectbased twitter sentiment classification.</title>
<date>2013</date>
<booktitle>In Tools with Artificial Intelligence (ICTAI), 2013 IEEE 25th International Conference on,</booktitle>
<pages>366--373</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="6564" citStr="Lek and Poo, 2013" startWordPosition="963" endWordPosition="966">dels. While some other works utilize the microblogging features uncommon in the formal literature, such as hashtags, emoticons (Hu et al., 2013a; Liu et al., 2012). Speriosu et al. (2011) propose a unified graph propagation model to leverage textual features (such as emoticons) as well as social information. Semantic concept or entity based approaches lead another research direction. Saif et al. (2012a; 2012b) make use of sentiment-topic features and entities extracted by a third-party service to ease data sparsity. Aspect-based models are also exploited to improve the tweet-level classifier (Lek and Poo, 2013). 2.2 Representation Learning and Deep Models Bengio et al. (2003) use distributed representations for words to fight the curse of dimensionality when training a neural probabilistic language model. Such word vectors ease the syntactic and semantic sparsity of bag-of-words representations. Much recent research has explored such representations (Turian et al., 2010; Huang et al., 2012). Recent works reveal that modifying word vectors during training could capture polarity information for the sentiment words effectively (Socher et al., 2011; Tang et al., 2014). It would be also insightful to ana</context>
<context position="21591" citStr="Lek and Poo (2013)" startWordPosition="3357" endWordPosition="3360"> Speriosu et emoticon 84.7 al. (2011) hashtag Saif et sentiment-topic 86.3 al. (2012a) semantic 84.1 Lek and aspect-based 88.3 Poo (2013) This work 87.2 Table 2: Comparison with different feature engineering methods. Table 2 shows the comparison with different feature engineering methods. In Speriosu et al. (2011)’s work, sentiment labels propagated in a graph constructed on the basis of contextual relations (e.g. word presence in a tweet) as well as social relations. Saif et al. (2012a) eased the data sparsity by adding sentiment-topic features that extracted using traditional lexicon. While Lek and Poo (2013) extracted tuple of [aspect, word, sentiment] with hand-crafted templates. With the help of opinion lexicon and POS tagger especially designed for twitter data, their approach achieved a state-of-the-art result. Even though these methods rely on lexicons and extracted entities, our data-driven model outperforms most of them, except the aspect-based one that introduced twitter-specific resources. This is due to the fact that traditional lexicons, even emoticons added, are not able to cover the diversification of twitter sentiment expressions, while LSTM learns appropriate representations of sen</context>
</contexts>
<marker>Lek, Poo, 2013</marker>
<rawString>Hsiang Hui Lek and Danny CC Poo. 2013. Aspectbased twitter sentiment classification. In Tools with Artificial Intelligence (ICTAI), 2013 IEEE 25th International Conference on, pages 366–373. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kun-Lin Liu</author>
<author>Wu-Jun Li</author>
<author>Minyi Guo</author>
</authors>
<title>Emoticon smoothed language models for twitter sentiment analysis.</title>
<date>2012</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="6109" citStr="Liu et al., 2012" startWordPosition="893" endWordPosition="896"> our model simulating complex linguistic phenomena with gates and constant error carousels in the LSTM blocks. 2 Related Work 2.1 Microblogs Sentiment Analysis There have been a large amount of works on sentiment analysis over tweets. Some research makes use of social network information (Tan et al., 2011; Calais Guerra et al., 2011). These works reveal that social network relations of opinion holders could bring an influential bias to the textual models. While some other works utilize the microblogging features uncommon in the formal literature, such as hashtags, emoticons (Hu et al., 2013a; Liu et al., 2012). Speriosu et al. (2011) propose a unified graph propagation model to leverage textual features (such as emoticons) as well as social information. Semantic concept or entity based approaches lead another research direction. Saif et al. (2012a; 2012b) make use of sentiment-topic features and entities extracted by a third-party service to ease data sparsity. Aspect-based models are also exploited to improve the tweet-level classifier (Lek and Poo, 2013). 2.2 Representation Learning and Deep Models Bengio et al. (2003) use distributed representations for words to fight the curse of dimensionality</context>
</contexts>
<marker>Liu, Li, Guo, 2012</marker>
<rawString>Kun-Lin Liu, Wu-Jun Li, and Minyi Guo. 2012. Emoticon smoothed language models for twitter sentiment analysis. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew L Maas</author>
<author>Raymond E Daly</author>
<author>Peter T Pham</author>
<author>Dan Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Learning word vectors for sentiment analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume</booktitle>
<volume>1</volume>
<pages>142--150</pages>
<contexts>
<context position="12395" citStr="Maas et al., 2011" startWordPosition="1901" endWordPosition="1904">an be recursively computed as: δt+1 h′ whh′ (6) 3.2 Elman Network with Trainable Lookup-Table Unsupervised trained word embeddings represent the syntactic and semantic similarity. However, in specific tasks, the importance and functions of different words vary. Negation words have similar unsupervised trained representations with other adverbs, but they make distinctive contributions in sentiment expressions. Besides the function words, tuning word vectors of sentiment words into polarity-representable ones turns out to be an effective way to improve the performance of sentiment classifiers. (Maas et al., 2011; Labutov and Lipson, 2013). Such tuned vectors work together with the deep models, gaining the ability to describe complex linguistic phenomena. RNN-TLT: To this end, we modify the word vectors in the Trainable Lookup-Table (TLT) via back propagation to get a better embedding of words. The gradient of lookup-table layer is: H H )δt i = g′ (at i E δthwih = E δthwih (7) h=1 h=1 where identity function g (x) = x is considered as the activation function of lookup-table layer. 3.3 Long Short-Term Memory The simple RNN has the ability to capture context information. However, the length of reachable</context>
</contexts>
<marker>Maas, Daly, Pham, Huang, Ng, Potts, 2011</marker>
<rawString>Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 142–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
<author>Jan Cernock`y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In INTERSPEECH,</booktitle>
<pages>1045--1048</pages>
<marker>Mikolov, Karafi´at, Burget, Cernock`y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In INTERSPEECH, pages 1045–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Zornitsa Kozareva</author>
<author>Alan Ritter</author>
<author>Sara Rosenthal</author>
<author>Veselin Stoyanov</author>
<author>Theresa Wilson</author>
</authors>
<title>Semeval-2013 task 2: Sentiment analysis in twitter.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation, SemEval,</booktitle>
<volume>13</volume>
<contexts>
<context position="22409" citStr="Nakov et al., 2013" startWordPosition="3475" endWordPosition="3478">-art result. Even though these methods rely on lexicons and extracted entities, our data-driven model outperforms most of them, except the aspect-based one that introduced twitter-specific resources. This is due to the fact that traditional lexicons, even emoticons added, are not able to cover the diversification of twitter sentiment expressions, while LSTM learns appropriate representations of sentiment information through compositional manner. 4.5 Experiments on Manually Labelled Data Different from STS dataset deciding the polarity based on emoticons, the benchmark dataset in SemEval 2013 (Nakov et al., 2013) is labelled by human annotators. In this work we focus on the binary polarity classification and abandon the neutral tweets. There are 4099/735/1742 available tweets in the training/dev/test set respectively. Since the training set is relatively small, we don’t apply fine tuning on word vectors. Namely we use fixed lookup-table for both RNN and LSTM. 300-dimensional vectors are learned on the 1.56M tweets of STS dataset using word2vec. Other settings stay the same as previous experiments. 1348 Method Accuracy(%) SVM 74.5 RAE 75.4 RNN-FLT 83.0 LSTM-FLT 84.0 Table 3: Accuracies of different met</context>
</contexts>
<marker>Nakov, Kozareva, Ritter, Rosenthal, Stoyanov, Wilson, 2013</marker>
<rawString>Preslav Nakov, Zornitsa Kozareva, Alan Ritter, Sara Rosenthal, Veselin Stoyanov, and Theresa Wilson. 2013. Semeval-2013 task 2: Sentiment analysis in twitter. In Proceedings of the International Workshop on Semantic Evaluation, SemEval, volume 13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10,</booktitle>
<pages>79--86</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2280" citStr="Pang et al., 2002" startWordPosition="326" endWordPosition="329">pioneer work of predicting sentiment in tweets using machine learning technology. They conducted comprehensive experiments on multiple classifiers based on bag-of-words feature. Such feature is widely used because it’s simple and surprisingly efficient in many tasks. However, there are also disadvantages of bag-of-words features represented by onehot vectors. Firstly, it bears a data sparsity issue (Saif et al., 2012a). In tweets, irregularities and 140-character limitation exacerbate the sparseness. Secondly, losing sequence information makes it difficult to figure out the polarity properly (Pang et al., 2002). A typical case is that the sentiment word in a negation phrase tends to express opposite sentiment to that of the context. Distributed representations of words can ease the sparseness, but there are limitations to the unsupervised-learned ones. Words with special functions in specific tasks are not distinguished. Such as negation words, which play a special role in polarity classification, are represented similarly with other adverbs. Such similarities will limit the compositional models’ abilities of describing a sentiment-specific interaction between words. Moreover, word vectors trained b</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, pages 79–86. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Pascanu</author>
<author>Tomas Mikolov</author>
<author>Yoshua Bengio</author>
</authors>
<title>On the difficulty of training recurrent neural networks.</title>
<date>2013</date>
<booktitle>In Proceedings of The 30th International Conference on Machine Learning,</booktitle>
<pages>1310--1318</pages>
<contexts>
<context position="3884" citStr="Pascanu et al., 2013" startWordPosition="566" endWordPosition="569">mplex sentiment expressions. One possible method to leverage context is connecting embeddings of words in a window and compose them to a fixlength vector (Collobert et al., 2011). However, window-based methods may have difficulty reaching long-distance words and simply connected vectors do not always represent the interactions of context properly. Theoretically, a recurrent neural network could process the whole sentence of arbitrary length by encoding the context cyclically. However, the length of reachable context is often limited when using stochastic gradient descent (Bengio et al., 1994; Pascanu et al., 2013). Besides that, a 1343 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1343–1353, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics traditional recurrent architecture is not powerful enough to deal with the complex sentiment expressions. Fixed input limits the network’s ability of learning task-specific representations and simple additive combination of hidden activations and input activations has difficulty capturing more complex linguistic </context>
<context position="13133" citStr="Pascanu et al., 2013" startWordPosition="2031" endWordPosition="2034">omplex linguistic phenomena. RNN-TLT: To this end, we modify the word vectors in the Trainable Lookup-Table (TLT) via back propagation to get a better embedding of words. The gradient of lookup-table layer is: H H )δt i = g′ (at i E δthwih = E δthwih (7) h=1 h=1 where identity function g (x) = x is considered as the activation function of lookup-table layer. 3.3 Long Short-Term Memory The simple RNN has the ability to capture context information. However, the length of reachable context is often limited. The gradient tends to vanish or blow up during the back propagation (Bengio et al., 1994; Pascanu et al., 2013). Moreover, Elman network simply combines previous hidden activations with the current inputs through addictive function. Such combination is not powerful enough to describe a complex interactions of words. An effective solution for these problems is the Long Short-Term Memory (LSTM) architecture (Hochreiter and Schmidhuber, 1997; Gers, Figure 3: Illustration of LSTM memory block with one cell. Constant Error Carousel (CEC) maintains the internal activation (called state) with a recurrent connection of fixed weight 1.0, which may be reset by the forget gate. The input and output gates scale th</context>
</contexts>
<marker>Pascanu, Mikolov, Bengio, 2013</marker>
<rawString>Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. 2013. On the difficulty of training recurrent neural networks. In Proceedings of The 30th International Conference on Machine Learning, pages 1310–1318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hassan Saif</author>
<author>Yulan He</author>
<author>Harith Alani</author>
</authors>
<title>Alleviating data sparsity for twitter sentiment analysis.</title>
<date>2012</date>
<booktitle>Making Sense of Microposts (# MSM2012).</booktitle>
<contexts>
<context position="2082" citStr="Saif et al., 2012" startWordPosition="298" endWordPosition="301">. Detecting sentiment in microblogs is a challenging task that has attracted increasing research interest in recent years (Hu et al., 2013b; Volkova et al., 2013). Go et al. (2009) carried out the pioneer work of predicting sentiment in tweets using machine learning technology. They conducted comprehensive experiments on multiple classifiers based on bag-of-words feature. Such feature is widely used because it’s simple and surprisingly efficient in many tasks. However, there are also disadvantages of bag-of-words features represented by onehot vectors. Firstly, it bears a data sparsity issue (Saif et al., 2012a). In tweets, irregularities and 140-character limitation exacerbate the sparseness. Secondly, losing sequence information makes it difficult to figure out the polarity properly (Pang et al., 2002). A typical case is that the sentiment word in a negation phrase tends to express opposite sentiment to that of the context. Distributed representations of words can ease the sparseness, but there are limitations to the unsupervised-learned ones. Words with special functions in specific tasks are not distinguished. Such as negation words, which play a special role in polarity classification, are rep</context>
<context position="6350" citStr="Saif et al. (2012" startWordPosition="931" endWordPosition="934">esearch makes use of social network information (Tan et al., 2011; Calais Guerra et al., 2011). These works reveal that social network relations of opinion holders could bring an influential bias to the textual models. While some other works utilize the microblogging features uncommon in the formal literature, such as hashtags, emoticons (Hu et al., 2013a; Liu et al., 2012). Speriosu et al. (2011) propose a unified graph propagation model to leverage textual features (such as emoticons) as well as social information. Semantic concept or entity based approaches lead another research direction. Saif et al. (2012a; 2012b) make use of sentiment-topic features and entities extracted by a third-party service to ease data sparsity. Aspect-based models are also exploited to improve the tweet-level classifier (Lek and Poo, 2013). 2.2 Representation Learning and Deep Models Bengio et al. (2003) use distributed representations for words to fight the curse of dimensionality when training a neural probabilistic language model. Such word vectors ease the syntactic and semantic sparsity of bag-of-words representations. Much recent research has explored such representations (Turian et al., 2010; Huang et al., 2012</context>
<context position="21463" citStr="Saif et al. (2012" startWordPosition="3339" endWordPosition="3342"> technology is employed in the experiments. 4.4 Comparison with Feature Engineering Approaches Method Craft feature Accuracy(%) Speriosu et emoticon 84.7 al. (2011) hashtag Saif et sentiment-topic 86.3 al. (2012a) semantic 84.1 Lek and aspect-based 88.3 Poo (2013) This work 87.2 Table 2: Comparison with different feature engineering methods. Table 2 shows the comparison with different feature engineering methods. In Speriosu et al. (2011)’s work, sentiment labels propagated in a graph constructed on the basis of contextual relations (e.g. word presence in a tweet) as well as social relations. Saif et al. (2012a) eased the data sparsity by adding sentiment-topic features that extracted using traditional lexicon. While Lek and Poo (2013) extracted tuple of [aspect, word, sentiment] with hand-crafted templates. With the help of opinion lexicon and POS tagger especially designed for twitter data, their approach achieved a state-of-the-art result. Even though these methods rely on lexicons and extracted entities, our data-driven model outperforms most of them, except the aspect-based one that introduced twitter-specific resources. This is due to the fact that traditional lexicons, even emoticons added, </context>
</contexts>
<marker>Saif, He, Alani, 2012</marker>
<rawString>Hassan Saif, Yulan He, and Harith Alani. 2012a. Alleviating data sparsity for twitter sentiment analysis. Making Sense of Microposts (# MSM2012).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Hassan Saif</author>
</authors>
<title>Yulan He, and Harith Alani. 2012b. Semantic sentiment analysis of twitter.</title>
<booktitle>In The Semantic Web–ISWC 2012,</booktitle>
<pages>508--524</pages>
<publisher>Springer.</publisher>
<marker>Saif, </marker>
<rawString>Hassan Saif, Yulan He, and Harith Alani. 2012b. Semantic sentiment analysis of twitter. In The Semantic Web–ISWC 2012, pages 508–524. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>151--161</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3081" citStr="Socher et al., 2011" startWordPosition="446" endWordPosition="449">ness, but there are limitations to the unsupervised-learned ones. Words with special functions in specific tasks are not distinguished. Such as negation words, which play a special role in polarity classification, are represented similarly with other adverbs. Such similarities will limit the compositional models’ abilities of describing a sentiment-specific interaction between words. Moreover, word vectors trained by cooccurrence statistics in a small window of context effectively represent the syntactic and semantic similarity. Thus, words like good and bad have very similar representations (Socher et al., 2011). It’s problematic for sentiment classifiers. Sentiment is expressed by phrases rather than by words (Socher et al., 2013). Seizing such sequence information would help to analyze complex sentiment expressions. One possible method to leverage context is connecting embeddings of words in a window and compose them to a fixlength vector (Collobert et al., 2011). However, window-based methods may have difficulty reaching long-distance words and simply connected vectors do not always represent the interactions of context properly. Theoretically, a recurrent neural network could process the whole se</context>
<context position="7108" citStr="Socher et al., 2011" startWordPosition="1045" endWordPosition="1048">re also exploited to improve the tweet-level classifier (Lek and Poo, 2013). 2.2 Representation Learning and Deep Models Bengio et al. (2003) use distributed representations for words to fight the curse of dimensionality when training a neural probabilistic language model. Such word vectors ease the syntactic and semantic sparsity of bag-of-words representations. Much recent research has explored such representations (Turian et al., 2010; Huang et al., 2012). Recent works reveal that modifying word vectors during training could capture polarity information for the sentiment words effectively (Socher et al., 2011; Tang et al., 2014). It would be also insightful to analyze the embeddings that changed the most during training. We conduct a comparison between initial and tuned vectors and show how the tuned vectors of task-distinctive function words cooperate with the proposed architecture to capture sequence information. Distributed word vectors help in various NLP tasks when using in neural models (Collobert et al., 2011; Kalchbrenner et al., 2014). Composing these representations to fix-length vectors that contain phrase or sentence level information also improves performance of sentiment analysis (Ye</context>
<context position="18405" citStr="Socher et al., 2011" startWordPosition="2880" endWordPosition="2883">lts of three non-neural models using unigram and bigram features. Dynamic Convolutional Neural Network (DCNN) (Kalchbrenner et al., 2014) is a generalization of MAX-TDNN (Collobert et al., 2011). It has a clear hierarchy and is able to capture long-range semantic relations. While the Neural Bag-of-Words (NBoW) takes the summation of word vectors as the input of a classification layer. Kalchbrenner et al. (2014) reported performances of the above three neural classifiers. Recursive Autoencoder (RAE) has proven to be an effective model to compose words vectors in sentiment classification tasks (Socher et al., 2011). We run RAE with randomly initialized word embeddings. We do not compare with RNTN (Socher et al., 2013) for lack of phrase-level sentiment labels and accurate parsing results. Table 1 shows the accuracies of different classifiers. Notably, RNN-TLT and LSTM-TLT out1347 perform the three non-neural classifiers. Trained on the considerable data, these classifiers provide strong baselines. However, bag-of-words representations are not powerful enough. Sparsity and losing sequence information hurt the performance of classifiers. Neural models overcome these problems by using distributed represent</context>
<context position="23420" citStr="Socher et al., 2011" startWordPosition="3633" endWordPosition="3636"> the 1.56M tweets of STS dataset using word2vec. Other settings stay the same as previous experiments. 1348 Method Accuracy(%) SVM 74.5 RAE 75.4 RNN-FLT 83.0 LSTM-FLT 84.0 Table 3: Accuracies of different methods on SemEval 2013 Table 3 shows our work compared to SVM and Recursive Autoencoder. From the result, we can see that the recurrent models outperforms the baselines by exploiting more context information of word interactions. 4.6 Representation Learning Recent works reveal that modifying word vectors during training could capture polarity information for the sentiment words effectively (Socher et al., 2011; Tang et al., 2014). However, it would be also helpful to analyse the embeddings that changed the most. Function words: We choose 1000 most frequent words. For each word, we compute the distance between unsupervised vector and tuned vector. 20 words that change most are shown in Fig.4. It’s noteworthy that there are five negation words (not, no, n’t, never and Not) in the notablychange group. The representations of negation words are quite similar with other adverbs in unsupervised learned embeddings, while the proposed model distinguishes them. This indicate that our polarity-supervised mode</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning. 2011. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 151–161. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1631--1642</pages>
<contexts>
<context position="3203" citStr="Socher et al., 2013" startWordPosition="464" endWordPosition="467">istinguished. Such as negation words, which play a special role in polarity classification, are represented similarly with other adverbs. Such similarities will limit the compositional models’ abilities of describing a sentiment-specific interaction between words. Moreover, word vectors trained by cooccurrence statistics in a small window of context effectively represent the syntactic and semantic similarity. Thus, words like good and bad have very similar representations (Socher et al., 2011). It’s problematic for sentiment classifiers. Sentiment is expressed by phrases rather than by words (Socher et al., 2013). Seizing such sequence information would help to analyze complex sentiment expressions. One possible method to leverage context is connecting embeddings of words in a window and compose them to a fixlength vector (Collobert et al., 2011). However, window-based methods may have difficulty reaching long-distance words and simply connected vectors do not always represent the interactions of context properly. Theoretically, a recurrent neural network could process the whole sentence of arbitrary length by encoding the context cyclically. However, the length of reachable context is often limited w</context>
<context position="7850" citStr="Socher et al., 2013" startWordPosition="1156" endWordPosition="1159">uct a comparison between initial and tuned vectors and show how the tuned vectors of task-distinctive function words cooperate with the proposed architecture to capture sequence information. Distributed word vectors help in various NLP tasks when using in neural models (Collobert et al., 2011; Kalchbrenner et al., 2014). Composing these representations to fix-length vectors that contain phrase or sentence level information also improves performance of sentiment analysis (Yessenalina and Cardie, 2011). Recursive neural networks model contextual interaction in binary trees (Socher et al., 2011; Socher et al., 2013). Words in the complex utterances are considered as leaf nodes and composed in a bottomup fashion. However, it’s difficult to get a binary tree structure from the irregular short comments like tweets. Not requiring structure information or parser, long short-term memory models encode the context in a chain and accommodate complex linguistic phenomena with structure of gates and constant error carousels. 3 Recurrent Neural Networks for Sentiment Analysis Recurrent Neural Networks (RNN) have gained attention in NLP field since Mikolov et al. (2010) developed a statistical language model based on</context>
<context position="18510" citStr="Socher et al., 2013" startWordPosition="2899" endWordPosition="2902">CNN) (Kalchbrenner et al., 2014) is a generalization of MAX-TDNN (Collobert et al., 2011). It has a clear hierarchy and is able to capture long-range semantic relations. While the Neural Bag-of-Words (NBoW) takes the summation of word vectors as the input of a classification layer. Kalchbrenner et al. (2014) reported performances of the above three neural classifiers. Recursive Autoencoder (RAE) has proven to be an effective model to compose words vectors in sentiment classification tasks (Socher et al., 2011). We run RAE with randomly initialized word embeddings. We do not compare with RNTN (Socher et al., 2013) for lack of phrase-level sentiment labels and accurate parsing results. Table 1 shows the accuracies of different classifiers. Notably, RNN-TLT and LSTM-TLT out1347 perform the three non-neural classifiers. Trained on the considerable data, these classifiers provide strong baselines. However, bag-of-words representations are not powerful enough. Sparsity and losing sequence information hurt the performance of classifiers. Neural models overcome these problems by using distributed representations and temporally encoding the contextual interaction. We notice a considerable increase in the perfo</context>
<context position="24412" citStr="Socher et al., 2013" startWordPosition="3788" endWordPosition="3791">the notablychange group. The representations of negation words are quite similar with other adverbs in unsupervised learned embeddings, while the proposed model distinguishes them. This indicate that our polarity-supervised models identify negation words as distinctive symbols in sentiment classification task, while unsupervised learned vectors do not contain such information. Besides the negation words and sentiment words, there are also other prepositions, pronouns and conjunctions change dramatically (e.g. and and but). Such function words also play a special role in sentiment expressions (Socher et al., 2013) and the model in this paper distinguishes them. However, the contributions of these words to the task are not that explainable as negation words (at least without sentiment strength information). To further explain how the tuned vectors work together with the network and describe interactions between words, we study the process of the model classifying negation phrases in the following subsection. Sentiment words: In order to study the em0 0.1 0.2 0.3 0.4 0.5 Figure 4: Word change scale to [0,1]. Distances are computed by reversing cosine similarity. bedding change of sentiment words, we choo</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1631–1642.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Speriosu</author>
<author>Nikita Sudan</author>
<author>Sid Upadhyay</author>
<author>Jason Baldridge</author>
</authors>
<title>Twitter polarity classification with label propagation over lexical links and the follower graph.</title>
<date>2011</date>
<booktitle>In Proceedings of the First workshop on Unsupervised Learning in NLP,</booktitle>
<pages>53--63</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6133" citStr="Speriosu et al. (2011)" startWordPosition="897" endWordPosition="900">ng complex linguistic phenomena with gates and constant error carousels in the LSTM blocks. 2 Related Work 2.1 Microblogs Sentiment Analysis There have been a large amount of works on sentiment analysis over tweets. Some research makes use of social network information (Tan et al., 2011; Calais Guerra et al., 2011). These works reveal that social network relations of opinion holders could bring an influential bias to the textual models. While some other works utilize the microblogging features uncommon in the formal literature, such as hashtags, emoticons (Hu et al., 2013a; Liu et al., 2012). Speriosu et al. (2011) propose a unified graph propagation model to leverage textual features (such as emoticons) as well as social information. Semantic concept or entity based approaches lead another research direction. Saif et al. (2012a; 2012b) make use of sentiment-topic features and entities extracted by a third-party service to ease data sparsity. Aspect-based models are also exploited to improve the tweet-level classifier (Lek and Poo, 2013). 2.2 Representation Learning and Deep Models Bengio et al. (2003) use distributed representations for words to fight the curse of dimensionality when training a neural </context>
<context position="21288" citStr="Speriosu et al. (2011)" startWordPosition="3309" endWordPosition="3312">ll be further discussed in subsection 4.7. Since the training set is large enough, we have not observed strong overfitting during the training process. Therefore, no regularization technology is employed in the experiments. 4.4 Comparison with Feature Engineering Approaches Method Craft feature Accuracy(%) Speriosu et emoticon 84.7 al. (2011) hashtag Saif et sentiment-topic 86.3 al. (2012a) semantic 84.1 Lek and aspect-based 88.3 Poo (2013) This work 87.2 Table 2: Comparison with different feature engineering methods. Table 2 shows the comparison with different feature engineering methods. In Speriosu et al. (2011)’s work, sentiment labels propagated in a graph constructed on the basis of contextual relations (e.g. word presence in a tweet) as well as social relations. Saif et al. (2012a) eased the data sparsity by adding sentiment-topic features that extracted using traditional lexicon. While Lek and Poo (2013) extracted tuple of [aspect, word, sentiment] with hand-crafted templates. With the help of opinion lexicon and POS tagger especially designed for twitter data, their approach achieved a state-of-the-art result. Even though these methods rely on lexicons and extracted entities, our data-driven mo</context>
</contexts>
<marker>Speriosu, Sudan, Upadhyay, Baldridge, 2011</marker>
<rawString>Michael Speriosu, Nikita Sudan, Sid Upadhyay, and Jason Baldridge. 2011. Twitter polarity classification with label propagation over lexical links and the follower graph. In Proceedings of the First workshop on Unsupervised Learning in NLP, pages 53– 63. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai Sheng Tai</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Improved semantic representations from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075.</title>
<date>2015</date>
<contexts>
<context position="9216" citStr="Tai et al., 2015" startWordPosition="1372" endWordPosition="1375">apata, 2014). Treating opinion expression extraction as a sequence labelling 1344 output hidden input t-1 t t+1 Figure 1: Illustration of simple recurrent neural network. The input of the hidden layer comes from both input layer and the hidden layer activations of previous time step. problem, Irsoy and Cardie (2014) leverage deep RNN models and achieve new state-of-the-art results for fine-grained extraction task. The lastest work propose a tree-structured LSTM and conduct a comprehensive study on using LSTM in predicting the semantic relatedness of two sentences and sentiment classification (Tai et al., 2015). Fig.1 shows the illustration of a recurrent network. By using self-connected layers, RNNs allow information cyclically encoded inside the networks. Such structures make it possible to get a fix-length representation of a whole tweet by temporally composing word vectors. The recurrent architecture we used in this work is shown in Fig.2. Each word is mapped to a vector through a Lookup-Table (LT) layer. The input of the hidden layer comes from both the current lookup-table layer activations and the hidden layer’s activations one step back in time. In this way, hidden layer encodes the past and</context>
</contexts>
<marker>Tai, Socher, Manning, 2015</marker>
<rawString>Kai Sheng Tai, Richard Socher, and Christopher D. Manning. 2015. Improved semantic representations from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chenhao Tan</author>
<author>Lillian Lee</author>
<author>Jie Tang</author>
<author>Long Jiang</author>
<author>Ming Zhou</author>
<author>Ping Li</author>
</authors>
<title>User-level sentiment analysis incorporating social networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>1397--1405</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5798" citStr="Tan et al., 2011" startWordPosition="840" endWordPosition="843">f opposite-polarity words are separated. Moreover, predicting result on negation test set shows our model is effective in dealing with negation phrases (a typical case of sentiment expressed by sequence). We study the process of the network handling the negation expressions and show the promising potential of our model simulating complex linguistic phenomena with gates and constant error carousels in the LSTM blocks. 2 Related Work 2.1 Microblogs Sentiment Analysis There have been a large amount of works on sentiment analysis over tweets. Some research makes use of social network information (Tan et al., 2011; Calais Guerra et al., 2011). These works reveal that social network relations of opinion holders could bring an influential bias to the textual models. While some other works utilize the microblogging features uncommon in the formal literature, such as hashtags, emoticons (Hu et al., 2013a; Liu et al., 2012). Speriosu et al. (2011) propose a unified graph propagation model to leverage textual features (such as emoticons) as well as social information. Semantic concept or entity based approaches lead another research direction. Saif et al. (2012a; 2012b) make use of sentiment-topic features a</context>
</contexts>
<marker>Tan, Lee, Tang, Jiang, Zhou, Li, 2011</marker>
<rawString>Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, Ming Zhou, and Ping Li. 2011. User-level sentiment analysis incorporating social networks. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1397–1405. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duyu Tang</author>
<author>Furu Wei</author>
<author>Nan Yang</author>
<author>Ming Zhou</author>
<author>Ting Liu</author>
<author>Bing Qin</author>
</authors>
<title>Learning sentimentspecific word embedding for twitter sentiment classification.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1555--1565</pages>
<contexts>
<context position="7128" citStr="Tang et al., 2014" startWordPosition="1049" endWordPosition="1052">improve the tweet-level classifier (Lek and Poo, 2013). 2.2 Representation Learning and Deep Models Bengio et al. (2003) use distributed representations for words to fight the curse of dimensionality when training a neural probabilistic language model. Such word vectors ease the syntactic and semantic sparsity of bag-of-words representations. Much recent research has explored such representations (Turian et al., 2010; Huang et al., 2012). Recent works reveal that modifying word vectors during training could capture polarity information for the sentiment words effectively (Socher et al., 2011; Tang et al., 2014). It would be also insightful to analyze the embeddings that changed the most during training. We conduct a comparison between initial and tuned vectors and show how the tuned vectors of task-distinctive function words cooperate with the proposed architecture to capture sequence information. Distributed word vectors help in various NLP tasks when using in neural models (Collobert et al., 2011; Kalchbrenner et al., 2014). Composing these representations to fix-length vectors that contain phrase or sentence level information also improves performance of sentiment analysis (Yessenalina and Cardie</context>
<context position="23440" citStr="Tang et al., 2014" startWordPosition="3637" endWordPosition="3640">STS dataset using word2vec. Other settings stay the same as previous experiments. 1348 Method Accuracy(%) SVM 74.5 RAE 75.4 RNN-FLT 83.0 LSTM-FLT 84.0 Table 3: Accuracies of different methods on SemEval 2013 Table 3 shows our work compared to SVM and Recursive Autoencoder. From the result, we can see that the recurrent models outperforms the baselines by exploiting more context information of word interactions. 4.6 Representation Learning Recent works reveal that modifying word vectors during training could capture polarity information for the sentiment words effectively (Socher et al., 2011; Tang et al., 2014). However, it would be also helpful to analyse the embeddings that changed the most. Function words: We choose 1000 most frequent words. For each word, we compute the distance between unsupervised vector and tuned vector. 20 words that change most are shown in Fig.4. It’s noteworthy that there are five negation words (not, no, n’t, never and Not) in the notablychange group. The representations of negation words are quite similar with other adverbs in unsupervised learned embeddings, while the proposed model distinguishes them. This indicate that our polarity-supervised models identify negation</context>
</contexts>
<marker>Tang, Wei, Yang, Zhou, Liu, Qin, 2014</marker>
<rawString>Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu, and Bing Qin. 2014. Learning sentimentspecific word embedding for twitter sentiment classification. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1555–1565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>384--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6930" citStr="Turian et al., 2010" startWordPosition="1017" endWordPosition="1020">r research direction. Saif et al. (2012a; 2012b) make use of sentiment-topic features and entities extracted by a third-party service to ease data sparsity. Aspect-based models are also exploited to improve the tweet-level classifier (Lek and Poo, 2013). 2.2 Representation Learning and Deep Models Bengio et al. (2003) use distributed representations for words to fight the curse of dimensionality when training a neural probabilistic language model. Such word vectors ease the syntactic and semantic sparsity of bag-of-words representations. Much recent research has explored such representations (Turian et al., 2010; Huang et al., 2012). Recent works reveal that modifying word vectors during training could capture polarity information for the sentiment words effectively (Socher et al., 2011; Tang et al., 2014). It would be also insightful to analyze the embeddings that changed the most during training. We conduct a comparison between initial and tuned vectors and show how the tuned vectors of task-distinctive function words cooperate with the proposed architecture to capture sequence information. Distributed word vectors help in various NLP tasks when using in neural models (Collobert et al., 2011; Kalch</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Svitlana Volkova</author>
<author>Theresa Wilson</author>
<author>David Yarowsky</author>
</authors>
<title>Exploring sentiment in social media: Bootstrapping subjectivity clues from multilingual twitter streams.</title>
<date>2013</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="1627" citStr="Volkova et al., 2013" startWordPosition="228" endWordPosition="231">bag-of-word features. Furthermore, words with special functions (such as negation and transition) are distinguished and the dissimilarities of words with opposite sentiment are magnified. An interesting case study on negation expression processing shows a promising potential of the architecture dealing with complex sentiment phrases. 1 Introduction Twitter and other similar microblogs are rich resources for opinions on various kinds of products and events. Detecting sentiment in microblogs is a challenging task that has attracted increasing research interest in recent years (Hu et al., 2013b; Volkova et al., 2013). Go et al. (2009) carried out the pioneer work of predicting sentiment in tweets using machine learning technology. They conducted comprehensive experiments on multiple classifiers based on bag-of-words feature. Such feature is widely used because it’s simple and surprisingly efficient in many tasks. However, there are also disadvantages of bag-of-words features represented by onehot vectors. Firstly, it bears a data sparsity issue (Saif et al., 2012a). In tweets, irregularities and 140-character limitation exacerbate the sparseness. Secondly, losing sequence information makes it difficult to</context>
</contexts>
<marker>Volkova, Wilson, Yarowsky, 2013</marker>
<rawString>Svitlana Volkova, Theresa Wilson, and David Yarowsky. 2013. Exploring sentiment in social media: Bootstrapping subjectivity clues from multilingual twitter streams. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul J Werbos</author>
</authors>
<title>Backpropagation through time: what it does and how to do it.</title>
<date>1990</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<pages>78--10</pages>
<contexts>
<context position="11216" citStr="Werbos, 1990" startWordPosition="1707" endWordPosition="1708">rrent architecture unfolded as a deep feedforward network. tion of position h at time t is: bth = f (ath) (1) wh′hbt−1 (2) h′ where et represents the E-length embedding of the tth word of the sentence, which stored in LT layer. wih is the weight of connection between input and hidden layer, while wh′h is the weights of recurrent connection (self-connection of hidden layer). f represents the sigmoid function. The binary classification loss function O is computed via cross entropy (CE) criterion and the network is trained by stochastic gradient descent using backpropagation through time (BPTT) (Werbos, 1990). Here, we introduce the notation: δti = ∂O (3) ∂at i Firstly, the error propagate from output layer to hidden layer of last time step T. The derivatives with respect to the hidden activation of position i at the last time step T are computed as follow: δTi = f′ (aT) ∂y vi (4) y where vi represents the weights of hidden-output connection and the activation of the output layer y is used to estimate probability of the tweet bearing EE i wiheti + t ah = H E h′ 1345 a particular polarity. IbT i vi (5) Then the gradients of hidden layer of previous time steps can be recursively computed as: δt+1 h′</context>
</contexts>
<marker>Werbos, 1990</marker>
<rawString>Paul J Werbos. 1990. Backpropagation through time: what it does and how to do it. Proceedings of the IEEE, 78(10):1550–1560.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ainur Yessenalina</author>
<author>Claire Cardie</author>
</authors>
<title>Compositional matrix-space models for sentiment analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>172--182</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7735" citStr="Yessenalina and Cardie, 2011" startWordPosition="1139" endWordPosition="1142">11; Tang et al., 2014). It would be also insightful to analyze the embeddings that changed the most during training. We conduct a comparison between initial and tuned vectors and show how the tuned vectors of task-distinctive function words cooperate with the proposed architecture to capture sequence information. Distributed word vectors help in various NLP tasks when using in neural models (Collobert et al., 2011; Kalchbrenner et al., 2014). Composing these representations to fix-length vectors that contain phrase or sentence level information also improves performance of sentiment analysis (Yessenalina and Cardie, 2011). Recursive neural networks model contextual interaction in binary trees (Socher et al., 2011; Socher et al., 2013). Words in the complex utterances are considered as leaf nodes and composed in a bottomup fashion. However, it’s difficult to get a binary tree structure from the irregular short comments like tweets. Not requiring structure information or parser, long short-term memory models encode the context in a chain and accommodate complex linguistic phenomena with structure of gates and constant error carousels. 3 Recurrent Neural Networks for Sentiment Analysis Recurrent Neural Networks (</context>
</contexts>
<marker>Yessenalina, Cardie, 2011</marker>
<rawString>Ainur Yessenalina and Claire Cardie. 2011. Compositional matrix-space models for sentiment analysis. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 172–182. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xingxing Zhang</author>
<author>Mirella Lapata</author>
</authors>
<title>Chinese poetry generation with recurrent neural networks.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>670--680</pages>
<contexts>
<context position="8611" citStr="Zhang and Lapata, 2014" startWordPosition="1277" endWordPosition="1280"> tree structure from the irregular short comments like tweets. Not requiring structure information or parser, long short-term memory models encode the context in a chain and accommodate complex linguistic phenomena with structure of gates and constant error carousels. 3 Recurrent Neural Networks for Sentiment Analysis Recurrent Neural Networks (RNN) have gained attention in NLP field since Mikolov et al. (2010) developed a statistical language model based on a simple form known as Elman network (Elman, 1990). Recent works used RNNs to predict words or characters in a sequence (Chrupała, 2014; Zhang and Lapata, 2014). Treating opinion expression extraction as a sequence labelling 1344 output hidden input t-1 t t+1 Figure 1: Illustration of simple recurrent neural network. The input of the hidden layer comes from both input layer and the hidden layer activations of previous time step. problem, Irsoy and Cardie (2014) leverage deep RNN models and achieve new state-of-the-art results for fine-grained extraction task. The lastest work propose a tree-structured LSTM and conduct a comprehensive study on using LSTM in predicting the semantic relatedness of two sentences and sentiment classification (Tai et al., </context>
</contexts>
<marker>Zhang, Lapata, 2014</marker>
<rawString>Xingxing Zhang and Mirella Lapata. 2014. Chinese poetry generation with recurrent neural networks. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 670–680.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>