<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.043667">
<title confidence="0.995949">
UNIBA: Combining Distributional Semantic Models and Word Sense
Disambiguation for Textual Similarity
</title>
<author confidence="0.970336">
Pierpaolo Basile and Annalina Caputo and Giovanni Semeraro
</author>
<affiliation confidence="0.997293">
Department of Computer Science
University of Bari Aldo Moro
</affiliation>
<address confidence="0.931933">
Via, E. Orabona, 4 - 70125 Bari (Italy)
</address>
<email confidence="0.998222">
{firstname.surname}@uniba.it
</email>
<sectionHeader confidence="0.997373" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999922642857143">
This paper describes the UNIBA team
participation in the Cross-Level Semantic
Similarity task at SemEval 2014. We pro-
pose to combine the output of different se-
mantic similarity measures which exploit
Word Sense Disambiguation and Distribu-
tional Semantic Models, among other lex-
ical features. The integration of similar-
ity measures is performed by means of
two supervised methods based on Gaus-
sian Process and Support Vector Machine.
Our systems obtained very encouraging
results, with the best one ranked 6th out
of 38 submitted systems.
</bodyText>
<sectionHeader confidence="0.99939" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.934652970588235">
Cross-Level Semantic Similarity (CLSS) is the
task of computing the similarity between two text
fragments of different sizes. The task focuses on
the comparison between texts at different lexical
levels, i.e. between a larger and a smaller text.
The task comprises four different levels: 1) para-
graph to sentence; 2) sentence to phrase; 3) phrase
to word; 4) word to sense. The task objective is
to provide a framework for evaluating general vs.
level-specialized methods.
Our general approach consists in combining
scores coming from different semantic similarity
algorithms. The combination is performed by a
supervised method using the training data pro-
vided by the task organizers. The data set com-
prises pairs of text fragments that can be rated with
a score between 0 and 4, where 4 indicates the
maximum level of similarity.
We select algorithms which provide similarities
at different levels of semantics: surface (or string-
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
based), lexical (word sense disambiguation), and
distributional level. The idea is to combine in a
unique system the semantic aspects that pertain
text fragments.
The following section gives more details about
the similarity measures and their combination in a
unique score through supervised methods (Section
2). Section 3 describes the system set up for the
evaluation and comments on the reported results,
while Section 4 concludes the paper.
</bodyText>
<sectionHeader confidence="0.993401" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.999985666666667">
The idea behind our system is to combine the
output of several similarity measures/features by
means of a supervised algorithm. Those features
were grouped in three main categories. The fol-
lowing three sub-sections describe in detail each
feature exploited by the system.
</bodyText>
<subsectionHeader confidence="0.984451">
2.1 Distributional Semantics Level
</subsectionHeader>
<bodyText confidence="0.999089">
Distributional Semantic Models (DSM) are an
easy way for building geometrical spaces of con-
cepts, also known as Semantic (or Word) Spaces,
by skimming through huge corpora of text in or-
der to learn the context of word usage. In the re-
sulting space, semantic relatedness/similarity be-
tween two words is expressed by the opposite of
the distance between points that represent those
words. Thus, the semantic similarity can be com-
puted as the cosine of the angle between the two
vectors that represent the words. This concept
of similarity can be extended to whole sentences
by combining words through vector addition (+),
which corresponds to the point-wise sum of the
vector components. Our DSM measure (DSM)
is based on a SemanticSpace, represented by a
co-occurrences matrix M, built by analysing the
distribution of words in the British National Cor-
pus (BNC). Then, M is reduced using the Latent
Semantic Analysis (LSA) (Landauer and Dumais,
1997). Vector addition and cosine similarity are
</bodyText>
<page confidence="0.951816">
748
</page>
<note confidence="0.7316465">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 748–753,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.998482666666667">
then used for building the vector representation of
each text fragment and computing their pairwise
similarity, respectively.
</bodyText>
<subsectionHeader confidence="0.999576">
2.2 Lexical Semantics Level
</subsectionHeader>
<bodyText confidence="0.991893">
Word Sense Disambiguation. Most of our
measures rely on the output of a Word Sense Dis-
ambiguation (WSD) algorithm. Our newest ap-
proach to WSD, recently presented in Basile et
al. (2014), is based on the simplified Lesk algo-
rithm (Vasilescu et al., 2004). Each word wi in
a sequence w1w2...wn is disambiguated individ-
ually by choosing the sense that maximizes the
similarity between the gloss and the context of wi
(i.e. the whole text where wi occurs). To boost
the overlap between the context and the gloss,
this last is expanded with glosses of related mean-
ings, following the approach described in Baner-
jee and Pedersen (2002). As sense inventory we
choose BabelNet 1.1, a huge multilingual seman-
tic network which comprises both WordNet and
Wikipedia (Navigli and Ponzetto, 2012). The al-
gorithm consists of the following steps:
1. Building the glosses. We retrieve all possible
word meanings for the target word wi that are
listed in BabelNet. BabelNet mixes senses
in WordNet and Wikipedia. First, senses
in WordNet are searched for; if no sense is
found (as often happens with named enti-
ties), senses for the target word are sought in
Wikipedia. We preferred that strategy rather
than retrieving senses from both sources at
once because this last approach produced
worse results when tuning the system. Once
the set of senses Si = {si1, si2, ..., sik} as-
sociated to the target word wi has been re-
trieved, gloss expansion occurs. For each
sense sij of wi, the algorithm builds the sense
extended gloss g�ij by appending the glosses
of meanings related to sij to its original gloss
gij. The related meanings, with the exception
of “antonym” senses, are the output of the
BabelNet function “getRelatedMap”. More-
over, each word in g�ij is weighted by a func-
tion inversely proportional to the distance be-
tween sij and its related meaning. The dis-
tance d is computed as the number of edges
linking two senses in the graph. The func-
tion takes also into account the frequencies
of the words in all the glosses giving more
emphasis to the most discriminative words;
this can be considered as a variation of the in-
verse document frequency (idf) for retrieval
that we named inverse gloss frequency (igf).
The igf for a word wk occurring gfk times in
the set of extended glosses for all the senses
in Si, the sense inventory of wi, is computed
as IGFk = 1 + log2sSi*1. The final weight
fk
for the word wk appearing h times in the ex-
tended gloss g�ij is given by:
</bodyText>
<equation confidence="0.987679">
1
weight(wk, g� ij) = h x IGFk x 1 + d (1)
</equation>
<listItem confidence="0.968506777777778">
2. Building the context. The context C for the
word wi is represented by all the words that
occur in the text.
3. Building the vector representations. The con-
text C and each extended gloss g�ij are repre-
sented as vectors in the SemanticSpace built
through the DSM described in Subsection
2.1.
4. Sense ranking. The algorithm computes the
</listItem>
<bodyText confidence="0.973752285714286">
cosine similarity between the vector repre-
sentation of each extended gloss g�ij and that
of the context C. Then, the cosine similar-
ity is linearly combined with the probability
p(sij|wi), which takes into account the sense
distribution of sij given the word wi. The
sense distribution is computed as the num-
ber of times the word wi was tagged with
the sense sij in SemCor, a collection of 352
documents manually annotated with Word-
Net synsets. T he additive (Laplace) smooth-
ing prevents zero probabilities, which can oc-
cur when some synsets do not appear in Sem-
Cor. The probability is computed as follows:
</bodyText>
<equation confidence="0.999265">
t(wi, sij) + 1
p(sij|wi) = (2)
#wi + |Si|
</equation>
<bodyText confidence="0.999929888888889">
The output of this step is a ranked list of
synsets.
The WSD measure (WSD) is computed on the top
of the output of the last step. For each text frag-
ment, we build a Bag-of-Synset (BoS) as the sum,
over the whole text, of the weighted synsets as-
sociated with each word. Then, we compute the
WSD similarity as the cosine similarity between
the two BoS.
</bodyText>
<page confidence="0.993497">
749
</page>
<bodyText confidence="0.999913173913043">
Graph. A sub-graph of BabelNet is built for
each text fragment starting from the synsets pro-
vided by the WSD algorithm. For each word the
synset with the highest score is selected, then this
initial set is expanded with the related synsets in
BabelNet. We apply the Personalized Page Rank
(Haveliwala, 2002) to each sub-graph where the
synset scores computed by the WSD algorithm are
exploited as prior probabilities. The weighted rank
of synsets provided by Page Rank is used to build
the BoS of the two text fragments, then the Person-
alized Page Rank (PPR) is computed as the cosine
similarity between them.
Synset Distributional Space. Generally, sim-
ilarity measures between synsets rely on the
synsets hierarchy in a semantic network (e.g.
WordNet). We define a new approach that is com-
pletely different, and represents synsets as points
in a geometric space that we call SDS (Synset Dis-
tributional Space). SDS is generated taking into
account the synset relationships, and similarity is
defined as the synsets closeness in the space. We
build a symmetric matrix S which contains synsets
on both rows and columns. Each cell in the matrix
is set to one if a semantic relation exists between
the corresponding synsets. The relationships are
extracted from BabelNet limiting synsets to those
occurring also in WordNet, while synsets coming
from Wikipedia are removed to reduce the size
of S. The method for building the matrix S re-
lies on Reflective Random Indexing (RRI) (Co-
hen et al., 2010), a variation of the Random In-
dexing technique for matrix reduction (Kanerva,
1988). RRI retains the advantages of RI which
incrementally builds a reduced space where dis-
tance between points is nearly preserved. More-
over, cyclical training, i.e. the retraining of a new
space exploiting the RI output as basis vectors,
makes indirect inference to emerge. Two differ-
ent similarity measures can be defined by exploit-
ing this space for representing synsets: WSD-SDS
and PPR-SDS, based on WSD and PPR respec-
tively. Each BoS is represented as the sum of the
synset vectors in the SDS space. Then, the simi-
larity is computed as the cosine similarity between
the two vector representations.
</bodyText>
<subsectionHeader confidence="0.998029">
2.3 Surface Level
</subsectionHeader>
<bodyText confidence="0.994628789473684">
At the surface level, we compute the following
features:
EDIT The edit, or Levenshtein, distance between
the two texts;
MCS The most common subsequence between
the two texts;
2-gram, 3-gram For each text fragment, we
build the Bag-of-n-gram (with n varying in
12,31); then we compute the cosine similar-
ity between the two Bag-of-n-gram repre-
sentations.
BOW For each tokenized text fragment, we build
its Bag-of-Word, and then compute the co-
sine similarity between the two BoW.
L1 The length in characters of the first text frag-
ment;
L2 The length in caracters of the second text frag-
ment;
DIFF The difference between L1 and L2.
</bodyText>
<subsectionHeader confidence="0.963107">
2.4 Word to Sense
</subsectionHeader>
<bodyText confidence="0.999997846153846">
The word to sense level is different from the other
ones: in this case the similarity is computed be-
tween a word and a particular word meaning.
Since a word meaning is not a text fragment, this
level poses a new challenge with respect to the
classical text similarity task. In this case we de-
cide to consider the word on its own as the first
text fragment, while for the second text fragment
we build a dummy text using the BabelNet gloss
assigned to the word sense. In that way, the distri-
butional and the lexical measures (WSD, Graph,
and DSM) can be applied to both fragments. Ta-
ble 1 recaps the features used for each task.
</bodyText>
<sectionHeader confidence="0.999057" genericHeader="evaluation">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999829214285714">
Dataset Description. The SemEval-2014 Task 3
Cross-Level Semantic Similarity is designed for
evaluating systems on their ability to capture the
semantic similarity between lexical items of dif-
ferent length (Jurgens et al., 2014). To this ex-
tent, the organizers provide four different levels
of comparison which correspond to four different
datasets: 1) Paragraph to Sentence (Par2Sent); 2)
Sentence to Phrase (Sent2Ph); 3) Phrase to Word
(Ph2W); and 4) Word to Sense (W2Sense).
For each dataset, the organizer released trial,
training and test data. While the trial includes a
few examples (approximately 40), both training
and test data comprise 500 pairs of text fragments.
</bodyText>
<page confidence="0.993875">
750
</page>
<table confidence="0.999853285714286">
Run Par2Sent Sent2Ph Ph2W W 2Sense Official Spearman
Rank Correlation
bestTrain .861 .793 .555 .420 - -
LCS .527 .562 .165 .109 - -
run1 .769 .729 .229 .165 7 10
run2 .784 .734 .255 .180 6 8
run3 .769 .729 .229 .165 8 11
</table>
<tableCaption confidence="0.987557">
Table 2: Task results.
</tableCaption>
<equation confidence="0.909554571428571">
Par2Sent
Feature Sent2Ph W2Sense
Ph2W
BOW I/ -
L1 I/ -
L2 I/ -
DIFF I/ -
</equation>
<tableCaption confidence="0.909722">
Table 1: Features per task.
</tableCaption>
<bodyText confidence="0.998524333333333">
Each pair is associated with a human-assigned
similarity score, which varies from 4 (very similar)
to 0 (unrelated). Organizers provide the normal-
ized Longest Common Substring (LCS) as base-
line. The evaluation is performed through the
Pearson (official rank) and the Spearman’s rank
correlation.
System setup. We develop our system in JAVA
relying on the following resources:
</bodyText>
<listItem confidence="0.999571266666667">
• Stanford CoreNLP to pre-process the text:
tokenization, lemmatization and PoS-tagging
are applied to the two text fragments;
• BabelNet 1.1 as knowledge-base in the WSD
algorithm;
• JAVA JUNG library for Personalized Page
Rank;
• British National Corpus (tokenized text with
stop word removal) and SVDLIB to build the
SemanticSpace described in Subsection 2.1;
• A proprietary implementation of Reflective
Random Indexing to build the distributional
space based on synsets (SDS) extracted from
BabelNet (we used two cycles of retraining);
• Weka for the supervised approach.
</listItem>
<bodyText confidence="0.999771885714286">
After a tuning step using both training and trial
data provided by organizers, we selected three dif-
ferent supervised systems: Gaussian Process with
Puk kernel (run1), Gaussian Process with RBF
kernel (run2), and Support Vector Machine Re-
gression with Puk kernel (run3). All the sys-
tems are implemented with the default parame-
ters set by Weka. We trained a different model on
each dataset. The DSM is built using the 100, 000
most frequent terms in the BNC, while the co-
occurrences are computed on a window size of 5
words. The vector dimension is set to 400, the
same value is adopted for building the SDS, where
the number of seeds (no zero components) gener-
ated in the random vectors is set to 10 with one
step of retraining. The total number of synset vec-
tors in the SDS is 576, 736. In the WSD algorithm,
we exploited the whole sentence as context. The
linear combination between the cosine similarity
and the probability p(sij|wi) is performed with a
factor of 0.5. The distance for expanding a synset
with its related meaning is set to one. The same
depth is used for building the graph in the PPR
method, where we fixed the maximum number of
iterations up to 50 and the dumpling factor to 0.85.
Results. Results of our three systems for
each similarity level are reported in Table 2 with
the baseline provided by the organizer (LCS).
Our three systems always outperform the LCS
baseline. Table 2 also shows the best results
(bestTrain) obtained on the training data by a
Gaussian Process with Puk kernel and a 10-fold
cross-validation. Support Vector Machine and
Gaussian Process with Puk kernel, run1 and run3
respectively, produce the same results. Comparing
</bodyText>
<figure confidence="0.9983854">
I/ I/
DSM I/ I/
WSD I/ I/
PPR
WSD-SDS I/ I/
I/ I/
P P R-SDS
I/
EDIT
I/
MCS
2-gram I/ -
3-gram I/ -
-
-
</figure>
<page confidence="0.985449">
751
</page>
<table confidence="0.99965925">
Par25ent .612 .697 .580 .129 .129 .461 .44 .630 .478 .585 .002 .231 .116
5ent2Ph .540 .649 .641 .110 .110 .526 .474 .376 .236 .584 .069 .357 .218
Ph2W .228 .095 .094 .087 .087 .136 .120 - - .095 .079 .013 .071
W25ense .147 .085 -.062 .084 .062
</table>
<tableCaption confidence="0.999567">
Table 3: Individual measures for each task.
</tableCaption>
<figure confidence="0.996851928571429">
L1
2-gram
WSD-SDS
W SD
EDIT
3-gram
BOW
MCS
PPR-SDS
DSM
PPR
DIFF
L2
Task
</figure>
<bodyText confidence="0.998867533333333">
these figures with those obtained on training data
(run1 and run3 vs. bestTrain), we can observe
that the Puk kernel tends to over-fit on training
data, while RBF kernel seems to be less sensitive
to this problem.
We analysed also the performance of each mea-
sure on its own; results in Table 3 are obtained by
training the best supervised system (run2) with
default parameters on each feature individually.
WSD obtains the best results in the first two lev-
els, while DSM is the best method in the last two
ones. This behaviour can be ascribed to the size
of the text fragments. In large text fragments the
WSD algorithm can rely on wider contexts to ob-
tain good performance; while in short texts infor-
mation about context is poor. At the W25ense
level, the measure based on the Personalized Page
Rank obtains the worst results; however, we no-
ticed that the ablation of that feature causes a drop
in performance of the supervised systems.
After the submission deadline, we noticed that
sometimes PoS-tagging produced wrong results
on small texts. This incorrect behaviour influenced
negatively the correct retrieval of synsets from Ba-
belNet. Thus, we decided to exclude PoS-tagging
for text fragments with less than three words. In
such a case, all the synsets for a given word are
retrieved. Making this adjustment, we were able
to obtain the improvements (A%) with respect to
the submitted runs reported on Table 4.
</bodyText>
<table confidence="0.99967775">
Run Ph2W A% W25ense A%
run1 .263 +14.85 .242 +46.67
run2 .257 +0.78 .237 +31.67
run3 .263 +14.85 .242 +46.66
</table>
<tableCaption confidence="0.9215635">
Table 4: Results after PoS-tagging removal for
short text (&lt; 3 words).
</tableCaption>
<sectionHeader confidence="0.999244" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999973363636364">
We have reported the results of our participa-
tion in the cross-level semantic similarity task
of SemEval-2014. Our systems combine differ-
ent similarity measures based on string-matching,
word sense disambiguation and distributional se-
mantic models. Our best system ranks 6th out
of the 38 participants in the task with respect to
the Pearson correlation, while it ranks 8th when
Spearman was used. These results suggest that our
methods are robust with respect to the evaluation
measures.
</bodyText>
<sectionHeader confidence="0.999074" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9996655">
This work fulfils the research objectives of the
PON 02 00563 3470993 project “VINCENTE -
A Virtual collective INtelligenCe ENvironment to
develop sustainable Technology Entrepreneurship
ecosystems” funded by the Italian Ministry of Uni-
versity and Research (MIUR).
</bodyText>
<sectionHeader confidence="0.999483" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996961555555556">
Satanjeev Banerjee and Ted Pedersen. 2002. An
adapted lesk algorithm for word sense disambigua-
tion using wordnet. In Computational linguis-
tics and intelligent text processing, pages 136–145.
Springer.
Pierpaolo Basile, Annalina Caputo, and Giovanni Se-
meraro. 2014. An Enhanced Lesk Word Sense
Disambiguation algorithm through a Distributional
Semantic Model. In Proceedings of COLING
2014, Dublin, Ireland, August. (http://www.coling-
2014.org/accepted-papers.php, in press).
Trevor Cohen, Roger Schvaneveldt, and Dominic Wid-
dows. 2010. Reflective random indexing and indi-
rect inference: A scalable method for discovery of
implicit connections. Journal of Biomedical Infor-
matics, 43(2):240 – 256.
Taher H. Haveliwala. 2002. Topic-sensitive pagerank.
In Proceedings of the 11th International Conference
</reference>
<page confidence="0.975001">
752
</page>
<reference confidence="0.99896284">
on World Wide Web, WWW ’02, pages 517–526,
New York, NY, USA. ACM.
David Jurgens, Mohammad Taher Pilehvar, and
Roberto Navigli. 2014. Semeval-2014 task 3:
Cross-level semantic similarity. In Proceedings of
the 8th International Workshop on Semantic Evalu-
ation (SemEval-2014), Dublin, Ireland, August 23–
24.
Pentti Kanerva. 1988. Sparse Distributed Memory.
MIT Press.
Thomas K. Landauer and Susan T. Dumais. 1997. A
Solution to Plato’s Problem: The Latent Semantic
Analysis Theory of Acquisition, Induction, and Rep-
resentation of Knowledge. Psychological Review,
104(2):211–240.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217–
250.
Florentina Vasilescu, Philippe Langlais, and Guy La-
palme. 2004. Evaluating variants of the lesk ap-
proach for disambiguating words. In Proceedings of
the Conference on Language Resources and Evalu-
ation (LREC), pages 633–636.
</reference>
<page confidence="0.999144">
753
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.922643">
<title confidence="0.9989475">UNIBA: Combining Distributional Semantic Models and Word Disambiguation for Textual Similarity</title>
<author confidence="0.99986">Basile Caputo</author>
<affiliation confidence="0.9996505">Department of Computer University of Bari Aldo</affiliation>
<address confidence="0.947723">Via, E. Orabona, 4 - 70125 Bari</address>
<abstract confidence="0.998356666666667">This paper describes the UNIBA team participation in the Cross-Level Semantic Similarity task at SemEval 2014. We propose to combine the output of different semantic similarity measures which exploit Word Sense Disambiguation and Distributional Semantic Models, among other lexical features. The integration of similarity measures is performed by means of two supervised methods based on Gaussian Process and Support Vector Machine. Our systems obtained very encouraging with the best one ranked out systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>An adapted lesk algorithm for word sense disambiguation using wordnet.</title>
<date>2002</date>
<booktitle>In Computational linguistics and intelligent text processing,</booktitle>
<pages>136--145</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="4686" citStr="Banerjee and Pedersen (2002)" startWordPosition="722" endWordPosition="726">ord Sense Disambiguation. Most of our measures rely on the output of a Word Sense Disambiguation (WSD) algorithm. Our newest approach to WSD, recently presented in Basile et al. (2014), is based on the simplified Lesk algorithm (Vasilescu et al., 2004). Each word wi in a sequence w1w2...wn is disambiguated individually by choosing the sense that maximizes the similarity between the gloss and the context of wi (i.e. the whole text where wi occurs). To boost the overlap between the context and the gloss, this last is expanded with glosses of related meanings, following the approach described in Banerjee and Pedersen (2002). As sense inventory we choose BabelNet 1.1, a huge multilingual semantic network which comprises both WordNet and Wikipedia (Navigli and Ponzetto, 2012). The algorithm consists of the following steps: 1. Building the glosses. We retrieve all possible word meanings for the target word wi that are listed in BabelNet. BabelNet mixes senses in WordNet and Wikipedia. First, senses in WordNet are searched for; if no sense is found (as often happens with named entities), senses for the target word are sought in Wikipedia. We preferred that strategy rather than retrieving senses from both sources at </context>
</contexts>
<marker>Banerjee, Pedersen, 2002</marker>
<rawString>Satanjeev Banerjee and Ted Pedersen. 2002. An adapted lesk algorithm for word sense disambiguation using wordnet. In Computational linguistics and intelligent text processing, pages 136–145. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierpaolo Basile</author>
<author>Annalina Caputo</author>
<author>Giovanni Semeraro</author>
</authors>
<title>An Enhanced Lesk Word Sense Disambiguation algorithm through a Distributional Semantic Model.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING 2014,</booktitle>
<location>Dublin, Ireland,</location>
<note>(http://www.coling2014.org/accepted-papers.php, in press).</note>
<contexts>
<context position="4242" citStr="Basile et al. (2014)" startWordPosition="647" endWordPosition="650">s (BNC). Then, M is reduced using the Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997). Vector addition and cosine similarity are 748 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 748–753, Dublin, Ireland, August 23-24, 2014. then used for building the vector representation of each text fragment and computing their pairwise similarity, respectively. 2.2 Lexical Semantics Level Word Sense Disambiguation. Most of our measures rely on the output of a Word Sense Disambiguation (WSD) algorithm. Our newest approach to WSD, recently presented in Basile et al. (2014), is based on the simplified Lesk algorithm (Vasilescu et al., 2004). Each word wi in a sequence w1w2...wn is disambiguated individually by choosing the sense that maximizes the similarity between the gloss and the context of wi (i.e. the whole text where wi occurs). To boost the overlap between the context and the gloss, this last is expanded with glosses of related meanings, following the approach described in Banerjee and Pedersen (2002). As sense inventory we choose BabelNet 1.1, a huge multilingual semantic network which comprises both WordNet and Wikipedia (Navigli and Ponzetto, 2012). T</context>
</contexts>
<marker>Basile, Caputo, Semeraro, 2014</marker>
<rawString>Pierpaolo Basile, Annalina Caputo, and Giovanni Semeraro. 2014. An Enhanced Lesk Word Sense Disambiguation algorithm through a Distributional Semantic Model. In Proceedings of COLING 2014, Dublin, Ireland, August. (http://www.coling2014.org/accepted-papers.php, in press).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohen</author>
<author>Roger Schvaneveldt</author>
<author>Dominic Widdows</author>
</authors>
<title>Reflective random indexing and indirect inference: A scalable method for discovery of implicit connections.</title>
<date>2010</date>
<journal>Journal of Biomedical Informatics,</journal>
<volume>43</volume>
<issue>2</issue>
<pages>256</pages>
<contexts>
<context position="9390" citStr="Cohen et al., 2010" startWordPosition="1547" endWordPosition="1551">l SDS (Synset Distributional Space). SDS is generated taking into account the synset relationships, and similarity is defined as the synsets closeness in the space. We build a symmetric matrix S which contains synsets on both rows and columns. Each cell in the matrix is set to one if a semantic relation exists between the corresponding synsets. The relationships are extracted from BabelNet limiting synsets to those occurring also in WordNet, while synsets coming from Wikipedia are removed to reduce the size of S. The method for building the matrix S relies on Reflective Random Indexing (RRI) (Cohen et al., 2010), a variation of the Random Indexing technique for matrix reduction (Kanerva, 1988). RRI retains the advantages of RI which incrementally builds a reduced space where distance between points is nearly preserved. Moreover, cyclical training, i.e. the retraining of a new space exploiting the RI output as basis vectors, makes indirect inference to emerge. Two different similarity measures can be defined by exploiting this space for representing synsets: WSD-SDS and PPR-SDS, based on WSD and PPR respectively. Each BoS is represented as the sum of the synset vectors in the SDS space. Then, the simi</context>
</contexts>
<marker>Cohen, Schvaneveldt, Widdows, 2010</marker>
<rawString>Trevor Cohen, Roger Schvaneveldt, and Dominic Widdows. 2010. Reflective random indexing and indirect inference: A scalable method for discovery of implicit connections. Journal of Biomedical Informatics, 43(2):240 – 256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taher H Haveliwala</author>
</authors>
<title>Topic-sensitive pagerank.</title>
<date>2002</date>
<booktitle>In Proceedings of the 11th International Conference on World Wide Web, WWW ’02,</booktitle>
<pages>517--526</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="8204" citStr="Haveliwala, 2002" startWordPosition="1353" endWordPosition="1354">ynsets. The WSD measure (WSD) is computed on the top of the output of the last step. For each text fragment, we build a Bag-of-Synset (BoS) as the sum, over the whole text, of the weighted synsets associated with each word. Then, we compute the WSD similarity as the cosine similarity between the two BoS. 749 Graph. A sub-graph of BabelNet is built for each text fragment starting from the synsets provided by the WSD algorithm. For each word the synset with the highest score is selected, then this initial set is expanded with the related synsets in BabelNet. We apply the Personalized Page Rank (Haveliwala, 2002) to each sub-graph where the synset scores computed by the WSD algorithm are exploited as prior probabilities. The weighted rank of synsets provided by Page Rank is used to build the BoS of the two text fragments, then the Personalized Page Rank (PPR) is computed as the cosine similarity between them. Synset Distributional Space. Generally, similarity measures between synsets rely on the synsets hierarchy in a semantic network (e.g. WordNet). We define a new approach that is completely different, and represents synsets as points in a geometric space that we call SDS (Synset Distributional Spac</context>
</contexts>
<marker>Haveliwala, 2002</marker>
<rawString>Taher H. Haveliwala. 2002. Topic-sensitive pagerank. In Proceedings of the 11th International Conference on World Wide Web, WWW ’02, pages 517–526, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Jurgens</author>
<author>Mohammad Taher Pilehvar</author>
<author>Roberto Navigli</author>
</authors>
<title>Semeval-2014 task 3: Cross-level semantic similarity.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014),</booktitle>
<volume>23</volume>
<pages>24</pages>
<location>Dublin, Ireland,</location>
<contexts>
<context position="11607" citStr="Jurgens et al., 2014" startWordPosition="1926" endWordPosition="1929">cal text similarity task. In this case we decide to consider the word on its own as the first text fragment, while for the second text fragment we build a dummy text using the BabelNet gloss assigned to the word sense. In that way, the distributional and the lexical measures (WSD, Graph, and DSM) can be applied to both fragments. Table 1 recaps the features used for each task. 3 Evaluation Dataset Description. The SemEval-2014 Task 3 Cross-Level Semantic Similarity is designed for evaluating systems on their ability to capture the semantic similarity between lexical items of different length (Jurgens et al., 2014). To this extent, the organizers provide four different levels of comparison which correspond to four different datasets: 1) Paragraph to Sentence (Par2Sent); 2) Sentence to Phrase (Sent2Ph); 3) Phrase to Word (Ph2W); and 4) Word to Sense (W2Sense). For each dataset, the organizer released trial, training and test data. While the trial includes a few examples (approximately 40), both training and test data comprise 500 pairs of text fragments. 750 Run Par2Sent Sent2Ph Ph2W W 2Sense Official Spearman Rank Correlation bestTrain .861 .793 .555 .420 - - LCS .527 .562 .165 .109 - - run1 .769 .729 .</context>
</contexts>
<marker>Jurgens, Pilehvar, Navigli, 2014</marker>
<rawString>David Jurgens, Mohammad Taher Pilehvar, and Roberto Navigli. 2014. Semeval-2014 task 3: Cross-level semantic similarity. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014), Dublin, Ireland, August 23– 24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pentti Kanerva</author>
</authors>
<title>Sparse Distributed Memory.</title>
<date>1988</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="9473" citStr="Kanerva, 1988" startWordPosition="1563" endWordPosition="1564">ationships, and similarity is defined as the synsets closeness in the space. We build a symmetric matrix S which contains synsets on both rows and columns. Each cell in the matrix is set to one if a semantic relation exists between the corresponding synsets. The relationships are extracted from BabelNet limiting synsets to those occurring also in WordNet, while synsets coming from Wikipedia are removed to reduce the size of S. The method for building the matrix S relies on Reflective Random Indexing (RRI) (Cohen et al., 2010), a variation of the Random Indexing technique for matrix reduction (Kanerva, 1988). RRI retains the advantages of RI which incrementally builds a reduced space where distance between points is nearly preserved. Moreover, cyclical training, i.e. the retraining of a new space exploiting the RI output as basis vectors, makes indirect inference to emerge. Two different similarity measures can be defined by exploiting this space for representing synsets: WSD-SDS and PPR-SDS, based on WSD and PPR respectively. Each BoS is represented as the sum of the synset vectors in the SDS space. Then, the similarity is computed as the cosine similarity between the two vector representations.</context>
</contexts>
<marker>Kanerva, 1988</marker>
<rawString>Pentti Kanerva. 1988. Sparse Distributed Memory. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A Solution to Plato’s Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="3718" citStr="Landauer and Dumais, 1997" startWordPosition="569" endWordPosition="572">site of the distance between points that represent those words. Thus, the semantic similarity can be computed as the cosine of the angle between the two vectors that represent the words. This concept of similarity can be extended to whole sentences by combining words through vector addition (+), which corresponds to the point-wise sum of the vector components. Our DSM measure (DSM) is based on a SemanticSpace, represented by a co-occurrences matrix M, built by analysing the distribution of words in the British National Corpus (BNC). Then, M is reduced using the Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997). Vector addition and cosine similarity are 748 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 748–753, Dublin, Ireland, August 23-24, 2014. then used for building the vector representation of each text fragment and computing their pairwise similarity, respectively. 2.2 Lexical Semantics Level Word Sense Disambiguation. Most of our measures rely on the output of a Word Sense Disambiguation (WSD) algorithm. Our newest approach to WSD, recently presented in Basile et al. (2014), is based on the simplified Lesk algorithm (Vasilescu et al., 2004). Each w</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas K. Landauer and Susan T. Dumais. 1997. A Solution to Plato’s Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge. Psychological Review, 104(2):211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network.</title>
<date>2012</date>
<journal>Artificial Intelligence,</journal>
<volume>193</volume>
<pages>250</pages>
<contexts>
<context position="4839" citStr="Navigli and Ponzetto, 2012" startWordPosition="746" endWordPosition="749">sented in Basile et al. (2014), is based on the simplified Lesk algorithm (Vasilescu et al., 2004). Each word wi in a sequence w1w2...wn is disambiguated individually by choosing the sense that maximizes the similarity between the gloss and the context of wi (i.e. the whole text where wi occurs). To boost the overlap between the context and the gloss, this last is expanded with glosses of related meanings, following the approach described in Banerjee and Pedersen (2002). As sense inventory we choose BabelNet 1.1, a huge multilingual semantic network which comprises both WordNet and Wikipedia (Navigli and Ponzetto, 2012). The algorithm consists of the following steps: 1. Building the glosses. We retrieve all possible word meanings for the target word wi that are listed in BabelNet. BabelNet mixes senses in WordNet and Wikipedia. First, senses in WordNet are searched for; if no sense is found (as often happens with named entities), senses for the target word are sought in Wikipedia. We preferred that strategy rather than retrieving senses from both sources at once because this last approach produced worse results when tuning the system. Once the set of senses Si = {si1, si2, ..., sik} associated to the target </context>
</contexts>
<marker>Navigli, Ponzetto, 2012</marker>
<rawString>Roberto Navigli and Simone Paolo Ponzetto. 2012. BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network. Artificial Intelligence, 193:217– 250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florentina Vasilescu</author>
<author>Philippe Langlais</author>
<author>Guy Lapalme</author>
</authors>
<title>Evaluating variants of the lesk approach for disambiguating words.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>633--636</pages>
<contexts>
<context position="4310" citStr="Vasilescu et al., 2004" startWordPosition="659" endWordPosition="662">SA) (Landauer and Dumais, 1997). Vector addition and cosine similarity are 748 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 748–753, Dublin, Ireland, August 23-24, 2014. then used for building the vector representation of each text fragment and computing their pairwise similarity, respectively. 2.2 Lexical Semantics Level Word Sense Disambiguation. Most of our measures rely on the output of a Word Sense Disambiguation (WSD) algorithm. Our newest approach to WSD, recently presented in Basile et al. (2014), is based on the simplified Lesk algorithm (Vasilescu et al., 2004). Each word wi in a sequence w1w2...wn is disambiguated individually by choosing the sense that maximizes the similarity between the gloss and the context of wi (i.e. the whole text where wi occurs). To boost the overlap between the context and the gloss, this last is expanded with glosses of related meanings, following the approach described in Banerjee and Pedersen (2002). As sense inventory we choose BabelNet 1.1, a huge multilingual semantic network which comprises both WordNet and Wikipedia (Navigli and Ponzetto, 2012). The algorithm consists of the following steps: 1. Building the glosse</context>
</contexts>
<marker>Vasilescu, Langlais, Lapalme, 2004</marker>
<rawString>Florentina Vasilescu, Philippe Langlais, and Guy Lapalme. 2004. Evaluating variants of the lesk approach for disambiguating words. In Proceedings of the Conference on Language Resources and Evaluation (LREC), pages 633–636.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>