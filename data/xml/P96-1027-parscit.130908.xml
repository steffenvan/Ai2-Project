<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001799">
<title confidence="0.950032">
Chart Generation
</title>
<author confidence="0.998445">
Martin Kay
</author>
<affiliation confidence="0.8921715">
Stanford University and
Xerox Palo Alto Research Center
</affiliation>
<email confidence="0.997335">
kay@parc.xerox.com
</email>
<sectionHeader confidence="0.993862" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999557166666667">
Charts constitute a natural uniform architecture for
parsing and generation provided string position is
replaced by a notion more appropriate to logical
forms and that measures are taken to curtail gener-
ation paths containing semantically incomplete
phrases.
</bodyText>
<sectionHeader confidence="0.99558" genericHeader="keywords">
1 Charts
</sectionHeader>
<bodyText confidence="0.999971606557378">
Shieber (1988) showed that parsing charts can be also used
in generation and raised the question, which we take up
again here, of whether they constitute a natural uniform
architecture for parsing and generation. In particular, we
will be interested in the extent to which they bring to the
generation process advantages comparable to those that
make them attractive in parsing.
Chart parsing is not a well defined notion. The usual
conception of it involves at least four related ideas:
Inactive edges. In context-free grammar, all phrases of a
given category that cover a given part of the string are
equivalent for the purposes of constructing larger
phrases. Efficiency comes from collecting equivalent
sets of phrases into (inactive) edges and constructing
edges from edges rather than phrases from phrases.
Active edges. New phrases of whatever size can be built
by considering existing edges pair-wise if provision is
made for partial phrases. Partial phrases are collected
into edges that are said to be active because they can be
thought of as actively seeking material to complete
them.
The algorithm schema. Newly created edges are placed
on an agenda. Edges are moved from the agenda to the
chart one by one until none remains to be moved.
When an edge is moved, all interactions between it and
edges already in the chart are considered and any new
edges that they give rise to are added to the agenda.
Indexing. The positions in the string at which phrases
begin and end can be used to index edges so that the
algorithm schema need consider interactions only
between adjacent pairs.
Chart parsing is attractive for the analysis of natural lan-
guages, as opposed to programming languages, for the way
in which it treats ambiguity. Regardless of the number of
alternative structures for a particular string that a given
phrase participates in, it will be constructed once and only
once. Although the number of structures of a string can
grow exponentially with the length of the string, the number
of edges that needs to be constructed grows only with the
square of the string length and the whole parsing process
can be accomplished in cubic time.
Innumerable variants of the basic chart parsing scheme
are possible. For example, if there were languages with
truly free word order, we might attempt to characterize
them by rules like those of context-free grammar, but with a
somewhat different interpretation. Instead of replacing non-
terminal symbols in a derivation with strings from the right-
hand side of corresponding rules, we would remove the
nonterminal symbol and insert the symbols from the right-
hand side of the rule at arbitrary places in the string.
A chart parser for languages with free word order
would be a minor variant of the standard one. An edge
would take the form where v is a vector with a bit for
every word in the string and showing which of those words
the edge covers. There is no longer any notion of adjacency
so that there would be no indexing by string position. Inter-
esting interactions occur between pairs of edges whose bit
vectors have empty intersections, indicating that they cover
disjoint sets of words. There can now be as many edges as
bit-vectors and, not surprisingly, the computational com-
plexity of the parsing process increases accordingly.
</bodyText>
<sectionHeader confidence="0.990829" genericHeader="introduction">
2 Generation
</sectionHeader>
<bodyText confidence="0.999718444444445">
A parser is a transducer from strings to structures or
logical forms. A generator, for our purposes, is the inverse.
One way to think of it, therefore, is as a parser of structures
or logical forms that delivers analyses in the form of strings.
This view has the apparent disadvantage of putting insignif-
icant differences in the syntax of a logical forms, such as
the relative order of the arguments to symmetric operators,
on the same footing as more significant facts about them.
We know that it will not generally be possible to reduce
</bodyText>
<page confidence="0.988914">
200
</page>
<bodyText confidence="0.999461071428571">
logical expressions to a canonical form but this does not
mean that we should expect our generator to be compro-
mised, or even greatly delayed, by trivial distinctions. Con-
siderations of this kind were, in part, responsible for the
recent resurgence of interest in &amp;quot;flat&amp;quot; representations of log-
ical form (Copestake et a/.,I 996) and for the representa-
tions used for transfer in Shake-and-Bake translation
(Whitelock, 1992). They have made semantic formalisms
like those now usually associated with Davison (Davidson,
1980, Parsons, 1990) attractive in artificial intelligence for
many years (Hobbs 1985, Kay, 1970). Operationally, the
attraction is that the notations can be analyzed largely as
free word-order languages in the manner outlined above.
Consider the expression (I)
</bodyText>
<listItem confidence="0.620294">
(1) r: run(r), past(r), fast(r), argl(r, j), name(j, John)
</listItem>
<bodyText confidence="0.998347153846154">
which we will take as a representation of the logical form of
the sentences John ran fast and John ran quickly. It consists
of a distinguished index (r) and a list of predicates whose
relative order is immaterial. The distinguished index identi-
fies this as a sentence that makes a claim about a running
event. &amp;quot;John&amp;quot; is the name of the entity that stands in the
`argl &apos; relation to the running which took place in the past
and which was fast. Nothing turns on these details which
will differ with differing ontologies, logics, and views of
semantic structure. What concerns us here is a procedure
for generating a sentence from a structure of this general
kind.
Assume that the lexicon contains entries like those in
</bodyText>
<listItem confidence="0.998704">
(2) in which the italicized arguments to the semantic predi-
cates are variables.
(2)
</listItem>
<table confidence="0.847160833333333">
Words Cat Semantics
John np(x) x: name(x, John)
ran vp(x, y) x: run(x), argl(x, y),
past(x)
fast adv(x) x: fast(x)
quickly adv(x) x: fast(x)
</table>
<bodyText confidence="0.999173">
A prima facie argument for the utility of these particular
words for expressing ( I) can be made simply by noting that,
modulo appropriate instantiation of the variables, the
semantics of each of these words subsumes (1).
</bodyText>
<sectionHeader confidence="0.99184" genericHeader="method">
3 The Algorithm Schema
</sectionHeader>
<bodyText confidence="0.993632571428572">
The entries in (2), with their variables suitably instantiated,
become the initial entries of an agenda and we begin to
move them to the chart in accordance with the algorithm
schema, say in the order given.
The variables in the &apos;Cat&apos; and &apos;Semantics&apos; columns of
(2) provide the essential link between syntax and semantics.
The predicates that represent the semantics of a phrase will
simply be the union of those representing the constituents.
The rules that sanction a phrase (e.g. (3) below) show
which variables from the two parts are to be identified.
When the entry for John is moved, no interactions are
possible because the chart is empty. When run is moved, the
sequence John ran is considered as a possible phrase on the
basis of rule (3).
</bodyText>
<listItem confidence="0.947536">
(3) s(x) —&gt; np(y), vp(x, y).
</listItem>
<bodyText confidence="0.975954">
With appropriate replacements for variables, this maps onto
the subset (4) of the original semantic specification in (1).
</bodyText>
<listItem confidence="0.917038">
(4) r: run(r), past(r), argl(r, j), name(j, John)
</listItem>
<bodyText confidence="0.989249">
Furthermore it is a complete sentence. However, it does not
count as an output to the generation process as a whole
because it subsumes some but not all of (1). It therefore
simply becomes a new edge on the agenda.
The string ran fast constitutes a verb phrase by virtue
of rule (5) giving the semantics (6), and the phrase ran
quickly with the same semantics is put on the agenda when
the quickly edge is move to the chart.
</bodyText>
<listItem confidence="0.782529">
(5) vp(x) —&gt; vp(x) adv(x)
(6) r: run(r), past(r), fast(r), argl(r, y)
The agenda now contains the entries in (7).
</listItem>
<table confidence="0.778843714285714">
Words Cat Semantics
John ran s(r) r: run(r), past(r), arg I (r, j),
name(j, John)
ran fast vp(r, j) r: run(r), past(r), fast(r),
argl(r, j)
ran quickly vp(r, j) r: run(r), past(r), fast(r),
arg 1 (r, j)
</table>
<bodyText confidence="0.999709230769231">
Assuming that adverbs modify verb phrases and not sen-
tences, there will be no interactions when the John ran edge
is moved to the chart.
When the edge for ran fast is moved, the possibility
arises of creating the phrase ran fast quickly as well as ran
fast fast. Both are rejected, however, on the grounds that
they would involve using a predicate from the original
semantic specification more than once. This would be simi-
lar to allowing a given word to be covered by overlapping
phrases in free word-order parsing. We proposed eliminat-
ing this by means of a bit vector and the same technique
applies here. The fruitful interactions that occur here are
between ran fast and ran quickly on the one hand, and John
</bodyText>
<page confidence="0.99158">
201
</page>
<bodyText confidence="0.96256425">
on the other. Both give sentences whose semantics sub-
sumes the entire input.
Several things are noteworthy about the process just
outlined.
</bodyText>
<listItem confidence="0.9349473125">
1. Nothing turns on the fact that it uses a primitive version
of event semantics. A scheme in which the indices
were handles referring to subexpressions in any variety
of flat semantics could have been treated in the same
way. Indeed, more conventional formalisms with richly
recursive syntax could be converted to this form on the
fly.
2. Because all our rules are binary, we make no use of
active edges.
3. While it fits the conception of chart parsing given at
the beginning of this paper, our generator does not
involve string positions centrally in the chart represen-
tation. In this respect, it differs from the proposal of
Shieber (1988) which starts with all word edges leav-
ing and entering a single vertex. But there is essentially
no information in such a representation. Neither the
chart nor any other special data structure is required to
capture the fact that a new phrase may be constructible
out of any given pair, and in either order, if they meet
certain syntactic and semantic criteria.
4. Interactions must be considered explicitly between
new edges and all edges currently in the chart, because
no indexing is used to identify the existing edges that
could interact with a given new one.
5. The process is exponential in the worst case because, if
a sentence contains a word with k modifiers, then a
version it will be generated with each of the 2k subsets
of those modifiers, all but one of them being rejected
when it is finally discovered that their semantics does
not subsume the entire input. If the relative orders of
the modifiers are unconstrained, matters only get
worse.
</listItem>
<bodyText confidence="0.999988285714286">
Points 4 and 5 are serious flaws in our scheme for which we
shall describe remedies. Point 2 will have some importance
for us because it will turn out that the indexing scheme we
propose will require the use of distinct active and inactive
edges, even when the rules are all binary. We take up the
complexity issue first, and then turn to how the efficiency of
the generation chart might be enhanced through indexing.
</bodyText>
<sectionHeader confidence="0.998819" genericHeader="method">
4 Internal and External Indices
</sectionHeader>
<bodyText confidence="0.999810521739131">
The exponential factor in the computational complexity of
our generation algorithm is apparent in an example like (8).
(8) Newspaper reports said the tall young Polish athlete
ran fast
The same set of predicates that generate this sentence
clearly also generate the same sentence with deletion of all
subsets of the words tall, young. and Polish for a total of 8
strings. Each is generated in its entirety, though finally
rejected because it fails to account for all of the semantic
material. The words newspaper and fast can also be deleted
independently giving a grand total of 32 strings.
We concentrate on the phrase tall young Polish athlete
which we assumed would be combined with the verb phrase
ran fast by the rule (3). The distinguished index of the noun
phrase, call it p, is identified with the variable y in the rule,
but this variable is not associated with the syntactic cate-
gory, s, on the left-hand side of the rule. The grammar has
access to indices only through the variables that annotate
grammatical categories in its rules, so that rules that incor-
porate this sentence into larger phrases can have no further
access to the index p. We therefore say that p is internal to
the sentence the tall young Polish athlete ran fast.
The index p would, of course, also be internal to the
sentences the young Polish athlete ran fast, the tall Polish
athlete ran fast, etc. However, in these cases, the semantic
material remaining to be expressed contains predicates that
refer to this internal index, say tall(p)&apos; , and `young(p)&apos;.
While the lexicon may have words to express these predi-
cates, the grammar has no way of associating their referents
with the above noun phrases because the variables corre-
sponding to those referents are internal. We conclude that,
as a matter of principle, no edge should be constructed if
the result of doing so would be to make internal an index
occurring in part of the input semantics that the new phrase
does not subsume. In other words, the semantics of a phrase
must contain all predicates from the input specification that
refer to any indices internal to it. This strategy does not pre-
vent the generation of an exponential number of variants of
phrases containing modifiers. It limits proliferation of the ill
effects, however, by allowing only the maximal one to be
incorporated in larger phrases. In other words, if the final
result has phrases with m and n modifiers respectively, then
2&apos; versions of the first and 211&apos; of the second will be created,
but only one of each set will be incorporated into larger
phrases and no factor of 2&apos;14-no will be introduced into the
cost of the process.
</bodyText>
<sectionHeader confidence="0.994746" genericHeader="evaluation">
5 Indexing
</sectionHeader>
<bodyText confidence="0.999946444444444">
String positions provide a natural way to index the strings
input to the parsing process for the simple reason that there
are as many of them as there are words but, for there to be
any possibility of interaction between a pair of edges, they
must come together at just one index. These are the natural
points of articulation in the domain of strings. They cannot
fill this role in generation because they are not natural prop-
erties of the semantic expressions that are the input to the
process. The corresponding natural points of articulation in
</bodyText>
<page confidence="0.995236">
202
</page>
<bodyText confidence="0.994519333333333">
flat semantic structures are the entities that we have already
been referring to as indices.
In the modified version of the procedure, whenever a
new inactive edge is created with label B(b ...). then for all
rules of the form in (9), an active edge is also created with
label A(...)/C(c ...).
</bodyText>
<listItem confidence="0.906771">
(9) A(...) ---&gt; B(b ...) C(c ...)
</listItem>
<bodyText confidence="0.999670076923077">
This represents a phrase of category A that requires a phrase
of category Con the right for its completion. In these labels,
b and c are (variables representing) the first, or distin-
guished indices associated with B and C. By analogy with
parsing charts, an inactive edge labeled B(b ...) can be
thought of as incident from vertex b, which means simply
that it is efficiently accessible through the index b. An active
edge A(...)IC(c ...) should be thought of as incident from, or
accessible through, the index c. The key property of this
scheme is that active and inactive edges interact by virtue of
indices that they share and, by letting vertices correspond to
indices, we collect together sets of edges that could interact.
We illustrate the modified procedure with the sentence
</bodyText>
<listItem confidence="0.880925111111111">
(10) whose semantics we will take to be (11), the grammar
rules (12)-(14), and the lexical entries in (15).
(10) The dog saw the cat.
(11) dog(d), def(d), saw(s), past(s), cat(c), def(c),
argl(s. d), arg2(s, c).
(12) s(x) np(y) vp(x, y)
(13) vp(x, --&gt; v(x, Y, z) np(z)
(14) np(x) ---&gt; det(x) n(x)
(15)
</listItem>
<table confidence="0.9888595">
Words Cat Semantics
cat n(x) x: cat(x)
saw v(x, y, z) x: see(x), past(x), argl(x, y),
arg2(x,z)
dog n(x) x: dog(x)
the det(x) x: def(x)
</table>
<bodyText confidence="0.999918416666667">
The procedure will be reminiscent of left-corner parsing.
Arguments have been made in favor of a head-driven strat-
egy which would, however, have been marginally more
complex (e.g. in Kay (1989), Shieber, et el. (1989)) and the
differences are, in any case, not germane to our current con-
cerns.
The initial agenda, including active edges, and collect-
ing edges by the vertices that they are incident from, is
given in (16).
The grammar is consulted only for the purpose of cre-
ating active edges and all interactions in the chart are
between active and inactive pairs of edges incident from the
</bodyText>
<table confidence="0.9261598">
same vertex.
(16)
Vert Words Cat Semantics
d the det(d) d: def(d)
the np(d)/n(d) d: def(d)
dog n(d) d: dog(d)
s saw v(s, d, c) s: see(s). past(s),
argl(s, d), arg2(s, c)
c saw vp(s, d)/np(c) r: see(s), past(s),
argl(r, j)
the det(c) c: def(c)
the np(c)/n(c) c: def(c)
cat n(c) c: dog(c)
(17)
Vert Words Cat Semantics
</table>
<bodyText confidence="0.950415045454546">
d the dog np(d) d: dog(d), def(d)
saw the vp(s, d)/np(d) s: see(s), past(s),
cat argl(s, d), arg2(s, c),
cat(c), def(c)
c the cat np(c) c: cat(c), def(c)
s saw the vp(s, d) s: see(s), past(s),
cat argl(s, d), arg2(s, c),
cat(c), def(c)
Among the edges in (16), there are two interactions,
one at vertices c and d. They cause the first and third edges
in (17) to be added to the agenda. The first interacts with the
active edge originally introduced by the verb &amp;quot;saw&amp;quot; produc-
ing the fourth entry in (17). The label on this edge matches
the first item on the right-hand side of rule (12) and the
active edge that we show in the second entry is also intro-
duced. The final interaction is between the first and second
edges in (17) which give rise to the edge in (18).
This procedure confirms perfectly to the standard algo-
rithm schema for chart parsing, especially in the version
that makes predictions immediately following the recogni-
tion of the first constituent of a phrase, that is, in the version
that is essentially a caching left-corner parser.
</bodyText>
<page confidence="0.995967">
203
</page>
<figure confidence="0.846752285714286">
(18)
Vert Words Cat Semantics
s The dog saw the cat s(s) dog(d), def(d),
see(s),
past( s),argl(s , d),
arg2(s, c), cat(c),
def(c).
</figure>
<sectionHeader confidence="0.875447" genericHeader="conclusions">
6 Acknowledgments
</sectionHeader>
<reference confidence="0.8951956">
Whatever there may be of value in this paper owes much to
the interest, encouragement, and tolerance of my colleagues
Marc Dymetman, Ronald Kaplan, John Maxwell, and
Hadar Shem Toy. I am also indebted to the anonymous
reviewers of this paper.
</reference>
<sectionHeader confidence="0.9193" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999669384615385">
Copestake, A., Dan Flickinger, Robert Malouf, and
Susanne Riehemann, and Ivan Sag (1996). Translation
Using Minimal Recursion Semantics. Proceedings of The
Sixth International Conference on Theoretical and Method-
ological Issues in Machine Translation, Leuven (in press).
Davidson, D. (1980). Essays on Actions and Events.
Oxford: The Clarendon Press.
Hobbs, J. R. (1985). Ontological Promiscuity. 23rd
Annual Meeting of the Association for Computational Lin-
guistics, Chicago, ACL.
Kay, M. (1970). From Semantics to Syntax. Progress
in Linguistics. Bierwisch Manfred, and K. E. Heidolf. The
Hague, Mouton: 114-126.
Kay, M. (1989). Head-driven Parsing. Proceedings of
Workshop on Parsing Technologies, Pittsburgh, PA.
Parsons, T. (1990). Events in the Semantics of English.
Cambridge, Mass.: MIT Press.
Shieber, S. (1988). A Uniform Architecture for Parsing
and Generation. COLING-88, Budapest, John von Neu-
mann Society for Computing Sciences.
Shieber, S. M. et al. (1989). A Semantic-Head-Driven
Generation Algorithm for Unification Based Formalisms.
27th Annual Meeting of the Association for Computational
Linguistics, Vancouver. B.C.
Whitelock, P. (1992). Shake and-Bake Translation.
COLING-92, Nantes.
</reference>
<page confidence="0.998889">
204
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.006711">
<title confidence="0.99953">Chart Generation</title>
<author confidence="0.999885">Martin Kay</author>
<affiliation confidence="0.996251">Stanford University and Xerox Palo Alto Research Center</affiliation>
<email confidence="0.999724">kay@parc.xerox.com</email>
<abstract confidence="0.979782120448181">Charts constitute a natural uniform architecture for parsing and generation provided string position is replaced by a notion more appropriate to logical forms and that measures are taken to curtail generation paths containing semantically incomplete phrases. 1 Charts Shieber (1988) showed that parsing charts can be also used in generation and raised the question, which we take up again here, of whether they constitute a natural uniform architecture for parsing and generation. In particular, we will be interested in the extent to which they bring to the generation process advantages comparable to those that make them attractive in parsing. Chart parsing is not a well defined notion. The usual conception of it involves at least four related ideas: edges. context-free grammar, all phrases of a given category that cover a given part of the string are equivalent for the purposes of constructing larger phrases. Efficiency comes from collecting equivalent of phrases into (inactive) constructing edges from edges rather than phrases from phrases. edges. phrases of whatever size can be built by considering existing edges pair-wise if provision is made for partial phrases. Partial phrases are collected edges that are said to be they can be thought of as actively seeking material to complete them. algorithm schema. created edges are placed an are moved from the agenda to the by one until none remains to be moved. When an edge is moved, all interactions between it and edges already in the chart are considered and any new edges that they give rise to are added to the agenda. positions in the string at which phrases begin and end can be used to index edges so that the algorithm schema need consider interactions only between adjacent pairs. Chart parsing is attractive for the analysis of natural languages, as opposed to programming languages, for the way in which it treats ambiguity. Regardless of the number of alternative structures for a particular string that a given phrase participates in, it will be constructed once and only once. Although the number of structures of a string can grow exponentially with the length of the string, the number of edges that needs to be constructed grows only with the square of the string length and the whole parsing process can be accomplished in cubic time. Innumerable variants of the basic chart parsing scheme are possible. For example, if there were languages with truly free word order, we might attempt to characterize them by rules like those of context-free grammar, but with a somewhat different interpretation. Instead of replacing nonterminal symbols in a derivation with strings from the righthand side of corresponding rules, we would remove the nonterminal symbol and insert the symbols from the righthand side of the rule at arbitrary places in the string. A chart parser for languages with free word order would be a minor variant of the standard one. An edge would take the form where v is a vector with a bit for every word in the string and showing which of those words the edge covers. There is no longer any notion of adjacency so that there would be no indexing by string position. Interesting interactions occur between pairs of edges whose bit vectors have empty intersections, indicating that they cover disjoint sets of words. There can now be as many edges as bit-vectors and, not surprisingly, the computational complexity of the parsing process increases accordingly. 2 Generation A parser is a transducer from strings to structures or logical forms. A generator, for our purposes, is the inverse. One way to think of it, therefore, is as a parser of structures or logical forms that delivers analyses in the form of strings. This view has the apparent disadvantage of putting insignificant differences in the syntax of a logical forms, such as the relative order of the arguments to symmetric operators, on the same footing as more significant facts about them. We know that it will not generally be possible to reduce 200 logical expressions to a canonical form but this does not mean that we should expect our generator to be compromised, or even greatly delayed, by trivial distinctions. Considerations of this kind were, in part, responsible for the recent resurgence of interest in &amp;quot;flat&amp;quot; representations of logform (Copestake 996) and for the representations used for transfer in Shake-and-Bake translation (Whitelock, 1992). They have made semantic formalisms like those now usually associated with Davison (Davidson, 1980, Parsons, 1990) attractive in artificial intelligence for many years (Hobbs 1985, Kay, 1970). Operationally, the attraction is that the notations can be analyzed largely as free word-order languages in the manner outlined above. Consider the expression (I) (1) r: run(r), past(r), fast(r), argl(r, j), name(j, John) which we will take as a representation of the logical form of sentences ran fast ran quickly. consists of a distinguished index (r) and a list of predicates whose relative order is immaterial. The distinguished index identifies this as a sentence that makes a claim about a running event. &amp;quot;John&amp;quot; is the name of the entity that stands in the `argl &apos; relation to the running which took place in the past and which was fast. Nothing turns on these details which will differ with differing ontologies, logics, and views of semantic structure. What concerns us here is a procedure for generating a sentence from a structure of this general kind. Assume that the lexicon contains entries like those in (2) in which the italicized arguments to the semantic predicates are variables. (2) Words Cat Semantics John np(x) John) ran vp(x, y) argl(x, y), past(x) fast adv(x) quickly adv(x) x: fast(x) facie for the utility of these particular words for expressing ( I) can be made simply by noting that, instantiation of the variables, the semantics of each of these words subsumes (1). 3 The Algorithm Schema The entries in (2), with their variables suitably instantiated, become the initial entries of an agenda and we begin to move them to the chart in accordance with the algorithm schema, say in the order given. The variables in the &apos;Cat&apos; and &apos;Semantics&apos; columns of (2) provide the essential link between syntax and semantics. The predicates that represent the semantics of a phrase will simply be the union of those representing the constituents. The rules that sanction a phrase (e.g. (3) below) show which variables from the two parts are to be identified. the entry for moved, no interactions are because the chart is empty. When moved, the ran considered as a possible phrase on the basis of rule (3). (3) s(x) —&gt; np(y), vp(x, y). With appropriate replacements for variables, this maps onto the subset (4) of the original semantic specification in (1). (4) r: run(r), past(r), argl(r, j), name(j, John) Furthermore it is a complete sentence. However, it does not count as an output to the generation process as a whole because it subsumes some but not all of (1). It therefore simply becomes a new edge on the agenda. string fast a verb phrase by virtue rule (5) giving the semantics (6), and the phrase the same semantics is put on the agenda when is move to the chart. (5) vp(x) —&gt; vp(x) adv(x) (6) r: run(r), past(r), fast(r), argl(r, y) agenda now contains the entries in Words Cat Semantics John ran s(r) r: run(r), past(r), arg I (r, j), name(j, John) ran fast vp(r, j) r: run(r), past(r), fast(r), argl(r, j) ran quickly vp(r, j) r: run(r), past(r), fast(r), arg 1 (r, j) Assuming that adverbs modify verb phrases and not senthere will be no interactions when the ran is moved to the chart. the edge for fast moved, the possibility of creating the phrase fast quickly well as fast. are rejected, however, on the grounds that they would involve using a predicate from the original semantic specification more than once. This would be similar to allowing a given word to be covered by overlapping phrases in free word-order parsing. We proposed eliminating this by means of a bit vector and the same technique applies here. The fruitful interactions that occur here are fast quickly the one hand, and 201 on the other. Both give sentences whose semantics subsumes the entire input. Several things are noteworthy about the process just outlined. 1. Nothing turns on the fact that it uses a primitive version of event semantics. A scheme in which the indices were handles referring to subexpressions in any variety of flat semantics could have been treated in the same way. Indeed, more conventional formalisms with richly recursive syntax could be converted to this form on the fly. 2. Because all our rules are binary, we make no use of active edges. 3. While it fits the conception of chart parsing given at the beginning of this paper, our generator does not involve string positions centrally in the chart representation. In this respect, it differs from the proposal of Shieber (1988) which starts with all word edges leaving and entering a single vertex. But there is essentially no information in such a representation. Neither the chart nor any other special data structure is required to capture the fact that a new phrase may be constructible out of any given pair, and in either order, if they meet certain syntactic and semantic criteria. 4. Interactions must be considered explicitly between new edges and all edges currently in the chart, because no indexing is used to identify the existing edges that could interact with a given new one. 5. The process is exponential in the worst case because, if a sentence contains a word with k modifiers, then a it will be generated with each of the subsets of those modifiers, all but one of them being rejected when it is finally discovered that their semantics does not subsume the entire input. If the relative orders of the modifiers are unconstrained, matters only get worse. Points 4 and 5 are serious flaws in our scheme for which we shall describe remedies. Point 2 will have some importance for us because it will turn out that the indexing scheme we propose will require the use of distinct active and inactive edges, even when the rules are all binary. We take up the complexity issue first, and then turn to how the efficiency of the generation chart might be enhanced through indexing. 4 Internal and External Indices The exponential factor in the computational complexity of our generation algorithm is apparent in an example like (8). (8) Newspaper reports said the tall young Polish athlete ran fast The same set of predicates that generate this sentence clearly also generate the same sentence with deletion of all of the words young. a total of 8 strings. Each is generated in its entirety, though finally rejected because it fails to account for all of the semantic The words also be deleted independently giving a grand total of 32 strings. concentrate on the phrase young Polish athlete which we assumed would be combined with the verb phrase fast the rule (3). The distinguished index of the noun call it p, is identified with the variable the rule, but this variable is not associated with the syntactic category, s, on the left-hand side of the rule. The grammar has access to indices only through the variables that annotate grammatical categories in its rules, so that rules that incorporate this sentence into larger phrases can have no further to the index p. We therefore say that p is sentence tall young Polish athlete ran fast. The index p would, of course, also be internal to the young Polish athlete ran fast, the tall Polish ran fast, However, in these cases, the semantic material remaining to be expressed contains predicates that refer to this internal index, say tall(p)&apos; , and `young(p)&apos;. While the lexicon may have words to express these predicates, the grammar has no way of associating their referents with the above noun phrases because the variables corresponding to those referents are internal. We conclude that, as a matter of principle, no edge should be constructed if the result of doing so would be to make internal an index occurring in part of the input semantics that the new phrase does not subsume. In other words, the semantics of a phrase must contain all predicates from the input specification that refer to any indices internal to it. This strategy does not prevent the generation of an exponential number of variants of phrases containing modifiers. It limits proliferation of the ill effects, however, by allowing only the maximal one to be incorporated in larger phrases. In other words, if the final has phrases with respectively, then of the first and of the second will be created, but only one of each set will be incorporated into larger and no factor of will be introduced into the cost of the process. 5 Indexing String positions provide a natural way to index the strings input to the parsing process for the simple reason that there are as many of them as there are words but, for there to be any possibility of interaction between a pair of edges, they must come together at just one index. These are the natural points of articulation in the domain of strings. They cannot fill this role in generation because they are not natural properties of the semantic expressions that are the input to the process. The corresponding natural points of articulation in 202 flat semantic structures are the entities that we have already referring to as In the modified version of the procedure, whenever a new inactive edge is created with label B(b ...). then for all rules of the form in (9), an active edge is also created with label A(...)/C(c ...). A(...) ---&gt; ...) C(c ...) This represents a phrase of category A that requires a phrase of category Con the right for its completion. In these labels, (variables representing) the first, or distinassociated with B and C. By analogy with parsing charts, an inactive edge labeled B(b ...) can be of as from means simply it is efficiently accessible through the index active ...) be thought of as incident from, or through, the index key property of this scheme is that active and inactive edges interact by virtue of indices that they share and, by letting vertices correspond to indices, we collect together sets of edges that could interact. We illustrate the modified procedure with the sentence (10) whose semantics we will take to be (11), the grammar rules (12)-(14), and the lexical entries in (15). (10) The dog saw the cat. (11) dog(d), def(d), saw(s), past(s), cat(c), def(c), argl(s. d), arg2(s, c). (12) s(x) np(y) vp(x, y) (13) vp(x, --&gt; v(x, Y, z) np(z) (14) np(x) ---&gt; det(x) n(x) (15) Words Cat Semantics cat n(x) saw z) x: see(x), past(x), argl(x, y), arg2(x,z) dog n(x) the det(x) The procedure will be reminiscent of left-corner parsing. Arguments have been made in favor of a head-driven strategy which would, however, have been marginally more (e.g. in Kay (1989), Shieber, el. and the differences are, in any case, not germane to our current concerns. The initial agenda, including active edges, and collecting edges by the vertices that they are incident from, is given in (16). The grammar is consulted only for the purpose of creating active edges and all interactions in the chart are between active and inactive pairs of edges incident from the same vertex. (16) Vert Words Cat Semantics d the det(d) d: def(d) the np(d)/n(d) d: def(d) dog n(d) d: dog(d) s saw v(s, d, c) s: see(s). past(s), d), arg2(s, c saw vp(s, d)/np(c) r: see(s), past(s), argl(r, j) the det(c) c: def(c) the np(c)/n(c) c: def(c) cat n(c) c: dog(c) (17) Vert Words Cat Semantics d the dog np(d) d: dog(d), def(d) saw the cat vp(s, d)/np(d) s: see(s), past(s), argl(s, d), arg2(s, c), cat(c), def(c) c the cat np(c) c: cat(c), def(c) s saw the cat vp(s, d) s: see(s), past(s), argl(s, d), arg2(s, c), cat(c), def(c) Among the edges in (16), there are two interactions, one at vertices c and d. They cause the first and third edges in (17) to be added to the agenda. The first interacts with the active edge originally introduced by the verb &amp;quot;saw&amp;quot; producing the fourth entry in (17). The label on this edge matches the first item on the right-hand side of rule (12) and the active edge that we show in the second entry is also introduced. The final interaction is between the first and second edges in (17) which give rise to the edge in (18). This procedure confirms perfectly to the standard algorithm schema for chart parsing, especially in the version that makes predictions immediately following the recognition of the first constituent of a phrase, that is, in the version that is essentially a caching left-corner parser. 203 (18) Vert Words Cat Semantics s The dog saw the cat s(s) dog(d), def(d), see(s), past( s),argl(s , d), arg2(s, c), cat(c), def(c). 6 Acknowledgments Whatever there may be of value in this paper owes much to the interest, encouragement, and tolerance of my colleagues Marc Dymetman, Ronald Kaplan, John Maxwell, and Hadar Shem Toy. I am also indebted to the anonymous reviewers of this paper.</abstract>
<note confidence="0.909153555555556">References Copestake, A., Dan Flickinger, Robert Malouf, and Susanne Riehemann, and Ivan Sag (1996). Translation Minimal Recursion Semantics. of The Sixth International Conference on Theoretical and Method- Issues in Machine Translation, (in press). D. (1980). on Actions and Events. Clarendon Press. J. R. (1985). Ontological Promiscuity. Annual Meeting of the Association for Computational Lin- ACL. M. (1970). From Semantics to Syntax. Linguistics. Manfred, and K. E. Heidolf. The Hague, Mouton: 114-126. M. (1989). Head-driven Parsing. of on Parsing Technologies, PA. T. (1990). in the Semantics of English. Cambridge, Mass.: MIT Press.</note>
<title confidence="0.659326">Shieber, S. (1988). A Uniform Architecture for Parsing</title>
<author confidence="0.674474">John von_Neu-</author>
<affiliation confidence="0.483813">mann Society for Computing Sciences.</affiliation>
<author confidence="0.783369">A Semantic-Head-Driven</author>
<note confidence="0.8354962">Generation Algorithm for Unification Based Formalisms. 27th Annual Meeting of the Association for Computational B.C. Whitelock, P. (1992). Shake and-Bake Translation. 204</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>Whatever there may be of value in this paper owes much to the interest, encouragement, and tolerance of my colleagues Marc Dymetman,</title>
<location>Ronald Kaplan, John</location>
<marker></marker>
<rawString>Whatever there may be of value in this paper owes much to the interest, encouragement, and tolerance of my colleagues Marc Dymetman, Ronald Kaplan, John Maxwell, and Hadar Shem Toy. I am also indebted to the anonymous reviewers of this paper.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Copestake</author>
<author>Dan Flickinger</author>
<author>Robert Malouf</author>
<author>Susanne Riehemann</author>
<author>Ivan Sag</author>
</authors>
<title>Translation Using Minimal Recursion Semantics.</title>
<date>1996</date>
<booktitle>Proceedings of The Sixth International Conference on Theoretical and Methodological Issues in Machine Translation,</booktitle>
<location>Leuven</location>
<note>(in press).</note>
<marker>Copestake, Flickinger, Malouf, Riehemann, Sag, 1996</marker>
<rawString>Copestake, A., Dan Flickinger, Robert Malouf, and Susanne Riehemann, and Ivan Sag (1996). Translation Using Minimal Recursion Semantics. Proceedings of The Sixth International Conference on Theoretical and Methodological Issues in Machine Translation, Leuven (in press).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Davidson</author>
</authors>
<date>1980</date>
<booktitle>Essays on Actions and Events.</booktitle>
<publisher>The Clarendon Press.</publisher>
<location>Oxford:</location>
<contexts>
<context position="4754" citStr="Davidson, 1980" startWordPosition="782" endWordPosition="783">re significant facts about them. We know that it will not generally be possible to reduce 200 logical expressions to a canonical form but this does not mean that we should expect our generator to be compromised, or even greatly delayed, by trivial distinctions. Considerations of this kind were, in part, responsible for the recent resurgence of interest in &amp;quot;flat&amp;quot; representations of logical form (Copestake et a/.,I 996) and for the representations used for transfer in Shake-and-Bake translation (Whitelock, 1992). They have made semantic formalisms like those now usually associated with Davison (Davidson, 1980, Parsons, 1990) attractive in artificial intelligence for many years (Hobbs 1985, Kay, 1970). Operationally, the attraction is that the notations can be analyzed largely as free word-order languages in the manner outlined above. Consider the expression (I) (1) r: run(r), past(r), fast(r), argl(r, j), name(j, John) which we will take as a representation of the logical form of the sentences John ran fast and John ran quickly. It consists of a distinguished index (r) and a list of predicates whose relative order is immaterial. The distinguished index identifies this as a sentence that makes a cl</context>
</contexts>
<marker>Davidson, 1980</marker>
<rawString>Davidson, D. (1980). Essays on Actions and Events. Oxford: The Clarendon Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Hobbs</author>
</authors>
<date>1985</date>
<booktitle>Ontological Promiscuity. 23rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Chicago, ACL.</location>
<contexts>
<context position="4835" citStr="Hobbs 1985" startWordPosition="793" endWordPosition="794">duce 200 logical expressions to a canonical form but this does not mean that we should expect our generator to be compromised, or even greatly delayed, by trivial distinctions. Considerations of this kind were, in part, responsible for the recent resurgence of interest in &amp;quot;flat&amp;quot; representations of logical form (Copestake et a/.,I 996) and for the representations used for transfer in Shake-and-Bake translation (Whitelock, 1992). They have made semantic formalisms like those now usually associated with Davison (Davidson, 1980, Parsons, 1990) attractive in artificial intelligence for many years (Hobbs 1985, Kay, 1970). Operationally, the attraction is that the notations can be analyzed largely as free word-order languages in the manner outlined above. Consider the expression (I) (1) r: run(r), past(r), fast(r), argl(r, j), name(j, John) which we will take as a representation of the logical form of the sentences John ran fast and John ran quickly. It consists of a distinguished index (r) and a list of predicates whose relative order is immaterial. The distinguished index identifies this as a sentence that makes a claim about a running event. &amp;quot;John&amp;quot; is the name of the entity that stands in the `a</context>
</contexts>
<marker>Hobbs, 1985</marker>
<rawString>Hobbs, J. R. (1985). Ontological Promiscuity. 23rd Annual Meeting of the Association for Computational Linguistics, Chicago, ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kay</author>
</authors>
<title>From Semantics to Syntax. Progress in Linguistics. Bierwisch</title>
<date>1970</date>
<pages>114--126</pages>
<location>Mouton:</location>
<contexts>
<context position="4847" citStr="Kay, 1970" startWordPosition="795" endWordPosition="796">ical expressions to a canonical form but this does not mean that we should expect our generator to be compromised, or even greatly delayed, by trivial distinctions. Considerations of this kind were, in part, responsible for the recent resurgence of interest in &amp;quot;flat&amp;quot; representations of logical form (Copestake et a/.,I 996) and for the representations used for transfer in Shake-and-Bake translation (Whitelock, 1992). They have made semantic formalisms like those now usually associated with Davison (Davidson, 1980, Parsons, 1990) attractive in artificial intelligence for many years (Hobbs 1985, Kay, 1970). Operationally, the attraction is that the notations can be analyzed largely as free word-order languages in the manner outlined above. Consider the expression (I) (1) r: run(r), past(r), fast(r), argl(r, j), name(j, John) which we will take as a representation of the logical form of the sentences John ran fast and John ran quickly. It consists of a distinguished index (r) and a list of predicates whose relative order is immaterial. The distinguished index identifies this as a sentence that makes a claim about a running event. &amp;quot;John&amp;quot; is the name of the entity that stands in the `argl &apos; relati</context>
</contexts>
<marker>Kay, 1970</marker>
<rawString>Kay, M. (1970). From Semantics to Syntax. Progress in Linguistics. Bierwisch Manfred, and K. E. Heidolf. The Hague, Mouton: 114-126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kay</author>
</authors>
<title>Head-driven Parsing.</title>
<date>1989</date>
<booktitle>Proceedings of Workshop on Parsing Technologies,</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="15810" citStr="Kay (1989)" startWordPosition="2695" endWordPosition="2696">we will take to be (11), the grammar rules (12)-(14), and the lexical entries in (15). (10) The dog saw the cat. (11) dog(d), def(d), saw(s), past(s), cat(c), def(c), argl(s. d), arg2(s, c). (12) s(x) np(y) vp(x, y) (13) vp(x, --&gt; v(x, Y, z) np(z) (14) np(x) ---&gt; det(x) n(x) (15) Words Cat Semantics cat n(x) x: cat(x) saw v(x, y, z) x: see(x), past(x), argl(x, y), arg2(x,z) dog n(x) x: dog(x) the det(x) x: def(x) The procedure will be reminiscent of left-corner parsing. Arguments have been made in favor of a head-driven strategy which would, however, have been marginally more complex (e.g. in Kay (1989), Shieber, et el. (1989)) and the differences are, in any case, not germane to our current concerns. The initial agenda, including active edges, and collecting edges by the vertices that they are incident from, is given in (16). The grammar is consulted only for the purpose of creating active edges and all interactions in the chart are between active and inactive pairs of edges incident from the same vertex. (16) Vert Words Cat Semantics d the det(d) d: def(d) the np(d)/n(d) d: def(d) dog n(d) d: dog(d) s saw v(s, d, c) s: see(s). past(s), argl(s, d), arg2(s, c) c saw vp(s, d)/np(c) r: see(s),</context>
</contexts>
<marker>Kay, 1989</marker>
<rawString>Kay, M. (1989). Head-driven Parsing. Proceedings of Workshop on Parsing Technologies, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Parsons</author>
</authors>
<title>Events in the Semantics of English.</title>
<date>1990</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, Mass.:</location>
<contexts>
<context position="4770" citStr="Parsons, 1990" startWordPosition="784" endWordPosition="785">acts about them. We know that it will not generally be possible to reduce 200 logical expressions to a canonical form but this does not mean that we should expect our generator to be compromised, or even greatly delayed, by trivial distinctions. Considerations of this kind were, in part, responsible for the recent resurgence of interest in &amp;quot;flat&amp;quot; representations of logical form (Copestake et a/.,I 996) and for the representations used for transfer in Shake-and-Bake translation (Whitelock, 1992). They have made semantic formalisms like those now usually associated with Davison (Davidson, 1980, Parsons, 1990) attractive in artificial intelligence for many years (Hobbs 1985, Kay, 1970). Operationally, the attraction is that the notations can be analyzed largely as free word-order languages in the manner outlined above. Consider the expression (I) (1) r: run(r), past(r), fast(r), argl(r, j), name(j, John) which we will take as a representation of the logical form of the sentences John ran fast and John ran quickly. It consists of a distinguished index (r) and a list of predicates whose relative order is immaterial. The distinguished index identifies this as a sentence that makes a claim about a runn</context>
</contexts>
<marker>Parsons, 1990</marker>
<rawString>Parsons, T. (1990). Events in the Semantics of English. Cambridge, Mass.: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shieber</author>
</authors>
<title>A Uniform Architecture for Parsing and Generation.</title>
<date>1988</date>
<institution>von Neumann Society for Computing Sciences.</institution>
<location>COLING-88, Budapest, John</location>
<contexts>
<context position="9450" citStr="Shieber (1988)" startWordPosition="1592" endWordPosition="1593">that it uses a primitive version of event semantics. A scheme in which the indices were handles referring to subexpressions in any variety of flat semantics could have been treated in the same way. Indeed, more conventional formalisms with richly recursive syntax could be converted to this form on the fly. 2. Because all our rules are binary, we make no use of active edges. 3. While it fits the conception of chart parsing given at the beginning of this paper, our generator does not involve string positions centrally in the chart representation. In this respect, it differs from the proposal of Shieber (1988) which starts with all word edges leaving and entering a single vertex. But there is essentially no information in such a representation. Neither the chart nor any other special data structure is required to capture the fact that a new phrase may be constructible out of any given pair, and in either order, if they meet certain syntactic and semantic criteria. 4. Interactions must be considered explicitly between new edges and all edges currently in the chart, because no indexing is used to identify the existing edges that could interact with a given new one. 5. The process is exponential in th</context>
</contexts>
<marker>Shieber, 1988</marker>
<rawString>Shieber, S. (1988). A Uniform Architecture for Parsing and Generation. COLING-88, Budapest, John von Neumann Society for Computing Sciences.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Shieber</author>
</authors>
<title>A Semantic-Head-Driven Generation Algorithm for Unification Based Formalisms.</title>
<date>1989</date>
<booktitle>27th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Vancouver. B.C.</location>
<marker>Shieber, 1989</marker>
<rawString>Shieber, S. M. et al. (1989). A Semantic-Head-Driven Generation Algorithm for Unification Based Formalisms. 27th Annual Meeting of the Association for Computational Linguistics, Vancouver. B.C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Whitelock</author>
</authors>
<date>1992</date>
<booktitle>Shake and-Bake Translation. COLING-92,</booktitle>
<location>Nantes.</location>
<contexts>
<context position="4655" citStr="Whitelock, 1992" startWordPosition="768" endWordPosition="769"> forms, such as the relative order of the arguments to symmetric operators, on the same footing as more significant facts about them. We know that it will not generally be possible to reduce 200 logical expressions to a canonical form but this does not mean that we should expect our generator to be compromised, or even greatly delayed, by trivial distinctions. Considerations of this kind were, in part, responsible for the recent resurgence of interest in &amp;quot;flat&amp;quot; representations of logical form (Copestake et a/.,I 996) and for the representations used for transfer in Shake-and-Bake translation (Whitelock, 1992). They have made semantic formalisms like those now usually associated with Davison (Davidson, 1980, Parsons, 1990) attractive in artificial intelligence for many years (Hobbs 1985, Kay, 1970). Operationally, the attraction is that the notations can be analyzed largely as free word-order languages in the manner outlined above. Consider the expression (I) (1) r: run(r), past(r), fast(r), argl(r, j), name(j, John) which we will take as a representation of the logical form of the sentences John ran fast and John ran quickly. It consists of a distinguished index (r) and a list of predicates whose </context>
</contexts>
<marker>Whitelock, 1992</marker>
<rawString>Whitelock, P. (1992). Shake and-Bake Translation. COLING-92, Nantes.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>