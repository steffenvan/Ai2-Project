<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001772">
<title confidence="0.986717">
LEXenstein: A Framework for Lexical Simplification
</title>
<author confidence="0.96103">
Gustavo Henrique Paetzold and Lucia Specia
</author>
<affiliation confidence="0.984675">
Department of Computer Science
University of Sheffield, UK
</affiliation>
<email confidence="0.993149">
{ghpaetzold1,l.specia}@sheffield.ac.uk
</email>
<sectionHeader confidence="0.994716" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998824066666667">
Lexical Simplification consists in replac-
ing complex words in a text with sim-
pler alternatives. We introduce LEXen-
stein, the first open source framework for
Lexical Simplification. It covers all ma-
jor stages of the process and allows for
easy benchmarking of various approaches.
We test the tool’s performance and report
comparisons on different datasets against
the state of the art approaches. The re-
sults show that combining the novel Sub-
stitution Selection and Substitution Rank-
ing approaches introduced in LEXenstein
is the most effective approach to Lexical
Simplification.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.985738085714286">
The goal of a Lexical Simplification (LS) ap-
proach is to replace complex words and expres-
sions in a given text, often a sentence, with sim-
pler alternatives of equivalent meaning in context.
Although very intuitive, this is a challenging task
since the substitutions must preserve both the orig-
inal meaning and the grammaticality of the sen-
tence being simplified.
The LS task has been gaining significant atten-
tion since the late 1990’s, thanks to the positive
influence of the early work presented by (Devlin
and Tait, 1998) and (Carroll et al., 1999). More re-
cently, the LS task at SemEval-2012 (Specia et al.,
2012) has given LS wider visibility. Participants
had the opportunity to compare their approaches
in the task of ranking candidate substitutions, all
of which were already known to fit the context,
according to their “simplicity”.
Despite its growth in popularity, the inexistence
of tools to support the process and help researchers
to build upon has been hampering progress in the
area. We were only able to find one tool for LS: a
set of scripts designed for the training and testing
of ranking models provided by (Jauhar and Spe-
cia, 2012)1. However, they cover only one step
of the process. In an effort to tackle this issue,
we present LEXenstein: a framework for Lexical
Simplification development and benchmarking.
LEXenstein is an easy-to-use framework that
provides simplified access to many approaches
for several sub-tasks of the LS pipeline, which
is illustrated in Figure 1. Its current version in-
cludes methods for the three main sub-tasks in
the pipeline: Substitution Generation, Substitution
Selection and Substitution Ranking.
</bodyText>
<figureCaption confidence="0.99918">
Figure 1: Lexical Simplification Pipeline
</figureCaption>
<bodyText confidence="0.999659428571429">
LEXenstein was devised to facilitate per-
formance comparisons among various LS ap-
proaches, as well as the creation of new strategies
for LS. In the following Sections we present LEX-
enstein’s components (Section 2) and discuss the
results of several experiments conducted with the
tool (Section 3).
</bodyText>
<sectionHeader confidence="0.983791" genericHeader="method">
2 System Overview
</sectionHeader>
<bodyText confidence="0.994872">
LEXenstein is a Python library that provides sev-
eral approaches for sub-tasks in LS. To increase
its flexibility, the library is structured in six mod-
ules: Substitution Generation, Substitution Selec-
tion, Substitution Ranking, Feature Estimation,
Evaluation and Text Adorning. In the following
Sections, we describe them in more detail.
</bodyText>
<footnote confidence="0.991034">
1https://github.com/sjauhar/simplex
</footnote>
<page confidence="0.993647">
85
</page>
<note confidence="0.7589525">
Proceedings of ACL-IJCNLP 2015 System Demonstrations, pages 85–90,
Beijing, China, July 26-31, 2015. c�2015 ACL and AFNLP
</note>
<subsectionHeader confidence="0.99608">
2.1 Substitution Generation
</subsectionHeader>
<bodyText confidence="0.996934695652174">
We define Substitution Generation (SG) as the task
of producing candidate substitutions for complex
words, which is normally done regardless of the
context of the complex word. Previous work com-
monly addresses this task by querying general do-
main thesauri such as WordNet (Fellbaum, 1998),
or domain specific ones such as UMLS (Boden-
reider, 2004). Examples of work resorting to this
strategy are (Devlin and Tait, 1998) and (Carroll
et al., 1999). Recent work focuses on learning
substitutions from sentence-aligned parallel cor-
pora of complex-simple texts (Paetzold and Spe-
cia, 2013; Horn et al., 2014).
LEXenstein’s SG module offers support for five
approaches. All approaches use LEXenstein’s
Text Adorning module to create substitutions for
all possible inflections of verbs and nouns. Each
approach is represented by one of the following
Python classes:
KauchakGenerator (Horn et al., 2014) Auto-
matically extracts substitutions from parallel cor-
pora. It requires a set of tagged parallel sentences
and the word alignments between them in Pharaoh
format (Och and Ney, 2000). It produces a dictio-
nary of complex-to-simple substitutions filtered by
the criteria described in (Horn et al., 2014).
BiranGenerator (Biran et al., 2011) Filters
substitutions based on the Cartesian product be-
tween vocabularies of complex and simple words.
It requires vocabularies of complex and simple
words, as well as two language models trained
over complex and simple corpora. It produces a
dictionary linking words to a set of synonyms and
hypernyms filtered by the criteria described in (Bi-
ran et al., 2011).
YamamotoGenerator (Kajiwara et al., 2013)
Extracts substitutions from dictionary definitions
of complex words. It requires an API key for the
Merriam Dictionary2, which can be obtained for
free. It produces a dictionary linking words in the
Merriam Dictionary and WordNet to words with
the same Part-of-Speech (POS) tag in its entries’
definitions and examples of usage.
MerriamGenerator Extracts a dictionary link-
ing words to their synonyms, as listed in the Mer-
riam Thesaurus. It requires an API key.
</bodyText>
<footnote confidence="0.846263">
2http://www.dictionaryapi.com/
</footnote>
<bodyText confidence="0.970081">
WordnetGenerator Extracts a dictionary link-
ing words to their synonyms, as listed in WordNet.
</bodyText>
<subsectionHeader confidence="0.999608">
2.2 Substitution Selection
</subsectionHeader>
<bodyText confidence="0.999859625">
Substitution Selection (SS) is the task of selecting
which substitutions – from a given list – can re-
place a complex word in a given sentence with-
out altering its meaning. Most work addresses
this task referring to the context of the complex
word by employing Word Sense Disambiguation
(WSD) approaches (Sedding and Kazakov, 2004;
Nunes et al., 2013), or by discarding substitutions
which do not share the same POS tag of the target
complex word (Kajiwara et al., 2013; Paetzold and
Specia, 2013).
LEXenstein’s SS module provides access to
three approaches. All approaches require as input
a dictionary of substitutions generated by a given
approach and a dataset in the VICTOR format (as
in Victor Frankenstein (Shelley, 2007)). As out-
put, they produce a set of selected substitutions for
each entry in the VICTOR dataset. The VICTOR
format is structured as illustrated in Example 1,
where Si is the ith sentence in the dataset, wi a
target complex word in the hith position of Si, cji
a substitution candidate and rji its simplicity rank-
ing. Each bracketed component is separated by a
tabulation marker.
</bodyText>
<equation confidence="0.954687">
(S1) (w1) (h1) (r1 1:c1 )···(rn 1:cn 1)
1
...
(Sm) (wm) (hm) (r1 m:c1 )· · ·(rn m:cn m)
m
</equation>
<bodyText confidence="0.999793277777778">
LEXenstein includes two resources for train-
ing/testing in the VICTOR format: the LexMTurk
(Horn et al., 2014) and the SemEval corpus (Spe-
cia et al., 2012). Each approach in the SS mod-
ule is represented by one of the following Python
classes:
WSDSelector Allows for the user to use one
among various classic WSD approaches in SS. It
requires the PyWSD (Tan, 2014) module to be in-
stalled, which includes the approaches presented
by (Lesk, 1986) and (Wu and Palmer, 1994), as
well as baselines such as random and first senses.
BiranSelector (Biran et al., 2011) Employs a
strategy in which a word co-occurrence model is
used to determine which substitutions have mean-
ing similar to that of a target complex word. It
requires a plain text file with each line in the for-
mat specified in Example 2, where (wi) is a word,
</bodyText>
<equation confidence="0.884976666666667">
�
� �
1 (1)
</equation>
<page confidence="0.887012">
86
</page>
<bodyText confidence="0.9340855">
(cij &gt; a co-occurring word and (fij &gt; its frequency
of occurrence.
</bodyText>
<equation confidence="0.994973">
(wi) �c0 ):�f0 ) ··· (cn i ):(fn i ) (2)
i i
</equation>
<bodyText confidence="0.999157842105263">
Each component in the format in 2 must be
separated by a tabulation marker. Given such a
model, the approach filters all substitutions which
are estimated to be more complex than the tar-
get word, and also those for which the distance
between their co-occurrence vector and the target
sentence’s vector is higher than a threshold set by
the user.
WordVectorSelector Employs a novel strategy,
in which a word vector model is used to deter-
mine which substitutions have the closest mean-
ing to that of the sentence being simplified. It
requires a binary word vector model produced by
Word2Vec3, and can be configured in many ways.
It retrieves a user-defined percentage of the substi-
tutions, which are ranked with respect to the co-
sine distance between their word vector and the
sum of some or all of the sentences’ words, de-
pending on the settings defined by the user.
</bodyText>
<subsectionHeader confidence="0.999666">
2.3 Substitution Ranking
</subsectionHeader>
<bodyText confidence="0.999817227272727">
Substitution Ranking (SR) is the task of ranking a
set of selected substitutions for a target complex
word with respect to their simplicity. Approaches
vary from simple word length and frequency-
based measures (Devlin and Tait, 1998; Carroll et
al., 1998; Carroll et al., 1999; Biran et al., 2011)
to more sophisticated linear combinations of scor-
ing functions (Jauhar and Specia, 2012), as well as
machine learning-based approaches (Horn et al.,
2014).
LEXenstein’s SR module provides access to
three approaches. All approaches receive as in-
put datasets in the VICTOR format, which can be
either training/testing datasets already containing
only valid substitutions in context, or datasets gen-
erated with (potentially noisy) substitutions by a
given SS approach. They also require as input a
FeatureEstimator object to calculate feature values
describing the candidate substitutes. More details
on the FeatureEstimator class are provided in Sec-
tion 2.4. Each approach in the SR module is rep-
resented by one of the following Python classes:
</bodyText>
<footnote confidence="0.797867">
3https://code.google.com/p/word2vec/
</footnote>
<bodyText confidence="0.999779928571429">
MetricRanker Employs a simple ranking strat-
egy based on the values of a single feature pro-
vided by the user. By configuring the input Fea-
tureEstimator object, the user can calculate values
of several features for the candidates in a given
dataset and easily rank the candidates according
to each of these features.
SVMRanker (Joachims, 2002) Use Support
Vector Machines in a setup that minimises a loss
function with respect to a ranking model. This
strategy is the one employed in the LS experiments
of (Horn et al., 2014), yielding promising results.
The user needs to provide a path to their SVM-
Rank installation, as well as SVM-related configu-
rations, such as the kernel type and parameter val-
ues for C, epsilon, etc.
BoundaryRanker Employs a novel strategy, in
which ranking is framed as a binary classification
task. During training, this approach assigns the la-
bel 1 to all candidates of rank 1 &gt; r &gt; p, where
p is a range set by the user, and 0 to the remain-
ing candidates. It then trains a stochastic descent
linear classifier based on the features specified in
the FeatureEstimator object. During testing, can-
didate substitutions are ranked based on how far
from 0 they are. This ranker allows the user to
provide several parameters during training, such
as loss function and penalty type.
</bodyText>
<subsectionHeader confidence="0.97473">
2.4 Feature Estimation
</subsectionHeader>
<bodyText confidence="0.9610367">
LEXenstein’s Feature Estimation module allows
the calculation of several features for LS-related
tasks. Its class FeatureEstimator allows the user
to select and configure many features commonly
used by LS approaches.
The FeatureEstimator object can be used either
for the creation of LEXenstein’s rankers, or in
stand-alone setups. For the latter, the class pro-
vides a function called calculateFeatures, which
produces a matrix MxN containing M feature
values for each of the N substitution candidates
listed in the dataset. Each of the 11 features sup-
ported must be configured individually. They can
be grouped in four categories:
Lexicon-oriented: Binary features which re-
ceive value 1 if a candidate appears in a given vo-
cabulary, and 0 otherwise.
Morphological: Features that exploit morpho-
logical characteristics of substitutions, such as
word length and number of syllables.
</bodyText>
<page confidence="0.993961">
87
</page>
<figure confidence="0.2432944">
Collocational: N-gram probabilities of the form
P (Sh−l c Sh+i) , where c is a candidate substi-
tution in the hth position in sentence S, and Sh−l
h−1
and Sh+r
</figure>
<bodyText confidence="0.932642">
h+1 are n-grams of size l and r, respectively.
Sense-oriented: Several features which are re-
lated to the meaning of a candidate substitution
such as number of senses, lemmas, synonyms, hy-
pernyms, hyponyms and maximum and minimum
distances among all of its senses.
</bodyText>
<subsectionHeader confidence="0.932464">
2.5 Evaluation
</subsectionHeader>
<bodyText confidence="0.999651666666667">
Since one of the goals of LEXenstein is to facili-
tate the benchmarking LS approaches, it is crucial
that it provides evaluation methods. This module
includes functions for the evaluation of all sub-
tasks, both individually and in combination. It
contains four Python classes:
GeneratorEvaluator: Provides evaluation met-
rics for SG methods. It requires a gold-standard
in the VICTOR format and a set of generated sub-
stitutions. It returns the Potential, Precision and
F-measure, where Potential is the proportion of in-
stances for which at least one of the substitutions
generated is present in the gold-standard, Preci-
sion the proportion of generated instances which
are present in the gold-standard, and F-measure
their harmonic mean.
SelectorEvaluator: Provides evaluation metrics
for SS methods. It requires a gold-standard in
the VICTOR format and a set of selected substi-
tutions. It returns the Potential, Precision and F-
measure of the SS approach, as defined above.
RankerEvaluator: Provides evaluation metrics
for SR methods. It requires a gold-standard in the
VICTOR format and a set of ranked substitutions.
It returns the TRank-at-1:3 and Recall-at-1:3 met-
rics (Specia et al., 2012), where Trank-at-i is the
proportion of instances for which a candidate of
gold-rank r &lt; i was ranked first, and Recall-at-i
the proportion of candidates of gold-rank r &lt; i
that are ranked in positions p &lt; i.
PipelineEvaluator: Provides evaluation metrics
for the entire LS pipeline. It requires as input
a gold-standard in the VICTOR format and a set
of ranked substitutions which have been gener-
ated and selected by a given set of approaches. It
returns the approaches’ Precision, Accuracy and
Change Proportion, where Precision is the pro-
portion of instances for which the highest ranking
substitution is not the target complex word itself
and is in the gold-standard, Accuracy is the pro-
portion of instances for which the highest ranking
substitution is in the gold-standard, and Change
Proportion is the proportion of instances for which
the highest ranking substitution is not the target
complex word itself.
</bodyText>
<subsectionHeader confidence="0.99386">
2.6 Text Adorning
</subsectionHeader>
<bodyText confidence="0.999946142857143">
This approach provides a Python interface to the
Morph Adorner Toolkit (Paetzold, 2015), a set
of Java tools that facilitates the access to Morph
Adorner’s functionalities. The class provides easy
access to word lemmatisation, word stemming,
syllable splitting, noun inflection, verb tensing and
verb conjugation.
</bodyText>
<subsectionHeader confidence="0.798474">
2.7 Resources
</subsectionHeader>
<bodyText confidence="0.999892714285714">
LEXenstein also provides a wide array of re-
sources for the user to explore in benchmarking
tasks. Among them are the aforementioned LexM-
turk and SemEval corpora in the VICTOR format,
lists of stop and basic words, as well as language
models and lexica built over Wikipedia and Sim-
ple Wikipedia.
</bodyText>
<sectionHeader confidence="0.999462" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.998117">
In this Section, we discuss the results obtained in
four benchmarking experiments.
</bodyText>
<subsectionHeader confidence="0.9998">
3.1 Substitution Generation
</subsectionHeader>
<bodyText confidence="0.974735631578947">
In this experiment we evaluate all SG approaches
in LEXenstein. For the KauchakGenerator, we
use the corpus provided by (Kauchak, 2013), com-
posed of 150, 569 complex-to-simple parallel sen-
tences, parsed by the Stanford Parser (Klein and
Manning, 1965). From the the same corpus, we
build the required vocabularies and language mod-
els for the BiranGenerator. We used the LexMturk
dataset as the gold-standard (Horn et al., 2014),
which is composed by 500 sentences, each with
a single target complex word and 50 substitutions
suggested by turkers. The results are presented in
Table 1.
The results in Table 1 show that the method of
(Horn et al., 2014) yields the best F-Measure re-
sults, although combining the output of all gener-
ation methods yields the highest Potential. This
shows that using parallel corpora to generate sub-
stitution candidates for complex words can be a
</bodyText>
<page confidence="0.998354">
88
</page>
<table confidence="0.999918">
Approach Pot. Prec. F
Kauchak 0.830 0.155 0.262
Wordnet 0.608 0.109 0.184
Biran 0.630 0.102 0.175
Merriam 0.540 0.067 0.120
Yamamoto 0.504 0.054 0.098
All 0.976 0.066 0.124
</table>
<tableCaption confidence="0.999782">
Table 1: SG benchmarking results
</tableCaption>
<bodyText confidence="0.999700833333333">
more efficient strategy than querying dictionaries
and databases. We must, however, keep in mind
that the sentences that compose the LexMturk cor-
pus were extracted from Wikipedia, which is the
same corpus from which the KauchakGenerator
learns substitutions.
</bodyText>
<subsectionHeader confidence="0.999827">
3.2 Substitution Selection
</subsectionHeader>
<bodyText confidence="0.995224769230769">
Here we evaluate of all SS approaches in LEX-
enstein. For the BiranSelector, we trained a co-
occurrence model over a corpus of 6+ billion
words extracted from the various sources sug-
gested in the Word2Vec documentation4, the same
sources over which the word vector model re-
quired by the WordVectorSelector was trained. In
order to summarise the results, we present the
scores obtained only with the best performing con-
figurations of each approach. The LexMturk cor-
pus is used as the gold-standard, and the initial set
of substitutions is the one produced by all SG ap-
proaches combined. The results are presented in
</bodyText>
<tableCaption confidence="0.975622">
Table 2.
</tableCaption>
<table confidence="0.999938125">
Approach Pot. Prec. F Size
Word Vec. 0.768 0.219 0.341 3,042
Biran 0.508 0.078 0.136 9,680
First 0.176 0.045 0.072 2,471
Lesk 0.246 0.041 0.070 4,716
Random 0.082 0.023 0.035 2,046
Wu-Pa 0.038 0.013 0.020 1,749
No Sel. 0.976 0.066 0.124 26,516
</table>
<tableCaption confidence="0.99843">
Table 2: SS benchmarking results
</tableCaption>
<bodyText confidence="0.999654">
“Size” in Table 2 represents the total number of
substitutions selected for all test instances. The
results in Table 2 show that our novel word vector
approach outperforms all others in F-Measure by a
considerable margin, including the method of not
performing selection at all. Note that not perform-
ing selection allows for Potential to be higher, but
</bodyText>
<footnote confidence="0.969817">
4https://code.google.com/p/word2vec/
</footnote>
<note confidence="0.330589">
yields very poor Precision.
</note>
<subsectionHeader confidence="0.998591">
3.3 Substitution Ranking
</subsectionHeader>
<bodyText confidence="0.999727538461538">
In Table 3 we present the results of the evaluation
of several SR approaches. We trained the SVM-
Ranker with features similar to the ones used in
(Horn et al., 2014), and the BoundaryRanker with
a set of 10 features selected through univariate
feature selection. We compare these approaches
to three baseline Metric Rankers, which use the
word’s frequency in Simple Wikipedia, its length
or its number of senses. The SemEval corpus is
used as the gold-standard so that we can compare
our results with the best one obtained at SemEval-
2012 (Jauhar and Specia, 2012) (SemEval, in Ta-
ble 3).
</bodyText>
<table confidence="0.999899">
Approach TR-1 Rec-1 Rec-2 Rec-3
Boundary 0.655 0.608 0.602 0.663
SVM 0.486 0.451 0.502 0.592
Freq. 0.226 0.220 0.236 0.300
Length 0.180 0.175 0.200 0.261
Senses 0.130 0.126 0.161 0.223
SemEval 0.602 0.575 0.689 0.769
</table>
<tableCaption confidence="0.999388">
Table 3: SR benchmarking results
</tableCaption>
<bodyText confidence="0.9999515">
The novel Boundary ranking approach outper-
forms all other approaches in both TRank-at-1
and Recall-at-1 by a considerable margin, but it
is worse than the best SemEval-2012 approach in
terms of Recall-at-2 and 3. This however reveals
not a limitation but a strength of our approach:
since the Boundary ranker focuses on placing the
best substitution in the highest rank, it becomes
more effective at doing so as opposed to at pro-
ducing a full ranking for all candidates.
</bodyText>
<subsectionHeader confidence="0.884975">
3.4 Round-Trip Evaluation
</subsectionHeader>
<bodyText confidence="0.999844642857143">
In this experiment we evaluate the performance of
different combinations of SS and SR approaches
in selecting suitable substitutions for complex
words from the ones produced by all generators
combined. Rankers and selectors are configured
in the same way as they were in the experiments
in Sections 3.3 and 3.2. The gold-standard used is
LexMturk, and the performance metric used is the
combination’s Precision: the proportion of times
in which the candidate ranked highest is not the
target complex word itself and belongs to the gold-
standard list. Results are shown in Table 4.
The results show that combining the Word-
VectorSelector with the BoundaryRanker yields
</bodyText>
<page confidence="0.998448">
89
</page>
<table confidence="0.9980315">
No Sel. Word Vector Biran
Boundary 0.342 0.550 0.197
SVM 0.108 0.219 0.003
Freq. 0.114 0.501 0.096
Length 0.120 0.408 0.092
Senses 0.214 0.448 0.122
</table>
<tableCaption confidence="0.999428">
Table 4: Round-trip benchmarking results
</tableCaption>
<bodyText confidence="0.998921">
the highest performance in the pipeline evalua-
tion. Interestingly, the SVMRanker, which per-
formed very well in the individual evaluation of
Section 3.3, was outperformed by all three base-
lines in this experiment.
</bodyText>
<sectionHeader confidence="0.998761" genericHeader="method">
4 Final Remarks
</sectionHeader>
<bodyText confidence="0.9999892">
We have presented LEXenstein, a framework for
Lexical Simplification distributed under the per-
missive BSD license. It provides a wide arrange
of useful resources and tools for the task, such
as feature estimators, text adorners, and various
approaches for Substitution Generation, Selection
and Ranking. These include methods from pre-
vious work, as well as novel approaches. LEX-
enstein’s modular structure also allows for one to
easily add new approaches to it.
We have conducted evaluation experiments in-
cluding various LS approaches in the literature.
Our results show that the novel approaches intro-
duced in this paper outperform those from previ-
ous work. In the future, we intend to incorporate in
LEXenstein approaches for Complex Word Iden-
tification, as well as more approaches for the re-
maining tasks of the usual LS pipeline.
The tool can be downloaded from: http://
ghpaetzold.github.io/LEXenstein/.
</bodyText>
<sectionHeader confidence="0.998984" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999695661016949">
O. Biran, S. Brody, and N. Elhadad. 2011. Putting it
Simply: a Context-Aware Approach to Lexical Sim-
plification. The 49th Annual Meeting of the ACL.
O. Bodenreider. 2004. The unified medical language
system (umls): integrating biomedical terminology.
Nucleic acids research.
J. Carroll, G. Minnen, Y. Canning, S. Devlin, and
J. Tait. 1998. Practical simplification of english
newspaper text to assist aphasic readers. In The 15th
AAAI.
J. Carroll, G. Minnen, D. Pearce, Y. Canning, S. De-
vlin, and J. Tait. 1999. Simplifying Text for Lan-
guage Impaired Readers. The 9th EACL.
S. Devlin and J. Tait. 1998. The use of a psy-
cholinguistic database in the simplification of text
for aphasic readers. Linguistic Databases.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. Bradford Books.
C. Horn, C. Manduca, and D. Kauchak. 2014. Learn-
ing a Lexical Simplifier Using Wikipedia. The 52nd
Annual Meeting of the ACL.
S.K. Jauhar and L. Specia. 2012. UOW-SHEF:
SimpLex–lexical simplicity ranking based on con-
textual and psycholinguistic features. The 1st *SEM.
T. Joachims. 2002. Optimizing search engines using
clickthrough data. In The 8th ACM.
T. Kajiwara, H. Matsumoto, and K. Yamamoto. 2013.
Selecting Proper Lexical Paraphrase for Children.
D. Kauchak. 2013. Improving Text Simplification
Language Modeling Using Unsimplified Text Data.
The 51st Annual Meeting of the ACL.
D. Klein and C.D. Manning. 1965. Accurate Unlexi-
calized Parsing. In The 41st Annual Meeting ofACL.
M. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In The 5th SIGDOC.
B.P. Nunes, R. Kawase, P. Siehndel, M.A. Casanova,
and S. Dietze. 2013. As Simple as It Gets - A Sen-
tence Simplifier for Different Learning Levels and
Contexts. IEEE 13th ICALT.
F.J. Och and H. Ney. 2000. Improved statistical align-
ment models. In The 38th Annual Meeting of the
ACL.
G.H. Paetzold and L. Specia. 2013. Text simplification
as tree transduction. In The 9th STIL.
G.H. Paetzold. 2015. Morph adorner
toolkit: Morph adorner made simple.
http://ghpaetzold.github.io/MorphAdornerToolkit/.
J. Sedding and D. Kazakov. 2004. Wordnet-based text
document clustering. In The 3rd ROMAND.
M. Shelley. 2007. Frankenstein. Pearson Education.
L. Specia, S.K. Jauhar, and R. Mihalcea. 2012.
Semeval-2012 task 1: English lexical simplification.
In The 1st *SEM.
L. Tan. 2014. Pywsd: Python implementa-
tions of word sense disambiguation technologies.
https://github.com/alvations/pywsd.
Z. Wu and M. Palmer. 1994. Verbs semantics and lex-
ical selection. In The 32nd Annual Meeting of ACL.
</reference>
<page confidence="0.998626">
90
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.498301">
<title confidence="0.999892">LEXenstein: A Framework for Lexical Simplification</title>
<author confidence="0.9907">Henrique Paetzold</author>
<affiliation confidence="0.9998255">Department of Computer University of Sheffield,</affiliation>
<abstract confidence="0.965595375">Lexical Simplification consists in replacing complex words in a text with simpler alternatives. We introduce LEXenstein, the first open source framework for Lexical Simplification. It covers all major stages of the process and allows for easy benchmarking of various approaches. We test the tool’s performance and report comparisons on different datasets against the state of the art approaches. The results show that combining the novel Substitution Selection and Substitution Ranking approaches introduced in LEXenstein is the most effective approach to Lexical Simplification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>O Biran</author>
<author>S Brody</author>
<author>N Elhadad</author>
</authors>
<title>Putting it Simply: a Context-Aware Approach to Lexical Simplification.</title>
<date>2011</date>
<booktitle>The 49th Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="4569" citStr="Biran et al., 2011" startWordPosition="692" endWordPosition="695">n’s SG module offers support for five approaches. All approaches use LEXenstein’s Text Adorning module to create substitutions for all possible inflections of verbs and nouns. Each approach is represented by one of the following Python classes: KauchakGenerator (Horn et al., 2014) Automatically extracts substitutions from parallel corpora. It requires a set of tagged parallel sentences and the word alignments between them in Pharaoh format (Och and Ney, 2000). It produces a dictionary of complex-to-simple substitutions filtered by the criteria described in (Horn et al., 2014). BiranGenerator (Biran et al., 2011) Filters substitutions based on the Cartesian product between vocabularies of complex and simple words. It requires vocabularies of complex and simple words, as well as two language models trained over complex and simple corpora. It produces a dictionary linking words to a set of synonyms and hypernyms filtered by the criteria described in (Biran et al., 2011). YamamotoGenerator (Kajiwara et al., 2013) Extracts substitutions from dictionary definitions of complex words. It requires an API key for the Merriam Dictionary2, which can be obtained for free. It produces a dictionary linking words in</context>
<context position="7342" citStr="Biran et al., 2011" startWordPosition="1149" endWordPosition="1152">rn 1:cn 1) 1 ... (Sm) (wm) (hm) (r1 m:c1 )· · ·(rn m:cn m) m LEXenstein includes two resources for training/testing in the VICTOR format: the LexMTurk (Horn et al., 2014) and the SemEval corpus (Specia et al., 2012). Each approach in the SS module is represented by one of the following Python classes: WSDSelector Allows for the user to use one among various classic WSD approaches in SS. It requires the PyWSD (Tan, 2014) module to be installed, which includes the approaches presented by (Lesk, 1986) and (Wu and Palmer, 1994), as well as baselines such as random and first senses. BiranSelector (Biran et al., 2011) Employs a strategy in which a word co-occurrence model is used to determine which substitutions have meaning similar to that of a target complex word. It requires a plain text file with each line in the format specified in Example 2, where (wi) is a word, � � � 1 (1) 86 (cij &gt; a co-occurring word and (fij &gt; its frequency of occurrence. (wi) �c0 ):�f0 ) ··· (cn i ):(fn i ) (2) i i Each component in the format in 2 must be separated by a tabulation marker. Given such a model, the approach filters all substitutions which are estimated to be more complex than the target word, and also those for w</context>
<context position="8912" citStr="Biran et al., 2011" startWordPosition="1430" endWordPosition="1433">produced by Word2Vec3, and can be configured in many ways. It retrieves a user-defined percentage of the substitutions, which are ranked with respect to the cosine distance between their word vector and the sum of some or all of the sentences’ words, depending on the settings defined by the user. 2.3 Substitution Ranking Substitution Ranking (SR) is the task of ranking a set of selected substitutions for a target complex word with respect to their simplicity. Approaches vary from simple word length and frequencybased measures (Devlin and Tait, 1998; Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011) to more sophisticated linear combinations of scoring functions (Jauhar and Specia, 2012), as well as machine learning-based approaches (Horn et al., 2014). LEXenstein’s SR module provides access to three approaches. All approaches receive as input datasets in the VICTOR format, which can be either training/testing datasets already containing only valid substitutions in context, or datasets generated with (potentially noisy) substitutions by a given SS approach. They also require as input a FeatureEstimator object to calculate feature values describing the candidate substitutes. More details o</context>
</contexts>
<marker>Biran, Brody, Elhadad, 2011</marker>
<rawString>O. Biran, S. Brody, and N. Elhadad. 2011. Putting it Simply: a Context-Aware Approach to Lexical Simplification. The 49th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Bodenreider</author>
</authors>
<title>The unified medical language system (umls): integrating biomedical terminology. Nucleic acids research.</title>
<date>2004</date>
<contexts>
<context position="3682" citStr="Bodenreider, 2004" startWordPosition="559" endWordPosition="561">ion and Text Adorning. In the following Sections, we describe them in more detail. 1https://github.com/sjauhar/simplex 85 Proceedings of ACL-IJCNLP 2015 System Demonstrations, pages 85–90, Beijing, China, July 26-31, 2015. c�2015 ACL and AFNLP 2.1 Substitution Generation We define Substitution Generation (SG) as the task of producing candidate substitutions for complex words, which is normally done regardless of the context of the complex word. Previous work commonly addresses this task by querying general domain thesauri such as WordNet (Fellbaum, 1998), or domain specific ones such as UMLS (Bodenreider, 2004). Examples of work resorting to this strategy are (Devlin and Tait, 1998) and (Carroll et al., 1999). Recent work focuses on learning substitutions from sentence-aligned parallel corpora of complex-simple texts (Paetzold and Specia, 2013; Horn et al., 2014). LEXenstein’s SG module offers support for five approaches. All approaches use LEXenstein’s Text Adorning module to create substitutions for all possible inflections of verbs and nouns. Each approach is represented by one of the following Python classes: KauchakGenerator (Horn et al., 2014) Automatically extracts substitutions from parallel</context>
</contexts>
<marker>Bodenreider, 2004</marker>
<rawString>O. Bodenreider. 2004. The unified medical language system (umls): integrating biomedical terminology. Nucleic acids research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>G Minnen</author>
<author>Y Canning</author>
<author>S Devlin</author>
<author>J Tait</author>
</authors>
<title>Practical simplification of english newspaper text to assist aphasic readers.</title>
<date>1998</date>
<booktitle>In The 15th AAAI.</booktitle>
<contexts>
<context position="8869" citStr="Carroll et al., 1998" startWordPosition="1422" endWordPosition="1425">ied. It requires a binary word vector model produced by Word2Vec3, and can be configured in many ways. It retrieves a user-defined percentage of the substitutions, which are ranked with respect to the cosine distance between their word vector and the sum of some or all of the sentences’ words, depending on the settings defined by the user. 2.3 Substitution Ranking Substitution Ranking (SR) is the task of ranking a set of selected substitutions for a target complex word with respect to their simplicity. Approaches vary from simple word length and frequencybased measures (Devlin and Tait, 1998; Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011) to more sophisticated linear combinations of scoring functions (Jauhar and Specia, 2012), as well as machine learning-based approaches (Horn et al., 2014). LEXenstein’s SR module provides access to three approaches. All approaches receive as input datasets in the VICTOR format, which can be either training/testing datasets already containing only valid substitutions in context, or datasets generated with (potentially noisy) substitutions by a given SS approach. They also require as input a FeatureEstimator object to calculate feature values describin</context>
</contexts>
<marker>Carroll, Minnen, Canning, Devlin, Tait, 1998</marker>
<rawString>J. Carroll, G. Minnen, Y. Canning, S. Devlin, and J. Tait. 1998. Practical simplification of english newspaper text to assist aphasic readers. In The 15th AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>G Minnen</author>
<author>D Pearce</author>
<author>Y Canning</author>
<author>S Devlin</author>
<author>J Tait</author>
</authors>
<title>Simplifying Text for Language Impaired Readers. The 9th EACL.</title>
<date>1999</date>
<contexts>
<context position="1347" citStr="Carroll et al., 1999" startWordPosition="202" endWordPosition="205">the most effective approach to Lexical Simplification. 1 Introduction The goal of a Lexical Simplification (LS) approach is to replace complex words and expressions in a given text, often a sentence, with simpler alternatives of equivalent meaning in context. Although very intuitive, this is a challenging task since the substitutions must preserve both the original meaning and the grammaticality of the sentence being simplified. The LS task has been gaining significant attention since the late 1990’s, thanks to the positive influence of the early work presented by (Devlin and Tait, 1998) and (Carroll et al., 1999). More recently, the LS task at SemEval-2012 (Specia et al., 2012) has given LS wider visibility. Participants had the opportunity to compare their approaches in the task of ranking candidate substitutions, all of which were already known to fit the context, according to their “simplicity”. Despite its growth in popularity, the inexistence of tools to support the process and help researchers to build upon has been hampering progress in the area. We were only able to find one tool for LS: a set of scripts designed for the training and testing of ranking models provided by (Jauhar and Specia, 20</context>
<context position="3782" citStr="Carroll et al., 1999" startWordPosition="575" endWordPosition="578">b.com/sjauhar/simplex 85 Proceedings of ACL-IJCNLP 2015 System Demonstrations, pages 85–90, Beijing, China, July 26-31, 2015. c�2015 ACL and AFNLP 2.1 Substitution Generation We define Substitution Generation (SG) as the task of producing candidate substitutions for complex words, which is normally done regardless of the context of the complex word. Previous work commonly addresses this task by querying general domain thesauri such as WordNet (Fellbaum, 1998), or domain specific ones such as UMLS (Bodenreider, 2004). Examples of work resorting to this strategy are (Devlin and Tait, 1998) and (Carroll et al., 1999). Recent work focuses on learning substitutions from sentence-aligned parallel corpora of complex-simple texts (Paetzold and Specia, 2013; Horn et al., 2014). LEXenstein’s SG module offers support for five approaches. All approaches use LEXenstein’s Text Adorning module to create substitutions for all possible inflections of verbs and nouns. Each approach is represented by one of the following Python classes: KauchakGenerator (Horn et al., 2014) Automatically extracts substitutions from parallel corpora. It requires a set of tagged parallel sentences and the word alignments between them in Pha</context>
<context position="8891" citStr="Carroll et al., 1999" startWordPosition="1426" endWordPosition="1429">ary word vector model produced by Word2Vec3, and can be configured in many ways. It retrieves a user-defined percentage of the substitutions, which are ranked with respect to the cosine distance between their word vector and the sum of some or all of the sentences’ words, depending on the settings defined by the user. 2.3 Substitution Ranking Substitution Ranking (SR) is the task of ranking a set of selected substitutions for a target complex word with respect to their simplicity. Approaches vary from simple word length and frequencybased measures (Devlin and Tait, 1998; Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011) to more sophisticated linear combinations of scoring functions (Jauhar and Specia, 2012), as well as machine learning-based approaches (Horn et al., 2014). LEXenstein’s SR module provides access to three approaches. All approaches receive as input datasets in the VICTOR format, which can be either training/testing datasets already containing only valid substitutions in context, or datasets generated with (potentially noisy) substitutions by a given SS approach. They also require as input a FeatureEstimator object to calculate feature values describing the candidate substi</context>
</contexts>
<marker>Carroll, Minnen, Pearce, Canning, Devlin, Tait, 1999</marker>
<rawString>J. Carroll, G. Minnen, D. Pearce, Y. Canning, S. Devlin, and J. Tait. 1999. Simplifying Text for Language Impaired Readers. The 9th EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Devlin</author>
<author>J Tait</author>
</authors>
<title>The use of a psycholinguistic database in the simplification of text for aphasic readers. Linguistic Databases.</title>
<date>1998</date>
<journal>C. Fellbaum.</journal>
<publisher>Bradford Books.</publisher>
<contexts>
<context position="1320" citStr="Devlin and Tait, 1998" startWordPosition="197" endWordPosition="200">introduced in LEXenstein is the most effective approach to Lexical Simplification. 1 Introduction The goal of a Lexical Simplification (LS) approach is to replace complex words and expressions in a given text, often a sentence, with simpler alternatives of equivalent meaning in context. Although very intuitive, this is a challenging task since the substitutions must preserve both the original meaning and the grammaticality of the sentence being simplified. The LS task has been gaining significant attention since the late 1990’s, thanks to the positive influence of the early work presented by (Devlin and Tait, 1998) and (Carroll et al., 1999). More recently, the LS task at SemEval-2012 (Specia et al., 2012) has given LS wider visibility. Participants had the opportunity to compare their approaches in the task of ranking candidate substitutions, all of which were already known to fit the context, according to their “simplicity”. Despite its growth in popularity, the inexistence of tools to support the process and help researchers to build upon has been hampering progress in the area. We were only able to find one tool for LS: a set of scripts designed for the training and testing of ranking models provide</context>
<context position="3755" citStr="Devlin and Tait, 1998" startWordPosition="570" endWordPosition="573"> more detail. 1https://github.com/sjauhar/simplex 85 Proceedings of ACL-IJCNLP 2015 System Demonstrations, pages 85–90, Beijing, China, July 26-31, 2015. c�2015 ACL and AFNLP 2.1 Substitution Generation We define Substitution Generation (SG) as the task of producing candidate substitutions for complex words, which is normally done regardless of the context of the complex word. Previous work commonly addresses this task by querying general domain thesauri such as WordNet (Fellbaum, 1998), or domain specific ones such as UMLS (Bodenreider, 2004). Examples of work resorting to this strategy are (Devlin and Tait, 1998) and (Carroll et al., 1999). Recent work focuses on learning substitutions from sentence-aligned parallel corpora of complex-simple texts (Paetzold and Specia, 2013; Horn et al., 2014). LEXenstein’s SG module offers support for five approaches. All approaches use LEXenstein’s Text Adorning module to create substitutions for all possible inflections of verbs and nouns. Each approach is represented by one of the following Python classes: KauchakGenerator (Horn et al., 2014) Automatically extracts substitutions from parallel corpora. It requires a set of tagged parallel sentences and the word ali</context>
<context position="8847" citStr="Devlin and Tait, 1998" startWordPosition="1418" endWordPosition="1421"> sentence being simplified. It requires a binary word vector model produced by Word2Vec3, and can be configured in many ways. It retrieves a user-defined percentage of the substitutions, which are ranked with respect to the cosine distance between their word vector and the sum of some or all of the sentences’ words, depending on the settings defined by the user. 2.3 Substitution Ranking Substitution Ranking (SR) is the task of ranking a set of selected substitutions for a target complex word with respect to their simplicity. Approaches vary from simple word length and frequencybased measures (Devlin and Tait, 1998; Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011) to more sophisticated linear combinations of scoring functions (Jauhar and Specia, 2012), as well as machine learning-based approaches (Horn et al., 2014). LEXenstein’s SR module provides access to three approaches. All approaches receive as input datasets in the VICTOR format, which can be either training/testing datasets already containing only valid substitutions in context, or datasets generated with (potentially noisy) substitutions by a given SS approach. They also require as input a FeatureEstimator object to calculate fe</context>
</contexts>
<marker>Devlin, Tait, 1998</marker>
<rawString>S. Devlin and J. Tait. 1998. The use of a psycholinguistic database in the simplification of text for aphasic readers. Linguistic Databases. C. Fellbaum. 1998. WordNet: An Electronic Lexical Database. Bradford Books.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Horn</author>
<author>C Manduca</author>
<author>D Kauchak</author>
</authors>
<title>Learning a Lexical Simplifier Using Wikipedia.</title>
<date>2014</date>
<booktitle>The 52nd Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="3939" citStr="Horn et al., 2014" startWordPosition="598" endWordPosition="601">ion Generation We define Substitution Generation (SG) as the task of producing candidate substitutions for complex words, which is normally done regardless of the context of the complex word. Previous work commonly addresses this task by querying general domain thesauri such as WordNet (Fellbaum, 1998), or domain specific ones such as UMLS (Bodenreider, 2004). Examples of work resorting to this strategy are (Devlin and Tait, 1998) and (Carroll et al., 1999). Recent work focuses on learning substitutions from sentence-aligned parallel corpora of complex-simple texts (Paetzold and Specia, 2013; Horn et al., 2014). LEXenstein’s SG module offers support for five approaches. All approaches use LEXenstein’s Text Adorning module to create substitutions for all possible inflections of verbs and nouns. Each approach is represented by one of the following Python classes: KauchakGenerator (Horn et al., 2014) Automatically extracts substitutions from parallel corpora. It requires a set of tagged parallel sentences and the word alignments between them in Pharaoh format (Och and Ney, 2000). It produces a dictionary of complex-to-simple substitutions filtered by the criteria described in (Horn et al., 2014). Biran</context>
<context position="6893" citStr="Horn et al., 2014" startWordPosition="1070" endWordPosition="1073">s in Victor Frankenstein (Shelley, 2007)). As output, they produce a set of selected substitutions for each entry in the VICTOR dataset. The VICTOR format is structured as illustrated in Example 1, where Si is the ith sentence in the dataset, wi a target complex word in the hith position of Si, cji a substitution candidate and rji its simplicity ranking. Each bracketed component is separated by a tabulation marker. (S1) (w1) (h1) (r1 1:c1 )···(rn 1:cn 1) 1 ... (Sm) (wm) (hm) (r1 m:c1 )· · ·(rn m:cn m) m LEXenstein includes two resources for training/testing in the VICTOR format: the LexMTurk (Horn et al., 2014) and the SemEval corpus (Specia et al., 2012). Each approach in the SS module is represented by one of the following Python classes: WSDSelector Allows for the user to use one among various classic WSD approaches in SS. It requires the PyWSD (Tan, 2014) module to be installed, which includes the approaches presented by (Lesk, 1986) and (Wu and Palmer, 1994), as well as baselines such as random and first senses. BiranSelector (Biran et al., 2011) Employs a strategy in which a word co-occurrence model is used to determine which substitutions have meaning similar to that of a target complex word.</context>
<context position="9067" citStr="Horn et al., 2014" startWordPosition="1453" endWordPosition="1456"> cosine distance between their word vector and the sum of some or all of the sentences’ words, depending on the settings defined by the user. 2.3 Substitution Ranking Substitution Ranking (SR) is the task of ranking a set of selected substitutions for a target complex word with respect to their simplicity. Approaches vary from simple word length and frequencybased measures (Devlin and Tait, 1998; Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011) to more sophisticated linear combinations of scoring functions (Jauhar and Specia, 2012), as well as machine learning-based approaches (Horn et al., 2014). LEXenstein’s SR module provides access to three approaches. All approaches receive as input datasets in the VICTOR format, which can be either training/testing datasets already containing only valid substitutions in context, or datasets generated with (potentially noisy) substitutions by a given SS approach. They also require as input a FeatureEstimator object to calculate feature values describing the candidate substitutes. More details on the FeatureEstimator class are provided in Section 2.4. Each approach in the SR module is represented by one of the following Python classes: 3https://co</context>
<context position="15636" citStr="Horn et al., 2014" startWordPosition="2497" endWordPosition="2500">uage models and lexica built over Wikipedia and Simple Wikipedia. 3 Experiments In this Section, we discuss the results obtained in four benchmarking experiments. 3.1 Substitution Generation In this experiment we evaluate all SG approaches in LEXenstein. For the KauchakGenerator, we use the corpus provided by (Kauchak, 2013), composed of 150, 569 complex-to-simple parallel sentences, parsed by the Stanford Parser (Klein and Manning, 1965). From the the same corpus, we build the required vocabularies and language models for the BiranGenerator. We used the LexMturk dataset as the gold-standard (Horn et al., 2014), which is composed by 500 sentences, each with a single target complex word and 50 substitutions suggested by turkers. The results are presented in Table 1. The results in Table 1 show that the method of (Horn et al., 2014) yields the best F-Measure results, although combining the output of all generation methods yields the highest Potential. This shows that using parallel corpora to generate substitution candidates for complex words can be a 88 Approach Pot. Prec. F Kauchak 0.830 0.155 0.262 Wordnet 0.608 0.109 0.184 Biran 0.630 0.102 0.175 Merriam 0.540 0.067 0.120 Yamamoto 0.504 0.054 0.09</context>
<context position="18085" citStr="Horn et al., 2014" startWordPosition="2898" endWordPosition="2901">ng results “Size” in Table 2 represents the total number of substitutions selected for all test instances. The results in Table 2 show that our novel word vector approach outperforms all others in F-Measure by a considerable margin, including the method of not performing selection at all. Note that not performing selection allows for Potential to be higher, but 4https://code.google.com/p/word2vec/ yields very poor Precision. 3.3 Substitution Ranking In Table 3 we present the results of the evaluation of several SR approaches. We trained the SVMRanker with features similar to the ones used in (Horn et al., 2014), and the BoundaryRanker with a set of 10 features selected through univariate feature selection. We compare these approaches to three baseline Metric Rankers, which use the word’s frequency in Simple Wikipedia, its length or its number of senses. The SemEval corpus is used as the gold-standard so that we can compare our results with the best one obtained at SemEval2012 (Jauhar and Specia, 2012) (SemEval, in Table 3). Approach TR-1 Rec-1 Rec-2 Rec-3 Boundary 0.655 0.608 0.602 0.663 SVM 0.486 0.451 0.502 0.592 Freq. 0.226 0.220 0.236 0.300 Length 0.180 0.175 0.200 0.261 Senses 0.130 0.126 0.161</context>
</contexts>
<marker>Horn, Manduca, Kauchak, 2014</marker>
<rawString>C. Horn, C. Manduca, and D. Kauchak. 2014. Learning a Lexical Simplifier Using Wikipedia. The 52nd Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S K Jauhar</author>
<author>L Specia</author>
</authors>
<title>UOW-SHEF: SimpLex–lexical simplicity ranking based on contextual and psycholinguistic features.</title>
<date>2012</date>
<journal>The</journal>
<booktitle>In The 8th ACM.</booktitle>
<volume>1</volume>
<contexts>
<context position="1950" citStr="Jauhar and Specia, 2012" startWordPosition="303" endWordPosition="307">Carroll et al., 1999). More recently, the LS task at SemEval-2012 (Specia et al., 2012) has given LS wider visibility. Participants had the opportunity to compare their approaches in the task of ranking candidate substitutions, all of which were already known to fit the context, according to their “simplicity”. Despite its growth in popularity, the inexistence of tools to support the process and help researchers to build upon has been hampering progress in the area. We were only able to find one tool for LS: a set of scripts designed for the training and testing of ranking models provided by (Jauhar and Specia, 2012)1. However, they cover only one step of the process. In an effort to tackle this issue, we present LEXenstein: a framework for Lexical Simplification development and benchmarking. LEXenstein is an easy-to-use framework that provides simplified access to many approaches for several sub-tasks of the LS pipeline, which is illustrated in Figure 1. Its current version includes methods for the three main sub-tasks in the pipeline: Substitution Generation, Substitution Selection and Substitution Ranking. Figure 1: Lexical Simplification Pipeline LEXenstein was devised to facilitate performance compar</context>
<context position="9001" citStr="Jauhar and Specia, 2012" startWordPosition="1443" endWordPosition="1446">ed percentage of the substitutions, which are ranked with respect to the cosine distance between their word vector and the sum of some or all of the sentences’ words, depending on the settings defined by the user. 2.3 Substitution Ranking Substitution Ranking (SR) is the task of ranking a set of selected substitutions for a target complex word with respect to their simplicity. Approaches vary from simple word length and frequencybased measures (Devlin and Tait, 1998; Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011) to more sophisticated linear combinations of scoring functions (Jauhar and Specia, 2012), as well as machine learning-based approaches (Horn et al., 2014). LEXenstein’s SR module provides access to three approaches. All approaches receive as input datasets in the VICTOR format, which can be either training/testing datasets already containing only valid substitutions in context, or datasets generated with (potentially noisy) substitutions by a given SS approach. They also require as input a FeatureEstimator object to calculate feature values describing the candidate substitutes. More details on the FeatureEstimator class are provided in Section 2.4. Each approach in the SR module </context>
<context position="18483" citStr="Jauhar and Specia, 2012" startWordPosition="2963" endWordPosition="2966">ord2vec/ yields very poor Precision. 3.3 Substitution Ranking In Table 3 we present the results of the evaluation of several SR approaches. We trained the SVMRanker with features similar to the ones used in (Horn et al., 2014), and the BoundaryRanker with a set of 10 features selected through univariate feature selection. We compare these approaches to three baseline Metric Rankers, which use the word’s frequency in Simple Wikipedia, its length or its number of senses. The SemEval corpus is used as the gold-standard so that we can compare our results with the best one obtained at SemEval2012 (Jauhar and Specia, 2012) (SemEval, in Table 3). Approach TR-1 Rec-1 Rec-2 Rec-3 Boundary 0.655 0.608 0.602 0.663 SVM 0.486 0.451 0.502 0.592 Freq. 0.226 0.220 0.236 0.300 Length 0.180 0.175 0.200 0.261 Senses 0.130 0.126 0.161 0.223 SemEval 0.602 0.575 0.689 0.769 Table 3: SR benchmarking results The novel Boundary ranking approach outperforms all other approaches in both TRank-at-1 and Recall-at-1 by a considerable margin, but it is worse than the best SemEval-2012 approach in terms of Recall-at-2 and 3. This however reveals not a limitation but a strength of our approach: since the Boundary ranker focuses on placin</context>
</contexts>
<marker>Jauhar, Specia, 2012</marker>
<rawString>S.K. Jauhar and L. Specia. 2012. UOW-SHEF: SimpLex–lexical simplicity ranking based on contextual and psycholinguistic features. The 1st *SEM. T. Joachims. 2002. Optimizing search engines using clickthrough data. In The 8th ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kajiwara</author>
<author>H Matsumoto</author>
<author>K Yamamoto</author>
</authors>
<title>Selecting Proper Lexical Paraphrase for Children.</title>
<date>2013</date>
<contexts>
<context position="4974" citStr="Kajiwara et al., 2013" startWordPosition="756" endWordPosition="759">alignments between them in Pharaoh format (Och and Ney, 2000). It produces a dictionary of complex-to-simple substitutions filtered by the criteria described in (Horn et al., 2014). BiranGenerator (Biran et al., 2011) Filters substitutions based on the Cartesian product between vocabularies of complex and simple words. It requires vocabularies of complex and simple words, as well as two language models trained over complex and simple corpora. It produces a dictionary linking words to a set of synonyms and hypernyms filtered by the criteria described in (Biran et al., 2011). YamamotoGenerator (Kajiwara et al., 2013) Extracts substitutions from dictionary definitions of complex words. It requires an API key for the Merriam Dictionary2, which can be obtained for free. It produces a dictionary linking words in the Merriam Dictionary and WordNet to words with the same Part-of-Speech (POS) tag in its entries’ definitions and examples of usage. MerriamGenerator Extracts a dictionary linking words to their synonyms, as listed in the Merriam Thesaurus. It requires an API key. 2http://www.dictionaryapi.com/ WordnetGenerator Extracts a dictionary linking words to their synonyms, as listed in WordNet. 2.2 Substitut</context>
</contexts>
<marker>Kajiwara, Matsumoto, Yamamoto, 2013</marker>
<rawString>T. Kajiwara, H. Matsumoto, and K. Yamamoto. 2013. Selecting Proper Lexical Paraphrase for Children.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Kauchak</author>
</authors>
<title>Improving Text Simplification Language Modeling Using Unsimplified Text Data.</title>
<date>2013</date>
<booktitle>The 51st Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="15344" citStr="Kauchak, 2013" startWordPosition="2452" endWordPosition="2453">flection, verb tensing and verb conjugation. 2.7 Resources LEXenstein also provides a wide array of resources for the user to explore in benchmarking tasks. Among them are the aforementioned LexMturk and SemEval corpora in the VICTOR format, lists of stop and basic words, as well as language models and lexica built over Wikipedia and Simple Wikipedia. 3 Experiments In this Section, we discuss the results obtained in four benchmarking experiments. 3.1 Substitution Generation In this experiment we evaluate all SG approaches in LEXenstein. For the KauchakGenerator, we use the corpus provided by (Kauchak, 2013), composed of 150, 569 complex-to-simple parallel sentences, parsed by the Stanford Parser (Klein and Manning, 1965). From the the same corpus, we build the required vocabularies and language models for the BiranGenerator. We used the LexMturk dataset as the gold-standard (Horn et al., 2014), which is composed by 500 sentences, each with a single target complex word and 50 substitutions suggested by turkers. The results are presented in Table 1. The results in Table 1 show that the method of (Horn et al., 2014) yields the best F-Measure results, although combining the output of all generation </context>
</contexts>
<marker>Kauchak, 2013</marker>
<rawString>D. Kauchak. 2013. Improving Text Simplification Language Modeling Using Unsimplified Text Data. The 51st Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing.</title>
<date>1965</date>
<booktitle>In The 41st Annual Meeting ofACL.</booktitle>
<contexts>
<context position="15460" citStr="Klein and Manning, 1965" startWordPosition="2468" endWordPosition="2471">ces for the user to explore in benchmarking tasks. Among them are the aforementioned LexMturk and SemEval corpora in the VICTOR format, lists of stop and basic words, as well as language models and lexica built over Wikipedia and Simple Wikipedia. 3 Experiments In this Section, we discuss the results obtained in four benchmarking experiments. 3.1 Substitution Generation In this experiment we evaluate all SG approaches in LEXenstein. For the KauchakGenerator, we use the corpus provided by (Kauchak, 2013), composed of 150, 569 complex-to-simple parallel sentences, parsed by the Stanford Parser (Klein and Manning, 1965). From the the same corpus, we build the required vocabularies and language models for the BiranGenerator. We used the LexMturk dataset as the gold-standard (Horn et al., 2014), which is composed by 500 sentences, each with a single target complex word and 50 substitutions suggested by turkers. The results are presented in Table 1. The results in Table 1 show that the method of (Horn et al., 2014) yields the best F-Measure results, although combining the output of all generation methods yields the highest Potential. This shows that using parallel corpora to generate substitution candidates for</context>
</contexts>
<marker>Klein, Manning, 1965</marker>
<rawString>D. Klein and C.D. Manning. 1965. Accurate Unlexicalized Parsing. In The 41st Annual Meeting ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone.</title>
<date>1986</date>
<booktitle>In The 5th SIGDOC.</booktitle>
<contexts>
<context position="7226" citStr="Lesk, 1986" startWordPosition="1131" endWordPosition="1132">plicity ranking. Each bracketed component is separated by a tabulation marker. (S1) (w1) (h1) (r1 1:c1 )···(rn 1:cn 1) 1 ... (Sm) (wm) (hm) (r1 m:c1 )· · ·(rn m:cn m) m LEXenstein includes two resources for training/testing in the VICTOR format: the LexMTurk (Horn et al., 2014) and the SemEval corpus (Specia et al., 2012). Each approach in the SS module is represented by one of the following Python classes: WSDSelector Allows for the user to use one among various classic WSD approaches in SS. It requires the PyWSD (Tan, 2014) module to be installed, which includes the approaches presented by (Lesk, 1986) and (Wu and Palmer, 1994), as well as baselines such as random and first senses. BiranSelector (Biran et al., 2011) Employs a strategy in which a word co-occurrence model is used to determine which substitutions have meaning similar to that of a target complex word. It requires a plain text file with each line in the format specified in Example 2, where (wi) is a word, � � � 1 (1) 86 (cij &gt; a co-occurring word and (fij &gt; its frequency of occurrence. (wi) �c0 ):�f0 ) ··· (cn i ):(fn i ) (2) i i Each component in the format in 2 must be separated by a tabulation marker. Given such a model, the </context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>M. Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone. In The 5th SIGDOC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B P Nunes</author>
<author>R Kawase</author>
<author>P Siehndel</author>
<author>M A Casanova</author>
<author>S Dietze</author>
</authors>
<title>As Simple as It Gets - A Sentence Simplifier for Different Learning Levels and Contexts.</title>
<date>2013</date>
<booktitle>IEEE 13th ICALT.</booktitle>
<contexts>
<context position="5938" citStr="Nunes et al., 2013" startWordPosition="906" endWordPosition="909">r Extracts a dictionary linking words to their synonyms, as listed in the Merriam Thesaurus. It requires an API key. 2http://www.dictionaryapi.com/ WordnetGenerator Extracts a dictionary linking words to their synonyms, as listed in WordNet. 2.2 Substitution Selection Substitution Selection (SS) is the task of selecting which substitutions – from a given list – can replace a complex word in a given sentence without altering its meaning. Most work addresses this task referring to the context of the complex word by employing Word Sense Disambiguation (WSD) approaches (Sedding and Kazakov, 2004; Nunes et al., 2013), or by discarding substitutions which do not share the same POS tag of the target complex word (Kajiwara et al., 2013; Paetzold and Specia, 2013). LEXenstein’s SS module provides access to three approaches. All approaches require as input a dictionary of substitutions generated by a given approach and a dataset in the VICTOR format (as in Victor Frankenstein (Shelley, 2007)). As output, they produce a set of selected substitutions for each entry in the VICTOR dataset. The VICTOR format is structured as illustrated in Example 1, where Si is the ith sentence in the dataset, wi a target complex </context>
</contexts>
<marker>Nunes, Kawase, Siehndel, Casanova, Dietze, 2013</marker>
<rawString>B.P. Nunes, R. Kawase, P. Siehndel, M.A. Casanova, and S. Dietze. 2013. As Simple as It Gets - A Sentence Simplifier for Different Learning Levels and Contexts. IEEE 13th ICALT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In The 38th Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="4413" citStr="Och and Ney, 2000" startWordPosition="669" endWordPosition="672"> focuses on learning substitutions from sentence-aligned parallel corpora of complex-simple texts (Paetzold and Specia, 2013; Horn et al., 2014). LEXenstein’s SG module offers support for five approaches. All approaches use LEXenstein’s Text Adorning module to create substitutions for all possible inflections of verbs and nouns. Each approach is represented by one of the following Python classes: KauchakGenerator (Horn et al., 2014) Automatically extracts substitutions from parallel corpora. It requires a set of tagged parallel sentences and the word alignments between them in Pharaoh format (Och and Ney, 2000). It produces a dictionary of complex-to-simple substitutions filtered by the criteria described in (Horn et al., 2014). BiranGenerator (Biran et al., 2011) Filters substitutions based on the Cartesian product between vocabularies of complex and simple words. It requires vocabularies of complex and simple words, as well as two language models trained over complex and simple corpora. It produces a dictionary linking words to a set of synonyms and hypernyms filtered by the criteria described in (Biran et al., 2011). YamamotoGenerator (Kajiwara et al., 2013) Extracts substitutions from dictionary</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>F.J. Och and H. Ney. 2000. Improved statistical alignment models. In The 38th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G H Paetzold</author>
<author>L Specia</author>
</authors>
<title>Text simplification as tree transduction.</title>
<date>2013</date>
<booktitle>In The 9th STIL.</booktitle>
<contexts>
<context position="3919" citStr="Paetzold and Specia, 2013" startWordPosition="593" endWordPosition="597">ACL and AFNLP 2.1 Substitution Generation We define Substitution Generation (SG) as the task of producing candidate substitutions for complex words, which is normally done regardless of the context of the complex word. Previous work commonly addresses this task by querying general domain thesauri such as WordNet (Fellbaum, 1998), or domain specific ones such as UMLS (Bodenreider, 2004). Examples of work resorting to this strategy are (Devlin and Tait, 1998) and (Carroll et al., 1999). Recent work focuses on learning substitutions from sentence-aligned parallel corpora of complex-simple texts (Paetzold and Specia, 2013; Horn et al., 2014). LEXenstein’s SG module offers support for five approaches. All approaches use LEXenstein’s Text Adorning module to create substitutions for all possible inflections of verbs and nouns. Each approach is represented by one of the following Python classes: KauchakGenerator (Horn et al., 2014) Automatically extracts substitutions from parallel corpora. It requires a set of tagged parallel sentences and the word alignments between them in Pharaoh format (Och and Ney, 2000). It produces a dictionary of complex-to-simple substitutions filtered by the criteria described in (Horn </context>
<context position="6084" citStr="Paetzold and Specia, 2013" startWordPosition="931" endWordPosition="934">api.com/ WordnetGenerator Extracts a dictionary linking words to their synonyms, as listed in WordNet. 2.2 Substitution Selection Substitution Selection (SS) is the task of selecting which substitutions – from a given list – can replace a complex word in a given sentence without altering its meaning. Most work addresses this task referring to the context of the complex word by employing Word Sense Disambiguation (WSD) approaches (Sedding and Kazakov, 2004; Nunes et al., 2013), or by discarding substitutions which do not share the same POS tag of the target complex word (Kajiwara et al., 2013; Paetzold and Specia, 2013). LEXenstein’s SS module provides access to three approaches. All approaches require as input a dictionary of substitutions generated by a given approach and a dataset in the VICTOR format (as in Victor Frankenstein (Shelley, 2007)). As output, they produce a set of selected substitutions for each entry in the VICTOR dataset. The VICTOR format is structured as illustrated in Example 1, where Si is the ith sentence in the dataset, wi a target complex word in the hith position of Si, cji a substitution candidate and rji its simplicity ranking. Each bracketed component is separated by a tabulatio</context>
</contexts>
<marker>Paetzold, Specia, 2013</marker>
<rawString>G.H. Paetzold and L. Specia. 2013. Text simplification as tree transduction. In The 9th STIL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G H Paetzold</author>
</authors>
<title>Morph adorner toolkit: Morph adorner made simple.</title>
<date>2015</date>
<note>http://ghpaetzold.github.io/MorphAdornerToolkit/.</note>
<contexts>
<context position="14548" citStr="Paetzold, 2015" startWordPosition="2329" endWordPosition="2330"> selected by a given set of approaches. It returns the approaches’ Precision, Accuracy and Change Proportion, where Precision is the proportion of instances for which the highest ranking substitution is not the target complex word itself and is in the gold-standard, Accuracy is the proportion of instances for which the highest ranking substitution is in the gold-standard, and Change Proportion is the proportion of instances for which the highest ranking substitution is not the target complex word itself. 2.6 Text Adorning This approach provides a Python interface to the Morph Adorner Toolkit (Paetzold, 2015), a set of Java tools that facilitates the access to Morph Adorner’s functionalities. The class provides easy access to word lemmatisation, word stemming, syllable splitting, noun inflection, verb tensing and verb conjugation. 2.7 Resources LEXenstein also provides a wide array of resources for the user to explore in benchmarking tasks. Among them are the aforementioned LexMturk and SemEval corpora in the VICTOR format, lists of stop and basic words, as well as language models and lexica built over Wikipedia and Simple Wikipedia. 3 Experiments In this Section, we discuss the results obtained i</context>
</contexts>
<marker>Paetzold, 2015</marker>
<rawString>G.H. Paetzold. 2015. Morph adorner toolkit: Morph adorner made simple. http://ghpaetzold.github.io/MorphAdornerToolkit/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Sedding</author>
<author>D Kazakov</author>
</authors>
<title>Wordnet-based text document clustering.</title>
<date>2004</date>
<booktitle>In The 3rd</booktitle>
<publisher>Frankenstein. Pearson Education.</publisher>
<contexts>
<context position="5917" citStr="Sedding and Kazakov, 2004" startWordPosition="902" endWordPosition="905">s of usage. MerriamGenerator Extracts a dictionary linking words to their synonyms, as listed in the Merriam Thesaurus. It requires an API key. 2http://www.dictionaryapi.com/ WordnetGenerator Extracts a dictionary linking words to their synonyms, as listed in WordNet. 2.2 Substitution Selection Substitution Selection (SS) is the task of selecting which substitutions – from a given list – can replace a complex word in a given sentence without altering its meaning. Most work addresses this task referring to the context of the complex word by employing Word Sense Disambiguation (WSD) approaches (Sedding and Kazakov, 2004; Nunes et al., 2013), or by discarding substitutions which do not share the same POS tag of the target complex word (Kajiwara et al., 2013; Paetzold and Specia, 2013). LEXenstein’s SS module provides access to three approaches. All approaches require as input a dictionary of substitutions generated by a given approach and a dataset in the VICTOR format (as in Victor Frankenstein (Shelley, 2007)). As output, they produce a set of selected substitutions for each entry in the VICTOR dataset. The VICTOR format is structured as illustrated in Example 1, where Si is the ith sentence in the dataset,</context>
</contexts>
<marker>Sedding, Kazakov, 2004</marker>
<rawString>J. Sedding and D. Kazakov. 2004. Wordnet-based text document clustering. In The 3rd ROMAND. M. Shelley. 2007. Frankenstein. Pearson Education.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Specia</author>
<author>S K Jauhar</author>
<author>R Mihalcea</author>
</authors>
<date>2012</date>
<booktitle>Semeval-2012 task 1: English lexical simplification. In The 1st *SEM.</booktitle>
<contexts>
<context position="1413" citStr="Specia et al., 2012" startWordPosition="214" endWordPosition="217">on The goal of a Lexical Simplification (LS) approach is to replace complex words and expressions in a given text, often a sentence, with simpler alternatives of equivalent meaning in context. Although very intuitive, this is a challenging task since the substitutions must preserve both the original meaning and the grammaticality of the sentence being simplified. The LS task has been gaining significant attention since the late 1990’s, thanks to the positive influence of the early work presented by (Devlin and Tait, 1998) and (Carroll et al., 1999). More recently, the LS task at SemEval-2012 (Specia et al., 2012) has given LS wider visibility. Participants had the opportunity to compare their approaches in the task of ranking candidate substitutions, all of which were already known to fit the context, according to their “simplicity”. Despite its growth in popularity, the inexistence of tools to support the process and help researchers to build upon has been hampering progress in the area. We were only able to find one tool for LS: a set of scripts designed for the training and testing of ranking models provided by (Jauhar and Specia, 2012)1. However, they cover only one step of the process. In an effo</context>
<context position="6938" citStr="Specia et al., 2012" startWordPosition="1078" endWordPosition="1082">As output, they produce a set of selected substitutions for each entry in the VICTOR dataset. The VICTOR format is structured as illustrated in Example 1, where Si is the ith sentence in the dataset, wi a target complex word in the hith position of Si, cji a substitution candidate and rji its simplicity ranking. Each bracketed component is separated by a tabulation marker. (S1) (w1) (h1) (r1 1:c1 )···(rn 1:cn 1) 1 ... (Sm) (wm) (hm) (r1 m:c1 )· · ·(rn m:cn m) m LEXenstein includes two resources for training/testing in the VICTOR format: the LexMTurk (Horn et al., 2014) and the SemEval corpus (Specia et al., 2012). Each approach in the SS module is represented by one of the following Python classes: WSDSelector Allows for the user to use one among various classic WSD approaches in SS. It requires the PyWSD (Tan, 2014) module to be installed, which includes the approaches presented by (Lesk, 1986) and (Wu and Palmer, 1994), as well as baselines such as random and first senses. BiranSelector (Biran et al., 2011) Employs a strategy in which a word co-occurrence model is used to determine which substitutions have meaning similar to that of a target complex word. It requires a plain text file with each line</context>
<context position="13528" citStr="Specia et al., 2012" startWordPosition="2162" endWordPosition="2165">nerated is present in the gold-standard, Precision the proportion of generated instances which are present in the gold-standard, and F-measure their harmonic mean. SelectorEvaluator: Provides evaluation metrics for SS methods. It requires a gold-standard in the VICTOR format and a set of selected substitutions. It returns the Potential, Precision and Fmeasure of the SS approach, as defined above. RankerEvaluator: Provides evaluation metrics for SR methods. It requires a gold-standard in the VICTOR format and a set of ranked substitutions. It returns the TRank-at-1:3 and Recall-at-1:3 metrics (Specia et al., 2012), where Trank-at-i is the proportion of instances for which a candidate of gold-rank r &lt; i was ranked first, and Recall-at-i the proportion of candidates of gold-rank r &lt; i that are ranked in positions p &lt; i. PipelineEvaluator: Provides evaluation metrics for the entire LS pipeline. It requires as input a gold-standard in the VICTOR format and a set of ranked substitutions which have been generated and selected by a given set of approaches. It returns the approaches’ Precision, Accuracy and Change Proportion, where Precision is the proportion of instances for which the highest ranking substitu</context>
</contexts>
<marker>Specia, Jauhar, Mihalcea, 2012</marker>
<rawString>L. Specia, S.K. Jauhar, and R. Mihalcea. 2012. Semeval-2012 task 1: English lexical simplification. In The 1st *SEM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Tan</author>
</authors>
<title>Pywsd: Python implementations of word sense disambiguation technologies.</title>
<date>2014</date>
<note>https://github.com/alvations/pywsd.</note>
<contexts>
<context position="7146" citStr="Tan, 2014" startWordPosition="1118" endWordPosition="1119">x word in the hith position of Si, cji a substitution candidate and rji its simplicity ranking. Each bracketed component is separated by a tabulation marker. (S1) (w1) (h1) (r1 1:c1 )···(rn 1:cn 1) 1 ... (Sm) (wm) (hm) (r1 m:c1 )· · ·(rn m:cn m) m LEXenstein includes two resources for training/testing in the VICTOR format: the LexMTurk (Horn et al., 2014) and the SemEval corpus (Specia et al., 2012). Each approach in the SS module is represented by one of the following Python classes: WSDSelector Allows for the user to use one among various classic WSD approaches in SS. It requires the PyWSD (Tan, 2014) module to be installed, which includes the approaches presented by (Lesk, 1986) and (Wu and Palmer, 1994), as well as baselines such as random and first senses. BiranSelector (Biran et al., 2011) Employs a strategy in which a word co-occurrence model is used to determine which substitutions have meaning similar to that of a target complex word. It requires a plain text file with each line in the format specified in Example 2, where (wi) is a word, � � � 1 (1) 86 (cij &gt; a co-occurring word and (fij &gt; its frequency of occurrence. (wi) �c0 ):�f0 ) ··· (cn i ):(fn i ) (2) i i Each component in th</context>
</contexts>
<marker>Tan, 2014</marker>
<rawString>L. Tan. 2014. Pywsd: Python implementations of word sense disambiguation technologies. https://github.com/alvations/pywsd.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Wu</author>
<author>M Palmer</author>
</authors>
<title>Verbs semantics and lexical selection.</title>
<date>1994</date>
<booktitle>In The 32nd Annual Meeting of ACL.</booktitle>
<contexts>
<context position="7252" citStr="Wu and Palmer, 1994" startWordPosition="1134" endWordPosition="1137">Each bracketed component is separated by a tabulation marker. (S1) (w1) (h1) (r1 1:c1 )···(rn 1:cn 1) 1 ... (Sm) (wm) (hm) (r1 m:c1 )· · ·(rn m:cn m) m LEXenstein includes two resources for training/testing in the VICTOR format: the LexMTurk (Horn et al., 2014) and the SemEval corpus (Specia et al., 2012). Each approach in the SS module is represented by one of the following Python classes: WSDSelector Allows for the user to use one among various classic WSD approaches in SS. It requires the PyWSD (Tan, 2014) module to be installed, which includes the approaches presented by (Lesk, 1986) and (Wu and Palmer, 1994), as well as baselines such as random and first senses. BiranSelector (Biran et al., 2011) Employs a strategy in which a word co-occurrence model is used to determine which substitutions have meaning similar to that of a target complex word. It requires a plain text file with each line in the format specified in Example 2, where (wi) is a word, � � � 1 (1) 86 (cij &gt; a co-occurring word and (fij &gt; its frequency of occurrence. (wi) �c0 ):�f0 ) ··· (cn i ):(fn i ) (2) i i Each component in the format in 2 must be separated by a tabulation marker. Given such a model, the approach filters all subst</context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Z. Wu and M. Palmer. 1994. Verbs semantics and lexical selection. In The 32nd Annual Meeting of ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>