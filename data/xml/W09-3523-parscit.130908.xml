<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004914">
<title confidence="0.9991005">
English—Hindi Transliteration Using Context-Informed PB-SMT:
the DCU System for NEWS 2009
</title>
<author confidence="0.9312715">
Rejwanul Haque, Sandipan Dandapat, Ankit Kumar Srivastava,
Sudip Kumar Naskar and Andy Way
</author>
<affiliation confidence="0.97277">
CNGL, School of Computing
Dublin City University, Dublin 9, Ireland
</affiliation>
<email confidence="0.997917">
{rhaque,sdandapat,snaskar,asrivastava,away}@computing.dcu.ie
</email>
<sectionHeader confidence="0.993879" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9987295625">
This paper presents English—Hindi translit-
eration in the NEWS 2009 Machine Translit-
eration Shared Task adding source context
modeling into state-of-the-art log-linear
phrase-based statistical machine translation
(PB-SMT). Source context features enable us
to exploit source similarity in addition to tar-
get similarity, as modelled by the language
model. We use a memory-based classification
framework that enables efficient estimation of
these features while avoiding data sparseness
problems.We carried out experiments both at
character and transliteration unit (TU) level.
Position-dependent source context features
produce significant improvements in terms of
all evaluation metrics.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999458">
Machine Transliteration is of key importance in
many cross-lingual natural language processing
applications, such as information retrieval, ques-
tion answering and machine translation (MT).
There are numerous ways of performing auto-
matic transliteration, such as noisy channel mod-
els (Knight and Graehl, 1998), joint source chan-
nel models (Li et al., 2004), decision-tree models
(Kang and Choi, 2000) and statistical MT models
(Matthews, 2007).
For the shared task, we built our machine
transliteration system based on phrase-based sta-
tistical MT (PB-SMT) (Koehn et al., 2003) using
Moses (Koehn et al., 2007). We adapt PB-SMT
models for transliteration by translating charac-
ters rather than words as in character-level trans-
lation systems (Lepage &amp; Denoual, 2006). How-
ever, we go a step further from the basic PB-
SMT model by using source-language context
features (Stroppa et al., 2007). We also create
translation models by constraining the character-
level segmentations, i.e. treating a consonant-
vowel cluster as one transliteration unit.
The remainder of the paper is organized as fol-
lows. In section 2 we give a brief overview of
PB-SMT. Section 3 describes how context-
informed features are incorporated into state-of-
art log-linear PB-SMT. Section 4 includes the
results obtained, together with some analysis.
Section 5 concludes the paper.
</bodyText>
<sectionHeader confidence="0.980734" genericHeader="method">
2 Log-Linear PB-SMT
</sectionHeader>
<bodyText confidence="0.9412915">
Translation is modelled in PB-SMT as a decision
process, in which the translation I
e1 = e1 . . . eI of
a source sentence J
</bodyText>
<equation confidence="0.995684363636364">
f1 = f1 . . . fJ is chosen to
maximize (1):
argmaxP(e,  |f,J) = argmaxP(f,J  |ei ) P(ei ) (1)
I e I
, I e I
,
1 1
where (1  |1 )
P f J e I and ( 1 )
P e denote respec-
I
</equation>
<bodyText confidence="0.9236825">
tively the translation model and the target lan-
guage model (Brown et al., 1993). In log-linear
phrase-based SMT, the posterior probability
P eI f J is directly modelled as a (log-linear)
( 1  |1 )
combination of features (Och and Ney, 2002),
that usually comprise M translational features,
and the language model, as in (2):
</bodyText>
<equation confidence="0.996039">
m
+ LM P e
log ( 1 )
I (2)
</equation>
<bodyText confidence="0.751648">
where s1K = s1...s denotes a segmentation of the
</bodyText>
<equation confidence="0.901463">
k
source and target sentences respectively into the
ˆ ˆ
sequences of phrases (eˆ1,...,ˆek) and (f1 ,..., fk)
such that (we set i0 = 0) (3):
V 1 &lt;_ k &lt;_ K , sk = (ik ; bk, jk),
fk = fbk ...fjk (3)
</equation>
<bodyText confidence="0.999452">
The translational features involved depend
only on a pair of source/target phrases and do not
take into account any context of these phrases.
This means that each feature hm in (2) can be
rewritten as in (4):
</bodyText>
<equation confidence="0.934495714285714">
log P(e
1  |f,J) = m hm (.f1J,e, ,s
1
E=
m
K)
1
ˆ
,
ek = eik
+1
... e i
1
k
</equation>
<page confidence="0.961551">
104
</page>
<note confidence="0.9672405">
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 104–107,
Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP
</note>
<equation confidence="0.94870575">
K
h f e s
( , , ) ˆ ( ˆ , ˆ ,
J I K 
m 1 1 1 m k k k
 h f e s
k 1
ˆ
</equation>
<bodyText confidence="0.9203675">
where hm is a feature that applies to a single
phrase-pair. Thus (2) can be rewritten as:
</bodyText>
<equation confidence="0.915843142857143">
m K K
 m hm(fk,ek,sk)   h(fk,ek,sk) (5)
m1 k1 k1
m
where, h ˆ  
 mh ˆ m . In this context, the transla-
m
</equation>
<bodyText confidence="0.999906666666667">
tion process amounts to: (i) choosing a segmen-
tation of the source sentence, (ii) translating each
source phrase.
</bodyText>
<sectionHeader confidence="0.9919285" genericHeader="method">
3 Source Context Features in Log-
Linear PB-SMT
</sectionHeader>
<bodyText confidence="0.995820846153846">
ˆ
The context of a source phrase fk is defined as
ˆ
the sequence before and after a focus phrase fk
= f i k ... f j k . Source context features (Stroppa et
al., 2007) include the direct left and right context
words (in our case, character/TU instead of word)
of length l (resp. fik  1...f i k l and fjk 1 . .. f j kl) of
ˆ
a given focus phrase fk = fi k ...fjk . A window of
size 2l+1 features including the focus phrase is
formed. Thus lexical contextual information (CI)
can be described as in (6):
</bodyText>
<equation confidence="0.866287">
CI = { i l ... i 1, j 1... j l }
</equation>
<bodyText confidence="0.9918111">
f k f k f k f k (6)
As in (Haque et al., 2009), we considered a
context window of ±1 and ±2 (i.e. l=1, 2) for our
experiments.
One natural way of expressing a context-
informed feature is as the conditional probability
of the target phrase given the source phrase and
its context information, as in (7):
into account, this estimation problem can only
become worse. To avoid such problems, in this
work we use three memory-based classifiers:
IGTree, IB1 and TRIBL 1 (Daelemans et al.,
2005). When predicting a target phrase given a
source phrase and its context, the source phrase
is intuitively the feature with the highest predic-
tion power; in all our experiments, it is the fea-
ture with the highest gain ratio (GR).
In order to build the set of examples required
to train the classifier, we modify the standard
phrase-extraction method of (Koehn et al., 2003)
to extract the context of the source phrases at the
same time as the phrases themselves. Importantly,
therefore, the context extraction comes at no ex-
tra cost.
We refer interested readers to (Stroppa et al.,
2007) and (Haque et al., 2009) as well as the ref-
erences therein for more details of how Memory-
Based Learning (MBL) is used for classification
of source examples for use in the log-linear MT
framework.
</bodyText>
<subsectionHeader confidence="0.997817">
3.2 Implementation Issues
</subsectionHeader>
<bodyText confidence="0.9754129375">
We split named entities (NE) into characters. We
break NEs into transliteration units (TU), which
bear close resemblance to syllables. We split
English NEs into TUs having C*V* pattern and
Hindi NEs are divided into TUs having Ch+M
pattern (M: Hindi Matra / vowel modifier, Ch:
Characters other than Matras). We carry out ex-
periments on both character-level (C-L) and TU-
level (TU-L) data. We use a 5-gram language
model for all our experiments. The Moses PB-
SMT system serves as our baseline system.
The distribution of target phrases given a
source phrase and its contextual information is
ˆ ˆ
normalised to estimate P( eˆk  |fk ,CI( fk )). There-
fore our expected feature is derived as in (8):
</bodyText>
<equation confidence="0.9229986">
) (4)
1

ˆ ˆ ˆ ˆ ˆ )) (7) ˆ ˆ ˆ )) (8)
hm ( fk ,CI( fk ), eˆk , sk) = log P( eˆk  |fk , CI( fk hmbl = log P( eˆk |fk ,CI( fk
</equation>
<subsectionHeader confidence="0.952164">
3.1 Memory-Based Classification
</subsectionHeader>
<bodyText confidence="0.956299714285714">
As (Stroppa et al., 2007) point out, directly esti-
ˆ ˆ
mating P( eˆk  |fk , CI( fk )) using relative fre-
quencies is problematic. Indeed, Zens and Ney
ˆ
(2004) showed that the estimation of P( eˆk  |fk )
using relative frequencies results in the overesti-
mation of the probabilities of long phrases, so
smoothing factors in the form of lexical-based
features are often used to counteract this bias
(Foster et al., 2006). In the case of context-
informed features, since the context is also taken
As for the standard phrase-based approach,
their weights are optimized using Minimum Er-
ror Rate Training (MERT) of (Och, 2003) for
each of the experiments.
As (Stroppa et al., 2007) point out, PB-SMT
decoders such as Pharaoh (Koehn, 2004) or
Moses (Koehn, 2007) rely on a static phrase-
table represented as a list of aligned phrases ac-
companied with several features. Since these fea-
</bodyText>
<footnote confidence="0.9989965">
1 An implementation of IGTree, IB1 and TRIBL is available
in the TiMBL software package (http://ilk.uvt.nl/timbl).
</footnote>
<page confidence="0.999096">
105
</page>
<bodyText confidence="0.981267617647059">
tures do not express the context in which those
phrases occur, no context information is kept in
the phrase-table, and there is no way to recover
this information from the phrase-table.
In order to take into account the context-
informed features for use with such decoders, the
devset and testset that need to be translated are
pre-processed. Each token appearing in the test-
set and devset is assigned a unique id. First we
prepare the phrase table using the training data.
Then we generate all possible phrases from the
devset and testset. These devset and testset
phrases are then searched for in the phrase table,
and if found, then the phrase along with its con-
textual information is given to MBL for classifi-
cation. MBL produces class distributions accord-
ing to the maximum-match of the features con-
tained in the source phrase. We derive new
scores from this class distribution and merge
them with the initial information contained in the
phrase table to take into account our feature
ˆ
functions (hmba ) in the log-linear model (2).
In this way we create a dynamic phrase table
containing both the standard and the context-
informed features. The new phrase table contains
the source phrase (represented by the sequence
of ids of the words composing the phrase), target
phrase and the new score.
Similarly, replacing all the words by their ids
in the development set, we perform MERT using
our new phrase table to optimize the feature
weights. We translate the test set (words repre-
sented by ids) using our new phrase table.
</bodyText>
<sectionHeader confidence="0.998771" genericHeader="evaluation">
4 Results and Analysis
</sectionHeader>
<bodyText confidence="0.999951588235294">
We used 10,000 NEs from the NEWS 2009 Eng-
lish—Hindi training data (Kumaran and Kellner,
2007) for the standard submission, and the addi-
tional English—Hindi parallel person names data
(105,905 distinct name pairs) of the Election
Commission of India2 for the non-standard sub-
missions. In addition to the baseline Moses sys-
tem, we carried out three different set of experi-
ments on IGTree, IB1 and TRIBL. Each of these
experiments was carried out on both the standard
data and the combined larger data, both at char-
acter level and the TU level, and considering
±1/±2 tokens as context. For each experiment,
we produce the 10-best distinct hypotheses. The
results are shown in Table 1.
We observed that many of the (unseen) TUs in
the testset remain untranslated in TU-L systems
</bodyText>
<footnote confidence="0.77964">
2 http://www.eci.gov.in/DevForum/Fullname.asp
</footnote>
<bodyText confidence="0.999864071428571">
due to the problems of data sparseness. When-
ever a TU-L system fails to translate a TU, we
fallback on the corresponding C-L system to
translate the TU as a post-processing step.
The accuracy of the TU-L baseline system
(0.391) is much higher compared to the C-L
baseline system (0.290) on standard dataset. Fur-
thermore, contextual modelling of the source
language gives an accuracy of 0.416 and 0.399
for TU-L system and C-L system respectively.
Similar trends are observed in case of larger
dataset. However, the highest accuracy (0.445)
has been achieved with the TU-L system using
the larger dataset.
</bodyText>
<sectionHeader confidence="0.996545" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999425">
In this work, we employed source context model-
ing into the state-of-the-art log-linear PB-SMT
for the English—Hindi transliteration task. We
have shown that taking source context into ac-
count substantially improve the system perform-
ance (an improvement of 43.44% and 26.42%
respectively for standard and larger datasets).
IGTree performs best for TU-L systems while
TRIBL seems to perform better for C-L systems
on both standard and non-standard datasets.
</bodyText>
<sectionHeader confidence="0.971809" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999376">
We would like to thank Antal van den Bosch for
his input on the use of memory based classifiers.
We are grateful to SFI (http://www.sfi.ie) for
generously sponsoring this research under grant
07/CE/I1142.
</bodyText>
<sectionHeader confidence="0.998334" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995963888888889">
Adimugan Kumaran and Tobias Kellner. A generic
framework for machine transliteration. Proc. of the
30th SIGIR, 2007.
Byung-Ju Kang and Key-Sun Choi. Automatic trans-
literation and back-transliteration by decision tree
learning. 2000. Proc. of LREC-2000, Athens,
Greece, pp. 1135-1141.
David Matthews. 2007. Machine Transliteration of
Proper Names. Master&apos;s Thesis, University of Ed-
inburgh, Edinburgh, United Kingdom.
Franz Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statisti-
cal machine translation. Proc. of ACL 2002, Phila-
delphia, PA, pp. 295–302.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable smoothing for statistical machine
translation. Proc. of EMNLP-2006, Sydney, Aus-
tralia, pp. 53-61.
</reference>
<page confidence="0.989039">
106
</page>
<table confidence="0.999831225806452">
S/B C/TU Context ACC M-F-Sc MRR MAP_ref MAP_10 MAP_sys
Baseline S C 0 .290 .814 .393 .286 .131 .131
Moses
TU 0 .391 .850 .483 .384 .160 .160
B C 0 .352 .830 .463 .346 .156 .156
TU 0 .407 .853 .500 .402 .165 .165
IB1 S C ±1 .391 .858 .501 .384 .166 .166
±2 .386 .860 .479 .379 .155 .155
TU ±1 .406 .858 .466 .398 .178 .178
±2 .359 .838 .402 .349 .165 .165
B C ±1 .431 .865 .534 .423 .177 .177
±2 (NSD1) .420 .867 .519 .413 .170 .170
TU ±1 .437 .863 .507 .429 .191 .191
±2 .427 .862 .487 .418 .194 .194
IGTree S C ±1 .372 .849 .482 .366 .160 .160
±2 .371 .847 .476 .364 .156 .156
TU ±1 .412 .859 .486 .404 .164 .164
±2 .416 .860 .493 .409 .166 .166
B C ±1 .413 .855 .518 .406 .173 .173
±2 (NSD2) .407 .856 .507 .399 .168 .168
TU ±1 .445 .864 .527 .440 .176 .176
±2 .427 .861 .516 .422 .173 .173
TRIBL S C ±1 .382 .854 .493 .375 .164 .164
±2 (SD) .399 .863 .488 .392 .157 .157
TU ±1 .408 .858 .474 .400 .181 .181
±2 .395 .857 .453 .385 .182 .182
B C ±1 .439 .866 .543 .430 .179 .179
±2 (NSD3) .421 .864 .519 .415 .171 .171
TU ±1 .444 .863 .512 .436 .193 .193
±2 .439 .865 .497 .430 .197 .197
S* C ±2 (NSD4) .419 .868 .464 .419 .338 .338
</table>
<tableCaption confidence="0.794779">
Table1: Experimental Results (S/B 4 Standard / Big data, S*4 TM on Standard data, but LM on Big data,
C/TU 4 Character / TU level, SD4 Standard submission, NSD4 Non-standard submission). Better results with
bold faces have not been submitted in the NEWS 2009 Machine Transliteration Shared Task.
</tableCaption>
<reference confidence="0.999676311111111">
Haizhou Li, Zhang Min and Su Jian. 2004. A joint
source-channel model for machine translitera-
tion. Proc. of ACL 2004, Barcelona, Spain,
pp.159-166.
Kevin Knight and Jonathan Graehl. 1998. Machine
Transliteration. Computational Linguistics,
24(4):559-612.
Nicolas Stroppa, Antal van den Bosch and Andy
Way. 2007. Exploiting Source Similarity for
SMT using Context-Informed Features. Proc. of
TMI-2007, Skövde, Sweden, pp. 231-240.
Peter F. Brown, S. A. D. Pietra, V. J. D. Pietra and
R. L. Mercer. 1993. The mathematics of statisti-
cal machine translation: parameter estimation.
Computational Linguistics 19 (2), pp. 263-311.
Philipp Koehn, F. J. Och, and D. Marcu. 2003. Sta-
tistical phrase-based translation. Proc. of HLT-
NAACL 2003, Edmonton, Canada, pp. 48-54.
Philipp Koehn. 2004. Pharaoh: a beam search de-
coder for phrase-based statistical machine trans-
lation models. Machine translation: from real
users to research: Proc. of AMTA 2004, Berlin:
Springer Verlag, 2004, pp. 115-124.
Philipp Koehn, H. Hoang, A. Birch, C. Callison-
Burch, M. Federico, N. Bertoldi, B. Cowan, W.
Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A.
Constantin and E. Herbst. 2007. Moses: open
source toolkit for statistical machine translation.
Proc. of ACL, Prague, Czech Republic, pp. 177-
180.
Rejwanul Haque, Sudip Kumar Naskar, Yanjun Ma
and Andy Way. 2009. Using Supertags as Source
Language Context in SMT. Proc. of EAMT-09,
Barcelona, Spain, pp. 234-241.
Richard Zens and Hermann Ney. 2004. Improve-
ments in phrase-based statistical machine trans-
lation. Proc. of HLT/NAACL 2004, Boston, MA,
pp. 257–264.
Walter Daelemans &amp; Antal van den Bosch. 2005.
Memory-based language processing. Cambridge,
UK, Cambridge University Press.
Yves Lepage and Etienne Denoual. 2006. Objective
evaluation of the analogy-based machine transla-
tion system ALEPH. Proc. of the 12th Annual
Meeting of the Association of NLP, pp. 873-876.
</reference>
<page confidence="0.998699">
107
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.184313">
<title confidence="0.6522755">Transliteration Using Context-Informed the DCU System for NEWS 2009</title>
<author confidence="0.614595">Rejwanul Haque</author>
<author confidence="0.614595">Sandipan Dandapat</author>
<author confidence="0.614595">Ankit Kumar Sudip Kumar Naskar</author>
<author confidence="0.614595">Andy</author>
<affiliation confidence="0.759831">CNGL, School of</affiliation>
<address confidence="0.877938">Dublin City University, Dublin 9, Ireland</address>
<email confidence="0.96863">rhaque@computing.dcu.ie</email>
<email confidence="0.96863">sdandapat@computing.dcu.ie</email>
<email confidence="0.96863">snaskar@computing.dcu.ie</email>
<email confidence="0.96863">asrivastava@computing.dcu.ie</email>
<email confidence="0.96863">away@computing.dcu.ie</email>
<abstract confidence="0.999793">This paper presents English—Hindi transliteration in the NEWS 2009 Machine Transliteration Shared Task adding source context modeling into state-of-the-art log-linear phrase-based statistical machine translation (PB-SMT). Source context features enable us to exploit source similarity in addition to target similarity, as modelled by the language model. We use a memory-based classification framework that enables efficient estimation of these features while avoiding data sparseness problems.We carried out experiments both at character and transliteration unit (TU) level. Position-dependent source context features produce significant improvements in terms of all evaluation metrics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adimugan Kumaran</author>
<author>Tobias Kellner</author>
</authors>
<title>A generic framework for machine transliteration.</title>
<date>2007</date>
<booktitle>Proc. of the 30th SIGIR,</booktitle>
<contexts>
<context position="9454" citStr="Kumaran and Kellner, 2007" startWordPosition="1675" endWordPosition="1678">g-linear model (2). In this way we create a dynamic phrase table containing both the standard and the contextinformed features. The new phrase table contains the source phrase (represented by the sequence of ids of the words composing the phrase), target phrase and the new score. Similarly, replacing all the words by their ids in the development set, we perform MERT using our new phrase table to optimize the feature weights. We translate the test set (words represented by ids) using our new phrase table. 4 Results and Analysis We used 10,000 NEs from the NEWS 2009 English—Hindi training data (Kumaran and Kellner, 2007) for the standard submission, and the additional English—Hindi parallel person names data (105,905 distinct name pairs) of the Election Commission of India2 for the non-standard submissions. In addition to the baseline Moses system, we carried out three different set of experiments on IGTree, IB1 and TRIBL. Each of these experiments was carried out on both the standard data and the combined larger data, both at character level and the TU level, and considering ±1/±2 tokens as context. For each experiment, we produce the 10-best distinct hypotheses. The results are shown in Table 1. We observed</context>
</contexts>
<marker>Kumaran, Kellner, 2007</marker>
<rawString>Adimugan Kumaran and Tobias Kellner. A generic framework for machine transliteration. Proc. of the 30th SIGIR, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Byung-Ju Kang</author>
<author>Key-Sun Choi</author>
</authors>
<title>Automatic transliteration and back-transliteration by decision tree learning.</title>
<date>2000</date>
<booktitle>Proc. of LREC-2000,</booktitle>
<pages>1135--1141</pages>
<location>Athens, Greece,</location>
<contexts>
<context position="1420" citStr="Kang and Choi, 2000" startWordPosition="182" endWordPosition="185">ss problems.We carried out experiments both at character and transliteration unit (TU) level. Position-dependent source context features produce significant improvements in terms of all evaluation metrics. 1 Introduction Machine Transliteration is of key importance in many cross-lingual natural language processing applications, such as information retrieval, question answering and machine translation (MT). There are numerous ways of performing automatic transliteration, such as noisy channel models (Knight and Graehl, 1998), joint source channel models (Li et al., 2004), decision-tree models (Kang and Choi, 2000) and statistical MT models (Matthews, 2007). For the shared task, we built our machine transliteration system based on phrase-based statistical MT (PB-SMT) (Koehn et al., 2003) using Moses (Koehn et al., 2007). We adapt PB-SMT models for transliteration by translating characters rather than words as in character-level translation systems (Lepage &amp; Denoual, 2006). However, we go a step further from the basic PBSMT model by using source-language context features (Stroppa et al., 2007). We also create translation models by constraining the characterlevel segmentations, i.e. treating a consonantvo</context>
</contexts>
<marker>Kang, Choi, 2000</marker>
<rawString>Byung-Ju Kang and Key-Sun Choi. Automatic transliteration and back-transliteration by decision tree learning. 2000. Proc. of LREC-2000, Athens, Greece, pp. 1135-1141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Matthews</author>
</authors>
<title>Machine Transliteration of Proper Names. Master&apos;s Thesis,</title>
<date>2007</date>
<institution>University of Edinburgh,</institution>
<location>Edinburgh, United Kingdom.</location>
<contexts>
<context position="1463" citStr="Matthews, 2007" startWordPosition="190" endWordPosition="191">haracter and transliteration unit (TU) level. Position-dependent source context features produce significant improvements in terms of all evaluation metrics. 1 Introduction Machine Transliteration is of key importance in many cross-lingual natural language processing applications, such as information retrieval, question answering and machine translation (MT). There are numerous ways of performing automatic transliteration, such as noisy channel models (Knight and Graehl, 1998), joint source channel models (Li et al., 2004), decision-tree models (Kang and Choi, 2000) and statistical MT models (Matthews, 2007). For the shared task, we built our machine transliteration system based on phrase-based statistical MT (PB-SMT) (Koehn et al., 2003) using Moses (Koehn et al., 2007). We adapt PB-SMT models for transliteration by translating characters rather than words as in character-level translation systems (Lepage &amp; Denoual, 2006). However, we go a step further from the basic PBSMT model by using source-language context features (Stroppa et al., 2007). We also create translation models by constraining the characterlevel segmentations, i.e. treating a consonantvowel cluster as one transliteration unit. Th</context>
</contexts>
<marker>Matthews, 2007</marker>
<rawString>David Matthews. 2007. Machine Transliteration of Proper Names. Master&apos;s Thesis, University of Edinburgh, Edinburgh, United Kingdom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>Proc. of ACL 2002,</booktitle>
<pages>295--302</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="2913" citStr="Och and Ney, 2002" startWordPosition="450" endWordPosition="453">obtained, together with some analysis. Section 5 concludes the paper. 2 Log-Linear PB-SMT Translation is modelled in PB-SMT as a decision process, in which the translation I e1 = e1 . . . eI of a source sentence J f1 = f1 . . . fJ is chosen to maximize (1): argmaxP(e, |f,J) = argmaxP(f,J |ei ) P(ei ) (1) I e I , I e I , 1 1 where (1 |1 ) P f J e I and ( 1 ) P e denote respecI tively the translation model and the target language model (Brown et al., 1993). In log-linear phrase-based SMT, the posterior probability P eI f J is directly modelled as a (log-linear) ( 1 |1 ) combination of features (Och and Ney, 2002), that usually comprise M translational features, and the language model, as in (2): m + LM P e log ( 1 ) I (2) where s1K = s1...s denotes a segmentation of the k source and target sentences respectively into the ˆ ˆ sequences of phrases (eˆ1,...,ˆek) and (f1 ,..., fk) such that (we set i0 = 0) (3): V 1 &lt;_ k &lt;_ K , sk = (ik ; bk, jk), fk = fbk ...fjk (3) The translational features involved depend only on a pair of source/target phrases and do not take into account any context of these phrases. This means that each feature hm in (2) can be rewritten as in (4): log P(e 1 |f,J) = m hm (.f1J,e, </context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. Proc. of ACL 2002, Philadelphia, PA, pp. 295–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Roland Kuhn</author>
<author>Howard Johnson</author>
</authors>
<title>Phrasetable smoothing for statistical machine translation.</title>
<date>2006</date>
<booktitle>Proc. of EMNLP-2006,</booktitle>
<pages>53--61</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="7234" citStr="Foster et al., 2006" startWordPosition="1300" endWordPosition="1303">efore our expected feature is derived as in (8): ) (4) 1  ˆ ˆ ˆ ˆ ˆ )) (7) ˆ ˆ ˆ )) (8) hm ( fk ,CI( fk ), eˆk , sk) = log P( eˆk |fk , CI( fk hmbl = log P( eˆk |fk ,CI( fk 3.1 Memory-Based Classification As (Stroppa et al., 2007) point out, directly estiˆ ˆ mating P( eˆk |fk , CI( fk )) using relative frequencies is problematic. Indeed, Zens and Ney ˆ (2004) showed that the estimation of P( eˆk |fk ) using relative frequencies results in the overestimation of the probabilities of long phrases, so smoothing factors in the form of lexical-based features are often used to counteract this bias (Foster et al., 2006). In the case of contextinformed features, since the context is also taken As for the standard phrase-based approach, their weights are optimized using Minimum Error Rate Training (MERT) of (Och, 2003) for each of the experiments. As (Stroppa et al., 2007) point out, PB-SMT decoders such as Pharaoh (Koehn, 2004) or Moses (Koehn, 2007) rely on a static phrasetable represented as a list of aligned phrases accompanied with several features. Since these fea1 An implementation of IGTree, IB1 and TRIBL is available in the TiMBL software package (http://ilk.uvt.nl/timbl). 105 tures do not express the</context>
</contexts>
<marker>Foster, Kuhn, Johnson, 2006</marker>
<rawString>George Foster, Roland Kuhn, and Howard Johnson. 2006. Phrasetable smoothing for statistical machine translation. Proc. of EMNLP-2006, Sydney, Australia, pp. 53-61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>Zhang Min</author>
<author>Su Jian</author>
</authors>
<title>A joint source-channel model for machine transliteration.</title>
<date>2004</date>
<booktitle>Proc. of ACL 2004,</booktitle>
<pages>159--166</pages>
<location>Barcelona,</location>
<contexts>
<context position="1376" citStr="Li et al., 2004" startWordPosition="176" endWordPosition="179">se features while avoiding data sparseness problems.We carried out experiments both at character and transliteration unit (TU) level. Position-dependent source context features produce significant improvements in terms of all evaluation metrics. 1 Introduction Machine Transliteration is of key importance in many cross-lingual natural language processing applications, such as information retrieval, question answering and machine translation (MT). There are numerous ways of performing automatic transliteration, such as noisy channel models (Knight and Graehl, 1998), joint source channel models (Li et al., 2004), decision-tree models (Kang and Choi, 2000) and statistical MT models (Matthews, 2007). For the shared task, we built our machine transliteration system based on phrase-based statistical MT (PB-SMT) (Koehn et al., 2003) using Moses (Koehn et al., 2007). We adapt PB-SMT models for transliteration by translating characters rather than words as in character-level translation systems (Lepage &amp; Denoual, 2006). However, we go a step further from the basic PBSMT model by using source-language context features (Stroppa et al., 2007). We also create translation models by constraining the characterleve</context>
</contexts>
<marker>Li, Min, Jian, 2004</marker>
<rawString>Haizhou Li, Zhang Min and Su Jian. 2004. A joint source-channel model for machine transliteration. Proc. of ACL 2004, Barcelona, Spain, pp.159-166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Jonathan Graehl</author>
</authors>
<date>1998</date>
<booktitle>Machine Transliteration. Computational Linguistics,</booktitle>
<pages>24--4</pages>
<contexts>
<context position="1329" citStr="Knight and Graehl, 1998" startWordPosition="167" endWordPosition="170">tion framework that enables efficient estimation of these features while avoiding data sparseness problems.We carried out experiments both at character and transliteration unit (TU) level. Position-dependent source context features produce significant improvements in terms of all evaluation metrics. 1 Introduction Machine Transliteration is of key importance in many cross-lingual natural language processing applications, such as information retrieval, question answering and machine translation (MT). There are numerous ways of performing automatic transliteration, such as noisy channel models (Knight and Graehl, 1998), joint source channel models (Li et al., 2004), decision-tree models (Kang and Choi, 2000) and statistical MT models (Matthews, 2007). For the shared task, we built our machine transliteration system based on phrase-based statistical MT (PB-SMT) (Koehn et al., 2003) using Moses (Koehn et al., 2007). We adapt PB-SMT models for transliteration by translating characters rather than words as in character-level translation systems (Lepage &amp; Denoual, 2006). However, we go a step further from the basic PBSMT model by using source-language context features (Stroppa et al., 2007). We also create trans</context>
</contexts>
<marker>Knight, Graehl, 1998</marker>
<rawString>Kevin Knight and Jonathan Graehl. 1998. Machine Transliteration. Computational Linguistics, 24(4):559-612.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicolas Stroppa</author>
<author>Antal van den Bosch</author>
<author>Andy Way</author>
</authors>
<title>Exploiting Source Similarity for SMT using Context-Informed Features.</title>
<date>2007</date>
<booktitle>Proc. of TMI-2007,</booktitle>
<pages>231--240</pages>
<location>Skövde,</location>
<marker>Stroppa, van den Bosch, Way, 2007</marker>
<rawString>Nicolas Stroppa, Antal van den Bosch and Andy Way. 2007. Exploiting Source Similarity for SMT using Context-Informed Features. Proc. of TMI-2007, Skövde, Sweden, pp. 231-240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>S A D Pietra</author>
<author>V J D Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<issue>2</issue>
<pages>263--311</pages>
<contexts>
<context position="2753" citStr="Brown et al., 1993" startWordPosition="422" endWordPosition="425">a brief overview of PB-SMT. Section 3 describes how contextinformed features are incorporated into state-ofart log-linear PB-SMT. Section 4 includes the results obtained, together with some analysis. Section 5 concludes the paper. 2 Log-Linear PB-SMT Translation is modelled in PB-SMT as a decision process, in which the translation I e1 = e1 . . . eI of a source sentence J f1 = f1 . . . fJ is chosen to maximize (1): argmaxP(e, |f,J) = argmaxP(f,J |ei ) P(ei ) (1) I e I , I e I , 1 1 where (1 |1 ) P f J e I and ( 1 ) P e denote respecI tively the translation model and the target language model (Brown et al., 1993). In log-linear phrase-based SMT, the posterior probability P eI f J is directly modelled as a (log-linear) ( 1 |1 ) combination of features (Och and Ney, 2002), that usually comprise M translational features, and the language model, as in (2): m + LM P e log ( 1 ) I (2) where s1K = s1...s denotes a segmentation of the k source and target sentences respectively into the ˆ ˆ sequences of phrases (eˆ1,...,ˆek) and (f1 ,..., fk) such that (we set i0 = 0) (3): V 1 &lt;_ k &lt;_ K , sk = (ik ; bk, jk), fk = fbk ...fjk (3) The translational features involved depend only on a pair of source/target phrases</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, S. A. D. Pietra, V. J. D. Pietra and R. L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics 19 (2), pp. 263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>Proc. of HLTNAACL 2003,</booktitle>
<pages>48--54</pages>
<location>Edmonton, Canada,</location>
<contexts>
<context position="1596" citStr="Koehn et al., 2003" startWordPosition="209" endWordPosition="212">s of all evaluation metrics. 1 Introduction Machine Transliteration is of key importance in many cross-lingual natural language processing applications, such as information retrieval, question answering and machine translation (MT). There are numerous ways of performing automatic transliteration, such as noisy channel models (Knight and Graehl, 1998), joint source channel models (Li et al., 2004), decision-tree models (Kang and Choi, 2000) and statistical MT models (Matthews, 2007). For the shared task, we built our machine transliteration system based on phrase-based statistical MT (PB-SMT) (Koehn et al., 2003) using Moses (Koehn et al., 2007). We adapt PB-SMT models for transliteration by translating characters rather than words as in character-level translation systems (Lepage &amp; Denoual, 2006). However, we go a step further from the basic PBSMT model by using source-language context features (Stroppa et al., 2007). We also create translation models by constraining the characterlevel segmentations, i.e. treating a consonantvowel cluster as one transliteration unit. The remainder of the paper is organized as follows. In section 2 we give a brief overview of PB-SMT. Section 3 describes how contextinf</context>
<context position="5533" citStr="Koehn et al., 2003" startWordPosition="989" endWordPosition="992">ven the source phrase and its context information, as in (7): into account, this estimation problem can only become worse. To avoid such problems, in this work we use three memory-based classifiers: IGTree, IB1 and TRIBL 1 (Daelemans et al., 2005). When predicting a target phrase given a source phrase and its context, the source phrase is intuitively the feature with the highest prediction power; in all our experiments, it is the feature with the highest gain ratio (GR). In order to build the set of examples required to train the classifier, we modify the standard phrase-extraction method of (Koehn et al., 2003) to extract the context of the source phrases at the same time as the phrases themselves. Importantly, therefore, the context extraction comes at no extra cost. We refer interested readers to (Stroppa et al., 2007) and (Haque et al., 2009) as well as the references therein for more details of how MemoryBased Learning (MBL) is used for classification of source examples for use in the log-linear MT framework. 3.2 Implementation Issues We split named entities (NE) into characters. We break NEs into transliteration units (TU), which bear close resemblance to syllables. We split English NEs into TU</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-based translation. Proc. of HLTNAACL 2003, Edmonton, Canada, pp. 48-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: a beam search decoder for phrase-based statistical machine translation models. Machine translation: from real users to research:</title>
<date>2004</date>
<booktitle>Proc. of AMTA 2004,</booktitle>
<pages>115--124</pages>
<publisher>Springer Verlag,</publisher>
<location>Berlin:</location>
<contexts>
<context position="7547" citStr="Koehn, 2004" startWordPosition="1354" endWordPosition="1355">roblematic. Indeed, Zens and Ney ˆ (2004) showed that the estimation of P( eˆk |fk ) using relative frequencies results in the overestimation of the probabilities of long phrases, so smoothing factors in the form of lexical-based features are often used to counteract this bias (Foster et al., 2006). In the case of contextinformed features, since the context is also taken As for the standard phrase-based approach, their weights are optimized using Minimum Error Rate Training (MERT) of (Och, 2003) for each of the experiments. As (Stroppa et al., 2007) point out, PB-SMT decoders such as Pharaoh (Koehn, 2004) or Moses (Koehn, 2007) rely on a static phrasetable represented as a list of aligned phrases accompanied with several features. Since these fea1 An implementation of IGTree, IB1 and TRIBL is available in the TiMBL software package (http://ilk.uvt.nl/timbl). 105 tures do not express the context in which those phrases occur, no context information is kept in the phrase-table, and there is no way to recover this information from the phrase-table. In order to take into account the contextinformed features for use with such decoders, the devset and testset that need to be translated are pre-proces</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: a beam search decoder for phrase-based statistical machine translation models. Machine translation: from real users to research: Proc. of AMTA 2004, Berlin: Springer Verlag, 2004, pp. 115-124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C CallisonBurch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>Proc. of ACL,</booktitle>
<pages>177--180</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1629" citStr="Koehn et al., 2007" startWordPosition="215" endWordPosition="218">troduction Machine Transliteration is of key importance in many cross-lingual natural language processing applications, such as information retrieval, question answering and machine translation (MT). There are numerous ways of performing automatic transliteration, such as noisy channel models (Knight and Graehl, 1998), joint source channel models (Li et al., 2004), decision-tree models (Kang and Choi, 2000) and statistical MT models (Matthews, 2007). For the shared task, we built our machine transliteration system based on phrase-based statistical MT (PB-SMT) (Koehn et al., 2003) using Moses (Koehn et al., 2007). We adapt PB-SMT models for transliteration by translating characters rather than words as in character-level translation systems (Lepage &amp; Denoual, 2006). However, we go a step further from the basic PBSMT model by using source-language context features (Stroppa et al., 2007). We also create translation models by constraining the characterlevel segmentations, i.e. treating a consonantvowel cluster as one transliteration unit. The remainder of the paper is organized as follows. In section 2 we give a brief overview of PB-SMT. Section 3 describes how contextinformed features are incorporated i</context>
</contexts>
<marker>Koehn, Hoang, Birch, CallisonBurch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, H. Hoang, A. Birch, C. CallisonBurch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin and E. Herbst. 2007. Moses: open source toolkit for statistical machine translation. Proc. of ACL, Prague, Czech Republic, pp. 177-180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rejwanul Haque</author>
<author>Sudip Kumar Naskar</author>
<author>Yanjun Ma</author>
<author>Andy Way</author>
</authors>
<title>Using Supertags as Source Language Context in SMT.</title>
<date>2009</date>
<booktitle>Proc. of EAMT-09,</booktitle>
<pages>234--241</pages>
<location>Barcelona,</location>
<contexts>
<context position="4720" citStr="Haque et al., 2009" startWordPosition="851" endWordPosition="854">t Features in LogLinear PB-SMT ˆ The context of a source phrase fk is defined as ˆ the sequence before and after a focus phrase fk = f i k ... f j k . Source context features (Stroppa et al., 2007) include the direct left and right context words (in our case, character/TU instead of word) of length l (resp. fik  1...f i k l and fjk 1 . .. f j kl) of ˆ a given focus phrase fk = fi k ...fjk . A window of size 2l+1 features including the focus phrase is formed. Thus lexical contextual information (CI) can be described as in (6): CI = { i l ... i 1, j 1... j l } f k f k f k f k (6) As in (Haque et al., 2009), we considered a context window of ±1 and ±2 (i.e. l=1, 2) for our experiments. One natural way of expressing a contextinformed feature is as the conditional probability of the target phrase given the source phrase and its context information, as in (7): into account, this estimation problem can only become worse. To avoid such problems, in this work we use three memory-based classifiers: IGTree, IB1 and TRIBL 1 (Daelemans et al., 2005). When predicting a target phrase given a source phrase and its context, the source phrase is intuitively the feature with the highest prediction power; in all</context>
</contexts>
<marker>Haque, Naskar, Ma, Way, 2009</marker>
<rawString>Rejwanul Haque, Sudip Kumar Naskar, Yanjun Ma and Andy Way. 2009. Using Supertags as Source Language Context in SMT. Proc. of EAMT-09, Barcelona, Spain, pp. 234-241.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Improvements in phrase-based statistical machine translation.</title>
<date>2004</date>
<booktitle>Proc. of HLT/NAACL 2004,</booktitle>
<pages>257--264</pages>
<location>Boston, MA,</location>
<marker>Zens, Ney, 2004</marker>
<rawString>Richard Zens and Hermann Ney. 2004. Improvements in phrase-based statistical machine translation. Proc. of HLT/NAACL 2004, Boston, MA, pp. 257–264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Antal van den Bosch</author>
</authors>
<title>Memory-based language processing.</title>
<date>2005</date>
<publisher>Cambridge, UK, Cambridge University Press.</publisher>
<marker>Daelemans, van den Bosch, 2005</marker>
<rawString>Walter Daelemans &amp; Antal van den Bosch. 2005. Memory-based language processing. Cambridge, UK, Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Lepage</author>
<author>Etienne Denoual</author>
</authors>
<title>Objective evaluation of the analogy-based machine translation system ALEPH.</title>
<date>2006</date>
<booktitle>Proc. of the 12th Annual Meeting of the Association of NLP,</booktitle>
<pages>873--876</pages>
<contexts>
<context position="1784" citStr="Lepage &amp; Denoual, 2006" startWordPosition="238" endWordPosition="241">question answering and machine translation (MT). There are numerous ways of performing automatic transliteration, such as noisy channel models (Knight and Graehl, 1998), joint source channel models (Li et al., 2004), decision-tree models (Kang and Choi, 2000) and statistical MT models (Matthews, 2007). For the shared task, we built our machine transliteration system based on phrase-based statistical MT (PB-SMT) (Koehn et al., 2003) using Moses (Koehn et al., 2007). We adapt PB-SMT models for transliteration by translating characters rather than words as in character-level translation systems (Lepage &amp; Denoual, 2006). However, we go a step further from the basic PBSMT model by using source-language context features (Stroppa et al., 2007). We also create translation models by constraining the characterlevel segmentations, i.e. treating a consonantvowel cluster as one transliteration unit. The remainder of the paper is organized as follows. In section 2 we give a brief overview of PB-SMT. Section 3 describes how contextinformed features are incorporated into state-ofart log-linear PB-SMT. Section 4 includes the results obtained, together with some analysis. Section 5 concludes the paper. 2 Log-Linear PB-SMT</context>
</contexts>
<marker>Lepage, Denoual, 2006</marker>
<rawString>Yves Lepage and Etienne Denoual. 2006. Objective evaluation of the analogy-based machine translation system ALEPH. Proc. of the 12th Annual Meeting of the Association of NLP, pp. 873-876.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>