<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.9660735">
Cluster-based Language Model for Sentence Retrieval in Chinese
Question Answering
</title>
<author confidence="0.993527">
Youzheng Wu Jun Zhao Bo Xu
</author>
<affiliation confidence="0.9943385">
National Laboratory of Pattern Recognition
Institute of Automation Chinese Academy of Sciences
</affiliation>
<address confidence="0.904837">
No.95 Zhongguancun East Road, 100080, Beijing, China
</address>
<email confidence="0.996112">
(yzwu, jzhao,boxu)@nlpr.ia.ac.cn
</email>
<sectionHeader confidence="0.996603" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999703476190476">
Sentence retrieval plays a very important
role in question answering system. In this
paper, we present a novel cluster-based
language model for sentence retrieval in
Chinese question answering which is mo-
tivated in part by sentence clustering and
language model. Sentence clustering is
used to group sentences into clusters.
Language model is used to properly rep-
resent sentences, which is combined with
sentences model, cluster/topic model and
collection model. For sentence clustering,
we propose two approaches that are One-
Sentence-Multi-Topics and One-
Sentence-One-Topic respectively. From
the experimental results on 807 Chinese
testing questions, we can conclude that
the proposed cluster-based language
model outperforms over the standard lan-
guage model for sentence retrieval in
Chinese question answering.
</bodyText>
<sectionHeader confidence="0.998885" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999947490566038">
To facilitate the answer extraction of question
answering, the task of retrieval module is to find
the most relevant passages or sentences to the
question. So, the retrieval module plays a very
important role in question answering system,
which influences both the performance and the
speed of question answering. In this paper, we
mainly focus on the research of improving the
performance of sentence retrieval in Chinese
question answering.
Many retrieval approaches have been pro-
posed for sentence retrieval in English question
answering. For example, Ittycheriach [Ittycheriah,
et al. 2002] and H. Yang [Hui Yang, et al. 2002]
proposed vector space model. Andres [Andres, et
al. 2004] and Vanessa [Vanessa, et al. 2004] pro-
posed language model and translation model re-
spectively. Compared to vector space model,
language model is theoretically attractive and a
potentially very effective probabilistic frame-
work for researching information retrieval prob-
lems [Jian-Yun Nie. 2005].
However, language model for sentence re-
trieval is not mature yet, which has a lot of diffi-
cult problems that cannot be solved at present.
For example, how to incorporate the structural
information, how to resolve data sparseness
problem. In this paper, we mainly focus on the
research of the smoothing approach of language
model because sparseness problem is more seri-
ous for sentence retrieval than for document re-
trieval.
At present, the most popular smoothing ap-
proaches for language model are Jelinek-Mercer
method, Bayesian smoothing using Dirichlet pri-
ors, absolute discounting and so on [C. Zhai, et al.
2001]. The main disadvantages of all these
smoothing approaches are that each document
model (which is estimated from each document)
is interpolated with the same collection model
(which is estimated from the whole collection)
through a unified parameter. Therefore, it does
not make any one particular document more
probable than any other, on the condition that
neither the documents originally contains the
query term. In other word, if a document is rele-
vant, but does not contain the query term, it is
still no more probable, even though it may be
topically related.
As we know, most smoothing approaches of
sentence retrieval in question answering are
learned from document retrieval without many
adaptations. In fact, question answering has some
</bodyText>
<page confidence="0.968595">
56
</page>
<note confidence="0.6577515">
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 56–63,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.99981575">
characteristics that are different from traditional
document retrieval, which could be used to im-
prove the performance of sentence retrieval.
These characteristics lie in:
</bodyText>
<listItem confidence="0.977685">
1. The input of question answering is natural
language question which is more unambiguous
than query in traditional document retrieval.
</listItem>
<bodyText confidence="0.999475916666667">
For traditional document retrieval, it’s difficult
to identify which kind of information the users
want to know. For example, if the user submit
the query {发明/invent, 电话/telephone}, search
engine does not know what information is
needed, who invented telephone, when telephone
was invented, or other information. On the other
hand, for question answering system, if the user
submit the question {谁发明了电话?/who in-
vented the telephone?}, it’s easy to know that the
user want to know the person who invented the
telephone, but not other information.
</bodyText>
<listItem confidence="0.87005675">
2. Candidate answers extracted according to
the semantic category of the question’s answer
could be used for sentence clustering of question
answering.
</listItem>
<bodyText confidence="0.993828034482758">
Although the first retrieved sentences are re-
lated to the question, they usually deal with one
or more topics. That is, relevant sentences for a
question may be distributed over several topics.
Therefore, treating the question’s words in re-
trieved sentences with different topics equally is
unreasonable. One of the solutions is to organize
the related sentences into several clusters, where
a sentence can belong to about one or more clus-
ters, each cluster is regarded as a topic. This is
sentence clustering. Obviously, cluster and topic
have the same meaning and can be replaced each
other. In the other word, a particular entity type
was expected for each question, and every spe-
cial entity of that type found in a retrieved sen-
tence was regarded as a cluster/topic.
In this paper, we propose two novel ap-
proaches for sentence clustering. The main idea
of the approaches is to conduct sentence cluster-
ing according to the candidate answers which are
also considered as the names of the clusters.
For example, given the question {谁发明了电
话?/who invented telephone?}, the top ten re-
trieved sentences and the corresponding candi-
date answers are shown as Table 1. Thus, we can
conduct sentence clustering according to the
candidate answers, that are, {贝尔/Bell, 西门子
/Siemens, 爱迪生/Edison,库珀/Cooper, 斯蒂芬
/Stephen}.
</bodyText>
<table confidence="0.957380944444444">
ID Top 10 Sentences Candidate Answer
S1 1876 年 3 月 10 日贝尔发明电话/Bell invented telephone 贝尔/Bell
on Oct. 3th, 1876.
S2 西门子发明了电机,贝尔发明电话,爱迪生发明电灯。 西门子/ Siemens
/ Bell, Siemens and Edison invented telephone, electromo- 贝尔/Bell
tor and electric light respectively. 爱迪生/ Edison
S3 话次 成 众焦点 库珀/Cooper
最。
/Recently, the public paid a great deal of attention to Cooper
who is Father of Mobile Phone.
S4 1876 年,发明家贝尔发明了电话。/In 1876, Bell in- 贝尔/Bell
vented telephone.
S5 接着,1876 年,美国科学家贝尔发明了电话;1879 年 贝尔/Bell
美国科学家爱迪生发明了电灯。/Subsequently, American 爱迪生/Edison
scientist Bell invented the phone in 1876; Edison invented
the electric light in 1879.
S6 1876 年 3 月 7 日,贝尔成为电话发明的专利人。/On 贝尔/Bell
March 7th, 1876, Bell became the patentee of telephone.
S7 贝尔不仅发明了电话,还成功地建立了自己的公司推广 贝尔/Bell
电话。/Bell not only invented telephone, but also estab-
lished his own company for spreading his invention.
S8 在首只移动电话投入使用 30年以后,其发明人库珀仍 库珀/Cooper
梦想着未来电话技术实现之日到来。/Thirty years after
the invention of first mobile phone, Cooper still anticipated
the date of the realization of future phone’s technology.
57
S9 库珀表示,消费者采纳移动电话的速度之快令他意外, 库珀/Cooper
但移动电话的普及率还没有达到无所不在,这让他有些
失望。/Cooper said, he was surprised at the speed that the
consumers switched to mobile phones; but the populariza-
tion of mobile phone isn’t omnipresent, which made him a
little bit disappointed.
S10 英国发明家斯蒂芬将移动电话的所有电子元件设计在一 斯蒂芬/Stephen
张纸一样厚薄的芯片上。/England inventor Stephen de-
signed the paper-clicked CMOS chip which included all
electronic components.
</table>
<tableCaption confidence="0.99112">
Table 1 The Top 10 Retrieved Sentences and its Candidate Answers
</tableCaption>
<bodyText confidence="0.999760333333333">
Based on the above analysis, this paper pre-
sents cluster-based language model for sentence
retrieval of Chinese question answering. It dif-
fers from most of the previous approaches
mainly as follows. 1. Sentence Clustering is con-
ducted according to the candidate answers ex-
tracted from the top 1000 sentences. 2. The in-
formation of the cluster of the sentence, which is
also called as topic, is incorporated into language
model through aspect model. For sentence clus-
tering, we propose two novel approaches that are
One-Sentence-Multi-Topics and One-Sentence-
One-Topic respectively. The experimental results
show that the performances of cluster-based lan-
guage model for sentence retrieval are improved
significantly.
The framework of cluster-based language
model for sentence retrieval is shown as Figure 1.
</bodyText>
<figure confidence="0.993403823529412">
Question
Sentence Clus-
tering
Document
Retrieval
Question
Analyzer
Sentence
Splitter
Language Model
for Sentence Re-
trieval
Candidate An-
swer Extraction
Cluster-based Lan-
guage Model for
Sentence Retrieval
</figure>
<figureCaption confidence="0.999137">
Figure 1 The Framework of Cluster-based Language Model for Sentence Retrieval
</figureCaption>
<sectionHeader confidence="0.582061" genericHeader="introduction">
Results
2 Language Model for Information Re-
trieval
</sectionHeader>
<bodyText confidence="0.999966411764706">
Language model for information retrieval is pre-
sented by Ponte &amp; Croft in 1998[J. Ponte, et al.
1998] which has more advantages than vector
space model. After that, many improved models
are proposed like J.F. Gao [J.F Gao, et al. 2004],
C. Zhai [C. Zhai, et al. 2001], and so on. In 1999,
Berger &amp; Lafferty [A. Berger, et al. 1999] pre-
sented statistical translation model for informa-
tion retrieval.
The basic approach of language model for in-
formation retrieval is to model the process of
generating query Q. The approach has two steps.
1. Constructing document model for each docu-
ment in the collection; 2. Ranking the documents
according to the probabilities p(Q|D). A classical
unigram language model for IR could be ex-
pressed in equation (1).
</bodyText>
<equation confidence="0.991093666666667">
p Q  |D
( ) = ∏p(w;  |D)
∈Q
</equation>
<bodyText confidence="0.999479142857143">
where, wi is a query term, p(wi|D) is document
model which represents terms distribution over
document. Obviously, estimating the probability
p(wi|D) is the key of document model. To solve
the sparseness problem, Jelinek-Mercer is com-
monly used which could be expressed by equa-
tion (2).
</bodyText>
<equation confidence="0.9905055">
p(w  |D) α p (w |D) (1 α) p (w |C)
= x ML + - x ML (2)
</equation>
<bodyText confidence="0.998836333333333">
where, puL(w|D) and puL(w|C) are document
model and collection model respectively esti-
mated via maximum likelihood.
</bodyText>
<equation confidence="0.9654425">
(1)
w;
</equation>
<page confidence="0.973732">
58
</page>
<bodyText confidence="0.999990166666667">
As described above, the disadvantages of
standard language model is that it does not make
any one particular document any more probable
than any other, on the condition that neither the
documents originally contain the query term. In
the other word, if a document is relevant, but
does not contain the query term, it is still no
more probable, even though it may be topically
related. Thus, the smoothing approaches based
on standard language model are improper. In this
paper, we propose a novel cluster-based lan-
guage model to overcome it.
</bodyText>
<sectionHeader confidence="0.77458" genericHeader="method">
3 Cluster-based Language Model for
Sentence Retrieval
</sectionHeader>
<bodyText confidence="0.999995142857143">
Note that document model p(w|D) in document
retrieval is replace by p(w|S) called sentence
model in sentence retrieval.
The assumption of cluster-based language
model for retrieval is that topic-related sentences
tend to be relevant to the same query. So, incor-
porating the topic of sentences into language
model can improve the performance of sentence
retrieval based on standard language model.
The proposed cluster-based language model is
a mixture model of three components, that are
sentence model pML(w|S), cluster/topic model
p_topicML(w|T) and collection model pML(w|C).
We can formulate our model as equation (3).
</bodyText>
<equation confidence="0.97905">
p(w  |S) =α×pML(w |S)+(1-α) ×
(β ×p_topicML(w|T)+(1-β)×pML(w|C)) (3)
</equation>
<bodyText confidence="0.999996222222222">
In fact, the cluster-based language model can
also be viewed as a two-stage smoothing ap-
proach. The cluster model is first smoothed using
the collection model, and the sentence model is
then smoothed with the smoothed cluster model.
In this paper, the cluster model is in the form
of term distribution over cluster/topic, associated
with the distribution of clusters/topics over sen-
tence, which can be expressed by equation (4).
</bodyText>
<equation confidence="0.978737333333333">
p_topic w  |T = p w  |t p t  |S
( ) ∑ ( ) ( ) (4)
t∈T
</equation>
<bodyText confidence="0.999274416666667">
where, T is the set of clusters/topics. p_topic(w|T)
is cluster model. p(t|S) is topic sentence distribu-
tion which means the distribution of topic over
sentence. And p(w|t) is term topic distribution
which means the term distribution over topics.
Before estimating the sentence model p(w|S),
topic-related sentences should be organized into
clusters/topics to estimate p(t|S) and p(w|t) prob-
abilities. For sentence clustering, this paper pre-
sents two novel approaches that are One-
Sentence-Multi-Topics and One-Sentence-One-
Topic respectively.
</bodyText>
<subsectionHeader confidence="0.99819">
3.1 One-Sentence-Multi-Topics
</subsectionHeader>
<bodyText confidence="0.999719">
The main idea of One-Sentence-Multi-Topics
can be summarized as follows.
</bodyText>
<listItem confidence="0.996258">
1. If a sentence includes M different candidate
answers, then the sentence consists of M different
topics.
</listItem>
<bodyText confidence="0.9701975">
For example, the sentence S5 in Table 1 includes
two topics which are “贝尔发明电话/Bell in-
vented telephone” and “爱迪生发明电灯/Edison
invented electric light” respectively.
</bodyText>
<listItem confidence="0.938473">
2. Different sentences have the same topic if two
candidate answers are same.
</listItem>
<bodyText confidence="0.996958571428571">
For example, the sentence S4 and S5 in Table 1
have the same topic “贝尔发明电话/Bell in-
vented telephone” because both of sentences
have the same candidate answer “贝尔/Bell”.
Based on the above ideas, the result of sen-
tence clustering based on One-Sentence-Multi-
Topics is shown in Table 2.
</bodyText>
<table confidence="0.999862333333333">
Name of Clusters Sentences
贝尔/Bell S1 S2 S4 S5 S6 S7 S8
西门子/Siemens S2
爱迪生/Edison S2 S5
库珀/Cooper S3 S8 S9
斯蒂芬/Stephen S10
</table>
<tableCaption confidence="0.997373">
Table 2 The Result of One-Sentence-Multi-
</tableCaption>
<subsectionHeader confidence="0.491989">
Topics Sentence Clustering
</subsectionHeader>
<bodyText confidence="0.9963815">
So, we could estimate term topic distribution
using equation (5).
</bodyText>
<equation confidence="0.980924666666667">
n(w, t)
p(w  |t) = ∑n(w &apos; , t) (5 )
w&apos;
</equation>
<bodyText confidence="0.9892095">
Topic sentence distribution can be estimated
using equation (6) and (7).
</bodyText>
<equation confidence="0.998556777777778">
(t  |S) = ∑1 /
(6)
p
st
klst = KL(s   ||t) = ∑pML (w  |s) × logpws
|
ML
pML(w  |t) (7)
w
</equation>
<bodyText confidence="0.999953833333333">
where, klst means the Kullback-Leibler diver-
gence between the sentence with the cluster/topic.
k denotes the number of cluster/topic. The main
idea of equation (6) is that the closer the Kull-
back-Leibler divergence, the larger the topic sen-
tence probability p(t|S).
</bodyText>
<subsectionHeader confidence="0.995873">
3.2 One-Sentence-One-Topic
</subsectionHeader>
<bodyText confidence="0.999836">
The main idea of One-Sentence-One-Topic also
could be summarized as follows.
</bodyText>
<equation confidence="0.944094">
1 k
/
t
l
kl
st
</equation>
<page confidence="0.984732">
59
</page>
<listItem confidence="0.943047333333333">
1. A sentence only has one kernel candidate an-
swer which represents the kernel topic no matter
how many candidate answers is included.
</listItem>
<bodyText confidence="0.99920075">
For example, the kernel topic of sentence S5 in
Table 1 is “Alt,,XAt,-i*/Bell invented tele-
phone” though it includes three different candi-
date answers.
</bodyText>
<listItem confidence="0.959653571428571">
2. Different sentences have the same topic if two
kernel candidate answers are same.
For example, the sentence S4 and S5 in Table 1
have the same topic “Alt,,X -i*/Bell in-
vented telephone”.
3. The kernel candidate answer has shortest av-
erage distance to all query terms.
</listItem>
<bodyText confidence="0.850785">
Based on the above ideas, the result of sen-
tence clustering based on One-Sentence-One-
Topic is shown in Table 3.
</bodyText>
<table confidence="0.99982225">
Name of Clusters Sentences
Alt,,/Bell S1 S2 S4 S5 S6 S7
AA/Cooper S3 S8 S9
-# */Stephen S10
</table>
<tableCaption confidence="0.8982805">
Table 3 The Result of One-Sentence-One-Topic
Sentence Clustering
</tableCaption>
<bodyText confidence="0.999721428571428">
Equation (8) and (9) can be used to estimate
the kernel candidate answer and the distances of
candidate answers respectively. Term topic dis-
tribution in One-Sentence-One-Topic can be es-
timated via equation (5). And topic sentence dis-
tribution is equal to 1 because a sentence only
belongs to one cluster/topic.
</bodyText>
<equation confidence="0.943327">
ai * = argmin �emDisai } (8)
a i
</equation>
<bodyText confidence="0.999179875">
presented by [Youzheng Wu, et al. 2004] which
is similar to TREC question answering track
[Ellen. M. Voorhees. 2004]. The documents col-
lection is downloaded from Internet which size is
1.8GB. The testing questions are collected via
four different approaches which has 7050 Chi-
nese questions currently.
In this section, we randomly select 807 testing
questions which are fact-based short-answer
questions. Moreover, the answers of all testing
questions are named entities identified by
[Youzheng Wu, et al. 2005]. Figure 2 gives the
details. Note that, LOC, ORG, PER, NUM and
TIM denote the questions which answer types
are location, organization, person, number and
time respectively, SUM means all question types.
</bodyText>
<figureCaption confidence="0.682321857142857">
Figure 2 The Distribution of Various Question
Types over Testing Questions
Chinese question answering system is to re-
turn a ranked list of five answer sentences per
question and will be strictly evaluated (unsup-
ported answers counted as wrong) using mean
reciprocal rank (MRR).
</figureCaption>
<figure confidence="0.942763230769231">
311
28
168
135
PER LOC ORG TIM NUM
400
300
200
100
0
165
∑SemDis ()ai, qj
j
</figure>
<equation confidence="0.546197">
SemDis =
ai
N
(9)
SemDis(ai,qj)= Positionai -Positionqj (10)
</equation>
<bodyText confidence="0.985105833333333">
where, ai* is the kernel candidate answer. ai is
the i-th candidate answer, SemDisai is the average
distance of i-th candidate answer. qj is the j-th
query term, N is the number of all query terms.
Positionqj and Positionai mean the position of
query term qj and candidate answer ai.
</bodyText>
<sectionHeader confidence="0.991729" genericHeader="method">
4 Experiments and Analysis
</sectionHeader>
<bodyText confidence="0.98484325">
Research on Chinese question answering, is still
at its early stage. And there is no public evalua-
tion platform for Chinese question answering. So
in this paper, we use the evaluation environment
</bodyText>
<subsectionHeader confidence="0.995748">
4.1 Baseline: Standard Language Model for
Sentence Retrieval
</subsectionHeader>
<bodyText confidence="0.999649">
Based on the standard language model for infor-
mation retrieval, we can get the baseline per-
formance, as is shown in Table 4, where α is the
weight of document model.
</bodyText>
<table confidence="0.948786428571428">
α 0.6 0.7 0.8 0.9
LOC 49.95 51.50 52.63 54.54
ORG 53.69 51.01 50.12 51.01
PER 63.10 64.42 65.94 65.69
NUM 48.43 49.86 51.78 53.26
TIM 56.97 58.38 58.77 61.49
SUM 53.98 55.28 56.40 57.93
</table>
<tableCaption confidence="0.995867">
Table 4 The Baseline MRR5 Performance
</tableCaption>
<page confidence="0.998443">
60
</page>
<bodyText confidence="0.999682">
In the following chapter, we conduct experi-
ments to answer two questions.
</bodyText>
<listItem confidence="0.998997666666667">
1. Whether cluster-based language model for
sentence retrieval could improve the perform-
ance of standard language model for sentence
retrieval?
2. What are the performances of sentence clus-
tering for various question types?
</listItem>
<subsectionHeader confidence="0.8789225">
4.2 Cluster-based Language Model for Sen-
tence Retrieval
</subsectionHeader>
<bodyText confidence="0.998621166666667">
In this part, we will conduct experiments to vali-
date the performances of cluster-based language
models which are based on One-Sentence-Multi-
Topics and One-Sentence-One-Topic sentence
clustering respectively. In the following experi-
ments, β = 0.9.
</bodyText>
<sectionHeader confidence="0.759808" genericHeader="method">
4.2.1 Cluster-based Language Model Based
on One-Sentence-Multi-Topics
</sectionHeader>
<bodyText confidence="0.9993584">
The experimental results of cluster-based lan-
guage model based on One-Sentence-Multi-
Topics sentence clustering are shown in Table 5.
The relative improvements are listed in the
bracket.
</bodyText>
<table confidence="0.998483692307692">
α 0.6 0.7 0.8 0.9
LOC 55.57 55.61 56.59 57.70
(+11.2) (+7.98) (+7.52) (+5.79)
ORG 59.05 59.46 59.46 59.76
(+9.98) (+16.6) (+18.6) (+17.2)
PER 67.73 68.03 67.71 67.45
(+7.34) (+5.60) (+2.68) (+2.68)
52.79 53.90 54.45 55.51
NUM (+9.00) (+8.10) (+5.16) (+4.22)
TIM 60.17 60.63 62.33 61.68
(+5.62) (+3.85) (+6.06) (+0.31)
SUM 58.14 58.63 59.30 59.54
(+7.71) (+6.06) (+5.14) (+2.78)
</table>
<tableCaption confidence="0.809267666666667">
Table 5 MRR5 Performance of Cluster-based
Language Model Based on One-Sentence-Multi-
Topics
</tableCaption>
<bodyText confidence="0.9999399">
From the experimental results, we can find
that by integrating the clusters/topics of the sen-
tence into language model, we can achieve much
improvement at each stage of α. For example, the
largest and smallest improvements for all types
of questions are about 7.7% and 2.8% respec-
tively. This experiment shows that the proposed
cluster-based language model based on One-
Sentence-Multi-Topics is effective for sentence
retrieval in Chinese question answering.
</bodyText>
<subsectionHeader confidence="0.410015">
4.2.2 Cluster-based Language Model Based
on One-Sentence-One-Topic
</subsectionHeader>
<bodyText confidence="0.998037">
The performance of cluster-based language
model based on One-Sentence-One-Topic sen-
tence clustering is shown in Table 6. The relative
improvements are listed in the bracket.
</bodyText>
<table confidence="0.999598846153846">
α 0.6 0.7 0.8 0.9
LOC 53.02 54.27 56.14 56.28
(+6.15) (+5.38) (+6.67) (+3.19)
ORG 58.75 58.75 59.46 59.46
(+9.42) (+17.2) (+18.6) (+16.6)
PER 66.57 67.07 67.44 67.29
(+5.50) (+4.11) (+2.27) (+2.44)
NUM 49.95 50.87 52.15 53.51
(+3.14) (+2.02) (+0.71) (+0.47)
TIM 59.75 60.65 62.71 62.20
(+4.88) (+3.89) (+6.70) (+1.15)
SUM 56.48 57.65 58.82 59.22
(+4.63) (+4.29) (+4.29) (+2.23)
</table>
<tableCaption confidence="0.861443">
Table 6 MRR5 Performance of Cluster-based
Language Model Based on One-Sentence-One-
Topic
</tableCaption>
<bodyText confidence="0.997640333333333">
In Comparison with Table 5, we can find that
the improvement of cluster-based language
model based on One-Sentence-One-Topic is
slightly lower than that of cluster-based language
model based on One-Sentence-Multi-Topics. The
reasons lie in that Clusters based on One-
Sentence-One-Topic approach are very coarse
and much information is lost. But the improve-
ments over baseline system are obvious.
Table 7 shows that MRR1 and MRR20 scores
of cluster-based language models for all question
types. The relative improvements over the base-
line are listed in the bracket. This experiment is
to validate whether the conclusion based on dif-
ferent measurements is consistent or not.
</bodyText>
<table confidence="0.998925454545455">
One-Sentence- One-Sentence-
Multi-Topics One-Topic
α MRR1 MRR20 MRR1 MRR20
0.6 50.00 59.60 48.33 57.70
(+14.97) (+7.66) (+10.37) (+4.23)
0.7 50.99 60.03 49.44 58.62
(+13.36) (+6.12) (+9.92) (+3.62)
0.8 51.05 60.68 51.05 60.01
(+8.99) (+5.06) (+8.99) (+3.90)
0.9 51.92 61.05 51.30 60.25
(+5.81) (+2.97) (+4.54) (+1.62)
</table>
<tableCaption confidence="0.9652055">
Table 7 MRR1 and MRR20 Performances of
Two Cluster-based Language Models
</tableCaption>
<page confidence="0.998982">
61
</page>
<bodyText confidence="0.955431888888889">
Table 7 also shows that the performances of
two cluster-based language models are higher
than that of the baseline system under different
measurements. For MRR1 scores, the largest
improvements of cluster-based language models
based on One-Sentence-Multi-Topics and One-
Sentence-One-Topic are about 15% and 10%
respectively. For MRR20, the largest improve-
ments are about 7% and 4% respectively.
</bodyText>
<figureCaption confidence="0.51928225">
Conclusion 1: The experiments show that the
proposed cluster-based language model can im-
prove the performance of sentence retrieval in
Chinese question answering under the various
measurements. Moreover, the performance of
clustering-based language model based on One-
Sentence-Multi-Topics is better than that based
on One-Sentence-One-Topic.
</figureCaption>
<subsectionHeader confidence="0.9994215">
4.3 The Analysis of Sentence Clustering for
Various Question Types
</subsectionHeader>
<bodyText confidence="0.999970666666667">
The parameter β in equation (3) denotes the bal-
ancing factor of the cluster model and the collec-
tion model. The larger β, the larger contribution
of the cluster model. The small β, the larger con-
tribution of the collection model. If the perform-
ance of sentence retrieval decreased with the in-
creasing of β, it means that there are many noises
in sentence clustering. Otherwise, sentence clus-
tering is satisfactory for cluster-based language
model. So the task of this experiment is to find
the performances of sentence clustering for vari-
ous question types, which is helpful to select the
most proper β to obtain the best performance of
sentence retrieval.
With the change of β and the fixed α (α = 0.9),
the performances of cluster-based language
model based on One-Sentence-Multi-Topics are
shown in Figure 3.
</bodyText>
<figure confidence="0.719833">
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
</figure>
<figureCaption confidence="0.893934">
Figure 3 MRR5 Performances of Cluster-based
Language Model Based on One-Sentence-Multi-
Topics with the Change of β
</figureCaption>
<bodyText confidence="0.999375666666667">
In Figure 3, the performances of TIM and
NUM type questions decreased with the increas-
ing of the parameter β (from 0.6 to 0.9), while
the performances of LOC, PER and ORG type
questions increased. This phenomenon showed
that the performance of sentence clustering based
on One-Sentence-Multi-Topics for TIM and
NUM type questions is not as good as that for
LOC, PER and ORG type questions. This is in
fact reasonable. The number and time words fre-
quently appeared in the sentence, which does not
represent a cluster/topic when they appear. While
PER, LOC and ORG entities can represent a
topic when they appeared in the sentence.
Similarly, with the change of β and the fixed α
(α=0.9), the performances of cluster-based lan-
guage model based on One-Sentence-One-Topic
are shown in Figure 4.
</bodyText>
<figure confidence="0.781739">
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
</figure>
<figureCaption confidence="0.989627666666667">
Figure 4 MRR5 Performance of Cluster-based
Language Model Based on One-Sentence-One-
Topic with the Change of β
</figureCaption>
<bodyText confidence="0.999762181818182">
In Figure 4, the performances of TIM, NUM,
LOC and SUM type questions decreased with the
increasing of β (from 0.6 to 0.9). This phenome-
non shows that the performances of sentence
clustering based on One-Sentence-One-Topic are
not satisfactory for most of question types. But,
compared to the baseline system, the cluster-
based language model based on this kind of sen-
tence clustering can still improve the perform-
ances of sentence retrieval in Chinese question
answering.
</bodyText>
<construct confidence="0.655861555555556">
Conclusion 2: The performance of the pro-
posed sentence clustering based on One-
Sentence-Multi-Topics for PER, LOC and ORG
type questions is higher than that for TIM and
NUM type questions. Thus, for PER, LOC and
ORG questions, we should choose the larger β
value (about 0.9) in cluster-based language
model based on One-Sentence-Multi-Topics.
While for TIM and NUM type questions, the
</construct>
<figure confidence="0.9993610625">
0.68
0.66
0.64
0.62
0.6
SUM
LOC
ORG
PER
NUM
TIM
0.58
0.56
0.54
0.52
0.5
0.68
0.66
0.64
0.62
0.6
0.58
0.56
0.54
0.52
0.5
SUM
LOC
ORG
PER
NUM
TIM
</figure>
<page confidence="0.993884">
62
</page>
<bodyText confidence="0.991140666666667">
value of β should be smaller (about 0.5). But, the
performance of sentence clustering based on
One-Sentence-One-Topic for all questions is not
ideal, so the value for cluster-based language
model based on One-Sentence-One-Topic should
be smaller (about 0.5) for all questions.
</bodyText>
<sectionHeader confidence="0.98593" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.99998796">
The input of a question answering system is
natural language question which contains richer
information than the query in traditional docu-
ment retrieval. Such richer information can be
used in each module of question answering sys-
tem. In this paper, we presented a novel cluster-
based language model for sentence retrieval in
Chinese question answering which combines the
sentence model, the cluster/topic model and the
collection model.
For sentence clustering, we presented two ap-
proaches that are One-Sentence-Multi-Topics
and One-Sentence-One-Topic respectively. The
experimental results showed that the proposed
cluster-based language model could improve the
performances of sentence retrieval in Chinese
question answering significantly.
However, we only conduct sentence clustering
for questions, which have the property that their
answers are named entities in this paper. In the
future work, we will focus on all other type ques-
tions and improve the performance of the sen-
tence retrieval by introducing the structural, syn-
tactic and semantic information into language
model.
</bodyText>
<sectionHeader confidence="0.9903" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.99990228358209">
J. Ponte, W. Bruce Croft. A Language Modeling Ap-
proach to Information Retrieval. In the Proceedings
of ACM SIGIR 1998, pp 275-281, 1998.
C. Zhai, J. Lafferty. A Study of Smoothing Tech-
niques for Language Modeling Applied to ad hoc
Information Retrieval. In the Proceedings of the
ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval, 2001.
Ittycheriah, S. Roukos. IBM&apos;s Statistical Question
Answering System-TREC 11. In the Eleventh Text
Retrieval Conference (TREC 2002), Gaithersburg,
Maryland, November 2002.
Hui Yang, Tat-Seng Chua. The Integration of Lexical
Knowledge and External Resources for Question
Answering. In the Proceedings of the Eleventh
Text REtrieval Conference (TREC’2002), Mary-
land, USA, 2002, page 155-161.
Andres Corrada-Emmanuel, W.Bruce Croft, Vanessa
Murdock. Answer Passage Retrieval for Question
Answering. In the Proceedings of the 27th Annual
International Conference on Research and Devel-
opment in Information Retrieval, pp. 516 – 517,
2004.
Ellen M. Voorhees. Overview of the TREC 2004
Question Answering Track. In Proceedings of the
Twelfth Text REtrieval Conference (TREC 2004),
2004.
Vanessa Murdock, W. Bruce Croft. Simple Transla-
tion Models for Sentence Retrieval in Factoid
Question Answering. In the Proceedings of the
SIGIR 2004 Workshop on Information Retrieval
for Question Answering, pp.31-35, 2004.
Thomas Hofmann. Probabilistic Latent Semantic In-
dexing. In the Proceedings of the Twenty-Second
Annual International SIGIR Conference on Re-
search and Development in Information Retrieval,
1999.
A. Berger and J. Lafferty. Information Retrieval as
Statistical Translation. In the Proceedings of ACM
SIGIR-1999, pp. 222—229, Berkeley, CA, August
1999.
A. Echihabi and D.Marcu. A noisy-channel approach
to question answering. In the Proceeding of the
41st Annual Meeting of the Association for Com-
putational Linguistics, Sappora, Japan, 2003.
Leif Azzopardi, Mark Girolami and Keith van
Rijsbergen. Topic Based Language Models for ad
hoc Information Retrieval. In the Proceeding of
IJCNN 2004 &amp; FUZZ-IEEE 2004, July 25-29,
2004, Budapest, Hungary.
Jian-Yun Nie. Integrating Term Relationships into
Language Models for Information Retrieval. Re-
port at ICT-CAS.
Jianfeng Gao, Jian-Yun Nie, Guangyuan Wu and
Guihong Cao. 2004b. Dependence language model
for information retrieval. In SIGIR-2004. Sheffield,
UK, July 25-29.
Youzheng Wu, Jun Zhao, Bo Xu. Chinese Named
Entity Recognition Model Based on Multiple Fea-
tures. In the Proceeding of HLT/EMNLP 2005,
Vancouver, B.C., Canada, pp.427-434, 2005.
Youzheng Wu, Jun Zhao, Xiangyu Duan and Bo Xu.
Building an Evaluation Platform for Chinese Ques-
tion Answering Systems. In Proceeding of the First
National Conference on Information Retrieval and
Content Security. Shanghai, China, December,
2004.(In Chinese)
</reference>
<page confidence="0.999457">
63
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.829691">
<title confidence="0.9962555">Cluster-based Language Model for Sentence Retrieval in Question Answering</title>
<author confidence="0.999363">Youzheng Wu Jun Zhao Bo Xu</author>
<affiliation confidence="0.9970955">National Laboratory of Pattern Institute of Automation Chinese Academy of</affiliation>
<address confidence="0.890072">No.95 Zhongguancun East Road, 100080, Beijing,</address>
<email confidence="0.989108">(yzwu,jzhao,boxu)@nlpr.ia.ac.cn</email>
<abstract confidence="0.997626181818182">retrieval plays a very important role in question answering system. In this paper, we present a novel cluster-based language model for sentence retrieval in Chinese question answering which is motivated in part by sentence clustering and language model. Sentence clustering is used to group sentences into clusters. Language model is used to properly represent sentences, which is combined with sentences model, cluster/topic model and collection model. For sentence clustering, we propose two approaches that are One- Sentence-Multi-Topics and One- Sentence-One-Topic respectively. From the experimental results on 807 Chinese testing questions, we can conclude that the proposed cluster-based language model outperforms over the standard language model for sentence retrieval in Chinese question answering.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Ponte</author>
<author>W Bruce Croft</author>
</authors>
<title>A Language Modeling Approach to Information Retrieval.</title>
<date>1998</date>
<booktitle>In the Proceedings of ACM SIGIR</booktitle>
<pages>275--281</pages>
<marker>Ponte, Croft, 1998</marker>
<rawString>J. Ponte, W. Bruce Croft. A Language Modeling Approach to Information Retrieval. In the Proceedings of ACM SIGIR 1998, pp 275-281, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Zhai</author>
<author>J Lafferty</author>
</authors>
<title>A Study of Smoothing Techniques for Language Modeling Applied to ad hoc Information Retrieval.</title>
<date>2001</date>
<booktitle>In the Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<marker>Zhai, Lafferty, 2001</marker>
<rawString>C. Zhai, J. Lafferty. A Study of Smoothing Techniques for Language Modeling Applied to ad hoc Information Retrieval. In the Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Roukos Ittycheriah</author>
</authors>
<title>IBM&apos;s Statistical Question Answering System-TREC 11.</title>
<date>2002</date>
<booktitle>In the Eleventh Text Retrieval Conference (TREC 2002),</booktitle>
<location>Gaithersburg, Maryland,</location>
<marker>Ittycheriah, 2002</marker>
<rawString>Ittycheriah, S. Roukos. IBM&apos;s Statistical Question Answering System-TREC 11. In the Eleventh Text Retrieval Conference (TREC 2002), Gaithersburg, Maryland, November 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Yang</author>
</authors>
<title>Tat-Seng Chua. The Integration of Lexical Knowledge and External Resources for Question Answering.</title>
<date>2002</date>
<booktitle>In the Proceedings of the Eleventh Text REtrieval Conference (TREC’2002),</booktitle>
<pages>155--161</pages>
<location>Maryland, USA,</location>
<marker>Yang, 2002</marker>
<rawString>Hui Yang, Tat-Seng Chua. The Integration of Lexical Knowledge and External Resources for Question Answering. In the Proceedings of the Eleventh Text REtrieval Conference (TREC’2002), Maryland, USA, 2002, page 155-161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andres Corrada-Emmanuel</author>
<author>W Bruce Croft</author>
<author>Vanessa Murdock</author>
</authors>
<title>Answer Passage Retrieval for Question Answering.</title>
<date>2004</date>
<booktitle>In the Proceedings of the 27th Annual International Conference on Research and Development in Information Retrieval,</booktitle>
<pages>516--517</pages>
<marker>Corrada-Emmanuel, Croft, Murdock, 2004</marker>
<rawString>Andres Corrada-Emmanuel, W.Bruce Croft, Vanessa Murdock. Answer Passage Retrieval for Question Answering. In the Proceedings of the 27th Annual International Conference on Research and Development in Information Retrieval, pp. 516 – 517, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>Overview of the TREC</title>
<date>2004</date>
<booktitle>In Proceedings of the Twelfth Text REtrieval Conference (TREC</booktitle>
<marker>Voorhees, 2004</marker>
<rawString>Ellen M. Voorhees. Overview of the TREC 2004 Question Answering Track. In Proceedings of the Twelfth Text REtrieval Conference (TREC 2004), 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vanessa Murdock</author>
<author>W Bruce Croft</author>
</authors>
<title>Simple Translation Models for Sentence Retrieval in Factoid Question Answering.</title>
<date>2004</date>
<booktitle>In the Proceedings of the SIGIR 2004 Workshop on Information Retrieval for Question Answering,</booktitle>
<pages>31--35</pages>
<marker>Murdock, Croft, 2004</marker>
<rawString>Vanessa Murdock, W. Bruce Croft. Simple Translation Models for Sentence Retrieval in Factoid Question Answering. In the Proceedings of the SIGIR 2004 Workshop on Information Retrieval for Question Answering, pp.31-35, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Probabilistic Latent Semantic Indexing.</title>
<date>1999</date>
<booktitle>In the Proceedings of the Twenty-Second Annual International SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<marker>Hofmann, 1999</marker>
<rawString>Thomas Hofmann. Probabilistic Latent Semantic Indexing. In the Proceedings of the Twenty-Second Annual International SIGIR Conference on Research and Development in Information Retrieval, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>J Lafferty</author>
</authors>
<title>Information Retrieval as Statistical Translation.</title>
<date>1999</date>
<booktitle>In the Proceedings of ACM SIGIR-1999,</booktitle>
<pages>222--229</pages>
<location>Berkeley, CA,</location>
<marker>Berger, Lafferty, 1999</marker>
<rawString>A. Berger and J. Lafferty. Information Retrieval as Statistical Translation. In the Proceedings of ACM SIGIR-1999, pp. 222—229, Berkeley, CA, August 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Echihabi</author>
<author>D Marcu</author>
</authors>
<title>A noisy-channel approach to question answering.</title>
<date>2003</date>
<booktitle>In the Proceeding of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Sappora, Japan,</location>
<marker>Echihabi, Marcu, 2003</marker>
<rawString>A. Echihabi and D.Marcu. A noisy-channel approach to question answering. In the Proceeding of the 41st Annual Meeting of the Association for Computational Linguistics, Sappora, Japan, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leif Azzopardi</author>
<author>Mark Girolami</author>
<author>Keith van Rijsbergen</author>
</authors>
<title>Topic Based Language Models for ad hoc Information Retrieval.</title>
<date>2004</date>
<booktitle>In the Proceeding of IJCNN 2004 &amp; FUZZ-IEEE</booktitle>
<location>Budapest, Hungary.</location>
<marker>Azzopardi, Girolami, van Rijsbergen, 2004</marker>
<rawString>Leif Azzopardi, Mark Girolami and Keith van Rijsbergen. Topic Based Language Models for ad hoc Information Retrieval. In the Proceeding of IJCNN 2004 &amp; FUZZ-IEEE 2004, July 25-29, 2004, Budapest, Hungary.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jian-Yun Nie</author>
</authors>
<title>Integrating Term Relationships into Language Models for Information Retrieval.</title>
<tech>Report at ICT-CAS.</tech>
<marker>Nie, </marker>
<rawString>Jian-Yun Nie. Integrating Term Relationships into Language Models for Information Retrieval. Report at ICT-CAS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Jian-Yun Nie</author>
<author>Guangyuan Wu</author>
<author>Guihong Cao</author>
</authors>
<title>Dependence language model for information retrieval.</title>
<date>2004</date>
<booktitle>In SIGIR-2004.</booktitle>
<location>Sheffield, UK,</location>
<contexts>
<context position="8867" citStr="Gao, et al. 2004" startWordPosition="1331" endWordPosition="1334">model for sentence retrieval is shown as Figure 1. Question Sentence Clustering Document Retrieval Question Analyzer Sentence Splitter Language Model for Sentence Retrieval Candidate Answer Extraction Cluster-based Language Model for Sentence Retrieval Figure 1 The Framework of Cluster-based Language Model for Sentence Retrieval Results 2 Language Model for Information Retrieval Language model for information retrieval is presented by Ponte &amp; Croft in 1998[J. Ponte, et al. 1998] which has more advantages than vector space model. After that, many improved models are proposed like J.F. Gao [J.F Gao, et al. 2004], C. Zhai [C. Zhai, et al. 2001], and so on. In 1999, Berger &amp; Lafferty [A. Berger, et al. 1999] presented statistical translation model for information retrieval. The basic approach of language model for information retrieval is to model the process of generating query Q. The approach has two steps. 1. Constructing document model for each document in the collection; 2. Ranking the documents according to the probabilities p(Q|D). A classical unigram language model for IR could be expressed in equation (1). p Q |D ( ) = ∏p(w; |D) ∈Q where, wi is a query term, p(wi|D) is document model which re</context>
</contexts>
<marker>Gao, Nie, Wu, Cao, 2004</marker>
<rawString>Jianfeng Gao, Jian-Yun Nie, Guangyuan Wu and Guihong Cao. 2004b. Dependence language model for information retrieval. In SIGIR-2004. Sheffield, UK, July 25-29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Youzheng Wu</author>
<author>Jun Zhao</author>
<author>Bo Xu</author>
</authors>
<title>Chinese Named Entity Recognition Model Based on Multiple Features.</title>
<date></date>
<booktitle>In the Proceeding of HLT/EMNLP 2005,</booktitle>
<pages>427--434</pages>
<location>Vancouver, B.C.,</location>
<marker>Wu, Zhao, Xu, </marker>
<rawString>Youzheng Wu, Jun Zhao, Bo Xu. Chinese Named Entity Recognition Model Based on Multiple Features. In the Proceeding of HLT/EMNLP 2005, Vancouver, B.C., Canada, pp.427-434, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Youzheng Wu</author>
<author>Jun Zhao</author>
<author>Xiangyu Duan</author>
<author>Bo Xu</author>
</authors>
<title>Building an Evaluation Platform for Chinese Question Answering Systems.</title>
<date>2004</date>
<booktitle>In Proceeding of the First National Conference on Information Retrieval and Content Security.</booktitle>
<publisher>Chinese</publisher>
<location>Shanghai, China,</location>
<contexts>
<context position="14964" citStr="Wu, et al. 2004" startWordPosition="2342" endWordPosition="2345"> of sentence clustering based on One-Sentence-OneTopic is shown in Table 3. Name of Clusters Sentences Alt,,/Bell S1 S2 S4 S5 S6 S7 AA/Cooper S3 S8 S9 -# */Stephen S10 Table 3 The Result of One-Sentence-One-Topic Sentence Clustering Equation (8) and (9) can be used to estimate the kernel candidate answer and the distances of candidate answers respectively. Term topic distribution in One-Sentence-One-Topic can be estimated via equation (5). And topic sentence distribution is equal to 1 because a sentence only belongs to one cluster/topic. ai * = argmin �emDisai } (8) a i presented by [Youzheng Wu, et al. 2004] which is similar to TREC question answering track [Ellen. M. Voorhees. 2004]. The documents collection is downloaded from Internet which size is 1.8GB. The testing questions are collected via four different approaches which has 7050 Chinese questions currently. In this section, we randomly select 807 testing questions which are fact-based short-answer questions. Moreover, the answers of all testing questions are named entities identified by [Youzheng Wu, et al. 2005]. Figure 2 gives the details. Note that, LOC, ORG, PER, NUM and TIM denote the questions which answer types are location, organ</context>
</contexts>
<marker>Wu, Zhao, Duan, Xu, 2004</marker>
<rawString>Youzheng Wu, Jun Zhao, Xiangyu Duan and Bo Xu. Building an Evaluation Platform for Chinese Question Answering Systems. In Proceeding of the First National Conference on Information Retrieval and Content Security. Shanghai, China, December, 2004.(In Chinese)</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>