<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000056">
<title confidence="0.857171">
SemEval-2007 Task 16: Evaluation of Wide Coverage Knowledge Resources
</title>
<author confidence="0.818166">
Montse Cuadros
</author>
<affiliation confidence="0.800672">
TALP Research Center
</affiliation>
<address confidence="0.835939">
Universitat Polit´ecnica de Catalunya
Barcelona, Spain
</address>
<email confidence="0.998124">
cuadros@lsi.upc.edu
</email>
<note confidence="0.5995005">
German Rigau
IXA NLP Group
</note>
<address confidence="0.45569">
Euskal Herriko Unibersitatea
Donostia, Spain
</address>
<email confidence="0.994591">
german.rigau@ehu.es
</email>
<sectionHeader confidence="0.998568" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999237466666667">
This task tries to establish the relative qual-
ity of available semantic resources (derived
by manual or automatic means). The qual-
ity of each large-scale knowledge resource
is indirectly evaluated on a Word Sense Dis-
ambiguation task. In particular, we use
Senseval-3 and SemEval-2007 English Lex-
ical Sample tasks as evaluation bechmarks
to evaluate the relative quality of each re-
source. Furthermore, trying to be as neu-
tral as possible with respect the knowledge
bases studied, we apply systematically the
same disambiguation method to all the re-
sources. A completely different behaviour is
observed on both lexical data sets (Senseval-
</bodyText>
<sectionHeader confidence="0.769129" genericHeader="keywords">
3 and SemEval-2007).
1 Introduction
</sectionHeader>
<bodyText confidence="0.997893934782609">
Using large-scale knowledge bases, such as Word-
Net (Fellbaum, 1998), has become a usual, often
necessary, practice for most current Natural Lan-
guage Processing (NLP) systems. Even now, build-
ing large and rich enough knowledge bases for
broad–coverage semantic processing takes a great
deal of expensive manual effort involving large re-
search groups during long periods of development.
In fact, dozens of person-years have been invested in
the development of wordnets for various languages
(Vossen, 1998). For example, in more than ten years
of manual construction (from version 1.5 to 2.1),
WordNet passed from 103,445 semantic relations to
245,509 semantic relations1. That is, around one
thousand new relations per month. But this data
does not seems to be rich enough to support ad-
vanced concept-based NLP applications directly. It
seems that applications will not scale up to work-
ing in open domains without more detailed and rich
general-purpose (and also domain-specific) seman-
tic knowledge built by automatic means.
Fortunately, during the last years, the research
community has devised a large set of innovative
methods and tools for large-scale automatic acqui-
sition of lexical knowledge from structured and un-
structured corpora. Among others we can men-
tion eXtended WordNet (Mihalcea and Moldovan,
2001), large collections of semantic preferences ac-
quired from SemCor (Agirre and Martinez, 2001;
Agirre and Martinez, 2002) or acquired from British
National Corpus (BNC) (McCarthy, 2001), large-
scale Topic Signatures for each synset acquired from
the web (Agirre and de la Calle, 2004) or acquired
from the BNC (Cuadros et al., 2005). Obviously,
these semantic resources have been acquired using a
very different set of methods, tools and corpora, re-
sulting on a different set of new semantic relations
between synsets (or between synsets and words).
Many international research groups are working
on knowledge-based WSD using a wide range of ap-
proaches (Mihalcea, 2006). However, less attention
has been devoted on analysing the quality of each
semantic resource. In fact, each resource presents
different volume and accuracy figures (Cuadros et
al., 2006).
In this paper, we evaluate those resources on the
</bodyText>
<footnote confidence="0.963137">
1Symmetric relations are counted only once.
</footnote>
<page confidence="0.993516">
81
</page>
<bodyText confidence="0.9608645">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 81–86,
Prague, June 2007. c�2007 Association for Computational Linguistics
SemEval-2007 English Lexical Sample task. For
comparison purposes, we also include the results of
the same resources on the Senseval-3 English Lex-
ical sample task. In both cases, we used only the
nominal part of both data sets and we also included
some basic baselines.
</bodyText>
<sectionHeader confidence="0.993499" genericHeader="method">
2 Evaluation Framework
</sectionHeader>
<bodyText confidence="0.999970366666666">
In order to compare the knowledge resources, all the
resources are evaluated as Topic Signatures (TS).
That is, word vectors with weights associated to a
particular synset. Normally, these word vectors are
obtained by collecting from the resource under study
the word senses appearing as direct relatives. This
simple representation tries to be as neutral as possi-
ble with respect to the resources studied.
A common WSD method has been applied to
all knowledge resources on the test examples of
Senseval-3 and SemEval-2007 English lexical sam-
ple tasks. A simple word overlapping counting is
performed between the Topic Signature and the test
example. The synset having higher overlapping
word counts is selected. In fact, this is a very sim-
ple WSD method which only considers the topical
information around the word to be disambiguated.
Finally, we should remark that the results are not
skewed (for instance, for resolving ties) by the most
frequent sense in WN or any other statistically pre-
dicted knowledge.
As an example, table 1 shows a test example of
SemEval-2007 corresponding to the first sense of the
noun capital. In bold there are the words that appear
in its corresponding Topic Signature acquired from
the web.
Note that although there are several important
related words, the WSD process implements ex-
act word form matching (no preprocessing is per-
formed).
</bodyText>
<subsectionHeader confidence="0.995478">
2.1 Basic Baselines
</subsectionHeader>
<bodyText confidence="0.906057142857143">
We have designed a number of basic baselines in
order to establish a complete evaluation framework
for comparing the performance of each semantic re-
source on the English WSD tasks.
RANDOM: For each target word, this method se-
lects a random sense. This baseline can be consid-
ered as a lower-bound.
</bodyText>
<table confidence="0.999852166666667">
Baselines P R F1
TRAIN 65.1 65.1 65.1
TRAIN-MFS 54.5 54.5 54.5
WN-MFS 53.0 53.0 53.0
SEMCOR-MFS 49.0 49.1 49.0
RANDOM 19.1 19.1 19.1
</table>
<tableCaption confidence="0.800649">
Table 2: P, R and F1 results for English Lexical Sam-
ple Baselines of Senseval-3
</tableCaption>
<bodyText confidence="0.996800897435897">
SemCor MFS (SEMCOR-MFS): This method
selects the most frequent sense of the target word
in SemCor.
WordNet MFS (WN-MFS): This method selects
the first sense in WN1.6 of the target word.
TRAIN-MFS: This method selects the most fre-
quent sense in the training corpus of the target word.
Train Topic Signatures (TRAIN): This baseline
uses the training corpus to directly build a Topic Sig-
nature using TFIDF measure for each word sense.
Note that this baseline can be considered as an
upper-bound of our evaluation.
Table 2 presents the precision (P), recall (R) and
F1 measure (harmonic mean of recall and preci-
sion) of the different baselines in the English Lexical
Sample exercise of Senseval-3. In this table, TRAIN
has been calculated with a vector size of at maxi-
mum 450 words. As expected, RANDOM baseline
obtains the poorest result. The most frequent senses
obtained from SemCor (SEMCOR-MFS) and WN
(WN-MFS) are both below the most frequent sense
of the training corpus (TRAIN-MFS). However, all
of them are far below the Topic Signatures acquired
using the training corpus (TRAIN).
Table 3 presents the precision (P), recall (R) and
F1 measure (harmonic mean of recall and preci-
sion) of the different baselines in the English Lexical
Sample exercise of SemEval-2007. Again, TRAIN
has been calculated with a vector size of at max-
imum 450 words. As before, RANDOM baseline
obtains the poorest result. The most frequent senses
obtained from SemCor (SEMCOR-MFS) and WN
(WN-MFS) are both far below the most frequent
sense of the training corpus (TRAIN-MFS), and all
of them are below the Topic Signatures acquired us-
ing the training corpus (TRAIN).
Comparing both lexical sample sets, SemEval-
2007 data appears to be more skewed and simple for
WSD systems than the data set from Senseval-3: less
</bodyText>
<page confidence="0.979155">
82
</page>
<bodyText confidence="0.969833375">
&lt;instance id=”19:0@11@wsj/01/wsj 0128@wsj@en@on” docsrc=”wsj”&gt; &lt;context&gt;
“ A sweeping restructuring of the industry is possible . ” Standard &amp; Poor ’s Corp. says First Boston , Shearson
and Drexel Burnham Lambert Inc. , in particular, are likely to have difficulty shoring up their credit standing in
months ahead . What worries credit-rating concerns the most is that Wall Street firms are taking long-term risks
with their own &lt;head&gt; capital &lt;/head&gt; via leveraged buy-out and junk bond financings . That ’s a departure from
their traditional practice of transferring almost all financing risks to investors . Whereas conventional securities
financings are structured to be sold quickly , Wall Street ’s new penchant for leveraged buy-outs and junk bonds is
resulting in long-term lending commitments that stretch out for months or years .
</bodyText>
<equation confidence="0.264268">
&lt;/context&gt; &lt;/instance&gt;
</equation>
<tableCaption confidence="0.999203">
Table 1: Example of test id for capital#n which its correct sense is 1
</tableCaption>
<table confidence="0.999953333333333">
Baselines P R F1
TRAIN 87.6 87.6 87.6
TRAIN-MFS 81.2 79.6 80.4
WN-MFS 66.2 59.9 62.9
SEMCOR-MFS 42.4 38.4 40.3
RANDOM 27.4 27.4 27.4
</table>
<tableCaption confidence="0.994494">
Table 3: P, R and F1 results for English Lexical Sam-
</tableCaption>
<bodyText confidence="0.947842125">
ple Baselines of SemEval-2007
polysemous (as shown by the RANDOM baseline),
less similar than SemCor word sense frequency dis-
tributions (as shown by SemCor-MFS), more simi-
lar to the first sense of WN (as shown by WN-MFS),
much more skewed to the first sense of the training
corpus (as shown by TRAIN-MFS), and much more
easy to be learned (as shown by TRAIN).
</bodyText>
<sectionHeader confidence="0.963172" genericHeader="method">
3 Large scale knowledge Resources
</sectionHeader>
<bodyText confidence="0.999683821428572">
The evaluation presented here covers a wide range
of large-scale semantic resources: WordNet (WN)
(Fellbaum, 1998), eXtended WordNet (Mihalcea
and Moldovan, 2001), large collections of seman-
tic preferences acquired from SemCor (Agirre and
Martinez, 2001; Agirre and Martinez, 2002) or ac-
quired from the BNC (McCarthy, 2001), large-scale
Topic Signatures for each synset acquired from the
web (Agirre and de la Calle, 2004) or SemCor (Lan-
des et al., 2006).
Although these resources have been derived us-
ing different WN versions, using the technology for
the automatic alignment of wordnets (Daud´e et al.,
2003), most of these resources have been integrated
into a common resource called Multilingual Cen-
tral Repository (MCR) (Atserias et al., 2004) main-
taining the compatibility among all the knowledge
resources which use a particular WN version as a
sense repository. Furthermore, these mappings al-
low to port the knowledge associated to a particular
WN version to the rest of WN versions.
The current version of the MCR contains 934,771
semantic relations between synsets, most of them
acquired by automatic means. This represents al-
most four times larger than the Princeton WordNet
(245,509 unique semantic relations in WordNet 2.1).
Hereinafter we will refer to each semantic re-
source as follows:
WN (Fellbaum, 1998): This resource uses the
direct relations encoded in WN1.6 or WN2.0 (for
instance, tree#n#1–hyponym–&gt;teak#n#2). We also
tested WN2 (using relations at distances 1 and 2),
WN3 (using relations at distances 1 to 3) and WN4
(using relations at distances 1 to 4).
XWN (Mihalcea and Moldovan, 2001): This re-
source uses the direct relations encoded in eXtended
WN (for instance, teak#n#2–gloss–&gt;wood#n#1).
WN+XWN: This resource uses the direct rela-
tions included in WN and XWN. We also tested
(WN+XWN)2 (using either WN or XWN relations
at distances 1 and 2, for instance, tree#n#1–related–
&gt;wood#n#1).
spBNC (McCarthy, 2001): This resource contains
707,618 selectional preferences acquired for sub-
jects and objects from BNC.
spSemCor (Agirre and Martinez, 2002): This re-
source contains the selectional preferences acquired
for subjects and objects from SemCor (for instance,
read#v#1–tobj–&gt;book#n#1).
MCR (Atserias et al., 2004): This resource
uses the direct relations included in MCR but ex-
cluding spBNC because of its poor performance.
Thus, MCR contains the direct relations from
WN (as tree#n#1–hyponym–&gt;teak#n#2), XWN
(as teak#n#2–gloss–&gt;wood#n#1), and spSemCor
(as read#v#1–tobj–&gt;book#n#1) but not the indi-
</bodyText>
<page confidence="0.989704">
83
</page>
<table confidence="0.999654">
Source #relations
Princeton WN1.6 138,091
Selectional Preferences from SemCor 203,546
New relations from Princeton WN2.0 42,212
Gold relations from eXtended WN 17,185
Silver relations from eXtended WN 239,249
Normal relations from eXtended WN 294,488
Total 934,771
</table>
<tableCaption confidence="0.998087">
Table 4: Semantic relations uploaded in the MCR
</tableCaption>
<figure confidence="0.969001181818182">
political party#n#1
party#n#1
election#n#1
nominee#n#1
candidate#n#1
campaigner#n#1
regime#n#1
identification#n#1
government#n#1
designation#n#3
authorities#n#1
2.3219
2.3219
1.0926
0.4780
0.4780
0.4780
0.3414
0.3414
0.3414
0.3414
0.3414
</figure>
<figureCaption confidence="0.5524505">
rect relations of (WN+XWN)2 (tree#n#1–related–
&gt;wood#n#1). We also tested MCR2 (using rela-
tions at distances 1 and 2), which also integrates
(WN+XWN)2 relations.
</figureCaption>
<bodyText confidence="0.887505">
Table 4 shows the number of semantic relations
between synset pairs in the MCR.
</bodyText>
<subsectionHeader confidence="0.99311">
3.1 Topic Signatures
</subsectionHeader>
<bodyText confidence="0.997551821428572">
Topic Signatures (TS) are word vectors related to a
particular topic (Lin and Hovy, 2000). Topic Signa-
tures are built by retrieving context words of a target
topic from large corpora. In our case, we consider
word senses as topics.
For this study, we use two different large-scale
Topic Signatures. The first constitutes one of the
largest available semantic resource with around 100
million relations (between synsets and words) ac-
quired from the web (Agirre and de la Calle, 2004).
The second has been derived directly from SemCor.
TSWEB2: Inspired by the work of (Leacock et
al., 1998), these Topic Signatures were constructed
using monosemous relatives from WordNet (syn-
onyms, hypernyms, direct and indirect hyponyms,
and siblings), querying Google and retrieving up to
one thousand snippets per query (that is, a word
sense), extracting the words with distinctive fre-
quency using TFIDF. For these experiments, we
used at maximum the first 700 words of each TS.
TSSEM: These Topic Signatures have been con-
structed using the part of SemCor having all words
tagged by PoS, lemmatized and sense tagged ac-
cording to WN1.6 totalizing 192,639 words. For
each word-sense appearing in SemCor, we gather
all sentences for that word sense, building a TS us-
ing TFIDF for all word-senses co-occurring in those
sentences.
</bodyText>
<footnote confidence="0.923754">
2http://ixa.si.ehu.es/Ixa/resources/
sensecorpus
</footnote>
<tableCaption confidence="0.9399065">
Table 5: Topic Signatures for party#n#1 obtained
from Semcor (11 out of 719 total word senses)
</tableCaption>
<bodyText confidence="0.99956525">
In table 5, there is an example of the first word-
senses we calculate from party#n#1.
The total number of relations between WN
synsets acquired from SemCor is 932,008.
</bodyText>
<sectionHeader confidence="0.966386" genericHeader="method">
4 Evaluating each resource
</sectionHeader>
<bodyText confidence="0.986535321428571">
Table 6 presents ordered by F1 measure, the perfor-
mance of each knowledge resource on Senseval-3
and the average size of the TS per word-sense. The
average size of the TS per word-sense is the number
of words associated to a synset on average. Obvi-
ously, the best resources would be those obtaining
better performances with a smaller number of asso-
ciated words per synset. The best results for preci-
sion, recall and F1 measures are shown in bold. We
also mark in italics those resources using non-direct
relations.
Surprisingly, the best results are obtained by
TSSEM (with F1 of 52.4). The lowest result is ob-
tained by the knowledge directly gathered from WN
mainly because of its poor coverage (R of 18.4 and
F1 of 26.1). Also interesting, is that the knowledge
integrated in the MCR although partly derived by
automatic means performs much better in terms of
precision, recall and F1 measures than using them
separately (F1 with 18.4 points higher than WN, 9.1
than XWN and 3.7 than spSemCor).
Despite its small size, the resources derived from
SemCor obtain better results than its counterparts
using much larger corpora (TSSEM vs. TSWEB and
spSemCor vs. spBNC).
Regarding the basic baselines, all knowledge re-
sources surpass RANDOM, but none achieves nei-
ther WN-MFS, TRAIN-MFS nor TRAIN. Only
</bodyText>
<page confidence="0.995946">
84
</page>
<table confidence="0.9999555">
KB P R F1 Av. Size
TSSEM 52.5 52.4 52.4 103
MCR2 45.1 45.1 45.1 26,429
MCR 45.3 43.7 44.5 129
spSemCor 43.1 38.7 40.8 56
(WN+XWN)2 38.5 38.0 38.3 5,730
WN+XWN 40.0 34.2 36.8 74
TSWEB 36.1 35.9 36.0 1,721
XWN 38.8 32.5 35.4 69
WN3 35.0 34.7 34.8 503
WN4 33.2 33.1 33.2 2,346
WN2 33.1 27.5 30.0 105
spBNC 36.3 25.4 29.9 128
WN 44.9 18.4 26.1 14
</table>
<tableCaption confidence="0.85333">
Table 6: P, R and F1 fine-grained results for the
resources evaluated individually at Senseval-03 En-
glish Lexical Sample Task.
</tableCaption>
<bodyText confidence="0.998958041666667">
TSSEM obtains better results than SEMCOR-MFS
and is very close to the most frequent sense of WN
(WN-MFS) and the training (TRAIN-MFS).
Table 7 presents ordered by F1 measure, the per-
formance of each knowledge resource on SemEval-
2007 and its average size of the TS per word-sense3.
The best results for precision, recall and F1 mea-
sures are shown in bold. We also mark in italics
those resources using non-direct relations.
Interestingly, on SemEval-2007, all the knowl-
edge resources behave differently. Now, the best
results are obtained by (WN+XWN)2 (with F1 of
52.9), followed by TSWEB (with F1 of 51.0). The
lowest result is obtained by the knowledge encoded
in spBNC mainly because of its poor precision (P of
24.4 and F1 of 20.8).
Regarding the basic baselines, spBNC, WN (and
also WN2 and WN4) and spSemCor do not sur-
pass RANDOM, and none achieves neither WN-
MFS, TRAIN-MFS nor TRAIN. Now, WN+XWN,
XWN, TSWEB and (WN+XWN)2 obtain better re-
sults than SEMCOR-MFS but far below the most
frequent sense of WN (WN-MFS) and the training
(TRAIN-MFS).
</bodyText>
<sectionHeader confidence="0.745313" genericHeader="method">
5 Combination of Knowledge Resources
</sectionHeader>
<bodyText confidence="0.999745">
In order to evaluate deeply the contribution of each
knowledge resource, we also provide some results
of the combined outcomes of several resources. The
</bodyText>
<footnote confidence="0.9965535">
3The average size is different with respect Senseval-3 be-
cause the words selected for this task are different
</footnote>
<table confidence="0.999941214285714">
KB P R F1 Av. Size
(WN+XWN)2 54.9 51.1 52.9 5,153
TSWEB 54.8 47.8 51.0 700
XWN 50.1 39.8 44.4 96
WN+XWN 45.4 36.8 40.7 101
MCR 40.2 35.5 37.7 149
TSSEM 35.1 32.7 33.9 428
MCR2 32.4 29.5 30.9 24,896
WN3 29.3 26.3 27.7 584
WN2 25.9 27.4 26.6 72
spSemCor 31.4 23.0 26.5 51.0
WN4 26.1 23.9 24.9 2,710
WN 36.8 16.1 22.4 13
spBNC 24.4 18.1 20.8 290
</table>
<tableCaption confidence="0.732430666666667">
Table 7: P, R and F1 fine-grained results for the
resources evaluated individually at SemEval-2007,
English Lexical Sample Task.
</tableCaption>
<table confidence="0.9806455">
KB Rank
MCR+(WN+XWN)2+TSWEB+TSSEM 55.5
</table>
<tableCaption confidence="0.994836">
Table 8: F1 fine-grained results for the 4 system-
combinations on Senseval-3
</tableCaption>
<bodyText confidence="0.97947235">
combinations are performed following a very basic
strategy (Brody et al., 2006).
Rank-Based Combination (Rank): Each se-
mantic resource provides a ranking of senses of the
word to be disambiguated. For each sense, its place-
ments according to each of the methods are summed
and the sense with the lowest total placement (clos-
est to first place) is selected.
Table 8 presents the F1 measure result with re-
spect this method when combining four different se-
mantic resources on the Senseval-3 test set.
Regarding the basic baselines, this combination
outperforms the most frequent sense of SemCor
(SEMCOR-MFS with F1 of 49.1), WN (WN-MFS
with F1 of 53.0) and, the training data (TRAIN-MFS
with F1 of 54.5).
Table 9 presents the F1 measure result with re-
spect the rank mthod when combining the same four
different semantic resources on the SemEval-2007
test set.
</bodyText>
<table confidence="0.8022925">
KB Rank
MCR+(WN+XWN)2+TSWEB+TSSEM 38.9
</table>
<tableCaption confidence="0.9956">
Table 9: F1 fine-grained results for the 4 system-
combinations on SemEval-2007
</tableCaption>
<page confidence="0.999535">
85
</page>
<bodyText confidence="0.999962461538462">
In this case, the combination of the four resources
obtains much lower result. Regarding the baselines,
this combination performs lower than the most fre-
quent senses from SEMCOR, WN or the training
data. This could be due to the poor individual per-
formance of the knowledge derived from SemCor
(spSemCor, TSSEM and MCR, which integrates
spSemCor). Possibly, in this case, the knowledge
comming from SemCor is counterproductive. Inter-
estingly, the knowledge derived from other sources
(XWN from WN glosses and TSWEB from the
web) seems to be more robust with respect corpus
changes.
</bodyText>
<sectionHeader confidence="0.999793" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9999803125">
Although this task had no participants, we provide
the performances of a large set of knowledge re-
sources on two different test sets: Senseval-3 and
SemEval-2007 English Lexical Sample task. We
also provide the results of a system combination of
four large-scale semantic resources. When evalu-
ated on Senseval-3, the combination of knowledge
sources surpass the most-frequent classifiers. How-
ever, a completely different behaviour is observed
on SemEval-2007 data test. In fact, both corpora
present very different characteristics. The results
show that some resources seems to be less depen-
dant than others to corpus changes.
Obviously, these results suggest that much more
research on acquiring, evaluating and using large-
scale semantic resources should be addressed.
</bodyText>
<sectionHeader confidence="0.999225" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999346">
We want to thank the valuable comments of the
anonymous reviewers. This work has been partially
supported by the projects KNOW (TIN2006-15049-
C03-01) and ADIMEN (EHU06/113).
</bodyText>
<sectionHeader confidence="0.999634" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999731462962963">
E. Agirre and O. Lopez de la Calle. 2004. Publicly avail-
able topic signatures for all wordnet nominal senses.
In Proceedings ofLREC, Lisbon, Portugal.
E. Agirre and D. Martinez. 2001. Learning class-to-class
selectional preferences. In Proceedings of CoNLL,
Toulouse, France.
E. Agirre and D. Martinez. 2002. Integrating selectional
preferences in wordnet. In Proceedings of GWC,
Mysore, India.
J. Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Car-
roll, B. Magnini, and Piek Vossen. 2004. The mean-
ing multilingual central repository. In Proceedings of
GWC, Brno, Czech Republic.
S. Brody, R. Navigli, and M. Lapata. 2006. Ensem-
ble methods for unsupervised wsd. In Proceedings of
COLING-ACL, pages 97–104.
M. Cuadros, L. Padr´o, and G. Rigau. 2005. Comparing
methods for automatic acquisition of topic signatures.
In Proceedings of RANLP, Borovets, Bulgaria.
M. Cuadros, L. Padr´o, and G. Rigau. 2006. An empirical
study for automatic acquisition of topic signatures. In
Proceedings of GWC, pages 51–59.
J. Daud´e, L. Padr´o, and G. Rigau. 2003. Validation and
Tuning of Wordnet Mapping Techniques. In Proceed-
ings ofRANLP, Borovets, Bulgaria.
C. Fellbaum, editor. 1998. WordNet. An Electronic Lexi-
cal Database. The MIT Press.
S. Landes, C. Leacock, and R. Tengi. 2006. Build-
ing a semantic concordance of english. In WordNet:
An electronic lexical database and some applications.
MIT Press, Cambridge,MA., 1998, pages 97–104.
C. Leacock, M. Chodorow, and G. Miller. 1998. Us-
ing Corpus Statistics and WordNet Relations for Sense
Identification. Computational Linguistics, 24(1):147–
166.
C. Lin and E. Hovy. 2000. The automated acquisition of
topic signatures for text summarization. In Proceed-
ings of COLING. Strasbourg, France.
D. McCarthy. 2001. Lexical Acquisition at the Syntax-
Semantics Interface: Diathesis Aternations, Subcate-
gorization Frames and Selectional Preferences. Ph.D.
thesis, University of Sussex.
R. Mihalcea and D. Moldovan. 2001. extended wordnet:
Progress report. In Proceedings of NAACL Workshop
on WordNet and Other Lexical Resources, Pittsburgh,
PA.
R. Mihalcea. 2006. Knowledge based methods for word
sense disambiguation. In E. Agirre and P. Edmonds
(Eds.) Word Sense Disambiguation: Algorithms and
applications., volume 33 of Text, Speech and Lan-
guage Technology. Springer.
P. Vossen, editor. 1998. EuroWordNet: A Multilingual
Database with Lexical Semantic Networks . Kluwer
Academic Publishers.
</reference>
<page confidence="0.998537">
86
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.311917">
<title confidence="0.985077">SemEval-2007 Task 16: Evaluation of Wide Coverage Knowledge Resources</title>
<author confidence="0.801018">Montse Cuadros</author>
<affiliation confidence="0.999141">TALP Research Center Universitat Polit´ecnica de Catalunya</affiliation>
<address confidence="0.785174">Barcelona, Spain</address>
<email confidence="0.999481">cuadros@lsi.upc.edu</email>
<author confidence="0.998357">German Rigau</author>
<affiliation confidence="0.992823">IXA NLP Group Euskal Herriko Unibersitatea</affiliation>
<address confidence="0.933169">Donostia, Spain</address>
<email confidence="0.990487">german.rigau@ehu.es</email>
<abstract confidence="0.996640375">This task tries to establish the relative quality of available semantic resources (derived by manual or automatic means). The quality of each large-scale knowledge resource is indirectly evaluated on a Word Sense Disambiguation task. In particular, we use Senseval-3 and SemEval-2007 English Lexical Sample tasks as evaluation bechmarks to evaluate the relative quality of each resource. Furthermore, trying to be as neutral as possible with respect the knowledge bases studied, we apply systematically the same disambiguation method to all the resources. A completely different behaviour is observed on both lexical data sets (Senseval-</abstract>
<note confidence="0.602488">3 and SemEval-2007).</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>O Lopez de la Calle</author>
</authors>
<title>Publicly available topic signatures for all wordnet nominal senses.</title>
<date>2004</date>
<booktitle>In Proceedings ofLREC,</booktitle>
<location>Lisbon, Portugal.</location>
<marker>Agirre, Calle, 2004</marker>
<rawString>E. Agirre and O. Lopez de la Calle. 2004. Publicly available topic signatures for all wordnet nominal senses. In Proceedings ofLREC, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>D Martinez</author>
</authors>
<title>Learning class-to-class selectional preferences.</title>
<date>2001</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<location>Toulouse, France.</location>
<contexts>
<context position="2358" citStr="Agirre and Martinez, 2001" startWordPosition="349" endWordPosition="352">rt advanced concept-based NLP applications directly. It seems that applications will not scale up to working in open domains without more detailed and rich general-purpose (and also domain-specific) semantic knowledge built by automatic means. Fortunately, during the last years, the research community has devised a large set of innovative methods and tools for large-scale automatic acquisition of lexical knowledge from structured and unstructured corpora. Among others we can mention eXtended WordNet (Mihalcea and Moldovan, 2001), large collections of semantic preferences acquired from SemCor (Agirre and Martinez, 2001; Agirre and Martinez, 2002) or acquired from British National Corpus (BNC) (McCarthy, 2001), largescale Topic Signatures for each synset acquired from the web (Agirre and de la Calle, 2004) or acquired from the BNC (Cuadros et al., 2005). Obviously, these semantic resources have been acquired using a very different set of methods, tools and corpora, resulting on a different set of new semantic relations between synsets (or between synsets and words). Many international research groups are working on knowledge-based WSD using a wide range of approaches (Mihalcea, 2006). However, less attention</context>
<context position="9145" citStr="Agirre and Martinez, 2001" startWordPosition="1450" endWordPosition="1453">al-2007 polysemous (as shown by the RANDOM baseline), less similar than SemCor word sense frequency distributions (as shown by SemCor-MFS), more similar to the first sense of WN (as shown by WN-MFS), much more skewed to the first sense of the training corpus (as shown by TRAIN-MFS), and much more easy to be learned (as shown by TRAIN). 3 Large scale knowledge Resources The evaluation presented here covers a wide range of large-scale semantic resources: WordNet (WN) (Fellbaum, 1998), eXtended WordNet (Mihalcea and Moldovan, 2001), large collections of semantic preferences acquired from SemCor (Agirre and Martinez, 2001; Agirre and Martinez, 2002) or acquired from the BNC (McCarthy, 2001), large-scale Topic Signatures for each synset acquired from the web (Agirre and de la Calle, 2004) or SemCor (Landes et al., 2006). Although these resources have been derived using different WN versions, using the technology for the automatic alignment of wordnets (Daud´e et al., 2003), most of these resources have been integrated into a common resource called Multilingual Central Repository (MCR) (Atserias et al., 2004) maintaining the compatibility among all the knowledge resources which use a particular WN version as a s</context>
</contexts>
<marker>Agirre, Martinez, 2001</marker>
<rawString>E. Agirre and D. Martinez. 2001. Learning class-to-class selectional preferences. In Proceedings of CoNLL, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>D Martinez</author>
</authors>
<title>Integrating selectional preferences in wordnet.</title>
<date>2002</date>
<booktitle>In Proceedings of GWC, Mysore,</booktitle>
<contexts>
<context position="2386" citStr="Agirre and Martinez, 2002" startWordPosition="353" endWordPosition="356">LP applications directly. It seems that applications will not scale up to working in open domains without more detailed and rich general-purpose (and also domain-specific) semantic knowledge built by automatic means. Fortunately, during the last years, the research community has devised a large set of innovative methods and tools for large-scale automatic acquisition of lexical knowledge from structured and unstructured corpora. Among others we can mention eXtended WordNet (Mihalcea and Moldovan, 2001), large collections of semantic preferences acquired from SemCor (Agirre and Martinez, 2001; Agirre and Martinez, 2002) or acquired from British National Corpus (BNC) (McCarthy, 2001), largescale Topic Signatures for each synset acquired from the web (Agirre and de la Calle, 2004) or acquired from the BNC (Cuadros et al., 2005). Obviously, these semantic resources have been acquired using a very different set of methods, tools and corpora, resulting on a different set of new semantic relations between synsets (or between synsets and words). Many international research groups are working on knowledge-based WSD using a wide range of approaches (Mihalcea, 2006). However, less attention has been devoted on analysi</context>
<context position="9173" citStr="Agirre and Martinez, 2002" startWordPosition="1454" endWordPosition="1457">n by the RANDOM baseline), less similar than SemCor word sense frequency distributions (as shown by SemCor-MFS), more similar to the first sense of WN (as shown by WN-MFS), much more skewed to the first sense of the training corpus (as shown by TRAIN-MFS), and much more easy to be learned (as shown by TRAIN). 3 Large scale knowledge Resources The evaluation presented here covers a wide range of large-scale semantic resources: WordNet (WN) (Fellbaum, 1998), eXtended WordNet (Mihalcea and Moldovan, 2001), large collections of semantic preferences acquired from SemCor (Agirre and Martinez, 2001; Agirre and Martinez, 2002) or acquired from the BNC (McCarthy, 2001), large-scale Topic Signatures for each synset acquired from the web (Agirre and de la Calle, 2004) or SemCor (Landes et al., 2006). Although these resources have been derived using different WN versions, using the technology for the automatic alignment of wordnets (Daud´e et al., 2003), most of these resources have been integrated into a common resource called Multilingual Central Repository (MCR) (Atserias et al., 2004) maintaining the compatibility among all the knowledge resources which use a particular WN version as a sense repository. Furthermore</context>
<context position="10969" citStr="Agirre and Martinez, 2002" startWordPosition="1736" endWordPosition="1739"> (using relations at distances 1 and 2), WN3 (using relations at distances 1 to 3) and WN4 (using relations at distances 1 to 4). XWN (Mihalcea and Moldovan, 2001): This resource uses the direct relations encoded in eXtended WN (for instance, teak#n#2–gloss–&gt;wood#n#1). WN+XWN: This resource uses the direct relations included in WN and XWN. We also tested (WN+XWN)2 (using either WN or XWN relations at distances 1 and 2, for instance, tree#n#1–related– &gt;wood#n#1). spBNC (McCarthy, 2001): This resource contains 707,618 selectional preferences acquired for subjects and objects from BNC. spSemCor (Agirre and Martinez, 2002): This resource contains the selectional preferences acquired for subjects and objects from SemCor (for instance, read#v#1–tobj–&gt;book#n#1). MCR (Atserias et al., 2004): This resource uses the direct relations included in MCR but excluding spBNC because of its poor performance. Thus, MCR contains the direct relations from WN (as tree#n#1–hyponym–&gt;teak#n#2), XWN (as teak#n#2–gloss–&gt;wood#n#1), and spSemCor (as read#v#1–tobj–&gt;book#n#1) but not the indi83 Source #relations Princeton WN1.6 138,091 Selectional Preferences from SemCor 203,546 New relations from Princeton WN2.0 42,212 Gold relations fr</context>
</contexts>
<marker>Agirre, Martinez, 2002</marker>
<rawString>E. Agirre and D. Martinez. 2002. Integrating selectional preferences in wordnet. In Proceedings of GWC, Mysore, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Atserias</author>
<author>L Villarejo</author>
<author>G Rigau</author>
<author>E Agirre</author>
<author>J Carroll</author>
<author>B Magnini</author>
<author>Piek Vossen</author>
</authors>
<title>The meaning multilingual central repository.</title>
<date>2004</date>
<booktitle>In Proceedings of GWC,</booktitle>
<location>Brno, Czech Republic.</location>
<contexts>
<context position="9640" citStr="Atserias et al., 2004" startWordPosition="1530" endWordPosition="1533">ordNet (Mihalcea and Moldovan, 2001), large collections of semantic preferences acquired from SemCor (Agirre and Martinez, 2001; Agirre and Martinez, 2002) or acquired from the BNC (McCarthy, 2001), large-scale Topic Signatures for each synset acquired from the web (Agirre and de la Calle, 2004) or SemCor (Landes et al., 2006). Although these resources have been derived using different WN versions, using the technology for the automatic alignment of wordnets (Daud´e et al., 2003), most of these resources have been integrated into a common resource called Multilingual Central Repository (MCR) (Atserias et al., 2004) maintaining the compatibility among all the knowledge resources which use a particular WN version as a sense repository. Furthermore, these mappings allow to port the knowledge associated to a particular WN version to the rest of WN versions. The current version of the MCR contains 934,771 semantic relations between synsets, most of them acquired by automatic means. This represents almost four times larger than the Princeton WordNet (245,509 unique semantic relations in WordNet 2.1). Hereinafter we will refer to each semantic resource as follows: WN (Fellbaum, 1998): This resource uses the di</context>
<context position="11136" citStr="Atserias et al., 2004" startWordPosition="1758" endWordPosition="1761">esource uses the direct relations encoded in eXtended WN (for instance, teak#n#2–gloss–&gt;wood#n#1). WN+XWN: This resource uses the direct relations included in WN and XWN. We also tested (WN+XWN)2 (using either WN or XWN relations at distances 1 and 2, for instance, tree#n#1–related– &gt;wood#n#1). spBNC (McCarthy, 2001): This resource contains 707,618 selectional preferences acquired for subjects and objects from BNC. spSemCor (Agirre and Martinez, 2002): This resource contains the selectional preferences acquired for subjects and objects from SemCor (for instance, read#v#1–tobj–&gt;book#n#1). MCR (Atserias et al., 2004): This resource uses the direct relations included in MCR but excluding spBNC because of its poor performance. Thus, MCR contains the direct relations from WN (as tree#n#1–hyponym–&gt;teak#n#2), XWN (as teak#n#2–gloss–&gt;wood#n#1), and spSemCor (as read#v#1–tobj–&gt;book#n#1) but not the indi83 Source #relations Princeton WN1.6 138,091 Selectional Preferences from SemCor 203,546 New relations from Princeton WN2.0 42,212 Gold relations from eXtended WN 17,185 Silver relations from eXtended WN 239,249 Normal relations from eXtended WN 294,488 Total 934,771 Table 4: Semantic relations uploaded in the MCR</context>
</contexts>
<marker>Atserias, Villarejo, Rigau, Agirre, Carroll, Magnini, Vossen, 2004</marker>
<rawString>J. Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Carroll, B. Magnini, and Piek Vossen. 2004. The meaning multilingual central repository. In Proceedings of GWC, Brno, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brody</author>
<author>R Navigli</author>
<author>M Lapata</author>
</authors>
<title>Ensemble methods for unsupervised wsd.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<pages>97--104</pages>
<contexts>
<context position="17675" citStr="Brody et al., 2006" startWordPosition="2822" endWordPosition="2825">53 TSWEB 54.8 47.8 51.0 700 XWN 50.1 39.8 44.4 96 WN+XWN 45.4 36.8 40.7 101 MCR 40.2 35.5 37.7 149 TSSEM 35.1 32.7 33.9 428 MCR2 32.4 29.5 30.9 24,896 WN3 29.3 26.3 27.7 584 WN2 25.9 27.4 26.6 72 spSemCor 31.4 23.0 26.5 51.0 WN4 26.1 23.9 24.9 2,710 WN 36.8 16.1 22.4 13 spBNC 24.4 18.1 20.8 290 Table 7: P, R and F1 fine-grained results for the resources evaluated individually at SemEval-2007, English Lexical Sample Task. KB Rank MCR+(WN+XWN)2+TSWEB+TSSEM 55.5 Table 8: F1 fine-grained results for the 4 systemcombinations on Senseval-3 combinations are performed following a very basic strategy (Brody et al., 2006). Rank-Based Combination (Rank): Each semantic resource provides a ranking of senses of the word to be disambiguated. For each sense, its placements according to each of the methods are summed and the sense with the lowest total placement (closest to first place) is selected. Table 8 presents the F1 measure result with respect this method when combining four different semantic resources on the Senseval-3 test set. Regarding the basic baselines, this combination outperforms the most frequent sense of SemCor (SEMCOR-MFS with F1 of 49.1), WN (WN-MFS with F1 of 53.0) and, the training data (TRAIN-</context>
</contexts>
<marker>Brody, Navigli, Lapata, 2006</marker>
<rawString>S. Brody, R. Navigli, and M. Lapata. 2006. Ensemble methods for unsupervised wsd. In Proceedings of COLING-ACL, pages 97–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Cuadros</author>
<author>L Padr´o</author>
<author>G Rigau</author>
</authors>
<title>Comparing methods for automatic acquisition of topic signatures.</title>
<date>2005</date>
<booktitle>In Proceedings of RANLP, Borovets,</booktitle>
<marker>Cuadros, Padr´o, Rigau, 2005</marker>
<rawString>M. Cuadros, L. Padr´o, and G. Rigau. 2005. Comparing methods for automatic acquisition of topic signatures. In Proceedings of RANLP, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Cuadros</author>
<author>L Padr´o</author>
<author>G Rigau</author>
</authors>
<title>An empirical study for automatic acquisition of topic signatures.</title>
<date>2006</date>
<booktitle>In Proceedings of GWC,</booktitle>
<pages>51--59</pages>
<marker>Cuadros, Padr´o, Rigau, 2006</marker>
<rawString>M. Cuadros, L. Padr´o, and G. Rigau. 2006. An empirical study for automatic acquisition of topic signatures. In Proceedings of GWC, pages 51–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Daud´e</author>
<author>L Padr´o</author>
<author>G Rigau</author>
</authors>
<title>Validation and Tuning of Wordnet Mapping Techniques.</title>
<date>2003</date>
<booktitle>In Proceedings ofRANLP, Borovets,</booktitle>
<marker>Daud´e, Padr´o, Rigau, 2003</marker>
<rawString>J. Daud´e, L. Padr´o, and G. Rigau. 2003. Validation and Tuning of Wordnet Mapping Techniques. In Proceedings ofRANLP, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<title>WordNet. An Electronic Lexical Database.</title>
<date>1998</date>
<editor>C. Fellbaum, editor.</editor>
<publisher>The MIT Press.</publisher>
<marker>1998</marker>
<rawString>C. Fellbaum, editor. 1998. WordNet. An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Landes</author>
<author>C Leacock</author>
<author>R Tengi</author>
</authors>
<title>Building a semantic concordance of english. In WordNet: An electronic lexical database and some applications.</title>
<date>2006</date>
<pages>97--104</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge,MA.,</location>
<contexts>
<context position="9346" citStr="Landes et al., 2006" startWordPosition="1484" endWordPosition="1488">ore skewed to the first sense of the training corpus (as shown by TRAIN-MFS), and much more easy to be learned (as shown by TRAIN). 3 Large scale knowledge Resources The evaluation presented here covers a wide range of large-scale semantic resources: WordNet (WN) (Fellbaum, 1998), eXtended WordNet (Mihalcea and Moldovan, 2001), large collections of semantic preferences acquired from SemCor (Agirre and Martinez, 2001; Agirre and Martinez, 2002) or acquired from the BNC (McCarthy, 2001), large-scale Topic Signatures for each synset acquired from the web (Agirre and de la Calle, 2004) or SemCor (Landes et al., 2006). Although these resources have been derived using different WN versions, using the technology for the automatic alignment of wordnets (Daud´e et al., 2003), most of these resources have been integrated into a common resource called Multilingual Central Repository (MCR) (Atserias et al., 2004) maintaining the compatibility among all the knowledge resources which use a particular WN version as a sense repository. Furthermore, these mappings allow to port the knowledge associated to a particular WN version to the rest of WN versions. The current version of the MCR contains 934,771 semantic relat</context>
</contexts>
<marker>Landes, Leacock, Tengi, 2006</marker>
<rawString>S. Landes, C. Leacock, and R. Tengi. 2006. Building a semantic concordance of english. In WordNet: An electronic lexical database and some applications. MIT Press, Cambridge,MA., 1998, pages 97–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>M Chodorow</author>
<author>G Miller</author>
</authors>
<title>Using Corpus Statistics and WordNet Relations for Sense Identification.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<pages>166</pages>
<contexts>
<context position="12826" citStr="Leacock et al., 1998" startWordPosition="2003" endWordPosition="2006">in the MCR. 3.1 Topic Signatures Topic Signatures (TS) are word vectors related to a particular topic (Lin and Hovy, 2000). Topic Signatures are built by retrieving context words of a target topic from large corpora. In our case, we consider word senses as topics. For this study, we use two different large-scale Topic Signatures. The first constitutes one of the largest available semantic resource with around 100 million relations (between synsets and words) acquired from the web (Agirre and de la Calle, 2004). The second has been derived directly from SemCor. TSWEB2: Inspired by the work of (Leacock et al., 1998), these Topic Signatures were constructed using monosemous relatives from WordNet (synonyms, hypernyms, direct and indirect hyponyms, and siblings), querying Google and retrieving up to one thousand snippets per query (that is, a word sense), extracting the words with distinctive frequency using TFIDF. For these experiments, we used at maximum the first 700 words of each TS. TSSEM: These Topic Signatures have been constructed using the part of SemCor having all words tagged by PoS, lemmatized and sense tagged according to WN1.6 totalizing 192,639 words. For each word-sense appearing in SemCor,</context>
</contexts>
<marker>Leacock, Chodorow, Miller, 1998</marker>
<rawString>C. Leacock, M. Chodorow, and G. Miller. 1998. Using Corpus Statistics and WordNet Relations for Sense Identification. Computational Linguistics, 24(1):147– 166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lin</author>
<author>E Hovy</author>
</authors>
<title>The automated acquisition of topic signatures for text summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING.</booktitle>
<location>Strasbourg, France.</location>
<contexts>
<context position="12327" citStr="Lin and Hovy, 2000" startWordPosition="1920" endWordPosition="1923">elations uploaded in the MCR political party#n#1 party#n#1 election#n#1 nominee#n#1 candidate#n#1 campaigner#n#1 regime#n#1 identification#n#1 government#n#1 designation#n#3 authorities#n#1 2.3219 2.3219 1.0926 0.4780 0.4780 0.4780 0.3414 0.3414 0.3414 0.3414 0.3414 rect relations of (WN+XWN)2 (tree#n#1–related– &gt;wood#n#1). We also tested MCR2 (using relations at distances 1 and 2), which also integrates (WN+XWN)2 relations. Table 4 shows the number of semantic relations between synset pairs in the MCR. 3.1 Topic Signatures Topic Signatures (TS) are word vectors related to a particular topic (Lin and Hovy, 2000). Topic Signatures are built by retrieving context words of a target topic from large corpora. In our case, we consider word senses as topics. For this study, we use two different large-scale Topic Signatures. The first constitutes one of the largest available semantic resource with around 100 million relations (between synsets and words) acquired from the web (Agirre and de la Calle, 2004). The second has been derived directly from SemCor. TSWEB2: Inspired by the work of (Leacock et al., 1998), these Topic Signatures were constructed using monosemous relatives from WordNet (synonyms, hypernym</context>
</contexts>
<marker>Lin, Hovy, 2000</marker>
<rawString>C. Lin and E. Hovy. 2000. The automated acquisition of topic signatures for text summarization. In Proceedings of COLING. Strasbourg, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McCarthy</author>
</authors>
<title>Lexical Acquisition at the SyntaxSemantics Interface: Diathesis Aternations, Subcategorization Frames and Selectional Preferences.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Sussex.</institution>
<contexts>
<context position="2450" citStr="McCarthy, 2001" startWordPosition="364" endWordPosition="365">orking in open domains without more detailed and rich general-purpose (and also domain-specific) semantic knowledge built by automatic means. Fortunately, during the last years, the research community has devised a large set of innovative methods and tools for large-scale automatic acquisition of lexical knowledge from structured and unstructured corpora. Among others we can mention eXtended WordNet (Mihalcea and Moldovan, 2001), large collections of semantic preferences acquired from SemCor (Agirre and Martinez, 2001; Agirre and Martinez, 2002) or acquired from British National Corpus (BNC) (McCarthy, 2001), largescale Topic Signatures for each synset acquired from the web (Agirre and de la Calle, 2004) or acquired from the BNC (Cuadros et al., 2005). Obviously, these semantic resources have been acquired using a very different set of methods, tools and corpora, resulting on a different set of new semantic relations between synsets (or between synsets and words). Many international research groups are working on knowledge-based WSD using a wide range of approaches (Mihalcea, 2006). However, less attention has been devoted on analysing the quality of each semantic resource. In fact, each resource</context>
<context position="9215" citStr="McCarthy, 2001" startWordPosition="1464" endWordPosition="1465">ord sense frequency distributions (as shown by SemCor-MFS), more similar to the first sense of WN (as shown by WN-MFS), much more skewed to the first sense of the training corpus (as shown by TRAIN-MFS), and much more easy to be learned (as shown by TRAIN). 3 Large scale knowledge Resources The evaluation presented here covers a wide range of large-scale semantic resources: WordNet (WN) (Fellbaum, 1998), eXtended WordNet (Mihalcea and Moldovan, 2001), large collections of semantic preferences acquired from SemCor (Agirre and Martinez, 2001; Agirre and Martinez, 2002) or acquired from the BNC (McCarthy, 2001), large-scale Topic Signatures for each synset acquired from the web (Agirre and de la Calle, 2004) or SemCor (Landes et al., 2006). Although these resources have been derived using different WN versions, using the technology for the automatic alignment of wordnets (Daud´e et al., 2003), most of these resources have been integrated into a common resource called Multilingual Central Repository (MCR) (Atserias et al., 2004) maintaining the compatibility among all the knowledge resources which use a particular WN version as a sense repository. Furthermore, these mappings allow to port the knowled</context>
<context position="10832" citStr="McCarthy, 2001" startWordPosition="1719" endWordPosition="1720">is resource uses the direct relations encoded in WN1.6 or WN2.0 (for instance, tree#n#1–hyponym–&gt;teak#n#2). We also tested WN2 (using relations at distances 1 and 2), WN3 (using relations at distances 1 to 3) and WN4 (using relations at distances 1 to 4). XWN (Mihalcea and Moldovan, 2001): This resource uses the direct relations encoded in eXtended WN (for instance, teak#n#2–gloss–&gt;wood#n#1). WN+XWN: This resource uses the direct relations included in WN and XWN. We also tested (WN+XWN)2 (using either WN or XWN relations at distances 1 and 2, for instance, tree#n#1–related– &gt;wood#n#1). spBNC (McCarthy, 2001): This resource contains 707,618 selectional preferences acquired for subjects and objects from BNC. spSemCor (Agirre and Martinez, 2002): This resource contains the selectional preferences acquired for subjects and objects from SemCor (for instance, read#v#1–tobj–&gt;book#n#1). MCR (Atserias et al., 2004): This resource uses the direct relations included in MCR but excluding spBNC because of its poor performance. Thus, MCR contains the direct relations from WN (as tree#n#1–hyponym–&gt;teak#n#2), XWN (as teak#n#2–gloss–&gt;wood#n#1), and spSemCor (as read#v#1–tobj–&gt;book#n#1) but not the indi83 Source #</context>
</contexts>
<marker>McCarthy, 2001</marker>
<rawString>D. McCarthy. 2001. Lexical Acquisition at the SyntaxSemantics Interface: Diathesis Aternations, Subcategorization Frames and Selectional Preferences. Ph.D. thesis, University of Sussex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>D Moldovan</author>
</authors>
<title>extended wordnet: Progress report.</title>
<date>2001</date>
<booktitle>In Proceedings of NAACL Workshop on WordNet and Other Lexical Resources,</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="2267" citStr="Mihalcea and Moldovan, 2001" startWordPosition="336" endWordPosition="339"> one thousand new relations per month. But this data does not seems to be rich enough to support advanced concept-based NLP applications directly. It seems that applications will not scale up to working in open domains without more detailed and rich general-purpose (and also domain-specific) semantic knowledge built by automatic means. Fortunately, during the last years, the research community has devised a large set of innovative methods and tools for large-scale automatic acquisition of lexical knowledge from structured and unstructured corpora. Among others we can mention eXtended WordNet (Mihalcea and Moldovan, 2001), large collections of semantic preferences acquired from SemCor (Agirre and Martinez, 2001; Agirre and Martinez, 2002) or acquired from British National Corpus (BNC) (McCarthy, 2001), largescale Topic Signatures for each synset acquired from the web (Agirre and de la Calle, 2004) or acquired from the BNC (Cuadros et al., 2005). Obviously, these semantic resources have been acquired using a very different set of methods, tools and corpora, resulting on a different set of new semantic relations between synsets (or between synsets and words). Many international research groups are working on kno</context>
<context position="9054" citStr="Mihalcea and Moldovan, 2001" startWordPosition="1437" endWordPosition="1440">NDOM 27.4 27.4 27.4 Table 3: P, R and F1 results for English Lexical Sample Baselines of SemEval-2007 polysemous (as shown by the RANDOM baseline), less similar than SemCor word sense frequency distributions (as shown by SemCor-MFS), more similar to the first sense of WN (as shown by WN-MFS), much more skewed to the first sense of the training corpus (as shown by TRAIN-MFS), and much more easy to be learned (as shown by TRAIN). 3 Large scale knowledge Resources The evaluation presented here covers a wide range of large-scale semantic resources: WordNet (WN) (Fellbaum, 1998), eXtended WordNet (Mihalcea and Moldovan, 2001), large collections of semantic preferences acquired from SemCor (Agirre and Martinez, 2001; Agirre and Martinez, 2002) or acquired from the BNC (McCarthy, 2001), large-scale Topic Signatures for each synset acquired from the web (Agirre and de la Calle, 2004) or SemCor (Landes et al., 2006). Although these resources have been derived using different WN versions, using the technology for the automatic alignment of wordnets (Daud´e et al., 2003), most of these resources have been integrated into a common resource called Multilingual Central Repository (MCR) (Atserias et al., 2004) maintaining t</context>
<context position="10506" citStr="Mihalcea and Moldovan, 2001" startWordPosition="1668" endWordPosition="1671">ions. The current version of the MCR contains 934,771 semantic relations between synsets, most of them acquired by automatic means. This represents almost four times larger than the Princeton WordNet (245,509 unique semantic relations in WordNet 2.1). Hereinafter we will refer to each semantic resource as follows: WN (Fellbaum, 1998): This resource uses the direct relations encoded in WN1.6 or WN2.0 (for instance, tree#n#1–hyponym–&gt;teak#n#2). We also tested WN2 (using relations at distances 1 and 2), WN3 (using relations at distances 1 to 3) and WN4 (using relations at distances 1 to 4). XWN (Mihalcea and Moldovan, 2001): This resource uses the direct relations encoded in eXtended WN (for instance, teak#n#2–gloss–&gt;wood#n#1). WN+XWN: This resource uses the direct relations included in WN and XWN. We also tested (WN+XWN)2 (using either WN or XWN relations at distances 1 and 2, for instance, tree#n#1–related– &gt;wood#n#1). spBNC (McCarthy, 2001): This resource contains 707,618 selectional preferences acquired for subjects and objects from BNC. spSemCor (Agirre and Martinez, 2002): This resource contains the selectional preferences acquired for subjects and objects from SemCor (for instance, read#v#1–tobj–&gt;book#n#1</context>
</contexts>
<marker>Mihalcea, Moldovan, 2001</marker>
<rawString>R. Mihalcea and D. Moldovan. 2001. extended wordnet: Progress report. In Proceedings of NAACL Workshop on WordNet and Other Lexical Resources, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
</authors>
<title>Knowledge based methods for word sense disambiguation. In E. Agirre and P. Edmonds (Eds.) Word Sense Disambiguation: Algorithms and applications., volume 33 of Text, Speech and Language Technology.</title>
<date>2006</date>
<publisher>Springer.</publisher>
<contexts>
<context position="2933" citStr="Mihalcea, 2006" startWordPosition="442" endWordPosition="443">d from SemCor (Agirre and Martinez, 2001; Agirre and Martinez, 2002) or acquired from British National Corpus (BNC) (McCarthy, 2001), largescale Topic Signatures for each synset acquired from the web (Agirre and de la Calle, 2004) or acquired from the BNC (Cuadros et al., 2005). Obviously, these semantic resources have been acquired using a very different set of methods, tools and corpora, resulting on a different set of new semantic relations between synsets (or between synsets and words). Many international research groups are working on knowledge-based WSD using a wide range of approaches (Mihalcea, 2006). However, less attention has been devoted on analysing the quality of each semantic resource. In fact, each resource presents different volume and accuracy figures (Cuadros et al., 2006). In this paper, we evaluate those resources on the 1Symmetric relations are counted only once. 81 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 81–86, Prague, June 2007. c�2007 Association for Computational Linguistics SemEval-2007 English Lexical Sample task. For comparison purposes, we also include the results of the same resources on the Senseval-3 English Lexi</context>
</contexts>
<marker>Mihalcea, 2006</marker>
<rawString>R. Mihalcea. 2006. Knowledge based methods for word sense disambiguation. In E. Agirre and P. Edmonds (Eds.) Word Sense Disambiguation: Algorithms and applications., volume 33 of Text, Speech and Language Technology. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Vossen</author>
<author>editor</author>
</authors>
<date>1998</date>
<booktitle>EuroWordNet: A Multilingual Database with Lexical Semantic Networks .</booktitle>
<publisher>Kluwer Academic Publishers.</publisher>
<marker>Vossen, editor, 1998</marker>
<rawString>P. Vossen, editor. 1998. EuroWordNet: A Multilingual Database with Lexical Semantic Networks . Kluwer Academic Publishers.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>