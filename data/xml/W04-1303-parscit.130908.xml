<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000467">
<title confidence="0.990378">
Putting Meaning into Grammar Learning
</title>
<author confidence="0.968231">
Nancy Chang
</author>
<affiliation confidence="0.8157155">
UC Berkeley, Dept. of Computer Science and
International Computer Science Institute
</affiliation>
<address confidence="0.706883">
1947 Center St., Suite 600
Berkeley, CA 94704 USA
</address>
<email confidence="0.9989">
nchang@icsi.berkeley.edu
</email>
<sectionHeader confidence="0.99389" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999830666666667">
This paper proposes a formulation of grammar
learning in which meaning plays a fundamental
role. We present a computational model that aims
to satisfy convergent constraints from cognitive lin-
guistics and crosslinguistic developmental evidence
within a statistically driven framework. The target
grammar, input data and goal of learning are all de-
signed to allow a tight coupling between language
learning and comprehension that drives the acqui-
sition of new constructions. The model is applied
to learn lexically specific multi-word constructions
from annotated child-directed transcript data.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999964507246377">
What role does meaning play in the acquisition of
grammar? Computational approaches to grammar
learning have tended to exclude semantic informa-
tion entirely, or else relegate it to lexical representa-
tions. Starting with Gold’s (1967) influential early
work on language identifiability in the limit and
continuing with work in the formalist learnability
paradigm, grammar learning has been equated with
syntax learning, with the target of learning consist-
ing of relatively abstract structures that govern the
combination of symbolic linguistic units. Statis-
tical, corpus-based efforts have likewise restricted
their attention to inducing syntactic patterns, though
in part due to more practical considerations, such as
the lack of large-scale semantically tagged corpora.
But a variety of cognitive, linguistic and develop-
mental considerations suggest that meaning plays a
central role in the acquisition of linguistic units at
all levels. We start with the proposition that lan-
guage use should drive language learning — that is,
the learner’s goal is to improve its ability to commu-
nicate, via comprehension and production. Cogni-
tive and constructional approaches to grammar as-
sume that the basic unit of linguistic knowledge
needed to support language use consists of pairings
of form and meaning, or constructions (Langacker,
1987; Goldberg, 1995; Fillmore and Kay, 1999).
Moreover, by the time children make the leap from
single words to complex combinations, they have
amassed considerable conceptual knowledge, in-
cluding familiarity with a wide variety of entities
and events and sophisticated pragmatic skills (such
as using joint attention to infer communicative in-
tentions (Tomasello, 1995) and subtle lexical dis-
tinctions (Bloom, 2000)). The developmental evi-
dence thus suggests that the input to grammar learn-
ing may in principle include not just surface strings
but also meaningful situation descriptions with rich
semantic and pragmatic information.
This paper formalizes the grammar learning
problem in line with the observations above, tak-
ing seriously the ideas that the target of learning,
for both lexical items and larger phrasal and clausal
units, is a bipolar structure in which meaning is on
par with form, and that meaningful language use
drives language learning. The resulting core com-
putational problem can be seen as a restricted type
of relational learning. In particular, a key step of
the learning task can be cast as learning relational
correspondences, that is, associations between form
relations (typically word order) and meaning rela-
tions (typically role-filler bindings). Such correla-
tions are essential for capturing complex multi-unit
constructions, both lexically specific constructions
and more general grammatical constructions.
The remainder of the paper is structured as fol-
lows. Section 2 states the learning task and pro-
vides an overview of the model and its assump-
tions. We then present algorithms for inducing
structured mappings, based on either specific input
examples or the current set of constructions (Sec-
tion 3), and describe how these are evaluated using
criteria based on minimum description length (Ris-
sanen, 1978). Initial results from applying the learn-
ing algorithms to a small corpus of child-directed
utterances demonstrate the viability of the approach
(Section 4). We conclude with a discussion of the
broader implications of this approach for language
learning and use.
</bodyText>
<page confidence="0.99833">
17
</page>
<bodyText confidence="0.985683615384615">
2 Overview of the learning problem
We begin with an informal description of our learn-
ing task, to be formalized below. At all stages
of language learning, children are assumed to ex-
ploit general cognitive abilities to make sense of
the flow of objects and events they experience. To
make sense of linguistic events sounds and ges-
tures used in their environments for communica-
tive purposes they also draw on specifically
linguistic knowledge of how forms map to mean-
ings, i.e., constructions. Comprehension consists of
two stages: identifying the constructions involved
and how their meanings are related (analysis), and
matching these constructionally sanctioned mean-
ings to the actual participants and relations present
in context (resolution). The set of linguistic con-
structions will typically provide only a partial anal-
ysis of the utterance in the given context; when this
happens, the agent may still draw on general infer-
ence to match even a partial analysis to the context.
The goal of construction learning is to acquire
a useful set of constructions, or grammar. This
grammar should allow constructional analysis to
produce increasingly complete interpretations of ut-
terances in context, thus requiring minimal recourse
to general resolution and inference procedures. In
the limit the grammar should stabilize, while still
being useful for comprehending novel input. A use-
ful grammar should also reflect the statistical prop-
erties of the input data, in that more frequent or spe-
cific constructions should be learned before more
infrequent and more general constructions.
Formally, we define our learning task as follows:
Given an initial grammar and a sequence of train-
ing examples consisting of an utterance paired with
its context, find the best grammar to fit seen data
and generalize to new data. The remainder of this
section describes the hypothesis space, prior knowl-
edge and input data relevant to the task.
</bodyText>
<subsectionHeader confidence="0.995247">
2.1 Hypothesis space: embodied constructions
</subsectionHeader>
<bodyText confidence="0.999729818181818">
The space of possible grammars (or sets of con-
structions) is defined by Embodied Construc-
tion Grammar (ECG), a computationally explicit
unification-based formalism for capturing insights
from the construction grammar and cognitive lin-
guistics literature (Bergen and Chang, in press;
Chang et al., 2002). ECG is designed to support
the analysis process mentioned above, which deter-
mines what constructions and schematic meanings
are present in an utterance, resulting in a semantic
specification (or semspec).1
</bodyText>
<footnote confidence="0.9975505">
1ECG is intended to support a simulation-based model of
language understanding, with the semspec parameterizing a
</footnote>
<bodyText confidence="0.999477523809524">
We highlight a few relevant aspects of the for-
malism, exemplified in Figure 1. Each construc-
tion has sections labeled form and meaning list-
ing the entities (or roles) and constraints (type con-
straints marked with :, filler constraints marked
with , and identification (or coindexation) con-
straints marked with ) of the respective do-
mains. These two sections, also called the form and
meaning poles, capture the basic intuition that con-
structions are form-meaning pairs. A subscripted
or allows reference to the form or meaning
pole of any construction, and the keyword self al-
lows self-reference. Thus, the construc-
tion simply links a form whose orthography role (or
feature) is bound to the string “throw” to a mean-
ing that is constrained to be of type Throw, a sepa-
rately defined conceptual schema corresponding to
throwing events (including roles for a thrower and
throwee). (Although not shown in the examples, the
formalism also includes a subcase of notation for
expressing constructional inheritance.)
</bodyText>
<figureCaption confidence="0.998808">
Figure 1: Embodied Construction Grammar repre-
</figureCaption>
<bodyText confidence="0.992481769230769">
sentation of the lexical and lexically spe-
cific construction (licensing
expressions like You throw the ball).
Multi-unit constructions such as the
construction also list their con-
stituents, each of which is itself a form-meaning
construction. These multi-unit constructions serve
as the target representation for the specific learn-
ing task at hand. The key representational insight
here is that the form and meaning constraints typi-
simulation using active representations (or embodied schemas)
to produce context-sensitive inferences. See Bergen and Chang
(in press) for details.
</bodyText>
<equation confidence="0.8875313125">
construction
constituents
t1 :
t2 :
t3 :
form
t1 before t2
t2 before t3
meaning
t2 .thrower t1
t2 .throwee t3
construction
form
self .orth “throw”
meaning
self : Throw
</equation>
<page confidence="0.986537">
18
</page>
<bodyText confidence="0.999820111111111">
cally involve relations among the form and meaning
poles of the constructional constituents. For cur-
rent purposes we limit the potential form relations
to word order, although many other form relations
are in principle allowed. In the meaning domain,
the primary relation is identification, or unification,
between two meaning entities. In particular, we
will focus on role-filler bindings, in which a role of
one constituent is identified with another constituent
or with one of its roles. The example construc-
tion pairs two word order constraints over its con-
stituents’ form poles with two identification con-
straints over its constituents’ meaning poles (these
specify the fillers of the thrower and throwee roles
of a Throw event, respectively).
Note that both lexical constructions and the
multi-unit constructions needed to express gram-
matical patterns can be seen as graphs of varying
complexity. Each domain (form or meaning) can
be represented as a subgraph of elements and re-
lations among them. Lexical constructions involve
a simple mapping between these two subgraphs,
whereas complex constructions with constituents
require structured relational mappings over the
two domains, that is, mappings between form and
meaning relations whose arguments are themselves
linked by known constructions.
</bodyText>
<subsectionHeader confidence="0.998083">
2.2 Prior knowledge
</subsectionHeader>
<bodyText confidence="0.998918857142857">
The model makes a number of assumptions based
on the child language literature about prior knowl-
edge brought to the task, including conceptual
knowledge, lexical knowledge and the language
comprehension process described earlier. Figure 2
depicts how these are related in a simple example;
each is described in more detail below.
</bodyText>
<figureCaption confidence="0.603117">
Figure 2: A constructional analysis of I throw the
</figureCaption>
<bodyText confidence="0.900142666666667">
ball, with form elements on the left, meaning ele-
ments (conceptual schemas) on the right and con-
structions linking the two domains in the center.
</bodyText>
<subsubsectionHeader confidence="0.605455">
2.2.1 Conceptual knowledge
</subsubsectionHeader>
<bodyText confidence="0.999936">
Conceptual knowledge is represented using an on-
tology of typed feature structures, or schemas.
These include schemas for people, objects (e.g. Ball
in the figure), locations, and actions familiar to chil-
dren by the time they enter the two-word stage (typ-
ically toward the end of the second year). Actions
like the Throw schema referred to in the example
construction and in the figure have roles
whose fillers are subject to type constraints, reflect-
ing children’s knowledge of what kinds of entities
can take place in different events.
</bodyText>
<subsectionHeader confidence="0.850942">
2.2.2 Lexical constructions
</subsectionHeader>
<bodyText confidence="0.999963090909091">
The input to learning includes a set of lexical con-
structions, represented using the ECG formalism,
linking simple forms (i.e. words) to specific con-
ceptual items. Examples of these include the and
constructions in the figure, as well as the
construction formally defined in Figure 1.
Lexical learning is not the focus of the current work,
but a number of previous computational approaches
have shown how simple mappings may be acquired
from experience (Regier, 1996; Bailey, 1997; Roy
and Pentland, 1998).
</bodyText>
<subsectionHeader confidence="0.974647">
2.2.3 Construction analyzer
</subsectionHeader>
<bodyText confidence="0.998769935483871">
As mentioned earlier, the ECG construction formal-
ism is designed to support processes of language
use. In particular, the model makes use of a con-
struction analyzer that identifies the constructions
responsible for a given utterance, much like a syn-
tactic parser in a traditional language understanding
system identifies which parse rules are responsible.
In this case, however, the basic representational unit
is a form-meaning pair. The analyzer must there-
fore also supply a semantic interpretation, called the
semspec, indicating which conceptual schemas are
involved and how they are related. The analyzer is
also required to be robust to input that is not cov-
ered by its current grammar, since that situation is
the norm during language learning.
Bryant (2003) describes an implemented con-
struction analyzer program that meets these needs.
The construction analyzer takes as input a set of
ECG constructions (linguistic knowledge), a set of
ECG schemas (conceptual knowledge) and an utter-
ance. The analyzer draws on partial parsing tech-
niques previously applied to syntactic parsing (Ab-
ney, 1996): utterances not covered by known con-
structions yield partially filled semspecs, and un-
known forms in the input are skipped. As a result,
even a small set of simple constructions can provide
skeletal interpretations of complex utterances.
Figure 2 gives an iconic representation of the re-
sult of analyzing the utterance I throw the ball us-
ing the and con-
structions shown earlier, along with some additional
</bodyText>
<figure confidence="0.998145333333333">
FORM
throw
ball
the
I
form
form
I
t1
THROW-TRANSITIVE
form
meaning
THROW
constructional
t2
form
meaning
THE- BALL
t3
meaning
meaning
MEANING
Speaker
Ball
thrower
throwee
Throw
</figure>
<page confidence="0.992576">
19
</page>
<bodyText confidence="0.99998234375">
lexical constructions (not shown). The analyzer
matches each input form with its lexical construc-
tion (if available) and corresponding meaning, and
then matches the clausal construction by checking
the relevant word order relations (implicitly rep-
resented by the dotted arrow in the figure) and
role bindings (denoted by the double-headed arrows
within the meaning domain) asserted on its candi-
date constituents. Note that at the stage shown, no
construction for the has yet been learned, result-
ing in a partial analysis. At an even earlier stage
of learning, before the con-
struction is learned, the lexical constructions are
matched without resulting in the role-filler bindings
on the Throw action schema.
Finally, note that the semspec produced by con-
structional analysis (right-hand side of the figure)
must be matched to the current situational con-
text using a contextual interpretation, or resolu-
tion, process. Like other resolution (e.g. refer-
ence resolution) procedures, this process relies on
category/type constraints and (provisional) identi-
fication bindings. The resolution procedure at-
tempts to unify each schema and constraint ap-
pearing in the semspec with a type-compatible en-
tity or relation in the context. In the example,
the schemas on the right-hand side of the figure
should be identified during resolution with particu-
lar schema instances available in context (e.g., the
Speaker schema should be linked to the specific
contextually available discourse speaker, the Ball
schema to a particular ball instance, etc.).
</bodyText>
<subsectionHeader confidence="0.987737">
2.3 Input data
</subsectionHeader>
<bodyText confidence="0.999953206896552">
The input is characterized as a set of input tokens,
each consisting of an utterance form (a string of
known and novel word-forms) paired with a specific
communicative context (a set of linked conceptual
schemas corresponding to the participants, salient
scene and discourse information available in the sit-
uation). The learning model receives only positive
examples, as in the child learning case. Note, how-
ever, that the interpretation a given utterance has
in context depends on the current state of linguis-
tic knowledge. Thus the same utterance at different
stages may lead to different learning behavior.
The specific training corpus used in learn-
ing experiments is a subset of the Sachs corpus
of the CHILDES database of parent-child tran-
scripts(Sachs, 1983; MacWhinney, 1991), with ad-
ditional annotations made by developmental psy-
chologists as part of a study of motion utterances
(Dan I. Slobin, p.c.). These annotations indicate
semantic and pragmatic features available in the
scene. A simple feature structure representation of
a sample input token is shown here; boxed numbers
indicate that the relevant entities are identified:
Many details have been omitted, and a number
of simplifying assumptions have been made. But
the rough outline given here nevertheless captures
the core computational problem faced by the child
learner in acquiring multi-word constructions in a
framework putting meaning on par with form.
</bodyText>
<sectionHeader confidence="0.971428" genericHeader="method">
3 Learning algorithms
</sectionHeader>
<bodyText confidence="0.9999935">
We model the learning task as a search through the
space of possible grammars, with new constructions
incrementally added based on encountered data. As
in the child learning situation, the goal of learning
is to converge on an optimal set of constructions,
i.e., a grammar that is both general enough to en-
compass significant novel data and specific enough
to accurately predict previously seen data.
A suitable overarching computational framework
for guiding the search is provided by the mini-
mum description length (MDL) heuristic (Rissanen,
1978), which is used to find the optimal analysis
of data in terms of (a) a compact representation of
the data (i.e., a grammar); and (b) a compact means
of describing the original data in terms of the com-
pressed representation (i.e., constructional analyses
using the grammar). The MDL heuristic exploits a
tradeoff between competing preferences for smaller
grammars (encouraging generalization) and for sim-
pler analyses of the data (encouraging the retention
of specific/frequent constructions).
The rest of this section makes the learning frame-
work concrete. Section 3.1 describes several heuris-
tics for moving through the space of grammars (i.e.,
how to update a grammar with new constructions
based on input data), and Section 3.2 describes how
to chose among these candidate moves to find op-
timal points in the search space (i.e., specific MDL
criteria for evaluating new grammars). These speci-
fications extend previous methods to accommodate
the relational structures of the ECG formalism and
the process-based assumptions of the model.
</bodyText>
<figure confidence="0.994476928571429">
Discourse
Throw
throwerNaomi 1
throweeBall 2
Scene
speakerMother 0
addresseeNaomi 1
speech actimperative
activityplay
joint attentionBall 2
textthrow the ball
Form
intonationfalling
ParticipantsMother 0 , Naomi 1 , Ball 2
</figure>
<page confidence="0.584998">
20
</page>
<subsectionHeader confidence="0.997658">
3.1 Updating the grammar
</subsectionHeader>
<bodyText confidence="0.99991476">
The grammar may be updated in three ways:
hypothesis forming new structured maps to ac-
count for mappings present in the input but un-
explained by the current grammar;
reorganization exploiting regularities in the set of
known constructions (merge two similar con-
structions into a more general construction, or
compose two constructions that cooccur into a
larger construction); and
reinforcement incrementing the weight associated
with constructions that are successfully used
during comprehension.
Hypothesis. The first operation addresses the
core computational challenge of learning new struc-
tured maps. The key idea here is that the learner is
assumed to have access to a partial analysis based
on linguistic knowledge, as well as a fuller situa-
tion interpretation it can infer from context. Any
difference between the two can directly prompt the
formation of new constructions that will improve
the agent’s ability to handle subsequent instances of
similar utterances in similar contexts. In particu-
lar, certain form and meaning relations that are un-
matched by the analysis but present in context may
be mapped using the procedure in Figure 3.
</bodyText>
<figureCaption confidence="0.999533">
Figure 3: Construction hypothesis.
</figureCaption>
<bodyText confidence="0.996937777777778">
The algorithm creates new constructions map-
ping form and meaning relations whose arguments
are already constructionally mapped. It is best illus-
trated by example, based on the sample input token
shown in Section 2.3 and depicted schematically in
Figure 4. Given the utterance “throw the ball” and a
grammar including constructions for throw and ball
(but not the), the analyzer produces a semspec in-
cluding a Ball schema and a Throw schema, without
indicating any relations between them. The reso-
lution process matches these schemas to the actual
context, which includes a particular throwing event
in which the addressee (Naomi) is the thrower of
a particular ball. The resulting resolved analysis
looks like Figure 4 but without the new construction
(marked with dashed lines): the two lexical con-
structions are shown mapping to particular utterance
forms and contextual items.
Figure 4: Hypothesizing a relational mapping for
the utterance throw ball. Heavy solid lines indi-
cate structures matched during analysis; heavy dot-
ted lines indicate the newly hypothesized mapping.
Next, an unmatched form relation (the word or-
der edge between throw and ball) is found, fol-
lowed by a corresponding unmatched meaning re-
lation (the binding between the Throw.throwee role
and the specific Ball in context); these are shown
in the figure using heavy dashed lines. Crucially,
these relations meet the condition in step 3 that
the relations be pseudo-isomorphic. This condition
captures three common patterns of relational form-
meaning mappings, i.e., ways in which a meaning
relation rel over A and B can be correlated
with a form relation rel over A and B (e.g., word
order); these are illustrated in Figure 5, where we
assume a simple form relation:
</bodyText>
<listItem confidence="0.7870358">
(a) strictly isomorphic: B is a role-filler of A (or
vice versa) (A .r1 B )
(b) shared role-filler: A and B each have a role
filled by the same entity (A .r1 B .r2)
(c) sibling role-fillers: A and B fill roles of the
</listItem>
<bodyText confidence="0.957748952380953">
same schema (Y.r1 A , Y.r2 B )
Hypothesize construction. Given utterance in
situational context and current grammar :
1. Call the construction analysis/resolution pro-
cesses on ( , , ) to produce a semspec con-
sisting of form and meaning graphs and .
Nodes and edges of and are marked as
matched or unmatched by the analysis.
2. Find rel (A ,B ), an unmatched edge in cor-
responding to an unused form relation over the
matched form poles of two constructs A and B.
3. Find rel (A , B ), an unmatched edge (or
subgraph) in corresponding to an unused
meaning relation (or set of bindings) over the
corresponding matched meaning poles A and
B . rel (A ,B ) is required to be pseudo-
isomorphic to rel (A ,B ).
4. Create a new construction with constituents
A and B and form and meaning constraints cor-
responding to rel (A ,B ) and rel (A ,B ),
respectively.
</bodyText>
<figure confidence="0.986891904761905">
UTTERANCE CONSTRUCTS CONTEXT
intonation: falling
throw
ball
NEW CONSTRUCTION
THROW
BALL
Throw
thrower
throwee
Ball
Naomi
Mom
Discourse
speaker:
addressee:
speech act: imperative
joint attention:
activity: play
temporality: ongoing
Block
</figure>
<page confidence="0.952996">
21
</page>
<figureCaption confidence="0.999460333333333">
Figure 5: Pseudo-isomorphic relational mappings
over constructs A and B: (a) strictly isomorphic; (b)
shared role-filler; and (c) sibling role-fillers.
</figureCaption>
<bodyText confidence="0.978554833333333">
This condition enforces structural similarity be-
tween the two relations while recognizing that con-
structions may involve relations that are not strictly
isomorphic. (The example mapping shown in the
figure is strictly isomorphic.) The resulting con-
struction is shown formally in Figure 6.
</bodyText>
<figureCaption confidence="0.993211">
Figure 6: Example learned construction.
</figureCaption>
<bodyText confidence="0.994075944444444">
Reorganization. Besides hypothesizing con-
structions based on new data, the model also allows
new constructions to be formed via constructional
reorganization, essentially by applying general cat-
egorization principles to the current grammar, as de-
scribed in Figure 7.
For example, the construction
and a similar construction can be
merged into a general construc-
tion; the resulting subcase constructions each retain
the appropriate type constraint. Similarly, a general
and construc-
tion may occur in many analyses in which they com-
pete for the constituent. Since they have
compatible constraints in both form and meaning (in
the latter case based on the same conceptual Throw
schema), repeated co-occurrence may lead to the
formation of a larger construction that includes all
</bodyText>
<figureCaption confidence="0.999692">
Figure 7: Construction reorganization.
</figureCaption>
<bodyText confidence="0.992907888888889">
three constituents.
Reinforcement. Each construction is associated
with a weight, which is incremented each time it is
used in an analysis that is successfully matched to
the context. A successful match covers a majority
of the contextually available bindings.
Both hypothesis and reorganization provide
means of proposing new constructions; we now
specify how proposed constructions are evaluated.
</bodyText>
<subsectionHeader confidence="0.999906">
3.2 Evaluating grammar cost
</subsectionHeader>
<bodyText confidence="0.997903555555556">
The MDL criteria used in the model is based on the
cost of the grammar given the data :
cost size cost
size size
size length
cost score
score weight type
height semfit
where and are learning parameters that con-
trol the relative bias toward model simplicity and
data compactness. The size( ) is the sum over the
size of each construction in the grammar ( is
the number of constituents in, is the number
of constraints in , and each element reference
in has a length, measured as slot chain length).
The cost (complexity) of the data given is the
sum of the analysis scores of each input token
using . This score sums over the constructions
</bodyText>
<figure confidence="0.997640615384616">
A
Bf
A
Bf
A
Bf
f
f
f
relr
f
relf
relf
A
B
A
B
A
B
Am
Am
A m
Bm
Bm
Bm
r1
r1
r2
r2
x
Y
construction
constituents
t1 :
t2 :
form
t1 before t2
meaning
t1 .throwee t2
</figure>
<bodyText confidence="0.965964375">
Reorganize constructions. Reorganize to con-
solidate similar and co-occurring constructions:
Merge: Pairs of constructions with significant
shared structure (same number of constituents,
minimal ontological distance (i.e., distance in
the type ontology) between corresponding con-
stituents, maximal overlap in constraints) may
be merged into a new construction containing
the shared structure; the original constructions
are rewritten as subcases of the new construc-
tion along with the non-overlapping information.
Compose: Pairs of constructions that co-occur
frequently with compatible constraints (are part
of competing analyses using the same con-
stituent, or appear in a constituency relation-
ship) may be composed into one construction.
</bodyText>
<page confidence="0.991347">
22
</page>
<bodyText confidence="0.999982176470589">
in the analysis of , where weight reflects rel-
ative (in)frequency, type denotes the number of
ontology items of type , summed over all the con-
stituents in the analysis and discounted by parame-
ter . The score also includes terms for the height
of the derivation graph and the semantic fit provided
by the analyzer as a measure of semantic coherence.
In sum, these criteria favor constructions that are
simply described (relative to the available meaning
representations and the current set of constructions),
frequently useful in analysis, and specific to the data
encountered. The MDL criteria thus approximate
Bayesian learning, where the minimizing of cost
corresponds to maximizing the posterior probabil-
ity, the structural prior corresponds to the grammar
size, and likelihood corresponds to the complexity
of the data relative to the grammar.
</bodyText>
<sectionHeader confidence="0.982491" genericHeader="method">
4 Learning verb islands
</sectionHeader>
<bodyText confidence="0.999918181818182">
The model was applied to the data set described in
Section 2.3 to determine whether lexically specific
multi-word constructions could be learned using the
MDL learning framework described. This task rep-
resents an important first step toward general gram-
matical constructions, and is of cognitive interest,
since item-based patterns appear to be learned on
independent trajectories (i.e., each verb forms its
own “island” of organization (Tomasello, 2003)).
We give results for drop ( =10 examples), throw
( =25), and fall ( =50).
</bodyText>
<figure confidence="0.956289">
0 10 20 30 40 50 60 70 80 90 100
Percent training examples encountered
</figure>
<figureCaption confidence="0.968667">
Figure 8: Incrementally improving comprehension
for three verb islands.
</figureCaption>
<bodyText confidence="0.99999088">
Given the small corpus sizes, the focus for this
experiment is not on the details of the statisti-
cal learning framework but instead on a qualita-
tive evaluation of whether learned constructions im-
prove the model’s comprehension over time, and
how verbs may differ in their learning trajectories.
Qualitatively, the model first learned item-specific
constructions as expected (e.g. throw bear, throw
books, you throw), later in learning generalizing
over different event participants (throw OBJECT,
PERSON throw, etc.).
A quantitative measure of comprehension over
time, coverage, was defined as the percentage of to-
tal bindings in the data accounted for at each learn-
ing step. This metric indicates how new construc-
tions incrementally improve the model’s compre-
hensive capacity, shown in Figure 8. The throw sub-
set, for example, contains 45 bindings to the roles of
the Throw schema (thrower, throwee, and goal loca-
tion). At the start of learning, the model has no com-
binatorial constructions and can account for none of
these. But the model gradually amasses construc-
tions with greater coverage, and by the tenth input
token, the model learns new constructions that ac-
count for the majority of the bindings in the data.
The learning trajectories do appear distinct:
throw constructions show a gradual build-up before
plateauing, while fall has a more fitful climb con-
verging at a higher coverage rate than throw. It is
interesting to note that the throw subset has a much
higher percentage of imperative utterances than fall
(since throwing is pragmatically more likely to be
done on command); the learning strategy used in
the current model focuses on relational mappings
and misses the association of an imperative speech-
act with the lack of an expressed agent, providing a
possible explanation for the different trajectories.
While further experimentation with larger train-
ing sets is needed, the results indicate that the model
is able to acquire useful item-based constructions
like those learned by children from a small number
examples. More importantly, the learned construc-
tions permit a limited degree of generalization that
allows for increasingly complete coverage (or com-
prehension) of new utterances, fulfilling the goal of
the learning model. Differences in verb learning
lend support to the verb island hypothesis and illus-
trate how the particular semantic, pragmatic and sta-
tistical properties of different verbs can affect their
learning course.
</bodyText>
<sectionHeader confidence="0.99303" genericHeader="discussions">
5 Discussion and future directions
</sectionHeader>
<bodyText confidence="0.999968">
The work presented here is intended to offer an al-
ternative formulation of the grammar learning prob-
lem in which meaning in context plays a pivotal role
in the acquisition of grammar. Specifically, mean-
ing is incorporated directly into the target grammar
(via the construction representation), the input data
(via the context representation) and the evaluation
criteria (which is usage-based, i.e. to improve com-
prehension). To the extent possible, the assump-
</bodyText>
<figure confidence="0.8568613">
Percent correct bindings
100
80
60
40
20
0
drop (n=10, b=18)
throw (n=25, b=45)
fall (n=53, b=86)
</figure>
<page confidence="0.993019">
23
</page>
<bodyText confidence="0.999972378378378">
tions made with respect to structures and processes
available to a human language learner in this stage
are consistent with evidence from across the cog-
nitive spectrum. Though only preliminary conclu-
sions can be made, the model is a concrete compu-
tational step toward validating a meaning-oriented
approach to grammar learning.
The model draws from a number of computa-
tional forerunners from both logical and probabilis-
tic traditions, including Bayesian models of word
learning (Bailey, 1997; Stolcke, 1994) for the over-
all optimization model, and work by Wolff (1982)
modeling language acquisition (primarily produc-
tion rules) using data compression techniques sim-
ilar to the MDL approach taken here. The use of
the results of analysis to hypothesize new mappings
can be seen as related to both explanation-based
learning (DeJong and Mooney, 1986) and inductive
logic programming (Muggleton and Raedt, 1994).
The model also has some precedents in the work
of Siskind (1997) and Thompson (1998), both of
which based learning on the discovery of isomor-
phic structures in syntactic and semantic representa-
tions, though in less linguistically rich formalisms.
In current work we are applying the model to the
full corpus of English verbs, as well as crosslin-
guistic data including Russian case markers and
Mandarin directional particles and aspect markers.
These experiments will further test the robustness
of the model’s theoretical assumptions and protect
against model overfitting and typological bias. We
are also developing alternative means of evaluating
the system’s progress based on a rudimentary model
of production, which would enable it to label scene
descriptions using its current grammar and thus fa-
cilitate detailed studies of how the system general-
izes (and overgeneralizes) to unseen data.
</bodyText>
<sectionHeader confidence="0.998362" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999234">
We thank members of the ICSI Neural Theory of Lan-
guage group and two anonymous reviewers.
</bodyText>
<sectionHeader confidence="0.999466" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999880402777778">
Steven Abney. 1996. Partial parsing via finite-state cas-
cades. In Workshop on Robust Parsing, 8th European
Summer School in Logic, Language and Information,
pages 8–15, Prague, Czech Republic.
David R. Bailey. 1997. When Push Comes to Shove: A
Computational Model of the Role ofMotor Control in
the Acquisition ofAction Verbs. Ph.D. thesis, Univer-
sity of California at Berkeley.
Benjamin K. Bergen and Nancy Chang. in press.
Simulation-based language understanding in Embod-
ied Construction Grammar. In Construction Gram-
mar(s): Cognitive and Cross-language dimensions.
John Benjamins.
Paul Bloom. 2000. How Children Learn the Meanings
of Words. MIT Press, Cambridge, MA.
John Bryant. 2003. Constructional analysis. Master’s
thesis, University of California at Berkeley.
Nancy Chang, Jerome Feldman, Robert Porzel, and
Keith Sanders. 2002. Scaling cognitive linguistics:
Formalisms for language understanding. In Proc.
1st International Workshop on Scalable Natural Lan-
guage Understanding, Heidelberg, Germany.
G.F. DeJong and R. Mooney. 1986. Explanation-based
learning: An alternative view. Machine Learning,
1(2):145–176.
Charles Fillmore and Paul Kay. 1999. Construction
grammar. CSLI, Stanford, CA. To appear.
E.M. Gold. 1967. Language identification in the limit.
Information and Control, 16:447–474.
Adele E. Goldberg. 1995. Constructions: A Construc-
tion Grammar Approach to Argument Structure. Uni-
versity of Chicago Press.
Ronald W. Langacker. 1987. Foundations of Cognitive
Grammar, Vol. 1. Stanford University Press.
Brian MacWhinney. 1991. The CHILDES project:
Tools for analyzing talk. Erlbaum, Hillsdale, NJ.
Stephen Muggleton and Luc De Raedt. 1994. Inductive
logic programming: Theory and methods. Journal of
Logic Programming, 19/20:629–679.
Terry Regier. 1996. The Human Semantic Potential.
MIT Press, Cambridge, MA.
Jorma Rissanen. 1978. Modeling by shortest data de-
scription. Automatica, 14:465–471.
Deb Roy and Alex Pentland. 1998. Learning audio-
visually grounded words from natural input. In Proc.
AAAI workshop, Grounding Word Meaning.
J. Sachs. 1983. Talking about the there and then: the
emergence of displaced reference in parent-child dis-
course. In K.E. Nelson, editor, Children’s language,
volume 4, pages 1–28. Lawrence Erlbaum Associates.
Jeffrey Mark Siskind, 1997. A computational study
of cross-situational techniques for learning word-to-
meaning mappings, chapter 2. MIT Press.
Andreas Stolcke. 1994. Bayesian Learning of Prob-
abilistic Language Models. Ph.D. thesis, Computer
Science Division, University of California at Berke-
ley.
Cynthia A. Thompson. 1998. Semantic Lexicon Acquisi-
tion for Learning Natural Language Interfaces. Ph.D.
thesis, Department of Computer Sciences, University
of Texas at Austin, December.
Michael Tomasello. 1995. Joint attention as social
cognition. In C.D. Moore P., editor, Joint atten-
tion: Its origins and role in development. Lawrence
Erlbaum Associates, Hillsdale, NJ. Educ/Psych
BF720.A85.J65 1995.
Michael Tomasello. 2003. Constructing a Language: A
Usage-Based Theory of Language Acquisition. Har-
vard University Press, Cambridge, MA.
J. Gerard Wolff. 1982. Language acquisition, data com-
pression and generalization. Language &amp; Communi-
cation, 2(1):57–89.
</reference>
<page confidence="0.999178">
24
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.867348">
<title confidence="0.997252">Putting Meaning into Grammar Learning</title>
<author confidence="0.907628">Nancy</author>
<affiliation confidence="0.9894085">UC Berkeley, Dept. of Computer Science International Computer Science</affiliation>
<address confidence="0.999008">1947 Center St., Suite Berkeley, CA 94704</address>
<email confidence="0.998782">nchang@icsi.berkeley.edu</email>
<abstract confidence="0.997898692307692">This paper proposes a formulation of grammar learning in which meaning plays a fundamental role. We present a computational model that aims to satisfy convergent constraints from cognitive linguistics and crosslinguistic developmental evidence within a statistically driven framework. The target grammar, input data and goal of learning are all designed to allow a tight coupling between language learning and comprehension that drives the acquisition of new constructions. The model is applied to learn lexically specific multi-word constructions from annotated child-directed transcript data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Partial parsing via finite-state cascades.</title>
<date>1996</date>
<booktitle>In Workshop on Robust Parsing, 8th European Summer School in Logic, Language and Information,</booktitle>
<pages>8--15</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="12745" citStr="Abney, 1996" startWordPosition="1949" endWordPosition="1951">erpretation, called the semspec, indicating which conceptual schemas are involved and how they are related. The analyzer is also required to be robust to input that is not covered by its current grammar, since that situation is the norm during language learning. Bryant (2003) describes an implemented construction analyzer program that meets these needs. The construction analyzer takes as input a set of ECG constructions (linguistic knowledge), a set of ECG schemas (conceptual knowledge) and an utterance. The analyzer draws on partial parsing techniques previously applied to syntactic parsing (Abney, 1996): utterances not covered by known constructions yield partially filled semspecs, and unknown forms in the input are skipped. As a result, even a small set of simple constructions can provide skeletal interpretations of complex utterances. Figure 2 gives an iconic representation of the result of analyzing the utterance I throw the ball using the and constructions shown earlier, along with some additional FORM throw ball the I form form I t1 THROW-TRANSITIVE form meaning THROW constructional t2 form meaning THE- BALL t3 meaning meaning MEANING Speaker Ball thrower throwee Throw 19 lexical constr</context>
</contexts>
<marker>Abney, 1996</marker>
<rawString>Steven Abney. 1996. Partial parsing via finite-state cascades. In Workshop on Robust Parsing, 8th European Summer School in Logic, Language and Information, pages 8–15, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Bailey</author>
</authors>
<title>When Push Comes to Shove: A Computational Model of the Role ofMotor Control in the Acquisition ofAction Verbs.</title>
<date>1997</date>
<tech>Ph.D. thesis,</tech>
<institution>University of California at Berkeley.</institution>
<contexts>
<context position="11589" citStr="Bailey, 1997" startWordPosition="1772" endWordPosition="1773">straints, reflecting children’s knowledge of what kinds of entities can take place in different events. 2.2.2 Lexical constructions The input to learning includes a set of lexical constructions, represented using the ECG formalism, linking simple forms (i.e. words) to specific conceptual items. Examples of these include the and constructions in the figure, as well as the construction formally defined in Figure 1. Lexical learning is not the focus of the current work, but a number of previous computational approaches have shown how simple mappings may be acquired from experience (Regier, 1996; Bailey, 1997; Roy and Pentland, 1998). 2.2.3 Construction analyzer As mentioned earlier, the ECG construction formalism is designed to support processes of language use. In particular, the model makes use of a construction analyzer that identifies the constructions responsible for a given utterance, much like a syntactic parser in a traditional language understanding system identifies which parse rules are responsible. In this case, however, the basic representational unit is a form-meaning pair. The analyzer must therefore also supply a semantic interpretation, called the semspec, indicating which concep</context>
<context position="30765" citStr="Bailey, 1997" startWordPosition="4808" endWordPosition="4809">possible, the assumpPercent correct bindings 100 80 60 40 20 0 drop (n=10, b=18) throw (n=25, b=45) fall (n=53, b=86) 23 tions made with respect to structures and processes available to a human language learner in this stage are consistent with evidence from across the cognitive spectrum. Though only preliminary conclusions can be made, the model is a concrete computational step toward validating a meaning-oriented approach to grammar learning. The model draws from a number of computational forerunners from both logical and probabilistic traditions, including Bayesian models of word learning (Bailey, 1997; Stolcke, 1994) for the overall optimization model, and work by Wolff (1982) modeling language acquisition (primarily production rules) using data compression techniques similar to the MDL approach taken here. The use of the results of analysis to hypothesize new mappings can be seen as related to both explanation-based learning (DeJong and Mooney, 1986) and inductive logic programming (Muggleton and Raedt, 1994). The model also has some precedents in the work of Siskind (1997) and Thompson (1998), both of which based learning on the discovery of isomorphic structures in syntactic and semanti</context>
</contexts>
<marker>Bailey, 1997</marker>
<rawString>David R. Bailey. 1997. When Push Comes to Shove: A Computational Model of the Role ofMotor Control in the Acquisition ofAction Verbs. Ph.D. thesis, University of California at Berkeley.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Benjamin K Bergen</author>
<author>Nancy Chang</author>
</authors>
<title>in press. Simulation-based language understanding in Embodied Construction Grammar. In Construction Grammar(s): Cognitive and Cross-language dimensions.</title>
<publisher>John Benjamins.</publisher>
<marker>Bergen, Chang, </marker>
<rawString>Benjamin K. Bergen and Nancy Chang. in press. Simulation-based language understanding in Embodied Construction Grammar. In Construction Grammar(s): Cognitive and Cross-language dimensions. John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Bloom</author>
</authors>
<title>How Children Learn the Meanings of Words.</title>
<date>2000</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2572" citStr="Bloom, 2000" startWordPosition="373" endWordPosition="374">e and constructional approaches to grammar assume that the basic unit of linguistic knowledge needed to support language use consists of pairings of form and meaning, or constructions (Langacker, 1987; Goldberg, 1995; Fillmore and Kay, 1999). Moreover, by the time children make the leap from single words to complex combinations, they have amassed considerable conceptual knowledge, including familiarity with a wide variety of entities and events and sophisticated pragmatic skills (such as using joint attention to infer communicative intentions (Tomasello, 1995) and subtle lexical distinctions (Bloom, 2000)). The developmental evidence thus suggests that the input to grammar learning may in principle include not just surface strings but also meaningful situation descriptions with rich semantic and pragmatic information. This paper formalizes the grammar learning problem in line with the observations above, taking seriously the ideas that the target of learning, for both lexical items and larger phrasal and clausal units, is a bipolar structure in which meaning is on par with form, and that meaningful language use drives language learning. The resulting core computational problem can be seen as a</context>
</contexts>
<marker>Bloom, 2000</marker>
<rawString>Paul Bloom. 2000. How Children Learn the Meanings of Words. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Bryant</author>
</authors>
<title>Constructional analysis. Master’s thesis,</title>
<date>2003</date>
<institution>University of California at Berkeley.</institution>
<contexts>
<context position="12409" citStr="Bryant (2003)" startWordPosition="1899" endWordPosition="1900">ion analyzer that identifies the constructions responsible for a given utterance, much like a syntactic parser in a traditional language understanding system identifies which parse rules are responsible. In this case, however, the basic representational unit is a form-meaning pair. The analyzer must therefore also supply a semantic interpretation, called the semspec, indicating which conceptual schemas are involved and how they are related. The analyzer is also required to be robust to input that is not covered by its current grammar, since that situation is the norm during language learning. Bryant (2003) describes an implemented construction analyzer program that meets these needs. The construction analyzer takes as input a set of ECG constructions (linguistic knowledge), a set of ECG schemas (conceptual knowledge) and an utterance. The analyzer draws on partial parsing techniques previously applied to syntactic parsing (Abney, 1996): utterances not covered by known constructions yield partially filled semspecs, and unknown forms in the input are skipped. As a result, even a small set of simple constructions can provide skeletal interpretations of complex utterances. Figure 2 gives an iconic </context>
</contexts>
<marker>Bryant, 2003</marker>
<rawString>John Bryant. 2003. Constructional analysis. Master’s thesis, University of California at Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Chang</author>
<author>Jerome Feldman</author>
<author>Robert Porzel</author>
<author>Keith Sanders</author>
</authors>
<title>Scaling cognitive linguistics: Formalisms for language understanding.</title>
<date>2002</date>
<booktitle>In Proc. 1st International Workshop on Scalable Natural Language Understanding,</booktitle>
<location>Heidelberg, Germany.</location>
<contexts>
<context position="6539" citStr="Chang et al., 2002" startWordPosition="985" endWordPosition="988">r and a sequence of training examples consisting of an utterance paired with its context, find the best grammar to fit seen data and generalize to new data. The remainder of this section describes the hypothesis space, prior knowledge and input data relevant to the task. 2.1 Hypothesis space: embodied constructions The space of possible grammars (or sets of constructions) is defined by Embodied Construction Grammar (ECG), a computationally explicit unification-based formalism for capturing insights from the construction grammar and cognitive linguistics literature (Bergen and Chang, in press; Chang et al., 2002). ECG is designed to support the analysis process mentioned above, which determines what constructions and schematic meanings are present in an utterance, resulting in a semantic specification (or semspec).1 1ECG is intended to support a simulation-based model of language understanding, with the semspec parameterizing a We highlight a few relevant aspects of the formalism, exemplified in Figure 1. Each construction has sections labeled form and meaning listing the entities (or roles) and constraints (type constraints marked with :, filler constraints marked with , and identification (or coinde</context>
</contexts>
<marker>Chang, Feldman, Porzel, Sanders, 2002</marker>
<rawString>Nancy Chang, Jerome Feldman, Robert Porzel, and Keith Sanders. 2002. Scaling cognitive linguistics: Formalisms for language understanding. In Proc. 1st International Workshop on Scalable Natural Language Understanding, Heidelberg, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G F DeJong</author>
<author>R Mooney</author>
</authors>
<title>Explanation-based learning: An alternative view.</title>
<date>1986</date>
<booktitle>Machine Learning,</booktitle>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="31122" citStr="DeJong and Mooney, 1986" startWordPosition="4862" endWordPosition="4865">del is a concrete computational step toward validating a meaning-oriented approach to grammar learning. The model draws from a number of computational forerunners from both logical and probabilistic traditions, including Bayesian models of word learning (Bailey, 1997; Stolcke, 1994) for the overall optimization model, and work by Wolff (1982) modeling language acquisition (primarily production rules) using data compression techniques similar to the MDL approach taken here. The use of the results of analysis to hypothesize new mappings can be seen as related to both explanation-based learning (DeJong and Mooney, 1986) and inductive logic programming (Muggleton and Raedt, 1994). The model also has some precedents in the work of Siskind (1997) and Thompson (1998), both of which based learning on the discovery of isomorphic structures in syntactic and semantic representations, though in less linguistically rich formalisms. In current work we are applying the model to the full corpus of English verbs, as well as crosslinguistic data including Russian case markers and Mandarin directional particles and aspect markers. These experiments will further test the robustness of the model’s theoretical assumptions and </context>
</contexts>
<marker>DeJong, Mooney, 1986</marker>
<rawString>G.F. DeJong and R. Mooney. 1986. Explanation-based learning: An alternative view. Machine Learning, 1(2):145–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Fillmore</author>
<author>Paul Kay</author>
</authors>
<date>1999</date>
<booktitle>Construction grammar. CSLI,</booktitle>
<location>Stanford, CA.</location>
<note>To appear.</note>
<contexts>
<context position="2201" citStr="Fillmore and Kay, 1999" startWordPosition="317" endWordPosition="320">ally tagged corpora. But a variety of cognitive, linguistic and developmental considerations suggest that meaning plays a central role in the acquisition of linguistic units at all levels. We start with the proposition that language use should drive language learning — that is, the learner’s goal is to improve its ability to communicate, via comprehension and production. Cognitive and constructional approaches to grammar assume that the basic unit of linguistic knowledge needed to support language use consists of pairings of form and meaning, or constructions (Langacker, 1987; Goldberg, 1995; Fillmore and Kay, 1999). Moreover, by the time children make the leap from single words to complex combinations, they have amassed considerable conceptual knowledge, including familiarity with a wide variety of entities and events and sophisticated pragmatic skills (such as using joint attention to infer communicative intentions (Tomasello, 1995) and subtle lexical distinctions (Bloom, 2000)). The developmental evidence thus suggests that the input to grammar learning may in principle include not just surface strings but also meaningful situation descriptions with rich semantic and pragmatic information. This paper </context>
</contexts>
<marker>Fillmore, Kay, 1999</marker>
<rawString>Charles Fillmore and Paul Kay. 1999. Construction grammar. CSLI, Stanford, CA. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Gold</author>
</authors>
<title>Language identification in the limit.</title>
<date>1967</date>
<journal>Information and Control,</journal>
<pages>16--447</pages>
<marker>Gold, 1967</marker>
<rawString>E.M. Gold. 1967. Language identification in the limit. Information and Control, 16:447–474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adele E Goldberg</author>
</authors>
<title>Constructions: A Construction Grammar Approach to Argument Structure.</title>
<date>1995</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="2176" citStr="Goldberg, 1995" startWordPosition="315" endWordPosition="316">e-scale semantically tagged corpora. But a variety of cognitive, linguistic and developmental considerations suggest that meaning plays a central role in the acquisition of linguistic units at all levels. We start with the proposition that language use should drive language learning — that is, the learner’s goal is to improve its ability to communicate, via comprehension and production. Cognitive and constructional approaches to grammar assume that the basic unit of linguistic knowledge needed to support language use consists of pairings of form and meaning, or constructions (Langacker, 1987; Goldberg, 1995; Fillmore and Kay, 1999). Moreover, by the time children make the leap from single words to complex combinations, they have amassed considerable conceptual knowledge, including familiarity with a wide variety of entities and events and sophisticated pragmatic skills (such as using joint attention to infer communicative intentions (Tomasello, 1995) and subtle lexical distinctions (Bloom, 2000)). The developmental evidence thus suggests that the input to grammar learning may in principle include not just surface strings but also meaningful situation descriptions with rich semantic and pragmatic</context>
</contexts>
<marker>Goldberg, 1995</marker>
<rawString>Adele E. Goldberg. 1995. Constructions: A Construction Grammar Approach to Argument Structure. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald W Langacker</author>
</authors>
<date>1987</date>
<journal>Foundations of Cognitive Grammar,</journal>
<volume>1</volume>
<publisher>Stanford University Press.</publisher>
<contexts>
<context position="2160" citStr="Langacker, 1987" startWordPosition="313" endWordPosition="314"> the lack of large-scale semantically tagged corpora. But a variety of cognitive, linguistic and developmental considerations suggest that meaning plays a central role in the acquisition of linguistic units at all levels. We start with the proposition that language use should drive language learning — that is, the learner’s goal is to improve its ability to communicate, via comprehension and production. Cognitive and constructional approaches to grammar assume that the basic unit of linguistic knowledge needed to support language use consists of pairings of form and meaning, or constructions (Langacker, 1987; Goldberg, 1995; Fillmore and Kay, 1999). Moreover, by the time children make the leap from single words to complex combinations, they have amassed considerable conceptual knowledge, including familiarity with a wide variety of entities and events and sophisticated pragmatic skills (such as using joint attention to infer communicative intentions (Tomasello, 1995) and subtle lexical distinctions (Bloom, 2000)). The developmental evidence thus suggests that the input to grammar learning may in principle include not just surface strings but also meaningful situation descriptions with rich semant</context>
</contexts>
<marker>Langacker, 1987</marker>
<rawString>Ronald W. Langacker. 1987. Foundations of Cognitive Grammar, Vol. 1. Stanford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian MacWhinney</author>
</authors>
<title>The CHILDES project: Tools for analyzing talk. Erlbaum,</title>
<date>1991</date>
<location>Hillsdale, NJ.</location>
<contexts>
<context position="15665" citStr="MacWhinney, 1991" startWordPosition="2407" endWordPosition="2408">ve context (a set of linked conceptual schemas corresponding to the participants, salient scene and discourse information available in the situation). The learning model receives only positive examples, as in the child learning case. Note, however, that the interpretation a given utterance has in context depends on the current state of linguistic knowledge. Thus the same utterance at different stages may lead to different learning behavior. The specific training corpus used in learning experiments is a subset of the Sachs corpus of the CHILDES database of parent-child transcripts(Sachs, 1983; MacWhinney, 1991), with additional annotations made by developmental psychologists as part of a study of motion utterances (Dan I. Slobin, p.c.). These annotations indicate semantic and pragmatic features available in the scene. A simple feature structure representation of a sample input token is shown here; boxed numbers indicate that the relevant entities are identified: Many details have been omitted, and a number of simplifying assumptions have been made. But the rough outline given here nevertheless captures the core computational problem faced by the child learner in acquiring multi-word constructions in</context>
</contexts>
<marker>MacWhinney, 1991</marker>
<rawString>Brian MacWhinney. 1991. The CHILDES project: Tools for analyzing talk. Erlbaum, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Muggleton</author>
<author>Luc De Raedt</author>
</authors>
<title>Inductive logic programming: Theory and methods.</title>
<date>1994</date>
<booktitle>Journal of Logic Programming,</booktitle>
<pages>19--20</pages>
<marker>Muggleton, De Raedt, 1994</marker>
<rawString>Stephen Muggleton and Luc De Raedt. 1994. Inductive logic programming: Theory and methods. Journal of Logic Programming, 19/20:629–679.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Regier</author>
</authors>
<title>The Human Semantic Potential.</title>
<date>1996</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="11575" citStr="Regier, 1996" startWordPosition="1770" endWordPosition="1771">ct to type constraints, reflecting children’s knowledge of what kinds of entities can take place in different events. 2.2.2 Lexical constructions The input to learning includes a set of lexical constructions, represented using the ECG formalism, linking simple forms (i.e. words) to specific conceptual items. Examples of these include the and constructions in the figure, as well as the construction formally defined in Figure 1. Lexical learning is not the focus of the current work, but a number of previous computational approaches have shown how simple mappings may be acquired from experience (Regier, 1996; Bailey, 1997; Roy and Pentland, 1998). 2.2.3 Construction analyzer As mentioned earlier, the ECG construction formalism is designed to support processes of language use. In particular, the model makes use of a construction analyzer that identifies the constructions responsible for a given utterance, much like a syntactic parser in a traditional language understanding system identifies which parse rules are responsible. In this case, however, the basic representational unit is a form-meaning pair. The analyzer must therefore also supply a semantic interpretation, called the semspec, indicatin</context>
</contexts>
<marker>Regier, 1996</marker>
<rawString>Terry Regier. 1996. The Human Semantic Potential. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorma Rissanen</author>
</authors>
<title>Modeling by shortest data description.</title>
<date>1978</date>
<journal>Automatica,</journal>
<pages>14--465</pages>
<contexts>
<context position="3999" citStr="Rissanen, 1978" startWordPosition="591" endWordPosition="593">meaning relations (typically role-filler bindings). Such correlations are essential for capturing complex multi-unit constructions, both lexically specific constructions and more general grammatical constructions. The remainder of the paper is structured as follows. Section 2 states the learning task and provides an overview of the model and its assumptions. We then present algorithms for inducing structured mappings, based on either specific input examples or the current set of constructions (Section 3), and describe how these are evaluated using criteria based on minimum description length (Rissanen, 1978). Initial results from applying the learning algorithms to a small corpus of child-directed utterances demonstrate the viability of the approach (Section 4). We conclude with a discussion of the broader implications of this approach for language learning and use. 17 2 Overview of the learning problem We begin with an informal description of our learning task, to be formalized below. At all stages of language learning, children are assumed to exploit general cognitive abilities to make sense of the flow of objects and events they experience. To make sense of linguistic events sounds and gesture</context>
<context position="16883" citStr="Rissanen, 1978" startWordPosition="2594" endWordPosition="2595">framework putting meaning on par with form. 3 Learning algorithms We model the learning task as a search through the space of possible grammars, with new constructions incrementally added based on encountered data. As in the child learning situation, the goal of learning is to converge on an optimal set of constructions, i.e., a grammar that is both general enough to encompass significant novel data and specific enough to accurately predict previously seen data. A suitable overarching computational framework for guiding the search is provided by the minimum description length (MDL) heuristic (Rissanen, 1978), which is used to find the optimal analysis of data in terms of (a) a compact representation of the data (i.e., a grammar); and (b) a compact means of describing the original data in terms of the compressed representation (i.e., constructional analyses using the grammar). The MDL heuristic exploits a tradeoff between competing preferences for smaller grammars (encouraging generalization) and for simpler analyses of the data (encouraging the retention of specific/frequent constructions). The rest of this section makes the learning framework concrete. Section 3.1 describes several heuristics fo</context>
</contexts>
<marker>Rissanen, 1978</marker>
<rawString>Jorma Rissanen. 1978. Modeling by shortest data description. Automatica, 14:465–471.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deb Roy</author>
<author>Alex Pentland</author>
</authors>
<title>Learning audiovisually grounded words from natural input.</title>
<date>1998</date>
<booktitle>In Proc. AAAI workshop, Grounding Word Meaning.</booktitle>
<contexts>
<context position="11614" citStr="Roy and Pentland, 1998" startWordPosition="1774" endWordPosition="1777">ecting children’s knowledge of what kinds of entities can take place in different events. 2.2.2 Lexical constructions The input to learning includes a set of lexical constructions, represented using the ECG formalism, linking simple forms (i.e. words) to specific conceptual items. Examples of these include the and constructions in the figure, as well as the construction formally defined in Figure 1. Lexical learning is not the focus of the current work, but a number of previous computational approaches have shown how simple mappings may be acquired from experience (Regier, 1996; Bailey, 1997; Roy and Pentland, 1998). 2.2.3 Construction analyzer As mentioned earlier, the ECG construction formalism is designed to support processes of language use. In particular, the model makes use of a construction analyzer that identifies the constructions responsible for a given utterance, much like a syntactic parser in a traditional language understanding system identifies which parse rules are responsible. In this case, however, the basic representational unit is a form-meaning pair. The analyzer must therefore also supply a semantic interpretation, called the semspec, indicating which conceptual schemas are involved</context>
</contexts>
<marker>Roy, Pentland, 1998</marker>
<rawString>Deb Roy and Alex Pentland. 1998. Learning audiovisually grounded words from natural input. In Proc. AAAI workshop, Grounding Word Meaning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Sachs</author>
</authors>
<title>Talking about the there and then: the emergence of displaced reference in parent-child discourse.</title>
<date>1983</date>
<booktitle>Children’s language,</booktitle>
<volume>4</volume>
<pages>1--28</pages>
<editor>In K.E. Nelson, editor,</editor>
<publisher>Lawrence Erlbaum Associates.</publisher>
<contexts>
<context position="15646" citStr="Sachs, 1983" startWordPosition="2404" endWordPosition="2406">c communicative context (a set of linked conceptual schemas corresponding to the participants, salient scene and discourse information available in the situation). The learning model receives only positive examples, as in the child learning case. Note, however, that the interpretation a given utterance has in context depends on the current state of linguistic knowledge. Thus the same utterance at different stages may lead to different learning behavior. The specific training corpus used in learning experiments is a subset of the Sachs corpus of the CHILDES database of parent-child transcripts(Sachs, 1983; MacWhinney, 1991), with additional annotations made by developmental psychologists as part of a study of motion utterances (Dan I. Slobin, p.c.). These annotations indicate semantic and pragmatic features available in the scene. A simple feature structure representation of a sample input token is shown here; boxed numbers indicate that the relevant entities are identified: Many details have been omitted, and a number of simplifying assumptions have been made. But the rough outline given here nevertheless captures the core computational problem faced by the child learner in acquiring multi-wo</context>
</contexts>
<marker>Sachs, 1983</marker>
<rawString>J. Sachs. 1983. Talking about the there and then: the emergence of displaced reference in parent-child discourse. In K.E. Nelson, editor, Children’s language, volume 4, pages 1–28. Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Mark Siskind</author>
</authors>
<title>A computational study of cross-situational techniques for learning word-tomeaning mappings, chapter 2.</title>
<date>1997</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="31248" citStr="Siskind (1997)" startWordPosition="4884" endWordPosition="4885"> computational forerunners from both logical and probabilistic traditions, including Bayesian models of word learning (Bailey, 1997; Stolcke, 1994) for the overall optimization model, and work by Wolff (1982) modeling language acquisition (primarily production rules) using data compression techniques similar to the MDL approach taken here. The use of the results of analysis to hypothesize new mappings can be seen as related to both explanation-based learning (DeJong and Mooney, 1986) and inductive logic programming (Muggleton and Raedt, 1994). The model also has some precedents in the work of Siskind (1997) and Thompson (1998), both of which based learning on the discovery of isomorphic structures in syntactic and semantic representations, though in less linguistically rich formalisms. In current work we are applying the model to the full corpus of English verbs, as well as crosslinguistic data including Russian case markers and Mandarin directional particles and aspect markers. These experiments will further test the robustness of the model’s theoretical assumptions and protect against model overfitting and typological bias. We are also developing alternative means of evaluating the system’s pr</context>
</contexts>
<marker>Siskind, 1997</marker>
<rawString>Jeffrey Mark Siskind, 1997. A computational study of cross-situational techniques for learning word-tomeaning mappings, chapter 2. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Bayesian Learning of Probabilistic Language Models.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>Computer Science Division, University of California at Berkeley.</institution>
<contexts>
<context position="30781" citStr="Stolcke, 1994" startWordPosition="4810" endWordPosition="4811">assumpPercent correct bindings 100 80 60 40 20 0 drop (n=10, b=18) throw (n=25, b=45) fall (n=53, b=86) 23 tions made with respect to structures and processes available to a human language learner in this stage are consistent with evidence from across the cognitive spectrum. Though only preliminary conclusions can be made, the model is a concrete computational step toward validating a meaning-oriented approach to grammar learning. The model draws from a number of computational forerunners from both logical and probabilistic traditions, including Bayesian models of word learning (Bailey, 1997; Stolcke, 1994) for the overall optimization model, and work by Wolff (1982) modeling language acquisition (primarily production rules) using data compression techniques similar to the MDL approach taken here. The use of the results of analysis to hypothesize new mappings can be seen as related to both explanation-based learning (DeJong and Mooney, 1986) and inductive logic programming (Muggleton and Raedt, 1994). The model also has some precedents in the work of Siskind (1997) and Thompson (1998), both of which based learning on the discovery of isomorphic structures in syntactic and semantic representation</context>
</contexts>
<marker>Stolcke, 1994</marker>
<rawString>Andreas Stolcke. 1994. Bayesian Learning of Probabilistic Language Models. Ph.D. thesis, Computer Science Division, University of California at Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia A Thompson</author>
</authors>
<title>Semantic Lexicon Acquisition for Learning Natural Language Interfaces.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Sciences, University of Texas at Austin,</institution>
<contexts>
<context position="31268" citStr="Thompson (1998)" startWordPosition="4887" endWordPosition="4888">runners from both logical and probabilistic traditions, including Bayesian models of word learning (Bailey, 1997; Stolcke, 1994) for the overall optimization model, and work by Wolff (1982) modeling language acquisition (primarily production rules) using data compression techniques similar to the MDL approach taken here. The use of the results of analysis to hypothesize new mappings can be seen as related to both explanation-based learning (DeJong and Mooney, 1986) and inductive logic programming (Muggleton and Raedt, 1994). The model also has some precedents in the work of Siskind (1997) and Thompson (1998), both of which based learning on the discovery of isomorphic structures in syntactic and semantic representations, though in less linguistically rich formalisms. In current work we are applying the model to the full corpus of English verbs, as well as crosslinguistic data including Russian case markers and Mandarin directional particles and aspect markers. These experiments will further test the robustness of the model’s theoretical assumptions and protect against model overfitting and typological bias. We are also developing alternative means of evaluating the system’s progress based on a ru</context>
</contexts>
<marker>Thompson, 1998</marker>
<rawString>Cynthia A. Thompson. 1998. Semantic Lexicon Acquisition for Learning Natural Language Interfaces. Ph.D. thesis, Department of Computer Sciences, University of Texas at Austin, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Tomasello</author>
</authors>
<title>Joint attention as social cognition.</title>
<date>1995</date>
<booktitle>Joint attention: Its origins and role in development. Lawrence Erlbaum Associates, Hillsdale, NJ. Educ/Psych BF720.A85.J65</booktitle>
<editor>In C.D. Moore P., editor,</editor>
<contexts>
<context position="2526" citStr="Tomasello, 1995" startWordPosition="366" endWordPosition="367">nicate, via comprehension and production. Cognitive and constructional approaches to grammar assume that the basic unit of linguistic knowledge needed to support language use consists of pairings of form and meaning, or constructions (Langacker, 1987; Goldberg, 1995; Fillmore and Kay, 1999). Moreover, by the time children make the leap from single words to complex combinations, they have amassed considerable conceptual knowledge, including familiarity with a wide variety of entities and events and sophisticated pragmatic skills (such as using joint attention to infer communicative intentions (Tomasello, 1995) and subtle lexical distinctions (Bloom, 2000)). The developmental evidence thus suggests that the input to grammar learning may in principle include not just surface strings but also meaningful situation descriptions with rich semantic and pragmatic information. This paper formalizes the grammar learning problem in line with the observations above, taking seriously the ideas that the target of learning, for both lexical items and larger phrasal and clausal units, is a bipolar structure in which meaning is on par with form, and that meaningful language use drives language learning. The resulti</context>
</contexts>
<marker>Tomasello, 1995</marker>
<rawString>Michael Tomasello. 1995. Joint attention as social cognition. In C.D. Moore P., editor, Joint attention: Its origins and role in development. Lawrence Erlbaum Associates, Hillsdale, NJ. Educ/Psych BF720.A85.J65 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Tomasello</author>
</authors>
<title>Constructing a Language: A Usage-Based Theory of Language Acquisition.</title>
<date>2003</date>
<publisher>Harvard University Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="26996" citStr="Tomasello, 2003" startWordPosition="4211" endWordPosition="4212">, the structural prior corresponds to the grammar size, and likelihood corresponds to the complexity of the data relative to the grammar. 4 Learning verb islands The model was applied to the data set described in Section 2.3 to determine whether lexically specific multi-word constructions could be learned using the MDL learning framework described. This task represents an important first step toward general grammatical constructions, and is of cognitive interest, since item-based patterns appear to be learned on independent trajectories (i.e., each verb forms its own “island” of organization (Tomasello, 2003)). We give results for drop ( =10 examples), throw ( =25), and fall ( =50). 0 10 20 30 40 50 60 70 80 90 100 Percent training examples encountered Figure 8: Incrementally improving comprehension for three verb islands. Given the small corpus sizes, the focus for this experiment is not on the details of the statistical learning framework but instead on a qualitative evaluation of whether learned constructions improve the model’s comprehension over time, and how verbs may differ in their learning trajectories. Qualitatively, the model first learned item-specific constructions as expected (e.g. t</context>
</contexts>
<marker>Tomasello, 2003</marker>
<rawString>Michael Tomasello. 2003. Constructing a Language: A Usage-Based Theory of Language Acquisition. Harvard University Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gerard Wolff</author>
</authors>
<title>Language acquisition, data compression and generalization.</title>
<date>1982</date>
<journal>Language &amp; Communication,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="30842" citStr="Wolff (1982)" startWordPosition="4821" endWordPosition="4822">18) throw (n=25, b=45) fall (n=53, b=86) 23 tions made with respect to structures and processes available to a human language learner in this stage are consistent with evidence from across the cognitive spectrum. Though only preliminary conclusions can be made, the model is a concrete computational step toward validating a meaning-oriented approach to grammar learning. The model draws from a number of computational forerunners from both logical and probabilistic traditions, including Bayesian models of word learning (Bailey, 1997; Stolcke, 1994) for the overall optimization model, and work by Wolff (1982) modeling language acquisition (primarily production rules) using data compression techniques similar to the MDL approach taken here. The use of the results of analysis to hypothesize new mappings can be seen as related to both explanation-based learning (DeJong and Mooney, 1986) and inductive logic programming (Muggleton and Raedt, 1994). The model also has some precedents in the work of Siskind (1997) and Thompson (1998), both of which based learning on the discovery of isomorphic structures in syntactic and semantic representations, though in less linguistically rich formalisms. In current </context>
</contexts>
<marker>Wolff, 1982</marker>
<rawString>J. Gerard Wolff. 1982. Language acquisition, data compression and generalization. Language &amp; Communication, 2(1):57–89.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>