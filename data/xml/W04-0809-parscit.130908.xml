<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.138032">
<affiliation confidence="0.559629666666667">
SENSEVAL-3: Third International Workshop on the Evaluation of Systems
for the Semantic Analysis of Text, Barcelona, Spain, July 2004
Association for Computational Linguistics
</affiliation>
<bodyText confidence="0.996422157894737">
Point (a) is done by merging manually con-
structed SCF distributions2 of 4-5 represen-
tative verbs using linear interpolation (e.g.,
(Manning and Schiitze, 1999)). For example,
the back-off estimates for the class of &amp;quot;Motion
verbs&amp;quot; are constructed by merging the SCF dis-
tributions for 4-5 &amp;quot;Motion verbs&amp;quot; e.g., move,
slide, arrive, travel, and sail.
For (b), we combine the different back-off es-
timates using linear interpolation (Chen and
Goodman, 1996) so that the contribution of
each set of estimates is weighted according to
the frequency of the corresponding senses in cor-
pus data. Let p3(scf,), j =1...nbo (where nbo
is the number of back-off estimates) be the prob-
abilities of SCFs in different back-off distribu-
tions. The estimated probability of the SCF in
the resulting combined back-off distribution is
calculated as follows:
</bodyText>
<equation confidence="0.933367">
nbo
P(scfi) = E Aj • pj(scfi)
j=1
</equation>
<bodyText confidence="0.999960222222222">
where the A3 denote weights for the different
distributions and sum to 1. The values for A.
are determined specific to a verb and are ob-
tained by converting the output of a WSD sys-
tem into probability distributions on senses for
each word.
As a final step, a simple empirically deter-
mined threshold is used on the probability esti-
mates after smoothing to filter out noisy SCFs.
</bodyText>
<sectionHeader confidence="0.999221" genericHeader="abstract">
3 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.999613">
3.1 Evaluation Corpus
</subsectionHeader>
<bodyText confidence="0.999815857142857">
Preiss et al. (2002) showed that high frequency
polysemous verbs whose predominant sense is
not very frequent are likely to benefit most from
WSD. We chose 29 of these verbs for inves-
tigation.3 The verbs were chosen at random,
subject to the constraint that they occur in
the SEMCOR data in at least two broad Levin-
style senses. To ensure that we cover all (or
most) senses of these verbs, the WordNet senses
of these verbs were mapped to Levin senses.
Senses very low in frequency and those which
could not be mapped to any extant Levin-style
senses were left out of consideration. The max-
imum number of Levin senses considered per
</bodyText>
<footnote confidence="0.9839552">
2The distributions are obtained analysing c. 300 oc-
currences of each verb in the British National Corpus
(BNC) (Leech, 1992).
3Note that these verbs are exceptionally difficult for
both WSD and SCF acquisition.
</footnote>
<bodyText confidence="0.999962823529412">
verb was 4. These typically map to several
WordNet senses, as Levin assumes more coarse-
grained sense distinctions than WordNet. The
29 verbs are presented in Table 1, together with
the number of Levin senses distinguished for
each verb.
The test corpus for this task consisted of
around 1000 sentences for each verb drawn from
the BNC. For each verb, WSD systems were
asked to annotate every occurrence of the verb
in its associated corpus, and each annotation
is converted into a probability distribution on
senses.4 The 1000 probability distributions are
averaged, to produce an overall probability dis-
tribution for the verb, which is used to guide
the construction of back-off estimates in SCF
acquisition.5
</bodyText>
<subsectionHeader confidence="0.998483">
3.2 Evaluation Method
</subsectionHeader>
<bodyText confidence="0.999955941176471">
The results obtained using the new back-off esti-
mates were evaluated against a manual analysis
of the corpus data which was was obtained by
analysing about 300 occurrences for each test
verb in our BNC test data. 5-21 gold standard
SCFs were found for each verb (16 SCFs per
verb on average).
We calculated type precision (the percentage
of SCF types that the system proposes which
are correct), type recall (the percentage of SCF
types in the gold standard that the system pro-
poses) and F-measure. We also calculated the
similarity between the acquired unfiltered and
gold standard SCF distributions using various
measures of distributional similarity (see Korho-
nen and Preiss (2003) for details of these mea-
sures).
</bodyText>
<sectionHeader confidence="0.9674115" genericHeader="keywords">
4 WSD Performance vs Acquired
Frames
</sectionHeader>
<bodyText confidence="0.99987425">
No teams participated in this task in SENSEVAL-
3, possibly being scared off by having to sense
tag around 1000 instances of each of the 29 verbs
used. Teams may also have found it easier to an-
notate the relevant number of instances just for
a subset of the verbs. We therefore used a rep-
resentative WSD system due to Preiss (2004)
to investigate the effect of WSD system perfor-
</bodyText>
<footnote confidence="0.980791">
4For a forced choice system, the chosen sense is given
a probability of one and the remaining senses are as-
signed zero probabilities.
3Note that this does not mean that 1000 sentences for
each verb have been manually sense tagged. No manual
sense tagging was done in this task, as the performance
of the WSD system is judged by the performance of sub-
categorization acquisition.
</footnote>
<table confidence="0.9318675">
Verb Num senses Verb Num senses
absorb 3 induce 2
bear 4 keep 3
choose 2 mark 3
compose 2 offer 2
conceive 2 proclaim 2
concentrate 2 provide 2
continue 2 roar 3
count 3 seek 4
descend 2 settle 3
distinguish 3 strike 3
embrace 2 submit 3
establish 3 wait 3
find 3 watch 2
force 2 write 3
grasp 2
</table>
<tableCaption confidence="0.995679">
Table 1: Test verbs and their senses
</tableCaption>
<table confidence="0.99960525">
Method Precision Recall F-measure
No smoothing 72.9% 31.3% 43.8%
Smoothing with most frequent sense 72.3% 38.9% 50.6%
Smoothing determined by WSD 75.2% 40.7% 52.8%
</table>
<tableCaption confidence="0.998428">
Table 2: Subcategorization acquisition performance
</tableCaption>
<bodyText confidence="0.998944970588235">
mance on the accuracy of SCF acquisition. The
summary of basic results is presented in Table 2.
The table gives the values for the baseline sys-
tems (no smoothing, and smoothing with the
most frequent sense), and for the SCF system
combined with the WSD system. The WSD
smoothed SCF yields a 2.2% better F-measure
than smoothing with the most frequent sense,
which in turn yields a 6.8% higher F-measure
than not smoothing at all. The effect of WSD
was clear also on the measures of distributional
similarity. These figures show that the WSD
system improves SCF acquisition.
To demonstrate the effectiveness of this task
in ranking systems, we investigated the corre-
lation between the F-measure of WSD systems
(on a gold standard task) and the F-measure on
the SCF task. Preiss&apos; probabilistic WSD system
is modular, with modules based on frequency of
sense from WordNet, part of speech of the tar-
get word, surrounding lemmas, the surrounding
words&apos; parts of speech, proximity to the near-
est phrasal head, and trigram information. A
number of WSD systems were obtained by re-
stricting the number of modules in Preiss&apos; prob-
abilistic modular WSD system, which resulted
in systems with varying performance. The ac-
curacy of the WSD system was found on the En-
glish all words task of SENSEVAL-2 (Palmer et
al., 2002). A correlation of p = 0.97 was found
between the two sets of results, showing a very
high correlation between WSD system perfor-
mance and the performance of SCF acquisition
when the WSD system is employed.
</bodyText>
<sectionHeader confidence="0.986208" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999988125">
We have described the subcategorization frame
acquisition as a method for evaluating WSD
task in SENSEVAL-3. We demonstrate a high
correlation between WSD system performance
and the performance of SCF acquisition, in-
dicating that any ranking obtained using this
method will complement gold standard meth-
ods of system evaluation.
</bodyText>
<sectionHeader confidence="0.996043" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998156">
We would like to thank Ted Briscoe for his help.
</bodyText>
<sectionHeader confidence="0.997443" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998235">
E. J. Briscoe and J. Carroll. 1997. Automatic
extraction of subcategorization from corpora.
In Proceedings of A CL ANL P97, pages 356-
363.
E. J. Briscoe and J. Carroll. 2002. Robust accu-
rate statistical annotation of general text. In
Proceedings of the 3rd International Confer-
ence on Language Resources and Evaluation,
pages 1499-1504.
S. F. Chen and J. Goodman. 1996. An em-
pirical study of smoothing techniques for lan-
guage modeling. In Proceedings of the Thirty-
Fourth Annual Meeting of the Association for
Computational Linguistics, pages 310-318.
A. Korhonen and J. Preiss. 2003. Improv-
ing subcategorization acquisition using word
sense disambiguation. In Proceedings of ACL,
pages 48-55.
A. Korhonen. 2002. Subcategorization Acquisi-
tion. Ph.D. thesis, University of Cambridge.
G. Leech. 1992. 100 million words of English:
the British National Corpus. Language Re-
search, 28(41-13.
B. Levin. 1993. English Verb Classes and Al-
ternations. Chicago University Press.
C. D. Manning and H. Schiitze. 1999. Founda-
tions of Statistical Natural Language Process-
ing. MIT Press.
M. Palmer, C. Fellbaum, S. Cotton, L. Delfs,
and H. T. Dang. 2002. English tasks: All-
words and verb lexical sample. In Preiss and
Yarowsky (Preiss and Yarowsky, 2002), pages
21-24.
J. Preiss and D. Yarowsky, editors. 2002. Pro-
ceedings of SENSEVAL-2: Second Interna-
tional Workshop on Evaluating Word Sense
Disambiguating Systems.
J. Preiss, A. Korhonen, and E. J. Briscoe. 2002.
Subcategorization acquisition as an evalua-
tion method for WSD. In Proceedings of
LREC, pages 1551-1556.
J. Preiss. 2004. Probabilistic word sense disam-
biguation. Computer Speech and Language.
Forthcoming.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.010814">
<note confidence="0.839685333333333">SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, Barcelona, Spain, July 2004 Association for Computational Linguistics</note>
<abstract confidence="0.992704760233918">Point (a) is done by merging manually con- SCF of 4-5 representative verbs using linear interpolation (e.g., (Manning and Schiitze, 1999)). For example, the back-off estimates for the class of &amp;quot;Motion verbs&amp;quot; are constructed by merging the SCF disfor 4-5 &amp;quot;Motion verbs&amp;quot; e.g., arrive, travel, For (b), we combine the different back-off estimates using linear interpolation (Chen and Goodman, 1996) so that the contribution of each set of estimates is weighted according to the frequency of the corresponding senses in cordata. Let j (where nbo is the number of back-off estimates) be the probabilities of SCFs in different back-off distributions. The estimated probability of the SCF in the resulting combined back-off distribution is calculated as follows: nbo = • j=1 the denote weights for the different and sum to 1. The values for are determined specific to a verb and are obtained by converting the output of a WSD system into probability distributions on senses for each word. As a final step, a simple empirically determined threshold is used on the probability estimates after smoothing to filter out noisy SCFs. 3 Evaluation 3.1 Evaluation Corpus Preiss et al. (2002) showed that high frequency polysemous verbs whose predominant sense is not very frequent are likely to benefit most from WSD. We chose 29 of these verbs for inves- The verbs were chosen at random, subject to the constraint that they occur in the SEMCOR data in at least two broad Levinstyle senses. To ensure that we cover all (or most) senses of these verbs, the WordNet senses of these verbs were mapped to Levin senses. Senses very low in frequency and those which could not be mapped to any extant Levin-style senses were left out of consideration. The maximum number of Levin senses considered per distributions are obtained analysing c. 300 occurrences of each verb in the British National Corpus (BNC) (Leech, 1992). that these verbs are exceptionally difficult for both WSD and SCF acquisition. verb was 4. These typically map to several WordNet senses, as Levin assumes more coarsegrained sense distinctions than WordNet. The 29 verbs are presented in Table 1, together with the number of Levin senses distinguished for each verb. The test corpus for this task consisted of around 1000 sentences for each verb drawn from the BNC. For each verb, WSD systems were asked to annotate every occurrence of the verb in its associated corpus, and each annotation is converted into a probability distribution on The 1000 probability distributions are averaged, to produce an overall probability distribution for the verb, which is used to guide the construction of back-off estimates in SCF 3.2 Evaluation Method The results obtained using the new back-off estimates were evaluated against a manual analysis of the corpus data which was was obtained by analysing about 300 occurrences for each test verb in our BNC test data. 5-21 gold standard SCFs were found for each verb (16 SCFs per verb on average). We calculated type precision (the percentage of SCF types that the system proposes which are correct), type recall (the percentage of SCF types in the gold standard that the system proposes) and F-measure. We also calculated the similarity between the acquired unfiltered and gold standard SCF distributions using various measures of distributional similarity (see Korhonen and Preiss (2003) for details of these measures). 4 WSD Performance vs Acquired Frames No teams participated in this task in SENSEVAL- 3, possibly being scared off by having to sense tag around 1000 instances of each of the 29 verbs used. Teams may also have found it easier to annotate the relevant number of instances just for a subset of the verbs. We therefore used a representative WSD system due to Preiss (2004) investigate the effect of WSD system perfora forced choice system, the chosen sense is given a probability of one and the remaining senses are assigned zero probabilities. that this does not mean that 1000 sentences for each verb have been manually sense tagged. No manual sense tagging was done in this task, as the performance of the WSD system is judged by the performance of subcategorization acquisition. Verb Num senses Verb Num senses absorb 3 induce 2 bear 4 keep 3 choose 2 mark 3 compose 2 offer 2 conceive 2 proclaim 2 concentrate 2 provide 2 continue 2 roar 3 count 3 seek 4 descend 2 settle 3 distinguish 3 strike 3 embrace 2 submit 3 establish 3 wait 3 find 3 watch 2 force 2 write 3 grasp 2 Table 1: Test verbs and their senses Method Precision Recall No smoothing 72.9% 31.3% 43.8% Smoothing with most frequent sense 72.3% 38.9% 50.6% determined WSD 75.2% 40.7% 52.8% Table 2: Subcategorization acquisition performance mance on the accuracy of SCF acquisition. The summary of basic results is presented in Table 2. The table gives the values for the baseline systems (no smoothing, and smoothing with the most frequent sense), and for the SCF system combined with the WSD system. The WSD smoothed SCF yields a 2.2% better F-measure than smoothing with the most frequent sense, which in turn yields a 6.8% higher F-measure than not smoothing at all. The effect of WSD was clear also on the measures of distributional similarity. These figures show that the WSD system improves SCF acquisition. To demonstrate the effectiveness of this task in ranking systems, we investigated the correlation between the F-measure of WSD systems (on a gold standard task) and the F-measure on the SCF task. Preiss&apos; probabilistic WSD system is modular, with modules based on frequency of sense from WordNet, part of speech of the target word, surrounding lemmas, the surrounding words&apos; parts of speech, proximity to the nearest phrasal head, and trigram information. A number of WSD systems were obtained by restricting the number of modules in Preiss&apos; probabilistic modular WSD system, which resulted in systems with varying performance. The accuracy of the WSD system was found on the Enall words task of et 2002). A correlation of = was found between the two sets of results, showing a very high correlation between WSD system performance and the performance of SCF acquisition when the WSD system is employed. 5 Conclusion We have described the subcategorization frame acquisition as a method for evaluating WSD in demonstrate a high correlation between WSD system performance and the performance of SCF acquisition, indicating that any ranking obtained using this method will complement gold standard methods of system evaluation. Acknowledgments We would like to thank Ted Briscoe for his help.</abstract>
<note confidence="0.822704681818182">References E. J. Briscoe and J. Carroll. 1997. Automatic extraction of subcategorization from corpora. of A CL ANL P97, 356- 363. E. J. Briscoe and J. Carroll. 2002. Robust accurate statistical annotation of general text. In Proceedings of the 3rd International Conference on Language Resources and Evaluation, pages 1499-1504. S. F. Chen and J. Goodman. 1996. An empirical study of smoothing techniques for lanmodeling. In of the Thirty- Fourth Annual Meeting of the Association for Linguistics, 310-318. A. Korhonen and J. Preiss. 2003. Improving subcategorization acquisition using word disambiguation. In of ACL, pages 48-55. Korhonen. 2002. Acquisithesis, University of Cambridge. G. Leech. 1992. 100 million words of English: British National Corpus. Re- Levin. 1993. Verb Classes and Al- University Press. D. Manning and H. Schiitze. 1999. Foundations of Statistical Natural Language Process- Press. M. Palmer, C. Fellbaum, S. Cotton, L. Delfs, and H. T. Dang. 2002. English tasks: Allwords and verb lexical sample. In Preiss and Yarowsky (Preiss and Yarowsky, 2002), pages 21-24. Preiss and D. Yarowsky, editors. 2002. Proceedings of SENSEVAL-2: Second International Workshop on Evaluating Word Sense Disambiguating Systems. J. Preiss, A. Korhonen, and E. J. Briscoe. 2002. Subcategorization acquisition as an evaluamethod for WSD. In of 1551-1556. J. Preiss. 2004. Probabilistic word sense disam- Speech and Language. Forthcoming.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E J Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Automatic extraction of subcategorization from corpora.</title>
<date>1997</date>
<booktitle>In Proceedings of A CL ANL P97,</booktitle>
<pages>356--363</pages>
<marker>Briscoe, Carroll, 1997</marker>
<rawString>E. J. Briscoe and J. Carroll. 1997. Automatic extraction of subcategorization from corpora. In Proceedings of A CL ANL P97, pages 356-363.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E J Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Robust accurate statistical annotation of general text.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Language Resources and Evaluation,</booktitle>
<pages>1499--1504</pages>
<marker>Briscoe, Carroll, 2002</marker>
<rawString>E. J. Briscoe and J. Carroll. 2002. Robust accurate statistical annotation of general text. In Proceedings of the 3rd International Conference on Language Resources and Evaluation, pages 1499-1504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
<author>J Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>In Proceedings of the ThirtyFourth Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>310--318</pages>
<contexts>
<context position="626" citStr="Chen and Goodman, 1996" startWordPosition="89" endWordPosition="92">SEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, Barcelona, Spain, July 2004 Association for Computational Linguistics Point (a) is done by merging manually constructed SCF distributions2 of 4-5 representative verbs using linear interpolation (e.g., (Manning and Schiitze, 1999)). For example, the back-off estimates for the class of &amp;quot;Motion verbs&amp;quot; are constructed by merging the SCF distributions for 4-5 &amp;quot;Motion verbs&amp;quot; e.g., move, slide, arrive, travel, and sail. For (b), we combine the different back-off estimates using linear interpolation (Chen and Goodman, 1996) so that the contribution of each set of estimates is weighted according to the frequency of the corresponding senses in corpus data. Let p3(scf,), j =1...nbo (where nbo is the number of back-off estimates) be the probabilities of SCFs in different back-off distributions. The estimated probability of the SCF in the resulting combined back-off distribution is calculated as follows: nbo P(scfi) = E Aj • pj(scfi) j=1 where the A3 denote weights for the different distributions and sum to 1. The values for A. are determined specific to a verb and are obtained by converting the output of a WSD syste</context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>S. F. Chen and J. Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proceedings of the ThirtyFourth Annual Meeting of the Association for Computational Linguistics, pages 310-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Korhonen</author>
<author>J Preiss</author>
</authors>
<title>Improving subcategorization acquisition using word sense disambiguation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>48--55</pages>
<contexts>
<context position="3718" citStr="Korhonen and Preiss (2003)" startWordPosition="606" endWordPosition="610"> evaluated against a manual analysis of the corpus data which was was obtained by analysing about 300 occurrences for each test verb in our BNC test data. 5-21 gold standard SCFs were found for each verb (16 SCFs per verb on average). We calculated type precision (the percentage of SCF types that the system proposes which are correct), type recall (the percentage of SCF types in the gold standard that the system proposes) and F-measure. We also calculated the similarity between the acquired unfiltered and gold standard SCF distributions using various measures of distributional similarity (see Korhonen and Preiss (2003) for details of these measures). 4 WSD Performance vs Acquired Frames No teams participated in this task in SENSEVAL3, possibly being scared off by having to sense tag around 1000 instances of each of the 29 verbs used. Teams may also have found it easier to annotate the relevant number of instances just for a subset of the verbs. We therefore used a representative WSD system due to Preiss (2004) to investigate the effect of WSD system perfor4For a forced choice system, the chosen sense is given a probability of one and the remaining senses are assigned zero probabilities. 3Note that this does</context>
</contexts>
<marker>Korhonen, Preiss, 2003</marker>
<rawString>A. Korhonen and J. Preiss. 2003. Improving subcategorization acquisition using word sense disambiguation. In Proceedings of ACL, pages 48-55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Korhonen</author>
</authors>
<title>Subcategorization Acquisition.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Cambridge.</institution>
<marker>Korhonen, 2002</marker>
<rawString>A. Korhonen. 2002. Subcategorization Acquisition. Ph.D. thesis, University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Leech</author>
</authors>
<title>100 million words of English:</title>
<date>1992</date>
<journal>the British National Corpus. Language Research,</journal>
<pages>28--41</pages>
<contexts>
<context position="2214" citStr="Leech, 1992" startWordPosition="366" endWordPosition="367">. We chose 29 of these verbs for investigation.3 The verbs were chosen at random, subject to the constraint that they occur in the SEMCOR data in at least two broad Levinstyle senses. To ensure that we cover all (or most) senses of these verbs, the WordNet senses of these verbs were mapped to Levin senses. Senses very low in frequency and those which could not be mapped to any extant Levin-style senses were left out of consideration. The maximum number of Levin senses considered per 2The distributions are obtained analysing c. 300 occurrences of each verb in the British National Corpus (BNC) (Leech, 1992). 3Note that these verbs are exceptionally difficult for both WSD and SCF acquisition. verb was 4. These typically map to several WordNet senses, as Levin assumes more coarsegrained sense distinctions than WordNet. The 29 verbs are presented in Table 1, together with the number of Levin senses distinguished for each verb. The test corpus for this task consisted of around 1000 sentences for each verb drawn from the BNC. For each verb, WSD systems were asked to annotate every occurrence of the verb in its associated corpus, and each annotation is converted into a probability distribution on sens</context>
</contexts>
<marker>Leech, 1992</marker>
<rawString>G. Leech. 1992. 100 million words of English: the British National Corpus. Language Research, 28(41-13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Levin</author>
</authors>
<title>English Verb Classes and Alternations.</title>
<date>1993</date>
<publisher>Chicago University Press.</publisher>
<marker>Levin, 1993</marker>
<rawString>B. Levin. 1993. English Verb Classes and Alternations. Chicago University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Schiitze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>1999</date>
<publisher>MIT Press.</publisher>
<marker>Manning, Schiitze, 1999</marker>
<rawString>C. D. Manning and H. Schiitze. 1999. Foundations of Statistical Natural Language Processing. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>C Fellbaum</author>
<author>S Cotton</author>
<author>L Delfs</author>
<author>H T Dang</author>
</authors>
<title>English tasks: Allwords and verb lexical sample.</title>
<date>2002</date>
<booktitle>In Preiss and Yarowsky (Preiss and Yarowsky,</booktitle>
<pages>21--24</pages>
<contexts>
<context position="6397" citStr="Palmer et al., 2002" startWordPosition="1076" endWordPosition="1079">easure of WSD systems (on a gold standard task) and the F-measure on the SCF task. Preiss&apos; probabilistic WSD system is modular, with modules based on frequency of sense from WordNet, part of speech of the target word, surrounding lemmas, the surrounding words&apos; parts of speech, proximity to the nearest phrasal head, and trigram information. A number of WSD systems were obtained by restricting the number of modules in Preiss&apos; probabilistic modular WSD system, which resulted in systems with varying performance. The accuracy of the WSD system was found on the English all words task of SENSEVAL-2 (Palmer et al., 2002). A correlation of p = 0.97 was found between the two sets of results, showing a very high correlation between WSD system performance and the performance of SCF acquisition when the WSD system is employed. 5 Conclusion We have described the subcategorization frame acquisition as a method for evaluating WSD task in SENSEVAL-3. We demonstrate a high correlation between WSD system performance and the performance of SCF acquisition, indicating that any ranking obtained using this method will complement gold standard methods of system evaluation. Acknowledgments We would like to thank Ted Briscoe f</context>
</contexts>
<marker>Palmer, Fellbaum, Cotton, Delfs, Dang, 2002</marker>
<rawString>M. Palmer, C. Fellbaum, S. Cotton, L. Delfs, and H. T. Dang. 2002. English tasks: Allwords and verb lexical sample. In Preiss and Yarowsky (Preiss and Yarowsky, 2002), pages 21-24.</rawString>
</citation>
<citation valid="true">
<date>2002</date>
<booktitle>Proceedings of SENSEVAL-2: Second International Workshop on Evaluating Word Sense Disambiguating Systems.</booktitle>
<editor>J. Preiss and D. Yarowsky, editors.</editor>
<contexts>
<context position="1477" citStr="(2002)" startWordPosition="239" endWordPosition="239">ent back-off distributions. The estimated probability of the SCF in the resulting combined back-off distribution is calculated as follows: nbo P(scfi) = E Aj • pj(scfi) j=1 where the A3 denote weights for the different distributions and sum to 1. The values for A. are determined specific to a verb and are obtained by converting the output of a WSD system into probability distributions on senses for each word. As a final step, a simple empirically determined threshold is used on the probability estimates after smoothing to filter out noisy SCFs. 3 Evaluation 3.1 Evaluation Corpus Preiss et al. (2002) showed that high frequency polysemous verbs whose predominant sense is not very frequent are likely to benefit most from WSD. We chose 29 of these verbs for investigation.3 The verbs were chosen at random, subject to the constraint that they occur in the SEMCOR data in at least two broad Levinstyle senses. To ensure that we cover all (or most) senses of these verbs, the WordNet senses of these verbs were mapped to Levin senses. Senses very low in frequency and those which could not be mapped to any extant Levin-style senses were left out of consideration. The maximum number of Levin senses co</context>
</contexts>
<marker>2002</marker>
<rawString>J. Preiss and D. Yarowsky, editors. 2002. Proceedings of SENSEVAL-2: Second International Workshop on Evaluating Word Sense Disambiguating Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Preiss</author>
<author>A Korhonen</author>
<author>E J Briscoe</author>
</authors>
<title>Subcategorization acquisition as an evaluation method for WSD.</title>
<date>2002</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>1551--1556</pages>
<contexts>
<context position="1477" citStr="Preiss et al. (2002)" startWordPosition="236" endWordPosition="239">SCFs in different back-off distributions. The estimated probability of the SCF in the resulting combined back-off distribution is calculated as follows: nbo P(scfi) = E Aj • pj(scfi) j=1 where the A3 denote weights for the different distributions and sum to 1. The values for A. are determined specific to a verb and are obtained by converting the output of a WSD system into probability distributions on senses for each word. As a final step, a simple empirically determined threshold is used on the probability estimates after smoothing to filter out noisy SCFs. 3 Evaluation 3.1 Evaluation Corpus Preiss et al. (2002) showed that high frequency polysemous verbs whose predominant sense is not very frequent are likely to benefit most from WSD. We chose 29 of these verbs for investigation.3 The verbs were chosen at random, subject to the constraint that they occur in the SEMCOR data in at least two broad Levinstyle senses. To ensure that we cover all (or most) senses of these verbs, the WordNet senses of these verbs were mapped to Levin senses. Senses very low in frequency and those which could not be mapped to any extant Levin-style senses were left out of consideration. The maximum number of Levin senses co</context>
</contexts>
<marker>Preiss, Korhonen, Briscoe, 2002</marker>
<rawString>J. Preiss, A. Korhonen, and E. J. Briscoe. 2002. Subcategorization acquisition as an evaluation method for WSD. In Proceedings of LREC, pages 1551-1556.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Preiss</author>
</authors>
<title>Probabilistic word sense disambiguation.</title>
<date>2004</date>
<journal>Computer Speech and Language. Forthcoming.</journal>
<contexts>
<context position="4117" citStr="Preiss (2004)" startWordPosition="683" endWordPosition="684">tem proposes) and F-measure. We also calculated the similarity between the acquired unfiltered and gold standard SCF distributions using various measures of distributional similarity (see Korhonen and Preiss (2003) for details of these measures). 4 WSD Performance vs Acquired Frames No teams participated in this task in SENSEVAL3, possibly being scared off by having to sense tag around 1000 instances of each of the 29 verbs used. Teams may also have found it easier to annotate the relevant number of instances just for a subset of the verbs. We therefore used a representative WSD system due to Preiss (2004) to investigate the effect of WSD system perfor4For a forced choice system, the chosen sense is given a probability of one and the remaining senses are assigned zero probabilities. 3Note that this does not mean that 1000 sentences for each verb have been manually sense tagged. No manual sense tagging was done in this task, as the performance of the WSD system is judged by the performance of subcategorization acquisition. Verb Num senses Verb Num senses absorb 3 induce 2 bear 4 keep 3 choose 2 mark 3 compose 2 offer 2 conceive 2 proclaim 2 concentrate 2 provide 2 continue 2 roar 3 count 3 seek </context>
</contexts>
<marker>Preiss, 2004</marker>
<rawString>J. Preiss. 2004. Probabilistic word sense disambiguation. Computer Speech and Language. Forthcoming.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>