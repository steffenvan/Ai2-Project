<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010927">
<title confidence="0.998426">
Dialogue Management in the Mercury Flight Reservation System
</title>
<author confidence="0.990719">
Stephanie Seneff and Joseph Polifroni
</author>
<affiliation confidence="0.995889">
Spoken Language Systems Group
Laboratory for Computer Science
Massachusetts Institute of Technology
</affiliation>
<address confidence="0.770622">
545 Technology Square
Cambridge, MA 02139
</address>
<email confidence="0.975101">
seneff,joegsts.lcs.mit.edu
</email>
<sectionHeader confidence="0.996944" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999845">
This paper describes the dialogue module of the
Mercury systemovhich has been under development
over the past year or two. Mercury provides tele-
phone access to an on-line flight database, and allows
users to plan and price itineraries between major air-
ports worldwide. The main focus of this paper is the
dialogue control strategy, which is based on a set of
ordered rules as a mechanism to manage complex di-
alogue interactions. The paper also describes the in-
teractions between the dialogue component and the
other servers of the system, mediated via a central
hub. We evaluated the system on 49 dialogues from
users booking real flights, and report on a number of
quantitative measures of the dialogue interaction.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999975734375">
Dialogue modeling is a critical and challenging as-
pect of conversational systems, particularly when
users are permitted flexibility with regard to defin-
ing the constraints of the task. For systems that
adopt a strict system-initiated approach, it is fea-
sible to define a set of states and state transitions
depending on the usually small number of possible
user actions at each state. However, if the user is
permitted to say anything within the scope of the
recognizer at any time, such a finite-state solution
becomes unwieldy. We are interested in the devel-
opment of mixed-initiative systems, where the sys-
tem may make specific requests or suggestions, but
the user is not required to be compliant. Instead
of a finite state dialogue model, we choose to de-
compose dialogue state into a set of state variables.
The activities for a given turn typically involve the
sequential execution of a number of specialized rou-
tines, each of which performs a specific part of the
dialogue requirements and alters the state variables
in particular ways. To determine which of the op-
erations should be performed, the system consults a
dialogue control table, which is specified in a simple
scripting language.
This paper describes experiments with using this
approach to dialogue modeling in the context of our
Mercury flight reservation system. Mercury allows
users to plan air travel between 226 cities worldwide.
Following log-on, the user interacts with the system
to select the flights of their trip. When the flight
plan is completed, the system takes the initiative
to offer to price and email the itinerary. Finally,
the system asks the user a few questions to help
determine user satisfation.
The overall system makes use of the GALAXY ar-
chitecture [Seneffet al (1999)], which consists of a
number of specialized servers that communicate with
one another via a central programmable hub. An au-
dio server captures the user&apos;s speech via a Dialogic
board, and transmits the waveform to the speech
recognizer [Glass et al (1996)1. The language under-
standing component [Seneff (1992)] parses a word
graph produced by the recognizer and delivers a se-
mantic frame, encoding the meaning of the utter-
ance, to the discourse component. The output of
the discourse component [Seneff (1996)] is the frame-
in-context, which is transformed into a flattened E-
form (electronic form) by the generation server. This
E-form is delivered to the turn manager, and pro-
vides the initial settings of the dialogue state.
The turn manager consults the dialogue control
table to decide which operations to perform, and
typically engages in a module-to-module subdia-
Logue to retrieve tables from the database. It pre-
pares a response frame, which may or may not in-
clude tabular entries. The response frame is sent
to the generation component [Glass (1994)] which
transforms it in parallel into both a text string and
an annotated string that specifies the input controls
for the speech synthesizer. Finally, the speech syn-
thesizer transmits a waveform to the audio server
which then relays the spoken response to the user
over the telephone. The entire dialogue is recorded
in detail in a log file for later examination.
</bodyText>
<sectionHeader confidence="0.975978" genericHeader="method">
2 Knowledge Representations
</sectionHeader>
<bodyText confidence="0.995966">
Mercury&apos;s dialogue manager manipulates linguistic
and world knowledge represented in the form of se-
mantic frames. At each turn it begins with an
E-form representing the constraints of the current
query as a set of (key: value) pairs. This E-form pro-
</bodyText>
<page confidence="0.999179">
11
</page>
<table confidence="0.999642717948718">
Ti: John Doe
S: Please speak the month and day you registered as your password, as in, may first.
U: april first
S: Welcome John. You have successfully logged onto Mercury. How may I help you?
U: i want to travel to washington dulles on november fourth .
S: From Boston to Washington Dulles on November 4. Can you provide the approximate
departure time or airline preference?
U: i need to arrive at around nine a m
1: Okay. Flights from Boston to Washington Dulles arriving at 9:00 am on November 4.
S: One moment please.
The best matching flight is a U S Airways flight arriving at 8:59 a.m. I also have a United
flight arriving at 8:36 a.m.
U: what time does the united flight leave
S: United flight 1819 departs at 7:00 am from Boston. Shall I book this flight for you?
U: yes please
S: United tight 1819 has been added to your itinerary. You depart Boston at 7:00 am and arrive
in Washington Dulles at 8:36 am. Where would you like to go next?
U: i would like to return to boston
S: What date will you be returning on? ...
U: the same day
I: Okay. United flights from Washington Dulles to Boston on November 4. One moment please.
S: Can you provide a departure or arrival time? The earliest nonstop leaves at 9:30 a.m.
and arrives at 10:57 a.m.
U: i would like to leave after three thirty
S: The best matching flight is a United flight departing at 4:30 p.m. I also have a United flight
departing at 5:30 p.m.
U: i will take the first one
S: United flight 1278 has been added to your itinerary. You depart Washington Dulles at 4:30
pm and arrive in Boston at 6:07 pm. Shall I price your itinerary?
U: yes please
I: Pricing your itinerary will take a while. In the meantime, Ill summarize your itinerary for you.
S: on November 4, you depart on United flight 1819 from Boston at 7:00 am and arrive in
Washington Dulles at 8:36 am. From there, you depart on United flight 1278 at 4:30 pm
and arrive in Boston at 6:07 pm.
The price for your itinerary is $249.00. Shall I email your itinerary to you?
U: yes
S: Your itinerary has been emailed. Is there something else I can do for you?
U: no thank you
S: Thank you for calling! Have a nice day!
</table>
<tableCaption confidence="0.9318785">
Table 1: Example dialogue between a user and the system involving a real trip. &amp;quot;U&amp;quot; denotes a user utterance,
&amp;quot;S&amp;quot; a system utterance, and &amp;quot;I&amp;quot; an intermediate system response spoken prior to database retrieval.
</tableCaption>
<bodyText confidence="0.999414576923077">
vides the initial values in the dialogue state, which
evolves over the course of the turn as rules are exe-
cuted.
In addition to the dialogue state, Mercury also
makes use of several other internal representations
which retain state. These are packaged up into
a turn-manager history frame which is passed to
the hub for safe-keeping at the end of each turn,
and returned with each subsequent turn. The user
model contains a record of the system&apos;s current be-
lief about the state of the user&apos;s flight plan. For
example, if the user&apos;s first query were &amp;quot;I want to go
to Denver next Friday morning, returning the fol-
lowing Wednesday,&amp;quot; the system would record that
this is a round trip flight and would save the return
date (unresolved, in case there was a recognition er-
ror on the forward leg date) in the user model. It
would retrieve from the user model, as the forward
leg source, the default value provided by the user at
enrollment. Thus the user model retains both the
initial user preferences and all of the details of the
current flight plan as they become available. The
system also retains internally a list of all the flights
that matched the user&apos;s constraint specifications, as
well as a list of the subset of the flights that it has
spoken about (e.g., the nonstop flights or the earliest
</bodyText>
<page confidence="0.959528">
12
</page>
<figure confidence="0.9913905">
-airline
• • •
</figure>
<figureCaption confidence="0.999878">
Figure 1: Au (incomplete) state diagram for the example system described in the text.
</figureCaption>
<bodyText confidence="0.993059846153846">
date
airline
flight). These flights become available for reference
in subsequent queries such as &amp;quot;book the third one,&amp;quot;
or &amp;quot;where does the united flight stop?&amp;quot;.
Whenever the system needs to go to the database,
it provides an intermediate reply frame which serves
both to ask the user for patience and to confirm a
consistent shared model of the dialogue history. The
system&apos;s final response is also a frame, and it often
contains a list of flights to be read back to the user.
In other cases, it may be a yes-no question as in
&amp;quot;Shall I email your itinerary to you?&amp;quot; The latter
case also necessitates sending both to the discourse
component a system-initiative context for yes/no re-
sponses, and to the language understanding compo-
nent a dialogue context filter that favors confirma-
tions and denials in the N-best selection. A similar
process takes place when the system prompts for in-
formation such as a date or a departure city.
Table 1 shows a dialogue between a user and the
system involving a real trip between Boston and
Washington D.C. The name and password informa-
tion have been omitted for privacy reasons. In the
figure, &amp;quot;U&amp;quot; stands for &amp;quot;user&amp;quot;, &amp;quot;S&amp;quot; for system, and
&amp;quot;I&amp;quot; for intermediate response.
</bodyText>
<sectionHeader confidence="0.990504" genericHeader="method">
3 Dialogue Management Strategies
</sectionHeader>
<bodyText confidence="0.995841">
A finite state network is a convenient way to formu-
late dialogue states, as long as the system is able to
maintain strong control over the order in which slots
are filled, and especially when slots are filled one at
a time. However, if the user is permitted the flexi-
bility to fill slots in any order, and to fill more than
one slot in a single turn, then it becomes difficult
and inefficient to define a finite state network that
covers the entire space of possibilities.
A finite state solution for a simple dialogue exam-
</bodyText>
<figureCaption confidence="0.945955">
Figure 2: A dialogue control table corresponding to
the state diagram in Figure 2.
</figureCaption>
<bodyText confidence="0.99955688">
pie is given in Figure 1. Here, we assume that the
user can fill four slots (source, destination, date, and
airline) and that all four must be filled before the
system can retrieve flights. The slots can be filled in
any order, and the user can fill any number of slots
in each turn. For notational convenience, we repre-
sent the states filled/unfilled by 1/0, and we order
the fields as [source, destination, date, airline], such
that state &amp;quot;1101&amp;quot; says that only date is missing. At
each state the system prompts the user for all miss-
ing slots. Thus, state 1000 would speak, &amp;quot;Can you
provide one or more of destination, date, and air-
line?&amp;quot; If the user provides more than a single field
in a given turn, the system advances by more than
one state in the state diagram. To fully specify all
possibilities, even for this simple example, requires
a large number of states in the state diagram.
Figure 2 shows a set of ordered rules which achieve
the same goals as the state diagram but with a
greatly simplified dialogue control specification. In
this case, state is encoded implicitly as a bit map
corresponding to the filled/unfilled values of the four
variables, The notation &amp;quot;1&amp;quot; stands for &amp;quot;NOT,&amp;quot;
meaning that this variable is not yet filled. Follow-
ing the arrow is the name of the operation to be
</bodyText>
<table confidence="0.604704833333333">
!source prompt—source
!destination prompt—destination
!airline prompt_airline
!date pr ompt_d at e
nprompts &gt; 1 rnark_multiple
nprompts = 0 retrieve_ffights
</table>
<page confidence="0.763868">
13
</page>
<figure confidence="0.61716925">
clause request_keypad
week day rel_date
flight_number &amp; !airline
clause price &amp; topic itinerary &amp; truth_value deny
clause book SE num_found
num_nonstops &gt; 1 &amp; num_nonstops &lt;4 &amp; arrival_time
keypadjnput
resolve_relative_date
need_airline
dont_priceitinerary
a.dd_flight_toitinerary
speak_three_arrivals
</figure>
<figureCaption confidence="0.993959">
Figure 3: Selected rules from the Mercury dialogue control table. &amp;quot;&amp;&amp;quot; stands for &amp;quot;AND&amp;quot; and &amp;quot;i&amp;quot; stands for
</figureCaption>
<bodyText confidence="0.999977265957447">
performed when the conditions are met. Each op-
eration is usually a specific function, and is free to
augment the variable space by either altering the val-
ues of pre-existing variables or introducing new vari-
ables. In our simple example the four prompt opera-
tions simply add their respective variables to the re-
sponse frame and increment the variable &amp;quot;nprompt.&amp;quot;
The final function &amp;quot;mark_multiple&amp;quot; fires only if more
than one of the preceding functions have fired. Its
action is to insert &amp;quot;one or more of&amp;quot; into the response.
The final &amp;quot;retrieve&amp;quot; operation only fires if none of the
preceding operations fired. It is significant that state
is dispersed not only over the variables but also over
the operations. Thus, any single prompt state of
the state transition matrix would involve up to two
operations in the dialogue control table, and, con-
versely, the &amp;quot;mark_multiple&amp;quot; operation is associated
with several states.
In addition to possibly altering the values of vari-
ables in the dialogue state, operations also have the
responsibility to return, upon completion, one of
three &amp;quot;move&amp;quot; states. The majority of the operations
return the state &amp;quot;CONTINUE,&amp;quot; which means simply
to continue execution just beyond the rule that just
fired. The two other possibilities are &amp;quot;STOP,&amp;quot; i.e.,
exit from the dialogue control table, and &amp;quot;RESTART&amp;quot;
which means to return to the top of the set of
dialogue rules and start over. An (optional) dis-
tinguished operation, which handles activities that
should occur routinely at every turn, is executed
upon exiting the dialogue control process. The tests
on variables can be binary, arithmetic, or string
matching on values.
Our Mercury system makes use of this &amp;quot;ordered
rules&amp;quot; strategy for dialogue control. However, the
rules are dealing with a great deal more complexity
than simply deciding what prompts to issue when
certain slots are unfilled. There are currently a total
of over 200 rules in Mercury&apos;s dialogue control table.
These rules can be grouped into several distinct cat-
egories. Only nine of the rules involve prompts for
missing information. Another nine are involved with
logging into the system, i.e., acquiring the name and
the password, which may have to be entered using
the telephone keypad, either upon user request or
as a consequence of recognition failure. Eleven rules
are concerned with meta-level interactions such as
apologies for missing services (no flight status in-
formation available), and requests for help or rep-
etition. Several rules have to do with determining
whether the current request plausibly involves a ref-
erence to a flight in a pre-existing flight list. This
could be an obvious reference to the nth flight, or it
could be an inquiry about the &amp;quot;united flight&amp;quot; or the
&amp;quot;nine a.m. flight.&amp;quot; Several more rules involve inter-
preting various references to relative dates and/or
times such as &amp;quot;the following Thursday,&amp;quot; or &amp;quot;that
same afternoon.&amp;quot;
The largest single category concerns preparing the
reply frame, after the database retrieval has already
taken place. Twenty six rules are concerned with
this task, and they are keyed on a combination of the
number and type of flights retrieved and the specific
request of the user (e.g., &amp;quot;where do they connect?&amp;quot;).
The purpose of these functions is to reduce the num-
ber of database tuples returned for a given query to
a manageable set for a spoken response. This re-
duction is based on a hierarchy of quality measures
for each flight, beginning with any stated or inferred
preference (e.g., a particular airport in the case of
cities with multiple airports, or a particular airline
in the case of a multi-leg booking where one leg has
already been established) and including number of
stops and length of flight. These functions also con-
solidate information about the flights to be spoken,
combining shared attributes. Our ultimate goal is to
produce a response such as &amp;quot;I have found three non-
stop United flights. Flight 100 leaves at 8:45, flight
200 leaves at 9:10, and flight 300 leaves at 9:30.&amp;quot;
The dialogue control table facilitates this interac-
tion by enabling the system developer to encode the
constraints of the hierarchy in the rules.
Finally, there are a miscellaneous set of rules that
have to do with updating the user model, prepar-
ing the intermediate reply, pricing or emailing the
itinerary, preparing the database query, filtering
flights, or updating the itinerary.
Since the operations are general functions, it is up
to the system developer to decide how to parcel up
the computational requirements into the individual
operations. With experience, one acquires a set of
guidelines to help formalize this process. As a gen-
eral rule, it is preferrable to limit the use of nested
function calls. Instead, an operation can set a vari-
</bodyText>
<page confidence="0.994416">
14
</page>
<figure confidence="0.3420025">
WER words/turn total turns total time
11.5% 7 11 229 sec.
</figure>
<figureCaption confidence="0.9784165">
Figure 4: Some easily computed statistics on the 36
successful bookings.
</figureCaption>
<bodyText confidence="0.997087181818182">
able to indicate that another operation needs to be
called, and then the intended subroutine gets pro-
moted to the status of a dialogue control operation.
This has the effect of exposing more of the internal
activities of the system to the dialogue control table,
which serves as a very useful outline of system con-
trol flow. Another general policy is that each oper-
ation should be concerned with a single well-defined
task, again in ortler-not to conceal complexity.
Figure 3 shows several examples of actual rules in
Mercury&apos;s dialogue control table.
</bodyText>
<sectionHeader confidence="0.864813" genericHeader="method">
4 Data Collection and Evaluation
</sectionHeader>
<bodyText confidence="0.991629794871795">
Mercury first became available for data collection
in October &apos;99. Prospective users must first enroll
by filling in a simple form on a Web page, where
they enter, minimally, their name, email address,
and password (a date). Once the user&apos;s name has
been added to the recognizer and language under-
standing components, they receive an email mes-
sage informing them of the telephone number. Users
are encouraged to attempt to book real trips. From
late October to early December, we collected 49 dia-
logues involving real flight bookings, and these form
the basis for our evaluation studies.
Overall, 73% of the bookings were successful
(36/49). We used a very strict requirement for suc-
cess. For example, in one dialogue considered un-
successful the system did not know the airline that
the user requested, and so the user compromised and
booked the trip on a different airline. Three of the
failures are due to the user simply hanging up in
frustration, and three others are due to the system
hanging up due to a misrecognized &amp;quot;good-bye.&amp;quot; Two
failures were due to user inattentiveness. The user
believed that the trip was correctly booked, but a
misrecognition produced a different itinerary than
the one they were specifying. Finally, four of the
failures involved completely correct bookings, but
the system was unable to follow through with the
pricing and/or emailing of the itinerary. Some of
these involved inadequacies in the dialogue module,
once the user did not provide the expected response
to a system request. There was a striking difference
in recognition error between the successful and the
incomplete bookings (11.5% vs 26% WER). A heavy
foreign accent accounted for some of the recognition
problems.
Some easily measurable statistics for the successes
are given in Figure 4. These numbers were computed
IBR: 0 1 2 3 4 total
Nutts: 41 90 55 31 9 226
</bodyText>
<figureCaption confidence="0.652531">
Figure 5: Distribution of evaluable user utterances
</figureCaption>
<bodyText confidence="0.985506555555556">
in terms of number of new attributes introduced
with each dialogue turn, IBR = Information Bit
Rate.
on the &amp;quot;core dialogue,&amp;quot; defined as the interval subse-
quent to logging on and up until the itinerary is fully
specified, but has not yet been priced. On average
users required less than four minutes to complete
the core dialogue, although three outliers took more
than seven minutes.
</bodyText>
<subsectionHeader confidence="0.996779">
4.1 Log File Evaluation
</subsectionHeader>
<bodyText confidence="0.999950536585366">
We have long been interested in seeking evaluation
metrics that are automatic and that can apply on a
per-utterance basis but evaluate a significant portion
of the system beyond the recognizer. In [Polifroni et
al. (1998)] we proposed an E-form evaluation met-
ric, which compares an Fr-form obtained by parsing
the original orthography against that obtained by
parsing the selected recognizer hypothesis. We be-
lieve this is a good metric for evaluating how well the
recognizer and parser are doing, but it says nothing
about the discourse and dialogue components.
We recently devised two new evaluation metrics,
which we believe are useful measures for assessing
the performance of the recognizer, parser, discourse,
and dialogue components, collectively. To compute
the measures, we must reprocess the log file after the
orthographic transcription has been provided for the
user queries. Basically, both the recognizer hypothe-
sis and the original orthography are run through the
system utterance by utterance, with the discourse
and dialogue states being maintained exclusively by
the recognizer branch. For both branches, the E-
form that is produced after the turn manager has fin-
ished processing the query is sent to a special evalu-
ation server. This server maintains a running record
of all the attributes that appear in the orthography
path, comparing them against their counterparts in
the recognizer path.
The two parameters that emerge from comparing
these E-forms we refer to as information bit rate
(IBR) and user frustration (UF). IBR measures the
average number of new attributes introduced per
user query. A subsequent query that reiterates the
same attribute is excluded since it did not intro-
duce any new information. Thus if the user said,
&amp;quot;I want to go from Seattle to Chicago on Decem-
ber 27,&amp;quot; and the system misrecognized the date as
&amp;quot;December 22,&amp;quot; then a subsequent query, &amp;quot;1 said
December 27&amp;quot; would be registered as contributing a
0 count to the IBR parameter. The UF parameter
tabulates how many turns it took, on average, for an
</bodyText>
<page confidence="0.992445">
15
</page>
<bodyText confidence="0.999993204081633">
intended attribute to be transmitted successfully to
the system. Thus, in the example above, the source
and destination each took one turn, but the date
took two.
There are some difficulties with rerunning the di-
alogue at a later time. Both the system and the
database are in a state of flux, and so the dialogue
can become incoherent. For example, in one case the
user said, &amp;quot;Book it,&amp;quot; in response to a single flight
being proposed, but due to changes in the flight
schedule, the system proposed three flights in the
rerun and the dialogue became incoherent from that
point on. To help alleviate incoherence, we provide
a mechanism to artificially offset the date, at least
to assure that the dates they have selected haven&apos;t
already passed. In spite of the above problems, we
feel that these—evaluation metrics show considerable
promise.
In a pilot study, we processed a subset of our data
through this evaluation configuration. We identi-
fied a set of 17 attributes that could be monitored.
Five percent of the utterances had orthographies
that failed to parse. These are unevaluable with-
out human reannotation, and are hence eliminated
from the pool in the discussion below, although they
clearly are likely to be very problematic. Figure 5
summarizes the results for information bit rate for
the remainder of the utterances. A surprisingly large
percentage of the utterances introduce no new con-
cepts. Some, but not all, of these are similar to
the date misrecognition example given above. Oth-
ers are cases where the user was confused about
the state of the system&apos;s knowledge, and decided to
simply repeat all the preceding constraints just to
make sure. Some are also misfirings of the endpoint
detector producing content-free utterances such as
&amp;quot;okay.&amp;quot; In other cases the user intended an action,
but the system&apos;s understanding mechanism was not
sophisticated enough. For example &amp;quot;That&apos;s good&amp;quot;
meaning &amp;quot;book it.&amp;quot; We were pleased with the per-
centage of sentences that contained more than one
attribute. We believe that a typical directed di-
alogue would have far fewer utterances with more
than one attribute.
Excluding the 5% of utterances whose orthogra-
phy failed to parse, our system achieved a 1.05% user
frustration rate. This means that, on average, one
out of every 20 attributes had to be entered twice.
We were very pleased with this number.
</bodyText>
<sectionHeader confidence="0.995144" genericHeader="conclusions">
5 Summary and Future Work
</sectionHeader>
<bodyText confidence="0.999965944444445">
This paper described our strategy for dialogue man-
agement in the Mercury system. Overall, we have
found it to be extremely powerful. While the Mer-
cury system is still under active development, we feel
that the anticipated extensions of capability will re-
quire a straightforward process of expansions in the
dialogue control table; i.e., the system has not at-
tained an unmanageable degree of complexity in or-
ganization. We believe that this is a direct conse-
quence of the use of the dialogue control table.
Mercury&apos;s turn manager deals with some issues,
such as the interpretation of dates and times, that
are of relevance to many other potential domains.
We envision that at some future time we will have
available a large library of operations of general util-
ity that can be inserted into a new system to greatly
accelerate the time required to bring the new domain
up to full operation.
</bodyText>
<sectionHeader confidence="0.999191" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999952083333334">
Seneff, S., R. Lau, and J. Polifroni. (1999) &amp;quot;Or-
ganization, Communication, and Control in the
GALAXY-II Conversational System,&amp;quot; Proc. Eu-
rospeech. &apos;99, Budapest, Hungary, pp. 1271-1274.
Glass, J., J. Chang and M. McCandless. (1996) &amp;quot;A
Probabilistic Framework for Feature-based Speech
Recognition,&amp;quot; Proc. ICSLP &apos;96, Philadelphia, PA,
pp. 2277-2280.
Seneff, S. (1992) &amp;quot;TINA: a Natural Language Sys-
tem for Spoken Language Applications,&amp;quot; Compu-
tational Linguistics, 18/1, pp. 61-86.
Seneff, S, D. Goddeau, C. Pao, and J. Polifroni.
(1996) &amp;quot;Multimodal Discourse Modelling • in a
Multi-user Multi-domain Environment,&amp;quot; Proceed-
ings, internationat Conference on Spoken Lan-
guage Processing &apos;96, pp 192-195, Oct. 3-6,
Glass, J., J. Polifroni, 84 S. Seneff. (1994). &amp;quot;Multi-
lingual Language Generation across Multiple Do-
mains.&amp;quot; Proc. International Conference on Spoken
Language Processing (pp. 983-986), Yokohama.
Polifroni, J., S. Seneff, J. Glass, and T.J. Hazen.
(1998) &amp;quot;Evaluation Methodology for a Telephone-
based Conversational System.&amp;quot; Proc. LREC &apos;98,
pp. 43-50, Granada, Spain.
</reference>
<page confidence="0.998702">
16
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.597571">
<title confidence="0.999291">Dialogue Management in the Mercury Flight Reservation System</title>
<author confidence="0.714736">Stephanie Seneff</author>
<author confidence="0.714736">Joseph</author>
<affiliation confidence="0.869872">Spoken Language Systems Laboratory for Computer Massachusetts Institute of</affiliation>
<address confidence="0.9444185">545 Technology Cambridge, MA</address>
<email confidence="0.99963">seneff,joegsts.lcs.mit.edu</email>
<abstract confidence="0.99977">This paper describes the dialogue module of the Mercury systemovhich has been under development over the past year or two. Mercury provides telephone access to an on-line flight database, and allows users to plan and price itineraries between major airports worldwide. The main focus of this paper is the dialogue control strategy, which is based on a set of ordered rules as a mechanism to manage complex dialogue interactions. The paper also describes the interactions between the dialogue component and the other servers of the system, mediated via a central hub. We evaluated the system on 49 dialogues from users booking real flights, and report on a number of quantitative measures of the dialogue interaction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Seneff</author>
<author>R Lau</author>
<author>J Polifroni</author>
</authors>
<title>Organization, Communication, and Control in the GALAXY-II Conversational System,&amp;quot;</title>
<date>1999</date>
<booktitle>Proc. Eurospeech. &apos;99,</booktitle>
<pages>1271--1274</pages>
<location>Budapest, Hungary,</location>
<marker>Seneff, Lau, Polifroni, 1999</marker>
<rawString>Seneff, S., R. Lau, and J. Polifroni. (1999) &amp;quot;Organization, Communication, and Control in the GALAXY-II Conversational System,&amp;quot; Proc. Eurospeech. &apos;99, Budapest, Hungary, pp. 1271-1274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Glass</author>
<author>J Chang</author>
<author>M McCandless</author>
</authors>
<title>A Probabilistic Framework for Feature-based Speech Recognition,&amp;quot;</title>
<date>1996</date>
<booktitle>Proc. ICSLP &apos;96,</booktitle>
<pages>2277--2280</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="3030" citStr="Glass et al (1996)" startWordPosition="481" endWordPosition="484">. Following log-on, the user interacts with the system to select the flights of their trip. When the flight plan is completed, the system takes the initiative to offer to price and email the itinerary. Finally, the system asks the user a few questions to help determine user satisfation. The overall system makes use of the GALAXY architecture [Seneffet al (1999)], which consists of a number of specialized servers that communicate with one another via a central programmable hub. An audio server captures the user&apos;s speech via a Dialogic board, and transmits the waveform to the speech recognizer [Glass et al (1996)1. The language understanding component [Seneff (1992)] parses a word graph produced by the recognizer and delivers a semantic frame, encoding the meaning of the utterance, to the discourse component. The output of the discourse component [Seneff (1996)] is the framein-context, which is transformed into a flattened Eform (electronic form) by the generation server. This E-form is delivered to the turn manager, and provides the initial settings of the dialogue state. The turn manager consults the dialogue control table to decide which operations to perform, and typically engages in a module-to-m</context>
</contexts>
<marker>Glass, Chang, McCandless, 1996</marker>
<rawString>Glass, J., J. Chang and M. McCandless. (1996) &amp;quot;A Probabilistic Framework for Feature-based Speech Recognition,&amp;quot; Proc. ICSLP &apos;96, Philadelphia, PA, pp. 2277-2280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Seneff</author>
</authors>
<title>TINA: a Natural Language System for Spoken Language Applications,&amp;quot;</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<pages>61--86</pages>
<contexts>
<context position="3084" citStr="Seneff (1992)" startWordPosition="490" endWordPosition="491">elect the flights of their trip. When the flight plan is completed, the system takes the initiative to offer to price and email the itinerary. Finally, the system asks the user a few questions to help determine user satisfation. The overall system makes use of the GALAXY architecture [Seneffet al (1999)], which consists of a number of specialized servers that communicate with one another via a central programmable hub. An audio server captures the user&apos;s speech via a Dialogic board, and transmits the waveform to the speech recognizer [Glass et al (1996)1. The language understanding component [Seneff (1992)] parses a word graph produced by the recognizer and delivers a semantic frame, encoding the meaning of the utterance, to the discourse component. The output of the discourse component [Seneff (1996)] is the framein-context, which is transformed into a flattened Eform (electronic form) by the generation server. This E-form is delivered to the turn manager, and provides the initial settings of the dialogue state. The turn manager consults the dialogue control table to decide which operations to perform, and typically engages in a module-to-module subdiaLogue to retrieve tables from the database</context>
</contexts>
<marker>Seneff, 1992</marker>
<rawString>Seneff, S. (1992) &amp;quot;TINA: a Natural Language System for Spoken Language Applications,&amp;quot; Computational Linguistics, 18/1, pp. 61-86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Seneff</author>
<author>D Goddeau</author>
<author>C Pao</author>
<author>J Polifroni</author>
</authors>
<title>Multimodal Discourse Modelling • in a Multi-user Multi-domain Environment,&amp;quot;</title>
<date>1996</date>
<booktitle>Proceedings, internationat Conference on Spoken Language Processing &apos;96,</booktitle>
<pages>192--195</pages>
<marker>Seneff, Goddeau, Pao, Polifroni, 1996</marker>
<rawString>Seneff, S, D. Goddeau, C. Pao, and J. Polifroni. (1996) &amp;quot;Multimodal Discourse Modelling • in a Multi-user Multi-domain Environment,&amp;quot; Proceedings, internationat Conference on Spoken Language Processing &apos;96, pp 192-195, Oct. 3-6,</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Glass</author>
<author>J Polifroni</author>
</authors>
<title>Multilingual Language Generation across Multiple Domains.&amp;quot;</title>
<date>1994</date>
<booktitle>Proc. International Conference on Spoken Language Processing</booktitle>
<pages>983--986</pages>
<location>Yokohama.</location>
<marker>Glass, Polifroni, 1994</marker>
<rawString>Glass, J., J. Polifroni, 84 S. Seneff. (1994). &amp;quot;Multilingual Language Generation across Multiple Domains.&amp;quot; Proc. International Conference on Spoken Language Processing (pp. 983-986), Yokohama.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Polifroni</author>
<author>S Seneff</author>
<author>J Glass</author>
<author>T J Hazen</author>
</authors>
<title>Evaluation Methodology for a Telephonebased Conversational System.&amp;quot;</title>
<date>1998</date>
<booktitle>Proc. LREC &apos;98,</booktitle>
<pages>43--50</pages>
<location>Granada,</location>
<contexts>
<context position="19963" citStr="Polifroni et al. (1998)" startWordPosition="3357" endWordPosition="3360">ms of number of new attributes introduced with each dialogue turn, IBR = Information Bit Rate. on the &amp;quot;core dialogue,&amp;quot; defined as the interval subsequent to logging on and up until the itinerary is fully specified, but has not yet been priced. On average users required less than four minutes to complete the core dialogue, although three outliers took more than seven minutes. 4.1 Log File Evaluation We have long been interested in seeking evaluation metrics that are automatic and that can apply on a per-utterance basis but evaluate a significant portion of the system beyond the recognizer. In [Polifroni et al. (1998)] we proposed an E-form evaluation metric, which compares an Fr-form obtained by parsing the original orthography against that obtained by parsing the selected recognizer hypothesis. We believe this is a good metric for evaluating how well the recognizer and parser are doing, but it says nothing about the discourse and dialogue components. We recently devised two new evaluation metrics, which we believe are useful measures for assessing the performance of the recognizer, parser, discourse, and dialogue components, collectively. To compute the measures, we must reprocess the log file after the </context>
</contexts>
<marker>Polifroni, Seneff, Glass, Hazen, 1998</marker>
<rawString>Polifroni, J., S. Seneff, J. Glass, and T.J. Hazen. (1998) &amp;quot;Evaluation Methodology for a Telephonebased Conversational System.&amp;quot; Proc. LREC &apos;98, pp. 43-50, Granada, Spain.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>