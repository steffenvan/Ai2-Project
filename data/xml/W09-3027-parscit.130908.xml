<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005555">
<title confidence="0.988017">
Annotating Semantic Relations Combining Facts and Opinions
</title>
<author confidence="0.9310705">
Koji Murakami† Shouko Masuda†$ Suguru Matsuyoshi†
Eric Nichols† Kentaro Inui† Yuji Matsumoto††Nara Institute of Science and Technology
</author>
<affiliation confidence="0.978462">
8916-5, Takayama, Ikoma, Nara 630-0192 JAPAN
$Osaka Prefecture University
</affiliation>
<address confidence="0.662794">
1-1, Gakuen, Naka-ku, Sakai, Osaka 599-8531 JAPAN
</address>
<email confidence="0.999001">
{kmurakami,shouko,matuyosi,eric-n,inui,matsu}@is.naist.jp
</email>
<sectionHeader confidence="0.993904" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999725588235294">
As part of the STATEMENT MAP project,
we are constructing a Japanese corpus an-
notated with the semantic relations bridg-
ing facts and opinions that are necessary
for online information credibility evalua-
tion. In this paper, we identify the se-
mantic relations essential to this task and
discuss how to efficiently collect valid ex-
amples from Web documents by splitting
complex sentences into fundamental units
of meaning called “statements” and an-
notating relations at the statement level.
We present a statement annotation scheme
and examine its reliability by annotating
around 1,500 pairs of statements. We are
preparing the corpus for release this win-
ter.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999927269230769">
The goal of the STATEMENT MAP project (Mu-
rakami et al., 2009) is to assist internet users with
evaluating the credibility of online information by
presenting them with a comprehensive survey of
opinions on a topic and showing how they relate
to each other. However, because real text on the
Web is often complex in nature, we target a sim-
pler and more fundamental unit of meaning which
we call the “statement.” To summarize opinions
for the statement map users, we first convert all
sentences into statements and then, organize them
into groups of agreeing and conflicting opinions
that show the logical support for each group.
For example, a user who is concerned about po-
tential connections between vaccines and autism
would be presented with a visualization of the
opinions for and against such a connection to-
gether with the evidence supporting each view as
shown in Figure 1.
When the concerned user in our example looks
at this STATEMENT MAP, he or she will see that
some opinions support the query ”Do vaccines
cause autism?” while other opinions do not, but
it will also show what support there is for each of
these viewpoints. So, STATEMENT MAP can help
user come to an informed conclusion.
</bodyText>
<sectionHeader confidence="0.993109" genericHeader="method">
2 Semantic Relations between
Statements
</sectionHeader>
<subsectionHeader confidence="0.999823">
2.1 Recognizing Semantic Relations
</subsectionHeader>
<bodyText confidence="0.9992045">
To generate STATEMENT MAPs, we need to an-
alyze a lot of online information retrieved on a
given topic, and STATEMENT MAP shows users
a summary with three major semantic relations.
AGREEMENT to group similar opinions
CONFLICT to capture differences of opinions
EVIDENCE to show support for opinions
Identifying logical relations between texts is the
focus of Recognizing Textual Entailment (RTE).
A major task of the RTE Challenge (Dagan et al.,
2005) is the identification of [ENTAILMENT] or
[CONTRADICTION] between Text (T) and Hy-
pothesis (H). For this task, several corpora have
been constructed over the past few years, and an-
notated with thousands of (T,H) pairs.
While our research objective is to recognize se-
mantic relations as well, our target domain is text
from Web documents. The definition of contradic-
tion in RTE is that T contradicts H if it is very un-
likely that both T and H can be true at the same
time. However, in real documents on the Web,
there are many examples which are partially con-
tradictory, or where one statement restricts the ap-
plicability of another like in the example below.
</bodyText>
<footnote confidence="0.5163585">
(1) a. Mercury-based vaccines actually cause autism in
children.
</footnote>
<page confidence="0.941954">
150
</page>
<note confidence="0.9796715">
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 150–153,
Suntec, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<figureCaption confidence="0.999578">
Figure 1: An example STATEMENT MAP for the query “Do vaccines cause autism?”
</figureCaption>
<figure confidence="0.875111629629629">
Query : &gt;Jo vaccines cause autism?
VACCINES CAUSE AUTISM
[FOCUS]
VACCINES DON&apos;T CAUSE AUTISM
!Mercury-based vaccine preservatives actually have caused autism in
children.
!It&apos;s biologically plausible that the MMR vaccine causes autism.
[EVIDENCE]
!My son then had the MMR, and then when he was three he was
diagnosed with autism.
!He then had the MMR, and then when he was three he was
diagnosed with autism.
MY CHILD WAS DIAGNOSED WITH AUTISM
RIGHT AFTER THE VACCINE
[CONFLICT]
[CONFLICT]
!There is no valid scientific evidence that vaccines
cause autism.
!The weight of the evidence indicates that vaccines
are not associated with autism.
ANECDOTES ARE NOT EVIDENCE
!Vaccinations are given around the same time
children can be first diagnosed.
!The plural of anecdote is not data.
[EVIDENCE]
b. Vaccines can trigger autism in a vulnerable subset of
children.
</figure>
<bodyText confidence="0.998219833333333">
While it is difficult to assign any relation to this
pair in an RTE framework, in order to construct
statement maps we need to recognize a contradic-
tion between (1a) and (1b).
There is another task of recognizing relations
between sentences, CST (Cross-Document Struc-
ture Theory) which was developed by Radev
(2000). CST is an expanded rhetorical structure
analysis based on RST (Mann and Thompson,
1988), and attempts to describe relations between
two or more sentences from both single and mul-
tiple document sets. The CSTBank corpus (Radev
et al., 2003) was constructed to annotate cross-
document relations. CSTBank is divided into clus-
ters in which topically-related articles are gath-
ered. There are 18 kinds of relations in this corpus,
including [EQUIVALENCE], [ELABORATION],
and [REFINEMENT].
</bodyText>
<subsectionHeader confidence="0.993069">
2.2 Facts and Opinions
</subsectionHeader>
<bodyText confidence="0.967843394736842">
RTE is used to recognize logical and factual re-
lations between sentences in a pair, and CST is
used for objective expressions because newspa-
per articles related to the same topic are used as
data. However, the task specifications of both RTE
and CST do not cover semantic relations between
opinions and facts as illustrated in the following
example.
(2) a. There must not be a connection between vaccines
and autism.
b. I do believe that there is a link between vaccinations
and autism.
Subjective statements, such as opinions, are re-
cently the focus of many NLP research topics,
such as review analysis, opinion extraction, opin-
ion QA, or sentiment analysis. In the corpus con-
structed by the MPQA Project (Multi-Perspective
Question Answering) (Wiebe et al., 2005), indi-
vidual expressions are marked that correspond to
explicit mentions of private states, speech events,
and expressive subjective elements.
Our goal is to annotate instances of the three
major relation classes: [AGREEMENT], [CON-
FLICT] and [EVIDENCE], between pairs of state-
ments in example texts. However, each relation
has a wide range, and it is very difficult to define
a comprehensive annotation scheme. For exam-
ple, different kinds of information can act as clues
to recognize the [AGREEMENT] relations. So,
we have prepared a wide spectrum of semantic re-
lations depending on different types of informa-
tion regarded as clues to identify a relation class,
such as [AGREEMENT] or [CONFLICT]. Table 1
shows the semantic relations needed for carry-
ing out the anotation. Although detecting [EVI-
DENCE] relations is also essential to the STATE-
MENT MAP project, we do not include them in our
current corpus construction.
</bodyText>
<sectionHeader confidence="0.984391" genericHeader="method">
3 Constructing a Japanese Corpus
</sectionHeader>
<subsectionHeader confidence="0.952431">
3.1 Targeting Semantic Relations Between
Statements
</subsectionHeader>
<bodyText confidence="0.999857117647059">
Real data on the Web generally has complex sen-
tence structures. That makes it difficult to rec-
ognize semantic relations between full sentences.
but it is possible to annotate semantic relation be-
tween parts extracted from each sentence in many
cases. For example, the two sentences A and B
in Figure 2 cannot be annotated with any of the
semantic relations in Table 1, because each sen-
tence include different types of information. How-
ever, if two parts extracted from these sentences C
and D are compared, the parts can be identified as
[EQUIVALENCE] because they are semantically
close and each extracted part does not contain a
different type of information. So, we attempt to
break sentences from the Web down into reason-
able text segments, which we call “statements.”
When a real sentence includes several pieces of se-
</bodyText>
<page confidence="0.998495">
151
</page>
<tableCaption confidence="0.999612">
Table 1: Definition of semantic relations and example in the corpus
</tableCaption>
<table confidence="0.9200555">
Relation Class Relation Label Example
CONFLICT A: Mercury-based vaccine preservatives actually have caused autism in children.
Contradiction
B: Vaccines don’t cause autism.
A: Vaccines can trigger autism in a vulnerable subset of children.
Confinement
B: Mercury-based vaccine actually have caused autism in children.
A: I don’t think vaccines cause autism.
Conflicting Opinion
B: I believe vaccines are the cause of my son’s autism.
</table>
<figure confidence="0.974006529411765">
A: The overwhelming evidence is that vaccines are unrelated to autism.
Equivalence
B: There is no link between the MMR vaccine and autism.
A: We think vaccines cause autism.
B: I am the mother of a 6 year old that regressed into autism because of his 18
month vaccinations.
A: Mercury-based vaccine preservatives actually have caused autism in children.
Specific
B: Vaccines cause autism.
AGREEMENT
Equivalent Opinion
(A) Real sentence (1) (B) Real sentence (2)
According to Department The weight of the
of Medicine, there is no evidence indicates that
link between the MMR vaccines are not
vaccine and autism. associated with autism.
(C) Statement (1) (E) [EQUIVALENCE] (D) Statement (2)
</figure>
<figureCaption confidence="0.873079">
There is no link between the Vaccines are not
MMR vaccine and autism. associated with autism.
Figure 2: Extracting statements from sentences
and annotating a semantic relation between them
</figureCaption>
<bodyText confidence="0.986956818181818">
mantic segments, more than one statement can be
extracted. So, a statement can reflect the writer’s
affirmation in the original sentence. If the ex-
tracted statements lack semantic information, such
as pronouns or other arguments, human annota-
tors manually add the missing information. Fi-
nally we label pairs of statements with either one
of the semantic relations from Table 1 or with “NO
RELATION,” which means that two sentences (1)
are not semantically related, or (2) have a relation
other than relations defined in Table 1.
</bodyText>
<subsectionHeader confidence="0.998636">
3.2 Corpus Construction Procedure
</subsectionHeader>
<bodyText confidence="0.9995315">
We automatically gather sentences on related top-
ics by following the procedure below:
</bodyText>
<listItem confidence="0.9948479">
1. Retrieve documents related to a set number of
topics using a search engine
2. Extract real sentences that include major sub-
topic words which are detected based on TF or
DF in the document set
3. Reduce noise in data by using heuristics to
eliminate advertisements and comment spam
4. Reduce the search space for identifying sen-
tence pairs and prepare pairs, which look fea-
sible to annotate.
</listItem>
<bodyText confidence="0.999745086956522">
Dolan and Brockett (2005) proposed a method
to narrow the range of sentence pair candidates
and collect candidates of sentence-level para-
phrases which correspond [EQUIVALENCE] in
[AGREEMENT] class in our task. It worked well
for collecting valid sentence pairs from a large
cluster which was constituted by topic-related sen-
tences. The method also seem to work well for
[CONFLICT] relations, because lexical similar-
ity based on bag-of-words (BOW) can narrow the
range of candidates with this relation as well.
We calculate the lexical similarity between the
two sentences based on BOW. We also used hy-
ponym and synonym dictionaries (Sumida et al.,
2008) and a database of relations between predi-
cate argument structures (Matsuyoshi et al., 2008)
as resources. According to our preliminary exper-
iments, unigrams of KANJI and KATAKANA ex-
pressions, single and compound nouns, verbs and
adjectives worked well as features, and we calcu-
late the similarity using cosine distance. We did
not use HIRAGANA expressions because they are
also used in function words.
</bodyText>
<sectionHeader confidence="0.874988" genericHeader="method">
4 Analyzing the Corpus
</sectionHeader>
<bodyText confidence="0.999840157894737">
Five annotators annotated semantic relations ac-
cording to our specifications in 22 document sets
as targets. We have annotated target statement
pairs with either [AGREEMENT], [CONFLICT]
or [NO RELATION]. We provided 2,303 real
sentence pairs to human annotators, and they
identified 1,375 pairs as being invalid and 928
pairs as being valid. The number of annotated
statement pairs are 1,505 ([AGREEMENT]:862,
[CONFLICT]:126, [NO RELATION]:517).
Next, to evaluate inter annotator agreement, 207
randomly selected statement pairs were annotated
by two human annotators. The annotators agreed
in their judgment for 81.6% of the examples,
which corresponds to a kappa level of 0.49. The
annotation results are evaluated by calculating re-
call and precision in which one annotation result
is treated as a gold standard and the other’s as the
output of the system, as shown in Talbe 2.
</bodyText>
<page confidence="0.998355">
152
</page>
<tableCaption confidence="0.961455">
Table 2: Inter-annotator agreement for 2 annota-
tors
</tableCaption>
<sectionHeader confidence="0.995418" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999923378378379">
The number of sentence pairs that annotators iden-
tified as invalid examples shows that around 60%
of all pairs were invalid, showing that there is still
room to improve our method of collecting sen-
tence pairs for the annotators. Developing more
effective methods of eliminating sentences pairs
that are unlikely to contain statements with plau-
sible relations is important to improve annotator
efficiency. We reviewed 50 such invalid sentence
pairs, and the results indicate two major consider-
ations: (1) negation, or antonyms have not been re-
garded as key information, and (2) verbs in KANJI
have to be handled more carefully. The polarities
of sentences in all pairs were the same although
there are sentences which can be paired up with
opposite polarities. So, we will consider the po-
larity of words and sentences as well as similarity
when considering candidate sentence pairs.
In Japanese, the words which consist of
KATAKANA expressions are generally nouns, but
those which contain KANJI can be nouns, verbs,
or adjectives. Sharing KATAKANA words was
the most common way of increasing the simi-
larity between sentences. We need to assign a
higher weight to verbs and adjectives that contain
KANJI, to more accurately calculate the similarity
between sentences.
Another approach to reducing the search space
for statement pairs is taken by Nichols et al.
(2009), who use category tags and in-article hyper-
links to organize scientific blog posts into discus-
sions on the same topic, making it easier to iden-
tify relevant statements. We are investigating the
applicability of these methods to the construction
of our Japanese corpus but suffer from the lack of
a richly-interlinked data source comparable to En-
glish scientific blogs.
</bodyText>
<sectionHeader confidence="0.999486" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999970444444444">
In this paper, we described the ongoing construc-
tion of a Japanese corpus consisting of statement
pairs annotated with semantic relations for han-
dling web arguments. We designed an annotation
scheme complete with the necessary semantic re-
lations to support the development of statement
maps that show [AGREEMENT], [CONFLICT],
and [EVIDENCE] between statements for assist-
ing users in analyzing credibility of information
in Web. We discussed the revelations made from
annotating our corpus, and discussed future direc-
tions for refining our specifications of the corpus.
We are planning to annotate relations for more
than 6,000 sentence pairs in this summer, and the
finished corpus will consist of around 10,000 sen-
tence pairs. The first release of our annotation
specifications and the corpus will be made avail-
able on the Web1 this winter.
</bodyText>
<sectionHeader confidence="0.998564" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997006">
This work is supported by the National Institute
of Information and Communications Technology
Japan.
</bodyText>
<sectionHeader confidence="0.999188" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999668891891892">
Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005.
The pascal recognising textual entailment challenge. In
Proc. of the PASCAL Challenges Workshop on RTE.
Bill Dolan and Chris Brockett. 2005. Automatical ly con-
structing a corpus of sentential paraphrases. In Proc. of
the IWP 2005, pages 9–16.
William Mann and Sandra Thompson. 1988. Rhetorical
structure theory: towards a functional theory of text or-
ganization. Text, 8(3):243–281.
Suguru Matsuyoshi, Koji Murakami, Yuji Matsumoto, , and
Kentaro Inui. 2008. A database of relations between
predicate argument structures for recognizing textual en-
tailment and contradiction. In Proc. of the ISUC 2008.
Koji Murakami, Eric Nichols, Suguru Matsuyoshi, Asuka
Sumida, Shouko Masuda, Kentaro Inui, and Yuji Mat-
sumoto. 2009. Statement map: Assisting information
credibility analysis by visualizing arguments. In Proc. of
the WICOW 2009, pages 43–50.
Eric Nichols, Koji Murakami, Kentaro Inui, and Yuji Mat-
sumoto. 2009. Constructing a scientific blog corpus for
information credibility analysis. In Proc. of the Annual
Meeting ofANLP.
Dragomir Radev, Jahna Otterbacher, and Zhu Zhang.
2003. CSTBank: Cross-document Structure Theory Bank.
http://tangra.si.umich.edu/clair/CSTBank.
Dragomir R. Radev. 2000. Common theory of informa-
tion fusion from multiple text sources step one: Cross-
document structure. In Proc. of the 1st SIGdial workshop
on Discourse and dialogue, pages 74–83.
Asuka Sumida, Naoki Yoshinaga, and Kentaro Torisawa.
2008. Boosting precision and recall of hyponymy rela-
tion acquisition from hierarchical layouts in wikipedia. In
Proc. of the LREC 2008.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in lan-
guage. Language Resources and Evaluation, 39(2-
3):165–210.
</reference>
<footnote confidence="0.934575">
1http://cl.naist.jp/stmap/corpus/ja
</footnote>
<table confidence="0.979613166666667">
Annotator A TOTAL
AGR. CON. NONE
AGR. 146 7 9 162
Anno- CON. 0 13 1 14
tator B NONE 17 4 10 31
TOTAL 163 24 20 207
</table>
<page confidence="0.995669">
153
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.151955">
<title confidence="0.979631">Annotating Semantic Relations Combining Facts and Opinions</title>
<affiliation confidence="0.95563">Institute of Science and</affiliation>
<address confidence="0.809577">8916-5, Takayama, Ikoma, Nara 630-0192</address>
<email confidence="0.373845">Prefecture</email>
<phone confidence="0.366072">1-1, Gakuen, Naka-ku, Sakai, Osaka 599-8531</phone>
<abstract confidence="0.984546166666667">part of the we are constructing a Japanese corpus annotated with the semantic relations bridging facts and opinions that are necessary for online information credibility evaluation. In this paper, we identify the semantic relations essential to this task and discuss how to efficiently collect valid examples from Web documents by splitting complex sentences into fundamental units of meaning called “statements” and annotating relations at the statement level. We present a statement annotation scheme and examine its reliability by annotating around 1,500 pairs of statements. We are preparing the corpus for release this winter.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The pascal recognising textual entailment challenge.</title>
<date>2005</date>
<booktitle>In Proc. of the PASCAL Challenges Workshop on RTE.</booktitle>
<contexts>
<context position="2783" citStr="Dagan et al., 2005" startWordPosition="431" endWordPosition="434">for each of these viewpoints. So, STATEMENT MAP can help user come to an informed conclusion. 2 Semantic Relations between Statements 2.1 Recognizing Semantic Relations To generate STATEMENT MAPs, we need to analyze a lot of online information retrieved on a given topic, and STATEMENT MAP shows users a summary with three major semantic relations. AGREEMENT to group similar opinions CONFLICT to capture differences of opinions EVIDENCE to show support for opinions Identifying logical relations between texts is the focus of Recognizing Textual Entailment (RTE). A major task of the RTE Challenge (Dagan et al., 2005) is the identification of [ENTAILMENT] or [CONTRADICTION] between Text (T) and Hypothesis (H). For this task, several corpora have been constructed over the past few years, and annotated with thousands of (T,H) pairs. While our research objective is to recognize semantic relations as well, our target domain is text from Web documents. The definition of contradiction in RTE is that T contradicts H if it is very unlikely that both T and H can be true at the same time. However, in real documents on the Web, there are many examples which are partially contradictory, or where one statement restrict</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2005</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The pascal recognising textual entailment challenge. In Proc. of the PASCAL Challenges Workshop on RTE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill Dolan</author>
<author>Chris Brockett</author>
</authors>
<title>Automatical ly constructing a corpus of sentential paraphrases.</title>
<date>2005</date>
<booktitle>In Proc. of the IWP</booktitle>
<pages>9--16</pages>
<contexts>
<context position="10460" citStr="Dolan and Brockett (2005)" startWordPosition="1669" endWordPosition="1672">semantically related, or (2) have a relation other than relations defined in Table 1. 3.2 Corpus Construction Procedure We automatically gather sentences on related topics by following the procedure below: 1. Retrieve documents related to a set number of topics using a search engine 2. Extract real sentences that include major subtopic words which are detected based on TF or DF in the document set 3. Reduce noise in data by using heuristics to eliminate advertisements and comment spam 4. Reduce the search space for identifying sentence pairs and prepare pairs, which look feasible to annotate. Dolan and Brockett (2005) proposed a method to narrow the range of sentence pair candidates and collect candidates of sentence-level paraphrases which correspond [EQUIVALENCE] in [AGREEMENT] class in our task. It worked well for collecting valid sentence pairs from a large cluster which was constituted by topic-related sentences. The method also seem to work well for [CONFLICT] relations, because lexical similarity based on bag-of-words (BOW) can narrow the range of candidates with this relation as well. We calculate the lexical similarity between the two sentences based on BOW. We also used hyponym and synonym dictio</context>
</contexts>
<marker>Dolan, Brockett, 2005</marker>
<rawString>Bill Dolan and Chris Brockett. 2005. Automatical ly constructing a corpus of sentential paraphrases. In Proc. of the IWP 2005, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Mann</author>
<author>Sandra Thompson</author>
</authors>
<title>Rhetorical structure theory: towards a functional theory of text organization.</title>
<date>1988</date>
<tech>Text, 8(3):243–281.</tech>
<contexts>
<context position="4998" citStr="Mann and Thompson, 1988" startWordPosition="792" endWordPosition="795">ANECDOTES ARE NOT EVIDENCE !Vaccinations are given around the same time children can be first diagnosed. !The plural of anecdote is not data. [EVIDENCE] b. Vaccines can trigger autism in a vulnerable subset of children. While it is difficult to assign any relation to this pair in an RTE framework, in order to construct statement maps we need to recognize a contradiction between (1a) and (1b). There is another task of recognizing relations between sentences, CST (Cross-Document Structure Theory) which was developed by Radev (2000). CST is an expanded rhetorical structure analysis based on RST (Mann and Thompson, 1988), and attempts to describe relations between two or more sentences from both single and multiple document sets. The CSTBank corpus (Radev et al., 2003) was constructed to annotate crossdocument relations. CSTBank is divided into clusters in which topically-related articles are gathered. There are 18 kinds of relations in this corpus, including [EQUIVALENCE], [ELABORATION], and [REFINEMENT]. 2.2 Facts and Opinions RTE is used to recognize logical and factual relations between sentences in a pair, and CST is used for objective expressions because newspaper articles related to the same topic are </context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>William Mann and Sandra Thompson. 1988. Rhetorical structure theory: towards a functional theory of text organization. Text, 8(3):243–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suguru Matsuyoshi</author>
<author>Koji Murakami</author>
<author>Yuji Matsumoto</author>
</authors>
<title>A database of relations between predicate argument structures for recognizing textual entailment and contradiction.</title>
<date>2008</date>
<booktitle>In Proc. of the ISUC</booktitle>
<contexts>
<context position="11180" citStr="Matsuyoshi et al., 2008" startWordPosition="1782" endWordPosition="1785">tence-level paraphrases which correspond [EQUIVALENCE] in [AGREEMENT] class in our task. It worked well for collecting valid sentence pairs from a large cluster which was constituted by topic-related sentences. The method also seem to work well for [CONFLICT] relations, because lexical similarity based on bag-of-words (BOW) can narrow the range of candidates with this relation as well. We calculate the lexical similarity between the two sentences based on BOW. We also used hyponym and synonym dictionaries (Sumida et al., 2008) and a database of relations between predicate argument structures (Matsuyoshi et al., 2008) as resources. According to our preliminary experiments, unigrams of KANJI and KATAKANA expressions, single and compound nouns, verbs and adjectives worked well as features, and we calculate the similarity using cosine distance. We did not use HIRAGANA expressions because they are also used in function words. 4 Analyzing the Corpus Five annotators annotated semantic relations according to our specifications in 22 document sets as targets. We have annotated target statement pairs with either [AGREEMENT], [CONFLICT] or [NO RELATION]. We provided 2,303 real sentence pairs to human annotators, and</context>
</contexts>
<marker>Matsuyoshi, Murakami, Matsumoto, 2008</marker>
<rawString>Suguru Matsuyoshi, Koji Murakami, Yuji Matsumoto, , and Kentaro Inui. 2008. A database of relations between predicate argument structures for recognizing textual entailment and contradiction. In Proc. of the ISUC 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koji Murakami</author>
</authors>
<title>Eric Nichols, Suguru Matsuyoshi, Asuka Sumida, Shouko Masuda, Kentaro Inui, and Yuji Matsumoto.</title>
<date>2009</date>
<booktitle>In Proc. of the WICOW</booktitle>
<pages>43--50</pages>
<marker>Murakami, 2009</marker>
<rawString>Koji Murakami, Eric Nichols, Suguru Matsuyoshi, Asuka Sumida, Shouko Masuda, Kentaro Inui, and Yuji Matsumoto. 2009. Statement map: Assisting information credibility analysis by visualizing arguments. In Proc. of the WICOW 2009, pages 43–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Nichols</author>
<author>Koji Murakami</author>
<author>Kentaro Inui</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Constructing a scientific blog corpus for information credibility analysis.</title>
<date>2009</date>
<booktitle>In Proc. of the Annual Meeting ofANLP.</booktitle>
<contexts>
<context position="13828" citStr="Nichols et al. (2009)" startWordPosition="2202" endWordPosition="2205">ite polarities. So, we will consider the polarity of words and sentences as well as similarity when considering candidate sentence pairs. In Japanese, the words which consist of KATAKANA expressions are generally nouns, but those which contain KANJI can be nouns, verbs, or adjectives. Sharing KATAKANA words was the most common way of increasing the similarity between sentences. We need to assign a higher weight to verbs and adjectives that contain KANJI, to more accurately calculate the similarity between sentences. Another approach to reducing the search space for statement pairs is taken by Nichols et al. (2009), who use category tags and in-article hyperlinks to organize scientific blog posts into discussions on the same topic, making it easier to identify relevant statements. We are investigating the applicability of these methods to the construction of our Japanese corpus but suffer from the lack of a richly-interlinked data source comparable to English scientific blogs. 6 Conclusion In this paper, we described the ongoing construction of a Japanese corpus consisting of statement pairs annotated with semantic relations for handling web arguments. We designed an annotation scheme complete with the </context>
</contexts>
<marker>Nichols, Murakami, Inui, Matsumoto, 2009</marker>
<rawString>Eric Nichols, Koji Murakami, Kentaro Inui, and Yuji Matsumoto. 2009. Constructing a scientific blog corpus for information credibility analysis. In Proc. of the Annual Meeting ofANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir Radev</author>
<author>Jahna Otterbacher</author>
<author>Zhu Zhang</author>
</authors>
<title>CSTBank: Cross-document Structure Theory Bank.</title>
<date>2003</date>
<note>http://tangra.si.umich.edu/clair/CSTBank.</note>
<contexts>
<context position="5149" citStr="Radev et al., 2003" startWordPosition="817" endWordPosition="820">Vaccines can trigger autism in a vulnerable subset of children. While it is difficult to assign any relation to this pair in an RTE framework, in order to construct statement maps we need to recognize a contradiction between (1a) and (1b). There is another task of recognizing relations between sentences, CST (Cross-Document Structure Theory) which was developed by Radev (2000). CST is an expanded rhetorical structure analysis based on RST (Mann and Thompson, 1988), and attempts to describe relations between two or more sentences from both single and multiple document sets. The CSTBank corpus (Radev et al., 2003) was constructed to annotate crossdocument relations. CSTBank is divided into clusters in which topically-related articles are gathered. There are 18 kinds of relations in this corpus, including [EQUIVALENCE], [ELABORATION], and [REFINEMENT]. 2.2 Facts and Opinions RTE is used to recognize logical and factual relations between sentences in a pair, and CST is used for objective expressions because newspaper articles related to the same topic are used as data. However, the task specifications of both RTE and CST do not cover semantic relations between opinions and facts as illustrated in the fol</context>
</contexts>
<marker>Radev, Otterbacher, Zhang, 2003</marker>
<rawString>Dragomir Radev, Jahna Otterbacher, and Zhu Zhang. 2003. CSTBank: Cross-document Structure Theory Bank. http://tangra.si.umich.edu/clair/CSTBank.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
</authors>
<title>Common theory of information fusion from multiple text sources step one: Crossdocument structure.</title>
<date>2000</date>
<booktitle>In Proc. of the 1st SIGdial workshop on Discourse and dialogue,</booktitle>
<pages>74--83</pages>
<contexts>
<context position="4909" citStr="Radev (2000)" startWordPosition="780" endWordPosition="781">ight of the evidence indicates that vaccines are not associated with autism. ANECDOTES ARE NOT EVIDENCE !Vaccinations are given around the same time children can be first diagnosed. !The plural of anecdote is not data. [EVIDENCE] b. Vaccines can trigger autism in a vulnerable subset of children. While it is difficult to assign any relation to this pair in an RTE framework, in order to construct statement maps we need to recognize a contradiction between (1a) and (1b). There is another task of recognizing relations between sentences, CST (Cross-Document Structure Theory) which was developed by Radev (2000). CST is an expanded rhetorical structure analysis based on RST (Mann and Thompson, 1988), and attempts to describe relations between two or more sentences from both single and multiple document sets. The CSTBank corpus (Radev et al., 2003) was constructed to annotate crossdocument relations. CSTBank is divided into clusters in which topically-related articles are gathered. There are 18 kinds of relations in this corpus, including [EQUIVALENCE], [ELABORATION], and [REFINEMENT]. 2.2 Facts and Opinions RTE is used to recognize logical and factual relations between sentences in a pair, and CST is</context>
</contexts>
<marker>Radev, 2000</marker>
<rawString>Dragomir R. Radev. 2000. Common theory of information fusion from multiple text sources step one: Crossdocument structure. In Proc. of the 1st SIGdial workshop on Discourse and dialogue, pages 74–83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asuka Sumida</author>
<author>Naoki Yoshinaga</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Boosting precision and recall of hyponymy relation acquisition from hierarchical layouts in wikipedia.</title>
<date>2008</date>
<booktitle>In Proc. of the LREC</booktitle>
<contexts>
<context position="11088" citStr="Sumida et al., 2008" startWordPosition="1768" endWordPosition="1771">d a method to narrow the range of sentence pair candidates and collect candidates of sentence-level paraphrases which correspond [EQUIVALENCE] in [AGREEMENT] class in our task. It worked well for collecting valid sentence pairs from a large cluster which was constituted by topic-related sentences. The method also seem to work well for [CONFLICT] relations, because lexical similarity based on bag-of-words (BOW) can narrow the range of candidates with this relation as well. We calculate the lexical similarity between the two sentences based on BOW. We also used hyponym and synonym dictionaries (Sumida et al., 2008) and a database of relations between predicate argument structures (Matsuyoshi et al., 2008) as resources. According to our preliminary experiments, unigrams of KANJI and KATAKANA expressions, single and compound nouns, verbs and adjectives worked well as features, and we calculate the similarity using cosine distance. We did not use HIRAGANA expressions because they are also used in function words. 4 Analyzing the Corpus Five annotators annotated semantic relations according to our specifications in 22 document sets as targets. We have annotated target statement pairs with either [AGREEMENT],</context>
</contexts>
<marker>Sumida, Yoshinaga, Torisawa, 2008</marker>
<rawString>Asuka Sumida, Naoki Yoshinaga, and Kentaro Torisawa. 2008. Boosting precision and recall of hyponymy relation acquisition from hierarchical layouts in wikipedia. In Proc. of the LREC 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language. Language Resources and Evaluation,</title>
<date>2005</date>
<pages>39--2</pages>
<contexts>
<context position="6180" citStr="Wiebe et al., 2005" startWordPosition="982" endWordPosition="985">ticles related to the same topic are used as data. However, the task specifications of both RTE and CST do not cover semantic relations between opinions and facts as illustrated in the following example. (2) a. There must not be a connection between vaccines and autism. b. I do believe that there is a link between vaccinations and autism. Subjective statements, such as opinions, are recently the focus of many NLP research topics, such as review analysis, opinion extraction, opinion QA, or sentiment analysis. In the corpus constructed by the MPQA Project (Multi-Perspective Question Answering) (Wiebe et al., 2005), individual expressions are marked that correspond to explicit mentions of private states, speech events, and expressive subjective elements. Our goal is to annotate instances of the three major relation classes: [AGREEMENT], [CONFLICT] and [EVIDENCE], between pairs of statements in example texts. However, each relation has a wide range, and it is very difficult to define a comprehensive annotation scheme. For example, different kinds of information can act as clues to recognize the [AGREEMENT] relations. So, we have prepared a wide spectrum of semantic relations depending on different types </context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 39(2-3):165–210.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>