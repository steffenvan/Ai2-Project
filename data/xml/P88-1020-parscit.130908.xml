<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000045">
<title confidence="0.465336">
PLANNING COHERENT
MULTISENTENTIAL TEXT
</title>
<author confidence="0.684109">
Eduard H. Hovy
</author>
<affiliation confidence="0.5575355">
USC/Information Sciences Institute
4676 Admiralty Way, Suite 1001
</affiliation>
<address confidence="0.558794">
Marina del Rey, CA 90292-6695, U.S.A.
</address>
<email confidence="0.991757">
HOVY@VAX.A.ISI.EDU
</email>
<sectionHeader confidence="0.993441" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997458">
Though most text generators are capable of sim-
ply stringing together more than one sentence,
they cannot determine which order will ensure
a coherent paragraph. A paragraph is coherent
when the information in successive sentences fol-
lows some pattern of inference or of knowledge
with which the hearer is familiar. To signal such
inferences, speakers usually use relations that link
successive sentences in fixed ways. A set of 20
relations that span most of what people usually
say in English is proposed in the Rhetorical Struc-
ture Theory of Mann and Thompson. This paper
describes the formalization of these relations and
their use in a prototype text planner that struc-
tures input elements into coherent paragraphs.
</bodyText>
<sectionHeader confidence="0.990402" genericHeader="keywords">
1 The Problem of Coherence
</sectionHeader>
<bodyText confidence="0.991618625">
The example texts in this paper are generated
by Penman, a systemic grammar-based genera-
tor with larger coverage than probably any other
existing text generator. Penman was developed
at ISI (see [Mann &amp; Matthiessen 831, [Mann 83],
[Matthiessen 841). The input to Penman is pro-
duced by PEA (Programming Enhancement Ad-
visor; see [Moore 87]), a program that inspects a
user&apos;s LISP program and suggests enhancements.
PEA is being developed to interact with the user
in order to answer his or her questions about the
suggested enhancements. Its theoretical focus is
the production of explanations over extended in-
teractions in ways that are superior to the simple
goal-tree traversal of systems such as TYRESIAS
([Davis 76]) and MYCIN ([Shortliffe 761).
</bodyText>
<subsectionHeader confidence="0.974519">
Supported by DARPA contract MDA903 81 C0335.
</subsectionHeader>
<bodyText confidence="0.993962333333333">
In answer to the question how does the system
enhance a program?, the following text (not gen-
erated by Penman) is not satisfactory:
</bodyText>
<construct confidence="0.8316068">
(a). The system performs the enhance-
Before that, the system resolves
conflicts. First, the system asks the
user to tell it the characteristic of the
program to be enhanced. The system
applies transformations to the program.
It confirms the enhancement with_the
user. It scans the program in order to
find opportunities to apply transforma-
tions to the program.
</construct>
<bodyText confidence="0.986072913043478">
...because you have to work too hard to make
sense of it. In contrast, using the same propo-
sitions (now rearranged and linked with appro-
priate connectives), paragraph (b) (generated by
Penman) is far easier to understand:
(b). The system asks the user to tell
it the characteristic of the program to
be enhanced. Then the system applies
transformations to the program. In par-
ticular, the system scans the program
in order to find opportunities to apply
transformations to the program. Then
the system resolves conflicts. It confirms
the enhancement with the user. Finally,
it performs the enhancement.
Clearly, you do not get coherent text simply by
stringing together sentences, even if they are re-
lated — note especially the underlined text in (b)
and its corresponding three propositions in (a).
The goal of this paper is to describe a method of
planning paragraphs to be coherent while avoiding
unintended spurious effects that result from the
juxtaposition of unrelated pieces of text.
</bodyText>
<page confidence="0.999353">
163
</page>
<sectionHeader confidence="0.966465" genericHeader="introduction">
2 Text Structuring
</sectionHeader>
<bodyText confidence="0.999563285714286">
This planning work, which can be called text
structuring, must obviously be done before the
actual generating of language can begin. Text
structuring is one of a number of pre-generation
text planning tasks. For some of the other tasks
Penman has special-purpose domain-specific solu-
tions. They include:
</bodyText>
<listItem confidence="0.990419454545454">
• aggregation: determining, for input ele-
ments, the appropriate level of detail (see
Iliovy 871), the scoping of sentences, and the
use of connectives
• reference: determining appropriate ways of
referring to items (see EAppelt 87a, 87b1)
• hypotheticals: determining the introduc-
tion, scope, and closing of hypothesis contexts
(spans of text in which some values are as-
sumed, as in &amp;quot;if you want to go to the game,
then ... &amp;quot;)
</listItem>
<bodyText confidence="0.999948861111111">
The problem of text coherence can be character-
ized in specific terms as follows. Assuming that in-
put elements are sentence- or clause-sized chunks
of representation, the permutation set of the input
elements defines the space of possible paragraphs.
A simplistic, brute-force way to achieve coherent
text would be to search this space and pick out
the coherent paragraphs. This search would be
factorially expensive. For example, in paragraph
(b) above, the 7 input clusters received from PEA
provide 7! = 5,040 candidate paragraphs. How-
ever, by utilizing the constraints imposed by co-
herence, one can formulate operators that guide
the search and significantly limit the search to a
manageable size. In the example, the operators
described below produced only 3 candidate para-
graphs. Then, from this set of remaining candi-
dates, the best paragraph can be found by apply-
ing a relatively simple evaluation metric.
The contention of this paper is that, exercis-
ing proper care, the coherence relations that hold
between successive pieces of text can be formu-
lated as the abovementioned search operators and
used in a hierarchical-expansion planner to limit
the search and to produce structures describing
the coherent paragraphs.
The illustrate this contention, the Penman text
structurer is a simplified top-down planner (as de-
scribed first by Pacerdoti 771). It uses a formal-
ized version of the relations of Rhetorical Struc-
ture Theory (see immediately below) as plans. Its
output is one (or more) tree(s) that describe the
structure(s) of coherent paragraphs built from the
input elements. Input elements are the leaves of
the tree(s); they are sent to the Penman generator
to be transformed into sentences.
</bodyText>
<sectionHeader confidence="0.981746" genericHeader="method">
3 Previous Approaches
</sectionHeader>
<bodyText confidence="0.999960644444445">
The heart of the problem is obviously coherence.
Coherent text can be defined as text in which the
hearer knows how each part of the text relates to
the whole; i.e., (a) the hearer knows why it is said,
and (b) the hearer can relate the semantics of each
part to a single overarching framework.
In 1978, Hobbs ((Hobbs 78, 79, 82]) recognized
that in coherent text successive pieces of text are
related in a specified set of ways. He produced
a set of relations organized into four categories,
which he postulated as the four types of phenom-
ena that occur during conversation. His argument,
unfortunately, contains a number of shortcomings;
not only is the categorization not well-motivated,
but the list of relations is incomplete.
In her thesis work, McKeown took a different
approach ([McKeown 82]). She defined a set of
relatively static schemas that represent the struc-
ture of stereotypical paragraphs for describing ob-
jects. In essence, these schemas are paragraph
templates; coherence is enforced by the correct
nesting and filling-in of templates. No explicit the-
ory of coherence was offered.
Mann and Thompson, alter a wide-ranging
study involving hundreds of paragraphs, proposed
that a set of 20 relations suffice to represent the
relations that hold within the texts that normally
occur in English ([Mann di Thompson 87, 86,
83]). These relations, called RST (rhetorical struc-
ture theory), are used recursively; the assumption
(never explicitly stated) is that a paragraph is only
coherent if all its parts can eventually be made to
fit into one overarching relation. The enterprise
was completely descriptive; no formal definition
of the relations or justification for their complete-
ness were given. However, the relations do include
most of Hobbs&apos;s relations and support McKeown&apos;s
schemas.
A number of similar descriptions exist. The de-
scription of how parts of purposive text can re-
late goes back at least to Aristotle ((Aristotle
Both Grimes and Shepherd categorize typical in-
tersentential relations ([Grimes 75] and [Shepherd
26]). Hovy ([llovy 86]) describes a program that
uses some relations to slant text.
</bodyText>
<page confidence="0.997359">
164
</page>
<sectionHeader confidence="0.99549" genericHeader="method">
4 Formalizing RST Relations
</sectionHeader>
<bodyText confidence="0.997557489361702">
As defined by Mann and Thompson, RST rela-
tions hold between two successive pieces of text
(at the lowest level, between two clauses; at the
highest level, between two parts that make up
a paragraph)&apos; . Therefore, each relation has two
parts, a nucleus and a satellite. To determine the
applicability of the relation, each part has a set
of constraints on the entities that can be related.
Relations may also have requirements on the com-
bination of the two parts. In addition, each rela-
tion has an effect field, which is intended to denote
the conditions which the speaker is attempting to
achieve.
In formalizing these relations and using them
generatively to plan paragraphs, rather than ana-
lytically to describe paragraph structure, a shift of
focus is required. Relations must be seen as plans
— the operators that guide the search through the
permutation space. The nucleus and satellite con-
straints become requirements that must be met by
any piece of text before it can be used in the re-
lation (i.e., before it can be coherently juxtaposed
with the preceding text). The effect field contains
a description of the intended effect of the relation
(i.e., the goal that the plan achieves, if properly
executed). Since the goals in generation are com-
municative, the intended effect must be seen as
the inferences that the speaker is licensed to make
about the hearer&apos;s knowledge after the successful
completion of the relation.
Since the relations are used as plans, and since
their satellite and nucleus constraints must be re-
formulated as subgoals to the structurer, these
constraints are best represented in terms of the
communicative intent of the speaker. That is, they
are best represented in terms of what the hearer
will know — i.e., what inferences the hearer would
run — upon being told the nucleus or satellite
filler.
As it turns out, suitable terms for this purpose
are provided by the formal theory of rational inter-
action currently being developed by, among oth-
ers, Cohen, Levesque, and Perrault. For example,
in (Cohen &amp; Levesque 851, Cohen and Levesque
present a proof that the indirect speech act of re-
questing can be derived from the following basic
modal operators
</bodyText>
<listItem confidence="0.96724">
• (BEL x p) — p follows from x&apos;s beliefs
</listItem>
<bodyText confidence="0.95784725">
&apos;This is not strictly true; a small number of relations,
such as Sequence, relate more than two pieces of text.
However, for ease of use, they have been implemented as
binary relations in the structurer.
</bodyText>
<listItem confidence="0.9906488">
• (BMB x y p) — p follows from x&apos;s beliefs
about what x and y mutually believe
• (GOAL x p) — p follows from x&apos;s goals
• (AFTER a p) — p is true in all courses of
events after action a
</listItem>
<bodyText confidence="0.999167857142857">
as well as from a few other operators such as AND
and OR. They then define summaries as, essen-
tially, speech act operators with activating condi-
tions (gates) and effects. These summaries closely
resemble, in structure, the RST plans described
here, with gates corresponding to satellite and nu-
cleus constraints and effects to intended effects.
</bodyText>
<sectionHeader confidence="0.967993" genericHeader="method">
5 An Example
</sectionHeader>
<bodyText confidence="0.826193333333333">
The RST relation Purpose expresses the relation
between an action and its intended result:
Purpose
</bodyText>
<subsectionHeader confidence="0.492543">
Nucleus Constraints:
</subsectionHeader>
<listItem confidence="0.9554376">
1. (BMB S H (ACTION ?act-1))
2. (BMB S H (ACTOR ?act-1 ?agt-1))
Satellite Constraints:
1. (BMB S H (STATE ?state-1))
2. (BMB S H (GOAL ?agt-1 ?state-1))
3. (BMB S H (RESULT ?act-1 ?act-2))
4. (BMB S H (OBJ ?act-2 ?state-1))
Intended Effects:
1. (BMB S H (BEL ?agt-1 (RESULT ?act-1 ?state-1)))
2. (BMB S H (PURPOSE ?act-1 ?state-1))
</listItem>
<bodyText confidence="0.99391975">
For example, when used to produce the sentence
The system scans the program in order to find op-
portunities to apply transformations to the pro-
gram, this relation is instantiated as
</bodyText>
<figure confidence="0.792141444444444">
Purpose
Nucleus Constraints:
1. (BMB S H (ACTION SCAN-1))
The program is scanned
2. (BMB S H (ACTOR SCAN-1 SYS-1))
The system scans it
Satellite Constraints:
1. (BMB S H (STATE OPP-1))
Opportunities to apply transformations exist
</figure>
<sectionHeader confidence="0.420038" genericHeader="method">
2. (BMB S H (GOAL SYS-1 OPP-1))
</sectionHeader>
<bodyText confidence="0.5425">
The system &apos;wants&amp;quot; to find them
</bodyText>
<listItem confidence="0.553415166666667">
3. (BMB S H (RESULT SCAN-1 FIND-1))
Scanning will result in finding
4. (BMB S H (OBJ FIND-1 OPP-1))
the opportunities
Intended Effects:
1. (BMB S H (BEL SYS-1
</listItem>
<note confidence="0.512675666666667">
(RESULT SCAN-1 OPP-1)))
The system &amp;quot;believes * that scanning
will disclose the opportunities
</note>
<sectionHeader confidence="0.686955" genericHeader="method">
2. (BMB S H (PURPOSE SCAN-1 OPP-1))
</sectionHeader>
<bodyText confidence="0.606704">
This is the purpose of the scanning
</bodyText>
<page confidence="0.95852">
165
</page>
<figure confidence="0.999213058823529">
SEGUEIC AUCLEUS-&lt;IMPUTREC with
AUCLEUS-(IMPATREC with (RI P4 E6))64
NRCLEUS-4/11PUTREC with (RI C4),(C)
e(SATELLITE-(IAPUTREC with (FI ES)) (44)
• tiLICLEVS-PURPOS
SATELLITE-ELROORATIO
AUCLEUS-4IRPUTREC with (52),(0
I
SATELLITE-SEC/UDC
t1(
AUCLEUS-(/APUTREC with (C2 04)) (f)
SATELLITE-5E1VDC
0(
SATELLITE-SERUM
0(
e(SATELLITS-(IMPUTREC with (P3),
(3)
</figure>
<figureCaption confidence="0.999978">
Figure 1: Paragraph Structure Tree
</figureCaption>
<bodyText confidence="0.916894375">
The elements SCAN-I, OPP-1, etc., are part
of a network provided to the Penman structurer
by PEA. These elements are defined as propo-
sitions in a property-inheritance network of the
usual kind written in NIKL aSchmolze dc Lipkis
831, [Kaczmarek et al. 861), a descendant of KL..
ONE Prachman 78D. Some input for this exam-
ple sentence is:
</bodyText>
<table confidence="0.989605555555556">
(PEA-SYSTEM SYS-1) (OPPORTUNITY OPP-1)
(PROGRAM PROG-1) (ENABLEMENT ENAB-6)
(SCAN SCAN-1) (DOMAIN ENAB-6 OPP-1)
(ACTOR SCAN-1 SYS-1) (RANGE ENAB-6 APPLY-3)
(OBJ SCAN-1 PROG-1) (APPLY APPLY-3)
(RESULT SCAN-1 FIND-1) (ACTOR APPLY-3 SYS-1)
(FIND FIND-1) (OBJ APPLY-3 TRANS-2)
(ACTOR FIND-1 SYS-1) (RECIP APPLY-3 PROG-1)
(OBJ FIND-1 OPP-1) (TRANSFORMATION TRANS-2)
</table>
<bodyText confidence="0.988865733333334">
The relations are used as plans; their intended
effects are interpreted as the goals they achieve.
In other words, in order to bring about the state
in which both speaker and hearer know that OPP-1
is the purpose of SCAN-1 (and know that they both
know it, etc.), the structurer uses Purpose as a
plan and tries to satisfy its constraints.
In this system, constraints and goals are inter-
changable; for example, in the event that (RESULT
SCAN-1 FIND-1) is believed not known by the
hearer, satellite constraint 3 of the Purpose re-
lation simply becomes the goal to achieve (BMB S
H (RESULT SCAN-1 FIND-1)). Similarly, the propo-
sitions Ow s H (RESULT SCAN-1 ?ACT-2)) (DMB S
H (08.1 ?ACT-2 oPP-1)) are interpreted as the goal
to find some element that could legitimately take
the place of ?ACT-2.
In order to enable the relations to nest recur-
sively, some relations&apos; nucleuses and satellites con-
tain requirements that specify additional relations,
such as examples, contrasts, etc. Of course, these
additional requirements may only be included if
such material can coherently follow the content of
the nucleus or satellite. The question of ordering
such additional constituents is still under investi-
gation. The question of whether such additional
material should be included at all is not addressed;
the structurer tries to say everything it is given.
The structurer produces all coherent paragraphs
(that is, coherent as defined by the relations) that
satisfy the given goal(s) for any set of input ele-
ments. For example, paragraph (b) is produced to
satisfy the initial goal (BY13 S H (SEQUENCE ASK-1
?NEM). This goal is produced by PEA, to-
gether with the appropriate representation ele-
ments (ASK-I. SCAN-1, etc.) in response to the
question how does the system enhance a program?.
Different initial goals will result in different para-
graphs.
Each paragraph is represented as a tree in which
branch points are RST relations and leaves are
input elements. Figure 1 is the tree for para-
graph (b). It contains the relations Sequence
(signalled by &apos;then&apos; and &apos;finally&amp;quot;), Elaboration
(&amp;quot;in particular&amp;quot;), and Purpose (&amp;quot;in order to&amp;quot;).
In the corresponding paragraph produced by Pen-
man, the relations&apos; characteristic words or phrases
(boldfaced below) appear between the blocks of
text they relate:
[The system asks the user to tell it
the characteristic of the program to be
enhanced.](.) Then [the system applies
transformations to the progran2.](0 In
particular, [the system scans the pro-
gram] o in order to [find opportu-
nities to apply transformations to the
program.[(d) Then [the system resolves
confiicts.](e) [It confirms the enhance-
ment with the user.[().) Finally, [it per-
forms the enhancement.ho
</bodyText>
<page confidence="0.994544">
166
</page>
<figure confidence="0.9930662">
input
sentence
generator
--ot
update agenda
choose final plan
get next bud
RST relations
expand bud
grow tree
</figure>
<figureCaption confidence="0.993488">
Figure 2: Hierarchical Planning Structurer
</figureCaption>
<subsectionHeader confidence="0.660712">
6-The Structurer
</subsectionHeader>
<bodyText confidence="0.999937555555555">
As stated above, the structurer is a simplified
top-down hierarchical expansion planner (see Fig-
ure 2). It operates as follows: given one or more
communicative goals, it finds RST relations whose
intended effects match (some of) these goals; it
then inspects which of the input elements match
the nucleus and subgoal constraints for each re-
lation. Unmatched constraints become subgoals
which are posted on an agenda for the next level
of planning. The tree can be expanded in either
depth-first or breadth-first fashion. Eventually,
the structuring process bottoms out when either:
(a) all input elements have been used and unsatis-
fied subgoals remain (in which case the structurer
could request more input with desired properties
from the encapsulating system); or (b) all goals
are satisfied. If more than one plan (i.e., para-
graph tree structure) is produced, the results are
ordered by preferring trees with the minimum un-
used number of input elements and the minimum
number of remaining unsatisfied subgoals. The
best tree is then traversed in left-to-right order;
leaves provide input to Penman to be generated
in English and relations at branch points provide
typical interclausal relation words or phrases. In
this way the structurer performs top-down goal re-
finement down to the level of the input elements.
</bodyText>
<sectionHeader confidence="0.917076" genericHeader="evaluation">
7 Shortcomings and Further
Work
</sectionHeader>
<bodyText confidence="0.985960565217391">
This work is also being tested in a completely sep-
arate domain: the generation of text in a multi-
media system that answers database queries. Pen-
man produces the following description of the ship
Knox (where CTG 070.10 designates a group of
ships):
(c). Knox is en route in order to ren-
dezvous with CTG 070.10, arriving in
Pearl Harbor on 4/24, for port visit until
4/30.
In this text, each clause (en route, rendezvous,
arrive, visit) is a separate input element; the
structurer linked them using the relations Se-
quence and Purpose (the same Purpose as
shown above; it is signalled by &amp;quot;in order to&amp;quot;).
However, Penman can also be made to produce
(d). Knox is en route in order to ren-
dezvous with CTG 070.10. It will arrive
in Pearl Harbor on 4/24. It will be on
port visit until 4/30.
The problem is clear: how should sentences in
the paragraph be scoped? At present, avoiding
any claims about a theory, the structurer can feed
</bodyText>
<page confidence="0.994898">
167
</page>
<bodyText confidence="0.999943756097561">
Penman either extreme: make everything one sen-
tence, or make each input element a separate sen-
tence. However, neither extreme is satisfactory;
as is clear from paragraph (b), &amp;quot;short&amp;quot; spans of
text can be linked and &amp;quot;long&amp;quot; ones left separate.
A simple way to implement this is to count the
number of leaves under each branch (nucleus or
satellite) in the paragraph structure tree.
Another shortcoming is the treatment of input
elements as indivisible entities. This shortcoming
is a result of factoring out the problem of aggre-
gation as a separate text planning task. Chunking
together input elements (to eliminate detail) or
taking them apart (to be more detailed) has re-
ceived scant mention — see Ellovy 871, and for the
related problem of paraphrase see [Schank 75] —
but this task should interact with text structur-
ing in order to provide text that is both optimally
detailed and coherent.
At the present time, only about 20% of the RST
relations have been formalized to the extent that
they can be used by the structurer. This formal-
ization process is difficult, because it goes hand-
in-hand with the development of terms with which
to characterize the relations&apos; goals/constraints.
Though the formalization can never be completely
finalized — who can hope to represent something
like motivation or justification complete with all
ramifications? — the hope is that, by having the
requirements stated in rather basic terms, the re-
lations will be easily adaptable to any new repre-
sentation scheme and domain. (It should be noted,
of course, that, to be useful, these formalizations
need only be as specific and as detailed as the do-
main model and representation requires.) In ad-
dition, the availability of a set of communicative
goals more detailed than just say or ask (for ex-
ample), should make it easier for programs that
require output text to interface with the gener-
ator. This is one focus of current text planning
work at ISI.
</bodyText>
<sectionHeader confidence="0.999189" genericHeader="acknowledgments">
8 Acknowledgments
</sectionHeader>
<bodyText confidence="0.917470666666667">
For help with Penman, Robert Albano, John Bate-
man, Bob Kasper, Christian Matthiessen, Lynn
Poulton, and Richard Whitney. For help with the
input, Bill Mann and Johanna Moore. For general
comments, all the above, and Cede Paris, Stuart
Shapiro, and Norm Sondheimer.
</bodyText>
<sectionHeader confidence="0.978541" genericHeader="references">
9 References
</sectionHeader>
<reference confidence="0.995852772727273">
1. Appek, D.E., 1987a.
A Computational Model of Referring, SRI
Technical Note 409.
2. Appelt, D.E., 1987b.
Towards a Plan-Based Theory of Referring
Actions, in Natural Language Generation:
Recent Advances in Artificial Intelligence,
Psychology, and Linguistics, Kempen, G.
(ed), (Kluwer Academic Publishers, Boston)
63-70.
3. Aristotle, 1954.
The Rhetoric, in The Rhetoric and the Po-
etics of Aristotle, W. Rhys Roberts (trans),
(Random House, New York).
4. Brac.hman, R.J., 1987.
A Structural Paradigm for Representing
Knowledge, Ph.D. dissertation, Harvard Uni-
versity; also BBN Research Report 3605.
5. Cohen, P.R. &amp; Levesque, H.J., 1985.
Speech Acts and Rationality, Proceedings of
the ACL Conference, Chicago (49-59).
6. Davis, R., 1976.
Applications of Meta-Level Knowledge to
the Constructions, Maintenance, and Use of
Large Knowledge Bases, Ph.D. dissertation,
Stanford University.
7. Grimes, J.E., 1975.
The Thread of Discourse (Mouton, The
Hague).
8. Hobbs, J.R., 1978.
Why is Discourse Coherent?, SRI Technical
Note 176.
9. Hobbs, J.R., 1979.
Coherence and Coreference, in Cognitive Sci-
ence 3(1), 67-90.
10. Hobbs, J.R., 1982.
Coherence in Discourse, in Strategies for Nat-
ural Language Processing, Lehnert, W.G. &amp;
Ringle, M.H. (eds), (Lawrence Erlbaum As-
sociates, Hillsdale NJ) 223-243.
11. Hovy, E.H., 1986.
Putting Affect into Text, Proceedings of
the Cognitive Science Society Conference,
Amherst (669-671).
</reference>
<page confidence="0.97965">
168
</page>
<reference confidence="0.995854803278689">
12. Hovy, E.H., 1987.
Interpretation in Generation, Proceedings of
the AAAI Conference, Seattle (545-549).
13. Kaczmarek, T.S., Bates, R. &amp; Robins, G.,
1986.
Recent Developments in NIKL, Proceedings
of the AAAI Conference, Philadelphia (978-
985).
14. Mann, W.C., 1983.
An Overview of the Nigel Text Generation
Grammar, USC/Information Sciences Insti-
tute Research Report RR-83-113.
15. Mann, W.C. &amp; Matthiessen, C.M.I.M., 1983.
Nigel: A Systemic Grammar for Text Gen-
eration, USC/Information Sciences Institute
Research Report RR-83-105.
16. Mann, W.C. &amp; Thompson, S.A., 1983.
Relational Propositions in Discourse, USC/-
Information Sciences Institute Research Re-
port RR-83-115.
17. Mann, W.C. &amp; Thompson, S.A., 1986.
Rhetorical Structure Theory: Description
and Construction of Text Structures, in Nat-
ural Language Generation: New Results in
Artificial Intelligence, Psychology, and Lin-
guistics, Kempen, G. (ed), (Kluwer Academic
Publishers, Dordrecht, Boston MA) 279-300.
18. Mann, W.C. &amp; Thompson, S.A., 1987.
Rhetorical Structure Theory: A Theory of
Text Organization, USC/Information Sci-
ences Institute Research Report RR-87-190.
19. Matthiessen, C.M.I.M., 1984.
Systemic Grammar in Computation: the
Nigel Case, USC/Information Sciences Insti-
tute Research Report RR-84-121.
20. McKeown, K.R., 1982.
Generating Natural Language Text in Re-
sponse to Questions about Database Queries,
Ph.D. dissertation, University Of Pennsylva-
nia.
21. Moore, J.D., 1988.
Enhanced Explanations in Expert and
Advice-Giving Systems, USC/Information
Sciences Institute Research Report (forth-
coming).
22. Sacerdoti, E., 1977.
A Structure for Plans and Behavior (North-
Holland, Amsterdam).
23. Schank, R.C., 1975.
Conceptual Information Processing, (North-
Holland, Amsterdam).
24. Schmolze, J.G. &amp; Lipkis, T.A., 1983.
Classification in the KL-ONE Knowledge
Representation System, Proceedings of the IJ-
CAI Conference, Karlsruhe (330-332).
25. Shepherd, H.R., 1926.
The Fine Art of Writing, (The Macmillan Co,
New York).
26. Shortliffe, E.H., 1976.
Computer-Based Medical Consultations:
MYCIN.
</reference>
<page confidence="0.998777">
169
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.999193">PLANNING COHERENT MULTISENTENTIAL TEXT</title>
<author confidence="0.999992">Eduard H Hovy</author>
<affiliation confidence="0.999852">USC/Information Sciences Institute</affiliation>
<address confidence="0.9988155">Way, Suite 1001 Marina del Rey, CA 90292-6695, U.S.A.</address>
<email confidence="0.993591">HOVY@VAX.A.ISI.EDU</email>
<abstract confidence="0.99669559375">Though most text generators are capable of simply stringing together more than one sentence, they cannot determine which order will ensure a coherent paragraph. A paragraph is coherent when the information in successive sentences follows some pattern of inference or of knowledge with which the hearer is familiar. To signal such inferences, speakers usually use relations that link successive sentences in fixed ways. A set of 20 relations that span most of what people usually say in English is proposed in the Rhetorical Struc- Theory of Mann and Thompson. This the formalization of these relations their use in a prototype text planner that structures input elements into coherent paragraphs. 1 The Problem of Coherence The example texts in this paper are generated by Penman, a systemic grammar-based generacoverage than probably any other existing text generator. Penman was developed at ISI (see [Mann &amp; Matthiessen 831, [Mann 83], [Matthiessen 841). The input to Penman is produced by PEA (Programming Enhancement Advisor; see [Moore 87]), a program that inspects a user&apos;s LISP program and suggests enhancements. PEA is being developed to interact with the user in order to answer his or her questions about the suggested enhancements. Its theoretical focus is the production of explanations over extended interactions in ways that are superior to the simple goal-tree traversal of systems such as TYRESIAS</abstract>
<note confidence="0.811474">Davis 76]) and MYCIN ([Shortliffe 761). Supported by DARPA contract MDA903 81 C0335.</note>
<abstract confidence="0.993231149038461">answer to question does the system a program?, the text (not generated by Penman) is not satisfactory: The system performs the enhancethat, resolves conflicts.First, the system asks the user to tell it the characteristic of the program to be enhanced. The system applies transformations to the program. It confirms the enhancement with_the user. It scans the program in order to find opportunities to apply transformations to the program. you have work too hard to make sense of it. In contrast, using the same propositions (now rearranged and linked with appropriate connectives), paragraph (b) (generated by Penman) is far easier to understand: (b). The system asks the user to tell it the characteristic of the program to be enhanced. Then the system applies transformations to the program. In particular, the system scans the program in order to find opportunities to apply transformations to the program. Then resolvesconflicts.It confirms the enhancement with the user. Finally, it performs the enhancement. Clearly, you do not get coherent text simply by stringing together sentences, even if they are related — note especially the underlined text in (b) and its corresponding three propositions in (a). The goal of this paper is to describe a method of planning paragraphs to be coherent while avoiding unintended spurious effects that result from the unrelated pieces of text. 163 2 Text Structuring planning work, which can be called obviously be done before the actual generating of language can begin. Text structuring is one of a number of pre-generation text planning tasks. For some of the other tasks Penman has special-purpose domain-specific solutions. They include: • aggregation: determining, for input elements, the appropriate level of detail (see Iliovy 871), the scoping of sentences, and the use of connectives • reference: determining appropriate ways of referring to items (see EAppelt 87a, 87b1) • hypotheticals: determining the introduction, scope, and closing of hypothesis contexts (spans of text in which some values are assumed, as in &amp;quot;if you want to go to the game, then ... &amp;quot;) The problem of text coherence can be characterized in specific terms as follows. Assuming that input elements are sentenceor clause-sized chunks of representation, the permutation set of the input elements defines the space of possible paragraphs. A simplistic, brute-force way to achieve coherent text would be to search this space and pick out the coherent paragraphs. This search would be factorially expensive. For example, in paragraph (b) above, the 7 input clusters received from PEA provide 7! = 5,040 candidate paragraphs. However, by utilizing the constraints imposed by coherence, one can formulate operators that guide the search and significantly limit the search to a manageable size. In the example, the operators described below produced only 3 candidate paragraphs. Then, from this set of remaining candidates, the best paragraph can be found by applying a relatively simple evaluation metric. The contention of this paper is that, exercising proper care, the coherence relations that hold between successive pieces of text can be formulated as the abovementioned search operators and used in a hierarchical-expansion planner to limit the search and to produce structures describing the coherent paragraphs. The illustrate this contention, the Penman text structurer is a simplified top-down planner (as described first by Pacerdoti 771). It uses a formalized version of the relations of Rhetorical Structure Theory (see immediately below) as plans. Its output is one (or more) tree(s) that describe the structure(s) of coherent paragraphs built from the input elements. Input elements are the leaves of the tree(s); they are sent to the Penman generator to be transformed into sentences. 3 Previous Approaches heart of the problem is obviously Coherent text can be defined as text in which the hearer knows how each part of the text relates to the whole; i.e., (a) the hearer knows why it is said, and (b) the hearer can relate the semantics of each part to a single overarching framework. In 1978, Hobbs ((Hobbs 78, 79, 82]) recognized that in coherent text successive pieces of text are related in a specified set of ways. He produced a set of relations organized into four categories, which he postulated as the four types of phenomena that occur during conversation. His argument, unfortunately, contains a number of shortcomings; not only is the categorization not well-motivated, but the list of relations is incomplete. In her thesis work, McKeown took a different approach ([McKeown 82]). She defined a set of relatively static schemas that represent the structure of stereotypical paragraphs for describing objects. In essence, these schemas are paragraph templates; coherence is enforced by the correct nesting and filling-in of templates. No explicit theory of coherence was offered. Mann and Thompson, alter a wide-ranging study involving hundreds of paragraphs, proposed that a set of 20 relations suffice to represent the relations that hold within the texts that normally in English ([Mann 87, 86, 83]). These relations, called RST (rhetorical structure theory), are used recursively; the assumption (never explicitly stated) is that a paragraph is only coherent if all its parts can eventually be made to fit into one overarching relation. The enterprise was completely descriptive; no formal definition of the relations or justification for their completeness were given. However, the relations do include most of Hobbs&apos;s relations and support McKeown&apos;s schemas. A number of similar descriptions exist. The description of how parts of purposive text can relate goes back at least to Aristotle ((Aristotle Both Grimes and Shepherd categorize typical intersentential relations ([Grimes 75] and [Shepherd 26]). Hovy ([llovy 86]) describes a program that uses some relations to slant text. 164 4 Formalizing RST Relations by Mann and Thompson, RST relations hold between two successive pieces of text (at the lowest level, between two clauses; at the highest level, between two parts that make up a paragraph)&apos; . Therefore, each relation has two a nucleus and a determine the applicability of the relation, each part has a set of constraints on the entities that can be related. Relations may also have requirements on the combination of the two parts. In addition, each relation has an effect field, which is intended to denote the conditions which the speaker is attempting to achieve. In formalizing these relations and using them generatively to plan paragraphs, rather than analytically to describe paragraph structure, a shift of focus is required. Relations must be seen as plans — the operators that guide the search through the permutation space. The nucleus and satellite constraints become requirements that must be met by any piece of text before it can be used in the relation (i.e., before it can be coherently juxtaposed with the preceding text). The effect field contains a description of the intended effect of the relation (i.e., the goal that the plan achieves, if properly executed). Since the goals in generation are communicative, the intended effect must be seen as the inferences that the speaker is licensed to make about the hearer&apos;s knowledge after the successful completion of the relation. Since the relations are used as plans, and since their satellite and nucleus constraints must be reformulated as subgoals to the structurer, these constraints are best represented in terms of the communicative intent of the speaker. That is, they are best represented in terms of what the hearer will know — i.e., what inferences the hearer would run — upon being told the nucleus or satellite filler. As it turns out, suitable terms for this purpose are provided by the formal theory of rational interaction currently being developed by, among others, Cohen, Levesque, and Perrault. For example, in (Cohen &amp; Levesque 851, Cohen and Levesque present a proof that the indirect speech act of requesting can be derived from the following basic modal operators • (BEL x p) — p follows from x&apos;s beliefs &apos;This is not strictly true; a small number of relations, such as Sequence, relate more than two pieces of text. for use, they have been implemented as binary relations in the structurer. (BMB x — p follows from x&apos;s beliefs what x and believe • (GOAL x p) — p follows from x&apos;s goals • (AFTER a p) — p is true in all courses of events after action a as well as from a few other operators such as AND and OR. They then define summaries as, essentially, speech act operators with activating condisummaries closely resemble, in structure, the RST plans described here, with gates corresponding to satellite and nucleus constraints and effects to intended effects. Example The RST relation Purpose expresses the relation between an action and its intended result: Purpose</abstract>
<note confidence="0.950786636363636">Nucleus Constraints: 1. (BMB S H (ACTION ?act-1)) 2. (BMB S H (ACTOR ?act-1 ?agt-1)) Satellite Constraints: 1. (BMB S H (STATE ?state-1)) 2. (BMB S H (GOAL ?agt-1 ?state-1)) 3. (BMB S H (RESULT ?act-1 ?act-2)) 4. (BMB S H (OBJ ?act-2 ?state-1)) Intended Effects: 1. (BMB S H (BEL ?agt-1 (RESULT ?act-1 ?state-1))) 2. (BMB S H (PURPOSE ?act-1 ?state-1</note>
<abstract confidence="0.942564428571429">For example, when used to produce the sentence scans the program in order to find opportunities to apply transformations to the prorelation is instantiated Purpose Nucleus Constraints: 1. (BMB S H (ACTION SCAN-1)) The program is scanned (BMB S H (ACTOR SCAN-1 The system scans it 1. (BMB S H (STATE OPP-1)) Opportunities to apply transformations exist 2. (BMB S H (GOAL SYS-1 OPP-1)) system &apos;wants&amp;quot; to 3. (BMB S H (RESULT SCAN-1 FIND-1)) Scanning will result in finding 4. (BMB S H (OBJ FIND-1 OPP-1)) the opportunities Intended Effects: 1. (BMB S H (BEL SYS-1 (RESULT SCAN-1 OPP-1))) The system &amp;quot;believes * that scanning will disclose the opportunities 2. (BMB S H (PURPOSE SCAN-1 OPP-1)) is purpose of the scanning 165 SEGUEIC AUCLEUS-&lt;IMPUTREC with AUCLEUS-(IMPATREC with (RI P4 E6))64 with (RI with (FI ES)) (44) • tiLICLEVS-PURPOS SATELLITE-ELROORATIO AUCLEUS-4IRPUTREC with (52),(0 SATELLITE-SEC/UDC with (C2 04)) SATELLITE-5E1VDC 0( SATELLITE-SERUM 0( with (P3), Figure 1: Paragraph Structure Tree The elements SCAN-I, OPP-1, etc., are part of a network provided to the Penman structurer by PEA. These elements are defined as propoin a property-inheritance network the kind written in aSchmolze [Kaczmarek et al. 861), a of KL.. ONE Prachman 78D. Some input for this example sentence is:</abstract>
<note confidence="0.893163888888889">(PEA-SYSTEM SYS-1) (OPPORTUNITY OPP-1) (PROGRAM PROG-1) (ENABLEMENT ENAB-6) (SCAN SCAN-1) (DOMAIN ENAB-6 OPP-1) (ACTOR SCAN-1 SYS-1) (RANGE ENAB-6 APPLY-3) (OBJ SCAN-1 PROG-1) (APPLY APPLY-3) (RESULT SCAN-1 FIND-1) (ACTOR APPLY-3 SYS-1) (FIND FIND-1) (OBJ APPLY-3 TRANS-2) (ACTOR FIND-1 SYS-1) (RECIP APPLY-3 PROG-1) (OBJ FIND-1 OPP-1) (TRANSFORMATION TRANS-2)</note>
<abstract confidence="0.9984015748503">The relations are used as plans; their intended effects are interpreted as the goals they achieve. In other words, in order to bring about the state which both speaker and hearer know that the purpose of know that they both know it, etc.), the structurer uses Purpose as a plan and tries to satisfy its constraints. In this system, constraints and goals are interfor example, in the event that believed not by the satellite constraint of the resimply becomes the goal to achieve (BMB (RESULT Similarly, the propo- Ow s SCAN-1 ?ACT-2)) (DMB (08.1 are interpreted as the goal to find some element that could legitimately take place of In order to enable the relations to nest recursome relations&apos; nucleuses and satellites contain requirements that specify additional relations, such as examples, contrasts, etc. Of course, these additional requirements may only be included if such material can coherently follow the content of the nucleus or satellite. The question of ordering such additional constituents is still under investigation. The question of whether such additional material should be included at all is not addressed; the structurer tries to say everything it is given. The structurer produces all coherent paragraphs (that is, coherent as defined by the relations) that satisfy the given goal(s) for any set of input elements. For example, paragraph (b) is produced to the initial goal S H (SEQUENCE goal is produced by PEA, together with the appropriate representation ele- (ASK-I. in response to the does the system a program?. Different initial goals will result in different paragraphs. Each paragraph is represented as a tree in which branch points are RST relations and leaves are input elements. Figure 1 is the tree for para- (b). It contains the relations (signalled by &apos;then&apos; and &apos;finally&amp;quot;), Elaboration (&amp;quot;in particular&amp;quot;), and Purpose (&amp;quot;in order to&amp;quot;). In the corresponding paragraph produced by Penman, the relations&apos; characteristic words or phrases (boldfaced below) appear between the blocks of text they relate: [The system asks the user to tell it the characteristic of the program to be system applies to the progran2.](0 system scans the proo order to opportunities to apply transformations to the system resolves [It confirms the enhancewith the [it performs the enhancement.ho 166 input sentence generator --ot update agenda choose final plan get next bud RST relations expand bud grow tree Figure 2: Hierarchical Planning Structurer 6-The Structurer As stated above, the structurer is a simplified top-down hierarchical expansion planner (see Figure 2). It operates as follows: given one or more communicative goals, it finds RST relations whose intended effects match (some of) these goals; it then inspects which of the input elements match the nucleus and subgoal constraints for each relation. Unmatched constraints become subgoals which are posted on an agenda for the next level of planning. The tree can be expanded in either depth-first or breadth-first fashion. Eventually, process bottoms out when either: (a) all input elements have been used and unsatisfied subgoals remain (in which case the structurer could request more input with desired properties from the encapsulating system); or (b) all goals are satisfied. If more than one plan (i.e., paragraph tree structure) is produced, the results are ordered by preferring trees with the minimum unused number of input elements and the minimum number of remaining unsatisfied subgoals. The best tree is then traversed in left-to-right order; leaves provide input to Penman to be generated in English and relations at branch points provide typical interclausal relation words or phrases. In this way the structurer performs top-down goal refinement down to the level of the input elements. and Further Work This work is also being tested in a completely separate domain: the generation of text in a multimedia system that answers database queries. Penman produces the following description of the ship Knox (where CTG 070.10 designates a group of ships): Knox is en route in rendezvous with CTG 070.10, arriving in Pearl Harbor on 4/24, for port visit until 4/30. In this text, each clause (en route, rendezvous, arrive, visit) is a separate input element; the structurer linked them using the relations Sequence and Purpose (the same Purpose as shown above; it is signalled by &amp;quot;in order to&amp;quot;). However, Penman can also be made to produce (d). Knox is en route in order to rendezvous with CTG 070.10. It will arrive in Pearl Harbor on 4/24. It will be on port visit until 4/30. The problem is clear: how should sentences in the paragraph be scoped? At present, avoiding any claims about a theory, the structurer can feed 167 Penman either extreme: make everything one sentence, or make each input element a separate sentence. However, neither extreme is satisfactory; as is clear from paragraph (b), &amp;quot;short&amp;quot; spans of text can be linked and &amp;quot;long&amp;quot; ones left separate. A simple way to implement this is to count the number of leaves under each branch (nucleus or satellite) in the paragraph structure tree. Another shortcoming is the treatment of input elements as indivisible entities. This shortcoming is a result of factoring out the problem of aggregation as a separate text planning task. Chunking together input elements (to eliminate detail) or taking them apart (to be more detailed) has received scant mention — see Ellovy 871, and for the related problem of paraphrase see [Schank 75] — but this task should interact with text structuring in order to provide text that is both optimally detailed and coherent. At the present time, only about 20% of the RST relations have been formalized to the extent that they can be used by the structurer. This formalization process is difficult, because it goes handin-hand with the development of terms with which to characterize the relations&apos; goals/constraints. Though the formalization can never be completely finalized — who can hope to represent something like motivation or justification complete with all ramifications? — the hope is that, by having the requirements stated in rather basic terms, the relations will be easily adaptable to any new representation scheme and domain. (It should be noted, of course, that, to be useful, these formalizations need only be as specific and as detailed as the domain model and representation requires.) In addition, the availability of a set of communicative goals more detailed than just say or ask (for example), should make it easier for programs that require output text to interface with the generator. This is one focus of current text planning work at ISI.</abstract>
<degree confidence="0.6460418">8 Acknowledgments For help with Penman, Robert Albano, John Bateman, Bob Kasper, Christian Matthiessen, Lynn Poulton, and Richard Whitney. For help with the input, Bill Mann and Johanna Moore. For general</degree>
<note confidence="0.922120142857143">comments, all the above, and Cede Paris, Stuart Shapiro, and Norm Sondheimer. 9 References 1. Appek, D.E., 1987a. A Computational Model of Referring, SRI Technical Note 409. 2. Appelt, D.E., 1987b.</note>
<title confidence="0.8723975">Towards a Plan-Based Theory of Referring in Natural Language</title>
<note confidence="0.917707762711864">Recent Advances in Artificial Intelligence, and Linguistics, G. (ed), (Kluwer Academic Publishers, Boston) 63-70. 3. Aristotle, 1954. Rhetoric, The Rhetoric Poof Aristotle, W. Roberts (trans), (Random House, New York). 4. Brac.hman, R.J., 1987. A Structural Paradigm for Representing Knowledge, Ph.D. dissertation, Harvard University; also BBN Research Report 3605. 5. Cohen, P.R. &amp; Levesque, H.J., 1985. Acts and Rationality, of ACL Conference, 6. Davis, R., 1976. Applications of Meta-Level Knowledge to the Constructions, Maintenance, and Use of Large Knowledge Bases, Ph.D. dissertation, Stanford University. 7. Grimes, J.E., 1975. Thread of Discourse The Hague). 8. Hobbs, J.R., 1978. Why is Discourse Coherent?, SRI Technical Note 176. 9. Hobbs, J.R., 1979. and Coreference, in Sci- 10. Hobbs, J.R., 1982. in Discourse, in for Nat- Language Processing, W.G. &amp; Ringle, M.H. (eds), (Lawrence Erlbaum Associates, Hillsdale NJ) 223-243. 11. Hovy, E.H., 1986. Affect into Text, of the Cognitive Science Society Conference, Amherst (669-671). 168 12. Hovy, E.H., 1987. in Generation, of AAAI Conference, (545-549). 13. Kaczmarek, T.S., Bates, R. &amp; Robins, G., 1986. Developments in NIKL, the AAAI Conference, (978- 985). 14. Mann, W.C., 1983. An Overview of the Nigel Text Generation Grammar, USC/Information Sciences Institute Research Report RR-83-113. 15. Mann, W.C. &amp; Matthiessen, C.M.I.M., 1983. Nigel: A Systemic Grammar for Text Generation, USC/Information Sciences Institute Research Report RR-83-105. 16. Mann, W.C. &amp; Thompson, S.A., 1983. Relational Propositions in Discourse, USC/- Information Sciences Institute Research Report RR-83-115. 17. Mann, W.C. &amp; Thompson, S.A., 1986.</note>
<title confidence="0.723236333333333">Rhetorical Structure Theory: Description Construction of Text Structures, in Natural Language Generation: New Results in</title>
<note confidence="0.943228684210526">Artificial Intelligence, Psychology, and Lin- G. (ed), (Kluwer Academic Publishers, Dordrecht, Boston MA) 279-300. 18. Mann, W.C. &amp; Thompson, S.A., 1987. Rhetorical Structure Theory: A Theory of Text Organization, USC/Information Sciences Institute Research Report RR-87-190. 19. Matthiessen, C.M.I.M., 1984. Systemic Grammar in Computation: the Nigel Case, USC/Information Sciences Institute Research Report RR-84-121. 20. McKeown, K.R., 1982. Generating Natural Language Text in Response to Questions about Database Queries, Ph.D. dissertation, University Of Pennsylvania. 21. Moore, J.D., 1988. Enhanced Explanations in Expert and Advice-Giving Systems, USC/Information Sciences Institute Research Report (forthcoming). 22. Sacerdoti, E., 1977. for Plans and Behavior (North- Holland, Amsterdam). 23. Schank, R.C., 1975. Information Processing, (North- Holland, Amsterdam). 24. Schmolze, J.G. &amp; Lipkis, T.A., 1983. Classification in the KL-ONE Knowledge System, of the IJ- Conference, (330-332). 25. Shepherd, H.R., 1926. Fine Art of Writing, Macmillan Co, New York). 26. Shortliffe, E.H., 1976. Computer-Based Medical Consultations: MYCIN. 169</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D E Appek</author>
</authors>
<date>1987</date>
<journal>A Computational Model of Referring, SRI Technical Note</journal>
<volume>409</volume>
<marker>1.</marker>
<rawString>Appek, D.E., 1987a. A Computational Model of Referring, SRI Technical Note 409.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Appelt</author>
</authors>
<title>Towards a Plan-Based Theory of Referring Actions, in Natural Language Generation: Recent Advances in</title>
<date>1987</date>
<journal>Artificial Intelligence, Psychology, and</journal>
<pages>63--70</pages>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Boston</location>
<marker>2.</marker>
<rawString>Appelt, D.E., 1987b. Towards a Plan-Based Theory of Referring Actions, in Natural Language Generation: Recent Advances in Artificial Intelligence, Psychology, and Linguistics, Kempen, G. (ed), (Kluwer Academic Publishers, Boston) 63-70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aristotle</author>
</authors>
<title>The Rhetoric, in The Rhetoric and the Poetics of Aristotle, W. Rhys Roberts (trans), (Random House,</title>
<date>1954</date>
<location>New York).</location>
<contexts>
<context position="12431" citStr="(3)" startWordPosition="2028" endWordPosition="2028">S H (OBJ FIND-1 OPP-1)) the opportunities Intended Effects: 1. (BMB S H (BEL SYS-1 (RESULT SCAN-1 OPP-1))) The system &amp;quot;believes * that scanning will disclose the opportunities 2. (BMB S H (PURPOSE SCAN-1 OPP-1)) This is the purpose of the scanning 165 SEGUEIC AUCLEUS-&lt;IMPUTREC with AUCLEUS-(IMPATREC with (RI P4 E6))64 NRCLEUS-4/11PUTREC with (RI C4),(C) e(SATELLITE-(IAPUTREC with (FI ES)) (44) • tiLICLEVS-PURPOS SATELLITE-ELROORATIO AUCLEUS-4IRPUTREC with (52),(0 I SATELLITE-SEC/UDC t1( AUCLEUS-(/APUTREC with (C2 04)) (f) SATELLITE-5E1VDC 0( SATELLITE-SERUM 0( e(SATELLITS-(IMPUTREC with (P3), (3) Figure 1: Paragraph Structure Tree The elements SCAN-I, OPP-1, etc., are part of a network provided to the Penman structurer by PEA. These elements are defined as propositions in a property-inheritance network of the usual kind written in NIKL aSchmolze dc Lipkis 831, [Kaczmarek et al. 861), a descendant of KL.. ONE Prachman 78D. Some input for this example sentence is: (PEA-SYSTEM SYS-1) (OPPORTUNITY OPP-1) (PROGRAM PROG-1) (ENABLEMENT ENAB-6) (SCAN SCAN-1) (DOMAIN ENAB-6 OPP-1) (ACTOR SCAN-1 SYS-1) (RANGE ENAB-6 APPLY-3) (OBJ SCAN-1 PROG-1) (APPLY APPLY-3) (RESULT SCAN-1 FIND-1) (ACTOR APPL</context>
</contexts>
<marker>3.</marker>
<rawString>Aristotle, 1954. The Rhetoric, in The Rhetoric and the Poetics of Aristotle, W. Rhys Roberts (trans), (Random House, New York).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Brac hman</author>
</authors>
<title>A Structural Paradigm for Representing Knowledge,</title>
<date>1987</date>
<tech>Ph.D. dissertation,</tech>
<institution>Harvard University;</institution>
<note>also BBN Research Report 3605.</note>
<marker>4.</marker>
<rawString>Brac.hman, R.J., 1987. A Structural Paradigm for Representing Knowledge, Ph.D. dissertation, Harvard University; also BBN Research Report 3605.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P R Cohen</author>
<author>H J Levesque</author>
</authors>
<title>Speech Acts and Rationality,</title>
<date>1985</date>
<booktitle>Proceedings of the ACL Conference,</booktitle>
<location>Chicago</location>
<marker>5.</marker>
<rawString>Cohen, P.R. &amp; Levesque, H.J., 1985. Speech Acts and Rationality, Proceedings of the ACL Conference, Chicago (49-59).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Davis</author>
</authors>
<title>Applications of Meta-Level Knowledge to the Constructions, Maintenance, and Use of Large Knowledge Bases,</title>
<date>1976</date>
<institution>Stanford University.</institution>
<note>Ph.D. dissertation,</note>
<marker>6.</marker>
<rawString>Davis, R., 1976. Applications of Meta-Level Knowledge to the Constructions, Maintenance, and Use of Large Knowledge Bases, Ph.D. dissertation, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J E Grimes</author>
</authors>
<title>The Thread of Discourse (Mouton, The Hague).</title>
<date>1975</date>
<marker>7.</marker>
<rawString>Grimes, J.E., 1975. The Thread of Discourse (Mouton, The Hague).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Hobbs</author>
</authors>
<date>1978</date>
<booktitle>Why is Discourse Coherent?, SRI Technical Note 176.</booktitle>
<marker>8.</marker>
<rawString>Hobbs, J.R., 1978. Why is Discourse Coherent?, SRI Technical Note 176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Hobbs</author>
</authors>
<title>Coherence and Coreference,</title>
<date>1979</date>
<journal>in Cognitive Science</journal>
<volume>3</volume>
<issue>1</issue>
<pages>67--90</pages>
<marker>9.</marker>
<rawString>Hobbs, J.R., 1979. Coherence and Coreference, in Cognitive Science 3(1), 67-90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Hobbs</author>
</authors>
<title>Coherence in Discourse,</title>
<date>1982</date>
<booktitle>in Strategies for Natural Language Processing, Lehnert, W.G. &amp; Ringle, M.H. (eds), (Lawrence Erlbaum Associates, Hillsdale NJ)</booktitle>
<pages>223--243</pages>
<marker>10.</marker>
<rawString>Hobbs, J.R., 1982. Coherence in Discourse, in Strategies for Natural Language Processing, Lehnert, W.G. &amp; Ringle, M.H. (eds), (Lawrence Erlbaum Associates, Hillsdale NJ) 223-243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E H Hovy</author>
</authors>
<title>Putting Affect into Text,</title>
<date>1986</date>
<booktitle>Proceedings of the Cognitive Science Society Conference,</booktitle>
<location>Amherst</location>
<marker>11.</marker>
<rawString>Hovy, E.H., 1986. Putting Affect into Text, Proceedings of the Cognitive Science Society Conference, Amherst (669-671).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E H Hovy</author>
</authors>
<title>Interpretation in Generation,</title>
<date>1987</date>
<booktitle>Proceedings of the AAAI Conference,</booktitle>
<location>Seattle</location>
<marker>12.</marker>
<rawString>Hovy, E.H., 1987. Interpretation in Generation, Proceedings of the AAAI Conference, Seattle (545-549).</rawString>
</citation>
<citation valid="false">
<authors>
<author>T S Kaczmarek</author>
<author>R Bates</author>
</authors>
<publisher>Robins, G.,</publisher>
<marker>13.</marker>
<rawString>Kaczmarek, T.S., Bates, R. &amp; Robins, G.,</rawString>
</citation>
<citation valid="true">
<title>Recent Developments in NIKL,</title>
<date></date>
<booktitle>Proceedings of the AAAI Conference,</booktitle>
<location>Philadelphia</location>
<marker>1986.</marker>
<rawString> Recent Developments in NIKL, Proceedings of the AAAI Conference, Philadelphia (978-985).</rawString>
</citation>
<citation valid="true">
<authors>
<author>W C Mann</author>
</authors>
<title>An Overview of the Nigel Text Generation Grammar, USC/Information Sciences Institute Research Report</title>
<date>1983</date>
<pages>83--113</pages>
<marker>14.</marker>
<rawString>Mann, W.C., 1983. An Overview of the Nigel Text Generation Grammar, USC/Information Sciences Institute Research Report RR-83-113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W C Mann</author>
<author>C M I M Matthiessen</author>
</authors>
<title>Nigel: A Systemic Grammar for Text Generation, USC/Information Sciences Institute Research Report</title>
<date>1983</date>
<pages>83--105</pages>
<marker>15.</marker>
<rawString>Mann, W.C. &amp; Matthiessen, C.M.I.M., 1983. Nigel: A Systemic Grammar for Text Generation, USC/Information Sciences Institute Research Report RR-83-105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W C Mann</author>
<author>S A Thompson</author>
</authors>
<date>1983</date>
<booktitle>Relational Propositions in Discourse, USC/-Information Sciences Institute Research Report</booktitle>
<pages>83--115</pages>
<marker>16.</marker>
<rawString>Mann, W.C. &amp; Thompson, S.A., 1983. Relational Propositions in Discourse, USC/-Information Sciences Institute Research Report RR-83-115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W C Mann</author>
<author>S A Thompson</author>
</authors>
<title>Rhetorical Structure Theory: Description and Construction of Text Structures,</title>
<date>1986</date>
<journal>in Natural Language Generation: New Results in Artificial Intelligence, Psychology, and</journal>
<pages>279--300</pages>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht, Boston MA</location>
<marker>17.</marker>
<rawString>Mann, W.C. &amp; Thompson, S.A., 1986. Rhetorical Structure Theory: Description and Construction of Text Structures, in Natural Language Generation: New Results in Artificial Intelligence, Psychology, and Linguistics, Kempen, G. (ed), (Kluwer Academic Publishers, Dordrecht, Boston MA) 279-300.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W C Mann</author>
<author>S A Thompson</author>
</authors>
<date>1987</date>
<booktitle>Rhetorical Structure Theory: A Theory of Text Organization, USC/Information Sciences Institute Research Report</booktitle>
<pages>87--190</pages>
<marker>18.</marker>
<rawString>Mann, W.C. &amp; Thompson, S.A., 1987. Rhetorical Structure Theory: A Theory of Text Organization, USC/Information Sciences Institute Research Report RR-87-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C M I M Matthiessen</author>
</authors>
<date>1984</date>
<booktitle>Systemic Grammar in Computation: the Nigel Case, USC/Information Sciences Institute Research Report</booktitle>
<pages>84--121</pages>
<marker>19.</marker>
<rawString>Matthiessen, C.M.I.M., 1984. Systemic Grammar in Computation: the Nigel Case, USC/Information Sciences Institute Research Report RR-84-121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K R McKeown</author>
</authors>
<title>Generating Natural Language Text in Response to Questions about Database Queries,</title>
<date>1982</date>
<institution>University Of Pennsylvania.</institution>
<note>Ph.D. dissertation,</note>
<marker>20.</marker>
<rawString>McKeown, K.R., 1982. Generating Natural Language Text in Response to Questions about Database Queries, Ph.D. dissertation, University Of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Moore</author>
</authors>
<date>1988</date>
<booktitle>Enhanced Explanations in Expert and Advice-Giving Systems, USC/Information Sciences Institute</booktitle>
<note>Research Report (forthcoming).</note>
<marker>21.</marker>
<rawString>Moore, J.D., 1988. Enhanced Explanations in Expert and Advice-Giving Systems, USC/Information Sciences Institute Research Report (forthcoming).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Sacerdoti</author>
</authors>
<title>A Structure for Plans and Behavior (NorthHolland,</title>
<date>1977</date>
<location>Amsterdam).</location>
<marker>22.</marker>
<rawString>Sacerdoti, E., 1977. A Structure for Plans and Behavior (NorthHolland, Amsterdam).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Schank</author>
</authors>
<date>1975</date>
<booktitle>Conceptual Information Processing,</booktitle>
<location>(NorthHolland, Amsterdam).</location>
<marker>23.</marker>
<rawString>Schank, R.C., 1975. Conceptual Information Processing, (NorthHolland, Amsterdam).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J G Schmolze</author>
<author>T A Lipkis</author>
</authors>
<date>1983</date>
<booktitle>Classification in the KL-ONE Knowledge Representation System, Proceedings of the IJCAI Conference,</booktitle>
<location>Karlsruhe</location>
<marker>24.</marker>
<rawString>Schmolze, J.G. &amp; Lipkis, T.A., 1983. Classification in the KL-ONE Knowledge Representation System, Proceedings of the IJCAI Conference, Karlsruhe (330-332).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H R Shepherd</author>
</authors>
<title>The Fine Art of Writing,</title>
<date>1926</date>
<publisher>The Macmillan Co,</publisher>
<location>New York).</location>
<marker>25.</marker>
<rawString>Shepherd, H.R., 1926. The Fine Art of Writing, (The Macmillan Co, New York).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E H Shortliffe</author>
</authors>
<title>Computer-Based Medical Consultations:</title>
<date>1976</date>
<publisher>MYCIN.</publisher>
<marker>26.</marker>
<rawString>Shortliffe, E.H., 1976. Computer-Based Medical Consultations: MYCIN.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>