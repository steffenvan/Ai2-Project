<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.191586">
<title confidence="0.869742">
HPSG Supertagging: A Sequence Labeling View
</title>
<author confidence="0.999801">
Yao-zhong Zhang † Takuya Matsuzaki † Jun’ichi Tsujii†‡§
</author>
<affiliation confidence="0.992663666666667">
† Department of Computer Science, University of Tokyo
‡ School of Computer Science, University of Manchester
§National Centre for Text Mining, UK
</affiliation>
<email confidence="0.996377">
{yaozhong.zhang, matuzaki, tsujii}@is.s.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.993894" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999960481481482">
Supertagging is a widely used speed-up
technique for deep parsing. In another
aspect, supertagging has been exploited
in other NLP tasks than parsing for
utilizing the rich syntactic information
given by the supertags. However, the
performance of supertagger is still a
bottleneck for such applications. In this
paper, we investigated the relationship
between supertagging and parsing, not
just to speed up the deep parser; We
started from a sequence labeling view
of HPSG supertagging, examining how
well a supertagger can do when separated
from parsing. Comparison of two types
of supertagging model, point-wise model
and sequential model, showed that the
former model works competitively well
despite its simplicity, which indicates
the true dependency among supertag
assignments is far more complex than the
crude first-order approximation made in
the sequential model. We then analyzed
the limitation of separated supertagging
by using a CFG-filter. The results showed
that big gains could be acquired by resort-
ing to a light-weight parser.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999963355555556">
Supertagging is an important part of lexicalized
grammar parsing. A high performance supertag-
ger greatly reduces the load of a parser and ac-
celerates its speed. A supertag represents a lin-
guistic word category, which encodes syntactic be-
havior of the word. The concept of supertagging
was first proposed for lexicalized tree adjoining
grammar (LTAG) (Bangalore and Joshi, 1999) and
then extended to other lexicalized grammars, such
as combinatory categorial grammar (CCG) (Clark,
2002) and Head-driven phrase structure grammar
(HPSG) (Ninomiya et al., 2006). Recently, syn-
tactic information in supertags has been exploited
for NLP tasks besides parsing, such as NP chunk-
ing (Shen and Joshi, 2003), semantic role label-
ing (Chen and Rambow, 2003) and machine trans-
lation (Hassan et al., 2007). Supertagging serves
there as an implicit and convenient way to incor-
porate rich syntactic information in those tasks.
Improving the performance of supertagging can
thus benefit these two aspects: as a preproces-
sor for deep parsing and as an independent, al-
ternative technique for “almost” parsing. How-
ever, supertags are derived from a grammar and
thus have a strong connection to parsing. To fur-
ther improve the supertagging accuracy, the rela-
tion between supertagging and parsing is crucial.
With this motivation, we investigate how well a se-
quence labeling model can do when it is separated
from a parser, and to what extent the ignorance of
long distance dependencies in the sequence label-
ing formulation affects the supertagging results.
Specifically, we evaluated two different types
of supertagging model, point-wise model and se-
quential model, for HPSG supertagging. CFG-
filter was then used to empirically evaluate the
effect of long distance dependencies in supertag-
ging. The point-wise model achieved competitive
result of 92.53% accuracy on WSJ-HPSG tree-
bank with fast training speed, while the sequen-
tial model augmented with supertag edge features
did not give much further improvement over the
point-wise model. Big gains acquired by using
CFG-filter indicates that further improvement may
be achieved by resorting to a light-weight parser.
</bodyText>
<sectionHeader confidence="0.994276" genericHeader="method">
2 HPSG Supertags
</sectionHeader>
<bodyText confidence="0.935161">
HPSG (Pollard and Sag, 1994) is a kind of lexi-
calized grammar. In HPSG, many lexical entries
are used to express word-specific characteristics,
</bodyText>
<page confidence="0.973439">
210
</page>
<bodyText confidence="0.971380375">
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 210–213,
Paris, October 2009. c�2009 Association for Computational Linguistics
while only small amount of rule schemas are used
to describe general constructions. A supertag in
HPSG corresponds to a template of lexical entry.
For example, one possible supertag for “big” is
“[&lt;ADJP&gt;]N lxm”, which indicates that the syn-
tactic category of “big” is adjective and it modi-
fies a noun to its right. The number of supertags
is generally much larger than the number of labels
used in other sequence labeling tasks; Comparing
to 45 POS tags used in PennTreebank, the HPSG
grammar used in our experiments includes 2,308
supertags. Because of this, it is often very hard or
even impossible to apply computationary demand-
ing methods to HPSG supertagging.
</bodyText>
<sectionHeader confidence="0.937661" genericHeader="method">
3 Perceptron and Bayes Point Machine
</sectionHeader>
<bodyText confidence="0.9999522">
Perceptron is an efficient online discriminative
training method. We used perceptron with weight-
averaging (Collins, 2002) as the basis of our su-
pertagging model. We also use perceptron-based
Bayes point machine (BPM) (Herbrich et al.,
2001) in some of the experiments. In short, a BPM
is an average of a number of averaged perceptrons’
weights. We use average of 10 averaged percep-
trons, each of which is trained on a different ran-
dom permutation of the training data.
</bodyText>
<subsectionHeader confidence="0.993599">
3.1 Formulation
</subsectionHeader>
<bodyText confidence="0.999488923076923">
Here we follow the definition of Collins’ per-
ceptron to learn a mapping from the input space
(w, p) E W x P to the supertag space s E S. We
use function GEN(w,p) to indicate all candidates
given input (w, p). Feature function f maps a train-
ing sample (w, p, s) E W xP xS to a point in the
feature space Rd. To get feature weights α E Rd
of feature function, we used the averaged percep-
tron training method described in (Collins, 2002),
and the average of its 10 different runs (i.e., BPM).
For decoding, given an input (w, p) and a vector
of feature weights α, we want to find an output s
which satisfies:
</bodyText>
<equation confidence="0.697863">
F(w, p) = argmax α · f(w,p,s)
sEGEN(w, p)
</equation>
<bodyText confidence="0.999353142857143">
For the input (w, p), we treat it in two fash-
ions: one is (w, p) representing a single word
and a POS tag. Another is (w, p) representing
whole word and POS tags sequence. We call them
point-wise model and sequential model respec-
tively. Viterbi algorithm is used for decoding in
sequential model.
</bodyText>
<table confidence="0.994503923076923">
template type template
Word wi,wi−1,wi+1,
wi−1&amp;wi, wi&amp;wi+1
POS pi, pi−1, pi−2, pi+1,
pi+2, pi−1&amp;pi, pi−2&amp;pi−1,
pi−1&amp;pi+1, pi&amp;pi+1, pi+1&amp;pi+2
Word-POS pi−1&amp;wi, pi&amp;wi, pi+1&amp;wi
Supertag† si−1 , si−2&amp;si−1
Substructure {ssi,1, ..., ssi,N}x Word
{ssi,1, ..., ssi,N}x POS
{ssi,1, ..., ssi,N}x Word-POS
{ssi−1,1, ..., ssi−1,N}x
{ssi,1, ..., ssi,N}†
</table>
<tableCaption confidence="0.998552">
Table 1: Feature templates for point-wise model
</tableCaption>
<bodyText confidence="0.9359678">
and sequential model. Templates with † are only
used by sequential model. ssi,j represents j-th
substructure of supertag at i. For briefness, si is
omitted for each template. “x” means set-product.
e.g., {a,b}x{A,B}={a&amp;A,a&amp;B,b&amp;A,b&amp;B}
</bodyText>
<subsectionHeader confidence="0.952409">
3.2 Features
</subsectionHeader>
<bodyText confidence="0.9999854">
Feature templates are listed in Table 1. To make
the results comparable with previous work, we
adopt the same feature templates as Matsuzaki et.
al. (2007). For sequential model, supertag con-
texts are added to the features. Because of the
large number of supertags, those supertag edge
features could be very sparse. To alleviate this
sparseness, we extracted sub-structures from the
lexical template of each supertag, and use them for
making generalized node/edge features as shown
in Table 1. The sub-structures we used include
subcategorization frames (e.g., subject=NP, ob-
ject=NP PP), direction and category of modifiee
phrase (e.g., mod left=VP), voice and tense of a
verb (e.g., passive past).
</bodyText>
<subsectionHeader confidence="0.997063">
3.3 CFG-filter
</subsectionHeader>
<bodyText confidence="0.999836181818182">
Long distance dependencies are also encoded in
supertags. For example, when a transitive verb
gets assigned a supertag that specifies it has a PP-
object, in most cases a preposition to its right must
be assigned an argument (not adjunct) supertag,
and vice versa. Such kind of long distance context
information might be important for supertag dis-
ambiguation, but is not easy to incorporate into a
sequence labeling model separated from a parser.
To examine the limitation of supertagging sep-
arated from a parser, we used CFG-filter as an ap-
</bodyText>
<page confidence="0.992982">
211
</page>
<table confidence="0.9997374">
Model Name Acc%
PW-AP 92.29
SEQ-AP 92.53
PW-AP+CFG 93.57
SEQ-AP+CFG 93.68
</table>
<tableCaption confidence="0.9847205">
Table 2: Averaged 10-cross validation of averaged
perceptron on Section 02-21.
</tableCaption>
<bodyText confidence="0.998765916666667">
proximation of an HPSG parser. We firstly cre-
ated a CFG that approximates the original HPSG
grammar, using the iterative method by Kiefer
and Krieger (2000). Given the supertags as pre-
terminals, the approximating CFG was then used
for finding a maximally scored sequence of su-
pertags which satisfies most of the grammatical
constraints in the original HPSG grammar (Mat-
suzaki et al., 2007). By comparing the supertag-
ging results before and after CFG-filtering, we can
quantify how many errors are caused by ignorance
of the long-range dependencies in supertagger.
</bodyText>
<sectionHeader confidence="0.995404" genericHeader="evaluation">
4 Experiments and Analysis
</sectionHeader>
<bodyText confidence="0.99953516">
We conducted experiments on WSJ-HPSG tree-
bank corpus (Miyao, 2006), which was semi-
automatically converted from the WSJ portion of
PennTreebank. The number of training iterations
was set to 5 for all models. Gold-standard POS
tags are used as input. The performance is evalu-
ated by accuracy1 and speed of supertagging on an
AMD Opteron 2.4GHz server.
Table 2 shows the averaged results of 10-
fold cross-validation of averaged perceptron (AP)
models2 on section 02-21. We can see the dif-
ference between point-wise AP model and se-
quential AP model is small (0.24%). It becomes
even smaller after CFG-filtering (0.11%). Table
3 shows the supertagging accuracy on section 22
based on BPM. Although not statistically signif-
icantly different from previous ME model (Mat-
suzaki et al., 2007), point-wise model (PW-BPM)
achieved competitive result 92.53% with faster
training. In addition, 0.27% and 0.29% gains were
brought by using BPM from PW-AP (92.26%) and
PW-SEQ (92.54%) with P-values less than 0.05.
The improvement by using sequential mod-
els (PW-AP→SEQ-AP: 0.24%, PW-BPM→SEQ-
BPM: 0.3%, statistically significantly different),
</bodyText>
<footnote confidence="0.992957333333333">
1“UNK” supertags are ignored in evaluation as previous.
2For time limitation, cross validation for BPM was not
conducted.
</footnote>
<table confidence="0.999794909090909">
Model Name Acc% Training/
Testing Time ‡
ME (Matsuzaki 07’) 92.45 ≈ 3h / 12s
PW-BPM 92.53 285s / 10s
SEQ-BPM 92.83 1721s / 13s
PW-BPM+SUB 92.68 1275s / 25s
SEQ-BPM+SUB 92.99 9468s / 107s
PW-BPM+CFG 93.60 285s / 78s
SEQ-BPM+CFG 93.70 1721s / 195s
PW-BPM+SUB+CFG 93.72 1275s / 170s
SEQ-BPM+SUB+CFG 93.88 9468s / 1011s
</table>
<tableCaption confidence="0.886551333333333">
Table 3: Supertagging accuracy and training&amp;
testing speed on section 22. (‡) Test time was cal-
culated on totally 1648 sentences.
</tableCaption>
<bodyText confidence="0.999888657142857">
compared to point-wise models, were not so large,
but the training time was around 6 times longer.
We think the reason is twofold. First, as previous
research showed, POS sequence is very informa-
tive in supertagging (Clark, 2004). A large amount
of local syntactic information can be captured in
POS tags of surrounding words, although a few
long-range dependencies are of course not. Sec-
ond, the number of supertags is large and the su-
pertag edge features used in sequential model are
inevitably suffered from data sparseness. To alle-
viate this, we extracted sub-structure from lexical
templates (i.e., lexical items corresponding to su-
pertags) to augment the supertag edge features, but
only got 0.16% improvement (SEQ-BPM+SUB).
Furthermore, we also got 0.15% gains with P-
value less than 0.05 by incorporating the sub-
structure features into point-wise model (PW-
BPM+SUB). We hence conclude that the contri-
bution of the first-order edge features is not large
in sequence modeling for HPSG supertagging.
As we explained in Section 3.3, sequence label-
ing models have inherent limitation in the ability
to capture long distance dependencies between su-
pertags. This kind of ambiguity could be easier to
solve in a parser. To examine this, we added CFG-
filter which works as an approximation of a full
HPSG parser, after the sequence labeling model.
As expected, there came big gains of 1.26% (from
PW-AP to PW-AP+CFG) and 1.15% (from PW-
BPM to PW-BPM+CFG). Even for the sequen-
tial model we also got 1.15% (from SEQ-AP to
SEQ-AP+CFG) and 0.87% (from SEQ-BPM to
SEQ-BPM+CFG) respectively. All these models
were statistically significantly different from orig-
</bodyText>
<page confidence="0.995524">
212
</page>
<bodyText confidence="0.99964134375">
inal ones.
We also gave error analysis on test results.
Comparing SEQ-AP with SEQ-AP+CFG, one of
the most frequent types of “correct supertag” by
the CFG-filter was for word “and”, wherein a su-
pertag for NP-coordination (“NP and NP”) was
corrected to one for VP-coordination (“VP and
VP” or “S and S”). It means the disambiguation
between the two coordination type is difficult for
supertaggers, presumably because they looks very
similar with a limited length of context since the
sequence of the NP-object of left conjunct, “and”,
the NP subject of right conjunct looks very similar
to a NP coordination. The different assignments
by SEQ-AP+CFG from SEQ-AP include 725 right
corrections, while it changes 298 correct predic-
tions by SEQ-AP to wrong assignments. One pos-
sible reason for some of “wrong correction” is re-
lated to the approximation of grammar. But this
gives clue that for supertagging task: just using
sequence labeling models is limited, and we can
resort to use some light-weight parser to handle
long distance dependencies.
Although some of the ambiguous supertags
could be left for deep parsing, like multi-tagging
technique (Clark, 2004), we also consider the
tasks where supertags can be used while conduct-
ing deep parsing is too computationally costly. Al-
ternatively, focusing on supertagging, we could
treat it as a sequence labeling task, while a conse-
quent light-weight parser is a disambiguator with
long distance constraint.
</bodyText>
<sectionHeader confidence="0.999568" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9999639">
In this paper, through treating HPSG supertag-
ging in a sequence labeling way, we examined
the relationship between supertagging and parsing
from an angle. In experiment, even for sequential
models, CFG-filter gave much larger improvement
than one gained by switching from a point-wise
model to a sequential model. The accuracy im-
provement given by the CFG-filter suggests that
we could gain further improvement by combining
a supertagger with a light-weight parser.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.995815285714286">
Thanks to the anonymous reviewers for valuable
comments. The first author was partially sup-
ported by University of Tokyo Fellowship (UT-
Fellowship). This work was partially supported
by Grant-in-Aid for Specially Promoted Research
and Special Coordination Funds for Promoting
Science and Technology (MEXT, Japan).
</bodyText>
<sectionHeader confidence="0.996289" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999637222222223">
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
putational Linguistics, 25:237–265.
John Chen and Owen Rambow. 2003. Use of deep
linguistic features for the recognition and labeling
of semantic arguments. In Proceedings of EMNLP-
2003, pages 41–48.
Stephen Clark. 2002. Supertagging for combinatory
categorial grammar. In Proceedings of the 6th In-
ternational Workshop on Tree Adjoining Grammars
and Related Frameworks (TAG+ 6), pages 19–24.
Stephen Clark. 2004. The importance of supertagging
for wide-coverage ccg parsing. In Proceedings of
COLING-04, pages 282–288.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. pages 1–8.
Hany Hassan, Mary Hearne, and Andy Way. 2007. Su-
pertagged phrase-based statistical machine transla-
tion. In Proceedings of ACL 2007, pages 288–295.
Ralf Herbrich, Thore Graepel, and Colin Campbell.
2001. Bayes point machines. Journal of Machine
Learning Research, 1:245–279.
Bernd Kiefer and Hans-Ulrich Krieger. 2000. A
context-free approximation of head-driven phrase
structure grammar. In Proceedings of IWPT-2000,
pages 135–146.
Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsu-
jii. 2007. Efficient hpsg parsing with supertagging
and cfg-filtering. In Proceedings of IJCAI-07, pages
1671–1676.
Yusuke Miyao. 2006. From Linguistic Theory to Syn-
tactic Analysis: Corpus-Oriented Grammar Devel-
opment and Feature Forest Model. Ph.D. Disserta-
tion, The University of Tokyo.
Takashi Ninomiya, Yoshimasa Tsuruoka, Takuya Mat-
suzaki, and Yusuke Miyao. 2006. Extremely lex-
icalized models for accurate and fast hpsg parsing.
In Proceedings of EMNLP-2006, pages 155–163.
Carl Pollard and Ivan A. Sag. 1994. Head-driven
Phrase Structure Grammar. University of Chicago /
CSLI.
Libin Shen and Aravind K. Joshi. 2003. A snow based
supertagger with application to np chunking. In Pro-
ceedings of ACL 2003, pages 505–512.
</reference>
<page confidence="0.999394">
213
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.839040">
<title confidence="0.998674">HPSG Supertagging: A Sequence Labeling View</title>
<author confidence="0.999894">Zhang Matsuzaki</author>
<affiliation confidence="0.986710333333333">of Computer Science, University of of Computer Science, University of Centre for Text Mining,</affiliation>
<email confidence="0.937243">matuzaki,</email>
<abstract confidence="0.997061642857142">Supertagging is a widely used speed-up technique for deep parsing. In another aspect, supertagging has been exploited in other NLP tasks than parsing for utilizing the rich syntactic information given by the supertags. However, the performance of supertagger is still a bottleneck for such applications. In this paper, we investigated the relationship between supertagging and parsing, not just to speed up the deep parser; We started from a sequence labeling view of HPSG supertagging, examining how well a supertagger can do when separated from parsing. Comparison of two types of supertagging model, point-wise model and sequential model, showed that the former model works competitively well despite its simplicity, which indicates the true dependency among supertag assignments is far more complex than the crude first-order approximation made in the sequential model. We then analyzed the limitation of separated supertagging by using a CFG-filter. The results showed that big gains could be acquired by resorting to a light-weight parser.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Aravind K Joshi</author>
</authors>
<title>Supertagging: An approach to almost parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<pages>25--237</pages>
<contexts>
<context position="1747" citStr="Bangalore and Joshi, 1999" startWordPosition="255" endWordPosition="258">crude first-order approximation made in the sequential model. We then analyzed the limitation of separated supertagging by using a CFG-filter. The results showed that big gains could be acquired by resorting to a light-weight parser. 1 Introduction Supertagging is an important part of lexicalized grammar parsing. A high performance supertagger greatly reduces the load of a parser and accelerates its speed. A supertag represents a linguistic word category, which encodes syntactic behavior of the word. The concept of supertagging was first proposed for lexicalized tree adjoining grammar (LTAG) (Bangalore and Joshi, 1999) and then extended to other lexicalized grammars, such as combinatory categorial grammar (CCG) (Clark, 2002) and Head-driven phrase structure grammar (HPSG) (Ninomiya et al., 2006). Recently, syntactic information in supertags has been exploited for NLP tasks besides parsing, such as NP chunking (Shen and Joshi, 2003), semantic role labeling (Chen and Rambow, 2003) and machine translation (Hassan et al., 2007). Supertagging serves there as an implicit and convenient way to incorporate rich syntactic information in those tasks. Improving the performance of supertagging can thus benefit these tw</context>
</contexts>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>Srinivas Bangalore and Aravind K. Joshi. 1999. Supertagging: An approach to almost parsing. Computational Linguistics, 25:237–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Chen</author>
<author>Owen Rambow</author>
</authors>
<title>Use of deep linguistic features for the recognition and labeling of semantic arguments.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP2003,</booktitle>
<pages>41--48</pages>
<contexts>
<context position="2114" citStr="Chen and Rambow, 2003" startWordPosition="311" endWordPosition="314">of a parser and accelerates its speed. A supertag represents a linguistic word category, which encodes syntactic behavior of the word. The concept of supertagging was first proposed for lexicalized tree adjoining grammar (LTAG) (Bangalore and Joshi, 1999) and then extended to other lexicalized grammars, such as combinatory categorial grammar (CCG) (Clark, 2002) and Head-driven phrase structure grammar (HPSG) (Ninomiya et al., 2006). Recently, syntactic information in supertags has been exploited for NLP tasks besides parsing, such as NP chunking (Shen and Joshi, 2003), semantic role labeling (Chen and Rambow, 2003) and machine translation (Hassan et al., 2007). Supertagging serves there as an implicit and convenient way to incorporate rich syntactic information in those tasks. Improving the performance of supertagging can thus benefit these two aspects: as a preprocessor for deep parsing and as an independent, alternative technique for “almost” parsing. However, supertags are derived from a grammar and thus have a strong connection to parsing. To further improve the supertagging accuracy, the relation between supertagging and parsing is crucial. With this motivation, we investigate how well a sequence l</context>
</contexts>
<marker>Chen, Rambow, 2003</marker>
<rawString>John Chen and Owen Rambow. 2003. Use of deep linguistic features for the recognition and labeling of semantic arguments. In Proceedings of EMNLP2003, pages 41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
</authors>
<title>Supertagging for combinatory categorial grammar.</title>
<date>2002</date>
<booktitle>In Proceedings of the 6th International Workshop on Tree Adjoining Grammars and Related Frameworks (TAG+ 6),</booktitle>
<pages>pages</pages>
<contexts>
<context position="1855" citStr="Clark, 2002" startWordPosition="272" endWordPosition="273"> using a CFG-filter. The results showed that big gains could be acquired by resorting to a light-weight parser. 1 Introduction Supertagging is an important part of lexicalized grammar parsing. A high performance supertagger greatly reduces the load of a parser and accelerates its speed. A supertag represents a linguistic word category, which encodes syntactic behavior of the word. The concept of supertagging was first proposed for lexicalized tree adjoining grammar (LTAG) (Bangalore and Joshi, 1999) and then extended to other lexicalized grammars, such as combinatory categorial grammar (CCG) (Clark, 2002) and Head-driven phrase structure grammar (HPSG) (Ninomiya et al., 2006). Recently, syntactic information in supertags has been exploited for NLP tasks besides parsing, such as NP chunking (Shen and Joshi, 2003), semantic role labeling (Chen and Rambow, 2003) and machine translation (Hassan et al., 2007). Supertagging serves there as an implicit and convenient way to incorporate rich syntactic information in those tasks. Improving the performance of supertagging can thus benefit these two aspects: as a preprocessor for deep parsing and as an independent, alternative technique for “almost” pars</context>
</contexts>
<marker>Clark, 2002</marker>
<rawString>Stephen Clark. 2002. Supertagging for combinatory categorial grammar. In Proceedings of the 6th International Workshop on Tree Adjoining Grammars and Related Frameworks (TAG+ 6), pages 19–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
</authors>
<title>The importance of supertagging for wide-coverage ccg parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING-04,</booktitle>
<pages>282--288</pages>
<contexts>
<context position="10540" citStr="Clark, 2004" startWordPosition="1669" endWordPosition="1670">45 ≈ 3h / 12s PW-BPM 92.53 285s / 10s SEQ-BPM 92.83 1721s / 13s PW-BPM+SUB 92.68 1275s / 25s SEQ-BPM+SUB 92.99 9468s / 107s PW-BPM+CFG 93.60 285s / 78s SEQ-BPM+CFG 93.70 1721s / 195s PW-BPM+SUB+CFG 93.72 1275s / 170s SEQ-BPM+SUB+CFG 93.88 9468s / 1011s Table 3: Supertagging accuracy and training&amp; testing speed on section 22. (‡) Test time was calculated on totally 1648 sentences. compared to point-wise models, were not so large, but the training time was around 6 times longer. We think the reason is twofold. First, as previous research showed, POS sequence is very informative in supertagging (Clark, 2004). A large amount of local syntactic information can be captured in POS tags of surrounding words, although a few long-range dependencies are of course not. Second, the number of supertags is large and the supertag edge features used in sequential model are inevitably suffered from data sparseness. To alleviate this, we extracted sub-structure from lexical templates (i.e., lexical items corresponding to supertags) to augment the supertag edge features, but only got 0.16% improvement (SEQ-BPM+SUB). Furthermore, we also got 0.15% gains with Pvalue less than 0.05 by incorporating the substructure </context>
<context position="13122" citStr="Clark, 2004" startWordPosition="2085" endWordPosition="2086">ct of right conjunct looks very similar to a NP coordination. The different assignments by SEQ-AP+CFG from SEQ-AP include 725 right corrections, while it changes 298 correct predictions by SEQ-AP to wrong assignments. One possible reason for some of “wrong correction” is related to the approximation of grammar. But this gives clue that for supertagging task: just using sequence labeling models is limited, and we can resort to use some light-weight parser to handle long distance dependencies. Although some of the ambiguous supertags could be left for deep parsing, like multi-tagging technique (Clark, 2004), we also consider the tasks where supertags can be used while conducting deep parsing is too computationally costly. Alternatively, focusing on supertagging, we could treat it as a sequence labeling task, while a consequent light-weight parser is a disambiguator with long distance constraint. 5 Conclusions In this paper, through treating HPSG supertagging in a sequence labeling way, we examined the relationship between supertagging and parsing from an angle. In experiment, even for sequential models, CFG-filter gave much larger improvement than one gained by switching from a point-wise model </context>
</contexts>
<marker>Clark, 2004</marker>
<rawString>Stephen Clark. 2004. The importance of supertagging for wide-coverage ccg parsing. In Proceedings of COLING-04, pages 282–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<pages>1--8</pages>
<contexts>
<context position="4671" citStr="Collins, 2002" startWordPosition="711" endWordPosition="712">”, which indicates that the syntactic category of “big” is adjective and it modifies a noun to its right. The number of supertags is generally much larger than the number of labels used in other sequence labeling tasks; Comparing to 45 POS tags used in PennTreebank, the HPSG grammar used in our experiments includes 2,308 supertags. Because of this, it is often very hard or even impossible to apply computationary demanding methods to HPSG supertagging. 3 Perceptron and Bayes Point Machine Perceptron is an efficient online discriminative training method. We used perceptron with weightaveraging (Collins, 2002) as the basis of our supertagging model. We also use perceptron-based Bayes point machine (BPM) (Herbrich et al., 2001) in some of the experiments. In short, a BPM is an average of a number of averaged perceptrons’ weights. We use average of 10 averaged perceptrons, each of which is trained on a different random permutation of the training data. 3.1 Formulation Here we follow the definition of Collins’ perceptron to learn a mapping from the input space (w, p) E W x P to the supertag space s E S. We use function GEN(w,p) to indicate all candidates given input (w, p). Feature function f maps a t</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hany Hassan</author>
<author>Mary Hearne</author>
<author>Andy Way</author>
</authors>
<title>Supertagged phrase-based statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>288--295</pages>
<contexts>
<context position="2160" citStr="Hassan et al., 2007" startWordPosition="319" endWordPosition="322">g represents a linguistic word category, which encodes syntactic behavior of the word. The concept of supertagging was first proposed for lexicalized tree adjoining grammar (LTAG) (Bangalore and Joshi, 1999) and then extended to other lexicalized grammars, such as combinatory categorial grammar (CCG) (Clark, 2002) and Head-driven phrase structure grammar (HPSG) (Ninomiya et al., 2006). Recently, syntactic information in supertags has been exploited for NLP tasks besides parsing, such as NP chunking (Shen and Joshi, 2003), semantic role labeling (Chen and Rambow, 2003) and machine translation (Hassan et al., 2007). Supertagging serves there as an implicit and convenient way to incorporate rich syntactic information in those tasks. Improving the performance of supertagging can thus benefit these two aspects: as a preprocessor for deep parsing and as an independent, alternative technique for “almost” parsing. However, supertags are derived from a grammar and thus have a strong connection to parsing. To further improve the supertagging accuracy, the relation between supertagging and parsing is crucial. With this motivation, we investigate how well a sequence labeling model can do when it is separated from</context>
</contexts>
<marker>Hassan, Hearne, Way, 2007</marker>
<rawString>Hany Hassan, Mary Hearne, and Andy Way. 2007. Supertagged phrase-based statistical machine translation. In Proceedings of ACL 2007, pages 288–295.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf Herbrich</author>
<author>Thore Graepel</author>
<author>Colin Campbell</author>
</authors>
<title>Bayes point machines.</title>
<date>2001</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>1--245</pages>
<contexts>
<context position="4790" citStr="Herbrich et al., 2001" startWordPosition="729" endWordPosition="732">umber of supertags is generally much larger than the number of labels used in other sequence labeling tasks; Comparing to 45 POS tags used in PennTreebank, the HPSG grammar used in our experiments includes 2,308 supertags. Because of this, it is often very hard or even impossible to apply computationary demanding methods to HPSG supertagging. 3 Perceptron and Bayes Point Machine Perceptron is an efficient online discriminative training method. We used perceptron with weightaveraging (Collins, 2002) as the basis of our supertagging model. We also use perceptron-based Bayes point machine (BPM) (Herbrich et al., 2001) in some of the experiments. In short, a BPM is an average of a number of averaged perceptrons’ weights. We use average of 10 averaged perceptrons, each of which is trained on a different random permutation of the training data. 3.1 Formulation Here we follow the definition of Collins’ perceptron to learn a mapping from the input space (w, p) E W x P to the supertag space s E S. We use function GEN(w,p) to indicate all candidates given input (w, p). Feature function f maps a training sample (w, p, s) E W xP xS to a point in the feature space Rd. To get feature weights α E Rd of feature functio</context>
</contexts>
<marker>Herbrich, Graepel, Campbell, 2001</marker>
<rawString>Ralf Herbrich, Thore Graepel, and Colin Campbell. 2001. Bayes point machines. Journal of Machine Learning Research, 1:245–279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Kiefer</author>
<author>Hans-Ulrich Krieger</author>
</authors>
<title>A context-free approximation of head-driven phrase structure grammar.</title>
<date>2000</date>
<booktitle>In Proceedings of IWPT-2000,</booktitle>
<pages>135--146</pages>
<contexts>
<context position="8183" citStr="Kiefer and Krieger (2000)" startWordPosition="1293" endWordPosition="1296"> adjunct) supertag, and vice versa. Such kind of long distance context information might be important for supertag disambiguation, but is not easy to incorporate into a sequence labeling model separated from a parser. To examine the limitation of supertagging separated from a parser, we used CFG-filter as an ap211 Model Name Acc% PW-AP 92.29 SEQ-AP 92.53 PW-AP+CFG 93.57 SEQ-AP+CFG 93.68 Table 2: Averaged 10-cross validation of averaged perceptron on Section 02-21. proximation of an HPSG parser. We firstly created a CFG that approximates the original HPSG grammar, using the iterative method by Kiefer and Krieger (2000). Given the supertags as preterminals, the approximating CFG was then used for finding a maximally scored sequence of supertags which satisfies most of the grammatical constraints in the original HPSG grammar (Matsuzaki et al., 2007). By comparing the supertagging results before and after CFG-filtering, we can quantify how many errors are caused by ignorance of the long-range dependencies in supertagger. 4 Experiments and Analysis We conducted experiments on WSJ-HPSG treebank corpus (Miyao, 2006), which was semiautomatically converted from the WSJ portion of PennTreebank. The number of trainin</context>
</contexts>
<marker>Kiefer, Krieger, 2000</marker>
<rawString>Bernd Kiefer and Hans-Ulrich Krieger. 2000. A context-free approximation of head-driven phrase structure grammar. In Proceedings of IWPT-2000, pages 135–146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Efficient hpsg parsing with supertagging and cfg-filtering.</title>
<date>2007</date>
<booktitle>In Proceedings of IJCAI-07,</booktitle>
<pages>1671--1676</pages>
<contexts>
<context position="8416" citStr="Matsuzaki et al., 2007" startWordPosition="1330" endWordPosition="1334">mitation of supertagging separated from a parser, we used CFG-filter as an ap211 Model Name Acc% PW-AP 92.29 SEQ-AP 92.53 PW-AP+CFG 93.57 SEQ-AP+CFG 93.68 Table 2: Averaged 10-cross validation of averaged perceptron on Section 02-21. proximation of an HPSG parser. We firstly created a CFG that approximates the original HPSG grammar, using the iterative method by Kiefer and Krieger (2000). Given the supertags as preterminals, the approximating CFG was then used for finding a maximally scored sequence of supertags which satisfies most of the grammatical constraints in the original HPSG grammar (Matsuzaki et al., 2007). By comparing the supertagging results before and after CFG-filtering, we can quantify how many errors are caused by ignorance of the long-range dependencies in supertagger. 4 Experiments and Analysis We conducted experiments on WSJ-HPSG treebank corpus (Miyao, 2006), which was semiautomatically converted from the WSJ portion of PennTreebank. The number of training iterations was set to 5 for all models. Gold-standard POS tags are used as input. The performance is evaluated by accuracy1 and speed of supertagging on an AMD Opteron 2.4GHz server. Table 2 shows the averaged results of 10- fold c</context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2007</marker>
<rawString>Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2007. Efficient hpsg parsing with supertagging and cfg-filtering. In Proceedings of IJCAI-07, pages 1671–1676.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
</authors>
<title>From Linguistic Theory to Syntactic Analysis: Corpus-Oriented Grammar Development and Feature Forest Model.</title>
<date>2006</date>
<tech>Ph.D. Dissertation,</tech>
<institution>The University of Tokyo.</institution>
<contexts>
<context position="8684" citStr="Miyao, 2006" startWordPosition="1373" endWordPosition="1374">ated a CFG that approximates the original HPSG grammar, using the iterative method by Kiefer and Krieger (2000). Given the supertags as preterminals, the approximating CFG was then used for finding a maximally scored sequence of supertags which satisfies most of the grammatical constraints in the original HPSG grammar (Matsuzaki et al., 2007). By comparing the supertagging results before and after CFG-filtering, we can quantify how many errors are caused by ignorance of the long-range dependencies in supertagger. 4 Experiments and Analysis We conducted experiments on WSJ-HPSG treebank corpus (Miyao, 2006), which was semiautomatically converted from the WSJ portion of PennTreebank. The number of training iterations was set to 5 for all models. Gold-standard POS tags are used as input. The performance is evaluated by accuracy1 and speed of supertagging on an AMD Opteron 2.4GHz server. Table 2 shows the averaged results of 10- fold cross-validation of averaged perceptron (AP) models2 on section 02-21. We can see the difference between point-wise AP model and sequential AP model is small (0.24%). It becomes even smaller after CFG-filtering (0.11%). Table 3 shows the supertagging accuracy on sectio</context>
</contexts>
<marker>Miyao, 2006</marker>
<rawString>Yusuke Miyao. 2006. From Linguistic Theory to Syntactic Analysis: Corpus-Oriented Grammar Development and Feature Forest Model. Ph.D. Dissertation, The University of Tokyo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takashi Ninomiya</author>
<author>Yoshimasa Tsuruoka</author>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
</authors>
<title>Extremely lexicalized models for accurate and fast hpsg parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP-2006,</booktitle>
<pages>155--163</pages>
<contexts>
<context position="1927" citStr="Ninomiya et al., 2006" startWordPosition="280" endWordPosition="283">e acquired by resorting to a light-weight parser. 1 Introduction Supertagging is an important part of lexicalized grammar parsing. A high performance supertagger greatly reduces the load of a parser and accelerates its speed. A supertag represents a linguistic word category, which encodes syntactic behavior of the word. The concept of supertagging was first proposed for lexicalized tree adjoining grammar (LTAG) (Bangalore and Joshi, 1999) and then extended to other lexicalized grammars, such as combinatory categorial grammar (CCG) (Clark, 2002) and Head-driven phrase structure grammar (HPSG) (Ninomiya et al., 2006). Recently, syntactic information in supertags has been exploited for NLP tasks besides parsing, such as NP chunking (Shen and Joshi, 2003), semantic role labeling (Chen and Rambow, 2003) and machine translation (Hassan et al., 2007). Supertagging serves there as an implicit and convenient way to incorporate rich syntactic information in those tasks. Improving the performance of supertagging can thus benefit these two aspects: as a preprocessor for deep parsing and as an independent, alternative technique for “almost” parsing. However, supertags are derived from a grammar and thus have a stron</context>
</contexts>
<marker>Ninomiya, Tsuruoka, Matsuzaki, Miyao, 2006</marker>
<rawString>Takashi Ninomiya, Yoshimasa Tsuruoka, Takuya Matsuzaki, and Yusuke Miyao. 2006. Extremely lexicalized models for accurate and fast hpsg parsing. In Proceedings of EMNLP-2006, pages 155–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<date>1994</date>
<institution>Head-driven Phrase Structure Grammar. University of Chicago / CSLI.</institution>
<contexts>
<context position="3562" citStr="Pollard and Sag, 1994" startWordPosition="537" endWordPosition="540">erent types of supertagging model, point-wise model and sequential model, for HPSG supertagging. CFGfilter was then used to empirically evaluate the effect of long distance dependencies in supertagging. The point-wise model achieved competitive result of 92.53% accuracy on WSJ-HPSG treebank with fast training speed, while the sequential model augmented with supertag edge features did not give much further improvement over the point-wise model. Big gains acquired by using CFG-filter indicates that further improvement may be achieved by resorting to a light-weight parser. 2 HPSG Supertags HPSG (Pollard and Sag, 1994) is a kind of lexicalized grammar. In HPSG, many lexical entries are used to express word-specific characteristics, 210 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 210–213, Paris, October 2009. c�2009 Association for Computational Linguistics while only small amount of rule schemas are used to describe general constructions. A supertag in HPSG corresponds to a template of lexical entry. For example, one possible supertag for “big” is “[&lt;ADJP&gt;]N lxm”, which indicates that the syntactic category of “big” is adjective and it modifies a noun to its right.</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl Pollard and Ivan A. Sag. 1994. Head-driven Phrase Structure Grammar. University of Chicago / CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Aravind K Joshi</author>
</authors>
<title>A snow based supertagger with application to np chunking.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>505--512</pages>
<contexts>
<context position="2066" citStr="Shen and Joshi, 2003" startWordPosition="303" endWordPosition="306">rformance supertagger greatly reduces the load of a parser and accelerates its speed. A supertag represents a linguistic word category, which encodes syntactic behavior of the word. The concept of supertagging was first proposed for lexicalized tree adjoining grammar (LTAG) (Bangalore and Joshi, 1999) and then extended to other lexicalized grammars, such as combinatory categorial grammar (CCG) (Clark, 2002) and Head-driven phrase structure grammar (HPSG) (Ninomiya et al., 2006). Recently, syntactic information in supertags has been exploited for NLP tasks besides parsing, such as NP chunking (Shen and Joshi, 2003), semantic role labeling (Chen and Rambow, 2003) and machine translation (Hassan et al., 2007). Supertagging serves there as an implicit and convenient way to incorporate rich syntactic information in those tasks. Improving the performance of supertagging can thus benefit these two aspects: as a preprocessor for deep parsing and as an independent, alternative technique for “almost” parsing. However, supertags are derived from a grammar and thus have a strong connection to parsing. To further improve the supertagging accuracy, the relation between supertagging and parsing is crucial. With this </context>
</contexts>
<marker>Shen, Joshi, 2003</marker>
<rawString>Libin Shen and Aravind K. Joshi. 2003. A snow based supertagger with application to np chunking. In Proceedings of ACL 2003, pages 505–512.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>