<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.022397">
<title confidence="0.963009">
Generating an Entailment Corpus from News Headlines
</title>
<author confidence="0.904098">
John Burger, Lisa Ferro
</author>
<affiliation confidence="0.573246">
The MITRE Corporation
</affiliation>
<address confidence="0.8915605">
202 Burlington Rd.
Bedford, MA 01730, USA
</address>
<email confidence="0.840993">
{john,lferro} @ mitre.org
</email>
<sectionHeader confidence="0.992325" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999850117647059">
We describe our efforts to generate a
large (100,000 instance) corpus of textual
entailment pairs from the lead paragraph
and headline of news articles. We manu-
ally inspected a small set of news stories
in order to locate the most productive
source of entailments, then built an anno-
tation interface for rapid manual evalua-
tion of further exemplars. With this
training data we built an SVM-based
document classifier, which we used for
corpus refinement purposes—we believe
that roughly three-quarters of the resulting
corpus are genuine entailment pairs. We
also discuss the difficulties inherent in
manual entailment judgment, and suggest
ways to ameliorate some of these.
</bodyText>
<sectionHeader confidence="0.999129" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999801769230769">
MITRE has a long-standing interest in robust text
understanding, and, like many, we believe that
adequate progress in such an endeavor requires a
well-designed evaluation methodology. We have
explored in great depth the use of human reading
comprehension exams for this purpose (Hirschman
et al., 1999, Wellner et al., 2005) as well as TREC-
style question answering (Burger, 2004).
In this context, the recent Pascal RTE evaluation
(Recognizing Textual Entailment, Dagan et al.,
2005) captured our interest. The goal of RTE is to
assess systems’ abilities at judging semantic en-
tailment with respect to a pair of sentences, e.g.:
</bodyText>
<listItem confidence="0.998897">
• Fred spilled wine on the carpet.
• The rug was wet.
</listItem>
<bodyText confidence="0.9999566">
In RTE parlance, the antecedent sentence is
known as the text, while the consequent sentence is
known as the hypothesis. Simply put, the chal-
lenge for an RTE system is to judge whether the
text entails the hypothesis. Judgments are Boo-
lean, and the primary evaluation metric is simple
accuracy, although there were other, secondary
metrics used in the evaluation.
The RTE organizers provided 567 exemplar sen-
tence pairs. This is adequate for system develop-
ment, but not for the application of large-scale
statistical models. In particular, we wished to cast
the problem as one of statistical alignment as used
in machine translation. MT systems typically use
millions of sentence pairs, and so we decided to
find or generate a much larger corpus. This paper
describes our efforts along these lines, as well as
some observations about the problems of annotat-
ing entailment data. In Section 2 we describe our
initial search for an entailment corpus. Section 3
briefly describes an annotation interface we de-
vised, as well as our efforts to refine our corpus.
Section 4 explains many of the issues and prob-
lems inherent in manual annotation of entailment
data.
</bodyText>
<sectionHeader confidence="0.917888" genericHeader="method">
2 Finding Entailment Data
</sectionHeader>
<bodyText confidence="0.9999106">
In our study of the Pascal RTE development cor-
pus, we found that a considerable majority of the
TRUE pairs exhibit a stronger relationship than
entailment; namely, the hypothesis is a paraphrase
of a subset of the text. For instance, given the text
</bodyText>
<page confidence="0.994488">
49
</page>
<note confidence="0.8832095">
Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 49–54,
Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<table confidence="0.996805777777778">
Source No. articles No. articles
examined in 1.5 mos.
miami-herald (US) 19 94,278
washington-post (US) 18 13,813
cs-monitor (US) 11 7,102
all-africa 18 68,521
dawn (Pakistan) 17 46,839
gulf-daily-news 10 26,837
national-post (Canada) 18 14,124
</table>
<figureCaption confidence="0.997568">
Figure 1: MiTAP News Sources Examined
</figureCaption>
<bodyText confidence="0.999860555555556">
John murdered Bill yesterday, the hypothesis Bill
is dead is an entailment, while the hypothesis Bill
was killed by John exhibits the stronger partial
paraphrase relationship to the text. We found that
94% (131/140) of the TRUE pairs in the Pascal
RTE dev2 corpus were these sorts of paraphrases.
In our search for an entailment corpus, we ob-
served that the headline of a news article is often a
partial paraphrase of the lead paragraph, much like
the RTE data, or is sometimes a genuine entail-
ment. We thus deduced that headlines and their
corresponding lead paragraphs might provide a
readily available source of training data. As an
initial test of this hypothesis, we manually in-
spected over 200 news stories from 11 different
sources. We found a great deal of variety in head-
line formats, and ultimately found the Xinhua
News Agency English Service articles from the
Gigaword corpus (Graff, 2003) to be the richest
source, though somewhat limited in subject do-
main. We describe here our data collection and
analysis process.
Because our goal was to automatically generate
an extremely large corpus of exemplars, we fo-
cused on large data sources. We first examined
111 news stories culled from MiTAP (Damianos et
al., 2003), which collects over one million articles
per month from approximately 75 different
sources. By first counting the number of articles
typically collected for each source, we selected a
mixture of sources that each had more than 10,000
articles for our sample period of one and half
months. As discussed further below, part way
through our investigation it became clear that we
needed to include more native English sources, so
the Christian Science Monitor articles were added,
</bodyText>
<table confidence="0.601889">
Yes No Maybe Total
All 54 (49%) 39 (35%) 18 (16%) 111
Pairs
Filtered 54 (53%) 33 (33%) 14 (14%) 101
</table>
<figureCaption confidence="0.994903">
Figure 2: MiTAP Corpus Results
</figureCaption>
<bodyText confidence="0.998867022727273">
though they fell below our arbitrary 10K mark.
Figure 1 summarizes the MiTAP news sources ex-
amined.
For each lead paragraph/headline pair, a human
rendered a judgment of yes, no, or maybe as to
whether the lead paragraph entailed the headline,
where maybe meant that the headline was very
close to being an entailment or paraphrase. This is
likely equivalent to the notion of “more or less se-
mantically equivalent” used in the Microsoft Re-
search Paraphrase Corpus (Dolan et al., 2005).
The purpose of maybe in this case was that we
thought that many of the near-miss pairs would
make adequate training data for statistical algo-
rithms, in spite of being less than perfect.
There were many types of news articles in the
MiTAP data that did not yield good headline/lead
paragraph pairs for our purposes. Many would be
difficult to filter out using automated heuristics.
Two frequent examples of this were opinion-ed
itorial pieces and daily Wall Street summaries.
Others would be more amenable to automatic
elimination, including obituaries and collections of
news snippets like the Washington Post’s “World
in Brief”. Articles consisting of personal narra-
tives never yielded good headlines, but these could
easily be eliminated by recognizing first person
pronouns in the lead paragraph. Figure 2 shows
the judgments for all the MiTAP articles examined,
where the Filtered row excludes these easily elimi-
nated article types.
As Figure 2 shows, the MiTAP data did not
yield a high percentage of good pairs. In addition,
whether due to poor machine translation or English
dialectal differences, our evaluator found it diffi-
cult to understand some of the text from sources
that were not English-primary. A certain amount
of ill-formed text was acceptable, since the Pascal
RTE challenge included training and test data
drawn from MT scenarios, but we did not wish our
data to be too dominated by such sources. Thus,
we selected additional native-English articles to
add to our sample set.
Despite the overall poor yield from this data, it
</bodyText>
<table confidence="0.492334666666667">
Source Yes No Maybe Total
APW 8 (31% ) 12 (46%) 6 (23%) 26
AFE 14 (56%) 4 (16%) 7 (28%) 25
NYT 8 (31%) 17 (65%) 1 (4%) 26
XIE 22 (85%) 4 (15%) 0 (0%) 26
Total 52 (50%) 37 (36%) 14 (14%) 103
</table>
<figureCaption confidence="0.976216">
Figure 3: Gigaword Corpus Results
</figureCaption>
<page confidence="0.968178">
50
</page>
<bodyText confidence="0.999981528735633">
document zoning tags visible so that the user can
easily identify the headline and lead paragraph. At
the top of the document are three buttons from
which to select a yes/no/maybe judgment. The
user can also add a comment before moving to the
next document. Typically several documents can
be judged per minute. The client-server architec-
ture supports multiple annotations of the same
document by different annotators—accordingly, it
has a mode enabling reconciliation of inter-
annotator disagreements. All further annotation
efforts discussed below were carried out with this
tool.
Using the tool, we tagged approximately 900
randomly chosen Gigaword documents, including
520 XIE documents. From this, we estimate that
70% of the XIE headlines in Gigaword are entailed
by the corresponding lead paragraph. (This is
lower than the rough estimate described in Section
2, but that was based on a very small sample.) We
decided to explore ways to refine the data in order
to arrive at a smaller, but less noisy subcorpus. We
observed that different subgenres within the news-
paper corpus evinced the lead-entails-headline
quality to different degrees. For example, articles
about sports or entertainment often had whimsical
(non-entailed) headlines, while articles about poli-
tics or business more frequently had the headline
quality we sought.
Accordingly, we decided to treat the data re-
finement process as a text classification problem,
one of finding the mix of genres or topics that
would most likely possess the lead-entails-headline
quality. We used SVM-light (Joachims, 2002) as a
document classifier, training it on the initial set of
annotated articles. (Note that these text classifica-
tion experiments made use of the entire article, not
just the lead and headline.) We experimented with
a variety of feature representations and SVM pa-
rameters, but found the best performance with a
Boolean bag-of-words representation, and a simple
linear kernel. Leave-one-out estimates indicate
that SVM-light could identify documents with the
requisite entailment quality with 77% accuracy.
We performed one round of active learning
(Tong &amp; Koller, 2000), in which we used SVM-
light to classify a large subset of the unannotated
corpus, and then selected a 100-document subset
about which the classifier was least certain. The
rationale is that annotating these uncertain docu-
ments will be more informative to further learning
runs than a randomly selected subset. In the case
of large-margin classifiers like SVMs, the natural
choice is to select the instances closest to the mar-
gin. These were then annotated, and added back to
the training data for the next learning run. How-
ever, leave-one-out estimates indicated that the
classifier benefited little from these new instances.
As described above, we estimate that the base
rate of the headline entailment property in the XIE
portion of Gigaword is 70%. Our hypothesis in
training the SVM was that we could identify a
smaller but less noisy subset. In order to evaluate
this, we ran the trained SVM on all 679,000 of the
unannotated XIE documents, and selected the
100,000 “best” instances—that is, the documents
most likely (according to the SVM) to evince the
headline quality. We selected a random subset of
these best documents, and annotated them to
evaluate our hypothesis. 74% of these possessed
the lead-entails-headline property, a difference of
4% absolute over the XIE base rate. We used the
lead-headline pairs from this 100,000-best subset
to train our MT-alignment-based system for the
RTE evaluation (Bayer et al., 2005). This system
was one of the best performers in the evaluation,
which we ascribe to our large training corpus
Later examination showed that the 4% “im-
provement” in purity is not statistically significant.
We intend to perform further experiments in data
refinement, but this may prove unnecessary. Per-
haps the base rate of the entailment phenomenon in
the XIE documents is sufficient to train an effec-
tive alignment-based entailment system. In this
case, all of the XIE documents could be used, per-
haps resulting in a more robust, and even better
performing system.
</bodyText>
<sectionHeader confidence="0.980657" genericHeader="method">
4 Judging Headline Entailments
</sectionHeader>
<bodyText confidence="0.9994925">
In the process of generating the training data, we
doubly-judged an additional 300 XIE documents to
measure inter-judge reliability. As in the pilot
phase described above, each pair was labeled as
yes, no, or maybe. In addition, the judges were
given a comment field to record their reasoning
and misgivings. The judging was performed in
two steps, first on a set of 100 documents and then
on a set of 200. One of the judges was already
well versed in the RTE task, and had performed the
earlier pilot investigations. Prior to judging the
first set, the second judge was given a brief verbal
</bodyText>
<page confidence="0.996004">
52
</page>
<table confidence="0.9839835">
Condition Set 1 Set 2
(100 docs) (200 docs)
strict match 75.00% 77.50%
maybe = yes 79.00% 90.00%
maybe = no 84.00% 81.00%
maybe = * 88.00% 94.00%
</table>
<figureCaption confidence="0.975896">
Figure 5: Agreement for Two XIE Data Sets
</figureCaption>
<bodyText confidence="0.999969928571429">
overview of the task. After the first 100 docu-
ments had been doubly-judged, the more experi-
enced judge then reviewed the differences and
drafted a set of guidelines. The guidelines pro-
vided a synopsis of the official RTE guidelines,
plus a few rules unique to headlines. For example,
one rule specified what to do when partial entail-
ment only held if the lead were combined with lo-
cation or date information from the dateline. The
two evaluators then judged the second set. The
results for both sets are shown in Figure 5.
As these results show, the guidelines had only a
small effect on the strict measure of agreement.
Three problem areas existed:
</bodyText>
<listItem confidence="0.9932881">
(1) Raw, messy data. The Gigaword corpus was
automatically collected and zoned. Thus, the head-
lines in particular contained a number of irregulari-
ties that made it difficult to judge their
appropriateness. Such irregularities included trun-
cations, phrases lacking any proposition, pre-
pended alerts like URGENT:, and bylines and date
lines miszoned into the headline.
(2) Disagreement on what constitutes synon-
ymy. Our judges found they had irreconcilable
differences about differences in meaning. For ex-
ample, in the following pair, the judges disagreed
about whether safe operation in the lead paragraph
meant the same thing as, and thus entailed, oper-
ates smoothly in the headline:
• Shanghai&apos;s Hongqiao Airport Operates Smoothly
• As of Saturday, Shanghai&apos;s Hongqiao Airport
has performed safe operation for some 2,600
consecutive days, setting a record in the country.
(3) Disagreement on the amount of world
</listItem>
<bodyText confidence="0.999472423728814">
knowledge permitted. Figure 5 shows that if
maybe is counted as equivalent to yes, the agree-
ment level improves significantly. This is likely
because there were two important aspects of the
RTE definition of entailment that were not im-
parted to the second judge until the written guide-
lines: that one can assume “common human
understanding of language and some common
background knowledge.” However, our judges did
not always agree on what counts as “common,”
which accounts for much of the high overlap be-
tween yes and maybe. Nevertheless, our 90%
agreement compares favorably to the 83% agree-
ment rate reported by Dolan et al. (2005) for their
judgments on “more or less semantically equiva-
lent” pairs. Our 78% strict agreement compares
favorably to the 80% agreement achieved by Da-
gan et al. (2005), given that our data was messier
than the pairs crafted for the RTE challenge.
Like Dagan et al. (2005), we did not force reso-
lution on all disagreements. Disagreements over
synonymy and common knowledge result in irrec-
oncilable differences, because it is neither possible
nor desirable to use guidelines to force a shared
understanding of an utterance. Thus, for the first
set of data 15 (15%) of the pairs were left unrecon-
ciled. In the second set, 42 (21%) were left un-
reconciled. Eleven (6%) of the irreconcilable pairs
in the second set were due to confusion stemming
from the telegraphic nature of headlines, which led
to misunderstandings about how to judge truncated
headlines (Chinese President Vows to Open New
Chapters With) vs. headlines lacking propositions
(subject headings like Mandela’s Speech) vs. well-
formed but terse headlines (Crackdown on Auto-
Mafia in Bulgaria).
Despite the high number of irreconcilable pairs,
one encouraging sign was evident from the com-
ment field. The judges’ comments revealed that on
pairs where they disagreed on how to label the
pair, they often agreed on what the problem was.
Our experience in generating a training corpus,
particularly the number of irreconcilable cases we
encountered, raises an important issue, namely, the
feasibility of semantic equivalence tasks. We sug-
gest that the optimum method for empirically
modeling semantic equivalence is to capture the
variation in human judgments. Three judges
would evaluate each pair, so that there would al-
ways be a tie breaker. After reconciling for dis-
agreements arising from human error, each distinct
judgment would become part of the data set. We
also recommend that where there is genuine dis-
agreement, the questionable portions of each pair
be annotated in some way to capture the source of
the problem, going one step further than the com-
ment field we found beneficial in our annotation
interface. The three judgments would result in a
four way classification of pairs:
</bodyText>
<page confidence="0.986709">
53
</page>
<bodyText confidence="0.9058028">
TTT = TRUE
TTF = Likely TRUE, but possibly FALSE
TFF = Likely FALSE, but possibly TRUE
FFF = FALSE
System developers could choose to train on all
the data, or limit themselves to the TTT/FFF cases.
For evaluation purposes, the systems’ results on
the TTF/TFF pairs could be evaluated in light of
the human variation, providing a more realistic
measure of the complexity of the task.
</bodyText>
<sectionHeader confidence="0.998865" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999733125">
Given the number of natural language processing
applications that require the ability to recognize
semantic equivalence and entailment, there is an
obvious need for both robust evaluation method-
ologies and adequate development and test data.
We’ve described here our work in generating sup-
plemental training data for the recent Pascal RTE
evaluation, with which we produced a competitive
system. Some news corpora provide a rich source
of exemplars, and an automatic document classifier
can be used to reduce the noisiness of the data.
There are lingering difficulties in achieving high
inter-judge agreement in determining paraphrase
and entailment, and we believe the best way to
cope with this is to allow the data to reflect the
variance that exists in cross-human judgments.
</bodyText>
<sectionHeader confidence="0.99764" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999907625">
This paper reports on work supported by the
MITRE Sponsored Research Program. We would
also like to extend our thanks to Sam Bayer, John
Henderson and Alex Yeh for their invaluable sug-
gestions and comments. Our gratitude also goes to
Laurie Damianos, who provided us with statistics
on MiTAP’s resources and served as one of the
evaluators in our inter-judge reliability study.
</bodyText>
<sectionHeader confidence="0.999278" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999709228070175">
Samuel Bayer, John Burger, Lisa Ferro, John
Henderson, and Alexander Yeh, 2005. MITRE’s
submissions to the EU Pascal RTE Challenge.
PASCAL Proceedings of the First Challenge Work-
shop, Recognizing Textual Entailment, 11–13 April,
2005, Southampton, U.K.
John D. Burger, 2004. MITRE’s Qanda at TREC-12.
The Twelvth Text REtrieval Conference. NIST Spe-
cial Publication SP 500–255.
Ido Dagan, Oren Glickman, and Bernado Magnini,
2005. The PASCAL recognizing textual entailment
challenge. PASCAL Proceedings of the First Chal-
lenge Workshop, Recognizing Textual Entailment,
11–13 April, 2005, Southampton, U.K.
Laurie Damianos, Steve Wohlever, Robyn Kozierok,
and Jay Ponte, 2003. mitap for real users, real data,
real problems. In Proceedings of the Conference on
Human Factors of Computing Systems (CHI 2003),
Fort Lauderdale, FL April 5–10.
David Day, John Aberdeen, Lynette Hirschman, Robyn
Kozierok, Patricia Robinson and Marc Vilain, 1997.
Mixed initiative development of language processing
systems. Proceedings of the Fifth Conference on Ap-
plied Natural Language Processing.
David Day, Chad McHenry, Robyn Kozierok, Laurel
Riek, 2004. Callisto: A configurable annotation
workbench. International Conference on Language
Resources and Evaluation.
Bill Dolan, Chris Brockett., and Chris Quirk, 2005.
Microsoft Research Paraphrase Corpus.
http://research.microsoft.com/research/nlp/msr_parap
hrase.htm
David Graff, 2003. English Gigaword.
http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2003T05
Dan Gusfield, 1997. Algorithms on Strings, Trees and
Sequences. Cambridge University Press.
Lynette Hirschman, Marc Light, Eric Breck, and John
D. Burger, 1999. Deep Read: A reading comprehen-
sion system. Proceedings of the 37th annual meeting
of the Association for Computational Linguistics.
Thorsten Joachims, 2002. Learning to Classify Text
Using Support Vector Machines. Kluwer.
Guido Minnen, John Carroll, and Darren Pearce, 2001.
Applied morphological processing of English. Natu-
ral Language Engineering, 7(3).
Franz Josef Och and Hermann Ney, 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1).
Simon Tong and Daphne Koller, 2000. support vector
machine active learning with applications to text
classification. Proceedings of ICML-00, 17th Inter-
national Conference on Machine Learning.
Ben Wellner, Lisa Ferro, Warren Greiff, and Lynette
Hirschman, 2005. Reading comprehension tests for
computer-based understanding evaluation. Natural
Language Engineering (to appear).
</reference>
<page confidence="0.999003">
54
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.899815">
<title confidence="0.999887">Generating an Entailment Corpus from News Headlines</title>
<author confidence="0.999879">John Burger</author>
<author confidence="0.999879">Lisa Ferro</author>
<affiliation confidence="0.963072">The MITRE</affiliation>
<address confidence="0.9959365">202 Burlington Bedford, MA 01730,</address>
<email confidence="0.97665">john@mitre.org</email>
<email confidence="0.97665">lferro@mitre.org</email>
<abstract confidence="0.997965222222222">We describe our efforts to generate a large (100,000 instance) corpus of textual entailment pairs from the lead paragraph and headline of news articles. We manually inspected a small set of news stories in order to locate the most productive source of entailments, then built an annotation interface for rapid manual evaluation of further exemplars. With this training data we built an SVM-based document classifier, which we used for corpus refinement purposes—we believe that roughly three-quarters of the resulting corpus are genuine entailment pairs. We also discuss the difficulties inherent in manual entailment judgment, and suggest ways to ameliorate some of these.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Samuel Bayer</author>
<author>John Burger</author>
<author>Lisa Ferro</author>
<author>John Henderson</author>
<author>Alexander Yeh</author>
</authors>
<title>MITRE’s submissions to the EU Pascal RTE Challenge.</title>
<date>2005</date>
<booktitle>PASCAL Proceedings of the First Challenge Workshop, Recognizing Textual Entailment,</booktitle>
<pages>11--13</pages>
<location>Southampton, U.K.</location>
<contexts>
<context position="11120" citStr="Bayer et al., 2005" startWordPosition="1800" endWordPosition="1803">a smaller but less noisy subset. In order to evaluate this, we ran the trained SVM on all 679,000 of the unannotated XIE documents, and selected the 100,000 “best” instances—that is, the documents most likely (according to the SVM) to evince the headline quality. We selected a random subset of these best documents, and annotated them to evaluate our hypothesis. 74% of these possessed the lead-entails-headline property, a difference of 4% absolute over the XIE base rate. We used the lead-headline pairs from this 100,000-best subset to train our MT-alignment-based system for the RTE evaluation (Bayer et al., 2005). This system was one of the best performers in the evaluation, which we ascribe to our large training corpus Later examination showed that the 4% “improvement” in purity is not statistically significant. We intend to perform further experiments in data refinement, but this may prove unnecessary. Perhaps the base rate of the entailment phenomenon in the XIE documents is sufficient to train an effective alignment-based entailment system. In this case, all of the XIE documents could be used, perhaps resulting in a more robust, and even better performing system. 4 Judging Headline Entailments In </context>
</contexts>
<marker>Bayer, Burger, Ferro, Henderson, Yeh, 2005</marker>
<rawString>Samuel Bayer, John Burger, Lisa Ferro, John Henderson, and Alexander Yeh, 2005. MITRE’s submissions to the EU Pascal RTE Challenge. PASCAL Proceedings of the First Challenge Workshop, Recognizing Textual Entailment, 11–13 April, 2005, Southampton, U.K.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Burger</author>
</authors>
<title>MITRE’s Qanda at TREC-12.</title>
<date>2004</date>
<booktitle>The Twelvth Text REtrieval Conference. NIST Special Publication SP</booktitle>
<pages>500--255</pages>
<contexts>
<context position="1243" citStr="Burger, 2004" startWordPosition="189" endWordPosition="190">rposes—we believe that roughly three-quarters of the resulting corpus are genuine entailment pairs. We also discuss the difficulties inherent in manual entailment judgment, and suggest ways to ameliorate some of these. 1 Introduction MITRE has a long-standing interest in robust text understanding, and, like many, we believe that adequate progress in such an endeavor requires a well-designed evaluation methodology. We have explored in great depth the use of human reading comprehension exams for this purpose (Hirschman et al., 1999, Wellner et al., 2005) as well as TRECstyle question answering (Burger, 2004). In this context, the recent Pascal RTE evaluation (Recognizing Textual Entailment, Dagan et al., 2005) captured our interest. The goal of RTE is to assess systems’ abilities at judging semantic entailment with respect to a pair of sentences, e.g.: • Fred spilled wine on the carpet. • The rug was wet. In RTE parlance, the antecedent sentence is known as the text, while the consequent sentence is known as the hypothesis. Simply put, the challenge for an RTE system is to judge whether the text entails the hypothesis. Judgments are Boolean, and the primary evaluation metric is simple accuracy, a</context>
</contexts>
<marker>Burger, 2004</marker>
<rawString>John D. Burger, 2004. MITRE’s Qanda at TREC-12. The Twelvth Text REtrieval Conference. NIST Special Publication SP 500–255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernado Magnini</author>
</authors>
<title>The PASCAL recognizing textual entailment challenge.</title>
<date>2005</date>
<booktitle>PASCAL Proceedings of the First Challenge Workshop, Recognizing Textual Entailment,</booktitle>
<pages>11--13</pages>
<location>Southampton, U.K.</location>
<contexts>
<context position="1347" citStr="Dagan et al., 2005" startWordPosition="202" endWordPosition="205">. We also discuss the difficulties inherent in manual entailment judgment, and suggest ways to ameliorate some of these. 1 Introduction MITRE has a long-standing interest in robust text understanding, and, like many, we believe that adequate progress in such an endeavor requires a well-designed evaluation methodology. We have explored in great depth the use of human reading comprehension exams for this purpose (Hirschman et al., 1999, Wellner et al., 2005) as well as TRECstyle question answering (Burger, 2004). In this context, the recent Pascal RTE evaluation (Recognizing Textual Entailment, Dagan et al., 2005) captured our interest. The goal of RTE is to assess systems’ abilities at judging semantic entailment with respect to a pair of sentences, e.g.: • Fred spilled wine on the carpet. • The rug was wet. In RTE parlance, the antecedent sentence is known as the text, while the consequent sentence is known as the hypothesis. Simply put, the challenge for an RTE system is to judge whether the text entails the hypothesis. Judgments are Boolean, and the primary evaluation metric is simple accuracy, although there were other, secondary metrics used in the evaluation. The RTE organizers provided 567 exem</context>
<context position="14864" citStr="Dagan et al. (2005)" startWordPosition="2424" endWordPosition="2428"> of the RTE definition of entailment that were not imparted to the second judge until the written guidelines: that one can assume “common human understanding of language and some common background knowledge.” However, our judges did not always agree on what counts as “common,” which accounts for much of the high overlap between yes and maybe. Nevertheless, our 90% agreement compares favorably to the 83% agreement rate reported by Dolan et al. (2005) for their judgments on “more or less semantically equivalent” pairs. Our 78% strict agreement compares favorably to the 80% agreement achieved by Dagan et al. (2005), given that our data was messier than the pairs crafted for the RTE challenge. Like Dagan et al. (2005), we did not force resolution on all disagreements. Disagreements over synonymy and common knowledge result in irreconcilable differences, because it is neither possible nor desirable to use guidelines to force a shared understanding of an utterance. Thus, for the first set of data 15 (15%) of the pairs were left unreconciled. In the second set, 42 (21%) were left unreconciled. Eleven (6%) of the irreconcilable pairs in the second set were due to confusion stemming from the telegraphic natur</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2005</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernado Magnini, 2005. The PASCAL recognizing textual entailment challenge. PASCAL Proceedings of the First Challenge Workshop, Recognizing Textual Entailment, 11–13 April, 2005, Southampton, U.K.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurie Damianos</author>
<author>Steve Wohlever</author>
<author>Robyn Kozierok</author>
<author>Jay Ponte</author>
</authors>
<title>mitap for real users, real data, real problems.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference on Human Factors of Computing Systems (CHI</booktitle>
<location>Fort Lauderdale, FL</location>
<contexts>
<context position="4670" citStr="Damianos et al., 2003" startWordPosition="750" endWordPosition="753">source of training data. As an initial test of this hypothesis, we manually inspected over 200 news stories from 11 different sources. We found a great deal of variety in headline formats, and ultimately found the Xinhua News Agency English Service articles from the Gigaword corpus (Graff, 2003) to be the richest source, though somewhat limited in subject domain. We describe here our data collection and analysis process. Because our goal was to automatically generate an extremely large corpus of exemplars, we focused on large data sources. We first examined 111 news stories culled from MiTAP (Damianos et al., 2003), which collects over one million articles per month from approximately 75 different sources. By first counting the number of articles typically collected for each source, we selected a mixture of sources that each had more than 10,000 articles for our sample period of one and half months. As discussed further below, part way through our investigation it became clear that we needed to include more native English sources, so the Christian Science Monitor articles were added, Yes No Maybe Total All 54 (49%) 39 (35%) 18 (16%) 111 Pairs Filtered 54 (53%) 33 (33%) 14 (14%) 101 Figure 2: MiTAP Corpu</context>
</contexts>
<marker>Damianos, Wohlever, Kozierok, Ponte, 2003</marker>
<rawString>Laurie Damianos, Steve Wohlever, Robyn Kozierok, and Jay Ponte, 2003. mitap for real users, real data, real problems. In Proceedings of the Conference on Human Factors of Computing Systems (CHI 2003), Fort Lauderdale, FL April 5–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Day</author>
<author>John Aberdeen</author>
<author>Lynette Hirschman</author>
<author>Robyn Kozierok</author>
<author>Patricia Robinson</author>
<author>Marc Vilain</author>
</authors>
<title>Mixed initiative development of language processing systems.</title>
<date>1997</date>
<booktitle>Proceedings of the Fifth Conference on Applied Natural Language Processing.</booktitle>
<marker>Day, Aberdeen, Hirschman, Kozierok, Robinson, Vilain, 1997</marker>
<rawString>David Day, John Aberdeen, Lynette Hirschman, Robyn Kozierok, Patricia Robinson and Marc Vilain, 1997. Mixed initiative development of language processing systems. Proceedings of the Fifth Conference on Applied Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Day</author>
<author>Chad McHenry</author>
<author>Robyn Kozierok</author>
<author>Laurel Riek</author>
</authors>
<title>Callisto: A configurable annotation workbench.</title>
<date>2004</date>
<booktitle>International Conference on Language Resources and Evaluation.</booktitle>
<marker>Day, McHenry, Kozierok, Riek, 2004</marker>
<rawString>David Day, Chad McHenry, Robyn Kozierok, Laurel Riek, 2004. Callisto: A configurable annotation workbench. International Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill Dolan</author>
<author>Chris Brockett</author>
<author>Chris Quirk</author>
</authors>
<date>2005</date>
<note>Microsoft Research Paraphrase Corpus. http://research.microsoft.com/research/nlp/msr_parap hrase.htm</note>
<contexts>
<context position="5764" citStr="Dolan et al., 2005" startWordPosition="934" endWordPosition="937">ed, Yes No Maybe Total All 54 (49%) 39 (35%) 18 (16%) 111 Pairs Filtered 54 (53%) 33 (33%) 14 (14%) 101 Figure 2: MiTAP Corpus Results though they fell below our arbitrary 10K mark. Figure 1 summarizes the MiTAP news sources examined. For each lead paragraph/headline pair, a human rendered a judgment of yes, no, or maybe as to whether the lead paragraph entailed the headline, where maybe meant that the headline was very close to being an entailment or paraphrase. This is likely equivalent to the notion of “more or less semantically equivalent” used in the Microsoft Research Paraphrase Corpus (Dolan et al., 2005). The purpose of maybe in this case was that we thought that many of the near-miss pairs would make adequate training data for statistical algorithms, in spite of being less than perfect. There were many types of news articles in the MiTAP data that did not yield good headline/lead paragraph pairs for our purposes. Many would be difficult to filter out using automated heuristics. Two frequent examples of this were opinion-ed itorial pieces and daily Wall Street summaries. Others would be more amenable to automatic elimination, including obituaries and collections of news snippets like the Wash</context>
<context position="14698" citStr="Dolan et al. (2005)" startWordPosition="2397" endWordPosition="2400">ted. Figure 5 shows that if maybe is counted as equivalent to yes, the agreement level improves significantly. This is likely because there were two important aspects of the RTE definition of entailment that were not imparted to the second judge until the written guidelines: that one can assume “common human understanding of language and some common background knowledge.” However, our judges did not always agree on what counts as “common,” which accounts for much of the high overlap between yes and maybe. Nevertheless, our 90% agreement compares favorably to the 83% agreement rate reported by Dolan et al. (2005) for their judgments on “more or less semantically equivalent” pairs. Our 78% strict agreement compares favorably to the 80% agreement achieved by Dagan et al. (2005), given that our data was messier than the pairs crafted for the RTE challenge. Like Dagan et al. (2005), we did not force resolution on all disagreements. Disagreements over synonymy and common knowledge result in irreconcilable differences, because it is neither possible nor desirable to use guidelines to force a shared understanding of an utterance. Thus, for the first set of data 15 (15%) of the pairs were left unreconciled. I</context>
</contexts>
<marker>Dolan, Brockett, Quirk, 2005</marker>
<rawString>Bill Dolan, Chris Brockett., and Chris Quirk, 2005. Microsoft Research Paraphrase Corpus. http://research.microsoft.com/research/nlp/msr_parap hrase.htm</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
</authors>
<date>2003</date>
<note>English Gigaword. http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp? catalogId=LDC2003T05</note>
<contexts>
<context position="4344" citStr="Graff, 2003" startWordPosition="698" endWordPosition="699">hrases. In our search for an entailment corpus, we observed that the headline of a news article is often a partial paraphrase of the lead paragraph, much like the RTE data, or is sometimes a genuine entailment. We thus deduced that headlines and their corresponding lead paragraphs might provide a readily available source of training data. As an initial test of this hypothesis, we manually inspected over 200 news stories from 11 different sources. We found a great deal of variety in headline formats, and ultimately found the Xinhua News Agency English Service articles from the Gigaword corpus (Graff, 2003) to be the richest source, though somewhat limited in subject domain. We describe here our data collection and analysis process. Because our goal was to automatically generate an extremely large corpus of exemplars, we focused on large data sources. We first examined 111 news stories culled from MiTAP (Damianos et al., 2003), which collects over one million articles per month from approximately 75 different sources. By first counting the number of articles typically collected for each source, we selected a mixture of sources that each had more than 10,000 articles for our sample period of one </context>
</contexts>
<marker>Graff, 2003</marker>
<rawString>David Graff, 2003. English Gigaword. http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp? catalogId=LDC2003T05</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gusfield</author>
</authors>
<title>Algorithms on Strings, Trees and Sequences.</title>
<date>1997</date>
<publisher>Cambridge University Press.</publisher>
<marker>Gusfield, 1997</marker>
<rawString>Dan Gusfield, 1997. Algorithms on Strings, Trees and Sequences. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynette Hirschman</author>
<author>Marc Light</author>
<author>Eric Breck</author>
<author>John D Burger</author>
</authors>
<title>Deep Read: A reading comprehension system.</title>
<date>1999</date>
<booktitle>Proceedings of the 37th annual meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1165" citStr="Hirschman et al., 1999" startWordPosition="174" endWordPosition="177"> data we built an SVM-based document classifier, which we used for corpus refinement purposes—we believe that roughly three-quarters of the resulting corpus are genuine entailment pairs. We also discuss the difficulties inherent in manual entailment judgment, and suggest ways to ameliorate some of these. 1 Introduction MITRE has a long-standing interest in robust text understanding, and, like many, we believe that adequate progress in such an endeavor requires a well-designed evaluation methodology. We have explored in great depth the use of human reading comprehension exams for this purpose (Hirschman et al., 1999, Wellner et al., 2005) as well as TRECstyle question answering (Burger, 2004). In this context, the recent Pascal RTE evaluation (Recognizing Textual Entailment, Dagan et al., 2005) captured our interest. The goal of RTE is to assess systems’ abilities at judging semantic entailment with respect to a pair of sentences, e.g.: • Fred spilled wine on the carpet. • The rug was wet. In RTE parlance, the antecedent sentence is known as the text, while the consequent sentence is known as the hypothesis. Simply put, the challenge for an RTE system is to judge whether the text entails the hypothesis. </context>
</contexts>
<marker>Hirschman, Light, Breck, Burger, 1999</marker>
<rawString>Lynette Hirschman, Marc Light, Eric Breck, and John D. Burger, 1999. Deep Read: A reading comprehension system. Proceedings of the 37th annual meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Learning to Classify Text Using Support Vector Machines.</title>
<date>2002</date>
<publisher>Kluwer.</publisher>
<contexts>
<context position="9118" citStr="Joachims, 2002" startWordPosition="1483" endWordPosition="1484">order to arrive at a smaller, but less noisy subcorpus. We observed that different subgenres within the newspaper corpus evinced the lead-entails-headline quality to different degrees. For example, articles about sports or entertainment often had whimsical (non-entailed) headlines, while articles about politics or business more frequently had the headline quality we sought. Accordingly, we decided to treat the data refinement process as a text classification problem, one of finding the mix of genres or topics that would most likely possess the lead-entails-headline quality. We used SVM-light (Joachims, 2002) as a document classifier, training it on the initial set of annotated articles. (Note that these text classification experiments made use of the entire article, not just the lead and headline.) We experimented with a variety of feature representations and SVM parameters, but found the best performance with a Boolean bag-of-words representation, and a simple linear kernel. Leave-one-out estimates indicate that SVM-light could identify documents with the requisite entailment quality with 77% accuracy. We performed one round of active learning (Tong &amp; Koller, 2000), in which we used SVMlight to </context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>Thorsten Joachims, 2002. Learning to Classify Text Using Support Vector Machines. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guido Minnen</author>
<author>John Carroll</author>
<author>Darren Pearce</author>
</authors>
<title>Applied morphological processing of English.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>3</issue>
<marker>Minnen, Carroll, Pearce, 2001</marker>
<rawString>Guido Minnen, John Carroll, and Darren Pearce, 2001. Applied morphological processing of English. Natural Language Engineering, 7(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney, 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Tong</author>
<author>Daphne Koller</author>
</authors>
<title>support vector machine active learning with applications to text classification.</title>
<date>2000</date>
<booktitle>Proceedings of ICML-00, 17th International Conference on Machine Learning.</booktitle>
<contexts>
<context position="9687" citStr="Tong &amp; Koller, 2000" startWordPosition="1567" endWordPosition="1570">eadline quality. We used SVM-light (Joachims, 2002) as a document classifier, training it on the initial set of annotated articles. (Note that these text classification experiments made use of the entire article, not just the lead and headline.) We experimented with a variety of feature representations and SVM parameters, but found the best performance with a Boolean bag-of-words representation, and a simple linear kernel. Leave-one-out estimates indicate that SVM-light could identify documents with the requisite entailment quality with 77% accuracy. We performed one round of active learning (Tong &amp; Koller, 2000), in which we used SVMlight to classify a large subset of the unannotated corpus, and then selected a 100-document subset about which the classifier was least certain. The rationale is that annotating these uncertain documents will be more informative to further learning runs than a randomly selected subset. In the case of large-margin classifiers like SVMs, the natural choice is to select the instances closest to the margin. These were then annotated, and added back to the training data for the next learning run. However, leave-one-out estimates indicated that the classifier benefited little </context>
</contexts>
<marker>Tong, Koller, 2000</marker>
<rawString>Simon Tong and Daphne Koller, 2000. support vector machine active learning with applications to text classification. Proceedings of ICML-00, 17th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Wellner</author>
<author>Lisa Ferro</author>
<author>Warren Greiff</author>
<author>Lynette Hirschman</author>
</authors>
<title>Reading comprehension tests for computer-based understanding evaluation.</title>
<date>2005</date>
<journal>Natural Language Engineering</journal>
<note>(to appear).</note>
<contexts>
<context position="1188" citStr="Wellner et al., 2005" startWordPosition="178" endWordPosition="181">sed document classifier, which we used for corpus refinement purposes—we believe that roughly three-quarters of the resulting corpus are genuine entailment pairs. We also discuss the difficulties inherent in manual entailment judgment, and suggest ways to ameliorate some of these. 1 Introduction MITRE has a long-standing interest in robust text understanding, and, like many, we believe that adequate progress in such an endeavor requires a well-designed evaluation methodology. We have explored in great depth the use of human reading comprehension exams for this purpose (Hirschman et al., 1999, Wellner et al., 2005) as well as TRECstyle question answering (Burger, 2004). In this context, the recent Pascal RTE evaluation (Recognizing Textual Entailment, Dagan et al., 2005) captured our interest. The goal of RTE is to assess systems’ abilities at judging semantic entailment with respect to a pair of sentences, e.g.: • Fred spilled wine on the carpet. • The rug was wet. In RTE parlance, the antecedent sentence is known as the text, while the consequent sentence is known as the hypothesis. Simply put, the challenge for an RTE system is to judge whether the text entails the hypothesis. Judgments are Boolean, </context>
</contexts>
<marker>Wellner, Ferro, Greiff, Hirschman, 2005</marker>
<rawString>Ben Wellner, Lisa Ferro, Warren Greiff, and Lynette Hirschman, 2005. Reading comprehension tests for computer-based understanding evaluation. Natural Language Engineering (to appear).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>