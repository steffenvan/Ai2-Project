<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006575">
<title confidence="0.917001">
Surface Realisation from Knowledge-Bases
</title>
<author confidence="0.899016">
Bikash Gyawali Claire Gardent
</author>
<affiliation confidence="0.845128">
Universit´e de Lorraine, LORIA CNRS, LORIA, UMR 7503
</affiliation>
<address confidence="0.8469">
Villers-l`es-Nancy, F-54600, France Vandoeuvre-l`es-Nancy, F-54500, France
</address>
<email confidence="0.990194">
bikash.gyawali@loria.fr claire.gardent@loria.fr
</email>
<sectionHeader confidence="0.993635" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999967588235294">
We present a simple, data-driven approach
to generation from knowledge bases (KB).
A key feature of this approach is that
grammar induction is driven by the ex-
tended domain of locality principle of
TAG (Tree Adjoining Grammar); and that
it takes into account both syntactic and
semantic information. The resulting ex-
tracted TAG includes a unification based
semantics and can be used by an existing
surface realiser to generate sentences from
KB data. Experimental evaluation on the
KBGen data shows that our model outper-
forms a data-driven generate-and-rank ap-
proach based on an automatically induced
probabilistic grammar; and is comparable
with a handcrafted symbolic approach.
</bodyText>
<sectionHeader confidence="0.998995" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998812689655173">
In this paper we present a grammar based ap-
proach for generating from knowledge bases (KB)
which is linguistically principled and conceptually
simple. A key feature of this approach is that
grammar induction is driven by the extended do-
main of locality principle of TAG (Tree Adjoining
Grammar) and takes into account both syntactic
and semantic information. The resulting extracted
TAGs include a unification based semantics and
can be used by an existing surface realiser to gen-
erate sentences from KB data.
To evaluate our approach, we use the bench-
mark provided by the KBGen challenge (Banik
et al., 2012; Banik et al., 2013), a challenge
designed to evaluate generation from knowledge
bases; where the input is a KB subset; and where
the expected output is a complex sentence convey-
ing the meaning represented by the input. When
compared with two other systems having taken
part in the KBGen challenge, our system outper-
forms a data-driven, generate-and-rank approach
based on an automatically induced probabilis-
tic grammar; and produces results comparable to
those obtained by a symbolic, rule based approach.
Most importantly, we obtain these results using a
general purpose approach that we believe is sim-
pler and more transparent than current state of the
art surface realisation systems generating from KB
or DB data.
</bodyText>
<sectionHeader confidence="0.999808" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999951322580645">
Our work is related to work on concept to text gen-
eration.
Earlier work on concept to text generation
mainly focuses on generation from logical forms
using rule-based methods. (Wang, 1980) uses
hand-written rules to generate sentences from an
extended predicate logic formalism; (Shieber et
al., 1990) introduces a head-driven algorithm for
generating from logical forms; (Kay, 1996) de-
fines a chart based algorithm which enhances effi-
ciency by minimising the number of semantically
incomplete phrases being built; and (Shemtov,
1996) presents an extension of the chart based gen-
eration algorithm presented in (Kay, 1996) which
supports the generation of multiple paraphrases
from underspecified semantic input. In all these
approaches, grammar and lexicon are developed
manually and it is assumed that the lexicon as-
sociates semantic sub-formulae with natural lan-
guage expressions. Our approach is similar to
these approaches in that it assumes a grammar en-
coding a compositional semantics. It differs from
them however in that, in our approach, grammar
and lexicon are automatically acquired from the
data.
With the development of the semantic web and
the proliferation of knowledge bases, generation
from knowledge bases has attracted increased in-
terest and so called ontology verbalisers have
been proposed which support the generation of
text from (parts of) knowledge bases. One main
</bodyText>
<page confidence="0.98462">
424
</page>
<note confidence="0.8309655">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 424–434,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.99996212745098">
strand of work maps each axiom in the knowledge
base to a clause. Thus the OWL verbaliser inte-
grated in the Prot´eg´e tool (Kaljurand and Fuchs,
2007) provides a verbalisation of every axiom
present in the ontology under consideration and
(Wilcock, 2003) describes an ontology verbaliser
using XML-based generation. As discussed in
(Power and Third, 2010), one important limita-
tion of these approaches is that they assume a
simple deterministic mapping between knowledge
representation languages and some controlled nat-
ural language (CNL). Specifically, the assump-
tion is that each atomic term (individual, class,
property) maps to a word and each axiom maps
to a sentence. As a result, the verbalisation of
larger ontology parts can produce very unnatural
text such as, Every cat is an animal. Every dog
is an animal. Every horse is an animal. Every
rabbit is an animal. More generally, the CNL
based approaches to ontology verbalisation gen-
erate clauses (one per axiom) rather than complex
sentences and thus cannot adequately handle the
verbalisation of more complex input such as the
KBGen data where the KB input often requires the
generation of a complex sentence rather than a se-
quence of base clauses.
To generate more complex output from KB
data, several alternative approaches have been pro-
posed.
The MIAKT project (Bontcheva and Wilks.,
2004) and the ONTOGENERATION project
(Aguado et al., 1998) use symbolic NLG tech-
niques to produce textual descriptions from some
semantic information contained in a knowledge
base. Both systems require some manual in-
put (lexicons and domain schemas). More so-
phisticated NLG systems such as TAILOR (Paris,
1988), MIGRAINE (Mittal et al., 1994), and
STOP (Reiter et al., 2003) offer tailored output
based on user/patient models. While offering
more flexibility and expressiveness, these systems
are difficult to adapt by non-NLG experts because
they require the user to understand the architec-
ture of the NLG systems (Bontcheva and Wilks.,
2004). Similarly, the NaturalOWL system (Gala-
nis et al., 2009) has been proposed to generate flu-
ent descriptions of museum exhibits from an OWL
ontology. This approach however relies on exten-
sive manual annotation of the input data.
The SWAT project has focused on producing
descriptions of ontologies that are both coherent
and efficient (Williams and Power, 2010). For in-
stance, instead of the above output, the SWAT sys-
tem would generate the sentence: The following
are kinds of animals: cats, dogs, horses and rab-
bits.. In this approach too however, the verbaliser
output is strongly constrained by a simple Definite
Clause Grammar covering simple clauses and sen-
tences verbalising aggregation patterns such as the
above. More generally, the sentences generated by
ontology verbalisers cover a limited set of linguis-
tics constructions; the grammar used is manually
defined; and the mapping between semantics and
strings is assumed to be deterministic (e.g., a verb
maps to a relation and a noun to a concept). In
constrast, we propose an approach which can gen-
erate complex sentences from KB data; where the
grammar is acquired from the data; and where no
assumption is made about the mapping between
semantics and NL expressions.
Recent work has focused on data-driven gener-
ation from frames, lambda terms and data base en-
tries.
(DeVault et al., 2008) describes an approach for
generating from the frames produced by a dialog
system. They induce a probabilistic Tree Adjoin-
ing Grammar from a training set aligning frames
and sentences using the grammar induction tech-
nique of (Chiang, 2000) and use a beam search
that uses weighted features learned from the train-
ing data to rank alternative expansions at each
step.
(Lu and Ng, 2011) focuses on generating nat-
ural language sentences from logical form (i.e.,
lambda terms) using a synchronous context-free
grammar. They introduce a novel synchronous
context free grammar formalism for generating
from lambda terms; induce such a synchronous
grammar using a generative model; and extract the
best output sentence from the generated forest us-
ing a log linear model.
(Wong and Mooney, 2007; Lu et al., 2009)
focuses on generating from variable-free tree-
structured representations such as the CLANG for-
mal language used in the ROBOCUP competition
and the database entries collected by (Liang et
al., 2009) for weather forecast generation and for
the air travel domain (ATIS dataset) by (Dahl et
al., 1994). (Wong and Mooney, 2007) uses syn-
chronous grammars to transform a variable free
tree structured meaning representation into sen-
tences. (Lu et al., 2009) uses a Conditional Ran-
</bodyText>
<page confidence="0.998243">
425
</page>
<bodyText confidence="0.978498">
The function of a gated channel is to release particles from the endoplasmic reticulum
</bodyText>
<equation confidence="0.9996972">
:TRIPLES (
(|Release-Of-Calcium646 ||object ||Particle-In-Motion64582|)
(|Release-Of-Calcium646 ||base ||Endoplasmic-Reticulum64603|)
(|Gated-Channel64605 ||has-function||Release-Of-Calcium646|)
(|Release-Of-Calcium646 ||agent ||Gated-Channel64605|))
:INSTANCE-TYPES
(|Particle-In-Motion64582 ||instance-of ||Particle-In-Motion|)
(|Endoplasmic-Reticulum64603 ||instance-of ||Endoplasmic-Reticulum|)
(|Gated-Channel64605 ||instance-of ||Gated-Channel|)
|Release-Of-Calcium646 ||instance-of ||Release-Of-Calcium|))
:ROOT-TYPES (
(|Release-Of-Calcium646 ||instance-of ||Event|)
(|Particle-In-Motion64582 ||instance-of ||Entity|)
(|Endoplasmic-Reticulum64603 ||instance-of ||Entity|)
(|Gated-Channel64605 ||instance-of ||Entity|)))
</equation>
<figureCaption confidence="0.998741">
Figure 1: Example KBGEN Scenario
</figureCaption>
<bodyText confidence="0.999878115384615">
dom Field to generate from the same meaning rep-
resentations.
Finally, more recent papers propose approaches
which perform both surface realisation and con-
tent selection. (Angeli et al., 2010) proposes a log
linear model which decomposes into a sequence
of discriminative local decisions. The first classi-
fier determines which records to mention; the sec-
ond, which fields of these records to select; and the
third, which words to use to verbalise the selected
fields. (Kim and Mooney, 2010) uses a genera-
tive model for content selection and verbalises the
selected input using WASP−1, an existing gener-
ator. Finally, (Konstas and Lapata, 2012b; Kon-
stas and Lapata, 2012a) develop a joint optimi-
sation approach for content selection and surface
realisation using a generic, domain independent
probabilistic grammar which captures the struc-
ture of the database and the mapping from fields
to strings. They intersect the grammar with a lan-
guage model to improve fluency; use a weighted
hypergraph to pack the derivations; and find the
best derivation tree using Viterbi algorithm.
Our approach differs from the approaches
which assume variable free tree structured repre-
sentations (Wong and Mooney, 2007; Lu et al.,
2009) and data-based entries (Kim and Mooney,
2010; Konstas and Lapata, 2012b; Konstas and
Lapata, 2012a) in that it handles graph-based, KB
input and assumes a compositional semantics. It
is closest to (DeVault et al., 2008) and (Lu and
Ng, 2011) who extract a grammar encoding syn-
tax and semantics from frames and lambda terms
respectively. It differs from the former however in
that it enforces a tighter syntax/semantics integra-
tion by requiring that the elementary trees of our
extracted grammar encode the appropriate linking
information. While (DeVault et al., 2008) extracts
a TAG grammar associating each elementary tree
with a semantics, we additionnally require that
these trees encode the appropriate linking between
syntactic and semantic arguments thereby restrict-
ing the space of possible tree combinations and
drastically reducing the search space. Although
conceptually related to (Lu and Ng, 2011), our ap-
proach extracts a unification based grammar rather
than one with lambda terms. The extraction pro-
cess and the generation algorithms are also funda-
mentally different. We use a simple mainly sym-
bolic approach whereas they use a generative ap-
proach for grammar induction and a discriminative
approach for sentence generation.
</bodyText>
<sectionHeader confidence="0.95311" genericHeader="method">
3 The KBGen Task
</sectionHeader>
<bodyText confidence="0.984331625">
The KBGen task was introduced as a new shared
task at Generation Challenges 2013 (Banik et al.,
2013)1 and aimed to compare different generation
systems on KB data. Specifically, the task is to
verbalise a subset of a knowledge base. For in-
stance, the KB input shown in Figure 1 can be ver-
balised as:
(1) The function of a gated channel is to release
particles from the endoplasmic reticulum
The KB subsets forming the KBGen input data
were pre-selected from the AURA biology knowl-
edge base (Gunning et al., 2010), a knowledge
base about biology which was manually encoded
by biology teachers and encodes knowledge about
events, entities, properties and relations where
relations include event-to-entity, event-to-event,
</bodyText>
<footnote confidence="0.996732">
1http://www.kbgen.org
</footnote>
<page confidence="0.998164">
426
</page>
<figure confidence="0.98894725">
NPER
DT NN NN
the endoplasmic reticulum
instance-of(ER,Endoplasmic-Reticulum)
</figure>
<figureCaption confidence="0.987075666666667">
Figure 2: Example FB-LTAG with Unification-Based Semantics. Dotted lines indicate substitution and
adjunction operations between trees. The variables decorating the tree nodes (e.g., GC) abbreviate fea-
ture structures of the form [idx : V ] where V is a unification variable shared with the semantics.
</figureCaption>
<figure confidence="0.9956403">
NPGC
DT NN NN
a gated channel
instance-of(GC,Gated-Channel)
SRoC1
NP↓GC RoC1
VPRoC
VBZRoC NP↓P M
releases
instance-of(RoC,Release-of-Calcium)
object(RoC,PM)
agent(RoC,GC)
NPPM
particles
instance-of(PM,Particle-In-Motion)
VPRoC
VP∗RoC PP
IN NPI ER
from J
base(RoC,ER)
</figure>
<bodyText confidence="0.962994666666667">
event-to-property and entity-to-property relations.
AURA uses a frame-based knowledge representa-
tion and reasoning system called Knowledge Ma-
chine (Clark and Porter, 1997) which was trans-
lated into first-order logic with equality and from
there, into multiple different formats including
SILK (Grosof, 2012) and OWL2 (Motik et al.,
2009). It is available for download in various for-
mats including OWL2.
4 Generating from the KBGen
Knowledge-Base
To generate from the KBGen data, we induce a
Feature-Based Lexicalised Tree Adjoining Gram-
mar (FB-LTAG, (Vijay-Shanker and Joshi, 1988))
augmented with a unification-based semantics
(Gardent and Kallmeyer, 2003) from the training
data. We then use this grammar and an existing
surface realiser to generate from the test data.
</bodyText>
<subsectionHeader confidence="0.992787">
4.1 Feature-Based Lexicalised Tree
Adjoining Grammar
</subsectionHeader>
<bodyText confidence="0.995850625">
Figure 2 shows an example FB-LTAG augmented
with a unification-based semantics.
Briefly, an FB-LTAG consists of a set of ele-
mentary trees which can be either initial or auxil-
iary. Initial trees are trees whose leaves are labeled
with substitution nodes (marked with a down-
arrow) or terminal categories. Auxiliary trees are
distinguished by a foot node (marked with a star)
</bodyText>
<footnote confidence="0.940469">
2http://www.ai.sri.com/halo/
halobook2010/exported-kb/biokb.html
</footnote>
<bodyText confidence="0.99989665625">
whose category must be the same as that of the
root node. In addition, in an FB-LTAG, each el-
ementary tree is anchored by a lexical item (lexi-
calisation) and the nodes in the elementary trees
are decorated with two feature structures called
top and bottom which are unified during deriva-
tion. Two tree-composition operations are used
to combine trees namely, substitution and adjunc-
tion. While substitution inserts a tree in a substi-
tution node of another tree, adjunction inserts an
auxiliary tree into a tree. In terms of unifications,
substitution unifies the top feature structure of the
substitution node with the top feature structure of
the root of the tree being substituted in. Adjunc-
tion unifies the top feature structure of the root of
the tree being adjoined with the top feature struc-
ture of the node being adjoined to; and the bottom
feature structure of the foot node of the auxiliary
tree being adjoined with the bottom feature struc-
ture of the node being adjoined to.
In an FB-LTAG augmented with a unification-
based semantics, each tree is associated with a
semantics i.e., a set of literals whose arguments
may be constants or unification variables. The
semantics of a derived tree is the union of the
semantics of the tree contributing to its deriva-
tion modulo unification. Importantly, semantic
variables are shared with syntactic variables
(i.e., variables occurring in the feature structures
decorating the tree nodes) so that when trees are
combined, the appropriate syntax/semantics link-
ing is enforced. For instance given the semantics:
</bodyText>
<page confidence="0.995468">
427
</page>
<construct confidence="0.868527">
instance-of(RoC,Release-Of-Calcium),
object(RoC,PM),agent(RoC,GC),base(RoC,ER),
instance-of(ER,Endoplasmic-Reticulum),
instance-of(GC,Gated-Channel),
instance-of(PM,Particle-In-Motion)
</construct>
<bodyText confidence="0.99613625">
the grammar will generate A gated channel re-
leases particles from the endoplasmic reticulum
but not e.g., Particles releases a gated channel
from the endoplasmic reticulum.
</bodyText>
<subsectionHeader confidence="0.985907">
4.2 Grammar Extraction
</subsectionHeader>
<bodyText confidence="0.9998175">
We extract our FB-LTAG with unification seman-
tics from the KBGen training data in two main
steps. First, we align the KB data with the input
string. Second, we induce a Tree Adjoining Gram-
mar augmented with a unification-based semantics
from the aligned data.
</bodyText>
<subsectionHeader confidence="0.659304">
4.2.1 Alignment
</subsectionHeader>
<bodyText confidence="0.999923606060606">
Given a Sentence/Input pair (S, I) provided by the
KBGen Challenge, the alignment procedure asso-
ciates each entity and event variable in I to a sub-
string in S. To do this, we use the entity and the
event lexicon provided by the KBGen organiser.
The event lexicon maps event types to verbs, their
inflected forms and nominalizations while the en-
tity lexicon maps entity types to a noun and its
plural form. For instance, the lexicon entries for
the event and entity types shown in Figure 1 are as
shown in Figure 3.
For each entity and each event vari-
able V in I, we retrieve the corresponding
type (e.g., Particle-Tn-Motion for
Particle-Tn-Motion64582); search
the KBGen lexicon for the corresponding phrases
(e.g., molecule in motion,molecules in motion);
and associate V with the phrase in S which
matches one of these phrases. Figure 3 shows
an example lexicon and the resulting alignment
obtained for the scenario shown in Figure 1. Note
that there is not always an exact match between
the phrase associated in the KBGen lexicon with
a type and the phrase occurring in the training
sentence. To account for this, we use some
additional similarity based heuristics to identify
the phrase in the input string that is most likely
to be associated with a variable lacking an exact
match in the input string. E.g., for entity variables
(e.g., Particle-Tn-Motion64582), we
search the input string for nouns (e.g., particles)
whose overlap with the variable type (e.g.,
Particle-In-Motion) is not empty.
</bodyText>
<subsectionHeader confidence="0.937277">
4.2.2 Inducing a based FB-LTAG from the
aligned data
</subsectionHeader>
<bodyText confidence="0.99543892">
To extract a Feature-Based Lexicalised Tree
Adjoining Grammar (FB-LTAG) from the KBGen
data, we parse the sentences of the training cor-
pus; project the entity and event variables to the
syntactic projection of the strings they are aligned
with; and extract the elementary trees of the result-
ing FB-LTAG from the parse tree using semantic
information. Figure 4 shows the trees extracted
from the scenario given in Figure 1.
To associate each training example sentence
with a syntactic parse, we use the Stanford parser.
After alignment, the entity and event variables oc-
curring in the input semantics are associated with
substrings of the yield of the syntactic parse tree.
We project these variables up the syntactic tree to
reflect headedness. A variable aligned with a noun
is projected to the NP level or to the immediately
dominating PP if it occurs in the subtree domi-
nated by the leftmost daughter of that PP. A vari-
able aligned with a verb is projected to the first S
node immediately dominating that verb or, in the
case of a predicative sentence, to the root of that
sentence3.
Once entity and event variables have been pro-
jected up the parse trees, we extract elementary
FB-LTAG trees and their semantics from the input
scenario as follows.
First, the subtrees whose root node is indexed
with an entity variable are extracted. This results
in a set of NP and PP trees anchored with entity
names and associated with the predication true of
the indexing variable.
Second, the subtrees capturing relations be-
tween variables are extracted. To perform this ex-
traction, each input variable X is associated with a
set of dependent variables i.e., the set of variables
Y such that X is related to Y (R(X, Y )). The
minimal tree containing all and only the dependent
variables D(X) of a variable X is then extracted
and associated with the set of literals Φ such that
Φ = {R(Y,Z)  |(Y = X ∧Z E D(X))v(Y,Z E
D(X))}. This procedure extracts the subtrees re-
lating the argument variables of a semantic func-
tors such as an event or a role e.g., a tree describ-
ing a verb and its arguments as shown in the top
3Initially, we used the head information provided by the
Stanford parser. In practice however, we found that the
heuristics we defined to project semantic variables to the cor-
responding syntactic projection were more accurate and bet-
ter supported our grammar extraction process.
</bodyText>
<page confidence="0.993261">
428
</page>
<table confidence="0.863633285714286">
Particle-In-Motion molecule in motion,molecules in motion
Endoplasmic-Reticulum endoplasmic reticulum,endoplasmic reticulum
Gated-Channel gated Channel,gated Channels
Release-Of-Calcium releases,release,released,release
The function of a (gated channel, Gated-Channel64605) is to (release,
Release-Of-Calcium646) (particles, Particle-In-Motion64582) from the (endoplas-
mic reticulum, Endoplasmic-Reticulum64603 )
</table>
<figureCaption confidence="0.963858">
Figure 3: Example Entries from the KBGen Lexicon and example alignment
</figureCaption>
<figure confidence="0.35840788">
SRoC3
NP VPRoC3
RoC2
SRoC2
NP PP VBZ
RoC1
DT NN IN NP↓GC is VPRoC1
RoC
the fn of TO VBRoC NP↓PM PP
to release IN NP I ER
from 1
NPGC instance-of(RoC,Release-of-Calcium) NPER
object(RoC,PM)
base(RoC,ER)
has-function(GC,RoC)
agent(RoC,GC)
NPPM
particles
instance-of(PM,Particle-In-Motion)
DT NN NN
the endoplasmic reticulum
instance-of(ER,Endoplasmic-Reticulum)
DT NN NN
a gated channel
instance-of(GC,Gated-Channel)
</figure>
<figureCaption confidence="0.885979666666667">
Figure 4: Extracted Grammar for “The function of a gated channel is to release particles from the endoplasmic reticulum”.
Variable names have been abbreviated and the KBGen tuple notation converted to terms so as to fit the input format expected by
our surface realiser.
</figureCaption>
<page confidence="0.998051">
429
</page>
<bodyText confidence="0.999967304347826">
part of Figure 4. Note that such a tree may cap-
ture a verb occurring in a relative or a subordinate
clause (together with its arguments) thus allowing
for complex sentences including a relative or re-
lating a main and a subordinate clause.
The resulting grammar extracted from the parse
trees (cf. e.g., Figure 4) is a Feature-Based
Tree Adjoining Grammar with a Unification-based
compositional semantics as described in (Gardent
and Kallmeyer, 2003). In particular, our gram-
mars differs from the traditional probabilistic Tree
Adjoining Grammar extracted as described in e.g.,
(Chiang, 2000) in that they encode both syntax and
semantics rather than just syntax. They also differ
from the semantic FB-TAG extracted by (DeVault
et al., 2008) in that (i) they encode the linking be-
tween syntactic and semantic arguments; (ii) they
allow for elementary trees spanning discontiguous
strings (e.g., The function of X is to release Y); and
(iii) they enforce the semantic principle underly-
ing TAG namely that an elementary tree contain-
ing a syntactic functor also contains its syntactic
arguments.
</bodyText>
<subsectionHeader confidence="0.990477">
4.3 Generation
</subsectionHeader>
<bodyText confidence="0.99679012">
To generate with the grammar extracted from the
KBGen data, we use the GenI surface realiser (Gar-
dent et al., 2007). Briefly, given an input seman-
tics and a FB-LTAG with a unification based se-
mantics, GenI selects all grammar entries whose
semantics subsumes the input semantics; com-
bines these entries using the FB-LTAG combina-
tion operations (i.e., adjunction and substitution);
and outputs the yield of all derived trees which are
syntactically complete and whose semantics is the
input semantics. To rank the generator output, we
train a language model on the GeniA corpus 4, a
corpus of 2000 MEDLINE asbtracts about biol-
ogy containing more than 400000 words (Kim et
al., 2003) and use this model to rank the generated
sentences by decreasing probability.
Thus for instance, given the input semantics
shown in Figure 1 and the grammar depicted in
Figure 4, the surface realiser will select all of these
trees; combine them using FB-LTAG substitution
operation; and output as generated sentence the
yield of the resulting derived tree namely the sen-
tence The function of a gated channel is to release
particles from the endoplasmic reticulum.
However, this procedure only works if the en-
</bodyText>
<footnote confidence="0.823983">
4http://www.nactem.ac.uk/genia/
</footnote>
<bodyText confidence="0.999664">
tries necessary to generate from the given input
are present in the grammar. To handle new, un-
seen input, we proceed in two ways. First, we try
to guess a grammar entry from the shape of the in-
put and the existing grammar. Second, we expand
the grammar by decomposing the extracted trees
into simpler ones.
</bodyText>
<subsectionHeader confidence="0.998285">
4.4 Guessing new grammar entries.
</subsectionHeader>
<bodyText confidence="0.99997588">
Given the limited size of the training data, it is of-
ten the case that input from the test data will have
no matching grammar unit. To handle such pre-
viously unseen input, we start by partitioning the
input semantics into sub-semantics corresponding
to events, entities and role.
For each entity variable X of type Type, we
create a default NP tree whose semantics is a lit-
eral of the form instance-of(X,Type).
For event variables, we search the lexicon for
an entry with a matching or similar semantics i.e.,
an entry with the same number and same type of
literals (literals with same arity and with identical
relations). When one is found, a grammar entry is
constructed for the unseen event variable by sub-
stituting the event type of the matching entry with
the type of the event variable. For instance, given
the input semantics instance-of(C,Carry), object(C,X),
base(C,Y), has-function(Z,C), agent(C,Z), this procedure
will create a grammar entry identical to that shown
at the top of Figure 4 except that the event type
Release-of-Calcium is changed to Carry and the ter-
minal release to the word form associated in the
KBGen lexicon with this concept, namely to the
verb carry.
</bodyText>
<subsectionHeader confidence="0.993671">
4.5 Expanding the Grammar
</subsectionHeader>
<bodyText confidence="0.998884133333333">
While the extracted grammar nicely captures pred-
icate/argument dependencies, it is very specific to
the items seen in the training data. To reduce over-
fitting, we generalise the extracted grammar by ex-
tracting from each event tree, subtrees that cap-
ture structures with fewer arguments and optional
modifiers.
For each event tree τ extracted from the train-
ing data which contains a subject-verb-object sub-
tree τ′, we add τ′ to the grammar and associate it
with the semantics of τ minus the relations associ-
ated with the arguments that have been removed.
For instance, given the extracted tree for the sen-
tence ”Aquaporin facilitates the movement of wa-
ter molecules through hydrophilic channels.”, this
</bodyText>
<page confidence="0.994953">
430
</page>
<bodyText confidence="0.999887714285714">
procedure will construct a new grammar tree cor-
responding to the subphrase “Aquaporin facili-
tates the movement of water molecules”.
We also construct both simpler event trees and
optional modifiers trees by extracting from event
trees, PP trees which are associated with a re-
lational semantics. For instance, given the tree
shown in Figure 4, the PP tree associated with
the relation base(RoC,ET) is removed thus creating
two new trees as illustrated in Figure 5: an S tree
corresponding to the sentence The function of a
gated channel is to release particles and an aux-
iliary PP tree corresponding to the phrase from
the endoplasmic reticulum. Similarly in the above
example, a PP tree corresponding to the phrase
”through hydrophilic channels.” will be extracted.
As with the base grammar, missing grammar
entries are guessed from the expanded grammar.
However we do this only in cases where a correct
grammar entry cannot be guessed from the base
grammar.
</bodyText>
<sectionHeader confidence="0.994882" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999991">
We evaluate our approach on the KBGen data and
compare it with the KBGen reference and two other
systems having taken part to the KBGen challenge.
</bodyText>
<subsectionHeader confidence="0.994965">
5.1 Training and test data.
</subsectionHeader>
<bodyText confidence="0.999920571428571">
Following a practice introduced by (Angeli et al.,
2010), we use the term scenario to denote a KB
subset paired with a sentence. The KBGen bench-
mark contains 207 scenarii for training and 72 for
testing. Each KB subset consists of a set of triples
and each scenario contains on average 16 triples
and 17 words.
</bodyText>
<subsectionHeader confidence="0.99832">
5.2 Systems
</subsectionHeader>
<bodyText confidence="0.999976416666667">
We evaluate three configurations of our approach
on the KBGen test data: one without grammar ex-
pansion (BASE); a second with a manual grammar
expansion MANEXP; and a third one with auto-
mated grammar expansion AUTEXP. We compare
the results obtained with those obtained by two
other systems participating in the KBGen chal-
lenge, namely the UDEL system, a symbolic rule
based system developed by a group of students at
the University of Delaware; and the IMS system,
a statistical system using a probabilistic grammar
induced from the training data.
</bodyText>
<subsectionHeader confidence="0.986795">
5.3 Metrics.
</subsectionHeader>
<bodyText confidence="0.999728777777778">
We evaluate system output automatically, using
the BLEU-4 modified precision score (Papineni et
al., 2002) with the human written sentences as ref-
erence. We also report results from a human based
evaluation. In this evaluation, participants were
asked to rate sentences along three dimensions:
fluency (Is the text easy to read?), grammatical-
ity and meaning similarity or adequacy (Does the
meaning conveyed by the generated sentence cor-
respond to the meaning conveyed by the reference
sentence?). The evaluation was done on line us-
ing the LG-Eval toolkit (Kow and Belz, 2012),
subjects used a sliding scale from -50 to +50 and
a Latin Square Experimental Design was used to
ensure that each evaluator sees the same number
of outputs from each system and for each test set
item. 12 subjects participated in the evaluation and
3 judgments were collected for each output.
</bodyText>
<sectionHeader confidence="0.996604" genericHeader="evaluation">
6 Results and Discussion
</sectionHeader>
<table confidence="0.999096">
System All Covered Coverage # Trees
IMS 0.12 0.12 100%
UDEL 0.32 0.32 100%
Base 0.04 0.39 30.5% 371
ManExp 0.28 0.34 83 % 412
AutExp 0.29 0.29 100% 477
</table>
<figureCaption confidence="0.954035">
Figure 6: BLEU scores and Grammar Size (Num-
ber of Elementary TAG trees
</figureCaption>
<bodyText confidence="0.99992015">
Table 6 summarises the results of the automatic
evaluation and shows the size (number of elemen-
tary TAG trees) of the grammars extracted from
the KBGen data.
The average BLEU score is given with respect
to all input (All) and to those inputs for which
the systems generate at least one sentence (Cov-
ered). While both the IMS and the UDEL system
have full coverage, our BASE system strongly un-
dergenerates failing to account for 69.5% of the
test data. However, because the extracted gram-
mar is linguistically principled and relatively com-
pact, it is possible to manually edit it. Indeed, the
MANEXP results show that, by adding 41 trees to
the grammar, coverage can be increased by 52.5
points reaching a coverage of 83%. Finally, the
AUTEXP results demonstrate that the automated
expansion mechanism permits achieving full cov-
erage while keeping a relative small grammar (477
trees).
</bodyText>
<page confidence="0.995112">
431
</page>
<figure confidence="0.969274444444445">
SRoC3
NP VPRoC3
RoC2
SRoC2
NP PP VBZ RoC1
DT NN IN NP↓GC is VPRoC1
RoC
the fn of TO VBRoC NP I PM
to release J
instance-of(RoC,Release-of-Calcium)
object(RoC,PM)
has-function(GC,RoC)
agent(RoC,GC)
VPRoC
VP∗,RoC PP
IN NP↓ER
from
base(RoC,ER)
</figure>
<figureCaption confidence="0.999394">
Figure 5: Trees Added by the Expansion Process
</figureCaption>
<table confidence="0.9989688">
Fluency Grammaticality Meaning Similarity
System Mean Homogeneous Subsets Mean Homogeneous Subsets Mean Homogeneous Subsets
UDEL 4.36 A 4.48 A 3.69 A
AutExp 3.45 B 3.55 B 3.65 A
IMS 1.91 C 2.05 C 1.31 B
</table>
<figureCaption confidence="0.949216">
Figure 7: Human Evaluation Results on a scale of 0 to 5. Homogeneous subsets are determined using
Tukey’s Post Hoc Test with p &lt; 0.05
</figureCaption>
<bodyText confidence="0.999966033333333">
In terms of BLEU score, the best version of our
system (AUTEXP) outperforms the probabilistic
approach of IMS by a large margin (+0.17) and
produces results similar to the fully handcrafted
UDEL system (-0.03).
In sum, our approach permits obtaining BLEU
scores and a coverage which are similar to that
obtained by a hand crafted system and outper-
forms a probabilistic approach. One key feature of
our approach is that the grammar extracted from
the training data is linguistically principled in that
it obeys the extended locality principle of Tree
Adjoining Grammars. As a result, the extracted
grammar is compact and can be manually modi-
fied to fit the need of an application as shown by
the good results obtained when using the MAN-
EXP configuration.
We now turn to the results of the human eval-
uation. Table 7 summarises the results whereby
systems are grouped by letters when there is no
significant difference between them (significance
level: p &lt; 0.05). We used ANOVAs and post-
hoc Tukey tests to test for significance. The dif-
ferences between systems are statistically signifi-
cant throughout except for meaning similarity (ad-
equacy) where UDEL and our system are on the
same level. Across the metrics, our system consis-
tently ranks second behind the symbolic, UDEL
system and before the statistical IMS one thus con-
firming the ranking based on BLEU.
</bodyText>
<sectionHeader confidence="0.998375" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.99984">
In Tree Adjoining Grammar, the extended domain
of locality principle ensures that TAG trees group
together in a single structure a syntactic predi-
cate and its arguments. Moreover, the semantic
principle requires that each elementary tree cap-
tures a single semantic unit. Together these two
principles ensure that TAG elementary trees cap-
ture basic semantic units and their dependencies.
In this paper, we presented a grammar extraction
approach which ensures that extracted grammars
comply with these two basic TAG principles. Us-
ing the KBGen benchmark, we then showed that
the resulting induced FB-LTAG compares favor-
ably with competing symbolic and statistical ap-
proaches when used to generate from knowledge
base data.
In the current version of the generator, the
output is ranked using a simple language model
trained on the GENIA corpus. We observed that
this often fails to return the best output in terms
of BLEU score, fluency, grammaticality and/or
meaning. In the future, we plan to remedy this us-
ing a ranking approach such as proposed in (Vell-
dal and Oepen, 2006; White and Rajkumar, 2009).
</bodyText>
<page confidence="0.997628">
432
</page>
<sectionHeader confidence="0.990303" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997784445454546">
G. Aguado, A. Ba˜n´on, J. Bateman, S. Bernardos,
M. Fern´andez, A. G´omez-P´erez, E. Nieto, A. Olalla,
R. Plaza, and A. S´anchez. 1998. Ontogeneration:
Reusing domain and linguistic ontologies for span-
ish text generation. In Workshop on Applications
of Ontologies and Problem Solving Methods, ECAI,
volume 98.
Gabor Angeli, Percy Liang, and Dan Klein. 2010. A
simple domain-independent probabilistic approach
to generation. In Proceedings of the 2010 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 502–512. Association for Com-
putational Linguistics.
Eva Banik, Claire Gardent, Donia Scott, Nikhil Dinesh,
and Fennie Liang. 2012. Kbgen: Text generation
from knowledge bases as a new shared task. In Pro-
ceedings of the seventh International Natural Lan-
guage Generation Conference, pages 141–145. As-
sociation for Computational Linguistics.
Eva Banik, Claire Gardent, Eric Kow, et al. 2013. The
kbgen challenge. In Proceedings of the 14th Eu-
ropean Workshop on Natural Language Generation
(ENLG), pages 94–97.
K. Bontcheva and Y. Wilks. 2004. Automatic re-
port generation from ontologies: the miakt ap-
proach. In Ninth International Conference on Appli-
cations of Natural Language to Information Systems
(NLDB’2004). Lecture Notes in Computer Science
3136, Springer, Manchester, UK.
David Chiang. 2000. Statistical parsing with an
automatically-extracted tree adjoining grammar. In
Proceedings of the 38th Annual Meeting on Associa-
tion for Computational Linguistics, pages 456–463.
Association for Computational Linguistics.
Peter Clark and Bruce Porter. 1997. Building con-
cept representations from reusable components. In
AAAI/IAAI, pages 369–376. Citeseer.
Deborah A Dahl, Madeleine Bates, Michael Brown,
William Fisher, Kate Hunicke-Smith, David Pallett,
Christine Pao, Alexander Rudnicky, and Elizabeth
Shriberg. 1994. Expanding the scope of the atis
task: The atis-3 corpus. In Proceedings of the work-
shop on Human Language Technology, pages 43–48.
Association for Computational Linguistics.
David DeVault, David Traum, and Ron Artstein. 2008.
Making grammar-based generation easier to deploy
in dialogue systems. In Proceedings of the 9th SIG-
dial Workshop on Discourse and Dialogue, pages
198–207. Association for Computational Linguis-
tics.
D. Galanis, G. Karakatsiotis, G. Lampouras, and I. An-
droutsopoulos. 2009. An open-source natural lan-
guage generator for owl ontologies and its use in
prot´eg´e and second life. In Proceedings of the 12th
Conference of the European Chapter of the Asso-
ciation for Computational Linguistics: Demonstra-
tions Session, pages 17–20. Association for Compu-
tational Linguistics.
Claire Gardent and Laura Kallmeyer. 2003. Semantic
construction in feature-based tag. In Proceedings of
the tenth conference on European chapter of the As-
sociation for Computational Linguistics-Volume 1,
pages 123–130. Association for Computational Lin-
guistics.
Claire Gardent, Eric Kow, et al. 2007. A symbolic ap-
proach to near-deterministic surface realisation us-
ing tree adjoining grammar. In ACL, volume 7,
pages 328–335.
B. Grosof. 2012. The silk project: Semantic infer-
encing on large knowledge. Technical report, SRI.
http://silk.semwebcentral.org/.
D. Gunning, V. K. Chaudhri, P. Clark, K. Barker, Shaw-
Yi Chaw, M. Greaves, B. Grosof, A. Leung, D. Mc-
Donald, S. Mishra, J. Pacheco, B. Porter, A. Spauld-
ing, D. Tecuci, and J. Tien. 2010. Project halo up-
date -progress toward digital aristotle. AI Magazine,
Fall:33–58.
K. Kaljurand and N.E. Fuchs. 2007. Verbalizing
owl in attempto controlled english. Proceedings of
OWLED07.
Martin Kay. 1996. Chart generation. In Proceedings
of the 34th annual meeting on Association for Com-
putational Linguistics, pages 200–204. Association
for Computational Linguistics.
Joohyun Kim and Raymond J Mooney. 2010. Gen-
erative alignment and semantic parsing for learn-
ing from ambiguous supervision. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics: Posters, pages 543–551. Associ-
ation for Computational Linguistics.
J-D Kim, Tomoko Ohta, Yuka Tateisi, and Junichi Tsu-
jii. 2003. Genia corpusa semantically annotated
corpus for bio-textmining. Bioinformatics, 19(suppl
1):i180–i182.
Ioannis Konstas and Mirella Lapata. 2012a. Concept-
to-text generation via discriminative reranking. In
Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics: Long
Papers-Volume 1, pages 369–378. Association for
Computational Linguistics.
Ioannis Konstas and Mirella Lapata. 2012b. Unsuper-
vised concept-to-text generation with hypergraphs.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 752–761. Association for Computational Lin-
guistics.
Eric Kow and Anja Belz. 2012. Lg-eval: A toolkit for
creating online language evaluation experiments. In
LREC, pages 4033–4037.
</reference>
<page confidence="0.992174">
433
</page>
<reference confidence="0.999886544444444">
Percy Liang, Michael I Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less super-
vision. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Pro-
cessing of the AFNLP: Volume 1-Volume 1, pages
91–99. Association for Computational Linguistics.
Wei Lu and Hwee Tou Ng. 2011. A probabilistic
forest-to-string model for language generation from
typed lambda calculus expressions. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, pages 1611–1622. Asso-
ciation for Computational Linguistics.
Wei Lu, Hwee Tou Ng, and Wee Sun Lee. 2009. Nat-
ural language generation with tree conditional ran-
dom fields. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Pro-
cessing: Volume 1-Volume 1, pages 400–409. As-
sociation for Computational Linguistics.
VO Mittal, G. Carenini, and JD Moore. 1994. Gen-
erating patient specific explanations in migraine. In
Proceedings of the eighteenth annual symposium on
computer applications in medical care. McGraw-
Hill Inc.
Boris Motik, Peter F Patel-Schneider, Bijan Parsia,
Conrad Bock, Achille Fokoue, Peter Haase, Rinke
Hoekstra, Ian Horrocks, Alan Ruttenberg, Uli Sat-
tler, et al. 2009. Owl 2 web ontology language:
Structural specification and functional-style syntax.
W3C recommendation, 27:17.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311–318. Association for
Computational Linguistics.
C.L. Paris. 1988. Tailoring object descriptions to a
user’s level of expertise. Computational Linguistics,
14(3):64–78.
R. Power and A. Third. 2010. Expressing owl ax-
ioms by english sentences: dubious in theory, fea-
sible in practice. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics:
Posters, pages 1006–1013. Association for Compu-
tational Linguistics.
E. Reiter, R. Robertson, and L.M. Osman. 2003.
Lessons from a failure: Generating tailored smoking
cessation letters. Artificial Intelligence, 144(1):41–
58.
Hadar Shemtov. 1996. Generation of paraphrases from
ambiguous logical forms. In Proceedings of the 16th
conference on Computational linguistics-Volume 2,
pages 919–924. Association for Computational Lin-
guistics.
Stuart M Shieber, Gertjan Van Noord, Fernando CN
Pereira, and Robert C Moore. 1990. Semantic-
head-driven generation. Computational Linguistics,
16(1):30–42.
Erik Velldal and Stephan Oepen. 2006. Statistical
ranking in tactical generation. In Proceedings of the
2006 Conference on Empirical Methods in Natural
Language Processing, pages 517–525. Association
for Computational Linguistics.
K. Vijay-Shanker and AK Joshi. 1988. Feature struc-
tures based tree adjoining grammars. In Proceed-
ings of the 12th International Conference on Com-
putational Linguistics, Budapest, Hungary.
Juen-tin Wang. 1980. On computational sentence gen-
eration from logical form. In Proceedings of the
8th conference on Computational linguistics, pages
405–411. Association for Computational Linguis-
tics.
Michael White and Rajakrishnan Rajkumar. 2009.
Perceptron reranking for ccg realization. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing: Volume 1-
Volume 1, pages 410–419. Association for Compu-
tational Linguistics.
G. Wilcock. 2003. Talking owls: Towards an ontology
verbalizer. Human Language Technology for the Se-
mantic Web and Web Services, ISWC, 3:109–112.
Sandra Williams and Richard Power. 2010. Grouping
axioms for more coherent ontology descriptions. In
Proceedings of the 6th International Natural Lan-
guage Generation Conference (INLG 2010), pages
197–202, Dublin.
Yuk Wah Wong and Raymond J Mooney. 2007. Gen-
eration by inverting a semantic parser that uses sta-
tistical machine translation. In HLT-NAACL, pages
172–179.
</reference>
<page confidence="0.999111">
434
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.904210">
<title confidence="0.999711">Surface Realisation from Knowledge-Bases</title>
<author confidence="0.992287">Bikash Gyawali Claire Gardent</author>
<affiliation confidence="0.966167">Universit´e de Lorraine, LORIA CNRS, LORIA, UMR</affiliation>
<address confidence="0.966144">Villers-l`es-Nancy, F-54600, France Vandoeuvre-l`es-Nancy, F-54500, France</address>
<email confidence="0.96465">bikash.gyawali@loria.frclaire.gardent@loria.fr</email>
<abstract confidence="0.999334777777778">We present a simple, data-driven approach to generation from knowledge bases (KB). A key feature of this approach is that grammar induction is driven by the extended domain of locality principle of TAG (Tree Adjoining Grammar); and that it takes into account both syntactic and semantic information. The resulting extracted TAG includes a unification based semantics and can be used by an existing surface realiser to generate sentences from KB data. Experimental evaluation on the KBGen data shows that our model outperforms a data-driven generate-and-rank approach based on an automatically induced probabilistic grammar; and is comparable with a handcrafted symbolic approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Aguado</author>
<author>A Ba˜n´on</author>
<author>J Bateman</author>
<author>S Bernardos</author>
<author>M Fern´andez</author>
<author>A G´omez-P´erez</author>
<author>E Nieto</author>
<author>A Olalla</author>
<author>R Plaza</author>
<author>A S´anchez</author>
</authors>
<title>Ontogeneration: Reusing domain and linguistic ontologies for spanish text generation.</title>
<date>1998</date>
<booktitle>In Workshop on Applications of Ontologies and Problem Solving Methods, ECAI,</booktitle>
<volume>98</volume>
<marker>Aguado, Ba˜n´on, Bateman, Bernardos, Fern´andez, G´omez-P´erez, Nieto, Olalla, Plaza, S´anchez, 1998</marker>
<rawString>G. Aguado, A. Ba˜n´on, J. Bateman, S. Bernardos, M. Fern´andez, A. G´omez-P´erez, E. Nieto, A. Olalla, R. Plaza, and A. S´anchez. 1998. Ontogeneration: Reusing domain and linguistic ontologies for spanish text generation. In Workshop on Applications of Ontologies and Problem Solving Methods, ECAI, volume 98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabor Angeli</author>
<author>Percy Liang</author>
<author>Dan Klein</author>
</authors>
<title>A simple domain-independent probabilistic approach to generation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>502--512</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9545" citStr="Angeli et al., 2010" startWordPosition="1412" endWordPosition="1415">Endoplasmic-Reticulum64603 ||instance-of ||Endoplasmic-Reticulum|) (|Gated-Channel64605 ||instance-of ||Gated-Channel|) |Release-Of-Calcium646 ||instance-of ||Release-Of-Calcium|)) :ROOT-TYPES ( (|Release-Of-Calcium646 ||instance-of ||Event|) (|Particle-In-Motion64582 ||instance-of ||Entity|) (|Endoplasmic-Reticulum64603 ||instance-of ||Entity|) (|Gated-Channel64605 ||instance-of ||Entity|))) Figure 1: Example KBGEN Scenario dom Field to generate from the same meaning representations. Finally, more recent papers propose approaches which perform both surface realisation and content selection. (Angeli et al., 2010) proposes a log linear model which decomposes into a sequence of discriminative local decisions. The first classifier determines which records to mention; the second, which fields of these records to select; and the third, which words to use to verbalise the selected fields. (Kim and Mooney, 2010) uses a generative model for content selection and verbalises the selected input using WASP−1, an existing generator. Finally, (Konstas and Lapata, 2012b; Konstas and Lapata, 2012a) develop a joint optimisation approach for content selection and surface realisation using a generic, domain independent </context>
<context position="27619" citStr="Angeli et al., 2010" startWordPosition="4264" endWordPosition="4267">ponding to the phrase from the endoplasmic reticulum. Similarly in the above example, a PP tree corresponding to the phrase ”through hydrophilic channels.” will be extracted. As with the base grammar, missing grammar entries are guessed from the expanded grammar. However we do this only in cases where a correct grammar entry cannot be guessed from the base grammar. 5 Experimental Setup We evaluate our approach on the KBGen data and compare it with the KBGen reference and two other systems having taken part to the KBGen challenge. 5.1 Training and test data. Following a practice introduced by (Angeli et al., 2010), we use the term scenario to denote a KB subset paired with a sentence. The KBGen benchmark contains 207 scenarii for training and 72 for testing. Each KB subset consists of a set of triples and each scenario contains on average 16 triples and 17 words. 5.2 Systems We evaluate three configurations of our approach on the KBGen test data: one without grammar expansion (BASE); a second with a manual grammar expansion MANEXP; and a third one with automated grammar expansion AUTEXP. We compare the results obtained with those obtained by two other systems participating in the KBGen challenge, namel</context>
</contexts>
<marker>Angeli, Liang, Klein, 2010</marker>
<rawString>Gabor Angeli, Percy Liang, and Dan Klein. 2010. A simple domain-independent probabilistic approach to generation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 502–512. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva Banik</author>
<author>Claire Gardent</author>
<author>Donia Scott</author>
<author>Nikhil Dinesh</author>
<author>Fennie Liang</author>
</authors>
<title>Kbgen: Text generation from knowledge bases as a new shared task.</title>
<date>2012</date>
<booktitle>In Proceedings of the seventh International Natural Language Generation Conference,</booktitle>
<pages>141--145</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1559" citStr="Banik et al., 2012" startWordPosition="228" endWordPosition="231">roduction In this paper we present a grammar based approach for generating from knowledge bases (KB) which is linguistically principled and conceptually simple. A key feature of this approach is that grammar induction is driven by the extended domain of locality principle of TAG (Tree Adjoining Grammar) and takes into account both syntactic and semantic information. The resulting extracted TAGs include a unification based semantics and can be used by an existing surface realiser to generate sentences from KB data. To evaluate our approach, we use the benchmark provided by the KBGen challenge (Banik et al., 2012; Banik et al., 2013), a challenge designed to evaluate generation from knowledge bases; where the input is a KB subset; and where the expected output is a complex sentence conveying the meaning represented by the input. When compared with two other systems having taken part in the KBGen challenge, our system outperforms a data-driven, generate-and-rank approach based on an automatically induced probabilistic grammar; and produces results comparable to those obtained by a symbolic, rule based approach. Most importantly, we obtain these results using a general purpose approach that we believe i</context>
</contexts>
<marker>Banik, Gardent, Scott, Dinesh, Liang, 2012</marker>
<rawString>Eva Banik, Claire Gardent, Donia Scott, Nikhil Dinesh, and Fennie Liang. 2012. Kbgen: Text generation from knowledge bases as a new shared task. In Proceedings of the seventh International Natural Language Generation Conference, pages 141–145. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva Banik</author>
<author>Claire Gardent</author>
<author>Eric Kow</author>
</authors>
<title>The kbgen challenge.</title>
<date>2013</date>
<booktitle>In Proceedings of the 14th European Workshop on Natural Language Generation (ENLG),</booktitle>
<pages>94--97</pages>
<contexts>
<context position="1580" citStr="Banik et al., 2013" startWordPosition="232" endWordPosition="235">per we present a grammar based approach for generating from knowledge bases (KB) which is linguistically principled and conceptually simple. A key feature of this approach is that grammar induction is driven by the extended domain of locality principle of TAG (Tree Adjoining Grammar) and takes into account both syntactic and semantic information. The resulting extracted TAGs include a unification based semantics and can be used by an existing surface realiser to generate sentences from KB data. To evaluate our approach, we use the benchmark provided by the KBGen challenge (Banik et al., 2012; Banik et al., 2013), a challenge designed to evaluate generation from knowledge bases; where the input is a KB subset; and where the expected output is a complex sentence conveying the meaning represented by the input. When compared with two other systems having taken part in the KBGen challenge, our system outperforms a data-driven, generate-and-rank approach based on an automatically induced probabilistic grammar; and produces results comparable to those obtained by a symbolic, rule based approach. Most importantly, we obtain these results using a general purpose approach that we believe is simpler and more tr</context>
<context position="11930" citStr="Banik et al., 2013" startWordPosition="1788" endWordPosition="1791">mantic arguments thereby restricting the space of possible tree combinations and drastically reducing the search space. Although conceptually related to (Lu and Ng, 2011), our approach extracts a unification based grammar rather than one with lambda terms. The extraction process and the generation algorithms are also fundamentally different. We use a simple mainly symbolic approach whereas they use a generative approach for grammar induction and a discriminative approach for sentence generation. 3 The KBGen Task The KBGen task was introduced as a new shared task at Generation Challenges 2013 (Banik et al., 2013)1 and aimed to compare different generation systems on KB data. Specifically, the task is to verbalise a subset of a knowledge base. For instance, the KB input shown in Figure 1 can be verbalised as: (1) The function of a gated channel is to release particles from the endoplasmic reticulum The KB subsets forming the KBGen input data were pre-selected from the AURA biology knowledge base (Gunning et al., 2010), a knowledge base about biology which was manually encoded by biology teachers and encodes knowledge about events, entities, properties and relations where relations include event-to-enti</context>
</contexts>
<marker>Banik, Gardent, Kow, 2013</marker>
<rawString>Eva Banik, Claire Gardent, Eric Kow, et al. 2013. The kbgen challenge. In Proceedings of the 14th European Workshop on Natural Language Generation (ENLG), pages 94–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Bontcheva</author>
<author>Y Wilks</author>
</authors>
<title>Automatic report generation from ontologies: the miakt approach.</title>
<date>2004</date>
<booktitle>In Ninth International Conference on Applications of Natural Language to Information Systems (NLDB’2004). Lecture Notes in Computer Science 3136,</booktitle>
<publisher>Springer,</publisher>
<location>Manchester, UK.</location>
<marker>Bontcheva, Wilks, 2004</marker>
<rawString>K. Bontcheva and Y. Wilks. 2004. Automatic report generation from ontologies: the miakt approach. In Ninth International Conference on Applications of Natural Language to Information Systems (NLDB’2004). Lecture Notes in Computer Science 3136, Springer, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Statistical parsing with an automatically-extracted tree adjoining grammar.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>456--463</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7465" citStr="Chiang, 2000" startWordPosition="1159" endWordPosition="1160">ation and a noun to a concept). In constrast, we propose an approach which can generate complex sentences from KB data; where the grammar is acquired from the data; and where no assumption is made about the mapping between semantics and NL expressions. Recent work has focused on data-driven generation from frames, lambda terms and data base entries. (DeVault et al., 2008) describes an approach for generating from the frames produced by a dialog system. They induce a probabilistic Tree Adjoining Grammar from a training set aligning frames and sentences using the grammar induction technique of (Chiang, 2000) and use a beam search that uses weighted features learned from the training data to rank alternative expansions at each step. (Lu and Ng, 2011) focuses on generating natural language sentences from logical form (i.e., lambda terms) using a synchronous context-free grammar. They introduce a novel synchronous context free grammar formalism for generating from lambda terms; induce such a synchronous grammar using a generative model; and extract the best output sentence from the generated forest using a log linear model. (Wong and Mooney, 2007; Lu et al., 2009) focuses on generating from variable</context>
<context position="22412" citStr="Chiang, 2000" startWordPosition="3407" endWordPosition="3408">surface realiser. 429 part of Figure 4. Note that such a tree may capture a verb occurring in a relative or a subordinate clause (together with its arguments) thus allowing for complex sentences including a relative or relating a main and a subordinate clause. The resulting grammar extracted from the parse trees (cf. e.g., Figure 4) is a Feature-Based Tree Adjoining Grammar with a Unification-based compositional semantics as described in (Gardent and Kallmeyer, 2003). In particular, our grammars differs from the traditional probabilistic Tree Adjoining Grammar extracted as described in e.g., (Chiang, 2000) in that they encode both syntax and semantics rather than just syntax. They also differ from the semantic FB-TAG extracted by (DeVault et al., 2008) in that (i) they encode the linking between syntactic and semantic arguments; (ii) they allow for elementary trees spanning discontiguous strings (e.g., The function of X is to release Y); and (iii) they enforce the semantic principle underlying TAG namely that an elementary tree containing a syntactic functor also contains its syntactic arguments. 4.3 Generation To generate with the grammar extracted from the KBGen data, we use the GenI surface </context>
</contexts>
<marker>Chiang, 2000</marker>
<rawString>David Chiang. 2000. Statistical parsing with an automatically-extracted tree adjoining grammar. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, pages 456–463. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Clark</author>
<author>Bruce Porter</author>
</authors>
<title>Building concept representations from reusable components.</title>
<date>1997</date>
<booktitle>In AAAI/IAAI,</booktitle>
<pages>369--376</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="13393" citStr="Clark and Porter, 1997" startWordPosition="1995" endWordPosition="1998">ions between trees. The variables decorating the tree nodes (e.g., GC) abbreviate feature structures of the form [idx : V ] where V is a unification variable shared with the semantics. NPGC DT NN NN a gated channel instance-of(GC,Gated-Channel) SRoC1 NP↓GC RoC1 VPRoC VBZRoC NP↓P M releases instance-of(RoC,Release-of-Calcium) object(RoC,PM) agent(RoC,GC) NPPM particles instance-of(PM,Particle-In-Motion) VPRoC VP∗RoC PP IN NPI ER from J base(RoC,ER) event-to-property and entity-to-property relations. AURA uses a frame-based knowledge representation and reasoning system called Knowledge Machine (Clark and Porter, 1997) which was translated into first-order logic with equality and from there, into multiple different formats including SILK (Grosof, 2012) and OWL2 (Motik et al., 2009). It is available for download in various formats including OWL2. 4 Generating from the KBGen Knowledge-Base To generate from the KBGen data, we induce a Feature-Based Lexicalised Tree Adjoining Grammar (FB-LTAG, (Vijay-Shanker and Joshi, 1988)) augmented with a unification-based semantics (Gardent and Kallmeyer, 2003) from the training data. We then use this grammar and an existing surface realiser to generate from the test data.</context>
</contexts>
<marker>Clark, Porter, 1997</marker>
<rawString>Peter Clark and Bruce Porter. 1997. Building concept representations from reusable components. In AAAI/IAAI, pages 369–376. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deborah A Dahl</author>
<author>Madeleine Bates</author>
<author>Michael Brown</author>
<author>William Fisher</author>
<author>Kate Hunicke-Smith</author>
<author>David Pallett</author>
<author>Christine Pao</author>
<author>Alexander Rudnicky</author>
<author>Elizabeth Shriberg</author>
</authors>
<title>Expanding the scope of the atis task: The atis-3 corpus.</title>
<date>1994</date>
<booktitle>In Proceedings of the workshop on Human Language Technology,</booktitle>
<pages>43--48</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8326" citStr="Dahl et al., 1994" startWordPosition="1295" endWordPosition="1298">nchronous context-free grammar. They introduce a novel synchronous context free grammar formalism for generating from lambda terms; induce such a synchronous grammar using a generative model; and extract the best output sentence from the generated forest using a log linear model. (Wong and Mooney, 2007; Lu et al., 2009) focuses on generating from variable-free treestructured representations such as the CLANG formal language used in the ROBOCUP competition and the database entries collected by (Liang et al., 2009) for weather forecast generation and for the air travel domain (ATIS dataset) by (Dahl et al., 1994). (Wong and Mooney, 2007) uses synchronous grammars to transform a variable free tree structured meaning representation into sentences. (Lu et al., 2009) uses a Conditional Ran425 The function of a gated channel is to release particles from the endoplasmic reticulum :TRIPLES ( (|Release-Of-Calcium646 ||object ||Particle-In-Motion64582|) (|Release-Of-Calcium646 ||base ||Endoplasmic-Reticulum64603|) (|Gated-Channel64605 ||has-function||Release-Of-Calcium646|) (|Release-Of-Calcium646 ||agent ||Gated-Channel64605|)) :INSTANCE-TYPES (|Particle-In-Motion64582 ||instance-of ||Particle-In-Motion|) (|E</context>
</contexts>
<marker>Dahl, Bates, Brown, Fisher, Hunicke-Smith, Pallett, Pao, Rudnicky, Shriberg, 1994</marker>
<rawString>Deborah A Dahl, Madeleine Bates, Michael Brown, William Fisher, Kate Hunicke-Smith, David Pallett, Christine Pao, Alexander Rudnicky, and Elizabeth Shriberg. 1994. Expanding the scope of the atis task: The atis-3 corpus. In Proceedings of the workshop on Human Language Technology, pages 43–48. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David DeVault</author>
<author>David Traum</author>
<author>Ron Artstein</author>
</authors>
<title>Making grammar-based generation easier to deploy in dialogue systems.</title>
<date>2008</date>
<booktitle>In Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>198--207</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7226" citStr="DeVault et al., 2008" startWordPosition="1119" endWordPosition="1122">rally, the sentences generated by ontology verbalisers cover a limited set of linguistics constructions; the grammar used is manually defined; and the mapping between semantics and strings is assumed to be deterministic (e.g., a verb maps to a relation and a noun to a concept). In constrast, we propose an approach which can generate complex sentences from KB data; where the grammar is acquired from the data; and where no assumption is made about the mapping between semantics and NL expressions. Recent work has focused on data-driven generation from frames, lambda terms and data base entries. (DeVault et al., 2008) describes an approach for generating from the frames produced by a dialog system. They induce a probabilistic Tree Adjoining Grammar from a training set aligning frames and sentences using the grammar induction technique of (Chiang, 2000) and use a beam search that uses weighted features learned from the training data to rank alternative expansions at each step. (Lu and Ng, 2011) focuses on generating natural language sentences from logical form (i.e., lambda terms) using a synchronous context-free grammar. They introduce a novel synchronous context free grammar formalism for generating from </context>
<context position="10791" citStr="DeVault et al., 2008" startWordPosition="1610" endWordPosition="1613">ch captures the structure of the database and the mapping from fields to strings. They intersect the grammar with a language model to improve fluency; use a weighted hypergraph to pack the derivations; and find the best derivation tree using Viterbi algorithm. Our approach differs from the approaches which assume variable free tree structured representations (Wong and Mooney, 2007; Lu et al., 2009) and data-based entries (Kim and Mooney, 2010; Konstas and Lapata, 2012b; Konstas and Lapata, 2012a) in that it handles graph-based, KB input and assumes a compositional semantics. It is closest to (DeVault et al., 2008) and (Lu and Ng, 2011) who extract a grammar encoding syntax and semantics from frames and lambda terms respectively. It differs from the former however in that it enforces a tighter syntax/semantics integration by requiring that the elementary trees of our extracted grammar encode the appropriate linking information. While (DeVault et al., 2008) extracts a TAG grammar associating each elementary tree with a semantics, we additionnally require that these trees encode the appropriate linking between syntactic and semantic arguments thereby restricting the space of possible tree combinations and</context>
<context position="22561" citStr="DeVault et al., 2008" startWordPosition="3430" endWordPosition="3433">th its arguments) thus allowing for complex sentences including a relative or relating a main and a subordinate clause. The resulting grammar extracted from the parse trees (cf. e.g., Figure 4) is a Feature-Based Tree Adjoining Grammar with a Unification-based compositional semantics as described in (Gardent and Kallmeyer, 2003). In particular, our grammars differs from the traditional probabilistic Tree Adjoining Grammar extracted as described in e.g., (Chiang, 2000) in that they encode both syntax and semantics rather than just syntax. They also differ from the semantic FB-TAG extracted by (DeVault et al., 2008) in that (i) they encode the linking between syntactic and semantic arguments; (ii) they allow for elementary trees spanning discontiguous strings (e.g., The function of X is to release Y); and (iii) they enforce the semantic principle underlying TAG namely that an elementary tree containing a syntactic functor also contains its syntactic arguments. 4.3 Generation To generate with the grammar extracted from the KBGen data, we use the GenI surface realiser (Gardent et al., 2007). Briefly, given an input semantics and a FB-LTAG with a unification based semantics, GenI selects all grammar entries</context>
</contexts>
<marker>DeVault, Traum, Artstein, 2008</marker>
<rawString>David DeVault, David Traum, and Ron Artstein. 2008. Making grammar-based generation easier to deploy in dialogue systems. In Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 198–207. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Galanis</author>
<author>G Karakatsiotis</author>
<author>G Lampouras</author>
<author>I Androutsopoulos</author>
</authors>
<title>An open-source natural language generator for owl ontologies and its use in prot´eg´e and second life.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics: Demonstrations Session,</booktitle>
<pages>17--20</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5932" citStr="Galanis et al., 2009" startWordPosition="907" endWordPosition="911">hniques to produce textual descriptions from some semantic information contained in a knowledge base. Both systems require some manual input (lexicons and domain schemas). More sophisticated NLG systems such as TAILOR (Paris, 1988), MIGRAINE (Mittal et al., 1994), and STOP (Reiter et al., 2003) offer tailored output based on user/patient models. While offering more flexibility and expressiveness, these systems are difficult to adapt by non-NLG experts because they require the user to understand the architecture of the NLG systems (Bontcheva and Wilks., 2004). Similarly, the NaturalOWL system (Galanis et al., 2009) has been proposed to generate fluent descriptions of museum exhibits from an OWL ontology. This approach however relies on extensive manual annotation of the input data. The SWAT project has focused on producing descriptions of ontologies that are both coherent and efficient (Williams and Power, 2010). For instance, instead of the above output, the SWAT system would generate the sentence: The following are kinds of animals: cats, dogs, horses and rabbits.. In this approach too however, the verbaliser output is strongly constrained by a simple Definite Clause Grammar covering simple clauses an</context>
</contexts>
<marker>Galanis, Karakatsiotis, Lampouras, Androutsopoulos, 2009</marker>
<rawString>D. Galanis, G. Karakatsiotis, G. Lampouras, and I. Androutsopoulos. 2009. An open-source natural language generator for owl ontologies and its use in prot´eg´e and second life. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics: Demonstrations Session, pages 17–20. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Gardent</author>
<author>Laura Kallmeyer</author>
</authors>
<title>Semantic construction in feature-based tag.</title>
<date>2003</date>
<booktitle>In Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics-Volume 1,</booktitle>
<pages>123--130</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="13879" citStr="Gardent and Kallmeyer, 2003" startWordPosition="2067" endWordPosition="2070">to-property relations. AURA uses a frame-based knowledge representation and reasoning system called Knowledge Machine (Clark and Porter, 1997) which was translated into first-order logic with equality and from there, into multiple different formats including SILK (Grosof, 2012) and OWL2 (Motik et al., 2009). It is available for download in various formats including OWL2. 4 Generating from the KBGen Knowledge-Base To generate from the KBGen data, we induce a Feature-Based Lexicalised Tree Adjoining Grammar (FB-LTAG, (Vijay-Shanker and Joshi, 1988)) augmented with a unification-based semantics (Gardent and Kallmeyer, 2003) from the training data. We then use this grammar and an existing surface realiser to generate from the test data. 4.1 Feature-Based Lexicalised Tree Adjoining Grammar Figure 2 shows an example FB-LTAG augmented with a unification-based semantics. Briefly, an FB-LTAG consists of a set of elementary trees which can be either initial or auxiliary. Initial trees are trees whose leaves are labeled with substitution nodes (marked with a downarrow) or terminal categories. Auxiliary trees are distinguished by a foot node (marked with a star) 2http://www.ai.sri.com/halo/ halobook2010/exported-kb/biokb</context>
<context position="22270" citStr="Gardent and Kallmeyer, 2003" startWordPosition="3385" endWordPosition="3388">e endoplasmic reticulum”. Variable names have been abbreviated and the KBGen tuple notation converted to terms so as to fit the input format expected by our surface realiser. 429 part of Figure 4. Note that such a tree may capture a verb occurring in a relative or a subordinate clause (together with its arguments) thus allowing for complex sentences including a relative or relating a main and a subordinate clause. The resulting grammar extracted from the parse trees (cf. e.g., Figure 4) is a Feature-Based Tree Adjoining Grammar with a Unification-based compositional semantics as described in (Gardent and Kallmeyer, 2003). In particular, our grammars differs from the traditional probabilistic Tree Adjoining Grammar extracted as described in e.g., (Chiang, 2000) in that they encode both syntax and semantics rather than just syntax. They also differ from the semantic FB-TAG extracted by (DeVault et al., 2008) in that (i) they encode the linking between syntactic and semantic arguments; (ii) they allow for elementary trees spanning discontiguous strings (e.g., The function of X is to release Y); and (iii) they enforce the semantic principle underlying TAG namely that an elementary tree containing a syntactic func</context>
</contexts>
<marker>Gardent, Kallmeyer, 2003</marker>
<rawString>Claire Gardent and Laura Kallmeyer. 2003. Semantic construction in feature-based tag. In Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics-Volume 1, pages 123–130. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Gardent</author>
<author>Eric Kow</author>
</authors>
<title>A symbolic approach to near-deterministic surface realisation using tree adjoining grammar.</title>
<date>2007</date>
<booktitle>In ACL,</booktitle>
<volume>7</volume>
<pages>328--335</pages>
<marker>Gardent, Kow, 2007</marker>
<rawString>Claire Gardent, Eric Kow, et al. 2007. A symbolic approach to near-deterministic surface realisation using tree adjoining grammar. In ACL, volume 7, pages 328–335.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Grosof</author>
</authors>
<title>The silk project: Semantic inferencing on large knowledge.</title>
<date>2012</date>
<tech>Technical report, SRI. http://silk.semwebcentral.org/.</tech>
<contexts>
<context position="13529" citStr="Grosof, 2012" startWordPosition="2017" endWordPosition="2018">on variable shared with the semantics. NPGC DT NN NN a gated channel instance-of(GC,Gated-Channel) SRoC1 NP↓GC RoC1 VPRoC VBZRoC NP↓P M releases instance-of(RoC,Release-of-Calcium) object(RoC,PM) agent(RoC,GC) NPPM particles instance-of(PM,Particle-In-Motion) VPRoC VP∗RoC PP IN NPI ER from J base(RoC,ER) event-to-property and entity-to-property relations. AURA uses a frame-based knowledge representation and reasoning system called Knowledge Machine (Clark and Porter, 1997) which was translated into first-order logic with equality and from there, into multiple different formats including SILK (Grosof, 2012) and OWL2 (Motik et al., 2009). It is available for download in various formats including OWL2. 4 Generating from the KBGen Knowledge-Base To generate from the KBGen data, we induce a Feature-Based Lexicalised Tree Adjoining Grammar (FB-LTAG, (Vijay-Shanker and Joshi, 1988)) augmented with a unification-based semantics (Gardent and Kallmeyer, 2003) from the training data. We then use this grammar and an existing surface realiser to generate from the test data. 4.1 Feature-Based Lexicalised Tree Adjoining Grammar Figure 2 shows an example FB-LTAG augmented with a unification-based semantics. Br</context>
</contexts>
<marker>Grosof, 2012</marker>
<rawString>B. Grosof. 2012. The silk project: Semantic inferencing on large knowledge. Technical report, SRI. http://silk.semwebcentral.org/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gunning</author>
<author>V K Chaudhri</author>
<author>P Clark</author>
<author>K Barker</author>
<author>ShawYi Chaw</author>
<author>M Greaves</author>
<author>B Grosof</author>
<author>A Leung</author>
<author>D McDonald</author>
<author>S Mishra</author>
<author>J Pacheco</author>
<author>B Porter</author>
<author>A Spaulding</author>
<author>D Tecuci</author>
<author>J Tien</author>
</authors>
<title>Project halo update -progress toward digital aristotle.</title>
<date>2010</date>
<journal>AI Magazine, Fall:33–58.</journal>
<contexts>
<context position="12342" citStr="Gunning et al., 2010" startWordPosition="1861" endWordPosition="1864">tive approach for grammar induction and a discriminative approach for sentence generation. 3 The KBGen Task The KBGen task was introduced as a new shared task at Generation Challenges 2013 (Banik et al., 2013)1 and aimed to compare different generation systems on KB data. Specifically, the task is to verbalise a subset of a knowledge base. For instance, the KB input shown in Figure 1 can be verbalised as: (1) The function of a gated channel is to release particles from the endoplasmic reticulum The KB subsets forming the KBGen input data were pre-selected from the AURA biology knowledge base (Gunning et al., 2010), a knowledge base about biology which was manually encoded by biology teachers and encodes knowledge about events, entities, properties and relations where relations include event-to-entity, event-to-event, 1http://www.kbgen.org 426 NPER DT NN NN the endoplasmic reticulum instance-of(ER,Endoplasmic-Reticulum) Figure 2: Example FB-LTAG with Unification-Based Semantics. Dotted lines indicate substitution and adjunction operations between trees. The variables decorating the tree nodes (e.g., GC) abbreviate feature structures of the form [idx : V ] where V is a unification variable shared with th</context>
</contexts>
<marker>Gunning, Chaudhri, Clark, Barker, Chaw, Greaves, Grosof, Leung, McDonald, Mishra, Pacheco, Porter, Spaulding, Tecuci, Tien, 2010</marker>
<rawString>D. Gunning, V. K. Chaudhri, P. Clark, K. Barker, ShawYi Chaw, M. Greaves, B. Grosof, A. Leung, D. McDonald, S. Mishra, J. Pacheco, B. Porter, A. Spaulding, D. Tecuci, and J. Tien. 2010. Project halo update -progress toward digital aristotle. AI Magazine, Fall:33–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kaljurand</author>
<author>N E Fuchs</author>
</authors>
<title>Verbalizing owl in attempto controlled english.</title>
<date>2007</date>
<booktitle>Proceedings of OWLED07.</booktitle>
<contexts>
<context position="4034" citStr="Kaljurand and Fuchs, 2007" startWordPosition="610" endWordPosition="613">the development of the semantic web and the proliferation of knowledge bases, generation from knowledge bases has attracted increased interest and so called ontology verbalisers have been proposed which support the generation of text from (parts of) knowledge bases. One main 424 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 424–434, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics strand of work maps each axiom in the knowledge base to a clause. Thus the OWL verbaliser integrated in the Prot´eg´e tool (Kaljurand and Fuchs, 2007) provides a verbalisation of every axiom present in the ontology under consideration and (Wilcock, 2003) describes an ontology verbaliser using XML-based generation. As discussed in (Power and Third, 2010), one important limitation of these approaches is that they assume a simple deterministic mapping between knowledge representation languages and some controlled natural language (CNL). Specifically, the assumption is that each atomic term (individual, class, property) maps to a word and each axiom maps to a sentence. As a result, the verbalisation of larger ontology parts can produce very unn</context>
</contexts>
<marker>Kaljurand, Fuchs, 2007</marker>
<rawString>K. Kaljurand and N.E. Fuchs. 2007. Verbalizing owl in attempto controlled english. Proceedings of OWLED07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Kay</author>
</authors>
<title>Chart generation.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th annual meeting on Association for Computational Linguistics,</booktitle>
<pages>200--204</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2677" citStr="Kay, 1996" startWordPosition="405" endWordPosition="406">Most importantly, we obtain these results using a general purpose approach that we believe is simpler and more transparent than current state of the art surface realisation systems generating from KB or DB data. 2 Related Work Our work is related to work on concept to text generation. Earlier work on concept to text generation mainly focuses on generation from logical forms using rule-based methods. (Wang, 1980) uses hand-written rules to generate sentences from an extended predicate logic formalism; (Shieber et al., 1990) introduces a head-driven algorithm for generating from logical forms; (Kay, 1996) defines a chart based algorithm which enhances efficiency by minimising the number of semantically incomplete phrases being built; and (Shemtov, 1996) presents an extension of the chart based generation algorithm presented in (Kay, 1996) which supports the generation of multiple paraphrases from underspecified semantic input. In all these approaches, grammar and lexicon are developed manually and it is assumed that the lexicon associates semantic sub-formulae with natural language expressions. Our approach is similar to these approaches in that it assumes a grammar encoding a compositional se</context>
</contexts>
<marker>Kay, 1996</marker>
<rawString>Martin Kay. 1996. Chart generation. In Proceedings of the 34th annual meeting on Association for Computational Linguistics, pages 200–204. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joohyun Kim</author>
<author>Raymond J Mooney</author>
</authors>
<title>Generative alignment and semantic parsing for learning from ambiguous supervision.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters,</booktitle>
<pages>543--551</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9843" citStr="Kim and Mooney, 2010" startWordPosition="1461" endWordPosition="1464">ndoplasmic-Reticulum64603 ||instance-of ||Entity|) (|Gated-Channel64605 ||instance-of ||Entity|))) Figure 1: Example KBGEN Scenario dom Field to generate from the same meaning representations. Finally, more recent papers propose approaches which perform both surface realisation and content selection. (Angeli et al., 2010) proposes a log linear model which decomposes into a sequence of discriminative local decisions. The first classifier determines which records to mention; the second, which fields of these records to select; and the third, which words to use to verbalise the selected fields. (Kim and Mooney, 2010) uses a generative model for content selection and verbalises the selected input using WASP−1, an existing generator. Finally, (Konstas and Lapata, 2012b; Konstas and Lapata, 2012a) develop a joint optimisation approach for content selection and surface realisation using a generic, domain independent probabilistic grammar which captures the structure of the database and the mapping from fields to strings. They intersect the grammar with a language model to improve fluency; use a weighted hypergraph to pack the derivations; and find the best derivation tree using Viterbi algorithm. Our approach</context>
</contexts>
<marker>Kim, Mooney, 2010</marker>
<rawString>Joohyun Kim and Raymond J Mooney. 2010. Generative alignment and semantic parsing for learning from ambiguous supervision. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, pages 543–551. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-D Kim</author>
<author>Tomoko Ohta</author>
<author>Yuka Tateisi</author>
<author>Junichi Tsujii</author>
</authors>
<title>Genia corpusa semantically annotated corpus for bio-textmining. Bioinformatics,</title>
<date>2003</date>
<pages>1--180</pages>
<contexts>
<context position="23609" citStr="Kim et al., 2003" startWordPosition="3601" endWordPosition="3604">se the GenI surface realiser (Gardent et al., 2007). Briefly, given an input semantics and a FB-LTAG with a unification based semantics, GenI selects all grammar entries whose semantics subsumes the input semantics; combines these entries using the FB-LTAG combination operations (i.e., adjunction and substitution); and outputs the yield of all derived trees which are syntactically complete and whose semantics is the input semantics. To rank the generator output, we train a language model on the GeniA corpus 4, a corpus of 2000 MEDLINE asbtracts about biology containing more than 400000 words (Kim et al., 2003) and use this model to rank the generated sentences by decreasing probability. Thus for instance, given the input semantics shown in Figure 1 and the grammar depicted in Figure 4, the surface realiser will select all of these trees; combine them using FB-LTAG substitution operation; and output as generated sentence the yield of the resulting derived tree namely the sentence The function of a gated channel is to release particles from the endoplasmic reticulum. However, this procedure only works if the en4http://www.nactem.ac.uk/genia/ tries necessary to generate from the given input are presen</context>
</contexts>
<marker>Kim, Ohta, Tateisi, Tsujii, 2003</marker>
<rawString>J-D Kim, Tomoko Ohta, Yuka Tateisi, and Junichi Tsujii. 2003. Genia corpusa semantically annotated corpus for bio-textmining. Bioinformatics, 19(suppl 1):i180–i182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Konstas</author>
<author>Mirella Lapata</author>
</authors>
<title>Conceptto-text generation via discriminative reranking.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>369--378</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9995" citStr="Konstas and Lapata, 2012" startWordPosition="1485" endWordPosition="1488">rate from the same meaning representations. Finally, more recent papers propose approaches which perform both surface realisation and content selection. (Angeli et al., 2010) proposes a log linear model which decomposes into a sequence of discriminative local decisions. The first classifier determines which records to mention; the second, which fields of these records to select; and the third, which words to use to verbalise the selected fields. (Kim and Mooney, 2010) uses a generative model for content selection and verbalises the selected input using WASP−1, an existing generator. Finally, (Konstas and Lapata, 2012b; Konstas and Lapata, 2012a) develop a joint optimisation approach for content selection and surface realisation using a generic, domain independent probabilistic grammar which captures the structure of the database and the mapping from fields to strings. They intersect the grammar with a language model to improve fluency; use a weighted hypergraph to pack the derivations; and find the best derivation tree using Viterbi algorithm. Our approach differs from the approaches which assume variable free tree structured representations (Wong and Mooney, 2007; Lu et al., 2009) and data-based entries </context>
</contexts>
<marker>Konstas, Lapata, 2012</marker>
<rawString>Ioannis Konstas and Mirella Lapata. 2012a. Conceptto-text generation via discriminative reranking. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 369–378. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Konstas</author>
<author>Mirella Lapata</author>
</authors>
<title>Unsupervised concept-to-text generation with hypergraphs.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>752--761</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9995" citStr="Konstas and Lapata, 2012" startWordPosition="1485" endWordPosition="1488">rate from the same meaning representations. Finally, more recent papers propose approaches which perform both surface realisation and content selection. (Angeli et al., 2010) proposes a log linear model which decomposes into a sequence of discriminative local decisions. The first classifier determines which records to mention; the second, which fields of these records to select; and the third, which words to use to verbalise the selected fields. (Kim and Mooney, 2010) uses a generative model for content selection and verbalises the selected input using WASP−1, an existing generator. Finally, (Konstas and Lapata, 2012b; Konstas and Lapata, 2012a) develop a joint optimisation approach for content selection and surface realisation using a generic, domain independent probabilistic grammar which captures the structure of the database and the mapping from fields to strings. They intersect the grammar with a language model to improve fluency; use a weighted hypergraph to pack the derivations; and find the best derivation tree using Viterbi algorithm. Our approach differs from the approaches which assume variable free tree structured representations (Wong and Mooney, 2007; Lu et al., 2009) and data-based entries </context>
</contexts>
<marker>Konstas, Lapata, 2012</marker>
<rawString>Ioannis Konstas and Mirella Lapata. 2012b. Unsupervised concept-to-text generation with hypergraphs. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 752–761. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Kow</author>
<author>Anja Belz</author>
</authors>
<title>Lg-eval: A toolkit for creating online language evaluation experiments.</title>
<date>2012</date>
<booktitle>In LREC,</booktitle>
<pages>4033--4037</pages>
<contexts>
<context position="29023" citStr="Kow and Belz, 2012" startWordPosition="4496" endWordPosition="4499">duced from the training data. 5.3 Metrics. We evaluate system output automatically, using the BLEU-4 modified precision score (Papineni et al., 2002) with the human written sentences as reference. We also report results from a human based evaluation. In this evaluation, participants were asked to rate sentences along three dimensions: fluency (Is the text easy to read?), grammaticality and meaning similarity or adequacy (Does the meaning conveyed by the generated sentence correspond to the meaning conveyed by the reference sentence?). The evaluation was done on line using the LG-Eval toolkit (Kow and Belz, 2012), subjects used a sliding scale from -50 to +50 and a Latin Square Experimental Design was used to ensure that each evaluator sees the same number of outputs from each system and for each test set item. 12 subjects participated in the evaluation and 3 judgments were collected for each output. 6 Results and Discussion System All Covered Coverage # Trees IMS 0.12 0.12 100% UDEL 0.32 0.32 100% Base 0.04 0.39 30.5% 371 ManExp 0.28 0.34 83 % 412 AutExp 0.29 0.29 100% 477 Figure 6: BLEU scores and Grammar Size (Number of Elementary TAG trees Table 6 summarises the results of the automatic evaluation</context>
</contexts>
<marker>Kow, Belz, 2012</marker>
<rawString>Eric Kow and Anja Belz. 2012. Lg-eval: A toolkit for creating online language evaluation experiments. In LREC, pages 4033–4037.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Michael I Jordan</author>
<author>Dan Klein</author>
</authors>
<title>Learning semantic correspondences with less supervision.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>1</volume>
<pages>91--99</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8226" citStr="Liang et al., 2009" startWordPosition="1278" endWordPosition="1281">1) focuses on generating natural language sentences from logical form (i.e., lambda terms) using a synchronous context-free grammar. They introduce a novel synchronous context free grammar formalism for generating from lambda terms; induce such a synchronous grammar using a generative model; and extract the best output sentence from the generated forest using a log linear model. (Wong and Mooney, 2007; Lu et al., 2009) focuses on generating from variable-free treestructured representations such as the CLANG formal language used in the ROBOCUP competition and the database entries collected by (Liang et al., 2009) for weather forecast generation and for the air travel domain (ATIS dataset) by (Dahl et al., 1994). (Wong and Mooney, 2007) uses synchronous grammars to transform a variable free tree structured meaning representation into sentences. (Lu et al., 2009) uses a Conditional Ran425 The function of a gated channel is to release particles from the endoplasmic reticulum :TRIPLES ( (|Release-Of-Calcium646 ||object ||Particle-In-Motion64582|) (|Release-Of-Calcium646 ||base ||Endoplasmic-Reticulum64603|) (|Gated-Channel64605 ||has-function||Release-Of-Calcium646|) (|Release-Of-Calcium646 ||agent ||Gate</context>
</contexts>
<marker>Liang, Jordan, Klein, 2009</marker>
<rawString>Percy Liang, Michael I Jordan, and Dan Klein. 2009. Learning semantic correspondences with less supervision. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1-Volume 1, pages 91–99. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Lu</author>
<author>Hwee Tou Ng</author>
</authors>
<title>A probabilistic forest-to-string model for language generation from typed lambda calculus expressions.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1611--1622</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7609" citStr="Lu and Ng, 2011" startWordPosition="1183" endWordPosition="1186">acquired from the data; and where no assumption is made about the mapping between semantics and NL expressions. Recent work has focused on data-driven generation from frames, lambda terms and data base entries. (DeVault et al., 2008) describes an approach for generating from the frames produced by a dialog system. They induce a probabilistic Tree Adjoining Grammar from a training set aligning frames and sentences using the grammar induction technique of (Chiang, 2000) and use a beam search that uses weighted features learned from the training data to rank alternative expansions at each step. (Lu and Ng, 2011) focuses on generating natural language sentences from logical form (i.e., lambda terms) using a synchronous context-free grammar. They introduce a novel synchronous context free grammar formalism for generating from lambda terms; induce such a synchronous grammar using a generative model; and extract the best output sentence from the generated forest using a log linear model. (Wong and Mooney, 2007; Lu et al., 2009) focuses on generating from variable-free treestructured representations such as the CLANG formal language used in the ROBOCUP competition and the database entries collected by (Li</context>
<context position="10813" citStr="Lu and Ng, 2011" startWordPosition="1615" endWordPosition="1618">f the database and the mapping from fields to strings. They intersect the grammar with a language model to improve fluency; use a weighted hypergraph to pack the derivations; and find the best derivation tree using Viterbi algorithm. Our approach differs from the approaches which assume variable free tree structured representations (Wong and Mooney, 2007; Lu et al., 2009) and data-based entries (Kim and Mooney, 2010; Konstas and Lapata, 2012b; Konstas and Lapata, 2012a) in that it handles graph-based, KB input and assumes a compositional semantics. It is closest to (DeVault et al., 2008) and (Lu and Ng, 2011) who extract a grammar encoding syntax and semantics from frames and lambda terms respectively. It differs from the former however in that it enforces a tighter syntax/semantics integration by requiring that the elementary trees of our extracted grammar encode the appropriate linking information. While (DeVault et al., 2008) extracts a TAG grammar associating each elementary tree with a semantics, we additionnally require that these trees encode the appropriate linking between syntactic and semantic arguments thereby restricting the space of possible tree combinations and drastically reducing </context>
</contexts>
<marker>Lu, Ng, 2011</marker>
<rawString>Wei Lu and Hwee Tou Ng. 2011. A probabilistic forest-to-string model for language generation from typed lambda calculus expressions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1611–1622. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Lu</author>
<author>Hwee Tou Ng</author>
<author>Wee Sun Lee</author>
</authors>
<title>Natural language generation with tree conditional random fields.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>1</volume>
<pages>400--409</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8029" citStr="Lu et al., 2009" startWordPosition="1248" endWordPosition="1251">ng the grammar induction technique of (Chiang, 2000) and use a beam search that uses weighted features learned from the training data to rank alternative expansions at each step. (Lu and Ng, 2011) focuses on generating natural language sentences from logical form (i.e., lambda terms) using a synchronous context-free grammar. They introduce a novel synchronous context free grammar formalism for generating from lambda terms; induce such a synchronous grammar using a generative model; and extract the best output sentence from the generated forest using a log linear model. (Wong and Mooney, 2007; Lu et al., 2009) focuses on generating from variable-free treestructured representations such as the CLANG formal language used in the ROBOCUP competition and the database entries collected by (Liang et al., 2009) for weather forecast generation and for the air travel domain (ATIS dataset) by (Dahl et al., 1994). (Wong and Mooney, 2007) uses synchronous grammars to transform a variable free tree structured meaning representation into sentences. (Lu et al., 2009) uses a Conditional Ran425 The function of a gated channel is to release particles from the endoplasmic reticulum :TRIPLES ( (|Release-Of-Calcium646 |</context>
<context position="10571" citStr="Lu et al., 2009" startWordPosition="1575" endWordPosition="1578">ator. Finally, (Konstas and Lapata, 2012b; Konstas and Lapata, 2012a) develop a joint optimisation approach for content selection and surface realisation using a generic, domain independent probabilistic grammar which captures the structure of the database and the mapping from fields to strings. They intersect the grammar with a language model to improve fluency; use a weighted hypergraph to pack the derivations; and find the best derivation tree using Viterbi algorithm. Our approach differs from the approaches which assume variable free tree structured representations (Wong and Mooney, 2007; Lu et al., 2009) and data-based entries (Kim and Mooney, 2010; Konstas and Lapata, 2012b; Konstas and Lapata, 2012a) in that it handles graph-based, KB input and assumes a compositional semantics. It is closest to (DeVault et al., 2008) and (Lu and Ng, 2011) who extract a grammar encoding syntax and semantics from frames and lambda terms respectively. It differs from the former however in that it enforces a tighter syntax/semantics integration by requiring that the elementary trees of our extracted grammar encode the appropriate linking information. While (DeVault et al., 2008) extracts a TAG grammar associat</context>
</contexts>
<marker>Lu, Ng, Lee, 2009</marker>
<rawString>Wei Lu, Hwee Tou Ng, and Wee Sun Lee. 2009. Natural language generation with tree conditional random fields. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1, pages 400–409. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>VO Mittal</author>
<author>G Carenini</author>
<author>JD Moore</author>
</authors>
<title>Generating patient specific explanations in migraine.</title>
<date>1994</date>
<booktitle>In Proceedings of the eighteenth annual symposium on computer applications in medical care. McGrawHill Inc.</booktitle>
<contexts>
<context position="5574" citStr="Mittal et al., 1994" startWordPosition="853" endWordPosition="856">input such as the KBGen data where the KB input often requires the generation of a complex sentence rather than a sequence of base clauses. To generate more complex output from KB data, several alternative approaches have been proposed. The MIAKT project (Bontcheva and Wilks., 2004) and the ONTOGENERATION project (Aguado et al., 1998) use symbolic NLG techniques to produce textual descriptions from some semantic information contained in a knowledge base. Both systems require some manual input (lexicons and domain schemas). More sophisticated NLG systems such as TAILOR (Paris, 1988), MIGRAINE (Mittal et al., 1994), and STOP (Reiter et al., 2003) offer tailored output based on user/patient models. While offering more flexibility and expressiveness, these systems are difficult to adapt by non-NLG experts because they require the user to understand the architecture of the NLG systems (Bontcheva and Wilks., 2004). Similarly, the NaturalOWL system (Galanis et al., 2009) has been proposed to generate fluent descriptions of museum exhibits from an OWL ontology. This approach however relies on extensive manual annotation of the input data. The SWAT project has focused on producing descriptions of ontologies th</context>
</contexts>
<marker>Mittal, Carenini, Moore, 1994</marker>
<rawString>VO Mittal, G. Carenini, and JD Moore. 1994. Generating patient specific explanations in migraine. In Proceedings of the eighteenth annual symposium on computer applications in medical care. McGrawHill Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boris Motik</author>
<author>Peter F Patel-Schneider</author>
<author>Bijan Parsia</author>
<author>Conrad Bock</author>
<author>Achille Fokoue</author>
<author>Peter Haase</author>
<author>Rinke Hoekstra</author>
<author>Ian Horrocks</author>
<author>Alan Ruttenberg</author>
<author>Uli Sattler</author>
</authors>
<title>web ontology language: Structural specification and functional-style syntax. W3C recommendation,</title>
<date>2009</date>
<journal>Owl</journal>
<volume>2</volume>
<pages>27--17</pages>
<contexts>
<context position="13559" citStr="Motik et al., 2009" startWordPosition="2021" endWordPosition="2024">the semantics. NPGC DT NN NN a gated channel instance-of(GC,Gated-Channel) SRoC1 NP↓GC RoC1 VPRoC VBZRoC NP↓P M releases instance-of(RoC,Release-of-Calcium) object(RoC,PM) agent(RoC,GC) NPPM particles instance-of(PM,Particle-In-Motion) VPRoC VP∗RoC PP IN NPI ER from J base(RoC,ER) event-to-property and entity-to-property relations. AURA uses a frame-based knowledge representation and reasoning system called Knowledge Machine (Clark and Porter, 1997) which was translated into first-order logic with equality and from there, into multiple different formats including SILK (Grosof, 2012) and OWL2 (Motik et al., 2009). It is available for download in various formats including OWL2. 4 Generating from the KBGen Knowledge-Base To generate from the KBGen data, we induce a Feature-Based Lexicalised Tree Adjoining Grammar (FB-LTAG, (Vijay-Shanker and Joshi, 1988)) augmented with a unification-based semantics (Gardent and Kallmeyer, 2003) from the training data. We then use this grammar and an existing surface realiser to generate from the test data. 4.1 Feature-Based Lexicalised Tree Adjoining Grammar Figure 2 shows an example FB-LTAG augmented with a unification-based semantics. Briefly, an FB-LTAG consists of </context>
</contexts>
<marker>Motik, Patel-Schneider, Parsia, Bock, Fokoue, Haase, Hoekstra, Horrocks, Ruttenberg, Sattler, 2009</marker>
<rawString>Boris Motik, Peter F Patel-Schneider, Bijan Parsia, Conrad Bock, Achille Fokoue, Peter Haase, Rinke Hoekstra, Ian Horrocks, Alan Ruttenberg, Uli Sattler, et al. 2009. Owl 2 web ontology language: Structural specification and functional-style syntax. W3C recommendation, 27:17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th annual meeting on association for computational linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="28553" citStr="Papineni et al., 2002" startWordPosition="4420" endWordPosition="4423">ach on the KBGen test data: one without grammar expansion (BASE); a second with a manual grammar expansion MANEXP; and a third one with automated grammar expansion AUTEXP. We compare the results obtained with those obtained by two other systems participating in the KBGen challenge, namely the UDEL system, a symbolic rule based system developed by a group of students at the University of Delaware; and the IMS system, a statistical system using a probabilistic grammar induced from the training data. 5.3 Metrics. We evaluate system output automatically, using the BLEU-4 modified precision score (Papineni et al., 2002) with the human written sentences as reference. We also report results from a human based evaluation. In this evaluation, participants were asked to rate sentences along three dimensions: fluency (Is the text easy to read?), grammaticality and meaning similarity or adequacy (Does the meaning conveyed by the generated sentence correspond to the meaning conveyed by the reference sentence?). The evaluation was done on line using the LG-Eval toolkit (Kow and Belz, 2012), subjects used a sliding scale from -50 to +50 and a Latin Square Experimental Design was used to ensure that each evaluator sees</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311–318. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C L Paris</author>
</authors>
<title>Tailoring object descriptions to a user’s level of expertise.</title>
<date>1988</date>
<journal>Computational Linguistics,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="5542" citStr="Paris, 1988" startWordPosition="850" endWordPosition="851">isation of more complex input such as the KBGen data where the KB input often requires the generation of a complex sentence rather than a sequence of base clauses. To generate more complex output from KB data, several alternative approaches have been proposed. The MIAKT project (Bontcheva and Wilks., 2004) and the ONTOGENERATION project (Aguado et al., 1998) use symbolic NLG techniques to produce textual descriptions from some semantic information contained in a knowledge base. Both systems require some manual input (lexicons and domain schemas). More sophisticated NLG systems such as TAILOR (Paris, 1988), MIGRAINE (Mittal et al., 1994), and STOP (Reiter et al., 2003) offer tailored output based on user/patient models. While offering more flexibility and expressiveness, these systems are difficult to adapt by non-NLG experts because they require the user to understand the architecture of the NLG systems (Bontcheva and Wilks., 2004). Similarly, the NaturalOWL system (Galanis et al., 2009) has been proposed to generate fluent descriptions of museum exhibits from an OWL ontology. This approach however relies on extensive manual annotation of the input data. The SWAT project has focused on produci</context>
</contexts>
<marker>Paris, 1988</marker>
<rawString>C.L. Paris. 1988. Tailoring object descriptions to a user’s level of expertise. Computational Linguistics, 14(3):64–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Power</author>
<author>A Third</author>
</authors>
<title>Expressing owl axioms by english sentences: dubious in theory, feasible in practice.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters,</booktitle>
<pages>1006--1013</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4239" citStr="Power and Third, 2010" startWordPosition="639" endWordPosition="642">t the generation of text from (parts of) knowledge bases. One main 424 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 424–434, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics strand of work maps each axiom in the knowledge base to a clause. Thus the OWL verbaliser integrated in the Prot´eg´e tool (Kaljurand and Fuchs, 2007) provides a verbalisation of every axiom present in the ontology under consideration and (Wilcock, 2003) describes an ontology verbaliser using XML-based generation. As discussed in (Power and Third, 2010), one important limitation of these approaches is that they assume a simple deterministic mapping between knowledge representation languages and some controlled natural language (CNL). Specifically, the assumption is that each atomic term (individual, class, property) maps to a word and each axiom maps to a sentence. As a result, the verbalisation of larger ontology parts can produce very unnatural text such as, Every cat is an animal. Every dog is an animal. Every horse is an animal. Every rabbit is an animal. More generally, the CNL based approaches to ontology verbalisation generate clauses</context>
</contexts>
<marker>Power, Third, 2010</marker>
<rawString>R. Power and A. Third. 2010. Expressing owl axioms by english sentences: dubious in theory, feasible in practice. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, pages 1006–1013. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
<author>R Robertson</author>
<author>L M Osman</author>
</authors>
<title>Lessons from a failure: Generating tailored smoking cessation letters.</title>
<date>2003</date>
<journal>Artificial Intelligence,</journal>
<volume>144</volume>
<issue>1</issue>
<pages>58</pages>
<contexts>
<context position="5606" citStr="Reiter et al., 2003" startWordPosition="859" endWordPosition="862">re the KB input often requires the generation of a complex sentence rather than a sequence of base clauses. To generate more complex output from KB data, several alternative approaches have been proposed. The MIAKT project (Bontcheva and Wilks., 2004) and the ONTOGENERATION project (Aguado et al., 1998) use symbolic NLG techniques to produce textual descriptions from some semantic information contained in a knowledge base. Both systems require some manual input (lexicons and domain schemas). More sophisticated NLG systems such as TAILOR (Paris, 1988), MIGRAINE (Mittal et al., 1994), and STOP (Reiter et al., 2003) offer tailored output based on user/patient models. While offering more flexibility and expressiveness, these systems are difficult to adapt by non-NLG experts because they require the user to understand the architecture of the NLG systems (Bontcheva and Wilks., 2004). Similarly, the NaturalOWL system (Galanis et al., 2009) has been proposed to generate fluent descriptions of museum exhibits from an OWL ontology. This approach however relies on extensive manual annotation of the input data. The SWAT project has focused on producing descriptions of ontologies that are both coherent and efficie</context>
</contexts>
<marker>Reiter, Robertson, Osman, 2003</marker>
<rawString>E. Reiter, R. Robertson, and L.M. Osman. 2003. Lessons from a failure: Generating tailored smoking cessation letters. Artificial Intelligence, 144(1):41– 58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hadar Shemtov</author>
</authors>
<title>Generation of paraphrases from ambiguous logical forms.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th conference on Computational linguistics-Volume 2,</booktitle>
<pages>919--924</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2828" citStr="Shemtov, 1996" startWordPosition="428" endWordPosition="429">e art surface realisation systems generating from KB or DB data. 2 Related Work Our work is related to work on concept to text generation. Earlier work on concept to text generation mainly focuses on generation from logical forms using rule-based methods. (Wang, 1980) uses hand-written rules to generate sentences from an extended predicate logic formalism; (Shieber et al., 1990) introduces a head-driven algorithm for generating from logical forms; (Kay, 1996) defines a chart based algorithm which enhances efficiency by minimising the number of semantically incomplete phrases being built; and (Shemtov, 1996) presents an extension of the chart based generation algorithm presented in (Kay, 1996) which supports the generation of multiple paraphrases from underspecified semantic input. In all these approaches, grammar and lexicon are developed manually and it is assumed that the lexicon associates semantic sub-formulae with natural language expressions. Our approach is similar to these approaches in that it assumes a grammar encoding a compositional semantics. It differs from them however in that, in our approach, grammar and lexicon are automatically acquired from the data. With the development of t</context>
</contexts>
<marker>Shemtov, 1996</marker>
<rawString>Hadar Shemtov. 1996. Generation of paraphrases from ambiguous logical forms. In Proceedings of the 16th conference on Computational linguistics-Volume 2, pages 919–924. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
<author>Gertjan Van Noord</author>
<author>Fernando CN Pereira</author>
<author>Robert C Moore</author>
</authors>
<title>Semantichead-driven generation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<marker>Shieber, Van Noord, Pereira, Moore, 1990</marker>
<rawString>Stuart M Shieber, Gertjan Van Noord, Fernando CN Pereira, and Robert C Moore. 1990. Semantichead-driven generation. Computational Linguistics, 16(1):30–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Velldal</author>
<author>Stephan Oepen</author>
</authors>
<title>Statistical ranking in tactical generation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>517--525</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Velldal, Oepen, 2006</marker>
<rawString>Erik Velldal and Stephan Oepen. 2006. Statistical ranking in tactical generation. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 517–525. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>AK Joshi</author>
</authors>
<title>Feature structures based tree adjoining grammars.</title>
<date>1988</date>
<booktitle>In Proceedings of the 12th International Conference on Computational Linguistics,</booktitle>
<location>Budapest, Hungary.</location>
<contexts>
<context position="13803" citStr="Vijay-Shanker and Joshi, 1988" startWordPosition="2058" endWordPosition="2061">n) VPRoC VP∗RoC PP IN NPI ER from J base(RoC,ER) event-to-property and entity-to-property relations. AURA uses a frame-based knowledge representation and reasoning system called Knowledge Machine (Clark and Porter, 1997) which was translated into first-order logic with equality and from there, into multiple different formats including SILK (Grosof, 2012) and OWL2 (Motik et al., 2009). It is available for download in various formats including OWL2. 4 Generating from the KBGen Knowledge-Base To generate from the KBGen data, we induce a Feature-Based Lexicalised Tree Adjoining Grammar (FB-LTAG, (Vijay-Shanker and Joshi, 1988)) augmented with a unification-based semantics (Gardent and Kallmeyer, 2003) from the training data. We then use this grammar and an existing surface realiser to generate from the test data. 4.1 Feature-Based Lexicalised Tree Adjoining Grammar Figure 2 shows an example FB-LTAG augmented with a unification-based semantics. Briefly, an FB-LTAG consists of a set of elementary trees which can be either initial or auxiliary. Initial trees are trees whose leaves are labeled with substitution nodes (marked with a downarrow) or terminal categories. Auxiliary trees are distinguished by a foot node (mar</context>
</contexts>
<marker>Vijay-Shanker, Joshi, 1988</marker>
<rawString>K. Vijay-Shanker and AK Joshi. 1988. Feature structures based tree adjoining grammars. In Proceedings of the 12th International Conference on Computational Linguistics, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juen-tin Wang</author>
</authors>
<title>On computational sentence generation from logical form.</title>
<date>1980</date>
<booktitle>In Proceedings of the 8th conference on Computational linguistics,</booktitle>
<pages>405--411</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2482" citStr="Wang, 1980" startWordPosition="378" endWordPosition="379">utperforms a data-driven, generate-and-rank approach based on an automatically induced probabilistic grammar; and produces results comparable to those obtained by a symbolic, rule based approach. Most importantly, we obtain these results using a general purpose approach that we believe is simpler and more transparent than current state of the art surface realisation systems generating from KB or DB data. 2 Related Work Our work is related to work on concept to text generation. Earlier work on concept to text generation mainly focuses on generation from logical forms using rule-based methods. (Wang, 1980) uses hand-written rules to generate sentences from an extended predicate logic formalism; (Shieber et al., 1990) introduces a head-driven algorithm for generating from logical forms; (Kay, 1996) defines a chart based algorithm which enhances efficiency by minimising the number of semantically incomplete phrases being built; and (Shemtov, 1996) presents an extension of the chart based generation algorithm presented in (Kay, 1996) which supports the generation of multiple paraphrases from underspecified semantic input. In all these approaches, grammar and lexicon are developed manually and it i</context>
</contexts>
<marker>Wang, 1980</marker>
<rawString>Juen-tin Wang. 1980. On computational sentence generation from logical form. In Proceedings of the 8th conference on Computational linguistics, pages 405–411. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
<author>Rajakrishnan Rajkumar</author>
</authors>
<title>Perceptron reranking for ccg realization.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>1</volume>
<pages>410--419</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>White, Rajkumar, 2009</marker>
<rawString>Michael White and Rajakrishnan Rajkumar. 2009. Perceptron reranking for ccg realization. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1, pages 410–419. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Wilcock</author>
</authors>
<title>Talking owls: Towards an ontology verbalizer. Human Language Technology for the Semantic Web and Web Services,</title>
<date>2003</date>
<booktitle>ISWC,</booktitle>
<pages>3--109</pages>
<contexts>
<context position="4138" citStr="Wilcock, 2003" startWordPosition="627" endWordPosition="628">tracted increased interest and so called ontology verbalisers have been proposed which support the generation of text from (parts of) knowledge bases. One main 424 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 424–434, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics strand of work maps each axiom in the knowledge base to a clause. Thus the OWL verbaliser integrated in the Prot´eg´e tool (Kaljurand and Fuchs, 2007) provides a verbalisation of every axiom present in the ontology under consideration and (Wilcock, 2003) describes an ontology verbaliser using XML-based generation. As discussed in (Power and Third, 2010), one important limitation of these approaches is that they assume a simple deterministic mapping between knowledge representation languages and some controlled natural language (CNL). Specifically, the assumption is that each atomic term (individual, class, property) maps to a word and each axiom maps to a sentence. As a result, the verbalisation of larger ontology parts can produce very unnatural text such as, Every cat is an animal. Every dog is an animal. Every horse is an animal. Every rab</context>
</contexts>
<marker>Wilcock, 2003</marker>
<rawString>G. Wilcock. 2003. Talking owls: Towards an ontology verbalizer. Human Language Technology for the Semantic Web and Web Services, ISWC, 3:109–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra Williams</author>
<author>Richard Power</author>
</authors>
<title>Grouping axioms for more coherent ontology descriptions.</title>
<date>2010</date>
<booktitle>In Proceedings of the 6th International Natural Language Generation Conference (INLG 2010),</booktitle>
<pages>197--202</pages>
<location>Dublin.</location>
<contexts>
<context position="6235" citStr="Williams and Power, 2010" startWordPosition="956" endWordPosition="959">er tailored output based on user/patient models. While offering more flexibility and expressiveness, these systems are difficult to adapt by non-NLG experts because they require the user to understand the architecture of the NLG systems (Bontcheva and Wilks., 2004). Similarly, the NaturalOWL system (Galanis et al., 2009) has been proposed to generate fluent descriptions of museum exhibits from an OWL ontology. This approach however relies on extensive manual annotation of the input data. The SWAT project has focused on producing descriptions of ontologies that are both coherent and efficient (Williams and Power, 2010). For instance, instead of the above output, the SWAT system would generate the sentence: The following are kinds of animals: cats, dogs, horses and rabbits.. In this approach too however, the verbaliser output is strongly constrained by a simple Definite Clause Grammar covering simple clauses and sentences verbalising aggregation patterns such as the above. More generally, the sentences generated by ontology verbalisers cover a limited set of linguistics constructions; the grammar used is manually defined; and the mapping between semantics and strings is assumed to be deterministic (e.g., a v</context>
</contexts>
<marker>Williams, Power, 2010</marker>
<rawString>Sandra Williams and Richard Power. 2010. Grouping axioms for more coherent ontology descriptions. In Proceedings of the 6th International Natural Language Generation Conference (INLG 2010), pages 197–202, Dublin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuk Wah Wong</author>
<author>Raymond J Mooney</author>
</authors>
<title>Generation by inverting a semantic parser that uses statistical machine translation.</title>
<date>2007</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>172--179</pages>
<contexts>
<context position="8011" citStr="Wong and Mooney, 2007" startWordPosition="1244" endWordPosition="1247">rames and sentences using the grammar induction technique of (Chiang, 2000) and use a beam search that uses weighted features learned from the training data to rank alternative expansions at each step. (Lu and Ng, 2011) focuses on generating natural language sentences from logical form (i.e., lambda terms) using a synchronous context-free grammar. They introduce a novel synchronous context free grammar formalism for generating from lambda terms; induce such a synchronous grammar using a generative model; and extract the best output sentence from the generated forest using a log linear model. (Wong and Mooney, 2007; Lu et al., 2009) focuses on generating from variable-free treestructured representations such as the CLANG formal language used in the ROBOCUP competition and the database entries collected by (Liang et al., 2009) for weather forecast generation and for the air travel domain (ATIS dataset) by (Dahl et al., 1994). (Wong and Mooney, 2007) uses synchronous grammars to transform a variable free tree structured meaning representation into sentences. (Lu et al., 2009) uses a Conditional Ran425 The function of a gated channel is to release particles from the endoplasmic reticulum :TRIPLES ( (|Relea</context>
<context position="10553" citStr="Wong and Mooney, 2007" startWordPosition="1571" endWordPosition="1574">SP−1, an existing generator. Finally, (Konstas and Lapata, 2012b; Konstas and Lapata, 2012a) develop a joint optimisation approach for content selection and surface realisation using a generic, domain independent probabilistic grammar which captures the structure of the database and the mapping from fields to strings. They intersect the grammar with a language model to improve fluency; use a weighted hypergraph to pack the derivations; and find the best derivation tree using Viterbi algorithm. Our approach differs from the approaches which assume variable free tree structured representations (Wong and Mooney, 2007; Lu et al., 2009) and data-based entries (Kim and Mooney, 2010; Konstas and Lapata, 2012b; Konstas and Lapata, 2012a) in that it handles graph-based, KB input and assumes a compositional semantics. It is closest to (DeVault et al., 2008) and (Lu and Ng, 2011) who extract a grammar encoding syntax and semantics from frames and lambda terms respectively. It differs from the former however in that it enforces a tighter syntax/semantics integration by requiring that the elementary trees of our extracted grammar encode the appropriate linking information. While (DeVault et al., 2008) extracts a TA</context>
</contexts>
<marker>Wong, Mooney, 2007</marker>
<rawString>Yuk Wah Wong and Raymond J Mooney. 2007. Generation by inverting a semantic parser that uses statistical machine translation. In HLT-NAACL, pages 172–179.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>