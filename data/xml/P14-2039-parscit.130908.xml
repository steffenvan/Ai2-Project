<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000057">
<title confidence="0.992108">
Sliding Alignment Windows for Real-Time Crowd Captioning
</title>
<author confidence="0.998089">
Mohammad Kazemi, Rahman Lavaee, Iftekhar Naim and Daniel Gildea
</author>
<affiliation confidence="0.998566666666667">
Dept. of Electrical and Computer Engineering and
Dept. of Computer Science
University of Rochester
</affiliation>
<address confidence="0.316395">
Rochester, NY 14627
</address>
<sectionHeader confidence="0.960629" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999783833333333">
The primary way of providing real-time
speech to text captioning for hard of hear-
ing people is to employ expensive profes-
sional stenographers who can type as fast
as natural speaking rates. Recent work has
shown that a feasible alternative is to com-
bine the partial captions of ordinary typ-
ists, each of whom is able to type only
part of what they hear. In this paper, we
extend the state of the art fixed-window
alignment algorithm (Naim et al., 2013)
for combining the individual captions into
a final output sequence. Our method per-
forms alignment on a sliding window of
the input sequences, drastically reducing
both the number of errors and the latency
of the system to the end user over the pre-
viously published approaches.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999621125">
Real-time captioning provides deaf or hard of
hearing people access to speech in mainstream
classrooms, at public events, and on live televi-
sion. Studies performed in the classroom set-
ting show that the latency between when a word
was said and when it is displayed must be under
five seconds to maintain consistency between the
captions being read and other visual cues (Wald,
2005; Kushalnagar et al., 2014). The most com-
mon approach to real-time captioning is to recruit
a trained stenographer with a special purpose pho-
netic keyboard, who transcribes the speech to text
with less than five seconds of latency. Unfortu-
nately, professional captionists are quite expensive
($150 per hour), must be recruited in blocks of an
hour or more, and are difficult to schedule on short
</bodyText>
<subsectionHeader confidence="0.817811">
Merging Incomplete Captions
</subsectionHeader>
<bodyText confidence="0.694943">
the quick brown fox jumped over the lazy dog
</bodyText>
<figureCaption confidence="0.71084975">
Figure 1: General layout of crowd captioning sys-
tems. Captionists (C1, C2, C3) submit partial cap-
tions that are automatically combined into a high-
quality output.
</figureCaption>
<bodyText confidence="0.999851952380952">
notice. Automatic speech recognition (ASR) sys-
tems (Saraclar et al., 2002), on the other hand, at-
tempts to provide a cheap and fully automated so-
lution to this problem. However, the accuracy of
ASR quickly plummets to below 30% when used
on an untrained speaker’s voice, in a new environ-
ment, or in the absence of a high quality micro-
phone (Wald, 2006). The accuracy of the ASR
systems can be improved using the ‘re-speaking’
technique, which requires a person that the ASR
has been trained on to repeat the words said by a
speaker as he hears them. Simultaneously hearing
and speaking, however, is not straightforward, and
requires some training.
An alternative approach is to combine the ef-
forts of multiple non-expert captionists (anyone
who can type), instead of relying on trained work-
ers (Lasecki et al., 2012; Naim et al., 2013). In
this approach, multiple non-expert human work-
ers transcribe an audio stream containing speech
in real-time. Workers type as much as they can of
</bodyText>
<figure confidence="0.98733875">
the brown fox jumped
quick fox lazy dog
fox jumped over the lazy
Combiner
Final Caption
C7
Cz
C3
</figure>
<page confidence="0.987283">
236
</page>
<bodyText confidence="0.990504076923077">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 236–240,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
the input, and, while no one worker’s transcript is
complete, the portions captured by various work-
ers tend to overlap. For each input word, a time-
stamp is recorded, indicating when the word is
typed by a worker. The partial inputs are com-
bined to produce a final transcript (see Figure 1).
This approach has been shown to dramatically out-
perform ASR in terms of both accuracy and Word
Error Rate (WER) (Lasecki et al., 2012; Naim et
al., 2013). Furthermore, recall of individual words
irrespective of their order approached and even ex-
ceeded that of a trained expert stenographer with
seven workers contributing, suggesting that the in-
formation is present to meet the performance of
a stenographer (Lasecki et al., 2012). However,
aligning these individual words in the correct se-
quential order remains a challenging problem.
Lasecki et al. (2012) addressed this alignment
problem using off-the-shelf multiple sequence
alignment tools, as well as an algorithm based on
incrementally building a precedence graph over
output words. Improved results for the alignment
problem were shown using weighted A* search
by Naim et al. (2013). To speed the search for
the best alignment, Naim et al. (2013) divided se-
quences into chunks of a fixed time duration, and
applied the A* alignment algorithm to each chunk
independently. Although this method speeds the
search for the best alignment, it introduces a sig-
nificant number of errors to the output of the sys-
tem due to inconsistency at the boundaries of the
chunks. In this paper, we introduce a novel slid-
ing window technique which avoids the errors pro-
duced by previous systems at the boundaries of
the chunks used for alignment. This technique
produces dramatically fewer errors for the same
amount of computation time.
</bodyText>
<sectionHeader confidence="0.544356" genericHeader="method">
2 Problem Overview and Background
</sectionHeader>
<bodyText confidence="0.999929833333334">
The problem of aligning and combining multiple
transcripts can be mapped to the well-studied Mul-
tiple Sequence Alignment (MSA) problem (Edgar
and Batzoglou, 2006). Let S1, ... , SK, K &gt; 2,
be the K sequences over an alphabet E, and hav-
ing length N1, ... , NK. For the caption align-
ment task, we treat each individual word as a sym-
bol in our alphabet E. The special gap symbol
‘−’ represents a missing word and does not be-
long to E. Let A = (aij) be a K x Nf matrix,
where aij E E U {−}, and the ith row has exactly
(Nf − Ni) gaps and is identical to Si if we ignore
</bodyText>
<construct confidence="0.376876857142857">
Algorithm 1 MSA-A* Algorithm
Require: K input sequences S = {Sl, ... , SK} having
length Nl, ... , NK, heuristic weight w, beam size b
input start E NK, goal E Nk
output an N x K matrix of integers indicating the index into
each input sequence of each position in the output se-
quence
</construct>
<listItem confidence="0.890374333333333">
1: g(start) +— 0, f(start) +— w x h(start).
2: Q +— {start}
3: while Q =� 0 do
4: n +— EXTRACT-MIN(Q)
5: for all s E {0, 1}K − {0K} do
6: nz +— n + s
7: if nz = goal then
8: Return the alignment matrix for the recon-
structed path from start to nz
9: else if nz E� Beam(b) then
10: continue;
11: else
12: g(nz) +— g(n) + c(n, nz)
13: f(nz) +— g(nz) + w x h(nz)
14: INSERT-ITEM(Q, nz, f(nz))
15: end if
16: end for
17: end while
</listItem>
<bodyText confidence="0.997683285714286">
the gaps. Every column of A must have at least
one non-gap symbol. Therefore, the jth column
of A indicates an alignment state for the jth posi-
tion, where the state can have one of the 2K − 1
possible combinations. Our goal is to find the op-
timum alignment matrix AOPT that minimizes the
sum of pairs (SOP) cost function:
</bodyText>
<equation confidence="0.986039">
c(A) = � c(Aij) (1)
1&lt;i&lt;j&lt;K
</equation>
<bodyText confidence="0.993432928571429">
where c(Aij) is the cost of the pairwise align-
ment between Si and Sj according to A. Formally,
c(Aij) = �N�
l=1 sub(ail, ajl), where sub(ail, ajl)
denotes the cost of substituting ajl for ail. If ail
and ajl are identical, the substitution cost is zero.
The substitution cost for two words is estimated
based on the edit distance between two words. The
exact solution to the SOP optimization problem is
NP-Complete (Wang and Jiang, 1994), but many
methods solve it approximately. Our approach is
based on weighted A* search for approximately
solving the MSA problem (Lermen and Reinert,
2000; Naim et al., 2013).
</bodyText>
<subsectionHeader confidence="0.990559">
2.1 Weighted A* Search for MSA
</subsectionHeader>
<bodyText confidence="0.999982666666667">
The problem of minimizing the SOP cost function
for K sequences is equivalent to estimating the
shortest path between a single source node and a
single sink node in a K-dimensional mesh graph,
where each node corresponds to a distinct position
in the K sequences. The source node is [0, ... , 0]
</bodyText>
<page confidence="0.976665">
237
</page>
<figure confidence="0.323228470588235">
Algorithm 2 Fixed Window Algorithm
Require: K input sequences S = {Sl, ... , SK} having
length Nl, ... , NK, window parameter chunk length.
1: start time +— 0
2: while goal � [Nl, ... , NK] do
3: for all i do
4: start[i] +— closest word(i, start time)
5: end for
6: end time +— start time + chunk length
7: for all i do
8: goal[i] +— closest word(i, end time) − 1
9: end for
10: alignmatrix +—MSA-A∗(start, goal)
11: concatenate alignmatrix onto end of finalmatrix
12: start time +— end time
13: end while
14: Return finalmatrix
</figure>
<bodyText confidence="0.600577071428571">
and the sink node is [N1, ... , NK]. The total num-
ber of nodes in the lattice is (N1 +1)x(N2 +1)x
· · ·x(NK +1), and each node has 2K −1 possible
successors and predecessors. The A∗ search algo-
rithm treats each node position n = [n1, ... , nK]
as a search state, and estimates the cost function
g(n) and the heuristic function h(n) for each state.
The cost function g(n) represents the exact min-
imum SOP cost to align the K sequences from
the beginning to the current position. The heuris-
tic function represents the approximate minimum
cost of aligning the suffixes of the K sequences,
starting after the current position n. The com-
monly used heuristic function is hpair(n):
</bodyText>
<equation confidence="0.9863745">
�hpair(n) = L(n —* t) = c(A∗p(uni , unj ))
1≤i&lt;j≤K
</equation>
<bodyText confidence="0.998634125">
(2)
where L(n —* t) denotes the lower bound on the
cost of the shortest path from n to destination t,
A∗p is the optimal pairwise alignment, and un iis
the suffix of node n in the i-th sequence. The
weighted A∗ search uses a priority queue Q to
store the search states n. At each step of the A∗
search algorithm, the node with the smallest eval-
uation function, f(n) = g(n) + whpair(n) (where
w &gt; 1), is extracted from the priority queue Q and
expanded by one edge. The search continues un-
til the goal node is extracted from Q. To further
speed up the search, a beam constraint is applied
on the search space using the timestamps of each
individual input words. If the beam size is set to b
seconds, then any state that aligns two words hav-
ing more than b seconds time lag is ignored. The
detailed procedure is shown in Algorithm 1. Af-
ter the alignment, the captions are combined via
majority voting at each position of the alignment
matrix. We ignore the alignment columns where
the majority vote is below a certain threshold t„
(typically t„ = 2), and thus filter out spurious er-
rors and spelling mistakes.
Although weighted A∗ significantly speeds the
search for the best alignment, it is still too slow
for very long sequences. For this reason, Naim
et al. (2013) divided the sequences into chunks of
a fixed time duration, and applied the A∗ align-
ment algorithm to each chunk independently. The
chunks were concatenated to produce the final out-
put sequence, as shown in Algorithm 2.
</bodyText>
<subsectionHeader confidence="0.999934">
2.2 Limitations of Fixed Window Algorithm
</subsectionHeader>
<bodyText confidence="0.999972869565217">
The fixed window based alignment has two key
limitations. First, aligning disjoint chunks inde-
pendently tends to introduce a large number of
errors at the boundary of each chunk. This is
because the chunk boundaries are defined with
respect to the timestamps associated with each
word in the captions, but the timestamps can
vary greatly between words that should in fact be
aligned. After all, if the timestamps corresponded
precisely to the original time at which each word
was spoken, the entire alignment problem would
be trivial. The fact that the various instances of
a single word in each transcription may fall on ei-
ther side of a chunk boundary leads to errors where
a word is either duplicated in the final output for
more than one chunk, or omitted entirely. This
problem also causes errors in ordering among the
words remaining within one chunk, because there
is less information available to constrain the order-
ing relations between transcriptions. Second, the
fixed window alignment algorithm requires longer
chunks (&gt; 10 seconds) to obtain reasonable accu-
racy, and thus introduces unsatisfactory latency.
</bodyText>
<sectionHeader confidence="0.984167" genericHeader="method">
3 Sliding Alignment Windows
</sectionHeader>
<bodyText confidence="0.999882166666667">
In order to address the problems described above,
we explore a technique based on a sliding align-
ment window, shown in Algorithm 3. We start
with alignment with a fixed chunk size. After
aligning the first chunk, we use the information
derived from the alignment to determine where
the next chunk should begin within each transcrip-
tion. We use a single point in the aligned output
as the starting point for the next chunk, and de-
termine the corresponding starting position within
each original transcription. This single point is
determined by a tunable parameter keep length
</bodyText>
<page confidence="0.988692">
238
</page>
<figure confidence="0.947693833333333">
Algorithm 3 Sliding Window Algorithm
Require: K input sequences S = {S1, ... , SK}
having length N1, ... , NK, window parameters
chunk length and keep length.
1: start +- 0K, goal +- 0K
2: while goal � [N1, ... , NK] do
3: endtime +- chunk length+maxi time(start[i])
4: for all i do
5: goal[i] +- closest word(i, endtime)
6: end for
7: alignmatrix +-MSA-A∗(start, goal)
8: concatenate first keep length columns of
alignmatrix onto end of finalmatrix
9: for all i do
10: start[i] +- alignmatrix[keep length][i]
11: end for
12: end while
13: Return finalmatrix
</figure>
<figureCaption confidence="0.86768">
Figure 2: Evaluation of different systems on using
WER metric for measuring transcription quality.
</figureCaption>
<bodyText confidence="0.997751875">
(line 10 of Algorithm 3). The materials in the
output alignment that follow this point is thrown
away, and replaced with the output produced by
aligning the next chunk starting from this point
(line 8). The process continues iteratively, allow-
ing us to avoid using the erroneous output align-
ments in the neighborhood of the arbitrary end-
points for each chunk.
</bodyText>
<sectionHeader confidence="0.990618" genericHeader="evaluation">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.9996001">
We evaluate our system on a dataset of four 5-
minute long audio clips of lectures in electrical
engineering and chemistry lectures taken from
MIT OpenCourseWare. The same dataset used
by (Lasecki et al., 2012) and (Naim et al., 2013).
Each audio clip is transcribed by 10 non-expert
human workers in real time. We measure the ac-
curacy in terms of Word Error Rate (WER) with
respect to a reference transcription.
We are interested in investigating how the three
</bodyText>
<figure confidence="0.991274866666667">
Latency (millisecond)
(a) varying keep-lengths for fixed heuristic weight
0.62
0.6
0.58
0.56
0.54
w = 3
w = 4
w = 6
w = 8
0.48
4000 6000 8000 10000 12000 14000
Average Running Time (millisecond)
(b) varying heuristic weights for fixed keep-length
</figure>
<figureCaption confidence="0.8004795">
Figure 3: Tradeoff between speed and accuracy
for different heuristic wights and keep-lengths
</figureCaption>
<bodyText confidence="0.999452523809524">
key parameters of the algorithm, i.e., the chunk
size (c), the heuristic weight (w) and the keep-
length (k), affect the system latency, the search
speed, and the alignment accuracy. The chunk size
directly determines the latency of the system to the
end user, as alignment cannot begin until an entire
chunk is captured. Furthermore, the chunk size,
the heuristic weight, and the keep-length help us
to trade-off speed versus accuracy. We also com-
pare the performance of our algorithm with that
of the most accurate fixed alignment window al-
gorithm (Naim et al., 2013). The performance
in terms of WER for sliding and fixed alignment
windows is presented in Figure 2. Out of the sys-
tems in Figure 2, the first three systems consist of
sliding alignment window algorithm with different
values of keep-length parameter: (1) keep-length
= 0.5; (2) keep-length = 0.67; and (3) keep-length
= 0.85. The other systems are the graph-based al-
gorithm of (Lasecki et al., 2012), the MUSCLE
algorithm of (Edgar, 2004), and the most accu-
</bodyText>
<figure confidence="0.94472625">
Accuracy (1 − WER)
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
Sliding−window, k = 0.50
Sliding−window, k = 0.67
Sliding−window, k = 0.85
Graph−based
MUSCLE
Fixed−window
0.62
0.6
0.58
0.56
0.54
0.52
sliding window, k = 50 %
sliding window, k = 67 %
sliding window, k = 85 %
fixed window
0.465 10 15 20 25 30
0.5
0.48
Accuracy (1−WER)
Accuracy (1 − WER)
0.52
0.5
</figure>
<page confidence="0.994594">
239
</page>
<bodyText confidence="0.9998994">
rate fixed alignment window algorithm of (Naim
et al., 2013). We set the heuristic weight param-
eter (w) to 3 and the chunk size parameter (c) to
5 seconds for all the three sliding window sys-
tems and the fixed window system. Sliding align-
ment window produces better results and outper-
forms the other algorithms even for large values of
the keep-length parameter. The sliding alignment
window with keep-length 0.5 achieves 0.5679 av-
erage accuracy in terms of (1-WER), providing a
18.09% improvement with respect to the most ac-
curate fixed alignment window (average accuracy
0.4857). On the same dataset, Lasecki et al. (2012)
reported 36.6% accuracy using the Dragon Natu-
rally Speaking ASR system (version 11.5 for Win-
dows).
To show the trade-off between latency and ac-
curacy, we fix the heuristic weight (w = 3) and
plot the accuracy as a function of chunk size in
Figure 3. We repeat this experiment for different
values of keep-length. We observe that the slid-
ing window approach dominates the fixed window
approach across a wide range of chunk sizes. Fur-
thermore, we can see that for smaller values of the
chunk size parameter, increasing the keep-length
makes the system less accurate. As the chunk
size parameter increases, the performance of slid-
ing window systems with different values of keep-
length parameter converges. Therefore, at larger
chunk sizes, for which there are smaller number of
boundaries, the keep-length parameter has lower
impact.
Next, we show the trade-off between computa-
tion speed and accuracy in Figure 3, as we fix the
heuristic weight and vary the chunk size over the
range [5, 10, 15, 20, 30] seconds. Larger chunks
are more accurately aligned but require computa-
tion time that grows as NK in the chunk size N in
the worst case. Furthermore, smaller weights al-
low faster alignment, but provide lower accuracy.
</bodyText>
<sectionHeader confidence="0.999271" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.997590727272727">
In this paper, we present a novel sliding win-
dow based text alignment algorithm for real-time
crowd captioning. By effectively addressing the
problem of alignment errors at chunk boundaries,
our sliding window approach outperforms the ex-
isting fixed window based system (Naim et al.,
2013) in terms of word error rate, particularly
when the chunk size is small, and thus achieves
higher accuracy at lower latency.
Acknowledgments Funded by NSF awards IIS-
1218209 and IIS-0910611.
</bodyText>
<sectionHeader confidence="0.995993" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999795744186047">
Robert C Edgar and Serafim Batzoglou. 2006. Mul-
tiple sequence alignment. Current opinion in struc-
tural biology, 16(3):368–373.
Robert C Edgar. 2004. MUSCLE: multiple sequence
alignment with high accuracy and high throughput.
Nucleic Acids Research, 32(5):1792–1797.
Raja S Kushalnagar, Walter S Lasecki, and Jeffrey P
Bigham. 2014. Accessibility evaluation of class-
room captions. ACM Transactions on Accessible
Computing (TACCESS), 5(3):7.
Walter Lasecki, Christopher Miller, Adam Sadilek, An-
drew Abumoussa, Donato Borrello, Raja Kushalna-
gar, and Jeffrey Bigham. 2012. Real-time caption-
ing by groups of non-experts. In Proceedings of the
25rd annual ACM symposium on User interface soft-
ware and technology, UIST ’12.
Martin Lermen and Knut Reinert. 2000. The practi-
cal use of the A* algorithm for exact multiple se-
quence alignment. Journal of Computational Biol-
ogy, 7(5):655–671.
Iftekhar Naim, Daniel Gildea, Walter Lasecki, and Jef-
frey Bigham. 2013. Text alignment for real-time
crowd captioning. In Proceedings of the 2013 Meet-
ing of the North American chapter of the Association
for Computational Linguistics (NAACL-13).
Murat Saraclar, Michael Riley, Enrico Bocchieri, and
Vincent Goffin. 2002. Towards automatic closed
captioning: Low latency real time broadcast news
transcription. In Proceedings of the International
Conference on Spoken Language Processing (IC-
SLP), pages 1741–1744.
Mike Wald. 2005. Using automatic speech recognition
to enhance education for all students: Turning a vi-
sion into reality. In Proceedings 35th Annual Con-
ference on Frontiers in Education, 2005. FIE ’05.,
pages S3G–S3G, Oct.
Mike Wald. 2006. Creating accessible educational
multimedia through editing automatic speech recog-
nition captioning in real time. Interactive Technol-
ogy and Smart Education, 3(2):131–141.
Lusheng Wang and Tao Jiang. 1994. On the complex-
ity of multiple sequence alignment. Journal of Com-
putational Biology, 1(4):337–348.
</reference>
<page confidence="0.997031">
240
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.426675">
<title confidence="0.998874">Sliding Alignment Windows for Real-Time Crowd Captioning</title>
<author confidence="0.449761">Naim</author>
<affiliation confidence="0.998898">Dept. of Electrical and Computer Engineering Dept. of Computer University of</affiliation>
<address confidence="0.999061">Rochester, NY 14627</address>
<abstract confidence="0.997345">The primary way of providing real-time speech to text captioning for hard of hearing people is to employ expensive professional stenographers who can type as fast as natural speaking rates. Recent work has shown that a feasible alternative is to combine the partial captions of ordinary typists, each of whom is able to type only part of what they hear. In this paper, we extend the state of the art fixed-window alignment algorithm (Naim et al., 2013) for combining the individual captions into a final output sequence. Our method performs alignment on a sliding window of the input sequences, drastically reducing both the number of errors and the latency of the system to the end user over the previously published approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Robert C Edgar</author>
<author>Serafim Batzoglou</author>
</authors>
<title>Multiple sequence alignment. Current opinion in structural biology,</title>
<date>2006</date>
<pages>16--3</pages>
<contexts>
<context position="5265" citStr="Edgar and Batzoglou, 2006" startWordPosition="860" endWordPosition="863">hod speeds the search for the best alignment, it introduces a significant number of errors to the output of the system due to inconsistency at the boundaries of the chunks. In this paper, we introduce a novel sliding window technique which avoids the errors produced by previous systems at the boundaries of the chunks used for alignment. This technique produces dramatically fewer errors for the same amount of computation time. 2 Problem Overview and Background The problem of aligning and combining multiple transcripts can be mapped to the well-studied Multiple Sequence Alignment (MSA) problem (Edgar and Batzoglou, 2006). Let S1, ... , SK, K &gt; 2, be the K sequences over an alphabet E, and having length N1, ... , NK. For the caption alignment task, we treat each individual word as a symbol in our alphabet E. The special gap symbol ‘−’ represents a missing word and does not belong to E. Let A = (aij) be a K x Nf matrix, where aij E E U {−}, and the ith row has exactly (Nf − Ni) gaps and is identical to Si if we ignore Algorithm 1 MSA-A* Algorithm Require: K input sequences S = {Sl, ... , SK} having length Nl, ... , NK, heuristic weight w, beam size b input start E NK, goal E Nk output an N x K matrix of integer</context>
</contexts>
<marker>Edgar, Batzoglou, 2006</marker>
<rawString>Robert C Edgar and Serafim Batzoglou. 2006. Multiple sequence alignment. Current opinion in structural biology, 16(3):368–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Edgar</author>
</authors>
<title>MUSCLE: multiple sequence alignment with high accuracy and high throughput.</title>
<date>2004</date>
<journal>Nucleic Acids Research,</journal>
<volume>32</volume>
<issue>5</issue>
<contexts>
<context position="15033" citStr="Edgar, 2004" startWordPosition="2617" endWordPosition="2618">elp us to trade-off speed versus accuracy. We also compare the performance of our algorithm with that of the most accurate fixed alignment window algorithm (Naim et al., 2013). The performance in terms of WER for sliding and fixed alignment windows is presented in Figure 2. Out of the systems in Figure 2, the first three systems consist of sliding alignment window algorithm with different values of keep-length parameter: (1) keep-length = 0.5; (2) keep-length = 0.67; and (3) keep-length = 0.85. The other systems are the graph-based algorithm of (Lasecki et al., 2012), the MUSCLE algorithm of (Edgar, 2004), and the most accuAccuracy (1 − WER) 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 Sliding−window, k = 0.50 Sliding−window, k = 0.67 Sliding−window, k = 0.85 Graph−based MUSCLE Fixed−window 0.62 0.6 0.58 0.56 0.54 0.52 sliding window, k = 50 % sliding window, k = 67 % sliding window, k = 85 % fixed window 0.465 10 15 20 25 30 0.5 0.48 Accuracy (1−WER) Accuracy (1 − WER) 0.52 0.5 239 rate fixed alignment window algorithm of (Naim et al., 2013). We set the heuristic weight parameter (w) to 3 and the chunk size parameter (c) to 5 seconds for all the three sliding window systems and the fixed window system. Slid</context>
</contexts>
<marker>Edgar, 2004</marker>
<rawString>Robert C Edgar. 2004. MUSCLE: multiple sequence alignment with high accuracy and high throughput. Nucleic Acids Research, 32(5):1792–1797.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raja S Kushalnagar</author>
<author>Walter S Lasecki</author>
<author>Jeffrey P Bigham</author>
</authors>
<title>Accessibility evaluation of classroom captions.</title>
<date>2014</date>
<journal>ACM Transactions on Accessible Computing (TACCESS),</journal>
<volume>5</volume>
<issue>3</issue>
<contexts>
<context position="1402" citStr="Kushalnagar et al., 2014" startWordPosition="227" endWordPosition="230">hod performs alignment on a sliding window of the input sequences, drastically reducing both the number of errors and the latency of the system to the end user over the previously published approaches. 1 Introduction Real-time captioning provides deaf or hard of hearing people access to speech in mainstream classrooms, at public events, and on live television. Studies performed in the classroom setting show that the latency between when a word was said and when it is displayed must be under five seconds to maintain consistency between the captions being read and other visual cues (Wald, 2005; Kushalnagar et al., 2014). The most common approach to real-time captioning is to recruit a trained stenographer with a special purpose phonetic keyboard, who transcribes the speech to text with less than five seconds of latency. Unfortunately, professional captionists are quite expensive ($150 per hour), must be recruited in blocks of an hour or more, and are difficult to schedule on short Merging Incomplete Captions the quick brown fox jumped over the lazy dog Figure 1: General layout of crowd captioning systems. Captionists (C1, C2, C3) submit partial captions that are automatically combined into a highquality outp</context>
</contexts>
<marker>Kushalnagar, Lasecki, Bigham, 2014</marker>
<rawString>Raja S Kushalnagar, Walter S Lasecki, and Jeffrey P Bigham. 2014. Accessibility evaluation of classroom captions. ACM Transactions on Accessible Computing (TACCESS), 5(3):7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Lasecki</author>
<author>Christopher Miller</author>
<author>Adam Sadilek</author>
<author>Andrew Abumoussa</author>
<author>Donato Borrello</author>
<author>Raja Kushalnagar</author>
<author>Jeffrey Bigham</author>
</authors>
<title>Real-time captioning by groups of non-experts.</title>
<date>2012</date>
<booktitle>In Proceedings of the 25rd annual ACM symposium on User interface software and technology, UIST ’12.</booktitle>
<contexts>
<context position="2821" citStr="Lasecki et al., 2012" startWordPosition="465" endWordPosition="468">SR quickly plummets to below 30% when used on an untrained speaker’s voice, in a new environment, or in the absence of a high quality microphone (Wald, 2006). The accuracy of the ASR systems can be improved using the ‘re-speaking’ technique, which requires a person that the ASR has been trained on to repeat the words said by a speaker as he hears them. Simultaneously hearing and speaking, however, is not straightforward, and requires some training. An alternative approach is to combine the efforts of multiple non-expert captionists (anyone who can type), instead of relying on trained workers (Lasecki et al., 2012; Naim et al., 2013). In this approach, multiple non-expert human workers transcribe an audio stream containing speech in real-time. Workers type as much as they can of the brown fox jumped quick fox lazy dog fox jumped over the lazy Combiner Final Caption C7 Cz C3 236 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 236–240, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics the input, and, while no one worker’s transcript is complete, the portions captured by various workers tend to overlap.</context>
<context position="4150" citStr="Lasecki et al. (2012)" startWordPosition="681" endWordPosition="684">puts are combined to produce a final transcript (see Figure 1). This approach has been shown to dramatically outperform ASR in terms of both accuracy and Word Error Rate (WER) (Lasecki et al., 2012; Naim et al., 2013). Furthermore, recall of individual words irrespective of their order approached and even exceeded that of a trained expert stenographer with seven workers contributing, suggesting that the information is present to meet the performance of a stenographer (Lasecki et al., 2012). However, aligning these individual words in the correct sequential order remains a challenging problem. Lasecki et al. (2012) addressed this alignment problem using off-the-shelf multiple sequence alignment tools, as well as an algorithm based on incrementally building a precedence graph over output words. Improved results for the alignment problem were shown using weighted A* search by Naim et al. (2013). To speed the search for the best alignment, Naim et al. (2013) divided sequences into chunks of a fixed time duration, and applied the A* alignment algorithm to each chunk independently. Although this method speeds the search for the best alignment, it introduces a significant number of errors to the output of the</context>
<context position="13436" citStr="Lasecki et al., 2012" startWordPosition="2346" endWordPosition="2349">ring transcription quality. (line 10 of Algorithm 3). The materials in the output alignment that follow this point is thrown away, and replaced with the output produced by aligning the next chunk starting from this point (line 8). The process continues iteratively, allowing us to avoid using the erroneous output alignments in the neighborhood of the arbitrary endpoints for each chunk. 4 Experimental Results We evaluate our system on a dataset of four 5- minute long audio clips of lectures in electrical engineering and chemistry lectures taken from MIT OpenCourseWare. The same dataset used by (Lasecki et al., 2012) and (Naim et al., 2013). Each audio clip is transcribed by 10 non-expert human workers in real time. We measure the accuracy in terms of Word Error Rate (WER) with respect to a reference transcription. We are interested in investigating how the three Latency (millisecond) (a) varying keep-lengths for fixed heuristic weight 0.62 0.6 0.58 0.56 0.54 w = 3 w = 4 w = 6 w = 8 0.48 4000 6000 8000 10000 12000 14000 Average Running Time (millisecond) (b) varying heuristic weights for fixed keep-length Figure 3: Tradeoff between speed and accuracy for different heuristic wights and keep-lengths key par</context>
<context position="14994" citStr="Lasecki et al., 2012" startWordPosition="2609" endWordPosition="2612">ize, the heuristic weight, and the keep-length help us to trade-off speed versus accuracy. We also compare the performance of our algorithm with that of the most accurate fixed alignment window algorithm (Naim et al., 2013). The performance in terms of WER for sliding and fixed alignment windows is presented in Figure 2. Out of the systems in Figure 2, the first three systems consist of sliding alignment window algorithm with different values of keep-length parameter: (1) keep-length = 0.5; (2) keep-length = 0.67; and (3) keep-length = 0.85. The other systems are the graph-based algorithm of (Lasecki et al., 2012), the MUSCLE algorithm of (Edgar, 2004), and the most accuAccuracy (1 − WER) 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 Sliding−window, k = 0.50 Sliding−window, k = 0.67 Sliding−window, k = 0.85 Graph−based MUSCLE Fixed−window 0.62 0.6 0.58 0.56 0.54 0.52 sliding window, k = 50 % sliding window, k = 67 % sliding window, k = 85 % fixed window 0.465 10 15 20 25 30 0.5 0.48 Accuracy (1−WER) Accuracy (1 − WER) 0.52 0.5 239 rate fixed alignment window algorithm of (Naim et al., 2013). We set the heuristic weight parameter (w) to 3 and the chunk size parameter (c) to 5 seconds for all the three sliding window sy</context>
</contexts>
<marker>Lasecki, Miller, Sadilek, Abumoussa, Borrello, Kushalnagar, Bigham, 2012</marker>
<rawString>Walter Lasecki, Christopher Miller, Adam Sadilek, Andrew Abumoussa, Donato Borrello, Raja Kushalnagar, and Jeffrey Bigham. 2012. Real-time captioning by groups of non-experts. In Proceedings of the 25rd annual ACM symposium on User interface software and technology, UIST ’12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Lermen</author>
<author>Knut Reinert</author>
</authors>
<title>The practical use of the A* algorithm for exact multiple sequence alignment.</title>
<date>2000</date>
<journal>Journal of Computational Biology,</journal>
<volume>7</volume>
<issue>5</issue>
<contexts>
<context position="7320" citStr="Lermen and Reinert, 2000" startWordPosition="1274" endWordPosition="1277">ion: c(A) = � c(Aij) (1) 1&lt;i&lt;j&lt;K where c(Aij) is the cost of the pairwise alignment between Si and Sj according to A. Formally, c(Aij) = �N� l=1 sub(ail, ajl), where sub(ail, ajl) denotes the cost of substituting ajl for ail. If ail and ajl are identical, the substitution cost is zero. The substitution cost for two words is estimated based on the edit distance between two words. The exact solution to the SOP optimization problem is NP-Complete (Wang and Jiang, 1994), but many methods solve it approximately. Our approach is based on weighted A* search for approximately solving the MSA problem (Lermen and Reinert, 2000; Naim et al., 2013). 2.1 Weighted A* Search for MSA The problem of minimizing the SOP cost function for K sequences is equivalent to estimating the shortest path between a single source node and a single sink node in a K-dimensional mesh graph, where each node corresponds to a distinct position in the K sequences. The source node is [0, ... , 0] 237 Algorithm 2 Fixed Window Algorithm Require: K input sequences S = {Sl, ... , SK} having length Nl, ... , NK, window parameter chunk length. 1: start time +— 0 2: while goal � [Nl, ... , NK] do 3: for all i do 4: start[i] +— closest word(i, start t</context>
</contexts>
<marker>Lermen, Reinert, 2000</marker>
<rawString>Martin Lermen and Knut Reinert. 2000. The practical use of the A* algorithm for exact multiple sequence alignment. Journal of Computational Biology, 7(5):655–671.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iftekhar Naim</author>
<author>Daniel Gildea</author>
<author>Walter Lasecki</author>
<author>Jeffrey Bigham</author>
</authors>
<title>Text alignment for real-time crowd captioning.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-13).</booktitle>
<contexts>
<context position="701" citStr="Naim et al., 2013" startWordPosition="111" endWordPosition="114">vaee, Iftekhar Naim and Daniel Gildea Dept. of Electrical and Computer Engineering and Dept. of Computer Science University of Rochester Rochester, NY 14627 Abstract The primary way of providing real-time speech to text captioning for hard of hearing people is to employ expensive professional stenographers who can type as fast as natural speaking rates. Recent work has shown that a feasible alternative is to combine the partial captions of ordinary typists, each of whom is able to type only part of what they hear. In this paper, we extend the state of the art fixed-window alignment algorithm (Naim et al., 2013) for combining the individual captions into a final output sequence. Our method performs alignment on a sliding window of the input sequences, drastically reducing both the number of errors and the latency of the system to the end user over the previously published approaches. 1 Introduction Real-time captioning provides deaf or hard of hearing people access to speech in mainstream classrooms, at public events, and on live television. Studies performed in the classroom setting show that the latency between when a word was said and when it is displayed must be under five seconds to maintain con</context>
<context position="2841" citStr="Naim et al., 2013" startWordPosition="469" endWordPosition="472"> below 30% when used on an untrained speaker’s voice, in a new environment, or in the absence of a high quality microphone (Wald, 2006). The accuracy of the ASR systems can be improved using the ‘re-speaking’ technique, which requires a person that the ASR has been trained on to repeat the words said by a speaker as he hears them. Simultaneously hearing and speaking, however, is not straightforward, and requires some training. An alternative approach is to combine the efforts of multiple non-expert captionists (anyone who can type), instead of relying on trained workers (Lasecki et al., 2012; Naim et al., 2013). In this approach, multiple non-expert human workers transcribe an audio stream containing speech in real-time. Workers type as much as they can of the brown fox jumped quick fox lazy dog fox jumped over the lazy Combiner Final Caption C7 Cz C3 236 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 236–240, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics the input, and, while no one worker’s transcript is complete, the portions captured by various workers tend to overlap. For each input word</context>
<context position="4433" citStr="Naim et al. (2013)" startWordPosition="723" endWordPosition="726"> approached and even exceeded that of a trained expert stenographer with seven workers contributing, suggesting that the information is present to meet the performance of a stenographer (Lasecki et al., 2012). However, aligning these individual words in the correct sequential order remains a challenging problem. Lasecki et al. (2012) addressed this alignment problem using off-the-shelf multiple sequence alignment tools, as well as an algorithm based on incrementally building a precedence graph over output words. Improved results for the alignment problem were shown using weighted A* search by Naim et al. (2013). To speed the search for the best alignment, Naim et al. (2013) divided sequences into chunks of a fixed time duration, and applied the A* alignment algorithm to each chunk independently. Although this method speeds the search for the best alignment, it introduces a significant number of errors to the output of the system due to inconsistency at the boundaries of the chunks. In this paper, we introduce a novel sliding window technique which avoids the errors produced by previous systems at the boundaries of the chunks used for alignment. This technique produces dramatically fewer errors for t</context>
<context position="7340" citStr="Naim et al., 2013" startWordPosition="1278" endWordPosition="1281">&lt;i&lt;j&lt;K where c(Aij) is the cost of the pairwise alignment between Si and Sj according to A. Formally, c(Aij) = �N� l=1 sub(ail, ajl), where sub(ail, ajl) denotes the cost of substituting ajl for ail. If ail and ajl are identical, the substitution cost is zero. The substitution cost for two words is estimated based on the edit distance between two words. The exact solution to the SOP optimization problem is NP-Complete (Wang and Jiang, 1994), but many methods solve it approximately. Our approach is based on weighted A* search for approximately solving the MSA problem (Lermen and Reinert, 2000; Naim et al., 2013). 2.1 Weighted A* Search for MSA The problem of minimizing the SOP cost function for K sequences is equivalent to estimating the shortest path between a single source node and a single sink node in a K-dimensional mesh graph, where each node corresponds to a distinct position in the K sequences. The source node is [0, ... , 0] 237 Algorithm 2 Fixed Window Algorithm Require: K input sequences S = {Sl, ... , SK} having length Nl, ... , NK, window parameter chunk length. 1: start time +— 0 2: while goal � [Nl, ... , NK] do 3: for all i do 4: start[i] +— closest word(i, start time) 5: end for 6: e</context>
<context position="10194" citStr="Naim et al. (2013)" startWordPosition="1809" endWordPosition="1812">input words. If the beam size is set to b seconds, then any state that aligns two words having more than b seconds time lag is ignored. The detailed procedure is shown in Algorithm 1. After the alignment, the captions are combined via majority voting at each position of the alignment matrix. We ignore the alignment columns where the majority vote is below a certain threshold t„ (typically t„ = 2), and thus filter out spurious errors and spelling mistakes. Although weighted A∗ significantly speeds the search for the best alignment, it is still too slow for very long sequences. For this reason, Naim et al. (2013) divided the sequences into chunks of a fixed time duration, and applied the A∗ alignment algorithm to each chunk independently. The chunks were concatenated to produce the final output sequence, as shown in Algorithm 2. 2.2 Limitations of Fixed Window Algorithm The fixed window based alignment has two key limitations. First, aligning disjoint chunks independently tends to introduce a large number of errors at the boundary of each chunk. This is because the chunk boundaries are defined with respect to the timestamps associated with each word in the captions, but the timestamps can vary greatly</context>
<context position="13460" citStr="Naim et al., 2013" startWordPosition="2351" endWordPosition="2354"> (line 10 of Algorithm 3). The materials in the output alignment that follow this point is thrown away, and replaced with the output produced by aligning the next chunk starting from this point (line 8). The process continues iteratively, allowing us to avoid using the erroneous output alignments in the neighborhood of the arbitrary endpoints for each chunk. 4 Experimental Results We evaluate our system on a dataset of four 5- minute long audio clips of lectures in electrical engineering and chemistry lectures taken from MIT OpenCourseWare. The same dataset used by (Lasecki et al., 2012) and (Naim et al., 2013). Each audio clip is transcribed by 10 non-expert human workers in real time. We measure the accuracy in terms of Word Error Rate (WER) with respect to a reference transcription. We are interested in investigating how the three Latency (millisecond) (a) varying keep-lengths for fixed heuristic weight 0.62 0.6 0.58 0.56 0.54 w = 3 w = 4 w = 6 w = 8 0.48 4000 6000 8000 10000 12000 14000 Average Running Time (millisecond) (b) varying heuristic weights for fixed keep-length Figure 3: Tradeoff between speed and accuracy for different heuristic wights and keep-lengths key parameters of the algorithm</context>
<context position="15464" citStr="Naim et al., 2013" startWordPosition="2699" endWordPosition="2702">(1) keep-length = 0.5; (2) keep-length = 0.67; and (3) keep-length = 0.85. The other systems are the graph-based algorithm of (Lasecki et al., 2012), the MUSCLE algorithm of (Edgar, 2004), and the most accuAccuracy (1 − WER) 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 Sliding−window, k = 0.50 Sliding−window, k = 0.67 Sliding−window, k = 0.85 Graph−based MUSCLE Fixed−window 0.62 0.6 0.58 0.56 0.54 0.52 sliding window, k = 50 % sliding window, k = 67 % sliding window, k = 85 % fixed window 0.465 10 15 20 25 30 0.5 0.48 Accuracy (1−WER) Accuracy (1 − WER) 0.52 0.5 239 rate fixed alignment window algorithm of (Naim et al., 2013). We set the heuristic weight parameter (w) to 3 and the chunk size parameter (c) to 5 seconds for all the three sliding window systems and the fixed window system. Sliding alignment window produces better results and outperforms the other algorithms even for large values of the keep-length parameter. The sliding alignment window with keep-length 0.5 achieves 0.5679 average accuracy in terms of (1-WER), providing a 18.09% improvement with respect to the most accurate fixed alignment window (average accuracy 0.4857). On the same dataset, Lasecki et al. (2012) reported 36.6% accuracy using the D</context>
</contexts>
<marker>Naim, Gildea, Lasecki, Bigham, 2013</marker>
<rawString>Iftekhar Naim, Daniel Gildea, Walter Lasecki, and Jeffrey Bigham. 2013. Text alignment for real-time crowd captioning. In Proceedings of the 2013 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-13).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Murat Saraclar</author>
<author>Michael Riley</author>
<author>Enrico Bocchieri</author>
<author>Vincent Goffin</author>
</authors>
<title>Towards automatic closed captioning: Low latency real time broadcast news transcription.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing (ICSLP),</booktitle>
<pages>1741--1744</pages>
<contexts>
<context position="2080" citStr="Saraclar et al., 2002" startWordPosition="337" endWordPosition="340"> recruit a trained stenographer with a special purpose phonetic keyboard, who transcribes the speech to text with less than five seconds of latency. Unfortunately, professional captionists are quite expensive ($150 per hour), must be recruited in blocks of an hour or more, and are difficult to schedule on short Merging Incomplete Captions the quick brown fox jumped over the lazy dog Figure 1: General layout of crowd captioning systems. Captionists (C1, C2, C3) submit partial captions that are automatically combined into a highquality output. notice. Automatic speech recognition (ASR) systems (Saraclar et al., 2002), on the other hand, attempts to provide a cheap and fully automated solution to this problem. However, the accuracy of ASR quickly plummets to below 30% when used on an untrained speaker’s voice, in a new environment, or in the absence of a high quality microphone (Wald, 2006). The accuracy of the ASR systems can be improved using the ‘re-speaking’ technique, which requires a person that the ASR has been trained on to repeat the words said by a speaker as he hears them. Simultaneously hearing and speaking, however, is not straightforward, and requires some training. An alternative approach is</context>
</contexts>
<marker>Saraclar, Riley, Bocchieri, Goffin, 2002</marker>
<rawString>Murat Saraclar, Michael Riley, Enrico Bocchieri, and Vincent Goffin. 2002. Towards automatic closed captioning: Low latency real time broadcast news transcription. In Proceedings of the International Conference on Spoken Language Processing (ICSLP), pages 1741–1744.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Wald</author>
</authors>
<title>Using automatic speech recognition to enhance education for all students: Turning a vision into reality.</title>
<date>2005</date>
<journal>FIE</journal>
<booktitle>In Proceedings 35th Annual Conference on Frontiers in Education,</booktitle>
<volume>05</volume>
<pages>3--3</pages>
<contexts>
<context position="1375" citStr="Wald, 2005" startWordPosition="225" endWordPosition="226">nce. Our method performs alignment on a sliding window of the input sequences, drastically reducing both the number of errors and the latency of the system to the end user over the previously published approaches. 1 Introduction Real-time captioning provides deaf or hard of hearing people access to speech in mainstream classrooms, at public events, and on live television. Studies performed in the classroom setting show that the latency between when a word was said and when it is displayed must be under five seconds to maintain consistency between the captions being read and other visual cues (Wald, 2005; Kushalnagar et al., 2014). The most common approach to real-time captioning is to recruit a trained stenographer with a special purpose phonetic keyboard, who transcribes the speech to text with less than five seconds of latency. Unfortunately, professional captionists are quite expensive ($150 per hour), must be recruited in blocks of an hour or more, and are difficult to schedule on short Merging Incomplete Captions the quick brown fox jumped over the lazy dog Figure 1: General layout of crowd captioning systems. Captionists (C1, C2, C3) submit partial captions that are automatically combi</context>
</contexts>
<marker>Wald, 2005</marker>
<rawString>Mike Wald. 2005. Using automatic speech recognition to enhance education for all students: Turning a vision into reality. In Proceedings 35th Annual Conference on Frontiers in Education, 2005. FIE ’05., pages S3G–S3G, Oct.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Wald</author>
</authors>
<title>Creating accessible educational multimedia through editing automatic speech recognition captioning in real time.</title>
<date>2006</date>
<booktitle>Interactive Technology and Smart Education,</booktitle>
<pages>3--2</pages>
<contexts>
<context position="2358" citStr="Wald, 2006" startWordPosition="391" endWordPosition="392">lt to schedule on short Merging Incomplete Captions the quick brown fox jumped over the lazy dog Figure 1: General layout of crowd captioning systems. Captionists (C1, C2, C3) submit partial captions that are automatically combined into a highquality output. notice. Automatic speech recognition (ASR) systems (Saraclar et al., 2002), on the other hand, attempts to provide a cheap and fully automated solution to this problem. However, the accuracy of ASR quickly plummets to below 30% when used on an untrained speaker’s voice, in a new environment, or in the absence of a high quality microphone (Wald, 2006). The accuracy of the ASR systems can be improved using the ‘re-speaking’ technique, which requires a person that the ASR has been trained on to repeat the words said by a speaker as he hears them. Simultaneously hearing and speaking, however, is not straightforward, and requires some training. An alternative approach is to combine the efforts of multiple non-expert captionists (anyone who can type), instead of relying on trained workers (Lasecki et al., 2012; Naim et al., 2013). In this approach, multiple non-expert human workers transcribe an audio stream containing speech in real-time. Work</context>
</contexts>
<marker>Wald, 2006</marker>
<rawString>Mike Wald. 2006. Creating accessible educational multimedia through editing automatic speech recognition captioning in real time. Interactive Technology and Smart Education, 3(2):131–141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lusheng Wang</author>
<author>Tao Jiang</author>
</authors>
<title>On the complexity of multiple sequence alignment.</title>
<date>1994</date>
<journal>Journal of Computational Biology,</journal>
<volume>1</volume>
<issue>4</issue>
<contexts>
<context position="7166" citStr="Wang and Jiang, 1994" startWordPosition="1250" endWordPosition="1253">an have one of the 2K − 1 possible combinations. Our goal is to find the optimum alignment matrix AOPT that minimizes the sum of pairs (SOP) cost function: c(A) = � c(Aij) (1) 1&lt;i&lt;j&lt;K where c(Aij) is the cost of the pairwise alignment between Si and Sj according to A. Formally, c(Aij) = �N� l=1 sub(ail, ajl), where sub(ail, ajl) denotes the cost of substituting ajl for ail. If ail and ajl are identical, the substitution cost is zero. The substitution cost for two words is estimated based on the edit distance between two words. The exact solution to the SOP optimization problem is NP-Complete (Wang and Jiang, 1994), but many methods solve it approximately. Our approach is based on weighted A* search for approximately solving the MSA problem (Lermen and Reinert, 2000; Naim et al., 2013). 2.1 Weighted A* Search for MSA The problem of minimizing the SOP cost function for K sequences is equivalent to estimating the shortest path between a single source node and a single sink node in a K-dimensional mesh graph, where each node corresponds to a distinct position in the K sequences. The source node is [0, ... , 0] 237 Algorithm 2 Fixed Window Algorithm Require: K input sequences S = {Sl, ... , SK} having lengt</context>
</contexts>
<marker>Wang, Jiang, 1994</marker>
<rawString>Lusheng Wang and Tao Jiang. 1994. On the complexity of multiple sequence alignment. Journal of Computational Biology, 1(4):337–348.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>