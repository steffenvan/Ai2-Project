<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015710">
<title confidence="0.9985355">
Clustering Words by Syntactic Similarity Improves Dependency
Parsing of Predicate-Argument Structures
</title>
<author confidence="0.97916">
Kenji Sagae and Andrew S. Gordon
</author>
<affiliation confidence="0.9935755">
Institute for Creative Technologies
University of Southern California
</affiliation>
<address confidence="0.933844">
13274 Fiji Way
Marina del Rey, CA 90292
</address>
<email confidence="0.999809">
{sagae,gordon}@ict.usc.edu
</email>
<sectionHeader confidence="0.995657" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999399076923077">
We present an approach for deriving syntactic
word clusters from parsed text, grouping
words according to their unlexicalized syntac-
tic contexts. We then explore the use of these
syntactic clusters in leveraging a large corpus
of trees generated by a high-accuracy parser to
improve the accuracy of another parser based
on a different formalism for representing a dif-
ferent level of sentence structure. In our ex-
periments, we use phrase-structure trees to
produce syntactic word clusters that are used
by a predicate-argument dependency parser,
significantly improving its accuracy.
</bodyText>
<sectionHeader confidence="0.998983" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999883770491804">
Syntactic parsing of natural language has ad-
vanced greatly in recent years, in large part due
to data-driven techniques (Collins, 1999;
Charniak, 2000; Miyao and Tsujii, 2005;
McDonald et al., 2005; Nivre et al., 2007) cou-
pled with the availability of large treebanks. Sev-
eral recent efforts have started to look for ways
to go beyond what individual annotated data sets
and individual parser models can offer, looking
to combine diverse parsing models, develop
cross-framework interoperability and evaluation,
and leverage the availability of large amounts of
text available. Two research directions that have
produced promising improvements on the accu-
racy of data-driven parsing are: (1) combining
different parsers using ensemble techniques, such
as voting (Henderson and Brill, 1999; Sagae and
Lavie, 2006; Hall et al., 2007) and stacking
(Nivre and McDonald, 2008; Martins et al.,
2008), and (2) semi-supervised learning, where
unlabeled data (plain text) is used in addition to a
treebank (McClosky et al., 2006; Koo et al.,
2008).
In this paper we explore a new way to obtain
improved parsing accuracy by using a large
amount of unlabeled text and two parsers that use
different ways of representing syntactic structure.
In contrast to previous work where automatically
generated constituent trees were used directly to
train a constituent parsing model (McClosky et
al., 2006), or where word clusters were derived
from a large corpus of plain text to improve a
dependency parser (Koo et al., 2008), we use a
large corpus of constituent trees (previously gen-
erated by an accurate constituent parser), which
we use to produce syntactically derived clusters
that are then used to improve a transition-based
parser that outputs dependency graphs that re-
flect predicate-argument structure where words
may be dependents of more than one parent.
This type of representation is more general than
dependency trees (Sagae and Tsujii, 2008;
Henderson et al., 2008), and is suitable for repre-
senting both surface relations and long-distance
dependencies (such as control, it-cleft and tough
movement).
The first contribution of this work is a novel
approach for deriving syntactic word clusters
from parsed text, grouping words by the general
syntactic contexts where they appear, and not by
n-gram word context (Brown et al., 1992) or by
immediate dependency context (Lin, 1998). Un-
like in clustering approaches that rely on lexical
context (either linear or grammatical) to group
words, resulting in a notion of word similarity
that blurs syntactic and semantic characteristics
of lexical items, we use unlexicalized syntactic
context, so that words are clustered based only
on their syntactic behavior. This way, we at-
tempt to generate clusters that are more concep-
tually similar to part-of-speech tags or supertags
</bodyText>
<page confidence="0.96987">
192
</page>
<note confidence="0.8785215">
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 192–201,
Paris, October 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999917458333334">
(Bangalore and Joshi, 1999), but organized hier-
archically to provide tagsets with varying levels
of granularity.
Our second contribution is a methodology for
leveraging a high-accuracy parser to improve the
accuracy of a parser that uses a different formal-
ism (that represents different structural informa-
tion), without the need to process the input with
both parsers at run-time. In our experiments, we
show that we can improve the accuracy of a fast
dependency parser for predicate-argument struc-
tures by using a corpus which was previously
automatically annotated using a highly accurate
but considerably slower phrase-structure tree
parser. This is accomplished by using the slower
parser only to parse the data used to create the
syntactic word clusters. During run-time, the
dependency parser uses these clusters, which
encapsulate syntactic knowledge from the
phrase-structure parser. Although our experi-
ments focus on the use of phrase-structure and
dependency parsers, the same framework can be
easily applied to data-driven parsing using other
syntactic formalisms, such as CCG or HPSG.
</bodyText>
<sectionHeader confidence="0.715975" genericHeader="method">
2 Clustering by Syntactic Similarity
</sectionHeader>
<bodyText confidence="0.999949388888889">
We developed a new approach to clustering
words according to their syntactic similarity. Our
method involves the use of hierarchical agglom-
erate clustering techniques using the calculated
syntactic distance between words. Syntactic dis-
tance between words is computed as the cosine
distance between vector representations of the
frequency of unique parse tree paths emanating
from the word in a corpus of parse trees. In this
research, we employ a novel encoding of syntac-
tic parse tree paths that includes direction infor-
mation and non-terminal node labels, but does
not include lexical information or part-of-speech
tags. Consequently, the resulting hierarchy
groups words that appear in similar places in
similar parse trees, regardless of its assigned
part-of-speech tag. In this section we describe
our approach in detail.
</bodyText>
<subsectionHeader confidence="0.994657">
2.1 Parse tree path representation
</subsectionHeader>
<bodyText confidence="0.99985125">
Gordon and Swanson (2007) first described a
corpus-based method for calculating a measure
of syntactic similarity between words, and dem-
onstrated its utility in improving the performance
of a syntax-based Semantic Role Labeling sys-
tem. The central idea behind their approach was
that parse tree paths could be used as features
for describing a word’s grammatical behavior.
</bodyText>
<figureCaption confidence="0.735601">
Figure 1: An example parse tree path from
the verb ate to the argument NP He, repre-
sented as TVBDTVPTSINP.
</figureCaption>
<bodyText confidence="0.998044024390244">
Parse tree paths are descriptions of tree transi-
tions from a terminal (e.g. a verb) to a different
node in a constituent parse tree of a sentence.
Parse tree paths gained popularity in early Se-
mantic Role Labeling research (Gildea and Juraf-
sky, 2002), where they were used as features de-
scribing the relationship between a verb and a
particular semantic role label. For example, Fig-
ure 1 illustrates a parse tree path between a verb
and a semantically related noun phrase.
Gordon and Swanson viewed parse tree paths
as features that could be used to describe the syn-
tactic contexts of words in a corpus. In their ap-
proach, all of the possible parse tree paths that
begin at a given word were identified in a large
set of automatically generated constituent parse
trees. The normalized frequency counts of
unique parse tree paths were combined into a
feature vector that describes the location that the
given word appears in the set of parse trees. This
syntactic profile was then compared with other
profiles using a cosine distance function, produc-
ing a quantitative value of word similarity. In
this manner, the syntactic similarity between the
verb “pluck” and the verb “whisk” was calcu-
lated as 0.849.
One drawback of the approach of Gordon and
Swanson was the inclusion of part-of-speech tags
in the encoding of the parse tree paths. As a con-
sequence, the cosine distance between words of
different classes was always zero, regardless of
their similarities in the remainder of the paths.
To correct this problem in our current research,
we removed part-of-speech tags from the encod-
ing of parse tree paths, deleting the tag that be-
gins each path and replacing tags when they ap-
pear at the end of a path with a generic terminal
label.
A second drawback of the approach of Gordon
and Swanson is that the path directionality is un-
derspecified. Consider the parse tree paths that
</bodyText>
<page confidence="0.998664">
193
</page>
<bodyText confidence="0.999769210526316">
emanate from each of the words “some” and
“pancakes” in Figure 1. In the original encoding,
the paths for each of these words would be iden-
tical (if the part of speech tags were removed),
despite their unique locations in this parse tree.
To correct this problem in our current research,
we elaborated the original set of two path identi-
fiers (T and ↓) to a set of six tags that included
information about the direction of the transition.
Up-Right () and Down-Left () transition are
used to and from nodes that are the first constitu-
ent of a non-terminal. Up-Left () and Down-
Right () transitions are used to and from nodes
that are the last constituent of a non-terminal.
Transitions to and from all other constituent
nodes are labeled Up-Middle (T) or Down-
Middle (↓), accordingly. For example, we repre-
sent the parse tree path depicted in Figure 1 as:
VPSNP.
</bodyText>
<subsectionHeader confidence="0.980102">
2.2 Profiles for BLLIP WSJ Corpus words
</subsectionHeader>
<bodyText confidence="0.999912288888889">
As in the previous work of Gordon and Swanson
(2007), we characterize the syntactic properties
of words as the normalized frequency of unique
parse tree paths emanating from the word in a
large corpus of syntactic parse trees.
In our research, we used the Brown Labora-
tory for Linguistic Information Processing
(BLLIP) 1987-89 WSJ Corpus Release 1
(Charniak et al., 2000), which contains approxi-
mately 30 million words of Wall Street Journal
news articles, parsed with Charniak (2000)
parser. Although the trees in the BLLIP corpus
are enriched with function tags and empty nodes,
we remove this information, leaving only the
trees produced by the Charniak parser. We iden-
tified the top five thousand most frequent words
(or, more generally, types, since these also in-
clude other sequences of characters, such as
numbers and punctuation) in this corpus, treating
words that differed in capitalization or in as-
signed part-of-speech tag as separate types.
These five thousand types correspond to ap-
proximately 85% of the tokens in the BLLIP
corpus. For each token instance of each of these
five thousand types, we identified every occur-
ring parse tree path emanating from the token in
each of the sentences in which it appeared. The
most frequent type was the comma, which ap-
peared 2.2 million times and produced 118 mil-
lion parse tree paths. The least frequent token in
this set was the singular noun “pollution,” with
731 instances producing 35,185 parse tree paths.
To generate syntactic profiles for a given type,
the frequency of unique parse tree paths was ta-
tabulated, and then normalized by dividing this
frequency by the number of tokens of that type in
the corpus. To reduce the dimensionality of these
normalized frequency vectors, parse tree paths
that appeared in less than 0.2% of the instances
were ignored. This threshold value produced
vectors with dimensionality that was comparable
across all five thousand types, and small enough
to process given our available computational re-
sources. The mean vector size was 2,228 dimen-
sions, with a standard deviation of 734.
</bodyText>
<subsectionHeader confidence="0.999629">
2.3 Distance calculation and clustering
</subsectionHeader>
<bodyText confidence="0.999994785714286">
Pairwise distances between each of the five thou-
sand types were computed as the cosine distance
between their profile vectors. We then grouped
similar types using hierarchical agglomerate
clustering techniques, where distance between
clusters is calculated as mean distance between
elements of each cluster (average link cluster-
ing).
The three most similar types (the first 2 clus-
tering steps) consisted of the capitalized subordi-
nating conjunctions “Although,” “While,” and
“Though.” The two most dissimilar types (the
last to be included in any existing cluster) were
the symbol “@” and the question mark.
</bodyText>
<subsectionHeader confidence="0.998023">
2.4 Cluster label selection
</subsectionHeader>
<bodyText confidence="0.999990076923077">
Hierarchical agglomerate clustering produces a
binary-branching tree structure, where each
branch point is ordered according to a similarity
value between 0 and 1. In our clustering of the
top five thousand most frequent types in the
BLLIP corpus, there are five thousand leaf nodes
representing individual tokens, and 4999 branch
points that cluster these types into a single tree.
We label each of these 4999 branch points, and
treat these cluster labels as features of the types
that they dominate. For example, the singular
noun “house” participates in 114 clusters of in-
creasing size. The syntactic features of this type
can therefore be characterized by 114 cluster la-
bels, which overlap with varying degrees with
other tokens in the set.
We view these cluster labels as conceptually
similar to traditional part-of-speech tags in that
they are indicative of the syntactic contexts in
which words are likely to appear. Because
words are clustered based on their unlexicalized
syntactic contexts, the resulting clusters are more
likely to reflect purely syntactic information than
are clusters derived from lexical context, such as
adjacent words (Brown et al., 1992) or immedi-
ate head-word (Lin, 1998). However, the extent
</bodyText>
<page confidence="0.99397">
194
</page>
<bodyText confidence="0.99157620754717">
to which these syntactic contexts are specified
can vary from a more general to a more fine-
grained level than that of parts-of-speech. As
clusters become more fine-grained, they become
more similar to supertags (Bangalore and Joshi,
1999). Clusters that represent more specific syn-
tactic contexts can encode information about, for
example, subcategorization. As these labels are
derived empirically from a large corpus of syn-
tactic parse trees, they accurately represent syn-
tactic distinctions in real discourse at different
granularities, in contrast to the single arbitrary
granularity of theoretically derived part-of-
speech tags used in existing treebanks (Marcus et
al., 1993).
While it is sometimes useful to view types as
having multiple part-of-speech tags at different
levels of granularity (e.g. the 114 tags for the
token “house”), it is often useful to select a sin-
gle level of granularity to use across all tokens.
For example, it is useful to know which one of
the 114 cluster labels for “house” to use if ex-
actly 100 part-of-speech distinctions are to be
made among tokens in the set. These cluster la-
bels can be identified by slicing the tree at the
level for which there are exactly 100 branches,
then using the label of the first branch point in
each branch as the label for all of its leaf-node
types, or the leaf-node itself in the case where no
further branching exists. Given our hierarchical
clustering, there are five thousand different ways
to slice the tree in this manner, yielding sets of
cluster labels (and un-clustered types) that vary
in size from 1 to 5000. We identified these sets
for use in the experiments described in the next
sections.
Figure 2 shows a dendrogram representation
of the cluster tree when it is sliced to produce
exactly 60 clusters, 19 of which are individual
types. For the other 41 clusters, we show only
the most frequent word in the cluster and the
number of additional words in the cluster. The
scale line in the lower left of Figure 2 indicates
the horizontal length of a calculated similarity
between clusters of 0.1.
3 Transition-based dependency parsing
with word clusters
The clusters obtained with the approach de-
scribed in section 2 provide sets of syntactic tags
with varying levels of granularity. Previous
work by Koo et al. (2008) and Miller et al.
(2004) suggests that different levels of cluster
granularity may be useful in natural language
</bodyText>
<figureCaption confidence="0.930928333333333">
Figure 2: A hierarchical clustering of the top
five thousand tokens in the BLLIP corpus, cut
at 60 clusters.
</figureCaption>
<page confidence="0.983819">
195
</page>
<figureCaption confidence="0.99954">
Figure 3: Predicate-argument dependency structure following the HPSG Treebank standard.
</figureCaption>
<bodyText confidence="0.999974270833333">
processing tasks with discriminative training.
We add the syntactic clusters as features in a
transition-based parser that uses a classifier to
decide among shift/reduce parser actions based
on the local context of the decision. This transi-
tion-based parsing approach has been found to be
efficient and accurate in dependency parsing of
surface syntactic dependencies (Yamada and
Matsumoto, 2003; Nivre et al., 2004; Hall et al.,
2007) and predicate-argument parsing (Hender-
son et al., 2008; Sagae and Tsujii, 2008).
Our experiments are based on an implementa-
tion of Sagae and Tsujii (2008)’s algorithm for
basic shift-reduce parsing with multiple heads,
which we use to identify predicate-argument de-
pendencies extracted from the HPSG Treebank
developed by Miyao et al. (2004). Using this
data set allows for a comparison of our results
with those obtained in previous work on data-
driven HPSG predicate-argument analysis, while
demonstrating the use of our clustering approach
for cross-framework parser improvement, since
the clusters were derived from syntactic trees in
Penn Treebank format (as produced by the Char-
niak parser, without empty nodes, co-indexation
or function tags), and used in the identification of
HPSG Treebank predicate-argument
dependencies. Figure 3 shows a predicate-
argument dependency structure following the
annotation standard of the HPSG Treebank,
where arrows point from head to modifier. We
note that unlike in the widely known PropBank
(Palmer et al., 2005) predicate-argument struc-
tures, argument labels start from ARG1 (not
ARG0), and predicate-argument relationships are
annotated for all words. One difference between
in our implementation is that, instead of maxi-
mum entropy classification used by Sagae and
Tsujii, we perform parser action classification
using the averaged perceptron (Freund and
Schapire, 1999; Collins, 2002), which allows for
the inclusion of all of Sagae and Tsujii’s fea-
tures, in addition to a set of cluster-based fea-
tures, while retaining fast training times.
We now describe the parsing approach, start-
ing with the dependency DAG parser that we use
as a baseline, followed by how the syntactic clus-
ter features were added to the baseline parser.
</bodyText>
<subsectionHeader confidence="0.9382655">
3.1 Arc-standard parsing for dependency
DAGs
</subsectionHeader>
<bodyText confidence="0.999972448275862">
Sagae and Tsujii (2008) describe two algorithms
for dependency parsing with words that have
multiple heads. Each corresponds to extensions
of Nivre (2004)’s arc-standard and arc-eager al-
gorithms for dependency (tree) parsing. In our
experiments, we used an implementation of the
arc-standard extension.
Nivre’s arc-standard dependency parsing algo-
rithm uses a stack to process the input string one
word at a time, from left to right, using two gen-
eral types of parser action: shift (push the next
input token onto the stack), and reduce (create a
dependency arc between the top two items on the
stack, and pop the item marked as the depend-
ent). Reduce actions are subdivided into reduce-
right and reduce-left, indicating which of the two
items on the top of the stack is the head, and
which is the dependent in the newly formed de-
pendency arc. These two reduce actions can be
further subdivided to reflect what type of de-
pendency arc is created, in the case of labeled
dependency parsing. The extension for allowing
multiple heads per word consists of the addition
a new type of parser action: attach, which creates
a dependency arc without removing anything
from the stack. As with reduce actions, there are
two types of attach: attach-left which creates a
dependency arc between the top two items on the
stack such that the item on top is the head, and
</bodyText>
<page confidence="0.995748">
196
</page>
<bodyText confidence="0.999992023809524">
right-attach, which creates a dependency arc be-
tween the top two items on the stack such that
the top item is the dependent, then pops it from
the stack and unshifts it back into the input. Fi-
nally, this algorithm for unlabeled graphs can be
extended to produce labeled dependencies in the
same way as Nivre’s algorithm, by replacing the
reduce and attach actions with sets of actions that
perform the reduce or attach operation and also
name the label of the arc created. Sagae and
Tsujii (2008) provide a more detailed description
of the algorithm, including an example that illus-
trates the new attach actions.
This basic algorithm is only capable of pro-
ducing labeled directed acyclic graphs where, if
the nodes (which correspond to words) are
placed on a left to right sequence on a horizontal
line in the order in which the words appear in the
input sentence, all arcs can be drawn above the
nodes without crossing. This corresponds to the
notion of projectivity that similarly limits the
types of trees produced by Nivre’s algorithm.
Just as in dependency parsing with tree struc-
tures, a way to effectively remove this limitation
is the use of pseudo-projective transformations
(Nivre and Nilsson, 2005), where arcs that cross
have their heads moved towards the root and
have their labels edited to reflect this change,
often making it reversible. Once crossing arcs
have been “lifted” so that no crossing arcs re-
main, the “projectivized” structures are used to
train a parsing model. Projective structures pro-
duced by this model can be “deprojectivized”
through the use of the edits in the arc labels, in
an attempt to produce structures that conform to
the scheme in the original data. Sagae and Tsujii
also propose a simple arc reversal transform,
which simply reverses the direction of a depend-
ency arc (editing the arc label to note this
change). This transformation, which can be re-
versed trivially, makes it possible to remove cy-
cles in dependency graphs.
</bodyText>
<subsectionHeader confidence="0.99818">
3.2 Baseline features
</subsectionHeader>
<bodyText confidence="0.99996647826087">
To create output graph structures for an input
sentence, the algorithm described in section 3.1
relies on an oracle that tells it what action to take
at each parser state, where the state is the con-
tents of the stack, remaining words in the input,
and the dependency arcs formed so far. In
grammar-based shift-reduce parsing, this oracle
may take the form of a look-up table derived
from grammar rules. In our data-driven setting,
where the parser learns to choose actions based
on examples of correctly parsed data, the (likely
imperfect) substitute for the oracle is a classifier
that takes features that represent the parser state
as input, and produces a matching parser action
as output. These features should represent as-
pects of the parser state that may be informative
as to what the corresponding appropriate action
is. Our baseline model uses the averaged percep-
tron with a core set of features derived from the
following templates, where S(n) denotes the n-th
item from the top of the stack (for example, S(1)
is the item on top of the stack), and I(n) denotes
the next n-th input token:
</bodyText>
<listItem confidence="0.976289533333333">
1. For the items S(1) and S(2):
a. the total number of dependents;
b. the number of dependents to the
right of the item;
c. the number of dependents to the left
of the item;
d. the part-of-speech tag of the right-
most dependent of the item;
e. the part-of-speech tag of the leftmost
dependent of the item;
f. the arc label of the rightmost de-
pendent of the item;
g. the arc label of the leftmost depend-
ent of the item;
2. the words in items S(1), S(2), S(3), I(1) and
I(2);
3. the part-of-speech tags in items S(1), S(2),
S(3), I(1), I(2) and I(3);
4. the part-of-speech tag of the word i mmedi-
aely to the right of S(2);
5. the part-of-speech tag of the word immedi-
ately to the left of S(1);
6. whether an arc exists between S(1) and S(2);
7. whether an arc exists between S(1) and I(1);
8. the direction of the arc between S(1) and
S(2), if any;
9. the label of the arc between S(1) and S(2), if
any;
10. the label of the arc between S(1) and I(1), if
any;
</listItem>
<bodyText confidence="0.82291575">
11. the distance, in linear sequence of words,
between S(1) and S(2);
12. the distance, in linear sequence of words,
between S(1) and I(1);
</bodyText>
<page confidence="0.996242">
197
</page>
<bodyText confidence="0.990391142857143">
13. the previous parser action.
In addition to the core set of features, we also
use features obtained by concatenating the part-
of-speech tags in S(1), S(2) and I(1) with the fea-
tures derived from templates 1-6, and additional
features derived from selected concatenation of
two or three core features.
</bodyText>
<subsectionHeader confidence="0.989919">
3.3 Cluster-based features
</subsectionHeader>
<bodyText confidence="0.978803916666667">
To take advantage of the clusters that reflect syn-
tactic similarity between words, we assign arbi-
trary unique labels to each of the hierarchical
clusters obtained using the procedure described
in section 2. These cluster labels are used to
generate additional features that help the parser
make its decisions base on the syntactic profile
of words. As explained in section 2.4, each there
may be several cluster labels (corresponding to
clusters of different granularities) associated with
each word. To select the set of cluster labels to
be used to generate features, we first select a de-
sired granularity for the clusters, and use the set
of labels resulting from slicing the cluster tree at
the appropriate level, as discussed in section 2.4.
We experimented with several levels of cluster
granularity using development data, and follow-
ing Koo et al. (2008), we also experimented with
using two sets of cluster labels with different
levels of granularity at the same time. Given a
specific level of granularity, the cluster-based
features we used are:
14. the cluster labels for the words in items S(1),
S(2), S(3), I(1), I(2), I(3);
15. the cluster labels for the words in the right-
most and leftmost dependents of S(1) and
S(2);
16. the concatenation of the cluster labels for the
words in S(1), S(2) and I(1), and the features
derived from feature templates 1-15.
In experiments where we used two sets of
cluster labels corresponding to different levels of
granularity, we added all the cluster-based fea-
tures in 14 and 15 for both sets of labels, and the
features in 16 only for the set corresponding to
the coarser-grained clusters.
</bodyText>
<sectionHeader confidence="0.999737" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999990666666667">
Following previous experiments with Penn Tree-
bank WSJ data, or annotations derived from it,
we used sections 02-21 of the HPSG Treebank as
training material, section 22 for development,
and section 23 for testing. Only the predicate-
argument dependencies were used, not the phrase
structures or other information from the HPSG
analyses. For all experiments described here,
part-of-speech tagging was done separately using
a CRF tagger with accuracy of 97.3% on sections
22-24. Our evaluation is based on labeled preci-
sion and recall of predicate-argument dependen-
cies. Although accuracy is commonly used for
evaluation of dependency parsers, in our task the
parser is not restricted to output a fixed number
of dependencies. Labeled precision and recall of
predicate-argument pairs are also the standard
evaluation metrics for data-driven HPSG and
CCG parsers (although the predicate-argument
pairs extracted from the HPSG Treebank and the
CCGBank are specific to their formalisms and
not quantitatively comparable).
We started by eliminating cycles from the de-
pendency graphs extracted from the HPSG Tree-
bank by using the arc reversal transform in the
following way: for each cycle detected in the
data, the shortest arc in the cycle was reversed
until no cycles remained. We then applied
pseudo-projective transformation to create data
that can be used to train our parser, described in
section 3. By detransforming the projective
graphs generated from gold-standard dependen-
cies, we obtain labeled precision of 98.1% and
labeled recall of 97.7%, which is below the accu-
racy expected for detransformation of syntactic
dependency trees. This is expected, since arc
crossing occurs more frequently in predicate-
argument graphs in the HPSG Treebank than in
surface syntactic dependencies.
We first trained a parsing model without clus-
ter-based features, using only the baseline set of
features, which was the product of experimenta-
tion using the development set. On the test set,
this baseline model has labeled precision and
recall of 88.7 and 88.2, respectively, slightly be-
low the precision and recall obtained by Sagae
and Tsujii on the same data (89.0 precision and
88.5 recall).
We then used the development set to explore
the effects of cluster sets with different levels of
granularity. The baseline model has precision
and recall of 88.6 and 88.0 on the development
set. We found that by slicing the cluster tree
relatively close to the root, resulting in a set of
50 to 100 distinct cluster labels (corresponding to
relatively coarse clusters), we obtain small (0.3
to 0.4), but statistically significant (p &lt; 0.005)
improvements on precision and recall over the
baseline model on the development set. By in-
creasing the number of cluster labels (making the
</bodyText>
<page confidence="0.997291">
198
</page>
<figureCaption confidence="0.808492285714286">
Figure 4: Effect of cluster granularity on
labeled the precision and recall of predicate-
argument pairs in the development set. The
improvement in precision and recall between
the baseline (zero cluster labels, where no
cluster information is added) and 600 cluster
labels is statistically significant (p &lt; 0.0005).
</figureCaption>
<bodyText confidence="0.999674424242424">
distinctions among members of different clusters
more fine-grained) in steps of 100, we observed
improvements in precision and recall until the
point where there were 600 distinct cluster la-
bels. This set of 600 cluster labels produced the
highest values of precision and recall (89.5 and
89.0) that we obtained for the development set
using only one set of cluster labels. Figure 4
shows how precision, recall and F-score on the
development set varied with the number of clus-
ter labels used.
Following Koo et al. (2008), we also experi-
mented with using two sets of cluster labels with
different levels of granularity. We found that
using the set of 600 labels and an additional set
with fewer than 600 labels did not improve or
hurt precision and recall. Finer grained clusters
with more than 1,000 labels (combined with the
set of 600 labels) improved results further. The
highest precision and recall figures of 90.1 and
89.6 were obtained with the sets of 600 and
1,400 labels.
We parsed the test set using the best configu-
ration of cluster-based features as determined
using the development set (the sets with 600 and
1,400 cluster labels) and obtained 90.2 precision,
89.8 recall and 90.0 f-score, a 13.8% reduction in
error over a strong baseline. Table 1 summarizes
our results on the test set. For comparison, we
also shows results published by Sagae and Tsujii
(2008), to our knowledge the highest f-score re-
ported for this test set, and Miyao and Tsujii
(2005), who first reported results on this data set.
</bodyText>
<table confidence="0.999593">
Precision Recall F-score
Baseline 88.7 88.2 88.4
Clusters 90.2 89.8 90.0
S &amp; T 89.9 88.5 88.7
Miyao et al. 85.0 84.3 84.6
</table>
<tableCaption confidence="0.999579">
Table 1: Results obtained on the test set us-
</tableCaption>
<bodyText confidence="0.64239725">
ing our baseline model and our best cluster-
based features. The results in the bottom two
rows are from Sagae and Tsujii (2008) and
Miyao and Tsujii (2005).
</bodyText>
<subsectionHeader confidence="0.8056125">
4.1 Surface dependency parsing with clus-
ter-based features
</subsectionHeader>
<bodyText confidence="0.999935947368421">
The parser used in our experiments with HPSG
Treebank predicate-argument structures can as-
sign more than one head for a single word, but
when the parser is trained using only dependency
trees, it behaves in exactly the same way as a
parser based on Nivre’s arc-standard algorithm,
since it never sees examples of attach actions
during training. To see whether our clusters can
improve surface dependency parsing, and to al-
low for comparison of our results to a larger
body of research on surface dependency parsing,
we used dependency trees extracted from the
Penn Treebank using the Yamada and Matsu-
moto (2003) version of the Penn Treebank head-
percolation rules to train parsing models that
produce dependency trees. However, no tuning
of the features or metaparameters was per-
formed; the parser was trained as-is on depend-
ency trees.
We used the standard train, development and
test sets splits to train two models, as in our ex-
periments with predicate-argument dependen-
cies: a baseline that uses no cluster information,
and a model that uses two sets of clusters that
were found to improve results in the develop-
ment set. The unlabeled accuracy of our baseline
model on the test set is 89.96%, which is consid-
erably lower than the best current results. Koo et
al. (2008) report 90.84% for a first-order edge-
factored model, and 92.02% for a second-order
model (and as high as 93.16% with a second-
order model enriched with cluster features de-
rived from plain text). Using two sets of clus-
ters, one with 600 and one with 1,200 labels, ac-
curacy improves by 1.32%, to reach 91.28% (a
13.15% reduction in error compared to our base-
line). While still below the level of the strongest
results for this dataset, it is interesting to see that
</bodyText>
<page confidence="0.996526">
199
</page>
<bodyText confidence="0.9995925">
the improvement in accuracy over the baseline
observed for surface dependency trees is similar
to the improvement observed for predicate-
argument dependency graphs.
</bodyText>
<sectionHeader confidence="0.999855" genericHeader="method">
5 Related work
</sectionHeader>
<bodyText confidence="0.999967774193548">
Many aspects of this research were inspired by
the recent work of Koo et al. (2008), who re-
ported impressive results on improving depend-
ency parsing accuracy using a second order
edge-factored model and word clusters derived
from plain text using the Brown et al. (1992) al-
gorithm. Our clustering approach is significantly
different, focusing on the use of parsed data to
produce strictly syntactic clusters. It is possible
that using both types of clusters may be benefi-
cial.
McClosky et al. (2006) used a large corpus of
parsed text to obtain improved parsing results
through self-training. A key difference in our
general framework is that it allows for a parser
with one type of syntactic representation to im-
prove the accuracy of a different parser with a
different type of formalism. In this regard, our
work is related to that of Sagae et al. (2007), who
used a stacking-like framework to allow a sur-
face dependency parser to improve an HPSG
parser. In that work, however, as in other work
that combines different parsers through stacking
(Martins et al., 2008; Nivre and McDonald,
2008) or voting (Henderson and Brill, 1999),
multiple parsers need to process new text at run-
time. In our approach for leveraging diverse
parsers, one of the parsers is used only to create a
parsed corpus from which we extract clusters of
words that have similar syntactic behaviors, and
only one parser is needed at run-time.
</bodyText>
<sectionHeader confidence="0.999249" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999966714285714">
We have presented a novel approach for deriving
word clusters based on syntactic similarity, and
shown how these word clusters can be applied in
a transition-based dependency parser.
Our experiments focused on predicate-
argument structures extracted from the HPSG
Treebank, which demonstrates that the syntactic
clusters are effective in leveraging cross-
framework parser representations to improve
parsing accuracy. However, we expect that simi-
lar accuracy improvements can be obtained in
parsing using other frameworks and formalisms,
and possibly in other natural language processing
tasks.
</bodyText>
<sectionHeader confidence="0.99549" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999872142857143">
The project or effort described here has been
sponsored by the U.S. Army Research, Devel-
opment, and Engineering Command (RDE-
COM). Statements and opinions expressed do
not necessarily reflect the position or the policy
of the United States Government, and no official
endorsement should be inferred.
</bodyText>
<sectionHeader confidence="0.996148" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993630209302325">
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: an approach to almost parsing. Compu-
tational Linguistics 25, 2 (Jun. 1999), 237-265.
Peter F. Brown, Vincent J. Della Pietra, Peter V.
deSouza, Jennifer C. Lai, and Robert L. Mercer.
1992. Class-Based n-gram Models of Natural Lan-
guage. Computational Linguistics, 18(4):467–479.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the First Meet-
ing of the North American Chapter of the Associa-
tion for Computational Linguistics (NAACL), pages
132–139.
Charniak, E., Blaheta, D., Ge, N., Hall, K., Hale, J.,
and Johnson, M. (2000) BLLIP 1987-89 WSJ Cor-
pus Release 1. Philadelphia, PA: Linguistic Data
Consortium.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Michael Collins. 2002. Discriminative Training Me-
thods for Hidden Markov Models: Theory and
Experiments with Perceptron Algorithms. In Pro-
ceedings of EMNLP, pages 1–8.
Yoav Freund and Robert E. Schapire. 1999. Large
Margin Classification Using the Perceptron Algo-
rithm. Machine Learning, 37(3):277–296.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
Labeling of Semantic Roles. Computational Lin-
guistics 28(3): 245-288.
Andrew Gordon and Reid Swanson. 2007. Generaliz-
ing semantic role annotations across syntactically
similar verbs. Proceedings of the 2007 meeting of
the Association for Computational Linguistics
(ACL-07), Prague, Czech Republic, June 23-30,
2007.
Johan Hall, Jens Nilsson, Joakim Nivre, Gulsen Ery-
igit, Beata Megyesi, Mattias Nilsson, and Markus
Saers. 2007. Single malt or blended? A study in
multilingual parser optimization. In Proceedings of
EMNLP-CoNLL.
James Henderson, Paola Merlo, G. Musillo, and Ivan
Titov. 2008. A latent variable model of synchro-
nous parsing for syntactic and semantic dependen-
</reference>
<page confidence="0.939974">
200
</page>
<reference confidence="0.999881255555555">
cies. In Proceedings of the Shared Task of the Con-
ference on Computational Natural Language
Learning (CoNLL), pages 178-182. Manchester,
UK.
John Henderson and Eric Brill. 1999. Exploiting di-
versity in natural language processing: combining
parsers. In Proceedings of the Fourth Conference
on Empirical Methods in Natural Language Proc-
essing (EMNLP).
Terry Koo, Xavier Carreras and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies (ACL-08:HLT), pages
595-603.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 17th inter-
national Conference on Computational Linguistics
- Volume 2. Montreal, Quebec, Canada.
Mitchell P. Marcus, Mary A. Marcinkiewicz, Beatrice
Santorini. 1993. Building a large annotated corpus
of English: The Penn Treebank, Computational
Linguistics, 19(2), June 1993.
André F. T. Martins, Dipanjan Das, Noah A. Smith,
and Eric P. Xing. 2008. Stacking Dependency
Parsers. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
Waikiki, HI.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective Self-Training for Parsing. In
Proceedings of HLT-NAACL, pages 152–159.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of ACL, pages
91–98.
Scott Miller, Jethran Guinness and Alex Zamanian.
2004. Name Tagging withWord Clusters and Dis-
criminative Training. In Proceedings of HLT-
NAACL, pages 337–342.
Miyao, Yusuke, Takashi Ninomiya, and Jun’ichi Tsu-
jii. 2004. Corpus-oriented grammar development
for acquiring a Head-driven Phrase Structure
Grammar from the Penn Treebank. In Proceedings
of the International Joint Conference on Natural
Language Processing (IJCNLP).
Miyao Yusuke and Jun&apos;ichi Tsujii. 2005. Probabilistic
disambiguation models for wide-coverage HPSG
parsing. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics.
Joakim Nivre.2004. Incrementality in Deterministic
Dependency Parsing. In Incremental Parsing:
Bringing Engineering and Cognition Together
(Workshop at ACL-2004).
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In Proceed-
ings of CoNLL, pages 49–56.
Joakim Nivre. and Jens Nilsson. 2005. Pseudo-
Projective Dependency Parsing. In Proceedings of
the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL), pp. 99-106.
Joakim Nivre, Johan Hall, Sandra Kubler, Ryan
McDonald, Jens Nilsson, Sebastian Riedel, and
Deniz Yuret. 2007. The CoNLL 2007 shared task
on dependency parsing. In Proceedings of
EMNLP-CoNLL, pages 915-932.
Nivre, J. and McDonald, R. (2008) Integrating Graph-
Based and Transition-Based Dependency Parsers.
In Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies (ACL-08: HLT), 950-958.
Martha Palmer, Dan Gildea and Paul Kingsbury.
2005. The Proposition Bank: A Corpus Annotated
with Semantic Roles. Computational Linguistics,
31:1.
Kenji Sagae and Alon Lavie. 2006. Parser combina-
tion by reparsing. In Proceedings of NAACL: Short
Papers, pages 129–132.
Kenji Sagae, Yusuke Miyao Jun’ichi and Tsujii. 2007.
HPSG Parsing with shallow dependency con-
straints. In Proceedings of the 44th Meeting of the
Association for Computational Linguistics.
Kenji Sagae and Jun’ichi Tsujii. 2008. Shift-reduce
dependency DAG parsing. In Proceedings of the
International Conference on Computational Lin-
guistics (COLING 2008).
Hiroyasu Yamada and Y. Matsumoto. 2003. Statisti-
cal Dependency Analysis With Support Vector
Machines. In Proceedings of the Eighth Interna-
tional Workshop on Parsing Technologies (IWPT),
pages 195–206.
</reference>
<page confidence="0.998393">
201
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.750574">
<title confidence="0.9689855">Clustering Words by Syntactic Similarity Improves Parsing of Predicate-Argument Structures</title>
<affiliation confidence="0.951450333333333">Sagae S. Institute for Creative University of Southern</affiliation>
<address confidence="0.98061">13274 Fiji Marina del Rey, CA 90292</address>
<email confidence="0.999231">sagae@ict.usc.edu</email>
<email confidence="0.999231">gordon@ict.usc.edu</email>
<abstract confidence="0.998902214285714">We present an approach for deriving syntactic word clusters from parsed text, grouping words according to their unlexicalized syntactic contexts. We then explore the use of these syntactic clusters in leveraging a large corpus of trees generated by a high-accuracy parser to improve the accuracy of another parser based on a different formalism for representing a different level of sentence structure. In our experiments, we use phrase-structure trees to produce syntactic word clusters that are used by a predicate-argument dependency parser, significantly improving its accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Aravind K Joshi</author>
</authors>
<title>Supertagging: an approach to almost parsing.</title>
<date>1999</date>
<journal>Computational Linguistics</journal>
<volume>25</volume>
<pages>237--265</pages>
<contexts>
<context position="3892" citStr="Bangalore and Joshi, 1999" startWordPosition="585" endWordPosition="588"> clustering approaches that rely on lexical context (either linear or grammatical) to group words, resulting in a notion of word similarity that blurs syntactic and semantic characteristics of lexical items, we use unlexicalized syntactic context, so that words are clustered based only on their syntactic behavior. This way, we attempt to generate clusters that are more conceptually similar to part-of-speech tags or supertags 192 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 192–201, Paris, October 2009. c�2009 Association for Computational Linguistics (Bangalore and Joshi, 1999), but organized hierarchically to provide tagsets with varying levels of granularity. Our second contribution is a methodology for leveraging a high-accuracy parser to improve the accuracy of a parser that uses a different formalism (that represents different structural information), without the need to process the input with both parsers at run-time. In our experiments, we show that we can improve the accuracy of a fast dependency parser for predicate-argument structures by using a corpus which was previously automatically annotated using a highly accurate but considerably slower phrase-struc</context>
<context position="13360" citStr="Bangalore and Joshi, 1999" startWordPosition="2117" endWordPosition="2120">y are indicative of the syntactic contexts in which words are likely to appear. Because words are clustered based on their unlexicalized syntactic contexts, the resulting clusters are more likely to reflect purely syntactic information than are clusters derived from lexical context, such as adjacent words (Brown et al., 1992) or immediate head-word (Lin, 1998). However, the extent 194 to which these syntactic contexts are specified can vary from a more general to a more finegrained level than that of parts-of-speech. As clusters become more fine-grained, they become more similar to supertags (Bangalore and Joshi, 1999). Clusters that represent more specific syntactic contexts can encode information about, for example, subcategorization. As these labels are derived empirically from a large corpus of syntactic parse trees, they accurately represent syntactic distinctions in real discourse at different granularities, in contrast to the single arbitrary granularity of theoretically derived part-ofspeech tags used in existing treebanks (Marcus et al., 1993). While it is sometimes useful to view types as having multiple part-of-speech tags at different levels of granularity (e.g. the 114 tags for the token “house</context>
</contexts>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>Srinivas Bangalore and Aravind K. Joshi. 1999. Supertagging: an approach to almost parsing. Computational Linguistics 25, 2 (Jun. 1999), 237-265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Peter V deSouza</author>
<author>Jennifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Class-Based n-gram Models of Natural Language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="3208" citStr="Brown et al., 1992" startWordPosition="485" endWordPosition="488">ed parser that outputs dependency graphs that reflect predicate-argument structure where words may be dependents of more than one parent. This type of representation is more general than dependency trees (Sagae and Tsujii, 2008; Henderson et al., 2008), and is suitable for representing both surface relations and long-distance dependencies (such as control, it-cleft and tough movement). The first contribution of this work is a novel approach for deriving syntactic word clusters from parsed text, grouping words by the general syntactic contexts where they appear, and not by n-gram word context (Brown et al., 1992) or by immediate dependency context (Lin, 1998). Unlike in clustering approaches that rely on lexical context (either linear or grammatical) to group words, resulting in a notion of word similarity that blurs syntactic and semantic characteristics of lexical items, we use unlexicalized syntactic context, so that words are clustered based only on their syntactic behavior. This way, we attempt to generate clusters that are more conceptually similar to part-of-speech tags or supertags 192 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 192–201, Paris, Octobe</context>
<context position="13061" citStr="Brown et al., 1992" startWordPosition="2069" endWordPosition="2072">s in 114 clusters of increasing size. The syntactic features of this type can therefore be characterized by 114 cluster labels, which overlap with varying degrees with other tokens in the set. We view these cluster labels as conceptually similar to traditional part-of-speech tags in that they are indicative of the syntactic contexts in which words are likely to appear. Because words are clustered based on their unlexicalized syntactic contexts, the resulting clusters are more likely to reflect purely syntactic information than are clusters derived from lexical context, such as adjacent words (Brown et al., 1992) or immediate head-word (Lin, 1998). However, the extent 194 to which these syntactic contexts are specified can vary from a more general to a more finegrained level than that of parts-of-speech. As clusters become more fine-grained, they become more similar to supertags (Bangalore and Joshi, 1999). Clusters that represent more specific syntactic contexts can encode information about, for example, subcategorization. As these labels are derived empirically from a large corpus of syntactic parse trees, they accurately represent syntactic distinctions in real discourse at different granularities,</context>
<context position="32698" citStr="Brown et al. (1992)" startWordPosition="5334" endWordPosition="5337">h 91.28% (a 13.15% reduction in error compared to our baseline). While still below the level of the strongest results for this dataset, it is interesting to see that 199 the improvement in accuracy over the baseline observed for surface dependency trees is similar to the improvement observed for predicateargument dependency graphs. 5 Related work Many aspects of this research were inspired by the recent work of Koo et al. (2008), who reported impressive results on improving dependency parsing accuracy using a second order edge-factored model and word clusters derived from plain text using the Brown et al. (1992) algorithm. Our clustering approach is significantly different, focusing on the use of parsed data to produce strictly syntactic clusters. It is possible that using both types of clusters may be beneficial. McClosky et al. (2006) used a large corpus of parsed text to obtain improved parsing results through self-training. A key difference in our general framework is that it allows for a parser with one type of syntactic representation to improve the accuracy of a different parser with a different type of formalism. In this regard, our work is related to that of Sagae et al. (2007), who used a s</context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jennifer C. Lai, and Robert L. Mercer. 1992. Class-Based n-gram Models of Natural Language. Computational Linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the First Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL),</booktitle>
<pages>132--139</pages>
<contexts>
<context position="1029" citStr="Charniak, 2000" startWordPosition="146" endWordPosition="147">c contexts. We then explore the use of these syntactic clusters in leveraging a large corpus of trees generated by a high-accuracy parser to improve the accuracy of another parser based on a different formalism for representing a different level of sentence structure. In our experiments, we use phrase-structure trees to produce syntactic word clusters that are used by a predicate-argument dependency parser, significantly improving its accuracy. 1 Introduction Syntactic parsing of natural language has advanced greatly in recent years, in large part due to data-driven techniques (Collins, 1999; Charniak, 2000; Miyao and Tsujii, 2005; McDonald et al., 2005; Nivre et al., 2007) coupled with the availability of large treebanks. Several recent efforts have started to look for ways to go beyond what individual annotated data sets and individual parser models can offer, looking to combine diverse parsing models, develop cross-framework interoperability and evaluation, and leverage the availability of large amounts of text available. Two research directions that have produced promising improvements on the accuracy of data-driven parsing are: (1) combining different parsers using ensemble techniques, such</context>
<context position="9617" citStr="Charniak (2000)" startWordPosition="1523" endWordPosition="1524">ordingly. For example, we represent the parse tree path depicted in Figure 1 as: VPSNP. 2.2 Profiles for BLLIP WSJ Corpus words As in the previous work of Gordon and Swanson (2007), we characterize the syntactic properties of words as the normalized frequency of unique parse tree paths emanating from the word in a large corpus of syntactic parse trees. In our research, we used the Brown Laboratory for Linguistic Information Processing (BLLIP) 1987-89 WSJ Corpus Release 1 (Charniak et al., 2000), which contains approximately 30 million words of Wall Street Journal news articles, parsed with Charniak (2000) parser. Although the trees in the BLLIP corpus are enriched with function tags and empty nodes, we remove this information, leaving only the trees produced by the Charniak parser. We identified the top five thousand most frequent words (or, more generally, types, since these also include other sequences of characters, such as numbers and punctuation) in this corpus, treating words that differed in capitalization or in assigned part-of-speech tag as separate types. These five thousand types correspond to approximately 85% of the tokens in the BLLIP corpus. For each token instance of each of th</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropyinspired parser. In Proceedings of the First Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL), pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>D Blaheta</author>
<author>N Ge</author>
<author>K Hall</author>
<author>J Hale</author>
<author>M Johnson</author>
</authors>
<date>2000</date>
<journal>BLLIP 1987-89 WSJ Corpus Release</journal>
<volume>1</volume>
<institution>Linguistic Data Consortium.</institution>
<location>Philadelphia, PA:</location>
<contexts>
<context position="9504" citStr="Charniak et al., 2000" startWordPosition="1504" endWordPosition="1507"> of a non-terminal. Transitions to and from all other constituent nodes are labeled Up-Middle (T) or DownMiddle (↓), accordingly. For example, we represent the parse tree path depicted in Figure 1 as: VPSNP. 2.2 Profiles for BLLIP WSJ Corpus words As in the previous work of Gordon and Swanson (2007), we characterize the syntactic properties of words as the normalized frequency of unique parse tree paths emanating from the word in a large corpus of syntactic parse trees. In our research, we used the Brown Laboratory for Linguistic Information Processing (BLLIP) 1987-89 WSJ Corpus Release 1 (Charniak et al., 2000), which contains approximately 30 million words of Wall Street Journal news articles, parsed with Charniak (2000) parser. Although the trees in the BLLIP corpus are enriched with function tags and empty nodes, we remove this information, leaving only the trees produced by the Charniak parser. We identified the top five thousand most frequent words (or, more generally, types, since these also include other sequences of characters, such as numbers and punctuation) in this corpus, treating words that differed in capitalization or in assigned part-of-speech tag as separate types. These five thousa</context>
</contexts>
<marker>Charniak, Blaheta, Ge, Hall, Hale, Johnson, 2000</marker>
<rawString>Charniak, E., Blaheta, D., Ge, N., Hall, K., Hale, J., and Johnson, M. (2000) BLLIP 1987-89 WSJ Corpus Release 1. Philadelphia, PA: Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="1013" citStr="Collins, 1999" startWordPosition="144" endWordPosition="145">alized syntactic contexts. We then explore the use of these syntactic clusters in leveraging a large corpus of trees generated by a high-accuracy parser to improve the accuracy of another parser based on a different formalism for representing a different level of sentence structure. In our experiments, we use phrase-structure trees to produce syntactic word clusters that are used by a predicate-argument dependency parser, significantly improving its accuracy. 1 Introduction Syntactic parsing of natural language has advanced greatly in recent years, in large part due to data-driven techniques (Collins, 1999; Charniak, 2000; Miyao and Tsujii, 2005; McDonald et al., 2005; Nivre et al., 2007) coupled with the availability of large treebanks. Several recent efforts have started to look for ways to go beyond what individual annotated data sets and individual parser models can offer, looking to combine diverse parsing models, develop cross-framework interoperability and evaluation, and leverage the availability of large amounts of text available. Two research directions that have produced promising improvements on the accuracy of data-driven parsing are: (1) combining different parsers using ensemble </context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="17604" citStr="Collins, 2002" startWordPosition="2790" endWordPosition="2791">dependencies. Figure 3 shows a predicateargument dependency structure following the annotation standard of the HPSG Treebank, where arrows point from head to modifier. We note that unlike in the widely known PropBank (Palmer et al., 2005) predicate-argument structures, argument labels start from ARG1 (not ARG0), and predicate-argument relationships are annotated for all words. One difference between in our implementation is that, instead of maximum entropy classification used by Sagae and Tsujii, we perform parser action classification using the averaged perceptron (Freund and Schapire, 1999; Collins, 2002), which allows for the inclusion of all of Sagae and Tsujii’s features, in addition to a set of cluster-based features, while retaining fast training times. We now describe the parsing approach, starting with the dependency DAG parser that we use as a baseline, followed by how the syntactic cluster features were added to the baseline parser. 3.1 Arc-standard parsing for dependency DAGs Sagae and Tsujii (2008) describe two algorithms for dependency parsing with words that have multiple heads. Each corresponds to extensions of Nivre (2004)’s arc-standard and arc-eager algorithms for dependency (</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms. In Proceedings of EMNLP, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
</authors>
<title>Large Margin Classification Using the Perceptron Algorithm.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="17588" citStr="Freund and Schapire, 1999" startWordPosition="2786" endWordPosition="2789">reebank predicate-argument dependencies. Figure 3 shows a predicateargument dependency structure following the annotation standard of the HPSG Treebank, where arrows point from head to modifier. We note that unlike in the widely known PropBank (Palmer et al., 2005) predicate-argument structures, argument labels start from ARG1 (not ARG0), and predicate-argument relationships are annotated for all words. One difference between in our implementation is that, instead of maximum entropy classification used by Sagae and Tsujii, we perform parser action classification using the averaged perceptron (Freund and Schapire, 1999; Collins, 2002), which allows for the inclusion of all of Sagae and Tsujii’s features, in addition to a set of cluster-based features, while retaining fast training times. We now describe the parsing approach, starting with the dependency DAG parser that we use as a baseline, followed by how the syntactic cluster features were added to the baseline parser. 3.1 Arc-standard parsing for dependency DAGs Sagae and Tsujii (2008) describe two algorithms for dependency parsing with words that have multiple heads. Each corresponds to extensions of Nivre (2004)’s arc-standard and arc-eager algorithms </context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Yoav Freund and Robert E. Schapire. 1999. Large Margin Classification Using the Perceptron Algorithm. Machine Learning, 37(3):277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic Labeling of Semantic Roles.</title>
<date>2002</date>
<journal>Computational Linguistics</journal>
<volume>28</volume>
<issue>3</issue>
<pages>245--288</pages>
<contexts>
<context position="6591" citStr="Gildea and Jurafsky, 2002" startWordPosition="1001" endWordPosition="1005">imilarity between words, and demonstrated its utility in improving the performance of a syntax-based Semantic Role Labeling system. The central idea behind their approach was that parse tree paths could be used as features for describing a word’s grammatical behavior. Figure 1: An example parse tree path from the verb ate to the argument NP He, represented as TVBDTVPTSINP. Parse tree paths are descriptions of tree transitions from a terminal (e.g. a verb) to a different node in a constituent parse tree of a sentence. Parse tree paths gained popularity in early Semantic Role Labeling research (Gildea and Jurafsky, 2002), where they were used as features describing the relationship between a verb and a particular semantic role label. For example, Figure 1 illustrates a parse tree path between a verb and a semantically related noun phrase. Gordon and Swanson viewed parse tree paths as features that could be used to describe the syntactic contexts of words in a corpus. In their approach, all of the possible parse tree paths that begin at a given word were identified in a large set of automatically generated constituent parse trees. The normalized frequency counts of unique parse tree paths were combined into a </context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic Labeling of Semantic Roles. Computational Linguistics 28(3): 245-288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Gordon</author>
<author>Reid Swanson</author>
</authors>
<title>Generalizing semantic role annotations across syntactically similar verbs.</title>
<date>2007</date>
<booktitle>Proceedings of the 2007 meeting of the Association for Computational Linguistics (ACL-07),</booktitle>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="5886" citStr="Gordon and Swanson (2007)" startWordPosition="885" endWordPosition="888">uted as the cosine distance between vector representations of the frequency of unique parse tree paths emanating from the word in a corpus of parse trees. In this research, we employ a novel encoding of syntactic parse tree paths that includes direction information and non-terminal node labels, but does not include lexical information or part-of-speech tags. Consequently, the resulting hierarchy groups words that appear in similar places in similar parse trees, regardless of its assigned part-of-speech tag. In this section we describe our approach in detail. 2.1 Parse tree path representation Gordon and Swanson (2007) first described a corpus-based method for calculating a measure of syntactic similarity between words, and demonstrated its utility in improving the performance of a syntax-based Semantic Role Labeling system. The central idea behind their approach was that parse tree paths could be used as features for describing a word’s grammatical behavior. Figure 1: An example parse tree path from the verb ate to the argument NP He, represented as TVBDTVPTSINP. Parse tree paths are descriptions of tree transitions from a terminal (e.g. a verb) to a different node in a constituent parse tree of a sentence</context>
<context position="9185" citStr="Gordon and Swanson (2007)" startWordPosition="1453" endWordPosition="1456">ntifiers (T and ↓) to a set of six tags that included information about the direction of the transition. Up-Right () and Down-Left () transition are used to and from nodes that are the first constituent of a non-terminal. Up-Left () and DownRight () transitions are used to and from nodes that are the last constituent of a non-terminal. Transitions to and from all other constituent nodes are labeled Up-Middle (T) or DownMiddle (↓), accordingly. For example, we represent the parse tree path depicted in Figure 1 as: VPSNP. 2.2 Profiles for BLLIP WSJ Corpus words As in the previous work of Gordon and Swanson (2007), we characterize the syntactic properties of words as the normalized frequency of unique parse tree paths emanating from the word in a large corpus of syntactic parse trees. In our research, we used the Brown Laboratory for Linguistic Information Processing (BLLIP) 1987-89 WSJ Corpus Release 1 (Charniak et al., 2000), which contains approximately 30 million words of Wall Street Journal news articles, parsed with Charniak (2000) parser. Although the trees in the BLLIP corpus are enriched with function tags and empty nodes, we remove this information, leaving only the trees produced by the Char</context>
</contexts>
<marker>Gordon, Swanson, 2007</marker>
<rawString>Andrew Gordon and Reid Swanson. 2007. Generalizing semantic role annotations across syntactically similar verbs. Proceedings of the 2007 meeting of the Association for Computational Linguistics (ACL-07), Prague, Czech Republic, June 23-30, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
<author>Joakim Nivre</author>
<author>Gulsen Eryigit</author>
<author>Beata Megyesi</author>
<author>Mattias Nilsson</author>
<author>Markus Saers</author>
</authors>
<title>Single malt or blended? A study in multilingual parser optimization.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="1709" citStr="Hall et al., 2007" startWordPosition="247" endWordPosition="250"> 2007) coupled with the availability of large treebanks. Several recent efforts have started to look for ways to go beyond what individual annotated data sets and individual parser models can offer, looking to combine diverse parsing models, develop cross-framework interoperability and evaluation, and leverage the availability of large amounts of text available. Two research directions that have produced promising improvements on the accuracy of data-driven parsing are: (1) combining different parsers using ensemble techniques, such as voting (Henderson and Brill, 1999; Sagae and Lavie, 2006; Hall et al., 2007) and stacking (Nivre and McDonald, 2008; Martins et al., 2008), and (2) semi-supervised learning, where unlabeled data (plain text) is used in addition to a treebank (McClosky et al., 2006; Koo et al., 2008). In this paper we explore a new way to obtain improved parsing accuracy by using a large amount of unlabeled text and two parsers that use different ways of representing syntactic structure. In contrast to previous work where automatically generated constituent trees were used directly to train a constituent parsing model (McClosky et al., 2006), or where word clusters were derived from a </context>
<context position="16172" citStr="Hall et al., 2007" startWordPosition="2577" endWordPosition="2580">chical clustering of the top five thousand tokens in the BLLIP corpus, cut at 60 clusters. 195 Figure 3: Predicate-argument dependency structure following the HPSG Treebank standard. processing tasks with discriminative training. We add the syntactic clusters as features in a transition-based parser that uses a classifier to decide among shift/reduce parser actions based on the local context of the decision. This transition-based parsing approach has been found to be efficient and accurate in dependency parsing of surface syntactic dependencies (Yamada and Matsumoto, 2003; Nivre et al., 2004; Hall et al., 2007) and predicate-argument parsing (Henderson et al., 2008; Sagae and Tsujii, 2008). Our experiments are based on an implementation of Sagae and Tsujii (2008)’s algorithm for basic shift-reduce parsing with multiple heads, which we use to identify predicate-argument dependencies extracted from the HPSG Treebank developed by Miyao et al. (2004). Using this data set allows for a comparison of our results with those obtained in previous work on datadriven HPSG predicate-argument analysis, while demonstrating the use of our clustering approach for cross-framework parser improvement, since the cluster</context>
</contexts>
<marker>Hall, Nilsson, Nivre, Eryigit, Megyesi, Nilsson, Saers, 2007</marker>
<rawString>Johan Hall, Jens Nilsson, Joakim Nivre, Gulsen Eryigit, Beata Megyesi, Mattias Nilsson, and Markus Saers. 2007. Single malt or blended? A study in multilingual parser optimization. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
<author>Paola Merlo</author>
<author>G Musillo</author>
<author>Ivan Titov</author>
</authors>
<title>A latent variable model of synchronous parsing for syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In Proceedings of the Shared Task of the Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>178--182</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="2841" citStr="Henderson et al., 2008" startWordPosition="428" endWordPosition="431">stituent parsing model (McClosky et al., 2006), or where word clusters were derived from a large corpus of plain text to improve a dependency parser (Koo et al., 2008), we use a large corpus of constituent trees (previously generated by an accurate constituent parser), which we use to produce syntactically derived clusters that are then used to improve a transition-based parser that outputs dependency graphs that reflect predicate-argument structure where words may be dependents of more than one parent. This type of representation is more general than dependency trees (Sagae and Tsujii, 2008; Henderson et al., 2008), and is suitable for representing both surface relations and long-distance dependencies (such as control, it-cleft and tough movement). The first contribution of this work is a novel approach for deriving syntactic word clusters from parsed text, grouping words by the general syntactic contexts where they appear, and not by n-gram word context (Brown et al., 1992) or by immediate dependency context (Lin, 1998). Unlike in clustering approaches that rely on lexical context (either linear or grammatical) to group words, resulting in a notion of word similarity that blurs syntactic and semantic c</context>
<context position="16227" citStr="Henderson et al., 2008" startWordPosition="2584" endWordPosition="2588">n the BLLIP corpus, cut at 60 clusters. 195 Figure 3: Predicate-argument dependency structure following the HPSG Treebank standard. processing tasks with discriminative training. We add the syntactic clusters as features in a transition-based parser that uses a classifier to decide among shift/reduce parser actions based on the local context of the decision. This transition-based parsing approach has been found to be efficient and accurate in dependency parsing of surface syntactic dependencies (Yamada and Matsumoto, 2003; Nivre et al., 2004; Hall et al., 2007) and predicate-argument parsing (Henderson et al., 2008; Sagae and Tsujii, 2008). Our experiments are based on an implementation of Sagae and Tsujii (2008)’s algorithm for basic shift-reduce parsing with multiple heads, which we use to identify predicate-argument dependencies extracted from the HPSG Treebank developed by Miyao et al. (2004). Using this data set allows for a comparison of our results with those obtained in previous work on datadriven HPSG predicate-argument analysis, while demonstrating the use of our clustering approach for cross-framework parser improvement, since the clusters were derived from syntactic trees in Penn Treebank fo</context>
</contexts>
<marker>Henderson, Merlo, Musillo, Titov, 2008</marker>
<rawString>James Henderson, Paola Merlo, G. Musillo, and Ivan Titov. 2008. A latent variable model of synchronous parsing for syntactic and semantic dependencies. In Proceedings of the Shared Task of the Conference on Computational Natural Language Learning (CoNLL), pages 178-182. Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Henderson</author>
<author>Eric Brill</author>
</authors>
<title>Exploiting diversity in natural language processing: combining parsers.</title>
<date>1999</date>
<booktitle>In Proceedings of the Fourth Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="1666" citStr="Henderson and Brill, 1999" startWordPosition="239" endWordPosition="242">Tsujii, 2005; McDonald et al., 2005; Nivre et al., 2007) coupled with the availability of large treebanks. Several recent efforts have started to look for ways to go beyond what individual annotated data sets and individual parser models can offer, looking to combine diverse parsing models, develop cross-framework interoperability and evaluation, and leverage the availability of large amounts of text available. Two research directions that have produced promising improvements on the accuracy of data-driven parsing are: (1) combining different parsers using ensemble techniques, such as voting (Henderson and Brill, 1999; Sagae and Lavie, 2006; Hall et al., 2007) and stacking (Nivre and McDonald, 2008; Martins et al., 2008), and (2) semi-supervised learning, where unlabeled data (plain text) is used in addition to a treebank (McClosky et al., 2006; Koo et al., 2008). In this paper we explore a new way to obtain improved parsing accuracy by using a large amount of unlabeled text and two parsers that use different ways of representing syntactic structure. In contrast to previous work where automatically generated constituent trees were used directly to train a constituent parsing model (McClosky et al., 2006), </context>
<context position="33560" citStr="Henderson and Brill, 1999" startWordPosition="5479" endWordPosition="5482"> large corpus of parsed text to obtain improved parsing results through self-training. A key difference in our general framework is that it allows for a parser with one type of syntactic representation to improve the accuracy of a different parser with a different type of formalism. In this regard, our work is related to that of Sagae et al. (2007), who used a stacking-like framework to allow a surface dependency parser to improve an HPSG parser. In that work, however, as in other work that combines different parsers through stacking (Martins et al., 2008; Nivre and McDonald, 2008) or voting (Henderson and Brill, 1999), multiple parsers need to process new text at runtime. In our approach for leveraging diverse parsers, one of the parsers is used only to create a parsed corpus from which we extract clusters of words that have similar syntactic behaviors, and only one parser is needed at run-time. 6 Conclusion We have presented a novel approach for deriving word clusters based on syntactic similarity, and shown how these word clusters can be applied in a transition-based dependency parser. Our experiments focused on predicateargument structures extracted from the HPSG Treebank, which demonstrates that the sy</context>
</contexts>
<marker>Henderson, Brill, 1999</marker>
<rawString>John Henderson and Eric Brill. 1999. Exploiting diversity in natural language processing: combining parsers. In Proceedings of the Fourth Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-08:HLT),</booktitle>
<pages>595--603</pages>
<contexts>
<context position="1916" citStr="Koo et al., 2008" startWordPosition="281" endWordPosition="284">o combine diverse parsing models, develop cross-framework interoperability and evaluation, and leverage the availability of large amounts of text available. Two research directions that have produced promising improvements on the accuracy of data-driven parsing are: (1) combining different parsers using ensemble techniques, such as voting (Henderson and Brill, 1999; Sagae and Lavie, 2006; Hall et al., 2007) and stacking (Nivre and McDonald, 2008; Martins et al., 2008), and (2) semi-supervised learning, where unlabeled data (plain text) is used in addition to a treebank (McClosky et al., 2006; Koo et al., 2008). In this paper we explore a new way to obtain improved parsing accuracy by using a large amount of unlabeled text and two parsers that use different ways of representing syntactic structure. In contrast to previous work where automatically generated constituent trees were used directly to train a constituent parsing model (McClosky et al., 2006), or where word clusters were derived from a large corpus of plain text to improve a dependency parser (Koo et al., 2008), we use a large corpus of constituent trees (previously generated by an accurate constituent parser), which we use to produce synt</context>
<context position="15422" citStr="Koo et al. (2008)" startWordPosition="2463" endWordPosition="2466">shows a dendrogram representation of the cluster tree when it is sliced to produce exactly 60 clusters, 19 of which are individual types. For the other 41 clusters, we show only the most frequent word in the cluster and the number of additional words in the cluster. The scale line in the lower left of Figure 2 indicates the horizontal length of a calculated similarity between clusters of 0.1. 3 Transition-based dependency parsing with word clusters The clusters obtained with the approach described in section 2 provide sets of syntactic tags with varying levels of granularity. Previous work by Koo et al. (2008) and Miller et al. (2004) suggests that different levels of cluster granularity may be useful in natural language Figure 2: A hierarchical clustering of the top five thousand tokens in the BLLIP corpus, cut at 60 clusters. 195 Figure 3: Predicate-argument dependency structure following the HPSG Treebank standard. processing tasks with discriminative training. We add the syntactic clusters as features in a transition-based parser that uses a classifier to decide among shift/reduce parser actions based on the local context of the decision. This transition-based parsing approach has been found to</context>
<context position="24733" citStr="Koo et al. (2008)" startWordPosition="4017" endWordPosition="4020">rate additional features that help the parser make its decisions base on the syntactic profile of words. As explained in section 2.4, each there may be several cluster labels (corresponding to clusters of different granularities) associated with each word. To select the set of cluster labels to be used to generate features, we first select a desired granularity for the clusters, and use the set of labels resulting from slicing the cluster tree at the appropriate level, as discussed in section 2.4. We experimented with several levels of cluster granularity using development data, and following Koo et al. (2008), we also experimented with using two sets of cluster labels with different levels of granularity at the same time. Given a specific level of granularity, the cluster-based features we used are: 14. the cluster labels for the words in items S(1), S(2), S(3), I(1), I(2), I(3); 15. the cluster labels for the words in the rightmost and leftmost dependents of S(1) and S(2); 16. the concatenation of the cluster labels for the words in S(1), S(2) and I(1), and the features derived from feature templates 1-15. In experiments where we used two sets of cluster labels corresponding to different levels o</context>
<context position="29130" citStr="Koo et al. (2008)" startWordPosition="4724" endWordPosition="4727"> cluster information is added) and 600 cluster labels is statistically significant (p &lt; 0.0005). distinctions among members of different clusters more fine-grained) in steps of 100, we observed improvements in precision and recall until the point where there were 600 distinct cluster labels. This set of 600 cluster labels produced the highest values of precision and recall (89.5 and 89.0) that we obtained for the development set using only one set of cluster labels. Figure 4 shows how precision, recall and F-score on the development set varied with the number of cluster labels used. Following Koo et al. (2008), we also experimented with using two sets of cluster labels with different levels of granularity. We found that using the set of 600 labels and an additional set with fewer than 600 labels did not improve or hurt precision and recall. Finer grained clusters with more than 1,000 labels (combined with the set of 600 labels) improved results further. The highest precision and recall figures of 90.1 and 89.6 were obtained with the sets of 600 and 1,400 labels. We parsed the test set using the best configuration of cluster-based features as determined using the development set (the sets with 600 a</context>
<context position="31782" citStr="Koo et al. (2008)" startWordPosition="5178" endWordPosition="5181"> rules to train parsing models that produce dependency trees. However, no tuning of the features or metaparameters was performed; the parser was trained as-is on dependency trees. We used the standard train, development and test sets splits to train two models, as in our experiments with predicate-argument dependencies: a baseline that uses no cluster information, and a model that uses two sets of clusters that were found to improve results in the development set. The unlabeled accuracy of our baseline model on the test set is 89.96%, which is considerably lower than the best current results. Koo et al. (2008) report 90.84% for a first-order edgefactored model, and 92.02% for a second-order model (and as high as 93.16% with a secondorder model enriched with cluster features derived from plain text). Using two sets of clusters, one with 600 and one with 1,200 labels, accuracy improves by 1.32%, to reach 91.28% (a 13.15% reduction in error compared to our baseline). While still below the level of the strongest results for this dataset, it is interesting to see that 199 the improvement in accuracy over the baseline observed for surface dependency trees is similar to the improvement observed for predic</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-08:HLT), pages 595-603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th international Conference on Computational Linguistics - Volume 2.</booktitle>
<location>Montreal, Quebec, Canada.</location>
<contexts>
<context position="3255" citStr="Lin, 1998" startWordPosition="494" endWordPosition="495">edicate-argument structure where words may be dependents of more than one parent. This type of representation is more general than dependency trees (Sagae and Tsujii, 2008; Henderson et al., 2008), and is suitable for representing both surface relations and long-distance dependencies (such as control, it-cleft and tough movement). The first contribution of this work is a novel approach for deriving syntactic word clusters from parsed text, grouping words by the general syntactic contexts where they appear, and not by n-gram word context (Brown et al., 1992) or by immediate dependency context (Lin, 1998). Unlike in clustering approaches that rely on lexical context (either linear or grammatical) to group words, resulting in a notion of word similarity that blurs syntactic and semantic characteristics of lexical items, we use unlexicalized syntactic context, so that words are clustered based only on their syntactic behavior. This way, we attempt to generate clusters that are more conceptually similar to part-of-speech tags or supertags 192 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 192–201, Paris, October 2009. c�2009 Association for Computational Li</context>
<context position="13096" citStr="Lin, 1998" startWordPosition="2077" endWordPosition="2078">ntactic features of this type can therefore be characterized by 114 cluster labels, which overlap with varying degrees with other tokens in the set. We view these cluster labels as conceptually similar to traditional part-of-speech tags in that they are indicative of the syntactic contexts in which words are likely to appear. Because words are clustered based on their unlexicalized syntactic contexts, the resulting clusters are more likely to reflect purely syntactic information than are clusters derived from lexical context, such as adjacent words (Brown et al., 1992) or immediate head-word (Lin, 1998). However, the extent 194 to which these syntactic contexts are specified can vary from a more general to a more finegrained level than that of parts-of-speech. As clusters become more fine-grained, they become more similar to supertags (Bangalore and Joshi, 1999). Clusters that represent more specific syntactic contexts can encode information about, for example, subcategorization. As these labels are derived empirically from a large corpus of syntactic parse trees, they accurately represent syntactic distinctions in real discourse at different granularities, in contrast to the single arbitrar</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th international Conference on Computational Linguistics - Volume 2. Montreal, Quebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary A Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank,</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="13802" citStr="Marcus et al., 1993" startWordPosition="2180" endWordPosition="2183">m a more general to a more finegrained level than that of parts-of-speech. As clusters become more fine-grained, they become more similar to supertags (Bangalore and Joshi, 1999). Clusters that represent more specific syntactic contexts can encode information about, for example, subcategorization. As these labels are derived empirically from a large corpus of syntactic parse trees, they accurately represent syntactic distinctions in real discourse at different granularities, in contrast to the single arbitrary granularity of theoretically derived part-ofspeech tags used in existing treebanks (Marcus et al., 1993). While it is sometimes useful to view types as having multiple part-of-speech tags at different levels of granularity (e.g. the 114 tags for the token “house”), it is often useful to select a single level of granularity to use across all tokens. For example, it is useful to know which one of the 114 cluster labels for “house” to use if exactly 100 part-of-speech distinctions are to be made among tokens in the set. These cluster labels can be identified by slicing the tree at the level for which there are exactly 100 branches, then using the label of the first branch point in each branch as th</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary A. Marcinkiewicz, Beatrice Santorini. 1993. Building a large annotated corpus of English: The Penn Treebank, Computational Linguistics, 19(2), June 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>André F T Martins</author>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>Stacking Dependency Parsers.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Waikiki, HI.</location>
<contexts>
<context position="1771" citStr="Martins et al., 2008" startWordPosition="257" endWordPosition="260">veral recent efforts have started to look for ways to go beyond what individual annotated data sets and individual parser models can offer, looking to combine diverse parsing models, develop cross-framework interoperability and evaluation, and leverage the availability of large amounts of text available. Two research directions that have produced promising improvements on the accuracy of data-driven parsing are: (1) combining different parsers using ensemble techniques, such as voting (Henderson and Brill, 1999; Sagae and Lavie, 2006; Hall et al., 2007) and stacking (Nivre and McDonald, 2008; Martins et al., 2008), and (2) semi-supervised learning, where unlabeled data (plain text) is used in addition to a treebank (McClosky et al., 2006; Koo et al., 2008). In this paper we explore a new way to obtain improved parsing accuracy by using a large amount of unlabeled text and two parsers that use different ways of representing syntactic structure. In contrast to previous work where automatically generated constituent trees were used directly to train a constituent parsing model (McClosky et al., 2006), or where word clusters were derived from a large corpus of plain text to improve a dependency parser (Koo</context>
<context position="33495" citStr="Martins et al., 2008" startWordPosition="5469" endWordPosition="5472">f clusters may be beneficial. McClosky et al. (2006) used a large corpus of parsed text to obtain improved parsing results through self-training. A key difference in our general framework is that it allows for a parser with one type of syntactic representation to improve the accuracy of a different parser with a different type of formalism. In this regard, our work is related to that of Sagae et al. (2007), who used a stacking-like framework to allow a surface dependency parser to improve an HPSG parser. In that work, however, as in other work that combines different parsers through stacking (Martins et al., 2008; Nivre and McDonald, 2008) or voting (Henderson and Brill, 1999), multiple parsers need to process new text at runtime. In our approach for leveraging diverse parsers, one of the parsers is used only to create a parsed corpus from which we extract clusters of words that have similar syntactic behaviors, and only one parser is needed at run-time. 6 Conclusion We have presented a novel approach for deriving word clusters based on syntactic similarity, and shown how these word clusters can be applied in a transition-based dependency parser. Our experiments focused on predicateargument structures</context>
</contexts>
<marker>Martins, Das, Smith, Xing, 2008</marker>
<rawString>André F. T. Martins, Dipanjan Das, Noah A. Smith, and Eric P. Xing. 2008. Stacking Dependency Parsers. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, Waikiki, HI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective Self-Training for Parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>152--159</pages>
<contexts>
<context position="1897" citStr="McClosky et al., 2006" startWordPosition="277" endWordPosition="280">ls can offer, looking to combine diverse parsing models, develop cross-framework interoperability and evaluation, and leverage the availability of large amounts of text available. Two research directions that have produced promising improvements on the accuracy of data-driven parsing are: (1) combining different parsers using ensemble techniques, such as voting (Henderson and Brill, 1999; Sagae and Lavie, 2006; Hall et al., 2007) and stacking (Nivre and McDonald, 2008; Martins et al., 2008), and (2) semi-supervised learning, where unlabeled data (plain text) is used in addition to a treebank (McClosky et al., 2006; Koo et al., 2008). In this paper we explore a new way to obtain improved parsing accuracy by using a large amount of unlabeled text and two parsers that use different ways of representing syntactic structure. In contrast to previous work where automatically generated constituent trees were used directly to train a constituent parsing model (McClosky et al., 2006), or where word clusters were derived from a large corpus of plain text to improve a dependency parser (Koo et al., 2008), we use a large corpus of constituent trees (previously generated by an accurate constituent parser), which we </context>
<context position="32927" citStr="McClosky et al. (2006)" startWordPosition="5371" endWordPosition="5374">or surface dependency trees is similar to the improvement observed for predicateargument dependency graphs. 5 Related work Many aspects of this research were inspired by the recent work of Koo et al. (2008), who reported impressive results on improving dependency parsing accuracy using a second order edge-factored model and word clusters derived from plain text using the Brown et al. (1992) algorithm. Our clustering approach is significantly different, focusing on the use of parsed data to produce strictly syntactic clusters. It is possible that using both types of clusters may be beneficial. McClosky et al. (2006) used a large corpus of parsed text to obtain improved parsing results through self-training. A key difference in our general framework is that it allows for a parser with one type of syntactic representation to improve the accuracy of a different parser with a different type of formalism. In this regard, our work is related to that of Sagae et al. (2007), who used a stacking-like framework to allow a surface dependency parser to improve an HPSG parser. In that work, however, as in other work that combines different parsers through stacking (Martins et al., 2008; Nivre and McDonald, 2008) or v</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective Self-Training for Parsing. In Proceedings of HLT-NAACL, pages 152–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>91--98</pages>
<contexts>
<context position="1076" citStr="McDonald et al., 2005" startWordPosition="152" endWordPosition="155">hese syntactic clusters in leveraging a large corpus of trees generated by a high-accuracy parser to improve the accuracy of another parser based on a different formalism for representing a different level of sentence structure. In our experiments, we use phrase-structure trees to produce syntactic word clusters that are used by a predicate-argument dependency parser, significantly improving its accuracy. 1 Introduction Syntactic parsing of natural language has advanced greatly in recent years, in large part due to data-driven techniques (Collins, 1999; Charniak, 2000; Miyao and Tsujii, 2005; McDonald et al., 2005; Nivre et al., 2007) coupled with the availability of large treebanks. Several recent efforts have started to look for ways to go beyond what individual annotated data sets and individual parser models can offer, looking to combine diverse parsing models, develop cross-framework interoperability and evaluation, and leverage the availability of large amounts of text available. Two research directions that have produced promising improvements on the accuracy of data-driven parsing are: (1) combining different parsers using ensemble techniques, such as voting (Henderson and Brill, 1999; Sagae an</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of ACL, pages 91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Miller</author>
<author>Jethran Guinness</author>
<author>Alex Zamanian</author>
</authors>
<title>Name Tagging withWord Clusters and Discriminative Training.</title>
<date>2004</date>
<booktitle>In Proceedings of HLTNAACL,</booktitle>
<pages>337--342</pages>
<contexts>
<context position="15447" citStr="Miller et al. (2004)" startWordPosition="2468" endWordPosition="2471">resentation of the cluster tree when it is sliced to produce exactly 60 clusters, 19 of which are individual types. For the other 41 clusters, we show only the most frequent word in the cluster and the number of additional words in the cluster. The scale line in the lower left of Figure 2 indicates the horizontal length of a calculated similarity between clusters of 0.1. 3 Transition-based dependency parsing with word clusters The clusters obtained with the approach described in section 2 provide sets of syntactic tags with varying levels of granularity. Previous work by Koo et al. (2008) and Miller et al. (2004) suggests that different levels of cluster granularity may be useful in natural language Figure 2: A hierarchical clustering of the top five thousand tokens in the BLLIP corpus, cut at 60 clusters. 195 Figure 3: Predicate-argument dependency structure following the HPSG Treebank standard. processing tasks with discriminative training. We add the syntactic clusters as features in a transition-based parser that uses a classifier to decide among shift/reduce parser actions based on the local context of the decision. This transition-based parsing approach has been found to be efficient and accurat</context>
</contexts>
<marker>Miller, Guinness, Zamanian, 2004</marker>
<rawString>Scott Miller, Jethran Guinness and Alex Zamanian. 2004. Name Tagging withWord Clusters and Discriminative Training. In Proceedings of HLTNAACL, pages 337–342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Takashi Ninomiya</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Corpus-oriented grammar development for acquiring a Head-driven Phrase Structure Grammar from the Penn Treebank.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Joint Conference on Natural Language Processing (IJCNLP).</booktitle>
<contexts>
<context position="16514" citStr="Miyao et al. (2004)" startWordPosition="2629" endWordPosition="2632">hift/reduce parser actions based on the local context of the decision. This transition-based parsing approach has been found to be efficient and accurate in dependency parsing of surface syntactic dependencies (Yamada and Matsumoto, 2003; Nivre et al., 2004; Hall et al., 2007) and predicate-argument parsing (Henderson et al., 2008; Sagae and Tsujii, 2008). Our experiments are based on an implementation of Sagae and Tsujii (2008)’s algorithm for basic shift-reduce parsing with multiple heads, which we use to identify predicate-argument dependencies extracted from the HPSG Treebank developed by Miyao et al. (2004). Using this data set allows for a comparison of our results with those obtained in previous work on datadriven HPSG predicate-argument analysis, while demonstrating the use of our clustering approach for cross-framework parser improvement, since the clusters were derived from syntactic trees in Penn Treebank format (as produced by the Charniak parser, without empty nodes, co-indexation or function tags), and used in the identification of HPSG Treebank predicate-argument dependencies. Figure 3 shows a predicateargument dependency structure following the annotation standard of the HPSG Treebank</context>
</contexts>
<marker>Miyao, Ninomiya, Tsujii, 2004</marker>
<rawString>Miyao, Yusuke, Takashi Ninomiya, and Jun’ichi Tsujii. 2004. Corpus-oriented grammar development for acquiring a Head-driven Phrase Structure Grammar from the Penn Treebank. In Proceedings of the International Joint Conference on Natural Language Processing (IJCNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miyao Yusuke</author>
<author>Jun&apos;ichi Tsujii</author>
</authors>
<title>Probabilistic disambiguation models for wide-coverage HPSG parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics.</booktitle>
<marker>Yusuke, Tsujii, 2005</marker>
<rawString>Miyao Yusuke and Jun&apos;ichi Tsujii. 2005. Probabilistic disambiguation models for wide-coverage HPSG parsing. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Joakim Nivre 2004</author>
</authors>
<title>Incrementality in Deterministic Dependency Parsing. In Incremental Parsing: Bringing Engineering and Cognition Together (Workshop at ACL-2004).</title>
<contexts>
<context position="15447" citStr="(2004)" startWordPosition="2471" endWordPosition="2471"> the cluster tree when it is sliced to produce exactly 60 clusters, 19 of which are individual types. For the other 41 clusters, we show only the most frequent word in the cluster and the number of additional words in the cluster. The scale line in the lower left of Figure 2 indicates the horizontal length of a calculated similarity between clusters of 0.1. 3 Transition-based dependency parsing with word clusters The clusters obtained with the approach described in section 2 provide sets of syntactic tags with varying levels of granularity. Previous work by Koo et al. (2008) and Miller et al. (2004) suggests that different levels of cluster granularity may be useful in natural language Figure 2: A hierarchical clustering of the top five thousand tokens in the BLLIP corpus, cut at 60 clusters. 195 Figure 3: Predicate-argument dependency structure following the HPSG Treebank standard. processing tasks with discriminative training. We add the syntactic clusters as features in a transition-based parser that uses a classifier to decide among shift/reduce parser actions based on the local context of the decision. This transition-based parsing approach has been found to be efficient and accurat</context>
<context position="18147" citStr="(2004)" startWordPosition="2880" endWordPosition="2880"> averaged perceptron (Freund and Schapire, 1999; Collins, 2002), which allows for the inclusion of all of Sagae and Tsujii’s features, in addition to a set of cluster-based features, while retaining fast training times. We now describe the parsing approach, starting with the dependency DAG parser that we use as a baseline, followed by how the syntactic cluster features were added to the baseline parser. 3.1 Arc-standard parsing for dependency DAGs Sagae and Tsujii (2008) describe two algorithms for dependency parsing with words that have multiple heads. Each corresponds to extensions of Nivre (2004)’s arc-standard and arc-eager algorithms for dependency (tree) parsing. In our experiments, we used an implementation of the arc-standard extension. Nivre’s arc-standard dependency parsing algorithm uses a stack to process the input string one word at a time, from left to right, using two general types of parser action: shift (push the next input token onto the stack), and reduce (create a dependency arc between the top two items on the stack, and pop the item marked as the dependent). Reduce actions are subdivided into reduceright and reduce-left, indicating which of the two items on the top </context>
</contexts>
<marker>2004, </marker>
<rawString>Joakim Nivre.2004. Incrementality in Deterministic Dependency Parsing. In Incremental Parsing: Bringing Engineering and Cognition Together (Workshop at ACL-2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Memory-based dependency parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>49--56</pages>
<contexts>
<context position="16152" citStr="Nivre et al., 2004" startWordPosition="2573" endWordPosition="2576">e Figure 2: A hierarchical clustering of the top five thousand tokens in the BLLIP corpus, cut at 60 clusters. 195 Figure 3: Predicate-argument dependency structure following the HPSG Treebank standard. processing tasks with discriminative training. We add the syntactic clusters as features in a transition-based parser that uses a classifier to decide among shift/reduce parser actions based on the local context of the decision. This transition-based parsing approach has been found to be efficient and accurate in dependency parsing of surface syntactic dependencies (Yamada and Matsumoto, 2003; Nivre et al., 2004; Hall et al., 2007) and predicate-argument parsing (Henderson et al., 2008; Sagae and Tsujii, 2008). Our experiments are based on an implementation of Sagae and Tsujii (2008)’s algorithm for basic shift-reduce parsing with multiple heads, which we use to identify predicate-argument dependencies extracted from the HPSG Treebank developed by Miyao et al. (2004). Using this data set allows for a comparison of our results with those obtained in previous work on datadriven HPSG predicate-argument analysis, while demonstrating the use of our clustering approach for cross-framework parser improvemen</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2004</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nilsson. 2004. Memory-based dependency parsing. In Proceedings of CoNLL, pages 49–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jens Nilsson</author>
</authors>
<title>PseudoProjective Dependency Parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>99--106</pages>
<contexts>
<context position="20560" citStr="Nilsson, 2005" startWordPosition="3293" endWordPosition="3294">w attach actions. This basic algorithm is only capable of producing labeled directed acyclic graphs where, if the nodes (which correspond to words) are placed on a left to right sequence on a horizontal line in the order in which the words appear in the input sentence, all arcs can be drawn above the nodes without crossing. This corresponds to the notion of projectivity that similarly limits the types of trees produced by Nivre’s algorithm. Just as in dependency parsing with tree structures, a way to effectively remove this limitation is the use of pseudo-projective transformations (Nivre and Nilsson, 2005), where arcs that cross have their heads moved towards the root and have their labels edited to reflect this change, often making it reversible. Once crossing arcs have been “lifted” so that no crossing arcs remain, the “projectivized” structures are used to train a parsing model. Projective structures produced by this model can be “deprojectivized” through the use of the edits in the arc labels, in an attempt to produce structures that conform to the scheme in the original data. Sagae and Tsujii also propose a simple arc reversal transform, which simply reverses the direction of a dependency </context>
</contexts>
<marker>Nilsson, 2005</marker>
<rawString>Joakim Nivre. and Jens Nilsson. 2005. PseudoProjective Dependency Parsing. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL), pp. 99-106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Sandra Kubler</author>
<author>Ryan McDonald</author>
<author>Jens Nilsson</author>
<author>Sebastian Riedel</author>
<author>Deniz Yuret</author>
</authors>
<title>shared task on dependency parsing.</title>
<date>2007</date>
<journal>The CoNLL</journal>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<pages>915--932</pages>
<contexts>
<context position="1097" citStr="Nivre et al., 2007" startWordPosition="156" endWordPosition="159"> in leveraging a large corpus of trees generated by a high-accuracy parser to improve the accuracy of another parser based on a different formalism for representing a different level of sentence structure. In our experiments, we use phrase-structure trees to produce syntactic word clusters that are used by a predicate-argument dependency parser, significantly improving its accuracy. 1 Introduction Syntactic parsing of natural language has advanced greatly in recent years, in large part due to data-driven techniques (Collins, 1999; Charniak, 2000; Miyao and Tsujii, 2005; McDonald et al., 2005; Nivre et al., 2007) coupled with the availability of large treebanks. Several recent efforts have started to look for ways to go beyond what individual annotated data sets and individual parser models can offer, looking to combine diverse parsing models, develop cross-framework interoperability and evaluation, and leverage the availability of large amounts of text available. Two research directions that have produced promising improvements on the accuracy of data-driven parsing are: (1) combining different parsers using ensemble techniques, such as voting (Henderson and Brill, 1999; Sagae and Lavie, 2006; Hall e</context>
</contexts>
<marker>Nivre, Hall, Kubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Sandra Kubler, Ryan McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The CoNLL 2007 shared task on dependency parsing. In Proceedings of EMNLP-CoNLL, pages 915-932.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>R McDonald</author>
</authors>
<title>Integrating GraphBased and Transition-Based Dependency Parsers.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-08: HLT),</booktitle>
<pages>950--958</pages>
<contexts>
<context position="1748" citStr="Nivre and McDonald, 2008" startWordPosition="253" endWordPosition="256">ity of large treebanks. Several recent efforts have started to look for ways to go beyond what individual annotated data sets and individual parser models can offer, looking to combine diverse parsing models, develop cross-framework interoperability and evaluation, and leverage the availability of large amounts of text available. Two research directions that have produced promising improvements on the accuracy of data-driven parsing are: (1) combining different parsers using ensemble techniques, such as voting (Henderson and Brill, 1999; Sagae and Lavie, 2006; Hall et al., 2007) and stacking (Nivre and McDonald, 2008; Martins et al., 2008), and (2) semi-supervised learning, where unlabeled data (plain text) is used in addition to a treebank (McClosky et al., 2006; Koo et al., 2008). In this paper we explore a new way to obtain improved parsing accuracy by using a large amount of unlabeled text and two parsers that use different ways of representing syntactic structure. In contrast to previous work where automatically generated constituent trees were used directly to train a constituent parsing model (McClosky et al., 2006), or where word clusters were derived from a large corpus of plain text to improve a</context>
<context position="33522" citStr="Nivre and McDonald, 2008" startWordPosition="5473" endWordPosition="5476">ficial. McClosky et al. (2006) used a large corpus of parsed text to obtain improved parsing results through self-training. A key difference in our general framework is that it allows for a parser with one type of syntactic representation to improve the accuracy of a different parser with a different type of formalism. In this regard, our work is related to that of Sagae et al. (2007), who used a stacking-like framework to allow a surface dependency parser to improve an HPSG parser. In that work, however, as in other work that combines different parsers through stacking (Martins et al., 2008; Nivre and McDonald, 2008) or voting (Henderson and Brill, 1999), multiple parsers need to process new text at runtime. In our approach for leveraging diverse parsers, one of the parsers is used only to create a parsed corpus from which we extract clusters of words that have similar syntactic behaviors, and only one parser is needed at run-time. 6 Conclusion We have presented a novel approach for deriving word clusters based on syntactic similarity, and shown how these word clusters can be applied in a transition-based dependency parser. Our experiments focused on predicateargument structures extracted from the HPSG Tr</context>
</contexts>
<marker>Nivre, McDonald, 2008</marker>
<rawString>Nivre, J. and McDonald, R. (2008) Integrating GraphBased and Transition-Based Dependency Parsers. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-08: HLT), 950-958.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Dan Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The Proposition Bank: A Corpus Annotated with Semantic Roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<pages>31--1</pages>
<contexts>
<context position="17228" citStr="Palmer et al., 2005" startWordPosition="2736" endWordPosition="2739">ork on datadriven HPSG predicate-argument analysis, while demonstrating the use of our clustering approach for cross-framework parser improvement, since the clusters were derived from syntactic trees in Penn Treebank format (as produced by the Charniak parser, without empty nodes, co-indexation or function tags), and used in the identification of HPSG Treebank predicate-argument dependencies. Figure 3 shows a predicateargument dependency structure following the annotation standard of the HPSG Treebank, where arrows point from head to modifier. We note that unlike in the widely known PropBank (Palmer et al., 2005) predicate-argument structures, argument labels start from ARG1 (not ARG0), and predicate-argument relationships are annotated for all words. One difference between in our implementation is that, instead of maximum entropy classification used by Sagae and Tsujii, we perform parser action classification using the averaged perceptron (Freund and Schapire, 1999; Collins, 2002), which allows for the inclusion of all of Sagae and Tsujii’s features, in addition to a set of cluster-based features, while retaining fast training times. We now describe the parsing approach, starting with the dependency </context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Dan Gildea and Paul Kingsbury. 2005. The Proposition Bank: A Corpus Annotated with Semantic Roles. Computational Linguistics, 31:1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Alon Lavie</author>
</authors>
<title>Parser combination by reparsing.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL: Short Papers,</booktitle>
<pages>129--132</pages>
<contexts>
<context position="1689" citStr="Sagae and Lavie, 2006" startWordPosition="243" endWordPosition="246">l., 2005; Nivre et al., 2007) coupled with the availability of large treebanks. Several recent efforts have started to look for ways to go beyond what individual annotated data sets and individual parser models can offer, looking to combine diverse parsing models, develop cross-framework interoperability and evaluation, and leverage the availability of large amounts of text available. Two research directions that have produced promising improvements on the accuracy of data-driven parsing are: (1) combining different parsers using ensemble techniques, such as voting (Henderson and Brill, 1999; Sagae and Lavie, 2006; Hall et al., 2007) and stacking (Nivre and McDonald, 2008; Martins et al., 2008), and (2) semi-supervised learning, where unlabeled data (plain text) is used in addition to a treebank (McClosky et al., 2006; Koo et al., 2008). In this paper we explore a new way to obtain improved parsing accuracy by using a large amount of unlabeled text and two parsers that use different ways of representing syntactic structure. In contrast to previous work where automatically generated constituent trees were used directly to train a constituent parsing model (McClosky et al., 2006), or where word clusters </context>
</contexts>
<marker>Sagae, Lavie, 2006</marker>
<rawString>Kenji Sagae and Alon Lavie. 2006. Parser combination by reparsing. In Proceedings of NAACL: Short Papers, pages 129–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
</authors>
<title>Yusuke Miyao Jun’ichi and Tsujii.</title>
<date>2007</date>
<booktitle>In Proceedings of the 44th Meeting of the Association for Computational Linguistics.</booktitle>
<marker>Sagae, 2007</marker>
<rawString>Kenji Sagae, Yusuke Miyao Jun’ichi and Tsujii. 2007. HPSG Parsing with shallow dependency constraints. In Proceedings of the 44th Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Shift-reduce dependency DAG parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics (COLING</booktitle>
<contexts>
<context position="2816" citStr="Sagae and Tsujii, 2008" startWordPosition="424" endWordPosition="427"> directly to train a constituent parsing model (McClosky et al., 2006), or where word clusters were derived from a large corpus of plain text to improve a dependency parser (Koo et al., 2008), we use a large corpus of constituent trees (previously generated by an accurate constituent parser), which we use to produce syntactically derived clusters that are then used to improve a transition-based parser that outputs dependency graphs that reflect predicate-argument structure where words may be dependents of more than one parent. This type of representation is more general than dependency trees (Sagae and Tsujii, 2008; Henderson et al., 2008), and is suitable for representing both surface relations and long-distance dependencies (such as control, it-cleft and tough movement). The first contribution of this work is a novel approach for deriving syntactic word clusters from parsed text, grouping words by the general syntactic contexts where they appear, and not by n-gram word context (Brown et al., 1992) or by immediate dependency context (Lin, 1998). Unlike in clustering approaches that rely on lexical context (either linear or grammatical) to group words, resulting in a notion of word similarity that blurs</context>
<context position="16252" citStr="Sagae and Tsujii, 2008" startWordPosition="2589" endWordPosition="2592">at 60 clusters. 195 Figure 3: Predicate-argument dependency structure following the HPSG Treebank standard. processing tasks with discriminative training. We add the syntactic clusters as features in a transition-based parser that uses a classifier to decide among shift/reduce parser actions based on the local context of the decision. This transition-based parsing approach has been found to be efficient and accurate in dependency parsing of surface syntactic dependencies (Yamada and Matsumoto, 2003; Nivre et al., 2004; Hall et al., 2007) and predicate-argument parsing (Henderson et al., 2008; Sagae and Tsujii, 2008). Our experiments are based on an implementation of Sagae and Tsujii (2008)’s algorithm for basic shift-reduce parsing with multiple heads, which we use to identify predicate-argument dependencies extracted from the HPSG Treebank developed by Miyao et al. (2004). Using this data set allows for a comparison of our results with those obtained in previous work on datadriven HPSG predicate-argument analysis, while demonstrating the use of our clustering approach for cross-framework parser improvement, since the clusters were derived from syntactic trees in Penn Treebank format (as produced by the </context>
<context position="18016" citStr="Sagae and Tsujii (2008)" startWordPosition="2858" endWordPosition="2861">in our implementation is that, instead of maximum entropy classification used by Sagae and Tsujii, we perform parser action classification using the averaged perceptron (Freund and Schapire, 1999; Collins, 2002), which allows for the inclusion of all of Sagae and Tsujii’s features, in addition to a set of cluster-based features, while retaining fast training times. We now describe the parsing approach, starting with the dependency DAG parser that we use as a baseline, followed by how the syntactic cluster features were added to the baseline parser. 3.1 Arc-standard parsing for dependency DAGs Sagae and Tsujii (2008) describe two algorithms for dependency parsing with words that have multiple heads. Each corresponds to extensions of Nivre (2004)’s arc-standard and arc-eager algorithms for dependency (tree) parsing. In our experiments, we used an implementation of the arc-standard extension. Nivre’s arc-standard dependency parsing algorithm uses a stack to process the input string one word at a time, from left to right, using two general types of parser action: shift (push the next input token onto the stack), and reduce (create a dependency arc between the top two items on the stack, and pop the item mark</context>
<context position="19847" citStr="Sagae and Tsujii (2008)" startWordPosition="3174" endWordPosition="3177">tach-left which creates a dependency arc between the top two items on the stack such that the item on top is the head, and 196 right-attach, which creates a dependency arc between the top two items on the stack such that the top item is the dependent, then pops it from the stack and unshifts it back into the input. Finally, this algorithm for unlabeled graphs can be extended to produce labeled dependencies in the same way as Nivre’s algorithm, by replacing the reduce and attach actions with sets of actions that perform the reduce or attach operation and also name the label of the arc created. Sagae and Tsujii (2008) provide a more detailed description of the algorithm, including an example that illustrates the new attach actions. This basic algorithm is only capable of producing labeled directed acyclic graphs where, if the nodes (which correspond to words) are placed on a left to right sequence on a horizontal line in the order in which the words appear in the input sentence, all arcs can be drawn above the nodes without crossing. This corresponds to the notion of projectivity that similarly limits the types of trees produced by Nivre’s algorithm. Just as in dependency parsing with tree structures, a wa</context>
<context position="29987" citStr="Sagae and Tsujii (2008)" startWordPosition="4870" endWordPosition="4873">. Finer grained clusters with more than 1,000 labels (combined with the set of 600 labels) improved results further. The highest precision and recall figures of 90.1 and 89.6 were obtained with the sets of 600 and 1,400 labels. We parsed the test set using the best configuration of cluster-based features as determined using the development set (the sets with 600 and 1,400 cluster labels) and obtained 90.2 precision, 89.8 recall and 90.0 f-score, a 13.8% reduction in error over a strong baseline. Table 1 summarizes our results on the test set. For comparison, we also shows results published by Sagae and Tsujii (2008), to our knowledge the highest f-score reported for this test set, and Miyao and Tsujii (2005), who first reported results on this data set. Precision Recall F-score Baseline 88.7 88.2 88.4 Clusters 90.2 89.8 90.0 S &amp; T 89.9 88.5 88.7 Miyao et al. 85.0 84.3 84.6 Table 1: Results obtained on the test set using our baseline model and our best clusterbased features. The results in the bottom two rows are from Sagae and Tsujii (2008) and Miyao and Tsujii (2005). 4.1 Surface dependency parsing with cluster-based features The parser used in our experiments with HPSG Treebank predicate-argument struc</context>
</contexts>
<marker>Sagae, Tsujii, 2008</marker>
<rawString>Kenji Sagae and Jun’ichi Tsujii. 2008. Shift-reduce dependency DAG parsing. In Proceedings of the International Conference on Computational Linguistics (COLING 2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Y Matsumoto</author>
</authors>
<title>Statistical Dependency Analysis With Support Vector Machines.</title>
<date>2003</date>
<booktitle>In Proceedings of the Eighth International Workshop on Parsing Technologies (IWPT),</booktitle>
<pages>195--206</pages>
<contexts>
<context position="16132" citStr="Yamada and Matsumoto, 2003" startWordPosition="2569" endWordPosition="2572">be useful in natural language Figure 2: A hierarchical clustering of the top five thousand tokens in the BLLIP corpus, cut at 60 clusters. 195 Figure 3: Predicate-argument dependency structure following the HPSG Treebank standard. processing tasks with discriminative training. We add the syntactic clusters as features in a transition-based parser that uses a classifier to decide among shift/reduce parser actions based on the local context of the decision. This transition-based parsing approach has been found to be efficient and accurate in dependency parsing of surface syntactic dependencies (Yamada and Matsumoto, 2003; Nivre et al., 2004; Hall et al., 2007) and predicate-argument parsing (Henderson et al., 2008; Sagae and Tsujii, 2008). Our experiments are based on an implementation of Sagae and Tsujii (2008)’s algorithm for basic shift-reduce parsing with multiple heads, which we use to identify predicate-argument dependencies extracted from the HPSG Treebank developed by Miyao et al. (2004). Using this data set allows for a comparison of our results with those obtained in previous work on datadriven HPSG predicate-argument analysis, while demonstrating the use of our clustering approach for cross-framewo</context>
<context position="31120" citStr="Yamada and Matsumoto (2003)" startWordPosition="5064" endWordPosition="5068">-based features The parser used in our experiments with HPSG Treebank predicate-argument structures can assign more than one head for a single word, but when the parser is trained using only dependency trees, it behaves in exactly the same way as a parser based on Nivre’s arc-standard algorithm, since it never sees examples of attach actions during training. To see whether our clusters can improve surface dependency parsing, and to allow for comparison of our results to a larger body of research on surface dependency parsing, we used dependency trees extracted from the Penn Treebank using the Yamada and Matsumoto (2003) version of the Penn Treebank headpercolation rules to train parsing models that produce dependency trees. However, no tuning of the features or metaparameters was performed; the parser was trained as-is on dependency trees. We used the standard train, development and test sets splits to train two models, as in our experiments with predicate-argument dependencies: a baseline that uses no cluster information, and a model that uses two sets of clusters that were found to improve results in the development set. The unlabeled accuracy of our baseline model on the test set is 89.96%, which is consi</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Hiroyasu Yamada and Y. Matsumoto. 2003. Statistical Dependency Analysis With Support Vector Machines. In Proceedings of the Eighth International Workshop on Parsing Technologies (IWPT), pages 195–206.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>