<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000122">
<title confidence="0.9957425">
Shallow Analysis Based Assessment of Syntactic Complexity for
Automated Speech Scoring
</title>
<author confidence="0.967482">
Suma Bhat
</author>
<affiliation confidence="0.95561">
Beckman Institute,
University of Illinois,
</affiliation>
<address confidence="0.658539">
Urbana, IL
</address>
<email confidence="0.997998">
spbhat2@illinois.edu
</email>
<author confidence="0.992187">
Huichao Xue
</author>
<affiliation confidence="0.9986795">
Dept. of Computer Science
University of Pittsburgh
</affiliation>
<address confidence="0.566604">
Pittsburgh, PA
</address>
<email confidence="0.996582">
hux10@cs.pitt.edu
</email>
<author confidence="0.679175">
Su-Youn Yoon
</author>
<affiliation confidence="0.586256">
Educational Testing Service
</affiliation>
<address confidence="0.72216">
Princeton, NJ
</address>
<email confidence="0.974611">
syoon@ets.org
</email>
<sectionHeader confidence="0.993377" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999759823529412">
Designing measures that capture various
aspects of language ability is a central
task in the design of systems for auto-
matic scoring of spontaneous speech. In
this study, we address a key aspect of lan-
guage proficiency assessment – syntactic
complexity. We propose a novel measure
of syntactic complexity for spontaneous
speech that shows optimum empirical per-
formance on real world data in multiple
ways. First, it is both robust and reliable,
producing automatic scores that agree well
with human rating compared to the state-
of-the-art. Second, the measure makes
sense theoretically, both from algorithmic
and native language acquisition points of
view.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999663645833333">
Assessment of a speaker’s proficiency in a second
language is the main task in the domain of au-
tomatic evaluation of spontaneous speech (Zech-
ner et al., 2009). Prior studies in language ac-
quisition and second language research have con-
clusively shown that proficiency in a second lan-
guage is characterized by several factors, some of
which are, fluency in language production, pro-
nunciation accuracy, choice of vocabulary, gram-
matical sophistication and accuracy. The design of
automated scoring systems for non-native speaker
speaking proficiency is guided by these studies in
the choice of pertinent objective measures of these
key aspects of language proficiency.
The focus of this study is the design and per-
formance analysis of a measure of the syntactic
complexity of non-native English responses for
use in automatic scoring systems. The state-of-
the art automated scoring system for spontaneous
speech (Zechner et al., 2009; Higgins et al., 2011)
currently uses measures of fluency and pronuncia-
tion (acoustic aspects) to produce scores that are in
reasonable agreement with human-rated scores of
proficiency. Despite its good performance, there
is a need to extend its coverage to higher order as-
pects of language ability. Fluency and pronunci-
ation may, by themselves, already be good indi-
cators of proficiency in non-native speakers, but
from a construct validity perspective1, it is neces-
sary that an automatic assessment model measure
higher-order aspects of language proficiency. Syn-
tactic complexity is one such aspect of proficiency.
By “syntactic complexity”, we mean a learner’s
ability to use a wide range of sophisticated gram-
matical structures.
This study is different from studies that fo-
cus on capturing grammatical errors in non-native
speakers (Foster and Skehan, 1996; Iwashita et al.,
2008). Instead of focusing on grammatical errors
that are found to be highly representative of lan-
guage proficiency, our interest is in capturing the
range of forms that surface in language production
and the degree of sophistication of such forms,
collectively referred to as syntactic complexity in
(Ortega, 2003).
The choice and design of objective measures of
language proficiency is governed by two crucial
constraints:
</bodyText>
<listItem confidence="0.986743">
1. Validity: a measure should show high dis-
criminative ability between various levels of
language proficiency, and the scores pro-
duced by the use of this measure should show
high agreement with human-assigned scores.
2. Robustness: a measures should be derived
automatically and should be robust to errors
in the measure generation process.
</listItem>
<bodyText confidence="0.99841">
A critical impediment to the robustness con-
straint in the state-of-the-art is the multi-stage au-
</bodyText>
<footnote confidence="0.998076666666667">
1Construct validity is the degree to which a test measures
what it claims, or purports, to be measuring and an important
criterion in the development and use of assessments or tests.
</footnote>
<page confidence="0.856862">
1305
</page>
<note confidence="0.834944">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1305–1315,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.811816363636364">
tomated process, where errors in the speech recog-
nition stage (the very first stage) affect subsequent
stages. Guided by studies in second language de-
velopment, we design a measure of syntactic com-
plexity that captures patterns indicative of profi-
cient and non-proficient grammatical structures by
a shallow-analysis of spoken language, as opposed
to a deep syntactic analysis, and analyze the per-
formance of the automatic scoring model with its
inclusion. We compare and contrast the proposed
measure with that found to be optimum in Yoon
and Bhat (2012).
Our primary contributions in this study are:
• We show that the measure of syntactic com-
plexity derived from a shallow-analysis of
spoken utterances satisfies the design con-
straint of high discriminative ability between
proficiency levels. In addition, including our
proposed measure of syntactic complexity in
an automatic scoring model results in a statis-
tically significant performance gain over the
state-of-the-art.
</bodyText>
<listItem confidence="0.99661475">
• The proposed measure, derived through a
completely automated process, satisfies the
robustness criterion reasonably well.
• In the domain of native language acquisition,
</listItem>
<bodyText confidence="0.911661375">
the presence or absence of a grammatical
structure indicates grammatical development.
We observe that the proposed approach ele-
gantly and effectively captures this presence-
based criterion of grammatical development,
since the feature indicative of presence or ab-
sence of a grammatical structure is optimal
from an algorithmic point of view.
</bodyText>
<sectionHeader confidence="0.999735" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999973328125">
Speaking in a non-native language requires diverse
abilities, including fluency, pronunciation, into-
nation, grammar, vocabulary, and discourse. In-
formed by studies in second language acquisition
and language testing that regard these factors as
key determiners of spoken language proficiency,
some researchers have focused on the objective
measurement of these aspects of spoken language
in the context of automatic assessment of language
ability. Notable are studies that have focused on
assessment of fluency (Cucchiarini et al., 2000;
Cucchiarini et al., 2002), pronunciation (Witt and
Young, 1997; Witt, 1999; Franco et al., 1997;
Neumeyer et al., 2000), and intonation (Zechner
et al., 2009). The relative success of these studies
has yielded objective measures of acoustic aspects
of speaking ability, resulting in a shift in focus
to more complex aspects of assessment of gram-
mar (Bernstein et al., 2010; Chen and Yoon, 2011;
Chen and Zechner, 2011), topic development (Xie
et al., 2012), and coherence (Wang et al., 2013).
In an effort to assess grammar and usage in a
second language learning environment, numerous
studies have focused on identifying relevant quan-
titative measures. These measures have been used
to estimate proficiency levels in English as a sec-
ond language (ESL) writing with reasonable suc-
cess. Wolf-Quintero et al. (1998), Ortega (2003),
and Lu (2010) found that measures such as mean
length of T-unit2 and dependent clauses per clause
(henceforth termed as length-based measures) are
well correlated with holistic proficiency scores
suggesting that these quantitative measures can be
used as objective indices of grammatical develop-
ment.
In the context of spoken ESL, these measures
have been studied as well but the results have been
inconclusive. The measures could only broadly
discriminate between students’ proficiency levels,
rated on a scale with moderate to weak correla-
tions, and strong data dependencies on the par-
ticipant groups were observed (Halleck, 1995;
Iwashita et al., 2008; Iwashita, 2010).
With the recent interest in the area of auto-
matic assessment of speech, there is a concur-
rent need to assess the grammatical development
of ESL students automatically. Studies that ex-
plored the applicability of length-based measures
in an automated scoring system (Chen and Zech-
ner, 2011; Chen and Yoon, 2011) observed another
important drawback of these measures in that set-
ting. Length-based measures do not meet the con-
straints of the design, that, in order for measures
to be effectively incorporated in the automated
speech scoring system, they must be generated in
a fully automated manner, via a multi-stage au-
tomated process that includes speech recognition,
part of speech (POS) tagging, and parsing.
A major bottleneck in the multi-stage process
of an automated speech scoring system for second
language is the stage of automated speech recog-
nition (ASR). Automatic recognition of non-native
speakers’ spontaneous speech is a challenging task
as evidenced by the error rate of the state-of-the-
</bodyText>
<footnote confidence="0.4453475">
2T-units are defined as “the shortest grammatically allow-
able sentences into which writing can be split.” (Hunt, 1965)
</footnote>
<page confidence="0.990588">
1306
</page>
<bodyText confidence="0.99981356097561">
art speech recognizer. For instance, Chen and
Zechner (2011) reported a 50.5% word error rate
(WER) and Yoon and Bhat (2012) reported a 30%
WER in the recognition of ESL students’ spoken
responses. These high error rates at the recogni-
tion stage negatively affect the subsequent stages
of the speech scoring system in general, and in
particular, during a deep syntactic analysis, which
operates on a long sequence of words as its con-
text. As a result, measures of grammatical com-
plexity that are closely tied to a correct syntac-
tic analysis are rendered unreliable. Not surpris-
ingly, Chen and Zechner (2011) studied measures
of grammatical complexity via syntactic parsing
and found that a Pearson’s correlation coefficient
of 0.49 between syntactic complexity measures
(derived from manual transcriptions) and profi-
ciency scores, was drastically reduced to near non-
existence when the measures were applied to ASR
word hypotheses. This suggests that measures
that rely on deep syntactic analysis are unreliable
in current ASR-based scoring systems for sponta-
neous speech.
In order to avoid the problems encountered
with deep analysis-based measures, Yoon and
Bhat (2012) explored a shallow analysis-based ap-
proach, based on the assumption that the level of
grammar sophistication at each proficiency level
is reflected in the distribution of part-of-speech
(POS) tag bigrams. The idea of capturing dif-
ferences in POS tag distributions for classification
has been explored in several previous studies. In
the area of text-genre classification, POS tag dis-
tributions have been found to capture genre differ-
ences in text (Feldman et al., 2009; Marin et al.,
2009); in a language testing context, it has been
used in grammatical error detection and essay
scoring (Chodorow and Leacock, 2000; Tetreault
and Chodorow, 2008). We will see next what as-
pects of syntactic complexity are captured by such
a shallow-analysis.
</bodyText>
<sectionHeader confidence="0.651866" genericHeader="method">
3 Shallow-analysis approach to
measuring syntactic complexity
</sectionHeader>
<bodyText confidence="0.9428825">
The measures of syntactic complexity in this ap-
proach are POS bigrams and are not obtained by a
deep analysis (syntactic parsing) of the structure of
the sentence. Hence we will refer to this approach
as ‘shallow analysis’. In a shallow-analysis ap-
proach to measuring syntactic complexity, we rely
on the distribution of POS bigrams at every profi-
ciency level to be representative of the range and
sophistication of grammatical constructions at that
level. At the outset, POS-bigrams may seem too
simplistic to represent any aspect of true syntactic
complexity. We illustrate to the contrary, that they
are indeed able to capture certain grammatical er-
rors and sophisticated constructions by means of
the following instances. Consider the two sentence
fragments below taken from actual responses (the
bigrams of interest and their associated POS tags
are bold-faced).
</bodyText>
<listItem confidence="0.998587666666667">
1. They can/MD to/TO survive ...
2. They created the culture/NN that/WDT
now/RB is common in the US.
</listItem>
<bodyText confidence="0.999987972972973">
We notice that Example 1 is not only less gram-
matically sophisticated than Example 2 but also
has a grammatical error. The error stems from the
fact that it has a modal verb followed by the word
“to”. On the other hand, Example 2 contains a
relative clause composed of a noun introduced by
“that”. Notice how these grammatical expressions
(one erroneous and the other sophisticated) can be
detected by the POS bigrams “MD-TO” and “NN-
WDT”, respectively.
The idea that the level of syntactic complex-
ity (in terms of its range and sophistication) can
be assessed based on the distribution of POS-tags
is informed by prior studies in second language
acquisition. It has been shown that the usage of
certain grammatical constructions (such as that of
the embedded relative clause in the second sen-
tence above) are indicators of specific milestones
in grammar development (Covington et al., 2006).
In addition, studies such as Foster and Skehan
(1996) have successfully explored the utility of
frequency of grammatical errors as objective mea-
sures of grammatical development.
Based on this idea, Yoon and Bhat (2012) de-
veloped a set of features of syntactic complex-
ity based on POS sequences extracted from a
large corpus of ESL learners’ spoken responses,
grouped by human-assigned scores of proficiency
level. Unlike previous studies, it did not rely
on the occurrence of normative grammatical con-
structions. The main assumption was that each
score level is characterized by different types of
prominent grammatical structures. These repre-
sentative constructions are gathered from a collec-
tion of ESL learners’ spoken responses rated for
overall proficiency. The syntactic complexity of
a test spoken response was estimated based on its
</bodyText>
<page confidence="0.967047">
1307
</page>
<bodyText confidence="0.999984555555556">
similarity to the proficiency groups in the refer-
ence corpus with respect to the score-specific con-
structions. A score was assigned to the response
based on how similar it was to the high score
group. In Section 4.1, we go over the approach
in further detail.
Our current work is inspired by the shallow
analysis-based approach of Yoon and Bhat (2012)
and operates under the same assumptions of cap-
turing the range and sophistication of grammati-
cal constructions at each score level. However,
the approaches differ in the way in which a spo-
ken response is assigned to a score group. We
first analyze the limitations of the model studied in
(Yoon and Bhat, 2012) and then describe how our
model can address those limitations. The result is
a new measure based on POS bigrams to assess
ESL learners’ mastery of syntactic complexity.
</bodyText>
<sectionHeader confidence="0.991563" genericHeader="method">
4 Models for Measuring Grammatical
Competence
</sectionHeader>
<bodyText confidence="0.999989125">
We mentioned that the measure proposed in this
study is derived from assumptions similar to those
studied in (Yoon and Bhat, 2012). Accordingly,
we will summarize the previously studied model,
outline its limitations, show how our proposed
measure addresses those limitations and compare
the two measures for the task of automatic scoring
of speech.
</bodyText>
<subsectionHeader confidence="0.884169">
4.1 Vector-Space Model based approach
</subsectionHeader>
<bodyText confidence="0.999768380952381">
Yoon and Bhat (2012) explored an approach in-
spired by information retrieval. They treat the con-
catenated collection of responses from a particular
score-class as a ‘super’ document. Then, regard-
ing POS bigrams as terms, they construct POS-
based vector space models for each score-class
(there are four score classes denoting levels of pro-
ficiency as will be explained in Section 5.2), thus
yielding four score-specific vector-space models
(VSMs). The terms of the VSM are weighted by
the term frequency-inverse document frequency
(tf-idf) weighting scheme (Salton et al., 1975).
The intuition behind the approach is that responses
in the same proficiency level often share similar
grammar and usage patterns. The similarity be-
tween a test response and a score-specific vector is
then calculated by a cosine similarity metric. Al-
though a total of 4 cosine similarity scores (one
per score group) were generated, only cos4from
among the four similarity scores, and cosmax,
were selected as features.
</bodyText>
<listItem confidence="0.99815825">
• cos4: the cosine similarity score between the
test response and the vector of POS bigrams
for the highest score class (level 4); and,
• cosmax: the score level of the VSM with
</listItem>
<bodyText confidence="0.978540130434783">
which the given response shows maximum
similarity.
Of these, cos4was selected based on its empir-
ical performance (it showed the strongest corre-
lation with human-assigned scores of proficiency
among the distance-based measures). In addition,
an intuitive justification for the choice is that the
score-4 vector is a grammatical “norm” represent-
ing the average grammar usage distribution of the
most proficient ESL students. The measure of syn-
tactic complexity of a response, cos4, is its simi-
larity to the highest score class.
The study found that the measures showed rea-
sonable discriminative ability across proficiency
levels. Despite its encouraging empirical perfor-
mance, the VSM method of capturing grammati-
cal sophistication has the following limitations.
First, the VSM-based method is likely to over-
estimate the contribution of the POS bigrams
when highly correlated bigrams occur as terms in
the VSM. Consider the presence of a grammar pat-
tern represented by more than one POS bigram.
For example, both “NN-WDT” and “WDT-RB” in
Sentence 2 reflect the learner’s usage of a relative
clause. However, we note that the two bigrams are
correlated and including them both results in an
over-estimation of their contribution. The VSM
set-up has no mechanism to handle correlated fea-
tures.
Second, the tf-idf weighting scheme for rela-
tively rare POS bigrams does not adequately cap-
ture their underlying distribution with respect to
score groups. Grammatical expressions that occur
frequently in one score level but rarely in other
levels can be assumed to be characteristic of a
specific score level. Therefore, the more uneven
the distribution of a grammatical expression across
score classes, the more important that grammatical
expression should be as an indicator of a particular
score class. However, the simple idf scheme can-
not capture this uneven distribution. A pattern that
occurs rarely but uniformly across different score
groups can get the same weight as a pattern which
is unevenly distributed to one score group. Mar-
tineau and Finin (2009) observed this weakness of
the tf-idf weighting in the domain of sentiment
</bodyText>
<page confidence="0.969518">
1308
</page>
<bodyText confidence="0.999939692307692">
analysis. When using tf-idf weighting to extract
words that were strongly associated with positive
sentiment in a movie review corpus (they consid-
ered each review as a document and a word as a
term), it was found that a substantial proportion
of words with the highest tf-idf were rare words
(e.g., proper nouns) which were not directly asso-
ciated with the sentiment.
We propose to address these important limita-
tions of the VSM approach by the use of a method
that accounts for each of the deficiencies. This is
done by resorting to a maximum entropy model
based approach, to which we turn next.
</bodyText>
<subsectionHeader confidence="0.97445">
4.2 Maximum Entropy-Based model
</subsectionHeader>
<bodyText confidence="0.999985295454546">
In order to address the limitations discussed in 4.1,
we propose a classification-based approach. Tak-
ing an approach different from previous studies,
we formulate the task of assigning a score of syn-
tactic complexity to a spoken response as a classi-
fication problem: given a spoken response, assign
the response to a proficiency class. A classifier is
trained in an inductive fashion, using a large cor-
pus of learner responses that is divided into pro-
ficiency scores as the training data and then used
to test data that is similar to the training data. A
distinguishing feature of the current study is that
the measure is based on a comparison of charac-
teristics of the test response to models trained on
large amounts of data from each score point, as op-
posed to measures that are simply characteristics
of the responses themselves (which is how mea-
sures have been considered in prior studies).
The inductive classifier we use here is the
maximum-entropy model (MaxEnt) which has
been used to solve several statistical natural lan-
guage processing problems with much success
(Berger et al., 1996; Borthwick et al., 1998; Borth-
wick, 1999; Pang et al., 2002; Klein et al., 2003;
Rosenfeld, 2005). The productive feature en-
gineering aspects of incorporating features into
the discriminative MaxEnt classifier motivate the
model choice for the problem at hand. In partic-
ular, the ability of the MaxEnt model’s estimation
routine to handle overlapping (correlated) features
makes it directly applicable to address the first lim-
itation of the VSM model. The second limitation,
related to the ineffective weighting of terms via
the the tf-idf scheme, seems to be addressed by
the fact that the MaxEnt model assigns a weight
to each feature (in our case, POS bigrams) on a
per-class basis (in our case, score group), by tak-
ing every instance into consideration. Therefore,
a MaxEnt model has an advantage over the model
described in 4.1 in that it uses four different weight
schemes (one per score level) and each scheme is
optimized for each score level. This is beneficial
in situations where the features are not evenly im-
portant across all score levels.
</bodyText>
<sectionHeader confidence="0.998688" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<bodyText confidence="0.991673">
Our experiments seek answers to the following
questions.
</bodyText>
<listItem confidence="0.984153625">
1. To what extent does a MaxEnt-score of syn-
tactic complexity discriminate between levels
of proficiency?
2. What is the effect of including the proposed
measure of syntactic complexity in the state-
of-the-art automatic scoring model?
3. How robust is the measure to errors in the var-
ious stages of automatic generation?
</listItem>
<subsectionHeader confidence="0.961187">
5.1 Tasks
</subsectionHeader>
<bodyText confidence="0.99993">
In order to answer the motivating questions of the
study, we set-up two tasks. In the first task, we
compare the extent to which the VSM-based mea-
sure and the MaxEnt-based measure (outlined in
4.1 and 4.2 above) discriminate between levels of
syntactic complexity. Additionally, we compare
the performance of an automatic scoring model of
overall proficiency that includes the measures of
syntactic complexity from each of the two mod-
els being compared and analyze the gains with re-
spect to the state-of-the-art. In the second task, we
study the measures’ robustness to errors incurred
by ASR.
</bodyText>
<subsectionHeader confidence="0.990404">
5.2 Data
</subsectionHeader>
<bodyText confidence="0.999971928571428">
In this study, we used a collection of responses
from an international English language assess-
ment. The assessment consisted of questions to
which speakers were prompted to provide sponta-
neous spoken responses lasting approximately 45-
60 seconds per question. Test takers read and/or
listened to stimulus materials and then responded
to questions based on the stimuli. All questions so-
licited spontaneous, unconstrained natural speech.
A small portion of the available data with inad-
equate audio quality and lack of student response
was excluded from the study. The remaining re-
sponses were partitioned into two datasets: the
ASR set and the scoring model training/test (SM)
</bodyText>
<page confidence="0.982674">
1309
</page>
<bodyText confidence="0.999809352941177">
set. The ASR set, with 47,227 responses, was
used for ASR training and POS similarity model
training. The SM set, with 2,950 responses, was
used for feature evaluation and automated scoring
model evaluation. There was no overlap in speak-
ers between the ASR set and the SM set.
Each response was rated for overall proficiency
by trained human scorers using a 4-point scoring
scale, where 1 indicates low speaking proficiency
and 4 indicated high speaking proficiency. The
distribution of proficiency scores, along with other
details of the data sets, are presented in Table 1.
As seen in Table 1, there is a strong bias towards
the middle scores (score 2 and 3) with approxi-
mately 84-85% of the responses belonging to these
two score levels. Although the skewed distribution
limits the number of score-specific instances for
the highest and lowest scores available for model
training, we used the data without modifying the
distribution since it is representative of responses
in a large-scale language assessment scenario.
Human raters’ extent of agreement in the sub-
jective task of rating responses for language pro-
ficiency constrains the extent to which we can ex-
pect a machine’s score to agree with that of hu-
mans. An estimate of the extent to which human
raters agree on the subjective task of proficiency
assessment, is obtained by two raters scoring ap-
proximately 5% of data (2,388 responses from
ASR set and 140 responses from SM set). Pear-
son correlation r between the scores assigned by
the two raters was 0.62 in ASR set and 0.58 in SM
set. This level of agreement will guide the evalua-
tion of the human-machine agreement on scores.
</bodyText>
<subsectionHeader confidence="0.9997205">
5.3 Stages of Automatic Grammatical
Competence Assessment
</subsectionHeader>
<bodyText confidence="0.9999111875">
Here we outline the multiple stages involved in the
automatic syntactic complexity assessment. The
first stage, ASR, yields an automatic transcription,
which is followed by the POS tagging stage. Sub-
sequently, the feature extraction stage (a VSM or
a MaxEnt model as the case may be) generates the
syntactic complexity feature which is then incor-
porated in a multiple linear regression model to
generate a score.
The steps for automatic assessment of overall
proficiency follow an analogous process (either in-
cluding the POS tagger or not), depending on the
objective measure being evaluated. The various
objective measures are then combined in the mul-
tiple regression scoring model to generate an over-
all score of proficiency.
</bodyText>
<subsectionHeader confidence="0.946287">
5.3.1 Automatic Speech Recognizer
</subsectionHeader>
<bodyText confidence="0.999983">
An HMM recognizer was trained using ASR set
(approximately 733 hours of non-native speech
collected from 7,872 speakers). A gender inde-
pendent triphone acoustic model and combination
of bigram, trigram, and four-gram language mod-
els were used. A word error rate (WER) of 31%
on the SM dataset was observed.
</bodyText>
<subsectionHeader confidence="0.885725">
5.3.2 POS tagger
</subsectionHeader>
<bodyText confidence="0.9998742">
POS tags were generated using the POS tagger
implemented in the Open-NLP toolkit3. It was
trained on the Switchboard (SWBD) corpus. This
POS tagger was trained on about 528K word/tag
pairs. A combination of 36 tags from the Penn
Treebank tag set and 6 tags generated for spoken
languages were used in the tagger.
The tagger achieved a tagging accuracy of
96.3% on a Switchboard evaluation set composed
of 379K words, suggesting high accuracy of the
tagger. However, due to substantial amount of
speech recognition errors in our data, the POS
error rate (resulting from the combined errors of
ASR and automated POS tagger) is expected to be
higher.
</bodyText>
<sectionHeader confidence="0.679412" genericHeader="method">
5.3.3 VSM-based Model
</sectionHeader>
<bodyText confidence="0.9999212">
We used the ASR data set to train a POS-bigram
VSM for the highest score class and generated
cos4 and cosmax reported in Yoon and Bhat
(2012), for the SM data set as outlined in Sec-
tion 4.1.
</bodyText>
<subsectionHeader confidence="0.724802">
5.3.4 Maximum Entropy Model Classifier
</subsectionHeader>
<bodyText confidence="0.999876692307692">
The input to the classifier is a set of POS bi-
grams (1366 bigrams in all) obtained from the
POS-tagged output of the data. We considered
binary-valued features (whether a POS bigram oc-
curred or not), occurrence frequency, and relative
frequency as input for the purpose of experimen-
tation. We used the maximum entropy classifier
implementation in the MaxEnt toolkit4. The clas-
sifier was trained using the LBFGS algorithm for
parameter estimation and used equal-scale gaus-
sian priors for smoothing. The results that fol-
low are based on MaxEnt classifier’s parameter
settings initialized to zero. Since a preliminary
</bodyText>
<footnote confidence="0.992201666666667">
3http://opennlp.apache.org
4http://homepages.inf.ed.ac.uk/
lzhang10/maxent_toolkit.html.
</footnote>
<page confidence="0.943331">
1310
</page>
<table confidence="0.999776142857143">
Data set No. of No. of Score Score distribution
responses speakers
Mean SD 1 2 3 4
ASR 47,227 7,872 2.67 0.73 1,953 16,834 23,106 5,334
4% 36% 49% 11%
SM 2,950 500 2.61 0.74 166 1,103 1,385 296
6% 37% 47% 10%
</table>
<tableCaption confidence="0.999861">
Table 1: Data size and score distribution
</tableCaption>
<bodyText confidence="0.999917078947368">
analysis of the effect of varying the feature (bi-
nary or frequency) revealed that the binary-valued
feature was optimal (in terms of yielding the best
agreement between human and machine scores),
we only report our results for this case. The ASR
data set was used to train the MaxEnt classifier and
the features generated from the SM data set were
used for evaluation.
One straightforward way of using the maximum
entropy classifier’s prediction for our case is to
directly use its predicted score-level – 1, 2, 3 or
4. However, this forces the classifier to make a
coarse-grained choice and may over-penalize the
classifier’s scoring errors. To illustrate this, con-
sider a scenario where the classifier assigns two
responses A and B to score level 2 (based on the
maximum a posteriori condition). Suppose that,
for response A, the score class with the second
highest probability corresponds to score level 1
and that, for response B, it corresponds to score
level 3. It is apparent that the classifier has an
overall tendency to assign a higher score to B, but
looking at its top preference alone (2 for both re-
sponses), masks this tendency.
We thus capture the classifier’s finer-grained
scoring tendency by calculating the expected value
of the classifier output. For a given response, the
MaxEnt classifier calculates the conditional prob-
ability of a score-class given the response, in turn
yielding conditional probabilities of each score
group given the observation – pi for score group
i E 11, 2, 3, 4}. In our case, we consider the pre-
dicted score of syntactic complexity to be the ex-
pected value of the class label given the observa-
tion as, mescore = 1xp1+2xp2+3xp3+4xp4.
This permits us to better represent the score as-
signed by the MaxEnt classifier as a relative pref-
erence over score assignments.
</bodyText>
<subsectionHeader confidence="0.9257">
5.3.5 Automatic Scoring System
</subsectionHeader>
<bodyText confidence="0.999927363636364">
We consider a multiple regression automatic scor-
ing model as studied in Zechner et al. (2009; Chen
and Zechner (2011; Higgins et al. (2011). In its
state-of-the-art set-up, the following model uses
the features – HMM acoustic model score (global
normalized), speaking rate, word types per sec-
ond, average chunk length in words and language
model score (global normalized). We use these
features by themselves (Base), and also in con-
junction with the VSM-based feature (cva4) and
the MaxEnt-based feature (mescore).
</bodyText>
<subsectionHeader confidence="0.98454">
5.4 Evaluation Metric
</subsectionHeader>
<bodyText confidence="0.99996932">
We evaluate the measures using the metrics cho-
sen in previous studies (Zechner et al., 2009; Chen
and Zechner, 2011; Yoon and Bhat, 2012). A
measure’s utility has been evaluated according to
its ability to discriminate between levels of pro-
ficiency assigned by human raters. This is done
by considering the Pearson correlation coefficient
between the feature and the human scores. In an
ideal situation, we would have compared machine
score with scores of grammatical skill assigned by
human raters. In our case, however, with only
access to the overall proficiency scores, we use
scores of language proficiency as those of gram-
matical skill.
A criterion for evaluating the performance of
the scoring model is the extent to which the au-
tomatic scores of overall proficiency agree with
the human scores. As in prior studies, here too
the level of agreement is evaluated by means of
the weighted kappa measure as well as unrounded
and rounded Pearson’s correlations between ma-
chine and human scores (since the output of the re-
gression model can either be rounded or regarded
as is). The feature that maximizes this degree of
agreement will be preferred.
</bodyText>
<sectionHeader confidence="0.992707" genericHeader="evaluation">
6 Experimental Results
</sectionHeader>
<bodyText confidence="0.9999496">
First, we compare the discriminative ability of
measures of syntactic complexity (VSM-model
based measure with that of the MaxEnt-based
measure) across proficiency levels. Table 2 sum-
marizes our experimental results for this task. We
</bodyText>
<page confidence="0.961558">
1311
</page>
<table confidence="0.9987675">
Features Manual Transcriptions ASR
mescore 0.57 0.52
cos4 0.48 0.43
cosmax - 0.31
</table>
<tableCaption confidence="0.729323666666667">
Table 2: Pearson correlation coefficients between measures and holistic proficiency scores. All values
are significant at level 0.01. Only the measures cos4 and mescore were compared for robustness using
manual and ASR transcriptions.
</tableCaption>
<bodyText confidence="0.989624924242425">
notice that of the measures compared, mescore
shows the highest correlation with scores of syn-
tactic complexity. The correlation was approxi-
mately 0.1 higher in absolute value than that of
cos4, which was the best performing feature in the
VSM-based model and the difference is statisti-
cally significant.
Seeking to study the robustness of the mea-
sures derived using a shallow analysis, we next
compare the two measures studied here, with re-
spect to the impact of speech recognition errors on
their correlation with scores of syntactic complex-
ity. Towards this end, we compare mescore and
cos4when POS bigrams are extracted from man-
ual transcriptions (ideal ASR) and ASR transcrip-
tions.
In Table 2, noticing that the correlations de-
crease going along a row, we can say that the er-
rors in the ASR system caused both mescore and
cos4to under-perform. However, the performance
drop (around 0.05) resulting from a shallow anal-
ysis is relatively small compared to the drop ob-
served while employing a deep syntactic analysis.
Chen and Zechner (2011) found that while using
measures of syntactic complexity obtained from
transcriptions, errors in ASR transcripts caused
over 0.40 drop in correlation from that found with
manual transcriptions5. This comparison suggests
that the current POS-based shallow analysis ap-
proach is more robust to ASR errors compared to
a syntactic analysis-based approach.
The effect of the measure of syntactic complex-
ity is best studied by including it in an automatic
scoring model of overall proficiency. We com-
pare the performance gains over the state-of-the-
art with the inclusion of additional features (VSM-
based and MaxEnt-based, in turn). Table 3 shows
the system performance with different grammar
sophistication measures. The results reported are
averaged over a 5-fold cross validation of the mul-
tiple regression model, where 80% of the SM data
5Due to differences in the dataset and ASR system, a di-
rect comparison between the current study and the cited prior
study was not possible.
set is used to train the model and the evaluation is
done using 20% of the data in every fold.
As seen in Table 3, using the proposed measure,
mescore, leads to an improved agreement be-
tween human and machine scores of proficiency.
Comparing the unrounded correlation results in
Table 3 we notice that the model Base+mescore
shows the highest correlation of predicted scores
with human scores. In addition, we test the sig-
nificance of the difference between two depen-
dent correlations using Steiger’s Z-test (via the
paired.r function in the R statistical package
(Revelle, 2012)). We note that the performance
gain of Base+mescore over Base as well as over
Base + cos4 is statistically significant at level =
0.01. The performance gain of Base+cos4 over
Base, however, is not statistically significant at
level = 0.01. Thus, the inclusion of the MaxEnt-
based measure of syntactic complexity results in
improved agreement between machine and hu-
man scores compared to the state-of-the-art model
(here, Base).
</bodyText>
<sectionHeader confidence="0.992974" genericHeader="conclusions">
7 Discussions
</sectionHeader>
<bodyText confidence="0.997817625">
We now discuss some of the observations and re-
sults of our study with respect to the following
items.
Improved performance: We sought to verify
empirically that the MaxEnt model really outper-
forms the VSM in the case of correlated POS
bigrams. To see this, we separate the test set
into three subsets A, B, C. Set A contains re-
sponses where MaxEnt outperforms VSM; set B
contains responses where VSM outperforms Max-
Ent; set C contains responses where their predic-
tions are comparable. For each group of responses
s ∈ {A, B, C}, we calculate the percentage of re-
sponses Ps where two highly correlated POS bi-
grams occur6. We found that the percentages fol-
low the order: PA = 12.93% &gt; PC = 7.29% &gt;
</bodyText>
<footnote confidence="0.994404333333333">
6We consider two POS bigrams to be highly correlated,
when the their pointwise-mutual information is higher than
4.
</footnote>
<page confidence="0.937637">
1312
</page>
<table confidence="0.99964">
Evaluation method Base Base+cos4 Base+mescore
Weighted kappa 0.503 0.524 0.546
Correlation (unrounded) 0.548 0.562 0.592
Correlation (rounded) 0.482 0.492 0.519
</table>
<tableCaption confidence="0.813033666666667">
Table 3: Comparison of scoring model performances using features of syntactic complexity studied in
this paper along with those available in the state-of-the-art. Here, Base is the scoring model without the
measures of syntactic complexity. All correlations are significant at level 0.01.
</tableCaption>
<bodyText confidence="0.999612163934427">
PB = 4.41%. This suggests that when correlated
POS bigrams occur, MaxEnt is more likely to pro-
vide better score predictions than VSM does.
Feature design: In the case of MaxEnt,
the observation that binary-valued features (pres-
ence/absence of POS bigrams) yield better perfor-
mance than features indicative of the occurrence
frequency of the bigram has interesting implica-
tions. This was also observed in Pang et al. (2002)
where it was interpreted to mean that overall senti-
ment is indicated by the presence/absence of key-
words, as opposed to topic of a text, which is in-
dicated by the repeated use of the same or simi-
lar terms. An analogous explanation is applicable
here.
At first glance, the use of the presence/absence
of grammatical structures may raise concerns
about a potential loss of information (e.g. the dis-
tinction between an expression that is used once
and another that is used multiple times is lost).
However, when considered in the context of lan-
guage acquisition studies, this approach seems to
be justified. Studies in native language acquisi-
tion, have considered multiple grammatical devel-
opmental indices that represent the grammatical
levels reached at various stages of language acqui-
sition. For instance, Covington et al. (2006) pro-
posed the revised D-level scale which was origi-
nally studied by Rosenberg and Abbeduto (1987).
The D-Level Scale categorizes grammatical de-
velopment into 8 levels according to the pres-
ence of a set of diverse grammatical expressions
varying in difficulty (for example, level 0 con-
sists of simple sentences, while level 5 consists
of sentences joined by a subordinating conjunc-
tion). Similarly, Scarborough (1990) proposed
the Index of Productive Syntax (IPSyn), accord-
ing to which, the presence of particular grammati-
cal structures, from a list of 60 structures (ranging
from simple ones such as including only subjects
and verbs, to more complex constructions such as
conjoined sentences) is evidence of language ac-
quisition milestones.
Despite the functional differences between the
indices, there is a fundamental operational simi-
larity - that they both use the presence or absence
of grammatical structures, rather than their oc-
currence count, as evidence of acquisition of cer-
tain grammatical levels. The assumption that a
presence-based view of grammatical level acquisi-
tion is also applicable to second language assess-
ment helps validate our observation that binary-
valued features yield a better performance when
compared with frequency-valued features.
Generalizability: The training and test sets
used in this study had similar underlying distribu-
tions – they both sought unconstrained responses
to a set of items with some minor differences in
item type. Looking ahead, an important question
is the extent to which our measure is sensitive to a
mismatch between training and test data.
</bodyText>
<sectionHeader confidence="0.998945" genericHeader="acknowledgments">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999537136363636">
Seeking alternatives to measuring syntactic com-
plexity of spoken responses via syntactic parsers,
we study a shallow-analysis based approach for
use in automatic scoring.
Empirically, we show that the proposed mea-
sure, based on a maximum entropy classification,
satisfied the constraints of the design of an objec-
tive measure to a high degree. In addition, the pro-
posed measure was found to be relatively robust to
ASR errors. The measure outperformed a related
measure of syntactic complexity (also based on
shallow-analysis of spoken response) previously
found to be well-suited for automatic scoring. In-
cluding the measure of syntactic complexity in
an automatic scoring model resulted in statisti-
cally significant performance gains over the state-
of-the-art. We also make an interesting observa-
tion that the impressionistic evaluation of syntactic
complexity is better approximated by the presence
or absence of grammar and usage patterns (and
not by their frequency of occurrence), an idea sup-
ported by studies in native language acquisition.
</bodyText>
<page confidence="0.984042">
1313
</page>
<sectionHeader confidence="0.989484" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998359666666667">
Adam L Berger, Vincent J Della Pietra, and Stephen
A Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computa-
tional linguistics, 22(1):39–71.
Jared Bernstein, Jian Cheng, and Masanori Suzuki.
2010. Fluency and structural complexity as predic-
tors of L2 oral proficiency. In Proceedings of Inter-
Speech, pages 1241–1244.
Andrew Borthwick, John Sterling, Eugene Agichtein,
and Ralph Grishman. 1998. Exploiting diverse
knowledge sources via maximum entropy in named
entity recognition. In Proc. of the Sixth Workshop
on Very Large Corpora.
Andrew Borthwick. 1999. A maximum entropy ap-
proach to named entity recognition. Ph.D. thesis,
New York University.
Lei Chen and Su-Youn Yoon. 2011. Detecting
structural events for assessing non-native speech.
In Proceedings of the 6th Workshop on Innovative
Use of NLP for Building Educational Applications,
IUNLPBEA ’11, pages 38–45, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Miao Chen and Klaus Zechner. 2011. Computing
and evaluating syntactic complexity features for au-
tomated scoring of spontaneous non-native speech.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics, pages 722–
731.
Martin Chodorow and Claudia Leacock. 2000. An un-
supervised method for detecting grammatical errors.
In Proceedings of NAACL, pages 140–147.
Michael A Covington, Congzhou He, Cati Brown, Lo-
rina Naci, and John Brown. 2006. How complex
is that sentence? a proposed revision of the rosen-
berg and abbeduto d-level scale. ReVision. Wash-
ington, DC http://www. ai. uga. edu/caspr/2006-01-
Covington. pdf.(Accessed May 10, 2010.).
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2000.
Quantitative assessment of second language learn-
ers’ fluency by means of automatic speech recogni-
tion technology. The Journal of the Acoustical Soci-
ety ofAmerica, 107(2):989–999.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2002.
Quantitative assessment of second language learn-
ers’ fluency: comparisons between read and sponta-
neous speech. The Journal of the Acoustical Society
ofAmerica, 111(6):2862–2873.
Sergey Feldman, M.A. Marin, Mari Ostendorf, and
Maya R. Gupta. 2009. Part-of-speech histograms
for genre classification of text. In Proceedings of
ICASSP, pages 4781 –4784.
Pauline Foster and Peter Skehan. 1996. The influence
of planning and task type on second language per-
formance. Studies in Second Language Acquisition,
18:299–324.
Horacio Franco, Leonardo Neumeyer, Yoon Kim, and
Orith Ronen. 1997. Automatic pronunciation scor-
ing for language instruction. In Proceedings of
ICASSP, pages 1471–1474.
Gene B Halleck. 1995. Assessing oral proficiency: a
comparison of holistic and objective measures. The
Modern Language Journal, 79(2):223–234.
Derrick Higgins, Xiaoming Xi, Klaus Zechner, and
David Williamson. 2011. A three-stage approach
to the automated scoring of spontaneous spoken re-
sponses. Computer Speech &amp; Language, 25(2):282–
306.
Kellogg W Hunt. 1965. Grammatical structures writ-
ten at three grade levels. ncte research report no. 3.
Noriko Iwashita, Annie Brown, Tim McNamara, and
Sally O’Hagan. 2008. Assessed levels of second
language speaking proficiency: How distinct? Ap-
plied Linguistics, 29(1):24–49.
Noriko Iwashita. 2010. Features of oral proficiency in
task performance by efl and jfl learners. In Selected
proceedings of the Second Language Research Fo-
rum, pages 32–47.
Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-
pher D Manning. 2003. Named entity recognition
with character-level models. In Proceedings of the
seventh conference on Natural language learning at
HLT-NAACL 2003-Volume 4, pages 180–183. Asso-
ciation for Computational Linguistics.
Xiaofei Lu. 2010. Automatic analysis of syntac-
tic complexity in second language writing. Inter-
national Journal of Corpus Linguistics, 15(4):474–
496.
M.A Marin, Sergey Feldman, Mari Ostendorf, and
Maya R. Gupta. 2009. Filtering web text to match
target genres. In Proceedings of ICASSP, pages
3705–3708.
Justin Martineau and Tim Finin. 2009. Delta tfidf: An
improved feature space for sentiment analysis. In
ICWSM.
Leonardo Neumeyer, Horacio Franco, Vassilios Di-
galakis, and Mitchel Weintraub. 2000. Automatic
scoring of pronunciation quality. Speech Communi-
cation, pages 88–93.
Lourdes Ortega. 2003. Syntactic complexity measures
and their relationship to L2 proficiency: A research
synthesis of college–level L2 writing. Applied Lin-
guistics, 24(4):492–518.
</reference>
<page confidence="0.865869">
1314
</page>
<reference confidence="0.999861810344828">
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 conference on Empirical methods in natural
language processing-Volume 10, pages 79–86. As-
sociation for Computational Linguistics.
William Revelle, 2012. psych: Procedures for Psycho-
logical, Psychometric, and Personality Research.
Northwestern University, Evanston, Illinois. R
package version 1.2.1.
Sheldon Rosenberg and Leonard Abbeduto. 1987. In-
dicators of linguistic competence in the peer group
conversational behavior of mildly retarded adults.
Applied Psycholinguistics, 8:19–32.
Ronald Rosenfeld. 2005. Adaptive statistical language
modeling: a maximum entropy approach. Ph.D. the-
sis, IBM.
Gerard Salton, Anita Wong, and Chung-Shu Yang.
1975. A vector space model for automatic indexing.
Communications of the ACM, 18(11):613–620.
Hollis S Scarborough. 1990. Index of productive syn-
tax. Applied Psycholinguistics, 11(1):1–22.
Joel R. Tetreault and Martin Chodorow. 2008. The
ups and downs of preposition error detection in ESL
writing. In Proceedings of COLING, pages 865–
872.
Xinhao Wang, Keelan Evanini, and Klaus Zechner.
2013. Coherence modeling for the automated as-
sessment of spontaneous spoken responses. In Pro-
ceedings of NAACL-HLT, pages 814–819.
Silke Witt and Steve Young. 1997. Performance
measures for phone-level pronunciation teaching in
CALL. In Proceedings of STiLL, pages 99–102.
Silke Witt. 1999. Use of the speech recognition in
computer-assisted language learning. Unpublished
dissertation, Cambridge University Engineering de-
partment, Cambridge, U.K.
Kate Wolf-Quintero, Shunji Inagaki, and Hae-Young
Kim. 1998. Second language development in writ-
ing: Measures of fluency, accuracy, and complexity.
Technical Report 17, Second Language Teaching
and curriculum Center, The University of Hawai’i,
Honolulu, HI.
Shasha Xie, Keelan Evanini, and Klaus Zechner. 2012.
Exploring content features for automated speech
scoring. In Proceedings of the NAACL-HLT, pages
103–111.
Su-Youn Yoon and Suma Bhat. 2012. Assessment of
esl learners’ syntactic competence based on similar-
ity measures. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 600–608. Association for Compu-
tational Linguistics.
Klaus Zechner, Derrick Higgins, Xiaoming Xi, and
David M Williamson. 2009. Automatic scoring of
non-native spontaneous speech in tests of spoken en-
glish. Speech Communication, 51(10):883–895.
</reference>
<page confidence="0.992798">
1315
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.053233">
<title confidence="0.789339333333333">Shallow Analysis Based Assessment of Syntactic Complexity Automated Speech Scoring Suma</title>
<author confidence="0.694755">Beckman</author>
<affiliation confidence="0.900841">University of Urbana,</affiliation>
<email confidence="0.999371">spbhat2@illinois.edu</email>
<author confidence="0.50151">Huichao</author>
<affiliation confidence="0.991867666666667">Dept. of Computer University of Pittsburgh,</affiliation>
<email confidence="0.997356">hux10@cs.pitt.edu</email>
<author confidence="0.439718">Su-Youn</author>
<affiliation confidence="0.761626">Educational Testing</affiliation>
<address confidence="0.86847">Princeton,</address>
<email confidence="0.9971">syoon@ets.org</email>
<abstract confidence="0.998097055555556">Designing measures that capture various aspects of language ability is a central task in the design of systems for automatic scoring of spontaneous speech. In this study, we address a key aspect of language proficiency assessment – syntactic complexity. We propose a novel measure of syntactic complexity for spontaneous speech that shows optimum empirical performance on real world data in multiple First, it is both producing automatic scores that agree well with human rating compared to the stateof-the-art. Second, the measure makes sense theoretically, both from algorithmic and native language acquisition points of view.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational linguistics,</journal>
<pages>22--1</pages>
<contexts>
<context position="19731" citStr="Berger et al., 1996" startWordPosition="3091" endWordPosition="3094">aining data and then used to test data that is similar to the training data. A distinguishing feature of the current study is that the measure is based on a comparison of characteristics of the test response to models trained on large amounts of data from each score point, as opposed to measures that are simply characteristics of the responses themselves (which is how measures have been considered in prior studies). The inductive classifier we use here is the maximum-entropy model (MaxEnt) which has been used to solve several statistical natural language processing problems with much success (Berger et al., 1996; Borthwick et al., 1998; Borthwick, 1999; Pang et al., 2002; Klein et al., 2003; Rosenfeld, 2005). The productive feature engineering aspects of incorporating features into the discriminative MaxEnt classifier motivate the model choice for the problem at hand. In particular, the ability of the MaxEnt model’s estimation routine to handle overlapping (correlated) features makes it directly applicable to address the first limitation of the VSM model. The second limitation, related to the ineffective weighting of terms via the the tf-idf scheme, seems to be addressed by the fact that the MaxEnt m</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L Berger, Vincent J Della Pietra, and Stephen A Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jared Bernstein</author>
<author>Jian Cheng</author>
<author>Masanori Suzuki</author>
</authors>
<title>Fluency and structural complexity as predictors of L2 oral proficiency.</title>
<date>2010</date>
<booktitle>In Proceedings of InterSpeech,</booktitle>
<pages>1241--1244</pages>
<contexts>
<context position="6491" citStr="Bernstein et al., 2010" startWordPosition="975" endWordPosition="978">ncy, some researchers have focused on the objective measurement of these aspects of spoken language in the context of automatic assessment of language ability. Notable are studies that have focused on assessment of fluency (Cucchiarini et al., 2000; Cucchiarini et al., 2002), pronunciation (Witt and Young, 1997; Witt, 1999; Franco et al., 1997; Neumeyer et al., 2000), and intonation (Zechner et al., 2009). The relative success of these studies has yielded objective measures of acoustic aspects of speaking ability, resulting in a shift in focus to more complex aspects of assessment of grammar (Bernstein et al., 2010; Chen and Yoon, 2011; Chen and Zechner, 2011), topic development (Xie et al., 2012), and coherence (Wang et al., 2013). In an effort to assess grammar and usage in a second language learning environment, numerous studies have focused on identifying relevant quantitative measures. These measures have been used to estimate proficiency levels in English as a second language (ESL) writing with reasonable success. Wolf-Quintero et al. (1998), Ortega (2003), and Lu (2010) found that measures such as mean length of T-unit2 and dependent clauses per clause (henceforth termed as length-based measures)</context>
</contexts>
<marker>Bernstein, Cheng, Suzuki, 2010</marker>
<rawString>Jared Bernstein, Jian Cheng, and Masanori Suzuki. 2010. Fluency and structural complexity as predictors of L2 oral proficiency. In Proceedings of InterSpeech, pages 1241–1244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Borthwick</author>
<author>John Sterling</author>
<author>Eugene Agichtein</author>
<author>Ralph Grishman</author>
</authors>
<title>Exploiting diverse knowledge sources via maximum entropy in named entity recognition.</title>
<date>1998</date>
<booktitle>In Proc. of the Sixth Workshop on Very Large Corpora.</booktitle>
<contexts>
<context position="19755" citStr="Borthwick et al., 1998" startWordPosition="3095" endWordPosition="3098">used to test data that is similar to the training data. A distinguishing feature of the current study is that the measure is based on a comparison of characteristics of the test response to models trained on large amounts of data from each score point, as opposed to measures that are simply characteristics of the responses themselves (which is how measures have been considered in prior studies). The inductive classifier we use here is the maximum-entropy model (MaxEnt) which has been used to solve several statistical natural language processing problems with much success (Berger et al., 1996; Borthwick et al., 1998; Borthwick, 1999; Pang et al., 2002; Klein et al., 2003; Rosenfeld, 2005). The productive feature engineering aspects of incorporating features into the discriminative MaxEnt classifier motivate the model choice for the problem at hand. In particular, the ability of the MaxEnt model’s estimation routine to handle overlapping (correlated) features makes it directly applicable to address the first limitation of the VSM model. The second limitation, related to the ineffective weighting of terms via the the tf-idf scheme, seems to be addressed by the fact that the MaxEnt model assigns a weight to</context>
</contexts>
<marker>Borthwick, Sterling, Agichtein, Grishman, 1998</marker>
<rawString>Andrew Borthwick, John Sterling, Eugene Agichtein, and Ralph Grishman. 1998. Exploiting diverse knowledge sources via maximum entropy in named entity recognition. In Proc. of the Sixth Workshop on Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Borthwick</author>
</authors>
<title>A maximum entropy approach to named entity recognition.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>New York University.</institution>
<contexts>
<context position="19772" citStr="Borthwick, 1999" startWordPosition="3099" endWordPosition="3101">s similar to the training data. A distinguishing feature of the current study is that the measure is based on a comparison of characteristics of the test response to models trained on large amounts of data from each score point, as opposed to measures that are simply characteristics of the responses themselves (which is how measures have been considered in prior studies). The inductive classifier we use here is the maximum-entropy model (MaxEnt) which has been used to solve several statistical natural language processing problems with much success (Berger et al., 1996; Borthwick et al., 1998; Borthwick, 1999; Pang et al., 2002; Klein et al., 2003; Rosenfeld, 2005). The productive feature engineering aspects of incorporating features into the discriminative MaxEnt classifier motivate the model choice for the problem at hand. In particular, the ability of the MaxEnt model’s estimation routine to handle overlapping (correlated) features makes it directly applicable to address the first limitation of the VSM model. The second limitation, related to the ineffective weighting of terms via the the tf-idf scheme, seems to be addressed by the fact that the MaxEnt model assigns a weight to each feature (in</context>
</contexts>
<marker>Borthwick, 1999</marker>
<rawString>Andrew Borthwick. 1999. A maximum entropy approach to named entity recognition. Ph.D. thesis, New York University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Chen</author>
<author>Su-Youn Yoon</author>
</authors>
<title>Detecting structural events for assessing non-native speech.</title>
<date>2011</date>
<booktitle>In Proceedings of the 6th Workshop on Innovative Use of NLP for Building Educational Applications, IUNLPBEA ’11,</booktitle>
<pages>38--45</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6512" citStr="Chen and Yoon, 2011" startWordPosition="979" endWordPosition="982">ve focused on the objective measurement of these aspects of spoken language in the context of automatic assessment of language ability. Notable are studies that have focused on assessment of fluency (Cucchiarini et al., 2000; Cucchiarini et al., 2002), pronunciation (Witt and Young, 1997; Witt, 1999; Franco et al., 1997; Neumeyer et al., 2000), and intonation (Zechner et al., 2009). The relative success of these studies has yielded objective measures of acoustic aspects of speaking ability, resulting in a shift in focus to more complex aspects of assessment of grammar (Bernstein et al., 2010; Chen and Yoon, 2011; Chen and Zechner, 2011), topic development (Xie et al., 2012), and coherence (Wang et al., 2013). In an effort to assess grammar and usage in a second language learning environment, numerous studies have focused on identifying relevant quantitative measures. These measures have been used to estimate proficiency levels in English as a second language (ESL) writing with reasonable success. Wolf-Quintero et al. (1998), Ortega (2003), and Lu (2010) found that measures such as mean length of T-unit2 and dependent clauses per clause (henceforth termed as length-based measures) are well correlated </context>
<context position="7931" citStr="Chen and Yoon, 2011" startWordPosition="1200" endWordPosition="1203">d as well but the results have been inconclusive. The measures could only broadly discriminate between students’ proficiency levels, rated on a scale with moderate to weak correlations, and strong data dependencies on the participant groups were observed (Halleck, 1995; Iwashita et al., 2008; Iwashita, 2010). With the recent interest in the area of automatic assessment of speech, there is a concurrent need to assess the grammatical development of ESL students automatically. Studies that explored the applicability of length-based measures in an automated scoring system (Chen and Zechner, 2011; Chen and Yoon, 2011) observed another important drawback of these measures in that setting. Length-based measures do not meet the constraints of the design, that, in order for measures to be effectively incorporated in the automated speech scoring system, they must be generated in a fully automated manner, via a multi-stage automated process that includes speech recognition, part of speech (POS) tagging, and parsing. A major bottleneck in the multi-stage process of an automated speech scoring system for second language is the stage of automated speech recognition (ASR). Automatic recognition of non-native speaker</context>
</contexts>
<marker>Chen, Yoon, 2011</marker>
<rawString>Lei Chen and Su-Youn Yoon. 2011. Detecting structural events for assessing non-native speech. In Proceedings of the 6th Workshop on Innovative Use of NLP for Building Educational Applications, IUNLPBEA ’11, pages 38–45, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miao Chen</author>
<author>Klaus Zechner</author>
</authors>
<title>Computing and evaluating syntactic complexity features for automated scoring of spontaneous non-native speech.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>722--731</pages>
<contexts>
<context position="6537" citStr="Chen and Zechner, 2011" startWordPosition="983" endWordPosition="986">ective measurement of these aspects of spoken language in the context of automatic assessment of language ability. Notable are studies that have focused on assessment of fluency (Cucchiarini et al., 2000; Cucchiarini et al., 2002), pronunciation (Witt and Young, 1997; Witt, 1999; Franco et al., 1997; Neumeyer et al., 2000), and intonation (Zechner et al., 2009). The relative success of these studies has yielded objective measures of acoustic aspects of speaking ability, resulting in a shift in focus to more complex aspects of assessment of grammar (Bernstein et al., 2010; Chen and Yoon, 2011; Chen and Zechner, 2011), topic development (Xie et al., 2012), and coherence (Wang et al., 2013). In an effort to assess grammar and usage in a second language learning environment, numerous studies have focused on identifying relevant quantitative measures. These measures have been used to estimate proficiency levels in English as a second language (ESL) writing with reasonable success. Wolf-Quintero et al. (1998), Ortega (2003), and Lu (2010) found that measures such as mean length of T-unit2 and dependent clauses per clause (henceforth termed as length-based measures) are well correlated with holistic proficiency</context>
<context position="7909" citStr="Chen and Zechner, 2011" startWordPosition="1195" endWordPosition="1199">easures have been studied as well but the results have been inconclusive. The measures could only broadly discriminate between students’ proficiency levels, rated on a scale with moderate to weak correlations, and strong data dependencies on the participant groups were observed (Halleck, 1995; Iwashita et al., 2008; Iwashita, 2010). With the recent interest in the area of automatic assessment of speech, there is a concurrent need to assess the grammatical development of ESL students automatically. Studies that explored the applicability of length-based measures in an automated scoring system (Chen and Zechner, 2011; Chen and Yoon, 2011) observed another important drawback of these measures in that setting. Length-based measures do not meet the constraints of the design, that, in order for measures to be effectively incorporated in the automated speech scoring system, they must be generated in a fully automated manner, via a multi-stage automated process that includes speech recognition, part of speech (POS) tagging, and parsing. A major bottleneck in the multi-stage process of an automated speech scoring system for second language is the stage of automated speech recognition (ASR). Automatic recognition</context>
<context position="9356" citStr="Chen and Zechner (2011)" startWordPosition="1429" endWordPosition="1432">unt, 1965) 1306 art speech recognizer. For instance, Chen and Zechner (2011) reported a 50.5% word error rate (WER) and Yoon and Bhat (2012) reported a 30% WER in the recognition of ESL students’ spoken responses. These high error rates at the recognition stage negatively affect the subsequent stages of the speech scoring system in general, and in particular, during a deep syntactic analysis, which operates on a long sequence of words as its context. As a result, measures of grammatical complexity that are closely tied to a correct syntactic analysis are rendered unreliable. Not surprisingly, Chen and Zechner (2011) studied measures of grammatical complexity via syntactic parsing and found that a Pearson’s correlation coefficient of 0.49 between syntactic complexity measures (derived from manual transcriptions) and proficiency scores, was drastically reduced to near nonexistence when the measures were applied to ASR word hypotheses. This suggests that measures that rely on deep syntactic analysis are unreliable in current ASR-based scoring systems for spontaneous speech. In order to avoid the problems encountered with deep analysis-based measures, Yoon and Bhat (2012) explored a shallow analysis-based ap</context>
<context position="29082" citStr="Chen and Zechner (2011" startWordPosition="4625" endWordPosition="4628">al probability of a score-class given the response, in turn yielding conditional probabilities of each score group given the observation – pi for score group i E 11, 2, 3, 4}. In our case, we consider the predicted score of syntactic complexity to be the expected value of the class label given the observation as, mescore = 1xp1+2xp2+3xp3+4xp4. This permits us to better represent the score assigned by the MaxEnt classifier as a relative preference over score assignments. 5.3.5 Automatic Scoring System We consider a multiple regression automatic scoring model as studied in Zechner et al. (2009; Chen and Zechner (2011; Higgins et al. (2011). In its state-of-the-art set-up, the following model uses the features – HMM acoustic model score (global normalized), speaking rate, word types per second, average chunk length in words and language model score (global normalized). We use these features by themselves (Base), and also in conjunction with the VSM-based feature (cva4) and the MaxEnt-based feature (mescore). 5.4 Evaluation Metric We evaluate the measures using the metrics chosen in previous studies (Zechner et al., 2009; Chen and Zechner, 2011; Yoon and Bhat, 2012). A measure’s utility has been evaluated a</context>
<context position="32277" citStr="Chen and Zechner (2011)" startWordPosition="5136" endWordPosition="5139">studied here, with respect to the impact of speech recognition errors on their correlation with scores of syntactic complexity. Towards this end, we compare mescore and cos4when POS bigrams are extracted from manual transcriptions (ideal ASR) and ASR transcriptions. In Table 2, noticing that the correlations decrease going along a row, we can say that the errors in the ASR system caused both mescore and cos4to under-perform. However, the performance drop (around 0.05) resulting from a shallow analysis is relatively small compared to the drop observed while employing a deep syntactic analysis. Chen and Zechner (2011) found that while using measures of syntactic complexity obtained from transcriptions, errors in ASR transcripts caused over 0.40 drop in correlation from that found with manual transcriptions5. This comparison suggests that the current POS-based shallow analysis approach is more robust to ASR errors compared to a syntactic analysis-based approach. The effect of the measure of syntactic complexity is best studied by including it in an automatic scoring model of overall proficiency. We compare the performance gains over the state-of-theart with the inclusion of additional features (VSMbased and</context>
</contexts>
<marker>Chen, Zechner, 2011</marker>
<rawString>Miao Chen and Klaus Zechner. 2011. Computing and evaluating syntactic complexity features for automated scoring of spontaneous non-native speech. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 722– 731.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Chodorow</author>
<author>Claudia Leacock</author>
</authors>
<title>An unsupervised method for detecting grammatical errors.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>140--147</pages>
<contexts>
<context position="10535" citStr="Chodorow and Leacock, 2000" startWordPosition="1607" endWordPosition="1610">hat (2012) explored a shallow analysis-based approach, based on the assumption that the level of grammar sophistication at each proficiency level is reflected in the distribution of part-of-speech (POS) tag bigrams. The idea of capturing differences in POS tag distributions for classification has been explored in several previous studies. In the area of text-genre classification, POS tag distributions have been found to capture genre differences in text (Feldman et al., 2009; Marin et al., 2009); in a language testing context, it has been used in grammatical error detection and essay scoring (Chodorow and Leacock, 2000; Tetreault and Chodorow, 2008). We will see next what aspects of syntactic complexity are captured by such a shallow-analysis. 3 Shallow-analysis approach to measuring syntactic complexity The measures of syntactic complexity in this approach are POS bigrams and are not obtained by a deep analysis (syntactic parsing) of the structure of the sentence. Hence we will refer to this approach as ‘shallow analysis’. In a shallow-analysis approach to measuring syntactic complexity, we rely on the distribution of POS bigrams at every proficiency level to be representative of the range and sophisticati</context>
</contexts>
<marker>Chodorow, Leacock, 2000</marker>
<rawString>Martin Chodorow and Claudia Leacock. 2000. An unsupervised method for detecting grammatical errors. In Proceedings of NAACL, pages 140–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael A Covington</author>
<author>Congzhou He</author>
<author>Cati Brown</author>
<author>Lorina Naci</author>
<author>John Brown</author>
</authors>
<title>How complex is that sentence? a proposed revision of the rosenberg and abbeduto d-level scale. ReVision.</title>
<date>2006</date>
<location>Washington, DC</location>
<note>http://www. ai. uga. edu/caspr/2006-01-Covington. pdf.(Accessed</note>
<contexts>
<context position="12584" citStr="Covington et al., 2006" startWordPosition="1937" endWordPosition="1940">f a noun introduced by “that”. Notice how these grammatical expressions (one erroneous and the other sophisticated) can be detected by the POS bigrams “MD-TO” and “NNWDT”, respectively. The idea that the level of syntactic complexity (in terms of its range and sophistication) can be assessed based on the distribution of POS-tags is informed by prior studies in second language acquisition. It has been shown that the usage of certain grammatical constructions (such as that of the embedded relative clause in the second sentence above) are indicators of specific milestones in grammar development (Covington et al., 2006). In addition, studies such as Foster and Skehan (1996) have successfully explored the utility of frequency of grammatical errors as objective measures of grammatical development. Based on this idea, Yoon and Bhat (2012) developed a set of features of syntactic complexity based on POS sequences extracted from a large corpus of ESL learners’ spoken responses, grouped by human-assigned scores of proficiency level. Unlike previous studies, it did not rely on the occurrence of normative grammatical constructions. The main assumption was that each score level is characterized by different types of </context>
<context position="36792" citStr="Covington et al. (2006)" startWordPosition="5866" endWordPosition="5869">n analogous explanation is applicable here. At first glance, the use of the presence/absence of grammatical structures may raise concerns about a potential loss of information (e.g. the distinction between an expression that is used once and another that is used multiple times is lost). However, when considered in the context of language acquisition studies, this approach seems to be justified. Studies in native language acquisition, have considered multiple grammatical developmental indices that represent the grammatical levels reached at various stages of language acquisition. For instance, Covington et al. (2006) proposed the revised D-level scale which was originally studied by Rosenberg and Abbeduto (1987). The D-Level Scale categorizes grammatical development into 8 levels according to the presence of a set of diverse grammatical expressions varying in difficulty (for example, level 0 consists of simple sentences, while level 5 consists of sentences joined by a subordinating conjunction). Similarly, Scarborough (1990) proposed the Index of Productive Syntax (IPSyn), according to which, the presence of particular grammatical structures, from a list of 60 structures (ranging from simple ones such as </context>
</contexts>
<marker>Covington, He, Brown, Naci, Brown, 2006</marker>
<rawString>Michael A Covington, Congzhou He, Cati Brown, Lorina Naci, and John Brown. 2006. How complex is that sentence? a proposed revision of the rosenberg and abbeduto d-level scale. ReVision. Washington, DC http://www. ai. uga. edu/caspr/2006-01-Covington. pdf.(Accessed May 10, 2010.).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catia Cucchiarini</author>
<author>Helmer Strik</author>
<author>Lou Boves</author>
</authors>
<title>Quantitative assessment of second language learners’ fluency by means of automatic speech recognition technology.</title>
<date>2000</date>
<journal>The Journal of the Acoustical Society ofAmerica,</journal>
<volume>107</volume>
<issue>2</issue>
<contexts>
<context position="6117" citStr="Cucchiarini et al., 2000" startWordPosition="915" endWordPosition="918">of a grammatical structure is optimal from an algorithmic point of view. 2 Related Work Speaking in a non-native language requires diverse abilities, including fluency, pronunciation, intonation, grammar, vocabulary, and discourse. Informed by studies in second language acquisition and language testing that regard these factors as key determiners of spoken language proficiency, some researchers have focused on the objective measurement of these aspects of spoken language in the context of automatic assessment of language ability. Notable are studies that have focused on assessment of fluency (Cucchiarini et al., 2000; Cucchiarini et al., 2002), pronunciation (Witt and Young, 1997; Witt, 1999; Franco et al., 1997; Neumeyer et al., 2000), and intonation (Zechner et al., 2009). The relative success of these studies has yielded objective measures of acoustic aspects of speaking ability, resulting in a shift in focus to more complex aspects of assessment of grammar (Bernstein et al., 2010; Chen and Yoon, 2011; Chen and Zechner, 2011), topic development (Xie et al., 2012), and coherence (Wang et al., 2013). In an effort to assess grammar and usage in a second language learning environment, numerous studies have</context>
</contexts>
<marker>Cucchiarini, Strik, Boves, 2000</marker>
<rawString>Catia Cucchiarini, Helmer Strik, and Lou Boves. 2000. Quantitative assessment of second language learners’ fluency by means of automatic speech recognition technology. The Journal of the Acoustical Society ofAmerica, 107(2):989–999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catia Cucchiarini</author>
<author>Helmer Strik</author>
<author>Lou Boves</author>
</authors>
<title>Quantitative assessment of second language learners’ fluency: comparisons between read and spontaneous speech.</title>
<date>2002</date>
<journal>The Journal of the Acoustical Society ofAmerica,</journal>
<volume>111</volume>
<issue>6</issue>
<contexts>
<context position="6144" citStr="Cucchiarini et al., 2002" startWordPosition="919" endWordPosition="922"> is optimal from an algorithmic point of view. 2 Related Work Speaking in a non-native language requires diverse abilities, including fluency, pronunciation, intonation, grammar, vocabulary, and discourse. Informed by studies in second language acquisition and language testing that regard these factors as key determiners of spoken language proficiency, some researchers have focused on the objective measurement of these aspects of spoken language in the context of automatic assessment of language ability. Notable are studies that have focused on assessment of fluency (Cucchiarini et al., 2000; Cucchiarini et al., 2002), pronunciation (Witt and Young, 1997; Witt, 1999; Franco et al., 1997; Neumeyer et al., 2000), and intonation (Zechner et al., 2009). The relative success of these studies has yielded objective measures of acoustic aspects of speaking ability, resulting in a shift in focus to more complex aspects of assessment of grammar (Bernstein et al., 2010; Chen and Yoon, 2011; Chen and Zechner, 2011), topic development (Xie et al., 2012), and coherence (Wang et al., 2013). In an effort to assess grammar and usage in a second language learning environment, numerous studies have focused on identifying rel</context>
</contexts>
<marker>Cucchiarini, Strik, Boves, 2002</marker>
<rawString>Catia Cucchiarini, Helmer Strik, and Lou Boves. 2002. Quantitative assessment of second language learners’ fluency: comparisons between read and spontaneous speech. The Journal of the Acoustical Society ofAmerica, 111(6):2862–2873.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Feldman</author>
<author>M A Marin</author>
<author>Mari Ostendorf</author>
<author>Maya R Gupta</author>
</authors>
<title>Part-of-speech histograms for genre classification of text.</title>
<date>2009</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<pages>4781--4784</pages>
<contexts>
<context position="10388" citStr="Feldman et al., 2009" startWordPosition="1583" endWordPosition="1586">nt ASR-based scoring systems for spontaneous speech. In order to avoid the problems encountered with deep analysis-based measures, Yoon and Bhat (2012) explored a shallow analysis-based approach, based on the assumption that the level of grammar sophistication at each proficiency level is reflected in the distribution of part-of-speech (POS) tag bigrams. The idea of capturing differences in POS tag distributions for classification has been explored in several previous studies. In the area of text-genre classification, POS tag distributions have been found to capture genre differences in text (Feldman et al., 2009; Marin et al., 2009); in a language testing context, it has been used in grammatical error detection and essay scoring (Chodorow and Leacock, 2000; Tetreault and Chodorow, 2008). We will see next what aspects of syntactic complexity are captured by such a shallow-analysis. 3 Shallow-analysis approach to measuring syntactic complexity The measures of syntactic complexity in this approach are POS bigrams and are not obtained by a deep analysis (syntactic parsing) of the structure of the sentence. Hence we will refer to this approach as ‘shallow analysis’. In a shallow-analysis approach to measu</context>
</contexts>
<marker>Feldman, Marin, Ostendorf, Gupta, 2009</marker>
<rawString>Sergey Feldman, M.A. Marin, Mari Ostendorf, and Maya R. Gupta. 2009. Part-of-speech histograms for genre classification of text. In Proceedings of ICASSP, pages 4781 –4784.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pauline Foster</author>
<author>Peter Skehan</author>
</authors>
<title>The influence of planning and task type on second language performance.</title>
<date>1996</date>
<booktitle>Studies in Second Language Acquisition,</booktitle>
<pages>18--299</pages>
<contexts>
<context position="2808" citStr="Foster and Skehan, 1996" startWordPosition="420" endWordPosition="423">tend its coverage to higher order aspects of language ability. Fluency and pronunciation may, by themselves, already be good indicators of proficiency in non-native speakers, but from a construct validity perspective1, it is necessary that an automatic assessment model measure higher-order aspects of language proficiency. Syntactic complexity is one such aspect of proficiency. By “syntactic complexity”, we mean a learner’s ability to use a wide range of sophisticated grammatical structures. This study is different from studies that focus on capturing grammatical errors in non-native speakers (Foster and Skehan, 1996; Iwashita et al., 2008). Instead of focusing on grammatical errors that are found to be highly representative of language proficiency, our interest is in capturing the range of forms that surface in language production and the degree of sophistication of such forms, collectively referred to as syntactic complexity in (Ortega, 2003). The choice and design of objective measures of language proficiency is governed by two crucial constraints: 1. Validity: a measure should show high discriminative ability between various levels of language proficiency, and the scores produced by the use of this me</context>
<context position="12639" citStr="Foster and Skehan (1996)" startWordPosition="1946" endWordPosition="1949">tical expressions (one erroneous and the other sophisticated) can be detected by the POS bigrams “MD-TO” and “NNWDT”, respectively. The idea that the level of syntactic complexity (in terms of its range and sophistication) can be assessed based on the distribution of POS-tags is informed by prior studies in second language acquisition. It has been shown that the usage of certain grammatical constructions (such as that of the embedded relative clause in the second sentence above) are indicators of specific milestones in grammar development (Covington et al., 2006). In addition, studies such as Foster and Skehan (1996) have successfully explored the utility of frequency of grammatical errors as objective measures of grammatical development. Based on this idea, Yoon and Bhat (2012) developed a set of features of syntactic complexity based on POS sequences extracted from a large corpus of ESL learners’ spoken responses, grouped by human-assigned scores of proficiency level. Unlike previous studies, it did not rely on the occurrence of normative grammatical constructions. The main assumption was that each score level is characterized by different types of prominent grammatical structures. These representative </context>
</contexts>
<marker>Foster, Skehan, 1996</marker>
<rawString>Pauline Foster and Peter Skehan. 1996. The influence of planning and task type on second language performance. Studies in Second Language Acquisition, 18:299–324.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Horacio Franco</author>
<author>Leonardo Neumeyer</author>
<author>Yoon Kim</author>
<author>Orith Ronen</author>
</authors>
<title>Automatic pronunciation scoring for language instruction.</title>
<date>1997</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<pages>1471--1474</pages>
<contexts>
<context position="6214" citStr="Franco et al., 1997" startWordPosition="930" endWordPosition="933">non-native language requires diverse abilities, including fluency, pronunciation, intonation, grammar, vocabulary, and discourse. Informed by studies in second language acquisition and language testing that regard these factors as key determiners of spoken language proficiency, some researchers have focused on the objective measurement of these aspects of spoken language in the context of automatic assessment of language ability. Notable are studies that have focused on assessment of fluency (Cucchiarini et al., 2000; Cucchiarini et al., 2002), pronunciation (Witt and Young, 1997; Witt, 1999; Franco et al., 1997; Neumeyer et al., 2000), and intonation (Zechner et al., 2009). The relative success of these studies has yielded objective measures of acoustic aspects of speaking ability, resulting in a shift in focus to more complex aspects of assessment of grammar (Bernstein et al., 2010; Chen and Yoon, 2011; Chen and Zechner, 2011), topic development (Xie et al., 2012), and coherence (Wang et al., 2013). In an effort to assess grammar and usage in a second language learning environment, numerous studies have focused on identifying relevant quantitative measures. These measures have been used to estimate</context>
</contexts>
<marker>Franco, Neumeyer, Kim, Ronen, 1997</marker>
<rawString>Horacio Franco, Leonardo Neumeyer, Yoon Kim, and Orith Ronen. 1997. Automatic pronunciation scoring for language instruction. In Proceedings of ICASSP, pages 1471–1474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gene B Halleck</author>
</authors>
<title>Assessing oral proficiency: a comparison of holistic and objective measures.</title>
<date>1995</date>
<journal>The Modern Language Journal,</journal>
<volume>79</volume>
<issue>2</issue>
<contexts>
<context position="7580" citStr="Halleck, 1995" startWordPosition="1145" endWordPosition="1146">d that measures such as mean length of T-unit2 and dependent clauses per clause (henceforth termed as length-based measures) are well correlated with holistic proficiency scores suggesting that these quantitative measures can be used as objective indices of grammatical development. In the context of spoken ESL, these measures have been studied as well but the results have been inconclusive. The measures could only broadly discriminate between students’ proficiency levels, rated on a scale with moderate to weak correlations, and strong data dependencies on the participant groups were observed (Halleck, 1995; Iwashita et al., 2008; Iwashita, 2010). With the recent interest in the area of automatic assessment of speech, there is a concurrent need to assess the grammatical development of ESL students automatically. Studies that explored the applicability of length-based measures in an automated scoring system (Chen and Zechner, 2011; Chen and Yoon, 2011) observed another important drawback of these measures in that setting. Length-based measures do not meet the constraints of the design, that, in order for measures to be effectively incorporated in the automated speech scoring system, they must be </context>
</contexts>
<marker>Halleck, 1995</marker>
<rawString>Gene B Halleck. 1995. Assessing oral proficiency: a comparison of holistic and objective measures. The Modern Language Journal, 79(2):223–234.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Derrick Higgins</author>
<author>Xiaoming Xi</author>
<author>Klaus Zechner</author>
<author>David Williamson</author>
</authors>
<title>A three-stage approach to the automated scoring of spontaneous spoken responses.</title>
<date>2011</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>25</volume>
<issue>2</issue>
<pages>306</pages>
<contexts>
<context position="1970" citStr="Higgins et al., 2011" startWordPosition="292" endWordPosition="295">f which are, fluency in language production, pronunciation accuracy, choice of vocabulary, grammatical sophistication and accuracy. The design of automated scoring systems for non-native speaker speaking proficiency is guided by these studies in the choice of pertinent objective measures of these key aspects of language proficiency. The focus of this study is the design and performance analysis of a measure of the syntactic complexity of non-native English responses for use in automatic scoring systems. The state-ofthe art automated scoring system for spontaneous speech (Zechner et al., 2009; Higgins et al., 2011) currently uses measures of fluency and pronunciation (acoustic aspects) to produce scores that are in reasonable agreement with human-rated scores of proficiency. Despite its good performance, there is a need to extend its coverage to higher order aspects of language ability. Fluency and pronunciation may, by themselves, already be good indicators of proficiency in non-native speakers, but from a construct validity perspective1, it is necessary that an automatic assessment model measure higher-order aspects of language proficiency. Syntactic complexity is one such aspect of proficiency. By “s</context>
<context position="29105" citStr="Higgins et al. (2011)" startWordPosition="4629" endWordPosition="4632">e-class given the response, in turn yielding conditional probabilities of each score group given the observation – pi for score group i E 11, 2, 3, 4}. In our case, we consider the predicted score of syntactic complexity to be the expected value of the class label given the observation as, mescore = 1xp1+2xp2+3xp3+4xp4. This permits us to better represent the score assigned by the MaxEnt classifier as a relative preference over score assignments. 5.3.5 Automatic Scoring System We consider a multiple regression automatic scoring model as studied in Zechner et al. (2009; Chen and Zechner (2011; Higgins et al. (2011). In its state-of-the-art set-up, the following model uses the features – HMM acoustic model score (global normalized), speaking rate, word types per second, average chunk length in words and language model score (global normalized). We use these features by themselves (Base), and also in conjunction with the VSM-based feature (cva4) and the MaxEnt-based feature (mescore). 5.4 Evaluation Metric We evaluate the measures using the metrics chosen in previous studies (Zechner et al., 2009; Chen and Zechner, 2011; Yoon and Bhat, 2012). A measure’s utility has been evaluated according to its ability</context>
</contexts>
<marker>Higgins, Xi, Zechner, Williamson, 2011</marker>
<rawString>Derrick Higgins, Xiaoming Xi, Klaus Zechner, and David Williamson. 2011. A three-stage approach to the automated scoring of spontaneous spoken responses. Computer Speech &amp; Language, 25(2):282– 306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kellogg W Hunt</author>
</authors>
<title>Grammatical structures written at three grade levels. ncte research report no.</title>
<date>1965</date>
<tech>3.</tech>
<contexts>
<context position="8743" citStr="Hunt, 1965" startWordPosition="1328" endWordPosition="1329">e automated speech scoring system, they must be generated in a fully automated manner, via a multi-stage automated process that includes speech recognition, part of speech (POS) tagging, and parsing. A major bottleneck in the multi-stage process of an automated speech scoring system for second language is the stage of automated speech recognition (ASR). Automatic recognition of non-native speakers’ spontaneous speech is a challenging task as evidenced by the error rate of the state-of-the2T-units are defined as “the shortest grammatically allowable sentences into which writing can be split.” (Hunt, 1965) 1306 art speech recognizer. For instance, Chen and Zechner (2011) reported a 50.5% word error rate (WER) and Yoon and Bhat (2012) reported a 30% WER in the recognition of ESL students’ spoken responses. These high error rates at the recognition stage negatively affect the subsequent stages of the speech scoring system in general, and in particular, during a deep syntactic analysis, which operates on a long sequence of words as its context. As a result, measures of grammatical complexity that are closely tied to a correct syntactic analysis are rendered unreliable. Not surprisingly, Chen and Z</context>
</contexts>
<marker>Hunt, 1965</marker>
<rawString>Kellogg W Hunt. 1965. Grammatical structures written at three grade levels. ncte research report no. 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noriko Iwashita</author>
<author>Annie Brown</author>
<author>Tim McNamara</author>
<author>Sally O’Hagan</author>
</authors>
<title>Assessed levels of second language speaking proficiency: How distinct?</title>
<date>2008</date>
<journal>Applied Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<marker>Iwashita, Brown, McNamara, O’Hagan, 2008</marker>
<rawString>Noriko Iwashita, Annie Brown, Tim McNamara, and Sally O’Hagan. 2008. Assessed levels of second language speaking proficiency: How distinct? Applied Linguistics, 29(1):24–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noriko Iwashita</author>
</authors>
<title>Features of oral proficiency in task performance by efl and jfl learners.</title>
<date>2010</date>
<booktitle>In Selected proceedings of the Second Language Research Forum,</booktitle>
<pages>32--47</pages>
<contexts>
<context position="7620" citStr="Iwashita, 2010" startWordPosition="1151" endWordPosition="1152"> T-unit2 and dependent clauses per clause (henceforth termed as length-based measures) are well correlated with holistic proficiency scores suggesting that these quantitative measures can be used as objective indices of grammatical development. In the context of spoken ESL, these measures have been studied as well but the results have been inconclusive. The measures could only broadly discriminate between students’ proficiency levels, rated on a scale with moderate to weak correlations, and strong data dependencies on the participant groups were observed (Halleck, 1995; Iwashita et al., 2008; Iwashita, 2010). With the recent interest in the area of automatic assessment of speech, there is a concurrent need to assess the grammatical development of ESL students automatically. Studies that explored the applicability of length-based measures in an automated scoring system (Chen and Zechner, 2011; Chen and Yoon, 2011) observed another important drawback of these measures in that setting. Length-based measures do not meet the constraints of the design, that, in order for measures to be effectively incorporated in the automated speech scoring system, they must be generated in a fully automated manner, v</context>
</contexts>
<marker>Iwashita, 2010</marker>
<rawString>Noriko Iwashita. 2010. Features of oral proficiency in task performance by efl and jfl learners. In Selected proceedings of the Second Language Research Forum, pages 32–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Joseph Smarr</author>
<author>Huy Nguyen</author>
<author>Christopher D Manning</author>
</authors>
<title>Named entity recognition with character-level models.</title>
<date>2003</date>
<booktitle>In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume 4,</booktitle>
<pages>180--183</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="19811" citStr="Klein et al., 2003" startWordPosition="3106" endWordPosition="3109">stinguishing feature of the current study is that the measure is based on a comparison of characteristics of the test response to models trained on large amounts of data from each score point, as opposed to measures that are simply characteristics of the responses themselves (which is how measures have been considered in prior studies). The inductive classifier we use here is the maximum-entropy model (MaxEnt) which has been used to solve several statistical natural language processing problems with much success (Berger et al., 1996; Borthwick et al., 1998; Borthwick, 1999; Pang et al., 2002; Klein et al., 2003; Rosenfeld, 2005). The productive feature engineering aspects of incorporating features into the discriminative MaxEnt classifier motivate the model choice for the problem at hand. In particular, the ability of the MaxEnt model’s estimation routine to handle overlapping (correlated) features makes it directly applicable to address the first limitation of the VSM model. The second limitation, related to the ineffective weighting of terms via the the tf-idf scheme, seems to be addressed by the fact that the MaxEnt model assigns a weight to each feature (in our case, POS bigrams) on a per-class </context>
</contexts>
<marker>Klein, Smarr, Nguyen, Manning, 2003</marker>
<rawString>Dan Klein, Joseph Smarr, Huy Nguyen, and Christopher D Manning. 2003. Named entity recognition with character-level models. In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume 4, pages 180–183. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofei Lu</author>
</authors>
<title>Automatic analysis of syntactic complexity in second language writing.</title>
<date>2010</date>
<journal>International Journal of Corpus Linguistics,</journal>
<volume>15</volume>
<issue>4</issue>
<pages>496</pages>
<contexts>
<context position="6962" citStr="Lu (2010)" startWordPosition="1052" endWordPosition="1053">oustic aspects of speaking ability, resulting in a shift in focus to more complex aspects of assessment of grammar (Bernstein et al., 2010; Chen and Yoon, 2011; Chen and Zechner, 2011), topic development (Xie et al., 2012), and coherence (Wang et al., 2013). In an effort to assess grammar and usage in a second language learning environment, numerous studies have focused on identifying relevant quantitative measures. These measures have been used to estimate proficiency levels in English as a second language (ESL) writing with reasonable success. Wolf-Quintero et al. (1998), Ortega (2003), and Lu (2010) found that measures such as mean length of T-unit2 and dependent clauses per clause (henceforth termed as length-based measures) are well correlated with holistic proficiency scores suggesting that these quantitative measures can be used as objective indices of grammatical development. In the context of spoken ESL, these measures have been studied as well but the results have been inconclusive. The measures could only broadly discriminate between students’ proficiency levels, rated on a scale with moderate to weak correlations, and strong data dependencies on the participant groups were obser</context>
</contexts>
<marker>Lu, 2010</marker>
<rawString>Xiaofei Lu. 2010. Automatic analysis of syntactic complexity in second language writing. International Journal of Corpus Linguistics, 15(4):474– 496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Marin</author>
<author>Sergey Feldman</author>
<author>Mari Ostendorf</author>
<author>Maya R Gupta</author>
</authors>
<title>Filtering web text to match target genres.</title>
<date>2009</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<pages>3705--3708</pages>
<contexts>
<context position="10409" citStr="Marin et al., 2009" startWordPosition="1587" endWordPosition="1590">ystems for spontaneous speech. In order to avoid the problems encountered with deep analysis-based measures, Yoon and Bhat (2012) explored a shallow analysis-based approach, based on the assumption that the level of grammar sophistication at each proficiency level is reflected in the distribution of part-of-speech (POS) tag bigrams. The idea of capturing differences in POS tag distributions for classification has been explored in several previous studies. In the area of text-genre classification, POS tag distributions have been found to capture genre differences in text (Feldman et al., 2009; Marin et al., 2009); in a language testing context, it has been used in grammatical error detection and essay scoring (Chodorow and Leacock, 2000; Tetreault and Chodorow, 2008). We will see next what aspects of syntactic complexity are captured by such a shallow-analysis. 3 Shallow-analysis approach to measuring syntactic complexity The measures of syntactic complexity in this approach are POS bigrams and are not obtained by a deep analysis (syntactic parsing) of the structure of the sentence. Hence we will refer to this approach as ‘shallow analysis’. In a shallow-analysis approach to measuring syntactic comple</context>
</contexts>
<marker>Marin, Feldman, Ostendorf, Gupta, 2009</marker>
<rawString>M.A Marin, Sergey Feldman, Mari Ostendorf, and Maya R. Gupta. 2009. Filtering web text to match target genres. In Proceedings of ICASSP, pages 3705–3708.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justin Martineau</author>
<author>Tim Finin</author>
</authors>
<title>Delta tfidf: An improved feature space for sentiment analysis.</title>
<date>2009</date>
<booktitle>In ICWSM.</booktitle>
<contexts>
<context position="17927" citStr="Martineau and Finin (2009)" startWordPosition="2785" endWordPosition="2789">to score groups. Grammatical expressions that occur frequently in one score level but rarely in other levels can be assumed to be characteristic of a specific score level. Therefore, the more uneven the distribution of a grammatical expression across score classes, the more important that grammatical expression should be as an indicator of a particular score class. However, the simple idf scheme cannot capture this uneven distribution. A pattern that occurs rarely but uniformly across different score groups can get the same weight as a pattern which is unevenly distributed to one score group. Martineau and Finin (2009) observed this weakness of the tf-idf weighting in the domain of sentiment 1308 analysis. When using tf-idf weighting to extract words that were strongly associated with positive sentiment in a movie review corpus (they considered each review as a document and a word as a term), it was found that a substantial proportion of words with the highest tf-idf were rare words (e.g., proper nouns) which were not directly associated with the sentiment. We propose to address these important limitations of the VSM approach by the use of a method that accounts for each of the deficiencies. This is done by</context>
</contexts>
<marker>Martineau, Finin, 2009</marker>
<rawString>Justin Martineau and Tim Finin. 2009. Delta tfidf: An improved feature space for sentiment analysis. In ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonardo Neumeyer</author>
<author>Horacio Franco</author>
<author>Vassilios Digalakis</author>
<author>Mitchel Weintraub</author>
</authors>
<title>Automatic scoring of pronunciation quality. Speech Communication,</title>
<date>2000</date>
<pages>88--93</pages>
<contexts>
<context position="6238" citStr="Neumeyer et al., 2000" startWordPosition="934" endWordPosition="937">equires diverse abilities, including fluency, pronunciation, intonation, grammar, vocabulary, and discourse. Informed by studies in second language acquisition and language testing that regard these factors as key determiners of spoken language proficiency, some researchers have focused on the objective measurement of these aspects of spoken language in the context of automatic assessment of language ability. Notable are studies that have focused on assessment of fluency (Cucchiarini et al., 2000; Cucchiarini et al., 2002), pronunciation (Witt and Young, 1997; Witt, 1999; Franco et al., 1997; Neumeyer et al., 2000), and intonation (Zechner et al., 2009). The relative success of these studies has yielded objective measures of acoustic aspects of speaking ability, resulting in a shift in focus to more complex aspects of assessment of grammar (Bernstein et al., 2010; Chen and Yoon, 2011; Chen and Zechner, 2011), topic development (Xie et al., 2012), and coherence (Wang et al., 2013). In an effort to assess grammar and usage in a second language learning environment, numerous studies have focused on identifying relevant quantitative measures. These measures have been used to estimate proficiency levels in E</context>
</contexts>
<marker>Neumeyer, Franco, Digalakis, Weintraub, 2000</marker>
<rawString>Leonardo Neumeyer, Horacio Franco, Vassilios Digalakis, and Mitchel Weintraub. 2000. Automatic scoring of pronunciation quality. Speech Communication, pages 88–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lourdes Ortega</author>
</authors>
<title>Syntactic complexity measures and their relationship to L2 proficiency: A research synthesis of college–level L2 writing.</title>
<date>2003</date>
<journal>Applied Linguistics,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="3142" citStr="Ortega, 2003" startWordPosition="474" endWordPosition="475"> is one such aspect of proficiency. By “syntactic complexity”, we mean a learner’s ability to use a wide range of sophisticated grammatical structures. This study is different from studies that focus on capturing grammatical errors in non-native speakers (Foster and Skehan, 1996; Iwashita et al., 2008). Instead of focusing on grammatical errors that are found to be highly representative of language proficiency, our interest is in capturing the range of forms that surface in language production and the degree of sophistication of such forms, collectively referred to as syntactic complexity in (Ortega, 2003). The choice and design of objective measures of language proficiency is governed by two crucial constraints: 1. Validity: a measure should show high discriminative ability between various levels of language proficiency, and the scores produced by the use of this measure should show high agreement with human-assigned scores. 2. Robustness: a measures should be derived automatically and should be robust to errors in the measure generation process. A critical impediment to the robustness constraint in the state-of-the-art is the multi-stage au1Construct validity is the degree to which a test mea</context>
<context position="6947" citStr="Ortega (2003)" startWordPosition="1049" endWordPosition="1050">tive measures of acoustic aspects of speaking ability, resulting in a shift in focus to more complex aspects of assessment of grammar (Bernstein et al., 2010; Chen and Yoon, 2011; Chen and Zechner, 2011), topic development (Xie et al., 2012), and coherence (Wang et al., 2013). In an effort to assess grammar and usage in a second language learning environment, numerous studies have focused on identifying relevant quantitative measures. These measures have been used to estimate proficiency levels in English as a second language (ESL) writing with reasonable success. Wolf-Quintero et al. (1998), Ortega (2003), and Lu (2010) found that measures such as mean length of T-unit2 and dependent clauses per clause (henceforth termed as length-based measures) are well correlated with holistic proficiency scores suggesting that these quantitative measures can be used as objective indices of grammatical development. In the context of spoken ESL, these measures have been studied as well but the results have been inconclusive. The measures could only broadly discriminate between students’ proficiency levels, rated on a scale with moderate to weak correlations, and strong data dependencies on the participant gr</context>
</contexts>
<marker>Ortega, 2003</marker>
<rawString>Lourdes Ortega. 2003. Syntactic complexity measures and their relationship to L2 proficiency: A research synthesis of college–level L2 writing. Applied Linguistics, 24(4):492–518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10,</booktitle>
<pages>79--86</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="19791" citStr="Pang et al., 2002" startWordPosition="3102" endWordPosition="3105">training data. A distinguishing feature of the current study is that the measure is based on a comparison of characteristics of the test response to models trained on large amounts of data from each score point, as opposed to measures that are simply characteristics of the responses themselves (which is how measures have been considered in prior studies). The inductive classifier we use here is the maximum-entropy model (MaxEnt) which has been used to solve several statistical natural language processing problems with much success (Berger et al., 1996; Borthwick et al., 1998; Borthwick, 1999; Pang et al., 2002; Klein et al., 2003; Rosenfeld, 2005). The productive feature engineering aspects of incorporating features into the discriminative MaxEnt classifier motivate the model choice for the problem at hand. In particular, the ability of the MaxEnt model’s estimation routine to handle overlapping (correlated) features makes it directly applicable to address the first limitation of the VSM model. The second limitation, related to the ineffective weighting of terms via the the tf-idf scheme, seems to be addressed by the fact that the MaxEnt model assigns a weight to each feature (in our case, POS bigr</context>
<context position="35961" citStr="Pang et al. (2002)" startWordPosition="5731" endWordPosition="5734">ied in this paper along with those available in the state-of-the-art. Here, Base is the scoring model without the measures of syntactic complexity. All correlations are significant at level 0.01. PB = 4.41%. This suggests that when correlated POS bigrams occur, MaxEnt is more likely to provide better score predictions than VSM does. Feature design: In the case of MaxEnt, the observation that binary-valued features (presence/absence of POS bigrams) yield better performance than features indicative of the occurrence frequency of the bigram has interesting implications. This was also observed in Pang et al. (2002) where it was interpreted to mean that overall sentiment is indicated by the presence/absence of keywords, as opposed to topic of a text, which is indicated by the repeated use of the same or similar terms. An analogous explanation is applicable here. At first glance, the use of the presence/absence of grammatical structures may raise concerns about a potential loss of information (e.g. the distinction between an expression that is used once and another that is used multiple times is lost). However, when considered in the context of language acquisition studies, this approach seems to be justi</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, pages 79–86. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Revelle</author>
</authors>
<title>psych: Procedures for Psychological, Psychometric, and Personality Research.</title>
<date>2012</date>
<institution>Northwestern University,</institution>
<location>Evanston, Illinois.</location>
<contexts>
<context position="33831" citStr="Revelle, 2012" startWordPosition="5387" endWordPosition="5388">ited prior study was not possible. set is used to train the model and the evaluation is done using 20% of the data in every fold. As seen in Table 3, using the proposed measure, mescore, leads to an improved agreement between human and machine scores of proficiency. Comparing the unrounded correlation results in Table 3 we notice that the model Base+mescore shows the highest correlation of predicted scores with human scores. In addition, we test the significance of the difference between two dependent correlations using Steiger’s Z-test (via the paired.r function in the R statistical package (Revelle, 2012)). We note that the performance gain of Base+mescore over Base as well as over Base + cos4 is statistically significant at level = 0.01. The performance gain of Base+cos4 over Base, however, is not statistically significant at level = 0.01. Thus, the inclusion of the MaxEntbased measure of syntactic complexity results in improved agreement between machine and human scores compared to the state-of-the-art model (here, Base). 7 Discussions We now discuss some of the observations and results of our study with respect to the following items. Improved performance: We sought to verify empirically th</context>
</contexts>
<marker>Revelle, 2012</marker>
<rawString>William Revelle, 2012. psych: Procedures for Psychological, Psychometric, and Personality Research. Northwestern University, Evanston, Illinois. R package version 1.2.1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sheldon Rosenberg</author>
<author>Leonard Abbeduto</author>
</authors>
<title>Indicators of linguistic competence in the peer group conversational behavior of mildly retarded adults. Applied Psycholinguistics,</title>
<date>1987</date>
<pages>8--19</pages>
<contexts>
<context position="36889" citStr="Rosenberg and Abbeduto (1987)" startWordPosition="5882" endWordPosition="5885">e of grammatical structures may raise concerns about a potential loss of information (e.g. the distinction between an expression that is used once and another that is used multiple times is lost). However, when considered in the context of language acquisition studies, this approach seems to be justified. Studies in native language acquisition, have considered multiple grammatical developmental indices that represent the grammatical levels reached at various stages of language acquisition. For instance, Covington et al. (2006) proposed the revised D-level scale which was originally studied by Rosenberg and Abbeduto (1987). The D-Level Scale categorizes grammatical development into 8 levels according to the presence of a set of diverse grammatical expressions varying in difficulty (for example, level 0 consists of simple sentences, while level 5 consists of sentences joined by a subordinating conjunction). Similarly, Scarborough (1990) proposed the Index of Productive Syntax (IPSyn), according to which, the presence of particular grammatical structures, from a list of 60 structures (ranging from simple ones such as including only subjects and verbs, to more complex constructions such as conjoined sentences) is </context>
</contexts>
<marker>Rosenberg, Abbeduto, 1987</marker>
<rawString>Sheldon Rosenberg and Leonard Abbeduto. 1987. Indicators of linguistic competence in the peer group conversational behavior of mildly retarded adults. Applied Psycholinguistics, 8:19–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Rosenfeld</author>
</authors>
<title>Adaptive statistical language modeling: a maximum entropy approach.</title>
<date>2005</date>
<tech>Ph.D. thesis, IBM.</tech>
<contexts>
<context position="19829" citStr="Rosenfeld, 2005" startWordPosition="3110" endWordPosition="3111"> of the current study is that the measure is based on a comparison of characteristics of the test response to models trained on large amounts of data from each score point, as opposed to measures that are simply characteristics of the responses themselves (which is how measures have been considered in prior studies). The inductive classifier we use here is the maximum-entropy model (MaxEnt) which has been used to solve several statistical natural language processing problems with much success (Berger et al., 1996; Borthwick et al., 1998; Borthwick, 1999; Pang et al., 2002; Klein et al., 2003; Rosenfeld, 2005). The productive feature engineering aspects of incorporating features into the discriminative MaxEnt classifier motivate the model choice for the problem at hand. In particular, the ability of the MaxEnt model’s estimation routine to handle overlapping (correlated) features makes it directly applicable to address the first limitation of the VSM model. The second limitation, related to the ineffective weighting of terms via the the tf-idf scheme, seems to be addressed by the fact that the MaxEnt model assigns a weight to each feature (in our case, POS bigrams) on a per-class basis (in our case</context>
</contexts>
<marker>Rosenfeld, 2005</marker>
<rawString>Ronald Rosenfeld. 2005. Adaptive statistical language modeling: a maximum entropy approach. Ph.D. thesis, IBM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Anita Wong</author>
<author>Chung-Shu Yang</author>
</authors>
<title>A vector space model for automatic indexing.</title>
<date>1975</date>
<journal>Communications of the ACM,</journal>
<volume>18</volume>
<issue>11</issue>
<contexts>
<context position="15273" citStr="Salton et al., 1975" startWordPosition="2364" endWordPosition="2367">eech. 4.1 Vector-Space Model based approach Yoon and Bhat (2012) explored an approach inspired by information retrieval. They treat the concatenated collection of responses from a particular score-class as a ‘super’ document. Then, regarding POS bigrams as terms, they construct POSbased vector space models for each score-class (there are four score classes denoting levels of proficiency as will be explained in Section 5.2), thus yielding four score-specific vector-space models (VSMs). The terms of the VSM are weighted by the term frequency-inverse document frequency (tf-idf) weighting scheme (Salton et al., 1975). The intuition behind the approach is that responses in the same proficiency level often share similar grammar and usage patterns. The similarity between a test response and a score-specific vector is then calculated by a cosine similarity metric. Although a total of 4 cosine similarity scores (one per score group) were generated, only cos4from among the four similarity scores, and cosmax, were selected as features. • cos4: the cosine similarity score between the test response and the vector of POS bigrams for the highest score class (level 4); and, • cosmax: the score level of the VSM with w</context>
</contexts>
<marker>Salton, Wong, Yang, 1975</marker>
<rawString>Gerard Salton, Anita Wong, and Chung-Shu Yang. 1975. A vector space model for automatic indexing. Communications of the ACM, 18(11):613–620.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hollis S Scarborough</author>
</authors>
<title>Index of productive syntax.</title>
<date>1990</date>
<journal>Applied Psycholinguistics,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="37208" citStr="Scarborough (1990)" startWordPosition="5933" endWordPosition="5934">age acquisition, have considered multiple grammatical developmental indices that represent the grammatical levels reached at various stages of language acquisition. For instance, Covington et al. (2006) proposed the revised D-level scale which was originally studied by Rosenberg and Abbeduto (1987). The D-Level Scale categorizes grammatical development into 8 levels according to the presence of a set of diverse grammatical expressions varying in difficulty (for example, level 0 consists of simple sentences, while level 5 consists of sentences joined by a subordinating conjunction). Similarly, Scarborough (1990) proposed the Index of Productive Syntax (IPSyn), according to which, the presence of particular grammatical structures, from a list of 60 structures (ranging from simple ones such as including only subjects and verbs, to more complex constructions such as conjoined sentences) is evidence of language acquisition milestones. Despite the functional differences between the indices, there is a fundamental operational similarity - that they both use the presence or absence of grammatical structures, rather than their occurrence count, as evidence of acquisition of certain grammatical levels. The as</context>
</contexts>
<marker>Scarborough, 1990</marker>
<rawString>Hollis S Scarborough. 1990. Index of productive syntax. Applied Psycholinguistics, 11(1):1–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel R Tetreault</author>
<author>Martin Chodorow</author>
</authors>
<title>The ups and downs of preposition error detection in ESL writing.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>865--872</pages>
<contexts>
<context position="10566" citStr="Tetreault and Chodorow, 2008" startWordPosition="1611" endWordPosition="1614">w analysis-based approach, based on the assumption that the level of grammar sophistication at each proficiency level is reflected in the distribution of part-of-speech (POS) tag bigrams. The idea of capturing differences in POS tag distributions for classification has been explored in several previous studies. In the area of text-genre classification, POS tag distributions have been found to capture genre differences in text (Feldman et al., 2009; Marin et al., 2009); in a language testing context, it has been used in grammatical error detection and essay scoring (Chodorow and Leacock, 2000; Tetreault and Chodorow, 2008). We will see next what aspects of syntactic complexity are captured by such a shallow-analysis. 3 Shallow-analysis approach to measuring syntactic complexity The measures of syntactic complexity in this approach are POS bigrams and are not obtained by a deep analysis (syntactic parsing) of the structure of the sentence. Hence we will refer to this approach as ‘shallow analysis’. In a shallow-analysis approach to measuring syntactic complexity, we rely on the distribution of POS bigrams at every proficiency level to be representative of the range and sophistication of grammatical constructions</context>
</contexts>
<marker>Tetreault, Chodorow, 2008</marker>
<rawString>Joel R. Tetreault and Martin Chodorow. 2008. The ups and downs of preposition error detection in ESL writing. In Proceedings of COLING, pages 865– 872.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinhao Wang</author>
<author>Keelan Evanini</author>
<author>Klaus Zechner</author>
</authors>
<title>Coherence modeling for the automated assessment of spontaneous spoken responses.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>814--819</pages>
<contexts>
<context position="6610" citStr="Wang et al., 2013" startWordPosition="995" endWordPosition="998">atic assessment of language ability. Notable are studies that have focused on assessment of fluency (Cucchiarini et al., 2000; Cucchiarini et al., 2002), pronunciation (Witt and Young, 1997; Witt, 1999; Franco et al., 1997; Neumeyer et al., 2000), and intonation (Zechner et al., 2009). The relative success of these studies has yielded objective measures of acoustic aspects of speaking ability, resulting in a shift in focus to more complex aspects of assessment of grammar (Bernstein et al., 2010; Chen and Yoon, 2011; Chen and Zechner, 2011), topic development (Xie et al., 2012), and coherence (Wang et al., 2013). In an effort to assess grammar and usage in a second language learning environment, numerous studies have focused on identifying relevant quantitative measures. These measures have been used to estimate proficiency levels in English as a second language (ESL) writing with reasonable success. Wolf-Quintero et al. (1998), Ortega (2003), and Lu (2010) found that measures such as mean length of T-unit2 and dependent clauses per clause (henceforth termed as length-based measures) are well correlated with holistic proficiency scores suggesting that these quantitative measures can be used as object</context>
</contexts>
<marker>Wang, Evanini, Zechner, 2013</marker>
<rawString>Xinhao Wang, Keelan Evanini, and Klaus Zechner. 2013. Coherence modeling for the automated assessment of spontaneous spoken responses. In Proceedings of NAACL-HLT, pages 814–819.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silke Witt</author>
<author>Steve Young</author>
</authors>
<title>Performance measures for phone-level pronunciation teaching in CALL.</title>
<date>1997</date>
<booktitle>In Proceedings of STiLL,</booktitle>
<pages>99--102</pages>
<contexts>
<context position="6181" citStr="Witt and Young, 1997" startWordPosition="924" endWordPosition="927">iew. 2 Related Work Speaking in a non-native language requires diverse abilities, including fluency, pronunciation, intonation, grammar, vocabulary, and discourse. Informed by studies in second language acquisition and language testing that regard these factors as key determiners of spoken language proficiency, some researchers have focused on the objective measurement of these aspects of spoken language in the context of automatic assessment of language ability. Notable are studies that have focused on assessment of fluency (Cucchiarini et al., 2000; Cucchiarini et al., 2002), pronunciation (Witt and Young, 1997; Witt, 1999; Franco et al., 1997; Neumeyer et al., 2000), and intonation (Zechner et al., 2009). The relative success of these studies has yielded objective measures of acoustic aspects of speaking ability, resulting in a shift in focus to more complex aspects of assessment of grammar (Bernstein et al., 2010; Chen and Yoon, 2011; Chen and Zechner, 2011), topic development (Xie et al., 2012), and coherence (Wang et al., 2013). In an effort to assess grammar and usage in a second language learning environment, numerous studies have focused on identifying relevant quantitative measures. These me</context>
</contexts>
<marker>Witt, Young, 1997</marker>
<rawString>Silke Witt and Steve Young. 1997. Performance measures for phone-level pronunciation teaching in CALL. In Proceedings of STiLL, pages 99–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silke Witt</author>
</authors>
<title>Use of the speech recognition in computer-assisted language learning. Unpublished dissertation,</title>
<date>1999</date>
<institution>Cambridge University Engineering department,</institution>
<location>Cambridge, U.K.</location>
<contexts>
<context position="6193" citStr="Witt, 1999" startWordPosition="928" endWordPosition="929">eaking in a non-native language requires diverse abilities, including fluency, pronunciation, intonation, grammar, vocabulary, and discourse. Informed by studies in second language acquisition and language testing that regard these factors as key determiners of spoken language proficiency, some researchers have focused on the objective measurement of these aspects of spoken language in the context of automatic assessment of language ability. Notable are studies that have focused on assessment of fluency (Cucchiarini et al., 2000; Cucchiarini et al., 2002), pronunciation (Witt and Young, 1997; Witt, 1999; Franco et al., 1997; Neumeyer et al., 2000), and intonation (Zechner et al., 2009). The relative success of these studies has yielded objective measures of acoustic aspects of speaking ability, resulting in a shift in focus to more complex aspects of assessment of grammar (Bernstein et al., 2010; Chen and Yoon, 2011; Chen and Zechner, 2011), topic development (Xie et al., 2012), and coherence (Wang et al., 2013). In an effort to assess grammar and usage in a second language learning environment, numerous studies have focused on identifying relevant quantitative measures. These measures have </context>
</contexts>
<marker>Witt, 1999</marker>
<rawString>Silke Witt. 1999. Use of the speech recognition in computer-assisted language learning. Unpublished dissertation, Cambridge University Engineering department, Cambridge, U.K.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kate Wolf-Quintero</author>
<author>Shunji Inagaki</author>
<author>Hae-Young Kim</author>
</authors>
<title>Second language development in writing: Measures of fluency, accuracy, and complexity.</title>
<date>1998</date>
<tech>Technical Report 17,</tech>
<institution>Second Language Teaching and curriculum Center, The University of Hawai’i,</institution>
<location>Honolulu, HI.</location>
<contexts>
<context position="6932" citStr="Wolf-Quintero et al. (1998)" startWordPosition="1045" endWordPosition="1048">ese studies has yielded objective measures of acoustic aspects of speaking ability, resulting in a shift in focus to more complex aspects of assessment of grammar (Bernstein et al., 2010; Chen and Yoon, 2011; Chen and Zechner, 2011), topic development (Xie et al., 2012), and coherence (Wang et al., 2013). In an effort to assess grammar and usage in a second language learning environment, numerous studies have focused on identifying relevant quantitative measures. These measures have been used to estimate proficiency levels in English as a second language (ESL) writing with reasonable success. Wolf-Quintero et al. (1998), Ortega (2003), and Lu (2010) found that measures such as mean length of T-unit2 and dependent clauses per clause (henceforth termed as length-based measures) are well correlated with holistic proficiency scores suggesting that these quantitative measures can be used as objective indices of grammatical development. In the context of spoken ESL, these measures have been studied as well but the results have been inconclusive. The measures could only broadly discriminate between students’ proficiency levels, rated on a scale with moderate to weak correlations, and strong data dependencies on the</context>
</contexts>
<marker>Wolf-Quintero, Inagaki, Kim, 1998</marker>
<rawString>Kate Wolf-Quintero, Shunji Inagaki, and Hae-Young Kim. 1998. Second language development in writing: Measures of fluency, accuracy, and complexity. Technical Report 17, Second Language Teaching and curriculum Center, The University of Hawai’i, Honolulu, HI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shasha Xie</author>
<author>Keelan Evanini</author>
<author>Klaus Zechner</author>
</authors>
<title>Exploring content features for automated speech scoring.</title>
<date>2012</date>
<booktitle>In Proceedings of the NAACL-HLT,</booktitle>
<pages>103--111</pages>
<contexts>
<context position="6575" citStr="Xie et al., 2012" startWordPosition="989" endWordPosition="992">n language in the context of automatic assessment of language ability. Notable are studies that have focused on assessment of fluency (Cucchiarini et al., 2000; Cucchiarini et al., 2002), pronunciation (Witt and Young, 1997; Witt, 1999; Franco et al., 1997; Neumeyer et al., 2000), and intonation (Zechner et al., 2009). The relative success of these studies has yielded objective measures of acoustic aspects of speaking ability, resulting in a shift in focus to more complex aspects of assessment of grammar (Bernstein et al., 2010; Chen and Yoon, 2011; Chen and Zechner, 2011), topic development (Xie et al., 2012), and coherence (Wang et al., 2013). In an effort to assess grammar and usage in a second language learning environment, numerous studies have focused on identifying relevant quantitative measures. These measures have been used to estimate proficiency levels in English as a second language (ESL) writing with reasonable success. Wolf-Quintero et al. (1998), Ortega (2003), and Lu (2010) found that measures such as mean length of T-unit2 and dependent clauses per clause (henceforth termed as length-based measures) are well correlated with holistic proficiency scores suggesting that these quantita</context>
</contexts>
<marker>Xie, Evanini, Zechner, 2012</marker>
<rawString>Shasha Xie, Keelan Evanini, and Klaus Zechner. 2012. Exploring content features for automated speech scoring. In Proceedings of the NAACL-HLT, pages 103–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su-Youn Yoon</author>
<author>Suma Bhat</author>
</authors>
<title>Assessment of esl learners’ syntactic competence based on similarity measures.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>600--608</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4630" citStr="Yoon and Bhat (2012)" startWordPosition="701" endWordPosition="704">une 23-25 2014. c�2014 Association for Computational Linguistics tomated process, where errors in the speech recognition stage (the very first stage) affect subsequent stages. Guided by studies in second language development, we design a measure of syntactic complexity that captures patterns indicative of proficient and non-proficient grammatical structures by a shallow-analysis of spoken language, as opposed to a deep syntactic analysis, and analyze the performance of the automatic scoring model with its inclusion. We compare and contrast the proposed measure with that found to be optimum in Yoon and Bhat (2012). Our primary contributions in this study are: • We show that the measure of syntactic complexity derived from a shallow-analysis of spoken utterances satisfies the design constraint of high discriminative ability between proficiency levels. In addition, including our proposed measure of syntactic complexity in an automatic scoring model results in a statistically significant performance gain over the state-of-the-art. • The proposed measure, derived through a completely automated process, satisfies the robustness criterion reasonably well. • In the domain of native language acquisition, the p</context>
<context position="8873" citStr="Yoon and Bhat (2012)" startWordPosition="1348" endWordPosition="1351">s that includes speech recognition, part of speech (POS) tagging, and parsing. A major bottleneck in the multi-stage process of an automated speech scoring system for second language is the stage of automated speech recognition (ASR). Automatic recognition of non-native speakers’ spontaneous speech is a challenging task as evidenced by the error rate of the state-of-the2T-units are defined as “the shortest grammatically allowable sentences into which writing can be split.” (Hunt, 1965) 1306 art speech recognizer. For instance, Chen and Zechner (2011) reported a 50.5% word error rate (WER) and Yoon and Bhat (2012) reported a 30% WER in the recognition of ESL students’ spoken responses. These high error rates at the recognition stage negatively affect the subsequent stages of the speech scoring system in general, and in particular, during a deep syntactic analysis, which operates on a long sequence of words as its context. As a result, measures of grammatical complexity that are closely tied to a correct syntactic analysis are rendered unreliable. Not surprisingly, Chen and Zechner (2011) studied measures of grammatical complexity via syntactic parsing and found that a Pearson’s correlation coefficient </context>
<context position="12804" citStr="Yoon and Bhat (2012)" startWordPosition="1971" endWordPosition="1974">complexity (in terms of its range and sophistication) can be assessed based on the distribution of POS-tags is informed by prior studies in second language acquisition. It has been shown that the usage of certain grammatical constructions (such as that of the embedded relative clause in the second sentence above) are indicators of specific milestones in grammar development (Covington et al., 2006). In addition, studies such as Foster and Skehan (1996) have successfully explored the utility of frequency of grammatical errors as objective measures of grammatical development. Based on this idea, Yoon and Bhat (2012) developed a set of features of syntactic complexity based on POS sequences extracted from a large corpus of ESL learners’ spoken responses, grouped by human-assigned scores of proficiency level. Unlike previous studies, it did not rely on the occurrence of normative grammatical constructions. The main assumption was that each score level is characterized by different types of prominent grammatical structures. These representative constructions are gathered from a collection of ESL learners’ spoken responses rated for overall proficiency. The syntactic complexity of a test spoken response was </context>
<context position="14093" citStr="Yoon and Bhat, 2012" startWordPosition="2182" endWordPosition="2185">he reference corpus with respect to the score-specific constructions. A score was assigned to the response based on how similar it was to the high score group. In Section 4.1, we go over the approach in further detail. Our current work is inspired by the shallow analysis-based approach of Yoon and Bhat (2012) and operates under the same assumptions of capturing the range and sophistication of grammatical constructions at each score level. However, the approaches differ in the way in which a spoken response is assigned to a score group. We first analyze the limitations of the model studied in (Yoon and Bhat, 2012) and then describe how our model can address those limitations. The result is a new measure based on POS bigrams to assess ESL learners’ mastery of syntactic complexity. 4 Models for Measuring Grammatical Competence We mentioned that the measure proposed in this study is derived from assumptions similar to those studied in (Yoon and Bhat, 2012). Accordingly, we will summarize the previously studied model, outline its limitations, show how our proposed measure addresses those limitations and compare the two measures for the task of automatic scoring of speech. 4.1 Vector-Space Model based appro</context>
<context position="26076" citStr="Yoon and Bhat (2012)" startWordPosition="4129" endWordPosition="4132"> combination of 36 tags from the Penn Treebank tag set and 6 tags generated for spoken languages were used in the tagger. The tagger achieved a tagging accuracy of 96.3% on a Switchboard evaluation set composed of 379K words, suggesting high accuracy of the tagger. However, due to substantial amount of speech recognition errors in our data, the POS error rate (resulting from the combined errors of ASR and automated POS tagger) is expected to be higher. 5.3.3 VSM-based Model We used the ASR data set to train a POS-bigram VSM for the highest score class and generated cos4 and cosmax reported in Yoon and Bhat (2012), for the SM data set as outlined in Section 4.1. 5.3.4 Maximum Entropy Model Classifier The input to the classifier is a set of POS bigrams (1366 bigrams in all) obtained from the POS-tagged output of the data. We considered binary-valued features (whether a POS bigram occurred or not), occurrence frequency, and relative frequency as input for the purpose of experimentation. We used the maximum entropy classifier implementation in the MaxEnt toolkit4. The classifier was trained using the LBFGS algorithm for parameter estimation and used equal-scale gaussian priors for smoothing. The results t</context>
<context position="29640" citStr="Yoon and Bhat, 2012" startWordPosition="4713" endWordPosition="4716">del as studied in Zechner et al. (2009; Chen and Zechner (2011; Higgins et al. (2011). In its state-of-the-art set-up, the following model uses the features – HMM acoustic model score (global normalized), speaking rate, word types per second, average chunk length in words and language model score (global normalized). We use these features by themselves (Base), and also in conjunction with the VSM-based feature (cva4) and the MaxEnt-based feature (mescore). 5.4 Evaluation Metric We evaluate the measures using the metrics chosen in previous studies (Zechner et al., 2009; Chen and Zechner, 2011; Yoon and Bhat, 2012). A measure’s utility has been evaluated according to its ability to discriminate between levels of proficiency assigned by human raters. This is done by considering the Pearson correlation coefficient between the feature and the human scores. In an ideal situation, we would have compared machine score with scores of grammatical skill assigned by human raters. In our case, however, with only access to the overall proficiency scores, we use scores of language proficiency as those of grammatical skill. A criterion for evaluating the performance of the scoring model is the extent to which the aut</context>
</contexts>
<marker>Yoon, Bhat, 2012</marker>
<rawString>Su-Youn Yoon and Suma Bhat. 2012. Assessment of esl learners’ syntactic competence based on similarity measures. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 600–608. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Zechner</author>
<author>Derrick Higgins</author>
<author>Xiaoming Xi</author>
<author>David M Williamson</author>
</authors>
<title>Automatic scoring of non-native spontaneous speech in tests of spoken english.</title>
<date>2009</date>
<journal>Speech Communication,</journal>
<volume>51</volume>
<issue>10</issue>
<contexts>
<context position="1175" citStr="Zechner et al., 2009" startWordPosition="169" endWordPosition="173">roficiency assessment – syntactic complexity. We propose a novel measure of syntactic complexity for spontaneous speech that shows optimum empirical performance on real world data in multiple ways. First, it is both robust and reliable, producing automatic scores that agree well with human rating compared to the stateof-the-art. Second, the measure makes sense theoretically, both from algorithmic and native language acquisition points of view. 1 Introduction Assessment of a speaker’s proficiency in a second language is the main task in the domain of automatic evaluation of spontaneous speech (Zechner et al., 2009). Prior studies in language acquisition and second language research have conclusively shown that proficiency in a second language is characterized by several factors, some of which are, fluency in language production, pronunciation accuracy, choice of vocabulary, grammatical sophistication and accuracy. The design of automated scoring systems for non-native speaker speaking proficiency is guided by these studies in the choice of pertinent objective measures of these key aspects of language proficiency. The focus of this study is the design and performance analysis of a measure of the syntacti</context>
<context position="6277" citStr="Zechner et al., 2009" startWordPosition="940" endWordPosition="943">ency, pronunciation, intonation, grammar, vocabulary, and discourse. Informed by studies in second language acquisition and language testing that regard these factors as key determiners of spoken language proficiency, some researchers have focused on the objective measurement of these aspects of spoken language in the context of automatic assessment of language ability. Notable are studies that have focused on assessment of fluency (Cucchiarini et al., 2000; Cucchiarini et al., 2002), pronunciation (Witt and Young, 1997; Witt, 1999; Franco et al., 1997; Neumeyer et al., 2000), and intonation (Zechner et al., 2009). The relative success of these studies has yielded objective measures of acoustic aspects of speaking ability, resulting in a shift in focus to more complex aspects of assessment of grammar (Bernstein et al., 2010; Chen and Yoon, 2011; Chen and Zechner, 2011), topic development (Xie et al., 2012), and coherence (Wang et al., 2013). In an effort to assess grammar and usage in a second language learning environment, numerous studies have focused on identifying relevant quantitative measures. These measures have been used to estimate proficiency levels in English as a second language (ESL) writi</context>
<context position="29058" citStr="Zechner et al. (2009" startWordPosition="4621" endWordPosition="4624">lculates the conditional probability of a score-class given the response, in turn yielding conditional probabilities of each score group given the observation – pi for score group i E 11, 2, 3, 4}. In our case, we consider the predicted score of syntactic complexity to be the expected value of the class label given the observation as, mescore = 1xp1+2xp2+3xp3+4xp4. This permits us to better represent the score assigned by the MaxEnt classifier as a relative preference over score assignments. 5.3.5 Automatic Scoring System We consider a multiple regression automatic scoring model as studied in Zechner et al. (2009; Chen and Zechner (2011; Higgins et al. (2011). In its state-of-the-art set-up, the following model uses the features – HMM acoustic model score (global normalized), speaking rate, word types per second, average chunk length in words and language model score (global normalized). We use these features by themselves (Base), and also in conjunction with the VSM-based feature (cva4) and the MaxEnt-based feature (mescore). 5.4 Evaluation Metric We evaluate the measures using the metrics chosen in previous studies (Zechner et al., 2009; Chen and Zechner, 2011; Yoon and Bhat, 2012). A measure’s util</context>
</contexts>
<marker>Zechner, Higgins, Xi, Williamson, 2009</marker>
<rawString>Klaus Zechner, Derrick Higgins, Xiaoming Xi, and David M Williamson. 2009. Automatic scoring of non-native spontaneous speech in tests of spoken english. Speech Communication, 51(10):883–895.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>