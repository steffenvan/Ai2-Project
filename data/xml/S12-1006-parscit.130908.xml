<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.022759">
<note confidence="0.611905">
The Use of Granularity in Rhetorical Relation Prediction
Blake Stephen Howald and Martha Abramson
Ultralingua, Inc.
1313 SE Fifth Street, Suite 108
</note>
<address confidence="0.771551">
Minneapolis, MN 55414
</address>
<email confidence="0.991489">
{howald, abramson}@ultralingua.com
</email>
<sectionHeader confidence="0.995491" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999979285714286">
We present the results of several machine
learning tasks designed to predict rhetori-
cal relations that hold between clauses in
discourse. We demonstrate that organizing
rhetorical relations into different granularity
categories (based on relative degree of detail)
increases average prediction accuracy from
58% to 70%. Accuracy further increases to
80% with the inclusion of clause types. These
results, which are competitive with existing
systems, hold across several modes of written
discourse and suggest that features of informa-
tion structure are an important consideration
in the machine learnability of discourse.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999857">
The rhetorical relations that hold between clauses
in discourse index temporal and event information
and contribute to a discourse’s pragmatic coherence
(Hobbs, 1985). For example, in (1) the NARRATION
relation holds between (1a) and (1b) as (1b) tempo-
rally follows (1a) at event time.
</bodyText>
<listItem confidence="0.87428975">
(1) a. Pascale closed the toy chest.
b. She walked to the gate.
c. The gate was locked securely.
d. So she couldn’t get into the kitchen.
</listItem>
<bodyText confidence="0.999916071428571">
The ELABORATION relation, describing the sur-
rounding state of affairs, holds between (1b) and
(1c). (1c) is temporally inclusive (subordinated)
with (1b) and there is no temporal progression at
event time. The RESULT relation holds between (1b-
c) and (1d). (1d) follows (1b) and its subordinated
ELABORATION relation (1c) at event time.
Additional pragmatic information is encoded in
these relations in terms of granularity. Granularity
refers to the relative increases or decreases in the
level of described detail. For example, moving from
(1b) to (1c), we learn more information about the
gate via the ELABORATION relation. Also, moving
from (1b-c) to (1d) there is a consolidation of infor-
mation associated with the RESULT relation.
Through several supervised machine learning
tasks, we investigate the degree to which granularity
(as well as additional elements of discourse struc-
ture (e.g. tense, aspect, event)) serves as a viable
organization and predictor of rhetorical relations in
a range of written discourses. This paper is orga-
nized as follows. Section 2 reviews prior research
on rhetorical relations, discourse structure, granular-
ity and prediction. Section 3 discusses the analyzed
data, the selection and annotation of features, and
the construction of several machine learning tasks.
Section 4 provides the results which are then dis-
cussed in Section 5.
</bodyText>
<sectionHeader confidence="0.976115" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.991524818181818">
Rhetorical relation prediction has received consid-
erable attention and has been shown to be useful
for text summarization (Marcu, 1998). Prediction
tasks rely on a number of features (discourse con-
nectives, part of speech, etc.) (Marcu and Echihabi,
2002; Lapata and Lascarides, 2004). A wide range
of accuracies are also reported - 33.96% (Marcu and
Echihabi, 2002) to 70.70% (Lapata and Lascarides,
2004) for all rhetorical relations and, for individ-
ual relations, CONTRAST (43.64%) and CONTINU-
ATION (83.35%) (Sporleder and Lascarides, 2005).
</bodyText>
<page confidence="0.991035">
44
</page>
<note confidence="0.5268555">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 44–48,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.99736112">
We seek to predict the inventory of rhetorical
relations defined in Segmented Discourse Repre-
sentation Theory (“SDRT”) (Asher and Lascarides,
2003). In addition to the relations illustrated in
(1), we consider: BACKGROUND: It was Christ-
mas. Pascale got a new toy.; EXPLANATION: The
aardvark was dirty. It fell into a puddle.; CONSE-
QUENCE: If the aardvark fell in the puddle, then it
got dirty.; ALTERNATION: Pascale got an aardvark
or a stuffed bunny.; and CONTINUATION: Pascale
got an aardvark. Grimsby got a rawhide.
Discourses were selected based on Smith (2003)
who defines five primary discourse modes by: (1)
the situations (events and states) they describe; (2)
the overarching temporality (tense, aspect); and (3)
the type of text progression (temporal - text and
event time progression are similar; atemporal - text
and event time progression are not similar). These
contrastive elements inform the features selected
for the machine learning tasks discussed in Section
3.2. The five modes, narratives, reports (news ar-
ticles), description (recipes), information (scientific
essays), and argument (editorials) were selected to
ensure a balanced range of theoretically supported
discourse types.
</bodyText>
<subsectionHeader confidence="0.99962">
2.1 Granularity of Information
</subsectionHeader>
<bodyText confidence="0.999683285714286">
Granularity in discourse refers to the relative degree
of detail. The higher the level of detail, the more
informative the discourse is. We assume that there
will be some pragmatic constraints on the informa-
tiveness of a discourse (e.g., consistent with Grice‘s
(1975) Maxim of Quantity). For our purposes, we
rely specifically on granularity as defined in Mulkar-
Mehta et al. (2011) (“MM”) who characterize gran-
ularity in terms of entities and events.
To illustrate, consider (2) where the rhetorical
structure indicates that (2b) is an ELABORATION of
(2a), the NARRATION relation holds between (2b)
and (2c) and (2c) and (2d), and the RESULT relation
between (2d) and (2e).
</bodyText>
<listItem confidence="0.99443">
(2) a. The Pittsburgh Steelers needed to win.
b. Batch took the first snap.
c. Then he threw the ball into the endzone.
d. Ward caught the ball.
e. A touchdown was scored.
</listItem>
<bodyText confidence="0.98455112">
Entities and events can stand in part-whole and
causality relationships with entities and events in
subsequent clauses. A positive granularity shift in-
dicates movement from whole to part (more detail)
- e.g., Batch (2b) is a part of the whole Pittsburgh
Steelers (2a). A negative granularity shift indicates
movement from part to whole (less detail), or if
one event causes a subsequent event (if an event is
caused by a subsequent event, this is a positive shift)
- e.g., Ward’s catching of the ball (2d) caused the
scoring of the touchdown (2e). Maintained granular-
ities (not considered by MM) are illustrated in (2b-c)
and (2c-d). Clauses (2b) through (2d) are temporally
linked events, but there is no part-whole shift in, nor
a causal relationship between, the entities or events;
the granularity remains the same.
We maintain that there is a close relationship be-
tween rhetorical relations and granularity. Con-
sequently, rhetorical relations can be organized as
follows: positive: BACKGROUND, ELABORATION,
EXPLANATION; negative: CONSEQUENCE, RE-
SULT; and maintained: ALTERNATION, CONTINU-
ATION, NARRATION. The machine learning tasks
discussed in the remainder of the paper consider this
information in the prediction of rhetorical relations.
</bodyText>
<sectionHeader confidence="0.9786" genericHeader="method">
3 Data and Methods
</sectionHeader>
<bodyText confidence="0.9999128">
Five written discourses of similar sentence length
were selected from each mode for 25 total dis-
courses. The discourses were segmented by inde-
pendent or dependent (subordinate) clauses, if the
clauses contained discourse markers (but, however),
and if the clauses were embedded in the sentence
provided in the orginal written discourse (e.g., John,
who is the director of NASA, gave a speech on Fri-
day). The total number of clauses is 1090, averaging
43.6 clauses per discourse (u=7.2).
</bodyText>
<subsectionHeader confidence="0.99853">
3.1 Feature Annotation
</subsectionHeader>
<bodyText confidence="0.999856222222222">
For prediction, we use a feature set distilled from
Smith’s classification of discourses: TENSE and
ASPECT; EVENT (from the TimeML annotation
scheme (Pustejovksy, et al., 2005), Aspectual, Oc-
curence, States, etc.); SEQUENCE information as
the clause position normalized to the unit interval;
and discourse MODE. We also include CLAUSE
type - independent (IC) or dependent clauses (DC)
with the inclusion of a discourse marker (M) or not,
</bodyText>
<page confidence="0.999414">
45
</page>
<tableCaption confidence="0.998643">
Table 1: Distribution of Relations by Granularity Type.
</tableCaption>
<table confidence="0.999578166666666">
Relation Number (Avg.)
Positive 515 (47%)
BACKGROUND 315 (61%)
ELABORATION 161 (31%)
EXPLANATION 39 (7%)
Negative 59 (5%)
CONSEQUENCE 16 (26%)
RESULT 43 (71%)
Maintenance 490 (44%)
ALTERNATION 76 (14%)
CONTINUATION 30 (6%)
NARRATION 384 (78%)
</table>
<bodyText confidence="0.983308318181818">
embedded (EM) or not - and GRANULARITY shift
categories which are an organization of the SDRT
rhetorical relations (Asher and Lascarides, 2003),
summarized in Table 1.
All 25 discourses were annotated by one of the au-
thors using only a reference sheet. The other author
independently coded 80% of the data (20 discourses,
four from each mode). Average agreement and Co-
hen’s Kappa (Cohen, 1960) statistics were computed
and are within acceptable ranges: TENSE (99.65
/ .9945), ASPECT (99.30 / .9937), SDRT (77.42 /
.6850), and EVENT (75.88 / .6362).
These results are consistent with previously re-
ported annotations for rhetorical relations (Sporleder
and Lascarides, 2005; Howald and Katz, 2011),
event verbs and durations, tense and aspect (Puscasu
and Mititelu, 2008; Wiebe et al., 1997). Positive,
negative and maintained granularities were not an-
notated, but MM report a Kappa between .8500 and
1. The distribution of these granularities, based on
the organization of the annotated rhetorical relations
is presented in Table 1.
</bodyText>
<subsectionHeader confidence="0.997829">
3.2 Machine Learning
</subsectionHeader>
<bodyText confidence="0.999713875">
Three supervised machine learning tasks were con-
structed to predict SDRT relations. The first task
(Uncollapsed) created a 8-way classifier to predict
the SDRT relations based on the feature set, omit-
ting the GRANULARITY feature. The second task
(Collapsed) created a 3-way classifier to predict
the GRANULARITY categories (the SDRT feature
was omitted). The third task (Combined) included
</bodyText>
<tableCaption confidence="0.986008">
Table 2: Relation Prediction - Combined Modes.
</tableCaption>
<table confidence="0.9894025">
Feature J48 K* NB MCB
Uncollapsed 58.99 55.41 56.69 35
Collapsed 69.90 70.18 69.81 41
Combined 78.62 71.92 80.00 35 (70)
</table>
<bodyText confidence="0.993381833333333">
the GRANULARITY feature back into the Uncol-
lapsed 8-way classifier. We utilized the WEKA
toolkit (Witten and Frank, 2005) and treated each
clause as a vector of information (SDRT, EVENT,
TENSE, ASPECT, SEQUENCE, CLAUSE, MODE,
GRANULARITY), illustrated in (3)1:
</bodyText>
<listItem confidence="0.9778245">
(3) a. The Pittsburgh Steelers needed to win.
START, State, Pa., N, .200, IC, NA, start
b. Batch took the first snap.
ELAB., Occ., Pa., N, .400, IC, NA, pos.
c. Then he threw the ball into the endzone.
NAR., Asp., Pa., N, .600, IC-M, NA, main.
d. Ward caught the ball.
NAR., Occ., Pa., N, .800, IC, NA, main.
e. A touchdown was scored.
RESULT, Occ., Pa., Perf., 1.00, IC, NA, neg.
</listItem>
<bodyText confidence="0.9997865">
We report results from the Naive Bayes (NB), J48
(C4.5 decision tree (Quinlan, 1993)) and K* (Cleary
and Trigg, 1995) classifiers, run at 10-fold cross-
validation.
</bodyText>
<sectionHeader confidence="0.99995" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.980608411764706">
Table 2 indicates that the best average accuracy for
the Uncollapsed task is 58.99 (J48). The accu-
racy increases to 70.18 (K*) for the Collapsed task.
The accuracy increases further to 80.00 (NB) for the
Combined task. All accuracies are statistically sig-
nificant over majority class baselines (“MCB”): Un-
collapsed (MCB = 35) - x2 = 15.11, d.f. = 0, p &lt;
.001; Collapsed (MCB = 41) - x2 = 20.51, d.f. =
0, p &lt; .001; and Combined (treating the best Col-
lapsed accuracy as the new baseline (MCB = 70)) -
x2 = 1.43, d.f. = 0, p &lt; .001.
As shown in Table 3, based on the NB 8-way
Combined classifier, the prediction accuracies of
1Note that what is being predicted is the rhetorical relation,
or associated granularity, with the second clause in a clause pair.
Tasks were performed where clause information was paired, but
this did not translate into improved accuracies.
</bodyText>
<page confidence="0.999799">
46
</page>
<tableCaption confidence="0.998608">
Table 3: Individual Relation Prediction Accuracies (%).
</tableCaption>
<table confidence="0.9989699">
Relation A I D N R T
NAR. 73 55 100 100 94 96
RES. 75 88 85 100 100 93
BACK. 93 92 96 87 94 92
ELAB. 57 41 69 21 48 69
CONSEQ. 20 0 0 0 0 37
ALTER. 50 42 0 0 43 27
CONTIN. 8 0 0 0 0 23
EXPLAN. 0 20 0 9 0 2
Total 68 72 92 74 74 80
</table>
<bodyText confidence="0.993911153846154">
the individual modes are no more than 12 percent-
age points off of the average (80.00). Accura-
cies range from 68% A(rgument) (Q=-12) to 92%
D(escription) (Q=+12) with N(arrative), R(eport),
and I(nformation) being closest to average (Q=-6-
8). For individual relation predictions, NARRATION,
RESULT and BACKGROUND have the highest total
accuracies followed by ELABORATION and CON-
TRAST. Performing less well is CONSEQUENCE,
ALTERNATION and CONTINUATION with EXPLA-
NATION performing the worst. All accuracies are
statistically significant above baseline (x2 = 341.89,
d.f. = 7, p &lt; .001).
</bodyText>
<sectionHeader confidence="0.985349" genericHeader="conclusions">
5 Discussion and Conclusion
</sectionHeader>
<bodyText confidence="0.999950865671642">
Using the Collapsed performance as a baseline for
the Combined classifier, we discuss the features
contributing to the 10 percentage point increase as
well as the optimal (minimal) set of features for pre-
diction. The best accuracies for the Combined ex-
periment only require CLAUSE and GRANULAR-
ITY information; achieving 79.08% (NB - 44 above
MCB, f-score=.750). Both CLAUSE and GRANU-
LARITY are necessary. Relying only on CLAUSE
achieves a 48.25% accuracy (J48) and relying only
on GRANULARITY achieves 70.36% for all clas-
sifiers, but this higher accuracy is an artifact of the
organization as evidenced by the f-score (.585).
The relationship between CLAUSE and the
rhetorical relations is straightforward. For example,
the CONSEQUENCE relation is often an “intersenten-
tial” relation (if the aardvark fell in the puddle, then
it got dirty), each of the 16 CONSEQUENCE relations
are embedded. Similarly, 93% of all ELABORATION
relations, which are temporally subordinating, are
embedded. Clause types appear to be a viable source
of co-varying information in rhetorical relation pre-
diction in the tasks under discussion.
The aspects of syntactic-semantic form and prag-
matic function in the relationship between granular-
ity and rhetorical relations is of central interest in
this investigation. Asher and Lascarides represent
discourses hierarchically through coordination and
subordination of information which corresponds to
changes in granularity. However, while the notion
of granularity enters into the motivation and formu-
lation of the SDRT inventory, it is not developed fur-
ther. These results potentailly allow us to say some-
thing deeper about the structural organization of dis-
course as it relates to granularity.
In particualr, while there is some probabilistic
leverage in collapsing categories, it is not the case
that arbitrary categorizations will perform similarly.
This observation holds true even for theoretically
informed categorizations. For example, organizing
the SDRT inventory into coordinated and subordi-
nated relations yields lower performance on relation
prediction. Coordinated and subordinated can be
predicted with 80% accuracy, but the prediction of
the individual relations given the category performs
only at 70%. Since the granularity-based organiza-
tion presented here performs better, we suggest that
the pragmatic function of the relation is more sys-
tematic than the syntactic-semantic form of the rela-
tion.
Future research will focus on more data, differ-
ent machine learning techniques (e.g. unsupervised
learning) and automatization. Where clause, tense,
aspect and event are readily automatable, rhetorical
relations and granularity are less so. Automatically
extracting such information from an annotated cor-
pus such as the Penn Discourse Tree Bank is cer-
tainly feasible. However, the distribution of genres
in this corpus is somewhat limited (i.e., predomi-
nately news text (Webber, 2009)) and calls into ques-
tion the generalizeability of results to other modes of
discourse. Overall, we have demonstrated that the
inclusion of a granularity-based organization in the
machine learning prediction of rhetorical relations
increases performance by 37%, which is roughly
14% above previous reported results for a broader
range of discourses and relations.
</bodyText>
<page confidence="0.99915">
47
</page>
<sectionHeader confidence="0.998331" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999623666666667">
Thank you to Jeff Ondich and Ultralingua for facil-
itating this research and to four anonymous *SEM
reviewers for insightful and constructive comments.
</bodyText>
<sectionHeader confidence="0.990007" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999682901408451">
Nicholas Asher and Alex Lascarides. 2003. Logics
of Conversation. Cambridge University Press, Cam-
bridge, UK.
John G. Cleary and Leonard E. Trigg 1995. K*: An
Instance-based Learner Using an Entropic Distance
Measure. In Proceedings of the 12 International Con-
ference on Machine Learning, 108–113.
Jacob Cohen. 1960. A Coefficient of Agreement for
Nominal Scales. Educational and Psychological Mea-
surement, 20(1):37–46.
H. Paul Grice. 1975. Logic and Conversation. In Syntax
and Semantics, Vol. 3, Speech Acts, 43–85. Academic
Press, New York.
Jerry R. Hobbs. 1985. On The Coherence and Structure
of Discourse. CSLI Technical Report, CSLI-85-37.
Blake Stephen Howald and Graham Katz. 2011. The
Exploitation of Spatial Information in Narrative Dis-
course. In Proceedings of the Ninth International
Workshop on Computational Semantics, 175–184.
Mirella Lapata and Alex Lascarides. 2004. Inferring
Sentence Internal Temporal Relations. In Proceedings
of the North American Association of Computational
Linguistics (NAACL-04) 2004, 153–160.
Daniel Marcu. 1998. Improving Summarization
Through Rhetorical Parsing Tuning. In Proceedings of
The 6th Workshop on Very Large Corpora, 206–215.
Daniel Marcu and Abdessamad Echihabi. 2002. An Un-
supervised Approach to Recognizing Discourse Rela-
tions. In Proceedings of the Association of Computa-
tional Linguistics (ACL-02) 2002, 368–375.
Rutu Mulkar-Mehta, Jerry R. Hobbs and Eduard Hovy.
2011. Granulairty in Natural Language Discourse.
In Proceedings of the Ninth International Conference
on Computational Semantics (IWCS 2011) 2011, 195–
204.
Georgiana Puscasu and Verginica Mititelu. 2008. Anno-
tation of WordNet Verbs with TimeML Event Classes.
Proceedings of the Sixth International Language Re-
sources and Evaluation (LREC08)
James Pustejovsky, Jos´e Casta˜no, Robert Ingria, Roser
Saur, Robert Gaizauskas, Andrea Setzer, and Graham
Katz. 2005. TimeML: Robust Specification of Event
and Temporal Expressions in Text. In Proceedings of
the Fith International Conference on Computational
Semantics (IWCS 2005)
Ross Quinlan. 1993 C4.5: Programsfor Machine Learn-
ing. Morgan Kaufmann, San Francisco, CA.
Carlota Smith. 2003. Modes of Discourse: The Local
Structure of Texts. Cambridge University Press, Cam-
bridge, UK.
Caroline Sporleder and Alex Lascarides. 2005. Exploit-
ing Linguistic Cues to Classify Rhetorical Relations.
In Proceedings of Recent Advances in Natural Lan-
guage Processing (RANLP-05), 532–539.
Caroline Sporleder and Alex Lascarides. 2008. Using
Automatically Labelled Examples to Classify Rhetori-
cal Relations: An Assessment. Natural Language En-
gineering, 14:369–416.
Janyce Wiebe, Thomas O’Hara, Thorsten ¨Ohrstr¨om-
Sandgren and Kenneth McKeever. 1997. An Em-
pirical Approach to Temporal Reference Resolution.
In Proceedings of the 2nd Conference on Empirical
Methods in Natural Language Processing (EMNLP-
97), 174–186.
Ian Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Techniques with Java Imple-
mentation (2nd Ed.) Morgan Kaufmann, San Fran-
cisco, CA.
Bonnie Webber 2009. Genre Distictions for Discourse
in the Penn TreeBank. In Proceedings of the 47th ACL
Conference, 674–682.
</reference>
<page confidence="0.999353">
48
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.926635">
<title confidence="0.999821">The Use of Granularity in Rhetorical Relation Prediction</title>
<author confidence="0.998975">Blake Stephen Howald</author>
<author confidence="0.998975">Martha</author>
<affiliation confidence="0.96918">Ultralingua,</affiliation>
<address confidence="0.9821245">1313 SE Fifth Street, Suite Minneapolis, MN</address>
<abstract confidence="0.999049466666667">We present the results of several machine learning tasks designed to predict rhetorical relations that hold between clauses in discourse. We demonstrate that organizing rhetorical relations into different granularity categories (based on relative degree of detail) increases average prediction accuracy from 58% to 70%. Accuracy further increases to 80% with the inclusion of clause types. These results, which are competitive with existing systems, hold across several modes of written discourse and suggest that features of information structure are an important consideration in the machine learnability of discourse.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nicholas Asher</author>
<author>Alex Lascarides</author>
</authors>
<title>Logics of Conversation.</title>
<date>2003</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="3521" citStr="Asher and Lascarides, 2003" startWordPosition="523" endWordPosition="526"> and Echihabi, 2002; Lapata and Lascarides, 2004). A wide range of accuracies are also reported - 33.96% (Marcu and Echihabi, 2002) to 70.70% (Lapata and Lascarides, 2004) for all rhetorical relations and, for individual relations, CONTRAST (43.64%) and CONTINUATION (83.35%) (Sporleder and Lascarides, 2005). 44 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 44–48, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics We seek to predict the inventory of rhetorical relations defined in Segmented Discourse Representation Theory (“SDRT”) (Asher and Lascarides, 2003). In addition to the relations illustrated in (1), we consider: BACKGROUND: It was Christmas. Pascale got a new toy.; EXPLANATION: The aardvark was dirty. It fell into a puddle.; CONSEQUENCE: If the aardvark fell in the puddle, then it got dirty.; ALTERNATION: Pascale got an aardvark or a stuffed bunny.; and CONTINUATION: Pascale got an aardvark. Grimsby got a rawhide. Discourses were selected based on Smith (2003) who defines five primary discourse modes by: (1) the situations (events and states) they describe; (2) the overarching temporality (tense, aspect); and (3) the type of text progress</context>
<context position="8113" citStr="Asher and Lascarides, 2003" startWordPosition="1239" endWordPosition="1242">on normalized to the unit interval; and discourse MODE. We also include CLAUSE type - independent (IC) or dependent clauses (DC) with the inclusion of a discourse marker (M) or not, 45 Table 1: Distribution of Relations by Granularity Type. Relation Number (Avg.) Positive 515 (47%) BACKGROUND 315 (61%) ELABORATION 161 (31%) EXPLANATION 39 (7%) Negative 59 (5%) CONSEQUENCE 16 (26%) RESULT 43 (71%) Maintenance 490 (44%) ALTERNATION 76 (14%) CONTINUATION 30 (6%) NARRATION 384 (78%) embedded (EM) or not - and GRANULARITY shift categories which are an organization of the SDRT rhetorical relations (Asher and Lascarides, 2003), summarized in Table 1. All 25 discourses were annotated by one of the authors using only a reference sheet. The other author independently coded 80% of the data (20 discourses, four from each mode). Average agreement and Cohen’s Kappa (Cohen, 1960) statistics were computed and are within acceptable ranges: TENSE (99.65 / .9945), ASPECT (99.30 / .9937), SDRT (77.42 / .6850), and EVENT (75.88 / .6362). These results are consistent with previously reported annotations for rhetorical relations (Sporleder and Lascarides, 2005; Howald and Katz, 2011), event verbs and durations, tense and aspect (P</context>
</contexts>
<marker>Asher, Lascarides, 2003</marker>
<rawString>Nicholas Asher and Alex Lascarides. 2003. Logics of Conversation. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John G Cleary</author>
<author>Leonard E Trigg</author>
</authors>
<title>K*: An Instance-based Learner Using an Entropic Distance Measure.</title>
<date>1995</date>
<booktitle>In Proceedings of the 12 International Conference on Machine Learning,</booktitle>
<pages>108--113</pages>
<contexts>
<context position="10340" citStr="Cleary and Trigg, 1995" startWordPosition="1600" endWordPosition="1603">ch clause as a vector of information (SDRT, EVENT, TENSE, ASPECT, SEQUENCE, CLAUSE, MODE, GRANULARITY), illustrated in (3)1: (3) a. The Pittsburgh Steelers needed to win. START, State, Pa., N, .200, IC, NA, start b. Batch took the first snap. ELAB., Occ., Pa., N, .400, IC, NA, pos. c. Then he threw the ball into the endzone. NAR., Asp., Pa., N, .600, IC-M, NA, main. d. Ward caught the ball. NAR., Occ., Pa., N, .800, IC, NA, main. e. A touchdown was scored. RESULT, Occ., Pa., Perf., 1.00, IC, NA, neg. We report results from the Naive Bayes (NB), J48 (C4.5 decision tree (Quinlan, 1993)) and K* (Cleary and Trigg, 1995) classifiers, run at 10-fold crossvalidation. 4 Results Table 2 indicates that the best average accuracy for the Uncollapsed task is 58.99 (J48). The accuracy increases to 70.18 (K*) for the Collapsed task. The accuracy increases further to 80.00 (NB) for the Combined task. All accuracies are statistically significant over majority class baselines (“MCB”): Uncollapsed (MCB = 35) - x2 = 15.11, d.f. = 0, p &lt; .001; Collapsed (MCB = 41) - x2 = 20.51, d.f. = 0, p &lt; .001; and Combined (treating the best Collapsed accuracy as the new baseline (MCB = 70)) - x2 = 1.43, d.f. = 0, p &lt; .001. As shown in T</context>
</contexts>
<marker>Cleary, Trigg, 1995</marker>
<rawString>John G. Cleary and Leonard E. Trigg 1995. K*: An Instance-based Learner Using an Entropic Distance Measure. In Proceedings of the 12 International Conference on Machine Learning, 108–113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A Coefficient of Agreement for Nominal Scales. Educational and Psychological Measurement,</title>
<date>1960</date>
<contexts>
<context position="8363" citStr="Cohen, 1960" startWordPosition="1284" endWordPosition="1285">ositive 515 (47%) BACKGROUND 315 (61%) ELABORATION 161 (31%) EXPLANATION 39 (7%) Negative 59 (5%) CONSEQUENCE 16 (26%) RESULT 43 (71%) Maintenance 490 (44%) ALTERNATION 76 (14%) CONTINUATION 30 (6%) NARRATION 384 (78%) embedded (EM) or not - and GRANULARITY shift categories which are an organization of the SDRT rhetorical relations (Asher and Lascarides, 2003), summarized in Table 1. All 25 discourses were annotated by one of the authors using only a reference sheet. The other author independently coded 80% of the data (20 discourses, four from each mode). Average agreement and Cohen’s Kappa (Cohen, 1960) statistics were computed and are within acceptable ranges: TENSE (99.65 / .9945), ASPECT (99.30 / .9937), SDRT (77.42 / .6850), and EVENT (75.88 / .6362). These results are consistent with previously reported annotations for rhetorical relations (Sporleder and Lascarides, 2005; Howald and Katz, 2011), event verbs and durations, tense and aspect (Puscasu and Mititelu, 2008; Wiebe et al., 1997). Positive, negative and maintained granularities were not annotated, but MM report a Kappa between .8500 and 1. The distribution of these granularities, based on the organization of the annotated rhetori</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Jacob Cohen. 1960. A Coefficient of Agreement for Nominal Scales. Educational and Psychological Measurement, 20(1):37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Paul Grice</author>
</authors>
<title>Logic and Conversation.</title>
<date>1975</date>
<journal>Speech Acts,</journal>
<booktitle>In Syntax and Semantics,</booktitle>
<volume>3</volume>
<pages>43--85</pages>
<publisher>Academic Press,</publisher>
<location>New York.</location>
<marker>Grice, 1975</marker>
<rawString>H. Paul Grice. 1975. Logic and Conversation. In Syntax and Semantics, Vol. 3, Speech Acts, 43–85. Academic Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
</authors>
<title>On The Coherence and Structure of Discourse.</title>
<date>1985</date>
<tech>CSLI Technical Report,</tech>
<pages>85--37</pages>
<contexts>
<context position="1016" citStr="Hobbs, 1985" startWordPosition="140" endWordPosition="141">into different granularity categories (based on relative degree of detail) increases average prediction accuracy from 58% to 70%. Accuracy further increases to 80% with the inclusion of clause types. These results, which are competitive with existing systems, hold across several modes of written discourse and suggest that features of information structure are an important consideration in the machine learnability of discourse. 1 Introduction The rhetorical relations that hold between clauses in discourse index temporal and event information and contribute to a discourse’s pragmatic coherence (Hobbs, 1985). For example, in (1) the NARRATION relation holds between (1a) and (1b) as (1b) temporally follows (1a) at event time. (1) a. Pascale closed the toy chest. b. She walked to the gate. c. The gate was locked securely. d. So she couldn’t get into the kitchen. The ELABORATION relation, describing the surrounding state of affairs, holds between (1b) and (1c). (1c) is temporally inclusive (subordinated) with (1b) and there is no temporal progression at event time. The RESULT relation holds between (1bc) and (1d). (1d) follows (1b) and its subordinated ELABORATION relation (1c) at event time. Additi</context>
</contexts>
<marker>Hobbs, 1985</marker>
<rawString>Jerry R. Hobbs. 1985. On The Coherence and Structure of Discourse. CSLI Technical Report, CSLI-85-37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Blake Stephen Howald</author>
<author>Graham Katz</author>
</authors>
<title>The Exploitation of Spatial Information in Narrative Discourse.</title>
<date>2011</date>
<booktitle>In Proceedings of the Ninth International Workshop on Computational Semantics,</booktitle>
<pages>175--184</pages>
<contexts>
<context position="8665" citStr="Howald and Katz, 2011" startWordPosition="1327" endWordPosition="1330">nization of the SDRT rhetorical relations (Asher and Lascarides, 2003), summarized in Table 1. All 25 discourses were annotated by one of the authors using only a reference sheet. The other author independently coded 80% of the data (20 discourses, four from each mode). Average agreement and Cohen’s Kappa (Cohen, 1960) statistics were computed and are within acceptable ranges: TENSE (99.65 / .9945), ASPECT (99.30 / .9937), SDRT (77.42 / .6850), and EVENT (75.88 / .6362). These results are consistent with previously reported annotations for rhetorical relations (Sporleder and Lascarides, 2005; Howald and Katz, 2011), event verbs and durations, tense and aspect (Puscasu and Mititelu, 2008; Wiebe et al., 1997). Positive, negative and maintained granularities were not annotated, but MM report a Kappa between .8500 and 1. The distribution of these granularities, based on the organization of the annotated rhetorical relations is presented in Table 1. 3.2 Machine Learning Three supervised machine learning tasks were constructed to predict SDRT relations. The first task (Uncollapsed) created a 8-way classifier to predict the SDRT relations based on the feature set, omitting the GRANULARITY feature. The second t</context>
</contexts>
<marker>Howald, Katz, 2011</marker>
<rawString>Blake Stephen Howald and Graham Katz. 2011. The Exploitation of Spatial Information in Narrative Discourse. In Proceedings of the Ninth International Workshop on Computational Semantics, 175–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
<author>Alex Lascarides</author>
</authors>
<title>Inferring Sentence Internal Temporal Relations.</title>
<date>2004</date>
<booktitle>In Proceedings of the North American Association of Computational Linguistics (NAACL-04) 2004,</booktitle>
<pages>153--160</pages>
<contexts>
<context position="2943" citStr="Lapata and Lascarides, 2004" startWordPosition="441" endWordPosition="444">ized as follows. Section 2 reviews prior research on rhetorical relations, discourse structure, granularity and prediction. Section 3 discusses the analyzed data, the selection and annotation of features, and the construction of several machine learning tasks. Section 4 provides the results which are then discussed in Section 5. 2 Background Rhetorical relation prediction has received considerable attention and has been shown to be useful for text summarization (Marcu, 1998). Prediction tasks rely on a number of features (discourse connectives, part of speech, etc.) (Marcu and Echihabi, 2002; Lapata and Lascarides, 2004). A wide range of accuracies are also reported - 33.96% (Marcu and Echihabi, 2002) to 70.70% (Lapata and Lascarides, 2004) for all rhetorical relations and, for individual relations, CONTRAST (43.64%) and CONTINUATION (83.35%) (Sporleder and Lascarides, 2005). 44 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 44–48, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics We seek to predict the inventory of rhetorical relations defined in Segmented Discourse Representation Theory (“SDRT”) (Asher and Lascarides, 2003). In addition to the r</context>
</contexts>
<marker>Lapata, Lascarides, 2004</marker>
<rawString>Mirella Lapata and Alex Lascarides. 2004. Inferring Sentence Internal Temporal Relations. In Proceedings of the North American Association of Computational Linguistics (NAACL-04) 2004, 153–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>Improving Summarization Through Rhetorical Parsing Tuning.</title>
<date>1998</date>
<booktitle>In Proceedings of The 6th Workshop on Very Large Corpora,</booktitle>
<pages>206--215</pages>
<contexts>
<context position="2794" citStr="Marcu, 1998" startWordPosition="420" endWordPosition="421">, event)) serves as a viable organization and predictor of rhetorical relations in a range of written discourses. This paper is organized as follows. Section 2 reviews prior research on rhetorical relations, discourse structure, granularity and prediction. Section 3 discusses the analyzed data, the selection and annotation of features, and the construction of several machine learning tasks. Section 4 provides the results which are then discussed in Section 5. 2 Background Rhetorical relation prediction has received considerable attention and has been shown to be useful for text summarization (Marcu, 1998). Prediction tasks rely on a number of features (discourse connectives, part of speech, etc.) (Marcu and Echihabi, 2002; Lapata and Lascarides, 2004). A wide range of accuracies are also reported - 33.96% (Marcu and Echihabi, 2002) to 70.70% (Lapata and Lascarides, 2004) for all rhetorical relations and, for individual relations, CONTRAST (43.64%) and CONTINUATION (83.35%) (Sporleder and Lascarides, 2005). 44 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 44–48, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics We seek to predict t</context>
</contexts>
<marker>Marcu, 1998</marker>
<rawString>Daniel Marcu. 1998. Improving Summarization Through Rhetorical Parsing Tuning. In Proceedings of The 6th Workshop on Very Large Corpora, 206–215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Abdessamad Echihabi</author>
</authors>
<title>An Unsupervised Approach to Recognizing Discourse Relations.</title>
<date>2002</date>
<booktitle>In Proceedings of the Association of Computational Linguistics (ACL-02) 2002,</booktitle>
<pages>368--375</pages>
<contexts>
<context position="2913" citStr="Marcu and Echihabi, 2002" startWordPosition="437" endWordPosition="440">urses. This paper is organized as follows. Section 2 reviews prior research on rhetorical relations, discourse structure, granularity and prediction. Section 3 discusses the analyzed data, the selection and annotation of features, and the construction of several machine learning tasks. Section 4 provides the results which are then discussed in Section 5. 2 Background Rhetorical relation prediction has received considerable attention and has been shown to be useful for text summarization (Marcu, 1998). Prediction tasks rely on a number of features (discourse connectives, part of speech, etc.) (Marcu and Echihabi, 2002; Lapata and Lascarides, 2004). A wide range of accuracies are also reported - 33.96% (Marcu and Echihabi, 2002) to 70.70% (Lapata and Lascarides, 2004) for all rhetorical relations and, for individual relations, CONTRAST (43.64%) and CONTINUATION (83.35%) (Sporleder and Lascarides, 2005). 44 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 44–48, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics We seek to predict the inventory of rhetorical relations defined in Segmented Discourse Representation Theory (“SDRT”) (Asher and Lascaride</context>
</contexts>
<marker>Marcu, Echihabi, 2002</marker>
<rawString>Daniel Marcu and Abdessamad Echihabi. 2002. An Unsupervised Approach to Recognizing Discourse Relations. In Proceedings of the Association of Computational Linguistics (ACL-02) 2002, 368–375.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rutu Mulkar-Mehta</author>
<author>Jerry R Hobbs</author>
<author>Eduard Hovy</author>
</authors>
<title>Granulairty in Natural Language Discourse.</title>
<date>2011</date>
<booktitle>In Proceedings of the Ninth International Conference on Computational Semantics (IWCS</booktitle>
<pages>195--204</pages>
<marker>Mulkar-Mehta, Hobbs, Hovy, 2011</marker>
<rawString>Rutu Mulkar-Mehta, Jerry R. Hobbs and Eduard Hovy. 2011. Granulairty in Natural Language Discourse. In Proceedings of the Ninth International Conference on Computational Semantics (IWCS 2011) 2011, 195– 204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Puscasu</author>
<author>Verginica Mititelu</author>
</authors>
<title>Annotation of WordNet Verbs with TimeML Event Classes.</title>
<date>2008</date>
<booktitle>Proceedings of the Sixth International Language Resources and Evaluation (LREC08)</booktitle>
<contexts>
<context position="8738" citStr="Puscasu and Mititelu, 2008" startWordPosition="1338" endWordPosition="1341">), summarized in Table 1. All 25 discourses were annotated by one of the authors using only a reference sheet. The other author independently coded 80% of the data (20 discourses, four from each mode). Average agreement and Cohen’s Kappa (Cohen, 1960) statistics were computed and are within acceptable ranges: TENSE (99.65 / .9945), ASPECT (99.30 / .9937), SDRT (77.42 / .6850), and EVENT (75.88 / .6362). These results are consistent with previously reported annotations for rhetorical relations (Sporleder and Lascarides, 2005; Howald and Katz, 2011), event verbs and durations, tense and aspect (Puscasu and Mititelu, 2008; Wiebe et al., 1997). Positive, negative and maintained granularities were not annotated, but MM report a Kappa between .8500 and 1. The distribution of these granularities, based on the organization of the annotated rhetorical relations is presented in Table 1. 3.2 Machine Learning Three supervised machine learning tasks were constructed to predict SDRT relations. The first task (Uncollapsed) created a 8-way classifier to predict the SDRT relations based on the feature set, omitting the GRANULARITY feature. The second task (Collapsed) created a 3-way classifier to predict the GRANULARITY cat</context>
</contexts>
<marker>Puscasu, Mititelu, 2008</marker>
<rawString>Georgiana Puscasu and Verginica Mititelu. 2008. Annotation of WordNet Verbs with TimeML Event Classes. Proceedings of the Sixth International Language Resources and Evaluation (LREC08)</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Jos´e Casta˜no</author>
<author>Robert Ingria</author>
<author>Roser Saur</author>
<author>Robert Gaizauskas</author>
<author>Andrea Setzer</author>
<author>Graham Katz</author>
</authors>
<title>TimeML: Robust Specification of Event and Temporal Expressions in Text.</title>
<date>2005</date>
<booktitle>In Proceedings of the Fith International Conference on Computational Semantics (IWCS</booktitle>
<marker>Pustejovsky, Casta˜no, Ingria, Saur, Gaizauskas, Setzer, Katz, 2005</marker>
<rawString>James Pustejovsky, Jos´e Casta˜no, Robert Ingria, Roser Saur, Robert Gaizauskas, Andrea Setzer, and Graham Katz. 2005. TimeML: Robust Specification of Event and Temporal Expressions in Text. In Proceedings of the Fith International Conference on Computational Semantics (IWCS 2005)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ross Quinlan</author>
</authors>
<title>C4.5: Programsfor Machine Learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="10307" citStr="Quinlan, 1993" startWordPosition="1596" endWordPosition="1597">nk, 2005) and treated each clause as a vector of information (SDRT, EVENT, TENSE, ASPECT, SEQUENCE, CLAUSE, MODE, GRANULARITY), illustrated in (3)1: (3) a. The Pittsburgh Steelers needed to win. START, State, Pa., N, .200, IC, NA, start b. Batch took the first snap. ELAB., Occ., Pa., N, .400, IC, NA, pos. c. Then he threw the ball into the endzone. NAR., Asp., Pa., N, .600, IC-M, NA, main. d. Ward caught the ball. NAR., Occ., Pa., N, .800, IC, NA, main. e. A touchdown was scored. RESULT, Occ., Pa., Perf., 1.00, IC, NA, neg. We report results from the Naive Bayes (NB), J48 (C4.5 decision tree (Quinlan, 1993)) and K* (Cleary and Trigg, 1995) classifiers, run at 10-fold crossvalidation. 4 Results Table 2 indicates that the best average accuracy for the Uncollapsed task is 58.99 (J48). The accuracy increases to 70.18 (K*) for the Collapsed task. The accuracy increases further to 80.00 (NB) for the Combined task. All accuracies are statistically significant over majority class baselines (“MCB”): Uncollapsed (MCB = 35) - x2 = 15.11, d.f. = 0, p &lt; .001; Collapsed (MCB = 41) - x2 = 20.51, d.f. = 0, p &lt; .001; and Combined (treating the best Collapsed accuracy as the new baseline (MCB = 70)) - x2 = 1.43, </context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>Ross Quinlan. 1993 C4.5: Programsfor Machine Learning. Morgan Kaufmann, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlota Smith</author>
</authors>
<title>Modes of Discourse: The Local Structure of Texts.</title>
<date>2003</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="3939" citStr="Smith (2003)" startWordPosition="594" endWordPosition="595">2012 Association for Computational Linguistics We seek to predict the inventory of rhetorical relations defined in Segmented Discourse Representation Theory (“SDRT”) (Asher and Lascarides, 2003). In addition to the relations illustrated in (1), we consider: BACKGROUND: It was Christmas. Pascale got a new toy.; EXPLANATION: The aardvark was dirty. It fell into a puddle.; CONSEQUENCE: If the aardvark fell in the puddle, then it got dirty.; ALTERNATION: Pascale got an aardvark or a stuffed bunny.; and CONTINUATION: Pascale got an aardvark. Grimsby got a rawhide. Discourses were selected based on Smith (2003) who defines five primary discourse modes by: (1) the situations (events and states) they describe; (2) the overarching temporality (tense, aspect); and (3) the type of text progression (temporal - text and event time progression are similar; atemporal - text and event time progression are not similar). These contrastive elements inform the features selected for the machine learning tasks discussed in Section 3.2. The five modes, narratives, reports (news articles), description (recipes), information (scientific essays), and argument (editorials) were selected to ensure a balanced range of the</context>
</contexts>
<marker>Smith, 2003</marker>
<rawString>Carlota Smith. 2003. Modes of Discourse: The Local Structure of Texts. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Sporleder</author>
<author>Alex Lascarides</author>
</authors>
<title>Exploiting Linguistic Cues to Classify Rhetorical Relations.</title>
<date>2005</date>
<booktitle>In Proceedings of Recent Advances in Natural Language Processing (RANLP-05),</booktitle>
<pages>532--539</pages>
<contexts>
<context position="3202" citStr="Sporleder and Lascarides, 2005" startWordPosition="480" endWordPosition="483">sks. Section 4 provides the results which are then discussed in Section 5. 2 Background Rhetorical relation prediction has received considerable attention and has been shown to be useful for text summarization (Marcu, 1998). Prediction tasks rely on a number of features (discourse connectives, part of speech, etc.) (Marcu and Echihabi, 2002; Lapata and Lascarides, 2004). A wide range of accuracies are also reported - 33.96% (Marcu and Echihabi, 2002) to 70.70% (Lapata and Lascarides, 2004) for all rhetorical relations and, for individual relations, CONTRAST (43.64%) and CONTINUATION (83.35%) (Sporleder and Lascarides, 2005). 44 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 44–48, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics We seek to predict the inventory of rhetorical relations defined in Segmented Discourse Representation Theory (“SDRT”) (Asher and Lascarides, 2003). In addition to the relations illustrated in (1), we consider: BACKGROUND: It was Christmas. Pascale got a new toy.; EXPLANATION: The aardvark was dirty. It fell into a puddle.; CONSEQUENCE: If the aardvark fell in the puddle, then it got dirty.; ALTERNATION: Pascale got an aardv</context>
<context position="8641" citStr="Sporleder and Lascarides, 2005" startWordPosition="1323" endWordPosition="1326">ift categories which are an organization of the SDRT rhetorical relations (Asher and Lascarides, 2003), summarized in Table 1. All 25 discourses were annotated by one of the authors using only a reference sheet. The other author independently coded 80% of the data (20 discourses, four from each mode). Average agreement and Cohen’s Kappa (Cohen, 1960) statistics were computed and are within acceptable ranges: TENSE (99.65 / .9945), ASPECT (99.30 / .9937), SDRT (77.42 / .6850), and EVENT (75.88 / .6362). These results are consistent with previously reported annotations for rhetorical relations (Sporleder and Lascarides, 2005; Howald and Katz, 2011), event verbs and durations, tense and aspect (Puscasu and Mititelu, 2008; Wiebe et al., 1997). Positive, negative and maintained granularities were not annotated, but MM report a Kappa between .8500 and 1. The distribution of these granularities, based on the organization of the annotated rhetorical relations is presented in Table 1. 3.2 Machine Learning Three supervised machine learning tasks were constructed to predict SDRT relations. The first task (Uncollapsed) created a 8-way classifier to predict the SDRT relations based on the feature set, omitting the GRANULARI</context>
</contexts>
<marker>Sporleder, Lascarides, 2005</marker>
<rawString>Caroline Sporleder and Alex Lascarides. 2005. Exploiting Linguistic Cues to Classify Rhetorical Relations. In Proceedings of Recent Advances in Natural Language Processing (RANLP-05), 532–539.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Sporleder</author>
<author>Alex Lascarides</author>
</authors>
<title>Using Automatically Labelled Examples to Classify Rhetorical Relations: An Assessment. Natural Language Engineering,</title>
<date>2008</date>
<pages>14--369</pages>
<marker>Sporleder, Lascarides, 2008</marker>
<rawString>Caroline Sporleder and Alex Lascarides. 2008. Using Automatically Labelled Examples to Classify Rhetorical Relations: An Assessment. Natural Language Engineering, 14:369–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Thomas O’Hara</author>
<author>Thorsten ¨Ohrstr¨omSandgren</author>
<author>Kenneth McKeever</author>
</authors>
<title>An Empirical Approach to Temporal Reference Resolution.</title>
<date>1997</date>
<booktitle>In Proceedings of the 2nd Conference on Empirical Methods in Natural Language Processing (EMNLP97),</booktitle>
<pages>174--186</pages>
<marker>Wiebe, O’Hara, ¨Ohrstr¨omSandgren, McKeever, 1997</marker>
<rawString>Janyce Wiebe, Thomas O’Hara, Thorsten ¨Ohrstr¨omSandgren and Kenneth McKeever. 1997. An Empirical Approach to Temporal Reference Resolution. In Proceedings of the 2nd Conference on Empirical Methods in Natural Language Processing (EMNLP97), 174–186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Witten</author>
<author>Eibe Frank</author>
</authors>
<date>2005</date>
<booktitle>Data Mining: Practical Machine Learning Techniques with Java Implementation (2nd Ed.)</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="9702" citStr="Witten and Frank, 2005" startWordPosition="1487" endWordPosition="1490"> to predict SDRT relations. The first task (Uncollapsed) created a 8-way classifier to predict the SDRT relations based on the feature set, omitting the GRANULARITY feature. The second task (Collapsed) created a 3-way classifier to predict the GRANULARITY categories (the SDRT feature was omitted). The third task (Combined) included Table 2: Relation Prediction - Combined Modes. Feature J48 K* NB MCB Uncollapsed 58.99 55.41 56.69 35 Collapsed 69.90 70.18 69.81 41 Combined 78.62 71.92 80.00 35 (70) the GRANULARITY feature back into the Uncollapsed 8-way classifier. We utilized the WEKA toolkit (Witten and Frank, 2005) and treated each clause as a vector of information (SDRT, EVENT, TENSE, ASPECT, SEQUENCE, CLAUSE, MODE, GRANULARITY), illustrated in (3)1: (3) a. The Pittsburgh Steelers needed to win. START, State, Pa., N, .200, IC, NA, start b. Batch took the first snap. ELAB., Occ., Pa., N, .400, IC, NA, pos. c. Then he threw the ball into the endzone. NAR., Asp., Pa., N, .600, IC-M, NA, main. d. Ward caught the ball. NAR., Occ., Pa., N, .800, IC, NA, main. e. A touchdown was scored. RESULT, Occ., Pa., Perf., 1.00, IC, NA, neg. We report results from the Naive Bayes (NB), J48 (C4.5 decision tree (Quinlan, </context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>Ian Witten and Eibe Frank. 2005. Data Mining: Practical Machine Learning Techniques with Java Implementation (2nd Ed.) Morgan Kaufmann, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Webber</author>
</authors>
<title>Genre Distictions for Discourse in the Penn TreeBank.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th ACL Conference,</booktitle>
<pages>674--682</pages>
<marker>Webber, 2009</marker>
<rawString>Bonnie Webber 2009. Genre Distictions for Discourse in the Penn TreeBank. In Proceedings of the 47th ACL Conference, 674–682.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>