<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000022">
<title confidence="0.9980485">
KB-LDA: Jointly Learning a Knowledge Base of Hierarchy, Relations,
and Facts
</title>
<author confidence="0.997417">
Dana Movshovitz-Attias
</author>
<affiliation confidence="0.982083">
Computer Science Department
Carnegie Mellon University
</affiliation>
<email confidence="0.998542">
dma@cs.cmu.edu
</email>
<sectionHeader confidence="0.994801" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999831318181818">
Many existing knowledge bases (KBs), in-
cluding Freebase, Yago, and NELL, rely
on a fixed ontology, given as an input
to the system, which defines the data to
be cataloged in the KB, i.e., a hierar-
chy of categories and relations between
them. The system then extracts facts that
match the predefined ontology. We pro-
pose an unsupervised model that jointly
learns a latent ontological structure of an
input corpus, and identifies facts from the
corpus that match the learned structure.
Our approach combines mixed member-
ship stochastic block models and topic
models to infer a structure by jointly mod-
eling text, a latent concept hierarchy, and
latent semantic relationships among the
entities mentioned in the text. As a case
study, we apply the model to a corpus
of Web documents from the software do-
main, and evaluate the accuracy of the var-
ious components of the learned ontology.
</bodyText>
<sectionHeader confidence="0.998781" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9991415625">
Knowledge base (KB) construction methods can
be broadly categorized along several dimensions.
One dimension is ontology-guided construction,
where the list of categories and relations that de-
fine the schema of the KB are explicit, versus open
IE methods, where they are not. Another dimen-
sion is the type of relations and types included in
the KB: some KBs, like WordNet, are hierarchi-
cal, in that they contain mainly concept types, su-
pertypes and instances, while other KBs contain
many types of relationships between concepts. Hi-
erarchical knowledge can be learned by methods
including distributional clustering (Pereira et al.,
1993), as well as Hearst patterns (Hearst, 1992)
and similar techniques (Snow et al., 2006). Re-
verb (Fader et al., 2011) and TextRunner (Yates
</bodyText>
<note confidence="0.906107333333333">
William W. Cohen
Machine Learning Department
Carnegie Mellon University
</note>
<email confidence="0.978473">
wcohen@cs.cmu.edu
</email>
<bodyText confidence="0.999837292682927">
et al., 2007) are open methods for learning multi-
relation KBs. Finally, NELL (Carlson et al., 2010;
Mitchell et al., 2015), FreeBase (Google, 2011)
and Yago (Suchanek et al., 2007; Hoffart et al.,
2013) are ontology-guided methods for extracting
KBs containing both hierarchies and relations.
One advantage of ontology-guided methods is
that the extracted knowledge is easier to reason
with. An advantage of open IE methods is that
ontologies may be incomplete, and are expensive
to construct for a new domain. Ontology design
involves assembling a set of categories, organized
in a meaningful hierarchical structure, often pro-
viding seeds, i.e., representative examples for each
category, and finally, defining inter-category rela-
tions. This process is often done manually (Carl-
son et al., 2010) leading to a rigid set of categories.
Redesigning a new ontology for a specialized do-
main represents an additional challenge as it re-
quires extensive knowledge of the domain.
In this paper, we propose an unsupervised
model that learns a latent hierarchical structure
of categories from an input corpus, learns latent
semantic relations between categories, and also
identifies facts from the corpus that match the
learned structure. In other words, the model learns
both the schema for a KB, and a set of facts that
are related to that schema, thus combining the
processes of KB population and ontology con-
struction. The intent is to build systems that ex-
tract facts which can be interpreted relative to a
meaningful ontology without requiring the effort
of manual ontology construction.
The input to the learning method is a cor-
pus of documents, plus two sets of resources ex-
tracted from the same corpus: a set of hypernym-
hyponym pairs (e.g., “animal”, “horse”) extracted
using Hearst patterns, and a set of subject-verb-
object triples (e.g., “horse”, “eats”, “hay”) ex-
tracted from parsed sentences. These resources
are analogous to the output of open IE systems for
</bodyText>
<page confidence="0.962442">
1449
</page>
<note confidence="0.977276">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1449–1459,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999720764705882">
hierarchies and relations, and as we demonstrate,
our method can be used to highlight domain-
specific data from open IE repositories.
Our approach combines mixed membership
stochastic block models and topic models to in-
fer a structure by jointly modeling text docu-
ments, and links that indicate hierarchy and rela-
tion among the entities mentioned in the text. Joint
modeling allows information on topics of nouns
(referred to as instances) and verbs (referred to
as relations) to be shared between text documents
and an ontological structure, resulting in a set of
compelling topics. This model offers a complete
solution for KB construction based on an input
corpus, and we therefore name it KB-LDA.
We additionally propose a method for recover-
ing meaningful names for concepts in the learned
hierarchy. These are equivalent to category names
in other KBs, however, following our method we
extract from the data a set of potential alterna-
tive concepts describing each category, including
probabilities for their strength of association.
To show the effectiveness of our method, we ap-
ply the model to a dataset of Web based documents
from the software domain, and learn a software
KB. This is an example of a specialized domain in
which, to our knowledge, no broad-coverage on-
tology exists. We evaluate the model on the in-
duced categories, relations, and facts, and we com-
pare the proposed categories with an independent
set of human-provided labels for documents. Fi-
nally, we use KB-LDA to retrieve domain-specific
relations from an open IE resource. We provide
the learned software KB as supplemental material.
</bodyText>
<sectionHeader confidence="0.97755" genericHeader="introduction">
2 KB-LDA
</sectionHeader>
<bodyText confidence="0.9999406">
Modeling latent sets of entities from observed in-
teractions among them is a well researched task,
often encountered in social network analysis for
the purpose of identifying specialized communi-
ties in the network. Mixed Membership Stochas-
tic Blockmodels (Airoldi et al., 2009; Parkkinen
et al., 2009) model entities as graph nodes with
pairwise relations drawn from latent blocks with
mixed membership. A related approach is taken
by topic models such as LDA (Latent Dirichlet
Allocation; (Blei et al., 2003)), which model doc-
uments as generated by a mixture of latent topics,
and words in the documents as generated by topic-
specific word distributions. The KB-LDA model
combines the two approaches. It models links be-
</bodyText>
<equation confidence="0.992621423076923">
πO – multinomial over ontology topic pairs, with Dirichlet
prior αO
πR – multinomial over relation topic tuples, with Dirichlet
prior αR
Od – topic multinomial for document d, with Dirichlet
prior αD
ak – multinomial over instances for topic k, with Dirichlet
prior γI
δk, – multinomial over relations for topic k&apos;, with Dirichlet
prior γR
CIi = hCi, Iii – i-th ontological assignment pair
SVOj = hSj, Oj, Vji – j-th relation assignment tuple
zCI
i = hzCi, zIii – topic pair chosen for example hCi, Iii
zSVO j= hzSj , zOj, zVji – topic tuple chosen for example
hSj, Oj, Vji
zDE1, zDE2 – topic chosen for instance entity E1, or relation
entity E2, respectively, in a document
nIz,i – number of times instance i is observed under topic
z (in either zD, zCI or zSV O)
nRz,r – number of times relation r is observed under topic
z (in either zD or zSV O)
nO (zc,zi) – count of ontological pairs assigned the topic
pair hzc, zii (in zCI)
nR(zs,zo,zv) – count of relation tuples assigned the topic
tuple hzs, zo, zvi (in zSV O)
</equation>
<tableCaption confidence="0.926519">
Table 1: KB-LDA notation.
</tableCaption>
<bodyText confidence="0.994101241379311">
tween tuples of two or three entities using stochas-
tic block models, and these are additionally influ-
enced by latent topic assignments of the entities in
a document corpus.
In the KB-LDA model, shown as a plate dia-
gram in Figure 1 with notation in Table 1, informa-
tion is shared between three components, through
common latent topics over noun and verb entities.
The Ontology component (upper right) models
hierarchical links between Concept-Instance (CI)
entity pairs. The Relations component (left) mod-
els links between Subject-Verb-Object (SVO) en-
tity triples, where the subject and object are nouns
and the verb represents a relation between them.
Finally, the Documents component (lower left) is
a link-LDA model (Erosheva et al., 2004) of text
documents containing a combination of noun and
verb entity types. In this formulation, distribu-
tions over noun and verb entities that are related
according to hierarchical or relational constraints,
are linked with a text model via shared parameters.
In more detail, the Documents component pro-
vides the context in which noun and verb entities
are being used in text. It is modeled as an exten-
sion of LDA, viewing documents as sets of “bags
of words”, where in this case, each bag contains
either noun or verb entities. Each entity type has a
topic-wise multinomial distribution over the set of
entities in the vocabulary of that type.
</bodyText>
<page confidence="0.993909">
1450
</page>
<figureCaption confidence="0.999583">
Figure 1: Plate Diagram of KB-LDA.
</figureCaption>
<figure confidence="0.992629242424242">
γI
αO
zCi
Ci
πO
Relations
Ontology
zSj
Sj
σk
K
Ii
zIi
NO
αR
πR
zOj
Oj
Documents
zVj
Vj
El1
zEl1
αD
NR
δkr
Nd,I
K
θd
El2
zEl2
γR
Nd,R ND
</figure>
<bodyText confidence="0.998786216216216">
The Ontology component is a generative model
representing hierarchal links between pairs of
nouns. The examples for this component are ex-
tracted using a small collection of Hearst patterns
indicating concept-instance or concept-concept
links, including, ’X such as Y’, and ’X including
Y’. For example, the sentence “websites such as
StackOverflow” indicates that Stackoverflow is a
type of website, leading to the extracted noun pair
(websites, StackOverflow). We refer to the exam-
ples extracted using these hierarchical patterns as
concept-instance pairs, and to the individual enti-
ties as instances.
The pairs have an underlying block structure
derived from a sparse block model (Parkkinen et
al., 2009). They are generated by topic specific
instance distributions conditioned on topic pair
edges, which are defined by the multinomial πO
over the Cartesian product of the noun topic set
with itself. The individual instances, therefore,
have a mixed membership in topics. Note that
we allow for a concept and instance to be drawn
from different noun topics, defined by σ. For ex-
ample, we may learn a topic highlighting concept
tokens like ’websites’, ’platforms’, ’applications’.
Another topic can highlight instances shared by
these concepts, such as, ’stackoverflow’, ’google’,
and ’facebook’. Finally, the observation that the
former topic frequently contains concepts of in-
stances from the latter topic, is encoded in the
multinomial distribution πO. From this we infer
that the former topic should be placed higher in
the induced hierarchy.
Similarly, the Relations component represents
relational links between a noun subject, a verb and
a noun object. The examples for this component
Let K be the number of target latent topics.
</bodyText>
<listItem confidence="0.999354571428572">
1. Generate topics: For topic k E 1, ... , K, sample:
• σk — Dirichlet(γI), the per-topic instance distribution
• δk — Dirichlet(γR), the per-topic relation distribution
2. Generate ontology: Sample πO — Dirichlet(αO), the
instance topic pair distribution.
• For each concept-instance pair CIi, i E 1, ... , NO:
– Sample topic pair zCI
</listItem>
<equation confidence="0.765824">
i — Multinomial(πO)
</equation>
<listItem confidence="0.9257348125">
– Sample instances Ci — Multinomial(σzCi ), Ii —
Multinomial(σzIi ), then CIi = (Ci, Ii)
3. Generate relations: Sample πR — Dirichlet(αR), the
relation topic tuple distribution.
• For each tuple SVOj, j E 1, ... , NR:
– Sample topic tuple zSV O j— Multinomial(πR)
– Sample instances, Sj — Multinomial(σzSj ), Oj —
Multinomial(σzOj ), and sample a relation Vj —
Multinomial(δzVj )
4. Generate documents: For document d E 1, ... ,D:
• Sample θd — Dirichlet(αD), the topic mixing distri-
bution for document d.
• For every noun entity (El1) and verb entity (El2), l1 E
1,...,Nd,I, l2E1,...,Nd,R:
– Sample topics zEl1, zEl2 —Multinomial(θd)
– Sample entities El1 — Multinomial(σzEl1) and
</listItem>
<equation confidence="0.607404">
El2 — Multinomial(δzEl2 )
</equation>
<tableCaption confidence="0.803902">
Table 2: KB-LDA generative process.
</tableCaption>
<bodyText confidence="0.9999145">
are extracted from SVO patterns found in the doc-
ument corpus, following Talukdar et al. (2012).
An extracted example looks like: (websites, ex-
ecute, javascript). Subject and object topics are
drawn from the noun topics (σ), while the verb
topics is drawn from the verb topics, defined by
δ. The multinomial πR encodes the interaction of
noun and verb topics based on the extracted rela-
tional links, and it is defined over the Cartesian
product of the noun topic set with itself and with
</bodyText>
<page confidence="0.953455">
1451
</page>
<bodyText confidence="0.993117166666667">
the verb topic set.
The generative process of KB-LDA is de-
scribed in Table 2. Given the hyperparameters
(αO, αR, αD, γI, γR), the joint distribution over
CI pairs, SVO tuples, documents, topics and topic
assignments is given by
</bodyText>
<equation confidence="0.986079428571429">
p(πO, πR, σ, δ, CI, zCI, SVO, zSV O, θ, E, zD|
αO, αR, αD,γI,γR) =
K K
Y Dir(σk|γI) × Y Dir(δk0|γR)× (1)
k=1 k0=1
hzCi,zIii Ci Ii
πO σzCi σzIi ×
</equation>
<bodyText confidence="0.9998136">
We sample a latent topic for an entity mention
in a document from the text corpus conditioned
on the assignments to all other entity mentions af-
ter collapsing θd. The following expression shows
topic sampling for a noun entity in a document:
</bodyText>
<equation confidence="0.9934126">
ˆp(zEl1|E, CI, SVO, zD, zCI, zSV O, αD, γI) (4)
nI¬l1
zEl1,El1 + γI
PE0l1 nI¬l1
zEl1,E0 l1 + TIγI
</equation>
<bodyText confidence="0.99883975">
The per-topic multinomial parameters and topic
distributions of CI pairs, SVO tuples and docu-
ments can be recovered with MLE estimates using
their observation counts:
</bodyText>
<equation confidence="0.998958921052632">
Dir(πO|αO) YNO
i=1
∝ (n¬l1
d,z + αD)
hzSj ,zOj ,zVj i
πzSj σOj
σSj zOj δVj
zVj ×
R
NR
Dir(πR|αR) Y
j=1
nI k,I + γI nR k,R + γR
ˆσI k = ˆδR k =
P , P
I0 nI k,I0 + TIγI R0 nRk,R0 + TRγR
D
θzEl1 El1
d σzD
El1
zD
θd δEl2
El2 zD
El2
Nd,RY
l2=1
NDY Dir(θd|αD) Nd,IY
d=1 l1=1
ˆθzd = nz,d + αD
P
z0 nz0,d + KαD
nO hzC,zIi + αO
ˆπhzC,zIi =
O
P
nO00+ K2· αO
hzC,zIi
z0C,z0I
</equation>
<subsectionHeader confidence="0.705758">
2.1 Inference in KB-LDA
</subsectionHeader>
<bodyText confidence="0.999513777777778">
Exact inference is intractable in the KB-LDA
model. We use a collapsed Gibbs sampler (Grif-
fiths and Steyvers, 2004) to perform approximate
inference in order to query the topic distributions
and assignments. It samples a latent topic pair for
a CI pair in the corpus conditioned on the assign-
ments to all other CI pairs, SVO tuples, and docu-
ment entities, using the following expression, after
collapsing πO:
</bodyText>
<equation confidence="0.99828675">
ˆp(zCI i|CIi, zCI ¬i ,zSV O, zD, CI¬i, αO, γI) (2)
� �
∝ nO¬i ×
zCI
i + αO
(nI¬i I¬i
zCi,Ci + γI)(nzIi,Ii + γI)
Ci
z
nI¬iC + TIγI)(P nI¬i
zIi,I + TIγI)
I
</equation>
<bodyText confidence="0.999786">
where counts of observations from the training set
are noted by n (see Table 1), and TI is the number
of instance entities (size of noun vocabulary).
We similarly sample topics for each SVO tuple
conditioned on the assignments to all other tuples,
CI pairs and document entities, using the follow-
ing expression, after collapsing πR:
</bodyText>
<equation confidence="0.997794066666667">
ˆp(zSV O
j |SVOj, zSV O
¬j , zCI, zD, SVO¬j, αR, YI, YR) (3)
∝ CnsvO + αR/ ×
j
(nI¬j
zSj ,Sj + YI)(nI¬j
zOj ,Oj + YI)(nR¬j
zVj ,Vj + YR)
nzS%,I+TIYI)(Enzo,,I+TIYI)(En Vj3&apos;V +TRYR)
nR
hzS,zO,zV i + αR
ˆπhzS,zO,zV i = P
R nR hz0 S,z0 O,z0 V i + K3 · αR
z0S,z0O,z0V
</equation>
<bodyText confidence="0.999991916666667">
Using the KB-LDA model we can describe the
latent topic hierarchy underlying the input cor-
pus. We consider the multinomial of the Ontology
component, πO, as an adjacency matrix describ-
ing a network where the nodes are instance topics
and edges indicate a hypernym-to-hyponym rela-
tion. By extracting the maximum spanning tree
over this adjacency matrix, we recover a hierarchy
over the input data. We recover relations among
instance topics by extracting from the Relations
multinomial, πR, the set of most probable tuples
of a hsubject topic, verb topic, object topici.
Our model is implemented using a fast, parallel
approximation of collapsed Gibbs sampling, fol-
lowing Newman et al. (2009). In each sampling
iteration, topics are sampled locally on a subset of
the training examples. At the end of each iteration,
data from worker threads is joined and model pa-
rameters are updated with complete information.
In the next iteration, thread-local sampling starts
with complete topic assignment information from
the previous iteration. In each thread, the process
can be viewed as a reordering of the input exam-
ples, where the examples sampled in that thread
</bodyText>
<equation confidence="0.844626333333333">
(P
(�
I
</equation>
<page confidence="0.937094">
1452
</page>
<bodyText confidence="0.999937076923077">
are viewed first. It has been shown that parallel ap-
proaches considerably speed up iterative inference
methods such as collapsed Gibbs sampling, result-
ing in test data log probabilities indistinguishable
from those obtained using serial methods (Porte-
ous et al., 2008; Newman et al., 2009). A paral-
lel approach is especially important when training
the KB-LDA model due to the large dimensions
of the multinomials of the Ontology and Relations
components (K2 and K3, respectively for a model
with K topics). We train KB-LDA over 2000 iter-
ations, more than what has traditionally been used
for collapsed Gibbs samplers.
</bodyText>
<subsectionHeader confidence="0.998566">
2.2 Data-driven discovery of topic concepts
</subsectionHeader>
<bodyText confidence="0.999982944444444">
The KB-LDA model described above clusters
noun entities into sets of instance topics, and re-
covers a latent hierarchical structure among these
topics. Each instance topic can be described by a
multinomial distribution of the underlying nouns.
It is often more intuitive, however, to refer to a
topic containing a set of high probability nouns by
a name, or category, just as traditional ontologies
describe hierarchies over categories.
Our model is trained over nouns that originate
from concept-instance example pairs (used to train
the Ontology component). We describe a method
for selecting a category name for a topic, based on
concepts that best represent high probability nouns
of the topic in the concept-instance examples.
We calculate the probability that a concept noun
c describes the set of instances I that have been
assigned the topic z using
</bodyText>
<equation confidence="0.9334305">
p(c, zJI) a p(IJc, z) * p(c, z) (5)
= p(IJc, z) * p(zJc) * p(c)
</equation>
<bodyText confidence="0.9101505">
Let rep(c, z) = Pi:Ci=c nIz,Ii describe how well
concept c represents topic z according to the as-
signments of instances with concept c to the topic.
Then,
</bodyText>
<equation confidence="0.9980205">
z c rep(c, z) (6)
p(�) _ Ez, rep(c, zI)
</equation>
<bodyText confidence="0.999559">
The concept prior, p(c), is based on the relative
weight of instances with concept c in the concept-
instance example set, and is an indicator of the
generality of a concept:
</bodyText>
<equation confidence="0.775546">
p(c) = Pi:Ci=c wc,Ii (7)
c,Pi:Ci=c, wc,,Ii
</equation>
<bodyText confidence="0.99886225">
where wC,I is the number of occurrences of
concept-instance pair
Finally, p(IJc, z) measures how specific are the
topic instances to the concept c,
</bodyText>
<equation confidence="0.997269">
p(IJ c, z) = PPi: CiCc w�wi Ii /Z (8)
</equation>
<bodyText confidence="0.999966181818182">
where I is the set of training instances assigned
with topic z, and Z is a normalizer over all con-
cepts and topics.
Following this method we extract concepts that
have a high probability p(c, zJI) with respect to a
topic z. These can be thought of as equivalent to
the single, fixed, category name provided by tra-
ditional KB ontologies; however, here we extract
from the data a set of potential alternative noun
phrases describing each topic, including a proba-
bility for the strength of this association.
</bodyText>
<sectionHeader confidence="0.998698" genericHeader="method">
3 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.9997206">
We evaluate the KB-LDA model on a corpus
of 5.5M documents from the software domain;
thereby we are using the model to construct a soft-
ware domain knowledge base. Our evaluation ex-
plores the following questions:
</bodyText>
<listItem confidence="0.990302428571428">
• Can KB-LDA learn categories, relations, a
hierarchy and topic concepts with high pre-
cision?
• How well do KB-LDA topics correspond
with human-provided document labels?
• Is KB-LDA useful in extracting facts from
existing open IE resources?
</listItem>
<subsectionHeader confidence="0.980334">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.999991714285714">
We use data from the Q&amp;A website StackOver-
flow1 where users ask and answer technical ques-
tions about software development, tools, algo-
rithms, etc’. We extracted 562K concept-instance
example pairs from the data, and kept the 17K ex-
amples appearing at least twice. Noun phrases
in these examples make up our Instance Dictio-
nary. Out of 6.8M SVO examples found in the
data we keep 37K in which the subject and ob-
ject are in the Instance Dictionary, and the exam-
ple appears at least twice in the corpus. The verbs
in these SVOs make up our Relation Dictionary.
Finally, we consider as documents the 5.5M ques-
tions from StackOverflow with all their answers.
</bodyText>
<subsectionHeader confidence="0.999472">
3.2 Evaluating the learned KB precision
</subsectionHeader>
<bodyText confidence="0.9962155">
In this section we evaluate the direct output of a
model trained with 50 topics: the extracted in-
</bodyText>
<footnote confidence="0.983377">
1Data source: https://archive.org/details/stackexchange
</footnote>
<equation confidence="0.545849">
(C, I) in the corpus.
</equation>
<page confidence="0.974623">
1453
</page>
<figureCaption confidence="0.82492625">
Figure 2: Average Match (top) and Group (bot-
tom) precision of top tokens of 50 topics learned
with KB-LDA, according to expert (dark blue) and
non-expert (light blue, stripes) labeling.
</figureCaption>
<bodyText confidence="0.9973227">
stance topics, topic hierarchy, relations among
topics and extracted topic concepts. In each of
the experiments below, we extract facts based on
one of the learned components and evaluate each
fact based on annotations from human judges: two
experts and three non-expert users, collected us-
ing Mechanical Turk, that were pre-tested on a
basic familiarity with concepts from the software
domain, such as programming languages, version
control systems, and databases.
</bodyText>
<subsectionHeader confidence="0.961334">
3.2.1 Precision of Instance Topics
</subsectionHeader>
<bodyText confidence="0.999345344827586">
We measure the coherence of instance topics us-
ing an approach called word intrusion (Chang et
al., 2009). We extract the top 30 instance tokens
of a topic ranked by the instance topic multinomial
σ. We present to workers tokens 1-5,6-10,...,26-
30, where each 5 tokens are randomly ordered and
augmented with an extra token that is ranked low
for the topic, (the intruder). We ask workers to
select all tokens that do not belong in the group
(and at least one). We define the topic Match Pre-
cision as the fraction of questions for which the
reviewer identified the correct intruder (out of 6
questions per topic), and the topic Group Precision
as the fraction of correct tokens (those not selected
as not belonging in the group). Thus Match Pre-
cision measures how well labelers understand the
topic, and Group Precision measures what fraction
of words appeared relevant to the topic.
Figure 2 shows the average Match and Group
precision over the top tokens of all 50 topics
learned with the model, as evaluated by expert and
non-expert workers. Both groups find the intruder
token in over 75% of questions. In the more subtle
task of validating each topic token (Group preci-
sion) we see a greater variance among the two la-
beler groups. This highlights the difficulty of eval-
uating domain specific facts with non-expert users.
Table 3 displays the top 20 instance topics learned
with KB-LDA, ranked by expert Group precision.
</bodyText>
<subsectionHeader confidence="0.9334">
3.2.2 Precision of Topic Concepts
</subsectionHeader>
<bodyText confidence="0.999648818181818">
We assess the precision of the top 5 concept names
proposed for instance topics, following the method
presented in Section 2.2. Top concepts for a sub-
set of topics are shown in Table 3. For each topic,
we present to the user a hypernym-hyponym pat-
tern of the topic based on the top concepts and top
instances of the topic. As an example, if the top 5
instances of a topic are ie, firefox, chrome, buttons,
safari and the top 5 concepts for this topic are web
browsers, web browser, browser, ie, chrome, the
pattern presented to workers is
</bodyText>
<listItem confidence="0.8877305">
• [ie, firefox, chrome, buttons, safari] is a [web browsers,
web browser, browser, ie, chrome]
</listItem>
<bodyText confidence="0.997710909090909">
Workers were asked to match at least 3 instances
to a proposed concept name. In addition, the same
assessment was applied for each topic using ran-
domly sampled concepts. We present in Table 4
the number and precision of patterns based on ex-
tracted concepts (Concepts) and random concepts
(Random), that were labeled by 1, 2 or 3 workers,
as well as the average results among experts. We
achieve nearly 90% precision according to expert
labeling, however we do not observe large agree-
ment among non-expert labelers.
</bodyText>
<subsectionHeader confidence="0.987247">
3.2.3 Precision of Relations
</subsectionHeader>
<bodyText confidence="0.999577142857143">
To assess the precision of the relations learned in
the KB-LDA model, we extract the top 100 rela-
tions learned according to their probability in the
relation multinomial πR. Relation patterns were
presented to workers as sets of the top subject-
verb-object tokens of the respective topics in the
relation. An example relation is
</bodyText>
<listItem confidence="0.999852333333333">
• Subject words: [user, users, people, customer, client]
• Verb words: [clicks, selects, submits, click, hits]
• Object words: [function, method, class, object, query]
</listItem>
<bodyText confidence="0.99953">
and workers are asked to state whether the pat-
tern indicates a valid relation or not, by check-
ing whether a reasonable number of combinations
of subject-verb-object triples extracted from each
of the relation groups can produce valid relations.
</bodyText>
<figure confidence="0.999366388888889">
1 1 2 2
a
.
.
.
.
.
.
.
1 1 2 2
r
1.
.
.
.
.
.
.
</figure>
<page confidence="0.952888">
1454
</page>
<tableCaption confidence="0.865745608695652">
Top 2 Topic Concepts Top 10 Topic Tokens
table, key table, query, database, sql, column, data, tables, mysql, index, columns
properties, css image, code, images, problem, point, color, data, size, screen, points
credentials, user information name, images, id, number, text, password, address, strings, files, string
page, content page, html, code, file, image, javascript, browser, http, jquery, js
orm tools, orm tool tomcat, hibernate, server, boost, apache, spring, mongodb, framework, nhibernate, png
clients, apps app, application, http, android, device, phone, code, api, iphone, google
applications, systems devices, systems, applications, services, platforms, tools, sites, apps, system, service
systems, platforms google, windows, linux, facebook, git, ant, database, gmail, android, so
limits, limit memory, time, thread, code, threads, process, file, program, data, object
data, table query, table, data, list, example, number, results, search, database, rows
type, value code, function, value, type, pointer, array, memory, compiler, example, string
table, request data, information, types, properties, details, fields, values, content, resources, attributes
dependencies, jar file libraries, library, framework, frameworks, formats, format, database, databases, tools, server
type, object value, focus, place, property, method, reference, interface, effect, pointer, data
kinds, code languages, language, features, objects, functions, methods, code, operations, structures, types
element, elements button, form, link, item, file, mouse, image, value, option, row
javascript libraries, javascript framework jquery, mysql, http, json, xml, library, html, sqlite, asp, php
process, operating system server, client, connection, data, http, socket, message, request, port, service
folder, files file, files, directory, folder, path, code, name, resources, project, folders
value, array array, list, value, values, number, string, code, elements, loop, object
Table 3: Top 20 instance topics learned with KB-LDA. For each topic we show the top 2 concepts
recovered for the topic, and top 10 tokens. In italics are words marked as out-of-topic by expert labelers.
</tableCaption>
<table confidence="0.999083333333333">
Workers Concepts Relations Subsumptions
KB-LDA (p) Random (p) KB-LDA (p) Random (p) KB-LDA (p) Random (p)
1 48 (0.96) 6 (0.12) 90 (0.9) 69 (0.69) 31 (0.63) 28 (0.57)
2 42 (0.84) 0 (0.0) 63 (0.63) 22 (0.22) 16 (0.33) 9 (0.18)
3 26 (0.52) 0 (0.0) 15 (0.15) 4 (0.05) 3 (0.06) 4 (0.08)
Experts 44 (0.88) 0 (0.0) 70 (0.7) 13 (0.13) 25 (0.51) 4 (0.08)
</table>
<tableCaption confidence="0.995087">
Table 4: Precision of topic concepts, relations, and subsumptions. For items extracted from the model
</tableCaption>
<bodyText confidence="0.924159692307692">
(KB-LDA), and randomly (Random), we show the number of items marked as correct, and precision in
parentheses (p), as labeled by 1, 2, or 3 non-expert workers, and the average precision by experts.
We present in Table 4 the number and precision of
patterns based on the top 100 relations (Relations)
and 100 random relations (Random), that were la-
beled by 1, 2 or 3 workers, and the average results
among experts. We achieve 80% precision accord-
ing to experts, and only 18% on random relations.
We observe similar agreement among expert and
non-expert workers as in the concept evaluation
experiment, however we note that random rela-
tions prove more confusing for non-experts and
more of them are (falsely) labeled as correct.
</bodyText>
<subsectionHeader confidence="0.978032">
3.2.4 Precision of Hierarchy
</subsectionHeader>
<bodyText confidence="0.99956425">
We assess the precision of subsumption relations
making up the ontology hierarchy. These are ex-
tracted using the maximum spanning tree over the
graph represented by the Ontology component, πO
(see Section 2.1 for details), resulting in 49 sub-
sumption relations. We compare their quality to
that of 49 randomly sampled subsumption rela-
tions. Subsumptions are presented to the worker
using is a patterns, similar to the ones described
above for concept evaluation, however in this case,
the concept tokens are the top tokens of the hyper-
nym topic. An example subsumption relation is
</bodyText>
<listItem confidence="0.730505">
• [java, python, javascript, lists, ruby] is a [languages,
language, features, objects, functions]
</listItem>
<bodyText confidence="0.9998785">
The results shown in Table 4 indicate a low pre-
cision among the extracted subsumption relations.
This might be explained by the fact that at the final
training iteration (2K) of the model, the perplexity
of the Ontology component was still improving,
while the perplexity of the other model compo-
nents seemed closer to convergence. It is possible
that the low precision observed here indicates that
more training iterations are needed to achieve an
accurate ontology using KB-LDA.
</bodyText>
<page confidence="0.997348">
1455
</page>
<tableCaption confidence="0.992592555555555">
Topic string, character, characters, text, line
Tags regex, string, python, php, ruby
Topic element, div, css, elements, http
Tags css, html, jquery, html5, javascript
Topic table, query, database, sql, column
Tags sql, mysql, database, performance, php
Topic jquery, mysql, http, json, xml
Tags jquery, json, javascript, ruby, string
Table 5: Top tags associated with sample topics.
</tableCaption>
<subsectionHeader confidence="0.9777875">
3.3 Overlap of KB-LDA topics with
human-provided labels
</subsectionHeader>
<bodyText confidence="0.993348952380953">
We evaluated how well topics from KB-LDA cor-
respond to document labels provided by humans,
over a randomly sampled set of 40K documents
from our corpus. In StackOverflow, questions
(which we consider as documents) can be labeled
with predefined tags. Here, we estimate the over-
lap with the most frequently used tags. First, for
topic k, we aggregate tags from documents where
k = argmaxk&apos; Bk&apos;
d , where Bd is the document topic
distribution. Table 5 shows examples of the top
tags associated with sample topics, indicating a
good correlation between top topic words and the
underlying concepts.
Next, for each tested document d E D, let Wd
be the top 30 words of the most probable topic in
Bd, and Td the set of human provided document
tags. We consider the following metrics:
measures the ratio of documents for which at least
one tag overlaps with a top topic word. The aver-
age ratio of overlapping tags per document is
</bodyText>
<equation confidence="0.987076">
1 D |t : t E Td n t E Wd|
Tag-Overlap =  |D  |1: |Td|
</equation>
<bodyText confidence="0.967033">
d
As a baseline, we measure similar overlap metrics
using the 30 most frequent instance tokens in the
document corpus. The results in Table 6 indicate
an overlap of nearly half of the 20, 50, 100, and
500 most frequent tags with top topic tokens – sig-
nificantly higher than the overlap with frequent to-
ken. Our evaluation is based on the subset of tags
found in the instance dictionary of KB-LDA.
</bodyText>
<table confidence="0.966884166666667">
Top Found in KB-LDA Frequent Tokens
Tags Dictionary Docs Tag Docs Tag
20 14 0.45 0.42 0.21 0.16
50 36 0.48 0.42 0.20 0.14
100 72 0.45 0.38 0.20 0.13
500 322 0.44 0.33 0.18 0.10
</table>
<tableCaption confidence="0.975336705882353">
Table 6: Docs and Tag overlap of human-provided
tags with KB-LDA topics, and frequent tokens.
Top 10 ranked triples: (server, not found, error),
(user, can access, file), (method, not found, error),
(user, can change, password), (page, not found, error),
(user, can upload, videos), (compiler, will generate,
error), (users, can upload, files), (users, can upload,
files), (object, not found, error)
Bottom 10 ranked triples: (france, will visit,
germany), (utilities, may include, heat), (iran, has had,
russia), (russia, can stop, germany), (macs, do not
support, windows media player), (cell phones, do not
make, phone calls), (houses, have made, equipment),
(guests, will find, restaurants), (guests, can request,
bbq), (inspectors, do not make, appointments)
Table 7: Top and bottom ReVerb software triples
ranked with KB-LDA.
</tableCaption>
<subsectionHeader confidence="0.952866">
3.4 Extracting facts from an open IE
resource
</subsectionHeader>
<bodyText confidence="0.999988166666667">
We use KB-LDA to extract domain specific triples
from an existing open IE KB, the 15M relations
extracted using ReVerb (Fader et al., 2011) from
ClueWeb09. By extracting the relations in which
the subject, verb and object noun phrases are in-
cluded in the KB-LDA dictionary, we are left with
under 5K triples, indicating the low coverage of
software related triples using open domain extrac-
tion, in comparison with the 37K triples extracted
from StackOverflow and given as an input to KB-
LDA.
Due to word polysemy, many of the 5K
extracted triples are themselves not specific
to the domain. This suggests a hybrid ap-
proach in which KB-LDA is used to rank
open IE triples for relevance to a domain. We
ranked the 5K open triples by the probability
of the triple given a trained KB-LDA model:
</bodyText>
<equation confidence="0.733204">
p(s, v, o) = EK EK EK ko π�ks,kv,ko&gt;σs ksσo koδv kv.
ks kv R
</equation>
<bodyText confidence="0.9989766">
Table 7 shows the top and bottom 10 triples
according to this ranking, which suggests that
the triples ranked higher by KB-LDA are more
relevant to the software domain.
We compare the ranking based on KB-LDA to
</bodyText>
<figure confidence="0.946669166666667">
D
Docs-Overlap = Ed 1{�tETd:tEWd}
|D|
1456
1.0 KB-LDA versus ReVerb Ranking
0.8
0.6
0.4
0.2
0.0
0.0 0.2 0.4 0.6 0.8 1.0
Recall
</figure>
<figureCaption confidence="0.9841675">
Figure 3: Precision-recall curves of rankers of
open IE triples by software relevance, based on
KB-LDA probabilities (blue), and ReVerb confi-
dence (red). A star is pointing the highest F1.
</figureCaption>
<bodyText confidence="0.999719818181818">
a ranking using a confidence score for the triple
as assigned by ReVerb. We manually labeled 500
of the triples according to their relevance to the
software domain, and measured the precision and
recall of the two rankings at any cutoff thresh-
old. Figure 3 shows precision-recall curves for
the two rankings, demonstrating that the ranking
using probabilities based on KB-LDA leads to a
more accurate detection of domain-relevant triples
(with AUC of 0.67 for KB-LDA versus 0.57 for
ReVerb).
</bodyText>
<sectionHeader confidence="0.999963" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999961282051282">
KB-LDA is an extension to LDA and link-LDA
(Blei et al., 2003; Erosheva et al., 2004), model-
ing documents as a mixed membership over en-
tity types with additional annotated metadata, such
as links (Nallapati et al., 2008; Chang and Blei,
2009). It is a generalization of Block-LDA (Bal-
asubramanyan and Cohen, 2011), however, KB-
LDA models two link components, and the input
links have a meaningful semantic correspondence
to a KB structure (hierarchical and relational). In
a related approach, Dalvi et al. (2012) cluster web
table concepts to non-probabilistically create hier-
archies with assigned concept names.
Our work is related to latent tensor representa-
tion of KBs, aimed at enhancing the ontological
structure of existing KBs with relational data in the
form of tensor structures. Nickel et al. (2012) fac-
torized the ontology of Yago 2 for relational learn-
ing. A related approach was using Neural Tensor
Networks to extract new facts from an existing KB
(Chen et al., 2013; Socher et al., 2013). In con-
trast, in KB-LDA, relational data is learned jointly
with the model through the Relations component.
Statistical language models have recently been
adapted for modeling software code and text
documents. Most tasks focused on enhancing
the software development workflow with code
and comment completion (Hindle et al., 2012;
Movshovitz-Attias and Cohen, 2013), learning
coding conventions (Allamanis et al., 2014), and
extracting actionable tasks from software doc-
umentation (Treude et al., 2014). In related
work, specific semantic relations, coordinate re-
lations, have been extracted for a restricted class
of software entities, ones that refer to Java classes
(Movshovitz-Attias and Cohen, 2015). KB-LDA
extends previous work by reasoning over a large
variety of semantic relations among general soft-
ware entities, as found in a document corpus.
</bodyText>
<sectionHeader confidence="0.999623" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999997714285714">
We presented a model that jointly learns a latent
ontological structure of a corpus augmented by re-
lations, and identifies facts matching the learned
structure. The quality of the produced structure
was demonstrated through a series of real-world
evaluations employing human judges, which mea-
sured the semantic coherence of instance topics,
relations, topic concepts, and hierarchy. We fur-
ther validated the semantic meaning of topic con-
cepts, by their correspondence to an independent
source of human-provided document tags. The ex-
perimental evaluation validates the usefulness of
the proposed model for corpus exploration.
The results highlight the benefits of generaliz-
ing pattern-based facts (hypernym-hyponym pairs
and subject-verb-object tuples), using text docu-
ments in a topic model framework. This modular
approach offers opportunities to further improve
an induced KB structure by posing additional con-
straints on corpus entities in the form of additional
components to the model.
</bodyText>
<sectionHeader confidence="0.998027" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9997352">
The authors wish to thank Premkumar Devanbu
and Kathryn Rivard Mazaitis for helpful discus-
sions, and the anonymous reviewers for their in-
sightful comments. This work was funded by NSF
under grant CCF-1414030.
</bodyText>
<note confidence="0.264153">
Precision
</note>
<page confidence="0.991538">
1457
</page>
<sectionHeader confidence="0.983445" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999756152380952">
Edoardo M Airoldi, David M Blei, Stephen E Fienberg,
and Eric P Xing. 2009. Mixed membership stochas-
tic blockmodels. In Advances in Neural Information
Processing Systems, pages 33–40.
Miltiadis Allamanis, Earl T Barr, and Charles Sutton.
2014. Learning natural coding conventions. arXiv
preprint arXiv:1402.4182.
Ramnath Balasubramanyan and William W. Cohen.
2011. Block-lda: Jointly modeling entity-annotated
text and entity-entity links. In Proceedings of the 7th
SIAM International Conference on Data Mining.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. Journal of Ma-
chine Learning Research.
A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E.R.
Hruschka Jr, and T.M. Mitchell. 2010. Toward an
architecture for never-ending language learning. In
Proceedings of the Twenty-Fourth Conference on Ar-
tificial Intelligence (AAAI 2010).
Jonathan Chang and David M Blei. 2009. Rela-
tional topic models for document networks. In In-
ternational Conference on Artificial Intelligence and
Statistics, pages 81–88.
Jonathan Chang, Sean Gerrish, Chong Wang, Jordan L
Boyd-graber, and David M Blei. 2009. Reading
tea leaves: How humans interpret topic models. In
Advances in neural information processing systems,
pages 288–296.
Danqi Chen, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2013. Learning new
facts from knowledge bases with neural tensor net-
works and semantic word vectors. arXiv preprint
arXiv:1301.3618.
Bhavana Bharat Dalvi, William W Cohen, and Jamie
Callan. 2012. Websets: Extracting sets of entities
from the web using unsupervised information ex-
traction. In Proceedings of the fifth ACM interna-
tional conference on Web search and data mining,
pages 243–252. ACM.
Elena Erosheva, Stephen Fienberg, and John Lafferty.
2004. Mixed-membership models of scientific pub-
lications. Proceedings of the National Academy of
Sciences of the United States of America.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 1535–1545. Association for Computational
Linguistics.
Google. 2011. Freebase data dumps.
http://download.freebase.com/datadumps/.
Thomas L Griffiths and Mark Steyvers. 2004. Finding
scientific topics. Proc. of the National Academy of
Sciences of the United States ofAmerica.
Marti A Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th conference on Computational linguistics.
ACL.
Abram Hindle, Earl T Barr, Zhendong Su, Mark Gabel,
and Premkumar Devanbu. 2012. On the naturalness
of software. In Software Engineering (ICSE), 2012
34th International Conference on, pages 837–847.
IEEE.
Johannes Hoffart, Fabian M Suchanek, Klaus
Berberich, and Gerhard Weikum. 2013. Yago2: a
spatially and temporally enhanced knowledge base
from wikipedia. Artificial Intelligence, 194:28–61.
T. Mitchell, W. Cohen, E. Hruschka, P. Talukdar,
J. Betteridge, A. Carlson, B. Dalvi, M. Gardner,
B. Kisiel, J. Krishnamurthy, N. Lao, K. Mazaitis,
T. Mohamed, N. Nakashole, E. Platanios, A. Rit-
ter, M. Samadi, B. Settles, R. Wang, D. Wijaya,
A. Gupta, X. Chen, A. Saparov, M. Greaves, and
J. Welling. 2015. Never-ending learning. In Pro-
ceedings of the Twenty-Ninth AAAI Conference on
Artificial Intelligence (AAAI-15).
Dana Movshovitz-Attias and William W. Cohen. 2013.
Natural language models for predicting program-
ming comments. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics. Association for Computational Linguistics.
Dana Movshovitz-Attias and William W. Cohen. 2015.
Grounded discovery of coordinate term relation-
ships between software entities. arXiv preprint
arXiv:1505.00277.
Ramesh M Nallapati, Amr Ahmed, Eric P Xing, and
William W Cohen. 2008. Joint latent topic mod-
els for text and citations. In Proceedings of the 14th
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 542–550.
ACM.
David Newman, Arthur Asuncion, Padhraic Smyth,
and Max Welling. 2009. Distributed algorithms for
topic models. The Journal of Machine Learning Re-
search, 10:1801–1828.
Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2012. Factorizing yago: scalable machine
learning for linked data. In Proceedings of the 21st
international conference on World Wide Web, pages
271–280. ACM.
Juuso Parkkinen, Janne Sinkkonen, Adam Gyenge, and
Samuel Kaski. 2009. A block model suitable for
sparse graphs. In Proceedings of the 7th Inter-
national Workshop on Mining and Learning with
Graphs (MLG 2009), Leuven.
</reference>
<page confidence="0.836235">
1458
</page>
<reference confidence="0.999858613636363">
Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of english words. In
Proceedings of the 31st annual meeting on Associa-
tion for Computational Linguistics, pages 183–190.
Association for Computational Linguistics.
Ian Porteous, David Newman, Alexander Ihler, Arthur
Asuncion, Padhraic Smyth, and Max Welling. 2008.
Fast collapsed gibbs sampling for latent dirichlet al-
location. In Proceedings of the 14th ACM SIGKDD
international conference on Knowledge discovery
and data mining, pages 569–577. ACM.
Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association for Compu-
tational Linguistics, pages 801–808. Association for
Computational Linguistics.
Richard Socher, Danqi Chen, Christopher D Manning,
and Andrew Ng. 2013. Reasoning with neural ten-
sor networks for knowledge base completion. In Ad-
vances in Neural Information Processing Systems,
pages 926–934.
Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In Proceedings of the 16th international con-
ference on World Wide Web, pages 697–706. ACM.
Partha Pratim Talukdar, Derry Wijaya, and Tom
Mitchell. 2012. Acquiring temporal constraints be-
tween relations. In Proceedings of the 21st ACM
international conference on Information and knowl-
edge management, pages 992–1001. ACM.
Christoph Treude, M Robillard, and Barth´el´emy Dage-
nais. 2014. Extracting development tasks to nav-
igate software documentation. IEEE Transactions
on Software Engineering.
Alexander Yates, Michael Cafarella, Michele Banko,
Oren Etzioni, Matthew Broadhead, and Stephen
Soderland. 2007. Textrunner: open information
extraction on the web. In Proceedings of Human
Language Technologies: The Annual Conference of
the North American Chapter of the Association for
Computational Linguistics: Demonstrations, pages
25–26. Association for Computational Linguistics.
</reference>
<page confidence="0.996055">
1459
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.875019">
<title confidence="0.971681">KB-LDA: Jointly Learning a Knowledge Base of Hierarchy, Relations, and Facts</title>
<author confidence="0.998502">Dana</author>
<affiliation confidence="0.994046">Computer Science Carnegie Mellon</affiliation>
<email confidence="0.999266">dma@cs.cmu.edu</email>
<abstract confidence="0.997205782608696">Many existing knowledge bases (KBs), including Freebase, Yago, and NELL, rely on a fixed ontology, given as an input to the system, which defines the data to be cataloged in the KB, i.e., a hierarchy of categories and relations between them. The system then extracts facts that match the predefined ontology. We propose an unsupervised model that jointly learns a latent ontological structure of an input corpus, and identifies facts from the corpus that match the learned structure. Our approach combines mixed membership stochastic block models and topic models to infer a structure by jointly modeling text, a latent concept hierarchy, and latent semantic relationships among the entities mentioned in the text. As a case study, we apply the model to a corpus of Web documents from the software domain, and evaluate the accuracy of the various components of the learned ontology.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Edoardo M Airoldi</author>
<author>David M Blei</author>
<author>Stephen E Fienberg</author>
<author>Eric P Xing</author>
</authors>
<title>Mixed membership stochastic blockmodels.</title>
<date>2009</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="6068" citStr="Airoldi et al., 2009" startWordPosition="954" endWordPosition="957">ology exists. We evaluate the model on the induced categories, relations, and facts, and we compare the proposed categories with an independent set of human-provided labels for documents. Finally, we use KB-LDA to retrieve domain-specific relations from an open IE resource. We provide the learned software KB as supplemental material. 2 KB-LDA Modeling latent sets of entities from observed interactions among them is a well researched task, often encountered in social network analysis for the purpose of identifying specialized communities in the network. Mixed Membership Stochastic Blockmodels (Airoldi et al., 2009; Parkkinen et al., 2009) model entities as graph nodes with pairwise relations drawn from latent blocks with mixed membership. A related approach is taken by topic models such as LDA (Latent Dirichlet Allocation; (Blei et al., 2003)), which model documents as generated by a mixture of latent topics, and words in the documents as generated by topicspecific word distributions. The KB-LDA model combines the two approaches. It models links beπO – multinomial over ontology topic pairs, with Dirichlet prior αO πR – multinomial over relation topic tuples, with Dirichlet prior αR Od – topic multinomi</context>
</contexts>
<marker>Airoldi, Blei, Fienberg, Xing, 2009</marker>
<rawString>Edoardo M Airoldi, David M Blei, Stephen E Fienberg, and Eric P Xing. 2009. Mixed membership stochastic blockmodels. In Advances in Neural Information Processing Systems, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miltiadis Allamanis</author>
<author>Earl T Barr</author>
<author>Charles Sutton</author>
</authors>
<title>Learning natural coding conventions. arXiv preprint arXiv:1402.4182.</title>
<date>2014</date>
<contexts>
<context position="34927" citStr="Allamanis et al., 2014" startWordPosition="5787" endWordPosition="5790">al. (2012) factorized the ontology of Yago 2 for relational learning. A related approach was using Neural Tensor Networks to extract new facts from an existing KB (Chen et al., 2013; Socher et al., 2013). In contrast, in KB-LDA, relational data is learned jointly with the model through the Relations component. Statistical language models have recently been adapted for modeling software code and text documents. Most tasks focused on enhancing the software development workflow with code and comment completion (Hindle et al., 2012; Movshovitz-Attias and Cohen, 2013), learning coding conventions (Allamanis et al., 2014), and extracting actionable tasks from software documentation (Treude et al., 2014). In related work, specific semantic relations, coordinate relations, have been extracted for a restricted class of software entities, ones that refer to Java classes (Movshovitz-Attias and Cohen, 2015). KB-LDA extends previous work by reasoning over a large variety of semantic relations among general software entities, as found in a document corpus. 5 Conclusions We presented a model that jointly learns a latent ontological structure of a corpus augmented by relations, and identifies facts matching the learned </context>
</contexts>
<marker>Allamanis, Barr, Sutton, 2014</marker>
<rawString>Miltiadis Allamanis, Earl T Barr, and Charles Sutton. 2014. Learning natural coding conventions. arXiv preprint arXiv:1402.4182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ramnath Balasubramanyan</author>
<author>William W Cohen</author>
</authors>
<title>Block-lda: Jointly modeling entity-annotated text and entity-entity links.</title>
<date>2011</date>
<booktitle>In Proceedings of the 7th SIAM International Conference on Data Mining.</booktitle>
<contexts>
<context position="33819" citStr="Balasubramanyan and Cohen, 2011" startWordPosition="5614" endWordPosition="5618">ion and recall of the two rankings at any cutoff threshold. Figure 3 shows precision-recall curves for the two rankings, demonstrating that the ranking using probabilities based on KB-LDA leads to a more accurate detection of domain-relevant triples (with AUC of 0.67 for KB-LDA versus 0.57 for ReVerb). 4 Related Work KB-LDA is an extension to LDA and link-LDA (Blei et al., 2003; Erosheva et al., 2004), modeling documents as a mixed membership over entity types with additional annotated metadata, such as links (Nallapati et al., 2008; Chang and Blei, 2009). It is a generalization of Block-LDA (Balasubramanyan and Cohen, 2011), however, KBLDA models two link components, and the input links have a meaningful semantic correspondence to a KB structure (hierarchical and relational). In a related approach, Dalvi et al. (2012) cluster web table concepts to non-probabilistically create hierarchies with assigned concept names. Our work is related to latent tensor representation of KBs, aimed at enhancing the ontological structure of existing KBs with relational data in the form of tensor structures. Nickel et al. (2012) factorized the ontology of Yago 2 for relational learning. A related approach was using Neural Tensor Ne</context>
</contexts>
<marker>Balasubramanyan, Cohen, 2011</marker>
<rawString>Ramnath Balasubramanyan and William W. Cohen. 2011. Block-lda: Jointly modeling entity-annotated text and entity-entity links. In Proceedings of the 7th SIAM International Conference on Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research.</journal>
<contexts>
<context position="6301" citStr="Blei et al., 2003" startWordPosition="991" endWordPosition="994">fic relations from an open IE resource. We provide the learned software KB as supplemental material. 2 KB-LDA Modeling latent sets of entities from observed interactions among them is a well researched task, often encountered in social network analysis for the purpose of identifying specialized communities in the network. Mixed Membership Stochastic Blockmodels (Airoldi et al., 2009; Parkkinen et al., 2009) model entities as graph nodes with pairwise relations drawn from latent blocks with mixed membership. A related approach is taken by topic models such as LDA (Latent Dirichlet Allocation; (Blei et al., 2003)), which model documents as generated by a mixture of latent topics, and words in the documents as generated by topicspecific word distributions. The KB-LDA model combines the two approaches. It models links beπO – multinomial over ontology topic pairs, with Dirichlet prior αO πR – multinomial over relation topic tuples, with Dirichlet prior αR Od – topic multinomial for document d, with Dirichlet prior αD ak – multinomial over instances for topic k, with Dirichlet prior γI δk, – multinomial over relations for topic k&apos;, with Dirichlet prior γR CIi = hCi, Iii – i-th ontological assignment pair </context>
<context position="33567" citStr="Blei et al., 2003" startWordPosition="5574" endWordPosition="5577">idence (red). A star is pointing the highest F1. a ranking using a confidence score for the triple as assigned by ReVerb. We manually labeled 500 of the triples according to their relevance to the software domain, and measured the precision and recall of the two rankings at any cutoff threshold. Figure 3 shows precision-recall curves for the two rankings, demonstrating that the ranking using probabilities based on KB-LDA leads to a more accurate detection of domain-relevant triples (with AUC of 0.67 for KB-LDA versus 0.57 for ReVerb). 4 Related Work KB-LDA is an extension to LDA and link-LDA (Blei et al., 2003; Erosheva et al., 2004), modeling documents as a mixed membership over entity types with additional annotated metadata, such as links (Nallapati et al., 2008; Chang and Blei, 2009). It is a generalization of Block-LDA (Balasubramanyan and Cohen, 2011), however, KBLDA models two link components, and the input links have a meaningful semantic correspondence to a KB structure (hierarchical and relational). In a related approach, Dalvi et al. (2012) cluster web table concepts to non-probabilistically create hierarchies with assigned concept names. Our work is related to latent tensor representati</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Carlson</author>
<author>J Betteridge</author>
<author>B Kisiel</author>
<author>B Settles</author>
<author>E R Hruschka Jr</author>
<author>T M Mitchell</author>
</authors>
<title>Toward an architecture for never-ending language learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the Twenty-Fourth Conference on Artificial Intelligence (AAAI</booktitle>
<contexts>
<context position="2039" citStr="Carlson et al., 2010" startWordPosition="317" endWordPosition="320">e KB: some KBs, like WordNet, are hierarchical, in that they contain mainly concept types, supertypes and instances, while other KBs contain many types of relationships between concepts. Hierarchical knowledge can be learned by methods including distributional clustering (Pereira et al., 1993), as well as Hearst patterns (Hearst, 1992) and similar techniques (Snow et al., 2006). Reverb (Fader et al., 2011) and TextRunner (Yates William W. Cohen Machine Learning Department Carnegie Mellon University wcohen@cs.cmu.edu et al., 2007) are open methods for learning multirelation KBs. Finally, NELL (Carlson et al., 2010; Mitchell et al., 2015), FreeBase (Google, 2011) and Yago (Suchanek et al., 2007; Hoffart et al., 2013) are ontology-guided methods for extracting KBs containing both hierarchies and relations. One advantage of ontology-guided methods is that the extracted knowledge is easier to reason with. An advantage of open IE methods is that ontologies may be incomplete, and are expensive to construct for a new domain. Ontology design involves assembling a set of categories, organized in a meaningful hierarchical structure, often providing seeds, i.e., representative examples for each category, and fina</context>
</contexts>
<marker>Carlson, Betteridge, Kisiel, Settles, Jr, Mitchell, 2010</marker>
<rawString>A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E.R. Hruschka Jr, and T.M. Mitchell. 2010. Toward an architecture for never-ending language learning. In Proceedings of the Twenty-Fourth Conference on Artificial Intelligence (AAAI 2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Chang</author>
<author>David M Blei</author>
</authors>
<title>Relational topic models for document networks.</title>
<date>2009</date>
<booktitle>In International Conference on Artificial Intelligence and Statistics,</booktitle>
<pages>81--88</pages>
<contexts>
<context position="33748" citStr="Chang and Blei, 2009" startWordPosition="5604" endWordPosition="5607">ir relevance to the software domain, and measured the precision and recall of the two rankings at any cutoff threshold. Figure 3 shows precision-recall curves for the two rankings, demonstrating that the ranking using probabilities based on KB-LDA leads to a more accurate detection of domain-relevant triples (with AUC of 0.67 for KB-LDA versus 0.57 for ReVerb). 4 Related Work KB-LDA is an extension to LDA and link-LDA (Blei et al., 2003; Erosheva et al., 2004), modeling documents as a mixed membership over entity types with additional annotated metadata, such as links (Nallapati et al., 2008; Chang and Blei, 2009). It is a generalization of Block-LDA (Balasubramanyan and Cohen, 2011), however, KBLDA models two link components, and the input links have a meaningful semantic correspondence to a KB structure (hierarchical and relational). In a related approach, Dalvi et al. (2012) cluster web table concepts to non-probabilistically create hierarchies with assigned concept names. Our work is related to latent tensor representation of KBs, aimed at enhancing the ontological structure of existing KBs with relational data in the form of tensor structures. Nickel et al. (2012) factorized the ontology of Yago 2</context>
</contexts>
<marker>Chang, Blei, 2009</marker>
<rawString>Jonathan Chang and David M Blei. 2009. Relational topic models for document networks. In International Conference on Artificial Intelligence and Statistics, pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Chang</author>
<author>Sean Gerrish</author>
<author>Chong Wang</author>
<author>Jordan L Boyd-graber</author>
<author>David M Blei</author>
</authors>
<title>Reading tea leaves: How humans interpret topic models. In Advances in neural information processing systems,</title>
<date>2009</date>
<pages>288--296</pages>
<contexts>
<context position="20889" citStr="Chang et al., 2009" startWordPosition="3492" endWordPosition="3495">es) labeling. stance topics, topic hierarchy, relations among topics and extracted topic concepts. In each of the experiments below, we extract facts based on one of the learned components and evaluate each fact based on annotations from human judges: two experts and three non-expert users, collected using Mechanical Turk, that were pre-tested on a basic familiarity with concepts from the software domain, such as programming languages, version control systems, and databases. 3.2.1 Precision of Instance Topics We measure the coherence of instance topics using an approach called word intrusion (Chang et al., 2009). We extract the top 30 instance tokens of a topic ranked by the instance topic multinomial σ. We present to workers tokens 1-5,6-10,...,26- 30, where each 5 tokens are randomly ordered and augmented with an extra token that is ranked low for the topic, (the intruder). We ask workers to select all tokens that do not belong in the group (and at least one). We define the topic Match Precision as the fraction of questions for which the reviewer identified the correct intruder (out of 6 questions per topic), and the topic Group Precision as the fraction of correct tokens (those not selected as not</context>
</contexts>
<marker>Chang, Gerrish, Wang, Boyd-graber, Blei, 2009</marker>
<rawString>Jonathan Chang, Sean Gerrish, Chong Wang, Jordan L Boyd-graber, and David M Blei. 2009. Reading tea leaves: How humans interpret topic models. In Advances in neural information processing systems, pages 288–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danqi Chen</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning new facts from knowledge bases with neural tensor networks and semantic word vectors. arXiv preprint arXiv:1301.3618.</title>
<date>2013</date>
<contexts>
<context position="34485" citStr="Chen et al., 2013" startWordPosition="5723" endWordPosition="5726">the input links have a meaningful semantic correspondence to a KB structure (hierarchical and relational). In a related approach, Dalvi et al. (2012) cluster web table concepts to non-probabilistically create hierarchies with assigned concept names. Our work is related to latent tensor representation of KBs, aimed at enhancing the ontological structure of existing KBs with relational data in the form of tensor structures. Nickel et al. (2012) factorized the ontology of Yago 2 for relational learning. A related approach was using Neural Tensor Networks to extract new facts from an existing KB (Chen et al., 2013; Socher et al., 2013). In contrast, in KB-LDA, relational data is learned jointly with the model through the Relations component. Statistical language models have recently been adapted for modeling software code and text documents. Most tasks focused on enhancing the software development workflow with code and comment completion (Hindle et al., 2012; Movshovitz-Attias and Cohen, 2013), learning coding conventions (Allamanis et al., 2014), and extracting actionable tasks from software documentation (Treude et al., 2014). In related work, specific semantic relations, coordinate relations, have </context>
</contexts>
<marker>Chen, Socher, Manning, Ng, 2013</marker>
<rawString>Danqi Chen, Richard Socher, Christopher D Manning, and Andrew Y Ng. 2013. Learning new facts from knowledge bases with neural tensor networks and semantic word vectors. arXiv preprint arXiv:1301.3618.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bhavana Bharat Dalvi</author>
<author>William W Cohen</author>
<author>Jamie Callan</author>
</authors>
<title>Websets: Extracting sets of entities from the web using unsupervised information extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of the fifth ACM international conference on Web search and data mining,</booktitle>
<pages>243--252</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="34017" citStr="Dalvi et al. (2012)" startWordPosition="5646" endWordPosition="5649">e detection of domain-relevant triples (with AUC of 0.67 for KB-LDA versus 0.57 for ReVerb). 4 Related Work KB-LDA is an extension to LDA and link-LDA (Blei et al., 2003; Erosheva et al., 2004), modeling documents as a mixed membership over entity types with additional annotated metadata, such as links (Nallapati et al., 2008; Chang and Blei, 2009). It is a generalization of Block-LDA (Balasubramanyan and Cohen, 2011), however, KBLDA models two link components, and the input links have a meaningful semantic correspondence to a KB structure (hierarchical and relational). In a related approach, Dalvi et al. (2012) cluster web table concepts to non-probabilistically create hierarchies with assigned concept names. Our work is related to latent tensor representation of KBs, aimed at enhancing the ontological structure of existing KBs with relational data in the form of tensor structures. Nickel et al. (2012) factorized the ontology of Yago 2 for relational learning. A related approach was using Neural Tensor Networks to extract new facts from an existing KB (Chen et al., 2013; Socher et al., 2013). In contrast, in KB-LDA, relational data is learned jointly with the model through the Relations component. S</context>
</contexts>
<marker>Dalvi, Cohen, Callan, 2012</marker>
<rawString>Bhavana Bharat Dalvi, William W Cohen, and Jamie Callan. 2012. Websets: Extracting sets of entities from the web using unsupervised information extraction. In Proceedings of the fifth ACM international conference on Web search and data mining, pages 243–252. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Erosheva</author>
<author>Stephen Fienberg</author>
<author>John Lafferty</author>
</authors>
<title>Mixed-membership models of scientific publications.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences of the United States of America.</booktitle>
<contexts>
<context position="8299" citStr="Erosheva et al., 2004" startWordPosition="1337" endWordPosition="1340">pic assignments of the entities in a document corpus. In the KB-LDA model, shown as a plate diagram in Figure 1 with notation in Table 1, information is shared between three components, through common latent topics over noun and verb entities. The Ontology component (upper right) models hierarchical links between Concept-Instance (CI) entity pairs. The Relations component (left) models links between Subject-Verb-Object (SVO) entity triples, where the subject and object are nouns and the verb represents a relation between them. Finally, the Documents component (lower left) is a link-LDA model (Erosheva et al., 2004) of text documents containing a combination of noun and verb entity types. In this formulation, distributions over noun and verb entities that are related according to hierarchical or relational constraints, are linked with a text model via shared parameters. In more detail, the Documents component provides the context in which noun and verb entities are being used in text. It is modeled as an extension of LDA, viewing documents as sets of “bags of words”, where in this case, each bag contains either noun or verb entities. Each entity type has a topic-wise multinomial distribution over the set</context>
<context position="33591" citStr="Erosheva et al., 2004" startWordPosition="5578" endWordPosition="5581">r is pointing the highest F1. a ranking using a confidence score for the triple as assigned by ReVerb. We manually labeled 500 of the triples according to their relevance to the software domain, and measured the precision and recall of the two rankings at any cutoff threshold. Figure 3 shows precision-recall curves for the two rankings, demonstrating that the ranking using probabilities based on KB-LDA leads to a more accurate detection of domain-relevant triples (with AUC of 0.67 for KB-LDA versus 0.57 for ReVerb). 4 Related Work KB-LDA is an extension to LDA and link-LDA (Blei et al., 2003; Erosheva et al., 2004), modeling documents as a mixed membership over entity types with additional annotated metadata, such as links (Nallapati et al., 2008; Chang and Blei, 2009). It is a generalization of Block-LDA (Balasubramanyan and Cohen, 2011), however, KBLDA models two link components, and the input links have a meaningful semantic correspondence to a KB structure (hierarchical and relational). In a related approach, Dalvi et al. (2012) cluster web table concepts to non-probabilistically create hierarchies with assigned concept names. Our work is related to latent tensor representation of KBs, aimed at enha</context>
</contexts>
<marker>Erosheva, Fienberg, Lafferty, 2004</marker>
<rawString>Elena Erosheva, Stephen Fienberg, and John Lafferty. 2004. Mixed-membership models of scientific publications. Proceedings of the National Academy of Sciences of the United States of America.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>Identifying relations for open information extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1535--1545</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1828" citStr="Fader et al., 2011" startWordPosition="287" endWordPosition="290">ruction, where the list of categories and relations that define the schema of the KB are explicit, versus open IE methods, where they are not. Another dimension is the type of relations and types included in the KB: some KBs, like WordNet, are hierarchical, in that they contain mainly concept types, supertypes and instances, while other KBs contain many types of relationships between concepts. Hierarchical knowledge can be learned by methods including distributional clustering (Pereira et al., 1993), as well as Hearst patterns (Hearst, 1992) and similar techniques (Snow et al., 2006). Reverb (Fader et al., 2011) and TextRunner (Yates William W. Cohen Machine Learning Department Carnegie Mellon University wcohen@cs.cmu.edu et al., 2007) are open methods for learning multirelation KBs. Finally, NELL (Carlson et al., 2010; Mitchell et al., 2015), FreeBase (Google, 2011) and Yago (Suchanek et al., 2007; Hoffart et al., 2013) are ontology-guided methods for extracting KBs containing both hierarchies and relations. One advantage of ontology-guided methods is that the extracted knowledge is easier to reason with. An advantage of open IE methods is that ontologies may be incomplete, and are expensive to cons</context>
<context position="31761" citStr="Fader et al., 2011" startWordPosition="5259" endWordPosition="5262">) Bottom 10 ranked triples: (france, will visit, germany), (utilities, may include, heat), (iran, has had, russia), (russia, can stop, germany), (macs, do not support, windows media player), (cell phones, do not make, phone calls), (houses, have made, equipment), (guests, will find, restaurants), (guests, can request, bbq), (inspectors, do not make, appointments) Table 7: Top and bottom ReVerb software triples ranked with KB-LDA. 3.4 Extracting facts from an open IE resource We use KB-LDA to extract domain specific triples from an existing open IE KB, the 15M relations extracted using ReVerb (Fader et al., 2011) from ClueWeb09. By extracting the relations in which the subject, verb and object noun phrases are included in the KB-LDA dictionary, we are left with under 5K triples, indicating the low coverage of software related triples using open domain extraction, in comparison with the 37K triples extracted from StackOverflow and given as an input to KBLDA. Due to word polysemy, many of the 5K extracted triples are themselves not specific to the domain. This suggests a hybrid approach in which KB-LDA is used to rank open IE triples for relevance to a domain. We ranked the 5K open triples by the probab</context>
</contexts>
<marker>Fader, Soderland, Etzioni, 2011</marker>
<rawString>Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open information extraction. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1535–1545. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Google</author>
</authors>
<title>Freebase data dumps.</title>
<date>2011</date>
<note>http://download.freebase.com/datadumps/.</note>
<contexts>
<context position="2088" citStr="Google, 2011" startWordPosition="326" endWordPosition="327">they contain mainly concept types, supertypes and instances, while other KBs contain many types of relationships between concepts. Hierarchical knowledge can be learned by methods including distributional clustering (Pereira et al., 1993), as well as Hearst patterns (Hearst, 1992) and similar techniques (Snow et al., 2006). Reverb (Fader et al., 2011) and TextRunner (Yates William W. Cohen Machine Learning Department Carnegie Mellon University wcohen@cs.cmu.edu et al., 2007) are open methods for learning multirelation KBs. Finally, NELL (Carlson et al., 2010; Mitchell et al., 2015), FreeBase (Google, 2011) and Yago (Suchanek et al., 2007; Hoffart et al., 2013) are ontology-guided methods for extracting KBs containing both hierarchies and relations. One advantage of ontology-guided methods is that the extracted knowledge is easier to reason with. An advantage of open IE methods is that ontologies may be incomplete, and are expensive to construct for a new domain. Ontology design involves assembling a set of categories, organized in a meaningful hierarchical structure, often providing seeds, i.e., representative examples for each category, and finally, defining inter-category relations. This proc</context>
</contexts>
<marker>Google, 2011</marker>
<rawString>Google. 2011. Freebase data dumps. http://download.freebase.com/datadumps/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proc. of the National Academy of Sciences of the United States ofAmerica.</booktitle>
<contexts>
<context position="13834" citStr="Griffiths and Steyvers, 2004" startWordPosition="2297" endWordPosition="2301">topic distributions of CI pairs, SVO tuples and documents can be recovered with MLE estimates using their observation counts: Dir(πO|αO) YNO i=1 ∝ (n¬l1 d,z + αD) hzSj ,zOj ,zVj i πzSj σOj σSj zOj δVj zVj × R NR Dir(πR|αR) Y j=1 nI k,I + γI nR k,R + γR ˆσI k = ˆδR k = P , P I0 nI k,I0 + TIγI R0 nRk,R0 + TRγR D θzEl1 El1 d σzD El1 zD θd δEl2 El2 zD El2 Nd,RY l2=1 NDY Dir(θd|αD) Nd,IY d=1 l1=1 ˆθzd = nz,d + αD P z0 nz0,d + KαD nO hzC,zIi + αO ˆπhzC,zIi = O P nO00+ K2· αO hzC,zIi z0C,z0I 2.1 Inference in KB-LDA Exact inference is intractable in the KB-LDA model. We use a collapsed Gibbs sampler (Griffiths and Steyvers, 2004) to perform approximate inference in order to query the topic distributions and assignments. It samples a latent topic pair for a CI pair in the corpus conditioned on the assignments to all other CI pairs, SVO tuples, and document entities, using the following expression, after collapsing πO: ˆp(zCI i|CIi, zCI ¬i ,zSV O, zD, CI¬i, αO, γI) (2) � � ∝ nO¬i × zCI i + αO (nI¬i I¬i zCi,Ci + γI)(nzIi,Ii + γI) Ci z nI¬iC + TIγI)(P nI¬i zIi,I + TIγI) I where counts of observations from the training set are noted by n (see Table 1), and TI is the number of instance entities (size of noun vocabulary). We</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L Griffiths and Mark Steyvers. 2004. Finding scientific topics. Proc. of the National Academy of Sciences of the United States ofAmerica.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th conference on Computational linguistics. ACL.</booktitle>
<contexts>
<context position="1756" citStr="Hearst, 1992" startWordPosition="276" endWordPosition="277">d along several dimensions. One dimension is ontology-guided construction, where the list of categories and relations that define the schema of the KB are explicit, versus open IE methods, where they are not. Another dimension is the type of relations and types included in the KB: some KBs, like WordNet, are hierarchical, in that they contain mainly concept types, supertypes and instances, while other KBs contain many types of relationships between concepts. Hierarchical knowledge can be learned by methods including distributional clustering (Pereira et al., 1993), as well as Hearst patterns (Hearst, 1992) and similar techniques (Snow et al., 2006). Reverb (Fader et al., 2011) and TextRunner (Yates William W. Cohen Machine Learning Department Carnegie Mellon University wcohen@cs.cmu.edu et al., 2007) are open methods for learning multirelation KBs. Finally, NELL (Carlson et al., 2010; Mitchell et al., 2015), FreeBase (Google, 2011) and Yago (Suchanek et al., 2007; Hoffart et al., 2013) are ontology-guided methods for extracting KBs containing both hierarchies and relations. One advantage of ontology-guided methods is that the extracted knowledge is easier to reason with. An advantage of open IE</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 14th conference on Computational linguistics. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abram Hindle</author>
<author>Earl T Barr</author>
<author>Zhendong Su</author>
<author>Mark Gabel</author>
<author>Premkumar Devanbu</author>
</authors>
<title>On the naturalness of software.</title>
<date>2012</date>
<booktitle>In Software Engineering (ICSE), 2012 34th International Conference on,</booktitle>
<pages>837--847</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="34837" citStr="Hindle et al., 2012" startWordPosition="5776" endWordPosition="5779">ture of existing KBs with relational data in the form of tensor structures. Nickel et al. (2012) factorized the ontology of Yago 2 for relational learning. A related approach was using Neural Tensor Networks to extract new facts from an existing KB (Chen et al., 2013; Socher et al., 2013). In contrast, in KB-LDA, relational data is learned jointly with the model through the Relations component. Statistical language models have recently been adapted for modeling software code and text documents. Most tasks focused on enhancing the software development workflow with code and comment completion (Hindle et al., 2012; Movshovitz-Attias and Cohen, 2013), learning coding conventions (Allamanis et al., 2014), and extracting actionable tasks from software documentation (Treude et al., 2014). In related work, specific semantic relations, coordinate relations, have been extracted for a restricted class of software entities, ones that refer to Java classes (Movshovitz-Attias and Cohen, 2015). KB-LDA extends previous work by reasoning over a large variety of semantic relations among general software entities, as found in a document corpus. 5 Conclusions We presented a model that jointly learns a latent ontologica</context>
</contexts>
<marker>Hindle, Barr, Su, Gabel, Devanbu, 2012</marker>
<rawString>Abram Hindle, Earl T Barr, Zhendong Su, Mark Gabel, and Premkumar Devanbu. 2012. On the naturalness of software. In Software Engineering (ICSE), 2012 34th International Conference on, pages 837–847. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Hoffart</author>
<author>Fabian M Suchanek</author>
<author>Klaus Berberich</author>
<author>Gerhard Weikum</author>
</authors>
<title>Yago2: a spatially and temporally enhanced knowledge base from wikipedia.</title>
<date>2013</date>
<journal>Artificial Intelligence,</journal>
<pages>194--28</pages>
<contexts>
<context position="2143" citStr="Hoffart et al., 2013" startWordPosition="334" endWordPosition="337">nd instances, while other KBs contain many types of relationships between concepts. Hierarchical knowledge can be learned by methods including distributional clustering (Pereira et al., 1993), as well as Hearst patterns (Hearst, 1992) and similar techniques (Snow et al., 2006). Reverb (Fader et al., 2011) and TextRunner (Yates William W. Cohen Machine Learning Department Carnegie Mellon University wcohen@cs.cmu.edu et al., 2007) are open methods for learning multirelation KBs. Finally, NELL (Carlson et al., 2010; Mitchell et al., 2015), FreeBase (Google, 2011) and Yago (Suchanek et al., 2007; Hoffart et al., 2013) are ontology-guided methods for extracting KBs containing both hierarchies and relations. One advantage of ontology-guided methods is that the extracted knowledge is easier to reason with. An advantage of open IE methods is that ontologies may be incomplete, and are expensive to construct for a new domain. Ontology design involves assembling a set of categories, organized in a meaningful hierarchical structure, often providing seeds, i.e., representative examples for each category, and finally, defining inter-category relations. This process is often done manually (Carlson et al., 2010) leadi</context>
</contexts>
<marker>Hoffart, Suchanek, Berberich, Weikum, 2013</marker>
<rawString>Johannes Hoffart, Fabian M Suchanek, Klaus Berberich, and Gerhard Weikum. 2013. Yago2: a spatially and temporally enhanced knowledge base from wikipedia. Artificial Intelligence, 194:28–61.</rawString>
</citation>
<citation valid="false">
<authors>
<author>T Mitchell</author>
<author>W Cohen</author>
<author>E Hruschka</author>
<author>P Talukdar</author>
<author>J Betteridge</author>
<author>A Carlson</author>
<author>B Dalvi</author>
<author>M Gardner</author>
<author>B Kisiel</author>
<author>J Krishnamurthy</author>
<author>N Lao</author>
<author>K Mazaitis</author>
<author>T Mohamed</author>
<author>N Nakashole</author>
<author>E Platanios</author>
<author>A Ritter</author>
<author>M Samadi</author>
<author>B Settles</author>
<author>R Wang</author>
<author>D Wijaya</author>
<author>A Gupta</author>
<author>X Chen</author>
<author>A Saparov</author>
<author>M Greaves</author>
<author>J Welling</author>
</authors>
<title>Never-ending learning.</title>
<date>2015</date>
<booktitle>In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-15).</booktitle>
<contexts>
<context position="2063" citStr="Mitchell et al., 2015" startWordPosition="321" endWordPosition="324">ordNet, are hierarchical, in that they contain mainly concept types, supertypes and instances, while other KBs contain many types of relationships between concepts. Hierarchical knowledge can be learned by methods including distributional clustering (Pereira et al., 1993), as well as Hearst patterns (Hearst, 1992) and similar techniques (Snow et al., 2006). Reverb (Fader et al., 2011) and TextRunner (Yates William W. Cohen Machine Learning Department Carnegie Mellon University wcohen@cs.cmu.edu et al., 2007) are open methods for learning multirelation KBs. Finally, NELL (Carlson et al., 2010; Mitchell et al., 2015), FreeBase (Google, 2011) and Yago (Suchanek et al., 2007; Hoffart et al., 2013) are ontology-guided methods for extracting KBs containing both hierarchies and relations. One advantage of ontology-guided methods is that the extracted knowledge is easier to reason with. An advantage of open IE methods is that ontologies may be incomplete, and are expensive to construct for a new domain. Ontology design involves assembling a set of categories, organized in a meaningful hierarchical structure, often providing seeds, i.e., representative examples for each category, and finally, defining inter-cate</context>
</contexts>
<marker>Mitchell, Cohen, Hruschka, Talukdar, Betteridge, Carlson, Dalvi, Gardner, Kisiel, Krishnamurthy, Lao, Mazaitis, Mohamed, Nakashole, Platanios, Ritter, Samadi, Settles, Wang, Wijaya, Gupta, Chen, Saparov, Greaves, Welling, 2015</marker>
<rawString>T. Mitchell, W. Cohen, E. Hruschka, P. Talukdar, J. Betteridge, A. Carlson, B. Dalvi, M. Gardner, B. Kisiel, J. Krishnamurthy, N. Lao, K. Mazaitis, T. Mohamed, N. Nakashole, E. Platanios, A. Ritter, M. Samadi, B. Settles, R. Wang, D. Wijaya, A. Gupta, X. Chen, A. Saparov, M. Greaves, and J. Welling. 2015. Never-ending learning. In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-15).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dana Movshovitz-Attias</author>
<author>William W Cohen</author>
</authors>
<title>Natural language models for predicting programming comments.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="34873" citStr="Movshovitz-Attias and Cohen, 2013" startWordPosition="5780" endWordPosition="5783">with relational data in the form of tensor structures. Nickel et al. (2012) factorized the ontology of Yago 2 for relational learning. A related approach was using Neural Tensor Networks to extract new facts from an existing KB (Chen et al., 2013; Socher et al., 2013). In contrast, in KB-LDA, relational data is learned jointly with the model through the Relations component. Statistical language models have recently been adapted for modeling software code and text documents. Most tasks focused on enhancing the software development workflow with code and comment completion (Hindle et al., 2012; Movshovitz-Attias and Cohen, 2013), learning coding conventions (Allamanis et al., 2014), and extracting actionable tasks from software documentation (Treude et al., 2014). In related work, specific semantic relations, coordinate relations, have been extracted for a restricted class of software entities, ones that refer to Java classes (Movshovitz-Attias and Cohen, 2015). KB-LDA extends previous work by reasoning over a large variety of semantic relations among general software entities, as found in a document corpus. 5 Conclusions We presented a model that jointly learns a latent ontological structure of a corpus augmented by</context>
</contexts>
<marker>Movshovitz-Attias, Cohen, 2013</marker>
<rawString>Dana Movshovitz-Attias and William W. Cohen. 2013. Natural language models for predicting programming comments. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dana Movshovitz-Attias</author>
<author>William W Cohen</author>
</authors>
<title>Grounded discovery of coordinate term relationships between software entities. arXiv preprint arXiv:1505.00277.</title>
<date>2015</date>
<contexts>
<context position="35212" citStr="Movshovitz-Attias and Cohen, 2015" startWordPosition="5828" endWordPosition="5831">odel through the Relations component. Statistical language models have recently been adapted for modeling software code and text documents. Most tasks focused on enhancing the software development workflow with code and comment completion (Hindle et al., 2012; Movshovitz-Attias and Cohen, 2013), learning coding conventions (Allamanis et al., 2014), and extracting actionable tasks from software documentation (Treude et al., 2014). In related work, specific semantic relations, coordinate relations, have been extracted for a restricted class of software entities, ones that refer to Java classes (Movshovitz-Attias and Cohen, 2015). KB-LDA extends previous work by reasoning over a large variety of semantic relations among general software entities, as found in a document corpus. 5 Conclusions We presented a model that jointly learns a latent ontological structure of a corpus augmented by relations, and identifies facts matching the learned structure. The quality of the produced structure was demonstrated through a series of real-world evaluations employing human judges, which measured the semantic coherence of instance topics, relations, topic concepts, and hierarchy. We further validated the semantic meaning of topic c</context>
</contexts>
<marker>Movshovitz-Attias, Cohen, 2015</marker>
<rawString>Dana Movshovitz-Attias and William W. Cohen. 2015. Grounded discovery of coordinate term relationships between software entities. arXiv preprint arXiv:1505.00277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ramesh M Nallapati</author>
<author>Amr Ahmed</author>
<author>Eric P Xing</author>
<author>William W Cohen</author>
</authors>
<title>Joint latent topic models for text and citations.</title>
<date>2008</date>
<booktitle>In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>542--550</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="33725" citStr="Nallapati et al., 2008" startWordPosition="5600" endWordPosition="5603">triples according to their relevance to the software domain, and measured the precision and recall of the two rankings at any cutoff threshold. Figure 3 shows precision-recall curves for the two rankings, demonstrating that the ranking using probabilities based on KB-LDA leads to a more accurate detection of domain-relevant triples (with AUC of 0.67 for KB-LDA versus 0.57 for ReVerb). 4 Related Work KB-LDA is an extension to LDA and link-LDA (Blei et al., 2003; Erosheva et al., 2004), modeling documents as a mixed membership over entity types with additional annotated metadata, such as links (Nallapati et al., 2008; Chang and Blei, 2009). It is a generalization of Block-LDA (Balasubramanyan and Cohen, 2011), however, KBLDA models two link components, and the input links have a meaningful semantic correspondence to a KB structure (hierarchical and relational). In a related approach, Dalvi et al. (2012) cluster web table concepts to non-probabilistically create hierarchies with assigned concept names. Our work is related to latent tensor representation of KBs, aimed at enhancing the ontological structure of existing KBs with relational data in the form of tensor structures. Nickel et al. (2012) factorized</context>
</contexts>
<marker>Nallapati, Ahmed, Xing, Cohen, 2008</marker>
<rawString>Ramesh M Nallapati, Amr Ahmed, Eric P Xing, and William W Cohen. 2008. Joint latent topic models for text and citations. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 542–550. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Newman</author>
<author>Arthur Asuncion</author>
<author>Padhraic Smyth</author>
<author>Max Welling</author>
</authors>
<title>Distributed algorithms for topic models.</title>
<date>2009</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>10--1801</pages>
<contexts>
<context position="15569" citStr="Newman et al. (2009)" startWordPosition="2608" endWordPosition="2611">lying the input corpus. We consider the multinomial of the Ontology component, πO, as an adjacency matrix describing a network where the nodes are instance topics and edges indicate a hypernym-to-hyponym relation. By extracting the maximum spanning tree over this adjacency matrix, we recover a hierarchy over the input data. We recover relations among instance topics by extracting from the Relations multinomial, πR, the set of most probable tuples of a hsubject topic, verb topic, object topici. Our model is implemented using a fast, parallel approximation of collapsed Gibbs sampling, following Newman et al. (2009). In each sampling iteration, topics are sampled locally on a subset of the training examples. At the end of each iteration, data from worker threads is joined and model parameters are updated with complete information. In the next iteration, thread-local sampling starts with complete topic assignment information from the previous iteration. In each thread, the process can be viewed as a reordering of the input examples, where the examples sampled in that thread (P (� I 1452 are viewed first. It has been shown that parallel approaches considerably speed up iterative inference methods such as c</context>
</contexts>
<marker>Newman, Asuncion, Smyth, Welling, 2009</marker>
<rawString>David Newman, Arthur Asuncion, Padhraic Smyth, and Max Welling. 2009. Distributed algorithms for topic models. The Journal of Machine Learning Research, 10:1801–1828.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maximilian Nickel</author>
<author>Volker Tresp</author>
<author>Hans-Peter Kriegel</author>
</authors>
<title>Factorizing yago: scalable machine learning for linked data.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st international conference on World Wide Web,</booktitle>
<pages>271--280</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="34314" citStr="Nickel et al. (2012)" startWordPosition="5692" endWordPosition="5695">s links (Nallapati et al., 2008; Chang and Blei, 2009). It is a generalization of Block-LDA (Balasubramanyan and Cohen, 2011), however, KBLDA models two link components, and the input links have a meaningful semantic correspondence to a KB structure (hierarchical and relational). In a related approach, Dalvi et al. (2012) cluster web table concepts to non-probabilistically create hierarchies with assigned concept names. Our work is related to latent tensor representation of KBs, aimed at enhancing the ontological structure of existing KBs with relational data in the form of tensor structures. Nickel et al. (2012) factorized the ontology of Yago 2 for relational learning. A related approach was using Neural Tensor Networks to extract new facts from an existing KB (Chen et al., 2013; Socher et al., 2013). In contrast, in KB-LDA, relational data is learned jointly with the model through the Relations component. Statistical language models have recently been adapted for modeling software code and text documents. Most tasks focused on enhancing the software development workflow with code and comment completion (Hindle et al., 2012; Movshovitz-Attias and Cohen, 2013), learning coding conventions (Allamanis </context>
</contexts>
<marker>Nickel, Tresp, Kriegel, 2012</marker>
<rawString>Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. 2012. Factorizing yago: scalable machine learning for linked data. In Proceedings of the 21st international conference on World Wide Web, pages 271–280. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juuso Parkkinen</author>
<author>Janne Sinkkonen</author>
<author>Adam Gyenge</author>
<author>Samuel Kaski</author>
</authors>
<title>A block model suitable for sparse graphs.</title>
<date>2009</date>
<booktitle>In Proceedings of the 7th International Workshop on Mining and Learning with Graphs (MLG 2009),</booktitle>
<location>Leuven.</location>
<contexts>
<context position="6093" citStr="Parkkinen et al., 2009" startWordPosition="958" endWordPosition="961">ate the model on the induced categories, relations, and facts, and we compare the proposed categories with an independent set of human-provided labels for documents. Finally, we use KB-LDA to retrieve domain-specific relations from an open IE resource. We provide the learned software KB as supplemental material. 2 KB-LDA Modeling latent sets of entities from observed interactions among them is a well researched task, often encountered in social network analysis for the purpose of identifying specialized communities in the network. Mixed Membership Stochastic Blockmodels (Airoldi et al., 2009; Parkkinen et al., 2009) model entities as graph nodes with pairwise relations drawn from latent blocks with mixed membership. A related approach is taken by topic models such as LDA (Latent Dirichlet Allocation; (Blei et al., 2003)), which model documents as generated by a mixture of latent topics, and words in the documents as generated by topicspecific word distributions. The KB-LDA model combines the two approaches. It models links beπO – multinomial over ontology topic pairs, with Dirichlet prior αO πR – multinomial over relation topic tuples, with Dirichlet prior αR Od – topic multinomial for document d, with D</context>
<context position="9826" citStr="Parkkinen et al., 2009" startWordPosition="1593" endWordPosition="1596">pairs of nouns. The examples for this component are extracted using a small collection of Hearst patterns indicating concept-instance or concept-concept links, including, ’X such as Y’, and ’X including Y’. For example, the sentence “websites such as StackOverflow” indicates that Stackoverflow is a type of website, leading to the extracted noun pair (websites, StackOverflow). We refer to the examples extracted using these hierarchical patterns as concept-instance pairs, and to the individual entities as instances. The pairs have an underlying block structure derived from a sparse block model (Parkkinen et al., 2009). They are generated by topic specific instance distributions conditioned on topic pair edges, which are defined by the multinomial πO over the Cartesian product of the noun topic set with itself. The individual instances, therefore, have a mixed membership in topics. Note that we allow for a concept and instance to be drawn from different noun topics, defined by σ. For example, we may learn a topic highlighting concept tokens like ’websites’, ’platforms’, ’applications’. Another topic can highlight instances shared by these concepts, such as, ’stackoverflow’, ’google’, and ’facebook’. Finally</context>
</contexts>
<marker>Parkkinen, Sinkkonen, Gyenge, Kaski, 2009</marker>
<rawString>Juuso Parkkinen, Janne Sinkkonen, Adam Gyenge, and Samuel Kaski. 2009. A block model suitable for sparse graphs. In Proceedings of the 7th International Workshop on Mining and Learning with Graphs (MLG 2009), Leuven.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Naftali Tishby</author>
<author>Lillian Lee</author>
</authors>
<title>Distributional clustering of english words.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st annual meeting on Association for Computational Linguistics,</booktitle>
<pages>183--190</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1713" citStr="Pereira et al., 1993" startWordPosition="267" endWordPosition="270">(KB) construction methods can be broadly categorized along several dimensions. One dimension is ontology-guided construction, where the list of categories and relations that define the schema of the KB are explicit, versus open IE methods, where they are not. Another dimension is the type of relations and types included in the KB: some KBs, like WordNet, are hierarchical, in that they contain mainly concept types, supertypes and instances, while other KBs contain many types of relationships between concepts. Hierarchical knowledge can be learned by methods including distributional clustering (Pereira et al., 1993), as well as Hearst patterns (Hearst, 1992) and similar techniques (Snow et al., 2006). Reverb (Fader et al., 2011) and TextRunner (Yates William W. Cohen Machine Learning Department Carnegie Mellon University wcohen@cs.cmu.edu et al., 2007) are open methods for learning multirelation KBs. Finally, NELL (Carlson et al., 2010; Mitchell et al., 2015), FreeBase (Google, 2011) and Yago (Suchanek et al., 2007; Hoffart et al., 2013) are ontology-guided methods for extracting KBs containing both hierarchies and relations. One advantage of ontology-guided methods is that the extracted knowledge is eas</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993. Distributional clustering of english words. In Proceedings of the 31st annual meeting on Association for Computational Linguistics, pages 183–190. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Porteous</author>
<author>David Newman</author>
<author>Alexander Ihler</author>
<author>Arthur Asuncion</author>
<author>Padhraic Smyth</author>
<author>Max Welling</author>
</authors>
<title>Fast collapsed gibbs sampling for latent dirichlet allocation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>569--577</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="16316" citStr="Porteous et al., 2008" startWordPosition="2727" endWordPosition="2731">ata from worker threads is joined and model parameters are updated with complete information. In the next iteration, thread-local sampling starts with complete topic assignment information from the previous iteration. In each thread, the process can be viewed as a reordering of the input examples, where the examples sampled in that thread (P (� I 1452 are viewed first. It has been shown that parallel approaches considerably speed up iterative inference methods such as collapsed Gibbs sampling, resulting in test data log probabilities indistinguishable from those obtained using serial methods (Porteous et al., 2008; Newman et al., 2009). A parallel approach is especially important when training the KB-LDA model due to the large dimensions of the multinomials of the Ontology and Relations components (K2 and K3, respectively for a model with K topics). We train KB-LDA over 2000 iterations, more than what has traditionally been used for collapsed Gibbs samplers. 2.2 Data-driven discovery of topic concepts The KB-LDA model described above clusters noun entities into sets of instance topics, and recovers a latent hierarchical structure among these topics. Each instance topic can be described by a multinomial</context>
</contexts>
<marker>Porteous, Newman, Ihler, Asuncion, Smyth, Welling, 2008</marker>
<rawString>Ian Porteous, David Newman, Alexander Ihler, Arthur Asuncion, Padhraic Smyth, and Max Welling. 2008. Fast collapsed gibbs sampling for latent dirichlet allocation. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 569–577. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic taxonomy induction from heterogenous evidence.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>801--808</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1799" citStr="Snow et al., 2006" startWordPosition="281" endWordPosition="284">ion is ontology-guided construction, where the list of categories and relations that define the schema of the KB are explicit, versus open IE methods, where they are not. Another dimension is the type of relations and types included in the KB: some KBs, like WordNet, are hierarchical, in that they contain mainly concept types, supertypes and instances, while other KBs contain many types of relationships between concepts. Hierarchical knowledge can be learned by methods including distributional clustering (Pereira et al., 1993), as well as Hearst patterns (Hearst, 1992) and similar techniques (Snow et al., 2006). Reverb (Fader et al., 2011) and TextRunner (Yates William W. Cohen Machine Learning Department Carnegie Mellon University wcohen@cs.cmu.edu et al., 2007) are open methods for learning multirelation KBs. Finally, NELL (Carlson et al., 2010; Mitchell et al., 2015), FreeBase (Google, 2011) and Yago (Suchanek et al., 2007; Hoffart et al., 2013) are ontology-guided methods for extracting KBs containing both hierarchies and relations. One advantage of ontology-guided methods is that the extracted knowledge is easier to reason with. An advantage of open IE methods is that ontologies may be incomple</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2006</marker>
<rawString>Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2006. Semantic taxonomy induction from heterogenous evidence. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 801–808. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Danqi Chen</author>
<author>Christopher D Manning</author>
<author>Andrew Ng</author>
</authors>
<title>Reasoning with neural tensor networks for knowledge base completion.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>926--934</pages>
<contexts>
<context position="34507" citStr="Socher et al., 2013" startWordPosition="5727" endWordPosition="5730">e a meaningful semantic correspondence to a KB structure (hierarchical and relational). In a related approach, Dalvi et al. (2012) cluster web table concepts to non-probabilistically create hierarchies with assigned concept names. Our work is related to latent tensor representation of KBs, aimed at enhancing the ontological structure of existing KBs with relational data in the form of tensor structures. Nickel et al. (2012) factorized the ontology of Yago 2 for relational learning. A related approach was using Neural Tensor Networks to extract new facts from an existing KB (Chen et al., 2013; Socher et al., 2013). In contrast, in KB-LDA, relational data is learned jointly with the model through the Relations component. Statistical language models have recently been adapted for modeling software code and text documents. Most tasks focused on enhancing the software development workflow with code and comment completion (Hindle et al., 2012; Movshovitz-Attias and Cohen, 2013), learning coding conventions (Allamanis et al., 2014), and extracting actionable tasks from software documentation (Treude et al., 2014). In related work, specific semantic relations, coordinate relations, have been extracted for a r</context>
</contexts>
<marker>Socher, Chen, Manning, Ng, 2013</marker>
<rawString>Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng. 2013. Reasoning with neural tensor networks for knowledge base completion. In Advances in Neural Information Processing Systems, pages 926–934.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>Yago: a core of semantic knowledge.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th international conference on World Wide Web,</booktitle>
<pages>697--706</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2120" citStr="Suchanek et al., 2007" startWordPosition="330" endWordPosition="333">ept types, supertypes and instances, while other KBs contain many types of relationships between concepts. Hierarchical knowledge can be learned by methods including distributional clustering (Pereira et al., 1993), as well as Hearst patterns (Hearst, 1992) and similar techniques (Snow et al., 2006). Reverb (Fader et al., 2011) and TextRunner (Yates William W. Cohen Machine Learning Department Carnegie Mellon University wcohen@cs.cmu.edu et al., 2007) are open methods for learning multirelation KBs. Finally, NELL (Carlson et al., 2010; Mitchell et al., 2015), FreeBase (Google, 2011) and Yago (Suchanek et al., 2007; Hoffart et al., 2013) are ontology-guided methods for extracting KBs containing both hierarchies and relations. One advantage of ontology-guided methods is that the extracted knowledge is easier to reason with. An advantage of open IE methods is that ontologies may be incomplete, and are expensive to construct for a new domain. Ontology design involves assembling a set of categories, organized in a meaningful hierarchical structure, often providing seeds, i.e., representative examples for each category, and finally, defining inter-category relations. This process is often done manually (Carl</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2007</marker>
<rawString>Fabian M Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: a core of semantic knowledge. In Proceedings of the 16th international conference on World Wide Web, pages 697–706. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Partha Pratim Talukdar</author>
<author>Derry Wijaya</author>
<author>Tom Mitchell</author>
</authors>
<title>Acquiring temporal constraints between relations.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st ACM international conference on Information and knowledge management,</booktitle>
<pages>992--1001</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="12053" citStr="Talukdar et al. (2012)" startWordPosition="1955" endWordPosition="1958">: – Sample topic tuple zSV O j— Multinomial(πR) – Sample instances, Sj — Multinomial(σzSj ), Oj — Multinomial(σzOj ), and sample a relation Vj — Multinomial(δzVj ) 4. Generate documents: For document d E 1, ... ,D: • Sample θd — Dirichlet(αD), the topic mixing distribution for document d. • For every noun entity (El1) and verb entity (El2), l1 E 1,...,Nd,I, l2E1,...,Nd,R: – Sample topics zEl1, zEl2 —Multinomial(θd) – Sample entities El1 — Multinomial(σzEl1) and El2 — Multinomial(δzEl2 ) Table 2: KB-LDA generative process. are extracted from SVO patterns found in the document corpus, following Talukdar et al. (2012). An extracted example looks like: (websites, execute, javascript). Subject and object topics are drawn from the noun topics (σ), while the verb topics is drawn from the verb topics, defined by δ. The multinomial πR encodes the interaction of noun and verb topics based on the extracted relational links, and it is defined over the Cartesian product of the noun topic set with itself and with 1451 the verb topic set. The generative process of KB-LDA is described in Table 2. Given the hyperparameters (αO, αR, αD, γI, γR), the joint distribution over CI pairs, SVO tuples, documents, topics and topi</context>
</contexts>
<marker>Talukdar, Wijaya, Mitchell, 2012</marker>
<rawString>Partha Pratim Talukdar, Derry Wijaya, and Tom Mitchell. 2012. Acquiring temporal constraints between relations. In Proceedings of the 21st ACM international conference on Information and knowledge management, pages 992–1001. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Treude</author>
<author>M Robillard</author>
<author>Barth´el´emy Dagenais</author>
</authors>
<title>Extracting development tasks to navigate software documentation.</title>
<date>2014</date>
<journal>IEEE Transactions on Software Engineering.</journal>
<contexts>
<context position="35010" citStr="Treude et al., 2014" startWordPosition="5799" endWordPosition="5802">ch was using Neural Tensor Networks to extract new facts from an existing KB (Chen et al., 2013; Socher et al., 2013). In contrast, in KB-LDA, relational data is learned jointly with the model through the Relations component. Statistical language models have recently been adapted for modeling software code and text documents. Most tasks focused on enhancing the software development workflow with code and comment completion (Hindle et al., 2012; Movshovitz-Attias and Cohen, 2013), learning coding conventions (Allamanis et al., 2014), and extracting actionable tasks from software documentation (Treude et al., 2014). In related work, specific semantic relations, coordinate relations, have been extracted for a restricted class of software entities, ones that refer to Java classes (Movshovitz-Attias and Cohen, 2015). KB-LDA extends previous work by reasoning over a large variety of semantic relations among general software entities, as found in a document corpus. 5 Conclusions We presented a model that jointly learns a latent ontological structure of a corpus augmented by relations, and identifies facts matching the learned structure. The quality of the produced structure was demonstrated through a series </context>
</contexts>
<marker>Treude, Robillard, Dagenais, 2014</marker>
<rawString>Christoph Treude, M Robillard, and Barth´el´emy Dagenais. 2014. Extracting development tasks to navigate software documentation. IEEE Transactions on Software Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yates</author>
<author>Michael Cafarella</author>
<author>Michele Banko</author>
<author>Oren Etzioni</author>
<author>Matthew Broadhead</author>
<author>Stephen Soderland</author>
</authors>
<title>Textrunner: open information extraction on the web.</title>
<date>2007</date>
<booktitle>In Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,</booktitle>
<pages>25--26</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Yates, Cafarella, Banko, Etzioni, Broadhead, Soderland, 2007</marker>
<rawString>Alexander Yates, Michael Cafarella, Michele Banko, Oren Etzioni, Matthew Broadhead, and Stephen Soderland. 2007. Textrunner: open information extraction on the web. In Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations, pages 25–26. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>