<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.7175832">
Sentence Comparison
using Robust Minimal Recursion Semantics
and an Ontology
•
Reb ecca Dridan } and Francis Bond
</note>
<title confidence="0.2944">
} •
</title>
<author confidence="0.226716">
rdrid@csse.unimelb.edu.au bond@cslab.kecl.ntt.co.jp
</author>
<affiliation confidence="0.802966">
} The University of Melbourne
</affiliation>
<listItem confidence="0.496761">
• NTT Communication Science Laboratories,
</listItem>
<category confidence="0.26583">
Nippon Telegraph and Telephone Corporation
</category>
<sectionHeader confidence="0.970261" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999930071428571">
We design and test a sentence com-
parison method using the framework
of Robust Minimal Recursion Seman-
tics which allows us to utilise the deep
parse information produced by Jacy, a
Japanese HPSG based parser and the
lexical information available in our on-
tology. Our method was used for both
paraphrase detection and also for an-
swer sentence selection for question an-
swering. In both tasks, results showed
an improvement over Bag-of-Words, as
well as providing extra information use-
ful to the applications.
</bodyText>
<sectionHeader confidence="0.997864" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999011268292683">
Comparison between sentences is required for
many NLP applications, including question
answering, paraphrasing, text summarization
and entailment tasks. In this paper we show
an RMRS (Robust Minimal Recursion Seman-
tics, see Section 1.1) comparison algorithm
that can be used to compare sentences in
any language that has RMRS generating tools
available. Lexical resources of any language
can b e plugged in to give a more accurate and
informative comparison.
The simplest and most commonly used
methods of judging sentence similarity use
word overlap — either looking for matching
word sequences, or comparing a Bag-of-Words
representation of each sentence. Bag-of-Words
discards word order, and any structure desig-
nated by such, so that the cat snored and the
dog slept is equivalent to the dog snored and
the cat slept. Sequence matching on the other
hand requires exact word order matching and
hence the game began quietly and the game qui-
etly began are not considered a match. Neither
method allows for synonym matching.
Hirao et al. (2004) showed that they could
get a much more robust comparison using
dependency information rather than Bag-of-
Words, since they could abstract away from
word order but still compare the important
elements of a sentence. Using deep parsing
information, such as dependencies, but also
deep lexical resources where available, enables
a much more informative and robust compar-
ison, which goes beyond lexical similarity. We
use the RMRS framework as our comparison
format because it has the descriptive power to
encode the full semantics, including argument
structure. It also enables easy combination of
deep and shallow information and, due to its
flat structure, is easy to manage computation-
ally.
</bodyText>
<subsectionHeader confidence="0.7098525">
1.1 Robust Minimal Recursion
Semantics
</subsectionHeader>
<bodyText confidence="0.966347863636364">
Robust Minimal Recursion Semantics
(RMRS) is a form of flat semantics which is
designed to allow deep and shallow processing
to use a compatible semantic representation,
while being rich enough to support gener-
alized quantifiers (Frank, 2004). The main
component of an RMRS representation is
a bag of elementary predicates and their
arguments.
An elementary predicate always has a
unique label, a relation type, a relation name
and an ARG0 feature. The example in Fig-
ure 1 has a label of h5 which uniquely identi-
fies this predicate. Relation types can either
be REALPRED for a predicate that relates di-
rectly to a content word from the input text, or
GPRED for grammatical predicates which may
not have a direct referent in the text. For ex-
amples in this paper, a REALPRED is distin-
guished by an underscore ( ) before the rela-
tion name.
The GPRED relation names come from a
</bodyText>
<page confidence="0.996193">
35
</page>
<note confidence="0.8995595">
Proceedings of the Workshop on Linguistic Distances, pages 35–42,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<figure confidence="0.463466333333333">
unten s
LsL h5
ARC0 e6
</figure>
<figureCaption confidence="0.9683055">
Figure 1: Elementary predicate for ft unten
&amp;quot;drive&amp;quot;
</figureCaption>
<bodyText confidence="0.999975333333333">
closed-set which specify common grammatical
relations, but the REALPRED names are formed
from the word in the text they relate to and
this is one way in which RMRS allows under-
specification. A full relation name is of the
form lemma pos sense, where the pos (part
of speech) is drawn from a small set of general
types including noun, verb and sahen (verbal
noun). The sense is a number that identifies
the sense of the word within a particular gram-
mar being used. The POS and sense informa-
tion are only used when available and hence
the unten s 1 is more specific but compati-
ble with unten s or even unten.
The ARG0 feature (e6 in Figure 1) is the
referential index of the predicate. Predicates
with the same ARG0 are said to b e referen-
tially co-indexed and therefore have the same
referent in the text.
A shallow parse might provide only the fea-
tures shown in Figure 1, but a deep parse can
also give information about other arguments
as well as scoping constraints. The features
ARG1..ARG4 specify the indices of the semantic
arguments of the relevant predicate, similar to
PropBank&apos;s argument annotation (Kingsbury
et al., 2002). While the RMRS specification
does not define semantic roles for the ARGn
features, in practice ARG1 is generally used for
the AGENT and ARG2 for the PATIENT. Fea-
tures ARG3 and ARG4 have less consistency in
their roles.
We will use (1) and (2) as examples of sim-
ilar sentences. They are definition sentences
for one sense of l` 7&apos;f 1�-- doraiba- &amp;quot;driver&amp;quot;,
taken from two different lexicons.
</bodyText>
<listItem confidence="0.71438825">
(1) [AR* ;L- ft T6 A
jidosha wo unten suru hito
car ACC drive do person
&amp;quot;a person who drives a car&amp;quot;
(2) [AR* /.� Z 0 ft t
jidosha nado no unten sha
car etc. ADN drive -er
&amp;quot;a driver of cars etc.&amp;quot;
Examples of deep and shallow RMRS results
for (1) are given in Figure 2. Deep results for
(2) are given in Figure 3.
2 Algorithm
</listItem>
<bodyText confidence="0.999975142857143">
The matching algorithm is loosely based on
RMRS comparison code included in the LKB
(Copestake, 2002: (http://www.delph-in.
net/lkb/)), which was used in Ritchie (2004),
however that code used no outside lexical re-
sources and we have substantially changed the
matching algorithm.
The comparison algorithm is language inde-
pendent and can b e used for any RMRS struc-
tures. It first compares all elementary predi-
cates from the RMRSs to construct a list of
match records and then examines, and poten-
tially alters, the list of match records accord-
ing to constraints encoded in the ARGn vari-
ables. Using the list of scored matches, the
lowest scoring possible match set is found and,
after further processing on that set, a similar-
ity score is returned. The threshold for de-
ciding whether a pair of sentences should be
considered similar or not can be determined
separately for different applications.
</bodyText>
<subsectionHeader confidence="0.989619">
2.1 Matching Predicates
</subsectionHeader>
<bodyText confidence="0.9830084">
The elementary predicates (EPs) of our RMRS
structures are divided into two groups - those
that have a referent in the text, hereafter
known as content EPs, and those that don&apos;t.
There are three kinds of content EP: REAL-
PREDs, which correspond to content bearing
words that the grammar knows; GPREDs with
a CARG (Constant ARGument) feature, which
are used to represent proper names and num-
bers; and GPREDs with a predicate name start-
ing with generic such as generic verb which
are used for unknown words that have only
been identified by their part of speech. All
other EPs have no referent and are used to
provide information about the content EPs or
about the structure of the sentence as a whole.
These non-content EPs can provide some use-
ful information, but generally only in relation
to other content EPs.
Each content EP of the first RMRS is com-
pared to all content EPs in the second RMRS,
as shown in Figure 4.
Matches are categorised as EXACT, SYN-
ONYM, HYPERNYM, HYPONYM or NO MATCH
and a numerical score is assigned. The nu-
</bodyText>
<page confidence="0.994807">
36
</page>
<figure confidence="0.870562933333333">
2
6
6
6
6
6
6
6
6
6
6
6
6
6
4
</figure>
<equation confidence="0.969890133333333">
TEXT &apos;,)%+ $*
TOP h1
2 3 2 3
proposition m rel unknown rel udef rel unten s
r LBL h1 1 r LBL h4 1 jidousha n LBL h8 LBL h11
L ARG0 e2 J L ARG0 e2 J LBL h66ARG0 h96
7ARG0 e13
ARG0 x7 RSTR h9 ARG1 u12
MARG h3 ARG x5
BODY h10 ARG2 x7
RELS
hito n LBL h15
LBL h146ARG0 x5
ARG0 x5 RSTR h16
BODY h17
</equation>
<figure confidence="0.991862239583333">
HCONS
h3 qeq h4 , h9 qeq h6 , h16 qeq h14 , h18 qeq
ING
h
ing h10002 , h14 ing h10001
&amp;quot;
f
1g
f
11
g
8
&gt;
&gt;
&gt;
&gt;
&gt;
&gt;
&gt;
&gt;
&lt;
&gt;
&gt;
&gt;
&gt;
&gt;
&gt;
&gt;
&gt;
:
proposition
m rel topic rel
#
LBL h10002
ARG0 e13 ARG0 e19
MARG h18 ARG 1 e13
2 5
ARG Z
LBL h10001
3
9
7
7
&gt;
&gt;
&gt; 7
&gt;
&gt; 7
&gt;
&gt; 7
&gt;
=7
7
7
7
&gt;
&gt;
7
&gt;
&gt;
7
&gt;
&gt;
7
&gt;
&gt;
;
7
5
TEXT
TOP h9
jidousha n wo rel
s suru rel hito n
RELS
h3
h5
h7
h9
u4
e6
e8
HCONS
ING
&apos;,)%+ $*
� � � � � � � � � �
unten
LBLh1LBL
LBL
LBL
LBL
ARG0x2ARG0
ARG0
ARG0
ARG0x10
fg
fg
</figure>
<figureCaption confidence="0.993017">
Figure 2: Deep (top) and shallow (bottom) RMRS results for [AM* ;L1_ W bra A
</figureCaption>
<equation confidence="0.996888368421053">
TEXT &apos;,) &amp;quot;! # + (
TOP h1
2 3
&amp;quot; # &amp;quot; # &amp;quot;
udef rel
� �
proposition m rel unknown rel nado n
jidousha n LBL h8
LBL h1 LBL h4 LBL h10001
LBL h66ARG0 x7
4 5
ARG0 e2 ARG0 e2 ARG0 u11
h3 5 BODY h10
ARG0 x7 RSTR h9 ARG1 x7
&amp;quot; # &amp;quot; # &amp;quot;
noun-relation proposition m rel sha n
LBL h17 LBL h18 LBL h10002
ARG0 x5 ARG0 x5 ARG0 u20
ARG1 h18 MARG h19 ARG1 x5
</equation>
<figureCaption confidence="0.681827">
Figure 3: RMRS representation for [AM* /.� Z 0 W t
</figureCaption>
<equation confidence="0.851787125">
foreach ep1 in contentEPs1
foreach ep2 in contentEPs2
(score, match) = match_EPs(ep1, ep2)
if match != NO_MATCH
add_to_matches(ep1, ep2, score, match)
endif
done
done
</equation>
<figureCaption confidence="0.995437">
Figure 4: Predicate match pseudo-code
</figureCaption>
<bodyText confidence="0.997950647058823">
merical score represents the distance between
the two EPs, and hence an EXACT match is
assigned a score of zero.
The level of matching possible depends on
the lexical resources available. With no extra
resources, or only a dictionary to pick up or-
thographic variants, the only match types pos-
sible are EXACT and NO MATCH. By adding
a thesaurus, an ontology ora gazetteer, it is
then possible to return SYNONYM, HYPERNYM
and HYPONYM match relations. In our ex-
periments we used the ontology described in
Section 3.2.2, which provides all three extra
match types. Adding a thesaurus only would
enable SYNONYM matching, while a gazetteer
could be added to give, for example, Tokyo is
a HYPONYM of city.
</bodyText>
<equation confidence="0.684124333333333">
Matches:
hito_n - sha_n : HYPERNYM (2)
jidosha_n - jidosha_n: EXACT (0)
unten_s_2 - unten_s_2: EXACT (0)
Figure 5: First pass match list for (1) and (2)
udef rel
I
LBL
5
</equation>
<bodyText confidence="0.843315428571429">
At the end of the first pass, a list of match
records shows all EP matches with their match
type and score. Each EP can have multiple
possible matches. The output of comparing
(1) and (2), with the RMRSes in Figures 2
and 3, is shown in Figure 5. This shows hito n
(A hito &amp;quot;person&amp;quot;) tagged as a HYPERNYM of
</bodyText>
<page confidence="0.991025">
37
</page>
<figure confidence="0.997771010309278">
2
6
6
6
6
6
6
6
6
6
6
6
6
6
4
2
6
6
6
4
3
7
7
7
5
RELS
7ten s
AR
u16
ARG2
HCONS fh3 qeq h4 , h9 qeq h6 , h13 qeq h17 , h19 qeq h15 g
ING fh6 ing h10001 , h17 ing h10002 g
x
G0x
1
8
&gt;
&gt;
&gt;
&gt;
&gt;
&gt;
&gt;
&gt;
&lt;
&gt;
&gt;
&gt;
&gt;
&gt;
&gt;
&gt;
&gt;
:
G
MAR
ARG Z
def rel
u
LBL h12
BODY h14
AR
5h15
RSTR h13 ARG
G0x
I1[
3
9
7
&gt;
7
&gt;
7
&gt;
&gt;
&gt; 7
&gt;
&gt; 7
&gt;
=7
7
7
7
&gt;
&gt;
7
&gt;
&gt;
7
&gt;
&gt;
7
&gt;
&gt;
;
7
5
</figure>
<equation confidence="0.977371222222222">
foreach match in matches
gpreds1 = get_gpreds_arg0(ep1farg0D
gpreds2 = get_gpreds_arg0(ep2farg0D
totalgpreds = len gpreds1 + len gpreds2
foreach ep1 in gpreds1
foreach ep2 in gpreds2
if(match_gram_eps(ep1, ep2)
remove(ep1, gpreds1)
remove(ep2, gpreds2)
</equation>
<figure confidence="0.657097857142857">
endif
done
done
gpreds_left = len gpreds1 + len gpreds2
left = gpreds_left/totalgpreds
matchfscorel+= left*gpredWeight
done
</figure>
<figureCaption confidence="0.998778">
Figure 6: Matching ARG0s
</figureCaption>
<bodyText confidence="0.99965">
sha n (t sha &amp;quot;-er&amp;quot; is a suffix indicating a per-
son, normally the agent of a verb: it is more re-
strictive than English -er , in that it only refers
to people).
</bodyText>
<subsectionHeader confidence="0.966316">
2.2 Constraints Pass
</subsectionHeader>
<bodyText confidence="0.998627838709677">
For each possible match, all the non-content
EPs that have the same ARG0 value as the
content EPs in the match are examined, since
these have the same referent. If each non-
content EP related to the content EP on one
side of the match can be matched to the non-
content EPs related to the other content EP ,
no change is made. If not, however, a penalty
is added to the match score, as shown in Fig-
ure 6. In our example, unten s 2 from the first
sentence has a proposition m rel referen-
tially co-indexed, while the second unten s 2
has a proposition m rel, a noun-relation
and a udef rel, and so a small penalty is
added as shown in Figure 7.
The second check in the constraint match
pass examines the arguments (ARG1, ARG2,
ARG3, ARG4) of each of the matches. It looks
for possible matches found between the EPs
listed as ARGn for each match. This check can
result in three separate results: both EPs have
an ARGn but there is no potential match found
between the respective ARGn EPs, a potential
match has been found between the ARGn EPs,
or only one of the EPs in the match has an
ARGn feature.
Where both EPs have an ARGn feature, the
score (distance) of the match is decreased or
increased depending on whether a match be-
tween the ARGn variables was found. Given
that the RMRS definition does not specify a
</bodyText>
<equation confidence="0.968189">
Matches:
hito_n - sha_n : HYPERNYM (2.1)
jidosha_n - jidosha_n: EXACT (0)
unten_s_2 - unten_s_2: EXACT (0.05)
</equation>
<figureCaption confidence="0.97226">
Figure 7: Match list
</figureCaption>
<bodyText confidence="0.999940318181818">
Slight penalty added to unten s 2 and hito n
for non-matching non-content EPs
`meaning&apos; for the ARGn variables, comparing,
for example, ARG1 variables from two differ-
ent predicates may not necessarily be compar-
ing the same semantic roles. However, be-
cause of the consistency found in ARG1 and
ARG2 meaning this is still a useful check. Of
course, if we are comparing the same relation,
the ARGs will all have the same meaning. The
comparison method allows for different penal-
ties for each of ARG1 to ARG4, and also in-
cludes a scaling factor so that mismatches in
ARGs when comparing EXACT EP matches will
have more effect on the score than in non
EXACT matches. If one EP does not have
the ARGn feature, no change is made to the
score. This allows for the use of underspeci-
fied RMRSs, in the case where the parse fails.
At the end of this pass, the scores of the
matches in the match list may have changed
but the number of matches is still the same.
</bodyText>
<subsectionHeader confidence="0.996502">
2.3 Constructing the Sets
</subsectionHeader>
<bodyText confidence="0.999541818181818">
Match sets are constructed by using a branch-
and-bound decision tree. Each match is con-
sidered in order, and the tree is branched if
the next match is possible, given the proceed-
ing decisions. Any branch which is more than
two decisions away from the best score so far
is pruned. At the end of this stage, the lowest
scoring match set is returned and then this is
further processed.
If no matches were found, processing stops
and a sentinel value is returned. Otherwise,
the non matching predicates are grouped to-
gether by their ARG0 value. Scoping con-
straints are checked and if any non matching
predicate outscopes a content predicate it is
added to that grouping. Hence if it outscopes
a matching EP it becomes part of the match,
otherwise it becomes part of a non-matching
EP group.
Any group of grammatical EPs that shares
an ARG0 but does not contain a content pred-
icate is matched against any similar groupings
</bodyText>
<page confidence="0.995757">
38
</page>
<figure confidence="0.995910222222222">
Best score is 0.799 for the match set:
MATCHES:
hito_n-sha_n: HYPERNYM:2.1
jidousha_n-jidousha_n:EXACT:0
unten_s_2-unten_s_2:EXACT:0.05
proposition_m_rel-proposition_m_rel:EXACT:0
UNMATCHED1:
UNMATCHED2:
u11: h10001:nado_n
</figure>
<figureCaption confidence="0.999966">
Figure 8: Verbose comparison output
</figureCaption>
<bodyText confidence="0.998277">
in the other RMRS. This type of match can
only be EXACT or No MATCH and will make
only a small difference in the final score.
Content predicates that have not been
matched by this stage are not processed any
further, although this is an area for further
investigation. Potentially negation and other
modifiers could b e processed at this point.
</bodyText>
<subsectionHeader confidence="0.69765">
2.4 Output
</subsectionHeader>
<bodyText confidence="0.9999271875">
The output of the comparison algorithm is a
numeric score and also a representation of the
final best match found.
The numerical score, using the default scor-
ing parameters, ranges between 0 (perfect
match) and 3. As well as the no match score
(-5), sentinel values are used to indicate miss-
ing input data so it is possible to fall back to
a shallow parse if the deep parse failed.
Details of the match set are also returned for
further processing or examination if the appli-
cation requires. This shows which predicates
were deemed to match, and with what score,
and also shows the unmatched predicates. Fig-
ure 8 shows the output of our example com-
parison.
</bodyText>
<sectionHeader confidence="0.997781" genericHeader="introduction">
3 Resources
</sectionHeader>
<bodyText confidence="0.999974428571429">
While the comparison method is language in-
dependent, the resources required are lan-
guage specific. The resources fall in to two
different categories: parsing and morpholog-
ical analysis tools that produce the RMRSs,
and lexical resources such as ontologies, dictio-
naries and gazetteers for evaluating matches.
</bodyText>
<subsectionHeader confidence="0.998139">
3.1 Parsing
</subsectionHeader>
<bodyText confidence="0.999953857142857">
Japanese language processing tools are freely
available. We used the Japanese grammar
Jacy (Siegel and Bender, 2002), a deep parsing
HPSG grammar that produces RMRSs for our
primary input source.
When parsing with Jacy failed, compar-
isons could still b e made with RMRS produced
from shallow tools such as ChaSen (Mat-
sumoto et al., 2000), a morphological analyser
or CaboCha (Kudo and Matsumoto, 2002), a
Japanese dependency parser. Tools have been
built to produced RMRS from the standard
output of both those tools.
The CaboCha output supplies similar de-
pendency information to that of the Basic El-
ements (BE) tool used by Hovy et al. (2005b)
for multi-document summarization. Even this
intermediate level of parsing gives better com-
parisons than either word or sequence overlap,
since it is easier to compare meaningful ele-
ments (Hovy et al., 2005a).
</bodyText>
<subsectionHeader confidence="0.998981">
3.2 Lexical Resources
</subsectionHeader>
<bodyText confidence="0.999991214285714">
Whilst deep lexical resources are not available
for every language, where they are available,
they should be used to make comparisons more
informative. The comparison framework al-
lows for different lexical resources to b e added
to a pipeline. The pipeline starts with a sim-
ple relation name match, but this could be fol-
lowed by a dictionary to extract orthographic
variants and then by ontologies such as Word-
Net (Fellbaum, 1998) or GoiTaikei (Ikehara
et al., 1997), gazetteers or named entity recog-
nisers to recognise names of people and places.
The sections below detail the lexical resources
we used within our experiments.
</bodyText>
<subsubsectionHeader confidence="0.62698">
3.2.1 The Lexeed Semantic Database
</subsubsectionHeader>
<bodyText confidence="0.999947555555555">
The Lexeed Semantic Database of Japanese
is a machine readable dictionary that covers
the most familiar words in Japanese, based
on a series of psycholinguistic tests (Kasahara
et al., 2004). Lexeed has 28,000 words divided
into 46,000 senses and defined with 75,000 def-
inition sentences. Each entry includes a list of
orthographic variants, and the pronunciation,
in addition to the definitions.
</bodyText>
<subsectionHeader confidence="0.435903">
3.2.2 Ontology
</subsectionHeader>
<bodyText confidence="0.999880625">
The lexicon has been sense-tagged and
parsed to give an ontology linking senses with
various relations, principally hypernym and
synonym (Nichols et al., 2005). For example,
(HYPERNYM, K74.1�-- doraibd &amp;quot;driver&amp;quot;, �r
77 kurabu &amp;quot;club&amp;quot;). The ontology entries for
nouns have been hand checked and corrected,
including adding hypernyms for words where
</bodyText>
<page confidence="0.998583">
39
</page>
<bodyText confidence="0.99992775">
the genus term in the definition was very gen-
eral, e.g &amp;quot;a word used to refer insultingly to
men&amp;quot; where man is a more useful hypernym
than word for the defined term yarou.
</bodyText>
<sectionHeader confidence="0.971935" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999847571428572">
We evaluated the performance of the RMRS
comparison method in two tasks. First it was
used to indicate whether two sentences were
possible paraphrases. In the second task, we
used the comparison scores to select the most
likely sentence to contain the answer to a ques-
tion.
</bodyText>
<subsectionHeader confidence="0.989855">
4.1 Paraphrasing
</subsectionHeader>
<bodyText confidence="0.999973481481482">
In this task we compared definitions sen-
tences for the same head word from two differ-
ent Japanese dictionaries - the Lexeed dictio-
nary (§3.2.1) and the Iwanami Kokugo Jiten
(Iwanami: Nishio et al., 1994), the Japanese
dictionary used in the SENSEVAL-2 Japanese
lexical task (Shirai, 2002).
There are 60,321 headwords and 85,870
word senses in Iwanami. Each sense in the
dictionary consists of a sense ID and morpho-
logical information (word segmentation, POS
tag, base form and reading, all manually post-
edited).
The definitions in Lexeed and Iwanami were
linked by headword and three Japanese native
speakers assessed each potential pair of sense
definitions for the same head word to judge
which definitions were describing the same
sense. This annotation not only described
which sense from each dictionary matched, but
also whether the definitions were equal, equiv-
alent, or subsuming.
The examples (1) and (2) are the definitions
of sense 2 of V 74.1-- doraiba &amp;quot;driver&amp;quot; from
Lexeed and Iwanami respectively. They were
judged to be equivalent definitions by all three
annotators.
</bodyText>
<subsectionHeader confidence="0.726571">
4.1.1 Method
</subsectionHeader>
<bodyText confidence="0.999925931034483">
Test sets were built consisting of the Lexeed
and Iwanami definition pairs that had been an-
notated in the gold standard to be either non-
matching, equal or equivalent. Leaving out
those pairs annotated as having a subsump-
tion relation made it a clearer task judging
between paraphrase or not, rather than ex-
amining partial meaning overlap. Ten sets of
5,845 definition pairs were created, with each
set being equally split between matching and
non-matching pairs. This gives data that is to
some extent semantically equivalent (the same
word sense is being defined), but with no guar-
antee of syntactic equivalence.
Comparisons were made between the first
sentence of each definition with both a Bag-
of-Words comparison method and our RMRS
based method. If RMRS output was not avail-
able from Jacy (due to a failed parse), RMRS
from CaboCha was used as a fall back shallow
parse result.
Scores were output and then the best
threshold score for each method was calculated
on one of the 10 sets. Using the calculated
threshold score, pairs were classified as either
matching or non-matching. Pairs classified as
matching were evaluated as correct if the gold
standard annotation was either equal or equiv-
alent.
</bodyText>
<sectionHeader confidence="0.844333" genericHeader="method">
4.1.2 Results
</sectionHeader>
<bodyText confidence="0.9999637">
The Bag-of-Words comparison got an av-
erage accuracy over all sets of 73.9% with
100% coverage. A break down of the results
shows that this method was more accurate
(78%) in correctly classifying non-matches
than matches (70%). This is to b e expected
since it won&apos;t pick up equivalences where a
word has been changed for its synonym.
The RMRS comparison had an accuracy
was 78.4% with almost 100% coverage, an im-
provement over the Bag-of-Words. The RMRS
based method was also more accurate over
non matches (79.9%) than matches (76.6%),
although the difference is not as large. Con-
sidering only those sentences with a parse from
JACY gave an accuracy of 81.1% but with a
coverage of only 46.1%. This shows that deep
parsing improves precision, but must b e used
in conjunction with a shallower fallback.
To explore what effect the ontology was hav-
ing on the results, another evaluation was per-
formed without the ontology matching. This
had an accuracy of 77.3% (78.1% using Jacy,
46.1% coverage). This shows that the infor-
mation available in the ontology definitely im-
proves scores, but that even without that sort
of deep lexical resource, the RMRS matching
can still improve on Bag-of-Words using just
surface form abstraction and argument match-
ing.
</bodyText>
<page confidence="0.99264">
40
</page>
<subsectionHeader confidence="0.927627">
4.2 Answer Sentence Selection
</subsectionHeader>
<bodyText confidence="0.999937454545455">
To emulate a part of the question answering
pipeline, we used a freely available set of 2000
Japanese questions, annotated with, among
other things, answer and answer document ID
(Sekine et al., 2002). The document IDs for
the answer containing documents refer to the
Mainichi Newspaper 1995 corpus which has
been used as part of the document collection
for NTCIR&apos;s Question Answering Challenges.
The documents range in length from 2 to 83
sentences.
</bodyText>
<subsectionHeader confidence="0.437457">
4.2.1 Method
</subsectionHeader>
<bodyText confidence="0.999972052631579">
For every question, we compared it to each
sentence in the answer document. The sen-
tence that has the best similarity to the ques-
tion is returned as the most likely to con-
tain the answer. For this sort of compari-
son, an entails option was added that changes
the similarity scoring method slightly so that
only non-matches in the first sentence increase
the score. The rationale being that in Ques-
tion Answering (and also in entailment), ev-
erything present in the question (or hypoth-
esis) should b e matched by something in the
answer, but having extra, unmatched informa-
tion in the answer should not b e penalised.
The task is evaluated by checking if the an-
swer does exist in the sentence selected. This
means that more than one sentence can be the
correct answer for any question (if the answer
is mentioned multiple times in the article).
</bodyText>
<sectionHeader confidence="0.620572" genericHeader="method">
4.2.2 Results
</sectionHeader>
<bodyText confidence="0.999981055555555">
The Bag-of-Words comparison correctly
found a sentence containing the answer for
62.5% of the 2000 questions. The RMRS com-
parison method gave a small improvement,
with a result of 64.3%. Examining the data
showed this to be much harder than the para-
phrase task because of the language level in-
volved. In the paraphrasing task, the sen-
tences averaged around 10 predicates each,
while the questions and sentences in this task
averaged over 3 times longer, with about 34
predicates. The words used were also less
likely to b e in the lexical resources both be-
cause more formal, less familiar words were
used, and also because of the preponderance
of named entities. Adding name lists of peo-
ple, places and organisations would greatly im-
prove the matching in this instance.
</bodyText>
<sectionHeader confidence="0.902144" genericHeader="method">
5 Future Directions
5.1 Applications
</sectionHeader>
<bodyText confidence="0.999989607142857">
Since the comparison method was written
to b e language independent, the next stage
of evaluation would b e to use it in a non-
Japanese task. The PASCAL Recognising
Textual Entailment (RTE) Challenge (Dagan
et al., 2005) is one recent English task where
participants used sentence comparison exten-
sively. While the task appears to call for in-
ference and reasoning, the top 5 participat-
ing groups used statistical methods and word
overlap only. Vanderwende et al. (2005) did a
manual evaluation of the test data and found
that 37% could b e decided on syntactic infor-
mation alone, while adding a thesaurus could
increase that coverage to 49%. This means
that RMRS comparison has the potential to
perform well. Not only does it improve on
basic word overlap, but it allows for easy ad-
dition of a thesaurus or dictionary. Further,
because of the detailed match output avail-
able, the method could be extended in post
processing to encompass some basic inference
methods.
Aside from comparing sentences, the RMRS
comparison can b e used to compare the RMRS
output of different tools for the same sentence
so that the compatibility of the outputs can
be evaluated and improved.
</bodyText>
<subsectionHeader confidence="0.953796">
5.2 Extensions
</subsectionHeader>
<bodyText confidence="0.999884736842105">
One immediate future improvement planned
is to add named entity lists to the lexical re-
sources so that names of people and places
could be looked up. This would allow partial
matches between, e.g., Clinton is a HYPONYM
of person, which would be particularly useful
for Question Answering.
Another idea is to add a bilingual dictio-
nary and try cross-lingual comparisons. As
the RMRS abstracts away much of the surface
specific details, this might be useful for sen-
tence alignment.
To go beyond sentence by sentence compar-
ison, we have plans to implement a method
for multi-sentence comparisons by either com-
bining the RMRS structures before compari-
son, or post-processing the sentence compari-
son outputs. This could be particularly inter-
esting for text summarization.
</bodyText>
<page confidence="0.999204">
41
</page>
<sectionHeader confidence="0.993986" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999736045454546">
Deep parsing information is useful for com-
paring sentences and RMRS gives us a use-
ful framework for utilising this information
when it is available. Our RMRS compari-
son was more accurate then basic word over-
lap similarity measurement particularly in the
paraphrase task where synonyms were of-
ten used. Even when the ontology was not
used, abstracting away from surface form, and
matching arguments did give an improvement.
Falling back to shallow parse methods in-
creases the robustness which is often an issue
for tools that use deep processing, while still
allowing the use of the most accurate informa-
tion available.
The comparison method is language agnos-
tic and can b e used for any language that has
RMRS generating tools. The output is much
more informative than Bag-of-Words, mak-
ing it useful in many applications that need
to know exactly how a sentence matched or
aligned.
</bodyText>
<sectionHeader confidence="0.974633" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999924285714286">
This work was started when the first author was a vis-
itor at the NTT Communication Science Laboratories,
Nippon Telegraph and Telephone Corporation. The
first author was also supported by the Pam Todd schol-
arship from St Hilda&apos;s College. We would like to thank
the NTT Natural Language Research Group and two
anonymous reviewers for their valuable input.
</bodyText>
<sectionHeader confidence="0.992263" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999886675">
Ann Copestake. 2002. Implementing Typed Feature
Structure Grammars. CSLI Publications.
Ido Dagan, Oren Glickman, and Bernado Magnini.
2005. The PASCAL recognising textual entailment
challenge. In Proceedings of the PASCAL Challenges
Workshop on Recognising Textual Entailment.
Christine Fellbaum. 1998. A semantic network of En-
glish verbs. In Christine Fellbaum, editor, WordNet:
An Electronic Lexical Database, chapter 3, pages 70{
104. MIT Press.
Anette Frank. 2004. Constraint-based RMRS con-
struction from shallow grammars. In 20th Inter-
national Conference on Computational Linguistics:
COLING-2004, pages 1269{1272. Geneva.
Tsutomu Hirao, Jun Suzuki, Hideki Isozaki, and
Eisaku Maeda. 2004. Dependency-based sentence
alignment for multiple document summarization. In
Proceedings of the COLING.
Eduard Hovy, Junichi Fukumoto, Chin-Yew Lin, and
Liang Zhao. 2005a. Basic elements. (http://www.
isi.edu/-cyl/BE).
Eduard Hovy, Chin-Yew Lin, and Liang Zhao. 2005b.
A BE-based multi-document summarizer with sen-
tence compression. In Proceedings of Multilingual
Summarization Evaluation.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai,
Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogura,
Yoshifumi Ooyama, and Yoshihiko Hayashi. 1997.
Goi-Taikei A Japanese Lexicon. Iwanami Shoten,
Tokyo. 5 volumes/CDROM.
Kaname Kasahara, Hiroshi Sato, Francis Bond,
Takaaki Tanaka, Sanae Fujita, Tomoko Kanasugi,
and Shigeaki Amano. 2004. Construction of a
Japanese semantic lexicon: Lexeed. SIG NLC-159,
IPSJ, Tokyo. (in Japanese).
Paul Kingsbury, Martha Palmer, and Mitch Marcus.
2002. Adding semantic annotation to the penn tree-
bank. In Proceedings of the Human Language Tech-
nology 2002 Conference.
Taku Kudo and Yuji Matsumoto. 2002. Japanese
dependency analysis using cascaded chunking. In
CoNLL 2002: Proceedings of the 6th Conference
on Natural Language Learning 2002 (COLING 2002
Post-Conference Workshops), pages 63{69. Taipei.
Yuji Matsumoto, Kitauchi, Yamashita, Hirano, Mat-
suda, and Asahara. 2000. Nihongo Keitaiso Kaiseki
System: Chasen, version 2.2.1 manual edition.
http://chasen.aist-nara.ac.jp.
Eric Nichols, Francis Bond, and Daniel Flickinger.
2005. Robust ontology acquisition from machine-
readable dictionaries. In Proceedings of the Inter-
national Joint Conference on Artificial Intelligence
IJCAI-2005, pages 1111{1116. Edinburgh.
Minoru Nishio, Etsutaro Iwabuchi, and Shizuo Mizu-
tani. 1994. Iwanami Kokugo Jiten Dai Go Han
Iwanami Japanese Dictionary Edition 5]. Iwanami
Shoten, Tokyo. (in Japanese).
Anna Ritchie. 2004. Compatible RMRS representa-
tions from RASP and the ERG. Technical Report
UCAM-CL-TR-661.
Satoshi Sekine, Kiyoshi Sudo, Yusuke Shinyama,
Chikashi Nobata, Kiyotaka Uchimoto, and Hitoshi
Isahara. 2002. NYU/CRL QA system, QAC ques-
tion analysis and CRL QA data. In Working Notes
of NTCIR Workshop 3.
Kiyoaki Shirai. 2002. Construction of a word sense
tagged corpus for SENSEVAL-2 Japanese dictio-
nary task. In Third International Conference on
Language Resources and Evaluation (LREC-2002),
pages 605{608.
Melanie Siegel and Emily M. Bender. 2002. Effi-
cient deep processing of Japanese. In Proceedings
of the 3rd Workshop on Asian Language Resources
and International Standardization at the 19th Inter-
national Conference on Computational Linguistics.
Taipei.
Lucy Vanderwende, Deborah Coughlin, and Bill Dolan.
2005. What syntax can contribute in entailment
task. In Proceedings of the PASCAL Challenges
Workshop on Recognising Textual Entailment.
</reference>
<page confidence="0.999292">
42
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.327836">
<title confidence="0.8164245">Sentence Comparison using Robust Minimal Recursion and an Ontology •</title>
<author confidence="0.939989">ecca Dridan Francis Bond</author>
<affiliation confidence="0.879996">rdrid@csse.unimelb.edu.au bond@cslab.kecl.ntt.co.jp University of Melbourne • NTT Communication Science Nippon Telegraph and Telephone Corporation</affiliation>
<abstract confidence="0.993379133333333">We design and test a sentence comparison method using the framework of Robust Minimal Recursion Semantics which allows us to utilise the deep parse information produced by Jacy, a Japanese HPSG based parser and the lexical information available in our ontology. Our method was used for both paraphrase detection and also for answer sentence selection for question answering. In both tasks, results showed an improvement over Bag-of-Words, as well as providing extra information useful to the applications.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
</authors>
<title>Implementing Typed Feature Structure Grammars.</title>
<date>2002</date>
<publisher>CSLI Publications.</publisher>
<contexts>
<context position="5623" citStr="Copestake, 2002" startWordPosition="937" endWordPosition="938">ncy in their roles. We will use (1) and (2) as examples of similar sentences. They are definition sentences for one sense of l` 7&apos;f 1�-- doraiba- &amp;quot;driver&amp;quot;, taken from two different lexicons. (1) [AR* ;L- ft T6 A jidosha wo unten suru hito car ACC drive do person &amp;quot;a person who drives a car&amp;quot; (2) [AR* /.� Z 0 ft t jidosha nado no unten sha car etc. ADN drive -er &amp;quot;a driver of cars etc.&amp;quot; Examples of deep and shallow RMRS results for (1) are given in Figure 2. Deep results for (2) are given in Figure 3. 2 Algorithm The matching algorithm is loosely based on RMRS comparison code included in the LKB (Copestake, 2002: (http://www.delph-in. net/lkb/)), which was used in Ritchie (2004), however that code used no outside lexical resources and we have substantially changed the matching algorithm. The comparison algorithm is language independent and can b e used for any RMRS structures. It first compares all elementary predicates from the RMRSs to construct a list of match records and then examines, and potentially alters, the list of match records according to constraints encoded in the ARGn variables. Using the list of scored matches, the lowest scoring possible match set is found and, after further processi</context>
</contexts>
<marker>Copestake, 2002</marker>
<rawString>Ann Copestake. 2002. Implementing Typed Feature Structure Grammars. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernado Magnini</author>
</authors>
<title>The PASCAL recognising textual entailment challenge.</title>
<date>2005</date>
<booktitle>In Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment.</booktitle>
<contexts>
<context position="24730" citStr="Dagan et al., 2005" startWordPosition="4373" endWordPosition="4376">ces in this task averaged over 3 times longer, with about 34 predicates. The words used were also less likely to b e in the lexical resources both because more formal, less familiar words were used, and also because of the preponderance of named entities. Adding name lists of people, places and organisations would greatly improve the matching in this instance. 5 Future Directions 5.1 Applications Since the comparison method was written to b e language independent, the next stage of evaluation would b e to use it in a nonJapanese task. The PASCAL Recognising Textual Entailment (RTE) Challenge (Dagan et al., 2005) is one recent English task where participants used sentence comparison extensively. While the task appears to call for inference and reasoning, the top 5 participating groups used statistical methods and word overlap only. Vanderwende et al. (2005) did a manual evaluation of the test data and found that 37% could b e decided on syntactic information alone, while adding a thesaurus could increase that coverage to 49%. This means that RMRS comparison has the potential to perform well. Not only does it improve on basic word overlap, but it allows for easy addition of a thesaurus or dictionary. F</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2005</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernado Magnini. 2005. The PASCAL recognising textual entailment challenge. In Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christine Fellbaum</author>
</authors>
<title>A semantic network of English verbs.</title>
<date>1998</date>
<booktitle>WordNet: An Electronic Lexical Database, chapter 3,</booktitle>
<pages>70--104</pages>
<editor>In Christine Fellbaum, editor,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="17315" citStr="Fellbaum, 1998" startWordPosition="3145" endWordPosition="3146">termediate level of parsing gives better comparisons than either word or sequence overlap, since it is easier to compare meaningful elements (Hovy et al., 2005a). 3.2 Lexical Resources Whilst deep lexical resources are not available for every language, where they are available, they should be used to make comparisons more informative. The comparison framework allows for different lexical resources to b e added to a pipeline. The pipeline starts with a simple relation name match, but this could be followed by a dictionary to extract orthographic variants and then by ontologies such as WordNet (Fellbaum, 1998) or GoiTaikei (Ikehara et al., 1997), gazetteers or named entity recognisers to recognise names of people and places. The sections below detail the lexical resources we used within our experiments. 3.2.1 The Lexeed Semantic Database The Lexeed Semantic Database of Japanese is a machine readable dictionary that covers the most familiar words in Japanese, based on a series of psycholinguistic tests (Kasahara et al., 2004). Lexeed has 28,000 words divided into 46,000 senses and defined with 75,000 definition sentences. Each entry includes a list of orthographic variants, and the pronunciation, in</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christine Fellbaum. 1998. A semantic network of English verbs. In Christine Fellbaum, editor, WordNet: An Electronic Lexical Database, chapter 3, pages 70{ 104. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anette Frank</author>
</authors>
<title>Constraint-based RMRS construction from shallow grammars.</title>
<date>2004</date>
<booktitle>In 20th International Conference on Computational Linguistics: COLING-2004,</booktitle>
<pages>1269--1272</pages>
<location>Geneva.</location>
<contexts>
<context position="2831" citStr="Frank, 2004" startWordPosition="436" endWordPosition="437"> comparison, which goes beyond lexical similarity. We use the RMRS framework as our comparison format because it has the descriptive power to encode the full semantics, including argument structure. It also enables easy combination of deep and shallow information and, due to its flat structure, is easy to manage computationally. 1.1 Robust Minimal Recursion Semantics Robust Minimal Recursion Semantics (RMRS) is a form of flat semantics which is designed to allow deep and shallow processing to use a compatible semantic representation, while being rich enough to support generalized quantifiers (Frank, 2004). The main component of an RMRS representation is a bag of elementary predicates and their arguments. An elementary predicate always has a unique label, a relation type, a relation name and an ARG0 feature. The example in Figure 1 has a label of h5 which uniquely identifies this predicate. Relation types can either be REALPRED for a predicate that relates directly to a content word from the input text, or GPRED for grammatical predicates which may not have a direct referent in the text. For examples in this paper, a REALPRED is distinguished by an underscore ( ) before the relation name. The G</context>
</contexts>
<marker>Frank, 2004</marker>
<rawString>Anette Frank. 2004. Constraint-based RMRS construction from shallow grammars. In 20th International Conference on Computational Linguistics: COLING-2004, pages 1269{1272. Geneva.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tsutomu Hirao</author>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
<author>Eisaku Maeda</author>
</authors>
<title>Dependency-based sentence alignment for multiple document summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of the COLING.</booktitle>
<contexts>
<context position="1859" citStr="Hirao et al. (2004)" startWordPosition="285" endWordPosition="288">urate and informative comparison. The simplest and most commonly used methods of judging sentence similarity use word overlap — either looking for matching word sequences, or comparing a Bag-of-Words representation of each sentence. Bag-of-Words discards word order, and any structure designated by such, so that the cat snored and the dog slept is equivalent to the dog snored and the cat slept. Sequence matching on the other hand requires exact word order matching and hence the game began quietly and the game quietly began are not considered a match. Neither method allows for synonym matching. Hirao et al. (2004) showed that they could get a much more robust comparison using dependency information rather than Bag-ofWords, since they could abstract away from word order but still compare the important elements of a sentence. Using deep parsing information, such as dependencies, but also deep lexical resources where available, enables a much more informative and robust comparison, which goes beyond lexical similarity. We use the RMRS framework as our comparison format because it has the descriptive power to encode the full semantics, including argument structure. It also enables easy combination of deep </context>
</contexts>
<marker>Hirao, Suzuki, Isozaki, Maeda, 2004</marker>
<rawString>Tsutomu Hirao, Jun Suzuki, Hideki Isozaki, and Eisaku Maeda. 2004. Dependency-based sentence alignment for multiple document summarization. In Proceedings of the COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Junichi Fukumoto</author>
<author>Chin-Yew Lin</author>
<author>Liang Zhao</author>
</authors>
<date>2005</date>
<contexts>
<context position="16651" citStr="Hovy et al. (2005" startWordPosition="3036" endWordPosition="3039">ing tools are freely available. We used the Japanese grammar Jacy (Siegel and Bender, 2002), a deep parsing HPSG grammar that produces RMRSs for our primary input source. When parsing with Jacy failed, comparisons could still b e made with RMRS produced from shallow tools such as ChaSen (Matsumoto et al., 2000), a morphological analyser or CaboCha (Kudo and Matsumoto, 2002), a Japanese dependency parser. Tools have been built to produced RMRS from the standard output of both those tools. The CaboCha output supplies similar dependency information to that of the Basic Elements (BE) tool used by Hovy et al. (2005b) for multi-document summarization. Even this intermediate level of parsing gives better comparisons than either word or sequence overlap, since it is easier to compare meaningful elements (Hovy et al., 2005a). 3.2 Lexical Resources Whilst deep lexical resources are not available for every language, where they are available, they should be used to make comparisons more informative. The comparison framework allows for different lexical resources to b e added to a pipeline. The pipeline starts with a simple relation name match, but this could be followed by a dictionary to extract orthographic </context>
</contexts>
<marker>Hovy, Fukumoto, Lin, Zhao, 2005</marker>
<rawString>Eduard Hovy, Junichi Fukumoto, Chin-Yew Lin, and Liang Zhao. 2005a. Basic elements. (http://www. isi.edu/-cyl/BE).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Chin-Yew Lin</author>
<author>Liang Zhao</author>
</authors>
<title>A BE-based multi-document summarizer with sentence compression.</title>
<date>2005</date>
<booktitle>In Proceedings of Multilingual Summarization Evaluation.</booktitle>
<contexts>
<context position="16651" citStr="Hovy et al. (2005" startWordPosition="3036" endWordPosition="3039">ing tools are freely available. We used the Japanese grammar Jacy (Siegel and Bender, 2002), a deep parsing HPSG grammar that produces RMRSs for our primary input source. When parsing with Jacy failed, comparisons could still b e made with RMRS produced from shallow tools such as ChaSen (Matsumoto et al., 2000), a morphological analyser or CaboCha (Kudo and Matsumoto, 2002), a Japanese dependency parser. Tools have been built to produced RMRS from the standard output of both those tools. The CaboCha output supplies similar dependency information to that of the Basic Elements (BE) tool used by Hovy et al. (2005b) for multi-document summarization. Even this intermediate level of parsing gives better comparisons than either word or sequence overlap, since it is easier to compare meaningful elements (Hovy et al., 2005a). 3.2 Lexical Resources Whilst deep lexical resources are not available for every language, where they are available, they should be used to make comparisons more informative. The comparison framework allows for different lexical resources to b e added to a pipeline. The pipeline starts with a simple relation name match, but this could be followed by a dictionary to extract orthographic </context>
</contexts>
<marker>Hovy, Lin, Zhao, 2005</marker>
<rawString>Eduard Hovy, Chin-Yew Lin, and Liang Zhao. 2005b. A BE-based multi-document summarizer with sentence compression. In Proceedings of Multilingual Summarization Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoru Ikehara</author>
<author>Masahiro Miyazaki</author>
<author>Satoshi Shirai</author>
</authors>
<title>Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi Ooyama, and Yoshihiko Hayashi.</title>
<date>1997</date>
<volume>5</volume>
<pages>volumes/CDROM.</pages>
<location>Tokyo.</location>
<contexts>
<context position="17351" citStr="Ikehara et al., 1997" startWordPosition="3149" endWordPosition="3152">ves better comparisons than either word or sequence overlap, since it is easier to compare meaningful elements (Hovy et al., 2005a). 3.2 Lexical Resources Whilst deep lexical resources are not available for every language, where they are available, they should be used to make comparisons more informative. The comparison framework allows for different lexical resources to b e added to a pipeline. The pipeline starts with a simple relation name match, but this could be followed by a dictionary to extract orthographic variants and then by ontologies such as WordNet (Fellbaum, 1998) or GoiTaikei (Ikehara et al., 1997), gazetteers or named entity recognisers to recognise names of people and places. The sections below detail the lexical resources we used within our experiments. 3.2.1 The Lexeed Semantic Database The Lexeed Semantic Database of Japanese is a machine readable dictionary that covers the most familiar words in Japanese, based on a series of psycholinguistic tests (Kasahara et al., 2004). Lexeed has 28,000 words divided into 46,000 senses and defined with 75,000 definition sentences. Each entry includes a list of orthographic variants, and the pronunciation, in addition to the definitions. 3.2.2 </context>
</contexts>
<marker>Ikehara, Miyazaki, Shirai, 1997</marker>
<rawString>Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi Ooyama, and Yoshihiko Hayashi. 1997. Goi-Taikei A Japanese Lexicon. Iwanami Shoten, Tokyo. 5 volumes/CDROM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kaname Kasahara</author>
<author>Hiroshi Sato</author>
<author>Francis Bond</author>
</authors>
<title>Takaaki Tanaka, Sanae Fujita, Tomoko Kanasugi, and Shigeaki Amano.</title>
<date>2004</date>
<booktitle>Construction of a Japanese semantic lexicon: Lexeed. SIG NLC-159, IPSJ,</booktitle>
<location>Tokyo.</location>
<note>(in Japanese).</note>
<contexts>
<context position="17738" citStr="Kasahara et al., 2004" startWordPosition="3209" endWordPosition="3212"> a pipeline. The pipeline starts with a simple relation name match, but this could be followed by a dictionary to extract orthographic variants and then by ontologies such as WordNet (Fellbaum, 1998) or GoiTaikei (Ikehara et al., 1997), gazetteers or named entity recognisers to recognise names of people and places. The sections below detail the lexical resources we used within our experiments. 3.2.1 The Lexeed Semantic Database The Lexeed Semantic Database of Japanese is a machine readable dictionary that covers the most familiar words in Japanese, based on a series of psycholinguistic tests (Kasahara et al., 2004). Lexeed has 28,000 words divided into 46,000 senses and defined with 75,000 definition sentences. Each entry includes a list of orthographic variants, and the pronunciation, in addition to the definitions. 3.2.2 Ontology The lexicon has been sense-tagged and parsed to give an ontology linking senses with various relations, principally hypernym and synonym (Nichols et al., 2005). For example, (HYPERNYM, K74.1�-- doraibd &amp;quot;driver&amp;quot;, �r 77 kurabu &amp;quot;club&amp;quot;). The ontology entries for nouns have been hand checked and corrected, including adding hypernyms for words where 39 the genus term in the definit</context>
</contexts>
<marker>Kasahara, Sato, Bond, 2004</marker>
<rawString>Kaname Kasahara, Hiroshi Sato, Francis Bond, Takaaki Tanaka, Sanae Fujita, Tomoko Kanasugi, and Shigeaki Amano. 2004. Construction of a Japanese semantic lexicon: Lexeed. SIG NLC-159, IPSJ, Tokyo. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Kingsbury</author>
<author>Martha Palmer</author>
<author>Mitch Marcus</author>
</authors>
<title>Adding semantic annotation to the penn treebank.</title>
<date>2002</date>
<booktitle>In Proceedings of the Human Language Technology</booktitle>
<note>Conference.</note>
<contexts>
<context position="4807" citStr="Kingsbury et al., 2002" startWordPosition="782" endWordPosition="785">hen available and hence the unten s 1 is more specific but compatible with unten s or even unten. The ARG0 feature (e6 in Figure 1) is the referential index of the predicate. Predicates with the same ARG0 are said to b e referentially co-indexed and therefore have the same referent in the text. A shallow parse might provide only the features shown in Figure 1, but a deep parse can also give information about other arguments as well as scoping constraints. The features ARG1..ARG4 specify the indices of the semantic arguments of the relevant predicate, similar to PropBank&apos;s argument annotation (Kingsbury et al., 2002). While the RMRS specification does not define semantic roles for the ARGn features, in practice ARG1 is generally used for the AGENT and ARG2 for the PATIENT. Features ARG3 and ARG4 have less consistency in their roles. We will use (1) and (2) as examples of similar sentences. They are definition sentences for one sense of l` 7&apos;f 1�-- doraiba- &amp;quot;driver&amp;quot;, taken from two different lexicons. (1) [AR* ;L- ft T6 A jidosha wo unten suru hito car ACC drive do person &amp;quot;a person who drives a car&amp;quot; (2) [AR* /.� Z 0 ft t jidosha nado no unten sha car etc. ADN drive -er &amp;quot;a driver of cars etc.&amp;quot; Examples of d</context>
</contexts>
<marker>Kingsbury, Palmer, Marcus, 2002</marker>
<rawString>Paul Kingsbury, Martha Palmer, and Mitch Marcus. 2002. Adding semantic annotation to the penn treebank. In Proceedings of the Human Language Technology 2002 Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese dependency analysis using cascaded chunking.</title>
<date>2002</date>
<booktitle>In CoNLL 2002: Proceedings of the 6th Conference on Natural Language Learning 2002 (COLING 2002 Post-Conference Workshops),</booktitle>
<pages>63--69</pages>
<location>Taipei.</location>
<contexts>
<context position="16410" citStr="Kudo and Matsumoto, 2002" startWordPosition="2994" endWordPosition="2997">. The resources fall in to two different categories: parsing and morphological analysis tools that produce the RMRSs, and lexical resources such as ontologies, dictionaries and gazetteers for evaluating matches. 3.1 Parsing Japanese language processing tools are freely available. We used the Japanese grammar Jacy (Siegel and Bender, 2002), a deep parsing HPSG grammar that produces RMRSs for our primary input source. When parsing with Jacy failed, comparisons could still b e made with RMRS produced from shallow tools such as ChaSen (Matsumoto et al., 2000), a morphological analyser or CaboCha (Kudo and Matsumoto, 2002), a Japanese dependency parser. Tools have been built to produced RMRS from the standard output of both those tools. The CaboCha output supplies similar dependency information to that of the Basic Elements (BE) tool used by Hovy et al. (2005b) for multi-document summarization. Even this intermediate level of parsing gives better comparisons than either word or sequence overlap, since it is easier to compare meaningful elements (Hovy et al., 2005a). 3.2 Lexical Resources Whilst deep lexical resources are not available for every language, where they are available, they should be used to make com</context>
</contexts>
<marker>Kudo, Matsumoto, 2002</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2002. Japanese dependency analysis using cascaded chunking. In CoNLL 2002: Proceedings of the 6th Conference on Natural Language Learning 2002 (COLING 2002 Post-Conference Workshops), pages 63{69. Taipei.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuji Matsumoto</author>
<author>Yamashita Kitauchi</author>
<author>Matsuda Hirano</author>
<author>Asahara</author>
</authors>
<title>Nihongo Keitaiso Kaiseki System: Chasen, version 2.2.1 manual edition.</title>
<date>2000</date>
<note>http://chasen.aist-nara.ac.jp.</note>
<contexts>
<context position="16346" citStr="Matsumoto et al., 2000" startWordPosition="2984" endWordPosition="2988">uage independent, the resources required are language specific. The resources fall in to two different categories: parsing and morphological analysis tools that produce the RMRSs, and lexical resources such as ontologies, dictionaries and gazetteers for evaluating matches. 3.1 Parsing Japanese language processing tools are freely available. We used the Japanese grammar Jacy (Siegel and Bender, 2002), a deep parsing HPSG grammar that produces RMRSs for our primary input source. When parsing with Jacy failed, comparisons could still b e made with RMRS produced from shallow tools such as ChaSen (Matsumoto et al., 2000), a morphological analyser or CaboCha (Kudo and Matsumoto, 2002), a Japanese dependency parser. Tools have been built to produced RMRS from the standard output of both those tools. The CaboCha output supplies similar dependency information to that of the Basic Elements (BE) tool used by Hovy et al. (2005b) for multi-document summarization. Even this intermediate level of parsing gives better comparisons than either word or sequence overlap, since it is easier to compare meaningful elements (Hovy et al., 2005a). 3.2 Lexical Resources Whilst deep lexical resources are not available for every lan</context>
</contexts>
<marker>Matsumoto, Kitauchi, Hirano, Asahara, 2000</marker>
<rawString>Yuji Matsumoto, Kitauchi, Yamashita, Hirano, Matsuda, and Asahara. 2000. Nihongo Keitaiso Kaiseki System: Chasen, version 2.2.1 manual edition. http://chasen.aist-nara.ac.jp.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Nichols</author>
<author>Francis Bond</author>
<author>Daniel Flickinger</author>
</authors>
<title>Robust ontology acquisition from machinereadable dictionaries.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Joint Conference on Artificial Intelligence IJCAI-2005,</booktitle>
<pages>1111--1116</pages>
<location>Edinburgh.</location>
<contexts>
<context position="18119" citStr="Nichols et al., 2005" startWordPosition="3266" endWordPosition="3269">r experiments. 3.2.1 The Lexeed Semantic Database The Lexeed Semantic Database of Japanese is a machine readable dictionary that covers the most familiar words in Japanese, based on a series of psycholinguistic tests (Kasahara et al., 2004). Lexeed has 28,000 words divided into 46,000 senses and defined with 75,000 definition sentences. Each entry includes a list of orthographic variants, and the pronunciation, in addition to the definitions. 3.2.2 Ontology The lexicon has been sense-tagged and parsed to give an ontology linking senses with various relations, principally hypernym and synonym (Nichols et al., 2005). For example, (HYPERNYM, K74.1�-- doraibd &amp;quot;driver&amp;quot;, �r 77 kurabu &amp;quot;club&amp;quot;). The ontology entries for nouns have been hand checked and corrected, including adding hypernyms for words where 39 the genus term in the definition was very general, e.g &amp;quot;a word used to refer insultingly to men&amp;quot; where man is a more useful hypernym than word for the defined term yarou. 4 Evaluation We evaluated the performance of the RMRS comparison method in two tasks. First it was used to indicate whether two sentences were possible paraphrases. In the second task, we used the comparison scores to select the most likel</context>
</contexts>
<marker>Nichols, Bond, Flickinger, 2005</marker>
<rawString>Eric Nichols, Francis Bond, and Daniel Flickinger. 2005. Robust ontology acquisition from machinereadable dictionaries. In Proceedings of the International Joint Conference on Artificial Intelligence IJCAI-2005, pages 1111{1116. Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minoru Nishio</author>
<author>Etsutaro Iwabuchi</author>
<author>Shizuo Mizutani</author>
</authors>
<date>1994</date>
<booktitle>Iwanami Kokugo Jiten Dai Go Han Iwanami Japanese Dictionary Edition 5]. Iwanami Shoten,</booktitle>
<location>Tokyo.</location>
<note>(in Japanese).</note>
<contexts>
<context position="18987" citStr="Nishio et al., 1994" startWordPosition="3413" endWordPosition="3416">word used to refer insultingly to men&amp;quot; where man is a more useful hypernym than word for the defined term yarou. 4 Evaluation We evaluated the performance of the RMRS comparison method in two tasks. First it was used to indicate whether two sentences were possible paraphrases. In the second task, we used the comparison scores to select the most likely sentence to contain the answer to a question. 4.1 Paraphrasing In this task we compared definitions sentences for the same head word from two different Japanese dictionaries - the Lexeed dictionary (§3.2.1) and the Iwanami Kokugo Jiten (Iwanami: Nishio et al., 1994), the Japanese dictionary used in the SENSEVAL-2 Japanese lexical task (Shirai, 2002). There are 60,321 headwords and 85,870 word senses in Iwanami. Each sense in the dictionary consists of a sense ID and morphological information (word segmentation, POS tag, base form and reading, all manually postedited). The definitions in Lexeed and Iwanami were linked by headword and three Japanese native speakers assessed each potential pair of sense definitions for the same head word to judge which definitions were describing the same sense. This annotation not only described which sense from each dicti</context>
</contexts>
<marker>Nishio, Iwabuchi, Mizutani, 1994</marker>
<rawString>Minoru Nishio, Etsutaro Iwabuchi, and Shizuo Mizutani. 1994. Iwanami Kokugo Jiten Dai Go Han Iwanami Japanese Dictionary Edition 5]. Iwanami Shoten, Tokyo. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Ritchie</author>
</authors>
<date>2004</date>
<booktitle>Compatible RMRS representations from RASP and the ERG.</booktitle>
<tech>Technical Report UCAM-CL-TR-661.</tech>
<contexts>
<context position="5691" citStr="Ritchie (2004)" startWordPosition="945" endWordPosition="946">ntences. They are definition sentences for one sense of l` 7&apos;f 1�-- doraiba- &amp;quot;driver&amp;quot;, taken from two different lexicons. (1) [AR* ;L- ft T6 A jidosha wo unten suru hito car ACC drive do person &amp;quot;a person who drives a car&amp;quot; (2) [AR* /.� Z 0 ft t jidosha nado no unten sha car etc. ADN drive -er &amp;quot;a driver of cars etc.&amp;quot; Examples of deep and shallow RMRS results for (1) are given in Figure 2. Deep results for (2) are given in Figure 3. 2 Algorithm The matching algorithm is loosely based on RMRS comparison code included in the LKB (Copestake, 2002: (http://www.delph-in. net/lkb/)), which was used in Ritchie (2004), however that code used no outside lexical resources and we have substantially changed the matching algorithm. The comparison algorithm is language independent and can b e used for any RMRS structures. It first compares all elementary predicates from the RMRSs to construct a list of match records and then examines, and potentially alters, the list of match records according to constraints encoded in the ARGn variables. Using the list of scored matches, the lowest scoring possible match set is found and, after further processing on that set, a similarity score is returned. The threshold for de</context>
</contexts>
<marker>Ritchie, 2004</marker>
<rawString>Anna Ritchie. 2004. Compatible RMRS representations from RASP and the ERG. Technical Report UCAM-CL-TR-661.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
<author>Kiyoshi Sudo</author>
<author>Yusuke Shinyama</author>
<author>Chikashi Nobata</author>
<author>Kiyotaka Uchimoto</author>
<author>Hitoshi Isahara</author>
</authors>
<date>2002</date>
<booktitle>NYU/CRL QA system, QAC question analysis and CRL QA data. In Working Notes of NTCIR Workshop</booktitle>
<volume>3</volume>
<contexts>
<context position="22584" citStr="Sekine et al., 2002" startWordPosition="4005" endWordPosition="4008">, another evaluation was performed without the ontology matching. This had an accuracy of 77.3% (78.1% using Jacy, 46.1% coverage). This shows that the information available in the ontology definitely improves scores, but that even without that sort of deep lexical resource, the RMRS matching can still improve on Bag-of-Words using just surface form abstraction and argument matching. 40 4.2 Answer Sentence Selection To emulate a part of the question answering pipeline, we used a freely available set of 2000 Japanese questions, annotated with, among other things, answer and answer document ID (Sekine et al., 2002). The document IDs for the answer containing documents refer to the Mainichi Newspaper 1995 corpus which has been used as part of the document collection for NTCIR&apos;s Question Answering Challenges. The documents range in length from 2 to 83 sentences. 4.2.1 Method For every question, we compared it to each sentence in the answer document. The sentence that has the best similarity to the question is returned as the most likely to contain the answer. For this sort of comparison, an entails option was added that changes the similarity scoring method slightly so that only non-matches in the first s</context>
</contexts>
<marker>Sekine, Sudo, Shinyama, Nobata, Uchimoto, Isahara, 2002</marker>
<rawString>Satoshi Sekine, Kiyoshi Sudo, Yusuke Shinyama, Chikashi Nobata, Kiyotaka Uchimoto, and Hitoshi Isahara. 2002. NYU/CRL QA system, QAC question analysis and CRL QA data. In Working Notes of NTCIR Workshop 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyoaki Shirai</author>
</authors>
<title>Construction of a word sense tagged corpus for SENSEVAL-2 Japanese dictionary task.</title>
<date>2002</date>
<booktitle>In Third International Conference on Language Resources and Evaluation (LREC-2002),</booktitle>
<pages>605--608</pages>
<contexts>
<context position="19072" citStr="Shirai, 2002" startWordPosition="3427" endWordPosition="3428"> defined term yarou. 4 Evaluation We evaluated the performance of the RMRS comparison method in two tasks. First it was used to indicate whether two sentences were possible paraphrases. In the second task, we used the comparison scores to select the most likely sentence to contain the answer to a question. 4.1 Paraphrasing In this task we compared definitions sentences for the same head word from two different Japanese dictionaries - the Lexeed dictionary (§3.2.1) and the Iwanami Kokugo Jiten (Iwanami: Nishio et al., 1994), the Japanese dictionary used in the SENSEVAL-2 Japanese lexical task (Shirai, 2002). There are 60,321 headwords and 85,870 word senses in Iwanami. Each sense in the dictionary consists of a sense ID and morphological information (word segmentation, POS tag, base form and reading, all manually postedited). The definitions in Lexeed and Iwanami were linked by headword and three Japanese native speakers assessed each potential pair of sense definitions for the same head word to judge which definitions were describing the same sense. This annotation not only described which sense from each dictionary matched, but also whether the definitions were equal, equivalent, or subsuming.</context>
</contexts>
<marker>Shirai, 2002</marker>
<rawString>Kiyoaki Shirai. 2002. Construction of a word sense tagged corpus for SENSEVAL-2 Japanese dictionary task. In Third International Conference on Language Resources and Evaluation (LREC-2002), pages 605{608.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Melanie Siegel</author>
<author>Emily M Bender</author>
</authors>
<title>Efficient deep processing of Japanese.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd Workshop on Asian Language Resources and International Standardization at the 19th International Conference on Computational Linguistics.</booktitle>
<location>Taipei.</location>
<contexts>
<context position="16125" citStr="Siegel and Bender, 2002" startWordPosition="2946" endWordPosition="2949">requires. This shows which predicates were deemed to match, and with what score, and also shows the unmatched predicates. Figure 8 shows the output of our example comparison. 3 Resources While the comparison method is language independent, the resources required are language specific. The resources fall in to two different categories: parsing and morphological analysis tools that produce the RMRSs, and lexical resources such as ontologies, dictionaries and gazetteers for evaluating matches. 3.1 Parsing Japanese language processing tools are freely available. We used the Japanese grammar Jacy (Siegel and Bender, 2002), a deep parsing HPSG grammar that produces RMRSs for our primary input source. When parsing with Jacy failed, comparisons could still b e made with RMRS produced from shallow tools such as ChaSen (Matsumoto et al., 2000), a morphological analyser or CaboCha (Kudo and Matsumoto, 2002), a Japanese dependency parser. Tools have been built to produced RMRS from the standard output of both those tools. The CaboCha output supplies similar dependency information to that of the Basic Elements (BE) tool used by Hovy et al. (2005b) for multi-document summarization. Even this intermediate level of parsi</context>
</contexts>
<marker>Siegel, Bender, 2002</marker>
<rawString>Melanie Siegel and Emily M. Bender. 2002. Efficient deep processing of Japanese. In Proceedings of the 3rd Workshop on Asian Language Resources and International Standardization at the 19th International Conference on Computational Linguistics. Taipei.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucy Vanderwende</author>
<author>Deborah Coughlin</author>
<author>Bill Dolan</author>
</authors>
<title>What syntax can contribute in entailment task.</title>
<date>2005</date>
<booktitle>In Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment.</booktitle>
<contexts>
<context position="24979" citStr="Vanderwende et al. (2005)" startWordPosition="4413" endWordPosition="4416">ed entities. Adding name lists of people, places and organisations would greatly improve the matching in this instance. 5 Future Directions 5.1 Applications Since the comparison method was written to b e language independent, the next stage of evaluation would b e to use it in a nonJapanese task. The PASCAL Recognising Textual Entailment (RTE) Challenge (Dagan et al., 2005) is one recent English task where participants used sentence comparison extensively. While the task appears to call for inference and reasoning, the top 5 participating groups used statistical methods and word overlap only. Vanderwende et al. (2005) did a manual evaluation of the test data and found that 37% could b e decided on syntactic information alone, while adding a thesaurus could increase that coverage to 49%. This means that RMRS comparison has the potential to perform well. Not only does it improve on basic word overlap, but it allows for easy addition of a thesaurus or dictionary. Further, because of the detailed match output available, the method could be extended in post processing to encompass some basic inference methods. Aside from comparing sentences, the RMRS comparison can b e used to compare the RMRS output of differe</context>
</contexts>
<marker>Vanderwende, Coughlin, Dolan, 2005</marker>
<rawString>Lucy Vanderwende, Deborah Coughlin, and Bill Dolan. 2005. What syntax can contribute in entailment task. In Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>