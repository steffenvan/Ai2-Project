<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.084332">
<title confidence="0.9994225">
Reducing redundancy in multi-document summarization
using lexical semantic similarity
</title>
<author confidence="0.997951">
Iris Hendrickx, Walter Daelemans
</author>
<affiliation confidence="0.9035165">
University of Antwerp
Antwerpen, Belgium
</affiliation>
<email confidence="0.931549">
iris.hendrickx@ua.ac.be
walter.daelemans@ua.ac.be
</email>
<sectionHeader confidence="0.993593" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999484">
We present an automatic multi-document
summarization system for Dutch based on
the MEAD system. We focus on redun-
dancy detection, an essential ingredient of
multi-document summarization. We in-
troduce a semantic overlap detection tool,
which goes beyond simple string match-
ing. Our results so far do not confirm
our expectation that this tool would out-
perform the other tested methods.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999748142857143">
One of the main issues in automatic multi-
document summarization is avoiding redundancy.
As the source documents are all related to the
same topic, at least some of their content is likely
to overlap. In fact, this is in part what makes
multi-document summarization feasible. For ex-
ample, news articles that report on a particular
event, or that are based on the same source, often
contain similar information expressed in differ-
ent ways. A multi-document summarizer should
include this overlapping information not more
than once. The backbone of most current ap-
proaches to automatic summarization is a vector
space model in which a sentence is regarded as
a bag of words and a weighted cosine similarity
measure is used to quantify the amount of shared
information between a pair of sentences. Cosine
similarity (in this context) essentially amounts to
calculating word overlap, albeit with weighting of
the terms and normalization for differences in sen-
tence length. It is clear that this approach to detect-
ing redundancy is far from satisfactory, because it
only covers redundancy in its most trivial form,
i.e., identical words. In contrast, the redundancy
that we ultimately want to avoid in summarization
is that at the semantic level. As an extreme case
in point, two sentences with no words in common
can still carry virtually the same meaning.
</bodyText>
<author confidence="0.838912">
Erwin Marsi, Emiel Krahmer
</author>
<affiliation confidence="0.8095975">
Tilburg University
Tilburg, The Netherlands
</affiliation>
<email confidence="0.8058075">
e.j.krahmer@uvt.nl
e.c.marsi@uvt.nl
</email>
<bodyText confidence="0.998650222222222">
The remainder of this paper is structured in
the following way. In Section 2 we introduce a
tool for detecting semantic overlap. In section 3
we present a Dutch multi-document summariza-
tion system, based on the MEAD summarization
toolkit (Radev et al., 2004). Next, in section 4 we
describe the experimental setup and the data set
that we used. Section 5 reports on the results, and
we conclude in section 6.
</bodyText>
<sectionHeader confidence="0.919041" genericHeader="method">
2 Detecting semantic overlap
</sectionHeader>
<bodyText confidence="0.999492928571428">
In this section, we detail the semantic overlap de-
tection tool and the resources we build on.
Parallel/comparable text corpus The basis for
our semantic overlap detection tool is a mono-
lingual parallel/comparable tree-bank of 1 million
words of Dutch text (Marsi and Krahmer, 2007).
Half of the text material has so far been manually
aligned at the sentence level. Subsequently, the
sentences have been parsed and the resulting parse
trees have been aligned at the level of syntactic
nodes. Moreover, aligned nodes have been labeled
according to a set of semantic similarity labels that
express the type of similarity relation between the
nodes. The following five labels are used: gen-
eralize, specify, intersect, restate, and equal. The
corpus serves as the basis for developing tools for
automatic alignment and relation labeling.
Word aligner The word alignment tool takes as
input a pair of source and target sentences and
produces a matching between the words, that is,
a (possibly partial) one-to-one mapping of source
to target words. This aligner is a part of the full
fledged tree aligner currently under development.
The alignment task comprises several subtasks.
First, the input sentences are tokenized and parsed
with the Alpino syntactic parser for Dutch (Bouma
et al., 2001). Apart from the syntactic analysis,
which we disregard in the current work, the parser
</bodyText>
<page confidence="0.99537">
63
</page>
<note confidence="0.9852385">
Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 63–66,
Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999887979591837">
performs lemmatization, part-of-speech tagging
and compound analysis, all of which are used here.
In addition, the aligner uses lexical-semantic
knowledge from Cornetto, a lexical database for
Dutch (40K entries) similar to the well-known En-
glish WordNet (Vossen et al., 2008). The rela-
tions we use are synonym, hyperonym, and xpos-
near-synonym (align near synonyms with differ-
ent POS labels). In addition we check whether
a pair of content words has a least common sub-
sumer (LCS) in the hyperonym hierarchy. As path
length has been shown to be a poor predictor in
this respect, we calculate the Lin similarity, which
combines the Information Content of the words in-
volved (Lin, 1998). A current limitation is that
we lack word sense disambiguation, hence we take
the maximal score over all the senses of the words.
The components described above can be con-
sidered as experts which predict word alignments
with a certain probability. Since alignments can
support, complement or contradict each other, we
are faced with the problem of how to combine
the evidence. Our approach is to view the align-
ment as a weighted bipartite multigraph. That is,
a graph where source and target nodes are in dis-
joint sets, multiple edges are allowed between the
same pair of nodes, and edges have an associated
weight. Our goal is on the one hand to maximize
the sum of the edge weights, and on the other hand
to reduce this graph to a model in which every
node can have at most one associated edge. This
is a combinatorial optimization problem known as
the assignment problem for which efficient algo-
rithms exist. We use a variant of the The Hungar-
ian Algorithm1 (Kuhn, 1955), for the computation
of the matches.
Sentence similarity score Given a word align-
ment between a pair of sentences, a similarity
score is required to measure the amount of se-
mantic overlap or redundancy. Evidently the sim-
ilarity score should be proportional to the relative
number of aligned words. However, some align-
ments are more important than others. For exam-
ple, the alignment between two determiners (e.g.
the) is less significant than that between two com-
mon nouns. This is modeled in our similarity score
by weighting alignments according to the idf (in-
verse document frequency) (Sp¨arck Jones, 1972)
of the words involved.
</bodyText>
<footnote confidence="0.504051">
1Also known as the Munkres algorithm
</footnote>
<equation confidence="0.990613">
11 idf(wz)
sim(s1, s2) = wi∈A
</equation>
<bodyText confidence="0.99764825">
Here s1 and s2 are sentences, 5 is the longest of
the two sentences, wj are the words in 5, A is the
subsequence of aligned words in 5, and wz are the
words in A.
</bodyText>
<sectionHeader confidence="0.996429" genericHeader="method">
3 Multi-document summarization
</sectionHeader>
<bodyText confidence="0.99989735">
The Dutch Multi-Document Summarizer pre-
sented here is based on the MEAD summariza-
tion toolkit (Radev et al., 2004), which offers a
wide range of summarization algorithms and has a
flexible structure. The system creates a summary
by extracting a subset of sentences from the orig-
inal documents. The summarizer reads in a clus-
ter of documents, i.e. a set of documents relevant
for the same topic, and for each sentence it ex-
tracts a set of features. These features are com-
bined to determine an importance score for each
sentence. Next the sentences are sorted accord-
ing to their importance score. The system starts a
summary by adding the sentence with the highest
weight. Then it examines the second most impor-
tant sentence and measures the similarity with the
sentence that is already added. If the overlap is
limited, the sentence is added to the summary, oth-
erwise it is disregarded. This process is repeated
until the intended summary size is reached. The
module that performs this last step of determining
which sentences end up in the final summary is
called the reranker.
We use two baseline systems: the random base-
line system randomly selects a set of sentences
and the lead-based system which selects a sub-
set of initial sentences as summary. We investi-
gated the following features. A simple and effec-
tive features is the position: each sentence gets a
score of 1/position where ‘position’ is the place
in the document. The length feature is a filter that
removes sentences shorter than the given thresh-
old. The simwf feature presents the overlap of a
sentence with the title of the document computed
with cosine similarity. One of MEAD’s main fea-
tures is centroid-based summarization. Centroids
of clusters are used to determine which words
are important for the cluster and sentences con-
taining these words are considered to be central
sentences. The words are weighted with tf*idf.
</bodyText>
<equation confidence="0.679094">
11 idf(wj)
wj∈S
(1)
</equation>
<page confidence="0.99043">
64
</page>
<bodyText confidence="0.999934896551724">
The aim of query-based summarization is to cre-
ate summaries that are relevant with respect to a
particular query. This can easily be done with fea-
tures that express the overlap between the query
and a source sentence. We examined three differ-
ent query-based features that measure simple word
overlap between the query and the sentence, co-
sine similarity with tf*idf weighting of words and
cosine similarity without tf*idf weighting.
The MEAD toolkit implements multiple
reranker modules, we investigated the following
three: the cosine-reranker, the mmr-reranker and
novelty-reranker. We compare these rerankers
against the semantic overlap detection (sod)
tool detailed in section 2. The cosine-reranker
represents two sentences as tf*idf weighted word
vectors and computes a cosine similarity score
between them. Sentences with a cosine similarity
above the threshold are disregarded. The mmr-
reranker module is based on the maximal margin
relevance criterion (Carbonell and Goldstein,
1998). MMR models the trade-off between a
focused summary and a summary with a wide
scope. The novelty-reranker is an extension of the
cosine-reranker and boosts sentences occurring
after an important sentence by multiplying with
1.2. The reranker tries to mimic human behavior
as people tend to pick clusters of sentences when
summarizing.
</bodyText>
<sectionHeader confidence="0.998522" genericHeader="method">
4 Experimental setup
</sectionHeader>
<bodyText confidence="0.999981558139535">
To perform proper evaluation of the summariza-
tion system we constructed a new data set for eval-
uating Dutch multi-document summarization. It
consists of 30 query-based document clusters. The
document clusters were created manually follow-
ing the guidelines of DUC 2006 (Dang, 2006).
Each cluster contains a query description and 5 to
25 newspaper articles relevant for that particular
question. For each cluster five annotators wrote
an abstract of approximately 250 words. These
summaries serve as a gold standard for compari-
son with automatically generated extracts.
We split our data set in a test set of 20 clus-
ters and a development set of 10 clusters. We use
the development set for parameter tuning and fea-
ture selection for the summarizer. We try out each
of the characteristics discussed in section 3. The
best combination found on the development set is
the feature combination position, centroid, length
with cut-off 13, and queryCosine. We tested the
different rerankers and vary the similarity thresh-
olds to determine their optimal threshold value. As
the novelty-reranker scored lower than the other
rerankers on the development set, we did not in-
clude it in our experiments on the test set.
For the experiments on the development set, we
compare each of the automatically produced ex-
tracts with five manually written summaries and
report macro-average Rouge-2 and Rouge-SU4
scores (Lin and Hovy, 2003). For the experiments
on the test set, we also perform a manual evalu-
ation. We follow the DUC 2006 guidelines for
manual evaluation of responsiveness and the lin-
guistic quality of the produced summaries. The re-
sponsiveness scores express the information con-
tent of the summary with respect to the query. The
linguistic quality is evaluated on five different ob-
jectives: grammaticality, non-redundancy, coher-
ence, referential clarity and focus. The annotators
can choose a value on a five point scale where
1 means ‘very poor’ and 5 means ‘very good’.
We use two independent annotators to evaluate the
summaries and we report the average scores.
</bodyText>
<sectionHeader confidence="0.999813" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.99994152173913">
The evaluation of the results on the test set are
shown in table 1. The Rouge scores of the different
rerankers are all above both baselines, and they are
very close to each other. The scores for the content
measure and responsiveness show that the values
for the automatic summaries are between 2 (poor)
and 3 (barely acceptable). The optimized summa-
rizers score higher than the two baselines on this
point.
We are most interested in the aspect of ’non-
redundancy’. The random baseline system
achieves a good result here, and the optimized
summarizers all score lower. The chance of over-
lap between randomly selected sentences seems
to be lower than when an automatic summarizer
tries to select only the most important sentences.
When we compare the three optimized systems
with different rerankers on this aspect we see that
the scores are very close. Our semantic overlap de-
tection (sod) reranker does not do any better than
the other two. The optimized summarizers do per-
form better than the baseline systems with respect
to focus and structure.
</bodyText>
<page confidence="0.998887">
65
</page>
<table confidence="0.999667333333333">
setting Rouge-2 Rouge-SU4 gram redun ref focus struct respons
rand baseline 0.101 0.153 4.08 3.9 2.58 2.6 2 2.25
lead baseline 0.139 0.179 3.05 3.6 3.25 2.88 2.38 2.4
optim-cosine 0.152 0.193 3.9 3.18 2.65 3.15 2.43 2.75
optim-mmr 0.149 0.191 3.98 3.13 2.55 3.13 2.38 2.7
optim-sod 0.150 0.193 4.05 3.13 2.85 3.23 2.5 2.7
</table>
<tableCaption confidence="0.9489975">
Table 1: Macro-average Rouge scores and manual evaluation on the test set on these aspects:
grammaticality, non-redundancy, referential clarity, focus, structure and responsiveness.
</tableCaption>
<sectionHeader confidence="0.990517" genericHeader="conclusions">
6 Discussion and conclusion
</sectionHeader>
<bodyText confidence="0.956320285714286">
We presented an automatic multi-document sum-
marization system for Dutch based on the MEAD
system, supporting the claim that MEAD is largely
language-independent. We experimented with dif-
ferent features and parameter settings of the sum-
marizer, and optimized it for summarization of
Dutch newspaper text. We presented a semantic
overlap detection tool, developed on the basis of a
monolingual corpus of parallel/comparable Dutch
text, which goes beyond simple string matching.
We expected this tool to improve the sentence
reranking step, thereby reducing redundancy in the
summaries. However, we were unable to show a
significant effect. We have several possible expla-
nations for this. First, many of the sentence pairs
that share the same semantic content, also share a
number of identical words. To detect these cases,
therefore, computing cosine similarity may be just
as effective. Second, the accuracy of the align-
ment tool may not be good enough, partly because
of errors in the linguistic analysis or lack of cover-
age, and partly because certain types of knowledge
(word sense, syntactic structure) are not yet ex-
ploited. Third, reranking of sentences is unlikely
to improve the summary in cases where the pre-
ceding step of sentence ranking within documents
performs poorly. We are currently still investigat-
ing this matter and hope to obtain significant re-
sults with an improved version of our tool for de-
tecting semantic overlap.
We plan to work on a more refined version that
not only uses word alignment but also considers
alignments at the parse tree level. This idea is
in line with the work of Barzilay and McKeown
(2005) who use this type of technique to fuse sim-
ilar sentences for multi-document summarization.
Acknowledgements This work was conducted within the
DAESO http://daeso.uvt.nl project funded by the
Stevin program (De Nederlandse Taalunie). The construction
of the evaluation corpus described in this paper was financed
by KP BOF 2008, University of Antwerp. We would like to
thank NIST for kindly sharing their DUC 2006 guidelines.
</bodyText>
<sectionHeader confidence="0.997949" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999912078947368">
Regina Barzilay and Kathleen R. McKeown. 2005. Sentence
fusion for multidocument news summarization. Compu-
tational Linguistics, 31(3):297–328.
Gosse Bouma, Gertjan van Noord, and Robert Malouf. 2001.
Alpino: Wide-coverage computational analysis of Dutch.
In Computational Linguistics in the Netherlands 2000.,
pages 45–59. Rodopi, Amsterdam, New York.
Jaime Carbonell and Jade Goldstein. 1998. The use of
mmr, diversity-based reranking for reordering documents
and producing summaries. In Proceedings of SIGIR 1998,
pages 335–336, New York, NY, USA. ACM.
H.T. Dang. 2006. Overview of DUC 2006. In Proceedings
of the Document Understanding Workshop, pages 1–10,
Brooklyn, USA.
Harold W. Kuhn. 1955. The Hungarian Method for the as-
signment problem. Naval Research Logistics Quarterly,
2:83–97.
C.-Y. Lin and E.H. Hovy. 2003. Automatic evaluation
of summaries using n-gram co-occurrence statistics. In
Proceedings of HLT-NAACL, pages 71 – 78, Edmonton,
Canada.
D. Lin. 1998. An information-theoretic definition of similar-
ity. In Proceedings of the ICML, pages 296–304.
Erwin Marsi and Emiel Krahmer. 2007. Annotating a par-
allel monolingual treebank with semantic similarity re-
lations. In Proceedings of the 6th International Work-
shop on Treebanks and Linguistic Theories, pages 85–96,
Bergen, Norway.
Dragomir Radev et al. 2004. Mead - a platform for multidoc-
ument multilingual text summarization. In Proceedings of
LREC 2004, Lisabon, Portugal.
Karen Sp¨arck Jones. 1972. A statistical interpretation of
term specificity and its application in retrieval. Journal
of Documentation, 28(1):11–21.
P. Vossen, I. Maks, R. Segers, and H. van der Vliet. 2008.
Integrating lexical units, synsets and ontology in the Cor-
netto Database. In Proceedings of LREC 2008, Mar-
rakech, Morocco.
</reference>
<page confidence="0.989004">
66
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.635553">
<title confidence="0.993677">Reducing redundancy in multi-document using lexical semantic similarity</title>
<author confidence="0.999771">Iris Hendrickx</author>
<author confidence="0.999771">Walter</author>
<affiliation confidence="0.9997">University of</affiliation>
<address confidence="0.653966">Antwerpen,</address>
<email confidence="0.996515">walter.daelemans@ua.ac.be</email>
<abstract confidence="0.998596181818182">We present an automatic multi-document summarization system for Dutch based on the MEAD system. We focus on redundancy detection, an essential ingredient of multi-document summarization. We introduce a semantic overlap detection tool, which goes beyond simple string matching. Our results so far do not confirm our expectation that this tool would outperform the other tested methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Sentence fusion for multidocument news summarization.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>3</issue>
<marker>Barzilay, McKeown, 2005</marker>
<rawString>Regina Barzilay and Kathleen R. McKeown. 2005. Sentence fusion for multidocument news summarization. Computational Linguistics, 31(3):297–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gosse Bouma</author>
<author>Gertjan van Noord</author>
<author>Robert Malouf</author>
</authors>
<title>Alpino: Wide-coverage computational analysis of Dutch.</title>
<date>2001</date>
<booktitle>In Computational Linguistics in the Netherlands</booktitle>
<pages>45--59</pages>
<location>Rodopi, Amsterdam, New York.</location>
<marker>Bouma, van Noord, Malouf, 2001</marker>
<rawString>Gosse Bouma, Gertjan van Noord, and Robert Malouf. 2001. Alpino: Wide-coverage computational analysis of Dutch. In Computational Linguistics in the Netherlands 2000., pages 45–59. Rodopi, Amsterdam, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaime Carbonell</author>
<author>Jade Goldstein</author>
</authors>
<title>The use of mmr, diversity-based reranking for reordering documents and producing summaries.</title>
<date>1998</date>
<booktitle>In Proceedings of SIGIR</booktitle>
<pages>335--336</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="9517" citStr="Carbonell and Goldstein, 1998" startWordPosition="1538" endWordPosition="1541">idf weighting of words and cosine similarity without tf*idf weighting. The MEAD toolkit implements multiple reranker modules, we investigated the following three: the cosine-reranker, the mmr-reranker and novelty-reranker. We compare these rerankers against the semantic overlap detection (sod) tool detailed in section 2. The cosine-reranker represents two sentences as tf*idf weighted word vectors and computes a cosine similarity score between them. Sentences with a cosine similarity above the threshold are disregarded. The mmrreranker module is based on the maximal margin relevance criterion (Carbonell and Goldstein, 1998). MMR models the trade-off between a focused summary and a summary with a wide scope. The novelty-reranker is an extension of the cosine-reranker and boosts sentences occurring after an important sentence by multiplying with 1.2. The reranker tries to mimic human behavior as people tend to pick clusters of sentences when summarizing. 4 Experimental setup To perform proper evaluation of the summarization system we constructed a new data set for evaluating Dutch multi-document summarization. It consists of 30 query-based document clusters. The document clusters were created manually following th</context>
</contexts>
<marker>Carbonell, Goldstein, 1998</marker>
<rawString>Jaime Carbonell and Jade Goldstein. 1998. The use of mmr, diversity-based reranking for reordering documents and producing summaries. In Proceedings of SIGIR 1998, pages 335–336, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Dang</author>
</authors>
<title>Overview of DUC</title>
<date>2006</date>
<booktitle>In Proceedings of the Document Understanding Workshop,</booktitle>
<pages>1--10</pages>
<location>Brooklyn, USA.</location>
<contexts>
<context position="10154" citStr="Dang, 2006" startWordPosition="1638" endWordPosition="1639"> between a focused summary and a summary with a wide scope. The novelty-reranker is an extension of the cosine-reranker and boosts sentences occurring after an important sentence by multiplying with 1.2. The reranker tries to mimic human behavior as people tend to pick clusters of sentences when summarizing. 4 Experimental setup To perform proper evaluation of the summarization system we constructed a new data set for evaluating Dutch multi-document summarization. It consists of 30 query-based document clusters. The document clusters were created manually following the guidelines of DUC 2006 (Dang, 2006). Each cluster contains a query description and 5 to 25 newspaper articles relevant for that particular question. For each cluster five annotators wrote an abstract of approximately 250 words. These summaries serve as a gold standard for comparison with automatically generated extracts. We split our data set in a test set of 20 clusters and a development set of 10 clusters. We use the development set for parameter tuning and feature selection for the summarizer. We try out each of the characteristics discussed in section 3. The best combination found on the development set is the feature combi</context>
</contexts>
<marker>Dang, 2006</marker>
<rawString>H.T. Dang. 2006. Overview of DUC 2006. In Proceedings of the Document Understanding Workshop, pages 1–10, Brooklyn, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold W Kuhn</author>
</authors>
<title>The Hungarian Method for the assignment problem.</title>
<date>1955</date>
<journal>Naval Research Logistics Quarterly,</journal>
<pages>2--83</pages>
<contexts>
<context position="5714" citStr="Kuhn, 1955" startWordPosition="915" endWordPosition="916"> the evidence. Our approach is to view the alignment as a weighted bipartite multigraph. That is, a graph where source and target nodes are in disjoint sets, multiple edges are allowed between the same pair of nodes, and edges have an associated weight. Our goal is on the one hand to maximize the sum of the edge weights, and on the other hand to reduce this graph to a model in which every node can have at most one associated edge. This is a combinatorial optimization problem known as the assignment problem for which efficient algorithms exist. We use a variant of the The Hungarian Algorithm1 (Kuhn, 1955), for the computation of the matches. Sentence similarity score Given a word alignment between a pair of sentences, a similarity score is required to measure the amount of semantic overlap or redundancy. Evidently the similarity score should be proportional to the relative number of aligned words. However, some alignments are more important than others. For example, the alignment between two determiners (e.g. the) is less significant than that between two common nouns. This is modeled in our similarity score by weighting alignments according to the idf (inverse document frequency) (Sp¨arck Jon</context>
</contexts>
<marker>Kuhn, 1955</marker>
<rawString>Harold W. Kuhn. 1955. The Hungarian Method for the assignment problem. Naval Research Logistics Quarterly, 2:83–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Lin</author>
<author>E H Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram co-occurrence statistics.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL, pages 71 – 78,</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="11289" citStr="Lin and Hovy, 2003" startWordPosition="1820" endWordPosition="1823">in section 3. The best combination found on the development set is the feature combination position, centroid, length with cut-off 13, and queryCosine. We tested the different rerankers and vary the similarity thresholds to determine their optimal threshold value. As the novelty-reranker scored lower than the other rerankers on the development set, we did not include it in our experiments on the test set. For the experiments on the development set, we compare each of the automatically produced extracts with five manually written summaries and report macro-average Rouge-2 and Rouge-SU4 scores (Lin and Hovy, 2003). For the experiments on the test set, we also perform a manual evaluation. We follow the DUC 2006 guidelines for manual evaluation of responsiveness and the linguistic quality of the produced summaries. The responsiveness scores express the information content of the summary with respect to the query. The linguistic quality is evaluated on five different objectives: grammaticality, non-redundancy, coherence, referential clarity and focus. The annotators can choose a value on a five point scale where 1 means ‘very poor’ and 5 means ‘very good’. We use two independent annotators to evaluate the</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>C.-Y. Lin and E.H. Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proceedings of HLT-NAACL, pages 71 – 78, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of the ICML,</booktitle>
<pages>296--304</pages>
<contexts>
<context position="4739" citStr="Lin, 1998" startWordPosition="743" endWordPosition="744"> of which are used here. In addition, the aligner uses lexical-semantic knowledge from Cornetto, a lexical database for Dutch (40K entries) similar to the well-known English WordNet (Vossen et al., 2008). The relations we use are synonym, hyperonym, and xposnear-synonym (align near synonyms with different POS labels). In addition we check whether a pair of content words has a least common subsumer (LCS) in the hyperonym hierarchy. As path length has been shown to be a poor predictor in this respect, we calculate the Lin similarity, which combines the Information Content of the words involved (Lin, 1998). A current limitation is that we lack word sense disambiguation, hence we take the maximal score over all the senses of the words. The components described above can be considered as experts which predict word alignments with a certain probability. Since alignments can support, complement or contradict each other, we are faced with the problem of how to combine the evidence. Our approach is to view the alignment as a weighted bipartite multigraph. That is, a graph where source and target nodes are in disjoint sets, multiple edges are allowed between the same pair of nodes, and edges have an a</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998. An information-theoretic definition of similarity. In Proceedings of the ICML, pages 296–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erwin Marsi</author>
<author>Emiel Krahmer</author>
</authors>
<title>Annotating a parallel monolingual treebank with semantic similarity relations.</title>
<date>2007</date>
<booktitle>In Proceedings of the 6th International Workshop on Treebanks and Linguistic Theories,</booktitle>
<pages>85--96</pages>
<location>Bergen,</location>
<contexts>
<context position="2797" citStr="Marsi and Krahmer, 2007" startWordPosition="431" endWordPosition="434">l for detecting semantic overlap. In section 3 we present a Dutch multi-document summarization system, based on the MEAD summarization toolkit (Radev et al., 2004). Next, in section 4 we describe the experimental setup and the data set that we used. Section 5 reports on the results, and we conclude in section 6. 2 Detecting semantic overlap In this section, we detail the semantic overlap detection tool and the resources we build on. Parallel/comparable text corpus The basis for our semantic overlap detection tool is a monolingual parallel/comparable tree-bank of 1 million words of Dutch text (Marsi and Krahmer, 2007). Half of the text material has so far been manually aligned at the sentence level. Subsequently, the sentences have been parsed and the resulting parse trees have been aligned at the level of syntactic nodes. Moreover, aligned nodes have been labeled according to a set of semantic similarity labels that express the type of similarity relation between the nodes. The following five labels are used: generalize, specify, intersect, restate, and equal. The corpus serves as the basis for developing tools for automatic alignment and relation labeling. Word aligner The word alignment tool takes as in</context>
</contexts>
<marker>Marsi, Krahmer, 2007</marker>
<rawString>Erwin Marsi and Emiel Krahmer. 2007. Annotating a parallel monolingual treebank with semantic similarity relations. In Proceedings of the 6th International Workshop on Treebanks and Linguistic Theories, pages 85–96, Bergen, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir Radev</author>
</authors>
<title>Mead - a platform for multidocument multilingual text summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC</booktitle>
<location>Lisabon, Portugal.</location>
<marker>Radev, 2004</marker>
<rawString>Dragomir Radev et al. 2004. Mead - a platform for multidocument multilingual text summarization. In Proceedings of LREC 2004, Lisabon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Sp¨arck Jones</author>
</authors>
<title>A statistical interpretation of term specificity and its application in retrieval.</title>
<date>1972</date>
<journal>Journal of Documentation,</journal>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="6323" citStr="Jones, 1972" startWordPosition="1014" endWordPosition="1015">55), for the computation of the matches. Sentence similarity score Given a word alignment between a pair of sentences, a similarity score is required to measure the amount of semantic overlap or redundancy. Evidently the similarity score should be proportional to the relative number of aligned words. However, some alignments are more important than others. For example, the alignment between two determiners (e.g. the) is less significant than that between two common nouns. This is modeled in our similarity score by weighting alignments according to the idf (inverse document frequency) (Sp¨arck Jones, 1972) of the words involved. 1Also known as the Munkres algorithm 11 idf(wz) sim(s1, s2) = wi∈A Here s1 and s2 are sentences, 5 is the longest of the two sentences, wj are the words in 5, A is the subsequence of aligned words in 5, and wz are the words in A. 3 Multi-document summarization The Dutch Multi-Document Summarizer presented here is based on the MEAD summarization toolkit (Radev et al., 2004), which offers a wide range of summarization algorithms and has a flexible structure. The system creates a summary by extracting a subset of sentences from the original documents. The summarizer reads </context>
</contexts>
<marker>Jones, 1972</marker>
<rawString>Karen Sp¨arck Jones. 1972. A statistical interpretation of term specificity and its application in retrieval. Journal of Documentation, 28(1):11–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Vossen</author>
<author>I Maks</author>
<author>R Segers</author>
<author>H van der Vliet</author>
</authors>
<title>Integrating lexical units, synsets and ontology in the Cornetto Database.</title>
<date>2008</date>
<booktitle>In Proceedings of LREC 2008,</booktitle>
<location>Marrakech, Morocco.</location>
<marker>Vossen, Maks, Segers, van der Vliet, 2008</marker>
<rawString>P. Vossen, I. Maks, R. Segers, and H. van der Vliet. 2008. Integrating lexical units, synsets and ontology in the Cornetto Database. In Proceedings of LREC 2008, Marrakech, Morocco.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>