<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002457">
<title confidence="0.993285">
Annotating Expressions of Appraisal in English
</title>
<author confidence="0.998058">
Jonathon Read, David Hope and John Carroll
</author>
<affiliation confidence="0.9965035">
Department of Informatics
University of Sussex
</affiliation>
<address confidence="0.796081">
United Kingdom
</address>
<email confidence="0.999156">
{j.l.read,drh21,j.a.carroll}@sussex.ac.uk
</email>
<sectionHeader confidence="0.998601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999148">
The Appraisal framework is a theory of the
language of evaluation, developed within the
tradition of systemic functional linguistics.
The framework describes a taxonomy of the
types of language used to convey evaluation
and position oneself with respect to the eval-
uations of other people. Accurate automatic
recognition of these types of language can
inform an analysis of document sentiment.
This paper describes the preparation of test
data for algorithms for automatic Appraisal
analysis. The difficulty of the task is as-
sessed by way of an inter-annotator agree-
ment study, based on measures analogous to
those used in the MUC-7 evaluation.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999576125">
The Appraisal framework (Martin and White, 2005)
describes a taxonomy of the language employed in
communicating evaluation, explaining how users of
English convey attitude (emotion, judgement of peo-
ple and appreciation of objects), engagement (as-
sessment of the evaluations of other people) and
how writers may modify the strength of their atti-
tude/engagement. Accurate automatic analysis of
these aspects of language will augment existing re-
search in the fields of sentiment (Pang et al., 2002)
and subjectivity analysis (Wiebe et al., 2004), but as-
sessing the usefulness of analysis algorithms lever-
aging the Appraisal framework will require test data.
At present there are no machine-readable
Appraisal-annotated texts publicly available. Real-
world instances of Appraisal in use are limited
</bodyText>
<page confidence="0.982463">
93
</page>
<bodyText confidence="0.999822733333333">
to example extracts that demonstrate the theory,
coming from a wide variety of genres as disparate
as news reporting (White, 2002; Martin, 2004) and
poetry (Martin and White, 2005). These examples,
while useful in demonstrating the various aspects
of Appraisal, can only be employed in a qualitative
analysis and would bring about inconsistencies
if analysed collectively — one can expect the
writing style to depend upon the genre, resulting in
significantly different syntactic constructions and
lexical choices.
We therefore need to examine Appraisal across
documents in the same genre and investigate pat-
terns within that particular register. This paper dis-
cusses the methodology of an Appraisal annotation
study and an analysis of the inter-annotator agree-
ment exhibited by two human judges. The output
of this study has the additional benefit of bringing
a set of machine-readable annotations of Appraisal
into the public domain for further research.
This paper is structured as follows. The next sec-
tion offers an overview of the Appraisal framework.
Section 3 discusses the methodology adopted for
the annotation study. Section 4 discusses the mea-
sures employed to assess inter-annotator agreement
and reports the results of these measures. Section
5 offers an analysis of cases of systematic disagree-
ment. Other computational work utilising the Ap-
praisal framework is reviewed in Section 6. Section
7 summarises the paper and outlines future work.
</bodyText>
<sectionHeader confidence="0.784963" genericHeader="method">
2 The linguistic framework of Appraisal
</sectionHeader>
<bodyText confidence="0.9274075">
The Appraisal framework (Martin and White, 2005)
is a development of work in Systemic Functional
</bodyText>
<note confidence="0.8424065">
Proceedings of the Linguistic Annotation Workshop, pages 93–100,
Prague, June 2007. c�2007 Association for Computational Linguistics
</note>
<figure confidence="0.847943">
inclination
distance
distribution (space)
distribution (time)
</figure>
<figureCaption confidence="0.999952">
Figure 1: The Appraisal framework.
</figureCaption>
<bodyText confidence="0.980284285714286">
Linguistics (Halliday, 1994) and is concerned with
interpersonal meaning in text—the negotiation of
social relationships by communicating emotion,
judgement and appreciation. The taxonomy de-
scribed by the Appraisal framework is depicted in
Figure 1.
Appraisal consists of three subsystems that oper-
ate in parallel: attitude looks at how one expresses
private state (Quirk et al., 1985) (one’s emotion and
opinions); engagement considers the positioning of
oneself with respect to the opinions of others and
graduation investigates how the use of language
functions to amplify or diminish the attitude and en-
gagement conveyed by a text.
</bodyText>
<subsectionHeader confidence="0.98621">
2.1 Attitude: emotion, ethics and aesthetics
</subsectionHeader>
<bodyText confidence="0.999819777777778">
The Attitude sub-system describes three areas of pri-
vate state: emotion, ethics and aesthetics. An atti-
tude is further qualified by its polarity (positive or
negative). Affect identifies feelings—author’s emo-
tions as represented by their text. Judgement deals
with authors’ attitude towards the behaviour of peo-
ple; how authors applaud or reproach the actions
of others. Appreciation considers the evaluation of
things—both man-made and natural phenomena.
</bodyText>
<subsectionHeader confidence="0.996749">
2.2 Engagement: appraisals of appraisals
</subsectionHeader>
<bodyText confidence="0.999977473684211">
Through engagement, Martin and White (2005) deal
with the linguistic constructions by which authors
construe their point of view and the resources used
to adopt stances towards the opinions of other peo-
ple. The theory of engagement follows Stubbs
(1996) in that it assumes that all utterances convey
point of view and Bakhtin (1981) in supposing that
all utterances occur in a miscellany of other utter-
ances on the same motif, and that they carry both
implicit and explicit responses to one another. In
other words, all text is inherently dialogistic as it en-
codes authors’ reactions to their experiences (includ-
ing previous interaction with other writers). Engage-
ment can be both retrospective (that is, an author will
acknowledge and agree or disagree with the stances
of others who have previously appraised a subject),
and prospective (one may anticipate the responses of
an intended audience and include counter-responses
in the original text).
</bodyText>
<subsectionHeader confidence="0.999378">
2.3 Graduation: strength of evaluations
</subsectionHeader>
<bodyText confidence="0.9943315">
Martin and White (2005) consider the resources by
which writers alter the strength of their evaluation
as a system of graduation. Graduation is a general
property of both attitude and engagement. In atti-
tude it enables authors to convey greater or lesser
degrees of positivity or negativity, while graduation
of engagements scales authors’ conviction in their
utterance.
Graduation is divided into two subsystems. Force
alters appraisal propositions in terms of its inten-
</bodyText>
<figure confidence="0.999285953488372">
attitude
appreciation
composition
valuation
reaction
complexity
balance
quality
impact
deny
counter
disclaim
pronounce
contract proclaim endorse
engagement
attribute
concur
acknowledge
appraisal
expand
entertain
judgement
affect
satisfaction
happiness
sanction
security
esteem
normality
propriety
capacity
tenacity
veracity
number
graduation force quantification mass
focus extent
degree
intensification
vigour
affirm
concede
proximity (space)
proximity (time)
</figure>
<page confidence="0.998411">
94
</page>
<bodyText confidence="0.999458">
sity, quantity or temporality, or by means of spatial
metaphor. Focus considers the resolution of seman-
tic categories, for example:
They play real jazz.
They play jazz, sort of.
In real terms a musician either plays jazz or they
do not, but these examples demonstrate how authors
blur the lines of semantic sets and how binary rela-
tionships can be turned into scalar ones.
</bodyText>
<sectionHeader confidence="0.980757" genericHeader="method">
3 Annotation methodology
</sectionHeader>
<bodyText confidence="0.998291481012659">
The corpus used in this study consists of unedited
book reviews. Book reviews are good candidates for
this study as, while they are likely to contain similar
language by virtue of being from the same genre of
writing, we can also expect examples of Appraisal’s
many classes (for example, the emotion attributed
to the characters in reviews of novels, judgements
of authors’ competence and character, appreciation
of the qualities of books and engagement with the
propositions put forth by the authors under review).
The articles were taken from the web sites of
four British newspapers (The Guardian, The Inde-
pendent, The Telegraph and The Times) on two dif-
ferent dates—31 July 2006 and 11 September 2006.
Each review is attributed to a unique author. The
corpus is comprised of 38 documents, containing a
total of 36,997 tokens in 1,245 sentences.
Two human annotators, d and j, participated in
this study, assigning tags independently. The anno-
tators were well-versed in the Appraisal framework,
having studied the latest literature. The judges were
asked to annotate appraisal-bearing terms with the
appraisal type presumed to be intended by the au-
thor of the text. They were asked to highlight each
example of appraisal and specify the type of atti-
tude, engagement or graduation present. They also
assigned a polarity (positive or negative) to attitudi-
nal items and a scaling (up or down) to graduating
items, employing a custom-developed software tool
to annotate the documents.
Four alternative annotation strategies were con-
sidered. One approach is to allow only a single token
per annotation. However, this is too simplistic for
an Appraisal annotation study—a unit of Appraisal
is frequently larger than a single token. Consider the
following examples:
The design was deceptively–VERACITY simple–
COMPLEXITY. (*)
The design was deceptively simple–COMPLEXITY.
Example 1 demonstrates that a single-token ap-
proach is inappropriate as it ascribes a judgement
of someone’s honesty, whereas Example 2 indicates
the correct analysis—the sentence is an apprecia-
tion of the simplicity of the “design”. This example
shows how it is necessary to annotate larger units of
appraisal-bearing language.
Including more tokens, however, increases the
complexity of the annotation task, and reduces the
likelihood of agreement between the judges, as the
annotated tokens of one judge may be a subset of,
or overlap with, those of another. We therefore ex-
perimented with tagging entire sentences in order to
constrain the annotators’ range of choices. This re-
sulted in its own problems as there is often more than
one appraisal in a sentence, for example:
The design was deceptively simple–COMPLEXITY
and belied his ingenuity–CAPACITY.
An alternative approach is to permit annotators
to tag an arbitrary number of contiguous tokens.
Arbitrary-length tagging is disadvantageous as the
judges will frequently tag units of differing length,
but this can be compensated for by relaxing the rules
for agreement—for example, by allowing intersect-
ing annotations to match successfully (Wiebe et al.,
2005). Bruce and Wiebe (1999) employ another
approach, creating units from every non-compound
sentence and each conjunct of every compound sen-
tence. This side-steps the problem of ambiguity in
appraisal unit length, but will still fail to capture both
appraisals demonstrated in the second conjunct of
Example 4.
The design was deceptively simple–COMPLEXITY
and belied his remarkable–NORMALITY
ingenuity–CAPACITY.
Ultimately in this study, we permitted judges to
annotate any number of tokens in order to allow
for multiple Appraisal units of differing sizes within
sentences. Annotation was carried out over two
rounds, punctuated by an intermediary analysis of
</bodyText>
<page confidence="0.994431">
95
</page>
<table confidence="0.997891083333333">
d j d j d j
Inclination 1.26 3.50 Balance 2.64 1.84 Distance 0.69 0.59
Happiness 2.80 2.32 Complexity 2.52 2.74 Number 0.82 2.63
Security 4.31 2.22 Valuation 6.08 9.29 Mass 0.22 1.63
Satisfaction 1.67 2.32 Deny 3.05 3.67 Proximity (Space) 0.09 0.14
Normality 8.00 4.44 Counter 4.79 3.78 Proximity (Time) 0.03 0.55
Capacity 11.46 9.63 Pronounce 3.84 1.21 Distribution (Space) 0.41 1.39
Tenacity 3.72 4.44 Endorse 2.05 1.49 Distribution (Time) 0.82 2.56
Veracity 3.15 2.01 Affirm 0.54 1.14 Degree 4.38 5.72
Propriety 13.32 12.61 Concede 0.38 0.03 Vigour 0.60 0.45
Impact 6.11 4.23 Entertain 2.27 2.43 Focus 3.02 2.29
Quality 2.55 3.40 Acknowledge 2.42 3.33
</table>
<tableCaption confidence="0.997881">
Table 1: The distribution of the Appraisal types selected by each annotator (%).
</tableCaption>
<table confidence="0.997032">
d j
Documents 115.74 77.21
Sentences 3.65 2.43
Words 0.12 0.08
</table>
<tableCaption confidence="0.997622">
Table 2: The density of annotations relative to the
</tableCaption>
<bodyText confidence="0.967709461538462">
number of documents, sentences and words.
agreement and disagreement between the two anno-
tators. The judges discussed examples of the most
common types of disagreement in an attempt to ac-
quire a common understanding for the second round,
but annotations from the first round were left unal-
tered.
Following the methodology described above, d
made 3,176 annotations whilst j made 2,886 anno-
tations. The distribution of the Appraisal types as-
cribed is shown in Table 1, while Table 2 details the
density of annotations in documents, sentences and
words.
</bodyText>
<sectionHeader confidence="0.995274" genericHeader="method">
4 Measuring inter-annotator agreement
</sectionHeader>
<bodyText confidence="0.97938488">
The study of inter-annotator agreement begins by
considering the level of agreement exhibited by the
annotators in deciding which tokens are representa-
tive of Appraisal, irrespective of the type. As dis-
cussed, this is problematic as judges are liable to
choose different length token spans when marking
up what is essentially the same appraisal, as demon-
strated by Example 5.
(5)
[d] It is tempting to point to the bombs in Lon-
don and elsewhere, to the hideous mess–QUALITY
in Iraq, to recent victories of the Islamists, to
the violent and polarised rhetoric–PROPRIETY and
answer yes.
[j] It is tempting to point to the bombs in
London and elsewhere, to the hideous–QUALITY
mess–BALANCE in Iraq, to recent victories of Is-
lamists, to the violent–PROPRIETY and polarised–
PROPRIETY rhetoric and answer yes.
Wiebe et al. (2005), who faced this problem when
annotating expressions of opinion under their own
framework, accept that it is necessary to consider the
validity of all judges’ interpretations and therefore
consider intersecting annotations (such as “hideous”
and “hideous mess”) to be matches. The same relax-
ation of constraints is employed in this study.
Tasks with a known number of annotative units
can be analysed with measures of agreement such as
Cohen’s r. Coefficient (1960), but the judges’ free-
dom in this task prohibits meaningful application of
this measure. For example, consider how word sense
annotators are obliged to choose from a limited fixed
set of senses for each token, whereas judges anno-
tating Appraisal are free to select one of thirty-two
classes for any contiguous substring of any length
within each document; there are 16 W − n) pos-
sible choices in a document of n tokens (approxi-
mately 6.5 x 108 possibilities in this corpus).
A wide range of evaluation metrics have been em-
ployed by the Message Understanding Conferences
(MUCs). The MUC-7 tasks included extraction of
named entities, equivalence classes, attributes, facts
and events (Chinchor, 1998). The participating sys-
tems were evaluated using a variety of related mea-
sures, defined in Table 3. These tasks are similar to
Appraisal annotation in that the units are formed of
an arbitrary number of contiguous tokens.
In this study the agreement exhibited by an an-
notator a is evaluated as a pair-wise comparison
against the other annotator b. Annotator b provides
</bodyText>
<page confidence="0.996637">
96
</page>
<tableCaption confidence="0.99943">
Table 3: MUC-7 score definitions (Chinchor 1998).
</tableCaption>
<table confidence="0.9995885">
FSC REC PRE ERR UND OVG
d 0.682 0.706 0.660 0.482 0.294 0.340
j 0.715 0.667 0.770 0.444 0.333 0.230
x 0.698 0.686 0.711 0.462 0.312 0.274
</table>
<tableCaption confidence="0.997717">
Table 4: MUC-7 test scores, evaluating the agree-
</tableCaption>
<bodyText confidence="0.925479384615385">
ment in text anchors selected by the annotators. x
denotes the average value, calculated using the har-
monic mean.
a presumed gold standard for the purposes of evalu-
ating agreement. Note, however, that in this case it
does not necessarily follow that REC (a w.r.t. b) _
PRE (b w.r.t. a). Consider that a may tend to make
one-word annotations whilst b prefers to annotate
phrases; the set of a’s annotations will contain mul-
tiple matches for some of the phrases annotated by b
(refer to Example 5, for instance). The ‘number cor-
rect’ will differ for each annotator in the pair under
evaluation.
Table 4 lists the values for the MUC-7 measures
applied to the text spans selected by the annota-
tors. Annotator d is inclined to identify text as Ap-
praisal more frequently than annotator j. This re-
sults in higher recall for d, but with lower preci-
sion. Naturally, the opposite observation can be
made about annotator j. Both annotators exhibit a
high error rate at 48.2% and 44.4% for d and j re-
spectively. The substitution rate is not listed as there
are no classes to substitute when considering only
text anchor agreement. The second round of anno-
tation achieved slightly higher agreement (the mean
F-score increased by 0.033).
</bodyText>
<table confidence="0.997530714285714">
FSC REC PRE SUB ERR
0 0.698 0.686 0.711 0.000 0.462
1 0.635 0.624 0.647 0.090 0.511
2 0.528 0.518 0.538 0.244 0.594
3 0.448 0.441 0.457 0.357 0.655
4 0.396 0.388 0.403 0.433 0.696
5 0.395 0.388 0.403 0.433 0.696
</table>
<tableCaption confidence="0.989807">
Table 5: Harmonic means of the MUC-7 test scores
</tableCaption>
<bodyText confidence="0.982442282051282">
evaluating the agreement in text anchors and Ap-
praisal classes selected by the annotators, at each
level of hierarchical abstraction.
Having considered the annotators’ agreement
with respect to text anchors, we go on to analyse
the agreement exhibited by the annotators with re-
spect to the types of Appraisal assigned to the text
anchors. The Appraisal framework is a hierarchi-
cal system—a tree with leaves corresponding to the
annotation types chosen by the judges. When in-
vestigating agreement in Appraisal type, the follow-
ing measures include not just the leaf nodes but also
their parent types, collapsing the nodes into increas-
ingly abstract representations. For example happi-
ness is a kind of affect, which is a kind of attitude,
which is a kind of appraisal. These relationships are
depicted in full in Figure 2. Note that in the follow-
ing measurements of inter-annotator agreement leaf
nodes are included in subsequent levels (for exam-
ple, focus is a leaf node at level 2, but is also consid-
ered to be a member of levels 3, 4 and 5).
Table 5 shows the harmonic means of the MUC-
7 measures of the annotators’ agreement at each of
the levels depicted in Figure 2. As one might ex-
pect, the agreement steadily drops as the classes be-
come more concrete—classes become more specific
and more numerous so the complexity of the task
increases.
Table 5 also lists the average rate of substitutions
as the annotation task’s complexity increases, show-
ing that the annotators were able to fairly easily
distinguish between instances of the three subsys-
tems of Appraisal (Attitude, Engagement and Grad-
uation) as the substitution rate at level 1 is low (only
9%). As the number of possible classes increases an-
notators are more likely to confuse appraisal types,
with disagreement occurring on approximately 44%
of annotations at level 5. The second round of an-
notations resulted in slightly improved agreement at
</bodyText>
<figure confidence="0.9579855">
Number correct = COR + INC + MIS
Number incorrect = COR + INC + SPU
Number missing
Number spurious
Number possible
Number actual
F-score = (2 x REC x PRE)
/ (REC + PRE)
Precision = COR/POS
Recall = COR/ACT
Substitution = INC/ (COR + INC)
Error per response = (INC + SPU + MIS)
/ (COR + INC + SPU + MIS)
Under-generation = MIS/POS
Over-generation = SPU/ACT
COR
INC
MIS
SPU
POS
ACT
FSC
REC
PRE
SUB
ERR
UND
OVG
97
Level 0: .698
appraisal
Level 1: .635
engagement: .507
graduation: .479
attitude: .701
Level 2: .528
appreciation: .567
judgement: .586
contract: .502
expand: .445
affect: .519
focus: .287
force: .420
Level 3: .448
intensification: .513
quantification: .233
composition: .432
satisfaction: .374
inclination: .249
happiness: .448
valuation: .299
proclaim: .336
entertain: .459
disclaim: .555
attribute: .427
sanction: .575
reaction: .510
security: .335
esteem: .489
Level 4: .396
acknowledge: .390
complexity: .314
pronounce: .195
normality: .289
propriety: .540
capacity: .431
distance: .415
tenacity: .395
veracity: .519
endorse: .331
balance: .300
counter: .603
number: .191
impact: .462
quality: .336
concur: .297
degree: .510
vigour: .117
extent: .242
deny: .451
mass: .104
Level 5: .395
distribution (space): .110
distribution (time): .352
proximity (space): .000
proximity (time): .000
concede: .000
affirm: .325
</figure>
<figureCaption confidence="0.973715">
Figure 2: The Appraisal framework with hierarchical levels highlighted. Appraisal classes and levels are
accompanied by the harmonic mean of the F-scores of the annotators for that class/level.
</figureCaption>
<bodyText confidence="0.999108111111111">
each level of abstraction (the mean F-score increased
by 0.051 at the most abstract level).
Of course, some Appraisal classes are easier to
identify than others. Figure 2 summarises the agree-
ment for each node in the Appraisal hierarchy with
the harmonic mean of the F-scores of the annotators
for each class. Typically, the attitude annotations are
easiest to identify, whereas the other subsystems of
engagement and graduation tend to be more difficult.
The Proximity children of Extent exhibited no
agreement whatsoever. This seems to have arisen
from the differences in the judges’ interpretations of
proximity. In the case of Proximity (Space), for ex-
ample, one judge annotated words that function to
modify the spatial distance of other concepts (e.g.
near), whereas the other selected words placing con-
cepts at a specific location (e.g. homegrown, local).
This confusion between modifying words and spe-
</bodyText>
<page confidence="0.991861">
98
</page>
<bodyText confidence="0.99975121875">
cific locations also accounts for the low agreement
in the Distribution (Space) type.
The measures show that it is also difficult to
achieve a consensus on what qualifies as engage-
ments of the Pronounce type. Both annotators select
expressions that assert the irrefutability of a propo-
sition (e.g. certainly or in fact or it has to be said).
Judge d, however, tends to perceive pronouncement
as occurring wherever the author makes an assertion
(e.g. this is or there will be). Judge j seems to re-
quire that the assertion carry a degree of emphasis to
include a term in the Pronounce class.
The low agreement of the Mass graduations can
also be explained in this way, as both d and j se-
lect strong expressions relating to size (e.g. massive
or scant). Annotator j found additional but weaker
terms like largely or slightly.
The Pronounce and Mass classes provide typical
examples of the disagreement exhibited by the an-
notators. It is not that the judges have wildly differ-
ent understandings of the system, but rather they dis-
agree in the bounds of a class—one annotator may
require a greater degree of strength of a term to war-
rant its inclusion in a class.
Contingency tables (not depicted due to space
constraints) reveal some interesting tendencies for
confusion between the two annotators. Approxi-
mately 33% of d’s annotations of Proximity (Space)
were ascribed as Capacity by j. The high percent-
age is due to the rarity of annotations of Proxim-
ity (Space), but the confusion comes from differing
units of Appraisal, as shown in Example 6.
</bodyText>
<equation confidence="0.645144">
(6)
</equation>
<bodyText confidence="0.990480071428571">
[d] But at key points in this story, one gets
the feeling that the essential factors are op-
erating just outside–PROXIMITY (SPACE)
James’s field of vision–CAPACITY.
[j] But at key points in this story, one gets the
feeling that the essential factors are operating just
outside James’s field of vision–CAPACITY.
Another interesting case of frequent confusion is
the pair of Satisfaction and Propriety. Though not
closely related in the Attitude subsystem, j chooses
Propriety for 21% of d’s annotations of Satisfaction.
The confusion is typified by Example 7, where it is
apparent that there is disagreement in terms of who
is being appraised.
</bodyText>
<equation confidence="0.814164">
(7)
</equation>
<bodyText confidence="0.987694285714286">
[d] Like him, Vermeer – or so he chose to be-
lieve – was an artist neglected–SATISFACTION and
wronged–SATISFACTION by critics and who had
died an almost unknown.
[j] Like him, Vermeer – or so he chose to believe
– was an artist neglected and wronged–PROPRIETY
by critics and who had died an almost unknown.
Annotator d believes that the author is communi-
cating the artist’s dissatisfaction with the way he is
treated by critics, whereas j believes that the critics
are being reproached for their treatment of the artist.
This highlights a problem with the coding scheme,
which simplifies the task by assuming only one type
of Appraisal is conveyed by each unit.
</bodyText>
<sectionHeader confidence="0.999987" genericHeader="method">
5 Related work
</sectionHeader>
<bodyText confidence="0.998820363636364">
Taboada and Grieve (2004) initiated computational
experimentation with the Appraisal framework, as-
signing adjectives into one of the three broad atti-
tude classes. The authors apply SO-PMI-IR (Turney,
2002) to extract and determine the polarity of adjec-
tives. They then use a variant of SO-PMI-IR to de-
termine a ‘potential’ value for affect, judgement and
appreciation, calculating the mutual information be-
tween the adjective and three pronoun-copular pairs:
I was (affect); he was (judgement) and it was (ap-
preciation). While the pairs seem compelling mark-
ers of the respective attitude types, they incorrectly
assume that appraisals of affect are limited to the
first person whilst judgements are made only of the
third person. We can expect a high degree of overlap
between the sets of documents retrieved by queries
formed using these pairs (e.g. I was a happy (X);
he was a happy (X); It was a happy (X)).
Whitelaw et al. (2005) use the Appraisal frame-
work to specify frames of sentiment. These “Ap-
praisal Groups” are derived from aspects of Attitude
and Graduation:
</bodyText>
<construct confidence="0.554604">
Attitude: affect  |judgement  |appreciation
Orientation positive  |negative
Force: low  |neutral  |high
Focus: low  |neutral  |high
Polarity: marked  |unmarked
</construct>
<bodyText confidence="0.9997548">
Their process begins with a semi-automatically con-
structed lexicon of these Appraisal groups, built us-
ing example terms from Martin and White (2005) as
seeds into WordNet synsets. The frames supplement
bag of words-based machine learning techniques for
</bodyText>
<page confidence="0.995488">
99
</page>
<bodyText confidence="0.9990375">
sentiment analysis and they achieve minor improve-
ments over unigram features.
</bodyText>
<sectionHeader confidence="0.999258" genericHeader="conclusions">
6 Summary
</sectionHeader>
<bodyText confidence="0.999976357142857">
This paper has discussed the methodology of an ex-
ercise annotating book reviews according to the Ap-
praisal framework, a functional linguistic theory of
evaluation in English. The agreement exhibited by
two human judges was measured by analogy with
the evaluation employed for the MUC-7 shared tasks
(Chinchor, 1998).
The agreement varied greatly depending on the
level of abstraction in the Appraisal hierarchy
(a mean F-score of 0.698 at the most abstract
level through to 0.395 at the most concrete level).
The agreement also depended on the type being
annotated—there was more agreement evident for
types of attitude compared to types of engagement
or graduation.
The exercise is the first step in an ongoing study
of approaches for the automatic analysis of expres-
sions of Appraisal. The primary output of this work
is a corpus of book reviews independently annotated
with Appraisal types by two coders. Agreement was
in general low, but if one assumes that the intersec-
tion of both sets of annotations contains reliable ex-
amples, this leaves 2,223 usable annotations.
Future work will employ these annotations to
evaluate algorithms for the analysis of Appraisal,
and investigate the usefulness of the Appraisal
framework when in the computational analysis of
document sentiment and subjectivity.
</bodyText>
<sectionHeader confidence="0.998965" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998605">
We would like to thank Bill Keller for advice when
designing the annotation methodology. The work of
the first author is supported by a UK EPSRC stu-
dentship.
</bodyText>
<sectionHeader confidence="0.999643" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999926927272727">
M. M. Bakhtin. 1981. The Dialogic Imagination. Uni-
versity of Texas Press, Austin. Translated by C. Emer-
son &amp; M. Holquist.
Rebecca Bruce and Janyce Wiebe. 1999. Recognizing
subjectivity: a case study in manual tagging. Natural
Language Engineering, 5(1):1–16.
N. Chinchor. 1998. MUC-7 test scores introduction.
In Proceedings of the Seventh Message Understanding
Conference.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measures,
20:37–46.
M. A. K. Halliday. 1994. An Introduction to Functional
Grammar. Edward Arnold, London.
J. R. Martin and P. R. R. White. 2005. Language of Eval-
uation: Appraisal in English. Palgrave Macmillan.
J. R. Martin. 2004. Mourning: how we get aligned. Dis-
course &amp; Society, 15(2-3):321–344.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing, Philadelphia, PA, USA.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,
and Jan Svartvik. 1985. A Comprehensive Grammar
of the English Language. Longman.
M. Stubbs. 1996. Towards a modal grammar of English:
a matter of prolonged fieldwork. In Text and Corpus
Analysis. Blackwell, Oxford.
Maite Taboada and Jack Grieve. 2004. Analyzing Ap-
praisal automatically. In Spring Symposium on Ex-
ploring Attitude and Affect in Text. American Associa-
tion for Artificial Intelligence, Stanford. AAAI Tech-
nical Report SS-04-07.
Peter D. Turney. 2002. Thumbs up or thumbs down?
Semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics, Philadelphia, PA, USA.
P. R. R. White. 2002. Appraisal — the language of evalu-
ation and stance. In Jef Verschueren, Jan-Ola ¨Ostman,
Jan Blommaert, and Chris Bulcaen, editors, Handbook
of Pragmatics, pages 1–27. John Benjamins, Amster-
dam.
Casey Whitelaw, Navendu Garg, and Shlomo Argamon.
2005. Using appraisal groups for sentiment analysis.
In Proceedings of the 14th ACM international confer-
ence on Information and knowledge management.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew
Bell, and Melanie Martin. 2004. Learning subjective
language. Computational linguistics, 30(3):277–308.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation, 39(2-
3):165–210.
</reference>
<page confidence="0.990756">
100
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.556262">
<title confidence="0.999626">Annotating Expressions of Appraisal in English</title>
<author confidence="0.994958">Jonathon Read</author>
<author confidence="0.994958">David Hope</author>
<author confidence="0.994958">John</author>
<affiliation confidence="0.9949285">Department of University of</affiliation>
<note confidence="0.622564">United</note>
<abstract confidence="0.99425775">The Appraisal framework is a theory of the language of evaluation, developed within the tradition of systemic functional linguistics. The framework describes a taxonomy of the types of language used to convey evaluation and position oneself with respect to the evaluations of other people. Accurate automatic recognition of these types of language can inform an analysis of document sentiment. This paper describes the preparation of test data for algorithms for automatic Appraisal analysis. The difficulty of the task is assessed by way of an inter-annotator agreement study, based on measures analogous to those used in the MUC-7 evaluation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M M Bakhtin</author>
</authors>
<title>The Dialogic Imagination.</title>
<date>1981</date>
<publisher>University of Texas Press,</publisher>
<location>Austin.</location>
<note>Translated by</note>
<contexts>
<context position="4994" citStr="Bakhtin (1981)" startWordPosition="741" endWordPosition="742">s represented by their text. Judgement deals with authors’ attitude towards the behaviour of people; how authors applaud or reproach the actions of others. Appreciation considers the evaluation of things—both man-made and natural phenomena. 2.2 Engagement: appraisals of appraisals Through engagement, Martin and White (2005) deal with the linguistic constructions by which authors construe their point of view and the resources used to adopt stances towards the opinions of other people. The theory of engagement follows Stubbs (1996) in that it assumes that all utterances convey point of view and Bakhtin (1981) in supposing that all utterances occur in a miscellany of other utterances on the same motif, and that they carry both implicit and explicit responses to one another. In other words, all text is inherently dialogistic as it encodes authors’ reactions to their experiences (including previous interaction with other writers). Engagement can be both retrospective (that is, an author will acknowledge and agree or disagree with the stances of others who have previously appraised a subject), and prospective (one may anticipate the responses of an intended audience and include counter-responses in th</context>
</contexts>
<marker>Bakhtin, 1981</marker>
<rawString>M. M. Bakhtin. 1981. The Dialogic Imagination. University of Texas Press, Austin. Translated by C. Emerson &amp; M. Holquist.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Bruce</author>
<author>Janyce Wiebe</author>
</authors>
<title>Recognizing subjectivity: a case study in manual tagging.</title>
<date>1999</date>
<journal>Natural Language Engineering,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="10097" citStr="Bruce and Wiebe (1999)" startWordPosition="1519" endWordPosition="1522">rder to constrain the annotators’ range of choices. This resulted in its own problems as there is often more than one appraisal in a sentence, for example: The design was deceptively simple–COMPLEXITY and belied his ingenuity–CAPACITY. An alternative approach is to permit annotators to tag an arbitrary number of contiguous tokens. Arbitrary-length tagging is disadvantageous as the judges will frequently tag units of differing length, but this can be compensated for by relaxing the rules for agreement—for example, by allowing intersecting annotations to match successfully (Wiebe et al., 2005). Bruce and Wiebe (1999) employ another approach, creating units from every non-compound sentence and each conjunct of every compound sentence. This side-steps the problem of ambiguity in appraisal unit length, but will still fail to capture both appraisals demonstrated in the second conjunct of Example 4. The design was deceptively simple–COMPLEXITY and belied his remarkable–NORMALITY ingenuity–CAPACITY. Ultimately in this study, we permitted judges to annotate any number of tokens in order to allow for multiple Appraisal units of differing sizes within sentences. Annotation was carried out over two rounds, punctuat</context>
</contexts>
<marker>Bruce, Wiebe, 1999</marker>
<rawString>Rebecca Bruce and Janyce Wiebe. 1999. Recognizing subjectivity: a case study in manual tagging. Natural Language Engineering, 5(1):1–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chinchor</author>
</authors>
<title>MUC-7 test scores introduction.</title>
<date>1998</date>
<booktitle>In Proceedings of the Seventh Message Understanding Conference.</booktitle>
<contexts>
<context position="14162" citStr="Chinchor, 1998" startWordPosition="2167" endWordPosition="2168">ure. For example, consider how word sense annotators are obliged to choose from a limited fixed set of senses for each token, whereas judges annotating Appraisal are free to select one of thirty-two classes for any contiguous substring of any length within each document; there are 16 W − n) possible choices in a document of n tokens (approximately 6.5 x 108 possibilities in this corpus). A wide range of evaluation metrics have been employed by the Message Understanding Conferences (MUCs). The MUC-7 tasks included extraction of named entities, equivalence classes, attributes, facts and events (Chinchor, 1998). The participating systems were evaluated using a variety of related measures, defined in Table 3. These tasks are similar to Appraisal annotation in that the units are formed of an arbitrary number of contiguous tokens. In this study the agreement exhibited by an annotator a is evaluated as a pair-wise comparison against the other annotator b. Annotator b provides 96 Table 3: MUC-7 score definitions (Chinchor 1998). FSC REC PRE ERR UND OVG d 0.682 0.706 0.660 0.482 0.294 0.340 j 0.715 0.667 0.770 0.444 0.333 0.230 x 0.698 0.686 0.711 0.462 0.312 0.274 Table 4: MUC-7 test scores, evaluating t</context>
<context position="25351" citStr="Chinchor, 1998" startWordPosition="4019" endWordPosition="4020">ly constructed lexicon of these Appraisal groups, built using example terms from Martin and White (2005) as seeds into WordNet synsets. The frames supplement bag of words-based machine learning techniques for 99 sentiment analysis and they achieve minor improvements over unigram features. 6 Summary This paper has discussed the methodology of an exercise annotating book reviews according to the Appraisal framework, a functional linguistic theory of evaluation in English. The agreement exhibited by two human judges was measured by analogy with the evaluation employed for the MUC-7 shared tasks (Chinchor, 1998). The agreement varied greatly depending on the level of abstraction in the Appraisal hierarchy (a mean F-score of 0.698 at the most abstract level through to 0.395 at the most concrete level). The agreement also depended on the type being annotated—there was more agreement evident for types of attitude compared to types of engagement or graduation. The exercise is the first step in an ongoing study of approaches for the automatic analysis of expressions of Appraisal. The primary output of this work is a corpus of book reviews independently annotated with Appraisal types by two coders. Agreeme</context>
</contexts>
<marker>Chinchor, 1998</marker>
<rawString>N. Chinchor. 1998. MUC-7 test scores introduction. In Proceedings of the Seventh Message Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological Measures,</booktitle>
<pages>20--37</pages>
<marker>Cohen, 1960</marker>
<rawString>Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measures, 20:37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A K Halliday</author>
</authors>
<title>An Introduction to Functional Grammar. Edward</title>
<date>1994</date>
<location>Arnold, London.</location>
<contexts>
<context position="3512" citStr="Halliday, 1994" startWordPosition="518" endWordPosition="519">e measures. Section 5 offers an analysis of cases of systematic disagreement. Other computational work utilising the Appraisal framework is reviewed in Section 6. Section 7 summarises the paper and outlines future work. 2 The linguistic framework of Appraisal The Appraisal framework (Martin and White, 2005) is a development of work in Systemic Functional Proceedings of the Linguistic Annotation Workshop, pages 93–100, Prague, June 2007. c�2007 Association for Computational Linguistics inclination distance distribution (space) distribution (time) Figure 1: The Appraisal framework. Linguistics (Halliday, 1994) and is concerned with interpersonal meaning in text—the negotiation of social relationships by communicating emotion, judgement and appreciation. The taxonomy described by the Appraisal framework is depicted in Figure 1. Appraisal consists of three subsystems that operate in parallel: attitude looks at how one expresses private state (Quirk et al., 1985) (one’s emotion and opinions); engagement considers the positioning of oneself with respect to the opinions of others and graduation investigates how the use of language functions to amplify or diminish the attitude and engagement conveyed by </context>
</contexts>
<marker>Halliday, 1994</marker>
<rawString>M. A. K. Halliday. 1994. An Introduction to Functional Grammar. Edward Arnold, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Martin</author>
<author>P R R White</author>
</authors>
<title>Language of Evaluation: Appraisal in English.</title>
<date>2005</date>
<publisher>Palgrave Macmillan.</publisher>
<contexts>
<context position="911" citStr="Martin and White, 2005" startWordPosition="129" endWordPosition="132">he tradition of systemic functional linguistics. The framework describes a taxonomy of the types of language used to convey evaluation and position oneself with respect to the evaluations of other people. Accurate automatic recognition of these types of language can inform an analysis of document sentiment. This paper describes the preparation of test data for algorithms for automatic Appraisal analysis. The difficulty of the task is assessed by way of an inter-annotator agreement study, based on measures analogous to those used in the MUC-7 evaluation. 1 Introduction The Appraisal framework (Martin and White, 2005) describes a taxonomy of the language employed in communicating evaluation, explaining how users of English convey attitude (emotion, judgement of people and appreciation of objects), engagement (assessment of the evaluations of other people) and how writers may modify the strength of their attitude/engagement. Accurate automatic analysis of these aspects of language will augment existing research in the fields of sentiment (Pang et al., 2002) and subjectivity analysis (Wiebe et al., 2004), but assessing the usefulness of analysis algorithms leveraging the Appraisal framework will require test</context>
<context position="3205" citStr="Martin and White, 2005" startWordPosition="478" endWordPosition="481">ublic domain for further research. This paper is structured as follows. The next section offers an overview of the Appraisal framework. Section 3 discusses the methodology adopted for the annotation study. Section 4 discusses the measures employed to assess inter-annotator agreement and reports the results of these measures. Section 5 offers an analysis of cases of systematic disagreement. Other computational work utilising the Appraisal framework is reviewed in Section 6. Section 7 summarises the paper and outlines future work. 2 The linguistic framework of Appraisal The Appraisal framework (Martin and White, 2005) is a development of work in Systemic Functional Proceedings of the Linguistic Annotation Workshop, pages 93–100, Prague, June 2007. c�2007 Association for Computational Linguistics inclination distance distribution (space) distribution (time) Figure 1: The Appraisal framework. Linguistics (Halliday, 1994) and is concerned with interpersonal meaning in text—the negotiation of social relationships by communicating emotion, judgement and appreciation. The taxonomy described by the Appraisal framework is depicted in Figure 1. Appraisal consists of three subsystems that operate in parallel: attitu</context>
<context position="4705" citStr="Martin and White (2005)" startWordPosition="691" endWordPosition="694">de and engagement conveyed by a text. 2.1 Attitude: emotion, ethics and aesthetics The Attitude sub-system describes three areas of private state: emotion, ethics and aesthetics. An attitude is further qualified by its polarity (positive or negative). Affect identifies feelings—author’s emotions as represented by their text. Judgement deals with authors’ attitude towards the behaviour of people; how authors applaud or reproach the actions of others. Appreciation considers the evaluation of things—both man-made and natural phenomena. 2.2 Engagement: appraisals of appraisals Through engagement, Martin and White (2005) deal with the linguistic constructions by which authors construe their point of view and the resources used to adopt stances towards the opinions of other people. The theory of engagement follows Stubbs (1996) in that it assumes that all utterances convey point of view and Bakhtin (1981) in supposing that all utterances occur in a miscellany of other utterances on the same motif, and that they carry both implicit and explicit responses to one another. In other words, all text is inherently dialogistic as it encodes authors’ reactions to their experiences (including previous interaction with o</context>
<context position="24840" citStr="Martin and White (2005)" startWordPosition="3939" endWordPosition="3942">h degree of overlap between the sets of documents retrieved by queries formed using these pairs (e.g. I was a happy (X); he was a happy (X); It was a happy (X)). Whitelaw et al. (2005) use the Appraisal framework to specify frames of sentiment. These “Appraisal Groups” are derived from aspects of Attitude and Graduation: Attitude: affect |judgement |appreciation Orientation positive |negative Force: low |neutral |high Focus: low |neutral |high Polarity: marked |unmarked Their process begins with a semi-automatically constructed lexicon of these Appraisal groups, built using example terms from Martin and White (2005) as seeds into WordNet synsets. The frames supplement bag of words-based machine learning techniques for 99 sentiment analysis and they achieve minor improvements over unigram features. 6 Summary This paper has discussed the methodology of an exercise annotating book reviews according to the Appraisal framework, a functional linguistic theory of evaluation in English. The agreement exhibited by two human judges was measured by analogy with the evaluation employed for the MUC-7 shared tasks (Chinchor, 1998). The agreement varied greatly depending on the level of abstraction in the Appraisal hie</context>
</contexts>
<marker>Martin, White, 2005</marker>
<rawString>J. R. Martin and P. R. R. White. 2005. Language of Evaluation: Appraisal in English. Palgrave Macmillan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Martin</author>
</authors>
<title>Mourning: how we get aligned.</title>
<date>2004</date>
<pages>15--2</pages>
<publisher>Discourse &amp; Society,</publisher>
<contexts>
<context position="1804" citStr="Martin, 2004" startWordPosition="266" endWordPosition="267">th of their attitude/engagement. Accurate automatic analysis of these aspects of language will augment existing research in the fields of sentiment (Pang et al., 2002) and subjectivity analysis (Wiebe et al., 2004), but assessing the usefulness of analysis algorithms leveraging the Appraisal framework will require test data. At present there are no machine-readable Appraisal-annotated texts publicly available. Realworld instances of Appraisal in use are limited 93 to example extracts that demonstrate the theory, coming from a wide variety of genres as disparate as news reporting (White, 2002; Martin, 2004) and poetry (Martin and White, 2005). These examples, while useful in demonstrating the various aspects of Appraisal, can only be employed in a qualitative analysis and would bring about inconsistencies if analysed collectively — one can expect the writing style to depend upon the genre, resulting in significantly different syntactic constructions and lexical choices. We therefore need to examine Appraisal across documents in the same genre and investigate patterns within that particular register. This paper discusses the methodology of an Appraisal annotation study and an analysis of the inte</context>
</contexts>
<marker>Martin, 2004</marker>
<rawString>J. R. Martin. 2004. Mourning: how we get aligned. Discourse &amp; Society, 15(2-3):321–344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="1358" citStr="Pang et al., 2002" startWordPosition="197" endWordPosition="200">y of an inter-annotator agreement study, based on measures analogous to those used in the MUC-7 evaluation. 1 Introduction The Appraisal framework (Martin and White, 2005) describes a taxonomy of the language employed in communicating evaluation, explaining how users of English convey attitude (emotion, judgement of people and appreciation of objects), engagement (assessment of the evaluations of other people) and how writers may modify the strength of their attitude/engagement. Accurate automatic analysis of these aspects of language will augment existing research in the fields of sentiment (Pang et al., 2002) and subjectivity analysis (Wiebe et al., 2004), but assessing the usefulness of analysis algorithms leveraging the Appraisal framework will require test data. At present there are no machine-readable Appraisal-annotated texts publicly available. Realworld instances of Appraisal in use are limited 93 to example extracts that demonstrate the theory, coming from a wide variety of genres as disparate as news reporting (White, 2002; Martin, 2004) and poetry (Martin and White, 2005). These examples, while useful in demonstrating the various aspects of Appraisal, can only be employed in a qualitativ</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Randolph Quirk</author>
<author>Sidney Greenbaum</author>
<author>Geoffrey Leech</author>
<author>Jan Svartvik</author>
</authors>
<date>1985</date>
<journal>A Comprehensive Grammar of the English Language. Longman.</journal>
<contexts>
<context position="3869" citStr="Quirk et al., 1985" startWordPosition="569" endWordPosition="572">nal Proceedings of the Linguistic Annotation Workshop, pages 93–100, Prague, June 2007. c�2007 Association for Computational Linguistics inclination distance distribution (space) distribution (time) Figure 1: The Appraisal framework. Linguistics (Halliday, 1994) and is concerned with interpersonal meaning in text—the negotiation of social relationships by communicating emotion, judgement and appreciation. The taxonomy described by the Appraisal framework is depicted in Figure 1. Appraisal consists of three subsystems that operate in parallel: attitude looks at how one expresses private state (Quirk et al., 1985) (one’s emotion and opinions); engagement considers the positioning of oneself with respect to the opinions of others and graduation investigates how the use of language functions to amplify or diminish the attitude and engagement conveyed by a text. 2.1 Attitude: emotion, ethics and aesthetics The Attitude sub-system describes three areas of private state: emotion, ethics and aesthetics. An attitude is further qualified by its polarity (positive or negative). Affect identifies feelings—author’s emotions as represented by their text. Judgement deals with authors’ attitude towards the behaviour</context>
</contexts>
<marker>Quirk, Greenbaum, Leech, Svartvik, 1985</marker>
<rawString>Randolph Quirk, Sidney Greenbaum, Geoffrey Leech, and Jan Svartvik. 1985. A Comprehensive Grammar of the English Language. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stubbs</author>
</authors>
<title>Towards a modal grammar of English: a matter of prolonged fieldwork.</title>
<date>1996</date>
<booktitle>In Text and Corpus Analysis.</booktitle>
<location>Blackwell, Oxford.</location>
<contexts>
<context position="4915" citStr="Stubbs (1996)" startWordPosition="727" endWordPosition="728">olarity (positive or negative). Affect identifies feelings—author’s emotions as represented by their text. Judgement deals with authors’ attitude towards the behaviour of people; how authors applaud or reproach the actions of others. Appreciation considers the evaluation of things—both man-made and natural phenomena. 2.2 Engagement: appraisals of appraisals Through engagement, Martin and White (2005) deal with the linguistic constructions by which authors construe their point of view and the resources used to adopt stances towards the opinions of other people. The theory of engagement follows Stubbs (1996) in that it assumes that all utterances convey point of view and Bakhtin (1981) in supposing that all utterances occur in a miscellany of other utterances on the same motif, and that they carry both implicit and explicit responses to one another. In other words, all text is inherently dialogistic as it encodes authors’ reactions to their experiences (including previous interaction with other writers). Engagement can be both retrospective (that is, an author will acknowledge and agree or disagree with the stances of others who have previously appraised a subject), and prospective (one may antic</context>
</contexts>
<marker>Stubbs, 1996</marker>
<rawString>M. Stubbs. 1996. Towards a modal grammar of English: a matter of prolonged fieldwork. In Text and Corpus Analysis. Blackwell, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maite Taboada</author>
<author>Jack Grieve</author>
</authors>
<title>Analyzing Appraisal automatically.</title>
<date>2004</date>
<booktitle>In Spring Symposium on Exploring Attitude and Affect in Text. American Association for Artificial Intelligence,</booktitle>
<tech>AAAI Technical Report SS-04-07.</tech>
<location>Stanford.</location>
<contexts>
<context position="23493" citStr="Taboada and Grieve (2004)" startWordPosition="3727" endWordPosition="3730">SATISFACTION and wronged–SATISFACTION by critics and who had died an almost unknown. [j] Like him, Vermeer – or so he chose to believe – was an artist neglected and wronged–PROPRIETY by critics and who had died an almost unknown. Annotator d believes that the author is communicating the artist’s dissatisfaction with the way he is treated by critics, whereas j believes that the critics are being reproached for their treatment of the artist. This highlights a problem with the coding scheme, which simplifies the task by assuming only one type of Appraisal is conveyed by each unit. 5 Related work Taboada and Grieve (2004) initiated computational experimentation with the Appraisal framework, assigning adjectives into one of the three broad attitude classes. The authors apply SO-PMI-IR (Turney, 2002) to extract and determine the polarity of adjectives. They then use a variant of SO-PMI-IR to determine a ‘potential’ value for affect, judgement and appreciation, calculating the mutual information between the adjective and three pronoun-copular pairs: I was (affect); he was (judgement) and it was (appreciation). While the pairs seem compelling markers of the respective attitude types, they incorrectly assume that a</context>
</contexts>
<marker>Taboada, Grieve, 2004</marker>
<rawString>Maite Taboada and Jack Grieve. 2004. Analyzing Appraisal automatically. In Spring Symposium on Exploring Attitude and Affect in Text. American Association for Artificial Intelligence, Stanford. AAAI Technical Report SS-04-07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="23673" citStr="Turney, 2002" startWordPosition="3754" endWordPosition="3755">s and who had died an almost unknown. Annotator d believes that the author is communicating the artist’s dissatisfaction with the way he is treated by critics, whereas j believes that the critics are being reproached for their treatment of the artist. This highlights a problem with the coding scheme, which simplifies the task by assuming only one type of Appraisal is conveyed by each unit. 5 Related work Taboada and Grieve (2004) initiated computational experimentation with the Appraisal framework, assigning adjectives into one of the three broad attitude classes. The authors apply SO-PMI-IR (Turney, 2002) to extract and determine the polarity of adjectives. They then use a variant of SO-PMI-IR to determine a ‘potential’ value for affect, judgement and appreciation, calculating the mutual information between the adjective and three pronoun-copular pairs: I was (affect); he was (judgement) and it was (appreciation). While the pairs seem compelling markers of the respective attitude types, they incorrectly assume that appraisals of affect are limited to the first person whilst judgements are made only of the third person. We can expect a high degree of overlap between the sets of documents retrie</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter D. Turney. 2002. Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P R R White</author>
</authors>
<title>Appraisal — the language of evaluation and stance.</title>
<date>2002</date>
<booktitle>Handbook of Pragmatics,</booktitle>
<pages>1--27</pages>
<editor>In Jef Verschueren, Jan-Ola ¨Ostman, Jan Blommaert, and Chris Bulcaen, editors,</editor>
<publisher>John Benjamins,</publisher>
<location>Amsterdam.</location>
<contexts>
<context position="1789" citStr="White, 2002" startWordPosition="264" endWordPosition="265">fy the strength of their attitude/engagement. Accurate automatic analysis of these aspects of language will augment existing research in the fields of sentiment (Pang et al., 2002) and subjectivity analysis (Wiebe et al., 2004), but assessing the usefulness of analysis algorithms leveraging the Appraisal framework will require test data. At present there are no machine-readable Appraisal-annotated texts publicly available. Realworld instances of Appraisal in use are limited 93 to example extracts that demonstrate the theory, coming from a wide variety of genres as disparate as news reporting (White, 2002; Martin, 2004) and poetry (Martin and White, 2005). These examples, while useful in demonstrating the various aspects of Appraisal, can only be employed in a qualitative analysis and would bring about inconsistencies if analysed collectively — one can expect the writing style to depend upon the genre, resulting in significantly different syntactic constructions and lexical choices. We therefore need to examine Appraisal across documents in the same genre and investigate patterns within that particular register. This paper discusses the methodology of an Appraisal annotation study and an analy</context>
</contexts>
<marker>White, 2002</marker>
<rawString>P. R. R. White. 2002. Appraisal — the language of evaluation and stance. In Jef Verschueren, Jan-Ola ¨Ostman, Jan Blommaert, and Chris Bulcaen, editors, Handbook of Pragmatics, pages 1–27. John Benjamins, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Casey Whitelaw</author>
<author>Navendu Garg</author>
<author>Shlomo Argamon</author>
</authors>
<title>Using appraisal groups for sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th ACM international conference on Information and knowledge management.</booktitle>
<contexts>
<context position="24401" citStr="Whitelaw et al. (2005)" startWordPosition="3876" endWordPosition="3879">‘potential’ value for affect, judgement and appreciation, calculating the mutual information between the adjective and three pronoun-copular pairs: I was (affect); he was (judgement) and it was (appreciation). While the pairs seem compelling markers of the respective attitude types, they incorrectly assume that appraisals of affect are limited to the first person whilst judgements are made only of the third person. We can expect a high degree of overlap between the sets of documents retrieved by queries formed using these pairs (e.g. I was a happy (X); he was a happy (X); It was a happy (X)). Whitelaw et al. (2005) use the Appraisal framework to specify frames of sentiment. These “Appraisal Groups” are derived from aspects of Attitude and Graduation: Attitude: affect |judgement |appreciation Orientation positive |negative Force: low |neutral |high Focus: low |neutral |high Polarity: marked |unmarked Their process begins with a semi-automatically constructed lexicon of these Appraisal groups, built using example terms from Martin and White (2005) as seeds into WordNet synsets. The frames supplement bag of words-based machine learning techniques for 99 sentiment analysis and they achieve minor improvement</context>
</contexts>
<marker>Whitelaw, Garg, Argamon, 2005</marker>
<rawString>Casey Whitelaw, Navendu Garg, and Shlomo Argamon. 2005. Using appraisal groups for sentiment analysis. In Proceedings of the 14th ACM international conference on Information and knowledge management.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Rebecca Bruce</author>
<author>Matthew Bell</author>
<author>Melanie Martin</author>
</authors>
<title>Learning subjective language.</title>
<date>2004</date>
<journal>Computational linguistics,</journal>
<volume>30</volume>
<issue>3</issue>
<contexts>
<context position="1405" citStr="Wiebe et al., 2004" startWordPosition="204" endWordPosition="207"> on measures analogous to those used in the MUC-7 evaluation. 1 Introduction The Appraisal framework (Martin and White, 2005) describes a taxonomy of the language employed in communicating evaluation, explaining how users of English convey attitude (emotion, judgement of people and appreciation of objects), engagement (assessment of the evaluations of other people) and how writers may modify the strength of their attitude/engagement. Accurate automatic analysis of these aspects of language will augment existing research in the fields of sentiment (Pang et al., 2002) and subjectivity analysis (Wiebe et al., 2004), but assessing the usefulness of analysis algorithms leveraging the Appraisal framework will require test data. At present there are no machine-readable Appraisal-annotated texts publicly available. Realworld instances of Appraisal in use are limited 93 to example extracts that demonstrate the theory, coming from a wide variety of genres as disparate as news reporting (White, 2002; Martin, 2004) and poetry (Martin and White, 2005). These examples, while useful in demonstrating the various aspects of Appraisal, can only be employed in a qualitative analysis and would bring about inconsistencie</context>
</contexts>
<marker>Wiebe, Wilson, Bruce, Bell, Martin, 2004</marker>
<rawString>Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew Bell, and Melanie Martin. 2004. Learning subjective language. Computational linguistics, 30(3):277–308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language. Language Resources and Evaluation,</title>
<date>2005</date>
<pages>39--2</pages>
<contexts>
<context position="10073" citStr="Wiebe et al., 2005" startWordPosition="1515" endWordPosition="1518">entire sentences in order to constrain the annotators’ range of choices. This resulted in its own problems as there is often more than one appraisal in a sentence, for example: The design was deceptively simple–COMPLEXITY and belied his ingenuity–CAPACITY. An alternative approach is to permit annotators to tag an arbitrary number of contiguous tokens. Arbitrary-length tagging is disadvantageous as the judges will frequently tag units of differing length, but this can be compensated for by relaxing the rules for agreement—for example, by allowing intersecting annotations to match successfully (Wiebe et al., 2005). Bruce and Wiebe (1999) employ another approach, creating units from every non-compound sentence and each conjunct of every compound sentence. This side-steps the problem of ambiguity in appraisal unit length, but will still fail to capture both appraisals demonstrated in the second conjunct of Example 4. The design was deceptively simple–COMPLEXITY and belied his remarkable–NORMALITY ingenuity–CAPACITY. Ultimately in this study, we permitted judges to annotate any number of tokens in order to allow for multiple Appraisal units of differing sizes within sentences. Annotation was carried out o</context>
<context position="12998" citStr="Wiebe et al. (2005)" startWordPosition="1979" endWordPosition="1982">sed, this is problematic as judges are liable to choose different length token spans when marking up what is essentially the same appraisal, as demonstrated by Example 5. (5) [d] It is tempting to point to the bombs in London and elsewhere, to the hideous mess–QUALITY in Iraq, to recent victories of the Islamists, to the violent and polarised rhetoric–PROPRIETY and answer yes. [j] It is tempting to point to the bombs in London and elsewhere, to the hideous–QUALITY mess–BALANCE in Iraq, to recent victories of Islamists, to the violent–PROPRIETY and polarised– PROPRIETY rhetoric and answer yes. Wiebe et al. (2005), who faced this problem when annotating expressions of opinion under their own framework, accept that it is necessary to consider the validity of all judges’ interpretations and therefore consider intersecting annotations (such as “hideous” and “hideous mess”) to be matches. The same relaxation of constraints is employed in this study. Tasks with a known number of annotative units can be analysed with measures of agreement such as Cohen’s r. Coefficient (1960), but the judges’ freedom in this task prohibits meaningful application of this measure. For example, consider how word sense annotator</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 39(2-3):165–210.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>