<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.063181">
<title confidence="0.998868333333333">
Addressing the Resource
Bottleneck to Create
Large-Scale Annotated Texts
</title>
<author confidence="0.999228">
Jon Chamberlain
</author>
<affiliation confidence="0.998244">
University of Essex (UK)
</affiliation>
<email confidence="0.995709">
email: jchamb@essex.ac.uk
</email>
<author confidence="0.995788">
Massimo Poesio
</author>
<affiliation confidence="0.99625">
University of Essex (UK) &amp; Università di Trento (Italy)
</affiliation>
<email confidence="0.996105">
email: poesio@essex.ac.uk
</email>
<author confidence="0.993095">
Udo Kruschwitz
</author>
<affiliation confidence="0.994716">
University of Essex (UK)
</affiliation>
<email confidence="0.994968">
email: udo@essex.ac.uk
</email>
<sectionHeader confidence="0.993009" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999093923076923">
Large-scale linguistically annotated resources have become available in
recent years. This is partly due to sophisticated automatic and semi-
automatic approaches that work well on specific tasks such as part-of-
speech tagging. For more complex linguistic phenomena like anaphora
resolution there are no tools that result in high-quality annotations with-
out massive user intervention. Annotated corpora of the size needed for
modern computational linguistics research cannot however be created by
small groups of hand annotators. The ANAWIKI project strikes a balance
between collecting high-quality annotations from experts and applying a
game-like approach to collecting linguistic annotation from the general
Web population. More generally, ANAWIKI is a project that explores to
what extend expert annotations can be substituted by a critical mass of
non-expert judgements.
</bodyText>
<page confidence="0.79364">
375
376 Chamberlain, Poesio, and Kruschwitz
</page>
<sectionHeader confidence="0.998132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99992088">
Syntactically annotated language resources have long been around, but the greatest
obstacle to progress towards systems able to extract semantic information from text
is the lack of semantically annotated corpora large enough to be used to train and
evaluate semantic interpretation methods. Recent efforts to create resources to sup-
port large evaluation initiatives in the USA such as Automatic Context Extraction
(ACE), Translingual Information Detection, Extraction and Summarization (TIDES),
and GALE are beginning to change this, but just at a point when the community is
beginning to realize that even the 1M word annotated corpora created in substantial
efforts such as Prop-Bank (Palmer et al., 2005) and the OntoNotes initiative (Hovy
et al., 2006) are likely to be too small.
Unfortunately, the creation of 100M-plus corpora via hand annotation is likely to
be prohibitively expensive. Such a large hand-annotation effort would be even less
sensible in the case of semantic annotation tasks such as coreference or wordsense
disambiguation, given on the one side the greater difficulty of agreeing on a “neutral”
theoretical framework, on the other the difficulty of achieving more than moderate
agreement on semantic judgments (Poesio and Artstein, 2005).
The ANAWIKI project1 presents an effort to create high-quality, large-scale anaphor-
ically annotated resources (Poesio et al., 2008) by taking advantage of the collabora-
tion of the Web community, both through co-operative annotation efforts using tra-
ditional annotation tools and through the use of game-like interfaces. This makes
ANAWIKI a very ambitious project. It is not clear to what extend expert annotations
can in fact be substituted by those judgements submitted by the general public as part
of a game. If successful, ANAWIKI will actually be more than just an anaphora anno-
tation tool. We see it as a framework aimed at creating large-scale annotated corpora
in general.
</bodyText>
<sectionHeader confidence="0.854155" genericHeader="method">
2 Creating Resources through Web Collaboration
</sectionHeader>
<bodyText confidence="0.998143125">
Large-scale annotation of low-level linguistic information (part-of-speech tags) be-
gan with the Brown Corpus, in which very low-tech and time consuming methods
were used; but already for the creation of the British National Corpus (BNC), the first
100M-word linguistically annotated corpus, a faster methodology was developed con-
sisting of preliminary annotation with automatic methods followed by partial hand-
correction (Burnard, 2000). Medium and large-scale semantic annotation projects
(coreference, wordsense) are a fairly recent innovation in Computational Linguistics
(CL). The semi-automatic annotation methodology cannot yet be used for this type of
annotation, as the quality of, for instance, coreference resolvers is not yet high enough
on general text.
Collective resource creation on the Web offers a different way to the solution of
this problem. Wikipedia is perhaps the best example of collective resource creation,
but it is not an isolated case. The willingness of Web users to volunteer on the Web
extends to projects to create resources for Artificial Intelligence. One example is the
Open Mind Commonsense project, a project to mine commonsense knowledge (Singh,
2002) to which 14,500 participants contributed nearly 700,000 sentences. A more
</bodyText>
<footnote confidence="0.989091">
1http://www.anawiki.org
</footnote>
<note confidence="0.512594">
Addressing the Resource Bottleneck to Create Annotated Texts 377
</note>
<bodyText confidence="0.993808875">
recent, and perhaps more intriguing, development is the use of interactive game-style
interfaces to collect knowledge such as von Ahn et al. (2006). Perhaps the best known
example of this approach is the ESP game, a project to label images with tags through
a competitive game (von Ahn, 2006); 13,500 users played the game, creating 1.3M
labels in 3 months. If we managed to attract 15,000 volunteers, and each of them were
to annotate 10 texts of 700 words, we would get a corpus of the size of the BNC.
ANAWIKI builds on the proposals for marking anaphoric information allowing for
ambiguity developed in ARRAU (Poesio and Artstein, 2005) and previous projects.
The ARRAU project found that (i) using numerous annotators (up to 20 in some ex-
periments) leads to a much more robust identification of the major interpretation al-
ternatives (although outliers are also frequent); and (ii) the identification of alternative
interpretations is much more frequently a case of implicit ambiguity (each annotator
identifies only one interpretation, but these are different) than of explicit ambiguity
(annotators identifying multiple interpretations). The ARRAU project also developed
methods to analyze collections of such alternative interpretations and to identify out-
liers via clustering that will be exploited in this project.
</bodyText>
<figureCaption confidence="0.997091">
Figure 1: A screenshot of the Serengeti expert annotation tool.
</figureCaption>
<sectionHeader confidence="0.991984" genericHeader="method">
3 Annotation Tools
</sectionHeader>
<bodyText confidence="0.9995298">
Attempts to create hand annotated corpora face the dilemma of either going for the
traditional CL approach of high-quality annotation (of limited size) by experts or to
involve a large population of non-experts which could result in large-scale corpora
of inferior quality. The ANAWIKI project bridges this gap by combining both ap-
proaches to annotate the data: an expert annotation tool and a game interface. Both
</bodyText>
<note confidence="0.79751">
378 Chamberlain, Poesio, and Kruschwitz
</note>
<figureCaption confidence="0.999569">
Figure 2: A screenshot of the Game Interface (Annotation Mode).
</figureCaption>
<bodyText confidence="0.9974135">
tools are essential parts of ANAWIKI. We briefly describe both, with a particular focus
on the game interface.
</bodyText>
<subsectionHeader confidence="0.984144">
3.1 Expert Annotation Tool
</subsectionHeader>
<bodyText confidence="0.999966166666667">
An expert annotation tool is used to obtain Gold Standard annotations from computa-
tional linguists. In the case of anaphora annotation we use the Serengeti tool developed
at the University of Bielefeld (Stührenberg et al., 2007). The anaphoric annotation of
markables within this environment will be very detailed and will serve as a training
corpus as well as quality check for the second tool (see below). Figure 1 is a screen-
shot of this interface.
</bodyText>
<subsectionHeader confidence="0.997713">
3.2 Game Interface
</subsectionHeader>
<bodyText confidence="0.998445625">
A game interface is used to collect annotations from the general Web population. The
game interface integrates with the database of the expert annotation tool but aims to
collect large-scale (rather than detailed) anaphoric relations. Users are simply asked
to assign an anaphoric link but are not asked to specify what type (or what features)
are present.
Phrase Detectives2 is a game offering a simple user interface for non-expert users
to learn how to annotate text and to make annotation decisions. The goal of the game
is to identify relationships between words and phrases in a short text. Markables are
</bodyText>
<footnote confidence="0.984572">
2http://www.phrasedetectives.org
</footnote>
<note confidence="0.718297">
Addressing the Resource Bottleneck to Create Annotated Texts 379
</note>
<bodyText confidence="0.999956">
identified in the text by automatic pre-processing. There are 2 ways to annotate within
the game: by selecting the markable that is the antecedent of the anaphor (Annotation
Mode — see Figure 2); or by validating a decision previously submitted by another
user (Validation Mode). One motivation for Validation Mode is that we anticipate it
to be twice as fast as Annotation Mode (Chklovski and Gil, 2005).
Users begin the game at the training level and are given a set of annotation tasks
created from the Gold Standard. They are given feedback and guidance when they
select an incorrect answer and points when they select the correct answer. When
the user gives enough correct answers they graduate to annotating texts that will be
included in the corpus. Occasionally, a graduated user will be covertly given a Gold
Standard text to annotate. This is the foundation of the user rating system used to
judge the quality of the user’s annotations.
The game is designed to motivate users to annotate the text correctly by using com-
parative scoring (awarding points for agreeing with the Gold Standard), and retroac-
tive scoring (awarding points to the previous user if they are agreed with by the current
user). Using leader boards and assigning levels for points has been proven to be an
effective motivator, with users often using these as targets (von Ahn, 2006). The game
interface is described in more detail elsewhere (Chamberlain et al., 2008).
</bodyText>
<sectionHeader confidence="0.996593" genericHeader="method">
4 Challenges
</sectionHeader>
<bodyText confidence="0.999977947368421">
We are aiming at a balanced corpus, similar to the BNC, that includes texts from
Project Gutenberg, the Open American National Corpus, the Enron corpus and other
freely available sources. The chosen texts are stripped of all presentation formatting,
HTML and links to create the raw text. This is automatically parsed to extract mark-
ables consisting of noun phrases. The resulting XML format is stored in a relational
database that can be used in both the expert annotation tool and the game.
There are a number of challenges remaining in the project. First of all, the fully
automated processing of a substantial (i.e. multi-million) word corpus comprising
more than just news articles turned out to be non-trivial both in terms of robustness of
the processing tools as well as in terms of linguistic quality.
A second challenge is to recruit enough volunteers to annotate a 100 million word
corpus within the timescale of the project. It is our intention to use social networking
sites (including Facebook, Bebo, and MySpace) to attract volunteers to the game and
motivate participation by providing widgets (code segments that display the user’s
score and links to the game) to add to their profile pages.
Finally, the project’s aim is to generate a sufficiently large collection of annotations
from which semantically annotated corpora can be constructed. The usefulness of the
created resources can only be proven, for example, by training anaphora resolution
algorithms on the resulting annotations. This will be future work.
</bodyText>
<sectionHeader confidence="0.997678" genericHeader="method">
5 Next Steps
</sectionHeader>
<bodyText confidence="0.99953325">
We are currently in the process of building up a critical mass of source texts. Our aim
is to have a corpus size of 1M words by September 2008. By this time we also intend
having a multilingual user interface (initially English, Italian and German) with the
capacity to annotate texts in different languages although this is not the main focus.
</bodyText>
<note confidence="0.645499">
380 Chamberlain, Poesio, and Kruschwitz
</note>
<bodyText confidence="0.9921075">
In the future we will be considering extending the interface to include different anno-
tation tasks, for example marking coreference chains or Semantic Web mark-up. We
would like to present the game interface to gain feedback from the linguistic commu-
nity.
</bodyText>
<sectionHeader confidence="0.996879" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.84202">
ANAWIKI is funded by EPSRC (EP/F00575X/1). Thanks to Daniela Goecke, Maik
Stührenberg, Nils Diewald and Dieter Metzing. We also want to thank all volunteers
who have already contributed to the project and the reviewers for valuable feedback.
</bodyText>
<sectionHeader confidence="0.999236" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999404518518519">
Burnard, L. (2000). The British National Corpus Reference guide. Technical report,
Oxford University Computing Services, Oxford.
Chamberlain, J., M. Poesio, and U. Kruschwitz (2008). Phrase Detectives: A Web-
based Collaborative Annotation Game. In Proceedings of the International Con-
ference on Semantic Systems (I-Semantics’08), Graz. Forthcoming.
Chklovski, T. and Y. Gil (2005). Improving the design of intelligent acquisition in-
terfaces for collecting world knowledge from web contributors. In Proceedings of
K-CAP ’05, pp. 35–42.
Hovy, E., M. Marcus, M. Palmer, L. Ramshaw, and R. Weischedel (2006). OntoNotes:
The 90% Solution. In Proceedings of HLT-NAACL06.
Palmer, M., D. Gildea, and P. Kingsbury (2005). The proposition bank: An annotated
corpus of semantic roles. Computational Linguistics 31(1), 71–106.
Poesio, M. and R. Artstein (2005). The reliability of anaphoric annotation, recon-
sidered: Taking ambiguity into account. In Proceedings of the ACL Workshop on
Frontiers in Corpus Annotation, pp. 76–83.
Poesio, M., U. Kruschwitz, and J. Chamberlain (2008). ANAWIKI: Creating anaphor-
ically annotated resources through Web cooperation. In Proceedings of LREC’08,
Marrakech.
Singh, P. (2002). The public acquisition of commonsense knowledge. In Proceedings
of the AAAI Spring Symposium on Acquiring (and Using) Linguistic (and World)
Knowledge for Information Access, Palo Alto, CA.
Stührenberg, M., D. Goecke, N. Diewald, A. Mehler, and I. Cramer (2007). Web-
based annotation of anaphoric relations and lexical chains. In Proceedings of the
ACL Linguistic Annotation Workshop, pp. 140–147.
von Ahn, L. (2006). Games with a purpose. Computer 39(6), 92–94.
von Ahn, L., R. Liu, and M. Blum (2006). Peekaboom: a game for locating objects in
images. In Proceedings of CHI ’06, pp. 55–64.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.153260">
<title confidence="0.935368333333333">Addressing the Resource Bottleneck to Create Large-Scale Annotated Texts</title>
<author confidence="0.999898">Jon Chamberlain</author>
<affiliation confidence="0.999749">University of Essex (UK)</affiliation>
<author confidence="0.985141">Massimo Poesio</author>
<affiliation confidence="0.999639">University of Essex (UK) &amp; Università di Trento (Italy)</affiliation>
<author confidence="0.543583">Udo Kruschwitz</author>
<affiliation confidence="0.998518">University of Essex (UK)</affiliation>
<abstract confidence="0.999890928571428">Large-scale linguistically annotated resources have become available in recent years. This is partly due to sophisticated automatic and semiautomatic approaches that work well on specific tasks such as part-ofspeech tagging. For more complex linguistic phenomena like anaphora resolution there are no tools that result in high-quality annotations without massive user intervention. Annotated corpora of the size needed for modern computational linguistics research cannot however be created by small groups of hand annotators. The ANAWIKI project strikes a balance between collecting high-quality annotations from experts and applying a game-like approach to collecting linguistic annotation from the general Web population. More generally, ANAWIKI is a project that explores to what extend expert annotations can be substituted by a critical mass of non-expert judgements.</abstract>
<address confidence="0.4512745">375 376 Chamberlain, Poesio, and Kruschwitz</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Burnard</author>
</authors>
<title>The British National Corpus Reference guide.</title>
<date>2000</date>
<tech>Technical report,</tech>
<institution>Oxford University Computing Services,</institution>
<location>Oxford.</location>
<contexts>
<context position="3673" citStr="Burnard, 2000" startWordPosition="535" endWordPosition="536">e more than just an anaphora annotation tool. We see it as a framework aimed at creating large-scale annotated corpora in general. 2 Creating Resources through Web Collaboration Large-scale annotation of low-level linguistic information (part-of-speech tags) began with the Brown Corpus, in which very low-tech and time consuming methods were used; but already for the creation of the British National Corpus (BNC), the first 100M-word linguistically annotated corpus, a faster methodology was developed consisting of preliminary annotation with automatic methods followed by partial handcorrection (Burnard, 2000). Medium and large-scale semantic annotation projects (coreference, wordsense) are a fairly recent innovation in Computational Linguistics (CL). The semi-automatic annotation methodology cannot yet be used for this type of annotation, as the quality of, for instance, coreference resolvers is not yet high enough on general text. Collective resource creation on the Web offers a different way to the solution of this problem. Wikipedia is perhaps the best example of collective resource creation, but it is not an isolated case. The willingness of Web users to volunteer on the Web extends to project</context>
</contexts>
<marker>Burnard, 2000</marker>
<rawString>Burnard, L. (2000). The British National Corpus Reference guide. Technical report, Oxford University Computing Services, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chamberlain</author>
<author>M Poesio</author>
<author>U Kruschwitz</author>
</authors>
<title>Phrase Detectives: A Webbased Collaborative Annotation Game.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference on Semantic Systems (I-Semantics’08),</booktitle>
<location>Graz. Forthcoming.</location>
<contexts>
<context position="9282" citStr="Chamberlain et al., 2008" startWordPosition="1420" endWordPosition="1423">dard text to annotate. This is the foundation of the user rating system used to judge the quality of the user’s annotations. The game is designed to motivate users to annotate the text correctly by using comparative scoring (awarding points for agreeing with the Gold Standard), and retroactive scoring (awarding points to the previous user if they are agreed with by the current user). Using leader boards and assigning levels for points has been proven to be an effective motivator, with users often using these as targets (von Ahn, 2006). The game interface is described in more detail elsewhere (Chamberlain et al., 2008). 4 Challenges We are aiming at a balanced corpus, similar to the BNC, that includes texts from Project Gutenberg, the Open American National Corpus, the Enron corpus and other freely available sources. The chosen texts are stripped of all presentation formatting, HTML and links to create the raw text. This is automatically parsed to extract markables consisting of noun phrases. The resulting XML format is stored in a relational database that can be used in both the expert annotation tool and the game. There are a number of challenges remaining in the project. First of all, the fully automated</context>
</contexts>
<marker>Chamberlain, Poesio, Kruschwitz, 2008</marker>
<rawString>Chamberlain, J., M. Poesio, and U. Kruschwitz (2008). Phrase Detectives: A Webbased Collaborative Annotation Game. In Proceedings of the International Conference on Semantic Systems (I-Semantics’08), Graz. Forthcoming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Chklovski</author>
<author>Y Gil</author>
</authors>
<title>Improving the design of intelligent acquisition interfaces for collecting world knowledge from web contributors.</title>
<date>2005</date>
<booktitle>In Proceedings of K-CAP ’05,</booktitle>
<pages>35--42</pages>
<contexts>
<context position="8239" citStr="Chklovski and Gil, 2005" startWordPosition="1245" endWordPosition="1248">ion decisions. The goal of the game is to identify relationships between words and phrases in a short text. Markables are 2http://www.phrasedetectives.org Addressing the Resource Bottleneck to Create Annotated Texts 379 identified in the text by automatic pre-processing. There are 2 ways to annotate within the game: by selecting the markable that is the antecedent of the anaphor (Annotation Mode — see Figure 2); or by validating a decision previously submitted by another user (Validation Mode). One motivation for Validation Mode is that we anticipate it to be twice as fast as Annotation Mode (Chklovski and Gil, 2005). Users begin the game at the training level and are given a set of annotation tasks created from the Gold Standard. They are given feedback and guidance when they select an incorrect answer and points when they select the correct answer. When the user gives enough correct answers they graduate to annotating texts that will be included in the corpus. Occasionally, a graduated user will be covertly given a Gold Standard text to annotate. This is the foundation of the user rating system used to judge the quality of the user’s annotations. The game is designed to motivate users to annotate the te</context>
</contexts>
<marker>Chklovski, Gil, 2005</marker>
<rawString>Chklovski, T. and Y. Gil (2005). Improving the design of intelligent acquisition interfaces for collecting world knowledge from web contributors. In Proceedings of K-CAP ’05, pp. 35–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>M Marcus</author>
<author>M Palmer</author>
<author>L Ramshaw</author>
<author>R Weischedel</author>
</authors>
<title>OntoNotes: The 90% Solution.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL06.</booktitle>
<contexts>
<context position="1999" citStr="Hovy et al., 2006" startWordPosition="281" endWordPosition="284">antic information from text is the lack of semantically annotated corpora large enough to be used to train and evaluate semantic interpretation methods. Recent efforts to create resources to support large evaluation initiatives in the USA such as Automatic Context Extraction (ACE), Translingual Information Detection, Extraction and Summarization (TIDES), and GALE are beginning to change this, but just at a point when the community is beginning to realize that even the 1M word annotated corpora created in substantial efforts such as Prop-Bank (Palmer et al., 2005) and the OntoNotes initiative (Hovy et al., 2006) are likely to be too small. Unfortunately, the creation of 100M-plus corpora via hand annotation is likely to be prohibitively expensive. Such a large hand-annotation effort would be even less sensible in the case of semantic annotation tasks such as coreference or wordsense disambiguation, given on the one side the greater difficulty of agreeing on a “neutral” theoretical framework, on the other the difficulty of achieving more than moderate agreement on semantic judgments (Poesio and Artstein, 2005). The ANAWIKI project1 presents an effort to create high-quality, large-scale anaphorically a</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Hovy, E., M. Marcus, M. Palmer, L. Ramshaw, and R. Weischedel (2006). OntoNotes: The 90% Solution. In Proceedings of HLT-NAACL06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>D Gildea</author>
<author>P Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics</journal>
<volume>31</volume>
<issue>1</issue>
<pages>71--106</pages>
<contexts>
<context position="1950" citStr="Palmer et al., 2005" startWordPosition="273" endWordPosition="276">cle to progress towards systems able to extract semantic information from text is the lack of semantically annotated corpora large enough to be used to train and evaluate semantic interpretation methods. Recent efforts to create resources to support large evaluation initiatives in the USA such as Automatic Context Extraction (ACE), Translingual Information Detection, Extraction and Summarization (TIDES), and GALE are beginning to change this, but just at a point when the community is beginning to realize that even the 1M word annotated corpora created in substantial efforts such as Prop-Bank (Palmer et al., 2005) and the OntoNotes initiative (Hovy et al., 2006) are likely to be too small. Unfortunately, the creation of 100M-plus corpora via hand annotation is likely to be prohibitively expensive. Such a large hand-annotation effort would be even less sensible in the case of semantic annotation tasks such as coreference or wordsense disambiguation, given on the one side the greater difficulty of agreeing on a “neutral” theoretical framework, on the other the difficulty of achieving more than moderate agreement on semantic judgments (Poesio and Artstein, 2005). The ANAWIKI project1 presents an effort to</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Palmer, M., D. Gildea, and P. Kingsbury (2005). The proposition bank: An annotated corpus of semantic roles. Computational Linguistics 31(1), 71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Poesio</author>
<author>R Artstein</author>
</authors>
<title>The reliability of anaphoric annotation, reconsidered: Taking ambiguity into account.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Frontiers in Corpus Annotation,</booktitle>
<pages>76--83</pages>
<contexts>
<context position="2506" citStr="Poesio and Artstein, 2005" startWordPosition="358" endWordPosition="361">created in substantial efforts such as Prop-Bank (Palmer et al., 2005) and the OntoNotes initiative (Hovy et al., 2006) are likely to be too small. Unfortunately, the creation of 100M-plus corpora via hand annotation is likely to be prohibitively expensive. Such a large hand-annotation effort would be even less sensible in the case of semantic annotation tasks such as coreference or wordsense disambiguation, given on the one side the greater difficulty of agreeing on a “neutral” theoretical framework, on the other the difficulty of achieving more than moderate agreement on semantic judgments (Poesio and Artstein, 2005). The ANAWIKI project1 presents an effort to create high-quality, large-scale anaphorically annotated resources (Poesio et al., 2008) by taking advantage of the collaboration of the Web community, both through co-operative annotation efforts using traditional annotation tools and through the use of game-like interfaces. This makes ANAWIKI a very ambitious project. It is not clear to what extend expert annotations can in fact be substituted by those judgements submitted by the general public as part of a game. If successful, ANAWIKI will actually be more than just an anaphora annotation tool. W</context>
<context position="5231" citStr="Poesio and Artstein, 2005" startWordPosition="776" endWordPosition="779">rhaps more intriguing, development is the use of interactive game-style interfaces to collect knowledge such as von Ahn et al. (2006). Perhaps the best known example of this approach is the ESP game, a project to label images with tags through a competitive game (von Ahn, 2006); 13,500 users played the game, creating 1.3M labels in 3 months. If we managed to attract 15,000 volunteers, and each of them were to annotate 10 texts of 700 words, we would get a corpus of the size of the BNC. ANAWIKI builds on the proposals for marking anaphoric information allowing for ambiguity developed in ARRAU (Poesio and Artstein, 2005) and previous projects. The ARRAU project found that (i) using numerous annotators (up to 20 in some experiments) leads to a much more robust identification of the major interpretation alternatives (although outliers are also frequent); and (ii) the identification of alternative interpretations is much more frequently a case of implicit ambiguity (each annotator identifies only one interpretation, but these are different) than of explicit ambiguity (annotators identifying multiple interpretations). The ARRAU project also developed methods to analyze collections of such alternative interpretati</context>
</contexts>
<marker>Poesio, Artstein, 2005</marker>
<rawString>Poesio, M. and R. Artstein (2005). The reliability of anaphoric annotation, reconsidered: Taking ambiguity into account. In Proceedings of the ACL Workshop on Frontiers in Corpus Annotation, pp. 76–83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Poesio</author>
<author>U Kruschwitz</author>
<author>J Chamberlain</author>
</authors>
<title>ANAWIKI: Creating anaphorically annotated resources through Web cooperation.</title>
<date>2008</date>
<booktitle>In Proceedings of LREC’08,</booktitle>
<location>Marrakech.</location>
<contexts>
<context position="2639" citStr="Poesio et al., 2008" startWordPosition="376" endWordPosition="379">o small. Unfortunately, the creation of 100M-plus corpora via hand annotation is likely to be prohibitively expensive. Such a large hand-annotation effort would be even less sensible in the case of semantic annotation tasks such as coreference or wordsense disambiguation, given on the one side the greater difficulty of agreeing on a “neutral” theoretical framework, on the other the difficulty of achieving more than moderate agreement on semantic judgments (Poesio and Artstein, 2005). The ANAWIKI project1 presents an effort to create high-quality, large-scale anaphorically annotated resources (Poesio et al., 2008) by taking advantage of the collaboration of the Web community, both through co-operative annotation efforts using traditional annotation tools and through the use of game-like interfaces. This makes ANAWIKI a very ambitious project. It is not clear to what extend expert annotations can in fact be substituted by those judgements submitted by the general public as part of a game. If successful, ANAWIKI will actually be more than just an anaphora annotation tool. We see it as a framework aimed at creating large-scale annotated corpora in general. 2 Creating Resources through Web Collaboration La</context>
</contexts>
<marker>Poesio, Kruschwitz, Chamberlain, 2008</marker>
<rawString>Poesio, M., U. Kruschwitz, and J. Chamberlain (2008). ANAWIKI: Creating anaphorically annotated resources through Web cooperation. In Proceedings of LREC’08, Marrakech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Singh</author>
</authors>
<title>The public acquisition of commonsense knowledge.</title>
<date>2002</date>
<booktitle>In Proceedings of the AAAI Spring Symposium on Acquiring (and Using) Linguistic (and World) Knowledge for Information Access,</booktitle>
<location>Palo Alto, CA.</location>
<contexts>
<context position="4427" citStr="Singh, 2002" startWordPosition="649" endWordPosition="650">). The semi-automatic annotation methodology cannot yet be used for this type of annotation, as the quality of, for instance, coreference resolvers is not yet high enough on general text. Collective resource creation on the Web offers a different way to the solution of this problem. Wikipedia is perhaps the best example of collective resource creation, but it is not an isolated case. The willingness of Web users to volunteer on the Web extends to projects to create resources for Artificial Intelligence. One example is the Open Mind Commonsense project, a project to mine commonsense knowledge (Singh, 2002) to which 14,500 participants contributed nearly 700,000 sentences. A more 1http://www.anawiki.org Addressing the Resource Bottleneck to Create Annotated Texts 377 recent, and perhaps more intriguing, development is the use of interactive game-style interfaces to collect knowledge such as von Ahn et al. (2006). Perhaps the best known example of this approach is the ESP game, a project to label images with tags through a competitive game (von Ahn, 2006); 13,500 users played the game, creating 1.3M labels in 3 months. If we managed to attract 15,000 volunteers, and each of them were to annotate </context>
</contexts>
<marker>Singh, 2002</marker>
<rawString>Singh, P. (2002). The public acquisition of commonsense knowledge. In Proceedings of the AAAI Spring Symposium on Acquiring (and Using) Linguistic (and World) Knowledge for Information Access, Palo Alto, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stührenberg</author>
<author>D Goecke</author>
<author>N Diewald</author>
<author>A Mehler</author>
<author>I Cramer</author>
</authors>
<title>Webbased annotation of anaphoric relations and lexical chains.</title>
<date>2007</date>
<journal>Computer</journal>
<booktitle>In Proceedings of the ACL Linguistic Annotation Workshop,</booktitle>
<volume>39</volume>
<issue>6</issue>
<pages>140--147</pages>
<contexts>
<context position="6883" citStr="Stührenberg et al., 2007" startWordPosition="1026" endWordPosition="1029">rior quality. The ANAWIKI project bridges this gap by combining both approaches to annotate the data: an expert annotation tool and a game interface. Both 378 Chamberlain, Poesio, and Kruschwitz Figure 2: A screenshot of the Game Interface (Annotation Mode). tools are essential parts of ANAWIKI. We briefly describe both, with a particular focus on the game interface. 3.1 Expert Annotation Tool An expert annotation tool is used to obtain Gold Standard annotations from computational linguists. In the case of anaphora annotation we use the Serengeti tool developed at the University of Bielefeld (Stührenberg et al., 2007). The anaphoric annotation of markables within this environment will be very detailed and will serve as a training corpus as well as quality check for the second tool (see below). Figure 1 is a screenshot of this interface. 3.2 Game Interface A game interface is used to collect annotations from the general Web population. The game interface integrates with the database of the expert annotation tool but aims to collect large-scale (rather than detailed) anaphoric relations. Users are simply asked to assign an anaphoric link but are not asked to specify what type (or what features) are present. </context>
</contexts>
<marker>Stührenberg, Goecke, Diewald, Mehler, Cramer, 2007</marker>
<rawString>Stührenberg, M., D. Goecke, N. Diewald, A. Mehler, and I. Cramer (2007). Webbased annotation of anaphoric relations and lexical chains. In Proceedings of the ACL Linguistic Annotation Workshop, pp. 140–147. von Ahn, L. (2006). Games with a purpose. Computer 39(6), 92–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L von Ahn</author>
<author>R Liu</author>
<author>M Blum</author>
</authors>
<title>Peekaboom: a game for locating objects in images.</title>
<date>2006</date>
<booktitle>In Proceedings of CHI ’06,</booktitle>
<pages>55--64</pages>
<marker>von Ahn, Liu, Blum, 2006</marker>
<rawString>von Ahn, L., R. Liu, and M. Blum (2006). Peekaboom: a game for locating objects in images. In Proceedings of CHI ’06, pp. 55–64.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>