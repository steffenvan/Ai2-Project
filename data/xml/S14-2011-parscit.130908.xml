<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015626">
<title confidence="0.980797">
AI-KU: Using Co-Occurrence Modeling for Semantic Similarity
</title>
<author confidence="0.968371">
Osman Bas¸kaya
</author>
<affiliation confidence="0.895243">
Artificial Intelligence Laboratory
Koc¸ University, Istanbul, Turkey
</affiliation>
<email confidence="0.984117">
obaskaya@ku.edu.tr
</email>
<sectionHeader confidence="0.993591" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996644333333333">
In this paper, we describe our unsupervised
method submitted to the Cross-Level Se-
mantic Similarity task in Semeval 2014 that
computes semantic similarity between two
different sized text fragments. Our method
models each text fragment by using the co-
occurrence statistics of either occurred words
or their substitutes. The co-occurrence mod-
eling step provides dense, low-dimensional
embedding for each fragment which allows
us to calculate semantic similarity using
various similarity metrics. Although our
current model avoids the syntactic infor-
mation, we achieved promising results and
outperformed all baselines.
</bodyText>
<sectionHeader confidence="0.999001" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.963112016129033">
Semantic similarity is a measure that specifies the
similarity of one text’s meaning to another’s. Se-
mantic similarity plays an important role in vari-
ous Natural Language Processing (NLP) tasks such
as textual entailment (Berant et al., 2012), summa-
rization (Lin and Hovy, 2003), question answering
(Surdeanu et al., 2011), text classification (Sebas-
tiani, 2002), word sense disambiguation (Sch¨utze,
1998) and information retrieval (Park et al., 2005).
There are three main approaches to computing
the semantic similarity between two text fragments.
The first approach uses Vector Space Models (see
Turney &amp; Pantel (2010) for an overview) where
each text is represented as a bag-of-word model.
The similarity between two text fragments can then
be computed with various metrics such as cosine
similarity. Sparseness in the input nature is the
key problem for these models. Therefore, later
works such as Latent Semantic Indexing (?) and
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
Topic Models (Blei et al., 2003) overcome spar-
sity problems via reducing the dimensionality of
the model by introducing latent variables. The sec-
ond approach blends various lexical and syntactic
features and attacks the problem through machine
learning models. The third approach is based on
word-to-word similarity alignment (Pilehvar et al.,
2013; Islam and Inkpen, 2008).
The Cross-Level Semantic Similarity (CLSS) task
in SemEval 20141 (Jurgens et al., 2014) provides
an evaluation framework to assess similarity meth-
ods for texts in different volumes (i.e., lexical lev-
els). Unlike previous SemEval and *SEM tasks
that were interested in comparing texts with simi-
lar volume, this task consists of four subtasks (para-
graph2sentence, sentence2phrase, phrase2word and
word2sense) that investigate the performance of
systems based on pairs of texts of different sizes.
A system should report the similarity score of a
given pair, ranging from 4 (two items have very
similar meanings and the most important ideas,
concepts, or actions in the larger text are repre-
sented in the smaller text) to 0 (two items do not
mean the same thing and are not on the same topic).
In this paper, we describe our two unsupervised
systems that are based on co-occurrence statistics
of words. The only difference between the sys-
tems is the input they use. The first system uses the
words directly (after lemmatization, stop-word re-
moval and excluding the non-alphanumeric char-
acters) in text while the second system utilizes the
most likely substitutes consulted by a 4-gram lan-
guage model for each observed word position (i.e.,
context). Note that we participated two subtasks
which are paragraph2sentence and sentence2phrase.
The remainder of the paper proceeds as follows.
Section 2 explains the preprocessing part, the dif-
ference between the systems, co-occurrence mod-
eling, and how we calculate the similarity between
</bodyText>
<footnote confidence="0.9990785">
1http://alt.qcri.org/semeval2014/
task3/
</footnote>
<page confidence="0.934186">
92
</page>
<note confidence="0.7618365">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 92–96,
Dublin, Ireland, August 23-24, 2014.
</note>
<table confidence="0.992029428571429">
Type-ID Lemma
Sent-33 choose
Sent-33 buy
Sent-33 gift
Sent-33 card
Sent-33 hard
Sent-33 decision
</table>
<tableCaption confidence="0.9767575">
Table 1: Instance id-word pairs for a given sen-
tence.
</tableCaption>
<bodyText confidence="0.9995566">
two texts after co-occurrence modeling has been
done. Section 3 discusses the results of our sys-
tems and compares them to other participants’. Sec-
tion 4 discusses the findings and concludes with
plans for future work.
</bodyText>
<sectionHeader confidence="0.991849" genericHeader="introduction">
2 Algorithm
</sectionHeader>
<bodyText confidence="0.999738444444444">
This section explains preprocessing steps of the
data and the details of our two systems2. Both
systems rely on the co-occurrence statistics. The
slight difference between the two is that the first
one uses the words that occur in the given text
fragment (e.g., paragraph, sentence), whereas the
latter employs co-occurrence statistics on 100 sub-
stitute samples for each word within the given text
fragment.
</bodyText>
<subsectionHeader confidence="0.991455">
2.1 Data Preprocessing
</subsectionHeader>
<bodyText confidence="0.99990325">
Two AI-KU systems can be distinguished by their
inputs. One uses the raw input words, whereas the
other uses words’ likely substitutes according to a
language model.
AI-KUj: This system uses the words that were
in the text. All words are transformed into lower-
case equivalents. Lemmatization3 and stop-word
removal were performed, and non-alphanumeric
characters were excluded. Table 1 displays the
pairs for the following sentence which is an in-
stance from paragraph2sentence test set:
“Choosing what to buy with a $35 gift
card is a hard decision.”
Note that the input that we used to model co-
occurrence statistics consists of all such pairs for
each fragment in a given subtask.
</bodyText>
<footnote confidence="0.9818008">
2The code to replicate our work can be found
at https://github.com/osmanbaskaya/
semeval14-task3.
3Lemmatization is carried out with Stanford CoreNLP
and transforms a word into its canonical or base form.
</footnote>
<bodyText confidence="0.998296875">
AI-KU2: Previously, the utilization of high prob-
ability substitutes and their co-occurrence statis-
tics achieved notable performance on Word Sense
Induction (WSI) (Baskaya et al., 2013) and Part-
of-Speech Induction (Yatbaz et al., 2012) prob-
lems. AI-KU2 represents each context of a word
by finding the most likely 100 substitutes suggested
by the 4-gram language model we built from ukWaC4
(Ferraresi et al., 2008), a 2-billion word web-gathered
corpus. Since S-CODE algorithm works with dis-
crete input, for each context we sample 100 substi-
tute words with replacement using their probabili-
ties. Table 2 illustrates the context and substitutes
of each context using a bigram language model.
No lemmatization, stop-word removal and lower-
case transformation were performed.
</bodyText>
<subsectionHeader confidence="0.991531">
2.2 Co-Occurrence Modeling
</subsectionHeader>
<bodyText confidence="0.998909387096774">
This subsection will explain the unsupervised method
we employed to model co-occurrence statistics: the
Co-occurrence data Embedding (CODE) method
(Globerson et al., 2007) and its spherical exten-
sion (S-CODE) proposed by Maron et al. (2010).
Unlike in our WSI work, where we ended up with
an embedding for each word in the co-occurrence
modeling step in this task, we model each text unit
such as a paragraph, a sentence or a phrase, to ob-
tain embeddings for each instance.
Input data for S-CODE algorithm consist of instance-
id and each word in the text unit for the first sys-
tem (Table 1 illustrates the pairs for only one text
fragment) instance-ids and 100 substitute samples
of each word in text for the second system. In
the initial step, S-CODE puts all instance-ids and
words (or substitutes, depending on the system)
randomly on an n-dimensional sphere. If two dif-
ferent instances have the same word or substitute,
then these two instances attract one another — oth-
erwise they repel each other. When S-CODE con-
verges, instances that have similar words or sub-
stitutes will be closely located or else, they will be
distant from each other.
AI-KUj: According to the training set perfor-
mances for various n (i.e., number of dimensions
for S-CODE algorithm), we picked 100 for both
tasks.
AI-KU2: We picked n to be 200 and 100 for
paragraph2sentence and sentence2phrase subtasks,
respectively.
</bodyText>
<footnote confidence="0.997727">
4Available here: http://wacky.sslmit.unibo.it
</footnote>
<page confidence="0.993818">
93
</page>
<figure confidence="0.811119333333333">
Word
Context
Substitutes
the
dog
bites
</figure>
<bodyText confidence="0.653697">
The (0.12), A (0.11), If (0.02), As (0.07), Stray (0.001),..., w,,, (0.02)
cat (0.007), dog (0.005), animal (0.002), wolve (0.001), ..., w,,, (0.01)
runs (0.14), bites (0.13), catches (0.04), barks (0.001), ..., w,,, (0.01)
</bodyText>
<figure confidence="0.713696">
&lt;s&gt; dog
the
dog .
</figure>
<tableCaption confidence="0.9154445">
Table 2: Contexts and substitute distributions when a bigram language model is used. w and n denote an
arbitrary word in the vocabulary and the vocabulary size, respectively.
</tableCaption>
<table confidence="0.999890714285714">
System Pearson Spearman
Paragraph-2-Sentence AI-KU, 0.671 0.676
AI-KU2 0.542 0.531
LCS 0.499 0.602
lch 0.584 0.596
lin 0.568 0.562
JI 0.613 0.644
</table>
<tableCaption confidence="0.614166">
Table 3: Paragraph-2-Sentence subtask scores for
the training data. Subscripts in AI-KU systems
specify the run number.
</tableCaption>
<table confidence="0.999921285714286">
System Pearson Spearman
Sentence-2-Phrase AI-KU, 0.607 0.568
AI-KU2 0.620 0.579
LCS 0.500 0.582
lch 0.484 0.491
lin 0.492 0.470
JI 0.465 0.465
</table>
<tableCaption confidence="0.9914875">
Table 4: Sentence2phrase subtask scores for the
training data.
</tableCaption>
<sectionHeader confidence="0.993117" genericHeader="method">
3 Evaluation Results
</sectionHeader>
<bodyText confidence="0.9998945">
Since this step is unsupervised, we tried to en-
rich the data with ukWaC, however, enrichment
with ukWaC did not work well on the training data.
To this end, proposed scores were obtained using
only the training and the test data provided by or-
ganizers.
</bodyText>
<subsectionHeader confidence="0.99628">
2.3 Similarity Calculation
</subsectionHeader>
<bodyText confidence="0.999951093023256">
When the S-CODE converges, there is an n-dimen-
sional embedding for each textual level (e.g., para-
graph, sentence, phrase) instance. We can use a
similarity metric to calculate the similarity between
these embeddings. For this task, systems should
report only the similarity between two specific cross
level instances. Note that we used cosine simi-
larity to calculate similarity between two textual
units. This similarity is the eventual similarity for
two instances; no further processing (e.g., scaling)
has been done.
In this task, two correlation metrics were used
to evaluate the systems: Pearson correlation and
Spearman’s rank correlation. Pearson correlation
tests the degree of similarity between the system’s
similarity ratings and the gold standard ratings. Spear-
man’s rank correlation measures the degree of sim-
ilarity between two rankings; similarity ratings pro-
vided by a system and the gold standard ratings.
Tables 3 and 4 show the scores for Paragraph-2-
Sentence and Sentence-2-Phrase subtasks on the
training data, respectively. These tables contain
the best individual scores for the performance met-
rics, Normalized Longest Common Substring (LCS)
baseline, which was given by task organizers, and
three additional baselines: lin (Lin, 1998), lch (Lea-
cock and Chodorow, 1998), and the Jaccard In-
dex (JI) baseline. lin uses the information content
(Resnik, 1995) of the least common subsumer of
concepts A and B. Information content (IC) indi-
cates the specificity of a concept; the least com-
mon subsumer of a concept A and B is the most
specific concept from which A and B are inherited.
lin similarity5 returns the difference between two
times of the IC of the least common subsumer of
A and B, and the sum of IC of both concepts. On
the other hand, lch is a score denoting how similar
two concepts are, calculated by using the shortest
path that connects the concept and the maximum
depth of the taxonomy in which the concepts oc-
cur6 (please see Pedersen et al. (2004) for further
details of these measures). These two baselines
were calculated as follows. First, using the Stan-
</bodyText>
<footnote confidence="0.9980488">
5lin similarity = 2 ∗ IC(lcs)/(IC(A) + IC(B)) where
lcs indicates the least common subsumer of concepts A and
B.
6The exact formulation is −log(L/2d) where L is the
shortest path length and d is the taxonomy depth.
</footnote>
<page confidence="0.998099">
94
</page>
<table confidence="0.999366">
System Pearson Spearman
Paragraph-2-Sentence Best 0.837 0.821
2nd Best 0.834 0.820
3rd Best 0.826 0.817
AI-KU, 0.732 0.727
AI-KU2 0.698 0.700
LCS 0.527 0.613
lch 0.629 0.627
lin 0.612 0.601
JI 0.640 0.687
</table>
<tableCaption confidence="0.992443">
Table 5: Paragraph-2-Sentence subtask scores for
</tableCaption>
<bodyText confidence="0.78457125">
the test data. Best indicates the best correlation
score for the subtask. LCS stands for Normalized
Longest Common Substring. Subscripts in AI-KU
systems specify the run number.
</bodyText>
<table confidence="0.9985185">
System Pearson Spearman
Sentence-2-Phrase Best 0.777 0.642
2nd Best 0.771 0.760
3rd Best 0.760 0.757
AI-KU, 0.680 0.646
AI-KU2 0.617 0.612
LCS 0.562 0.626
lch 0.526 0.544
lin 0.501 0.498
JI 0.540 0.555
</table>
<tableCaption confidence="0.961136">
Table 6: Sentence2phrase subtask scores for the
test data.
</tableCaption>
<bodyText confidence="0.997601454545455">
graph2Sentence subtask, since smaller textual units
(such as phrases) make the problem more difficult.
ford Part-of-Speech Tagger (Toutanova and Man-
ning, 2000) we tagged words across all textual lev-
els. After tagging, we found the synsets of each
word matched with its part-of-speech using Word-
Net 3.0 (Miller and Fellbaum, 1998). For each
synset of a word in the shorter textual unit (e.g.,
sentence is shorter than paragraph), we calculated
the lin/lch measure of each synset of all words
in the longer textual unit and picked the highest
score. When we found the scores for all words,
we calculated the mean to find out the similarity
between one pair in the test set. Finally, Jaccard
Index baseline was used to simply calculate the
number of words in common (intersection) with
two cross textual levels, normalized by the total
number of words (union). Table 5 and 6 demon-
strate the AI-KU runs on the test data. Next, we
present our results pertaining to the test data.
Paragraph2Sentence: Both systems outperformed
all the baselines for both metrics. The best score
for this subtask was .837 and our systems achieved
.732 and .698 on Pearson and did similar on Spear-
man metric. These scores are promising since our
current unsupervised systems are based on bag-of-
words approach — they do not utilize any syntac-
tic information.
Sentence2Phrase: In this subtask, AI-KU sys-
tems outperformed all baselines with the excep-
tion of the AI-KU2 system which performed slightly
worse than LCS on Spearman metric. Performances
of systems and baselines were lower than Para-
</bodyText>
<sectionHeader confidence="0.997758" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.9999803">
In this work, we introduced two unsupervised sys-
tems that utilize co-occurrence statistics and rep-
resent textual units as dense, low dimensional em-
beddings. Although current systems are based on
bag-of-word approach and discard the syntactic in-
formation, they achieved promising results in both
paragraph2sentence and sentence2phrase subtasks.
For future work, we will extend our algorithm by
adding syntactic information (e.g, dependency pars-
ing output) into the co-occurrence modeling step.
</bodyText>
<sectionHeader confidence="0.99856" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99679365">
Osman Baskaya, Enis Sert, Volkan Cirik, and Deniz
Yuret. 2013. AI-KU: Using substitute vectors and
co-occurrence modeling for word sense induction
and disambiguation. In Proceedings of the Second
Joint Conference on Lexical and Computational Se-
mantics (*SEM), Volume 2: Seventh International
Workshop on Semantic Evaluation (SemEval 2013),
pages 300–306.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2012. Learning entailment relations by global graph
structure optimization. Computational Linguistics,
38(1):73–111.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. The Journal of
Machine Learning Research, 3:993–1022.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluating
ukwac, a very large web-derived corpus of english.
In In Proceedings of the 4th Web as Corpus Work-
shop (WAC-4).
</reference>
<page confidence="0.993336">
95
</page>
<reference confidence="0.9896502875">
Amir Globerson, Gal Chechik, Fernando Pereira, and
Naftali Tishby. 2007. Euclidean embedding of co-
occurrence data. Journal of Machine Learning Re-
search, 8(10).
Aminul Islam and Diana Inkpen. 2008. Semantic text
similarity using corpus-based word similarity and
string similarity. ACM Transactions on Knowledge
Discovery from Data (TKDD), 2(2):10.
David Jurgens, Mohammed Taher Pilehvar, and
Roberto Navigli. 2014. Semeval-2014 task 3:
Cross-level semantic similarity. In Proceedings of
the 8th International Workshop on Semantic Evalu-
ation (SemEval-2014). August 23-24, 2014, Dublin,
Ireland.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and wordnet similarity for word
sense identification. WordNet: An electronic lexical
database, 49(2):265–283.
Chin-Yew Lin and Eduard Hovy. 2003. Auto-
matic evaluation of summaries using n-gram co-
occurrence statistics. In Proceedings of the 2003
Conference of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology-Volume 1, pages 71–78.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In ICML, volume 98, pages 296–
304.
Yariv Maron, Michael Lamar, and Elie Bienenstock.
2010. Sphere Embedding: An Application to Part-
of-Speech Induction. In J Lafferty, C K I Williams,
J Shawe-Taylor, R S Zemel, and A Culotta, editors,
Advances in Neural Information Processing Systems
23, pages 1567–1575.
George Miller and Christiane Fellbaum. 1998. Word-
net: An electronic lexical database.
Eui-Kyu Park, Dong-Yul Ra, and Myung-Gil Jang.
2005. Techniques for improving web retrieval ef-
fectiveness. Information processing &amp; management,
41(5):1207–1223.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet:: Similarity: measuring the re-
latedness of concepts. In Demonstration Papers at
HLT-NAACL 2004, pages 38–41.
Mohammad Taher Pilehvar, David Jurgens, and
Roberto Navigli. 2013. Align, disambiguate and
walk: A unified approach for measuring semantic
similarity. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2013).
Philip Resnik. 1995. Using information content to
evaluate semantic similarity in a taxonomy. arXiv
preprint cmp-lg/9511007.
Hinrich Sch¨utze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97–
123.
Fabrizio Sebastiani. 2002. Machine learning in auto-
mated text categorization. ACM computing surveys
(CSUR), 34(1):1–47.
Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2011. Learning to rank answers to non-
factoid questions from web collections. Computa-
tional Linguistics, 37(2):351–383.
Kristina Toutanova and Christopher D Manning. 2000.
Enriching the knowledge sources used in a maxi-
mum entropy part-of-speech tagger. In Proceedings
of the 2000 Joint SIGDAT conference on Empirical
methods in natural language processing and very
large corpora: held in conjunction with the 38th An-
nual Meeting of the Association for Computational
Linguistics-Volume 13, pages 63–70.
Peter D. Turney and Patrick Pantel. 2010. From Fre-
quency to Meaning: Vector Space Models of Se-
mantics. Journal of Artificial Intelligence Research,
37:141–188.
Mehmet Ali Yatbaz, Enis Sert, and Deniz Yuret. 2012.
Learning syntactic categories using paradigmatic
representations of word context. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 940–951.
</reference>
<page confidence="0.998155">
96
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.463135">
<title confidence="0.838595">AI-KU: Using Co-Occurrence Modeling for Semantic Similarity</title>
<affiliation confidence="0.932506">Artificial Intelligence</affiliation>
<address confidence="0.609995">University, Istanbul,</address>
<email confidence="0.995412">obaskaya@ku.edu.tr</email>
<abstract confidence="0.997150125">In this paper, we describe our unsupervised method submitted to the Cross-Level Semantic Similarity task in Semeval 2014 that computes semantic similarity between two different sized text fragments. Our method models each text fragment by using the cooccurrence statistics of either occurred words or their substitutes. The co-occurrence modeling step provides dense, low-dimensional embedding for each fragment which allows us to calculate semantic similarity using various similarity metrics. Although our current model avoids the syntactic information, we achieved promising results and outperformed all baselines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Osman Baskaya</author>
<author>Enis Sert</author>
<author>Volkan Cirik</author>
<author>Deniz Yuret</author>
</authors>
<title>AI-KU: Using substitute vectors and co-occurrence modeling for word sense induction and disambiguation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>300--306</pages>
<contexts>
<context position="5922" citStr="Baskaya et al., 2013" startWordPosition="891" endWordPosition="894">ce from paragraph2sentence test set: “Choosing what to buy with a $35 gift card is a hard decision.” Note that the input that we used to model cooccurrence statistics consists of all such pairs for each fragment in a given subtask. 2The code to replicate our work can be found at https://github.com/osmanbaskaya/ semeval14-task3. 3Lemmatization is carried out with Stanford CoreNLP and transforms a word into its canonical or base form. AI-KU2: Previously, the utilization of high probability substitutes and their co-occurrence statistics achieved notable performance on Word Sense Induction (WSI) (Baskaya et al., 2013) and Partof-Speech Induction (Yatbaz et al., 2012) problems. AI-KU2 represents each context of a word by finding the most likely 100 substitutes suggested by the 4-gram language model we built from ukWaC4 (Ferraresi et al., 2008), a 2-billion word web-gathered corpus. Since S-CODE algorithm works with discrete input, for each context we sample 100 substitute words with replacement using their probabilities. Table 2 illustrates the context and substitutes of each context using a bigram language model. No lemmatization, stop-word removal and lowercase transformation were performed. 2.2 Co-Occurr</context>
</contexts>
<marker>Baskaya, Sert, Cirik, Yuret, 2013</marker>
<rawString>Osman Baskaya, Enis Sert, Volkan Cirik, and Deniz Yuret. 2013. AI-KU: Using substitute vectors and co-occurrence modeling for word sense induction and disambiguation. In Proceedings of the Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 300–306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Ido Dagan</author>
<author>Jacob Goldberger</author>
</authors>
<title>Learning entailment relations by global graph structure optimization.</title>
<date>2012</date>
<journal>Computational Linguistics,</journal>
<volume>38</volume>
<issue>1</issue>
<contexts>
<context position="1047" citStr="Berant et al., 2012" startWordPosition="142" endWordPosition="145">rrence statistics of either occurred words or their substitutes. The co-occurrence modeling step provides dense, low-dimensional embedding for each fragment which allows us to calculate semantic similarity using various similarity metrics. Although our current model avoids the syntactic information, we achieved promising results and outperformed all baselines. 1 Introduction Semantic similarity is a measure that specifies the similarity of one text’s meaning to another’s. Semantic similarity plays an important role in various Natural Language Processing (NLP) tasks such as textual entailment (Berant et al., 2012), summarization (Lin and Hovy, 2003), question answering (Surdeanu et al., 2011), text classification (Sebastiani, 2002), word sense disambiguation (Sch¨utze, 1998) and information retrieval (Park et al., 2005). There are three main approaches to computing the semantic similarity between two text fragments. The first approach uses Vector Space Models (see Turney &amp; Pantel (2010) for an overview) where each text is represented as a bag-of-word model. The similarity between two text fragments can then be computed with various metrics such as cosine similarity. Sparseness in the input nature is th</context>
</contexts>
<marker>Berant, Dagan, Goldberger, 2012</marker>
<rawString>Jonathan Berant, Ido Dagan, and Jacob Goldberger. 2012. Learning entailment relations by global graph structure optimization. Computational Linguistics, 38(1):73–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="1987" citStr="Blei et al., 2003" startWordPosition="281" endWordPosition="284">roach uses Vector Space Models (see Turney &amp; Pantel (2010) for an overview) where each text is represented as a bag-of-word model. The similarity between two text fragments can then be computed with various metrics such as cosine similarity. Sparseness in the input nature is the key problem for these models. Therefore, later works such as Latent Semantic Indexing (?) and This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ Topic Models (Blei et al., 2003) overcome sparsity problems via reducing the dimensionality of the model by introducing latent variables. The second approach blends various lexical and syntactic features and attacks the problem through machine learning models. The third approach is based on word-to-word similarity alignment (Pilehvar et al., 2013; Islam and Inkpen, 2008). The Cross-Level Semantic Similarity (CLSS) task in SemEval 20141 (Jurgens et al., 2014) provides an evaluation framework to assess similarity methods for texts in different volumes (i.e., lexical levels). Unlike previous SemEval and *SEM tasks that were int</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. The Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
</authors>
<title>Introducing and evaluating ukwac, a very large web-derived corpus of english. In</title>
<date>2008</date>
<booktitle>In Proceedings of the 4th Web as Corpus Workshop (WAC-4).</booktitle>
<contexts>
<context position="6151" citStr="Ferraresi et al., 2008" startWordPosition="929" endWordPosition="932">ask. 2The code to replicate our work can be found at https://github.com/osmanbaskaya/ semeval14-task3. 3Lemmatization is carried out with Stanford CoreNLP and transforms a word into its canonical or base form. AI-KU2: Previously, the utilization of high probability substitutes and their co-occurrence statistics achieved notable performance on Word Sense Induction (WSI) (Baskaya et al., 2013) and Partof-Speech Induction (Yatbaz et al., 2012) problems. AI-KU2 represents each context of a word by finding the most likely 100 substitutes suggested by the 4-gram language model we built from ukWaC4 (Ferraresi et al., 2008), a 2-billion word web-gathered corpus. Since S-CODE algorithm works with discrete input, for each context we sample 100 substitute words with replacement using their probabilities. Table 2 illustrates the context and substitutes of each context using a bigram language model. No lemmatization, stop-word removal and lowercase transformation were performed. 2.2 Co-Occurrence Modeling This subsection will explain the unsupervised method we employed to model co-occurrence statistics: the Co-occurrence data Embedding (CODE) method (Globerson et al., 2007) and its spherical extension (S-CODE) propos</context>
</contexts>
<marker>Ferraresi, Zanchetta, Baroni, Bernardini, 2008</marker>
<rawString>Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and Silvia Bernardini. 2008. Introducing and evaluating ukwac, a very large web-derived corpus of english. In In Proceedings of the 4th Web as Corpus Workshop (WAC-4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amir Globerson</author>
<author>Gal Chechik</author>
<author>Fernando Pereira</author>
<author>Naftali Tishby</author>
</authors>
<title>Euclidean embedding of cooccurrence data.</title>
<date>2007</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>8</volume>
<issue>10</issue>
<contexts>
<context position="6707" citStr="Globerson et al., 2007" startWordPosition="1008" endWordPosition="1011">4-gram language model we built from ukWaC4 (Ferraresi et al., 2008), a 2-billion word web-gathered corpus. Since S-CODE algorithm works with discrete input, for each context we sample 100 substitute words with replacement using their probabilities. Table 2 illustrates the context and substitutes of each context using a bigram language model. No lemmatization, stop-word removal and lowercase transformation were performed. 2.2 Co-Occurrence Modeling This subsection will explain the unsupervised method we employed to model co-occurrence statistics: the Co-occurrence data Embedding (CODE) method (Globerson et al., 2007) and its spherical extension (S-CODE) proposed by Maron et al. (2010). Unlike in our WSI work, where we ended up with an embedding for each word in the co-occurrence modeling step in this task, we model each text unit such as a paragraph, a sentence or a phrase, to obtain embeddings for each instance. Input data for S-CODE algorithm consist of instanceid and each word in the text unit for the first system (Table 1 illustrates the pairs for only one text fragment) instance-ids and 100 substitute samples of each word in text for the second system. In the initial step, S-CODE puts all instance-id</context>
</contexts>
<marker>Globerson, Chechik, Pereira, Tishby, 2007</marker>
<rawString>Amir Globerson, Gal Chechik, Fernando Pereira, and Naftali Tishby. 2007. Euclidean embedding of cooccurrence data. Journal of Machine Learning Research, 8(10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aminul Islam</author>
<author>Diana Inkpen</author>
</authors>
<title>Semantic text similarity using corpus-based word similarity and string similarity.</title>
<date>2008</date>
<journal>ACM Transactions on Knowledge Discovery from Data (TKDD),</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="2328" citStr="Islam and Inkpen, 2008" startWordPosition="331" endWordPosition="334">h as Latent Semantic Indexing (?) and This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ Topic Models (Blei et al., 2003) overcome sparsity problems via reducing the dimensionality of the model by introducing latent variables. The second approach blends various lexical and syntactic features and attacks the problem through machine learning models. The third approach is based on word-to-word similarity alignment (Pilehvar et al., 2013; Islam and Inkpen, 2008). The Cross-Level Semantic Similarity (CLSS) task in SemEval 20141 (Jurgens et al., 2014) provides an evaluation framework to assess similarity methods for texts in different volumes (i.e., lexical levels). Unlike previous SemEval and *SEM tasks that were interested in comparing texts with similar volume, this task consists of four subtasks (paragraph2sentence, sentence2phrase, phrase2word and word2sense) that investigate the performance of systems based on pairs of texts of different sizes. A system should report the similarity score of a given pair, ranging from 4 (two items have very simila</context>
</contexts>
<marker>Islam, Inkpen, 2008</marker>
<rawString>Aminul Islam and Diana Inkpen. 2008. Semantic text similarity using corpus-based word similarity and string similarity. ACM Transactions on Knowledge Discovery from Data (TKDD), 2(2):10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Jurgens</author>
<author>Mohammed Taher Pilehvar</author>
<author>Roberto Navigli</author>
</authors>
<title>Semeval-2014 task 3: Cross-level semantic similarity.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014).</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="2417" citStr="Jurgens et al., 2014" startWordPosition="344" endWordPosition="347">ution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ Topic Models (Blei et al., 2003) overcome sparsity problems via reducing the dimensionality of the model by introducing latent variables. The second approach blends various lexical and syntactic features and attacks the problem through machine learning models. The third approach is based on word-to-word similarity alignment (Pilehvar et al., 2013; Islam and Inkpen, 2008). The Cross-Level Semantic Similarity (CLSS) task in SemEval 20141 (Jurgens et al., 2014) provides an evaluation framework to assess similarity methods for texts in different volumes (i.e., lexical levels). Unlike previous SemEval and *SEM tasks that were interested in comparing texts with similar volume, this task consists of four subtasks (paragraph2sentence, sentence2phrase, phrase2word and word2sense) that investigate the performance of systems based on pairs of texts of different sizes. A system should report the similarity score of a given pair, ranging from 4 (two items have very similar meanings and the most important ideas, concepts, or actions in the larger text are repr</context>
</contexts>
<marker>Jurgens, Pilehvar, Navigli, 2014</marker>
<rawString>David Jurgens, Mohammed Taher Pilehvar, and Roberto Navigli. 2014. Semeval-2014 task 3: Cross-level semantic similarity. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014). August 23-24, 2014, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
</authors>
<title>Combining local context and wordnet similarity for word sense identification. WordNet: An electronic lexical database,</title>
<date>1998</date>
<pages>49--2</pages>
<contexts>
<context position="10504" citStr="Leacock and Chodorow, 1998" startWordPosition="1610" endWordPosition="1614">tests the degree of similarity between the system’s similarity ratings and the gold standard ratings. Spearman’s rank correlation measures the degree of similarity between two rankings; similarity ratings provided by a system and the gold standard ratings. Tables 3 and 4 show the scores for Paragraph-2- Sentence and Sentence-2-Phrase subtasks on the training data, respectively. These tables contain the best individual scores for the performance metrics, Normalized Longest Common Substring (LCS) baseline, which was given by task organizers, and three additional baselines: lin (Lin, 1998), lch (Leacock and Chodorow, 1998), and the Jaccard Index (JI) baseline. lin uses the information content (Resnik, 1995) of the least common subsumer of concepts A and B. Information content (IC) indicates the specificity of a concept; the least common subsumer of a concept A and B is the most specific concept from which A and B are inherited. lin similarity5 returns the difference between two times of the IC of the least common subsumer of A and B, and the sum of IC of both concepts. On the other hand, lch is a score denoting how similar two concepts are, calculated by using the shortest path that connects the concept and the</context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>Claudia Leacock and Martin Chodorow. 1998. Combining local context and wordnet similarity for word sense identification. WordNet: An electronic lexical database, 49(2):265–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram cooccurrence statistics.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume</booktitle>
<volume>1</volume>
<pages>71--78</pages>
<contexts>
<context position="1083" citStr="Lin and Hovy, 2003" startWordPosition="148" endWordPosition="151">words or their substitutes. The co-occurrence modeling step provides dense, low-dimensional embedding for each fragment which allows us to calculate semantic similarity using various similarity metrics. Although our current model avoids the syntactic information, we achieved promising results and outperformed all baselines. 1 Introduction Semantic similarity is a measure that specifies the similarity of one text’s meaning to another’s. Semantic similarity plays an important role in various Natural Language Processing (NLP) tasks such as textual entailment (Berant et al., 2012), summarization (Lin and Hovy, 2003), question answering (Surdeanu et al., 2011), text classification (Sebastiani, 2002), word sense disambiguation (Sch¨utze, 1998) and information retrieval (Park et al., 2005). There are three main approaches to computing the semantic similarity between two text fragments. The first approach uses Vector Space Models (see Turney &amp; Pantel (2010) for an overview) where each text is represented as a bag-of-word model. The similarity between two text fragments can then be computed with various metrics such as cosine similarity. Sparseness in the input nature is the key problem for these models. Ther</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2003. Automatic evaluation of summaries using n-gram cooccurrence statistics. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pages 71–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In ICML,</booktitle>
<volume>98</volume>
<pages>296--304</pages>
<contexts>
<context position="10470" citStr="Lin, 1998" startWordPosition="1607" endWordPosition="1608">rson correlation tests the degree of similarity between the system’s similarity ratings and the gold standard ratings. Spearman’s rank correlation measures the degree of similarity between two rankings; similarity ratings provided by a system and the gold standard ratings. Tables 3 and 4 show the scores for Paragraph-2- Sentence and Sentence-2-Phrase subtasks on the training data, respectively. These tables contain the best individual scores for the performance metrics, Normalized Longest Common Substring (LCS) baseline, which was given by task organizers, and three additional baselines: lin (Lin, 1998), lch (Leacock and Chodorow, 1998), and the Jaccard Index (JI) baseline. lin uses the information content (Resnik, 1995) of the least common subsumer of concepts A and B. Information content (IC) indicates the specificity of a concept; the least common subsumer of a concept A and B is the most specific concept from which A and B are inherited. lin similarity5 returns the difference between two times of the IC of the least common subsumer of A and B, and the sum of IC of both concepts. On the other hand, lch is a score denoting how similar two concepts are, calculated by using the shortest path</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An information-theoretic definition of similarity. In ICML, volume 98, pages 296– 304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yariv Maron</author>
<author>Michael Lamar</author>
<author>Elie Bienenstock</author>
</authors>
<title>Sphere Embedding: An Application to Partof-Speech Induction. In</title>
<date>2010</date>
<booktitle>Advances in Neural Information Processing Systems 23,</booktitle>
<pages>1567--1575</pages>
<editor>J Lafferty, C K I Williams, J Shawe-Taylor, R S Zemel, and A Culotta, editors,</editor>
<contexts>
<context position="6776" citStr="Maron et al. (2010)" startWordPosition="1020" endWordPosition="1023">billion word web-gathered corpus. Since S-CODE algorithm works with discrete input, for each context we sample 100 substitute words with replacement using their probabilities. Table 2 illustrates the context and substitutes of each context using a bigram language model. No lemmatization, stop-word removal and lowercase transformation were performed. 2.2 Co-Occurrence Modeling This subsection will explain the unsupervised method we employed to model co-occurrence statistics: the Co-occurrence data Embedding (CODE) method (Globerson et al., 2007) and its spherical extension (S-CODE) proposed by Maron et al. (2010). Unlike in our WSI work, where we ended up with an embedding for each word in the co-occurrence modeling step in this task, we model each text unit such as a paragraph, a sentence or a phrase, to obtain embeddings for each instance. Input data for S-CODE algorithm consist of instanceid and each word in the text unit for the first system (Table 1 illustrates the pairs for only one text fragment) instance-ids and 100 substitute samples of each word in text for the second system. In the initial step, S-CODE puts all instance-ids and words (or substitutes, depending on the system) randomly on an </context>
</contexts>
<marker>Maron, Lamar, Bienenstock, 2010</marker>
<rawString>Yariv Maron, Michael Lamar, and Elie Bienenstock. 2010. Sphere Embedding: An Application to Partof-Speech Induction. In J Lafferty, C K I Williams, J Shawe-Taylor, R S Zemel, and A Culotta, editors, Advances in Neural Information Processing Systems 23, pages 1567–1575.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
<author>Christiane Fellbaum</author>
</authors>
<title>Wordnet: An electronic lexical database.</title>
<date>1998</date>
<contexts>
<context position="12548" citStr="Miller and Fellbaum, 1998" startWordPosition="1954" endWordPosition="1957">-KU systems specify the run number. System Pearson Spearman Sentence-2-Phrase Best 0.777 0.642 2nd Best 0.771 0.760 3rd Best 0.760 0.757 AI-KU, 0.680 0.646 AI-KU2 0.617 0.612 LCS 0.562 0.626 lch 0.526 0.544 lin 0.501 0.498 JI 0.540 0.555 Table 6: Sentence2phrase subtask scores for the test data. graph2Sentence subtask, since smaller textual units (such as phrases) make the problem more difficult. ford Part-of-Speech Tagger (Toutanova and Manning, 2000) we tagged words across all textual levels. After tagging, we found the synsets of each word matched with its part-of-speech using WordNet 3.0 (Miller and Fellbaum, 1998). For each synset of a word in the shorter textual unit (e.g., sentence is shorter than paragraph), we calculated the lin/lch measure of each synset of all words in the longer textual unit and picked the highest score. When we found the scores for all words, we calculated the mean to find out the similarity between one pair in the test set. Finally, Jaccard Index baseline was used to simply calculate the number of words in common (intersection) with two cross textual levels, normalized by the total number of words (union). Table 5 and 6 demonstrate the AI-KU runs on the test data. Next, we pre</context>
</contexts>
<marker>Miller, Fellbaum, 1998</marker>
<rawString>George Miller and Christiane Fellbaum. 1998. Wordnet: An electronic lexical database.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eui-Kyu Park</author>
<author>Dong-Yul Ra</author>
<author>Myung-Gil Jang</author>
</authors>
<title>Techniques for improving web retrieval effectiveness.</title>
<date>2005</date>
<booktitle>Information processing &amp; management,</booktitle>
<pages>41--5</pages>
<contexts>
<context position="1257" citStr="Park et al., 2005" startWordPosition="171" endWordPosition="174">various similarity metrics. Although our current model avoids the syntactic information, we achieved promising results and outperformed all baselines. 1 Introduction Semantic similarity is a measure that specifies the similarity of one text’s meaning to another’s. Semantic similarity plays an important role in various Natural Language Processing (NLP) tasks such as textual entailment (Berant et al., 2012), summarization (Lin and Hovy, 2003), question answering (Surdeanu et al., 2011), text classification (Sebastiani, 2002), word sense disambiguation (Sch¨utze, 1998) and information retrieval (Park et al., 2005). There are three main approaches to computing the semantic similarity between two text fragments. The first approach uses Vector Space Models (see Turney &amp; Pantel (2010) for an overview) where each text is represented as a bag-of-word model. The similarity between two text fragments can then be computed with various metrics such as cosine similarity. Sparseness in the input nature is the key problem for these models. Therefore, later works such as Latent Semantic Indexing (?) and This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings</context>
</contexts>
<marker>Park, Ra, Jang, 2005</marker>
<rawString>Eui-Kyu Park, Dong-Yul Ra, and Myung-Gil Jang. 2005. Techniques for improving web retrieval effectiveness. Information processing &amp; management, 41(5):1207–1223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>Wordnet:: Similarity: measuring the relatedness of concepts. In Demonstration Papers at HLT-NAACL</title>
<date>2004</date>
<pages>38--41</pages>
<contexts>
<context position="11198" citStr="Pedersen et al. (2004)" startWordPosition="1739" endWordPosition="1742">(Resnik, 1995) of the least common subsumer of concepts A and B. Information content (IC) indicates the specificity of a concept; the least common subsumer of a concept A and B is the most specific concept from which A and B are inherited. lin similarity5 returns the difference between two times of the IC of the least common subsumer of A and B, and the sum of IC of both concepts. On the other hand, lch is a score denoting how similar two concepts are, calculated by using the shortest path that connects the concept and the maximum depth of the taxonomy in which the concepts occur6 (please see Pedersen et al. (2004) for further details of these measures). These two baselines were calculated as follows. First, using the Stan5lin similarity = 2 ∗ IC(lcs)/(IC(A) + IC(B)) where lcs indicates the least common subsumer of concepts A and B. 6The exact formulation is −log(L/2d) where L is the shortest path length and d is the taxonomy depth. 94 System Pearson Spearman Paragraph-2-Sentence Best 0.837 0.821 2nd Best 0.834 0.820 3rd Best 0.826 0.817 AI-KU, 0.732 0.727 AI-KU2 0.698 0.700 LCS 0.527 0.613 lch 0.629 0.627 lin 0.612 0.601 JI 0.640 0.687 Table 5: Paragraph-2-Sentence subtask scores for the test data. Bes</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. Wordnet:: Similarity: measuring the relatedness of concepts. In Demonstration Papers at HLT-NAACL 2004, pages 38–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammad Taher Pilehvar</author>
<author>David Jurgens</author>
<author>Roberto Navigli</author>
</authors>
<title>Align, disambiguate and walk: A unified approach for measuring semantic similarity.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="2303" citStr="Pilehvar et al., 2013" startWordPosition="327" endWordPosition="330">refore, later works such as Latent Semantic Indexing (?) and This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ Topic Models (Blei et al., 2003) overcome sparsity problems via reducing the dimensionality of the model by introducing latent variables. The second approach blends various lexical and syntactic features and attacks the problem through machine learning models. The third approach is based on word-to-word similarity alignment (Pilehvar et al., 2013; Islam and Inkpen, 2008). The Cross-Level Semantic Similarity (CLSS) task in SemEval 20141 (Jurgens et al., 2014) provides an evaluation framework to assess similarity methods for texts in different volumes (i.e., lexical levels). Unlike previous SemEval and *SEM tasks that were interested in comparing texts with similar volume, this task consists of four subtasks (paragraph2sentence, sentence2phrase, phrase2word and word2sense) that investigate the performance of systems based on pairs of texts of different sizes. A system should report the similarity score of a given pair, ranging from 4 (t</context>
</contexts>
<marker>Pilehvar, Jurgens, Navigli, 2013</marker>
<rawString>Mohammad Taher Pilehvar, David Jurgens, and Roberto Navigli. 2013. Align, disambiguate and walk: A unified approach for measuring semantic similarity. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy. arXiv preprint cmp-lg/9511007.</title>
<date>1995</date>
<contexts>
<context position="10590" citStr="Resnik, 1995" startWordPosition="1627" endWordPosition="1628">. Spearman’s rank correlation measures the degree of similarity between two rankings; similarity ratings provided by a system and the gold standard ratings. Tables 3 and 4 show the scores for Paragraph-2- Sentence and Sentence-2-Phrase subtasks on the training data, respectively. These tables contain the best individual scores for the performance metrics, Normalized Longest Common Substring (LCS) baseline, which was given by task organizers, and three additional baselines: lin (Lin, 1998), lch (Leacock and Chodorow, 1998), and the Jaccard Index (JI) baseline. lin uses the information content (Resnik, 1995) of the least common subsumer of concepts A and B. Information content (IC) indicates the specificity of a concept; the least common subsumer of a concept A and B is the most specific concept from which A and B are inherited. lin similarity5 returns the difference between two times of the IC of the least common subsumer of A and B, and the sum of IC of both concepts. On the other hand, lch is a score denoting how similar two concepts are, calculated by using the shortest path that connects the concept and the maximum depth of the taxonomy in which the concepts occur6 (please see Pedersen et al</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. arXiv preprint cmp-lg/9511007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<pages>123</pages>
<marker>Sch¨utze, 1998</marker>
<rawString>Hinrich Sch¨utze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97– 123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Machine learning in automated text categorization.</title>
<date>2002</date>
<journal>ACM computing surveys (CSUR),</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="1167" citStr="Sebastiani, 2002" startWordPosition="160" endWordPosition="162">ional embedding for each fragment which allows us to calculate semantic similarity using various similarity metrics. Although our current model avoids the syntactic information, we achieved promising results and outperformed all baselines. 1 Introduction Semantic similarity is a measure that specifies the similarity of one text’s meaning to another’s. Semantic similarity plays an important role in various Natural Language Processing (NLP) tasks such as textual entailment (Berant et al., 2012), summarization (Lin and Hovy, 2003), question answering (Surdeanu et al., 2011), text classification (Sebastiani, 2002), word sense disambiguation (Sch¨utze, 1998) and information retrieval (Park et al., 2005). There are three main approaches to computing the semantic similarity between two text fragments. The first approach uses Vector Space Models (see Turney &amp; Pantel (2010) for an overview) where each text is represented as a bag-of-word model. The similarity between two text fragments can then be computed with various metrics such as cosine similarity. Sparseness in the input nature is the key problem for these models. Therefore, later works such as Latent Semantic Indexing (?) and This work is licensed un</context>
</contexts>
<marker>Sebastiani, 2002</marker>
<rawString>Fabrizio Sebastiani. 2002. Machine learning in automated text categorization. ACM computing surveys (CSUR), 34(1):1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Massimiliano Ciaramita</author>
<author>Hugo Zaragoza</author>
</authors>
<title>Learning to rank answers to nonfactoid questions from web collections.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>2</issue>
<contexts>
<context position="1127" citStr="Surdeanu et al., 2011" startWordPosition="154" endWordPosition="157">ence modeling step provides dense, low-dimensional embedding for each fragment which allows us to calculate semantic similarity using various similarity metrics. Although our current model avoids the syntactic information, we achieved promising results and outperformed all baselines. 1 Introduction Semantic similarity is a measure that specifies the similarity of one text’s meaning to another’s. Semantic similarity plays an important role in various Natural Language Processing (NLP) tasks such as textual entailment (Berant et al., 2012), summarization (Lin and Hovy, 2003), question answering (Surdeanu et al., 2011), text classification (Sebastiani, 2002), word sense disambiguation (Sch¨utze, 1998) and information retrieval (Park et al., 2005). There are three main approaches to computing the semantic similarity between two text fragments. The first approach uses Vector Space Models (see Turney &amp; Pantel (2010) for an overview) where each text is represented as a bag-of-word model. The similarity between two text fragments can then be computed with various metrics such as cosine similarity. Sparseness in the input nature is the key problem for these models. Therefore, later works such as Latent Semantic I</context>
</contexts>
<marker>Surdeanu, Ciaramita, Zaragoza, 2011</marker>
<rawString>Mihai Surdeanu, Massimiliano Ciaramita, and Hugo Zaragoza. 2011. Learning to rank answers to nonfactoid questions from web collections. Computational Linguistics, 37(2):351–383.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Christopher D Manning</author>
</authors>
<title>Enriching the knowledge sources used in a maximum entropy part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2000 Joint SIGDAT conference on Empirical methods in natural language processing and very large corpora: held in conjunction with the 38th Annual Meeting of the Association for Computational Linguistics-Volume 13,</booktitle>
<pages>63--70</pages>
<contexts>
<context position="12378" citStr="Toutanova and Manning, 2000" startWordPosition="1924" endWordPosition="1928">2-Sentence subtask scores for the test data. Best indicates the best correlation score for the subtask. LCS stands for Normalized Longest Common Substring. Subscripts in AI-KU systems specify the run number. System Pearson Spearman Sentence-2-Phrase Best 0.777 0.642 2nd Best 0.771 0.760 3rd Best 0.760 0.757 AI-KU, 0.680 0.646 AI-KU2 0.617 0.612 LCS 0.562 0.626 lch 0.526 0.544 lin 0.501 0.498 JI 0.540 0.555 Table 6: Sentence2phrase subtask scores for the test data. graph2Sentence subtask, since smaller textual units (such as phrases) make the problem more difficult. ford Part-of-Speech Tagger (Toutanova and Manning, 2000) we tagged words across all textual levels. After tagging, we found the synsets of each word matched with its part-of-speech using WordNet 3.0 (Miller and Fellbaum, 1998). For each synset of a word in the shorter textual unit (e.g., sentence is shorter than paragraph), we calculated the lin/lch measure of each synset of all words in the longer textual unit and picked the highest score. When we found the scores for all words, we calculated the mean to find out the similarity between one pair in the test set. Finally, Jaccard Index baseline was used to simply calculate the number of words in com</context>
</contexts>
<marker>Toutanova, Manning, 2000</marker>
<rawString>Kristina Toutanova and Christopher D Manning. 2000. Enriching the knowledge sources used in a maximum entropy part-of-speech tagger. In Proceedings of the 2000 Joint SIGDAT conference on Empirical methods in natural language processing and very large corpora: held in conjunction with the 38th Annual Meeting of the Association for Computational Linguistics-Volume 13, pages 63–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From Frequency to Meaning: Vector Space Models of Semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>37--141</pages>
<contexts>
<context position="1427" citStr="Turney &amp; Pantel (2010)" startWordPosition="197" endWordPosition="200">Semantic similarity is a measure that specifies the similarity of one text’s meaning to another’s. Semantic similarity plays an important role in various Natural Language Processing (NLP) tasks such as textual entailment (Berant et al., 2012), summarization (Lin and Hovy, 2003), question answering (Surdeanu et al., 2011), text classification (Sebastiani, 2002), word sense disambiguation (Sch¨utze, 1998) and information retrieval (Park et al., 2005). There are three main approaches to computing the semantic similarity between two text fragments. The first approach uses Vector Space Models (see Turney &amp; Pantel (2010) for an overview) where each text is represented as a bag-of-word model. The similarity between two text fragments can then be computed with various metrics such as cosine similarity. Sparseness in the input nature is the key problem for these models. Therefore, later works such as Latent Semantic Indexing (?) and This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ Topic Models (Blei et al., 2003) overcome sparsity problems via reducing</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From Frequency to Meaning: Vector Space Models of Semantics. Journal of Artificial Intelligence Research, 37:141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehmet Ali Yatbaz</author>
<author>Enis Sert</author>
<author>Deniz Yuret</author>
</authors>
<title>Learning syntactic categories using paradigmatic representations of word context.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>940--951</pages>
<contexts>
<context position="5972" citStr="Yatbaz et al., 2012" startWordPosition="899" endWordPosition="902"> to buy with a $35 gift card is a hard decision.” Note that the input that we used to model cooccurrence statistics consists of all such pairs for each fragment in a given subtask. 2The code to replicate our work can be found at https://github.com/osmanbaskaya/ semeval14-task3. 3Lemmatization is carried out with Stanford CoreNLP and transforms a word into its canonical or base form. AI-KU2: Previously, the utilization of high probability substitutes and their co-occurrence statistics achieved notable performance on Word Sense Induction (WSI) (Baskaya et al., 2013) and Partof-Speech Induction (Yatbaz et al., 2012) problems. AI-KU2 represents each context of a word by finding the most likely 100 substitutes suggested by the 4-gram language model we built from ukWaC4 (Ferraresi et al., 2008), a 2-billion word web-gathered corpus. Since S-CODE algorithm works with discrete input, for each context we sample 100 substitute words with replacement using their probabilities. Table 2 illustrates the context and substitutes of each context using a bigram language model. No lemmatization, stop-word removal and lowercase transformation were performed. 2.2 Co-Occurrence Modeling This subsection will explain the uns</context>
</contexts>
<marker>Yatbaz, Sert, Yuret, 2012</marker>
<rawString>Mehmet Ali Yatbaz, Enis Sert, and Deniz Yuret. 2012. Learning syntactic categories using paradigmatic representations of word context. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 940–951.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>