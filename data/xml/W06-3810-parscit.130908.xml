<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.019694">
<title confidence="0.999366">
Graph-based Generalized Latent Semantic Analysis
for Document Representation
</title>
<author confidence="0.997186">
Irina Matveeva
</author>
<affiliation confidence="0.998233">
Dept. of Computer Science
University of Chicago
</affiliation>
<address confidence="0.8089">
Chicago, IL 60637
</address>
<email confidence="0.998928">
matveeva@cs.uchicago.edu
</email>
<author confidence="0.984207">
Gina-Anne Levow
</author>
<affiliation confidence="0.9981615">
Dept. of Computer Science
University of Chicago
</affiliation>
<address confidence="0.807952">
Chicago, IL 60637
</address>
<email confidence="0.99727">
levow@cs.uchicago.edu
</email>
<sectionHeader confidence="0.9956" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999766444444445">
Document indexing and representation of
term-document relations are very impor-
tant for document clustering and retrieval.
In this paper, we combine a graph-based
dimensionality reduction method with a
corpus-based association measure within
the Generalized Latent Semantic Analysis
framework. We evaluate the graph-based
GLSA on the document clustering task.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99978041025641">
Document indexing and representation of term-
document relations are very important issues for
document clustering and retrieval. Although the
vocabulary space is very large, content bearing
words are often combined into semantic classes that
contain synonyms and semantically related words.
Hence there has been a considerable interest in low-
dimensional term and document representations.
Latent Semantic Analysis (LSA) (Deerwester et
al., 1990) is one of the best known dimensionality
reduction algorithms. The dimensions of the LSA
vector space can be interpreted as latent semantic
concepts. The cosine similarity between the LSA
document vectors corresponds to documents’ sim-
ilarity in the input space. LSA preserves the docu-
ments similarities which are based on the inner prod-
ucts of the input bag-of-word documents and it pre-
serves these similarities globally.
More recently, a number of graph-based dimen-
sionality reduction techniques were successfully ap-
plied to document clustering and retrieval (Belkin
and Niyogi, 2003; He et al., 2004). The main ad-
vantage of the graph-based approaches over LSA is
the notion of locality. Laplacian Eigenmaps Embed-
ding (Belkin and Niyogi, 2003) and Locality Pre-
serving Indexing (LPI) (He et al., 2004) discover the
local structure of the term and document space and
compute a semantic subspace with a stronger dis-
criminative power. Laplacian Eigenmaps Embed-
ding and LPI preserve the input similarities only
locally, because this information is most reliable.
Laplacian Eigenmaps Embedding does not provide
a fold-in procedure for unseen documents. LPI
is a linear approximation to Laplacian Eigenmaps
Embedding that eliminates this problem. Similar
to LSA, the input similarities to LPI are based on
the inner products of the bag-of-word documents.
Laplacian Eigenmaps Embedding can use any kind
of similarity in the original space.
</bodyText>
<listItem confidence="0.635543875">
Generalized Latent Semantic Analysis
(GLSA) (Matveeva et al., 2005) is a frame-
work for computing semantically motivated term
and document vectors. It extends the LSA approach
by focusing on term vectors instead of the dual
document-term representation. GLSA requires a
measure of semantic association between terms and
a method of dimensionality reduction.
</listItem>
<bodyText confidence="0.9996735">
In this paper, we use GLSA with point-wise mu-
tual information as a term association measure. We
introduce the notion of locality into this framework
and propose to use Laplacian Eigenmaps Embed-
ding as a dimensionality reduction algorithm. We
evaluate the importance of locality for document
representation in document clustering experiments.
The rest of the paper is organized as follows. Sec-
</bodyText>
<page confidence="0.989886">
61
</page>
<bodyText confidence="0.560012">
Workshop on TextGraphs, at HLT-NAACL 2006, pages 61–64,
</bodyText>
<subsectionHeader confidence="0.606907">
New York City, June 2006. c�2006 Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.994846666666667">
tion 2 contains the outline of the graph-based GLSA
algorithm. Section 3 presents our experiments, fol-
lowed by conclusion in section 4.
</bodyText>
<sectionHeader confidence="0.993999" genericHeader="method">
2 Graph-based GLSA
</sectionHeader>
<subsectionHeader confidence="0.766819">
2.1 GLSA Framework
</subsectionHeader>
<bodyText confidence="0.998648">
The GLSA algorithm (Matveeva et al., 2005) has the
following setup. The input is a document collection
C with vocabulary V and a large corpus W.
</bodyText>
<listItem confidence="0.982581875">
1. For the vocabulary in V , obtain a matrix of
pair-wise similarities, S, using the corpus W
2. Obtain the matrix UT of a low dimensional
vector space representation of terms that pre-
serves the similarities in S, UT E Rk×|V |
3. Construct the term document matrix D for C
4. Compute document vectors by taking linear
combinations of term vectors Dˆ = UT D
</listItem>
<bodyText confidence="0.999814384615385">
The columns of Dˆ are documents in the k-
dimensional space.
GLSA approach can combine any kind of simi-
larity measure on the space of terms with any suit-
able method of dimensionality reduction. The inner
product between the term and document vectors in
the GLSA space preserves the semantic association
in the input space. The traditional term-document
matrix is used in the last step to provide the weights
in the linear combination of term vectors. LSA is
a special case of GLSA that uses inner product in
step 1 and singular value decomposition in step 2,
see (Bartell et al., 1992).
</bodyText>
<subsectionHeader confidence="0.997496">
2.2 Singular Value Decomposition
</subsectionHeader>
<bodyText confidence="0.999974071428572">
Given any matrix S, its singular value decompo-
sition (SVD) is S = UEVT . The matrix Sk =
UEkVT is obtained by setting all but the first k di-
agonal elements in E to zero. If S is symmetric, as
in the GLSA case, U = V and Sk = UEkUT. The
inner product between the GLSA term vectors com-
puted as UE1/2
k optimally preserves the similarities
in S wrt square loss.
The basic GLSA computes the SVD of S and uses
k eigenvectors corresponding to the largest eigenval-
ues as a representation for term vectors. We will re-
fer to this approach as GLSA. As for LSA, the simi-
larities are preserved globally.
</bodyText>
<subsectionHeader confidence="0.998608">
2.3 Laplacian Eigenmaps Embedding
</subsectionHeader>
<bodyText confidence="0.9994905">
We used the Laplacian Embedding algo-
rithm (Belkin and Niyogi, 2003) in step 2 of
the GLSA algorithm to compute low-dimensional
term vectors. Laplacian Eigenmaps Embedding
preserves the similarities in S only locally since
local information is often more reliable. We will
refer to this variant of GLSA as GLSAL.
The Laplacian Eigenmaps Embedding algorithm
computes the low dimensional vectors y to minimize
under certain constraints
</bodyText>
<equation confidence="0.705435">
� ||yi − yj||2Wij.
ij
</equation>
<bodyText confidence="0.999399777777778">
W is the weight matrix based on the graph adjacency
matrix. Wij is large if terms i and j are similar ac-
cording to S. Wij can be interpreted as the penalty
of mapping similar terms far apart in the Laplacian
Embedding space, see (Belkin and Niyogi, 2003)
for details. In our experiments we used a binary ad-
jacency matrix W. Wij = 1 if terms i and j are
among the k nearest neighbors of each other and is
zero otherwise.
</bodyText>
<subsectionHeader confidence="0.999419">
2.4 Measure of Semantic Association
</subsectionHeader>
<bodyText confidence="0.9998686">
Following (Matveeva et al., 2005), we primarily
used point-wise mutual information (PMI) as a mea-
sures of semantic association in step 1 of GLSA.
PMI between random variables representing two
words, w1 and w2, is computed as
</bodyText>
<equation confidence="0.999966666666667">
P(W1 = 1, W2 = 1)
PMI(w1, w2) = log
P(W1 = 1)P(W2 = 1).
</equation>
<subsectionHeader confidence="0.922688">
2.5 GLSA Space
</subsectionHeader>
<bodyText confidence="0.999697538461538">
GLSA offers a greater flexibility in exploring the
notion of semantic relatedness between terms. In
our preliminary experiments, we obtained the matrix
of semantic associations in step 1 of GLSA using
point-wise mutual information (PMI), likelihood ra-
tio and χ2 test. Although PMI showed the best per-
formance, other measures are particularly interest-
ing in combination with the Laplacian Embedding.
Related approaches, such as LSA, the Word Space
Model (WS) (Sch¨utze, 1998) and Latent Relational
Analysis (LRA) (Turney, 2004) are limited to only
one measure of semantic association and preserve
the similarities globally.
</bodyText>
<page confidence="0.996321">
62
</page>
<bodyText confidence="0.997563882352941">
Assuming that the vocabulary space has some un-
derlying low dimensional semantic manifold. Lapla-
cian Embedding algorithm tries to approximate this
manifold by relying only on the local similarity in-
formation. It uses the nearest neighbors graph con-
structed using the pair-wise term similarities. The
computations of the Laplacian Embedding uses the
graph adjacency matrix W. This matrix can be bi-
nary or use weighted similarities. The advantage
of the binary adjacency matrix is that it conveys
the neighborhood information without relying on in-
dividual similarity values. It is important for co-
occurrence based similarity measures, see discus-
sion in (Manning and Sch¨utze, 1999).
The Locality Preserving Indexing (He et al.,
2004) has a similar notion of locality but has to use
bag-of-words document vectors.
</bodyText>
<sectionHeader confidence="0.996161" genericHeader="method">
3 Document Clustering Experiments
</sectionHeader>
<bodyText confidence="0.99989536">
We conducted a document clustering experiment for
the Reuters-21578 collection. To collect the co-
occurrence statistics for the similarities matrix S
we used a subset of the English Gigaword collec-
tion (LDC), containing New York Times articles la-
beled as “story”. We had 1,119,364 documents with
771,451 terms. We used the Lemur toolkit1 to tok-
enize and index all document collections used in our
experiments, with stemming and a list of stop words.
Since Locality Preserving Indexing algorithm
(LPI) is most related to the graph-based GLSAL, we
ran experiments similar to those reported in (He et
al., 2004). We computed the GLSA document vec-
tors for the 20 largest categories from the Reuters-
21578 document collection. We had 8564 docu-
ments and 7173 terms. We used the same list of 30
TREC words as in (He et al., 2004) which are listed
in table 12. For each word on this list, we generated
a cluster as a subset of Reuters documents that con-
tained this word. Clusters are not disjoint and con-
tain documents from different Reuters categories.
We computed GLSA, GLSAL, LSA and LPI rep-
resentations. We report the results for k = 5 for
the k nearest neighbors graph for LPI and Laplacian
Embedding, and binary weights for the adjacency
</bodyText>
<footnote confidence="0.9362805">
1http://www.lemurproject.org/
2We used 28 words because we used stemming whereas (He
et al., 2004) did not, so that in two cases, two words were re-
duces to the same stem.
</footnote>
<bodyText confidence="0.990636857142857">
matrix. We report results for 300 embedding dimen-
sions for GLSA, LPI and LSA and 500 dimensions
for GLSAL.
We evaluate these representations in terms of how
well the cosine similarity between the document
vectors within each cluster corresponds to the true
semantic similarity. We expect documents from the
same Reuters category to have higher similarity.
For each cluster we computed all pair-wise doc-
ument similarities. All pair-wise similarities were
sorted in decreasing order. The term “inter-pair” de-
scribes a pair of documents that have the same label.
For the kth inter-pair, we computed precision at k as:
#inter − pairs pj, s.t. j &lt; k
</bodyText>
<equation confidence="0.939632">
precision(pk) = ,
k
</equation>
<bodyText confidence="0.9998663">
where pj refers to the jth inter-pair. The average
of the precision values for each of the inter-pairs was
used as the average precision for the particular doc-
ument cluster.
Table 1 summarizes the results. The first column
shows the words according to which document clus-
ters were generated and the entropy of the category
distribution within that cluster. The baseline was to
use the tf document vectors. We report results for
GLSA, GLSAL, LSA and LPI. The LSA and LPI
computations were based solely on the Reuters col-
lection. For GLSA and GLSALwe used the term as-
sociations computed for the Gigaword collection, as
described above. Therefore, the similarities that are
preserved are quite different. For LSA and LPI they
reflect the term distribution specific for the Reuters
collection whereas for GLSA they are more general.
By paired 2-tailed t-test, at p ≤ 0.05, GLSA outper-
formed all other approaches. There was no signifi-
cant difference in performance of GLSAL, LSA and
the baseline. Disappointingly, we could not achieve
good performance with LPI. Its performance varies
over clusters similar to that of other approaches but
the average is significantly lower. We would like
to stress that the comparison of our results to those
presented in (He et al., 2004) are only suggestive
since (He et al., 2004) applied LPI to each cluster
separately and used PCA as preprocessing. We com-
puted the LPI representation for the full collection
and did not use PCA.
</bodyText>
<page confidence="0.997926">
63
</page>
<table confidence="0.999876833333333">
word tf glsa glsaL lsa lpi
agreement(1) 0.74 0.73 0.73 0.75 0.46
american(0.8) 0.63 0.72 0.59 0.64 0.36
bank(1.4) 0.45 0.52 0.40 0.48 0.28
control(0.7) 0.78 0.82 0.80 0.80 0.58
domestic(0.8) 0.64 0.68 0.66 0.68 0.35
export(0.8) 0.64 0.65 0.70 0.67 0.37
five(1.3) 0.74 0.77 0.71 0.70 0.40
foreign(1.2) 0.51 0.58 0.55 0.56 0.28
growth(1) 0.51 0.58 0.48 0.54 0.32
income(0.5) 0.84 0.86 0.83 0.80 0.69
increase(1.3) 0.51 0.61 0.53 0.53 0.29
industrial(1.2) 0.59 0.66 0.58 0.61 0.34
internat.(1.1) 0.58 0.59 0.54 0.61 0.34
investment(1) 0.68 0.77 0.70 0.72 0.46
loss(0.3) 0.98 0.99 0.98 0.98 0.88
money(1.1) 0.70 0.62 0.71 0.65 0.38
national(1.3) 0.49 0.58 0.49 0.55 0.27
price(1.2) 0.53 0.63 0.57 0.57 0.29
production(1) 0.56 0.66 0.58 0.59 0.29
public(1.2) 0.58 0.60 0.57 0.57 0.31
rate(1.1) 0.61 0.62 0.64 0.60 0.35
report(1.2) 0.66 0.72 0.62 0.65 0.35
service(0.9) 0.59 0.66 0.56 0.61 0.39
source(1.2) 0.56 0.54 0.59 0.60 0.27
talk(0.9) 0.74 0.67 0.73 0.74 0.39
tax(0.7) 0.91 0.93 0.90 0.89 0.67
trade(1) 0.85 0.74 0.82 0.60 0.33
world(1.1) 0.63 0.65 0.68 0.66 0.33
Av. Acc 0.65 0.68 0.65 0.66 0.40
</table>
<tableCaption confidence="0.99985">
Table 1: Average inter-pairs accuracy.
</tableCaption>
<bodyText confidence="0.99977">
The inter-pair accuracy depended on the cate-
gories distribution within clusters. For more homo-
geneous clusters, e.g. “loss”, all methods (except
LPI) achieve similar precision. For less homoge-
neous clusters, e.g. “national”, ”industrial”, ”bank”,
GLSA and LSA outperformed the tf document vec-
tors more significantly.
</bodyText>
<sectionHeader confidence="0.997414" genericHeader="conclusions">
4 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.99998445">
We introduced a graph-based method of dimension-
ality reduction into the GLSA framework. Lapla-
cian Eigenmaps Embedding preserves the similar-
ities only locally, thus providing a potentially bet-
ter approximation to the low dimensional semantic
space. We explored the role of locality in the GLSA
representation and used binary adjacency matrix as
similarity which was preserved and compared it to
GLSA with unnormalized PMI scores.
Our results did not show an advantage of GLSAL.
GLSAL and LPI seem to be very sensitive to the pa-
rameters of the neighborhood graph. We tried dif-
ferent parameter settings but more experiments are
required for a thorough analysis. We are also plan-
ning to use a different document collection to elimi-
nate the possible effect of the specific term distribu-
tion in the Reuters collection. Further experiments
are needed to make conclusions about the geometry
of the vocabulary space and the appropriateness of
these methods for term and document embedding.
</bodyText>
<sectionHeader confidence="0.999437" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999854464285714">
Brian T. Bartell, Garrison W. Cottrell, and Richard K.
Belew. 1992. Latent semantic indexing is an optimal
special case of multidimensional scaling. In Proc. of
the 15th ACM SIGIR, pages 161–167. ACM Press.
Mikhail Belkin and Partha Niyogi. 2003. Laplacian
eigenmaps for dimensionality reduction and data rep-
resentation. Neural Computation, 15(6):1373–1396.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society of Information Science,
41(6):391–407.
Xiaofei He, Deng Cai, Haifeng Liu, and Wei-Ying Ma.
2004. Locality preserving indexing for document rep-
resentation. In Proc. of the 27rd ACM SIGIR, pages
96–103. ACM Press.
Chris Manning and Hinrich Sch¨utze. 1999. Founda-
tions ofStatistical Natural Language Processing. MIT
Press. Cambridge, MA.
Irina Matveeva, Gina-Anne Levow, Ayman Farahat, and
Christian Royer. 2005. Generalized latent semantic
analysis for term representation. In Proc. ofRANLP.
Hinrich Sch¨utze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(21):97–124.
Peter D. Turney. 2004. Human-level performance on
word analogy questions by latent relational analysis.
Technical report, Technical Report ERB-1118, NRC-
47422.
</reference>
<page confidence="0.999418">
64
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.178308">
<title confidence="0.9989105">Graph-based Generalized Latent Semantic for Document Representation</title>
<author confidence="0.957484">Irina</author>
<affiliation confidence="0.921403666666667">Dept. of Computer University of Chicago, IL</affiliation>
<email confidence="0.997363">matveeva@cs.uchicago.edu</email>
<author confidence="0.517166">Gina-Anne</author>
<affiliation confidence="0.9993085">Dept. of Computer University of</affiliation>
<address confidence="0.498072">Chicago, IL</address>
<email confidence="0.9992">levow@cs.uchicago.edu</email>
<abstract confidence="0.9945663">Document indexing and representation of term-document relations are very important for document clustering and retrieval. In this paper, we combine a graph-based dimensionality reduction method with a corpus-based association measure within the Generalized Latent Semantic Analysis framework. We evaluate the graph-based GLSA on the document clustering task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Brian T Bartell</author>
<author>Garrison W Cottrell</author>
<author>Richard K Belew</author>
</authors>
<title>Latent semantic indexing is an optimal special case of multidimensional scaling.</title>
<date>1992</date>
<booktitle>In Proc. of the 15th ACM SIGIR,</booktitle>
<pages>161--167</pages>
<publisher>ACM Press.</publisher>
<contexts>
<context position="4689" citStr="Bartell et al., 1992" startWordPosition="719" endWordPosition="722">ombinations of term vectors Dˆ = UT D The columns of Dˆ are documents in the kdimensional space. GLSA approach can combine any kind of similarity measure on the space of terms with any suitable method of dimensionality reduction. The inner product between the term and document vectors in the GLSA space preserves the semantic association in the input space. The traditional term-document matrix is used in the last step to provide the weights in the linear combination of term vectors. LSA is a special case of GLSA that uses inner product in step 1 and singular value decomposition in step 2, see (Bartell et al., 1992). 2.2 Singular Value Decomposition Given any matrix S, its singular value decomposition (SVD) is S = UEVT . The matrix Sk = UEkVT is obtained by setting all but the first k diagonal elements in E to zero. If S is symmetric, as in the GLSA case, U = V and Sk = UEkUT. The inner product between the GLSA term vectors computed as UE1/2 k optimally preserves the similarities in S wrt square loss. The basic GLSA computes the SVD of S and uses k eigenvectors corresponding to the largest eigenvalues as a representation for term vectors. We will refer to this approach as GLSA. As for LSA, the similariti</context>
</contexts>
<marker>Bartell, Cottrell, Belew, 1992</marker>
<rawString>Brian T. Bartell, Garrison W. Cottrell, and Richard K. Belew. 1992. Latent semantic indexing is an optimal special case of multidimensional scaling. In Proc. of the 15th ACM SIGIR, pages 161–167. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikhail Belkin</author>
<author>Partha Niyogi</author>
</authors>
<title>Laplacian eigenmaps for dimensionality reduction and data representation.</title>
<date>2003</date>
<journal>Neural Computation,</journal>
<volume>15</volume>
<issue>6</issue>
<contexts>
<context position="1698" citStr="Belkin and Niyogi, 2003" startWordPosition="232" endWordPosition="235">nalysis (LSA) (Deerwester et al., 1990) is one of the best known dimensionality reduction algorithms. The dimensions of the LSA vector space can be interpreted as latent semantic concepts. The cosine similarity between the LSA document vectors corresponds to documents’ similarity in the input space. LSA preserves the documents similarities which are based on the inner products of the input bag-of-word documents and it preserves these similarities globally. More recently, a number of graph-based dimensionality reduction techniques were successfully applied to document clustering and retrieval (Belkin and Niyogi, 2003; He et al., 2004). The main advantage of the graph-based approaches over LSA is the notion of locality. Laplacian Eigenmaps Embedding (Belkin and Niyogi, 2003) and Locality Preserving Indexing (LPI) (He et al., 2004) discover the local structure of the term and document space and compute a semantic subspace with a stronger discriminative power. Laplacian Eigenmaps Embedding and LPI preserve the input similarities only locally, because this information is most reliable. Laplacian Eigenmaps Embedding does not provide a fold-in procedure for unseen documents. LPI is a linear approximation to Lap</context>
<context position="5417" citStr="Belkin and Niyogi, 2003" startWordPosition="854" endWordPosition="857">T . The matrix Sk = UEkVT is obtained by setting all but the first k diagonal elements in E to zero. If S is symmetric, as in the GLSA case, U = V and Sk = UEkUT. The inner product between the GLSA term vectors computed as UE1/2 k optimally preserves the similarities in S wrt square loss. The basic GLSA computes the SVD of S and uses k eigenvectors corresponding to the largest eigenvalues as a representation for term vectors. We will refer to this approach as GLSA. As for LSA, the similarities are preserved globally. 2.3 Laplacian Eigenmaps Embedding We used the Laplacian Embedding algorithm (Belkin and Niyogi, 2003) in step 2 of the GLSA algorithm to compute low-dimensional term vectors. Laplacian Eigenmaps Embedding preserves the similarities in S only locally since local information is often more reliable. We will refer to this variant of GLSA as GLSAL. The Laplacian Eigenmaps Embedding algorithm computes the low dimensional vectors y to minimize under certain constraints � ||yi − yj||2Wij. ij W is the weight matrix based on the graph adjacency matrix. Wij is large if terms i and j are similar according to S. Wij can be interpreted as the penalty of mapping similar terms far apart in the Laplacian Embe</context>
</contexts>
<marker>Belkin, Niyogi, 2003</marker>
<rawString>Mikhail Belkin and Partha Niyogi. 2003. Laplacian eigenmaps for dimensionality reduction and data representation. Neural Computation, 15(6):1373–1396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott C Deerwester</author>
<author>Susan T Dumais</author>
<author>Thomas K Landauer</author>
<author>George W Furnas</author>
<author>Richard A Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society of Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="1114" citStr="Deerwester et al., 1990" startWordPosition="143" endWordPosition="146">with a corpus-based association measure within the Generalized Latent Semantic Analysis framework. We evaluate the graph-based GLSA on the document clustering task. 1 Introduction Document indexing and representation of termdocument relations are very important issues for document clustering and retrieval. Although the vocabulary space is very large, content bearing words are often combined into semantic classes that contain synonyms and semantically related words. Hence there has been a considerable interest in lowdimensional term and document representations. Latent Semantic Analysis (LSA) (Deerwester et al., 1990) is one of the best known dimensionality reduction algorithms. The dimensions of the LSA vector space can be interpreted as latent semantic concepts. The cosine similarity between the LSA document vectors corresponds to documents’ similarity in the input space. LSA preserves the documents similarities which are based on the inner products of the input bag-of-word documents and it preserves these similarities globally. More recently, a number of graph-based dimensionality reduction techniques were successfully applied to document clustering and retrieval (Belkin and Niyogi, 2003; He et al., 200</context>
</contexts>
<marker>Deerwester, Dumais, Landauer, Furnas, Harshman, 1990</marker>
<rawString>Scott C. Deerwester, Susan T. Dumais, Thomas K. Landauer, George W. Furnas, and Richard A. Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society of Information Science, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofei He</author>
<author>Deng Cai</author>
<author>Haifeng Liu</author>
<author>Wei-Ying Ma</author>
</authors>
<title>Locality preserving indexing for document representation.</title>
<date>2004</date>
<booktitle>In Proc. of the 27rd ACM SIGIR,</booktitle>
<pages>96--103</pages>
<publisher>ACM Press.</publisher>
<contexts>
<context position="1716" citStr="He et al., 2004" startWordPosition="236" endWordPosition="239"> et al., 1990) is one of the best known dimensionality reduction algorithms. The dimensions of the LSA vector space can be interpreted as latent semantic concepts. The cosine similarity between the LSA document vectors corresponds to documents’ similarity in the input space. LSA preserves the documents similarities which are based on the inner products of the input bag-of-word documents and it preserves these similarities globally. More recently, a number of graph-based dimensionality reduction techniques were successfully applied to document clustering and retrieval (Belkin and Niyogi, 2003; He et al., 2004). The main advantage of the graph-based approaches over LSA is the notion of locality. Laplacian Eigenmaps Embedding (Belkin and Niyogi, 2003) and Locality Preserving Indexing (LPI) (He et al., 2004) discover the local structure of the term and document space and compute a semantic subspace with a stronger discriminative power. Laplacian Eigenmaps Embedding and LPI preserve the input similarities only locally, because this information is most reliable. Laplacian Eigenmaps Embedding does not provide a fold-in procedure for unseen documents. LPI is a linear approximation to Laplacian Eigenmaps E</context>
<context position="7913" citStr="He et al., 2004" startWordPosition="1262" endWordPosition="1265">rithm tries to approximate this manifold by relying only on the local similarity information. It uses the nearest neighbors graph constructed using the pair-wise term similarities. The computations of the Laplacian Embedding uses the graph adjacency matrix W. This matrix can be binary or use weighted similarities. The advantage of the binary adjacency matrix is that it conveys the neighborhood information without relying on individual similarity values. It is important for cooccurrence based similarity measures, see discussion in (Manning and Sch¨utze, 1999). The Locality Preserving Indexing (He et al., 2004) has a similar notion of locality but has to use bag-of-words document vectors. 3 Document Clustering Experiments We conducted a document clustering experiment for the Reuters-21578 collection. To collect the cooccurrence statistics for the similarities matrix S we used a subset of the English Gigaword collection (LDC), containing New York Times articles labeled as “story”. We had 1,119,364 documents with 771,451 terms. We used the Lemur toolkit1 to tokenize and index all document collections used in our experiments, with stemming and a list of stop words. Since Locality Preserving Indexing al</context>
<context position="9359" citStr="He et al., 2004" startWordPosition="1505" endWordPosition="1508">on. We had 8564 documents and 7173 terms. We used the same list of 30 TREC words as in (He et al., 2004) which are listed in table 12. For each word on this list, we generated a cluster as a subset of Reuters documents that contained this word. Clusters are not disjoint and contain documents from different Reuters categories. We computed GLSA, GLSAL, LSA and LPI representations. We report the results for k = 5 for the k nearest neighbors graph for LPI and Laplacian Embedding, and binary weights for the adjacency 1http://www.lemurproject.org/ 2We used 28 words because we used stemming whereas (He et al., 2004) did not, so that in two cases, two words were reduces to the same stem. matrix. We report results for 300 embedding dimensions for GLSA, LPI and LSA and 500 dimensions for GLSAL. We evaluate these representations in terms of how well the cosine similarity between the document vectors within each cluster corresponds to the true semantic similarity. We expect documents from the same Reuters category to have higher similarity. For each cluster we computed all pair-wise document similarities. All pair-wise similarities were sorted in decreasing order. The term “inter-pair” describes a pair of doc</context>
<context position="11365" citStr="He et al., 2004" startWordPosition="1841" endWordPosition="1844">ilarities that are preserved are quite different. For LSA and LPI they reflect the term distribution specific for the Reuters collection whereas for GLSA they are more general. By paired 2-tailed t-test, at p ≤ 0.05, GLSA outperformed all other approaches. There was no significant difference in performance of GLSAL, LSA and the baseline. Disappointingly, we could not achieve good performance with LPI. Its performance varies over clusters similar to that of other approaches but the average is significantly lower. We would like to stress that the comparison of our results to those presented in (He et al., 2004) are only suggestive since (He et al., 2004) applied LPI to each cluster separately and used PCA as preprocessing. We computed the LPI representation for the full collection and did not use PCA. 63 word tf glsa glsaL lsa lpi agreement(1) 0.74 0.73 0.73 0.75 0.46 american(0.8) 0.63 0.72 0.59 0.64 0.36 bank(1.4) 0.45 0.52 0.40 0.48 0.28 control(0.7) 0.78 0.82 0.80 0.80 0.58 domestic(0.8) 0.64 0.68 0.66 0.68 0.35 export(0.8) 0.64 0.65 0.70 0.67 0.37 five(1.3) 0.74 0.77 0.71 0.70 0.40 foreign(1.2) 0.51 0.58 0.55 0.56 0.28 growth(1) 0.51 0.58 0.48 0.54 0.32 income(0.5) 0.84 0.86 0.83 0.80 0.69 incr</context>
</contexts>
<marker>He, Cai, Liu, Ma, 2004</marker>
<rawString>Xiaofei He, Deng Cai, Haifeng Liu, and Wei-Ying Ma. 2004. Locality preserving indexing for document representation. In Proc. of the 27rd ACM SIGIR, pages 96–103. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<date>1999</date>
<booktitle>Foundations ofStatistical Natural Language Processing.</booktitle>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Chris Manning and Hinrich Sch¨utze. 1999. Foundations ofStatistical Natural Language Processing. MIT Press. Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irina Matveeva</author>
<author>Gina-Anne Levow</author>
<author>Ayman Farahat</author>
<author>Christian Royer</author>
</authors>
<title>Generalized latent semantic analysis for term representation. In</title>
<date>2005</date>
<booktitle>Proc. ofRANLP.</booktitle>
<contexts>
<context position="2614" citStr="Matveeva et al., 2005" startWordPosition="373" endWordPosition="376">mantic subspace with a stronger discriminative power. Laplacian Eigenmaps Embedding and LPI preserve the input similarities only locally, because this information is most reliable. Laplacian Eigenmaps Embedding does not provide a fold-in procedure for unseen documents. LPI is a linear approximation to Laplacian Eigenmaps Embedding that eliminates this problem. Similar to LSA, the input similarities to LPI are based on the inner products of the bag-of-word documents. Laplacian Eigenmaps Embedding can use any kind of similarity in the original space. Generalized Latent Semantic Analysis (GLSA) (Matveeva et al., 2005) is a framework for computing semantically motivated term and document vectors. It extends the LSA approach by focusing on term vectors instead of the dual document-term representation. GLSA requires a measure of semantic association between terms and a method of dimensionality reduction. In this paper, we use GLSA with point-wise mutual information as a term association measure. We introduce the notion of locality into this framework and propose to use Laplacian Eigenmaps Embedding as a dimensionality reduction algorithm. We evaluate the importance of locality for document representation in d</context>
<context position="6294" citStr="Matveeva et al., 2005" startWordPosition="1006" endWordPosition="1009">ian Eigenmaps Embedding algorithm computes the low dimensional vectors y to minimize under certain constraints � ||yi − yj||2Wij. ij W is the weight matrix based on the graph adjacency matrix. Wij is large if terms i and j are similar according to S. Wij can be interpreted as the penalty of mapping similar terms far apart in the Laplacian Embedding space, see (Belkin and Niyogi, 2003) for details. In our experiments we used a binary adjacency matrix W. Wij = 1 if terms i and j are among the k nearest neighbors of each other and is zero otherwise. 2.4 Measure of Semantic Association Following (Matveeva et al., 2005), we primarily used point-wise mutual information (PMI) as a measures of semantic association in step 1 of GLSA. PMI between random variables representing two words, w1 and w2, is computed as P(W1 = 1, W2 = 1) PMI(w1, w2) = log P(W1 = 1)P(W2 = 1). 2.5 GLSA Space GLSA offers a greater flexibility in exploring the notion of semantic relatedness between terms. In our preliminary experiments, we obtained the matrix of semantic associations in step 1 of GLSA using point-wise mutual information (PMI), likelihood ratio and χ2 test. Although PMI showed the best performance, other measures are particul</context>
</contexts>
<marker>Matveeva, Levow, Farahat, Royer, 2005</marker>
<rawString>Irina Matveeva, Gina-Anne Levow, Ayman Farahat, and Christian Royer. 2005. Generalized latent semantic analysis for term representation. In Proc. ofRANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>21</issue>
<marker>Sch¨utze, 1998</marker>
<rawString>Hinrich Sch¨utze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(21):97–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Human-level performance on word analogy questions by latent relational analysis.</title>
<date>2004</date>
<tech>Technical report, Technical Report ERB-1118, NRC47422.</tech>
<contexts>
<context position="7083" citStr="Turney, 2004" startWordPosition="1137" endWordPosition="1138"> is computed as P(W1 = 1, W2 = 1) PMI(w1, w2) = log P(W1 = 1)P(W2 = 1). 2.5 GLSA Space GLSA offers a greater flexibility in exploring the notion of semantic relatedness between terms. In our preliminary experiments, we obtained the matrix of semantic associations in step 1 of GLSA using point-wise mutual information (PMI), likelihood ratio and χ2 test. Although PMI showed the best performance, other measures are particularly interesting in combination with the Laplacian Embedding. Related approaches, such as LSA, the Word Space Model (WS) (Sch¨utze, 1998) and Latent Relational Analysis (LRA) (Turney, 2004) are limited to only one measure of semantic association and preserve the similarities globally. 62 Assuming that the vocabulary space has some underlying low dimensional semantic manifold. Laplacian Embedding algorithm tries to approximate this manifold by relying only on the local similarity information. It uses the nearest neighbors graph constructed using the pair-wise term similarities. The computations of the Laplacian Embedding uses the graph adjacency matrix W. This matrix can be binary or use weighted similarities. The advantage of the binary adjacency matrix is that it conveys the ne</context>
</contexts>
<marker>Turney, 2004</marker>
<rawString>Peter D. Turney. 2004. Human-level performance on word analogy questions by latent relational analysis. Technical report, Technical Report ERB-1118, NRC47422.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>