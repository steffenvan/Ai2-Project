<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012503">
<title confidence="0.99233">
Crawling microblogging services to gather language-classified URLs
Workflow and case study
</title>
<author confidence="0.94098">
Adrien Barbaresi
</author>
<affiliation confidence="0.754912">
ICAR Lab
ENS Lyon &amp; University of Lyon
</affiliation>
<address confidence="0.871078">
15 parvis Ren´e Descartes, 69007 Lyon, France
</address>
<email confidence="0.92642">
adrien.barbaresi@ens-lyon.fr
</email>
<sectionHeader confidence="0.991336" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999932619047619">
We present a way to extract links from
messages published on microblogging
platforms and we classify them according
to the language and possible relevance of
their target in order to build a text cor-
pus. Three platforms are taken into con-
sideration: FriendFeed, identi.ca and Red-
dit, as they account for a relative diver-
sity of user profiles and more importantly
user languages. In order to explore them,
we introduce a traversal algorithm based
on user pages. As we target lesser-known
languages, we try to focus on non-English
posts by filtering out English text. Us-
ing mature open-source software from the
NLP research field, a spell checker (as-
pell) and a language identification sys-
tem (langid.py), our case study and
our benchmarks give an insight into the
linguistic structure of the considered ser-
vices.
</bodyText>
<sectionHeader confidence="0.999645" genericHeader="introduction">
1 Introduction
</sectionHeader>
<subsectionHeader confidence="0.999721">
1.1 The ’Web as Corpus’ paradigm
</subsectionHeader>
<bodyText confidence="0.999972827586207">
The state of the art tools of the ’Web as Corpus’
framework rely heavily on URLs obtained from
search engines. As a matter of fact, the approach
followed by the most researchers of this field con-
sists in querying search engines (e.g. by tuples)
to gather links that are crawled in order to build a
corpus (Baroni et al., 2009).
This method could be used in free corpus build-
ing approach until recently, when it was made im-
possible because of increasing limitations on the
search engines’ APIs, which make the gathering
process on a low budget very slow or impossible.
All in all, the APIs may be too expensive and/or
too unstable in time to support large-scale corpus
building projects.
Moreover, the question whether the method
used so far, i.e. randomizing keywords, provides
a good overview of a language is still open. Other
technical difficulties include diverse and partly un-
known search biases due, in part, to search en-
gine optimization tricks as well as undocumented
PageRank adjustments. Using diverse sources of
seed URLs could at least ensure that there is not a
single bias, but several ones.
The crawling method using these seeds for cor-
pus building may then yield better results, e.g.
ensure better randomness in a population of web
documents as described by (Henzinger et al.,
2000).
</bodyText>
<subsectionHeader confidence="0.966533">
1.2 User-based URL gathering
</subsectionHeader>
<bodyText confidence="0.999984333333333">
Our hypothesis is that microblogging services are
a good alternative to overcome the limitations of
seed URL collections and the biases implied by
search engine optimization techniques, PageRank
and link classification.
It is a user-based language approach. Its obvi-
ous limits are the amount of spam and advertise-
ment. Its obvious bias consists in the technology-
prone users who are familiar with these platforms
and account for numerous short messages which
in turn over-represent their own interests and hob-
bies.
However, user-related biases also have advan-
tages, most notably the fact that documents that
are most likely to be important are being shared,
which has benefits when it comes to gather links
in lesser-known languages, below the English-
speaking spammer’s radar.
</bodyText>
<subsectionHeader confidence="0.958441">
1.3 Interest
</subsectionHeader>
<bodyText confidence="0.9998734">
The main goal is to provide well-documented,
feature-rich software and databases relevant for
linguistic studies. More specifically, we would
like to be able to cover languages which are more
rarely seen on the Internet, which implies the gath-
</bodyText>
<page confidence="0.952361">
9
</page>
<note confidence="0.504074">
Proceedings of the ACL Student Research Workshop, pages 9–15,
</note>
<page confidence="0.36052">
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</page>
<bodyText confidence="0.999921142857143">
ering of higher proportions of URLs leading to
lesser-known languages. We think that social net-
works and microblogging services may be of great
help when it comes to focus on them.
In fact, the most engaged social networking na-
tions do arguably not use English as a first com-
municating language1. In addition, crawling these
services gives an opportunity to perform a case
study of existing tools and platforms.
Finally, the method presented here could be
used in other contexts : microtext collections, user
lists and relations could prove useful for microtext
corpus building, network visualization or social
network sampling purposes (Gjoka et al., 2011).
</bodyText>
<sectionHeader confidence="0.983119" genericHeader="method">
2 Data Sources
</sectionHeader>
<bodyText confidence="0.999226714285714">
FriendFeed, identi.ca and Reddit are taken into
consideration for this study. These services pro-
vide a good overview of the peculiarities of social
networks. At least by the last two of them a crawl
appears to be manageable in terms of both API ac-
cessibility and corpus size, which is not the case
concerning Twitter for example.
</bodyText>
<subsectionHeader confidence="0.997232">
2.1 identi.ca
</subsectionHeader>
<bodyText confidence="0.999788">
identi.ca is a social microblogging service built on
open source tools and open standards, which is the
reason why we chose to crawl it at first.
The advantages compared to Twitter include the
Creative Commons license of the content, the ab-
sence of limitations on the total number of pages
seen (to our knowledge) and the relatively small
amount of messages, which can also be a prob-
lem. A full coverage of the network is theoreti-
cally possible, where all the information may be
publicly available. Thus, all interesting informa-
tion is collected and no language filtering is used
concerning this website.
</bodyText>
<subsectionHeader confidence="0.995562">
2.2 FriendFeed
</subsectionHeader>
<bodyText confidence="0.999957625">
To our knowledge, FriendFeed is the most active
of the three microblogging services considered
here. It is also the one which seems to have been
studied the most by the research community. The
service works as an aggregator (Gupta et al., 2009)
that offers a broader spectrum of retrieved infor-
mation. Technically, FriendFeed and identi.ca can
overlap, as the latter is integrated in the former.
</bodyText>
<footnote confidence="0.989995">
1http://www.comscore.com/Press Events/Press Releases/
2011/12/Social Networking Leads as Top Online
Activity Globally
</footnote>
<bodyText confidence="0.9972498">
But the size difference between the two platforms
makes this hypothesis unlikely.
The API of FriendFeed is somewhat liberal, as
no explicit limits are enforced. Nonetheless, our
tests showed that after a certain number of suc-
cessful requests with little or no sleep, the servers
start dropping most of the inbound connections.
All in all, the relative tolerance of this website
makes it a good candidate to gather a lot of text
in a short period of time.
</bodyText>
<subsectionHeader confidence="0.996112">
2.3 Reddit
</subsectionHeader>
<bodyText confidence="0.999907105263158">
Reddit is a social bookmarking and a microblog-
ging platform, which ranks to the 7th place world-
wide in the news category according to Alexa.2
The entries are organized into areas of interest
called ’reddits’ or ’subreddits’. The users account
for the linguistic relevance of their channel, the
moderation processes are mature, and since the
channels (or subreddits) have to be hand-picked,
they ensure a certain stability.
There are 16 target languages so far, which
can be accessed via so-called ’multi-reddit ex-
pressions’, i.e. compilations of subreddits: Croa-
tian, Czech, Danish, Finnish, French, German,
Hindi, Italian, Norwegian, Polish, Portuguese, Ro-
manian, Russian, Spanish, Swedish and Turkish3.
Sadly, it is currently not possible to go back in
time further than the 500th oldest post due to API
limitations, which severely restricts the number of
links one may crawl.
</bodyText>
<sectionHeader confidence="0.998347" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.9996925">
The following workflow describes how the results
below are obtained:
</bodyText>
<listItem confidence="0.9879451">
1. URL harvesting: social network traversal,
obvious spam and non-text documents filter-
ing, optional spell check of the short message
to see if it could be English text, optional
record of user IDs for later crawls.
2. Operations on the URL queue: redirection
checks, sampling by domain name.
3. Download of the web documents and analy-
sis: HTML code stripping, document validity
check, language identification.
</listItem>
<footnote confidence="0.999525333333333">
2http://www.alexa.com/topsites/category/Top/News
3Here is a possible expression to target Norwegian users:
http://www.reddit.com/r/norge+oslo+norskenyheter
</footnote>
<page confidence="0.998289">
10
</page>
<bodyText confidence="0.999965214285714">
The only difference between FriendFeed and
Reddit on one hand and identi.ca on the other hand
is the spell check performed on the short messages
in order to target non-English ones. Indeed, all
new messages can be taken into consideration on
the latter, making a selection unnecessary.
Links pointing to media documents, which rep-
resent a high volume of links shared on microblog-
ging services, are excluded from this study, as its
final purpose is to be able to build a text corpus. As
a page is downloaded or a query is executed, links
are filtered on the fly using a series of heuristics
described below, and finally the rest of the links is
stored.
</bodyText>
<subsectionHeader confidence="0.9907535">
3.1 TRUC: an algorithm for TRaversal and
User-based Crawls
</subsectionHeader>
<bodyText confidence="0.9999888">
Starting from a publicly available homepage, the
crawl engine selects users according to their lin-
guistic relevance based on a language filter (see
below), and then retrieves their messages, eventu-
ally discovering friends of friends and expanding
its scope and the size of the network it traverses.
As this is a breadth-first approach its applicability
depends greatly on the size of the network.
In this study, the goal is to concentrate on non-
English speaking messages in the hope of find-
ing non-English links. The main ’timeline’ fos-
ters a users discovery approach, which then be-
comes user-centered as the spider focuses on a list
of users who are expected not to post messages in
English and/or spam. The messages are filtered at
each step to ensure relevant URLs are collected.
This implies that a lot of subtrees are pruned, so
that the chances of completing the traversal in-
crease. In fact, experience shows that a relatively
small fraction of users and URLs is selected.
This approach is ’static’, as it does not rely on
any long poll requests (which are for instance used
to capture a fraction of Twitter’s messages as they
are made public), it actively fetches the required
pages.
</bodyText>
<subsectionHeader confidence="0.994931">
3.2 Check for redirection and sampling
</subsectionHeader>
<bodyText confidence="0.999968541666667">
Further work on the URL queue before the lan-
guage identification task ensures an even smaller
fraction of URLs really goes through the resource-
expensive process of fetching and analyzing web
documents.
The first step of preprocessing consists in find-
ing those URLs that lead to a redirect, which is
done using a list comprising all the major URL
shortening services and adding all intriguingly
short URLs, i.e. less than 26 characters in length,
which according to our FriendFeed data occurs at
a frequency of about 3%. To deal with shortened
URLs, one can perform HTTP HEAD requests for
each member of the list in order to determine and
store the final URL.
The second step is a sampling that reduces both
the size of the list and the probable impact of an
overrepresented domain names in the result set. If
several URLs contain the same domain name, the
group is reduced to a randomly chosen URL.
Due to the overlaps of domain names and the
amount of spam and advertisement on social net-
works such an approach is very useful when it
comes to analyze a large list of URLs.
</bodyText>
<subsectionHeader confidence="0.994291">
3.3 Language identification
</subsectionHeader>
<bodyText confidence="0.999960633333333">
Microtext has characteristics that make it hard for
’classical’ NLP approaches like web page lan-
guage identification based on URLs (Baykan et
al., 2008) to predict with certainty the languages
of the links. That is why mature NLP tools have to
be used to filter the incoming messages.
A similar work on language identification and
FriendFeed is described in (Celli, 2009), who uses
a dictionary-based approach: the software tries
to guess the language of microtext by identifying
very frequent words.
However, the fast-paced evolution of the vocab-
ulary used on social networks makes it hard to
rely only on lists of frequent terms, so that our ap-
proach seems more complete.
A first dictionary-based filter First, a quick test
is used in order to guess whether a microtext is En-
glish or not. Indeed, this operation cuts the amount
of microtexts in half and enables to select the users
or the friends which feature the desired response,
thus directing the traversal in a more fruitful direc-
tion.
The library used, enchant4, allows the use of
a variety of spell-checking backends, like aspell,
hunspell or ispell, with one or several locales5.
Basically, this approach can be used with other
languages as well, even if they are not used as
discriminating factors in this study. We consider
this option to be a well-balanced solution between
processing speed on one hand and coverage on
</bodyText>
<footnote confidence="0.9998495">
4http://www.abisource.com/projects/enchant/
5All software mentioned here is open-source.
</footnote>
<page confidence="0.998951">
11
</page>
<bodyText confidence="0.9997375">
the other. Spell checking algorithms benefit from
years of optimization in both areas.
This first filter uses a threshold to discriminate
between short messages, expressed as a percent-
age of tokens which do not pass the spell check.
The filter also relies on software biases, like Uni-
code errors, which make it nearly certain that the
given input microtext is not English.
langid.py A language identification tool is
used to classify the web documents and to bench-
mark the efficiency of the test mentioned above.
langid.py (Lui and Baldwin, 2011; Lui and
Baldwin, 2012) is open-source, it incorporates
a pre-trained model and it covers 97 languages,
which is ideal to tackle the diversity of the web.
Its use as a web service makes it a fast solution
enabling distant or distributed work.
The server version of langid.py was used,
the texts were downloaded, all the HTML markup
was stripped and the resulting text was discarded
if it was less than 1,000 characters long. Accord-
ing to its authors, langid.py could be used di-
rectly on microtexts. However, this feature was
discarded because it did not prove as efficient as
the approach used here when it comes to a sub-
stantial amounts of short messages.
</bodyText>
<sectionHeader confidence="0.999958" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.992366444444444">
The surface crawl dealing with the main time-
line and one level of depth has been performed
on the three platforms6. In the case of identi.ca,
a deep miner was launched to explore the net-
work. FriendFeed proved too large to start such a
breadth-first crawler so that other strategies ought
to be used (Gjoka et al., 2011), whereas the multi-
reddit expressions used did not yield enough users.
FriendFeed is the biggest link provider on a reg-
ular basis (about 10,000 or 15,000 messages per
hour can easily be collected), whereas Reddit is
the weakest, as the total figures show.
The total number of English websites may be
a relevant indication when it comes to establish
a baseline for finding possibly non-English docu-
ments. Accordingly, English accounts for about
55 % of the websites7, with the second most-
used content-language, German, only representing
</bodyText>
<footnote confidence="0.875273666666667">
6Several techniques are used to keep the number of re-
quests as low as possible, most notably user profiling accord-
ing to the tweeting frequency. In the case of identi.ca this
results into approximately 300 page views every hour.
7http://w3techs.com/technologies/overview/content
language/all
</footnote>
<bodyText confidence="0.9049885">
about 6 % of the web pages. So, there is a gap be-
tween English and the other languages, and there
is also a discrepancy between the number of Inter-
net users and the content languages.
</bodyText>
<subsectionHeader confidence="0.958363">
4.1 FriendFeed
</subsectionHeader>
<bodyText confidence="0.999962086956522">
To test whether the first language filter was ef-
ficient, a testing sample of URLs and users was
collected randomly. The first filter was emu-
lated by selecting about 8% of messages (based
on a random function) in the spam and media-
filtered posts of the public timeline. Indeed, the
messages selected by the algorithm approximately
amount to this fraction of the total. At the same
time, the corresponding users were retrieved, ex-
actly as described above, and then the user-based
step was run, keeping one half of the user’s mes-
sages, which is also realistic according to real-
world data.
The datasets compared here were both of an
order of magnitude of at least 105 unique URLs
before the redirection checks. At the end of the
toolchain, the randomly selected benchmark set
comprises 7,047 URLs and the regular set 19,573
URLs8. The first was collected in about 30 hours
and the second one in several weeks. According
to the methodology used, this phenomenon may
be explained by the fact that the domain names in
the URLs tend to be mentioned repeatedly.
</bodyText>
<table confidence="0.998926333333333">
Language URLs %
English 4,978 70.6
German 491 7.0
Japanese 297 4.2
Spanish 258 3.7
French 247 3.5
</table>
<tableCaption confidence="0.988804">
Table 1: 5 most frequent languages of URLs taken
at random on FriendFeed
</tableCaption>
<bodyText confidence="0.953500636363636">
According to the language identification system
(langid.py), the first language filter beats the
random function by nearly 30 points (see Table
2). The other top languages are accordingly better
represented. Other noteworthy languages are to be
found in the top 20, e.g. Indonesian and Persian
(Farsi).
8The figures given describe the situation at the end, after
the sampling by domain name and after the selection of doc-
uments based on a minimum length. The word URL is used
as a shortcut for the web documents they link to.
</bodyText>
<page confidence="0.996719">
12
</page>
<table confidence="0.999595363636363">
Language URLs %
English 8,031 41.0
Russian 2,475 12.6
Japanese 1,757 9.0
Turkish 1,415 7.2
German 1,289 6.6
Spanish 954 4.9
French 703 3.6
Italian 658 3.4
Portuguese 357 1.8
Arabic 263 1.3
</table>
<tableCaption confidence="0.994485">
Table 2: 10 most frequent languages of spell-
check-filtered URLs gathered on FriendFeed
</tableCaption>
<subsectionHeader confidence="0.98592">
4.2 identi.ca
</subsectionHeader>
<bodyText confidence="0.998849428571429">
The results of the two strategies followed on
identi.ca led to a total of 1,113,783 URLs checked
for redirection, which were collected in about a
week (the deep crawler reached 37,485 user IDs).
A large majority of the 192,327 total URLs ap-
parently lead to English texts (64.9 %), since no
language filter was used but only a spam filter.
</bodyText>
<table confidence="0.999595272727273">
Language URLs %
English 124,740 64.9
German 15,484 8.1
Spanish 15,295 8.0
French 12,550 6.5
Portuguese 5,485 2.9
Italian 3,384 1.8
Japanese 1,758 0.9
Dutch 1,610 0.8
Indonesian 1,229 0.6
Polish 1,151 0.6
</table>
<tableCaption confidence="0.992192">
Table 3: 10 most frequent languages of URLs
gathered on identi.ca
</tableCaption>
<subsectionHeader confidence="0.997">
4.3 Reddit
</subsectionHeader>
<bodyText confidence="0.9995549">
The figures presented here are the results of a sin-
gle crawl of all available languages altogether, but
regular crawls are needed to compensate for the
500 posts limit. English accounted for 18.1 % of
the links found on channel pages (for a total of
4,769 URLs) and 55.9 % of the sum of the links
found on channel and on user pages (for a total of
20,173 URLs).
The results in Table 5 show that the first filter
was nearly sufficient to discriminate between the
</bodyText>
<table confidence="0.999865727272727">
Language URLs % Comb. %
English 863 18.1 55.9
Spanish 798 16.7 9.7
German 519 10.9 6.3
French 512 10.7 7.2
Swedish 306 6.4 2.9
Romanian 265 5.6 2.5
Portuguese 225 4.7 2.1
Finnish 213 4.5 1.6
Czech 199 4.2 1.4
Norwegian 194 4.1 2.1
</table>
<tableCaption confidence="0.740144333333333">
Table 4: 10 most frequent languages of filtered
URLs gathered on Reddit channels and on a com-
bination of channels and user pages
</tableCaption>
<bodyText confidence="0.992294">
links. Indeed, the microtexts that were under the
threshold led to a total of 204,170 URLs. 28,605
URLs remained at the end of the toolchain and En-
glish accounted for 76.7 % of the documents they
linked to.
</bodyText>
<table confidence="0.999425">
Language URLs % of total
English 21,926 76.7
Spanish 1,402 4.9
French 1,141 4.0
German 997 3.5
Swedish 445 1.6
</table>
<tableCaption confidence="0.917738">
Table 5: 5 most frequent languages of links seen
</tableCaption>
<bodyText confidence="0.96751">
on Reddit and rejected by the primary language
filter
The threshold was set at 90 % of the words for
FriendFeed and 33% for Reddit, each time after
a special punctuation strip to avoid the influence
of special uses of punctuation on social networks.
Yet, the lower filter achieved better results, which
may be explained by the moderation system of the
subreddits as well as by the greater regularity in
the posts of this platform.
</bodyText>
<sectionHeader confidence="0.998712" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999968625">
Three main technical challenges had to be ad-
dressed, which resulted in a separate workflow:
the shortened URLs are numerous, yet they ought
to be resolved in order to enable the use of heuris-
tics based on the nature of the URLs or a proper
sampling of the URLs themselves. The con-
frontation with the constantly increasing number
of URLs to analyze and the necessarily limited re-
</bodyText>
<page confidence="0.996727">
13
</page>
<bodyText confidence="0.999976571428572">
sources make a website sampling by domain name
useful. Finally, the diversity of the web documents
put the language recognition tools to a test, so that
a few tweaks are necessary to correct the results.
The relatively low number of results for Russian
may be explained by weaknesses of langid.py
with deviations of encoding standards. Indeed, a
few tweaks are necessary to correct the biases of
the software in its pre-trained version, in particular
regarding texts falsely considered as being written
in Chinese, although URL-based heuristics indi-
cate that the website is most probably hosted in
Russia or in Japan. A few charset encodings found
in Asian countries are also a source of classifica-
tion problems. The low-confidence responses as
well as a few well-delimited cases were discarded
in this study, they account for no more than 2 % of
the results. Ideally, a full-fledged comparison with
other language identification software may be nec-
essary to identify its areas of expertise.
A common practice known as cloaking has not
been addressed so far: a substantial fraction of
web pages show a different content to crawler en-
gines and to browsers. This Janus-faced behavior
tends to alter the language characteristics of the
web page in favor of English results.
Regarding topics, a major user bias was not ad-
dressed either: among the most frequently shared
links on identi.ca for example, many are related to
technology, IT or software and are mostly written
in English. The social media analyzed here tend
to be dominated by English-speaking users, either
native speakers or second-language learners.
In general, there is room for improvement con-
cerning the first filter, the threshold could be tested
and adapted to several scenarios. This may involve
larger datasets for testing purposes and machine
learning techniques relying on feature extraction.
The contrasted results on Reddit shed a different
light on the exploration of user pages: in all like-
lihood, users mainly share links in English when
they are not posting them on a language-relevant
channel. The results on FriendFeed are better from
this point of view, which may suggest that English
is not used equally on all platforms by users who
speak other languages than English. Nonetheless,
the fact that the microblogging services studied
here are mainly English-speaking seems to be a
strong tendency.
Last but not least, the adequateness of the web
documents shared on social networks has yet to
be thoroughly assessed. From the output of this
toolchain to a full-fledged web corpus, other fine-
grained instruments (Sch¨afer and Bildhauer, 2012)
as well as further decisions processes (Sch¨afer et
al., 2013) are needed along the way.
</bodyText>
<sectionHeader confidence="0.998618" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999989025641026">
We presented a methodology to gather multilin-
gual URLs on three microblogging platforms. In
order to do so, we perform traversals of the plat-
forms and use already available tools to filter the
URLs accordingly and identify their language.
We provide open source software to access the
APIs (FriendFeed and Reddit) and the HTML ver-
sion of identi.ca, as an authentication is mandatory
for the API. The TRUC algorithm is fully imple-
mented. All the operations described in this paper
can be reproduced using the same tools, which are
part of repositories currently hosted on the GitHub
platform9.
The main goal is achieved, as hundreds if not
thousands of URLs for lesser-known languages
such as Romanian or Indonesian can be gathered
on social networks and microblogging services.
When it comes to filter out English posts, a first
step using an English spell checker gives better
results than the baseline established using micro-
texts selected at random. However, the discrep-
ancy between the languages one would expect to
find based on demographic indicators and the re-
sults of the study is remarkable. English websites
stay numerous even when one tries to filter them
out.
This proof of concept is usable, but a better fil-
tering process and longer crawls may be necessary
to unlock the full potential of this approach. Last,
a random-walk crawl using these seeds and a state
of the art text categorization may provide more in-
formation on what is really shared on microblog-
ging platforms.
Future work perspectives include dealing with
live tweets (as Twitter and FriendFeed can be
queried continuously), exploring the depths of
identi.ca and FriendFeed and making the directory
of language-classified URLs collected during this
study publicly available.
</bodyText>
<footnote confidence="0.980122">
9https://github.com/adbar/microblog-explorer
</footnote>
<page confidence="0.99933">
14
</page>
<sectionHeader confidence="0.999076" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998964">
This work has been partially funded by an inter-
nal grant of the FU Berlin (COW project at the
German Grammar Dept.). Many thanks to Roland
Sch¨afer and two anonymous reviewers for their
useful comments.
</bodyText>
<sectionHeader confidence="0.999182" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997177591836735">
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The WaCky Wide
Web: A collection of very large linguistically pro-
cessed web-crawled corpora. Language Resources
and Evaluation, 43(3):209–226.
Eda Baykan, Monika Henzinger, and Ingmar Weber.
2008. Web Page Language Identification Based
on URLs. Proceedings of the VLDB Endowment,
1(1):176–187.
Fabio Celli. 2009. Improving Language identifica-
tion performance with FriendFeed data. Technical
report, CLIC, University of Trento.
Minas Gjoka, Maciej Kurant, Carter T. Butts, and
Athina Markopoulou. 2011. Practical recom-
mendations on crawling online social networks.
IEEE Journal on Selected Areas in Communica-
tions, 29(9):1872–1892.
Trinabh Gupta, Sanchit Garg, Niklas Carlsson, Anirban
Mahanti, and Martin Arlitt. 2009. Characterization
of FriendFeed – A Web-based Social Aggregation
Service. In Proceedings of the AAAI ICWSM, vol-
ume 9.
Monika R. Henzinger, Allan Heydon, Michael Mitzen-
macher, and Marc Najork. 2000. On near-uniform
URL sampling. In Proceedings of the 9th Inter-
national World Wide Web conference on Computer
Networks: The International Journal of Computer
and Telecommunications Networking, pages 295–
308. North-Holland Publishing Co.
Marco Lui and Timothy Baldwin. 2011. Cross-domain
Feature Selection for Language Identification. In
Proceedings of the Fifth International Joint Con-
ference on Natural Language Processing (IJCNLP
2011), pages 553–561, Chiang Mai, Thailand.
Marco Lui and Timothy Baldwin. 2012. langid.py: An
Off-the-shelf Language Identification Tool. In Pro-
ceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2012),
Jeju, Republic of Korea.
Roland Sch¨afer and Felix Bildhauer. 2012. Building
large corpora from the web using a new efficient
tool chain. In Proceedings of the Eight International
Conference on Language Resources and Evaluation
(LREC’12), pages 486–493.
Roland Sch¨afer, Adrien Barbaresi, and Felix Bildhauer.
2013. The Good, the Bad, and the Hazy: Design
Decisions in Web Corpus Construction. In Proceed-
ings of the 8th Web as Corpus Workshop (WAC8).
To appear.
</reference>
<page confidence="0.997601">
15
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.639924">
<title confidence="0.9905975">Crawling microblogging services to gather language-classified Workflow and case study</title>
<author confidence="0.991273">Adrien</author>
<affiliation confidence="0.906516">ICAR ENS Lyon &amp; University of</affiliation>
<address confidence="0.860048">15 parvis Ren´e Descartes, 69007 Lyon,</address>
<email confidence="0.993859">adrien.barbaresi@ens-lyon.fr</email>
<abstract confidence="0.9926615">We present a way to extract links from messages published on microblogging platforms and we classify them according to the language and possible relevance of their target in order to build a text corpus. Three platforms are taken into consideration: FriendFeed, identi.ca and Reddit, as they account for a relative diversity of user profiles and more importantly user languages. In order to explore them, we introduce a traversal algorithm based on user pages. As we target lesser-known languages, we try to focus on non-English posts by filtering out English text. Using mature open-source software from the NLP research field, a spell checker (aspell) and a language identification sysour case study and our benchmarks give an insight into the linguistic structure of the considered services.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The WaCky Wide Web: A collection of very large linguistically processed web-crawled corpora. Language Resources and Evaluation,</title>
<date>2009</date>
<pages>43--3</pages>
<contexts>
<context position="1417" citStr="Baroni et al., 2009" startWordPosition="228" endWordPosition="231">Using mature open-source software from the NLP research field, a spell checker (aspell) and a language identification system (langid.py), our case study and our benchmarks give an insight into the linguistic structure of the considered services. 1 Introduction 1.1 The ’Web as Corpus’ paradigm The state of the art tools of the ’Web as Corpus’ framework rely heavily on URLs obtained from search engines. As a matter of fact, the approach followed by the most researchers of this field consists in querying search engines (e.g. by tuples) to gather links that are crawled in order to build a corpus (Baroni et al., 2009). This method could be used in free corpus building approach until recently, when it was made impossible because of increasing limitations on the search engines’ APIs, which make the gathering process on a low budget very slow or impossible. All in all, the APIs may be too expensive and/or too unstable in time to support large-scale corpus building projects. Moreover, the question whether the method used so far, i.e. randomizing keywords, provides a good overview of a language is still open. Other technical difficulties include diverse and partly unknown search biases due, in part, to search e</context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The WaCky Wide Web: A collection of very large linguistically processed web-crawled corpora. Language Resources and Evaluation, 43(3):209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eda Baykan</author>
<author>Monika Henzinger</author>
<author>Ingmar Weber</author>
</authors>
<title>Web Page Language Identification Based on URLs.</title>
<date>2008</date>
<booktitle>Proceedings of the VLDB Endowment,</booktitle>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="10944" citStr="Baykan et al., 2008" startWordPosition="1777" endWordPosition="1780">and store the final URL. The second step is a sampling that reduces both the size of the list and the probable impact of an overrepresented domain names in the result set. If several URLs contain the same domain name, the group is reduced to a randomly chosen URL. Due to the overlaps of domain names and the amount of spam and advertisement on social networks such an approach is very useful when it comes to analyze a large list of URLs. 3.3 Language identification Microtext has characteristics that make it hard for ’classical’ NLP approaches like web page language identification based on URLs (Baykan et al., 2008) to predict with certainty the languages of the links. That is why mature NLP tools have to be used to filter the incoming messages. A similar work on language identification and FriendFeed is described in (Celli, 2009), who uses a dictionary-based approach: the software tries to guess the language of microtext by identifying very frequent words. However, the fast-paced evolution of the vocabulary used on social networks makes it hard to rely only on lists of frequent terms, so that our approach seems more complete. A first dictionary-based filter First, a quick test is used in order to guess </context>
</contexts>
<marker>Baykan, Henzinger, Weber, 2008</marker>
<rawString>Eda Baykan, Monika Henzinger, and Ingmar Weber. 2008. Web Page Language Identification Based on URLs. Proceedings of the VLDB Endowment, 1(1):176–187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Celli</author>
</authors>
<title>Improving Language identification performance with FriendFeed data.</title>
<date>2009</date>
<tech>Technical report, CLIC,</tech>
<institution>University of Trento.</institution>
<contexts>
<context position="11163" citStr="Celli, 2009" startWordPosition="1816" endWordPosition="1817"> is reduced to a randomly chosen URL. Due to the overlaps of domain names and the amount of spam and advertisement on social networks such an approach is very useful when it comes to analyze a large list of URLs. 3.3 Language identification Microtext has characteristics that make it hard for ’classical’ NLP approaches like web page language identification based on URLs (Baykan et al., 2008) to predict with certainty the languages of the links. That is why mature NLP tools have to be used to filter the incoming messages. A similar work on language identification and FriendFeed is described in (Celli, 2009), who uses a dictionary-based approach: the software tries to guess the language of microtext by identifying very frequent words. However, the fast-paced evolution of the vocabulary used on social networks makes it hard to rely only on lists of frequent terms, so that our approach seems more complete. A first dictionary-based filter First, a quick test is used in order to guess whether a microtext is English or not. Indeed, this operation cuts the amount of microtexts in half and enables to select the users or the friends which feature the desired response, thus directing the traversal in a mo</context>
</contexts>
<marker>Celli, 2009</marker>
<rawString>Fabio Celli. 2009. Improving Language identification performance with FriendFeed data. Technical report, CLIC, University of Trento.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minas Gjoka</author>
<author>Maciej Kurant</author>
<author>Carter T Butts</author>
<author>Athina Markopoulou</author>
</authors>
<title>Practical recommendations on crawling online social networks.</title>
<date>2011</date>
<journal>IEEE Journal on Selected Areas in Communications,</journal>
<volume>29</volume>
<issue>9</issue>
<contexts>
<context position="4256" citStr="Gjoka et al., 2011" startWordPosition="679" endWordPosition="682">ading to lesser-known languages. We think that social networks and microblogging services may be of great help when it comes to focus on them. In fact, the most engaged social networking nations do arguably not use English as a first communicating language1. In addition, crawling these services gives an opportunity to perform a case study of existing tools and platforms. Finally, the method presented here could be used in other contexts : microtext collections, user lists and relations could prove useful for microtext corpus building, network visualization or social network sampling purposes (Gjoka et al., 2011). 2 Data Sources FriendFeed, identi.ca and Reddit are taken into consideration for this study. These services provide a good overview of the peculiarities of social networks. At least by the last two of them a crawl appears to be manageable in terms of both API accessibility and corpus size, which is not the case concerning Twitter for example. 2.1 identi.ca identi.ca is a social microblogging service built on open source tools and open standards, which is the reason why we chose to crawl it at first. The advantages compared to Twitter include the Creative Commons license of the content, the a</context>
<context position="13800" citStr="Gjoka et al., 2011" startWordPosition="2256" endWordPosition="2259">was discarded if it was less than 1,000 characters long. According to its authors, langid.py could be used directly on microtexts. However, this feature was discarded because it did not prove as efficient as the approach used here when it comes to a substantial amounts of short messages. 4 Results The surface crawl dealing with the main timeline and one level of depth has been performed on the three platforms6. In the case of identi.ca, a deep miner was launched to explore the network. FriendFeed proved too large to start such a breadth-first crawler so that other strategies ought to be used (Gjoka et al., 2011), whereas the multireddit expressions used did not yield enough users. FriendFeed is the biggest link provider on a regular basis (about 10,000 or 15,000 messages per hour can easily be collected), whereas Reddit is the weakest, as the total figures show. The total number of English websites may be a relevant indication when it comes to establish a baseline for finding possibly non-English documents. Accordingly, English accounts for about 55 % of the websites7, with the second mostused content-language, German, only representing 6Several techniques are used to keep the number of requests as l</context>
</contexts>
<marker>Gjoka, Kurant, Butts, Markopoulou, 2011</marker>
<rawString>Minas Gjoka, Maciej Kurant, Carter T. Butts, and Athina Markopoulou. 2011. Practical recommendations on crawling online social networks. IEEE Journal on Selected Areas in Communications, 29(9):1872–1892.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trinabh Gupta</author>
<author>Sanchit Garg</author>
<author>Niklas Carlsson</author>
<author>Anirban Mahanti</author>
<author>Martin Arlitt</author>
</authors>
<title>Characterization of FriendFeed – A Web-based Social Aggregation Service.</title>
<date>2009</date>
<booktitle>In Proceedings of the AAAI ICWSM,</booktitle>
<volume>9</volume>
<contexts>
<context position="5481" citStr="Gupta et al., 2009" startWordPosition="887" endWordPosition="890"> of limitations on the total number of pages seen (to our knowledge) and the relatively small amount of messages, which can also be a problem. A full coverage of the network is theoretically possible, where all the information may be publicly available. Thus, all interesting information is collected and no language filtering is used concerning this website. 2.2 FriendFeed To our knowledge, FriendFeed is the most active of the three microblogging services considered here. It is also the one which seems to have been studied the most by the research community. The service works as an aggregator (Gupta et al., 2009) that offers a broader spectrum of retrieved information. Technically, FriendFeed and identi.ca can overlap, as the latter is integrated in the former. 1http://www.comscore.com/Press Events/Press Releases/ 2011/12/Social Networking Leads as Top Online Activity Globally But the size difference between the two platforms makes this hypothesis unlikely. The API of FriendFeed is somewhat liberal, as no explicit limits are enforced. Nonetheless, our tests showed that after a certain number of successful requests with little or no sleep, the servers start dropping most of the inbound connections. All</context>
</contexts>
<marker>Gupta, Garg, Carlsson, Mahanti, Arlitt, 2009</marker>
<rawString>Trinabh Gupta, Sanchit Garg, Niklas Carlsson, Anirban Mahanti, and Martin Arlitt. 2009. Characterization of FriendFeed – A Web-based Social Aggregation Service. In Proceedings of the AAAI ICWSM, volume 9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Monika R Henzinger</author>
<author>Allan Heydon</author>
<author>Michael Mitzenmacher</author>
<author>Marc Najork</author>
</authors>
<title>On near-uniform URL sampling.</title>
<date>2000</date>
<booktitle>In Proceedings of the 9th International World Wide Web conference on Computer Networks: The International Journal of Computer and Telecommunications Networking,</booktitle>
<pages>295--308</pages>
<publisher>North-Holland Publishing Co.</publisher>
<contexts>
<context position="2389" citStr="Henzinger et al., 2000" startWordPosition="389" endWordPosition="392">g projects. Moreover, the question whether the method used so far, i.e. randomizing keywords, provides a good overview of a language is still open. Other technical difficulties include diverse and partly unknown search biases due, in part, to search engine optimization tricks as well as undocumented PageRank adjustments. Using diverse sources of seed URLs could at least ensure that there is not a single bias, but several ones. The crawling method using these seeds for corpus building may then yield better results, e.g. ensure better randomness in a population of web documents as described by (Henzinger et al., 2000). 1.2 User-based URL gathering Our hypothesis is that microblogging services are a good alternative to overcome the limitations of seed URL collections and the biases implied by search engine optimization techniques, PageRank and link classification. It is a user-based language approach. Its obvious limits are the amount of spam and advertisement. Its obvious bias consists in the technologyprone users who are familiar with these platforms and account for numerous short messages which in turn over-represent their own interests and hobbies. However, user-related biases also have advantages, most</context>
</contexts>
<marker>Henzinger, Heydon, Mitzenmacher, Najork, 2000</marker>
<rawString>Monika R. Henzinger, Allan Heydon, Michael Mitzenmacher, and Marc Najork. 2000. On near-uniform URL sampling. In Proceedings of the 9th International World Wide Web conference on Computer Networks: The International Journal of Computer and Telecommunications Networking, pages 295– 308. North-Holland Publishing Co.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Lui</author>
<author>Timothy Baldwin</author>
</authors>
<title>Cross-domain Feature Selection for Language Identification.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifth International Joint Conference on Natural Language Processing (IJCNLP 2011),</booktitle>
<pages>553--561</pages>
<location>Chiang Mai, Thailand.</location>
<contexts>
<context position="12812" citStr="Lui and Baldwin, 2011" startWordPosition="2082" endWordPosition="2085">ource.com/projects/enchant/ 5All software mentioned here is open-source. 11 the other. Spell checking algorithms benefit from years of optimization in both areas. This first filter uses a threshold to discriminate between short messages, expressed as a percentage of tokens which do not pass the spell check. The filter also relies on software biases, like Unicode errors, which make it nearly certain that the given input microtext is not English. langid.py A language identification tool is used to classify the web documents and to benchmark the efficiency of the test mentioned above. langid.py (Lui and Baldwin, 2011; Lui and Baldwin, 2012) is open-source, it incorporates a pre-trained model and it covers 97 languages, which is ideal to tackle the diversity of the web. Its use as a web service makes it a fast solution enabling distant or distributed work. The server version of langid.py was used, the texts were downloaded, all the HTML markup was stripped and the resulting text was discarded if it was less than 1,000 characters long. According to its authors, langid.py could be used directly on microtexts. However, this feature was discarded because it did not prove as efficient as the approach used here </context>
</contexts>
<marker>Lui, Baldwin, 2011</marker>
<rawString>Marco Lui and Timothy Baldwin. 2011. Cross-domain Feature Selection for Language Identification. In Proceedings of the Fifth International Joint Conference on Natural Language Processing (IJCNLP 2011), pages 553–561, Chiang Mai, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Lui</author>
<author>Timothy Baldwin</author>
</authors>
<title>langid.py: An Off-the-shelf Language Identification Tool.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012), Jeju,</booktitle>
<location>Republic of</location>
<contexts>
<context position="12836" citStr="Lui and Baldwin, 2012" startWordPosition="2086" endWordPosition="2089">ant/ 5All software mentioned here is open-source. 11 the other. Spell checking algorithms benefit from years of optimization in both areas. This first filter uses a threshold to discriminate between short messages, expressed as a percentage of tokens which do not pass the spell check. The filter also relies on software biases, like Unicode errors, which make it nearly certain that the given input microtext is not English. langid.py A language identification tool is used to classify the web documents and to benchmark the efficiency of the test mentioned above. langid.py (Lui and Baldwin, 2011; Lui and Baldwin, 2012) is open-source, it incorporates a pre-trained model and it covers 97 languages, which is ideal to tackle the diversity of the web. Its use as a web service makes it a fast solution enabling distant or distributed work. The server version of langid.py was used, the texts were downloaded, all the HTML markup was stripped and the resulting text was discarded if it was less than 1,000 characters long. According to its authors, langid.py could be used directly on microtexts. However, this feature was discarded because it did not prove as efficient as the approach used here when it comes to a subst</context>
</contexts>
<marker>Lui, Baldwin, 2012</marker>
<rawString>Marco Lui and Timothy Baldwin. 2012. langid.py: An Off-the-shelf Language Identification Tool. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012), Jeju, Republic of Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roland Sch¨afer</author>
<author>Felix Bildhauer</author>
</authors>
<title>Building large corpora from the web using a new efficient tool chain.</title>
<date>2012</date>
<booktitle>In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12),</booktitle>
<pages>486--493</pages>
<marker>Sch¨afer, Bildhauer, 2012</marker>
<rawString>Roland Sch¨afer and Felix Bildhauer. 2012. Building large corpora from the web using a new efficient tool chain. In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12), pages 486–493.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roland Sch¨afer</author>
<author>Adrien Barbaresi</author>
<author>Felix Bildhauer</author>
</authors>
<title>The Good, the Bad, and the Hazy: Design Decisions in Web Corpus Construction.</title>
<date>2013</date>
<booktitle>In Proceedings of the 8th Web as Corpus Workshop (WAC8).</booktitle>
<note>To appear.</note>
<marker>Sch¨afer, Barbaresi, Bildhauer, 2013</marker>
<rawString>Roland Sch¨afer, Adrien Barbaresi, and Felix Bildhauer. 2013. The Good, the Bad, and the Hazy: Design Decisions in Web Corpus Construction. In Proceedings of the 8th Web as Corpus Workshop (WAC8). To appear.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>