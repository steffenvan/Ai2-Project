<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002711">
<title confidence="0.9989765">
IITB-Sentiment-Analysts: Participation in Sentiment Analysis
in Twitter SemEval 2013 Task
</title>
<author confidence="0.997775">
Karan Chawla, Ankit Ramteke, Pushpak Bhattacharyya
</author>
<affiliation confidence="0.999161">
Dept. of Computer Science and Engineering, IIT Bombay
</affiliation>
<email confidence="0.974251">
{chawlakaran,ankitr,pb}@cse.iitb.ac.in
</email>
<bodyText confidence="0.855392">
and satellite in the sentence.
</bodyText>
<sectionHeader confidence="0.978353" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997018285714286">
We propose a method for using discourse rela-
tions for polarity detection of tweets. We have
focused on unstructured and noisy text like
tweets on which linguistic tools like parsers and
POS-taggers don’t work properly. We have
showed how conjunctions, connectives, modals
and conditionals affect the sentiments in tweets.
We have also handled the commonly used ab-
breviations, slangs and collocations which are
usually used in short text messages like tweets.
This work focuses on a Web based application
which produces results in real time. This ap-
proach is an extension of the previous work
(Mukherjee et al. 2012).
</bodyText>
<sectionHeader confidence="0.99876" genericHeader="introduction">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999880461538462">
Discourse relation is an important component of
natural language processing which connects
phrases and clauses together to establish a cohe-
rent relation. Linguistic constructs like conjunc-
tions, connectives, modals, conditionals and ne-
gation do alter the sentiments of a sentence. For
example, the movie had quite a few memorable
moments but I still did not like it. The overall
polarity of the sentence is negative even though
it has one positive and one negative clause. This
is because of the presence of the conjunction but
which gives more weightage to the clause fol-
lowing the conjunction.
Traditional works in discourse analysis use a
discourse parser (Marcu et al., 2003; Polanyi et
al., 2004; Wolf et al., 2005; Welner et al., 2006;
Narayanan et al., 2009; Prasad et al., 2010).
Many of these works and some other works in
discourse (Taboada et al., 2008; Zhou et al.,
2011) build on the Rhetorical Structure Theory
(RTS) proposed by Mann et al. (1988) which
tries to identify the relations between the nucleus
Most of the work is based on well-structured text
and the methods applied on that text is not suita-
ble for the discourse analysis on micro-blogs
because of the following reasons:
</bodyText>
<listItem confidence="0.63186025">
1. Micro-blogs like Twitter restricts a post
(tweet) to be of only 140 characters. Thus, users
do not use formal language to discuss their
views. Thus, there are abundant spelling mis-
</listItem>
<bodyText confidence="0.985588">
takes, abbreviations, slangs, collocations, discon-
tinuities and grammatical errors.
These differences cause NLP tools like POS-
taggers and parsers to fail frequently, as these
tools are built for well-structured text. Thus,
most of the methods described in the previous
works are not well suited for discourse analysis
on Micro-blogs like text.
2. The web-based applications require a
fast response time. Using a heavy linguistic re-
source like parsing increases the processing time
and slows down the application.
Most of the previous work on discourse analysis
does not take into consideration the conjunc-
tions, connectives, modals, conditionals etc and
are based on bag-of-words model with features
like part-of-speech information, unigrams, bi-
grams etc. along with other domain-specific fea-
tures like emoticons, hashtags etc. Our work
harness the importance of discourse connectives
like conjunctions, connectives, modals, condi-
tionals etc and show that along with bag-of-
words model, it gives better sentiment classifica-
tion accuracy. This work is the extension of
(Mukherjee et al. 2012).
The roadmap for the rest of the paper is as fol-
lows: Section 2 studies the effect of discourse
relations on sentiment analysis and identifies the
</bodyText>
<page confidence="0.986136">
495
</page>
<bodyText confidence="0.9851297">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 495–500, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
critical ones. Section 3 talks about the semantic
operators which influence the discourse rela-
tions. Section 4 discusses the lexicon based clas-
sification approach. Section 5 describes the fea-
ture engineering of the important features. Sec-
tion 6 gives the list of experiments conducted
and analysis of the results. Conclusion and Fu-
ture Work is presented in Section 7.
</bodyText>
<sectionHeader confidence="0.941506" genericHeader="method">
2. Discourse Relations Critical for Sen-
</sectionHeader>
<subsectionHeader confidence="0.821058">
timent Analysis
</subsectionHeader>
<bodyText confidence="0.999459212121212">
(Mukherjee et al. 2012) showed that that the fol-
lowing discourse relations are critical for SA as
all relations are not useful for SA. Table 1 pro-
vides examples of various discourse relations.
Violated Expectations and Contrast: In Exam-
ple 2, a simple bag-of-words feature based clas-
sifier will classify it as positive. However, it ac-
tually represents a negative sentiment. Such cas-
es need to be handled separately. In Example 5,
“memorable&amp;quot; has (+1) score and “not like&amp;quot; has (-
1) score and overall polarity is 0 or objective
whereas it should be negative as the final verdict
following “but&amp;quot; is the deciding factor.
These kinds of sentences refute the neighboring
clause. They can be classified as Conj_Prev in
which the clause preceding the conjunction is
preferred and Conj_Fol in which the clause fol-
lowing the conjunction is preferred.
Conclusive or Inferential Conjunctions: These
are the set of conjunctions, Conj_infer, that tend
to draw a conclusion or inference. Hence, the
discourse segment following them (subsequently
in Example 11) should be given more weight.
Conditionals: In Example 3, “amazing&amp;quot;
represent a positive sentiment. But the final po-
larity should be objective as we are talking of a
hypothetical situation.
Other Discourse Relations: Sentences under
Cause-Effect, Similarity, Temporal Sequence,
Attribution, Example, Generalization and Elabo-
ration, provide no contrasting, conflicting or hy-
pothetical information. They can be handled by
taking a simple bag-of-words model.
</bodyText>
<sectionHeader confidence="0.6496405" genericHeader="method">
3. Semantic Operators Influencing Dis-
course Relations
</sectionHeader>
<bodyText confidence="0.999863666666667">
There are connectives or semantic operators
present in the sentences which influence the dis-
course relation within a sentence. For example,
in the sentence the cannon camera may bad de-
spite good battery life. The connective despite
increases the weightage of the previous dis-
course element i.e. bad is weighted up but may
introduces a certain kind of uncertainty which
cannot be ignored.
</bodyText>
<listItem confidence="0.99513815">
1. (I did not study anything throughout the seme-
ster), so (I failed in the exams).
2. (Sourav failed to deliver in the penultimate test)
despite (great expectations).
3. If (I had bought the amazing Nokia phone), I
would not be crying).
4. (I love Cannon) and (I also love Sony).
5. (The movie had quite a few memorable moments)
but (I still did not like it).
6. (The theater became interesting) after a while.
7. According (to the reviews), (the movie must be
bad).
8. (Salman is a bad guy), for instance (he is always
late).
9. In addition (to the bad battery life), (the camera
is also very costly).
10. In general, (cameras from cannon (take great
pictures).
11. (They were not in favour of that camera) and
subsequently (decided not to buy it).
</listItem>
<tableCaption confidence="0.722991">
Table 1: Examples of Discourse Coherent
Relations
</tableCaption>
<bodyText confidence="0.9992865">
Similarity, in the sentence He gave his best in
the movie, but still it was not good enough to win
an Oscar. The connective but increases the
weight of the following discourse i.e. good and
win are weighted up but presence of negation
operator also cannot be ignored.
</bodyText>
<page confidence="0.993034">
496
</page>
<listItem confidence="0.93284">
1. Modals: Events that are happening or are
</listItem>
<bodyText confidence="0.908099047619047">
bound to happen are called realis events. And
those events that have possibly occurred or have
some probability to occur in distant future are
known as irrealis events. And it is important to
distinguish between the two as it also alters the
sentiments in a piece of text. Modals depict ir-
realis events and just cannot be handled by sim-
ple majority valence model.
(Mukherjee et al. 2012) divided modals into two
categories: Strong_Mod and Weak_Mod.
Strong_Mod is the set of modals that express a
higher degree of uncertainty in any situation.
Weak_Mod is the set of modals that express
lesser degree of uncertainty and more emphasis
on certain events or situations.
Like conditionals, sentences with strong modals
express higher degree of uncertainty, thus dis-
course elements near strong modals are weighted
down. Thus, in the previous example the cannon
camera may bad despite good battery life bad is
toned down.
</bodyText>
<listItem confidence="0.881594">
2. Negation: The negation operator inverts the
polarity of the sentence following it. Usually, to
</listItem>
<bodyText confidence="0.987646333333333">
handle negation a window (typically 3-5 words)
is considered and the polarities of all the words
are reversed. We have considered the window
size to be 5 and reverse the polarities of all the
words within the window, till either a conjunc-
tion comes or window size exceeds. For example
In the sentence He gave his best in the movie,
but still it was not good enough to win an Oscar
polarities of good and win are reversed.
</bodyText>
<sectionHeader confidence="0.959402" genericHeader="method">
4. Lexicon Based Classification
</sectionHeader>
<bodyText confidence="0.999945285714286">
We have used Senti-WordNet (Esuli et al. 2006),
Inquirer (Stone et. al 1996) and the Bing Liu
sentiment lexicon (Hu et al. 2004) to find out the
word polarities. To compensate the bias effects
introduced by the individual lexicons, we have
used three different lexicons. The polarities of
the reviews are given by (Mukherjee et al. 2012)
</bodyText>
<equation confidence="0.986485">
m ni
sign ( I Ifij * f lipij * p(wij ))
i=1
i=1
</equation>
<table confidence="0.998863294117647">
Relations Attributes
Conj_Fol but, however, never-
theless, otherwise, yet,
still, nonetheless
Conj_Prev till, until, despite, in
spite, though, although
Conj_Inf therefore, furthermore,
consequently, thus, as
a result, subsequently,
eventually, hence
Conditionals If
Strong_Mod might, could, can,
would, may
Weak_Mod should, ought to, need
not, shall, will, must
Neg not, neither, never, no,
nor
</table>
<tableCaption confidence="0.867559">
Table 2: Discourse Relations and Semantic
Operators Essential for Sentiment Analysis
</tableCaption>
<bodyText confidence="0.997552076923077">
where p (wij) = pol (wij ) if hypij = 0
pol(wij )
= 2 if hypij = 1
Above equation finds the weighted, signed po-
larity of a review. The polarity of each word,
pol(wij) being +1 or -1, is multiplied with its dis-
course weight fij and all the weighted polarities
are added. Flipij indicates if the polarity of wij is
to be negated.
In case there is any conditional or strong modal
in the sentence (indicated by hypij = 1 ), then
the polarity of every word in the sentence is
toned down, by considering half of its assigned
</bodyText>
<equation confidence="0.963767">
polarity (+1 −1)
2 , 2
</equation>
<bodyText confidence="0.9983532">
Thus, if good occurs in the user post twice, it
will contribute a polarity of +1 X 2 = +2 to the
overall review polarity, if hypij = 0. In the
presence of a strong modal or conditional, it will
contribute a polarity of +12 * 2 = +1.
</bodyText>
<page confidence="0.996966">
497
</page>
<bodyText confidence="0.999950333333333">
All the stop words, discourse connectives and
modals are ignored during the classification
phase, as they have a zero polarity in the lexicon.
We have handled commonly used slangs, ab-
breviations and collocations by manually tagging
them as positive, negative or neutral.
</bodyText>
<sectionHeader confidence="0.983857" genericHeader="method">
5. Feature Engineering
</sectionHeader>
<bodyText confidence="0.994788648648649">
The features specific for lexicon based classifi-
cation for the task sentiment Analysis, identified
in Section 2.4, are handled as follows:
a) The words following the Conj_Fol (Table 2)
are given more weightage. Hence their frequency
count is incremented by 1.
We follow a naive weighting scheme whereby
we give a (+1) weightage to every word we con-
sider important. In Example 5, “memorable&amp;quot; gets
(+1) score, while “did not like&amp;quot; gets a (-2) score,
making the overall score (-1) i.e. the example
suggests a negative sentiment.
b) The weightage of the words occurring before
the Conj_Prev (Table 2) is increased by 1. In
Example 2, “failed&amp;quot; will have polarity (-2) in-
stead of (-1) and “great expectations&amp;quot; will have
polarity (+1), making the overall polarity (-1),
which conforms to the overall sentiment.
c) The weightage of the words in the sentences
containing conditionals (if) and strong modals
(might, could, can, would, may) are toned down.
e) The polarity of all words appearing within a
window of 5 from the occurrence of a negation
operator (not, neither, nor, no, never) and before
the occurrence of a violating expectation con-
junction is reversed.
f) Exploiting sentence position information, the
words appearing in the first k and last k sen-
tences, are given more weightage. The value of k
is set empirically.
g) The Negation Bias factor is treated as a para-
meter which is learnt from a small set of nega-
tive polarity tagged documents. The frequency
count of all the negative words (in a rule based
system) is multiplied with this factor to give
negative words more weightage than positive
words.
</bodyText>
<sectionHeader confidence="0.98284" genericHeader="evaluation">
6. Experiments and Evaluation
</sectionHeader>
<bodyText confidence="0.998700666666667">
For the lexicon-based approach, we performed
two types of experiments- sentiment pertaining
to a particular instance in a tweet (SemEval-
2013 Task A) and generic sentiment analysis of
a tweet (SemEval-2013 Task B). We treat both
the tasks similarly.
</bodyText>
<subsectionHeader confidence="0.992661">
6.1 Dataset
</subsectionHeader>
<bodyText confidence="0.999556">
We performed experiments on two Datasets:
</bodyText>
<listItem confidence="0.991952">
1) SemEval-2013-task 2 Twitter Dataset A con-
taining 4435 tweets without any external data.
2) SemEval-2013-task 2 Twitter Dataset B con-
taining 3813 tweets without any external data.
</listItem>
<subsectionHeader confidence="0.991279">
6.2 Results on the Twitter Dataset A and B
</subsectionHeader>
<bodyText confidence="0.999608333333333">
The system performs best for the positive class
tweets as shown in Table 3 and Table 4 and per-
forms badly for the negative class which is due
to the fact that negative tweets can contain sar-
casm which is a difficult phenomenon to capture.
Also the results of the neutral category are very
less which suggests that our system is biased
towards subjective tweets and we wish to give
the majority sentiment in the tweets.
</bodyText>
<table confidence="0.9991835">
Class Precision Recall F-score
Positive 0.6706 0.5958 0.6310
Negative 0.4124 0.5328 0.4649
Neutral 0.0667 0.0063 0.0114
</table>
<tableCaption confidence="0.998831">
Table 3: Results on Twitter Dataset A
</tableCaption>
<table confidence="0.9998145">
Class Precision Recall F-score
Positive 0.4809 0.5941 0.5316
Negative 0.1753 0.5374 0.2643
Neutral 0.6071 0.0104 0.0204
</table>
<tableCaption confidence="0.999703">
Table 4: Results on Twitter Dataset B
</tableCaption>
<page confidence="0.996586">
498
</page>
<subsectionHeader confidence="0.819911">
6.3 Discussion
</subsectionHeader>
<bodyText confidence="0.999976818181818">
The lexicon based classifier suffers from the
problem of lexeme space where it is not able
handle all the word senses. Also, short-noisy text
like tweets often contain various spelling mis-
takes like great can be grt, g8t etc. or tomorrow
can be tom, tomm, tommrrw etc. which will not
be detected and handled properly.
We suggest that a supervised approach compris-
ing of the discourse features along with the bag-
of-words model and the sense based features will
improve the results.
</bodyText>
<sectionHeader confidence="0.959704" genericHeader="conclusions">
7. Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999998">
We have showed that discourse connectives,
conjunctions, negations and conditionals do alter
the sentiments of a piece of text. Most of the
work on Micro-blogs like twitter is build on bag-
of-words model and does not incorporate dis-
course relations. We discussed an approach
where we can incorporate discourse relations
along-with bag-of-words model for a web-
application where parsers and taggers cannot be
used as the results are required in real time.
We need to take into consideration word senses
and a supervised approach to use all the features
collectively. Also, a spell checker would really
help in the noisy text like in tweets.
</bodyText>
<sectionHeader confidence="0.998859" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998843545454545">
A Agarwal and Pushpak Bhattacharyya. 2005. Senti-
ment Analysis: A New Approach for Effective Use of
Linguistic Knowledge and Exploiting Similarities in a
Set of Documents to be classified. International Con-
ference on Natural Language Processing (ICON 05),
IIT Kanpur, India, December
AR Balamurali, Aditya Joshi and Pushpak Bhattacha-
ryya. 2011. Harnessing WordNet Senses for Super-
vised Sentiment Classification. In Proceedings of
Empirical Methods in Natural Language Processing
(EMNLP).
A Esuli and F Sebastiani, 2006. SentiWordNet: A
Publicly Available Lexical Resource for Opinion
Mining. In Proceedings from International Confe-
rence on Language Resources and Evaluation
(LREC), Genoa.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proc. of ACM SIGKDD.
Aditya Joshi, AR Balamurali, Pushpak Bhattacharyya
and R Mohanty. 2010. C-Feel-It: A Sentiment Ana-
lyzer for Micro-blogs&apos;, Annual Meeting of the Associ-
ation of Computational Linguistics (ACL 2011), Ore-
gon, USA.
William C. Mann and Sandra A. Thompson. Rhetori-
cal Structure Theory: Toward a functional theory of
text organization. Text, 8 (3), 243-281. 1988
R Narayanan, Bing Liu and A Choudhary. 2009. Sen-
timent Analysis of Conditional Sentences. In Pro-
ceedings of Conference on Empirical Methods in
Natural Language Processing (EMNLP-09).
L Polanyi and A Zaenen. 2004. Contextual Valence
Shifters. In James G. Shanahan, Yan Qu, Janyce
Wiebe (eds.), Computing Attitude and Affect in Text:
Theory and Applications, pp. 1-10.
BP Ramesh, R Prasad and H Yu. 2010. Identifying
explicit discourse connective in biomedical text. In
Annual Symposium proceedings, AMIA Symposium,
Vol. 2010, pp. 657-661.
R Soricut and D Marcu. 2003. Sentence level dis-
course parsing using syntactic and lexical informa-
tion. In Proc. of HLT-NAACL
PJ Stone, DC Dunphy, MS Smith, DM Ogilvie and
Associates. 1996. The General Inquirer: A Computer
Approach to Content Analysis. The MIT Press
Subhabrata Mukherjee and Pushpak Bhattacharyya.
2012. Sentiment Analysis in Twitter with Lightweight
Discourse Analysis. In Proceedings of COLING
2012
Subhabrata Mukherjee and Pushpak Bhattacharyya.
2012. Sentiment Analysis in Twitter with Lightweight
Discourse Analysis. In Proceedings of the 21st ACM
Conference on Information and Knowledge Manage-
ment (CIKM), short paper.
Subhabrata Mukherjee, AR Balamurali, Akshat Malu
and Pushpak Bhattacharyya. 2012. TwiSent: A Ro-
</reference>
<page confidence="0.989717">
499
</page>
<reference confidence="0.998321210526316">
bust Multistage System for Analyzing Sentiment on
Twitter. In Proceedings of the 21st ACM Conference
on Information and Knowledge Management (CIKM),
poster paper.
Maite Taboada, Julian Brooke and Kimberly Voll.
2008. Extracting Sentiment as a Function of Dis-
course Structure and Topicality. Simon Fraser Unive-
risty School of Computing Science Technical Report.
B Wellner, J Pustejovski, A Havasi, A Rumshiskym
and R Suair. 2006. Classification of discourse cohe-
rence relations: An exploratory study using multiple
knowledge sources. In Proc. of SIGDIAL
F Wolf and E Gibson. 2005. Representing Discourse
Coherence: A Corpus-based Study. Computational
Linguistics, 31(2), pp. 249-287.
Lanjun Zhou, Binyang Li, Wei Gao, Zhongyu Wei
and Kam-Fai Wong. 2011. Unsupervised discovery of
discourse relations for eliminating intra-sentence po-
larity ambiguities. In Proceedings of EMNLP.
</reference>
<page confidence="0.995599">
500
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.293204">
<title confidence="0.949847">IITB-Sentiment-Analysts: Participation in Sentiment Analysis in Twitter SemEval 2013 Task</title>
<author confidence="0.888192">Ankit Ramteke Chawla</author>
<author confidence="0.888192">Bhattacharyya</author>
<affiliation confidence="0.980979">Dept. of Computer Science and Engineering, IIT Bombay</affiliation>
<abstract confidence="0.9424564375">{chawlakaran,ankitr,pb}@cse.iitb.ac.in and satellite in the sentence. Abstract We propose a method for using discourse relations for polarity detection of tweets. We have focused on unstructured and noisy text like tweets on which linguistic tools like parsers and don’t work properly. We have showed how conjunctions, connectives, modals and conditionals affect the sentiments in tweets. We have also handled the commonly used abbreviations, slangs and collocations which are usually used in short text messages like tweets. This work focuses on a Web based application which produces results in real time. This approach is an extension of the previous work</abstract>
<note confidence="0.701283">(Mukherjee et al. 2012).</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Agarwal</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>Sentiment Analysis: A New Approach for Effective Use of Linguistic Knowledge and Exploiting Similarities in a Set of Documents to be classified.</title>
<date>2005</date>
<booktitle>International Conference on Natural Language Processing (ICON 05), IIT Kanpur,</booktitle>
<location>India,</location>
<marker>Agarwal, Bhattacharyya, 2005</marker>
<rawString>A Agarwal and Pushpak Bhattacharyya. 2005. Sentiment Analysis: A New Approach for Effective Use of Linguistic Knowledge and Exploiting Similarities in a Set of Documents to be classified. International Conference on Natural Language Processing (ICON 05), IIT Kanpur, India, December</rawString>
</citation>
<citation valid="true">
<authors>
<author>AR Balamurali</author>
<author>Aditya Joshi</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>Harnessing WordNet Senses for Supervised Sentiment Classification.</title>
<date>2011</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<marker>Balamurali, Joshi, Bhattacharyya, 2011</marker>
<rawString>AR Balamurali, Aditya Joshi and Pushpak Bhattacharyya. 2011. Harnessing WordNet Senses for Supervised Sentiment Classification. In Proceedings of Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Esuli</author>
<author>F Sebastiani</author>
</authors>
<title>SentiWordNet: A Publicly Available Lexical Resource for Opinion Mining.</title>
<date>2006</date>
<booktitle>In Proceedings from International Conference on Language Resources and Evaluation (LREC),</booktitle>
<location>Genoa.</location>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>A Esuli and F Sebastiani, 2006. SentiWordNet: A Publicly Available Lexical Resource for Opinion Mining. In Proceedings from International Conference on Language Resources and Evaluation (LREC), Genoa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proc. of ACM SIGKDD.</booktitle>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proc. of ACM SIGKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aditya Joshi</author>
<author>AR Balamurali</author>
<author>Pushpak Bhattacharyya</author>
<author>R Mohanty</author>
</authors>
<title>C-Feel-It: A Sentiment Analyzer for Micro-blogs&apos;,</title>
<date>2010</date>
<booktitle>Annual Meeting of the Association of Computational Linguistics (ACL 2011),</booktitle>
<location>Oregon, USA.</location>
<marker>Joshi, Balamurali, Bhattacharyya, Mohanty, 2010</marker>
<rawString>Aditya Joshi, AR Balamurali, Pushpak Bhattacharyya and R Mohanty. 2010. C-Feel-It: A Sentiment Analyzer for Micro-blogs&apos;, Annual Meeting of the Association of Computational Linguistics (ACL 2011), Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical Structure Theory: Toward a functional theory of text organization.</title>
<date>1988</date>
<journal>Text,</journal>
<volume>8</volume>
<issue>3</issue>
<pages>243--281</pages>
<marker>Mann, Thompson, 1988</marker>
<rawString>William C. Mann and Sandra A. Thompson. Rhetorical Structure Theory: Toward a functional theory of text organization. Text, 8 (3), 243-281. 1988</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Narayanan</author>
<author>Bing Liu</author>
<author>A Choudhary</author>
</authors>
<title>Sentiment Analysis of Conditional Sentences.</title>
<date>2009</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP-09).</booktitle>
<contexts>
<context position="1671" citStr="Narayanan et al., 2009" startWordPosition="253" endWordPosition="256">t relation. Linguistic constructs like conjunctions, connectives, modals, conditionals and negation do alter the sentiments of a sentence. For example, the movie had quite a few memorable moments but I still did not like it. The overall polarity of the sentence is negative even though it has one positive and one negative clause. This is because of the presence of the conjunction but which gives more weightage to the clause following the conjunction. Traditional works in discourse analysis use a discourse parser (Marcu et al., 2003; Polanyi et al., 2004; Wolf et al., 2005; Welner et al., 2006; Narayanan et al., 2009; Prasad et al., 2010). Many of these works and some other works in discourse (Taboada et al., 2008; Zhou et al., 2011) build on the Rhetorical Structure Theory (RTS) proposed by Mann et al. (1988) which tries to identify the relations between the nucleus Most of the work is based on well-structured text and the methods applied on that text is not suitable for the discourse analysis on micro-blogs because of the following reasons: 1. Micro-blogs like Twitter restricts a post (tweet) to be of only 140 characters. Thus, users do not use formal language to discuss their views. Thus, there are abu</context>
</contexts>
<marker>Narayanan, Liu, Choudhary, 2009</marker>
<rawString>R Narayanan, Bing Liu and A Choudhary. 2009. Sentiment Analysis of Conditional Sentences. In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP-09).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Polanyi</author>
<author>A Zaenen</author>
</authors>
<title>Contextual Valence Shifters.</title>
<date>2004</date>
<booktitle>Computing Attitude and Affect in Text: Theory and Applications,</booktitle>
<pages>1--10</pages>
<editor>In James G. Shanahan, Yan Qu, Janyce Wiebe (eds.),</editor>
<marker>Polanyi, Zaenen, 2004</marker>
<rawString>L Polanyi and A Zaenen. 2004. Contextual Valence Shifters. In James G. Shanahan, Yan Qu, Janyce Wiebe (eds.), Computing Attitude and Affect in Text: Theory and Applications, pp. 1-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>BP Ramesh</author>
<author>R Prasad</author>
<author>H Yu</author>
</authors>
<title>Identifying explicit discourse connective in biomedical text.</title>
<date>2010</date>
<booktitle>In Annual Symposium proceedings, AMIA Symposium,</booktitle>
<volume>Vol.</volume>
<pages>657--661</pages>
<marker>Ramesh, Prasad, Yu, 2010</marker>
<rawString>BP Ramesh, R Prasad and H Yu. 2010. Identifying explicit discourse connective in biomedical text. In Annual Symposium proceedings, AMIA Symposium, Vol. 2010, pp. 657-661.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Soricut</author>
<author>D Marcu</author>
</authors>
<title>Sentence level discourse parsing using syntactic and lexical information.</title>
<date>2003</date>
<booktitle>In Proc. of HLT-NAACL</booktitle>
<marker>Soricut, Marcu, 2003</marker>
<rawString>R Soricut and D Marcu. 2003. Sentence level discourse parsing using syntactic and lexical information. In Proc. of HLT-NAACL</rawString>
</citation>
<citation valid="true">
<authors>
<author>PJ Stone</author>
<author>DC Dunphy</author>
<author>MS Smith</author>
<author>DM Ogilvie</author>
<author>Associates</author>
</authors>
<title>The General Inquirer: A Computer Approach to Content Analysis.</title>
<date>1996</date>
<publisher>The MIT Press</publisher>
<marker>Stone, Dunphy, Smith, Ogilvie, Associates, 1996</marker>
<rawString>PJ Stone, DC Dunphy, MS Smith, DM Ogilvie and Associates. 1996. The General Inquirer: A Computer Approach to Content Analysis. The MIT Press</rawString>
</citation>
<citation valid="true">
<authors>
<author>Subhabrata Mukherjee</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>Sentiment Analysis in Twitter with Lightweight Discourse Analysis.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING</booktitle>
<marker>Mukherjee, Bhattacharyya, 2012</marker>
<rawString>Subhabrata Mukherjee and Pushpak Bhattacharyya. 2012. Sentiment Analysis in Twitter with Lightweight Discourse Analysis. In Proceedings of COLING 2012</rawString>
</citation>
<citation valid="true">
<authors>
<author>Subhabrata Mukherjee</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>Sentiment Analysis in Twitter with Lightweight Discourse Analysis.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st ACM Conference on Information and Knowledge Management (CIKM), short</booktitle>
<pages>paper.</pages>
<marker>Mukherjee, Bhattacharyya, 2012</marker>
<rawString>Subhabrata Mukherjee and Pushpak Bhattacharyya. 2012. Sentiment Analysis in Twitter with Lightweight Discourse Analysis. In Proceedings of the 21st ACM Conference on Information and Knowledge Management (CIKM), short paper.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Subhabrata Mukherjee</author>
<author>AR Balamurali</author>
</authors>
<title>Akshat Malu and Pushpak Bhattacharyya.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st ACM Conference on Information and Knowledge Management (CIKM), poster</booktitle>
<pages>paper.</pages>
<marker>Mukherjee, Balamurali, 2012</marker>
<rawString>Subhabrata Mukherjee, AR Balamurali, Akshat Malu and Pushpak Bhattacharyya. 2012. TwiSent: A Robust Multistage System for Analyzing Sentiment on Twitter. In Proceedings of the 21st ACM Conference on Information and Knowledge Management (CIKM), poster paper.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maite Taboada</author>
<author>Julian Brooke</author>
<author>Kimberly Voll</author>
</authors>
<title>Extracting Sentiment as a Function of Discourse Structure and Topicality. Simon Fraser Univeristy School of Computing Science</title>
<date>2008</date>
<tech>Technical Report.</tech>
<contexts>
<context position="1770" citStr="Taboada et al., 2008" startWordPosition="271" endWordPosition="274">o alter the sentiments of a sentence. For example, the movie had quite a few memorable moments but I still did not like it. The overall polarity of the sentence is negative even though it has one positive and one negative clause. This is because of the presence of the conjunction but which gives more weightage to the clause following the conjunction. Traditional works in discourse analysis use a discourse parser (Marcu et al., 2003; Polanyi et al., 2004; Wolf et al., 2005; Welner et al., 2006; Narayanan et al., 2009; Prasad et al., 2010). Many of these works and some other works in discourse (Taboada et al., 2008; Zhou et al., 2011) build on the Rhetorical Structure Theory (RTS) proposed by Mann et al. (1988) which tries to identify the relations between the nucleus Most of the work is based on well-structured text and the methods applied on that text is not suitable for the discourse analysis on micro-blogs because of the following reasons: 1. Micro-blogs like Twitter restricts a post (tweet) to be of only 140 characters. Thus, users do not use formal language to discuss their views. Thus, there are abundant spelling mistakes, abbreviations, slangs, collocations, discontinuities and grammatical error</context>
</contexts>
<marker>Taboada, Brooke, Voll, 2008</marker>
<rawString>Maite Taboada, Julian Brooke and Kimberly Voll. 2008. Extracting Sentiment as a Function of Discourse Structure and Topicality. Simon Fraser Univeristy School of Computing Science Technical Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Wellner</author>
<author>J Pustejovski</author>
</authors>
<title>A Havasi, A Rumshiskym</title>
<date>2006</date>
<booktitle>In Proc. of SIGDIAL</booktitle>
<marker>Wellner, Pustejovski, 2006</marker>
<rawString>B Wellner, J Pustejovski, A Havasi, A Rumshiskym and R Suair. 2006. Classification of discourse coherence relations: An exploratory study using multiple knowledge sources. In Proc. of SIGDIAL</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Wolf</author>
<author>E Gibson</author>
</authors>
<title>Representing Discourse Coherence: A Corpus-based Study.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>2</issue>
<pages>249--287</pages>
<marker>Wolf, Gibson, 2005</marker>
<rawString>F Wolf and E Gibson. 2005. Representing Discourse Coherence: A Corpus-based Study. Computational Linguistics, 31(2), pp. 249-287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lanjun Zhou</author>
<author>Binyang Li</author>
<author>Wei Gao</author>
<author>Zhongyu Wei</author>
<author>Kam-Fai Wong</author>
</authors>
<title>Unsupervised discovery of discourse relations for eliminating intra-sentence polarity ambiguities.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1790" citStr="Zhou et al., 2011" startWordPosition="275" endWordPosition="278"> of a sentence. For example, the movie had quite a few memorable moments but I still did not like it. The overall polarity of the sentence is negative even though it has one positive and one negative clause. This is because of the presence of the conjunction but which gives more weightage to the clause following the conjunction. Traditional works in discourse analysis use a discourse parser (Marcu et al., 2003; Polanyi et al., 2004; Wolf et al., 2005; Welner et al., 2006; Narayanan et al., 2009; Prasad et al., 2010). Many of these works and some other works in discourse (Taboada et al., 2008; Zhou et al., 2011) build on the Rhetorical Structure Theory (RTS) proposed by Mann et al. (1988) which tries to identify the relations between the nucleus Most of the work is based on well-structured text and the methods applied on that text is not suitable for the discourse analysis on micro-blogs because of the following reasons: 1. Micro-blogs like Twitter restricts a post (tweet) to be of only 140 characters. Thus, users do not use formal language to discuss their views. Thus, there are abundant spelling mistakes, abbreviations, slangs, collocations, discontinuities and grammatical errors. These differences</context>
</contexts>
<marker>Zhou, Li, Gao, Wei, Wong, 2011</marker>
<rawString>Lanjun Zhou, Binyang Li, Wei Gao, Zhongyu Wei and Kam-Fai Wong. 2011. Unsupervised discovery of discourse relations for eliminating intra-sentence polarity ambiguities. In Proceedings of EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>