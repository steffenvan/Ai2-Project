<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002052">
<title confidence="0.988406">
Chinese Word Segmentation as LMR Tagging
</title>
<author confidence="0.997364">
Nianwen Xue
</author>
<affiliation confidence="0.882107">
Inst. for Research in Cognitive Science
University of Pennsylvania
Philadelphia, PA 19104, USA
</affiliation>
<email confidence="0.992686">
xueniwen@linc.cis.upenn.edu
</email>
<author confidence="0.99554">
Libin Shen
</author>
<affiliation confidence="0.882727333333333">
Dept. of Computer and Info. Science
University of Pennsylvania
Philadelphia, PA 19104, USA
</affiliation>
<email confidence="0.994539">
libin@linc.cis.upenn.edu
</email>
<sectionHeader confidence="0.5842545" genericHeader="abstract">
Abstract
1 Segmentation as Tagging
</sectionHeader>
<bodyText confidence="0.999620352112677">
Unlike English text in which sentences are se-
quences of words delimited by white spaces, in Chi-
nese text, sentences are represented as strings of
Chinese characters or hanzi without similar natural
delimiters. Therefore, the first step in a Chinese lan-
guage processing task is to identify the sequence of
words in a sentence and mark boundaries in appro-
priate places. This may sound simple enough but in
reality identifying words in Chinese is a non-trivial
problem that has drawn a large body of research in
the Chinese language processing community (Fan
and Tsai, 1988; Gan et al., 1996; Sproat et al., 1996;
Wu, 2003; Xue, 2003).
The key to accurate automatic word identification
in Chinese lies in the successful resolution of ambi-
guities and aproper way to handle out-of-vocabulary
words. The ambiguities in Chinese word segmenta-
tion is due to the fact that a hanzi can occur in differ-
ent word-internal positions (Xue, 2003). Given the
proper context, generally provided by the sentence
in which it occurs, the position of a hanzi can be de-
termined. In this paper, we model the Chinese word
segmentation as a hanzi tagging problem and use a
machine-learning algorithm to determine the appro-
priate position for a hanzi. There are several reasons
why we may expect this approach to work. First,
Chinese words generally have fewer than four char-
acters. As a result, the number of positions is small.
Second, although each hanzi can in principle occur
in all possible positions, not all hanzi behave this
way. A substantial number of hanzi are distributed
in a constrained manner. For example, , the plu-
ral marker, almost always occurs in the word-final
position. Finally, although Chinese words cannot be
exhaustively listed and new words are bound to oc-
cur in naturally occurring text, the same is not true
for hanzi. The number of hanzi stays fairly constant
and we do not generally expect to see new hanzi.
We represent the positions of a hanzi with four
different tags (Table 1): LM for a hanzi that oc-
curs on the left periphery of a word, followed by
other hanzi, MM for a hanzi that occurs in the mid-
dle of a word, MR for a hanzi that occurs on the
right periphery of word, preceded by other hanzi,
and LR for hanzi that is a word by itself. We call
this LMR tagging. With this approach, word seg-
mentation is a process where each hanzi is assigned
an LMR tag and sequences of hanzi are then con-
verted into sequences of words based on the LMR
tags. The use of four tags is linguistically intuitive
in that LM tags morphemes that are prefixes or stems
in the absence of prefixes, MR tags morphemes that
are suffixes or stems in the absence of suffixes, MM
tags stems with affixes and LR tags stems without
affixes. Representing the distributions of hanzi with
LMR tags also makes it easy to use machine learn-
ing algorithms which has been successfully applied
to other tagging problems, such as POS-tagging and
IOB tagging used in text chunking.
In this paper we present Chinese word
segmentation algorithms based on the so-
called LMR tagging. Our LMR taggers
are implemented with the Maximum En-
tropy Markov Model and we then use
Transformation-Based Learning to com-
bine the results of the two LMR taggers
that scan the input in opposite directions.
Our system achieves F-scores of
and on the Academia Sinica corpus
and the Hong Kong City University corpus
respectively.
</bodyText>
<table confidence="0.989244666666667">
Right Boundary (R) Not Right Boundary (M)
Left Boundary (L) LR LM
Not Left Boundary (M) MR MM
</table>
<tableCaption confidence="0.998326">
Table 1: LMR Tagging
</tableCaption>
<sectionHeader confidence="0.870087" genericHeader="method">
2 Tagging Algorithms
</sectionHeader>
<bodyText confidence="0.9999865">
Our algorithm consists of two parts. We first imple-
ment two Maximum Entropy taggers, one of which
scans the input from left to right and the other scans
the input from right to left. Then we implement a
Transformation Based Algorithm to combine the re-
sults of the two taggers.
</bodyText>
<subsectionHeader confidence="0.988353">
2.1 The Maximum Entropy Tagger
</subsectionHeader>
<bodyText confidence="0.9925076">
The Maximum Entropy Markov Model (MEMM)
has been successfully used in some tagging prob-
lems. MEMM models are capable of utilizing a
large set of features that generative models cannot
use. On the other hand, MEMM approaches scan
the input incrementally as generative models do.
The Maximum Entropy Markov Model used in
POS-tagging is described in detail in (Ratnaparkhi,
1996) and the LMR tagger here uses the same prob-
ability model. The probability model is defined over
, where is the set of possible contexts or
”histories” and is the set of possible tags. The
model’s joint probability of a history and a tag is
defined as
where is a normalization constant,
are the model parameters and are known
as features, where . Each fea-
ture has a corresponding parameter , that ef-
fectively serves as a ”weight” of this feature. In
the training process, given a sequence of characters
and their LMR tags as train-
ing data, the purpose is to determine the parameters
that maximize the likelihood of the
training data using :
The success of the model in tagging depends to
a large extent on the selection of suitable features.
Given , a feature must encode information that
helps to predict . The features we used in our ex-
periments are instantiations of the feature templates
in (1). Feature templates (b) to (e) represent charac-
ter features while (f) represents tag features. In the
following list, are characters and
are LMR tags.
),
the previous two characters ( ), and
the next two characters ( )
(e) The previous and the next character ( )
(f) The tag of the previous character ( ), and
the tag of the character two before the current
character ( )
</bodyText>
<subsectionHeader confidence="0.979014">
2.2 Transformation-Based Learning
</subsectionHeader>
<bodyText confidence="0.999797588235294">
One potential problem with the MEMM is that it
can only scan the input in one direction, from left
to right or from right to left. It is noted in (Lafferty
et al., 2001) that non-generative finite-state models,
MEMM models included, share a weakness which
they call the Label Bias Problem (LBP): a transition
leaving a given state compete only against all other
transitions in the model. They proposed Conditional
Random Fields (CRFs) as a solution to address this
problem.
A partial solution to the LBP is to compute the
probability of transitions in both directions. This
way we can use two MEMM taggers, one of which
scans the input from left to right and the other scans
the input from right to left. This strategy has been
successfully used in (Shen and Joshi, 2003). In that
paper, pairwise voting (van Halteren et al., 1998) has
</bodyText>
<figure confidence="0.994759571428571">
(1) Feature templates
(a) Default feature
(b) The current character ( )
(c) The previous (next) two characters
( , , , )
(d) The previous (next) character and the current
character ( ,
</figure>
<bodyText confidence="0.9858812">
been used to combine the results of two supertaggers
that scan the input in the opposite directions.
The pairwise voting is not suitable in this appli-
cation because we must make sure that the LMR
tags assigned to consecutive words are compatible.
For example, an LM tag cannot immediately follow
an MM. Pairwise voting does not use any contex-
tual information, so it cannot prevent incompatible
tags from occurring. Therefore, in our experiments
described here, we use the Transformation-Based
Learning (Brill, 1995) to combine the results of two
MEMM taggers. The feature set used in the TBL al-
gorithm is similar to those used in the NP Chunking
task in (Ngai and Florian, 2001).
❳
</bodyText>
<sectionHeader confidence="0.997879" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.959494714285715">
We conducted closed track experiments on three
data sources: the Academia Sinica (AS) corpus,
the Beijing University (PKU) corpus and the Hong
Kong City University (CityU) corpus. We first split
the training data from each of the three sources into
two portions. of the official training data is
used to train the MEMM taggers, and the other
is held out as the development test data (the devel-
opment set). The development set is used to esti-
mate the optimal number of iterations in the MEMM
training. Figure (1), (2) and (3) show the curves of
F-scores on the development set with respect to the
number of iterations in MEMM training.
iteration
</bodyText>
<figureCaption confidence="0.879086">
Figure 1: Learning curves on the development
</figureCaption>
<bodyText confidence="0.8957335">
dataset of the Academia Sinica corpus. X-axis
stands for the number of iteration in training. Y-axis
stands for the -score.
Experiments show that the MEMM models
</bodyText>
<figure confidence="0.869414">
100 150 200 250 300
iteration
</figure>
<figureCaption confidence="0.907365">
Figure 2: Learning curves on the development
dataset of the HK City Univ. corpus.
</figureCaption>
<figure confidence="0.9581615">
200 300 400 500 600 700 800
iteration
</figure>
<figureCaption confidence="0.7432545">
Figure 3: Learning curves on the development
dataset of the Beijing Univ. corpus.
</figureCaption>
<bodyText confidence="0.99993205882353">
achieve the best results after 500 and 400 rounds (it-
erations) of training on the AS data and the PKU
data respectively. However, the results on the CityU
data is not very clear. From Round 100 through 200,
the F-score on the development set almost stays un-
changed. We think this is because the CityU data
is from three different sources, which differ in the
optimal number of iterations. We decided to train
the MEMM taggers for 160 iterations the HK City
University data.
We implemented two MEMM taggers, one scans
the input from left to right and one from right to
left. We then used these two MEMM taggers to tag
both the training and the development data. We use
the LMR tagging output to train a Transformation-
Based learner, using fast TBL (Ngai and Florian,
2001). The middle in Table 2 shows the F-score
</bodyText>
<figure confidence="0.998536808510639">
0.9595
AS
0.95945
0.9594
0.95935
0.9593
❳
F-score
c
❳
0.95925
0.9592
0.95915
0.9591
0.95905
0.959
200 300 400 500 600 700 800
0.9148
0.9146
0.9144
0.9142
0.914
0.9138
0.9136
0.9134
0.9132
0.913
0.9128
0.9126
HK
F-score
❳
sco
❳ 0.9386
0.9391
0.9389
0.9388
0.9387
0.9385
0.9384
0.9383
0.9382
0.9381
0.939
PK
F-score
❳
</figure>
<page confidence="0.590443">
c
</page>
<bodyText confidence="0.9998786">
on the development set achieved by the MEMM tag-
ger that scans the input from left to right and the
last column is the results after the Transformation-
Based Learner is applied. The results show that us-
ing Transformation-Based learning only give rise to
slight improvements. It seems that the bidirectional
approach does not help much for the LMR tagging.
Therefore, we only submitted the results of our left-
to-right MEMM tagger, retrained on the entire train-
ing sets, as our official results.
</bodyText>
<table confidence="0.9920635">
F-score MEMM MEMM+TBL
AS 0.9595 0.9603
HK 0.9143 N/A
PK 0.9391 0.9398
</table>
<tableCaption confidence="0.998321">
Table 2: F-score on development data
</tableCaption>
<bodyText confidence="0.999772076923077">
The results on the official test data is similar to
what we have got on our development set, except
that the F-score on the Beijing Univ. corpus is over
2 lower in absolute accuracy than what we ex-
pected. The reason is that in the training data of
Beijing University corpus, all the numbers are en-
coded in GBK, while in the test data many numbers
are encoded in ASCII, which are unknown to our
tagger. With this problem fixed, the results of the
official test data are compatible with the results on
our development set. However, we have withdrawn
our segmentation results on the Beijing University
corpus.
</bodyText>
<table confidence="0.992248666666667">
corpus R P F
AS 0.961 0.958 0.959 0.729 0.966
HK 0.917 0.915 0.916 0.670 0.936
</table>
<tableCaption confidence="0.995311">
Table 3: Official Bakeoff Outcome
</tableCaption>
<sectionHeader confidence="0.981237" genericHeader="conclusions">
4 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.998668">
Our closed track experiments on the first Sighan
Bakeoff data show that the LMR algorithm pro-
duces promising results. Our system ranks the sec-
ond when tested on the Academia Sinica corpus and
third on the Hong Kong City University corpus. In
the future, we will try to incorporate a large word list
into our tagger to test its performance on open track
experiments. Its high accuracy on makes it a
good candidate as a general purpose segmenter.
</bodyText>
<sectionHeader confidence="0.996168" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999379175">
E. Brill. 1995. Transformation-based error-driven learn-
ing and natural language processing: A case study
in part-of-speech tagging. Computational Linguistics,
21(4):543–565.
C. K. Fan and W. H. Tsai. 1988. Automatic word iden-
tification in chinese sentences by the relaxation tech-
nique. Computer Processing of Chinese and Oriental
Languages, 4(1):33–56.
Kok-Wee Gan, Martha Palmer, and Kim-Teng Lua. 1996.
A statistically emergent approach for language pro-
cessing: Application to modeling context effects in
ambiguous chinese word boundary perception. Com-
putational Linguistics, 22(4):531–53.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional random fields: Probabilistic models for stgmen-
tation and labeling sequence data. In Proceedings of
ICML 2001.
G. Ngai and R. Florian. 2001. Transformation-based
learning in the fast lane. In Proceedings of NAACL-
2001, pages 40–47.
Adwait Ratnaparkhi. 1996. A maximum entropy part-of-
speech tagger. In Proceedings of the Empirical Meth-
ods in Natural Language Processing Conference, Uni-
versity of Pennsylvania.
L. Shen and A. K. Joshi. 2003. A SNoW based supertag-
ger with application to NP chunking. In Proceedings
ofACL 2003.
R. Sproat, Chilin Shih, William Gale, and Nancy Chang.
1996. A stochastic finite-state word-segmentation
algorithm for chinese. Computational Linguistics,
22(3):377–404.
H. van Halteren, J. Zavrel, and W. Daelmans. 1998. Im-
proving data driven wordclass tagging by system com-
bination. In Proceedings of COLING-ACL 98.
Andi Wu. 2003. Customizable segmentation of mor-
phologically derived words in chinese. Computational
Linguistics and Chinese Language Processing.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. Computational Linguistics and
Chinese Language Processing.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.998643">Chinese Word Segmentation as LMR Tagging</title>
<author confidence="0.679524">Nianwen</author>
<affiliation confidence="0.9186795">Inst. for Research in Cognitive University of</affiliation>
<address confidence="0.926766">Philadelphia, PA 19104,</address>
<email confidence="0.99929">xueniwen@linc.cis.upenn.edu</email>
<affiliation confidence="0.986436">Libin Dept. of Computer and Info. University of</affiliation>
<address confidence="0.924106">Philadelphia, PA 19104,</address>
<email confidence="0.999802">libin@linc.cis.upenn.edu</email>
<abstract confidence="0.979899406698565">1 Segmentation as Tagging Unlike English text in which sentences are sequences of words delimited by white spaces, in Chinese text, sentences are represented as strings of characters or similar natural delimiters. Therefore, the first step in a Chinese language processing task is to identify the sequence of words in a sentence and mark boundaries in appropriate places. This may sound simple enough but in reality identifying words in Chinese is a non-trivial problem that has drawn a large body of research in the Chinese language processing community (Fan and Tsai, 1988; Gan et al., 1996; Sproat et al., 1996; Wu, 2003; Xue, 2003). The key to accurate automatic word identification in Chinese lies in the successful resolution of ambiguities and aproper way to handle out-of-vocabulary words. The ambiguities in Chinese word segmentais due to the fact that a occur in different word-internal positions (Xue, 2003). Given the proper context, generally provided by the sentence which it occurs, the position of a be determined. In this paper, we model the Chinese word as a problem and use a machine-learning algorithm to determine the approposition for a There are several reasons why we may expect this approach to work. First, Chinese words generally have fewer than four characters. As a result, the number of positions is small. although each in principle occur all possible positions, not all this A substantial number of distributed in a constrained manner. For example, , the plural marker, almost always occurs in the word-final position. Finally, although Chinese words cannot be exhaustively listed and new words are bound to occur in naturally occurring text, the same is not true The number of fairly constant we do not generally expect to see new represent the positions of a four tags (Table 1): LM for a occurs on the left periphery of a word, followed by MM for a occurs in the midof a word, MR for a occurs on the periphery of word, preceded by other LR for is a word by itself. We call this LMR tagging. With this approach, word segis a process where each assigned LMR tag and sequences of then converted into sequences of words based on the LMR tags. The use of four tags is linguistically intuitive in that LM tags morphemes that are prefixes or stems in the absence of prefixes, MR tags morphemes that are suffixes or stems in the absence of suffixes, MM tags stems with affixes and LR tags stems without Representing the distributions of LMR tags also makes it easy to use machine learning algorithms which has been successfully applied to other tagging problems, such as POS-tagging and IOB tagging used in text chunking. In this paper we present Chinese word segmentation algorithms based on the socalled LMR tagging. Our LMR taggers are implemented with the Maximum Entropy Markov Model and we then use Transformation-Based Learning to combine the results of the two LMR taggers that scan the input in opposite directions. Our system achieves F-scores of and on the Academia Sinica and the Hong Kong City University corpus respectively. Right Boundary (R) Not Right Boundary (M) Left Boundary (L) MR LM MM Not Left Boundary (M) Table 1: LMR Tagging 2 Tagging Algorithms Our algorithm consists of two parts. We first implement two Maximum Entropy taggers, one of which scans the input from left to right and the other scans the input from right to left. Then we implement a Transformation Based Algorithm to combine the results of the two taggers. 2.1 The Maximum Entropy Tagger The Maximum Entropy Markov Model (MEMM) has been successfully used in some tagging problems. MEMM models are capable of utilizing a large set of features that generative models cannot use. On the other hand, MEMM approaches scan the input incrementally as generative models do. The Maximum Entropy Markov Model used in POS-tagging is described in detail in (Ratnaparkhi, 1996) and the LMR tagger here uses the same probability model. The probability model is defined over , where is the set of possible contexts or ”histories” and is the set of possible tags. The model’s joint probability of a history and a tag is defined as where is a normalization constant, are the model parameters and are known features, where . Each feahas a corresponding parameter , that effectively serves as a ”weight” of this feature. In the training process, given a sequence of characters their LMR tags as ing data, the purpose is to determine the parameters that maximize the likelihood of the training data using : The success of the model in tagging depends to a large extent on the selection of suitable features. Given , a feature must encode information helps to predict . The features we used in our experiments are instantiations of the feature templates in (1). Feature templates (b) to (e) represent character features while (f) represents tag features. In the following list, are characters are LMR tags. ), the previous two characters ( ), and the next two characters ( ) (e) The previous and the next character ( ) (f) The tag of the previous character ( ), the tag of the character two before the current character ( ) 2.2 Transformation-Based Learning One potential problem with the MEMM is that it can only scan the input in one direction, from left to right or from right to left. It is noted in (Lafferty et al., 2001) that non-generative finite-state models, MEMM models included, share a weakness which they call the Label Bias Problem (LBP): a transition leaving a given state compete only against all other transitions in the model. They proposed Conditional Random Fields (CRFs) as a solution to address this problem. A partial solution to the LBP is to compute the probability of transitions in both directions. This way we can use two MEMM taggers, one of which scans the input from left to right and the other scans the input from right to left. This strategy has been successfully used in (Shen and Joshi, 2003). In that paper, pairwise voting (van Halteren et al., 1998) has (1) Feature templates (a) Default feature (b) The current character ( ) (c) The previous (next) two characters ( , , , ) (d) The previous (next) character and the current character ( , been used to combine the results of two supertaggers that scan the input in the opposite directions. The pairwise voting is not suitable in this application because we must make sure that the LMR tags assigned to consecutive words are compatible. For example, an LM tag cannot immediately follow an MM. Pairwise voting does not use any contextual information, so it cannot prevent incompatible tags from occurring. Therefore, in our experiments described here, we use the Transformation-Based Learning (Brill, 1995) to combine the results of two MEMM taggers. The feature set used in the TBL algorithm is similar to those used in the NP Chunking task in (Ngai and Florian, 2001). ❳ 3 Experiments We conducted closed track experiments on three data sources: the Academia Sinica (AS) corpus, the Beijing University (PKU) corpus and the Hong Kong City University (CityU) corpus. We first split the training data from each of the three sources into two portions. of the official training data used to train the MEMM taggers, and the other held out as the development test data (the devel- The set used to estimate the optimal number of iterations in the MEMM training. Figure (1), (2) and (3) show the curves of on the set respect to the number of iterations in MEMM training. iteration Figure 1: Learning curves on the development dataset of the Academia Sinica corpus. X-axis stands for the number of iteration in training. Y-axis stands for the -score. Experiments show that the MEMM models 100 150 200 250 300 iteration Figure 2: Learning curves on the development dataset of the HK City Univ. corpus. 200 300 400 500 600 700 800 iteration Figure 3: Learning curves on the development dataset of the Beijing Univ. corpus. achieve the best results after 500 and 400 rounds (iterations) of training on the AS data and the PKU data respectively. However, the results on the CityU data is not very clear. From Round 100 through 200, F-score on the set stays unchanged. We think this is because the CityU data is from three different sources, which differ in the optimal number of iterations. We decided to train the MEMM taggers for 160 iterations the HK City University data. We implemented two MEMM taggers, one scans the input from left to right and one from right to left. We then used these two MEMM taggers to tag both the training and the development data. We use the LMR tagging output to train a Transformationlearner, using TBL and Florian,</abstract>
<address confidence="0.746511285714286">2001). The middle in Table 2 shows the F-score 0.9595 AS 0.95945 0.9594 0.95935 0.9593</address>
<email confidence="0.354645">❳F-scorec</email>
<affiliation confidence="0.531878"></affiliation>
<address confidence="0.782130315789474">0.95925 0.9592 0.95915 0.9591 0.95905 0.959 200 300 400 500 600 700 800 0.9148 0.9146 0.9144 0.9142 0.914 0.9138 0.9136 0.9134 0.9132 0.913 0.9128 0.9126</address>
<email confidence="0.499131">HK</email>
<author confidence="0.393745">F-score</author>
<affiliation confidence="0.614343"></affiliation>
<address confidence="0.799757555555555">0.9391 0.9389 0.9388 0.9387 0.9385 0.9384 0.9383 0.9382 0.9381</address>
<date confidence="0.482321">0.939</date>
<abstract confidence="0.961309047619048">PK F-score ❳ c the set by the MEMM tagger that scans the input from left to right and the last column is the results after the Transformation- Based Learner is applied. The results show that using Transformation-Based learning only give rise to slight improvements. It seems that the bidirectional approach does not help much for the LMR tagging. Therefore, we only submitted the results of our leftto-right MEMM tagger, retrained on the entire training sets, as our official results. F-score MEMM MEMM+TBL AS 0.9595 0.9603 HK 0.9143 N/A PK 0.9391 0.9398 Table 2: F-score on development data The results on the official test data is similar to we have got on our except that the F-score on the Beijing Univ. corpus is over 2 lower in absolute accuracy than what we expected. The reason is that in the training data of Beijing University corpus, all the numbers are encoded in GBK, while in the test data many numbers are encoded in ASCII, which are unknown to our tagger. With this problem fixed, the results of the official test data are compatible with the results on However, we have withdrawn our segmentation results on the Beijing University corpus. corpus R P F AS 0.961 0.958 0.959 0.729 0.966 HK 0.917 0.915 0.916 0.670 0.936 Table 3: Official Bakeoff Outcome 4 Conclusions and Future Work Our closed track experiments on the first Sighan Bakeoff data show that the LMR algorithm produces promising results. Our system ranks the second when tested on the Academia Sinica corpus and third on the Hong Kong City University corpus. In the future, we will try to incorporate a large word list into our tagger to test its performance on open track experiments. Its high accuracy on makes it a good candidate as a general purpose segmenter. References E. Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study part-of-speech tagging. 21(4):543–565. C. K. Fan and W. H. Tsai. 1988. Automatic word identification in chinese sentences by the relaxation tech- Processing of Chinese and Oriental 4(1):33–56. Kok-Wee Gan, Martha Palmer, and Kim-Teng Lua. 1996. A statistically emergent approach for language processing: Application to modeling context effects in chinese word boundary perception. Com- 22(4):531–53. J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for stgmenand labeling sequence data. In of G. Ngai and R. Florian. 2001. Transformation-based in the fast lane. In of NAACLpages 40–47. Adwait Ratnaparkhi. 1996. A maximum entropy part-oftagger. In of the Empirical Methin Natural Language Processing University of Pennsylvania. L. Shen and A. K. Joshi. 2003. A SNoW based supertagwith application to NP chunking. In R. Sproat, Chilin Shih, William Gale, and Nancy Chang. 1996. A stochastic finite-state word-segmentation for chinese. 22(3):377–404. H. van Halteren, J. Zavrel, and W. Daelmans. 1998. Improving data driven wordclass tagging by system com- In of COLING-ACL Andi Wu. 2003. Customizable segmentation of morderived words in chinese. and Chinese Language Nianwen Xue. 2003. Chinese word segmentation as tagging. Linguistics and</abstract>
<intro confidence="0.906361">Language</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="7372" citStr="Brill, 1995" startWordPosition="1262" endWordPosition="1263"> (next) two characters ( , , , ) (d) The previous (next) character and the current character ( , been used to combine the results of two supertaggers that scan the input in the opposite directions. The pairwise voting is not suitable in this application because we must make sure that the LMR tags assigned to consecutive words are compatible. For example, an LM tag cannot immediately follow an MM. Pairwise voting does not use any contextual information, so it cannot prevent incompatible tags from occurring. Therefore, in our experiments described here, we use the Transformation-Based Learning (Brill, 1995) to combine the results of two MEMM taggers. The feature set used in the TBL algorithm is similar to those used in the NP Chunking task in (Ngai and Florian, 2001). ❳ 3 Experiments We conducted closed track experiments on three data sources: the Academia Sinica (AS) corpus, the Beijing University (PKU) corpus and the Hong Kong City University (CityU) corpus. We first split the training data from each of the three sources into two portions. of the official training data is used to train the MEMM taggers, and the other is held out as the development test data (the development set). The developme</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>E. Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging. Computational Linguistics, 21(4):543–565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C K Fan</author>
<author>W H Tsai</author>
</authors>
<title>Automatic word identification in chinese sentences by the relaxation technique.</title>
<date>1988</date>
<booktitle>Computer Processing of Chinese and Oriental Languages,</booktitle>
<pages>4--1</pages>
<contexts>
<context position="908" citStr="Fan and Tsai, 1988" startWordPosition="134" endWordPosition="137">is.upenn.edu Abstract 1 Segmentation as Tagging Unlike English text in which sentences are sequences of words delimited by white spaces, in Chinese text, sentences are represented as strings of Chinese characters or hanzi without similar natural delimiters. Therefore, the first step in a Chinese language processing task is to identify the sequence of words in a sentence and mark boundaries in appropriate places. This may sound simple enough but in reality identifying words in Chinese is a non-trivial problem that has drawn a large body of research in the Chinese language processing community (Fan and Tsai, 1988; Gan et al., 1996; Sproat et al., 1996; Wu, 2003; Xue, 2003). The key to accurate automatic word identification in Chinese lies in the successful resolution of ambiguities and aproper way to handle out-of-vocabulary words. The ambiguities in Chinese word segmentation is due to the fact that a hanzi can occur in different word-internal positions (Xue, 2003). Given the proper context, generally provided by the sentence in which it occurs, the position of a hanzi can be determined. In this paper, we model the Chinese word segmentation as a hanzi tagging problem and use a machine-learning algorit</context>
</contexts>
<marker>Fan, Tsai, 1988</marker>
<rawString>C. K. Fan and W. H. Tsai. 1988. Automatic word identification in chinese sentences by the relaxation technique. Computer Processing of Chinese and Oriental Languages, 4(1):33–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kok-Wee Gan</author>
<author>Martha Palmer</author>
<author>Kim-Teng Lua</author>
</authors>
<title>A statistically emergent approach for language processing: Application to modeling context effects in ambiguous chinese word boundary perception.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>4</issue>
<contexts>
<context position="926" citStr="Gan et al., 1996" startWordPosition="138" endWordPosition="141">t 1 Segmentation as Tagging Unlike English text in which sentences are sequences of words delimited by white spaces, in Chinese text, sentences are represented as strings of Chinese characters or hanzi without similar natural delimiters. Therefore, the first step in a Chinese language processing task is to identify the sequence of words in a sentence and mark boundaries in appropriate places. This may sound simple enough but in reality identifying words in Chinese is a non-trivial problem that has drawn a large body of research in the Chinese language processing community (Fan and Tsai, 1988; Gan et al., 1996; Sproat et al., 1996; Wu, 2003; Xue, 2003). The key to accurate automatic word identification in Chinese lies in the successful resolution of ambiguities and aproper way to handle out-of-vocabulary words. The ambiguities in Chinese word segmentation is due to the fact that a hanzi can occur in different word-internal positions (Xue, 2003). Given the proper context, generally provided by the sentence in which it occurs, the position of a hanzi can be determined. In this paper, we model the Chinese word segmentation as a hanzi tagging problem and use a machine-learning algorithm to determine th</context>
</contexts>
<marker>Gan, Palmer, Lua, 1996</marker>
<rawString>Kok-Wee Gan, Martha Palmer, and Kim-Teng Lua. 1996. A statistically emergent approach for language processing: Application to modeling context effects in ambiguous chinese word boundary perception. Computational Linguistics, 22(4):531–53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for stgmentation and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML</booktitle>
<contexts>
<context position="6005" citStr="Lafferty et al., 2001" startWordPosition="1032" endWordPosition="1035"> instantiations of the feature templates in (1). Feature templates (b) to (e) represent character features while (f) represents tag features. In the following list, are characters and are LMR tags. ), the previous two characters ( ), and the next two characters ( ) (e) The previous and the next character ( ) (f) The tag of the previous character ( ), and the tag of the character two before the current character ( ) 2.2 Transformation-Based Learning One potential problem with the MEMM is that it can only scan the input in one direction, from left to right or from right to left. It is noted in (Lafferty et al., 2001) that non-generative finite-state models, MEMM models included, share a weakness which they call the Label Bias Problem (LBP): a transition leaving a given state compete only against all other transitions in the model. They proposed Conditional Random Fields (CRFs) as a solution to address this problem. A partial solution to the LBP is to compute the probability of transitions in both directions. This way we can use two MEMM taggers, one of which scans the input from left to right and the other scans the input from right to left. This strategy has been successfully used in (Shen and Joshi, 200</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for stgmentation and labeling sequence data. In Proceedings of ICML 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Ngai</author>
<author>R Florian</author>
</authors>
<title>Transformation-based learning in the fast lane.</title>
<date>2001</date>
<booktitle>In Proceedings of NAACL2001,</booktitle>
<pages>40--47</pages>
<contexts>
<context position="7535" citStr="Ngai and Florian, 2001" startWordPosition="1292" endWordPosition="1295">at scan the input in the opposite directions. The pairwise voting is not suitable in this application because we must make sure that the LMR tags assigned to consecutive words are compatible. For example, an LM tag cannot immediately follow an MM. Pairwise voting does not use any contextual information, so it cannot prevent incompatible tags from occurring. Therefore, in our experiments described here, we use the Transformation-Based Learning (Brill, 1995) to combine the results of two MEMM taggers. The feature set used in the TBL algorithm is similar to those used in the NP Chunking task in (Ngai and Florian, 2001). ❳ 3 Experiments We conducted closed track experiments on three data sources: the Academia Sinica (AS) corpus, the Beijing University (PKU) corpus and the Hong Kong City University (CityU) corpus. We first split the training data from each of the three sources into two portions. of the official training data is used to train the MEMM taggers, and the other is held out as the development test data (the development set). The development set is used to estimate the optimal number of iterations in the MEMM training. Figure (1), (2) and (3) show the curves of F-scores on the development set with r</context>
<context position="9407" citStr="Ngai and Florian, 2001" startWordPosition="1618" endWordPosition="1621">e CityU data is not very clear. From Round 100 through 200, the F-score on the development set almost stays unchanged. We think this is because the CityU data is from three different sources, which differ in the optimal number of iterations. We decided to train the MEMM taggers for 160 iterations the HK City University data. We implemented two MEMM taggers, one scans the input from left to right and one from right to left. We then used these two MEMM taggers to tag both the training and the development data. We use the LMR tagging output to train a TransformationBased learner, using fast TBL (Ngai and Florian, 2001). The middle in Table 2 shows the F-score 0.9595 AS 0.95945 0.9594 0.95935 0.9593 ❳ F-score c ❳ 0.95925 0.9592 0.95915 0.9591 0.95905 0.959 200 300 400 500 600 700 800 0.9148 0.9146 0.9144 0.9142 0.914 0.9138 0.9136 0.9134 0.9132 0.913 0.9128 0.9126 HK F-score ❳ sco ❳ 0.9386 0.9391 0.9389 0.9388 0.9387 0.9385 0.9384 0.9383 0.9382 0.9381 0.939 PK F-score ❳ c on the development set achieved by the MEMM tagger that scans the input from left to right and the last column is the results after the TransformationBased Learner is applied. The results show that using Transformation-Based learning only g</context>
</contexts>
<marker>Ngai, Florian, 2001</marker>
<rawString>G. Ngai and R. Florian. 2001. Transformation-based learning in the fast lane. In Proceedings of NAACL2001, pages 40–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy part-ofspeech tagger.</title>
<date>1996</date>
<booktitle>In Proceedings of the Empirical Methods in Natural Language Processing Conference,</booktitle>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="4533" citStr="Ratnaparkhi, 1996" startWordPosition="767" endWordPosition="768">m Entropy taggers, one of which scans the input from left to right and the other scans the input from right to left. Then we implement a Transformation Based Algorithm to combine the results of the two taggers. 2.1 The Maximum Entropy Tagger The Maximum Entropy Markov Model (MEMM) has been successfully used in some tagging problems. MEMM models are capable of utilizing a large set of features that generative models cannot use. On the other hand, MEMM approaches scan the input incrementally as generative models do. The Maximum Entropy Markov Model used in POS-tagging is described in detail in (Ratnaparkhi, 1996) and the LMR tagger here uses the same probability model. The probability model is defined over , where is the set of possible contexts or ”histories” and is the set of possible tags. The model’s joint probability of a history and a tag is defined as where is a normalization constant, are the model parameters and are known as features, where . Each feature has a corresponding parameter , that effectively serves as a ”weight” of this feature. In the training process, given a sequence of characters and their LMR tags as training data, the purpose is to determine the parameters that maximize the </context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy part-ofspeech tagger. In Proceedings of the Empirical Methods in Natural Language Processing Conference, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Shen</author>
<author>A K Joshi</author>
</authors>
<title>A SNoW based supertagger with application to NP chunking.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL</booktitle>
<contexts>
<context position="6607" citStr="Shen and Joshi, 2003" startWordPosition="1133" endWordPosition="1136">ferty et al., 2001) that non-generative finite-state models, MEMM models included, share a weakness which they call the Label Bias Problem (LBP): a transition leaving a given state compete only against all other transitions in the model. They proposed Conditional Random Fields (CRFs) as a solution to address this problem. A partial solution to the LBP is to compute the probability of transitions in both directions. This way we can use two MEMM taggers, one of which scans the input from left to right and the other scans the input from right to left. This strategy has been successfully used in (Shen and Joshi, 2003). In that paper, pairwise voting (van Halteren et al., 1998) has (1) Feature templates (a) Default feature (b) The current character ( ) (c) The previous (next) two characters ( , , , ) (d) The previous (next) character and the current character ( , been used to combine the results of two supertaggers that scan the input in the opposite directions. The pairwise voting is not suitable in this application because we must make sure that the LMR tags assigned to consecutive words are compatible. For example, an LM tag cannot immediately follow an MM. Pairwise voting does not use any contextual inf</context>
</contexts>
<marker>Shen, Joshi, 2003</marker>
<rawString>L. Shen and A. K. Joshi. 2003. A SNoW based supertagger with application to NP chunking. In Proceedings ofACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sproat</author>
<author>Chilin Shih</author>
<author>William Gale</author>
<author>Nancy Chang</author>
</authors>
<title>A stochastic finite-state word-segmentation algorithm for chinese.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>3</issue>
<contexts>
<context position="947" citStr="Sproat et al., 1996" startWordPosition="142" endWordPosition="145">s Tagging Unlike English text in which sentences are sequences of words delimited by white spaces, in Chinese text, sentences are represented as strings of Chinese characters or hanzi without similar natural delimiters. Therefore, the first step in a Chinese language processing task is to identify the sequence of words in a sentence and mark boundaries in appropriate places. This may sound simple enough but in reality identifying words in Chinese is a non-trivial problem that has drawn a large body of research in the Chinese language processing community (Fan and Tsai, 1988; Gan et al., 1996; Sproat et al., 1996; Wu, 2003; Xue, 2003). The key to accurate automatic word identification in Chinese lies in the successful resolution of ambiguities and aproper way to handle out-of-vocabulary words. The ambiguities in Chinese word segmentation is due to the fact that a hanzi can occur in different word-internal positions (Xue, 2003). Given the proper context, generally provided by the sentence in which it occurs, the position of a hanzi can be determined. In this paper, we model the Chinese word segmentation as a hanzi tagging problem and use a machine-learning algorithm to determine the appropriate positio</context>
</contexts>
<marker>Sproat, Shih, Gale, Chang, 1996</marker>
<rawString>R. Sproat, Chilin Shih, William Gale, and Nancy Chang. 1996. A stochastic finite-state word-segmentation algorithm for chinese. Computational Linguistics, 22(3):377–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H van Halteren</author>
<author>J Zavrel</author>
<author>W Daelmans</author>
</authors>
<title>Improving data driven wordclass tagging by system combination.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL 98.</booktitle>
<marker>van Halteren, Zavrel, Daelmans, 1998</marker>
<rawString>H. van Halteren, J. Zavrel, and W. Daelmans. 1998. Improving data driven wordclass tagging by system combination. In Proceedings of COLING-ACL 98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andi Wu</author>
</authors>
<title>Customizable segmentation of morphologically derived words in chinese.</title>
<date>2003</date>
<booktitle>Computational Linguistics and Chinese Language Processing.</booktitle>
<contexts>
<context position="957" citStr="Wu, 2003" startWordPosition="146" endWordPosition="147">ish text in which sentences are sequences of words delimited by white spaces, in Chinese text, sentences are represented as strings of Chinese characters or hanzi without similar natural delimiters. Therefore, the first step in a Chinese language processing task is to identify the sequence of words in a sentence and mark boundaries in appropriate places. This may sound simple enough but in reality identifying words in Chinese is a non-trivial problem that has drawn a large body of research in the Chinese language processing community (Fan and Tsai, 1988; Gan et al., 1996; Sproat et al., 1996; Wu, 2003; Xue, 2003). The key to accurate automatic word identification in Chinese lies in the successful resolution of ambiguities and aproper way to handle out-of-vocabulary words. The ambiguities in Chinese word segmentation is due to the fact that a hanzi can occur in different word-internal positions (Xue, 2003). Given the proper context, generally provided by the sentence in which it occurs, the position of a hanzi can be determined. In this paper, we model the Chinese word segmentation as a hanzi tagging problem and use a machine-learning algorithm to determine the appropriate position for a ha</context>
</contexts>
<marker>Wu, 2003</marker>
<rawString>Andi Wu. 2003. Customizable segmentation of morphologically derived words in chinese. Computational Linguistics and Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Chinese word segmentation as character tagging.</title>
<date>2003</date>
<booktitle>Computational Linguistics and Chinese Language Processing.</booktitle>
<contexts>
<context position="969" citStr="Xue, 2003" startWordPosition="148" endWordPosition="149">n which sentences are sequences of words delimited by white spaces, in Chinese text, sentences are represented as strings of Chinese characters or hanzi without similar natural delimiters. Therefore, the first step in a Chinese language processing task is to identify the sequence of words in a sentence and mark boundaries in appropriate places. This may sound simple enough but in reality identifying words in Chinese is a non-trivial problem that has drawn a large body of research in the Chinese language processing community (Fan and Tsai, 1988; Gan et al., 1996; Sproat et al., 1996; Wu, 2003; Xue, 2003). The key to accurate automatic word identification in Chinese lies in the successful resolution of ambiguities and aproper way to handle out-of-vocabulary words. The ambiguities in Chinese word segmentation is due to the fact that a hanzi can occur in different word-internal positions (Xue, 2003). Given the proper context, generally provided by the sentence in which it occurs, the position of a hanzi can be determined. In this paper, we model the Chinese word segmentation as a hanzi tagging problem and use a machine-learning algorithm to determine the appropriate position for a hanzi. There a</context>
</contexts>
<marker>Xue, 2003</marker>
<rawString>Nianwen Xue. 2003. Chinese word segmentation as character tagging. Computational Linguistics and Chinese Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>