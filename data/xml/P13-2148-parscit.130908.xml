<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000225">
<title confidence="0.9984785">
Identifying Sentiment Words
Using an Optimization-based Model without Seed Words
</title>
<author confidence="0.994378">
Hongliang Yu 1, Zhi-Hong Deng 2∗, Shiyingxue Li 3
</author>
<affiliation confidence="0.966057">
Key Laboratory of Machine Perception (Ministry of Education),
School of Electronics Engineering and Computer Science,
Peking University, Beijing 100871, China
</affiliation>
<email confidence="0.962157333333333">
1yuhongliang324@gmail.com
2zhdeng@cis.pku.edu.cn
3rachellieinspace@gmail.com
</email>
<sectionHeader confidence="0.993652" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998132833333333">
Sentiment Word Identification (SWI) is a
basic technique in many sentiment analy-
sis applications. Most existing research-
es exploit seed words, and lead to low ro-
bustness. In this paper, we propose a novel
optimization-based model for SWI. Unlike
previous approaches, our model exploits
the sentiment labels of documents instead
of seed words. Several experiments on re-
al datasets show that WEED is effective
and outperforms the state-of-the-art meth-
ods with seed words.
</bodyText>
<sectionHeader confidence="0.99879" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9817356875">
In recent years, sentiment analysis (Pang et al.,
2002) has become a hotspot in opinion mining and
attracted much attention. Sentiment analysis is to
classify a text span into different sentiment polar-
ities, i.e. positive, negative or neutral. Sentimen-
t Word Identification (SWI) is a basic technique
in sentiment analysis. According to (Ku et al.,
2006)(Chen et al., 2012)(Fan et al., 2011), SWI
can be applied to many fields, such as determin-
ing critics opinions about a given product, tweeter
classification, summarization of reviews, and mes-
sage filtering, etc. Thus in this paper, we focus on
SWI.
Here is a simple example of how SWI is applied
to comment analysis. The sentence below is an
movie review in IMDB database:
</bodyText>
<listItem confidence="0.7796935">
• Bored performers and a lackluster plot and
script, do not make a good action movie.
</listItem>
<bodyText confidence="0.987915276595745">
In order to judge the sentence polarity (thus we can
learn about the preference of this user), one must
recognize which words are able to express senti-
ment. In this sentence, “bored” and “lackluster”
are negative while “good” should be positive, yet
*Corresponding author
its polarity is reversed by “not”. By such analy-
sis, we then conclude such movie review is a nega-
tive comment. But how do we recognize sentiment
words?
To achieve this, previous supervised approach-
es need labeled polarity words, also called seed
words, usually manually selected. The words
to be classified by their sentiment polarities are
called candidate words. Prior works study the re-
lations between labeled seed words and unlabeled
candidate words, and then obtain sentiment polar-
ities of candidate words by these relations. There
are many ways to generate word relations. The
authors of (Turney and Littman, 2003) and (Kaji
and Kitsuregawa, 2007) use statistical measures,
such as point wise mutual information (PMI), to
compute similarities in words or phrases. Kanaya-
ma and Nasukawa (2006) assume sentiment word-
s successively appear in the text, so one could
find sentiment words in the context of seed words
(Kanayama and Nasukawa, 2006). In (Hassan and
Radev, 2010) and (Hassan et al., 2011), a Markov
random walk model is applied to a large word re-
latedness graph, constructed according to the syn-
onyms and hypernyms in WordNet (Miller, 1995).
However, approaches based on seed words has
obvious shortcomings. First, polarities of seed
words are not reliable for various domains. As
a simple example, “rise” is a neutral word most
often, but becomes positive in stock market. Sec-
ond, manually selection of seed words can be very
subjective even if the application domain is deter-
mined. Third, algorithms using seed words have
low robustness. Any missing key word in the set
of seed words could lead to poor performance.
Therefore, the seed word set of such algorithms
demands high completeness (by containing com-
mon polarity words as many as possible).
Unlike the previous research work, we identi-
fy sentiment words without any seed words in this
paper. Instead, the documents’ bag-of-words in-
</bodyText>
<page confidence="0.984106">
855
</page>
<bodyText confidence="0.948287">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 855–859,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
formation and their polarity labels are exploited in
the identification process. Intuitively, polarities of
the document and its most component sentimen-
t words are the same. We call such phenomenon
as “sentiment matching”. Moreover, if a word is
found mostly in positive documents, it is very like-
ly a positive word, and vice versa.
We present an optimization-based model, called
WEED, to exploit the phenomenon of “sentimen-
t matching”. We first measure the importance of
the component words in the labeled documents se-
mantically. Here, the basic assumption is that im-
portant words are more sentiment related to the
document than those less important. Then, we
estimate the polarity of each document using it-
s component words’ importance along with their
sentiment values, and compare the estimation to
the real polarity. After that, we construct an op-
timization model for the whole corpus to weigh
the overall estimation error, which is minimized
by the best sentiment values of candidate words.
Finally, several experiments demonstrate the ef-
fectiveness of our approach. To the best of our
knowledge, this paper is the first work that identi-
fies sentiment words without seed words.
</bodyText>
<sectionHeader confidence="0.988862" genericHeader="method">
2 The Proposed Approach
</sectionHeader>
<subsectionHeader confidence="0.99338">
2.1 Preliminary
</subsectionHeader>
<bodyText confidence="0.942050363636364">
We formulate the sentiment word identification
problem as follows. Let D = {d1, ... , dn} denote
�
� �represents their
labels. If document di is a positive sample, then
li = 1; if di is negative, then li = −1. We use the
notation C = {c1, ... , cV } to represent candidate
word set, and V is the number of candidate words.
Each document is formed by consecutive words in
C. Our task is to predict the sentiment polarity of
each word cj ∈ C.
</bodyText>
<subsectionHeader confidence="0.998955">
2.2 Word Importance
</subsectionHeader>
<bodyText confidence="0.969522">
We assume each document di ∈ D is presented
by a bag-of-words feature vector ⃗fi = �
�
�
where fij describes the importance of cj to di. A
high value of fij indicates word cj contributes a
lot to document di in semantic view, and vice ver-
sa. Note that fij &gt; 0 if cj appears in di, while
</bodyText>
<equation confidence="0.8976585">
⃗
fij = 0 if not. For simplicity, every fi is normal-
</equation>
<bodyText confidence="0.998629166666667">
ized to a unit vector, such that features of different
documents are relatively comparable.
There are several ways to define the word
importance, and we choose normalized TF-IDF
(Jones, 1972). Therefore, we have fij ∝
TF−IDF(di, cj), and ∥ ⃗fi∥ = 1.
</bodyText>
<subsectionHeader confidence="0.99917">
2.3 Polarity Value
</subsectionHeader>
<bodyText confidence="0.922296055555556">
In the above description, the sentiment polarity has
only two states, positive or negative. We extend
both word and document polarities to polarity val-
ues in this section.
Definition 1 Word Polarity Value: For each word
cj ∈ C, we denote its word polarity value as
w(cj). w(cj) &gt; 0 indicates cj is a positive word,
while w(cj) &lt; 0 indicates cj is a negative word.
|w(cj) |indicates the strength of the belief of cj’s
polarity. Denote w(cj) as wj, and the word polar-
�
�
ity value vector w⃗ = �
For example, if w(“bad”) &lt; w(“greedy”) &lt; 0, we
can say “bad” is more likely to be a negative word
than “greedy”.
Definition 2 Document Polarity Value: For each
document di, document polarity value is
</bodyText>
<equation confidence="0.9938185">
T
fi · w⃗ ∥⃗w∥ .(1)
</equation>
<bodyText confidence="0.996311947368421">
We denote y(di) as yi for short.
Here, we can regard yi as a polarity estimate
for di based on ⃗w. To explain this, Table 1 shows
an example. “MR1”, “MR2” and “MR3” are
three movie review documents, and “compelling”
and “boring” are polarity words in the vocabu-
lary. we simply use TF to construct the document
feature vectors without normalization. In the ta-
ble, these three vectors, ⃗f1, ⃗f2 and ⃗f3, are (3, 1),
(2, 1) and (1, 3) respectively. Similarly, we can get
w⃗ = (1, −1), indicating “compelling” is a positive
word while “boring” is negative. After normaliz-
ing ⃗f1, ⃗f2 and ⃗
f3, and calculating their cosine sim-
ilarities with ⃗w, we obtain y1 &gt; y2 &gt; 0 &gt; y3.
These inequalities tell us the first two reviews are
positive, while the last review is negative. Further-
more, we believe that “MR1” is more positive than
“MR2”.
</bodyText>
<figure confidence="0.967608384615385">
�l1
�
document set. Vector l⃗ = .
� ..
ln
fi1 �
... � �,
fiV
w1
...
I.
wV
y(di) = cosine( ⃗fi, ⃗w) =
</figure>
<page confidence="0.987458">
856
</page>
<table confidence="0.9990516">
“compelling” “boring”
MR1 3 1
MR2 2 1
MR3 1 3
w 1 -1
</table>
<tableCaption confidence="0.995648">
Table 1: Three rows in the middle shows the fea-
</tableCaption>
<bodyText confidence="0.9406425">
ture vectors of three movie reviews, and the last
row shows the word polarity value vector ⃗w. For
simplicity, we use TF value to represent the word
importance feature.
</bodyText>
<subsectionHeader confidence="0.931936">
2.4 Optimization Model
</subsectionHeader>
<bodyText confidence="0.999884">
As mentioned above, we can regard yi as a polari-
ty estimate for document di. A precise prediction
makes the positive document’s estimator close to
1, and the negative’s close to -1. We define the
polarity estimate error for document di as:
</bodyText>
<equation confidence="0.75116">
ei = |yi − li |= |
</equation>
<bodyText confidence="0.995681">
Our learning procedure tries to decrease ei. We
obtain w⃗ by minimizing the overall estimation er-
</bodyText>
<equation confidence="0.9544106">
n
ror of all document samples e2i . Thus, the op-
i=1
timization problem can be described as
li)2. (3)
</equation>
<bodyText confidence="0.999875">
After solving this problem, we not only obtain the
polarity of each word cj according to the sign of
wj, but also its polarity belief based on |wj|.
</bodyText>
<subsectionHeader confidence="0.993578">
2.5 Model Solution
</subsectionHeader>
<bodyText confidence="0.844759333333333">
We use normalized vector x⃗ to substitute w⃗
∥ ⃗w∥, and
derive an equivalent optimization problem:
</bodyText>
<equation confidence="0.978317">
(⃗fi T · x⃗ − li)2 (4)
s.t. ∥⃗x∥ = 1.
</equation>
<bodyText confidence="0.983964923076923">
The equality constraint of above model makes
the problem non-convex. We relax the equality
constraint to ∥⃗x∥ ≤ 1, then the problem becomes
convex. We can rewrite the objective function
as the form of least square regression: E(⃗x) =
∥F · x⃗− ⃗l∥2, where F is the feature matrix, and
equals to
Now we can solve the problem by convex op-
timization algorithms (Boyd and Vandenberghe,
2004), such as gradient descend method. In each
iteration step, we update x⃗ by O⃗x = η · (−∇E) =
2η · (FT⃗l − FT F⃗x), where η &gt; 0 is the learning
rate.
</bodyText>
<sectionHeader confidence="0.999762" genericHeader="method">
3 Experiment
</sectionHeader>
<subsectionHeader confidence="0.99937">
3.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9998600625">
We leverage two widely used document dataset-
s. The first dataset is the Cornell Movie Review
Data 1, containing 1,000 positive and 1,000 nega-
tive processed reviews. The other is the Stanford
Large Dataset 2 (Maas et al., 2011), a collection
of 50,000 comments from IMDB, evenly divided
into training and test sets.
The ground-truth is generated with the help of
a sentiment lexicon, MPQA subjective lexicon 3.
We randomly select 20% polarity words as the
seed words, and the remaining are candidate ones.
Here, the seed words are provided for the baseline
methods but not for ours. In order to increase the
difficulty of our task, several non-polarity words
are added to the candidate word set. Table 2 shows
the word distribution of two datasets.
</bodyText>
<table confidence="0.996695">
Dataset Word Set pos neg non total
Cornell seed 135 201 - 336
candidate 541 806 1232 2579
Stanford seed 202 343 - 545
candidate 808 1370 2566 4744
</table>
<tableCaption confidence="0.995635">
Table 2: Word Distribution
</tableCaption>
<bodyText confidence="0.9998895">
In order to demonstrate the effectiveness of our
model, we select two baselines, SO-PMI (Turney
and Littman, 2003) and COM (Chen et al., 2012).
Both of them need seed words.
</bodyText>
<subsectionHeader confidence="0.981807">
3.2 Top-K Test
</subsectionHeader>
<bodyText confidence="0.99851125">
In face of the long lists of recommended polarity
words, people are only concerned about the top-
ranked words with the highest sentiment value. In
this experiment we consider the accuracy of the
top K polarity words. The quality of a polarity
word list is measured by p@K = Nrz9ht,K K , where
Nright,K is the number of top-K words which are
correctly recommended.
</bodyText>
<footnote confidence="0.999408">
1http://www.cs.cornell.edu/people/pabo/movie-review-
data/
2http://ai.stanford.edu/ amaas/data/sentiment/
3http://www.cs.pitt.edu/mpqa/
</footnote>
<equation confidence="0.994075125">
T
fi · w⃗
∥⃗w∥
li|. (2)
min n ( T
w⃗ i=1 fi · w⃗
∥⃗w∥
n
i=1
E(⃗x) =
min
x⃗
�
� � �
T
⃗f1
</equation>
<page confidence="0.50258975">
1
...
⃗fn T
857
</page>
<table confidence="0.994969714285714">
WEED SO-PMI COM
positive words negative words positive words negative words positive words negative words
great excellent bad stupid destiny lush cheap worst best great ridiculous bad
perfect perfectly worst mess brilliant skillfully ridiculous annoying will star plot evil
terrific best boring ridiculous courtesy courtesy damn pathetic bad fun star garish
true wonderfully awful plot gorgeous magnificent inconsistencies fool better plot dreadfully stupid
brilliant outstanding worse terrible temptation marvelously desperate giddy love horror pretty fun
</table>
<tableCaption confidence="0.999571">
Table 3: Case Study
</tableCaption>
<figure confidence="0.999158">
(a) Cornell Dataset
(b) Stanford Dataset
</figure>
<figureCaption confidence="0.999984">
Figure 1: Top-K Test
</figureCaption>
<bodyText confidence="0.939857428571429">
Figure 1 shows the final result of p@K, which
is the average score of the positive and negative
list. We can see that in both datasets, our approach
highly outperforms two baselines, and the preci-
sion is 14.4%-33.0% higher than the best baseline.
p@10s of WEED for Cornell and Stanford dataset-
s reach to 93.5% and 89.0%, and it shows the top
10 words in our recommended list is exceptionally
reliable. As the size of K increases, the accuracy
of all methods falls accordingly. This shows three
approaches rank the most probable polarity words
in the front of the word list. Compared with the
small dataset, we obtain a better result with large
K on the Stanford dataset.
</bodyText>
<subsectionHeader confidence="0.999956">
3.3 Case Study
</subsectionHeader>
<bodyText confidence="0.999981346153846">
We conduct an experiment to illustrate the char-
acteristics of three methods. Table 3 shows top-
10 positive and negative words for each method,
where the bold words are the ones with correc-
t polarities. From the first two columns, we can
see the accuracy of WEED is very high, where
positive words are absolutely correct and negative
word list makes only one mistake, “plot”. The oth-
er columns of this table shows the baseline meth-
ods both achieve reasonable results but do not per-
form as well as WEED.
Our approach is able to identify frequently used
sentiment words, which are vital for the applica-
tions without prior sentiment lexicons. The sen-
timent words identified by SO-PMI are not so
representative as WEED and COM. For example,
“skillfully” and “giddy” are correctly classified but
they are not very frequently used. COM tends to
assign wrong polarities to the sentiment words al-
though these words are often used. In the 5th and
6th columns of Table 3, “bad” and “horror” are
recognized as positive words, while “pretty” and
“fun” are recognized as negative ones. These con-
crete results show that WEED captures the gener-
ality of the sentiment words, and achieves a higher
accuracy than the baselines.
</bodyText>
<sectionHeader confidence="0.996724" genericHeader="conclusions">
4 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999841928571429">
We propose an effective optimization-based mod-
el, WEED, to identify sentiment words from the
corpus without seed words. The algorithm exploit-
s the sentiment information provided by the docu-
ments. To the best of our knowledge, this paper is
the first work that identifies sentiment words with-
out any seed words. Several experiments on real
datasets show that WEED outperforms the state-
of-the-art methods with seed words.
Our work can be considered as the first step
of building a domain-specific sentiment lexicon.
Once some sentiment words are obtained in a cer-
tain domain, our future work is to improve WEED
by utilizing these words.
</bodyText>
<figure confidence="0.999364833333333">
1 WEED
0.9 SO_PMI
0.8 COM
0.7
0.6
0.5
0.4
0.3
p@10 p@20 p@50 p@100
1 WEED
0.9 SO_PMI
0.8 COM
0.7
0.6
0.5
0.4
0.3
p@10 p@20 p@50 p@100
</figure>
<page confidence="0.989924">
858
</page>
<sectionHeader confidence="0.998223" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.994906333333333">
This work is partially supported by National Nat-
ural Science Foundation of China (Grant No.
61170091).
</bodyText>
<sectionHeader confidence="0.998339" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999695081967213">
S. Boyd and L. Vandenberghe. 2004. Convex optimiza-
tion. Cambridge university press.
L. Chen, W. Wang, M. Nagarajan, S. Wang, and A.P.
Sheth. 2012. Extracting diverse sentiment expres-
sions with target-dependent polarity from twitter. In
Proceedings of the Sixth International AAAI Confer-
ence on Weblogs and Social Media (ICWSM), pages
50–57.
Wen Fan, Shutao Sun, and Guohui Song. 2011.
Probability adjustment naive bayes algorithm based
on nondomain-specific sentiment and evaluation
word for domain-transfer sentiment analysis. In
Fuzzy Systems and Knowledge Discovery (FSKD),
2011 Eighth International Conference on, volume 2,
pages 1043–1046. IEEE.
A. Hassan and D. Radev. 2010. Identifying text po-
larity using random walks. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 395–403. Association for
Computational Linguistics.
A. Hassan, A. Abu-Jbara, R. Jha, and D. Radev. 2011.
Identifying the semantic orientation of foreign word-
s. In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 592–597.
K.S. Jones. 1972. A statistical interpretation of term
specificity and its application in retrieval. Journal of
documentation, 28(1):11–21.
N. Kaji and M. Kitsuregawa. 2007. Building lexicon
for sentiment analysis from massive collection of
html documents. In Proceedings of the joint confer-
ence on empirical methods in natural language pro-
cessing and computational natural language learn-
ing (EMNLP-CoNLL), pages 1075–1083.
H. Kanayama and T. Nasukawa. 2006. Fully automat-
ic lexicon expansion for domain-oriented sentiment
analysis. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Process-
ing, pages 355–363. Association for Computational
Linguistics.
Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen.
2006. Opinion extraction, summarization and track-
ing in news and blog corpora. In Proceedings of
AAAI-2006 spring symposium on computational ap-
proaches to analyzing weblogs, volume 2001.
A.L. Maas, R.E. Daly, P.T. Pham, D. Huang, A.Y. Ng,
and C. Potts. 2011. Learning word vectors for sen-
timent analysis. In Proceedings of the 49th annu-
al meeting of the association for computational Lin-
guistics (acL-2011).
Miller. 1995. Wordnet: a lexical database for english.
Communications of the ACM, 38(11):39–41.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up?: sentiment classification using machine learn-
ing techniques. In Proceedings of the ACL-02 con-
ference on Empirical methods in natural language
processing-Volume 10, pages 79–86. Association for
Computational Linguistics.
P. Turney and M.L. Littman. 2003. Measuring praise
and criticism: Inference of semantic orientation
from association.
</reference>
<page confidence="0.999055">
859
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.860270">
<title confidence="0.999651">Identifying Sentiment Words Using an Optimization-based Model without Seed Words</title>
<author confidence="0.998158">Yu Zhi-Hong Deng Shiyingxue Li</author>
<affiliation confidence="0.99977">Key Laboratory of Machine Perception (Ministry of Education), School of Electronics Engineering and Computer Science,</affiliation>
<address confidence="0.901797">Peking University, Beijing 100871, China</address>
<abstract confidence="0.996665384615385">Sentiment Word Identification (SWI) is a basic technique in many sentiment analysis applications. Most existing researches exploit seed words, and lead to low robustness. In this paper, we propose a novel optimization-based model for SWI. Unlike previous approaches, our model exploits the sentiment labels of documents instead of seed words. Several experiments on real datasets show that WEED is effective and outperforms the state-of-the-art methods with seed words.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Boyd</author>
<author>L Vandenberghe</author>
</authors>
<title>Convex optimization. Cambridge university press.</title>
<date>2004</date>
<contexts>
<context position="9424" citStr="Boyd and Vandenberghe, 2004" startWordPosition="1611" endWordPosition="1614">each word cj according to the sign of wj, but also its polarity belief based on |wj|. 2.5 Model Solution We use normalized vector x⃗ to substitute w⃗ ∥ ⃗w∥, and derive an equivalent optimization problem: (⃗fi T · x⃗ − li)2 (4) s.t. ∥⃗x∥ = 1. The equality constraint of above model makes the problem non-convex. We relax the equality constraint to ∥⃗x∥ ≤ 1, then the problem becomes convex. We can rewrite the objective function as the form of least square regression: E(⃗x) = ∥F · x⃗− ⃗l∥2, where F is the feature matrix, and equals to Now we can solve the problem by convex optimization algorithms (Boyd and Vandenberghe, 2004), such as gradient descend method. In each iteration step, we update x⃗ by O⃗x = η · (−∇E) = 2η · (FT⃗l − FT F⃗x), where η &gt; 0 is the learning rate. 3 Experiment 3.1 Experimental Setup We leverage two widely used document datasets. The first dataset is the Cornell Movie Review Data 1, containing 1,000 positive and 1,000 negative processed reviews. The other is the Stanford Large Dataset 2 (Maas et al., 2011), a collection of 50,000 comments from IMDB, evenly divided into training and test sets. The ground-truth is generated with the help of a sentiment lexicon, MPQA subjective lexicon 3. We ra</context>
</contexts>
<marker>Boyd, Vandenberghe, 2004</marker>
<rawString>S. Boyd and L. Vandenberghe. 2004. Convex optimization. Cambridge university press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Chen</author>
<author>W Wang</author>
<author>M Nagarajan</author>
<author>S Wang</author>
<author>A P Sheth</author>
</authors>
<title>Extracting diverse sentiment expressions with target-dependent polarity from twitter.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixth International AAAI Conference on Weblogs and Social Media (ICWSM),</booktitle>
<pages>50--57</pages>
<contexts>
<context position="1234" citStr="Chen et al., 2012" startWordPosition="173" endWordPosition="176">nlike previous approaches, our model exploits the sentiment labels of documents instead of seed words. Several experiments on real datasets show that WEED is effective and outperforms the state-of-the-art methods with seed words. 1 Introduction In recent years, sentiment analysis (Pang et al., 2002) has become a hotspot in opinion mining and attracted much attention. Sentiment analysis is to classify a text span into different sentiment polarities, i.e. positive, negative or neutral. Sentiment Word Identification (SWI) is a basic technique in sentiment analysis. According to (Ku et al., 2006)(Chen et al., 2012)(Fan et al., 2011), SWI can be applied to many fields, such as determining critics opinions about a given product, tweeter classification, summarization of reviews, and message filtering, etc. Thus in this paper, we focus on SWI. Here is a simple example of how SWI is applied to comment analysis. The sentence below is an movie review in IMDB database: • Bored performers and a lackluster plot and script, do not make a good action movie. In order to judge the sentence polarity (thus we can learn about the preference of this user), one must recognize which words are able to express sentiment. In </context>
<context position="10673" citStr="Chen et al., 2012" startWordPosition="1832" endWordPosition="1835">s as the seed words, and the remaining are candidate ones. Here, the seed words are provided for the baseline methods but not for ours. In order to increase the difficulty of our task, several non-polarity words are added to the candidate word set. Table 2 shows the word distribution of two datasets. Dataset Word Set pos neg non total Cornell seed 135 201 - 336 candidate 541 806 1232 2579 Stanford seed 202 343 - 545 candidate 808 1370 2566 4744 Table 2: Word Distribution In order to demonstrate the effectiveness of our model, we select two baselines, SO-PMI (Turney and Littman, 2003) and COM (Chen et al., 2012). Both of them need seed words. 3.2 Top-K Test In face of the long lists of recommended polarity words, people are only concerned about the topranked words with the highest sentiment value. In this experiment we consider the accuracy of the top K polarity words. The quality of a polarity word list is measured by p@K = Nrz9ht,K K , where Nright,K is the number of top-K words which are correctly recommended. 1http://www.cs.cornell.edu/people/pabo/movie-reviewdata/ 2http://ai.stanford.edu/ amaas/data/sentiment/ 3http://www.cs.pitt.edu/mpqa/ T fi · w⃗ ∥⃗w∥ li|. (2) min n ( T w⃗ i=1 fi · w⃗ ∥⃗w∥ n </context>
</contexts>
<marker>Chen, Wang, Nagarajan, Wang, Sheth, 2012</marker>
<rawString>L. Chen, W. Wang, M. Nagarajan, S. Wang, and A.P. Sheth. 2012. Extracting diverse sentiment expressions with target-dependent polarity from twitter. In Proceedings of the Sixth International AAAI Conference on Weblogs and Social Media (ICWSM), pages 50–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen Fan</author>
<author>Shutao Sun</author>
<author>Guohui Song</author>
</authors>
<title>Probability adjustment naive bayes algorithm based on nondomain-specific sentiment and evaluation word for domain-transfer sentiment analysis.</title>
<date>2011</date>
<booktitle>In Fuzzy Systems and Knowledge Discovery (FSKD), 2011 Eighth International Conference on,</booktitle>
<volume>2</volume>
<pages>1043--1046</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="1252" citStr="Fan et al., 2011" startWordPosition="176" endWordPosition="179">oaches, our model exploits the sentiment labels of documents instead of seed words. Several experiments on real datasets show that WEED is effective and outperforms the state-of-the-art methods with seed words. 1 Introduction In recent years, sentiment analysis (Pang et al., 2002) has become a hotspot in opinion mining and attracted much attention. Sentiment analysis is to classify a text span into different sentiment polarities, i.e. positive, negative or neutral. Sentiment Word Identification (SWI) is a basic technique in sentiment analysis. According to (Ku et al., 2006)(Chen et al., 2012)(Fan et al., 2011), SWI can be applied to many fields, such as determining critics opinions about a given product, tweeter classification, summarization of reviews, and message filtering, etc. Thus in this paper, we focus on SWI. Here is a simple example of how SWI is applied to comment analysis. The sentence below is an movie review in IMDB database: • Bored performers and a lackluster plot and script, do not make a good action movie. In order to judge the sentence polarity (thus we can learn about the preference of this user), one must recognize which words are able to express sentiment. In this sentence, “bo</context>
</contexts>
<marker>Fan, Sun, Song, 2011</marker>
<rawString>Wen Fan, Shutao Sun, and Guohui Song. 2011. Probability adjustment naive bayes algorithm based on nondomain-specific sentiment and evaluation word for domain-transfer sentiment analysis. In Fuzzy Systems and Knowledge Discovery (FSKD), 2011 Eighth International Conference on, volume 2, pages 1043–1046. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hassan</author>
<author>D Radev</author>
</authors>
<title>Identifying text polarity using random walks.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>395--403</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2922" citStr="Hassan and Radev, 2010" startWordPosition="451" endWordPosition="454">date words. Prior works study the relations between labeled seed words and unlabeled candidate words, and then obtain sentiment polarities of candidate words by these relations. There are many ways to generate word relations. The authors of (Turney and Littman, 2003) and (Kaji and Kitsuregawa, 2007) use statistical measures, such as point wise mutual information (PMI), to compute similarities in words or phrases. Kanayama and Nasukawa (2006) assume sentiment words successively appear in the text, so one could find sentiment words in the context of seed words (Kanayama and Nasukawa, 2006). In (Hassan and Radev, 2010) and (Hassan et al., 2011), a Markov random walk model is applied to a large word relatedness graph, constructed according to the synonyms and hypernyms in WordNet (Miller, 1995). However, approaches based on seed words has obvious shortcomings. First, polarities of seed words are not reliable for various domains. As a simple example, “rise” is a neutral word most often, but becomes positive in stock market. Second, manually selection of seed words can be very subjective even if the application domain is determined. Third, algorithms using seed words have low robustness. Any missing key word i</context>
</contexts>
<marker>Hassan, Radev, 2010</marker>
<rawString>A. Hassan and D. Radev. 2010. Identifying text polarity using random walks. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 395–403. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hassan</author>
<author>A Abu-Jbara</author>
<author>R Jha</author>
<author>D Radev</author>
</authors>
<title>Identifying the semantic orientation of foreign words.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>592--597</pages>
<contexts>
<context position="2948" citStr="Hassan et al., 2011" startWordPosition="456" endWordPosition="459"> the relations between labeled seed words and unlabeled candidate words, and then obtain sentiment polarities of candidate words by these relations. There are many ways to generate word relations. The authors of (Turney and Littman, 2003) and (Kaji and Kitsuregawa, 2007) use statistical measures, such as point wise mutual information (PMI), to compute similarities in words or phrases. Kanayama and Nasukawa (2006) assume sentiment words successively appear in the text, so one could find sentiment words in the context of seed words (Kanayama and Nasukawa, 2006). In (Hassan and Radev, 2010) and (Hassan et al., 2011), a Markov random walk model is applied to a large word relatedness graph, constructed according to the synonyms and hypernyms in WordNet (Miller, 1995). However, approaches based on seed words has obvious shortcomings. First, polarities of seed words are not reliable for various domains. As a simple example, “rise” is a neutral word most often, but becomes positive in stock market. Second, manually selection of seed words can be very subjective even if the application domain is determined. Third, algorithms using seed words have low robustness. Any missing key word in the set of seed words co</context>
</contexts>
<marker>Hassan, Abu-Jbara, Jha, Radev, 2011</marker>
<rawString>A. Hassan, A. Abu-Jbara, R. Jha, and D. Radev. 2011. Identifying the semantic orientation of foreign words. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 592–597.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K S Jones</author>
</authors>
<title>A statistical interpretation of term specificity and its application in retrieval.</title>
<date>1972</date>
<journal>Journal of documentation,</journal>
<pages>28--1</pages>
<contexts>
<context position="6257" citStr="Jones, 1972" startWordPosition="1020" endWordPosition="1021">ask is to predict the sentiment polarity of each word cj ∈ C. 2.2 Word Importance We assume each document di ∈ D is presented by a bag-of-words feature vector ⃗fi = � � � where fij describes the importance of cj to di. A high value of fij indicates word cj contributes a lot to document di in semantic view, and vice versa. Note that fij &gt; 0 if cj appears in di, while ⃗ fij = 0 if not. For simplicity, every fi is normalized to a unit vector, such that features of different documents are relatively comparable. There are several ways to define the word importance, and we choose normalized TF-IDF (Jones, 1972). Therefore, we have fij ∝ TF−IDF(di, cj), and ∥ ⃗fi∥ = 1. 2.3 Polarity Value In the above description, the sentiment polarity has only two states, positive or negative. We extend both word and document polarities to polarity values in this section. Definition 1 Word Polarity Value: For each word cj ∈ C, we denote its word polarity value as w(cj). w(cj) &gt; 0 indicates cj is a positive word, while w(cj) &lt; 0 indicates cj is a negative word. |w(cj) |indicates the strength of the belief of cj’s polarity. Denote w(cj) as wj, and the word polar� � ity value vector w⃗ = � For example, if w(“bad”) &lt; w(</context>
</contexts>
<marker>Jones, 1972</marker>
<rawString>K.S. Jones. 1972. A statistical interpretation of term specificity and its application in retrieval. Journal of documentation, 28(1):11–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kaji</author>
<author>M Kitsuregawa</author>
</authors>
<title>Building lexicon for sentiment analysis from massive collection of html documents.</title>
<date>2007</date>
<booktitle>In Proceedings of the joint conference on empirical methods in natural language processing and computational natural language learning (EMNLP-CoNLL),</booktitle>
<pages>1075--1083</pages>
<contexts>
<context position="2599" citStr="Kaji and Kitsuregawa, 2007" startWordPosition="399" endWordPosition="402">y “not”. By such analysis, we then conclude such movie review is a negative comment. But how do we recognize sentiment words? To achieve this, previous supervised approaches need labeled polarity words, also called seed words, usually manually selected. The words to be classified by their sentiment polarities are called candidate words. Prior works study the relations between labeled seed words and unlabeled candidate words, and then obtain sentiment polarities of candidate words by these relations. There are many ways to generate word relations. The authors of (Turney and Littman, 2003) and (Kaji and Kitsuregawa, 2007) use statistical measures, such as point wise mutual information (PMI), to compute similarities in words or phrases. Kanayama and Nasukawa (2006) assume sentiment words successively appear in the text, so one could find sentiment words in the context of seed words (Kanayama and Nasukawa, 2006). In (Hassan and Radev, 2010) and (Hassan et al., 2011), a Markov random walk model is applied to a large word relatedness graph, constructed according to the synonyms and hypernyms in WordNet (Miller, 1995). However, approaches based on seed words has obvious shortcomings. First, polarities of seed words</context>
</contexts>
<marker>Kaji, Kitsuregawa, 2007</marker>
<rawString>N. Kaji and M. Kitsuregawa. 2007. Building lexicon for sentiment analysis from massive collection of html documents. In Proceedings of the joint conference on empirical methods in natural language processing and computational natural language learning (EMNLP-CoNLL), pages 1075–1083.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kanayama</author>
<author>T Nasukawa</author>
</authors>
<title>Fully automatic lexicon expansion for domain-oriented sentiment analysis.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>355--363</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2744" citStr="Kanayama and Nasukawa (2006)" startWordPosition="420" endWordPosition="424">revious supervised approaches need labeled polarity words, also called seed words, usually manually selected. The words to be classified by their sentiment polarities are called candidate words. Prior works study the relations between labeled seed words and unlabeled candidate words, and then obtain sentiment polarities of candidate words by these relations. There are many ways to generate word relations. The authors of (Turney and Littman, 2003) and (Kaji and Kitsuregawa, 2007) use statistical measures, such as point wise mutual information (PMI), to compute similarities in words or phrases. Kanayama and Nasukawa (2006) assume sentiment words successively appear in the text, so one could find sentiment words in the context of seed words (Kanayama and Nasukawa, 2006). In (Hassan and Radev, 2010) and (Hassan et al., 2011), a Markov random walk model is applied to a large word relatedness graph, constructed according to the synonyms and hypernyms in WordNet (Miller, 1995). However, approaches based on seed words has obvious shortcomings. First, polarities of seed words are not reliable for various domains. As a simple example, “rise” is a neutral word most often, but becomes positive in stock market. Second, ma</context>
</contexts>
<marker>Kanayama, Nasukawa, 2006</marker>
<rawString>H. Kanayama and T. Nasukawa. 2006. Fully automatic lexicon expansion for domain-oriented sentiment analysis. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 355–363. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lun-Wei Ku</author>
<author>Yu-Ting Liang</author>
<author>Hsin-Hsi Chen</author>
</authors>
<title>Opinion extraction, summarization and tracking in news and blog corpora.</title>
<date>2006</date>
<booktitle>In Proceedings of AAAI-2006</booktitle>
<contexts>
<context position="1215" citStr="Ku et al., 2006" startWordPosition="170" endWordPosition="173"> model for SWI. Unlike previous approaches, our model exploits the sentiment labels of documents instead of seed words. Several experiments on real datasets show that WEED is effective and outperforms the state-of-the-art methods with seed words. 1 Introduction In recent years, sentiment analysis (Pang et al., 2002) has become a hotspot in opinion mining and attracted much attention. Sentiment analysis is to classify a text span into different sentiment polarities, i.e. positive, negative or neutral. Sentiment Word Identification (SWI) is a basic technique in sentiment analysis. According to (Ku et al., 2006)(Chen et al., 2012)(Fan et al., 2011), SWI can be applied to many fields, such as determining critics opinions about a given product, tweeter classification, summarization of reviews, and message filtering, etc. Thus in this paper, we focus on SWI. Here is a simple example of how SWI is applied to comment analysis. The sentence below is an movie review in IMDB database: • Bored performers and a lackluster plot and script, do not make a good action movie. In order to judge the sentence polarity (thus we can learn about the preference of this user), one must recognize which words are able to exp</context>
</contexts>
<marker>Ku, Liang, Chen, 2006</marker>
<rawString>Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen. 2006. Opinion extraction, summarization and tracking in news and blog corpora. In Proceedings of AAAI-2006 spring symposium on computational approaches to analyzing weblogs, volume 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Maas</author>
<author>R E Daly</author>
<author>P T Pham</author>
<author>D Huang</author>
<author>A Y Ng</author>
<author>C Potts</author>
</authors>
<title>Learning word vectors for sentiment analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th annual meeting of the association for computational Linguistics (acL-2011).</booktitle>
<contexts>
<context position="9835" citStr="Maas et al., 2011" startWordPosition="1688" endWordPosition="1691">nction as the form of least square regression: E(⃗x) = ∥F · x⃗− ⃗l∥2, where F is the feature matrix, and equals to Now we can solve the problem by convex optimization algorithms (Boyd and Vandenberghe, 2004), such as gradient descend method. In each iteration step, we update x⃗ by O⃗x = η · (−∇E) = 2η · (FT⃗l − FT F⃗x), where η &gt; 0 is the learning rate. 3 Experiment 3.1 Experimental Setup We leverage two widely used document datasets. The first dataset is the Cornell Movie Review Data 1, containing 1,000 positive and 1,000 negative processed reviews. The other is the Stanford Large Dataset 2 (Maas et al., 2011), a collection of 50,000 comments from IMDB, evenly divided into training and test sets. The ground-truth is generated with the help of a sentiment lexicon, MPQA subjective lexicon 3. We randomly select 20% polarity words as the seed words, and the remaining are candidate ones. Here, the seed words are provided for the baseline methods but not for ours. In order to increase the difficulty of our task, several non-polarity words are added to the candidate word set. Table 2 shows the word distribution of two datasets. Dataset Word Set pos neg non total Cornell seed 135 201 - 336 candidate 541 80</context>
</contexts>
<marker>Maas, Daly, Pham, Huang, Ng, Potts, 2011</marker>
<rawString>A.L. Maas, R.E. Daly, P.T. Pham, D. Huang, A.Y. Ng, and C. Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational Linguistics (acL-2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miller</author>
</authors>
<title>Wordnet: a lexical database for english.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="3100" citStr="Miller, 1995" startWordPosition="484" endWordPosition="485">many ways to generate word relations. The authors of (Turney and Littman, 2003) and (Kaji and Kitsuregawa, 2007) use statistical measures, such as point wise mutual information (PMI), to compute similarities in words or phrases. Kanayama and Nasukawa (2006) assume sentiment words successively appear in the text, so one could find sentiment words in the context of seed words (Kanayama and Nasukawa, 2006). In (Hassan and Radev, 2010) and (Hassan et al., 2011), a Markov random walk model is applied to a large word relatedness graph, constructed according to the synonyms and hypernyms in WordNet (Miller, 1995). However, approaches based on seed words has obvious shortcomings. First, polarities of seed words are not reliable for various domains. As a simple example, “rise” is a neutral word most often, but becomes positive in stock market. Second, manually selection of seed words can be very subjective even if the application domain is determined. Third, algorithms using seed words have low robustness. Any missing key word in the set of seed words could lead to poor performance. Therefore, the seed word set of such algorithms demands high completeness (by containing common polarity words as many as </context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
<author>S Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10,</booktitle>
<pages>79--86</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="916" citStr="Pang et al., 2002" startWordPosition="123" endWordPosition="126">iang324@gmail.com 2zhdeng@cis.pku.edu.cn 3rachellieinspace@gmail.com Abstract Sentiment Word Identification (SWI) is a basic technique in many sentiment analysis applications. Most existing researches exploit seed words, and lead to low robustness. In this paper, we propose a novel optimization-based model for SWI. Unlike previous approaches, our model exploits the sentiment labels of documents instead of seed words. Several experiments on real datasets show that WEED is effective and outperforms the state-of-the-art methods with seed words. 1 Introduction In recent years, sentiment analysis (Pang et al., 2002) has become a hotspot in opinion mining and attracted much attention. Sentiment analysis is to classify a text span into different sentiment polarities, i.e. positive, negative or neutral. Sentiment Word Identification (SWI) is a basic technique in sentiment analysis. According to (Ku et al., 2006)(Chen et al., 2012)(Fan et al., 2011), SWI can be applied to many fields, such as determining critics opinions about a given product, tweeter classification, summarization of reviews, and message filtering, etc. Thus in this paper, we focus on SWI. Here is a simple example of how SWI is applied to co</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, pages 79–86. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
<author>M L Littman</author>
</authors>
<title>Measuring praise and criticism: Inference of semantic orientation from association.</title>
<date>2003</date>
<contexts>
<context position="2566" citStr="Turney and Littman, 2003" startWordPosition="394" endWordPosition="397">thor its polarity is reversed by “not”. By such analysis, we then conclude such movie review is a negative comment. But how do we recognize sentiment words? To achieve this, previous supervised approaches need labeled polarity words, also called seed words, usually manually selected. The words to be classified by their sentiment polarities are called candidate words. Prior works study the relations between labeled seed words and unlabeled candidate words, and then obtain sentiment polarities of candidate words by these relations. There are many ways to generate word relations. The authors of (Turney and Littman, 2003) and (Kaji and Kitsuregawa, 2007) use statistical measures, such as point wise mutual information (PMI), to compute similarities in words or phrases. Kanayama and Nasukawa (2006) assume sentiment words successively appear in the text, so one could find sentiment words in the context of seed words (Kanayama and Nasukawa, 2006). In (Hassan and Radev, 2010) and (Hassan et al., 2011), a Markov random walk model is applied to a large word relatedness graph, constructed according to the synonyms and hypernyms in WordNet (Miller, 1995). However, approaches based on seed words has obvious shortcomings</context>
<context position="10645" citStr="Turney and Littman, 2003" startWordPosition="1826" endWordPosition="1829">e randomly select 20% polarity words as the seed words, and the remaining are candidate ones. Here, the seed words are provided for the baseline methods but not for ours. In order to increase the difficulty of our task, several non-polarity words are added to the candidate word set. Table 2 shows the word distribution of two datasets. Dataset Word Set pos neg non total Cornell seed 135 201 - 336 candidate 541 806 1232 2579 Stanford seed 202 343 - 545 candidate 808 1370 2566 4744 Table 2: Word Distribution In order to demonstrate the effectiveness of our model, we select two baselines, SO-PMI (Turney and Littman, 2003) and COM (Chen et al., 2012). Both of them need seed words. 3.2 Top-K Test In face of the long lists of recommended polarity words, people are only concerned about the topranked words with the highest sentiment value. In this experiment we consider the accuracy of the top K polarity words. The quality of a polarity word list is measured by p@K = Nrz9ht,K K , where Nright,K is the number of top-K words which are correctly recommended. 1http://www.cs.cornell.edu/people/pabo/movie-reviewdata/ 2http://ai.stanford.edu/ amaas/data/sentiment/ 3http://www.cs.pitt.edu/mpqa/ T fi · w⃗ ∥⃗w∥ li|. (2) min </context>
</contexts>
<marker>Turney, Littman, 2003</marker>
<rawString>P. Turney and M.L. Littman. 2003. Measuring praise and criticism: Inference of semantic orientation from association.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>