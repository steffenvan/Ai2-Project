<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000790">
<title confidence="0.983785">
WordNet-based Text Document Clustering
</title>
<author confidence="0.998922">
Julian Sedding
</author>
<affiliation confidence="0.998453">
Department of Computer Science
University of York
</affiliation>
<address confidence="0.906565">
Heslington, York YO10 5DD,
United Kingdom,
</address>
<email confidence="0.974243">
juliansedding@gmx.de
</email>
<author confidence="0.991056">
Dimitar Kazakov
</author>
<affiliation confidence="0.9982905">
AIG, Department of Computer Science
University of York
</affiliation>
<address confidence="0.91101">
Heslington, York YO10 5DD,
United Kingdom,
</address>
<email confidence="0.988661">
kazakov@cs.york.ac.uk
</email>
<sectionHeader confidence="0.982796" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999811">
Text document clustering can greatly simplify
browsing large collections of documents by re-
organizing them into a smaller number of man-
ageable clusters. Algorithms to solve this task
exist; however, the algorithms are only as good
as the data they work on. Problems include am-
biguity and synonymy, the former allowing for
erroneous groupings and the latter causing sim-
ilarities between documents to go unnoticed. In
this research, naive, syntax-based disambigua-
tion is attempted by assigning each word a
part-of-speech tag and by enriching the ‘bag-of-
words’ data representation often used for docu-
ment clustering with synonyms and hypernyms
from WordNet.
</bodyText>
<sectionHeader confidence="0.995156" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999936516129033">
Text document clustering is the grouping of text
documents into semantically related groups, or
as Hayes puts it, “they are grouped because they
are likely to be wanted together”(Hayes, 1963).
Initially, document clustering was developed to
improve precision and recall of information re-
trieval systems. More recently, however, driven
by the ever increasing amount of text docu-
ments available in corporate document reposi-
tories and on the Internet, the focus has shifted
towards providing ways to efficiently browse
large collections of documents and to reorganise
search results for display in a structured, often
hierarchical manner.
The clustering of Internet search results has
attracted particular attention. Some recent
studies explored the feasibility of clustering ‘in
real-time’ and the problem of adequately label-
ing clusters. Zamir and Etzioni (1999) have
created a clustering interface for the meta-
search engine ‘HuskySearch’ and Zhang and
Dong (2001) present their work on a system
called SHOC. The reader is also referred to
Vivisimo,i a commercial clustering interface
based on results from a number of search-
engines.
Ways to increase clustering speed are ex-
plored in many research papers, and the recent
trend towards web-based clustering, requiring
real-time performance, does not seem to change
this. However, van Rijsbergen points out, “it
seems to me a little early in the day to insist on
efficiency even before we know much about the
behaviour of clustered files in terms of the ef-
fectiveness of retrieval” (van Rijsbergen, 1989).
Indeed, it may be worth exploring which factors
influence the quality (or effectiveness) of docu-
ment clustering.
Clustering can be broken down into two
stages. The first one is to preprocess the doc-
uments, i.e. transforming the documents into
a suitable and useful data representation. The
second stage is to analyse the prepared data and
divide it into clusters, i.e. the clustering algo-
rithm.
Steinbach et al. (2000) compare the suitabil-
ity of a number of algorithms for text clustering
and conclude that bisecting k-means, a parti-
tional algorithm, is the current state-of-the-art.
Its processing time increases linearly with the
number of documents and its quality is similar
to that of hierarchical algorithms.
Preprocessing the documents is probably at
least as important as the choice of an algorithm,
since an algorithm can only be as good as the
data it works on. While there are a number of
preprocessing steps, that are almost standard
now, the effects of adding background knowl-
edge are still not very extensively researched.
This work explores if and how the two following
methods can improve the effectiveness of clus-
tering.
</bodyText>
<footnote confidence="0.919026">
1http://www.vivisimo.com
</footnote>
<note confidence="0.530769">
Part-of-Speech Tagging. Segond et al.
</note>
<bodyText confidence="0.997516291666666">
(1997) observe that part-of-speech tagging
(PoS) solves semantic ambiguity to some
extent (40% in one of their tests). Based
on this observation, we study whether
naive word sense disambiguation by PoS
tagging can help to improve clustering
results.
WordNet. Synonymy and hypernymy can re-
veal hidden similarities, potentially leading
to better clusters. WordNet,2 an ontology
which models these two relations (among
many others) (Miller et al., 1991), is used
to include synonyms and hypernyms in the
data representation and the effects on clus-
tering quality are observed and analysed.
The overall aim of the approach outlined above
is to cluster documents by meaning, hence it
is relevant to language understanding. The ap-
proach has some of the characteristics expected
from a robust language understanding system.
Firstly, learning only relies on unannoted text
data, which is abundant and does not contain
the individual bias of an annotator. Secondly,
the approach is based on general-purpose re-
sources (Brill’s PoS Tagger, WordNet), and the
performance is studied under pessimistic (hence
more realistic) assumptions, e.g., that the tag-
ger is trained on a standard dataset with poten-
tially different properties from the documents
to be clustered. Similarly, the approach studies
the potential benefits of using all possible senses
(and hypernyms) from WordNet, in an attempt
to postpone (or avoid altogether) the need for
Word Sense Disambiguation (WSD), and the re-
lated pitfalls of a WSD tool which may be biased
towards a specific domain or language style.
The remainder of the document is structured
as follows. Section 2 describes related work
and the techniques used to preprocess the data,
as well as cluster it and evaluate the results
achieved. Section 3 provides some background
on the selected corpus, the Reuters-21578 test
collection (Lewis, 1997b), and presents the sub-
corpora that are extracted for use in the exper-
iments. Section 4 describes the experimental
framework, while Section 5 presents the results
and their evaluation. Finally, conclusions are
drawn and further work discussed in Section 6.
</bodyText>
<footnote confidence="0.981344">
2available at http://www.cogsci.princeton.edu/∼wn
</footnote>
<sectionHeader confidence="0.919941" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999916807017544">
This work is most closely related to the recently
published research of Hotho et al. (2003b), and
can be seen as a logical continuation of their ex-
periments. While these authors have analysed
the benefits of using WordNet synonyms and up
to five levels of hypernyms for document clus-
tering (using the bisecting k-means algorithm),
this work describes the impact of tagging the
documents with PoS tags and/or adding all hy-
pernyms to the information available for each
document.
Here we use the vector space model, as de-
scribed in the work of Salton et al. (1975), in
which a document is represented as a vector or
‘bag of words’, i.e., by the words it contains
and their frequency, regardless of their order.
A number of fairly standard techniques have
been used to preprocess the data. In addition,
a combination of standard and custom software
tools have been used to add PoS tags and Word-
Net categories to the data set. These will be
described briefly below to allow for the experi-
ments to be repeated.
The first preprocessing step is to PoS tag the
corpus. The PoS tagger relies on the text struc-
ture and morphological differences to determine
the appropriate part-of-speech. For this reason,
if it is required, PoS tagging is the first step
to be carried out. After this, stopword removal
is performed, followed by stemming. This or-
der is chosen to reduce the amount of words
to be stemmed. The stemmed words are then
looked up in WordNet and their correspond-
ing synonyms and hypernyms are added to the
bag-of-words. Once the document vectors are
completed in this way, the frequency of each
word across the corpus can be counted and ev-
ery word occurring less often than the pre speci-
fied threshold is pruned. Finally, after the prun-
ing step, the term weights are converted to tf idf
as described below.
Stemming, stopword removal and pruning all
aim to improve clustering quality by removing
noise, i.e. meaningless data. They all lead to
a reduction in the number of dimensions in the
term-space. Weighting is concerned with the es-
timation of the importance of individual terms.
All of these have been used extensively and are
considered the baseline for comparison in this
work. However, the two techniques under in-
vestigation both add data to the representa-
tion. PoS tagging adds syntactic information
and WordNet is used to add synonyms and hy-
pernyms. The rest of this section discusses pre-
processing, clustering and evaluation in more
detail.
PoS Tagging PoS tags are assigned to the
corpus using Brill’s PoS tagger. As PoS tagging
requires the words to be in their original order
this is done before any other modifications on
the corpora.
Stopword Removal Stopwords, i.e. words
thought not to convey any meaning, are re-
moved from the text. The approach taken in
this work does not compile a static list of stop-
words, as usually done. Instead PoS informa-
tion is exploited and all tokens that are not
nouns, verbs or adjectives are removed.
Stemming Words with the same meaning ap-
pear in various morphological forms. To cap-
ture their similarity they are normalised into
a common root-form, the stem. The morphol-
ogy function provided with WordNet is used for
stemming, because it only yields stems that are
contained in the WordNet dictionary.
WordNet Categories WordNet, the lexical
database developed by Miller et al., is used to
include background information on each word.
Depending on the experiment setup, words are
replaced with their synset IDs, which constitute
their different possible senses, and also different
levels of hypernyms, more general terms for the
a word, are added.
Pruning Words appearing with low fre-
quency throughout the corpus are unlikely to
appear in more than a handful of documents
and would therefore, even if they contributed
any discriminating power, be likely to cause too
fine grained distinctions for us to be useful, i.e
clusters containing only one or two documents.
Therefore all words (or synset IDs) that ap-
pear less often than a pre-specified threshold are
pruned.
Weighting Weights are assigned to give an
indication of the importance of a word. The
most trivial weight is the word-frequency. How-
ever, more sophisticated methods can provide
better results. Throughout this work, tf idf
(term frequency x inverse document frequency)
as described by Salton et al. (1975), is used.
One problem with term frequency is that the
lengths of the documents are not taken into ac-
count. The straight forward solution to this
problem is to divide the term frequency by the
total number of terms in the document, the
document length. Effectively, this approach is
equivalent to normalising each document vector
to length one and is called relative term fre-
quency.
However, for this research a more sophisti-
cated measure is used: the product of term fre-
quency and inverse document frequency tf idf.
Salton et al. define the inverse document fre-
quency idf as
</bodyText>
<equation confidence="0.904349">
idft = log2 n − log2 dft + 1 (1)
</equation>
<bodyText confidence="0.994262392857143">
where dft is the number of documents in which
term t appears and n the total number of doc-
uments. Consequently, the tf idf measure is cal-
culated as
tf idft = tf · (log2 n − log2 dft + 1) (2)
simply the multiplication of tf and idf. This
means that larger weights are assigned to terms
that appear relatively rarely throughout the
corpus, but very frequently in individual doc-
uments. Salton et al. (1975) measure a 14%
improvement in recall and precision for tf idf in
comparison to the standard term frequency tf.
Clustering is done with the bisecting k-
means algorithm as it is described by Stein-
bach et al. (2000). In their comparison of
different algorithms they conclude that bisect-
ing k-means is the current state of the art for
document clustering. Bisecting k-means com-
bines the strengths of partitional and hierarchi-
cal clustering methods by iteratively splitting
the biggest cluster using the basic k-means algo-
rithm. Basic k-means is a partitional clustering
algorithm based on the vector space model. At
the heart of such algorithms is a similarity mea-
sure. We choose the cosine distance, which mea-
sures the similarity of two documents by cal-
culating the cosine of the angle between them.
The cosine distance is defined as follows:
</bodyText>
<equation confidence="0.9917845">
dj (3)
�dj|
</equation>
<bodyText confidence="0.999967555555556">
where |�di |and  |dj |are the lengths of vectors
di and dj, respectively, and di · dj is the dot-
product of the two vectors. When the lengths of
the vectors are normalised, the cosine distance
is equivalent to the dot-product of the vectors,
i.e. d1 · d2.
Evaluation Three different evaluation mea-
sures are used in this work, namely purity, en-
tropy and overall similarity. Purity and entropy
</bodyText>
<equation confidence="0.989814666666667">
s(di, dj) = cos(Q( di, dj)) =
|�di |· |
di ·
</equation>
<bodyText confidence="0.950389888888889">
are both based on precision,
where each cluster C from a clustering C of the
set of documents D is compared with the man-
ually assigned category labels L from the man-
ual categorisation L, which requires a category-
labeled corpus. Precision is the probability of a
document in cluster C being labeled L.
Purity is the percentage of correctly clustered
documents and can be calculated as:
</bodyText>
<equation confidence="0.984122333333333">
prec(C L) := |C ∩ L|
,
|C |, (4)
�
purity(C, L) :_
CEC
|C |· max prec(C,L) (5)
|D |LEL
yielding values in the range between 0 and 1.
</equation>
<bodyText confidence="0.9994325">
The intra-cluster entropy (ice) of a cluster C,
as described by Steinbach et al. (2000), consid-
ers the dispersion of documents in a cluster, and
is defined as:
</bodyText>
<equation confidence="0.9853525">
�
ice(C) :_
LEL
prec(C, L) · log(prec(C, L)) (6)
</equation>
<bodyText confidence="0.9998622">
Based on the intra-cluster entropy of all clus-
ters, the average, weighted by the cluster size,
is calculated. This results in the following for-
mula, which is based on the one used by Stein-
bach et al. (2000):
</bodyText>
<equation confidence="0.9740164">
�
entropy(C) :_
CEC
|C |· ice(C) (7)
|D|
</equation>
<bodyText confidence="0.9961948">
Overall similarity is independent of pre-
annotation. Instead the intra-cluster similari-
ties are calculated, giving an idea of the cohe-
siveness of a cluster. This is the average similar-
ity between each pair of documents in a cluster,
including the similarity of a document with it-
self. Steinbach et al. (2000) show that this is
equivalent to the squared length of the cluster
centroid, i.e. |612. The overall similarity is then
calculated as
</bodyText>
<equation confidence="0.975785">
�overall similarity(C) :_ |D |·|~c|2 (8)
CEC |C|
</equation>
<bodyText confidence="0.999460666666667">
Similarity is expressed as a percentage, there-
fore the possible values for overall similarity
range from 0 to 1.
</bodyText>
<sectionHeader confidence="0.991059" genericHeader="method">
3 The Corpus
</sectionHeader>
<bodyText confidence="0.999983032258065">
Here we look at what kind of corpus is required
to assess the quality of clusters, and present
our choice, the Reuters-21578 test collection.
This is followed by a discussion of the ways sub-
corpora can be extracted from the whole corpus
in order to address some of the problems of the
Reuters corpus.
A corpus useful for evaluating text document
clustering needs to be annotated with class or
category labels. This is not a straightforward
task, as even human annotators sometimes dis-
agree on which label to assign to a specific doc-
ument. Therefore, all results depend on the
quality of annotation. It is therefore unrealis-
tic to aim at high rates of agreement with re-
gard to the corpus, and any evaluation should
rather focus on the relative comparison of the
results achieved by different experiment setups
and configurations.
Due to the aforementioned difficulty of agree-
ing on a categorisation and the lack of a defini-
tion of ‘correct’ classification, no standard cor-
pora for evaluation of clustering techniques ex-
ist. Still, although not standardised, a number
of pre-categorised corpora are available. Apart
from various domain-specific corpora with class
annotations, there is the Reuters-21578 test col-
lection (Lewis, 1997b), which consists of 21578
newswire articles from 1987.
The Reuters corpus is chosen for use in the
experiments of this projects for four reasons.
</bodyText>
<listItem confidence="0.99506625">
1. Its domain is not specific, therefore it can
be understood by a non-expert.
2. WordNet, an ontology, which is not tailored
to a specific domain, would not be effective
for domains with a very specific vocabulary.
3. It is freely available for download.
4. It has been used in comparable studies be-
fore (Hotho et al., 2003b).
</listItem>
<bodyText confidence="0.9985145">
On closer inspection of the corpus, there re-
main some problems to solve. First of all, only
about half of the documents are annotated with
category-labels. On the other hand some doc-
uments are attributed to multiple categories,
meaning that categories overlap. Some confu-
sion seems to have been caused in the research
community by the fact that there is a TOPICS
attribute in the SGML, the value of which is ei-
ther set to YES or NO (or BYPASS). However,
this does not correspond to the values observed
within the TOPICS tag; sometimes categories
can be found, even if the TOPICS attribute is
set to NO and sometimes there are no categories
assigned, even if the attribute indicates YES.
Lewis explains that this is not an error in the
corpus, but has to do with the evolution of the
corpus and is kept for historic reasons (Lewis,
1997a).
Therefore, to prepare a base-corpus, the
TOPICS attribute is ignored and all documents
that have precisely one category assigned to
them are selected. Additionally, all documents
with an empty document body are also dis-
carded. This results in the corpus ‘reut-base’
containing 9446 documents. The distribution
of category sizes in the ‘reut-base’ is shown in
Figure 1. It illustrates that there are a few cat-
egories occurring extremely often, in fact the
two biggest categories contain about two thirds
of all documents in the corpus. This unbal-
anced distribution would blur test results, be-
cause even ‘random clustering’ would poten-
tially obtain purity values of 30% and more only
due to the contribution of the two main cate-
gories.
</bodyText>
<figureCaption confidence="0.7353205">
Figure 1: Category Distribution for Corpus
‘reut-base’ (only selected categories are listed).
</figureCaption>
<bodyText confidence="0.999927833333333">
Similar to Hotho et al. (2003b), we get around
this problem by deriving new corpora from the
base corpus. Their maximum category size is
reduced to 20, 50 and 100 documents respec-
tively. Categories containing more documents
are not excluded, but instead they are reduced
in size to comply with the defined maximum,
i.e., all documents in excess of the maximum
are removed.
Creating derived corpora has the further ad-
vantages of reducing the size of corpora and thus
computational requirements for the test runs.
Also, tests can be run on more and less homo-
geneous corpora, homogeneous with regard to
the cluster size, that is, which can give an idea
of how the method performs under different con-
ditions. Especially for this purpose a fourth, ex-
tremely homogeneous test corpus, ‘reut-min15-
max20’ is derived. It is like the ‘reut-max20’
corpus, but all categories containing less than
15 documents are entirely removed. The ‘reut-
min15-max20’ is thus the most homogeneous
test corpus, with a standard deviation in cluster
size of only 0.7 documents.
A summary of the derived test corpora is
shown in Table 1, including the number of doc-
uments they contain, i.e. their size, the average
category size and the standard deviation. Fig-
ure 2 shows the distribution of categories within
the derived corpora graphically.
</bodyText>
<tableCaption confidence="0.991667">
Table 1: Corpus Statistics
</tableCaption>
<table confidence="0.752256333333333">
Name Size Category Size
ø stdev
reut-min15-max20 713 20 0.7
reut-max20 881 13 7.7
reut-max50 1690 24 19.9
reut-max100 2244 34 35.2
reut-base 9446 143 553.2
Note: The average category size is rounded to the nearest
whole number, the standard deviation to the first decimal
place.
4 Clustering with PoS and
Background Knowledge
</table>
<bodyText confidence="0.9970122">
The aim of this work is to explore the bene-
fits of partial disambiguation of words by their
PoS and the inclusion of WordNet concepts.
This has been tested on five different setups,
as shown in Table 2.
</bodyText>
<tableCaption confidence="0.975356">
Table 2: Experiment Configurations
</tableCaption>
<table confidence="0.901596083333333">
Name Base PoS Syns Hyper
Baseline yes
PoS Only yes yes
Syns yes yes yes
Hyper 5 yes yes yes 5
Hyper All yes yes yes all
Base: stopword removal, stemming, pruning and tf idf weighting
are performed; PoS tags are stripped.
PoS: PoS tags are kept attached to the words.
Syns: all senses of a word are included using synset offsets.
Hyper: hypernyms to the specified depth are included.
Empty fields indicate ‘no’ or ‘0’.
</table>
<bodyText confidence="0.98595175">
Baseline The first configuration setting is used
to get a baseline for comparison. All ba-
sic preprocessing techniques are used, i.e.
stopword removal, stemming, pruning and
</bodyText>
<figure confidence="0.994323">
category (ordered by frequency)
4000
number �� documents
3500
3000
2500
2000
1500
1000
500
0
reut-min15-max20
reut-max20
reut-max50
reut-max100
</figure>
<figureCaption confidence="0.913564">
Figure 2: Category Distributions for Derived
Corpora
</figureCaption>
<bodyText confidence="0.989931878048781">
weighting. PoS tags are removed from the
tokens to get the equivalent of a raw text
corpus.
PoS Only Identical to the baseline, but the
PoS tags are not removed.
Syns In addition to the previous configuration,
all WordNet senses (synset IDs) of each
PoS tagged token are included.
Hyper 5 Here five levels of hypernyms are in-
cluded in addition to the synset IDs.
Hyper All Same as above, but all hypernyms
for each word token are included.
Each of the configurations is used to create
16, 32 and 64 clusters from each of the four
test-corpora. Due to the random choice of ini-
tial cluster centroids in the bisecting k-means
algorithm, the means of three test-runs with
the same configuration is calculated for corpora
‘reut-max20’ and ‘reut-max50’. The existing
project time constraints allowed us to gain some
additional insight by doing one test-run for each
of ‘reut-max100’ and ‘reut-min15-max20’. This
results in 120 experiments in total.
All configurations use tf idf weighting and
pruning. The pruning thresholds vary. For all
experiments using the ‘reut-max20’ corpus all
terms occurring less than 20 times are pruned.
The experiments on corpora ‘reut-max50’ and
‘reut-min15-max20’ are carried out with a prun-
ing threshold of 50. For the corpus ‘reut-
max100’, the pruning threshold is set to 50
when configurations Baseline, PoS Only or
Syns are used and to 200 otherwise. This rel-
atively high threshold is chosen, in order to re-
duce memory requirements. To ensure that this
inconsistency does not distort the conclusions
drawn from the test data, the results of these
tests are considered with great care and are ex-
plicitly referred to when used.
Further details of this research are described
in an unpublished report (Sedding, 2004).
</bodyText>
<sectionHeader confidence="0.998487" genericHeader="evaluation">
5 Results and Evaluation
</sectionHeader>
<bodyText confidence="0.9999265">
The results are presented in the format of one
graph per corpus, showing the entropy, purity
and overall similarity values for each of the con-
figurations shown in Table 2.
On the X-axis, the different configuration set-
tings are listed. On the right-hand side, hype
refers to the hypernym depth, syn refers to
whether synonyms were included or not, pos
refers to the presence or absence of PoS tags
and clusters refers to the number of clusters cre-
ated. For improved readability, lines are drawn,
splitting the graphs into three sections, one for
each number of clusters. For experiments on
the corpora ‘reut-max20’ and ‘reut-max50’, the
values in the graphs are the average of three
test runs, whereas for the corpora ‘reut-min15-
</bodyText>
<figure confidence="0.986545617647059">
number of documents
25
20
15
10
5
0
category (ordered by frequency)
number of documents
25
20
15
10
5
0
category (ordered by frequency)
number of documents
60
50
40
30
20
10
0
category (ordered by frequency)
number of documents
120
100
80
60
40
20
0
category (ordered by frequency)
</figure>
<bodyText confidence="0.999852368421053">
max20’ and ‘reut-max100’, the values are those
obtained from a single test run.
The Y-axis indicates the numerical values for
each of the measures. Note that the values
for purity and similarity are percentages, and
thus limited to the range between 0 and 1. For
those two measures, higher values indicate bet-
ter quality. High entropy values, on the other
hand, indicate lower quality. Entropy values are
always greater than 0 and for the particular ex-
periments carried out, they never exceed 1.3.
In analysing the test results, the main focus is
on the data of corpora ‘reut-max20’ and ‘reut-
max50’, shown in Figure 3 and Figure 4, re-
spectively. This data is more reliable, because
it is the average of repeated test runs. Figures
6–7 show the test data obtained from cluster-
ing the corpora ‘reut-min15-max20’ and ‘reut-
max100’, respectively.
The fact that the purity and similarity values
are far from 100 percent is not unusual. In many
cases, not even human annotators agree on how
to categorise a particular document (Hotho et
al., 2003a). More importantly, the number of
categories are not adjusted to the number of la-
bels present in a corpus, which makes complete
agreement impossible.
All three measures indicate that the qual-
ity increases with the number of clusters. The
graph in Figure 5 illustrates this for the entropy
in ‘reut-max50’. For any given configuration, it
appears that the decrease in entropy is almost
constant when the number of clusters increases.
This is easily explained by the average cluster
sizes, which decrease with an increasing number
of clusters; when clusters are smaller, the proba-
bility of having a high percentage of documents
with the same label in a cluster increases. This
becomes obvious when very small clusters are
looked at. For instance, the minimum purity
value for a cluster containing three documents
is 33 percent, for two documents it is 50 percent,
and, in the extreme case of a single document
per cluster, purity is always 100 percent.
The PoS Only experiment results in perfor-
mance, which is very similar to the Baseline,
and is sometimes a little better, sometimes a lit-
tle worse. This is expected, and the experiment
is included to allow for a more accurate inter-
pretation of the subsequent experiments using
synonyms and hypernyms.
A more interesting observation is that purity
and entropy values indicate better clusters for
Baseline than for any of the configurations us-
ing background knowledge from WordNet (i.e.
Syns, Hyper 5 and Hyper All). One possi-
ble conclusion is that adding background knowl-
edge is not helpful at all. However, the reasons
for the relatively poor performance could also
be due to the way the experiments are set up.
Therefore, a possible explanation for these re-
sults could be that the benefit of extra overlap
between documents, which the added synonyms
and hypernyms should provide, is outweighed
by the additional noise they create. WordNet
does often provide five or more senses for a
word, which means that for one correct sense
a number of incorrect senses are added, even if
the PoS tags eliminate some of them.
The overall similarity measure gives a differ-
ent indication. Its values appear to increase
for the cases where background knowledge is in-
cluded, especially when hypernyms are added.
Overall similarity is the weighted average of the
intra-cluster similarities of all clusters. So the
intra-cluster similarity actually increases with
</bodyText>
<figureCaption confidence="0.9998205">
Figure 3: Test Results for ‘reut-max20’
Figure 4: Test Results for ‘reut-max50’
</figureCaption>
<figure confidence="0.999885902439025">
0.800
0.600
0.400
0.200
0.000
1.200
1.000
false false
false true
0
0
true true
true true
16
0
5
true false
true false
all
purity entropy similarity
0 0
false true
true true
32
0
true true
true true
5
all
false false
false true
0
0
64 clusters
true true
true true
0
5
true syn
true pos
all hype
0.800
0.600
0.400
0.200
0.000
1.200
1.000
false false
false true
0
0
true true
true true
16
0
5
true false
true false
all
purity entropy similarity
0 0
false true
true true
32
0
true true
true true
5
all
false false
false true
0
0
true true
true true
64 clusters
0
5
true syn
true pos
all hype
</figure>
<figureCaption confidence="0.996363666666667">
Figure 5: Entropies for Different Cluster Sizes
in ‘reut-max50’
Figure 6: Test Results for ‘reut-min15-max20’
</figureCaption>
<bodyText confidence="0.996252909090909">
added information. As similarity increases with
additional overlap, the overall similarity mea-
sure shows that additional overlap is achieved.
The main problem with the approach of
adding all synonyms and all hypernyms into
the document vectors seems to be the added
noise. The expectation that tf idf weighting
would take care of these quasi-random new con-
cepts is not met, but the results also indicate
possible improvements to this approach.
If word-by-word disambiguation would be
</bodyText>
<figureCaption confidence="0.994307">
Figure 7: Test Results for ‘reut-max100’
</figureCaption>
<bodyText confidence="0.976215120689655">
used, the correct sense of a word could be cho-
sen and only the hypernyms for the correct
sense of the word could be taken into account.
This should drastically reduce noise. The ben-
efit of the added ‘correct’ concepts would then
probably improve cluster quality. Hotho et al.
(2003a) experimented successfully with simple
disambiguation strategies, e.g., they used only
the first sense provided by WordNet.
As an alternative to word-by-word disam-
biguation, a strategy to disambiguate based on
document vectors could be devised; after adding
all alternative senses of the terms, the least fre-
quent ones could be removed. This is similar
to pruning but would be done on a document
by document basis, rather than globally on the
whole corpus. The basis for this idea is that only
concepts that appear repeatedly in a document
contribute (significantly) to the meaning of the
document. It is important that this is done be-
fore hypernyms are added, especially when all
levels of hypernyms are added, because the most
general terms are bound to appear more often
than the more specific ones. This would lead
to lots of very similar, but meaningless bags of
words or bags of concepts.
Comparing Syns, Hyper 5 and Hyper All
with each other, in many cases Hyper 5 gives
the best results. A possible explanation could
again be the equilibrium between valuable in-
formation and noise that are added to the vec-
tor representations. From these results it seems
that there is a point where the amount of in-
formation added reaches its maximum benefit;
adding more knowledge afterwards results in
decreased cluster quality again. It should be
noted that a fixed threshold for the levels of hy-
pernyms used is unlikely to be optimal for all
words. Instead, a more refined approach could
set this threshold as a function of the semantic
distance (Resnik and Yarowsky, 2000; Stetina,
1997) between the word and its hypernyms.
The maximised benefit is most evident in
the ‘reut-max100’ corpus (Figure 7). However,
it needs to be kept in mind that for the last
two data points, Hyper 5 and Hyper All,
the pruning threshold is 200. Therefore, the
comparison with Syns needs to be done with
care. This is not much of a problem, because
the other graphs consistently show that the
performance for Syns is worse than for Hy-
per 5. The difference between Hyper 5 and
Hyper All in ‘reut-max100’, can be directly
compared though, because the pruning thresh-
16 clusters 32 clusters 64 clusters
0 0 0 5 all hype
false false true true true syn
false true true true true pos
</bodyText>
<figure confidence="0.990373134328358">
1.300
1.200
1.100
1.000
0.900
0.800
0.700
0.600
0.500
0.800
0.600
0.400
0.200
0.000
1.200
1.000
false false
false true
0
0
true true
true true
0
16
5
true false
true false
all
purity entropy similarity
0
false true
true true
0
0
32
true true
true true
5
all
false false
false true
0
0
true true
true true
64 clusters
0
5
true syn
true pos
all hype
1.200
1.000
0.800
0.600
0.400
0.200
0.000
purity entropy similarity
hype
syn
pos
clusters
0 0 0 5 all 0 0 0 5 all 0 0 0 5 all
false false true true true false false true true true false false true true true
false true true true true false true true true true false true true true true
16 32 64
</figure>
<bodyText confidence="0.999466025">
old of 200 is used for both configurations.
Surprisingly, there is a sharp drop in the
overall similarity from Hyper 5 to Hyper All,
much more evident than in the other three
corpora. One possible explanation could be
the different structure of the corpus. It seems
more probable, however, that the high prun-
ing threshold is the cause again. Assuming
that Hyper 5 seldom includes the most gen-
eral concepts, whereas Hyper All always in-
cludes them, their frequency in Hyper All be-
comes so high that the frequencies of all the
other terms are very low in comparison. The
document vectors in case of Hyper All end
up containing mostly meaningless concepts, be-
cause most of the others are pruned. This leads
to decreased cluster quality because the gen-
eral concepts have little discriminating power.
In the corresponding experiments on other cor-
pora, more of the specific concepts are retained.
Therefore, a better balance between general
and specific concepts is maintained, keeping the
cluster quality higher than in the case of corpus
‘reut-max100’.
PoS Only performs similar to Baseline, al-
though usually a slight decrease in quality can
be observed. Despite the assumption that the
disambiguation achieved by the PoS tags should
improve clustering results, this is clearly not
the case. PoS tags only disambiguate the cases
where different word classes are represented by
the same stem, e.g., the noun ‘run’ and the verb
‘run’. Clearly the meanings of these pairs are
in most cases related. Therefore, distinguishing
between them reduces the weight of their com-
mon concept by splitting it between two con-
cepts. In the worst case, they are pruned if
treated separately, instead of contributing sig-
nificantly to the document vector as a joint con-
cept.
</bodyText>
<sectionHeader confidence="0.99959" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999988235294118">
The main finding of this work is that includ-
ing synonyms and hypernyms, disambiguated
only by PoS tags, is not successful in improv-
ing clustering effectiveness. This could be at-
tributed to the noise introduced by all incor-
rect senses that are retrieved from WordNet. It
appears that disambiguation by PoS alone is in-
sufficient to reveal the full potential of including
background knowledge. One obviously imprac-
tical alternative would be manual sense disam-
biguation. The automated approach of only us-
ing the most common sense adopted by Hotho
et al.(2003b) seems more realistic yet beneficial.
When comparing the use of different levels of
hypernyms, the results indicate that including
only five levels is better than including all. A
possible explanation of this is that the terms
become too general when all hypernym levels
are included.
Further research is needed to determine
whether this way of document clustering can
be improved by appropriately selecting a sub-
set of the synonyms and hypernyms used
here. There is a number of corpus-based ap-
proaches to word-sense disambiguation (Resnik
and Yarowsky, 2000), which could be used for
this purpose.
The other point of interest that could be fur-
ther analysed is to find out why using five lev-
els of hypernyms produces better results than
using all levels of hypernyms. It would be in-
teresting to see whether this effect persists when
better disambiguation is used to determine ‘cor-
rect’ word senses.
</bodyText>
<sectionHeader confidence="0.992143" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9997895">
The authors wish to thank the three anonymous
referees for their valuable comments.
</bodyText>
<sectionHeader confidence="0.99645" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999728142857143">
R.M. Hayes. 1963. Mathematical models in in-
formation retrieval. In P.L. Garvin, editor,
Natural Language and the Computer, page
287. McGraw-Hill, New York.
A. Hotho, S. Staab, and G. Stumme. 2003a.
Text clustering based on background knowl-
edge. Technical Report No. 425.
A. Hotho, S. Staab, and G. Stumme. 2003b.
Wordnet improves text document clustering.
In Proceedings of the Semantic Web Work-
shop at SIGIR-2003, 26th Annual Interna-
tional ACM SIGIR Conference.
D.D. Lewis. 1997a. Readme file of Reuters-
21578 text categorization test collection, dis-
tribution 1.0.
D.D. Lewis. 1997b. Reuters-21578 text catego-
rization test collection, distribution 1.0.
G. Miller, R. Beckwith, C. Fellbaum, D. Gross,
and K. Miller. 1991. Five papers on wordnet.
International Journal of Lexicography.
P. Resnik and D. Yarowsky. 2000. Distin-
guishing systems and distinguishing senses:
New evaluation methods for word sense dis-
ambiguation. Natural Language Engineering,
5(2):113–133.
G. Salton, A. Wong, and C.S. Yang. 1975. A
vector space model for automatic indexing.
Communications of the ACM, 18:613–620.
J. Sedding. 2004. Wordnet-based text docu-
ment clustering. Bachelor’s Thesis.
F. Segond, A. Schiller, G. Grefenstette, and
J.P. Chanod. 1997. An experiment in seman-
tic tagging using hidden Markov model tag-
ging. In Automatic Information Extraction
and Building of Lexical Semantic Resources
for NLP Applications, pages 78–81. Asso-
ciation for Computational Linguistics, New
Brunswick, New Jersey.
M. Steinbach, G. Karypis, and V. Kumar. 2000.
A comparison of document clustering tech-
niques. In KDD Workshop on Text Mining.
Jiri Stetina. 1997. Corpus Based Natural Lan-
guage Ambiguity Resolution. Ph.D. thesis,
Kyoto University.
C.J. van Rijsbergen. 1989. Information Re-
trieval (Second Edition). Buttersworth, Lon-
don.
O. Zamir and O. Etzioni. 1999. Grouper: A
dynamic clustering interface to Web search
results. Computer Networks, Amsterdam,
Netherlands, 31(11–16):1361–1374.
D. Zhang and Y. Dong. 2001. Semantic, hierar-
chical, online clustering of web search results.
3rd International Workshop on Web Informa-
tion and Data Management, Atlanta, Geor-
gia.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.045869">
<title confidence="0.999906">WordNet-based Text Document Clustering</title>
<author confidence="0.999065">Julian</author>
<affiliation confidence="0.998212">Department of Computer University of</affiliation>
<address confidence="0.811471">Heslington, York YO10</address>
<note confidence="0.724901">United</note>
<email confidence="0.875904">juliansedding@gmx.de</email>
<author confidence="0.288877">Dimitar</author>
<affiliation confidence="0.9885225">AIG, Department of Computer University of</affiliation>
<address confidence="0.789884">Heslington, York YO10</address>
<note confidence="0.544575">United</note>
<email confidence="0.961028">kazakov@cs.york.ac.uk</email>
<abstract confidence="0.9846630625">Text document clustering can greatly simplify browsing large collections of documents by reorganizing them into a smaller number of manageable clusters. Algorithms to solve this task exist; however, the algorithms are only as good as the data they work on. Problems include ambiguity and synonymy, the former allowing for erroneous groupings and the latter causing similarities between documents to go unnoticed. In this research, naive, syntax-based disambiguation is attempted by assigning each word a part-of-speech tag and by enriching the ‘bag-ofwords’ data representation often used for document clustering with synonyms and hypernyms from WordNet.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R M Hayes</author>
</authors>
<title>Mathematical models in information retrieval.</title>
<date>1963</date>
<booktitle>Natural Language and the Computer,</booktitle>
<pages>287</pages>
<editor>In P.L. Garvin, editor,</editor>
<publisher>McGraw-Hill,</publisher>
<location>New York.</location>
<contexts>
<context position="1174" citStr="Hayes, 1963" startWordPosition="169" endWordPosition="170">ork on. Problems include ambiguity and synonymy, the former allowing for erroneous groupings and the latter causing similarities between documents to go unnoticed. In this research, naive, syntax-based disambiguation is attempted by assigning each word a part-of-speech tag and by enriching the ‘bag-ofwords’ data representation often used for document clustering with synonyms and hypernyms from WordNet. 1 Introduction Text document clustering is the grouping of text documents into semantically related groups, or as Hayes puts it, “they are grouped because they are likely to be wanted together”(Hayes, 1963). Initially, document clustering was developed to improve precision and recall of information retrieval systems. More recently, however, driven by the ever increasing amount of text documents available in corporate document repositories and on the Internet, the focus has shifted towards providing ways to efficiently browse large collections of documents and to reorganise search results for display in a structured, often hierarchical manner. The clustering of Internet search results has attracted particular attention. Some recent studies explored the feasibility of clustering ‘in real-time’ and</context>
</contexts>
<marker>Hayes, 1963</marker>
<rawString>R.M. Hayes. 1963. Mathematical models in information retrieval. In P.L. Garvin, editor, Natural Language and the Computer, page 287. McGraw-Hill, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hotho</author>
<author>S Staab</author>
<author>G Stumme</author>
</authors>
<title>Text clustering based on background knowledge.</title>
<date>2003</date>
<tech>Technical Report No. 425.</tech>
<contexts>
<context position="6011" citStr="Hotho et al. (2003" startWordPosition="919" endWordPosition="922"> the techniques used to preprocess the data, as well as cluster it and evaluate the results achieved. Section 3 provides some background on the selected corpus, the Reuters-21578 test collection (Lewis, 1997b), and presents the subcorpora that are extracted for use in the experiments. Section 4 describes the experimental framework, while Section 5 presents the results and their evaluation. Finally, conclusions are drawn and further work discussed in Section 6. 2available at http://www.cogsci.princeton.edu/∼wn 2 Background This work is most closely related to the recently published research of Hotho et al. (2003b), and can be seen as a logical continuation of their experiments. While these authors have analysed the benefits of using WordNet synonyms and up to five levels of hypernyms for document clustering (using the bisecting k-means algorithm), this work describes the impact of tagging the documents with PoS tags and/or adding all hypernyms to the information available for each document. Here we use the vector space model, as described in the work of Salton et al. (1975), in which a document is represented as a vector or ‘bag of words’, i.e., by the words it contains and their frequency, regardles</context>
<context position="15834" citStr="Hotho et al., 2003" startWordPosition="2607" endWordPosition="2610"> corpora are available. Apart from various domain-specific corpora with class annotations, there is the Reuters-21578 test collection (Lewis, 1997b), which consists of 21578 newswire articles from 1987. The Reuters corpus is chosen for use in the experiments of this projects for four reasons. 1. Its domain is not specific, therefore it can be understood by a non-expert. 2. WordNet, an ontology, which is not tailored to a specific domain, would not be effective for domains with a very specific vocabulary. 3. It is freely available for download. 4. It has been used in comparable studies before (Hotho et al., 2003b). On closer inspection of the corpus, there remain some problems to solve. First of all, only about half of the documents are annotated with category-labels. On the other hand some documents are attributed to multiple categories, meaning that categories overlap. Some confusion seems to have been caused in the research community by the fact that there is a TOPICS attribute in the SGML, the value of which is either set to YES or NO (or BYPASS). However, this does not correspond to the values observed within the TOPICS tag; sometimes categories can be found, even if the TOPICS attribute is set </context>
<context position="17534" citStr="Hotho et al. (2003" startWordPosition="2893" endWordPosition="2896"> in the corpus ‘reut-base’ containing 9446 documents. The distribution of category sizes in the ‘reut-base’ is shown in Figure 1. It illustrates that there are a few categories occurring extremely often, in fact the two biggest categories contain about two thirds of all documents in the corpus. This unbalanced distribution would blur test results, because even ‘random clustering’ would potentially obtain purity values of 30% and more only due to the contribution of the two main categories. Figure 1: Category Distribution for Corpus ‘reut-base’ (only selected categories are listed). Similar to Hotho et al. (2003b), we get around this problem by deriving new corpora from the base corpus. Their maximum category size is reduced to 20, 50 and 100 documents respectively. Categories containing more documents are not excluded, but instead they are reduced in size to comply with the defined maximum, i.e., all documents in excess of the maximum are removed. Creating derived corpora has the further advantages of reducing the size of corpora and thus computational requirements for the test runs. Also, tests can be run on more and less homogeneous corpora, homogeneous with regard to the cluster size, that is, wh</context>
<context position="24018" citStr="Hotho et al., 2003" startWordPosition="3976" endWordPosition="3979">han 0 and for the particular experiments carried out, they never exceed 1.3. In analysing the test results, the main focus is on the data of corpora ‘reut-max20’ and ‘reutmax50’, shown in Figure 3 and Figure 4, respectively. This data is more reliable, because it is the average of repeated test runs. Figures 6–7 show the test data obtained from clustering the corpora ‘reut-min15-max20’ and ‘reutmax100’, respectively. The fact that the purity and similarity values are far from 100 percent is not unusual. In many cases, not even human annotators agree on how to categorise a particular document (Hotho et al., 2003a). More importantly, the number of categories are not adjusted to the number of labels present in a corpus, which makes complete agreement impossible. All three measures indicate that the quality increases with the number of clusters. The graph in Figure 5 illustrates this for the entropy in ‘reut-max50’. For any given configuration, it appears that the decrease in entropy is almost constant when the number of clusters increases. This is easily explained by the average cluster sizes, which decrease with an increasing number of clusters; when clusters are smaller, the probability of having a h</context>
<context position="28001" citStr="Hotho et al. (2003" startWordPosition="4650" endWordPosition="4653">ll synonyms and all hypernyms into the document vectors seems to be the added noise. The expectation that tf idf weighting would take care of these quasi-random new concepts is not met, but the results also indicate possible improvements to this approach. If word-by-word disambiguation would be Figure 7: Test Results for ‘reut-max100’ used, the correct sense of a word could be chosen and only the hypernyms for the correct sense of the word could be taken into account. This should drastically reduce noise. The benefit of the added ‘correct’ concepts would then probably improve cluster quality. Hotho et al. (2003a) experimented successfully with simple disambiguation strategies, e.g., they used only the first sense provided by WordNet. As an alternative to word-by-word disambiguation, a strategy to disambiguate based on document vectors could be devised; after adding all alternative senses of the terms, the least frequent ones could be removed. This is similar to pruning but would be done on a document by document basis, rather than globally on the whole corpus. The basis for this idea is that only concepts that appear repeatedly in a document contribute (significantly) to the meaning of the document.</context>
</contexts>
<marker>Hotho, Staab, Stumme, 2003</marker>
<rawString>A. Hotho, S. Staab, and G. Stumme. 2003a. Text clustering based on background knowledge. Technical Report No. 425.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hotho</author>
<author>S Staab</author>
<author>G Stumme</author>
</authors>
<title>Wordnet improves text document clustering.</title>
<date>2003</date>
<booktitle>In Proceedings of the Semantic Web Workshop at SIGIR-2003, 26th Annual International ACM SIGIR Conference.</booktitle>
<contexts>
<context position="6011" citStr="Hotho et al. (2003" startWordPosition="919" endWordPosition="922"> the techniques used to preprocess the data, as well as cluster it and evaluate the results achieved. Section 3 provides some background on the selected corpus, the Reuters-21578 test collection (Lewis, 1997b), and presents the subcorpora that are extracted for use in the experiments. Section 4 describes the experimental framework, while Section 5 presents the results and their evaluation. Finally, conclusions are drawn and further work discussed in Section 6. 2available at http://www.cogsci.princeton.edu/∼wn 2 Background This work is most closely related to the recently published research of Hotho et al. (2003b), and can be seen as a logical continuation of their experiments. While these authors have analysed the benefits of using WordNet synonyms and up to five levels of hypernyms for document clustering (using the bisecting k-means algorithm), this work describes the impact of tagging the documents with PoS tags and/or adding all hypernyms to the information available for each document. Here we use the vector space model, as described in the work of Salton et al. (1975), in which a document is represented as a vector or ‘bag of words’, i.e., by the words it contains and their frequency, regardles</context>
<context position="15834" citStr="Hotho et al., 2003" startWordPosition="2607" endWordPosition="2610"> corpora are available. Apart from various domain-specific corpora with class annotations, there is the Reuters-21578 test collection (Lewis, 1997b), which consists of 21578 newswire articles from 1987. The Reuters corpus is chosen for use in the experiments of this projects for four reasons. 1. Its domain is not specific, therefore it can be understood by a non-expert. 2. WordNet, an ontology, which is not tailored to a specific domain, would not be effective for domains with a very specific vocabulary. 3. It is freely available for download. 4. It has been used in comparable studies before (Hotho et al., 2003b). On closer inspection of the corpus, there remain some problems to solve. First of all, only about half of the documents are annotated with category-labels. On the other hand some documents are attributed to multiple categories, meaning that categories overlap. Some confusion seems to have been caused in the research community by the fact that there is a TOPICS attribute in the SGML, the value of which is either set to YES or NO (or BYPASS). However, this does not correspond to the values observed within the TOPICS tag; sometimes categories can be found, even if the TOPICS attribute is set </context>
<context position="17534" citStr="Hotho et al. (2003" startWordPosition="2893" endWordPosition="2896"> in the corpus ‘reut-base’ containing 9446 documents. The distribution of category sizes in the ‘reut-base’ is shown in Figure 1. It illustrates that there are a few categories occurring extremely often, in fact the two biggest categories contain about two thirds of all documents in the corpus. This unbalanced distribution would blur test results, because even ‘random clustering’ would potentially obtain purity values of 30% and more only due to the contribution of the two main categories. Figure 1: Category Distribution for Corpus ‘reut-base’ (only selected categories are listed). Similar to Hotho et al. (2003b), we get around this problem by deriving new corpora from the base corpus. Their maximum category size is reduced to 20, 50 and 100 documents respectively. Categories containing more documents are not excluded, but instead they are reduced in size to comply with the defined maximum, i.e., all documents in excess of the maximum are removed. Creating derived corpora has the further advantages of reducing the size of corpora and thus computational requirements for the test runs. Also, tests can be run on more and less homogeneous corpora, homogeneous with regard to the cluster size, that is, wh</context>
<context position="24018" citStr="Hotho et al., 2003" startWordPosition="3976" endWordPosition="3979">han 0 and for the particular experiments carried out, they never exceed 1.3. In analysing the test results, the main focus is on the data of corpora ‘reut-max20’ and ‘reutmax50’, shown in Figure 3 and Figure 4, respectively. This data is more reliable, because it is the average of repeated test runs. Figures 6–7 show the test data obtained from clustering the corpora ‘reut-min15-max20’ and ‘reutmax100’, respectively. The fact that the purity and similarity values are far from 100 percent is not unusual. In many cases, not even human annotators agree on how to categorise a particular document (Hotho et al., 2003a). More importantly, the number of categories are not adjusted to the number of labels present in a corpus, which makes complete agreement impossible. All three measures indicate that the quality increases with the number of clusters. The graph in Figure 5 illustrates this for the entropy in ‘reut-max50’. For any given configuration, it appears that the decrease in entropy is almost constant when the number of clusters increases. This is easily explained by the average cluster sizes, which decrease with an increasing number of clusters; when clusters are smaller, the probability of having a h</context>
<context position="28001" citStr="Hotho et al. (2003" startWordPosition="4650" endWordPosition="4653">ll synonyms and all hypernyms into the document vectors seems to be the added noise. The expectation that tf idf weighting would take care of these quasi-random new concepts is not met, but the results also indicate possible improvements to this approach. If word-by-word disambiguation would be Figure 7: Test Results for ‘reut-max100’ used, the correct sense of a word could be chosen and only the hypernyms for the correct sense of the word could be taken into account. This should drastically reduce noise. The benefit of the added ‘correct’ concepts would then probably improve cluster quality. Hotho et al. (2003a) experimented successfully with simple disambiguation strategies, e.g., they used only the first sense provided by WordNet. As an alternative to word-by-word disambiguation, a strategy to disambiguate based on document vectors could be devised; after adding all alternative senses of the terms, the least frequent ones could be removed. This is similar to pruning but would be done on a document by document basis, rather than globally on the whole corpus. The basis for this idea is that only concepts that appear repeatedly in a document contribute (significantly) to the meaning of the document.</context>
</contexts>
<marker>Hotho, Staab, Stumme, 2003</marker>
<rawString>A. Hotho, S. Staab, and G. Stumme. 2003b. Wordnet improves text document clustering. In Proceedings of the Semantic Web Workshop at SIGIR-2003, 26th Annual International ACM SIGIR Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Lewis</author>
</authors>
<title>Readme file of Reuters21578 text categorization test collection, distribution 1.0.</title>
<date>1997</date>
<contexts>
<context position="5600" citStr="Lewis, 1997" startWordPosition="860" endWordPosition="861">Similarly, the approach studies the potential benefits of using all possible senses (and hypernyms) from WordNet, in an attempt to postpone (or avoid altogether) the need for Word Sense Disambiguation (WSD), and the related pitfalls of a WSD tool which may be biased towards a specific domain or language style. The remainder of the document is structured as follows. Section 2 describes related work and the techniques used to preprocess the data, as well as cluster it and evaluate the results achieved. Section 3 provides some background on the selected corpus, the Reuters-21578 test collection (Lewis, 1997b), and presents the subcorpora that are extracted for use in the experiments. Section 4 describes the experimental framework, while Section 5 presents the results and their evaluation. Finally, conclusions are drawn and further work discussed in Section 6. 2available at http://www.cogsci.princeton.edu/∼wn 2 Background This work is most closely related to the recently published research of Hotho et al. (2003b), and can be seen as a logical continuation of their experiments. While these authors have analysed the benefits of using WordNet synonyms and up to five levels of hypernyms for document </context>
<context position="15362" citStr="Lewis, 1997" startWordPosition="2527" endWordPosition="2528">alistic to aim at high rates of agreement with regard to the corpus, and any evaluation should rather focus on the relative comparison of the results achieved by different experiment setups and configurations. Due to the aforementioned difficulty of agreeing on a categorisation and the lack of a definition of ‘correct’ classification, no standard corpora for evaluation of clustering techniques exist. Still, although not standardised, a number of pre-categorised corpora are available. Apart from various domain-specific corpora with class annotations, there is the Reuters-21578 test collection (Lewis, 1997b), which consists of 21578 newswire articles from 1987. The Reuters corpus is chosen for use in the experiments of this projects for four reasons. 1. Its domain is not specific, therefore it can be understood by a non-expert. 2. WordNet, an ontology, which is not tailored to a specific domain, would not be effective for domains with a very specific vocabulary. 3. It is freely available for download. 4. It has been used in comparable studies before (Hotho et al., 2003b). On closer inspection of the corpus, there remain some problems to solve. First of all, only about half of the documents are </context>
<context position="16673" citStr="Lewis, 1997" startWordPosition="2758" endWordPosition="2759">es, meaning that categories overlap. Some confusion seems to have been caused in the research community by the fact that there is a TOPICS attribute in the SGML, the value of which is either set to YES or NO (or BYPASS). However, this does not correspond to the values observed within the TOPICS tag; sometimes categories can be found, even if the TOPICS attribute is set to NO and sometimes there are no categories assigned, even if the attribute indicates YES. Lewis explains that this is not an error in the corpus, but has to do with the evolution of the corpus and is kept for historic reasons (Lewis, 1997a). Therefore, to prepare a base-corpus, the TOPICS attribute is ignored and all documents that have precisely one category assigned to them are selected. Additionally, all documents with an empty document body are also discarded. This results in the corpus ‘reut-base’ containing 9446 documents. The distribution of category sizes in the ‘reut-base’ is shown in Figure 1. It illustrates that there are a few categories occurring extremely often, in fact the two biggest categories contain about two thirds of all documents in the corpus. This unbalanced distribution would blur test results, because</context>
</contexts>
<marker>Lewis, 1997</marker>
<rawString>D.D. Lewis. 1997a. Readme file of Reuters21578 text categorization test collection, distribution 1.0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Lewis</author>
</authors>
<title>Reuters-21578 text categorization test collection, distribution 1.0.</title>
<date>1997</date>
<contexts>
<context position="5600" citStr="Lewis, 1997" startWordPosition="860" endWordPosition="861">Similarly, the approach studies the potential benefits of using all possible senses (and hypernyms) from WordNet, in an attempt to postpone (or avoid altogether) the need for Word Sense Disambiguation (WSD), and the related pitfalls of a WSD tool which may be biased towards a specific domain or language style. The remainder of the document is structured as follows. Section 2 describes related work and the techniques used to preprocess the data, as well as cluster it and evaluate the results achieved. Section 3 provides some background on the selected corpus, the Reuters-21578 test collection (Lewis, 1997b), and presents the subcorpora that are extracted for use in the experiments. Section 4 describes the experimental framework, while Section 5 presents the results and their evaluation. Finally, conclusions are drawn and further work discussed in Section 6. 2available at http://www.cogsci.princeton.edu/∼wn 2 Background This work is most closely related to the recently published research of Hotho et al. (2003b), and can be seen as a logical continuation of their experiments. While these authors have analysed the benefits of using WordNet synonyms and up to five levels of hypernyms for document </context>
<context position="15362" citStr="Lewis, 1997" startWordPosition="2527" endWordPosition="2528">alistic to aim at high rates of agreement with regard to the corpus, and any evaluation should rather focus on the relative comparison of the results achieved by different experiment setups and configurations. Due to the aforementioned difficulty of agreeing on a categorisation and the lack of a definition of ‘correct’ classification, no standard corpora for evaluation of clustering techniques exist. Still, although not standardised, a number of pre-categorised corpora are available. Apart from various domain-specific corpora with class annotations, there is the Reuters-21578 test collection (Lewis, 1997b), which consists of 21578 newswire articles from 1987. The Reuters corpus is chosen for use in the experiments of this projects for four reasons. 1. Its domain is not specific, therefore it can be understood by a non-expert. 2. WordNet, an ontology, which is not tailored to a specific domain, would not be effective for domains with a very specific vocabulary. 3. It is freely available for download. 4. It has been used in comparable studies before (Hotho et al., 2003b). On closer inspection of the corpus, there remain some problems to solve. First of all, only about half of the documents are </context>
<context position="16673" citStr="Lewis, 1997" startWordPosition="2758" endWordPosition="2759">es, meaning that categories overlap. Some confusion seems to have been caused in the research community by the fact that there is a TOPICS attribute in the SGML, the value of which is either set to YES or NO (or BYPASS). However, this does not correspond to the values observed within the TOPICS tag; sometimes categories can be found, even if the TOPICS attribute is set to NO and sometimes there are no categories assigned, even if the attribute indicates YES. Lewis explains that this is not an error in the corpus, but has to do with the evolution of the corpus and is kept for historic reasons (Lewis, 1997a). Therefore, to prepare a base-corpus, the TOPICS attribute is ignored and all documents that have precisely one category assigned to them are selected. Additionally, all documents with an empty document body are also discarded. This results in the corpus ‘reut-base’ containing 9446 documents. The distribution of category sizes in the ‘reut-base’ is shown in Figure 1. It illustrates that there are a few categories occurring extremely often, in fact the two biggest categories contain about two thirds of all documents in the corpus. This unbalanced distribution would blur test results, because</context>
</contexts>
<marker>Lewis, 1997</marker>
<rawString>D.D. Lewis. 1997b. Reuters-21578 text categorization test collection, distribution 1.0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
<author>R Beckwith</author>
<author>C Fellbaum</author>
<author>D Gross</author>
<author>K Miller</author>
</authors>
<title>Five papers on wordnet.</title>
<date>1991</date>
<journal>International Journal of Lexicography.</journal>
<contexts>
<context position="4187" citStr="Miller et al., 1991" startWordPosition="635" endWordPosition="638">ched. This work explores if and how the two following methods can improve the effectiveness of clustering. 1http://www.vivisimo.com Part-of-Speech Tagging. Segond et al. (1997) observe that part-of-speech tagging (PoS) solves semantic ambiguity to some extent (40% in one of their tests). Based on this observation, we study whether naive word sense disambiguation by PoS tagging can help to improve clustering results. WordNet. Synonymy and hypernymy can reveal hidden similarities, potentially leading to better clusters. WordNet,2 an ontology which models these two relations (among many others) (Miller et al., 1991), is used to include synonyms and hypernyms in the data representation and the effects on clustering quality are observed and analysed. The overall aim of the approach outlined above is to cluster documents by meaning, hence it is relevant to language understanding. The approach has some of the characteristics expected from a robust language understanding system. Firstly, learning only relies on unannoted text data, which is abundant and does not contain the individual bias of an annotator. Secondly, the approach is based on general-purpose resources (Brill’s PoS Tagger, WordNet), and the perf</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1991</marker>
<rawString>G. Miller, R. Beckwith, C. Fellbaum, D. Gross, and K. Miller. 1991. Five papers on wordnet. International Journal of Lexicography.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
<author>D Yarowsky</author>
</authors>
<title>Distinguishing systems and distinguishing senses: New evaluation methods for word sense disambiguation.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<volume>5</volume>
<issue>2</issue>
<contexts>
<context position="29571" citStr="Resnik and Yarowsky, 2000" startWordPosition="4913" endWordPosition="4916">other, in many cases Hyper 5 gives the best results. A possible explanation could again be the equilibrium between valuable information and noise that are added to the vector representations. From these results it seems that there is a point where the amount of information added reaches its maximum benefit; adding more knowledge afterwards results in decreased cluster quality again. It should be noted that a fixed threshold for the levels of hypernyms used is unlikely to be optimal for all words. Instead, a more refined approach could set this threshold as a function of the semantic distance (Resnik and Yarowsky, 2000; Stetina, 1997) between the word and its hypernyms. The maximised benefit is most evident in the ‘reut-max100’ corpus (Figure 7). However, it needs to be kept in mind that for the last two data points, Hyper 5 and Hyper All, the pruning threshold is 200. Therefore, the comparison with Syns needs to be done with care. This is not much of a problem, because the other graphs consistently show that the performance for Syns is worse than for Hyper 5. The difference between Hyper 5 and Hyper All in ‘reut-max100’, can be directly compared though, because the pruning thresh16 clusters 32 clusters 64 </context>
</contexts>
<marker>Resnik, Yarowsky, 2000</marker>
<rawString>P. Resnik and D. Yarowsky. 2000. Distinguishing systems and distinguishing senses: New evaluation methods for word sense disambiguation. Natural Language Engineering, 5(2):113–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>A Wong</author>
<author>C S Yang</author>
</authors>
<title>A vector space model for automatic indexing.</title>
<date>1975</date>
<journal>Communications of the ACM,</journal>
<pages>18--613</pages>
<contexts>
<context position="6482" citStr="Salton et al. (1975)" startWordPosition="1000" endWordPosition="1003">ilable at http://www.cogsci.princeton.edu/∼wn 2 Background This work is most closely related to the recently published research of Hotho et al. (2003b), and can be seen as a logical continuation of their experiments. While these authors have analysed the benefits of using WordNet synonyms and up to five levels of hypernyms for document clustering (using the bisecting k-means algorithm), this work describes the impact of tagging the documents with PoS tags and/or adding all hypernyms to the information available for each document. Here we use the vector space model, as described in the work of Salton et al. (1975), in which a document is represented as a vector or ‘bag of words’, i.e., by the words it contains and their frequency, regardless of their order. A number of fairly standard techniques have been used to preprocess the data. In addition, a combination of standard and custom software tools have been used to add PoS tags and WordNet categories to the data set. These will be described briefly below to allow for the experiments to be repeated. The first preprocessing step is to PoS tag the corpus. The PoS tagger relies on the text structure and morphological differences to determine the appropriat</context>
<context position="10243" citStr="Salton et al. (1975)" startWordPosition="1630" endWordPosition="1633">handful of documents and would therefore, even if they contributed any discriminating power, be likely to cause too fine grained distinctions for us to be useful, i.e clusters containing only one or two documents. Therefore all words (or synset IDs) that appear less often than a pre-specified threshold are pruned. Weighting Weights are assigned to give an indication of the importance of a word. The most trivial weight is the word-frequency. However, more sophisticated methods can provide better results. Throughout this work, tf idf (term frequency x inverse document frequency) as described by Salton et al. (1975), is used. One problem with term frequency is that the lengths of the documents are not taken into account. The straight forward solution to this problem is to divide the term frequency by the total number of terms in the document, the document length. Effectively, this approach is equivalent to normalising each document vector to length one and is called relative term frequency. However, for this research a more sophisticated measure is used: the product of term frequency and inverse document frequency tf idf. Salton et al. define the inverse document frequency idf as idft = log2 n − log2 dft</context>
</contexts>
<marker>Salton, Wong, Yang, 1975</marker>
<rawString>G. Salton, A. Wong, and C.S. Yang. 1975. A vector space model for automatic indexing. Communications of the ACM, 18:613–620.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Sedding</author>
</authors>
<title>Wordnet-based text document clustering. Bachelor’s Thesis.</title>
<date>2004</date>
<contexts>
<context position="21939" citStr="Sedding, 2004" startWordPosition="3622" endWordPosition="3623">he experiments on corpora ‘reut-max50’ and ‘reut-min15-max20’ are carried out with a pruning threshold of 50. For the corpus ‘reutmax100’, the pruning threshold is set to 50 when configurations Baseline, PoS Only or Syns are used and to 200 otherwise. This relatively high threshold is chosen, in order to reduce memory requirements. To ensure that this inconsistency does not distort the conclusions drawn from the test data, the results of these tests are considered with great care and are explicitly referred to when used. Further details of this research are described in an unpublished report (Sedding, 2004). 5 Results and Evaluation The results are presented in the format of one graph per corpus, showing the entropy, purity and overall similarity values for each of the configurations shown in Table 2. On the X-axis, the different configuration settings are listed. On the right-hand side, hype refers to the hypernym depth, syn refers to whether synonyms were included or not, pos refers to the presence or absence of PoS tags and clusters refers to the number of clusters created. For improved readability, lines are drawn, splitting the graphs into three sections, one for each number of clusters. Fo</context>
</contexts>
<marker>Sedding, 2004</marker>
<rawString>J. Sedding. 2004. Wordnet-based text document clustering. Bachelor’s Thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Segond</author>
<author>A Schiller</author>
<author>G Grefenstette</author>
<author>J P Chanod</author>
</authors>
<title>An experiment in semantic tagging using hidden Markov model tagging.</title>
<date>1997</date>
<booktitle>In Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications,</booktitle>
<pages>78--81</pages>
<location>New Brunswick, New Jersey.</location>
<contexts>
<context position="3743" citStr="Segond et al. (1997)" startWordPosition="569" endWordPosition="572"> processing time increases linearly with the number of documents and its quality is similar to that of hierarchical algorithms. Preprocessing the documents is probably at least as important as the choice of an algorithm, since an algorithm can only be as good as the data it works on. While there are a number of preprocessing steps, that are almost standard now, the effects of adding background knowledge are still not very extensively researched. This work explores if and how the two following methods can improve the effectiveness of clustering. 1http://www.vivisimo.com Part-of-Speech Tagging. Segond et al. (1997) observe that part-of-speech tagging (PoS) solves semantic ambiguity to some extent (40% in one of their tests). Based on this observation, we study whether naive word sense disambiguation by PoS tagging can help to improve clustering results. WordNet. Synonymy and hypernymy can reveal hidden similarities, potentially leading to better clusters. WordNet,2 an ontology which models these two relations (among many others) (Miller et al., 1991), is used to include synonyms and hypernyms in the data representation and the effects on clustering quality are observed and analysed. The overall aim of t</context>
</contexts>
<marker>Segond, Schiller, Grefenstette, Chanod, 1997</marker>
<rawString>F. Segond, A. Schiller, G. Grefenstette, and J.P. Chanod. 1997. An experiment in semantic tagging using hidden Markov model tagging. In Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications, pages 78–81. Association for Computational Linguistics, New Brunswick, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steinbach</author>
<author>G Karypis</author>
<author>V Kumar</author>
</authors>
<title>A comparison of document clustering techniques.</title>
<date>2000</date>
<booktitle>In KDD Workshop on Text Mining.</booktitle>
<contexts>
<context position="2954" citStr="Steinbach et al. (2000)" startWordPosition="445" endWordPosition="448">s out, “it seems to me a little early in the day to insist on efficiency even before we know much about the behaviour of clustered files in terms of the effectiveness of retrieval” (van Rijsbergen, 1989). Indeed, it may be worth exploring which factors influence the quality (or effectiveness) of document clustering. Clustering can be broken down into two stages. The first one is to preprocess the documents, i.e. transforming the documents into a suitable and useful data representation. The second stage is to analyse the prepared data and divide it into clusters, i.e. the clustering algorithm. Steinbach et al. (2000) compare the suitability of a number of algorithms for text clustering and conclude that bisecting k-means, a partitional algorithm, is the current state-of-the-art. Its processing time increases linearly with the number of documents and its quality is similar to that of hierarchical algorithms. Preprocessing the documents is probably at least as important as the choice of an algorithm, since an algorithm can only be as good as the data it works on. While there are a number of preprocessing steps, that are almost standard now, the effects of adding background knowledge are still not very exten</context>
<context position="11466" citStr="Steinbach et al. (2000)" startWordPosition="1852" endWordPosition="1856"> + 1 (1) where dft is the number of documents in which term t appears and n the total number of documents. Consequently, the tf idf measure is calculated as tf idft = tf · (log2 n − log2 dft + 1) (2) simply the multiplication of tf and idf. This means that larger weights are assigned to terms that appear relatively rarely throughout the corpus, but very frequently in individual documents. Salton et al. (1975) measure a 14% improvement in recall and precision for tf idf in comparison to the standard term frequency tf. Clustering is done with the bisecting kmeans algorithm as it is described by Steinbach et al. (2000). In their comparison of different algorithms they conclude that bisecting k-means is the current state of the art for document clustering. Bisecting k-means combines the strengths of partitional and hierarchical clustering methods by iteratively splitting the biggest cluster using the basic k-means algorithm. Basic k-means is a partitional clustering algorithm based on the vector space model. At the heart of such algorithms is a similarity measure. We choose the cosine distance, which measures the similarity of two documents by calculating the cosine of the angle between them. The cosine dist</context>
<context position="13146" citStr="Steinbach et al. (2000)" startWordPosition="2151" endWordPosition="2154"> = |�di |· | di · are both based on precision, where each cluster C from a clustering C of the set of documents D is compared with the manually assigned category labels L from the manual categorisation L, which requires a categorylabeled corpus. Precision is the probability of a document in cluster C being labeled L. Purity is the percentage of correctly clustered documents and can be calculated as: prec(C L) := |C ∩ L| , |C |, (4) � purity(C, L) :_ CEC |C |· max prec(C,L) (5) |D |LEL yielding values in the range between 0 and 1. The intra-cluster entropy (ice) of a cluster C, as described by Steinbach et al. (2000), considers the dispersion of documents in a cluster, and is defined as: � ice(C) :_ LEL prec(C, L) · log(prec(C, L)) (6) Based on the intra-cluster entropy of all clusters, the average, weighted by the cluster size, is calculated. This results in the following formula, which is based on the one used by Steinbach et al. (2000): � entropy(C) :_ CEC |C |· ice(C) (7) |D| Overall similarity is independent of preannotation. Instead the intra-cluster similarities are calculated, giving an idea of the cohesiveness of a cluster. This is the average similarity between each pair of documents in a cluste</context>
</contexts>
<marker>Steinbach, Karypis, Kumar, 2000</marker>
<rawString>M. Steinbach, G. Karypis, and V. Kumar. 2000. A comparison of document clustering techniques. In KDD Workshop on Text Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiri Stetina</author>
</authors>
<title>Corpus Based Natural Language Ambiguity Resolution.</title>
<date>1997</date>
<tech>Ph.D. thesis,</tech>
<institution>Kyoto University.</institution>
<contexts>
<context position="29587" citStr="Stetina, 1997" startWordPosition="4917" endWordPosition="4918">5 gives the best results. A possible explanation could again be the equilibrium between valuable information and noise that are added to the vector representations. From these results it seems that there is a point where the amount of information added reaches its maximum benefit; adding more knowledge afterwards results in decreased cluster quality again. It should be noted that a fixed threshold for the levels of hypernyms used is unlikely to be optimal for all words. Instead, a more refined approach could set this threshold as a function of the semantic distance (Resnik and Yarowsky, 2000; Stetina, 1997) between the word and its hypernyms. The maximised benefit is most evident in the ‘reut-max100’ corpus (Figure 7). However, it needs to be kept in mind that for the last two data points, Hyper 5 and Hyper All, the pruning threshold is 200. Therefore, the comparison with Syns needs to be done with care. This is not much of a problem, because the other graphs consistently show that the performance for Syns is worse than for Hyper 5. The difference between Hyper 5 and Hyper All in ‘reut-max100’, can be directly compared though, because the pruning thresh16 clusters 32 clusters 64 clusters 0 0 0 5</context>
</contexts>
<marker>Stetina, 1997</marker>
<rawString>Jiri Stetina. 1997. Corpus Based Natural Language Ambiguity Resolution. Ph.D. thesis, Kyoto University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J van Rijsbergen</author>
</authors>
<title>Information Retrieval (Second Edition).</title>
<date>1989</date>
<location>Buttersworth, London.</location>
<marker>van Rijsbergen, 1989</marker>
<rawString>C.J. van Rijsbergen. 1989. Information Retrieval (Second Edition). Buttersworth, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Zamir</author>
<author>O Etzioni</author>
</authors>
<title>Grouper: A dynamic clustering interface to Web search results. Computer Networks,</title>
<date>1999</date>
<pages>31--11</pages>
<location>Amsterdam, Netherlands,</location>
<contexts>
<context position="1844" citStr="Zamir and Etzioni (1999)" startWordPosition="264" endWordPosition="267"> to improve precision and recall of information retrieval systems. More recently, however, driven by the ever increasing amount of text documents available in corporate document repositories and on the Internet, the focus has shifted towards providing ways to efficiently browse large collections of documents and to reorganise search results for display in a structured, often hierarchical manner. The clustering of Internet search results has attracted particular attention. Some recent studies explored the feasibility of clustering ‘in real-time’ and the problem of adequately labeling clusters. Zamir and Etzioni (1999) have created a clustering interface for the metasearch engine ‘HuskySearch’ and Zhang and Dong (2001) present their work on a system called SHOC. The reader is also referred to Vivisimo,i a commercial clustering interface based on results from a number of searchengines. Ways to increase clustering speed are explored in many research papers, and the recent trend towards web-based clustering, requiring real-time performance, does not seem to change this. However, van Rijsbergen points out, “it seems to me a little early in the day to insist on efficiency even before we know much about the behav</context>
</contexts>
<marker>Zamir, Etzioni, 1999</marker>
<rawString>O. Zamir and O. Etzioni. 1999. Grouper: A dynamic clustering interface to Web search results. Computer Networks, Amsterdam, Netherlands, 31(11–16):1361–1374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zhang</author>
<author>Y Dong</author>
</authors>
<title>Semantic, hierarchical, online clustering of web search results.</title>
<date>2001</date>
<booktitle>3rd International Workshop on Web Information and Data Management,</booktitle>
<location>Atlanta,</location>
<contexts>
<context position="1946" citStr="Zhang and Dong (2001)" startWordPosition="280" endWordPosition="283">ever increasing amount of text documents available in corporate document repositories and on the Internet, the focus has shifted towards providing ways to efficiently browse large collections of documents and to reorganise search results for display in a structured, often hierarchical manner. The clustering of Internet search results has attracted particular attention. Some recent studies explored the feasibility of clustering ‘in real-time’ and the problem of adequately labeling clusters. Zamir and Etzioni (1999) have created a clustering interface for the metasearch engine ‘HuskySearch’ and Zhang and Dong (2001) present their work on a system called SHOC. The reader is also referred to Vivisimo,i a commercial clustering interface based on results from a number of searchengines. Ways to increase clustering speed are explored in many research papers, and the recent trend towards web-based clustering, requiring real-time performance, does not seem to change this. However, van Rijsbergen points out, “it seems to me a little early in the day to insist on efficiency even before we know much about the behaviour of clustered files in terms of the effectiveness of retrieval” (van Rijsbergen, 1989). Indeed, it</context>
</contexts>
<marker>Zhang, Dong, 2001</marker>
<rawString>D. Zhang and Y. Dong. 2001. Semantic, hierarchical, online clustering of web search results. 3rd International Workshop on Web Information and Data Management, Atlanta, Georgia.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>