<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000875">
<title confidence="0.990311">
Ensemble-based Semantic Lexicon Induction for Semantic Tagging
</title>
<author confidence="0.992436">
Ashequl Qadir
</author>
<affiliation confidence="0.918649666666667">
University of Utah
School of Computing
Salt Lake City, UT 84112, USA
</affiliation>
<email confidence="0.997326">
asheq@cs.utah.edu
</email>
<author confidence="0.996413">
Ellen Riloff
</author>
<affiliation confidence="0.918786">
University of Utah
School of Computing
Salt Lake City, UT 84112, USA
</affiliation>
<email confidence="0.998699">
riloff@cs.utah.edu
</email>
<sectionHeader confidence="0.995642" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99965125">
We present an ensemble-based framework
for semantic lexicon induction that incorpo-
rates three diverse approaches for semantic
class identification. Our architecture brings
together previous bootstrapping methods for
pattern-based semantic lexicon induction and
contextual semantic tagging, and incorpo-
rates a novel approach for inducing semantic
classes from coreference chains. The three
methods are embedded in a bootstrapping ar-
chitecture where they produce independent
hypotheses, consensus words are added to the
lexicon, and the process repeats. Our results
show that the ensemble outperforms individ-
ual methods in terms of both lexicon quality
and instance-based semantic tagging.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999960333333334">
One of the most fundamental aspects of meaning is
the association between words and semantic cate-
gories, which allows us to understand that a “cow”
is an animal and a “house” is a structure. We will
use the term semantic lexicon to refer to a dictionary
that associates words with semantic classes. Se-
mantic dictionaries are useful for many NLP tasks,
as evidenced by the widespread use of WordNet
(Miller, 1990). However, off-the-shelf resources are
not always sufficient for specialized domains, such
as medicine, chemistry, or microelectronics. Fur-
thermore, in virtually every domain, texts contain
lexical variations that are often missing from dic-
tionaries, such as acronyms, abbreviations, spelling
variants, informal shorthand terms (e.g., “abx” for
“antibiotics”), and composite terms (e.g., “may-
december” or “virus/worm”). To address this prob-
lem, techniques have been developed to automate
the construction of semantic lexicons from text cor-
pora using bootstrapping methods (Riloff and Shep-
herd, 1997; Roark and Charniak, 1998; Phillips and
Riloff, 2002; Thelen and Riloff, 2002; Ng, 2007;
McIntosh and Curran, 2009; McIntosh, 2010), but
accuracy is still far from perfect.
Our research explores the use of ensemble meth-
ods to improve the accuracy of semantic lexicon in-
duction. Our observation is that semantic class as-
sociations can be learned using several fundamen-
tally different types of corpus analysis. Bootstrap-
ping methods for semantic lexicon induction (e.g.,
(Riloff and Jones, 1999; Thelen and Riloff, 2002;
McIntosh and Curran, 2009)) collect corpus-wide
statistics for individual words based on shared con-
textual patterns. In contrast, classifiers for semantic
tagging (e.g., (Collins and Singer, 1999; Niu et al.,
2003; Huang and Riloff, 2010)) label word instances
and focus on the local context surrounding each in-
stance. The difference between these approaches is
that semantic taggers make decisions based on a sin-
gle context and can assign different labels to differ-
ent instances, whereas lexicon induction algorithms
compile corpus statistics from multiple instances of
a word and typically assign each word to a single
semantic category.1 We also hypothesize that coref-
erence resolution can be exploited to infer semantic
</bodyText>
<footnote confidence="0.9781405">
1This approach would be untenable for broad-coverage se-
mantic knowledge acquisition, but within a specialized domain
most words have a dominant word sense. Our experimental re-
sults support this assumption.
</footnote>
<page confidence="0.948086">
199
</page>
<note confidence="0.9732665">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 199–208,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999459153846154">
class labels. Intuitively, if we know that two noun
phrases are coreferent, then they probably belong to
the same high-level semantic category (e.g., “dog”
and “terrier” are both animals).
In this paper, we present an ensemble-based
framework for semantic lexicon induction. We in-
corporate a pattern-based bootstrapping method for
lexicon induction, a contextual semantic tagger, and
a new coreference-based method for lexicon induc-
tion. Our results show that coalescing the decisions
produced by diverse methods produces a better dic-
tionary than any individual method alone.
A second contribution of this paper is an analysis
of the effectiveness of dictionaries for semantic tag-
ging. In principle, an NLP system should be able to
assign different semantic labels to different senses
of a word. But within a specialized domain, most
words have a dominant sense and we argue that us-
ing domain-specific dictionaries for tagging may be
equally, if not more, effective. We analyze the trade-
offs between using an instance-based semantic tag-
ger versus dictionary lookup on a collection of dis-
ease outbreak articles. Our results show that the in-
duced dictionaries yield better performance than an
instance-based semantic tagger, achieving higher ac-
curacy with comparable levels of recall.
</bodyText>
<sectionHeader confidence="0.999694" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999976648148148">
Several techniques have been developed for seman-
tic class induction (also called set expansion) using
bootstrapping methods that consider co-occurrence
statistics based on nouns (Riloff and Shepherd,
1997), syntactic structures (Roark and Charniak,
1998; Phillips and Riloff, 2002), and contextual pat-
terns (Riloff and Jones, 1999; Thelen and Riloff,
2002; McIntosh and Curran, 2008; McIntosh and
Curran, 2009). To improve the accuracy of in-
duced lexicons, some research has incorporated neg-
ative information from human judgements (Vyas
and Pantel, 2009), automatically discovered neg-
ative classes (McIntosh, 2010), and distributional
similarity metrics to recognize concept drift (McIn-
tosh and Curran, 2009). Phillips and Riloff (2002)
used co-training (Blum and Mitchell, 1998) to ex-
ploit three simple classifiers that each recognized a
different type of syntactic structure. The research
most closely related to ours is an ensemble-based
method for automatic thesaurus construction (Cur-
ran, 2002). However, that goal was to acquire fine-
grained semantic information that is more akin to
synonymy (e.g., words similar to “house”), whereas
we associate words with high-level semantic classes
(e.g., a “house” is a transient structure).
Semantic class tagging is closely related to named
entity recognition (NER) (e.g., (Bikel et al., 1997;
Collins and Singer, 1999; Cucerzan and Yarowsky,
1999; Fleischman and Hovy, 2002)). Some boot-
strapping methods have been used for NER (e.g.,
(Collins and Singer, 1999; Niu et al., 2003) to
learn from unannotated texts. However, most NER
systems will not label nominal noun phrases (e.g.,
they will not identify “the dentist” as a person)
or recognize semantic classes that are not associ-
ated with proper named entities (e.g., symptoms).2
ACE mention detection systems (e.g., (ACE, 2007;
ACE, 2008)) can label noun phrases that are asso-
ciated with 5-7 semantic classes and are typically
trained with supervised learning. Recently, (Huang
and Riloff, 2010) developed a bootstrapping tech-
nique that induces a semantic tagger from unanno-
tated texts. We use their system in our ensemble.
There has also been work on extracting semantic
class members from the Web (e.g., (Pas¸ca, 2004; Et-
zioni et al., 2005; Kozareva et al., 2008; Carlson et
al., 2009)). This line of research is fundamentally
different from ours because these techniques benefit
from the vast repository of information available on
the Web and are therefore designed to harvest a wide
swath of general-purpose semantic information. Our
research is aimed at acquiring domain-specific se-
mantic dictionaries using a collection of documents
representing a specialized domain.
</bodyText>
<sectionHeader confidence="0.9980995" genericHeader="method">
3 Ensemble-based Semantic Lexicon
Induction
</sectionHeader>
<subsectionHeader confidence="0.999882">
3.1 Motivation
</subsectionHeader>
<bodyText confidence="0.998960166666667">
Our research combines three fundamentally differ-
ent techniques into an ensemble-based bootstrap-
ping framework for semantic lexicon induction:
pattern-based dictionary induction, contextual se-
mantic tagging, and coreference resolution. Our
motivation for using an ensemble of different tech-
</bodyText>
<footnote confidence="0.8662935">
2Some NER systems will handle special constructions such
as dates and monetary amounts.
</footnote>
<page confidence="0.995593">
200
</page>
<bodyText confidence="0.999978517241379">
niques is driven by the observation that these meth-
ods exploit different types of information to infer se-
mantic class knowledge. The coreference resolver
uses features associated with coreference, such as
syntactic constructions (e.g., appositives, predicate
nominals), word overlap, semantic similarity, prox-
imity, etc. The pattern-based lexicon induction al-
gorithm uses corpus-wide statistics gathered from
the contexts of all instances of a word and compares
them with the contexts of known category members.
The contextual semantic tagger uses local context
windows around words and classifies each word in-
stance independently from the others.
Since each technique draws its conclusions from
different types of information, they represent inde-
pendent sources of evidence to confirm whether a
word belongs to a semantic class. Our hypothe-
sis is that, combining these different sources of ev-
idence in an ensemble-based learning framework
should produce better accuracy than using any one
method alone. Based on this intuition, we create
an ensemble-based bootstrapping framework that it-
eratively collects the hypotheses produced by each
individual learner and selects the words that were
hypothesized by at least 2 of the 3 learners. This
approach produces a bootstrapping process with
improved precision, both at the critical beginning
stages of the bootstrapping process and during sub-
sequent bootstrapping iterations.
</bodyText>
<subsectionHeader confidence="0.999942">
3.2 Component Systems in the Ensemble
</subsectionHeader>
<bodyText confidence="0.9998895">
In the following sections, we describe each of the
component systems used in our ensemble.
</bodyText>
<subsectionHeader confidence="0.76453">
3.2.1 Pattern-based Lexicon Induction
</subsectionHeader>
<bodyText confidence="0.995866428571429">
The first component of our ensemble is Basilisk
(Thelen and Riloff, 2002), which identifies nouns
belonging to a semantic class based on collec-
tive information over lexico-syntactic pattern con-
texts. The patterns are automatically generated us-
ing AutoSlog-TS (Riloff, 1996). Basilisk begins
with a small set of seed words for each seman-
tic class and a collection of unannotated documents
for the domain. In an iterative bootstrapping pro-
cess, Basilisk identifies candidate nouns, ranks them
based on its scoring criteria, selects the 5 most confi-
dent words for inclusion in the lexicon, and this pro-
cess repeats using the new words as additional seeds
in subsequent iterations.
</bodyText>
<subsectionHeader confidence="0.487611">
3.2.2 Lexicon Induction with a Contextual
Semantic Tagger
</subsectionHeader>
<bodyText confidence="0.999988722222222">
The second component in our ensemble is a con-
textual semantic tagger (Huang and Riloff, 2010).
Like Basilisk, the semantic tagger also begins with
seed nouns, trains itself on a large collection of
unannotated documents using bootstrapping, and it-
eratively labels new instances. This tagger labels
noun instances and does not produce a dictionary.
To adapt it for our purposes, we ran the bootstrap-
ping process over the training texts to induce a se-
mantic classifier. We then applied the classifier to
the same set of training documents and compiled a
lexicon by collecting the set of nouns that were as-
signed to each semantic class. We ignored words
that were assigned different labels in different con-
texts to avoid conflicts in the lexicons. We used
the identical configuration described by (Huang and
Riloff, 2010) that applies a 1.0 confidence threshold
for semantic class assignment.
</bodyText>
<subsectionHeader confidence="0.820049">
3.2.3 Coreference-Based Lexicon Construction
</subsectionHeader>
<bodyText confidence="0.999918388888889">
The third component of our ensemble is a new
method for semantic lexicon induction that exploits
coreference resolution. Members of a coreference
chain represent the same entity, so all references to
the entity should belong to the same semantic class.
For example, suppose “Paris” and “the city” are in
the same coreference chain. If we know that city is
a Fixed Location, then we can infer that Paris is also
a Fixed Location.
We induced lexicons from coreference chains us-
ing a similar bootstrapping framework that begins
with seed nouns and unannotated texts. Let 5 de-
note a set of semantic classes and W denote a set of
unknown words. For any s E 5 and w E W, let
N,,,, denote the number of instances of s in the cur-
rent lexicon3 that are coreferent with w in the text
corpus. Then we estimate the probability that word
w belongs to semantic class s as:
</bodyText>
<equation confidence="0.957646">
P(s |w) = Ns,w
l rls&apos;ES Ns&apos;,w
</equation>
<bodyText confidence="0.6998035">
We hypothesize the semantic class of w,
5emClass(w) by:
</bodyText>
<equation confidence="0.8352">
5emClass(w) = arg max, P(s|w)
</equation>
<footnote confidence="0.826122">
3In the first iteration, the lexicon is initialized with the seeds.
</footnote>
<page confidence="0.994475">
201
</page>
<bodyText confidence="0.999972823529412">
To ensure high precision for the induced lexicons,
we use a threshold of 0.5. All words with a prob-
ability above this thresold are added to the lexicon,
and the bootstrapping process repeats. Although the
coreference chains remain the same throughout the
process, the lexicon grows so more words in the
chains have semantic class labels as bootstrapping
progresses. Bootstrapping ends when fewer than 5
words are learned for each of the semantic classes.
Many noun phrases are singletons (i.e., they are
not coreferent with any other NPs), which limits the
set of words that can be learned using coreference
chains. Furthermore, coreference resolvers make
mistakes, so the accuracy of the induced lexicons
depends on the quality of the chains. For our experi-
ments, we used Reconcile (Stoyanov et al., 2010), a
freely available supervised coreference resolver.
</bodyText>
<subsectionHeader confidence="0.5031675">
3.3 Ensemble-based Bootstrapping
Framework
</subsectionHeader>
<bodyText confidence="0.998878769230769">
Figure 1 shows the architecture of our ensemble-
based bootstrapping framework. Initially, each lexi-
con only contains the seed nouns. Each component
hypothesizes a set of candidate words for each se-
mantic class, based on its own criteria. The word
lists produced by the three systems are then com-
pared, and we retain only the words that were hy-
pothesized with the same class label by at least two
of the three systems. The remaining words are dis-
carded. The consenus words are added to the lexi-
con, and the bootstrapping process repeats. As soon
as fewer than 5 words are learned for each of the
semantic classes, bootstrapping stops.
</bodyText>
<figureCaption confidence="0.991471">
Figure 1: Ensemble-based bootstrapping framework
</figureCaption>
<bodyText confidence="0.993524375">
We ran each individual system with the same seed
words. Since bootstrapping typically yields the best
precision during the earliest stages, we used the se-
mantic tagger’s trained model immediately after its
first bootstrapping iteration. Basilisk generates 5
words per cycle, so we report results for lexicons
generated after 20 bootstrapping cycles (100 words)
and after 80 bootstrapping cycles (400 words).
</bodyText>
<subsectionHeader confidence="0.904859">
3.4 Co-Training Framework
</subsectionHeader>
<bodyText confidence="0.999969333333333">
The three components in our ensemble use different
types of features (views) to identify semantic class
members, so we also experimented with co-training.
Our co-training model uses an identical framework,
but the hypotheses produced by the different meth-
ods are all added to the lexicon, so each method can
benefit from the hypotheses produced by the others.
To be conservative, each time we added only the 10
most confident words hypothesized by each method.
In contrast, the ensemble approach only adds
words to the lexicon if they are hypothesized by two
different methods. As we will see in Section 4.4,
the ensemble performs much better than co-training.
The reason is that the individual methods do not con-
sistently achieve high precision on their own. Con-
sequently, many mistakes are added to the lexicon,
which is used as training data for subsequent boot-
strapping. The benefit of the ensemble is that con-
sensus is required across two methods, which serves
as a form of cross-checking to boost precision and
maintain a high-quality lexicon.
</bodyText>
<sectionHeader confidence="0.999788" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.992849">
4.1 Semantic Class Definitions
</subsectionHeader>
<bodyText confidence="0.9981036">
We evaluated our approach on nine semantic cate-
gories associated with disease outbreaks. The se-
mantic classes are defined below.
Animal: Mammals, birds, fish, insects and other
animal groups. (e.g., cow, crow, mosquito, herd)
</bodyText>
<footnote confidence="0.995898181818182">
4http://www.nlm.nih.gov/research/umls/
5http://www.maxmind.com/app/worldcities
6http://www.listofcountriesoftheworld.
com/
7http://names.mongabay.com/most_common_
surnames.htm
8http://www.sec.gov/rules/other/
4-460list.htm
9http://www.utexas.edu/world/univ/state/
10http://www.uta.fi/FAST/GC/usabacro.
html/
</footnote>
<page confidence="0.966421">
202
</page>
<table confidence="0.9993813125">
Semantic External Word List Sources
Class
Animal WordNet: [animal], [mammal family], [animal group]
Body Part WordNet: [body part], [body substance], [body covering], [body waste]
DisSym WordNet: [symptom], [physical condition], [infectious agent]; Wikipedia: common and infectious
diseases, symptoms, disease acronyms; UMLS Thesaurus¢: diseases, abnormalities, microorganisms
(Archaea, Bacteria, Fungus, Virus)
Fixed Loc. WordNet: [geographic area], [land], [district, territory], [region]; Wiki:US-states; Other:cities5, countriess
Human WordNet: [person], [people], [personnel]; Wikipedia: people names, office holder titles, nationalities,
occupations, medical personnels &amp; acronyms, players; Other: common people names &amp; surnames
Org WordNet: [organization], [assembly]; Wikipedia: acronyms in healthcare, medical organization acronyms,
news agencies, pharmaceutical companies; Other: companies8, US-universities9, organizations10
Plant &amp; Food WordNet: [food], [plant, flora], [plant part]
Temp. Ref. WordNet: [time], [time interval], [time unit],[time period]
TimeBank: TimeBank1.2 (Pustejovsky et al., 2003) TIMEX3 expressions
Trans. Struct. WordNet: [structure, construction], [road, route], [facility, installation], [work place]
</table>
<tableCaption confidence="0.999322">
Table 1: External Word List Sources
</tableCaption>
<bodyText confidence="0.97495378125">
Body Part: A part of a human or animal body, in-
cluding organs, bodily fluids, and microscopic parts.
(e.g., hand, heart, blood, DNA)
Diseases and Symptoms (DisSym): Diseases
and symptoms. We also include fungi and disease
carriers because, in this domain, they almost always
refer to the disease that they carry. (e.g. FMD, An-
thrax, fever, virus)
Fixed Location (Fixed Loc.): Named locations,
including countries, cities, states, etc. We also in-
clude directions and well-defined geographic areas
or geo-political entities. (e.g., Brazil, north, valley)
Human: All references to people, including
names, titles, professions, and groups. (e.g., John,
farmer, traders)
Organization (Org.): An entity that represents a
group of people acting as a single recognized body,
including named organizations, departments, gov-
ernments, and their acronyms. (e.g., department,
WHO, commission, council)
Temporal Reference (Temp. Ref.): Any refer-
ence to a time or duration, including months, days,
seasons, etc. (e.g., night, May, summer, week)
Plants &amp; Food11: plants, plant parts, or any type
of food. (e.g., seed, mango, beef, milk)
Transient Structures (Trans. Struct.): Transient
physical structures. (e.g., hospital, building, home)
Additionally, we defined a Miscellaneous class
for words that do not belong to any of the other cat-
11We merged plants and food into a single category as it is
difficult to separate them because many food items are plants.
egories. (e.g., output, information, media, point).
</bodyText>
<subsectionHeader confidence="0.974372">
4.2 Data Set
</subsectionHeader>
<bodyText confidence="0.999989384615385">
We ran our experiments on ProMED-mail12 articles.
ProMED-mail is an internet based reporting system
for infectious disease outbreaks, which can involve
people, animals, and plants grown for food. Our
ProMED corpus contains 5004 documents. We used
4959 documents as (unannotated) training data for
bootstrapping. For the remaining 45 documents,
we used 22 documents to train the coreference re-
solver (Reconcile) and 23 documents as our test set.
The coreference training set contains MUC-7 style
(Hirschman, 1997) coreference annotations13. Once
trained, Reconcile was applied to the 4959 unanno-
tated documents to produce coreference chains.
</bodyText>
<subsectionHeader confidence="0.999261">
4.3 Gold Standard Semantic Class Annotations
</subsectionHeader>
<bodyText confidence="0.999961625">
To obtain gold standard annotations for the test set,
two annotators assigned one of the 9 semantic class
labels, or Miscellaneous, to each head noun based on
its surrounding context. A noun with multiple senses
could get assigned different semantic class labels in
different contexts. The annotators first annotated 13
of the 23 documents, and discussed the cases where
they disagreed. Then they independelty annotated
</bodyText>
<footnote confidence="0.9516352">
12http://www.promedmail.org/
13We omit the details of the coreference annotations since
it is not the focus of this research. However, the annotators
measured their agreement on 10 documents and achieved MUC
scores of Precision = .82, Recall = .86, F-measure = .84.
</footnote>
<page confidence="0.999153">
203
</page>
<bodyText confidence="0.9997735">
the remaining 10 documents and measured inter-
annotator agreement with Cohen’s Kappa (n) (Car-
letta, 1996). The n score for these 10 documents was
0.91, indicating a high level of agreement. The an-
notators then adjudicated their disagreements on all
23 documents to create the gold standard.
</bodyText>
<subsectionHeader confidence="0.996123">
4.4 Dictionary Evaluation
</subsectionHeader>
<bodyText confidence="0.999960078947368">
To assess the quality of the lexicons, we estimated
their accuracy by compiling external word lists
from freely available sources such as Wikipedia14
and WordNet (Miller, 1990). Table 1 shows the
sources that we used, where the bracketed items re-
fer to WordNet hypernym categories. We searched
each WordNet hypernym tree (also, instance-
relationship) for all senses of the word. Addition-
ally, we collected the manually labeled words in our
test set and included them in our gold standard lists.
Since the induced lexicons contain individual
nouns, we extracted only the head nouns of multi-
word phrases in the external resources. This
can produce incorrect entries for non-compositional
phrases, but we found this issue to be relatively rare
and we manually removed obviously wrong entries.
We adopted a conservative strategy and assumed that
any lexicon entries not present in our gold standard
lists are incorrect. But we observed many correct en-
tries that were missing from the external resources,
so our results should be interpreted as a lower bound
on the true accuracy of the induced lexicons.
We generated lexicons for each method sepa-
rately, and also for the ensemble and co-training
models. We ran Basilisk for 100 iterations (500
words). We refer to a Basilisk lexicon of size N
using the notation B[N]. For example, B400 refers
to a lexicon containing 400 words, which was gen-
erated from 80 bootstrapping cycles. We refer to the
lexicon obtained from the semantic tagger as ST Lex.
Figure 2 shows the dictionary evaluation results.
We plotted Basilisk’s accuracy after every 5 boot-
strapping cycles (25 words). For ST Lex, we sorted
the words by their confidence scores and plotted the
accuracy of the top-ranked words in increments of
50. The plots for Coref, Co-Training, and Ensemble
B[N] are based on the lexicons produced after each
bootstrapping cycle.
</bodyText>
<footnote confidence="0.583804">
14www.wikipedia.org/
</footnote>
<bodyText confidence="0.999889466666667">
The ensemble-based framework yields consis-
tently better accuracy than the individual methods
for Animal, Body Part, Human and Temporal Refer-
ence, and similar if not better for Disease &amp; Symp-
tom, Fixed Location, Organization, Plant &amp; Food.
However, relying on consensus from multiple mod-
els produce smaller dictionaries. Big dictionaries are
not always better than small dictionaries in practice,
though. We believe, it matters more whether a dic-
tionary contains the most frequent words for a do-
main, because they account for a disproportionate
number of instances. Basilisk, for example, often
learns infrequent words, so its dictionaries may have
high accuracy but often fail to recognize common
words. We investigate this issue in the next section.
</bodyText>
<subsectionHeader confidence="0.992348">
4.5 Instance-based Tagging Evaluation
</subsectionHeader>
<bodyText confidence="0.9805476">
We also evaluated the effectiveness of the induced
lexicons with respect to instance-based semantic
tagging. Our goal was to determine how useful the
dictionaries are in two respects: (1) do the lexicons
contain words that appear frequently in the domain,
and (2) is dictionary look-up sufficient for instance-
based labeling? Our bootstrapping processes en-
force a constraint that a word can only belong to one
semantic class, so if polysemy is common, then dic-
tionary look-up will be problematic.15
The instance-based evaluation assigns a semantic
label to each instance of a head noun. When using a
lexicon, all instances of the same noun are assigned
the same semantic class via dictionary look-up. The
semantic tagger (SemTag), however, is applied di-
rectly since it was designed to label instances.
Table 2 presents the results. As a baseline, the
W.Net row shows the performance of WordNet for
instance tagging. For words with multiple senses,
we only used the first sense listed in WordNet.
The Seeds row shows the results when perform-
ing dictionary look-up using only the seed words.
The remaining rows show the results for Basilisk
(B100 and B400), coreference-based lexicon induc-
tion (Coref), lexicon induction using the semantic
tagger (ST Lex), and the original instance-based tag-
ger (SemTag). The following rows show the results
for co-training (after 4 iterations and 20 iterations)
15Only coarse polysemy across semantic classes is an issue
(e.g., “plant” as a living thing vs. a factory).
</bodyText>
<page confidence="0.997604">
204
</page>
<figureCaption confidence="0.999414">
Figure 2: Dictionary Evaluation Results
</figureCaption>
<bodyText confidence="0.999887692307692">
and for the ensemble (using Basilisk size 100 and
size 400). Table 3 shows the micro &amp; macro average
results across all semantic categories.
Table 3 shows that the dictionaries produced by
the Ensemble w/B100 achieved better results than
the individual methods and co-training with an F
score of 80%. Table 2 shows that the ensemble
achieved better performance than the other methods
for 4 of the 9 classes, and was usually competitive
on the remaining 5 classes. WordNet (W.Net) con-
sistently produced high precision, but with compar-
atively lower recall, indicating that WordNet does
not have sufficient coverage for this domain.
</bodyText>
<subsectionHeader confidence="0.996497">
4.6 Analysis
</subsectionHeader>
<bodyText confidence="0.999985684210526">
Table 4 shows the performance of our ensemble
when using only 2 of the 3 component methods.
Removing any one method decreases the average
F-measure by at least 3-5%. Component pairs
that include induced lexicons from coreference (ST
Lex+Coref and B100+Coref) yield high precision
but low recall. The component pair ST Lex+B100
produces higher recall but with slightly lower accu-
racy. The ensemble framework boosted recall even
more, while maintaining the same precision.
We observe that some of the smallest lexicons
produced the best results for instance-based seman-
tic tagging (e.g., Organization). Our hypothesis is
that consensus decisions across different methods
helps to promote the acquisition of high frequency
domain words, which are crucial to have in the dic-
tionary. The fact that dictionary look-up performed
better than an instance-based semantic tagger also
suggests that coarse polysemy (different senses that
</bodyText>
<page confidence="0.996035">
205
</page>
<table confidence="0.999914">
Method Animal Body DisSym Fixed Human Org. Plant &amp; Temp. Trans.
P R F Part P R F Loc. P R F P R F Food Ref. Struct.
P R F P R F P R F P R F P R F
Individual Methods
W.Net 92 88 90 93 59 72 99 77 87 86 58 69 83 55 66 86 44 59 65 79 71 93 85 89 85 64 73
Seeds 100 54 70 92 55 69 100 59 74 95 10 18 100 22 36 100 41 58 100 61 76 100 52 69 100 09 17
B100 99 77 86 94 73 82 100 66 80 96 23 37 96 31 47 91 58 71 82 64 72 68 83 75 67 22 33
B400 94 90 92 51 86 64 100 69 81 97 35 51 91 51 65 79 77 78 46 82 59 49 94 64 83 78 80
Coref 90 67 77 92 55 69 66 83 73 65 46 54 57 50 53 54 68 60 81 61 69 60 74 67 45 09 15
ST Lex 94 89 91 68 77 72 80 91 85 91 74 82 79 43 55 84 62 71 51 68 58 73 91 81 82 49 61
SemTag 91 90 90 52 68 59 77 90 83 91 78 84 81 48 60 80 63 70 43 82 56 77 93 84 83 53 64
Co-Training
pass4 64 76 70 67 73 70 91 79 85 91 39 54 98 44 61 83 69 76 43 68 53 73 94 82 49 36 42
pass20 60 89 71 56 91 69 88 91 90 83 64 72 92 54 68 72 77 74 28 71 40 65 98 78 46 40 43
Ensembles
w/B100 93 94 94 74 77 76 93 81 86 92 73 81 94 55 70 90 78 84 56 89 68 55 94 70 79 75 77
w/B400 94 93 93 65 91 75 96 87 91 89 75 81 92 56 70 79 79 79 47 86 61 53 94 68 63 55 58
</table>
<tableCaption confidence="0.916206">
Table 2: Instance-based Semantic Tagging Results (P = Precision, R = Recall, F = F-measure)
</tableCaption>
<table confidence="0.9999180625">
Method Micro Average Macro Average
P R F P R F
Individual Systems
W.Net 88 66 75 87 68 76
Seeds 99 35 52 99 40 57
B100 89 50 64 88 55 68
B400 77 66 71 77 74 75
Coref 65 59 62 68 57 62
ST Lex 82 72 77 78 72 75
SemTag 80 74 77 75 74 74
Co-Training
pass4 77 61 68 73 64 68
pass20 69 74 71 65 75 70
Ensembles
w/B100 83 77 80 81 80 80
w/B400 79 78 78 75 79 77
</table>
<tableCaption confidence="0.999915">
Table 3: Micro &amp; Macro Average for Semantic Tagging
</tableCaption>
<bodyText confidence="0.9974415">
cut across semantic classes) is a relatively minor is-
sue within a specialized domain.
</bodyText>
<sectionHeader confidence="0.999441" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9999288">
Our research combined three diverse methods
for semantic lexicon induction in a bootstrapped
ensemble-based framework, including a novel ap-
proach for lexicon induction based on coreference
chains. Our ensemble-based approach performed
better than the individual methods, in terms of
both dictionary accuracy and instance-based seman-
tic tagging. In future work, we believe this ap-
proach could be enhanced further by adding new
types of techniques to the ensemble and by investi-
</bodyText>
<table confidence="0.999380375">
Method Micro Average Macro Average
P R F P R F
Ensemble with component pairs
ST Lex+Coref 92 59 72 92 57 70
B100+Coref 92 40 56 94 44 60
ST Lex+B100 82 69 75 81 75 77
Ensemble with all components
ST Lex+B100+Coref 83 77 80 81 80 80
</table>
<tableCaption confidence="0.999718">
Table 4: Ablation Study of the Ensemble Framework for
</tableCaption>
<subsectionHeader confidence="0.574834">
Semantic Tagging
</subsectionHeader>
<bodyText confidence="0.980803">
gating better methods for estimating the confidence
scores from the individual components.
</bodyText>
<sectionHeader confidence="0.997665" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999962230769231">
We are grateful to Lalindra de Silva for manually
annotating data, Nathan Gilbert for help with Rec-
oncile, and Ruihong Huang for help with the se-
mantic tagger. We gratefully acknowledge the sup-
port of the National Science Foundation under grant
IIS-1018314 and the Defense Advanced Research
Projects Agency (DARPA) Machine Reading Pro-
gram under Air Force Research Laboratory (AFRL)
prime contract no. FA8750-09-C-0172. Any opin-
ions, findings, and conclusions or recommenda-
tions expressed in this material are those of the au-
thors and do not necessarily reflect the view of the
DARPA, AFRL, or the U.S. government.
</bodyText>
<page confidence="0.99793">
206
</page>
<sectionHeader confidence="0.982168" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999280419047619">
ACE. 2007. NIST ACE evaluation website. In
http://www.nist.gov/speech/tests/ace/2007.
ACE. 2008. NIST ACE evaluation website. In
http://www.nist.gov/speech/tests/ace/2008.
Daniel M. Bikel, Scott Miller, Richard Schwartz, and
Ralph Weischedel. 1997. Nymble: a high-
performance learning name-finder. In Proceedings of
ANLP-97, pages 194–201.
A. Blum and T. Mitchell. 1998. Combining Labeled and
Unlabeled Data with Co-Training. In Proceedings of
the 11th Annual Conference on Computational Learn-
ing Theory (COLT-98).
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: the kappa statistic. Comput. Linguist.,
22:249–254, June.
Andrew Carlson, Justin Betteridge, Estevam R. Hr-
uschka Jr., and Tom M. Mitchell. 2009. Coupling
semi-supervised learning of categories and relations.
In HLT-NAACL 2009 Workshop on Semi-Supervised
Learning for NLP.
M. Collins and Y. Singer. 1999. Unsupervised Mod-
els for Named Entity Classification. In Proceedings
of the Joint SIGDAT Conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora (EMNLP/VLC-99).
S. Cucerzan and D. Yarowsky. 1999. Language Inde-
pendent Named Entity Recognition Combining Mor-
phologi cal and Contextual Evidence. In Proceedings
of the Joint SIGDAT Conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora (EMNLP/VLC-99).
J. Curran. 2002. Ensemble Methods for Automatic The-
saurus Extraction. In Proceedings of the 2002 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from the
web: an experimental study. Artificial Intelligence,
165(1):91–134, June.
M.B. Fleischman and E.H. Hovy. 2002. Fine grained
classification of named entities. In Proceedings of the
COLING conference, August.
L. Hirschman. 1997. MUC-7 Coreference Task Defini-
tion.
Ruihong Huang and Ellen Riloff. 2010. Inducing
domain-specific semantic class taggers from (almost)
nothing. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics.
Z. Kozareva, E. Riloff, and E. Hovy. 2008. Semantic
Class Learning from the Web with Hyponym Pattern
Linkage Graphs. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies (ACL-08).
T. McIntosh and J. Curran. 2008. Weighted mutual
exclusion bootstrapping for domain independent lex-
icon and template acquisition. In Proceedings of the
Australasian Language Technology Association Work-
shop.
T. McIntosh and J. Curran. 2009. Reducing Semantic
Drift with Bagging and Distributional Similarity. In
Proceedings of the 47th Annual Meeting of the Associ-
ation for Computational Linguistics.
T. McIntosh. 2010. Unsupervised Discovery of Negative
Categories in Lexicon Bootstrapping. In Proceedings
of the 2010 Conference on Empirical Methods in Nat-
ural Language Processing.
G. Miller. 1990. Wordnet: An On-line Lexical Database.
International Journal of Lexicography, 3(4).
V. Ng. 2007. Semantic Class Induction and Coreference
Resolution. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics.
Cheng Niu, Wei Li, Jihong Ding, and Rohini K. Srihari.
2003. A bootstrapping approach to named entity clas-
sification using successive learners. In Proceedings of
the 41st Annual Meeting on Association for Computa-
tional Linguistics (ACL-03), pages 335–342.
M. Pas¸ca. 2004. Acquisition of categorized named en-
tities for web search. In Proc. of the Thirteenth ACM
International Conference on Information and Knowl-
edge Management, pages 137–145.
W. Phillips and E. Riloff. 2002. Exploiting Strong Syn-
tactic Heuristics and Co-Training to Learn Semantic
Lexicons. In Proceedings of the 2002 Conference on
Empirical Methods in Natural Language Processing,
pages 125–132.
J. Pustejovsky, P. Hanks, R. Sauri, A. See, R. Gaizauskas,
A. Setzer, D. Radev, B. Sundheim, D. Day, L. Ferro,
and M. Lazo. 2003. The TIMEBANK Corpus. In
Proceedings of Corpus Linguistics 2003, pages 647–
656.
E. Riloff and R. Jones. 1999. Learning Dictionaries for
Information Extraction by Multi-Level Bootstrapping.
In Proceedings of the Sixteenth National Conference
on Artificial Intelligence.
E. Riloff and J. Shepherd. 1997. A Corpus-Based Ap-
proach for Building Semantic Lexicons. In Proceed-
ings of the Second Conference on Empirical Methods
in Natural Language Processing, pages 117–124.
E. Riloff. 1996. Automatically Generating Extraction
Patterns from Untagged Text. In Proceedings of the
Thirteenth National Conference on Artificial Intelli-
gence, pages 1044–1049. The AAAI Press/MIT Press.
B. Roark and E. Charniak. 1998. Noun-phrase Co-
occurrence Statistics for Semi-automatic Semantic
</reference>
<page confidence="0.968659">
207
</page>
<reference confidence="0.999455411764706">
Lexicon Construction. In Proceedings of the 36th
Annual Meeting of the Association for Computational
Linguistics, pages 1110–1116.
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen
Riloff, David Buttler, and David Hysom. 2010. Coref-
erence resolution with Reconcile. In Proceedings of
the ACL 2010 Conference Short Papers, pages 156–
161.
M. Thelen and E. Riloff. 2002. A Bootstrapping Method
for Learning Semantic Lexicons Using Extraction Pa
ttern Contexts. In Proceedings of the 2002 Conference
on Empirical Methods in Natural Language Process-
ing, pages 214–221.
V. Vyas and P. Pantel. 2009. Semi-automatic entity set
refinement. In Proceedings of North American Asso-
ciation for Computational Linguistics / Human Lan-
guage Technology (NAACL/HLT-09).
</reference>
<page confidence="0.997777">
208
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.448159">
<title confidence="0.995127">Ensemble-based Semantic Lexicon Induction for Semantic Tagging</title>
<author confidence="0.885266">Ashequl</author>
<affiliation confidence="0.9971105">University of School of</affiliation>
<address confidence="0.733568">Salt Lake City, UT 84112,</address>
<email confidence="0.998387">asheq@cs.utah.edu</email>
<author confidence="0.968929">Ellen</author>
<affiliation confidence="0.997744">University of School of</affiliation>
<address confidence="0.723244">Salt Lake City, UT 84112,</address>
<email confidence="0.999808">riloff@cs.utah.edu</email>
<abstract confidence="0.999483">We present an ensemble-based framework for semantic lexicon induction that incorporates three diverse approaches for semantic class identification. Our architecture brings together previous bootstrapping methods for pattern-based semantic lexicon induction and contextual semantic tagging, and incorporates a novel approach for inducing semantic classes from coreference chains. The three methods are embedded in a bootstrapping architecture where they produce independent hypotheses, consensus words are added to the lexicon, and the process repeats. Our results show that the ensemble outperforms individual methods in terms of both lexicon quality and instance-based semantic tagging.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>ACE</author>
</authors>
<title>NIST ACE evaluation website.</title>
<date>2007</date>
<booktitle>In http://www.nist.gov/speech/tests/ace/2007.</booktitle>
<contexts>
<context position="6733" citStr="ACE, 2007" startWordPosition="1001" endWordPosition="1002">ansient structure). Semantic class tagging is closely related to named entity recognition (NER) (e.g., (Bikel et al., 1997; Collins and Singer, 1999; Cucerzan and Yarowsky, 1999; Fleischman and Hovy, 2002)). Some bootstrapping methods have been used for NER (e.g., (Collins and Singer, 1999; Niu et al., 2003) to learn from unannotated texts. However, most NER systems will not label nominal noun phrases (e.g., they will not identify “the dentist” as a person) or recognize semantic classes that are not associated with proper named entities (e.g., symptoms).2 ACE mention detection systems (e.g., (ACE, 2007; ACE, 2008)) can label noun phrases that are associated with 5-7 semantic classes and are typically trained with supervised learning. Recently, (Huang and Riloff, 2010) developed a bootstrapping technique that induces a semantic tagger from unannotated texts. We use their system in our ensemble. There has also been work on extracting semantic class members from the Web (e.g., (Pas¸ca, 2004; Etzioni et al., 2005; Kozareva et al., 2008; Carlson et al., 2009)). This line of research is fundamentally different from ours because these techniques benefit from the vast repository of information avai</context>
</contexts>
<marker>ACE, 2007</marker>
<rawString>ACE. 2007. NIST ACE evaluation website. In http://www.nist.gov/speech/tests/ace/2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ACE</author>
</authors>
<title>NIST ACE evaluation website.</title>
<date>2008</date>
<booktitle>In http://www.nist.gov/speech/tests/ace/2008.</booktitle>
<contexts>
<context position="6745" citStr="ACE, 2008" startWordPosition="1003" endWordPosition="1004">ucture). Semantic class tagging is closely related to named entity recognition (NER) (e.g., (Bikel et al., 1997; Collins and Singer, 1999; Cucerzan and Yarowsky, 1999; Fleischman and Hovy, 2002)). Some bootstrapping methods have been used for NER (e.g., (Collins and Singer, 1999; Niu et al., 2003) to learn from unannotated texts. However, most NER systems will not label nominal noun phrases (e.g., they will not identify “the dentist” as a person) or recognize semantic classes that are not associated with proper named entities (e.g., symptoms).2 ACE mention detection systems (e.g., (ACE, 2007; ACE, 2008)) can label noun phrases that are associated with 5-7 semantic classes and are typically trained with supervised learning. Recently, (Huang and Riloff, 2010) developed a bootstrapping technique that induces a semantic tagger from unannotated texts. We use their system in our ensemble. There has also been work on extracting semantic class members from the Web (e.g., (Pas¸ca, 2004; Etzioni et al., 2005; Kozareva et al., 2008; Carlson et al., 2009)). This line of research is fundamentally different from ours because these techniques benefit from the vast repository of information available on the</context>
</contexts>
<marker>ACE, 2008</marker>
<rawString>ACE. 2008. NIST ACE evaluation website. In http://www.nist.gov/speech/tests/ace/2008.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Daniel M Bikel</author>
<author>Scott Miller</author>
</authors>
<location>Richard Schwartz, and</location>
<marker>Bikel, Miller, </marker>
<rawString>Daniel M. Bikel, Scott Miller, Richard Schwartz, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Weischedel</author>
</authors>
<title>Nymble: a highperformance learning name-finder.</title>
<date>1997</date>
<booktitle>In Proceedings of ANLP-97,</booktitle>
<pages>194--201</pages>
<marker>Weischedel, 1997</marker>
<rawString>Ralph Weischedel. 1997. Nymble: a highperformance learning name-finder. In Proceedings of ANLP-97, pages 194–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Blum</author>
<author>T Mitchell</author>
</authors>
<title>Combining Labeled and Unlabeled Data with Co-Training.</title>
<date>1998</date>
<booktitle>In Proceedings of the 11th Annual Conference on Computational Learning Theory (COLT-98).</booktitle>
<contexts>
<context position="5687" citStr="Blum and Mitchell, 1998" startWordPosition="837" endWordPosition="840">tistics based on nouns (Riloff and Shepherd, 1997), syntactic structures (Roark and Charniak, 1998; Phillips and Riloff, 2002), and contextual patterns (Riloff and Jones, 1999; Thelen and Riloff, 2002; McIntosh and Curran, 2008; McIntosh and Curran, 2009). To improve the accuracy of induced lexicons, some research has incorporated negative information from human judgements (Vyas and Pantel, 2009), automatically discovered negative classes (McIntosh, 2010), and distributional similarity metrics to recognize concept drift (McIntosh and Curran, 2009). Phillips and Riloff (2002) used co-training (Blum and Mitchell, 1998) to exploit three simple classifiers that each recognized a different type of syntactic structure. The research most closely related to ours is an ensemble-based method for automatic thesaurus construction (Curran, 2002). However, that goal was to acquire finegrained semantic information that is more akin to synonymy (e.g., words similar to “house”), whereas we associate words with high-level semantic classes (e.g., a “house” is a transient structure). Semantic class tagging is closely related to named entity recognition (NER) (e.g., (Bikel et al., 1997; Collins and Singer, 1999; Cucerzan and </context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>A. Blum and T. Mitchell. 1998. Combining Labeled and Unlabeled Data with Co-Training. In Proceedings of the 11th Annual Conference on Computational Learning Theory (COLT-98).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: the kappa statistic.</title>
<date>1996</date>
<journal>Comput. Linguist.,</journal>
<pages>22--249</pages>
<contexts>
<context position="20247" citStr="Carletta, 1996" startWordPosition="3015" endWordPosition="3017">ext. A noun with multiple senses could get assigned different semantic class labels in different contexts. The annotators first annotated 13 of the 23 documents, and discussed the cases where they disagreed. Then they independelty annotated 12http://www.promedmail.org/ 13We omit the details of the coreference annotations since it is not the focus of this research. However, the annotators measured their agreement on 10 documents and achieved MUC scores of Precision = .82, Recall = .86, F-measure = .84. 203 the remaining 10 documents and measured interannotator agreement with Cohen’s Kappa (n) (Carletta, 1996). The n score for these 10 documents was 0.91, indicating a high level of agreement. The annotators then adjudicated their disagreements on all 23 documents to create the gold standard. 4.4 Dictionary Evaluation To assess the quality of the lexicons, we estimated their accuracy by compiling external word lists from freely available sources such as Wikipedia14 and WordNet (Miller, 1990). Table 1 shows the sources that we used, where the bracketed items refer to WordNet hypernym categories. We searched each WordNet hypernym tree (also, instancerelationship) for all senses of the word. Additional</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Jean Carletta. 1996. Assessing agreement on classification tasks: the kappa statistic. Comput. Linguist., 22:249–254, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Carlson</author>
<author>Justin Betteridge</author>
<author>Estevam R Hruschka Jr</author>
<author>Tom M Mitchell</author>
</authors>
<title>Coupling semi-supervised learning of categories and relations.</title>
<date>2009</date>
<booktitle>In HLT-NAACL 2009 Workshop on Semi-Supervised Learning for NLP.</booktitle>
<contexts>
<context position="7194" citStr="Carlson et al., 2009" startWordPosition="1075" endWordPosition="1078">as a person) or recognize semantic classes that are not associated with proper named entities (e.g., symptoms).2 ACE mention detection systems (e.g., (ACE, 2007; ACE, 2008)) can label noun phrases that are associated with 5-7 semantic classes and are typically trained with supervised learning. Recently, (Huang and Riloff, 2010) developed a bootstrapping technique that induces a semantic tagger from unannotated texts. We use their system in our ensemble. There has also been work on extracting semantic class members from the Web (e.g., (Pas¸ca, 2004; Etzioni et al., 2005; Kozareva et al., 2008; Carlson et al., 2009)). This line of research is fundamentally different from ours because these techniques benefit from the vast repository of information available on the Web and are therefore designed to harvest a wide swath of general-purpose semantic information. Our research is aimed at acquiring domain-specific semantic dictionaries using a collection of documents representing a specialized domain. 3 Ensemble-based Semantic Lexicon Induction 3.1 Motivation Our research combines three fundamentally different techniques into an ensemble-based bootstrapping framework for semantic lexicon induction: pattern-bas</context>
</contexts>
<marker>Carlson, Betteridge, Jr, Mitchell, 2009</marker>
<rawString>Andrew Carlson, Justin Betteridge, Estevam R. Hruschka Jr., and Tom M. Mitchell. 2009. Coupling semi-supervised learning of categories and relations. In HLT-NAACL 2009 Workshop on Semi-Supervised Learning for NLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>Y Singer</author>
</authors>
<title>Unsupervised Models for Named Entity Classification.</title>
<date>1999</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-99).</booktitle>
<contexts>
<context position="2702" citStr="Collins and Singer, 1999" startWordPosition="391" endWordPosition="394">cIntosh and Curran, 2009; McIntosh, 2010), but accuracy is still far from perfect. Our research explores the use of ensemble methods to improve the accuracy of semantic lexicon induction. Our observation is that semantic class associations can be learned using several fundamentally different types of corpus analysis. Bootstrapping methods for semantic lexicon induction (e.g., (Riloff and Jones, 1999; Thelen and Riloff, 2002; McIntosh and Curran, 2009)) collect corpus-wide statistics for individual words based on shared contextual patterns. In contrast, classifiers for semantic tagging (e.g., (Collins and Singer, 1999; Niu et al., 2003; Huang and Riloff, 2010)) label word instances and focus on the local context surrounding each instance. The difference between these approaches is that semantic taggers make decisions based on a single context and can assign different labels to different instances, whereas lexicon induction algorithms compile corpus statistics from multiple instances of a word and typically assign each word to a single semantic category.1 We also hypothesize that coreference resolution can be exploited to infer semantic 1This approach would be untenable for broad-coverage semantic knowledge</context>
<context position="6272" citStr="Collins and Singer, 1999" startWordPosition="926" endWordPosition="929">d co-training (Blum and Mitchell, 1998) to exploit three simple classifiers that each recognized a different type of syntactic structure. The research most closely related to ours is an ensemble-based method for automatic thesaurus construction (Curran, 2002). However, that goal was to acquire finegrained semantic information that is more akin to synonymy (e.g., words similar to “house”), whereas we associate words with high-level semantic classes (e.g., a “house” is a transient structure). Semantic class tagging is closely related to named entity recognition (NER) (e.g., (Bikel et al., 1997; Collins and Singer, 1999; Cucerzan and Yarowsky, 1999; Fleischman and Hovy, 2002)). Some bootstrapping methods have been used for NER (e.g., (Collins and Singer, 1999; Niu et al., 2003) to learn from unannotated texts. However, most NER systems will not label nominal noun phrases (e.g., they will not identify “the dentist” as a person) or recognize semantic classes that are not associated with proper named entities (e.g., symptoms).2 ACE mention detection systems (e.g., (ACE, 2007; ACE, 2008)) can label noun phrases that are associated with 5-7 semantic classes and are typically trained with supervised learning. Rece</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>M. Collins and Y. Singer. 1999. Unsupervised Models for Named Entity Classification. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-99).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cucerzan</author>
<author>D Yarowsky</author>
</authors>
<title>Language Independent Named Entity Recognition Combining Morphologi cal and Contextual Evidence.</title>
<date>1999</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-99).</booktitle>
<contexts>
<context position="6301" citStr="Cucerzan and Yarowsky, 1999" startWordPosition="930" endWordPosition="933">tchell, 1998) to exploit three simple classifiers that each recognized a different type of syntactic structure. The research most closely related to ours is an ensemble-based method for automatic thesaurus construction (Curran, 2002). However, that goal was to acquire finegrained semantic information that is more akin to synonymy (e.g., words similar to “house”), whereas we associate words with high-level semantic classes (e.g., a “house” is a transient structure). Semantic class tagging is closely related to named entity recognition (NER) (e.g., (Bikel et al., 1997; Collins and Singer, 1999; Cucerzan and Yarowsky, 1999; Fleischman and Hovy, 2002)). Some bootstrapping methods have been used for NER (e.g., (Collins and Singer, 1999; Niu et al., 2003) to learn from unannotated texts. However, most NER systems will not label nominal noun phrases (e.g., they will not identify “the dentist” as a person) or recognize semantic classes that are not associated with proper named entities (e.g., symptoms).2 ACE mention detection systems (e.g., (ACE, 2007; ACE, 2008)) can label noun phrases that are associated with 5-7 semantic classes and are typically trained with supervised learning. Recently, (Huang and Riloff, 2010</context>
</contexts>
<marker>Cucerzan, Yarowsky, 1999</marker>
<rawString>S. Cucerzan and D. Yarowsky. 1999. Language Independent Named Entity Recognition Combining Morphologi cal and Contextual Evidence. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-99).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Curran</author>
</authors>
<title>Ensemble Methods for Automatic Thesaurus Extraction.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="5907" citStr="Curran, 2002" startWordPosition="871" endWordPosition="873">Intosh and Curran, 2009). To improve the accuracy of induced lexicons, some research has incorporated negative information from human judgements (Vyas and Pantel, 2009), automatically discovered negative classes (McIntosh, 2010), and distributional similarity metrics to recognize concept drift (McIntosh and Curran, 2009). Phillips and Riloff (2002) used co-training (Blum and Mitchell, 1998) to exploit three simple classifiers that each recognized a different type of syntactic structure. The research most closely related to ours is an ensemble-based method for automatic thesaurus construction (Curran, 2002). However, that goal was to acquire finegrained semantic information that is more akin to synonymy (e.g., words similar to “house”), whereas we associate words with high-level semantic classes (e.g., a “house” is a transient structure). Semantic class tagging is closely related to named entity recognition (NER) (e.g., (Bikel et al., 1997; Collins and Singer, 1999; Cucerzan and Yarowsky, 1999; Fleischman and Hovy, 2002)). Some bootstrapping methods have been used for NER (e.g., (Collins and Singer, 1999; Niu et al., 2003) to learn from unannotated texts. However, most NER systems will not label</context>
</contexts>
<marker>Curran, 2002</marker>
<rawString>J. Curran. 2002. Ensemble Methods for Automatic Thesaurus Extraction. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Etzioni</author>
<author>M Cafarella</author>
<author>D Downey</author>
<author>A Popescu</author>
<author>T Shaked</author>
<author>S Soderland</author>
<author>D Weld</author>
<author>A Yates</author>
</authors>
<title>Unsupervised named-entity extraction from the web: an experimental study.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<volume>165</volume>
<issue>1</issue>
<contexts>
<context position="7148" citStr="Etzioni et al., 2005" startWordPosition="1066" endWordPosition="1070"> (e.g., they will not identify “the dentist” as a person) or recognize semantic classes that are not associated with proper named entities (e.g., symptoms).2 ACE mention detection systems (e.g., (ACE, 2007; ACE, 2008)) can label noun phrases that are associated with 5-7 semantic classes and are typically trained with supervised learning. Recently, (Huang and Riloff, 2010) developed a bootstrapping technique that induces a semantic tagger from unannotated texts. We use their system in our ensemble. There has also been work on extracting semantic class members from the Web (e.g., (Pas¸ca, 2004; Etzioni et al., 2005; Kozareva et al., 2008; Carlson et al., 2009)). This line of research is fundamentally different from ours because these techniques benefit from the vast repository of information available on the Web and are therefore designed to harvest a wide swath of general-purpose semantic information. Our research is aimed at acquiring domain-specific semantic dictionaries using a collection of documents representing a specialized domain. 3 Ensemble-based Semantic Lexicon Induction 3.1 Motivation Our research combines three fundamentally different techniques into an ensemble-based bootstrapping framewo</context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderland, Weld, Yates, 2005</marker>
<rawString>O. Etzioni, M. Cafarella, D. Downey, A. Popescu, T. Shaked, S. Soderland, D. Weld, and A. Yates. 2005. Unsupervised named-entity extraction from the web: an experimental study. Artificial Intelligence, 165(1):91–134, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M B Fleischman</author>
<author>E H Hovy</author>
</authors>
<title>Fine grained classification of named entities.</title>
<date>2002</date>
<booktitle>In Proceedings of the COLING conference,</booktitle>
<contexts>
<context position="6329" citStr="Fleischman and Hovy, 2002" startWordPosition="934" endWordPosition="937">e simple classifiers that each recognized a different type of syntactic structure. The research most closely related to ours is an ensemble-based method for automatic thesaurus construction (Curran, 2002). However, that goal was to acquire finegrained semantic information that is more akin to synonymy (e.g., words similar to “house”), whereas we associate words with high-level semantic classes (e.g., a “house” is a transient structure). Semantic class tagging is closely related to named entity recognition (NER) (e.g., (Bikel et al., 1997; Collins and Singer, 1999; Cucerzan and Yarowsky, 1999; Fleischman and Hovy, 2002)). Some bootstrapping methods have been used for NER (e.g., (Collins and Singer, 1999; Niu et al., 2003) to learn from unannotated texts. However, most NER systems will not label nominal noun phrases (e.g., they will not identify “the dentist” as a person) or recognize semantic classes that are not associated with proper named entities (e.g., symptoms).2 ACE mention detection systems (e.g., (ACE, 2007; ACE, 2008)) can label noun phrases that are associated with 5-7 semantic classes and are typically trained with supervised learning. Recently, (Huang and Riloff, 2010) developed a bootstrapping </context>
</contexts>
<marker>Fleischman, Hovy, 2002</marker>
<rawString>M.B. Fleischman and E.H. Hovy. 2002. Fine grained classification of named entities. In Proceedings of the COLING conference, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hirschman</author>
</authors>
<date>1997</date>
<tech>MUC-7</tech>
<institution>Coreference Task Definition.</institution>
<contexts>
<context position="19279" citStr="Hirschman, 1997" startWordPosition="2870" endWordPosition="2871">em because many food items are plants. egories. (e.g., output, information, media, point). 4.2 Data Set We ran our experiments on ProMED-mail12 articles. ProMED-mail is an internet based reporting system for infectious disease outbreaks, which can involve people, animals, and plants grown for food. Our ProMED corpus contains 5004 documents. We used 4959 documents as (unannotated) training data for bootstrapping. For the remaining 45 documents, we used 22 documents to train the coreference resolver (Reconcile) and 23 documents as our test set. The coreference training set contains MUC-7 style (Hirschman, 1997) coreference annotations13. Once trained, Reconcile was applied to the 4959 unannotated documents to produce coreference chains. 4.3 Gold Standard Semantic Class Annotations To obtain gold standard annotations for the test set, two annotators assigned one of the 9 semantic class labels, or Miscellaneous, to each head noun based on its surrounding context. A noun with multiple senses could get assigned different semantic class labels in different contexts. The annotators first annotated 13 of the 23 documents, and discussed the cases where they disagreed. Then they independelty annotated 12http</context>
</contexts>
<marker>Hirschman, 1997</marker>
<rawString>L. Hirschman. 1997. MUC-7 Coreference Task Definition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruihong Huang</author>
<author>Ellen Riloff</author>
</authors>
<title>Inducing domain-specific semantic class taggers from (almost) nothing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2745" citStr="Huang and Riloff, 2010" startWordPosition="399" endWordPosition="402">ut accuracy is still far from perfect. Our research explores the use of ensemble methods to improve the accuracy of semantic lexicon induction. Our observation is that semantic class associations can be learned using several fundamentally different types of corpus analysis. Bootstrapping methods for semantic lexicon induction (e.g., (Riloff and Jones, 1999; Thelen and Riloff, 2002; McIntosh and Curran, 2009)) collect corpus-wide statistics for individual words based on shared contextual patterns. In contrast, classifiers for semantic tagging (e.g., (Collins and Singer, 1999; Niu et al., 2003; Huang and Riloff, 2010)) label word instances and focus on the local context surrounding each instance. The difference between these approaches is that semantic taggers make decisions based on a single context and can assign different labels to different instances, whereas lexicon induction algorithms compile corpus statistics from multiple instances of a word and typically assign each word to a single semantic category.1 We also hypothesize that coreference resolution can be exploited to infer semantic 1This approach would be untenable for broad-coverage semantic knowledge acquisition, but within a specialized doma</context>
<context position="6902" citStr="Huang and Riloff, 2010" startWordPosition="1025" endWordPosition="1028">zan and Yarowsky, 1999; Fleischman and Hovy, 2002)). Some bootstrapping methods have been used for NER (e.g., (Collins and Singer, 1999; Niu et al., 2003) to learn from unannotated texts. However, most NER systems will not label nominal noun phrases (e.g., they will not identify “the dentist” as a person) or recognize semantic classes that are not associated with proper named entities (e.g., symptoms).2 ACE mention detection systems (e.g., (ACE, 2007; ACE, 2008)) can label noun phrases that are associated with 5-7 semantic classes and are typically trained with supervised learning. Recently, (Huang and Riloff, 2010) developed a bootstrapping technique that induces a semantic tagger from unannotated texts. We use their system in our ensemble. There has also been work on extracting semantic class members from the Web (e.g., (Pas¸ca, 2004; Etzioni et al., 2005; Kozareva et al., 2008; Carlson et al., 2009)). This line of research is fundamentally different from ours because these techniques benefit from the vast repository of information available on the Web and are therefore designed to harvest a wide swath of general-purpose semantic information. Our research is aimed at acquiring domain-specific semantic </context>
<context position="10441" citStr="Huang and Riloff, 2010" startWordPosition="1553" endWordPosition="1556">tterns are automatically generated using AutoSlog-TS (Riloff, 1996). Basilisk begins with a small set of seed words for each semantic class and a collection of unannotated documents for the domain. In an iterative bootstrapping process, Basilisk identifies candidate nouns, ranks them based on its scoring criteria, selects the 5 most confident words for inclusion in the lexicon, and this process repeats using the new words as additional seeds in subsequent iterations. 3.2.2 Lexicon Induction with a Contextual Semantic Tagger The second component in our ensemble is a contextual semantic tagger (Huang and Riloff, 2010). Like Basilisk, the semantic tagger also begins with seed nouns, trains itself on a large collection of unannotated documents using bootstrapping, and iteratively labels new instances. This tagger labels noun instances and does not produce a dictionary. To adapt it for our purposes, we ran the bootstrapping process over the training texts to induce a semantic classifier. We then applied the classifier to the same set of training documents and compiled a lexicon by collecting the set of nouns that were assigned to each semantic class. We ignored words that were assigned different labels in dif</context>
</contexts>
<marker>Huang, Riloff, 2010</marker>
<rawString>Ruihong Huang and Ellen Riloff. 2010. Inducing domain-specific semantic class taggers from (almost) nothing. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Kozareva</author>
<author>E Riloff</author>
<author>E Hovy</author>
</authors>
<title>Semantic Class Learning from the Web with Hyponym Pattern Linkage Graphs.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-08).</booktitle>
<contexts>
<context position="7171" citStr="Kozareva et al., 2008" startWordPosition="1071" endWordPosition="1074">identify “the dentist” as a person) or recognize semantic classes that are not associated with proper named entities (e.g., symptoms).2 ACE mention detection systems (e.g., (ACE, 2007; ACE, 2008)) can label noun phrases that are associated with 5-7 semantic classes and are typically trained with supervised learning. Recently, (Huang and Riloff, 2010) developed a bootstrapping technique that induces a semantic tagger from unannotated texts. We use their system in our ensemble. There has also been work on extracting semantic class members from the Web (e.g., (Pas¸ca, 2004; Etzioni et al., 2005; Kozareva et al., 2008; Carlson et al., 2009)). This line of research is fundamentally different from ours because these techniques benefit from the vast repository of information available on the Web and are therefore designed to harvest a wide swath of general-purpose semantic information. Our research is aimed at acquiring domain-specific semantic dictionaries using a collection of documents representing a specialized domain. 3 Ensemble-based Semantic Lexicon Induction 3.1 Motivation Our research combines three fundamentally different techniques into an ensemble-based bootstrapping framework for semantic lexicon</context>
</contexts>
<marker>Kozareva, Riloff, Hovy, 2008</marker>
<rawString>Z. Kozareva, E. Riloff, and E. Hovy. 2008. Semantic Class Learning from the Web with Hyponym Pattern Linkage Graphs. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-08).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T McIntosh</author>
<author>J Curran</author>
</authors>
<title>Weighted mutual exclusion bootstrapping for domain independent lexicon and template acquisition.</title>
<date>2008</date>
<booktitle>In Proceedings of the Australasian Language Technology Association Workshop.</booktitle>
<contexts>
<context position="5290" citStr="McIntosh and Curran, 2008" startWordPosition="781" endWordPosition="784">ry lookup on a collection of disease outbreak articles. Our results show that the induced dictionaries yield better performance than an instance-based semantic tagger, achieving higher accuracy with comparable levels of recall. 2 Related Work Several techniques have been developed for semantic class induction (also called set expansion) using bootstrapping methods that consider co-occurrence statistics based on nouns (Riloff and Shepherd, 1997), syntactic structures (Roark and Charniak, 1998; Phillips and Riloff, 2002), and contextual patterns (Riloff and Jones, 1999; Thelen and Riloff, 2002; McIntosh and Curran, 2008; McIntosh and Curran, 2009). To improve the accuracy of induced lexicons, some research has incorporated negative information from human judgements (Vyas and Pantel, 2009), automatically discovered negative classes (McIntosh, 2010), and distributional similarity metrics to recognize concept drift (McIntosh and Curran, 2009). Phillips and Riloff (2002) used co-training (Blum and Mitchell, 1998) to exploit three simple classifiers that each recognized a different type of syntactic structure. The research most closely related to ours is an ensemble-based method for automatic thesaurus constructi</context>
</contexts>
<marker>McIntosh, Curran, 2008</marker>
<rawString>T. McIntosh and J. Curran. 2008. Weighted mutual exclusion bootstrapping for domain independent lexicon and template acquisition. In Proceedings of the Australasian Language Technology Association Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T McIntosh</author>
<author>J Curran</author>
</authors>
<title>Reducing Semantic Drift with Bagging and Distributional Similarity.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2102" citStr="McIntosh and Curran, 2009" startWordPosition="301" endWordPosition="304">ch as medicine, chemistry, or microelectronics. Furthermore, in virtually every domain, texts contain lexical variations that are often missing from dictionaries, such as acronyms, abbreviations, spelling variants, informal shorthand terms (e.g., “abx” for “antibiotics”), and composite terms (e.g., “maydecember” or “virus/worm”). To address this problem, techniques have been developed to automate the construction of semantic lexicons from text corpora using bootstrapping methods (Riloff and Shepherd, 1997; Roark and Charniak, 1998; Phillips and Riloff, 2002; Thelen and Riloff, 2002; Ng, 2007; McIntosh and Curran, 2009; McIntosh, 2010), but accuracy is still far from perfect. Our research explores the use of ensemble methods to improve the accuracy of semantic lexicon induction. Our observation is that semantic class associations can be learned using several fundamentally different types of corpus analysis. Bootstrapping methods for semantic lexicon induction (e.g., (Riloff and Jones, 1999; Thelen and Riloff, 2002; McIntosh and Curran, 2009)) collect corpus-wide statistics for individual words based on shared contextual patterns. In contrast, classifiers for semantic tagging (e.g., (Collins and Singer, 1999</context>
<context position="5318" citStr="McIntosh and Curran, 2009" startWordPosition="785" endWordPosition="788">f disease outbreak articles. Our results show that the induced dictionaries yield better performance than an instance-based semantic tagger, achieving higher accuracy with comparable levels of recall. 2 Related Work Several techniques have been developed for semantic class induction (also called set expansion) using bootstrapping methods that consider co-occurrence statistics based on nouns (Riloff and Shepherd, 1997), syntactic structures (Roark and Charniak, 1998; Phillips and Riloff, 2002), and contextual patterns (Riloff and Jones, 1999; Thelen and Riloff, 2002; McIntosh and Curran, 2008; McIntosh and Curran, 2009). To improve the accuracy of induced lexicons, some research has incorporated negative information from human judgements (Vyas and Pantel, 2009), automatically discovered negative classes (McIntosh, 2010), and distributional similarity metrics to recognize concept drift (McIntosh and Curran, 2009). Phillips and Riloff (2002) used co-training (Blum and Mitchell, 1998) to exploit three simple classifiers that each recognized a different type of syntactic structure. The research most closely related to ours is an ensemble-based method for automatic thesaurus construction (Curran, 2002). However, </context>
</contexts>
<marker>McIntosh, Curran, 2009</marker>
<rawString>T. McIntosh and J. Curran. 2009. Reducing Semantic Drift with Bagging and Distributional Similarity. In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T McIntosh</author>
</authors>
<title>Unsupervised Discovery of Negative Categories in Lexicon Bootstrapping.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="2119" citStr="McIntosh, 2010" startWordPosition="305" endWordPosition="306">or microelectronics. Furthermore, in virtually every domain, texts contain lexical variations that are often missing from dictionaries, such as acronyms, abbreviations, spelling variants, informal shorthand terms (e.g., “abx” for “antibiotics”), and composite terms (e.g., “maydecember” or “virus/worm”). To address this problem, techniques have been developed to automate the construction of semantic lexicons from text corpora using bootstrapping methods (Riloff and Shepherd, 1997; Roark and Charniak, 1998; Phillips and Riloff, 2002; Thelen and Riloff, 2002; Ng, 2007; McIntosh and Curran, 2009; McIntosh, 2010), but accuracy is still far from perfect. Our research explores the use of ensemble methods to improve the accuracy of semantic lexicon induction. Our observation is that semantic class associations can be learned using several fundamentally different types of corpus analysis. Bootstrapping methods for semantic lexicon induction (e.g., (Riloff and Jones, 1999; Thelen and Riloff, 2002; McIntosh and Curran, 2009)) collect corpus-wide statistics for individual words based on shared contextual patterns. In contrast, classifiers for semantic tagging (e.g., (Collins and Singer, 1999; Niu et al., 200</context>
<context position="5522" citStr="McIntosh, 2010" startWordPosition="816" endWordPosition="817"> Several techniques have been developed for semantic class induction (also called set expansion) using bootstrapping methods that consider co-occurrence statistics based on nouns (Riloff and Shepherd, 1997), syntactic structures (Roark and Charniak, 1998; Phillips and Riloff, 2002), and contextual patterns (Riloff and Jones, 1999; Thelen and Riloff, 2002; McIntosh and Curran, 2008; McIntosh and Curran, 2009). To improve the accuracy of induced lexicons, some research has incorporated negative information from human judgements (Vyas and Pantel, 2009), automatically discovered negative classes (McIntosh, 2010), and distributional similarity metrics to recognize concept drift (McIntosh and Curran, 2009). Phillips and Riloff (2002) used co-training (Blum and Mitchell, 1998) to exploit three simple classifiers that each recognized a different type of syntactic structure. The research most closely related to ours is an ensemble-based method for automatic thesaurus construction (Curran, 2002). However, that goal was to acquire finegrained semantic information that is more akin to synonymy (e.g., words similar to “house”), whereas we associate words with high-level semantic classes (e.g., a “house” is a </context>
</contexts>
<marker>McIntosh, 2010</marker>
<rawString>T. McIntosh. 2010. Unsupervised Discovery of Negative Categories in Lexicon Bootstrapping. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
</authors>
<title>Wordnet: An On-line Lexical Database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="1389" citStr="Miller, 1990" startWordPosition="203" endWordPosition="204">re added to the lexicon, and the process repeats. Our results show that the ensemble outperforms individual methods in terms of both lexicon quality and instance-based semantic tagging. 1 Introduction One of the most fundamental aspects of meaning is the association between words and semantic categories, which allows us to understand that a “cow” is an animal and a “house” is a structure. We will use the term semantic lexicon to refer to a dictionary that associates words with semantic classes. Semantic dictionaries are useful for many NLP tasks, as evidenced by the widespread use of WordNet (Miller, 1990). However, off-the-shelf resources are not always sufficient for specialized domains, such as medicine, chemistry, or microelectronics. Furthermore, in virtually every domain, texts contain lexical variations that are often missing from dictionaries, such as acronyms, abbreviations, spelling variants, informal shorthand terms (e.g., “abx” for “antibiotics”), and composite terms (e.g., “maydecember” or “virus/worm”). To address this problem, techniques have been developed to automate the construction of semantic lexicons from text corpora using bootstrapping methods (Riloff and Shepherd, 1997; </context>
<context position="20635" citStr="Miller, 1990" startWordPosition="3077" endWordPosition="3078"> measured their agreement on 10 documents and achieved MUC scores of Precision = .82, Recall = .86, F-measure = .84. 203 the remaining 10 documents and measured interannotator agreement with Cohen’s Kappa (n) (Carletta, 1996). The n score for these 10 documents was 0.91, indicating a high level of agreement. The annotators then adjudicated their disagreements on all 23 documents to create the gold standard. 4.4 Dictionary Evaluation To assess the quality of the lexicons, we estimated their accuracy by compiling external word lists from freely available sources such as Wikipedia14 and WordNet (Miller, 1990). Table 1 shows the sources that we used, where the bracketed items refer to WordNet hypernym categories. We searched each WordNet hypernym tree (also, instancerelationship) for all senses of the word. Additionally, we collected the manually labeled words in our test set and included them in our gold standard lists. Since the induced lexicons contain individual nouns, we extracted only the head nouns of multiword phrases in the external resources. This can produce incorrect entries for non-compositional phrases, but we found this issue to be relatively rare and we manually removed obviously wr</context>
</contexts>
<marker>Miller, 1990</marker>
<rawString>G. Miller. 1990. Wordnet: An On-line Lexical Database. International Journal of Lexicography, 3(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
</authors>
<title>Semantic Class Induction and Coreference Resolution.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2075" citStr="Ng, 2007" startWordPosition="299" endWordPosition="300">omains, such as medicine, chemistry, or microelectronics. Furthermore, in virtually every domain, texts contain lexical variations that are often missing from dictionaries, such as acronyms, abbreviations, spelling variants, informal shorthand terms (e.g., “abx” for “antibiotics”), and composite terms (e.g., “maydecember” or “virus/worm”). To address this problem, techniques have been developed to automate the construction of semantic lexicons from text corpora using bootstrapping methods (Riloff and Shepherd, 1997; Roark and Charniak, 1998; Phillips and Riloff, 2002; Thelen and Riloff, 2002; Ng, 2007; McIntosh and Curran, 2009; McIntosh, 2010), but accuracy is still far from perfect. Our research explores the use of ensemble methods to improve the accuracy of semantic lexicon induction. Our observation is that semantic class associations can be learned using several fundamentally different types of corpus analysis. Bootstrapping methods for semantic lexicon induction (e.g., (Riloff and Jones, 1999; Thelen and Riloff, 2002; McIntosh and Curran, 2009)) collect corpus-wide statistics for individual words based on shared contextual patterns. In contrast, classifiers for semantic tagging (e.g.</context>
</contexts>
<marker>Ng, 2007</marker>
<rawString>V. Ng. 2007. Semantic Class Induction and Coreference Resolution. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cheng Niu</author>
<author>Wei Li</author>
<author>Jihong Ding</author>
<author>Rohini K Srihari</author>
</authors>
<title>A bootstrapping approach to named entity classification using successive learners.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics (ACL-03),</booktitle>
<pages>335--342</pages>
<contexts>
<context position="2720" citStr="Niu et al., 2003" startWordPosition="395" endWordPosition="398">McIntosh, 2010), but accuracy is still far from perfect. Our research explores the use of ensemble methods to improve the accuracy of semantic lexicon induction. Our observation is that semantic class associations can be learned using several fundamentally different types of corpus analysis. Bootstrapping methods for semantic lexicon induction (e.g., (Riloff and Jones, 1999; Thelen and Riloff, 2002; McIntosh and Curran, 2009)) collect corpus-wide statistics for individual words based on shared contextual patterns. In contrast, classifiers for semantic tagging (e.g., (Collins and Singer, 1999; Niu et al., 2003; Huang and Riloff, 2010)) label word instances and focus on the local context surrounding each instance. The difference between these approaches is that semantic taggers make decisions based on a single context and can assign different labels to different instances, whereas lexicon induction algorithms compile corpus statistics from multiple instances of a word and typically assign each word to a single semantic category.1 We also hypothesize that coreference resolution can be exploited to infer semantic 1This approach would be untenable for broad-coverage semantic knowledge acquisition, but </context>
<context position="6433" citStr="Niu et al., 2003" startWordPosition="952" endWordPosition="955">ted to ours is an ensemble-based method for automatic thesaurus construction (Curran, 2002). However, that goal was to acquire finegrained semantic information that is more akin to synonymy (e.g., words similar to “house”), whereas we associate words with high-level semantic classes (e.g., a “house” is a transient structure). Semantic class tagging is closely related to named entity recognition (NER) (e.g., (Bikel et al., 1997; Collins and Singer, 1999; Cucerzan and Yarowsky, 1999; Fleischman and Hovy, 2002)). Some bootstrapping methods have been used for NER (e.g., (Collins and Singer, 1999; Niu et al., 2003) to learn from unannotated texts. However, most NER systems will not label nominal noun phrases (e.g., they will not identify “the dentist” as a person) or recognize semantic classes that are not associated with proper named entities (e.g., symptoms).2 ACE mention detection systems (e.g., (ACE, 2007; ACE, 2008)) can label noun phrases that are associated with 5-7 semantic classes and are typically trained with supervised learning. Recently, (Huang and Riloff, 2010) developed a bootstrapping technique that induces a semantic tagger from unannotated texts. We use their system in our ensemble. Th</context>
</contexts>
<marker>Niu, Li, Ding, Srihari, 2003</marker>
<rawString>Cheng Niu, Wei Li, Jihong Ding, and Rohini K. Srihari. 2003. A bootstrapping approach to named entity classification using successive learners. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics (ACL-03), pages 335–342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pas¸ca</author>
</authors>
<title>Acquisition of categorized named entities for web search.</title>
<date>2004</date>
<booktitle>In Proc. of the Thirteenth ACM International Conference on Information and Knowledge Management,</booktitle>
<pages>137--145</pages>
<marker>Pas¸ca, 2004</marker>
<rawString>M. Pas¸ca. 2004. Acquisition of categorized named entities for web search. In Proc. of the Thirteenth ACM International Conference on Information and Knowledge Management, pages 137–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Phillips</author>
<author>E Riloff</author>
</authors>
<title>Exploiting Strong Syntactic Heuristics and Co-Training to Learn Semantic Lexicons.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>125--132</pages>
<contexts>
<context position="2040" citStr="Phillips and Riloff, 2002" startWordPosition="291" endWordPosition="294">esources are not always sufficient for specialized domains, such as medicine, chemistry, or microelectronics. Furthermore, in virtually every domain, texts contain lexical variations that are often missing from dictionaries, such as acronyms, abbreviations, spelling variants, informal shorthand terms (e.g., “abx” for “antibiotics”), and composite terms (e.g., “maydecember” or “virus/worm”). To address this problem, techniques have been developed to automate the construction of semantic lexicons from text corpora using bootstrapping methods (Riloff and Shepherd, 1997; Roark and Charniak, 1998; Phillips and Riloff, 2002; Thelen and Riloff, 2002; Ng, 2007; McIntosh and Curran, 2009; McIntosh, 2010), but accuracy is still far from perfect. Our research explores the use of ensemble methods to improve the accuracy of semantic lexicon induction. Our observation is that semantic class associations can be learned using several fundamentally different types of corpus analysis. Bootstrapping methods for semantic lexicon induction (e.g., (Riloff and Jones, 1999; Thelen and Riloff, 2002; McIntosh and Curran, 2009)) collect corpus-wide statistics for individual words based on shared contextual patterns. In contrast, cla</context>
<context position="5189" citStr="Phillips and Riloff, 2002" startWordPosition="765" endWordPosition="768">e, effective. We analyze the tradeoffs between using an instance-based semantic tagger versus dictionary lookup on a collection of disease outbreak articles. Our results show that the induced dictionaries yield better performance than an instance-based semantic tagger, achieving higher accuracy with comparable levels of recall. 2 Related Work Several techniques have been developed for semantic class induction (also called set expansion) using bootstrapping methods that consider co-occurrence statistics based on nouns (Riloff and Shepherd, 1997), syntactic structures (Roark and Charniak, 1998; Phillips and Riloff, 2002), and contextual patterns (Riloff and Jones, 1999; Thelen and Riloff, 2002; McIntosh and Curran, 2008; McIntosh and Curran, 2009). To improve the accuracy of induced lexicons, some research has incorporated negative information from human judgements (Vyas and Pantel, 2009), automatically discovered negative classes (McIntosh, 2010), and distributional similarity metrics to recognize concept drift (McIntosh and Curran, 2009). Phillips and Riloff (2002) used co-training (Blum and Mitchell, 1998) to exploit three simple classifiers that each recognized a different type of syntactic structure. The</context>
</contexts>
<marker>Phillips, Riloff, 2002</marker>
<rawString>W. Phillips and E. Riloff. 2002. Exploiting Strong Syntactic Heuristics and Co-Training to Learn Semantic Lexicons. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, pages 125–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
<author>P Hanks</author>
<author>R Sauri</author>
<author>A See</author>
<author>R Gaizauskas</author>
<author>A Setzer</author>
<author>D Radev</author>
<author>B Sundheim</author>
<author>D Day</author>
<author>L Ferro</author>
<author>M Lazo</author>
</authors>
<title>The TIMEBANK Corpus.</title>
<date>2003</date>
<booktitle>In Proceedings of Corpus Linguistics</booktitle>
<pages>647--656</pages>
<contexts>
<context position="17095" citStr="Pustejovsky et al., 2003" startWordPosition="2546" endWordPosition="2549">]; Wiki:US-states; Other:cities5, countriess Human WordNet: [person], [people], [personnel]; Wikipedia: people names, office holder titles, nationalities, occupations, medical personnels &amp; acronyms, players; Other: common people names &amp; surnames Org WordNet: [organization], [assembly]; Wikipedia: acronyms in healthcare, medical organization acronyms, news agencies, pharmaceutical companies; Other: companies8, US-universities9, organizations10 Plant &amp; Food WordNet: [food], [plant, flora], [plant part] Temp. Ref. WordNet: [time], [time interval], [time unit],[time period] TimeBank: TimeBank1.2 (Pustejovsky et al., 2003) TIMEX3 expressions Trans. Struct. WordNet: [structure, construction], [road, route], [facility, installation], [work place] Table 1: External Word List Sources Body Part: A part of a human or animal body, including organs, bodily fluids, and microscopic parts. (e.g., hand, heart, blood, DNA) Diseases and Symptoms (DisSym): Diseases and symptoms. We also include fungi and disease carriers because, in this domain, they almost always refer to the disease that they carry. (e.g. FMD, Anthrax, fever, virus) Fixed Location (Fixed Loc.): Named locations, including countries, cities, states, etc. We a</context>
</contexts>
<marker>Pustejovsky, Hanks, Sauri, See, Gaizauskas, Setzer, Radev, Sundheim, Day, Ferro, Lazo, 2003</marker>
<rawString>J. Pustejovsky, P. Hanks, R. Sauri, A. See, R. Gaizauskas, A. Setzer, D. Radev, B. Sundheim, D. Day, L. Ferro, and M. Lazo. 2003. The TIMEBANK Corpus. In Proceedings of Corpus Linguistics 2003, pages 647– 656.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>R Jones</author>
</authors>
<title>Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping.</title>
<date>1999</date>
<booktitle>In Proceedings of the Sixteenth National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="2480" citStr="Riloff and Jones, 1999" startWordPosition="360" endWordPosition="363">eloped to automate the construction of semantic lexicons from text corpora using bootstrapping methods (Riloff and Shepherd, 1997; Roark and Charniak, 1998; Phillips and Riloff, 2002; Thelen and Riloff, 2002; Ng, 2007; McIntosh and Curran, 2009; McIntosh, 2010), but accuracy is still far from perfect. Our research explores the use of ensemble methods to improve the accuracy of semantic lexicon induction. Our observation is that semantic class associations can be learned using several fundamentally different types of corpus analysis. Bootstrapping methods for semantic lexicon induction (e.g., (Riloff and Jones, 1999; Thelen and Riloff, 2002; McIntosh and Curran, 2009)) collect corpus-wide statistics for individual words based on shared contextual patterns. In contrast, classifiers for semantic tagging (e.g., (Collins and Singer, 1999; Niu et al., 2003; Huang and Riloff, 2010)) label word instances and focus on the local context surrounding each instance. The difference between these approaches is that semantic taggers make decisions based on a single context and can assign different labels to different instances, whereas lexicon induction algorithms compile corpus statistics from multiple instances of a </context>
<context position="5238" citStr="Riloff and Jones, 1999" startWordPosition="773" endWordPosition="776">an instance-based semantic tagger versus dictionary lookup on a collection of disease outbreak articles. Our results show that the induced dictionaries yield better performance than an instance-based semantic tagger, achieving higher accuracy with comparable levels of recall. 2 Related Work Several techniques have been developed for semantic class induction (also called set expansion) using bootstrapping methods that consider co-occurrence statistics based on nouns (Riloff and Shepherd, 1997), syntactic structures (Roark and Charniak, 1998; Phillips and Riloff, 2002), and contextual patterns (Riloff and Jones, 1999; Thelen and Riloff, 2002; McIntosh and Curran, 2008; McIntosh and Curran, 2009). To improve the accuracy of induced lexicons, some research has incorporated negative information from human judgements (Vyas and Pantel, 2009), automatically discovered negative classes (McIntosh, 2010), and distributional similarity metrics to recognize concept drift (McIntosh and Curran, 2009). Phillips and Riloff (2002) used co-training (Blum and Mitchell, 1998) to exploit three simple classifiers that each recognized a different type of syntactic structure. The research most closely related to ours is an ense</context>
</contexts>
<marker>Riloff, Jones, 1999</marker>
<rawString>E. Riloff and R. Jones. 1999. Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping. In Proceedings of the Sixteenth National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>J Shepherd</author>
</authors>
<title>A Corpus-Based Approach for Building Semantic Lexicons.</title>
<date>1997</date>
<booktitle>In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>117--124</pages>
<contexts>
<context position="1987" citStr="Riloff and Shepherd, 1997" startWordPosition="282" endWordPosition="286">e of WordNet (Miller, 1990). However, off-the-shelf resources are not always sufficient for specialized domains, such as medicine, chemistry, or microelectronics. Furthermore, in virtually every domain, texts contain lexical variations that are often missing from dictionaries, such as acronyms, abbreviations, spelling variants, informal shorthand terms (e.g., “abx” for “antibiotics”), and composite terms (e.g., “maydecember” or “virus/worm”). To address this problem, techniques have been developed to automate the construction of semantic lexicons from text corpora using bootstrapping methods (Riloff and Shepherd, 1997; Roark and Charniak, 1998; Phillips and Riloff, 2002; Thelen and Riloff, 2002; Ng, 2007; McIntosh and Curran, 2009; McIntosh, 2010), but accuracy is still far from perfect. Our research explores the use of ensemble methods to improve the accuracy of semantic lexicon induction. Our observation is that semantic class associations can be learned using several fundamentally different types of corpus analysis. Bootstrapping methods for semantic lexicon induction (e.g., (Riloff and Jones, 1999; Thelen and Riloff, 2002; McIntosh and Curran, 2009)) collect corpus-wide statistics for individual words </context>
<context position="5113" citStr="Riloff and Shepherd, 1997" startWordPosition="755" endWordPosition="758">at using domain-specific dictionaries for tagging may be equally, if not more, effective. We analyze the tradeoffs between using an instance-based semantic tagger versus dictionary lookup on a collection of disease outbreak articles. Our results show that the induced dictionaries yield better performance than an instance-based semantic tagger, achieving higher accuracy with comparable levels of recall. 2 Related Work Several techniques have been developed for semantic class induction (also called set expansion) using bootstrapping methods that consider co-occurrence statistics based on nouns (Riloff and Shepherd, 1997), syntactic structures (Roark and Charniak, 1998; Phillips and Riloff, 2002), and contextual patterns (Riloff and Jones, 1999; Thelen and Riloff, 2002; McIntosh and Curran, 2008; McIntosh and Curran, 2009). To improve the accuracy of induced lexicons, some research has incorporated negative information from human judgements (Vyas and Pantel, 2009), automatically discovered negative classes (McIntosh, 2010), and distributional similarity metrics to recognize concept drift (McIntosh and Curran, 2009). Phillips and Riloff (2002) used co-training (Blum and Mitchell, 1998) to exploit three simple c</context>
</contexts>
<marker>Riloff, Shepherd, 1997</marker>
<rawString>E. Riloff and J. Shepherd. 1997. A Corpus-Based Approach for Building Semantic Lexicons. In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, pages 117–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
</authors>
<title>Automatically Generating Extraction Patterns from Untagged Text.</title>
<date>1996</date>
<booktitle>In Proceedings of the Thirteenth National Conference on Artificial Intelligence,</booktitle>
<pages>1044--1049</pages>
<publisher>The AAAI Press/MIT Press.</publisher>
<contexts>
<context position="9885" citStr="Riloff, 1996" startWordPosition="1464" endWordPosition="1465">produces a bootstrapping process with improved precision, both at the critical beginning stages of the bootstrapping process and during subsequent bootstrapping iterations. 3.2 Component Systems in the Ensemble In the following sections, we describe each of the component systems used in our ensemble. 3.2.1 Pattern-based Lexicon Induction The first component of our ensemble is Basilisk (Thelen and Riloff, 2002), which identifies nouns belonging to a semantic class based on collective information over lexico-syntactic pattern contexts. The patterns are automatically generated using AutoSlog-TS (Riloff, 1996). Basilisk begins with a small set of seed words for each semantic class and a collection of unannotated documents for the domain. In an iterative bootstrapping process, Basilisk identifies candidate nouns, ranks them based on its scoring criteria, selects the 5 most confident words for inclusion in the lexicon, and this process repeats using the new words as additional seeds in subsequent iterations. 3.2.2 Lexicon Induction with a Contextual Semantic Tagger The second component in our ensemble is a contextual semantic tagger (Huang and Riloff, 2010). Like Basilisk, the semantic tagger also be</context>
</contexts>
<marker>Riloff, 1996</marker>
<rawString>E. Riloff. 1996. Automatically Generating Extraction Patterns from Untagged Text. In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 1044–1049. The AAAI Press/MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
<author>E Charniak</author>
</authors>
<title>Noun-phrase Cooccurrence Statistics for Semi-automatic Semantic Lexicon Construction.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1110--1116</pages>
<contexts>
<context position="2013" citStr="Roark and Charniak, 1998" startWordPosition="287" endWordPosition="290">. However, off-the-shelf resources are not always sufficient for specialized domains, such as medicine, chemistry, or microelectronics. Furthermore, in virtually every domain, texts contain lexical variations that are often missing from dictionaries, such as acronyms, abbreviations, spelling variants, informal shorthand terms (e.g., “abx” for “antibiotics”), and composite terms (e.g., “maydecember” or “virus/worm”). To address this problem, techniques have been developed to automate the construction of semantic lexicons from text corpora using bootstrapping methods (Riloff and Shepherd, 1997; Roark and Charniak, 1998; Phillips and Riloff, 2002; Thelen and Riloff, 2002; Ng, 2007; McIntosh and Curran, 2009; McIntosh, 2010), but accuracy is still far from perfect. Our research explores the use of ensemble methods to improve the accuracy of semantic lexicon induction. Our observation is that semantic class associations can be learned using several fundamentally different types of corpus analysis. Bootstrapping methods for semantic lexicon induction (e.g., (Riloff and Jones, 1999; Thelen and Riloff, 2002; McIntosh and Curran, 2009)) collect corpus-wide statistics for individual words based on shared contextual</context>
<context position="5161" citStr="Roark and Charniak, 1998" startWordPosition="761" endWordPosition="764">may be equally, if not more, effective. We analyze the tradeoffs between using an instance-based semantic tagger versus dictionary lookup on a collection of disease outbreak articles. Our results show that the induced dictionaries yield better performance than an instance-based semantic tagger, achieving higher accuracy with comparable levels of recall. 2 Related Work Several techniques have been developed for semantic class induction (also called set expansion) using bootstrapping methods that consider co-occurrence statistics based on nouns (Riloff and Shepherd, 1997), syntactic structures (Roark and Charniak, 1998; Phillips and Riloff, 2002), and contextual patterns (Riloff and Jones, 1999; Thelen and Riloff, 2002; McIntosh and Curran, 2008; McIntosh and Curran, 2009). To improve the accuracy of induced lexicons, some research has incorporated negative information from human judgements (Vyas and Pantel, 2009), automatically discovered negative classes (McIntosh, 2010), and distributional similarity metrics to recognize concept drift (McIntosh and Curran, 2009). Phillips and Riloff (2002) used co-training (Blum and Mitchell, 1998) to exploit three simple classifiers that each recognized a different type</context>
</contexts>
<marker>Roark, Charniak, 1998</marker>
<rawString>B. Roark and E. Charniak. 1998. Noun-phrase Cooccurrence Statistics for Semi-automatic Semantic Lexicon Construction. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics, pages 1110–1116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veselin Stoyanov</author>
<author>Claire Cardie</author>
<author>Nathan Gilbert</author>
<author>Ellen Riloff</author>
<author>David Buttler</author>
<author>David Hysom</author>
</authors>
<title>Coreference resolution with Reconcile.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers,</booktitle>
<pages>156--161</pages>
<contexts>
<context position="13136" citStr="Stoyanov et al., 2010" startWordPosition="2006" endWordPosition="2009">epeats. Although the coreference chains remain the same throughout the process, the lexicon grows so more words in the chains have semantic class labels as bootstrapping progresses. Bootstrapping ends when fewer than 5 words are learned for each of the semantic classes. Many noun phrases are singletons (i.e., they are not coreferent with any other NPs), which limits the set of words that can be learned using coreference chains. Furthermore, coreference resolvers make mistakes, so the accuracy of the induced lexicons depends on the quality of the chains. For our experiments, we used Reconcile (Stoyanov et al., 2010), a freely available supervised coreference resolver. 3.3 Ensemble-based Bootstrapping Framework Figure 1 shows the architecture of our ensemblebased bootstrapping framework. Initially, each lexicon only contains the seed nouns. Each component hypothesizes a set of candidate words for each semantic class, based on its own criteria. The word lists produced by the three systems are then compared, and we retain only the words that were hypothesized with the same class label by at least two of the three systems. The remaining words are discarded. The consenus words are added to the lexicon, and th</context>
</contexts>
<marker>Stoyanov, Cardie, Gilbert, Riloff, Buttler, Hysom, 2010</marker>
<rawString>Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen Riloff, David Buttler, and David Hysom. 2010. Coreference resolution with Reconcile. In Proceedings of the ACL 2010 Conference Short Papers, pages 156– 161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Thelen</author>
<author>E Riloff</author>
</authors>
<title>A Bootstrapping Method for Learning Semantic Lexicons Using Extraction Pa ttern Contexts.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>214--221</pages>
<contexts>
<context position="2065" citStr="Thelen and Riloff, 2002" startWordPosition="295" endWordPosition="298">ficient for specialized domains, such as medicine, chemistry, or microelectronics. Furthermore, in virtually every domain, texts contain lexical variations that are often missing from dictionaries, such as acronyms, abbreviations, spelling variants, informal shorthand terms (e.g., “abx” for “antibiotics”), and composite terms (e.g., “maydecember” or “virus/worm”). To address this problem, techniques have been developed to automate the construction of semantic lexicons from text corpora using bootstrapping methods (Riloff and Shepherd, 1997; Roark and Charniak, 1998; Phillips and Riloff, 2002; Thelen and Riloff, 2002; Ng, 2007; McIntosh and Curran, 2009; McIntosh, 2010), but accuracy is still far from perfect. Our research explores the use of ensemble methods to improve the accuracy of semantic lexicon induction. Our observation is that semantic class associations can be learned using several fundamentally different types of corpus analysis. Bootstrapping methods for semantic lexicon induction (e.g., (Riloff and Jones, 1999; Thelen and Riloff, 2002; McIntosh and Curran, 2009)) collect corpus-wide statistics for individual words based on shared contextual patterns. In contrast, classifiers for semantic tag</context>
<context position="5263" citStr="Thelen and Riloff, 2002" startWordPosition="777" endWordPosition="780">ic tagger versus dictionary lookup on a collection of disease outbreak articles. Our results show that the induced dictionaries yield better performance than an instance-based semantic tagger, achieving higher accuracy with comparable levels of recall. 2 Related Work Several techniques have been developed for semantic class induction (also called set expansion) using bootstrapping methods that consider co-occurrence statistics based on nouns (Riloff and Shepherd, 1997), syntactic structures (Roark and Charniak, 1998; Phillips and Riloff, 2002), and contextual patterns (Riloff and Jones, 1999; Thelen and Riloff, 2002; McIntosh and Curran, 2008; McIntosh and Curran, 2009). To improve the accuracy of induced lexicons, some research has incorporated negative information from human judgements (Vyas and Pantel, 2009), automatically discovered negative classes (McIntosh, 2010), and distributional similarity metrics to recognize concept drift (McIntosh and Curran, 2009). Phillips and Riloff (2002) used co-training (Blum and Mitchell, 1998) to exploit three simple classifiers that each recognized a different type of syntactic structure. The research most closely related to ours is an ensemble-based method for aut</context>
<context position="9685" citStr="Thelen and Riloff, 2002" startWordPosition="1434" endWordPosition="1437">an ensemble-based bootstrapping framework that iteratively collects the hypotheses produced by each individual learner and selects the words that were hypothesized by at least 2 of the 3 learners. This approach produces a bootstrapping process with improved precision, both at the critical beginning stages of the bootstrapping process and during subsequent bootstrapping iterations. 3.2 Component Systems in the Ensemble In the following sections, we describe each of the component systems used in our ensemble. 3.2.1 Pattern-based Lexicon Induction The first component of our ensemble is Basilisk (Thelen and Riloff, 2002), which identifies nouns belonging to a semantic class based on collective information over lexico-syntactic pattern contexts. The patterns are automatically generated using AutoSlog-TS (Riloff, 1996). Basilisk begins with a small set of seed words for each semantic class and a collection of unannotated documents for the domain. In an iterative bootstrapping process, Basilisk identifies candidate nouns, ranks them based on its scoring criteria, selects the 5 most confident words for inclusion in the lexicon, and this process repeats using the new words as additional seeds in subsequent iterati</context>
</contexts>
<marker>Thelen, Riloff, 2002</marker>
<rawString>M. Thelen and E. Riloff. 2002. A Bootstrapping Method for Learning Semantic Lexicons Using Extraction Pa ttern Contexts. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, pages 214–221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vyas</author>
<author>P Pantel</author>
</authors>
<title>Semi-automatic entity set refinement.</title>
<date>2009</date>
<booktitle>In Proceedings of North American Association for Computational Linguistics / Human Language Technology (NAACL/HLT-09).</booktitle>
<contexts>
<context position="5462" citStr="Vyas and Pantel, 2009" startWordPosition="807" endWordPosition="810">ng higher accuracy with comparable levels of recall. 2 Related Work Several techniques have been developed for semantic class induction (also called set expansion) using bootstrapping methods that consider co-occurrence statistics based on nouns (Riloff and Shepherd, 1997), syntactic structures (Roark and Charniak, 1998; Phillips and Riloff, 2002), and contextual patterns (Riloff and Jones, 1999; Thelen and Riloff, 2002; McIntosh and Curran, 2008; McIntosh and Curran, 2009). To improve the accuracy of induced lexicons, some research has incorporated negative information from human judgements (Vyas and Pantel, 2009), automatically discovered negative classes (McIntosh, 2010), and distributional similarity metrics to recognize concept drift (McIntosh and Curran, 2009). Phillips and Riloff (2002) used co-training (Blum and Mitchell, 1998) to exploit three simple classifiers that each recognized a different type of syntactic structure. The research most closely related to ours is an ensemble-based method for automatic thesaurus construction (Curran, 2002). However, that goal was to acquire finegrained semantic information that is more akin to synonymy (e.g., words similar to “house”), whereas we associate w</context>
</contexts>
<marker>Vyas, Pantel, 2009</marker>
<rawString>V. Vyas and P. Pantel. 2009. Semi-automatic entity set refinement. In Proceedings of North American Association for Computational Linguistics / Human Language Technology (NAACL/HLT-09).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>