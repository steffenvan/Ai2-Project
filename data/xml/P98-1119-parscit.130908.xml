<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.7283485">
Automatic Acquisition of Language Model
based on Head-Dependent Relation between Words
</title>
<author confidence="0.948519">
Seungmi Lee and Key-Sun Choi
</author>
<affiliation confidence="0.998103333333333">
Department of Computer Science
Center for Artificial Intelligence Research
Korea Advanced Institute of Science and Technology
</affiliation>
<email confidence="0.977795">
e-mail: fleesm, kschoilOworld.kaist.ac.kr
</email>
<sectionHeader confidence="0.993852" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999876928571429">
Language modeling is to associate a sequence
of words with a priori probability, which is a
key part of many natural language applications
such as speech recognition and statistical ma-
chine translation. In this paper, we present a
language modeling based on a kind of simple
dependency grammar. The grammar consists
of head-dependent relations between words and
can be learned automatically from a raw corpus
using the reestimation algorithm which is also
introduced in this paper. Our experiments show
that the proposed model performs better than
n-gram models at 11% to 11.5% reductions in
test corpus entropy.
</bodyText>
<sectionHeader confidence="0.998428" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999945984615385">
Language modeling is to associate a priori prob-
ability to a sentence. It is a key part of many
natural language applications such as speech
recognition and statistical machine translation.
Previous works for language modeling can be
broadly divided into two approaches; one is n-
gram-based and the other is grammar-based.
N-gram model estimates the probability of a
sentence as the product of the probability of
each word in the sentence. It assumes that
probability of the nth word is dependent on
the previous 71 - 1 words. The n-gram prob-
abilities are estimated by simply counting the
n-gram frequencies in a training corpus. In
some cases, class (or part of speech) n-grams
are used instead of word n-grams(Brown et al.,
1992; Chang and Chen, 1996). N-gram model
has been widely used so far, but it has always
been clear that n-gram can not represent long
distance dependencies.
In contrast with n-gram model, grammar-
based approach assigns syntactic structures to
a sentence and coin putes the probability of the
sentence using the probabilities of the struc-
tures. Long distance dependencies can be rep-
resented well by means of the structures. The
approach usually makes use of phrase struc-
ture grammars such as probabilistic context-free
grammar and recursive transition network(Lari
and Young, 1991; Sneff, 1992; Chen, 1996). In
the approach, however, a sentence which is not
accepted by the grammar is assigned zero prob-
ability. Thus, the grammar must have broad-
coverage so that any sentence will get non-zero
probability. But acquisition of such a robust
grammar has been known to be very difficult.
Due to the difficulty, some works try to use an
integrated model of grammar and n-gram com-
pensating each other (McCandless, 1994; Meteer
and Rohlicek, 1993). Given a robust grammar,
grammar-based language modeling is expected
to be more powerful and compact in model size
than n-gram-based one.
In this paper we present a language modeling
based on a kind of simple dependency gram-
mar. The grammar consists of head-dependent
relations between words and can be learned au-
tomatically from a raw corpus using the rees-
timation algorithm which is also introduced in
this paper. Based on the dependencies, a sen-
tence is analyzed and assigned syntactic struc-
tures by which long distance dependences are
represented. Because the model can be thought
of as a linguistic bi-gram model, the smoothing
functions of n-gram models can be applied to it.
Thus, the model can be robust, adapt easily to
new domains, and be effective.
The paper is organized as follows. We intro-
duce some definitions and notations for the de-
pendency grammar and the reestimation algo-
rithm in section 2, and explain the algorithm in
section 3. In section 4, we show the experimen-
tal results for the suggested model compared to
n-gram models. Finally, section 5 concludes this
paper.
</bodyText>
<sectionHeader confidence="0.986546" genericHeader="method">
2 A Simple Dependency Grammar
</sectionHeader>
<bodyText confidence="0.963089">
In this paper, we assume a kind of simple de-
pendency grammar which describes a language
</bodyText>
<page confidence="0.997529">
723
</page>
<bodyText confidence="0.999517727272727">
by a set of head-dependent relations between
words. A sentence is analyzed by establishing
dependency links between individual words in
the sentence. A dependency analysis, D, of a
sentence can be represented with arrows point-
ing from head to dependent as depicted in Fig-
ure 1. For structural generality, we assume that
there is always a marking tag, &amp;quot;EOS&amp;quot; (End of
Sentence), at the end of a sentence and it has
the head word of the sentence as its own depen-
dent(&amp;quot;gave&amp;quot; in Figure 1).
</bodyText>
<figure confidence="0.439147">
I gave him a book EOS
</figure>
<figureCaption confidence="0.998881">
Figure 1: An example dependency analysis
</figureCaption>
<bodyText confidence="0.9986855">
A D is a set of inter-word dependencies which
satisfy the following conditions: (1) every word
in the sentence has its head in the sentence ex-
cept the head word of the sentence. (2) every
word can have only one head. (3) there is nei-
ther crossing nor cycle of dependencies.
The probabilistic model of the simple depen-
dency grammar is given by
</bodyText>
<equation confidence="0.9426518">
p(sentence) E p(V)
D x.-+yED
where p(x y) = P(Yix)
freq(x y)
EL, freq(x –+ z)•
</equation>
<sectionHeader confidence="0.534873" genericHeader="method">
Complete-Link and Complete-Sequence
</sectionHeader>
<bodyText confidence="0.999367625">
Here, we define complete-link and complete-
sequence which represent partial Ds for sub-
strings. They are used to construct overall
Ds and used as the basic structures for the rees-
timation algorithm in section 3.
A set of dependency relations on a word se-
quence, wio 1, is a complete-link when the fol-
lowing conditions are satisfied:
</bodyText>
<listItem confidence="0.999248833333333">
• there is (wi wj) or (wi &lt;— wj) exclu-
sively.
• Every inner word has a head in the word
sequence.
• Neither crossing nor cycle of dependency
relations is allowed.
</listItem>
<bodyText confidence="0.955013">
We use wi for ith word in a sentence and wi,j for the
word sequence from wi to wi(i &lt; j).
ive him a book her second child the bus
</bodyText>
<figure confidence="0.616238">
kJ
</figure>
<figureCaption confidence="0.996863">
Figure 2: Example complete-links
</figureCaption>
<bodyText confidence="0.9441905">
A complete-link has direction. A complete-link
on wio is said to be &amp;quot;rightward&amp;quot; if the outermost
relation is (wi wj), and &amp;quot;leftward&amp;quot; if the rela-
tion is (wi &lt;— w3). Unit complete-link is defined
on a string of two adjacent words, wi,i+1. In
Figure 2, (a) is a rightward complete-link, and
both of (b) and (c) are leftward ones.
bird in the cage the bus book
</bodyText>
<figure confidence="0.700103">
kJ
</figure>
<figureCaption confidence="0.999166">
Figure 3: Example complete-sequences
</figureCaption>
<bodyText confidence="0.9972813">
A complete-sequence is a sequence of 0 or
more adjacent complete-links that have the
same direction. A unit complete-sequence is de-
fined on a string of one word. It is 0 sequence
of complete-links. The direction of a complete-
sequence is determined by the direction of the
component complete-links. In Figure 3, (a) is a
rightward complete-sequence composed of two
complete-links, and (b) is a leftward one. (c) is a
complete-sequence composed of zero complete-
links, and it can be both leftward and rightward.
The word of &amp;quot;complete&amp;quot; means that the de-
pendency relations on the inner words are com-
pleted and that consequently there is no need
to process further on them. From now on,
we use /„.(i, j)/Lt(i, j) for rightward/leftward
complete-links and Sr(i, j)/Si(i, j) for right-
ward/leftward complete-sequences on wio.
Any complete-link on wio can be viewed as
the following combination.
</bodyText>
<listItem confidence="0.999921">
• Lr(i,i): {(wi wj), Sr(i, m), Si(m+1, j)}
• Li(i, j): {(wi w;), Sr(i, in), Si(ni+1, :7)}
</listItem>
<bodyText confidence="0.970022857142857">
for a m(i &lt; in &lt;i).
Otherwise, the set of dependencies does not sat-
isfy the conditions of no crossing, no cycle and
no multiple heads and is not a complete-link any
more.
Similarly, any complete-sequence on wio can
be viewed as the following combination.
</bodyText>
<listItem confidence="0.9998355">
• Sr(i, j): {Sr(i, m), Lr(rn, j)}
• Si(i, j): {Li(i, m), Si(rn, j)}
</listItem>
<bodyText confidence="0.926758">
for a m(i &lt; m &lt;j).
In the case of complete-sequence, we can
prevent multiple constructions of the same
</bodyText>
<page confidence="0.988652">
724
</page>
<bodyText confidence="0.6172835">
complete-sequence by the above combinational
restriction.
</bodyText>
<figureCaption confidence="0.832474166666667">
Figure 4: Abstract representation of D
Figure 4 shows an abstract representation of
a D of an n-word sentence. When wk(1 &lt; k &lt;
n) is the head of the sentence, any D of the
sentence can be represented by a SO, EOS)
uniquely by the assumption that there is always
</figureCaption>
<bodyText confidence="0.947753">
the dependency relation, (wk wEos)•
</bodyText>
<sectionHeader confidence="0.996311" genericHeader="method">
3 Reestimation Algorithm
</sectionHeader>
<bodyText confidence="0.998892357142857">
The reestimation algorithm is a variation of
Inside-Outside algorithm(Jelinek et at., 1990)
adapted to dependency grammar. In this sec-
tion we first define the inside-outside probabili-
ties of complete-links and complete-sequences,
and then describe the reestimation algorithm
based on them&apos;.
In the followings, indicates inside probabil-
ity and a is for outside probability. The su-
perscripts, 1 and s, are used for &amp;quot;complete-link&amp;quot;
and &amp;quot;complete-sequence&amp;quot; respectively. The sub-
scripts indicate direction: r for &amp;quot;rightward&amp;quot; and
1 for &amp;quot;leftward&amp;quot;.
The inside probabilities of complete-links
</bodyText>
<equation confidence="0.995294764705882">
(Lr (i, j), L1(i, j)) and complete-sequences
(Sr (i, j), j)) are as follows.
J-1
E p(wi w j) (3, s (i , m) fir (m +
m=i
j—i
E Pop= wimi,Troot(m + 1,j).
E
mi
01(2 On) Or (m, 3).
s
m=i+1
The basis probabilities are:
/3(i, i+ 1) = P(wi -4 wi+t)
01(i, -F 1) = P(wi wi+t)
i) = 07(i, = 1
Ot(1, EOS) = P(IDI,n)
</equation>
<footnote confidence="0.976393">
2A little more detailed explanation of the expressions
can be found in (Lee and Choi. 1997).
</footnote>
<equation confidence="0.997464">
/3(i, i+ 1) = p(Lr(i, + 1)) = p(wi w,+1)
OM, i 1) = p(LE(i, + 1)) = P(wi 4— w =4-1)
</equation>
<bodyText confidence="0.875090857142857">
/3t(1, EOS) is the sentence probability be-
cause every dependency analysis, V, is repre-
sented by a SO, EOS) and 07(1, EOS) is sum
of the probability of every S1(1, EOS).
The outside probabilities for complete-
links (Lr (i, j), L1 (i, j)) and complete-sequences
(Sr(i, j), Si(i, j)) are as follows.
</bodyText>
<equation confidence="0.930409928571429">
E .4(v,i)o(y, ti).
v=1
E a7(i, h)07 (j, h).
h=j
E
h=j+i
h)/3(j + 1, h)p(wi wh)
hpr(j+ 1, h)p(wi wh).
E aT(v, j)01(v,
v.t
+air (v, i)N(v, 1)P(wv -4 wi)
j)13,s.(v, i — 1)p(w0 w j).
The basis probability is
4(1, EOS) = 1.
</equation>
<bodyText confidence="0.999980727272727">
Given a training corpus, the initial grammar
is just a list of all pairs of unique words in
the corpus. The initial pairs represent the ten-
tative head-dependent relations of the words.
And the initial probabilities of the pairs can
be given randomly. The training starts with
the initial grammar. The train corpus is an-
alyzed with the grammar and the occurrence
frequency of each dependency relation is cal-
culated. Based on the frequencies, probabili-
ties of dependency relations are recalculated by
</bodyText>
<equation confidence="0.988451333333333">
C(wp wq)
pe(wp 4 wq) = . The process
Ewr c(w, -&gt; wr)
</equation>
<bodyText confidence="0.9986375">
continues until the entropy of the training cor-
pus becomes the minimum. The frequency of
</bodyText>
<equation confidence="0.899648">
occurrence, C(wi IDA is calculated by
C(wi wi) =Ep(Diwi,00„(w, wi,n)
1
ir(i, :1)13,-1 i)
P(wi,n)
</equation>
<bodyText confidence="0.983359">
where Occ(wi wj,D , is 1 if the depen-
dency relation, (wi wi), is used in the V,
</bodyText>
<equation confidence="0.985102428571428">
Olr j) =
131(i7 j) =
Ogit j)
=
air (i, .i)
=
=
</equation>
<page confidence="0.989288">
725
</page>
<bodyText confidence="0.9213095">
and 0 otherwise. Similarly, the occurrence fre-
quency of the dependency relation, (wi wi),
</bodyText>
<listItem confidence="0.716539">
• • •
</listItem>
<bodyText confidence="0.680352">
is computed by 1,
</bodyText>
<equation confidence="0.432716">
MW1,71.1
</equation>
<sectionHeader confidence="0.887111" genericHeader="method">
4 Preliminary experiments
</sectionHeader>
<bodyText confidence="0.999484931034483">
We have experimented with three language
models, tri-gram model (TRI), bi-gram model
(BI), and the proposed model (DEP) on a raw
corpus extracted from KAIST corpus3. The raw
corpus consists of 1,589 sentences with 13,139
words, describing animal life in nature. We
randomly divided the corpus into two parts: a
training set of 1,445 sentences and a test set of
144 sentences. And we made 15 partial training
sets which include the first s sentences in the
whole training set, for s ranging from 100 to
1,445 sentences. We trained the three language
models for each partial training set, and tested
the training and the test corpus entropies.
TRI and BI was trained by counting the oc-
currence of tri-grams and bi-grams respectively.
DEP was trained by running the reestimation
algorithm iteratively until it converges to an op-
timal dependency grammar. On the average, 26
iterations were done for the training sets.
Smoothing is needed for language modeling
due to the sparse data problem. It is to com-
pensate for the overestimated and the under-
estimated probabilities. Smoothing method it-
self is an important factor. But our goal is not
to find out a better smoothing method. So we
fixed on an interpolation method and applied it
for the three models. It can be represented as
(McCandless, 1994)
</bodyText>
<equation confidence="0.7814875">
Pn(Wi •••, Wi-1)= APn(Wi •••• Wi-1)
— A)Pn-l(Wi
</equation>
<bodyText confidence="0.838128">
where
</bodyText>
<equation confidence="0.8249885">
A = C(wi, •.., wn.-1) + Ks •
C(W1,7 •••7 11/7/1)
</equation>
<bodyText confidence="0.93491">
The Ks is the global smoothing factor. The big-
ger the Ks, the larger the degree of smoothing.
For the experiments we used 2 for K.
We take the performance of a language model
to be its cross-entropy on test corpus,
-
</bodyText>
<figure confidence="0.51882075">
&apos; E —log2p,n(si)
VI
sKAIST (Korean Advanced Institute of Science and
Technology) corpus has been under construction since
</figure>
<figureCaption confidence="0.47138">
1994. It consists of raw text collection(45,000,000
words). POS-tagged collection(6.750,000 words), and
tree-tagged collection(30.000 sentences) at present.
</figureCaption>
<bodyText confidence="0.9854305">
where the test corpus contains a total of VI
words and is composed of S sentences.
</bodyText>
<figure confidence="0.982784769230769">
3.4
3.2
3
2.8
2.6
2.4
2.2
2
1.8
1.6
1.4
0 200 400 600 800 1000 1200 1400 1600
No. of training sentences
</figure>
<figureCaption confidence="0.999932">
Figure 5: Training corpus entropies
</figureCaption>
<bodyText confidence="0.9984269">
Figure 5 shows the training corpus entropies
of the three models. It is not surprising that
DEP performs better than BI. DEP can be
thought of as a kind of linguistic bi-gram model
in which long distance dependencies can be rep-
resented through the head-dependent relations
between words. TRI shows better performance
than both BI and DEP. We think it is because
TRI overfits the training corpus, judging from
the experimental results for the test corpus.
</bodyText>
<figureCaption confidence="0.993362">
Figure 6: Test corpus entropies
</figureCaption>
<bodyText confidence="0.9999274375">
For the test corpus, BI shows slightly bet-
ter performance than TRI as depicted in Fig-
ure 6. Increase in the order of n-gram from
two to three shows no gains in entropy reduc-
tion. DEP, however, shows still better per-
formance than the n-gram models. It shows
about 11.5% entropy reduction to BI and about
11% entropy reduction to TRI. Figure 7 shows
the entropies for the mixed corpus of training
and test sets. From the results, we can see
that head-dependent relations between words
are more useful information than the naive n-
gram sequences, for language modeling. We can
see also that the reestimation algorithm can find
out properly the hidden head-dependent rela-
tions between words, from a raw corpus.
</bodyText>
<figure confidence="0.985424318181818">
200 400 600 800 1000 1200 1400 1600
No. of training sentences
9.5
9
8.5
8
7.5
7
6.5
0
726
10
9
8
7
6
5
4
3
0
200 400 600 800 1000 1200 1400 1600
No. of training sentences
</figure>
<figureCaption confidence="0.996606">
Figure 7: Mixed corpus entropies
</figureCaption>
<figure confidence="0.999461222222222">
60000
50000
40000
30000
20000
10000
0
0 200 400 600 800 1000 1200 1400 1600
No. of training sentences
</figure>
<figureCaption confidence="0.999995">
Figure 8: Model size
</figureCaption>
<bodyText confidence="0.999927166666667">
Related to the size of model, however, DEP
has much more parameters than TRI and BI
as depicted in Figure 8. This can be a serious
problem when we create a language model from
a large body of text. In the experiments, how-
ever, DEP used the grammar acquired automat-
ically as it is. In the grammar, many inter-word
dependencies have probabilities near 0. If we
exclude such dependencies as was experimented
for n-grams by Seymore and Rosenfeld (1996),
we may get much more compact DEP model
with very slight increase in entropy.
</bodyText>
<sectionHeader confidence="0.999237" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999948954545454">
In this paper, we presented a language model
based on a kind of simple dependency gram-
mar. The grammar consists of head-dependent
relations between words and can be learned au-
tomatically from a raw corpus by the reestima-
tion algorithm which is also introduced in this
paper. By the preliminary experiments, it was
shown that the proposed language model per-
forms better than n-gram models in test cor-
pus entropy. This means that the reestitnation
algorithm can find out the hidden information
of head-dependent relation between words in a
raw corpus, and the information is more useful
than the naive word sequences of n-gram, for
language modeling.
We are planning to experiment the perfor-
mance of the proposed language model for large
corpus, for various domains, and with various
smoothing methods. For the size of the model,
we are planning to test the effects of excluding
the dependency relations with near zero proba-
bilities.
</bodyText>
<sectionHeader confidence="0.998901" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998970794871795">
P. F. Brown, V. J. Della Pietra, P. V. deSouza,
J. C. Lai, and R. L. Mercer. 1992. &amp;quot;Class-
Based n-gram Models of Natural Language&amp;quot;.
Computational Linguistics, 18(4):467-480.
C. Chang and C. Chen. 1996. &amp;quot;Application Is-
sues of SA-class Bigram Language Models&amp;quot;.
Computer Processing of Oriental Languages,
10(1):1-15.
S. F. Chen. 1996. &amp;quot;Building Probabilistic
Models for Natural Language&amp;quot;. Ph.D. the-
sis, Havard University, Cambridge, Mas-
sachusetts.
F. Jelinek, J. D. Lafferty, and R. L. Mercer.
1990. &amp;quot;Basic Methods of Probabilistic Con-
text Free Grammars&amp;quot;. Technical report, IBM
— T.J. Watson Research Center.
K. Lan i and S. J. Young. 1991. &amp;quot;Applications
of stochastic context-free grammars using the
inside-outside algorithm&amp;quot;. Computer Speech
and Language, 5:237-257.
S. Lee and K. Choi. 1997. &amp;quot;Reestimation and
Best-First Parsing Algorithm for Probabilis-
tic Dependency Grammar&amp;quot;. In t1TVIC-5,
pages 11-21.
M. K. McCandless. 1994. &amp;quot;Automatic Acquisi-
tion of Language Models for Speech Recog-
nition&amp;quot;. Master&apos;s thesis, Massachusetts Insti-
tute of Technology.
M. Meteer and J.R. Rohlicek. 1993. &amp;quot;Statis-
tical Language Modeling Combining N-gram
and Context-free Grammars&amp;quot;. In ICASSP-
93, volume II, pages 37-40, January.
K. Seymore and R. Rosenfeld. 1996. &amp;quot;Scalable
Trigram Backoff Language Models&amp;quot;. Techni-
cal Report CMU-CS-96-139, Carnegie Mellon
University.
S. Sneff. 1992. &amp;quot;TINA: A natural language sys-
tem for spoken language applications&amp;quot;. Com-
putational Linguistics, 18(1):61-86.
</reference>
<figure confidence="0.8758755">
(DEP model -e--
1(Fi mode -a—
l l model -I--
BI
</figure>
<page confidence="0.930771">
727
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.489892">
<title confidence="0.998707">Automatic Acquisition of Language Model based on Head-Dependent Relation between Words</title>
<author confidence="0.999441">Seungmi Lee</author>
<author confidence="0.999441">Key-Sun Choi</author>
<affiliation confidence="0.999383">Department of Computer Science Center for Artificial Intelligence Research</affiliation>
<author confidence="0.529516">Korea Advanced Institute of Science</author>
<author confidence="0.529516">Technology</author>
<email confidence="0.997947">e-mail:fleesm,kschoilOworld.kaist.ac.kr</email>
<abstract confidence="0.9946094">Language modeling is to associate a sequence words with a which is a key part of many natural language applications such as speech recognition and statistical machine translation. In this paper, we present a language modeling based on a kind of simple dependency grammar. The grammar consists of head-dependent relations between words and can be learned automatically from a raw corpus using the reestimation algorithm which is also introduced in this paper. Our experiments show that the proposed model performs better than n-gram models at 11% to 11.5% reductions in test corpus entropy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>V J Della Pietra</author>
<author>P V deSouza</author>
<author>J C Lai</author>
<author>R L Mercer</author>
</authors>
<title>ClassBased n-gram Models of Natural Language&amp;quot;.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--4</pages>
<contexts>
<context position="1649" citStr="Brown et al., 1992" startWordPosition="253" endWordPosition="256">e applications such as speech recognition and statistical machine translation. Previous works for language modeling can be broadly divided into two approaches; one is ngram-based and the other is grammar-based. N-gram model estimates the probability of a sentence as the product of the probability of each word in the sentence. It assumes that probability of the nth word is dependent on the previous 71 - 1 words. The n-gram probabilities are estimated by simply counting the n-gram frequencies in a training corpus. In some cases, class (or part of speech) n-grams are used instead of word n-grams(Brown et al., 1992; Chang and Chen, 1996). N-gram model has been widely used so far, but it has always been clear that n-gram can not represent long distance dependencies. In contrast with n-gram model, grammarbased approach assigns syntactic structures to a sentence and coin putes the probability of the sentence using the probabilities of the structures. Long distance dependencies can be represented well by means of the structures. The approach usually makes use of phrase structure grammars such as probabilistic context-free grammar and recursive transition network(Lari and Young, 1991; Sneff, 1992; Chen, 1996</context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>P. F. Brown, V. J. Della Pietra, P. V. deSouza, J. C. Lai, and R. L. Mercer. 1992. &amp;quot;ClassBased n-gram Models of Natural Language&amp;quot;. Computational Linguistics, 18(4):467-480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Chang</author>
<author>C Chen</author>
</authors>
<title>Application Issues of SA-class Bigram Language Models&amp;quot;.</title>
<date>1996</date>
<booktitle>Computer Processing of Oriental Languages,</booktitle>
<pages>10--1</pages>
<contexts>
<context position="1672" citStr="Chang and Chen, 1996" startWordPosition="257" endWordPosition="260">as speech recognition and statistical machine translation. Previous works for language modeling can be broadly divided into two approaches; one is ngram-based and the other is grammar-based. N-gram model estimates the probability of a sentence as the product of the probability of each word in the sentence. It assumes that probability of the nth word is dependent on the previous 71 - 1 words. The n-gram probabilities are estimated by simply counting the n-gram frequencies in a training corpus. In some cases, class (or part of speech) n-grams are used instead of word n-grams(Brown et al., 1992; Chang and Chen, 1996). N-gram model has been widely used so far, but it has always been clear that n-gram can not represent long distance dependencies. In contrast with n-gram model, grammarbased approach assigns syntactic structures to a sentence and coin putes the probability of the sentence using the probabilities of the structures. Long distance dependencies can be represented well by means of the structures. The approach usually makes use of phrase structure grammars such as probabilistic context-free grammar and recursive transition network(Lari and Young, 1991; Sneff, 1992; Chen, 1996). In the approach, how</context>
</contexts>
<marker>Chang, Chen, 1996</marker>
<rawString>C. Chang and C. Chen. 1996. &amp;quot;Application Issues of SA-class Bigram Language Models&amp;quot;. Computer Processing of Oriental Languages, 10(1):1-15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
</authors>
<title>Building Probabilistic Models for Natural Language&amp;quot;.</title>
<date>1996</date>
<tech>Ph.D. thesis,</tech>
<institution>Havard University,</institution>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="1672" citStr="Chen, 1996" startWordPosition="259" endWordPosition="260">recognition and statistical machine translation. Previous works for language modeling can be broadly divided into two approaches; one is ngram-based and the other is grammar-based. N-gram model estimates the probability of a sentence as the product of the probability of each word in the sentence. It assumes that probability of the nth word is dependent on the previous 71 - 1 words. The n-gram probabilities are estimated by simply counting the n-gram frequencies in a training corpus. In some cases, class (or part of speech) n-grams are used instead of word n-grams(Brown et al., 1992; Chang and Chen, 1996). N-gram model has been widely used so far, but it has always been clear that n-gram can not represent long distance dependencies. In contrast with n-gram model, grammarbased approach assigns syntactic structures to a sentence and coin putes the probability of the sentence using the probabilities of the structures. Long distance dependencies can be represented well by means of the structures. The approach usually makes use of phrase structure grammars such as probabilistic context-free grammar and recursive transition network(Lari and Young, 1991; Sneff, 1992; Chen, 1996). In the approach, how</context>
</contexts>
<marker>Chen, 1996</marker>
<rawString>S. F. Chen. 1996. &amp;quot;Building Probabilistic Models for Natural Language&amp;quot;. Ph.D. thesis, Havard University, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
<author>J D Lafferty</author>
<author>R L Mercer</author>
</authors>
<title>Basic Methods of Probabilistic Context Free Grammars&amp;quot;.</title>
<date>1990</date>
<tech>Technical report, IBM —</tech>
<institution>T.J. Watson Research Center.</institution>
<marker>Jelinek, Lafferty, Mercer, 1990</marker>
<rawString>F. Jelinek, J. D. Lafferty, and R. L. Mercer. 1990. &amp;quot;Basic Methods of Probabilistic Context Free Grammars&amp;quot;. Technical report, IBM — T.J. Watson Research Center.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lan i</author>
<author>S J Young</author>
</authors>
<title>Applications of stochastic context-free grammars using the inside-outside algorithm&amp;quot;.</title>
<date>1991</date>
<journal>Computer Speech and Language,</journal>
<pages>5--237</pages>
<contexts>
<context position="2224" citStr="i and Young, 1991" startWordPosition="343" endWordPosition="346">instead of word n-grams(Brown et al., 1992; Chang and Chen, 1996). N-gram model has been widely used so far, but it has always been clear that n-gram can not represent long distance dependencies. In contrast with n-gram model, grammarbased approach assigns syntactic structures to a sentence and coin putes the probability of the sentence using the probabilities of the structures. Long distance dependencies can be represented well by means of the structures. The approach usually makes use of phrase structure grammars such as probabilistic context-free grammar and recursive transition network(Lari and Young, 1991; Sneff, 1992; Chen, 1996). In the approach, however, a sentence which is not accepted by the grammar is assigned zero probability. Thus, the grammar must have broadcoverage so that any sentence will get non-zero probability. But acquisition of such a robust grammar has been known to be very difficult. Due to the difficulty, some works try to use an integrated model of grammar and n-gram compensating each other (McCandless, 1994; Meteer and Rohlicek, 1993). Given a robust grammar, grammar-based language modeling is expected to be more powerful and compact in model size than n-gram-based one. I</context>
</contexts>
<marker>i, Young, 1991</marker>
<rawString>K. Lan i and S. J. Young. 1991. &amp;quot;Applications of stochastic context-free grammars using the inside-outside algorithm&amp;quot;. Computer Speech and Language, 5:237-257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lee</author>
<author>K Choi</author>
</authors>
<title>Reestimation and Best-First Parsing Algorithm for Probabilistic Dependency Grammar&amp;quot;. In</title>
<date>1997</date>
<booktitle>t1TVIC-5,</booktitle>
<pages>11--21</pages>
<marker>Lee, Choi, 1997</marker>
<rawString>S. Lee and K. Choi. 1997. &amp;quot;Reestimation and Best-First Parsing Algorithm for Probabilistic Dependency Grammar&amp;quot;. In t1TVIC-5, pages 11-21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M K McCandless</author>
</authors>
<title>Automatic Acquisition of Language Models for Speech Recognition&amp;quot;.</title>
<date>1994</date>
<tech>Master&apos;s thesis,</tech>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="2656" citStr="McCandless, 1994" startWordPosition="418" endWordPosition="419">by means of the structures. The approach usually makes use of phrase structure grammars such as probabilistic context-free grammar and recursive transition network(Lari and Young, 1991; Sneff, 1992; Chen, 1996). In the approach, however, a sentence which is not accepted by the grammar is assigned zero probability. Thus, the grammar must have broadcoverage so that any sentence will get non-zero probability. But acquisition of such a robust grammar has been known to be very difficult. Due to the difficulty, some works try to use an integrated model of grammar and n-gram compensating each other (McCandless, 1994; Meteer and Rohlicek, 1993). Given a robust grammar, grammar-based language modeling is expected to be more powerful and compact in model size than n-gram-based one. In this paper we present a language modeling based on a kind of simple dependency grammar. The grammar consists of head-dependent relations between words and can be learned automatically from a raw corpus using the reestimation algorithm which is also introduced in this paper. Based on the dependencies, a sentence is analyzed and assigned syntactic structures by which long distance dependences are represented. Because the model c</context>
<context position="11639" citStr="McCandless, 1994" startWordPosition="1994" endWordPosition="1995">the occurrence of tri-grams and bi-grams respectively. DEP was trained by running the reestimation algorithm iteratively until it converges to an optimal dependency grammar. On the average, 26 iterations were done for the training sets. Smoothing is needed for language modeling due to the sparse data problem. It is to compensate for the overestimated and the underestimated probabilities. Smoothing method itself is an important factor. But our goal is not to find out a better smoothing method. So we fixed on an interpolation method and applied it for the three models. It can be represented as (McCandless, 1994) Pn(Wi •••, Wi-1)= APn(Wi •••• Wi-1) — A)Pn-l(Wi where A = C(wi, •.., wn.-1) + Ks • C(W1,7 •••7 11/7/1) The Ks is the global smoothing factor. The bigger the Ks, the larger the degree of smoothing. For the experiments we used 2 for K. We take the performance of a language model to be its cross-entropy on test corpus, - &apos; E —log2p,n(si) VI sKAIST (Korean Advanced Institute of Science and Technology) corpus has been under construction since 1994. It consists of raw text collection(45,000,000 words). POS-tagged collection(6.750,000 words), and tree-tagged collection(30.000 sentences) at present. </context>
</contexts>
<marker>McCandless, 1994</marker>
<rawString>M. K. McCandless. 1994. &amp;quot;Automatic Acquisition of Language Models for Speech Recognition&amp;quot;. Master&apos;s thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Meteer</author>
<author>J R Rohlicek</author>
</authors>
<title>Statistical Language Modeling Combining N-gram and Context-free Grammars&amp;quot;.</title>
<date>1993</date>
<booktitle>In ICASSP93, volume II,</booktitle>
<pages>37--40</pages>
<contexts>
<context position="2684" citStr="Meteer and Rohlicek, 1993" startWordPosition="420" endWordPosition="423">ructures. The approach usually makes use of phrase structure grammars such as probabilistic context-free grammar and recursive transition network(Lari and Young, 1991; Sneff, 1992; Chen, 1996). In the approach, however, a sentence which is not accepted by the grammar is assigned zero probability. Thus, the grammar must have broadcoverage so that any sentence will get non-zero probability. But acquisition of such a robust grammar has been known to be very difficult. Due to the difficulty, some works try to use an integrated model of grammar and n-gram compensating each other (McCandless, 1994; Meteer and Rohlicek, 1993). Given a robust grammar, grammar-based language modeling is expected to be more powerful and compact in model size than n-gram-based one. In this paper we present a language modeling based on a kind of simple dependency grammar. The grammar consists of head-dependent relations between words and can be learned automatically from a raw corpus using the reestimation algorithm which is also introduced in this paper. Based on the dependencies, a sentence is analyzed and assigned syntactic structures by which long distance dependences are represented. Because the model can be thought of as a lingui</context>
</contexts>
<marker>Meteer, Rohlicek, 1993</marker>
<rawString>M. Meteer and J.R. Rohlicek. 1993. &amp;quot;Statistical Language Modeling Combining N-gram and Context-free Grammars&amp;quot;. In ICASSP93, volume II, pages 37-40, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Seymore</author>
<author>R Rosenfeld</author>
</authors>
<title>Scalable Trigram Backoff Language Models&amp;quot;.</title>
<date>1996</date>
<tech>Technical Report CMU-CS-96-139,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="14430" citStr="Seymore and Rosenfeld (1996)" startWordPosition="2487" endWordPosition="2490">. of training sentences Figure 7: Mixed corpus entropies 60000 50000 40000 30000 20000 10000 0 0 200 400 600 800 1000 1200 1400 1600 No. of training sentences Figure 8: Model size Related to the size of model, however, DEP has much more parameters than TRI and BI as depicted in Figure 8. This can be a serious problem when we create a language model from a large body of text. In the experiments, however, DEP used the grammar acquired automatically as it is. In the grammar, many inter-word dependencies have probabilities near 0. If we exclude such dependencies as was experimented for n-grams by Seymore and Rosenfeld (1996), we may get much more compact DEP model with very slight increase in entropy. 5 Conclusions In this paper, we presented a language model based on a kind of simple dependency grammar. The grammar consists of head-dependent relations between words and can be learned automatically from a raw corpus by the reestimation algorithm which is also introduced in this paper. By the preliminary experiments, it was shown that the proposed language model performs better than n-gram models in test corpus entropy. This means that the reestitnation algorithm can find out the hidden information of head-depende</context>
</contexts>
<marker>Seymore, Rosenfeld, 1996</marker>
<rawString>K. Seymore and R. Rosenfeld. 1996. &amp;quot;Scalable Trigram Backoff Language Models&amp;quot;. Technical Report CMU-CS-96-139, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sneff</author>
</authors>
<title>TINA: A natural language system for spoken language applications&amp;quot;.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--1</pages>
<contexts>
<context position="2237" citStr="Sneff, 1992" startWordPosition="347" endWordPosition="348">rams(Brown et al., 1992; Chang and Chen, 1996). N-gram model has been widely used so far, but it has always been clear that n-gram can not represent long distance dependencies. In contrast with n-gram model, grammarbased approach assigns syntactic structures to a sentence and coin putes the probability of the sentence using the probabilities of the structures. Long distance dependencies can be represented well by means of the structures. The approach usually makes use of phrase structure grammars such as probabilistic context-free grammar and recursive transition network(Lari and Young, 1991; Sneff, 1992; Chen, 1996). In the approach, however, a sentence which is not accepted by the grammar is assigned zero probability. Thus, the grammar must have broadcoverage so that any sentence will get non-zero probability. But acquisition of such a robust grammar has been known to be very difficult. Due to the difficulty, some works try to use an integrated model of grammar and n-gram compensating each other (McCandless, 1994; Meteer and Rohlicek, 1993). Given a robust grammar, grammar-based language modeling is expected to be more powerful and compact in model size than n-gram-based one. In this paper </context>
</contexts>
<marker>Sneff, 1992</marker>
<rawString>S. Sneff. 1992. &amp;quot;TINA: A natural language system for spoken language applications&amp;quot;. Computational Linguistics, 18(1):61-86.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>