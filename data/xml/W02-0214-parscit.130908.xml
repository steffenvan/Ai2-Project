<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000423">
<note confidence="0.9942245">
Proceedings of the Third SIGdial Workshop on Discourse and Dialogue,
Philadelphia, July 2002, pp. 95-102. Association for Computational Linguistics.
</note>
<title confidence="0.996572">
Topic Identification In Natural Language Dialogues Using
Neural Networks
</title>
<author confidence="0.994203">
Krista Lagus and Jukka Kuusisto
</author>
<affiliation confidence="0.995956">
Neural Networks Research Centre, Helsinki University of Technology
</affiliation>
<address confidence="0.930177">
P.O.Box 9800, FIN-02015 HUT, Finland
</address>
<email confidence="0.996048">
krista.lagus@hut.fi
</email>
<sectionHeader confidence="0.979832" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99997364516129">
In human–computer interaction sys-
tems using natural language, the
recognition of the topic from user’s
utterances is an important task. We
examine two different perspectives
to the problem of topic analysis
needed for carrying out a success-
ful dialogue. First, we apply self-
organized document maps for mod-
eling the broader subject of dis-
course based on the occurrence of
content words in the dialogue con-
text. On a Finnish corpus of 57
dialogues the method is shown to
work well for recognizing subjects of
longer dialogue segments, whereas
for individual utterances the sub-
ject recognition history should per-
haps be taken into account. Sec-
ond, we attempt to identify topically
relevant words in the utterances
and thus locate the old information
(’topic words’) and new information
(’focus words’). For this we define a
probabilistic model and compare dif-
ferent methods for model parameter
estimation on a corpus of 189 dia-
logues. Moreover, the utilization of
information regarding the position
of the word in the utterance is found
to improve the results.
</bodyText>
<sectionHeader confidence="0.995061" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999961">
The analysis of the topic of a sentence or a
document is an important task for many nat-
ural language applications. For example, in
interactive dialogue systems that attempt to
carry out and answer requests made by cus-
tomers, the response strategy employed may
depend on the topic of the request (Jokinen et
al., 2002). In large vocabulary speech recog-
nition knowledge of the topic can, in general,
be utilized for adjusting the language model
used (see, e.g., (Iyer and Ostendorf, 1999)).
We describe two approaches to analyzing
the topical information, namely the use of
topically ordered document maps for analyz-
ing the overall topic of dialogue segments, and
identification of topic and focus words in an
utterance for sentence-level analysis and iden-
tification of topically relevant specific infor-
mation in short contexts.
</bodyText>
<listItem confidence="0.360663">
1.1 Document map as a topically
ordered semantic space
</listItem>
<bodyText confidence="0.998070896551724">
The Self-Organizing Map (Kohonen, 1982;
Kohonen, 1995) is an unsupervised neural
network method suitable for ordering and vi-
sualization of complex data sets. It has been
shown that very large document collections
can be meaningfully organized onto maps that
are topically ordered: documents with similar
content are found near each other on the map
(Lin, 1992; Honkela et al., 1996; Lin, 1997;
Kohonen et al., 2000).
The document map can be considered to
form an ordered representation of possible
topics, i.e., a topical semantic space. Each
set of map coordinates specifies a point in the
semantic space, and additionally, corresponds
to a subset of the corpus, forming a kind of
associative topical-semantic memory.
Document maps have been found useful in
text mining and in improving information re-
trieval (Lagus, 2000). Recent experiments in-
dicate that the document maps ordered using
the SOM algorithm can be useful in focusing
the language model to the current active vo-
cabulary (Kurimo and Lagus, 2002).
In this article we examine the usefulness
of document maps for analyzing the topics of
transcripts of natural spoken dialogues. The
topic identification from both individual ut-
terances and longer segments is studied.
</bodyText>
<subsectionHeader confidence="0.7215485">
1.2 Conceptual analysis of individual
utterances
</subsectionHeader>
<bodyText confidence="0.996360051094891">
Within a single utterance or sentence the
speaker may provide several details that spec-
ify the request further or provide additional
information that specifies something said ear-
lier. Automatic extraction of the relevant
words and the concepts they relate to may be
useful, e.g., for a system filling out the fields
of a database query intended to answer the
user’s request.
If a small set of relevant semantic concepts
can be defined, and if the sentence structures
allowed are strictly limited, the semantic con-
cept identification problem can be solved, at
least to some degree, by manually designed
rule-based systems (Jokinen et al., 2002).
However, if the goal is the analysis of free-
form dialogues, one cannot count on hearing
full sentences. It is therefore important to try
to formulate the task as a learning problem
into which adaptive, statistical methods can
be applied.
The major challenge in adaptive language
modeling is the complexity of the learning
problem, caused by large vocabularies and
large amount of variation in sentence struc-
tures, compared to the amount of learning
data available. For English there already exist
various tagged and analyzed corpora. In con-
trast, for many smaller languages no tagged
corpora generally exist. Yet the methods that
are developed for English cannot as such be
applied for many other languages, such as
Finnish.
In the analysis of natural language di-
alogues, theories of information structure
(Sgall et al., 1986; Halliday, 1985) concern the
semantic concepts and their structural prop-
erties within an utterance. Such concepts in-
clude the attitudes, prior knowledge, beliefs
and intentions of the speaker, as well as con-
cepts identifying information that is shared
between the speakers. The terms ’topic’ and
’focus’ may be defined as follows: ’topic’ is
the general subject of which the user is talk-
ing about, and ’focus’ refers to the specific
additional information that the user now in-
troduces about the topic. An alternative way
of describing these terms is that ’topic’ con-
stitutes of the old information shared by both
dialogue participants and ’focus’ contains the
new information which is communicated re-
garding the topic.
A traditional way of finding the old and
new information is the ’question test’ (see
(Vilkuna, 1989) about using it for Finnish).
For any declarative sentence, a question is
composed so that the sentence would be a nat-
ural answer to that question. Then the items
of the sentence that are repeated in the ques-
tion belong to the topic and the new items to
the focus.
A usual approach for topic–focus identi-
fication is to use parsed data. The sen-
tence, or it’s semantic or syntactic-semantic
representation, is divided into two segments,
usually at the location of the main verb,
and the words or semantical concepts in
the first segment are regarded as ’topic’
words/concepts and those in the second as ’fo-
cus’ words/concepts. For example in (Meteer
and Iyer, 1996), the division point is placed
before the first strong verb, or, in the absence
of such a verb, behind the last weak verb of
the sentence. Similar division is also the start-
ing point for the algorithm for topic–focus
identification introduced in (Hajiˆcov´a et al.,
1995). The initial division is then modified
according to the verb’s position and meaning,
the subject’s definiteness or indefiniteness and
the number, type and order of the other sen-
tence constituents.
In language modeling for speech recogni-
tion improvements in perplexity and word er-
ror rate have been observed on English cor-
pora when using language models trained sep-
arately for the topic and the focus part of the
sentence (Meteer and Iyer, 1996; Ma et al.,
1998). Identification of these concepts is likely
to be important also for sentence comprehen-
sion and dialogue strategy selection.
In this article we examine the application of
a number of statistical approaches for identifi-
cation of these concepts. In particular, we ap-
ply the notions of topic and focus in informa-
tion structure (Sgall et al., 1986) to tagging a
set of natural dialogues in Finnish. We then
try several approaches for learning to identify
the occurrences of these concepts from new
data based on the statistical properties of the
old instances.
2 Experiments on recognizing the
dialogue topic of a dialogue turn
The ordered document map can be utilized
in the analysis of dialogue topics as follows:
encode a dialogue turn, i.e., an utterance u
(or an utterance combined with its recent his-
tory) as a document vector. Locate the best-
matching map unit, or several such units. Uti-
lize the identities of the best units as a seman-
tic representation of the topic of the u. In ef-
fect, this is a latent semantic representation
of the topical content of the utterance. Eval-
uation of such a latent representation directly
amounts to asking whether the dialogue man-
ager can benefit from the representation, and
must therefore be carried out by the dialogue
manager. This direct evaluation has not yet
been done.
Instead, we have utilized the following ap-
proach for evaluating the ordering of the maps
and the generalization to new, unseen dia-
logues: An intermediate set of named seman-
tic concepts has been defined in an attempt
to approximate what is considered to be inter-
esting for the dialogue manager. The latent
semantic representation of the map is then la-
beled or calibrated to reflect these named con-
cepts. In effect, each dialogue segment is cat-
egorized to a prior topical category. The or-
ganized map is labeled using part of the data
(’training data’), and the remaining part is
used to evaluate the map (’test data’)&apos;.
&apos;Note that even in this case the map is ordered in
Furthermore, a statistical model for docu-
ment classification can be defined on top of
the map. The probability model used for
topic estimation is
</bodyText>
<equation confidence="0.999679">
P(Ai|S) = P(XN|S)P(Ai|XN), (1)
</equation>
<bodyText confidence="0.999778545454545">
where Ai is the topic category, S denotes the
text transcription of the spoken sentence and
XN is the set of N best map vectors used for
the classification. We approximate the proba-
bility P(XN|S) to be equal for each map vec-
tor in XN. We assume that XN conveys all in-
formation about S. The terms P(Ai|XN) are
calculated as the relative frequencies of the
topics of the document vectors in the train-
ing data that were mapped to the nodes that
correspond to XN.
</bodyText>
<subsectionHeader confidence="0.462972">
2.1 Corpus: transcripts of 57 spoken
dialogues
</subsectionHeader>
<bodyText confidence="0.99991985">
The data used in the experiments were
Finnish dialogues, recorded from the cus-
tomer service phone line of Helsinki City
Transport. The dialogues, provided by the
Interact project (Jokinen et al., 2002), had
been transcribed into text by a person listen-
ing to the tapes.
The transcribed data is extremely collo-
quial. Both the customers and the customer
service personnel use a lot of expletive words,
such as ’nii’ (’so’, ’yea’) and ’tota’ (’hum’,
’er’, ’like’), often the words appear in re-
duced or otherwise non-standard forms. The
word order does not always follow grammat-
ical rules and quite frequently there is con-
siderable overlap between the dialogue turns.
For example, the utterance of speaker A may
be interjected by a confirmation from speaker
B. This had currently been transcribed as
three separate utterances: A1 B A2.
</bodyText>
<subsectionHeader confidence="0.9262595">
2.2 Tagging and segmentation of
dialogues
</subsectionHeader>
<bodyText confidence="0.9942324">
The data set was split into training and test
data so that the first 33 dialogues were used
for organization and calibration of the map
an unsupervised manner, although it is applied for the
classification of new instances based on old ones.
</bodyText>
<tableCaption confidence="0.958013">
Table 1: Proportions of customer utterances
in each topic category in the data sets.
</tableCaption>
<table confidence="0.998452666666667">
Training data Test data
Beginnings 0.08 0.11
Endings 0.12 0.14
Timetables 0.49 0.59
Tickets 0.16 0.11
OOD 0.15 0.06
</table>
<bodyText confidence="0.999376678571428">
and the 24 dialogues collected later for test-
ing.
A small number of broad topic categories
were selected so that they comprehensively
encompass the subjects of discussion occur-
ring in the data. The categories were ’timeta-
bles’, ’beginnings’, ’tickets’, ’endings’, and
’out of domain’.
The dialogues were then manually tagged
and segmented, so that each continuous dia-
logue segment of several utterances that be-
longed to one general topic category formed
a single document. This resulted in a total of
196 segments, 115 and 81 in training and test
sets, respectively. Each segment contained
data from both the customer and the assis-
tant.
Of particular interest is the analysis of the
topics of individual customer utterances. The
data was therefore split further into utter-
ances, resulting in 450 and 189 customer ut-
terances in the training and test set, respec-
tively. The relative frequencies of utterances
belonging to each topic category for both
training and test data are shown in Table 1.
Each individual utterance was labeled with
the topic category of the segment it belonged
to.
</bodyText>
<subsectionHeader confidence="0.993491">
2.3 Creation of the document map
</subsectionHeader>
<bodyText confidence="0.99629432">
The documents, whether segments or utter-
ances, were encoded as vectors using the
methods described in detail in (Kohonen et
al., 2000). In short, the encoding was as fol-
lows. Stopwords (function words etc.) and
words that appeared fewer than 2 times in the
training data were removed. The remaining
words were weighted using their entropy over
document classes. The documents were en-
coded using the vector space model by Salton
(Salton et al., 1975) with word weights. Fur-
thermore, sparse random projection of was
applied to reduce the dimensionality of the
document vectors from the original 1738 to
500 (for details of the method, see, e.g., (Ko-
honen et al., 2000)).
In organizing the map each longer dia-
logue segment was considered as a document.
The use of longer segments is likely to make
the organization of the map more robust.
The inclusion of the utterances by the assis-
tant is particularly important given the small
amount of data—all information must be uti-
lized. The document vectors were then orga-
nized on a SOM of 6 × 4 = 24 units.
</bodyText>
<subsectionHeader confidence="0.898186">
2.4 Experiments and results
</subsectionHeader>
<bodyText confidence="0.994498348484849">
We carried out three tests where the length
of dialogue segments was varied. In each
case, different values of N were tried. In
the first case, longer dialogue segments in the
training data were used to estimate the term
P(Ai|XN) whereas recognition accuracy was
calculated on customer utterances only. Next,
individual customer utterances were used also
in estimating the model term. The best recog-
nition accuracy in both cases were obtained
using the value N = 3, namely 60.3% for
the first case and 65.1% for the second case.
In the third case we used the longer dia-
logue segments both for estimating the model
and for evaluation, to examine the effect of
longer context on the recognition accuracy.
The recognition accuracy was now 87.7%, i.e.,
clearly better for the longer dialogue segments
than for the utterances.
It seems that many utterances taken out of
context are too short or nondescript to pro-
vide reliable cues regarding the topical cat-
egory. An example of such an utterance is
’Onks sinne mit¨a¨a muuta?’ (lit. ’Is to there
anything else?’, the intended meaning prob-
ably being ’Does any other bus go there?’).
In this case it is the surrounding dialogue (or
perhaps the Finnish morpheme corresponding
to ’to’) that would identify the correct cate-
gory, namely ’timetables’.
Moreover, results on comparing a docu-
ment map to Independent Component Analy-
sis on the same corpus are reported in (Bing-
ham et al., 2002). The slightly higher per-
centages in that paper are due to evaluating
longer segments and to reporting the results
on the whole data set instead of a separate
test set.
3 Identification of old and new
information in utterances
We define this task as the identification of
’topic words’ and ’focus words’ from utter-
ances of natural Finnish dialogues. There
are thus no restrictions regarding the vocabu-
lary or the grammar. By observing previous,
marked instances of these concepts we try to
recognize the instances in new dialogues. It
should be noted that this task definition dif-
fers somewhat from those discussed in Sec-
tion 1.2 in that we do not construct any con-
ceptual representation of the utterances, nor
do we segment them into a ’topic’ part and
a ’focus’ part. This choice is due to utiliz-
ing natural utterances in which the sentence
borders do not always coincide with the turn-
taking of the speakers—a turn may consist of
several sentences or a partial one (when inter-
rupted by a comment from the other speaker).
In other words, we try to identify the central
words that communicate the topic and focus
in an utterance. We assume that they can ap-
pear in any part of the sentence and between
them there may be other words that are not
relevant to the topic or focus. Whether these
central words form a single topic or focus or
several such concepts is left open.
</bodyText>
<subsectionHeader confidence="0.992708">
3.1 Corpus and tagging
</subsectionHeader>
<bodyText confidence="0.993817772727272">
The corpus used includes the same data as
in section 2 with additional 133 dialogues
collected from the same source. Basically
each dialogue turn was treated as an utter-
ance, with the exception that long turns were
segmented into sentence-like segments, which
were then considered to be utterances2. Ut-
terances consisting of only one word were re-
2Non-textual cues such as silences within turns
could not be considered for segmenting because they
were not marked in the data.
moved from the data. The training data con-
tained 11464 words in 1704 utterances. Of the
words 17 % were tagged as topic, and 28 % as
focus. The test data consisted of 11750 words
in 1415 utterances, with 14 % tagged as topic
and 25 % as focus.
In tagging the topic and focus words in
the corpus, the following definitions were em-
ployed: In interrogative clauses focus consists
of those words that form the exact entity that
is being asked and all the other words that de-
fine the subject are tagged as belonging to the
topic. In declarative sentences that function
as answers words that form the core of the
answer are tagged as ’focus’, and other words
that merely provide context for the specific
answer are tagged as ’topic’. In other declar-
ative sentences ’topics’ are words that define
the subject matter and ’focus’ is applied to
words that communicate what is being said
about the topic. Regardless, the tagging task
was in many cases quite difficult, and the re-
sulting choice of tags often debatable.
As is charasteristic of spoken language, the
data contained a noticeable percentage (35 %)
of elliptic utterances, which didn’t contain
any topic words. Multiple topic constructs,
on the other hand, were quite rare: more than
one topic concept occurred in only 1 % of the
utterances. The pronouns were quite evenly
distributed with regard to position in the ut-
terances: 32 % were in medial and 36 % in
final position3.
</bodyText>
<subsectionHeader confidence="0.998791">
3.2 The probabilistic model
</subsectionHeader>
<bodyText confidence="0.9997655">
The probability of a word belonging to the
class topic, focus or other is modeled as
</bodyText>
<equation confidence="0.997845">
P(Ti|W)P(Ti|S)
P(Ti|W, S) =
</equation>
<bodyText confidence="0.885316666666667">
where W denotes the word, S its position in
an utterance, and Ti E {topic, focus, other}
stands for the class. The model thus assumes
that being a topic or a focus word is depen-
dent on the properties of that particular word
as well as its position in the utterance. Due
3We interpreted ’medial’ to mean the middle third
of the sentence, and ’final’ to be the last third of the
sentence.
</bodyText>
<equation confidence="0.995411">
P(Ti)
, (2)
</equation>
<bodyText confidence="0.999797071428571">
to computational reasons we made the sim-
plifying assumption that these two effects are
independent, i.e., P(W, S) = P(W)P(S).
Maximum likelihood estimates are used for
the terms P(Ti|W) for already seen words.
Moreover, for unseen words we use the aver-
age of the models of words seen only rarely
(once or twice) in the training data.
For the term P(Ti|S) that describes the ef-
fect of the position of a word we use a softmax
model, namely
where the index j identifies the word and xj
is the position of the word j. The functions
qi are defined as simple linear functions
</bodyText>
<equation confidence="0.998693">
qi(xj) = aixj + bi (4)
</equation>
<bodyText confidence="0.99993325">
The parameters ai and bi are estimated from
the training data. For the class T3 (other),
these parameters are set to a constant value
of zero.
</bodyText>
<subsubsectionHeader confidence="0.546921">
3.2.1 ML estimation
</subsubsectionHeader>
<bodyText confidence="0.999975571428571">
When evaluating the rest of the model pa-
rameters we use two methods, first Maximum
Likelihood estimation and then Bayesian vari-
ational analysis.
In ML estimation the cost function is the
log likelihood of the training data D given
the model M, i.e,
</bodyText>
<equation confidence="0.997638166666667">
ln P (D|M) = ln n P (Ti|Sw) (5)
w
E= q1 + E q2 +
w∈T, w∈T2
11 (− ln(1 + eq, + eq2)). (6)
w
</equation>
<bodyText confidence="0.999926285714286">
The logarithmic term is approximated by a
Taylor series of first degree and the parame-
ters can then be solved as usual, by setting the
partial derivatives of lnP(D|M) to zero with
regard to each parameter. The parameters bi
can be solved analytically and the parameters
ai are solved using Newton iteration.
</bodyText>
<subsubsectionHeader confidence="0.593814">
3.2.2 Bayesian estimation
</subsubsectionHeader>
<bodyText confidence="0.999973588235294">
The ML estimation is known to be prone
to overlearning the properties of the train-
ing data. In contrast, in the Bayesian ap-
proach, also the model cost is included in the
cost function and can be used to avoid over-
learning. For comparison, we thus tried also
the Bayesian approach utilizing the software
and methodology introduced in (Valpola et
al., 2001). The method is based on variational
analysis and uses ensemble learning for esti-
mating the model parameters. The method-
ology and the software allows for the opti-
mization of the model structure with roughly
linear computational complexity without the
risk of over-fitting the model. However, in
these experiments the model structure was
not optimized.
</bodyText>
<subsectionHeader confidence="0.798494">
3.2.3 Disregarding position
information
</subsectionHeader>
<bodyText confidence="0.9999374">
Furthermore, to study the importance of
the position information, we calculated the
probabilities using only ML estimates for
P(T|W), i.e., disregarding the position of the
word.
</bodyText>
<subsectionHeader confidence="0.565217">
3.2.4 Tfxidf
</subsectionHeader>
<bodyText confidence="0.999975428571429">
As a comparison, we applied the tfxidf
weighting scheme, which is commonly used
in information retrieval for weighting content
words. This method does not benefit from the
labeling of the training data. For this reason,
it does not differentiate between ’topic’ and
’focus’ words.
</bodyText>
<subsectionHeader confidence="0.997338">
3.3 Experiments and results
</subsectionHeader>
<bodyText confidence="0.999989307692308">
The following experiment was performed us-
ing each described method: For each utter-
ance in the test data, n words were tagged
as topic, and likewise for the focus category.
Further, n was varied from 1 to 8 to produce
the results depicted in Figure 1.
As can be seen, the Bayesian variational
analysis and the maximum likelihood estima-
tion produce nearly identical performances.
This is perhaps due to the use of very smooth
model family, namely first-order polynomials,
for taking into account the effect of the posi-
tion of the word. For this reason, overlearn-
</bodyText>
<equation confidence="0.64923675">
eqi(xj)
P (Ti|Sj) =
qi(xj), (3)
di e
</equation>
<figure confidence="0.993960666666667">
Precision
Topics
Focuses
0.7
0.6
0.5
Precision
0.4
0.3
0.2
0.1
00 0.5 1
Recall
0.3
0.2
0.1
00 0.5 1
Recall
ML
Bayes
No pos. inf.
Idf
Random
0.7
0.6
0.5
0.4
</figure>
<figureCaption confidence="0.900391">
Figure 1: The precision–recall curves for
topic–focus estimation. (ML = maximum
</figureCaption>
<bodyText confidence="0.971979375">
likelihood, Bayes = Bayesian variational anal-
ysis, No pos. inf. = without position informa-
tion, Idf = tfxidf weighting, Random = the
average precision with random selection.)
ing is not problem even for the ML estima-
tion. However, since the nearly identical re-
sults were obtained using two completely dif-
ferent implementations of quite similar meth-
ods, this can be considered as a validation
experiment on either implementation and op-
timization method. In total, it seems that the
full statistical model designed works rather
well especially in focus identification.
When compared to the full model, disre-
garding position information altoghether re-
sults in inferior performance. The difference
is statistically significant (p G 0.05) in focus
identification for all values of n and in topic
identification for small values of n. More-
over, the performance of the tfxidf scheme
is clearly inferior in either task. However, it
seems that the tfxidf definition of word im-
portance corresponds more closely with the
definition of ’focus’ than that of ’topic’.
</bodyText>
<sectionHeader confidence="0.973673" genericHeader="discussions">
4 Discussion and conclusions
</sectionHeader>
<bodyText confidence="0.9999789">
We examined two different viewpoints for the
topic identification problem in natural lan-
guage understanding. In experiments utiliz-
ing document maps it was found that longer
dialogue segments are reliably modeled, but
especially for short segments the history of
the utterance must be consulted. A perhaps
more interesting idea would be to also look at
morphological features, such as cases, and in-
clude them in the encoding of the utterances.
We plan to study this possibility in further
work.
In the second viewpoint, individual utter-
ances were analyzed to automatically iden-
tify ’topics’ (what the user is talking about)
and ’focuses’ (what is being said about the
topic). Each word in an utterance was labeled
as ’topic’, ’focus’ or ’other’.
A statistical model that utilized the iden-
tity of the word and its position in the ut-
terance was found to be rather successful, es-
pecially for identification of words belonging
to the ’focus’ category. Without the position
information significantly lower performance
was observed, which indicates that position
information is indeed relevant for the iden-
tification. In this case, the Bayesian mod-
eling paradigm and the maximum likelihood
estimation produced nearly identical perfor-
mance. However, this is not the case in gen-
eral, when less smooth model families and op-
timization of model structure are applied. In
the future we plan to examine other kinds of
model structures for this task, perhaps inte-
grating new types of information sources re-
garding the words, as well. For example, it
would be interesting to see whether the ad-
dition of prosodic information would provide
additional cues to improved solving of this
task.
</bodyText>
<sectionHeader confidence="0.999401" genericHeader="acknowledgments">
5 Acknowledgements
</sectionHeader>
<bodyText confidence="0.99982475">
We thank Harri Valpola for his valuable ad-
vice concerning the estimation of the topic-
focus identification model and for the possi-
bility to apply the Bayesian software package
developed by his group.
This work is part of the collaborative ’Inter-
act’ project on natural language interaction in
Finnish.
</bodyText>
<sectionHeader confidence="0.99225" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999807639534884">
Ella Bingham, Jukka Kuusisto, and Krista Lagus.
2002. Ica and som in text document analy-
sis. In The 25th ACM SIGIR Conference on
Research and Development in Information Re-
trieval,August 11-15, 2002, Tampere, Finland.
Submitted.
Eva Hajiˆcov´a, Petr Sgall, and Hana Skoumalov´a.
1995. An automatic procedure for topic-
focus identification. Computational Linguis-
tics, 21(1):81–94.
M. A. Halliday. 1985. Introduction to Functional
Grammar. Oxford University Press, Oxford,
UK.
Timo Honkela, Samuel Kaski, Krista Lagus, and
Teuvo Kohonen. 1996. Newsgroup exploration
with WEBSOM method and browsing inter-
face. Technical Report A32, Helsinki University
of Technology, Laboratory of Computer and In-
formation Science, Espoo, Finland.
R.M. Iyer and M. Ostendorf. 1999. Modelling
long distance dependencies in language: Topic
mixtures versus dynamic cache model. IEEE
Trans. Speech and Audio Processing, 7.
Kristiina Jokinen, Antti Kerminen, Mauri
Kaipainen, Tommi Jauhiainen, Markku Tu-
runen, Jaakko Hakulinen, Jukka Kuusisto, and
Krista Lagus. 2002. Adaptive dialogue systems
— interaction with interact. In 3rd SIGdial
Workshop on Discourse and Dialogue, July 11
and 12, 2002. To appear.
Teuvo Kohonen, Samuel Kaski, Krista Lagus,
Jarkko Salojrvi, Vesa Paatero, and Antti
Saarela. 2000. Organization of a massive
document collection. IEEE Transactions on
Neural Networks, Special Issue on Neural Net-
works for Data Mining and Knowledge Discov-
ery, 11(3):574–585.
Teuvo Kohonen. 1982. Analysis of a simple
self-organizing process. Biological Cybernetics,
44(2):135–140.
Teuvo Kohonen. 1995. Self-Organizing Maps.
3rd, extended edition, 2001. Springer, Berlin.
Mikko Kurimo and Krista Lagus. 2002. An
efficiently focusing large vocabulary language
model. In International Conference on Arti-
ficial Neural Networks, ICANN’02. To appear.
Krista Lagus. 2000. Text mining with the WEB-
SOM. Acta Polytechnica Scandinavica, Mathe-
matics and Computing Series No. 110, 54 pp.
December. D.Sc(Tech) Thesis, Helsinki Univer-
sity of Technology, Finland.
Xia Lin. 1992. Visualization for the document
space. In Proceedings of Visualization ’92,
pages 274–81, Los Alamitos, CA, USA. Cen-
ter for Comput. Legal Res., Pace Univ., White
Plains, NY, USA, IEEE Comput. Soc. Press.
Xia Lin. 1997. Map displays for information re-
trieval. Journal of the American Society for
Information Science, 48:40–54.
Kristine Ma, George Zavaliagkos, and Marie
Meteer. 1998. Sub-sentence discourse models
for conversational speech recognition. In Pro-
ceedings of the 1998 IEEE International Con-
ference on Acoustics, Speech and Signal Pro-
cessing, vol. 2, Seattle, Washington, USA.
Marie Meteer and Rukmini Iyer. 1996. Model-
ing conversational speech for speech recogni-
tion. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Process-
ing, Philadelphia, PA, USA.
G. Salton, A. Wong, and C. S. Yang. 1975. A vec-
tor space model for automatic indexing. Com-
munications of the ACM, 18(11):613–620.
Petr Sgall, Eva Hajiˆcov´a, and Jarmila Panevov´a.
1986. The Meaning of the Sentence in Its Se-
mantic and Pragmatic Aspects. D. Reidel Pub-
lishing Company, Dordrecht, Holland.
Harri Valpola, Tapani Raiko, and Juha Karhunen.
2001. Building blocks for hierarchical latent
variable models. In In Proceedings of the 3rd
International Conference on Independent Com-
ponent Analysis and Blind Signal Separation,
San Diego, California, USA.
Maria Vilkuna. 1989. Free Word Order in
Finnish. Its Syntax and discourse functions.
Suomalaisen Kirjallisuuden Seura, Helsinki.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.264876">
<note confidence="0.5303005">Proceedings of the Third SIGdial Workshop on Discourse and Dialogue, Philadelphia, July 2002, pp. 95-102. Association for Computational Linguistics.</note>
<title confidence="0.9483875">Topic Identification In Natural Language Dialogues Using Neural Networks</title>
<author confidence="0.926764">Krista Lagus</author>
<author confidence="0.926764">Jukka</author>
<affiliation confidence="0.824626">Neural Networks Research Centre, Helsinki University of</affiliation>
<address confidence="0.657639">P.O.Box 9800, FIN-02015 HUT,</address>
<email confidence="0.897258">krista.lagus@hut.fi</email>
<abstract confidence="0.99953278125">In human–computer interaction systems using natural language, the recognition of the topic from user’s utterances is an important task. We examine two different perspectives to the problem of topic analysis needed for carrying out a successful dialogue. First, we apply selforganized document maps for modeling the broader subject of discourse based on the occurrence of content words in the dialogue context. On a Finnish corpus of 57 dialogues the method is shown to work well for recognizing subjects of longer dialogue segments, whereas for individual utterances the subject recognition history should perhaps be taken into account. Second, we attempt to identify topically relevant words in the utterances and thus locate the old information (’topic words’) and new information (’focus words’). For this we define a probabilistic model and compare different methods for model parameter estimation on a corpus of 189 dialogues. Moreover, the utilization of information regarding the position of the word in the utterance is found to improve the results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ella Bingham</author>
<author>Jukka Kuusisto</author>
<author>Krista Lagus</author>
</authors>
<title>Ica and som in text document analysis.</title>
<date>2002</date>
<booktitle>In The 25th ACM SIGIR Conference on Research and Development in Information Retrieval,August 11-15,</booktitle>
<location>Tampere, Finland. Submitted.</location>
<contexts>
<context position="14913" citStr="Bingham et al., 2002" startWordPosition="2454" endWordPosition="2458">the utterances. It seems that many utterances taken out of context are too short or nondescript to provide reliable cues regarding the topical category. An example of such an utterance is ’Onks sinne mit¨a¨a muuta?’ (lit. ’Is to there anything else?’, the intended meaning probably being ’Does any other bus go there?’). In this case it is the surrounding dialogue (or perhaps the Finnish morpheme corresponding to ’to’) that would identify the correct category, namely ’timetables’. Moreover, results on comparing a document map to Independent Component Analysis on the same corpus are reported in (Bingham et al., 2002). The slightly higher percentages in that paper are due to evaluating longer segments and to reporting the results on the whole data set instead of a separate test set. 3 Identification of old and new information in utterances We define this task as the identification of ’topic words’ and ’focus words’ from utterances of natural Finnish dialogues. There are thus no restrictions regarding the vocabulary or the grammar. By observing previous, marked instances of these concepts we try to recognize the instances in new dialogues. It should be noted that this task definition differs somewhat from t</context>
</contexts>
<marker>Bingham, Kuusisto, Lagus, 2002</marker>
<rawString>Ella Bingham, Jukka Kuusisto, and Krista Lagus. 2002. Ica and som in text document analysis. In The 25th ACM SIGIR Conference on Research and Development in Information Retrieval,August 11-15, 2002, Tampere, Finland. Submitted.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva Hajiˆcov´a</author>
<author>Petr Sgall</author>
<author>Hana Skoumalov´a</author>
</authors>
<title>An automatic procedure for topicfocus identification.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>1</issue>
<marker>Hajiˆcov´a, Sgall, Skoumalov´a, 1995</marker>
<rawString>Eva Hajiˆcov´a, Petr Sgall, and Hana Skoumalov´a. 1995. An automatic procedure for topicfocus identification. Computational Linguistics, 21(1):81–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Halliday</author>
</authors>
<title>Introduction to Functional Grammar.</title>
<date>1985</date>
<publisher>Oxford University Press,</publisher>
<location>Oxford, UK.</location>
<contexts>
<context position="5107" citStr="Halliday, 1985" startWordPosition="801" endWordPosition="802">e applied. The major challenge in adaptive language modeling is the complexity of the learning problem, caused by large vocabularies and large amount of variation in sentence structures, compared to the amount of learning data available. For English there already exist various tagged and analyzed corpora. In contrast, for many smaller languages no tagged corpora generally exist. Yet the methods that are developed for English cannot as such be applied for many other languages, such as Finnish. In the analysis of natural language dialogues, theories of information structure (Sgall et al., 1986; Halliday, 1985) concern the semantic concepts and their structural properties within an utterance. Such concepts include the attitudes, prior knowledge, beliefs and intentions of the speaker, as well as concepts identifying information that is shared between the speakers. The terms ’topic’ and ’focus’ may be defined as follows: ’topic’ is the general subject of which the user is talking about, and ’focus’ refers to the specific additional information that the user now introduces about the topic. An alternative way of describing these terms is that ’topic’ constitutes of the old information shared by both dia</context>
</contexts>
<marker>Halliday, 1985</marker>
<rawString>M. A. Halliday. 1985. Introduction to Functional Grammar. Oxford University Press, Oxford, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timo Honkela</author>
<author>Samuel Kaski</author>
<author>Krista Lagus</author>
<author>Teuvo Kohonen</author>
</authors>
<title>Newsgroup exploration with WEBSOM method and browsing interface.</title>
<date>1996</date>
<tech>Technical Report A32,</tech>
<institution>Helsinki University of Technology, Laboratory of Computer and Information Science,</institution>
<location>Espoo, Finland.</location>
<contexts>
<context position="2723" citStr="Honkela et al., 1996" startWordPosition="420" endWordPosition="423">ogue segments, and identification of topic and focus words in an utterance for sentence-level analysis and identification of topically relevant specific information in short contexts. 1.1 Document map as a topically ordered semantic space The Self-Organizing Map (Kohonen, 1982; Kohonen, 1995) is an unsupervised neural network method suitable for ordering and visualization of complex data sets. It has been shown that very large document collections can be meaningfully organized onto maps that are topically ordered: documents with similar content are found near each other on the map (Lin, 1992; Honkela et al., 1996; Lin, 1997; Kohonen et al., 2000). The document map can be considered to form an ordered representation of possible topics, i.e., a topical semantic space. Each set of map coordinates specifies a point in the semantic space, and additionally, corresponds to a subset of the corpus, forming a kind of associative topical-semantic memory. Document maps have been found useful in text mining and in improving information retrieval (Lagus, 2000). Recent experiments indicate that the document maps ordered using the SOM algorithm can be useful in focusing the language model to the current active vocabu</context>
</contexts>
<marker>Honkela, Kaski, Lagus, Kohonen, 1996</marker>
<rawString>Timo Honkela, Samuel Kaski, Krista Lagus, and Teuvo Kohonen. 1996. Newsgroup exploration with WEBSOM method and browsing interface. Technical Report A32, Helsinki University of Technology, Laboratory of Computer and Information Science, Espoo, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Iyer</author>
<author>M Ostendorf</author>
</authors>
<title>Modelling long distance dependencies in language: Topic mixtures versus dynamic cache model.</title>
<date>1999</date>
<journal>IEEE Trans. Speech and Audio Processing,</journal>
<volume>7</volume>
<contexts>
<context position="1946" citStr="Iyer and Ostendorf, 1999" startWordPosition="300" endWordPosition="303">reover, the utilization of information regarding the position of the word in the utterance is found to improve the results. 1 Introduction The analysis of the topic of a sentence or a document is an important task for many natural language applications. For example, in interactive dialogue systems that attempt to carry out and answer requests made by customers, the response strategy employed may depend on the topic of the request (Jokinen et al., 2002). In large vocabulary speech recognition knowledge of the topic can, in general, be utilized for adjusting the language model used (see, e.g., (Iyer and Ostendorf, 1999)). We describe two approaches to analyzing the topical information, namely the use of topically ordered document maps for analyzing the overall topic of dialogue segments, and identification of topic and focus words in an utterance for sentence-level analysis and identification of topically relevant specific information in short contexts. 1.1 Document map as a topically ordered semantic space The Self-Organizing Map (Kohonen, 1982; Kohonen, 1995) is an unsupervised neural network method suitable for ordering and visualization of complex data sets. It has been shown that very large document col</context>
</contexts>
<marker>Iyer, Ostendorf, 1999</marker>
<rawString>R.M. Iyer and M. Ostendorf. 1999. Modelling long distance dependencies in language: Topic mixtures versus dynamic cache model. IEEE Trans. Speech and Audio Processing, 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristiina Jokinen</author>
</authors>
<title>Antti Kerminen, Mauri Kaipainen, Tommi Jauhiainen, Markku Turunen, Jaakko Hakulinen, Jukka Kuusisto, and Krista Lagus.</title>
<date>2002</date>
<booktitle>In 3rd SIGdial Workshop on Discourse and Dialogue,</booktitle>
<volume>11</volume>
<note>To appear.</note>
<marker>Jokinen, 2002</marker>
<rawString>Kristiina Jokinen, Antti Kerminen, Mauri Kaipainen, Tommi Jauhiainen, Markku Turunen, Jaakko Hakulinen, Jukka Kuusisto, and Krista Lagus. 2002. Adaptive dialogue systems — interaction with interact. In 3rd SIGdial Workshop on Discourse and Dialogue, July 11 and 12, 2002. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teuvo Kohonen</author>
<author>Samuel Kaski</author>
<author>Krista Lagus</author>
<author>Jarkko Salojrvi</author>
<author>Vesa Paatero</author>
<author>Antti Saarela</author>
</authors>
<title>Organization of a massive document collection.</title>
<date>2000</date>
<journal>IEEE Transactions on Neural Networks, Special Issue on Neural Networks for Data Mining and Knowledge Discovery,</journal>
<volume>11</volume>
<issue>3</issue>
<contexts>
<context position="2757" citStr="Kohonen et al., 2000" startWordPosition="426" endWordPosition="429"> of topic and focus words in an utterance for sentence-level analysis and identification of topically relevant specific information in short contexts. 1.1 Document map as a topically ordered semantic space The Self-Organizing Map (Kohonen, 1982; Kohonen, 1995) is an unsupervised neural network method suitable for ordering and visualization of complex data sets. It has been shown that very large document collections can be meaningfully organized onto maps that are topically ordered: documents with similar content are found near each other on the map (Lin, 1992; Honkela et al., 1996; Lin, 1997; Kohonen et al., 2000). The document map can be considered to form an ordered representation of possible topics, i.e., a topical semantic space. Each set of map coordinates specifies a point in the semantic space, and additionally, corresponds to a subset of the corpus, forming a kind of associative topical-semantic memory. Document maps have been found useful in text mining and in improving information retrieval (Lagus, 2000). Recent experiments indicate that the document maps ordered using the SOM algorithm can be useful in focusing the language model to the current active vocabulary (Kurimo and Lagus, 2002). In </context>
<context position="12545" citStr="Kohonen et al., 2000" startWordPosition="2051" endWordPosition="2054">nt. Of particular interest is the analysis of the topics of individual customer utterances. The data was therefore split further into utterances, resulting in 450 and 189 customer utterances in the training and test set, respectively. The relative frequencies of utterances belonging to each topic category for both training and test data are shown in Table 1. Each individual utterance was labeled with the topic category of the segment it belonged to. 2.3 Creation of the document map The documents, whether segments or utterances, were encoded as vectors using the methods described in detail in (Kohonen et al., 2000). In short, the encoding was as follows. Stopwords (function words etc.) and words that appeared fewer than 2 times in the training data were removed. The remaining words were weighted using their entropy over document classes. The documents were encoded using the vector space model by Salton (Salton et al., 1975) with word weights. Furthermore, sparse random projection of was applied to reduce the dimensionality of the document vectors from the original 1738 to 500 (for details of the method, see, e.g., (Kohonen et al., 2000)). In organizing the map each longer dialogue segment was considered</context>
</contexts>
<marker>Kohonen, Kaski, Lagus, Salojrvi, Paatero, Saarela, 2000</marker>
<rawString>Teuvo Kohonen, Samuel Kaski, Krista Lagus, Jarkko Salojrvi, Vesa Paatero, and Antti Saarela. 2000. Organization of a massive document collection. IEEE Transactions on Neural Networks, Special Issue on Neural Networks for Data Mining and Knowledge Discovery, 11(3):574–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teuvo Kohonen</author>
</authors>
<title>Analysis of a simple self-organizing process.</title>
<date>1982</date>
<journal>Biological Cybernetics,</journal>
<volume>44</volume>
<issue>2</issue>
<contexts>
<context position="2380" citStr="Kohonen, 1982" startWordPosition="367" endWordPosition="368">al., 2002). In large vocabulary speech recognition knowledge of the topic can, in general, be utilized for adjusting the language model used (see, e.g., (Iyer and Ostendorf, 1999)). We describe two approaches to analyzing the topical information, namely the use of topically ordered document maps for analyzing the overall topic of dialogue segments, and identification of topic and focus words in an utterance for sentence-level analysis and identification of topically relevant specific information in short contexts. 1.1 Document map as a topically ordered semantic space The Self-Organizing Map (Kohonen, 1982; Kohonen, 1995) is an unsupervised neural network method suitable for ordering and visualization of complex data sets. It has been shown that very large document collections can be meaningfully organized onto maps that are topically ordered: documents with similar content are found near each other on the map (Lin, 1992; Honkela et al., 1996; Lin, 1997; Kohonen et al., 2000). The document map can be considered to form an ordered representation of possible topics, i.e., a topical semantic space. Each set of map coordinates specifies a point in the semantic space, and additionally, corresponds t</context>
</contexts>
<marker>Kohonen, 1982</marker>
<rawString>Teuvo Kohonen. 1982. Analysis of a simple self-organizing process. Biological Cybernetics, 44(2):135–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teuvo Kohonen</author>
</authors>
<title>Self-Organizing Maps. 3rd, extended edition,</title>
<date>1995</date>
<publisher>Springer,</publisher>
<location>Berlin.</location>
<contexts>
<context position="2396" citStr="Kohonen, 1995" startWordPosition="369" endWordPosition="370">large vocabulary speech recognition knowledge of the topic can, in general, be utilized for adjusting the language model used (see, e.g., (Iyer and Ostendorf, 1999)). We describe two approaches to analyzing the topical information, namely the use of topically ordered document maps for analyzing the overall topic of dialogue segments, and identification of topic and focus words in an utterance for sentence-level analysis and identification of topically relevant specific information in short contexts. 1.1 Document map as a topically ordered semantic space The Self-Organizing Map (Kohonen, 1982; Kohonen, 1995) is an unsupervised neural network method suitable for ordering and visualization of complex data sets. It has been shown that very large document collections can be meaningfully organized onto maps that are topically ordered: documents with similar content are found near each other on the map (Lin, 1992; Honkela et al., 1996; Lin, 1997; Kohonen et al., 2000). The document map can be considered to form an ordered representation of possible topics, i.e., a topical semantic space. Each set of map coordinates specifies a point in the semantic space, and additionally, corresponds to a subset of th</context>
</contexts>
<marker>Kohonen, 1995</marker>
<rawString>Teuvo Kohonen. 1995. Self-Organizing Maps. 3rd, extended edition, 2001. Springer, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikko Kurimo</author>
<author>Krista Lagus</author>
</authors>
<title>An efficiently focusing large vocabulary language model.</title>
<date>2002</date>
<booktitle>In International Conference on Artificial Neural Networks,</booktitle>
<pages>02</pages>
<note>To appear.</note>
<contexts>
<context position="3352" citStr="Kurimo and Lagus, 2002" startWordPosition="521" endWordPosition="524"> 1997; Kohonen et al., 2000). The document map can be considered to form an ordered representation of possible topics, i.e., a topical semantic space. Each set of map coordinates specifies a point in the semantic space, and additionally, corresponds to a subset of the corpus, forming a kind of associative topical-semantic memory. Document maps have been found useful in text mining and in improving information retrieval (Lagus, 2000). Recent experiments indicate that the document maps ordered using the SOM algorithm can be useful in focusing the language model to the current active vocabulary (Kurimo and Lagus, 2002). In this article we examine the usefulness of document maps for analyzing the topics of transcripts of natural spoken dialogues. The topic identification from both individual utterances and longer segments is studied. 1.2 Conceptual analysis of individual utterances Within a single utterance or sentence the speaker may provide several details that specify the request further or provide additional information that specifies something said earlier. Automatic extraction of the relevant words and the concepts they relate to may be useful, e.g., for a system filling out the fields of a database qu</context>
</contexts>
<marker>Kurimo, Lagus, 2002</marker>
<rawString>Mikko Kurimo and Krista Lagus. 2002. An efficiently focusing large vocabulary language model. In International Conference on Artificial Neural Networks, ICANN’02. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Krista Lagus</author>
</authors>
<title>Text mining with the WEBSOM.</title>
<date>2000</date>
<journal>Acta Polytechnica Scandinavica, Mathematics and Computing Series No.</journal>
<tech>D.Sc(Tech) Thesis,</tech>
<volume>110</volume>
<pages>pp.</pages>
<institution>Helsinki University of Technology,</institution>
<contexts>
<context position="3165" citStr="Lagus, 2000" startWordPosition="492" endWordPosition="493">s can be meaningfully organized onto maps that are topically ordered: documents with similar content are found near each other on the map (Lin, 1992; Honkela et al., 1996; Lin, 1997; Kohonen et al., 2000). The document map can be considered to form an ordered representation of possible topics, i.e., a topical semantic space. Each set of map coordinates specifies a point in the semantic space, and additionally, corresponds to a subset of the corpus, forming a kind of associative topical-semantic memory. Document maps have been found useful in text mining and in improving information retrieval (Lagus, 2000). Recent experiments indicate that the document maps ordered using the SOM algorithm can be useful in focusing the language model to the current active vocabulary (Kurimo and Lagus, 2002). In this article we examine the usefulness of document maps for analyzing the topics of transcripts of natural spoken dialogues. The topic identification from both individual utterances and longer segments is studied. 1.2 Conceptual analysis of individual utterances Within a single utterance or sentence the speaker may provide several details that specify the request further or provide additional information </context>
</contexts>
<marker>Lagus, 2000</marker>
<rawString>Krista Lagus. 2000. Text mining with the WEBSOM. Acta Polytechnica Scandinavica, Mathematics and Computing Series No. 110, 54 pp. December. D.Sc(Tech) Thesis, Helsinki University of Technology, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xia Lin</author>
</authors>
<title>Visualization for the document space.</title>
<date>1992</date>
<journal>IEEE Comput.</journal>
<booktitle>In Proceedings of Visualization ’92,</booktitle>
<pages>274--81</pages>
<publisher>Soc. Press.</publisher>
<institution>USA. Center for Comput. Legal Res., Pace Univ.,</institution>
<location>Los Alamitos, CA,</location>
<contexts>
<context position="2701" citStr="Lin, 1992" startWordPosition="418" endWordPosition="419">pic of dialogue segments, and identification of topic and focus words in an utterance for sentence-level analysis and identification of topically relevant specific information in short contexts. 1.1 Document map as a topically ordered semantic space The Self-Organizing Map (Kohonen, 1982; Kohonen, 1995) is an unsupervised neural network method suitable for ordering and visualization of complex data sets. It has been shown that very large document collections can be meaningfully organized onto maps that are topically ordered: documents with similar content are found near each other on the map (Lin, 1992; Honkela et al., 1996; Lin, 1997; Kohonen et al., 2000). The document map can be considered to form an ordered representation of possible topics, i.e., a topical semantic space. Each set of map coordinates specifies a point in the semantic space, and additionally, corresponds to a subset of the corpus, forming a kind of associative topical-semantic memory. Document maps have been found useful in text mining and in improving information retrieval (Lagus, 2000). Recent experiments indicate that the document maps ordered using the SOM algorithm can be useful in focusing the language model to the</context>
</contexts>
<marker>Lin, 1992</marker>
<rawString>Xia Lin. 1992. Visualization for the document space. In Proceedings of Visualization ’92, pages 274–81, Los Alamitos, CA, USA. Center for Comput. Legal Res., Pace Univ., White Plains, NY, USA, IEEE Comput. Soc. Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xia Lin</author>
</authors>
<title>Map displays for information retrieval.</title>
<date>1997</date>
<journal>Journal of the American Society for Information Science,</journal>
<pages>48--40</pages>
<contexts>
<context position="2734" citStr="Lin, 1997" startWordPosition="424" endWordPosition="425">ntification of topic and focus words in an utterance for sentence-level analysis and identification of topically relevant specific information in short contexts. 1.1 Document map as a topically ordered semantic space The Self-Organizing Map (Kohonen, 1982; Kohonen, 1995) is an unsupervised neural network method suitable for ordering and visualization of complex data sets. It has been shown that very large document collections can be meaningfully organized onto maps that are topically ordered: documents with similar content are found near each other on the map (Lin, 1992; Honkela et al., 1996; Lin, 1997; Kohonen et al., 2000). The document map can be considered to form an ordered representation of possible topics, i.e., a topical semantic space. Each set of map coordinates specifies a point in the semantic space, and additionally, corresponds to a subset of the corpus, forming a kind of associative topical-semantic memory. Document maps have been found useful in text mining and in improving information retrieval (Lagus, 2000). Recent experiments indicate that the document maps ordered using the SOM algorithm can be useful in focusing the language model to the current active vocabulary (Kurim</context>
</contexts>
<marker>Lin, 1997</marker>
<rawString>Xia Lin. 1997. Map displays for information retrieval. Journal of the American Society for Information Science, 48:40–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristine Ma</author>
<author>George Zavaliagkos</author>
<author>Marie Meteer</author>
</authors>
<title>Sub-sentence discourse models for conversational speech recognition.</title>
<date>1998</date>
<booktitle>In Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<volume>2</volume>
<location>Seattle, Washington, USA.</location>
<contexts>
<context position="7306" citStr="Ma et al., 1998" startWordPosition="1167" endWordPosition="1170">verb of the sentence. Similar division is also the starting point for the algorithm for topic–focus identification introduced in (Hajiˆcov´a et al., 1995). The initial division is then modified according to the verb’s position and meaning, the subject’s definiteness or indefiniteness and the number, type and order of the other sentence constituents. In language modeling for speech recognition improvements in perplexity and word error rate have been observed on English corpora when using language models trained separately for the topic and the focus part of the sentence (Meteer and Iyer, 1996; Ma et al., 1998). Identification of these concepts is likely to be important also for sentence comprehension and dialogue strategy selection. In this article we examine the application of a number of statistical approaches for identification of these concepts. In particular, we apply the notions of topic and focus in information structure (Sgall et al., 1986) to tagging a set of natural dialogues in Finnish. We then try several approaches for learning to identify the occurrences of these concepts from new data based on the statistical properties of the old instances. 2 Experiments on recognizing the dialogue </context>
</contexts>
<marker>Ma, Zavaliagkos, Meteer, 1998</marker>
<rawString>Kristine Ma, George Zavaliagkos, and Marie Meteer. 1998. Sub-sentence discourse models for conversational speech recognition. In Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, vol. 2, Seattle, Washington, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie Meteer</author>
<author>Rukmini Iyer</author>
</authors>
<title>Modeling conversational speech for speech recognition.</title>
<date>1996</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="6573" citStr="Meteer and Iyer, 1996" startWordPosition="1044" endWordPosition="1047">larative sentence, a question is composed so that the sentence would be a natural answer to that question. Then the items of the sentence that are repeated in the question belong to the topic and the new items to the focus. A usual approach for topic–focus identification is to use parsed data. The sentence, or it’s semantic or syntactic-semantic representation, is divided into two segments, usually at the location of the main verb, and the words or semantical concepts in the first segment are regarded as ’topic’ words/concepts and those in the second as ’focus’ words/concepts. For example in (Meteer and Iyer, 1996), the division point is placed before the first strong verb, or, in the absence of such a verb, behind the last weak verb of the sentence. Similar division is also the starting point for the algorithm for topic–focus identification introduced in (Hajiˆcov´a et al., 1995). The initial division is then modified according to the verb’s position and meaning, the subject’s definiteness or indefiniteness and the number, type and order of the other sentence constituents. In language modeling for speech recognition improvements in perplexity and word error rate have been observed on English corpora wh</context>
</contexts>
<marker>Meteer, Iyer, 1996</marker>
<rawString>Marie Meteer and Rukmini Iyer. 1996. Modeling conversational speech for speech recognition. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>A Wong</author>
<author>C S Yang</author>
</authors>
<title>A vector space model for automatic indexing.</title>
<date>1975</date>
<journal>Communications of the ACM,</journal>
<volume>18</volume>
<issue>11</issue>
<contexts>
<context position="12860" citStr="Salton et al., 1975" startWordPosition="2104" endWordPosition="2107">training and test data are shown in Table 1. Each individual utterance was labeled with the topic category of the segment it belonged to. 2.3 Creation of the document map The documents, whether segments or utterances, were encoded as vectors using the methods described in detail in (Kohonen et al., 2000). In short, the encoding was as follows. Stopwords (function words etc.) and words that appeared fewer than 2 times in the training data were removed. The remaining words were weighted using their entropy over document classes. The documents were encoded using the vector space model by Salton (Salton et al., 1975) with word weights. Furthermore, sparse random projection of was applied to reduce the dimensionality of the document vectors from the original 1738 to 500 (for details of the method, see, e.g., (Kohonen et al., 2000)). In organizing the map each longer dialogue segment was considered as a document. The use of longer segments is likely to make the organization of the map more robust. The inclusion of the utterances by the assistant is particularly important given the small amount of data—all information must be utilized. The document vectors were then organized on a SOM of 6 × 4 = 24 units. 2.</context>
</contexts>
<marker>Salton, Wong, Yang, 1975</marker>
<rawString>G. Salton, A. Wong, and C. S. Yang. 1975. A vector space model for automatic indexing. Communications of the ACM, 18(11):613–620.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petr Sgall</author>
<author>Eva Hajiˆcov´a</author>
<author>Jarmila Panevov´a</author>
</authors>
<date>1986</date>
<booktitle>The Meaning of the Sentence in Its Semantic and Pragmatic Aspects. D.</booktitle>
<publisher>Reidel Publishing Company,</publisher>
<location>Dordrecht, Holland.</location>
<marker>Sgall, Hajiˆcov´a, Panevov´a, 1986</marker>
<rawString>Petr Sgall, Eva Hajiˆcov´a, and Jarmila Panevov´a. 1986. The Meaning of the Sentence in Its Semantic and Pragmatic Aspects. D. Reidel Publishing Company, Dordrecht, Holland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harri Valpola</author>
<author>Tapani Raiko</author>
<author>Juha Karhunen</author>
</authors>
<title>Building blocks for hierarchical latent variable models. In</title>
<date>2001</date>
<booktitle>In Proceedings of the 3rd International Conference on Independent Component Analysis and Blind Signal Separation,</booktitle>
<location>San Diego, California, USA.</location>
<contexts>
<context position="20535" citStr="Valpola et al., 2001" startWordPosition="3456" endWordPosition="3459">ree and the parameters can then be solved as usual, by setting the partial derivatives of lnP(D|M) to zero with regard to each parameter. The parameters bi can be solved analytically and the parameters ai are solved using Newton iteration. 3.2.2 Bayesian estimation The ML estimation is known to be prone to overlearning the properties of the training data. In contrast, in the Bayesian approach, also the model cost is included in the cost function and can be used to avoid overlearning. For comparison, we thus tried also the Bayesian approach utilizing the software and methodology introduced in (Valpola et al., 2001). The method is based on variational analysis and uses ensemble learning for estimating the model parameters. The methodology and the software allows for the optimization of the model structure with roughly linear computational complexity without the risk of over-fitting the model. However, in these experiments the model structure was not optimized. 3.2.3 Disregarding position information Furthermore, to study the importance of the position information, we calculated the probabilities using only ML estimates for P(T|W), i.e., disregarding the position of the word. 3.2.4 Tfxidf As a comparison,</context>
</contexts>
<marker>Valpola, Raiko, Karhunen, 2001</marker>
<rawString>Harri Valpola, Tapani Raiko, and Juha Karhunen. 2001. Building blocks for hierarchical latent variable models. In In Proceedings of the 3rd International Conference on Independent Component Analysis and Blind Signal Separation, San Diego, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Vilkuna</author>
</authors>
<title>Free Word Order in Finnish. Its Syntax and discourse functions. Suomalaisen Kirjallisuuden Seura,</title>
<date>1989</date>
<location>Helsinki.</location>
<contexts>
<context position="5910" citStr="Vilkuna, 1989" startWordPosition="931" endWordPosition="932">oncepts identifying information that is shared between the speakers. The terms ’topic’ and ’focus’ may be defined as follows: ’topic’ is the general subject of which the user is talking about, and ’focus’ refers to the specific additional information that the user now introduces about the topic. An alternative way of describing these terms is that ’topic’ constitutes of the old information shared by both dialogue participants and ’focus’ contains the new information which is communicated regarding the topic. A traditional way of finding the old and new information is the ’question test’ (see (Vilkuna, 1989) about using it for Finnish). For any declarative sentence, a question is composed so that the sentence would be a natural answer to that question. Then the items of the sentence that are repeated in the question belong to the topic and the new items to the focus. A usual approach for topic–focus identification is to use parsed data. The sentence, or it’s semantic or syntactic-semantic representation, is divided into two segments, usually at the location of the main verb, and the words or semantical concepts in the first segment are regarded as ’topic’ words/concepts and those in the second as</context>
</contexts>
<marker>Vilkuna, 1989</marker>
<rawString>Maria Vilkuna. 1989. Free Word Order in Finnish. Its Syntax and discourse functions. Suomalaisen Kirjallisuuden Seura, Helsinki.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>