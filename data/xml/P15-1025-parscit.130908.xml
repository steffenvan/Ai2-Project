<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002412">
<title confidence="0.9991025">
Learning Continuous Word Embedding with Metadata for Question
Retrieval in Community Question Answering
</title>
<author confidence="0.999167">
Guangyou Zhou1, Tingting He1, Jun Zhao2, and Po Hu1
</author>
<affiliation confidence="0.9802265">
1 School of Computer, Central China Normal University, Wuhan 430079, China
2 National Laboratory of Pattern Recognition, CASIA, Beijing 100190, China
</affiliation>
<email confidence="0.993076">
{gyzhou,tthe,phu}@mail.ccnu.edu.cn jzhao@nlpr.ia.ac.cn
</email>
<sectionHeader confidence="0.993755" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999749565217391">
Community question answering (cQA)
has become an important issue due to the
popularity of cQA archives on the web.
This paper is concerned with the problem
of question retrieval. Question retrieval
in cQA archives aims to find the exist-
ing questions that are semantically equiv-
alent or relevant to the queried questions.
However, the lexical gap problem brings
about new challenge for question retrieval
in cQA. In this paper, we propose to learn
continuous word embeddings with meta-
data of category information within cQA
pages for question retrieval. To deal with
the variable size of word embedding vec-
tors, we employ the framework of fisher
kernel to aggregated them into the fixed-
length vectors. Experimental results on
large-scale real world cQA data set show
that our approach can significantly out-
perform state-of-the-art translation models
and topic-based models for question re-
trieval in cQA.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999974363636364">
Over the past few years, a large amount of user-
generated content have become an important in-
formation resource on the web. These include
the traditional Frequently Asked Questions (FAQ)
archives and the emerging community question
answering (cQA) services, such as Yahoo! An-
swers1, Live QnA2, and Baidu Zhidao3. The con-
tent in these web sites is usually organized as ques-
tions and lists of answers associated with meta-
data like user chosen categories to questions and
askers’ awards to the best answers. This data made
</bodyText>
<footnote confidence="0.999848666666667">
1http://answers.yahoo.com/
2http://qna.live.com/
3http://zhidao.baidu.com/
</footnote>
<bodyText confidence="0.999641878048781">
cQA archives valuable resources for various tasks
like question-answering (Jeon et al., 2005; Xue et
al., 2008) and knowledge mining (Adamic et al.,
2008), etc.
One fundamental task for reusing content in
cQA is finding similar questions for queried ques-
tions, as questions are the keys to accessing the
knowledge in cQA. Then the best answers of
these similar questions will be used to answer the
queried questions. Many studies have been done
along this line (Jeon et al., 2005; Xue et al., 2008;
Duan et al., 2008; Lee et al., 2008; Bernhard and
Gurevych, 2009; Cao et al., 2010; Zhou et al.,
2011; Singh, 2012; Zhang et al., 2014a). One big
challenge for question retrieval in cQA is the lexi-
cal gap between the queried questions and the ex-
isting questions in the archives. Lexical gap means
that the queried questions may contain words that
are different from, but related to, the words in the
existing questions. For example shown in (Zhang
et al., 2014a), we find that for a queried question
“how do I get knots out of my cats fur?”, there
are good answers under an existing question “how
can I remove a tangle in my cat’s fur?” in Yahoo!
Answers. Although the two questions share few
words in common, they have very similar mean-
ings, it is hard for traditional retrieval models (e.g.,
BM25 (Robertson et al., 1994)) to determine their
similarity. This lexical gap has become a major
barricade preventing traditional IR models (e.g.,
BM25) from retrieving similar questions in cQA.
To address the lexical gap problem in cQA, pre-
vious work in the literature can be divided into two
groups. The first group is the translation models,
which leverage the question-answer pairs to learn
the semantically related words to improve tradi-
tional IR models (Jeon et al., 2005; Xue et al.,
2008; Zhou et al., 2011). The basic assumption is
that question-answer pairs are “parallel texts” and
relationship of words (or phrases) can be estab-
lished through word-to-word (or phrase-to-phrase)
</bodyText>
<page confidence="0.94756">
250
</page>
<note confidence="0.976771333333333">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 250–259,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.998796809523809">
translation probabilities (Jeon et al., 2005; Xue
et al., 2008; Zhou et al., 2011). Experimental
results show that translation models obtain state-
of-the-art performance for question retrieval in
cQA. However, questions and answers are far from
“parallel” in practice, questions and answers are
highly asymmetric on the information they con-
tain (Zhang et al., 2014a). The second group is
the topic-based models (Cai et al., 2011; Ji et al.,
2012), which learn the latent topics aligned across
the question-answer pairs to alleviate the lexical
gap problem, with the assumption that a question
and its paired answers share the same topic distri-
bution. However, questions and answers are het-
erogeneous in many aspects, they do not share the
same topic distribution in practice.
Inspired by the recent success of continuous
space word representations in capturing the se-
mantic similarities in various natural language
processing tasks, we propose to incorporate an
embedding of words in a continuous space for
question representations. Due to the ability of
word embeddings, we firstly transform words in
a question into continuous vector representations
by looking up tables. These word embeddings are
learned in advance using a continuous skip-gram
model (Mikolov et al., 2013), or other continuous
word representation learning methods. Once the
words are embedded in a continuous space, one
can view a question as a Bag-of-Embedded-Words
(BoEW). Then, the variable-cardinality BoEW
will be aggregated into a fixed-length vector by
using the Fisher kernel (FK) framework of (Clin-
chant and Perronnin, 2013; Sanchez et al., 2013).
Through the two steps, the proposed approach can
map a question into a length invariable compact
vector, which can be efficiently and effectively for
large-scale question retrieval task in cQA.
We test the proposed approach on large-scale
Yahoo! Answers data and Baidu Zhidao data. Ya-
hoo! Answers and Baidu Zhidao represent the
largest and most popular cQA archives in English
and Chinese, respectively. We conduct both quan-
titative and qualitative evaluations. Experimental
results show that our approach can significantly
outperform state-of-the-art translation models and
topic-based models for question retrieval in cQA.
Our contribution in this paper are three-fold: (1)
we represent a question as a bag-of-embedded-
words (BoEW) in a continuous space; (2) we in-
troduce a novel method to aggregate the variable-
cardinality BoEW into a fixed-length vector by us-
ing the FK. The FK is just one possible way to sub-
sequently transform this bag representation into
a fixed-length vector which is more amenable to
large-scale processing; (3) an empirical verifica-
tion of the efficacy of the proposed framework on
large-scale English and Chinese cQA data.
The rest of this paper is organized as follows.
Section 2 summarizes the related work. Section 3
describes our proposed framework for question re-
trieval. Section 4 reports the experimental results.
Finally, we conclude the paper in Section 5.
</bodyText>
<sectionHeader confidence="0.999919" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.999871">
2.1 Question Retrieval in cQA
</subsectionHeader>
<bodyText confidence="0.999901166666667">
Significant research efforts have been conducted
over the years in attempt to improve question re-
trieval in cQA (Jeon et al., 2005; Xue et al., 2008;
Lee et al., 2008; Duan et al., 2008; Bernhard and
Gurevych, 2009; Cao et al., 2010; Zhou et al.,
2011; Singh, 2012; Zhang et al., 2014a). Most
of these works focus on finding similar questions
for the user queried questions. The major chal-
lenge for question retrieval in cQA is the lexical
gap problem. Jeon et al. (2005) proposed a word-
based translation model for automatically fixing
the lexical gap problem. Xue et al. (2008) pro-
posed a word-based translation language model
for question retrieval. Lee et al. (2008) tried to
further improve the translation probabilities based
on question-answer pairs by selecting the most im-
portant terms to build compact translation mod-
els. Bernhard and Gurevych (2009) proposed to
use as a parallel training data set the definitions
and glosses provided for the same term by differ-
ent lexical semantic resources. In order to improve
the word-based translation model with some con-
textual information, Riezler et al. (2007) and Zhou
et al. (2011) proposed a phrase-based translation
model for question and answer retrieval. The
phrase-based translation model can capture some
contextual information in modeling the transla-
tion of phrases as a whole, thus the more accurate
translations can better improve the retrieval per-
formance. Singh (2012) addressed the lexical gap
issues by extending the lexical word-based trans-
lation model to incorporate semantic information
(entities).
In contrast to the works described above that as-
sume question-answer pairs are “parallel text”, our
paper deals with the lexical gap by learning con-
</bodyText>
<page confidence="0.995299">
251
</page>
<bodyText confidence="0.999903266666667">
tinuous word embeddings in capturing the simi-
larities without any assumptions, which is much
more reasonable in practice.
Besides, some other studies model the semantic
relationship between questions and answers with
deep linguistic analysis (Duan et al., 2008; Wang
et al., 2009; Wang et al., 2010; Ji et al., 2012;
Zhang et al., 2014a) or a learning to rank strat-
egy (Surdeanu et al., 2008; Carmel et al., 2014).
Recently, Cao et al. (2010) and Zhou et al. (2013)
exploited the category metadata within cQA pages
to further improve the performance. On the con-
trary, we focus on the representation learning for
questions, with a different solution with those pre-
vious works.
</bodyText>
<subsectionHeader confidence="0.999435">
2.2 Word Embedding Learning
</subsectionHeader>
<bodyText confidence="0.999929459459459">
Representation of words as continuous vectors has
attracted increasing attention in the area of nat-
ural language processing (NLP). Recently, a se-
ries of works applied deep learning techniques to
learn high-quality word representations. Bengio
et al. (2003) proposed a probabilistic neural net-
work language model (NNLM) for word represen-
tations. Furthermore, Mikolov et al. (2013) pro-
posed efficient neural network models for learn-
ing word representations, including the continu-
ous skip-gram model and the continuous bag-of-
word model (CBOW), both of which are unsu-
pervised models learned from large-scale text cor-
pora. Besides, there are also a large number of
works addressing the task of learning word repre-
sentations (Huang et al., 2012; Maas et al., 2011;
Turian et al., 2010).
Nevertheless, since most the existing works
learned word representations mainly based on
the word co-occurrence information, the obtained
word embeddings cannot capture the relationship
between two syntactically or semantically similar
words if either of them yields very little context in-
formation. On the other hand, even though amount
of context could be noisy or biased such that they
cannot reflect the inherent relationship between
words and further mislead the training process.
Most recently, Yu et al. (2014) used semantic prior
knowledge to improve word representations. Xu
et al. (2014) used the knowledge graph to advance
the learning of word embeddings. In contrast to
all the aforementioned works, in this paper, we
present a general method to leverage the metadata
of category information within cQA pages to fur-
ther improve the word embedding representations.
To our knowledge, it is the first work to learn word
embeddings with metadata on cQA data set.
</bodyText>
<sectionHeader confidence="0.966392" genericHeader="method">
3 Our Approach
</sectionHeader>
<bodyText confidence="0.999889269230769">
In this Section, we describe the proposed ap-
proach: learning continuous word embedding with
metadata for question retrieval in cQA. The pro-
posed framework consists of two steps: (1) word
embedding learning step: given a cQA data collec-
tion, questions are treated as the basic units. For
each word in a question, we firstly transform it to a
continuous word vector through the looking up ta-
bles. Once the word embeddings are learned, each
question is represented by a variable-cardinality
word embedding vector (also called BoEW); (2)
fisher vector generation step: which uses a genera-
tive model in the FK framework to generate fisher
vectors (FVs) by aggregating the BoEWs for all
the questions. Question retrieval can be performed
through calculating the similarity between the FVs
of a queried question and an existing question in
the archive.
From the framework, we can see that although
the word embedding learning computations and
generative model estimation are time consuming,
they can run only once in advance. Meanwhile, the
computational requirements of FV generation and
similarity calculation are limited. Hence, the pro-
posed framework can efficiently achieve the large-
scale question retrieval task.
</bodyText>
<subsectionHeader confidence="0.999655">
3.1 Word Embedding Learning
</subsectionHeader>
<bodyText confidence="0.9999968">
In this paper, we consider a context-aware pre-
dicting model, more specifically, the Skip-gram
model (Mikolov et al., 2013) for learning word
embeddings, since it is much more efficient as well
as memory-saving than other approaches.4 Skip-
gram is recently proposed for learning word rep-
resentations using a neural network model, whose
underlying idea is that similar words should have
similar contexts. In the Skip-gram model (see Fig-
ure 1), a sliding window is employed on the input
text stream to generate the training data, and l in-
dicates the context window size to be 2l + 1. In
each slide window, the model aims to use the cen-
tral word wk as input to predict the context words.
Let Md×N denote the learned embedding matrix,
</bodyText>
<footnote confidence="0.77527125">
4Note that although we use the skip-gram model as an ex-
ample to illustrate our approach, the similar framework can
be developed on the basis of any other word embedding mod-
els.
</footnote>
<page confidence="0.992264">
252
</page>
<figureCaption confidence="0.999962">
Figure 1: The continuous skip-gram model.
</figureCaption>
<bodyText confidence="0.980891">
where N is the vocabulary size and d is the di-
mension of word embeddings. Each column of M
represents the embedding of a word. Let wk is first
mapped to its embedding ewk by selecting the cor-
responding column vector of M. The probability
of its context word wk+j is then computed using a
log-linear softmax function:
</bodyText>
<equation confidence="0.859868571428572">
1
E7w 1 exp(eTwewk)
where θ are the parameters we should learned, k =
1 · · · d, and j E [−l, l]. Then, the log-likelihood
over the entire training data can be computed as:
J(θ) = E logp(wk+j|wk; θ) (2)
(wk,wk+j)
</equation>
<bodyText confidence="0.997987125">
To calculate the prediction errors for back prop-
agation, we need to compute the derivative of
p(wk+j|wk; θ), whose computation cost is pro-
portional to the vocabulary size N. As N is of-
ten very large, it is difficult to directly compute
the derivative. To deal this problem, Mikolov
et al. (2013) proposed a simple negative sam-
pling method, which generates r noise samples
for each input word to estimate the target word,
in which r is a very small number compared with
N. Therefore, the training time yields linear scale
to the number of noise samples and it becomes
independent of the vocabulary size. Suppose the
frequency of word w is u(w), then the proba-
bility of sampling w is usually set to p(w) a
u(w)3/4 (Mikolov et al., 2013).
</bodyText>
<subsectionHeader confidence="0.998954">
3.2 Metadata Powered Model
</subsectionHeader>
<bodyText confidence="0.99983365625">
After briefing the skip-gram model, we introduce
how we equip it with the metadata information.
In cQA sites, there are several metadata, such as
“category”,“voting” and so on. In this paper, we
only consider the metadata of category informa-
tion for word embedding learning. All questions
in cQA are usually organized into a hierarchy of
categories. When an user asks a question, the user
typically required to choose a category label for
the question from a predefined hierarchy of cate-
gories (Cao et al., 2010; Zhou et al., 2013). Pre-
vious work in the literature has demonstrated the
effectiveness of the category information for ques-
tion retrieval (Cao et al., 2010; Zhou et al., 2013).
On the contrary, we argue that the category infor-
mation benefits the word embedding learning in
this work. The basic idea is that category informa-
tion encodes the attributes or properties of words,
from which we can group similar words according
to their categories. Here, a word’s category is as-
signed based on the questions it appeared in. For
example, a question “What are the security issues
with java?” is under the category of “Computers
&amp; Internet —* Security”, we simply put the cate-
gory of a word java as “Computers &amp; Internet —*
Security”. Then, we may require the representa-
tions of words that belong to the same category to
be close to each other.
Let s(wk, wi) be the similarity score between
wk and wi. Under the above assumption, we
use the following heuristic to constrain the simi-
lar scores:
</bodyText>
<equation confidence="0.951986">
s(wk, wi) = { 1 if c(rw)e c(wi) (3)
</equation>
<bodyText confidence="0.9999906">
where c(wk) denotes the category of wk. If the
central word wk shares the same category with the
word wi, their similarity score will become 1, oth-
erwise, we set to 0. Then we encode the category
information using a regularization function Ec:
</bodyText>
<equation confidence="0.754681">
s(wk, wi)d(wk, wi) (4)
</equation>
<bodyText confidence="0.999477363636364">
where d(wk, wi) is the distance for the words in
the embedding space and s(wk, wi) serves as a
weighting function. Again, for simplicity, we de-
fine d(wk, wi) as the Euclidean distance between
wk and wi.
We combine the skip-gram objective function
and the regularization function derived from the
metadata of category information, we get the fol-
lowing combined objective Jc that incorporates
category information into the word representation
learning process:
</bodyText>
<equation confidence="0.98958">
Jc = J(θ) + βEc (5)
</equation>
<bodyText confidence="0.996421">
where β is the combination coefficient. Our goal
is to maximize the combined objective Jc, which
</bodyText>
<figure confidence="0.9956616">
...
...
word embedding of
...
...
</figure>
<equation confidence="0.969074714285714">
p(wk+j|wk; θ) =
exp(eTwk+jewk)
N
i=1
Ec =
N
k=1
</equation>
<page confidence="0.996018">
253
</page>
<figureCaption confidence="0.958988">
Figure 2: The continuous skip-gram model with
metadata of category information, called M-NET.
</figureCaption>
<bodyText confidence="0.999505764705883">
can be optimized using back propagation neural
networks. We call this model as metadata powered
model (see Figure 2), and denote it by M-NET for
easy of reference.
In the implementation, we optimize the regu-
larization function derived from the metadata of
category information along with the training pro-
cess of the skip-gram model. During the pro-
cedure of learning word representations from the
context words in the sliding window, if the central
word wk hits the category information, the cor-
responding optimization process of the metadata
powered regularization function will be activated.
Therefore, we maximize the weighted Euclidean
distance between the representation of the central
word and that of its similar words according to the
objective function in Equation (5).
</bodyText>
<subsectionHeader confidence="0.997515">
3.3 Fisher Vector Generation
</subsectionHeader>
<bodyText confidence="0.9999139">
Once the word embeddings are learned, ques-
tions can be represented by variable length sets
of word embedding vectors, which can be viewed
as BoEWs. Semantic level similarities between
queried questions and the existing questions rep-
resented by BoEWs can be captured more accu-
rately than previous bag-of-words (BoW) meth-
ods. However, since BoEWs are variable-size sets
of word embeddings and most of the index meth-
ods in information retrieval field are not suitable
for this kinds of issues, BoEWs cannot be directly
used for large-scale question retrieval task.
Given a cQA data collection Q = {qi,1 ≤ i ≤
|Q|}, where qi is the ith question and |Q |is the
number of questions in the data collection. The ith
question qi is composed by a sequence of words
wi = {wij,1 ≤ j ≤ Ni}, where Ni denotes the
length of qi. Through looking up table (word em-
bedding matrix) of M, the ith question qi can be
represented by Ewi = {ewij,1 ≤ j ≤ Ni}, where
ewij is the word embedding of wij. According to
the framework of FK (Clinchant and Perronnin,
2013; Sanchez et al., 2013; Zhang et al., 2014b),
questions are modeled by a probability density
function. In this work, we use Gaussian mixture
model (GMM) to do it. We assume that the con-
tinuous word embedding Ewi for question qi have
been generated by a “universal” (e.g., question-
independent) probability density function (pdf).
As is a common practice, we choose this pdf to be
a GMM since any continuous distribution can be
approximated with arbitrary precision by a mix-
ture of Gaussian. In what follows, the pdf is de-
noted uλ where λ = {θi, µi, Σi, i = 1 · · · K}
is the set of parameters of the GMM. θi, µi and
Σi denote respectively the mixture weight, mean
vector and covariance matrix of Gaussian i. For
computational reasons, we assume that the covari-
ance matrices are diagonal and denote σ2i the vari-
ance vector of Gaussian i, e.g., σ2 i= diag(Ei).
In real applications, the GMM is estimated of-
fline with a set of continuous word embeddings
extracted from a representative set of questions.
The parameters λ are estimated through the op-
timization of a Maximum Likelihood (ML) crite-
rion using the Expectation-Maximization (EM) al-
gorithm. In the following, we follow the notations
used in (Sanchez et al., 2013).
Given uλ, one can characterize the question qi
using the following score function:
</bodyText>
<equation confidence="0.995302">
Gqiλ = 5Ni
λ loguλ(qi) (6)
</equation>
<bodyText confidence="0.999954">
where Gqiλ is a vector whose size depends only on
the number of parameters in λ. Assuming that the
word embedding ewij is iid (a simplifying assump-
tion), we get:
</bodyText>
<equation confidence="0.995468">
5λloguλ(ewij) (7)
</equation>
<bodyText confidence="0.999319333333333">
Following the literature (Sanchez et al., 2013),
we propose to measure the similarity between two
questions qi and qj using the FK:
</bodyText>
<equation confidence="0.722028">
K(qi, qj) = Ga Fλ1Ga (g)
</equation>
<bodyText confidence="0.8205555">
where Fλ is the Fisher Information Matrix (FIM)
of uλ:
</bodyText>
<equation confidence="0.772619">
(9)
Since Fλ is symmetric and positive definite,
F−1
</equation>
<bodyText confidence="0.995293333333333">
λ can be transformed to LT λ Lλ based on the
Cholesky decomposition. Hence, KFK(qi, qj) can
rewritten as follows:
</bodyText>
<equation confidence="0.993551">
KFK(qi, qj) = GqT
λ Gqj
i (10)
</equation>
<page confidence="0.697392">
λ
</page>
<figure confidence="0.9870569">
...
... ...
...
...
Ni
L
j=1
Gqi λ =
�
Fλ = Eqi∼uλ LGqiλ GqTiλ
</figure>
<page confidence="0.99725">
254
</page>
<bodyText confidence="0.973877307692308">
where
Gqi
λ =LλGqiλ = Lλ 5λ loguλ(qi) (11)
In (Sanchez et al., 2013), Gqi
λ refers to as the
Fisher Vector (FV) of qi. The dot product between
FVs can be used to calculate the semantic simi-
larities. Based on the specific probability density
function, GMM, FV of qi is respect to the mean µ
and standard deviation σ of all the mixed Gaussian
distributions. Let γj(k) be the soft assignment of
the jth word embedding ewij in qi to Guassian k
(uk):
</bodyText>
<equation confidence="0.995774571428572">
γj(k) = p(k|ewij)θiuk(ewij) (12)
PKj=1 θkuk(ewij)
Mathematical derivations lead to:
1 XNi h(ewij − µk)2 i
Gqi σ,k = Ni √2θi γj(k) − 1
σ2
j=1 k
</equation>
<bodyText confidence="0.999862875">
The division by the vector σk should be under-
stood as a term-by-term operation. The final gradi-
ent vector Gqi
λ is the concatenation of the Gqiµ,k and
Gqi σ,k vectors for k = 1 · · · K. Let d denote the di-
mensionality of the continuous word embeddings
and K be the number of Gaussians. The final
fisher vector Gqiλ is therefore 2Kd-dimensional.
</bodyText>
<sectionHeader confidence="0.999649" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999796">
In this section, we present the experiments to eval-
uate the performance of the proposed method for
question retrieval.
</bodyText>
<subsectionHeader confidence="0.999846">
4.1 Data Set and Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999965">
We collect the data sets from Yahoo! Answers
and Baidu Zhidao. Yahoo! Answers and Baidu
Zhidao represent the largest and the most popu-
lar cQA archives in English and Chinese, respec-
tively. More specifically, we utilized the resolved
questions at Yahoo! Answers and Baidu Zhidao.
The questions include 10 million items from Ya-
hoo! Answers and 8 million items from Baidu
Zhidao (also called retrieval data). Each resolved
question consists of three fields: “title”, “descrip-
tion” and “answers”, as well as some metadata,
such as “category”. For question retrieval, we use
only the “title” field and “category” metadata. It
</bodyText>
<table confidence="0.996393">
#queries #candidate #relevant
Yahoo data 1,000 13,000 2,671
Baidu data 1,000 8,000 2,104
</table>
<tableCaption confidence="0.999918">
Table 1: Statistics on the manually labeled data.
</tableCaption>
<bodyText confidence="0.999966348837209">
is assumed that the titles of questions already pro-
vide enough semantic information for understand-
ing users’ information needs (Duan et al., 2008).
We develop two test sets, one for “Yahoo data”,
and the other for “Baidu data”. In order to create
the test sets, we collect some extra questions that
have been posted more recently than the retrieval
data, and randomly sample 1, 000 questions for
Yahoo! Answers and Baidu Zhidao, respectively.
We take those questions as queries. All questions
are lowercased and stemmed. Stopwords5 are also
removed.
We separately index all data from Yahoo! An-
swers and Baidu Zhidao using an open source
Lucene with the BM25 scoring function6. For
each query from Yahoo! Answers and Baidu Zhi-
dao, we retrieve the several candidate questions
from the corresponding indexed data by using the
BM25 ranking algorithm in Lucene. On average,
each query from Yahoo! Answers has 13 candi-
date questions and the average number of candi-
date questions for Baidu Zhidao is 8.
We recruit students to label the relevance of
the candidate questions regarding to the queries.
Specifically, for each type of language, we let
three native students. Given a candidate question,
a student is asked to label it with “relevant” or “ir-
relevant”. If a candidate question is considered
semantically similar to the query, the student will
label it as “relevant”; otherwise, the student will
label it as “irrelevant”. As a result, each candi-
date question gets three labels and the majority of
the label is taken as the final decision for a query-
candidate pair. We randomly split each of the two
labeled data sets into a validation set and a test set
with a ration 1 : 3. The validation set is used for
tuning parameters of different models, while the
test set is used for evaluating how well the models
ranked relevant candidates in contrast to irrelevant
candidates. Table 1 presents the manually labeled
data.
Please note that rather than evaluate both re-
trieval and ranking capability of different meth-
</bodyText>
<equation confidence="0.669062">
σk
j=1
</equation>
<footnote confidence="0.91526325">
5http://truereader.com/manuals/onix/stopwords1.html
6We use the BM25 implementation provided by Apache
Lucene (http://lucene.apache.org/), using the default parame-
ter setting (kl = 1.2, b = 0.75)
</footnote>
<equation confidence="0.8964708">
Ni
1
Gqiµ
X hewij − µk i
,k = Ni √θi γj(k) (13)
</equation>
<page confidence="0.995406">
255
</page>
<bodyText confidence="0.999550818181818">
ods like the existing work (Cao et al., 2010), we
compare them in a ranking task. This may lose
recall for some methods, but it can enable large-
scale evaluation.
In order to evaluate the performance of dif-
ferent models, we employ Mean Average Preci-
sion (MAP), Mean Reciprocal Rank (MRR), R-
Precision (R-Prec), and Precision at K (P@5) as
evaluation measures. These measures are widely
used in the literature for question retrieval in
cQA (Cao et al., 2010).
</bodyText>
<subsectionHeader confidence="0.998231">
4.2 Parameter Setting
</subsectionHeader>
<bodyText confidence="0.999974942857143">
In our experiments, we train the word embeddings
on another large-scale data set from cQA sites. For
English, we train the word embeddings on the Ya-
hoo! Webscope dataset7. For Chinese, we train the
word embeddings on a data set with 1 billion web
pages from Baidu Zhidao. These two data sets do
not intersect with the above mentioned retrieval
data. Little pre-processing is conducted for the
training of word embeddings. The resulting text is
tokenized using the Stanford tokenizer,8, and ev-
ery word is converted to lowercase. Since the pro-
posed framework has no limits in using which of
the word embedding learning methods, we only
consider the following two representative meth-
ods: Skip-gram (baseline) and M-NET. To train the
word embedding using these two methods, we ap-
ply the same setting for their common parameters.
Specifically, the count of negative samples r is set
to 3; the context window size l is set to 5; each
model is trained through 1 epoch; the learning rate
is initialized as 0.025 and is set to decrease linearly
so that it approached zero at the end of training.
Besides, the combination weight Q used in M-
NET also plays an important role in producing
high quality word embedding. Overemphasizing
the weight of the original objective of Skip-gram
may result in weakened influence of metadata,
while putting too large weight on metadata pow-
ered objective may hurt the generality of learned
word embedding. Based on our experience, it is
a better way to decode the objective combination
weight of the Skip-gram model and metadata in-
formation based on the scale of their respective
derivatives during optimization. Finally, we set
Q = 0.001 empirically. Note that if the parameter
</bodyText>
<footnote confidence="0.999233">
7The Yahoo! Webscope dataset Yahoo answers com-
prehensive questions and answers version 1.0.2, available at
http://reseach.yahoo.com/Academic Relations.
8http://nlp.stanford.edu/software/tokenizer.shtml
</footnote>
<bodyText confidence="0.9973375">
is optimized on the validation set, the final perfor-
mance can be further improved.
For parameter K used in FV, we do an exper-
iment on the validation data set to determine the
best value among 1, 2, 4, · · · , 64 in terms of MAP.
As a result, we set K = 16 in the experiments
empirically as this setting yields the best perfor-
mance.
</bodyText>
<subsectionHeader confidence="0.999913">
4.3 Main Results
</subsectionHeader>
<bodyText confidence="0.999929926829268">
In this subsection, we present the experimental re-
sults on the test sets of Yahoo data and Baidu data.
We compare the baseline word embedding trained
by Skip-gram against this trained by M-NET. The
dimension of word embedding is set as 50,100 and
300. Since the motivation of this paper attempts to
tackle the lexical gap problem for queried ques-
tions and questions in the archive, we also com-
pare them with the two groups of methods which
also address the lexical gap in the literature. The
first group is the translation models: word-based
translation model (Jeon et al., 2005), word-based
translation language model (Xue et al., 2008),
and phrase-based translation model (Zhou et al.,
2011). We implement those three translation mod-
els based on the original papers and train those
models with (question, best answer) pairs from the
Yahoo! Webscope dataset Yahoo answers and the
1 billion web pages of Baidu Zhidao for English
and Chinese, respectively. Training the translation
models with different pairs (e.g., question-best an-
swer, question-description, question-answer) may
achieve inconsistent performance on Yahoo data
and Baidu data, but its comparison and analysis
are beyond the scope of this paper. The second
group is the topic-based methods: unsupervised
question-answer topic model (Ji et al., 2012) and
supervised question-answer topic model (Zhang et
al., 2014a). We re-implement these two topic-
based models and tune the parameter settings on
our data set. Besides, we also introduce a baseline
language model (LM) (Zhai and Lafferty, 2001)
for comparison.
Table 2 shows the question retrieval perfor-
mance by using different evaluation metrics. From
this table, we can see that learning continu-
ous word embedding representations (Skip-gram
+ FV, M-NET + FV) for question retrieval can
outperform the translation-based approaches and
topic-based approaches on all evaluation metrics.
We conduct a statistical test (t-test), the results
</bodyText>
<page confidence="0.993799">
256
</page>
<table confidence="0.999780285714286">
Model dim Yahoo data Baidu data
MAP MRR R-Prec P@5 MAP MRR R-Prec P@5
LM (baseline) - 0.435 0.472 0.381 0.305 0.392 0.413 0.325 0.247
(Jeon et al., 2005) - 0.463 0.495 0.396 0.332 0.414 0.428 0.341 0.256
(Xue et al., 2008) - 0.518 0.560 0.423 0.346 0.431 0.435 0.352 0.264
(Zhou et al., 2011) - 0.536 0.587 0.439 0.361 0.448 0.450 0.367 0.273
(Ji et al., 2012) - 0.508 0.544 0.405 0.324 0.425 0.431 0.349 0.258
(Zhang et al., 2014a) - 0.527 0.572 0.433 0.350 0.443 0.446 0.358 0.265
50 0.532 0.583 0.437 0.358 0.447 0.450 0.366 0.272
Skip-gram + FV 100 0.544 0.6051 0.440 0.363 0.454 0.457 0.373 0.274
300 0.5501 0.6191 0.444 0.365 0.4601 0.4641 0.374 0.277
50 0.5481 0.6121 0.441 0.363 0.4591 0.4621 0.374 0.276
M-NET + FV 100 0.562$ 0.628$ 0.4521 0.367$ 0.468$ 0.471 0.3781 0.2801
300 0.571$ 0.643$ 0.455$ 0.374$ 0.475$ 0.477$ 0.385$ 0.283$
</table>
<tableCaption confidence="0.7534015">
Table 2: Evaluation results on Yahoo data and Baidu data, where dim denotes the dimension of the
word embeddings. The bold formate indicates the best results for question retrieval. † indicates that
</tableCaption>
<bodyText confidence="0.999466631578947">
the difference between the results of our proposed approach (Skip-gram + FV, M-NET + FV) and other
methods are mildly significant with p &lt; 0.08 under a t-test; ‡ indicates the comparisons are statistically
significant with p &lt; 0.05.
show that the improvements between the pro-
posed M-NET + FV and the two groups of com-
pared methods (translation-based approaches and
topic-based approaches) are statistically signifi-
cant (p &lt; 0.05), while the improvements be-
tween Skip-gram + FV and the translation-based
approaches are mildly significant (p &lt; 0.08).
Moreover, the metadata of category information
powered model (M-NET + FV) outperforms the
baseline skip-gram model (Skip-gram + FV) and
yields the largest improvements. These results can
imply that the metadata powered word embedding
is of higher quality than the baseline model with
no metadata information regularization. Besides,
we also note that setting higher dimension brings
more improvements for question retrieval task.
Translation-based methods significantly outper-
form LM, which demonstrate that matching ques-
tions with the semantically related translation
words or phrases from question-answer pairs can
effectively address the word lexical gap problem.
Besides, we also note that phrase-based translation
model is more effective because it captures some
contextual information in modeling the transla-
tion of phrases as a whole. More precise transla-
tion can be determined for phrases than for words.
Similar observation has also been found in the pre-
vious work (Zhou et al., 2011).
On both data sets, topic-based models achieve
comparable performance with the translation-
based models and but they perform better than
LM. The results demonstrate that learning the
latent topics aligned across the question-answer
pairs can be an alternative for bridging lexical gap
problem for question retrieval.
</bodyText>
<sectionHeader confidence="0.998835" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999979894736842">
This paper proposes to learn continuous vector
representations for question retrieval in cQA. We
firstly introduce a new metadata powered word
embedding method, called M-NET, to leverage the
category information within cQA pages to obtain
word representations. Once the words are embed-
ded in a continuous space, we treat each ques-
tion as a BoEW. Then, the variable size BoEWs
are aggregated into fixed-length vectors by using
FK. Finally, the dot product between FVs are used
to calculate the semantic similarities for question
retrieval. Experiments on large-scale real world
cQA data demonstrate that the efficacy of the pro-
posed approach. For the future work, we will
explore how to incorporate more types of meta-
data information, such as the user ratings, like sig-
nals and Poll and Survey signals, into the learning
process to obtain more powerful word representa-
tions.
</bodyText>
<sectionHeader confidence="0.998314" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.7722085">
This work was supported by the National Natu-
ral Science Foundation of China (No. 61303180,
</bodyText>
<page confidence="0.987746">
257
</page>
<bodyText confidence="0.999519125">
No. 61272332 and 61402191), the Beijing Natu-
ral Science Foundation (No. 4144087), the Ma-
jor Project of National Social Science Found (No.
12&amp;2D223), the Fundamental Research Funds for
the Central Universities (No. CCNU15ZD003),
and also Sponsored by CCF-Tencent Open Re-
search Fund. We thank the anonymous reviewers
for their insightful comments.
</bodyText>
<sectionHeader confidence="0.998527" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999887195652175">
Lada A. Adamic, Jun Zhang, Eytan Bakshy, and
Mark S. Ackerman. 2008. Knowledge sharing and
yahoo answers: Everyone knows something. In Pro-
ceedings of WWW, pages 665–674.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. J. Mach. Learn. Res., 3.
Delphine Bernhard and Iryna Gurevych. 2009. Com-
bining lexical semantic resources with question &amp;
answer archives for translation-based answer find-
ing. In Proceedings ofACL-IJCNLP.
Li Cai, Guangyou Zhou, Kang Liu, and Jun Zhao.
2011. Learning the latent topics for question re-
trieval in community qa. In Proceedings of IJCNLP,
pages 273–281.
Xin Cao, Gao Cong, Bin Cui, and Christian S. Jensen.
2010. A generalized framework of exploring cate-
gory information for question retrieval in commu-
nity question answer archives. In Proceedings of
WWW, pages 201–210.
David Carmel, Avihai Mejer, Yuval Pinter, and Idan
Szpektor. 2014. Improving term weighting for com-
munity question answering search using syntactic
analysis. In Proceedings of CIKM, pages 351–360.
Stephane Clinchant and Florent Perronnin. 2013. Ag-
gregating continuous word embeddings for informa-
tion retrieval. In Proceedings of the Workshop on
Continuous Vector Space Models and their Compo-
sitionality, pages 100–109.
Huizhong Duan, Yunbo Cao, Chin yew Lin, and Yong
Yu. 2008. Searching questions by identifying ques-
tion topic and question focus. In Proceedings of
ACL.
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of ACL, pages 873–882.
Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005.
Finding similar questions in large question and an-
swer archives. In Proceedings of CIKM.
Zongcheng Ji, Fei Xu, Bin Wang, and Ben He. 2012.
Question-answer topic model for question retrieval
in community question answering. In Proceedings
of CIKM, pages 2471–2474.
Jung-Tae Lee, Sang-Bum Kim, Young-In Song, and
Hae-Chang Rim. 2008. Bridging lexical gaps be-
tween queries and questions on large online q&amp;a col-
lections with compact translation models. In Pro-
ceedings of EMNLP, pages 410–418.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analysis.
In Proceedings ofACL, pages 142–150.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proceedings of NIPS, pages 3111–3119.
Stefan Riezler, Er Vasserman, Ioannis Tsochantaridis,
Vibhu Mittal, and Yi Liu. 2007. Statistical machine
translation for query expansion in answer retrieval.
In Proceedings ofACL.
S. Robertson, S. Walker, S. Jones, M. Hancock-
Beaulieu, and M. Gatford. 1994. Okapi at trec-3.
In Proceedings of TREC, pages 109–126.
Jorge Sanchez, Florent Perronnin, Thomas Mensink,
and Jakob J. Verbeek. 2013. Image classification
with the fisher vector: Theory and practice. Interna-
tional Journal of Computer Vision, pages 222–245.
A. Singh. 2012. Entity based q&amp;a retrieval. In Pro-
ceedings of EMNLP, pages 1266–1277.
M. Surdeanu, M. Ciaramita, and H. Zaragoza. 2008.
Learning to rank answers on large online qa collec-
tions. In Proceedings ofACL, pages 719–727.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceed-
ings of ACL.
Kai Wang, Zhaoyan Ming, and Tat-Seng Chua. 2009.
A syntactic tree matching approach to finding sim-
ilar questions in community-based qa services. In
Proceedings of SIGIR, pages 187–194.
B. Wang, X. Wang, C. Sun, B. Liu, and L. Sun. 2010.
Modeling semantic relevance for question-answer
pairs in web social communities. In ACL.
Chang Xu, Yalong Bai, Jiang Bian, Bin Gao, Gang
Wang, Xiaoguang Liu, and Tie-Yan Liu. 2014. Rc-
net: A general framework for incorporating knowl-
edge into word representations. In Proceedings of
CIKM, pages 1219–1228.
Xiaobing Xue, Jiwoon Jeon, and W. Bruce Croft. 2008.
Retrieval models for question and answer archives.
In Proceedings of SIGIR, pages 475–482.
</reference>
<page confidence="0.963748">
258
</page>
<reference confidence="0.99940304">
Mo Yu and Mark Dredze. 2014. Improving lexical em-
beddings with semantic knowledge. In Proceedings
of ACL, pages 545–550.
Chengxiang Zhai and John Lafferty. 2001. A study
of smoothing methods for language models applied
to ad hoc information retrieval. In Proceedings of
SIGIR, pages 334–342.
Kai Zhang, Wei Wu, Haocheng Wu, Zhoujun Li, and
Ming Zhou. 2014a. Question retrieval with high
quality answers in community question answering.
In Proceedings of CIKM, pages 371–380.
Qi Zhang, Jihua Kang, Jin Qian, and Xuanjing Huang.
2014b. Continuous word embeddings for detect-
ing local text reuses at the semantic level. In Pro-
ceedings of the 37th International ACM SIGIR Con-
ference on Research &amp; Development in Information
Retrieval, SIGIR ’14, pages 797–806.
Guangyou Zhou, Li Cai, Jun Zhao, and Kang Liu.
2011. Phrase-based translation model for question
retrieval in community question answer archives. In
Proceedings of ACL, pages 653–662.
Guangyou Zhou, Yubo Chen, Daojian Zeng, and Jun
Zhao. 2013. Towards faster and better retrieval
models for question search. In Proceedings of
CIKM, pages 2139–2148.
</reference>
<page confidence="0.998533">
259
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.791395">
<title confidence="0.993904">Learning Continuous Word Embedding with Metadata for Retrieval in Community Question Answering</title>
<author confidence="0.971929">Tingting Jun</author>
<author confidence="0.971929">Po</author>
<affiliation confidence="0.981815">1School of Computer, Central China Normal University, Wuhan 430079,</affiliation>
<address confidence="0.912402">2National Laboratory of Pattern Recognition, CASIA, Beijing 100190,</address>
<email confidence="0.962468">jzhao@nlpr.ia.ac.cn</email>
<abstract confidence="0.997323208333333">Community question answering (cQA) has become an important issue due to the popularity of cQA archives on the web. This paper is concerned with the problem of question retrieval. Question retrieval in cQA archives aims to find the existing questions that are semantically equivalent or relevant to the queried questions. However, the lexical gap problem brings about new challenge for question retrieval in cQA. In this paper, we propose to learn continuous word embeddings with metadata of category information within cQA pages for question retrieval. To deal with the variable size of word embedding vectors, we employ the framework of fisher kernel to aggregated them into the fixedlength vectors. Experimental results on large-scale real world cQA data set show that our approach can significantly outperform state-of-the-art translation models and topic-based models for question retrieval in cQA.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Lada A Adamic</author>
<author>Jun Zhang</author>
<author>Eytan Bakshy</author>
<author>Mark S Ackerman</author>
</authors>
<title>Knowledge sharing and yahoo answers: Everyone knows something.</title>
<date>2008</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>665--674</pages>
<contexts>
<context position="2036" citStr="Adamic et al., 2008" startWordPosition="300" endWordPosition="303"> include the traditional Frequently Asked Questions (FAQ) archives and the emerging community question answering (cQA) services, such as Yahoo! Answers1, Live QnA2, and Baidu Zhidao3. The content in these web sites is usually organized as questions and lists of answers associated with metadata like user chosen categories to questions and askers’ awards to the best answers. This data made 1http://answers.yahoo.com/ 2http://qna.live.com/ 3http://zhidao.baidu.com/ cQA archives valuable resources for various tasks like question-answering (Jeon et al., 2005; Xue et al., 2008) and knowledge mining (Adamic et al., 2008), etc. One fundamental task for reusing content in cQA is finding similar questions for queried questions, as questions are the keys to accessing the knowledge in cQA. Then the best answers of these similar questions will be used to answer the queried questions. Many studies have been done along this line (Jeon et al., 2005; Xue et al., 2008; Duan et al., 2008; Lee et al., 2008; Bernhard and Gurevych, 2009; Cao et al., 2010; Zhou et al., 2011; Singh, 2012; Zhang et al., 2014a). One big challenge for question retrieval in cQA is the lexical gap between the queried questions and the existing que</context>
</contexts>
<marker>Adamic, Zhang, Bakshy, Ackerman, 2008</marker>
<rawString>Lada A. Adamic, Jun Zhang, Eytan Bakshy, and Mark S. Ackerman. 2008. Knowledge sharing and yahoo answers: Everyone knows something. In Proceedings of WWW, pages 665–674.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<volume>3</volume>
<contexts>
<context position="9877" citStr="Bengio et al. (2003)" startWordPosition="1552" endWordPosition="1555"> a learning to rank strategy (Surdeanu et al., 2008; Carmel et al., 2014). Recently, Cao et al. (2010) and Zhou et al. (2013) exploited the category metadata within cQA pages to further improve the performance. On the contrary, we focus on the representation learning for questions, with a different solution with those previous works. 2.2 Word Embedding Learning Representation of words as continuous vectors has attracted increasing attention in the area of natural language processing (NLP). Recently, a series of works applied deep learning techniques to learn high-quality word representations. Bengio et al. (2003) proposed a probabilistic neural network language model (NNLM) for word representations. Furthermore, Mikolov et al. (2013) proposed efficient neural network models for learning word representations, including the continuous skip-gram model and the continuous bag-ofword model (CBOW), both of which are unsupervised models learned from large-scale text corpora. Besides, there are also a large number of works addressing the task of learning word representations (Huang et al., 2012; Maas et al., 2011; Turian et al., 2010). Nevertheless, since most the existing works learned word representations ma</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. J. Mach. Learn. Res., 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delphine Bernhard</author>
<author>Iryna Gurevych</author>
</authors>
<title>Combining lexical semantic resources with question &amp; answer archives for translation-based answer finding.</title>
<date>2009</date>
<booktitle>In Proceedings ofACL-IJCNLP.</booktitle>
<contexts>
<context position="2445" citStr="Bernhard and Gurevych, 2009" startWordPosition="372" endWordPosition="375">wers.yahoo.com/ 2http://qna.live.com/ 3http://zhidao.baidu.com/ cQA archives valuable resources for various tasks like question-answering (Jeon et al., 2005; Xue et al., 2008) and knowledge mining (Adamic et al., 2008), etc. One fundamental task for reusing content in cQA is finding similar questions for queried questions, as questions are the keys to accessing the knowledge in cQA. Then the best answers of these similar questions will be used to answer the queried questions. Many studies have been done along this line (Jeon et al., 2005; Xue et al., 2008; Duan et al., 2008; Lee et al., 2008; Bernhard and Gurevych, 2009; Cao et al., 2010; Zhou et al., 2011; Singh, 2012; Zhang et al., 2014a). One big challenge for question retrieval in cQA is the lexical gap between the queried questions and the existing questions in the archives. Lexical gap means that the queried questions may contain words that are different from, but related to, the words in the existing questions. For example shown in (Zhang et al., 2014a), we find that for a queried question “how do I get knots out of my cats fur?”, there are good answers under an existing question “how can I remove a tangle in my cat’s fur?” in Yahoo! Answers. Although</context>
<context position="7410" citStr="Bernhard and Gurevych, 2009" startWordPosition="1160" endWordPosition="1163">essing; (3) an empirical verification of the efficacy of the proposed framework on large-scale English and Chinese cQA data. The rest of this paper is organized as follows. Section 2 summarizes the related work. Section 3 describes our proposed framework for question retrieval. Section 4 reports the experimental results. Finally, we conclude the paper in Section 5. 2 Related Work 2.1 Question Retrieval in cQA Significant research efforts have been conducted over the years in attempt to improve question retrieval in cQA (Jeon et al., 2005; Xue et al., 2008; Lee et al., 2008; Duan et al., 2008; Bernhard and Gurevych, 2009; Cao et al., 2010; Zhou et al., 2011; Singh, 2012; Zhang et al., 2014a). Most of these works focus on finding similar questions for the user queried questions. The major challenge for question retrieval in cQA is the lexical gap problem. Jeon et al. (2005) proposed a wordbased translation model for automatically fixing the lexical gap problem. Xue et al. (2008) proposed a word-based translation language model for question retrieval. Lee et al. (2008) tried to further improve the translation probabilities based on question-answer pairs by selecting the most important terms to build compact tra</context>
</contexts>
<marker>Bernhard, Gurevych, 2009</marker>
<rawString>Delphine Bernhard and Iryna Gurevych. 2009. Combining lexical semantic resources with question &amp; answer archives for translation-based answer finding. In Proceedings ofACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Li Cai</author>
<author>Guangyou Zhou</author>
<author>Kang Liu</author>
<author>Jun Zhao</author>
</authors>
<title>Learning the latent topics for question retrieval in community qa.</title>
<date>2011</date>
<booktitle>In Proceedings of IJCNLP,</booktitle>
<pages>273--281</pages>
<contexts>
<context position="4558" citStr="Cai et al., 2011" startWordPosition="713" endWordPosition="716">nal Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 250–259, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics translation probabilities (Jeon et al., 2005; Xue et al., 2008; Zhou et al., 2011). Experimental results show that translation models obtain stateof-the-art performance for question retrieval in cQA. However, questions and answers are far from “parallel” in practice, questions and answers are highly asymmetric on the information they contain (Zhang et al., 2014a). The second group is the topic-based models (Cai et al., 2011; Ji et al., 2012), which learn the latent topics aligned across the question-answer pairs to alleviate the lexical gap problem, with the assumption that a question and its paired answers share the same topic distribution. However, questions and answers are heterogeneous in many aspects, they do not share the same topic distribution in practice. Inspired by the recent success of continuous space word representations in capturing the semantic similarities in various natural language processing tasks, we propose to incorporate an embedding of words in a continuous space for question representati</context>
</contexts>
<marker>Cai, Zhou, Liu, Zhao, 2011</marker>
<rawString>Li Cai, Guangyou Zhou, Kang Liu, and Jun Zhao. 2011. Learning the latent topics for question retrieval in community qa. In Proceedings of IJCNLP, pages 273–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Cao</author>
<author>Gao Cong</author>
<author>Bin Cui</author>
<author>Christian S Jensen</author>
</authors>
<title>A generalized framework of exploring category information for question retrieval in community question answer archives.</title>
<date>2010</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>201--210</pages>
<contexts>
<context position="2463" citStr="Cao et al., 2010" startWordPosition="376" endWordPosition="379">ive.com/ 3http://zhidao.baidu.com/ cQA archives valuable resources for various tasks like question-answering (Jeon et al., 2005; Xue et al., 2008) and knowledge mining (Adamic et al., 2008), etc. One fundamental task for reusing content in cQA is finding similar questions for queried questions, as questions are the keys to accessing the knowledge in cQA. Then the best answers of these similar questions will be used to answer the queried questions. Many studies have been done along this line (Jeon et al., 2005; Xue et al., 2008; Duan et al., 2008; Lee et al., 2008; Bernhard and Gurevych, 2009; Cao et al., 2010; Zhou et al., 2011; Singh, 2012; Zhang et al., 2014a). One big challenge for question retrieval in cQA is the lexical gap between the queried questions and the existing questions in the archives. Lexical gap means that the queried questions may contain words that are different from, but related to, the words in the existing questions. For example shown in (Zhang et al., 2014a), we find that for a queried question “how do I get knots out of my cats fur?”, there are good answers under an existing question “how can I remove a tangle in my cat’s fur?” in Yahoo! Answers. Although the two questions</context>
<context position="7428" citStr="Cao et al., 2010" startWordPosition="1164" endWordPosition="1167">fication of the efficacy of the proposed framework on large-scale English and Chinese cQA data. The rest of this paper is organized as follows. Section 2 summarizes the related work. Section 3 describes our proposed framework for question retrieval. Section 4 reports the experimental results. Finally, we conclude the paper in Section 5. 2 Related Work 2.1 Question Retrieval in cQA Significant research efforts have been conducted over the years in attempt to improve question retrieval in cQA (Jeon et al., 2005; Xue et al., 2008; Lee et al., 2008; Duan et al., 2008; Bernhard and Gurevych, 2009; Cao et al., 2010; Zhou et al., 2011; Singh, 2012; Zhang et al., 2014a). Most of these works focus on finding similar questions for the user queried questions. The major challenge for question retrieval in cQA is the lexical gap problem. Jeon et al. (2005) proposed a wordbased translation model for automatically fixing the lexical gap problem. Xue et al. (2008) proposed a word-based translation language model for question retrieval. Lee et al. (2008) tried to further improve the translation probabilities based on question-answer pairs by selecting the most important terms to build compact translation models. B</context>
<context position="9359" citStr="Cao et al. (2010)" startWordPosition="1472" endWordPosition="1475">nformation (entities). In contrast to the works described above that assume question-answer pairs are “parallel text”, our paper deals with the lexical gap by learning con251 tinuous word embeddings in capturing the similarities without any assumptions, which is much more reasonable in practice. Besides, some other studies model the semantic relationship between questions and answers with deep linguistic analysis (Duan et al., 2008; Wang et al., 2009; Wang et al., 2010; Ji et al., 2012; Zhang et al., 2014a) or a learning to rank strategy (Surdeanu et al., 2008; Carmel et al., 2014). Recently, Cao et al. (2010) and Zhou et al. (2013) exploited the category metadata within cQA pages to further improve the performance. On the contrary, we focus on the representation learning for questions, with a different solution with those previous works. 2.2 Word Embedding Learning Representation of words as continuous vectors has attracted increasing attention in the area of natural language processing (NLP). Recently, a series of works applied deep learning techniques to learn high-quality word representations. Bengio et al. (2003) proposed a probabilistic neural network language model (NNLM) for word representa</context>
<context position="15384" citStr="Cao et al., 2010" startWordPosition="2470" endWordPosition="2473">bility of sampling w is usually set to p(w) a u(w)3/4 (Mikolov et al., 2013). 3.2 Metadata Powered Model After briefing the skip-gram model, we introduce how we equip it with the metadata information. In cQA sites, there are several metadata, such as “category”,“voting” and so on. In this paper, we only consider the metadata of category information for word embedding learning. All questions in cQA are usually organized into a hierarchy of categories. When an user asks a question, the user typically required to choose a category label for the question from a predefined hierarchy of categories (Cao et al., 2010; Zhou et al., 2013). Previous work in the literature has demonstrated the effectiveness of the category information for question retrieval (Cao et al., 2010; Zhou et al., 2013). On the contrary, we argue that the category information benefits the word embedding learning in this work. The basic idea is that category information encodes the attributes or properties of words, from which we can group similar words according to their categories. Here, a word’s category is assigned based on the questions it appeared in. For example, a question “What are the security issues with java?” is under the </context>
<context position="25439" citStr="Cao et al., 2010" startWordPosition="4201" endWordPosition="4204">t is used for tuning parameters of different models, while the test set is used for evaluating how well the models ranked relevant candidates in contrast to irrelevant candidates. Table 1 presents the manually labeled data. Please note that rather than evaluate both retrieval and ranking capability of different methσk j=1 5http://truereader.com/manuals/onix/stopwords1.html 6We use the BM25 implementation provided by Apache Lucene (http://lucene.apache.org/), using the default parameter setting (kl = 1.2, b = 0.75) Ni 1 Gqiµ X hewij − µk i ,k = Ni √θi γj(k) (13) 255 ods like the existing work (Cao et al., 2010), we compare them in a ranking task. This may lose recall for some methods, but it can enable largescale evaluation. In order to evaluate the performance of different models, we employ Mean Average Precision (MAP), Mean Reciprocal Rank (MRR), RPrecision (R-Prec), and Precision at K (P@5) as evaluation measures. These measures are widely used in the literature for question retrieval in cQA (Cao et al., 2010). 4.2 Parameter Setting In our experiments, we train the word embeddings on another large-scale data set from cQA sites. For English, we train the word embeddings on the Yahoo! Webscope data</context>
</contexts>
<marker>Cao, Cong, Cui, Jensen, 2010</marker>
<rawString>Xin Cao, Gao Cong, Bin Cui, and Christian S. Jensen. 2010. A generalized framework of exploring category information for question retrieval in community question answer archives. In Proceedings of WWW, pages 201–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Carmel</author>
<author>Avihai Mejer</author>
<author>Yuval Pinter</author>
<author>Idan Szpektor</author>
</authors>
<title>Improving term weighting for community question answering search using syntactic analysis.</title>
<date>2014</date>
<booktitle>In Proceedings of CIKM,</booktitle>
<pages>351--360</pages>
<contexts>
<context position="9330" citStr="Carmel et al., 2014" startWordPosition="1467" endWordPosition="1470"> model to incorporate semantic information (entities). In contrast to the works described above that assume question-answer pairs are “parallel text”, our paper deals with the lexical gap by learning con251 tinuous word embeddings in capturing the similarities without any assumptions, which is much more reasonable in practice. Besides, some other studies model the semantic relationship between questions and answers with deep linguistic analysis (Duan et al., 2008; Wang et al., 2009; Wang et al., 2010; Ji et al., 2012; Zhang et al., 2014a) or a learning to rank strategy (Surdeanu et al., 2008; Carmel et al., 2014). Recently, Cao et al. (2010) and Zhou et al. (2013) exploited the category metadata within cQA pages to further improve the performance. On the contrary, we focus on the representation learning for questions, with a different solution with those previous works. 2.2 Word Embedding Learning Representation of words as continuous vectors has attracted increasing attention in the area of natural language processing (NLP). Recently, a series of works applied deep learning techniques to learn high-quality word representations. Bengio et al. (2003) proposed a probabilistic neural network language mod</context>
</contexts>
<marker>Carmel, Mejer, Pinter, Szpektor, 2014</marker>
<rawString>David Carmel, Avihai Mejer, Yuval Pinter, and Idan Szpektor. 2014. Improving term weighting for community question answering search using syntactic analysis. In Proceedings of CIKM, pages 351–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephane Clinchant</author>
<author>Florent Perronnin</author>
</authors>
<title>Aggregating continuous word embeddings for information retrieval.</title>
<date>2013</date>
<booktitle>In Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality,</booktitle>
<pages>100--109</pages>
<contexts>
<context position="5733" citStr="Clinchant and Perronnin, 2013" startWordPosition="892" endWordPosition="896"> words in a continuous space for question representations. Due to the ability of word embeddings, we firstly transform words in a question into continuous vector representations by looking up tables. These word embeddings are learned in advance using a continuous skip-gram model (Mikolov et al., 2013), or other continuous word representation learning methods. Once the words are embedded in a continuous space, one can view a question as a Bag-of-Embedded-Words (BoEW). Then, the variable-cardinality BoEW will be aggregated into a fixed-length vector by using the Fisher kernel (FK) framework of (Clinchant and Perronnin, 2013; Sanchez et al., 2013). Through the two steps, the proposed approach can map a question into a length invariable compact vector, which can be efficiently and effectively for large-scale question retrieval task in cQA. We test the proposed approach on large-scale Yahoo! Answers data and Baidu Zhidao data. Yahoo! Answers and Baidu Zhidao represent the largest and most popular cQA archives in English and Chinese, respectively. We conduct both quantitative and qualitative evaluations. Experimental results show that our approach can significantly outperform state-of-the-art translation models and </context>
<context position="19277" citStr="Clinchant and Perronnin, 2013" startWordPosition="3134" endWordPosition="3137">ethods in information retrieval field are not suitable for this kinds of issues, BoEWs cannot be directly used for large-scale question retrieval task. Given a cQA data collection Q = {qi,1 ≤ i ≤ |Q|}, where qi is the ith question and |Q |is the number of questions in the data collection. The ith question qi is composed by a sequence of words wi = {wij,1 ≤ j ≤ Ni}, where Ni denotes the length of qi. Through looking up table (word embedding matrix) of M, the ith question qi can be represented by Ewi = {ewij,1 ≤ j ≤ Ni}, where ewij is the word embedding of wij. According to the framework of FK (Clinchant and Perronnin, 2013; Sanchez et al., 2013; Zhang et al., 2014b), questions are modeled by a probability density function. In this work, we use Gaussian mixture model (GMM) to do it. We assume that the continuous word embedding Ewi for question qi have been generated by a “universal” (e.g., questionindependent) probability density function (pdf). As is a common practice, we choose this pdf to be a GMM since any continuous distribution can be approximated with arbitrary precision by a mixture of Gaussian. In what follows, the pdf is denoted uλ where λ = {θi, µi, Σi, i = 1 · · · K} is the set of parameters of the G</context>
</contexts>
<marker>Clinchant, Perronnin, 2013</marker>
<rawString>Stephane Clinchant and Florent Perronnin. 2013. Aggregating continuous word embeddings for information retrieval. In Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 100–109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huizhong Duan</author>
<author>Yunbo Cao</author>
<author>Chin yew Lin</author>
<author>Yong Yu</author>
</authors>
<title>Searching questions by identifying question topic and question focus.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2398" citStr="Duan et al., 2008" startWordPosition="364" endWordPosition="367">t answers. This data made 1http://answers.yahoo.com/ 2http://qna.live.com/ 3http://zhidao.baidu.com/ cQA archives valuable resources for various tasks like question-answering (Jeon et al., 2005; Xue et al., 2008) and knowledge mining (Adamic et al., 2008), etc. One fundamental task for reusing content in cQA is finding similar questions for queried questions, as questions are the keys to accessing the knowledge in cQA. Then the best answers of these similar questions will be used to answer the queried questions. Many studies have been done along this line (Jeon et al., 2005; Xue et al., 2008; Duan et al., 2008; Lee et al., 2008; Bernhard and Gurevych, 2009; Cao et al., 2010; Zhou et al., 2011; Singh, 2012; Zhang et al., 2014a). One big challenge for question retrieval in cQA is the lexical gap between the queried questions and the existing questions in the archives. Lexical gap means that the queried questions may contain words that are different from, but related to, the words in the existing questions. For example shown in (Zhang et al., 2014a), we find that for a queried question “how do I get knots out of my cats fur?”, there are good answers under an existing question “how can I remove a tangl</context>
<context position="7381" citStr="Duan et al., 2008" startWordPosition="1156" endWordPosition="1159">to large-scale processing; (3) an empirical verification of the efficacy of the proposed framework on large-scale English and Chinese cQA data. The rest of this paper is organized as follows. Section 2 summarizes the related work. Section 3 describes our proposed framework for question retrieval. Section 4 reports the experimental results. Finally, we conclude the paper in Section 5. 2 Related Work 2.1 Question Retrieval in cQA Significant research efforts have been conducted over the years in attempt to improve question retrieval in cQA (Jeon et al., 2005; Xue et al., 2008; Lee et al., 2008; Duan et al., 2008; Bernhard and Gurevych, 2009; Cao et al., 2010; Zhou et al., 2011; Singh, 2012; Zhang et al., 2014a). Most of these works focus on finding similar questions for the user queried questions. The major challenge for question retrieval in cQA is the lexical gap problem. Jeon et al. (2005) proposed a wordbased translation model for automatically fixing the lexical gap problem. Xue et al. (2008) proposed a word-based translation language model for question retrieval. Lee et al. (2008) tried to further improve the translation probabilities based on question-answer pairs by selecting the most importa</context>
<context position="9177" citStr="Duan et al., 2008" startWordPosition="1436" endWordPosition="1439">anslations can better improve the retrieval performance. Singh (2012) addressed the lexical gap issues by extending the lexical word-based translation model to incorporate semantic information (entities). In contrast to the works described above that assume question-answer pairs are “parallel text”, our paper deals with the lexical gap by learning con251 tinuous word embeddings in capturing the similarities without any assumptions, which is much more reasonable in practice. Besides, some other studies model the semantic relationship between questions and answers with deep linguistic analysis (Duan et al., 2008; Wang et al., 2009; Wang et al., 2010; Ji et al., 2012; Zhang et al., 2014a) or a learning to rank strategy (Surdeanu et al., 2008; Carmel et al., 2014). Recently, Cao et al. (2010) and Zhou et al. (2013) exploited the category metadata within cQA pages to further improve the performance. On the contrary, we focus on the representation learning for questions, with a different solution with those previous works. 2.2 Word Embedding Learning Representation of words as continuous vectors has attracted increasing attention in the area of natural language processing (NLP). Recently, a series of wor</context>
<context position="23275" citStr="Duan et al., 2008" startWordPosition="3838" endWordPosition="3841">ions include 10 million items from Yahoo! Answers and 8 million items from Baidu Zhidao (also called retrieval data). Each resolved question consists of three fields: “title”, “description” and “answers”, as well as some metadata, such as “category”. For question retrieval, we use only the “title” field and “category” metadata. It #queries #candidate #relevant Yahoo data 1,000 13,000 2,671 Baidu data 1,000 8,000 2,104 Table 1: Statistics on the manually labeled data. is assumed that the titles of questions already provide enough semantic information for understanding users’ information needs (Duan et al., 2008). We develop two test sets, one for “Yahoo data”, and the other for “Baidu data”. In order to create the test sets, we collect some extra questions that have been posted more recently than the retrieval data, and randomly sample 1, 000 questions for Yahoo! Answers and Baidu Zhidao, respectively. We take those questions as queries. All questions are lowercased and stemmed. Stopwords5 are also removed. We separately index all data from Yahoo! Answers and Baidu Zhidao using an open source Lucene with the BM25 scoring function6. For each query from Yahoo! Answers and Baidu Zhidao, we retrieve the </context>
</contexts>
<marker>Duan, Cao, Lin, Yu, 2008</marker>
<rawString>Huizhong Duan, Yunbo Cao, Chin yew Lin, and Yong Yu. 2008. Searching questions by identifying question topic and question focus. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric H Huang</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Improving word representations via global context and multiple word prototypes.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>873--882</pages>
<contexts>
<context position="10359" citStr="Huang et al., 2012" startWordPosition="1628" endWordPosition="1631">ing (NLP). Recently, a series of works applied deep learning techniques to learn high-quality word representations. Bengio et al. (2003) proposed a probabilistic neural network language model (NNLM) for word representations. Furthermore, Mikolov et al. (2013) proposed efficient neural network models for learning word representations, including the continuous skip-gram model and the continuous bag-ofword model (CBOW), both of which are unsupervised models learned from large-scale text corpora. Besides, there are also a large number of works addressing the task of learning word representations (Huang et al., 2012; Maas et al., 2011; Turian et al., 2010). Nevertheless, since most the existing works learned word representations mainly based on the word co-occurrence information, the obtained word embeddings cannot capture the relationship between two syntactically or semantically similar words if either of them yields very little context information. On the other hand, even though amount of context could be noisy or biased such that they cannot reflect the inherent relationship between words and further mislead the training process. Most recently, Yu et al. (2014) used semantic prior knowledge to improv</context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric H. Huang, Richard Socher, Christopher D. Manning, and Andrew Y. Ng. 2012. Improving word representations via global context and multiple word prototypes. In Proceedings of ACL, pages 873–882.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwoon Jeon</author>
<author>W Bruce Croft</author>
<author>Joon Ho Lee</author>
</authors>
<title>Finding similar questions in large question and answer archives.</title>
<date>2005</date>
<booktitle>In Proceedings of CIKM.</booktitle>
<contexts>
<context position="1974" citStr="Jeon et al., 2005" startWordPosition="289" endWordPosition="292"> become an important information resource on the web. These include the traditional Frequently Asked Questions (FAQ) archives and the emerging community question answering (cQA) services, such as Yahoo! Answers1, Live QnA2, and Baidu Zhidao3. The content in these web sites is usually organized as questions and lists of answers associated with metadata like user chosen categories to questions and askers’ awards to the best answers. This data made 1http://answers.yahoo.com/ 2http://qna.live.com/ 3http://zhidao.baidu.com/ cQA archives valuable resources for various tasks like question-answering (Jeon et al., 2005; Xue et al., 2008) and knowledge mining (Adamic et al., 2008), etc. One fundamental task for reusing content in cQA is finding similar questions for queried questions, as questions are the keys to accessing the knowledge in cQA. Then the best answers of these similar questions will be used to answer the queried questions. Many studies have been done along this line (Jeon et al., 2005; Xue et al., 2008; Duan et al., 2008; Lee et al., 2008; Bernhard and Gurevych, 2009; Cao et al., 2010; Zhou et al., 2011; Singh, 2012; Zhang et al., 2014a). One big challenge for question retrieval in cQA is the </context>
<context position="3653" citStr="Jeon et al., 2005" startWordPosition="580" endWordPosition="583"> Although the two questions share few words in common, they have very similar meanings, it is hard for traditional retrieval models (e.g., BM25 (Robertson et al., 1994)) to determine their similarity. This lexical gap has become a major barricade preventing traditional IR models (e.g., BM25) from retrieving similar questions in cQA. To address the lexical gap problem in cQA, previous work in the literature can be divided into two groups. The first group is the translation models, which leverage the question-answer pairs to learn the semantically related words to improve traditional IR models (Jeon et al., 2005; Xue et al., 2008; Zhou et al., 2011). The basic assumption is that question-answer pairs are “parallel texts” and relationship of words (or phrases) can be established through word-to-word (or phrase-to-phrase) 250 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 250–259, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics translation probabilities (Jeon et al., 2005; Xue et al., 2008; Zhou et al., 2011). Experimental results show that transla</context>
<context position="7326" citStr="Jeon et al., 2005" startWordPosition="1144" endWordPosition="1147">tion into a fixed-length vector which is more amenable to large-scale processing; (3) an empirical verification of the efficacy of the proposed framework on large-scale English and Chinese cQA data. The rest of this paper is organized as follows. Section 2 summarizes the related work. Section 3 describes our proposed framework for question retrieval. Section 4 reports the experimental results. Finally, we conclude the paper in Section 5. 2 Related Work 2.1 Question Retrieval in cQA Significant research efforts have been conducted over the years in attempt to improve question retrieval in cQA (Jeon et al., 2005; Xue et al., 2008; Lee et al., 2008; Duan et al., 2008; Bernhard and Gurevych, 2009; Cao et al., 2010; Zhou et al., 2011; Singh, 2012; Zhang et al., 2014a). Most of these works focus on finding similar questions for the user queried questions. The major challenge for question retrieval in cQA is the lexical gap problem. Jeon et al. (2005) proposed a wordbased translation model for automatically fixing the lexical gap problem. Xue et al. (2008) proposed a word-based translation language model for question retrieval. Lee et al. (2008) tried to further improve the translation probabilities based</context>
<context position="28705" citStr="Jeon et al., 2005" startWordPosition="4749" endWordPosition="4752">the best performance. 4.3 Main Results In this subsection, we present the experimental results on the test sets of Yahoo data and Baidu data. We compare the baseline word embedding trained by Skip-gram against this trained by M-NET. The dimension of word embedding is set as 50,100 and 300. Since the motivation of this paper attempts to tackle the lexical gap problem for queried questions and questions in the archive, we also compare them with the two groups of methods which also address the lexical gap in the literature. The first group is the translation models: word-based translation model (Jeon et al., 2005), word-based translation language model (Xue et al., 2008), and phrase-based translation model (Zhou et al., 2011). We implement those three translation models based on the original papers and train those models with (question, best answer) pairs from the Yahoo! Webscope dataset Yahoo answers and the 1 billion web pages of Baidu Zhidao for English and Chinese, respectively. Training the translation models with different pairs (e.g., question-best answer, question-description, question-answer) may achieve inconsistent performance on Yahoo data and Baidu data, but its comparison and analysis are</context>
<context position="30236" citStr="Jeon et al., 2005" startWordPosition="4982" endWordPosition="4985">introduce a baseline language model (LM) (Zhai and Lafferty, 2001) for comparison. Table 2 shows the question retrieval performance by using different evaluation metrics. From this table, we can see that learning continuous word embedding representations (Skip-gram + FV, M-NET + FV) for question retrieval can outperform the translation-based approaches and topic-based approaches on all evaluation metrics. We conduct a statistical test (t-test), the results 256 Model dim Yahoo data Baidu data MAP MRR R-Prec P@5 MAP MRR R-Prec P@5 LM (baseline) - 0.435 0.472 0.381 0.305 0.392 0.413 0.325 0.247 (Jeon et al., 2005) - 0.463 0.495 0.396 0.332 0.414 0.428 0.341 0.256 (Xue et al., 2008) - 0.518 0.560 0.423 0.346 0.431 0.435 0.352 0.264 (Zhou et al., 2011) - 0.536 0.587 0.439 0.361 0.448 0.450 0.367 0.273 (Ji et al., 2012) - 0.508 0.544 0.405 0.324 0.425 0.431 0.349 0.258 (Zhang et al., 2014a) - 0.527 0.572 0.433 0.350 0.443 0.446 0.358 0.265 50 0.532 0.583 0.437 0.358 0.447 0.450 0.366 0.272 Skip-gram + FV 100 0.544 0.6051 0.440 0.363 0.454 0.457 0.373 0.274 300 0.5501 0.6191 0.444 0.365 0.4601 0.4641 0.374 0.277 50 0.5481 0.6121 0.441 0.363 0.4591 0.4621 0.374 0.276 M-NET + FV 100 0.562$ 0.628$ 0.4521 0.36</context>
</contexts>
<marker>Jeon, Croft, Lee, 2005</marker>
<rawString>Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005. Finding similar questions in large question and answer archives. In Proceedings of CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zongcheng Ji</author>
<author>Fei Xu</author>
<author>Bin Wang</author>
<author>Ben He</author>
</authors>
<title>Question-answer topic model for question retrieval in community question answering.</title>
<date>2012</date>
<booktitle>In Proceedings of CIKM,</booktitle>
<pages>2471--2474</pages>
<contexts>
<context position="4576" citStr="Ji et al., 2012" startWordPosition="717" endWordPosition="720">d the 7th International Joint Conference on Natural Language Processing, pages 250–259, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics translation probabilities (Jeon et al., 2005; Xue et al., 2008; Zhou et al., 2011). Experimental results show that translation models obtain stateof-the-art performance for question retrieval in cQA. However, questions and answers are far from “parallel” in practice, questions and answers are highly asymmetric on the information they contain (Zhang et al., 2014a). The second group is the topic-based models (Cai et al., 2011; Ji et al., 2012), which learn the latent topics aligned across the question-answer pairs to alleviate the lexical gap problem, with the assumption that a question and its paired answers share the same topic distribution. However, questions and answers are heterogeneous in many aspects, they do not share the same topic distribution in practice. Inspired by the recent success of continuous space word representations in capturing the semantic similarities in various natural language processing tasks, we propose to incorporate an embedding of words in a continuous space for question representations. Due to the ab</context>
<context position="9232" citStr="Ji et al., 2012" startWordPosition="1448" endWordPosition="1451">Singh (2012) addressed the lexical gap issues by extending the lexical word-based translation model to incorporate semantic information (entities). In contrast to the works described above that assume question-answer pairs are “parallel text”, our paper deals with the lexical gap by learning con251 tinuous word embeddings in capturing the similarities without any assumptions, which is much more reasonable in practice. Besides, some other studies model the semantic relationship between questions and answers with deep linguistic analysis (Duan et al., 2008; Wang et al., 2009; Wang et al., 2010; Ji et al., 2012; Zhang et al., 2014a) or a learning to rank strategy (Surdeanu et al., 2008; Carmel et al., 2014). Recently, Cao et al. (2010) and Zhou et al. (2013) exploited the category metadata within cQA pages to further improve the performance. On the contrary, we focus on the representation learning for questions, with a different solution with those previous works. 2.2 Word Embedding Learning Representation of words as continuous vectors has attracted increasing attention in the area of natural language processing (NLP). Recently, a series of works applied deep learning techniques to learn high-quali</context>
<context position="29441" citStr="Ji et al., 2012" startWordPosition="4857" endWordPosition="4860">mplement those three translation models based on the original papers and train those models with (question, best answer) pairs from the Yahoo! Webscope dataset Yahoo answers and the 1 billion web pages of Baidu Zhidao for English and Chinese, respectively. Training the translation models with different pairs (e.g., question-best answer, question-description, question-answer) may achieve inconsistent performance on Yahoo data and Baidu data, but its comparison and analysis are beyond the scope of this paper. The second group is the topic-based methods: unsupervised question-answer topic model (Ji et al., 2012) and supervised question-answer topic model (Zhang et al., 2014a). We re-implement these two topicbased models and tune the parameter settings on our data set. Besides, we also introduce a baseline language model (LM) (Zhai and Lafferty, 2001) for comparison. Table 2 shows the question retrieval performance by using different evaluation metrics. From this table, we can see that learning continuous word embedding representations (Skip-gram + FV, M-NET + FV) for question retrieval can outperform the translation-based approaches and topic-based approaches on all evaluation metrics. We conduct a s</context>
</contexts>
<marker>Ji, Xu, Wang, He, 2012</marker>
<rawString>Zongcheng Ji, Fei Xu, Bin Wang, and Ben He. 2012. Question-answer topic model for question retrieval in community question answering. In Proceedings of CIKM, pages 2471–2474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jung-Tae Lee</author>
<author>Sang-Bum Kim</author>
<author>Young-In Song</author>
<author>Hae-Chang Rim</author>
</authors>
<title>Bridging lexical gaps between queries and questions on large online q&amp;a collections with compact translation models.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>410--418</pages>
<contexts>
<context position="2416" citStr="Lee et al., 2008" startWordPosition="368" endWordPosition="371">a made 1http://answers.yahoo.com/ 2http://qna.live.com/ 3http://zhidao.baidu.com/ cQA archives valuable resources for various tasks like question-answering (Jeon et al., 2005; Xue et al., 2008) and knowledge mining (Adamic et al., 2008), etc. One fundamental task for reusing content in cQA is finding similar questions for queried questions, as questions are the keys to accessing the knowledge in cQA. Then the best answers of these similar questions will be used to answer the queried questions. Many studies have been done along this line (Jeon et al., 2005; Xue et al., 2008; Duan et al., 2008; Lee et al., 2008; Bernhard and Gurevych, 2009; Cao et al., 2010; Zhou et al., 2011; Singh, 2012; Zhang et al., 2014a). One big challenge for question retrieval in cQA is the lexical gap between the queried questions and the existing questions in the archives. Lexical gap means that the queried questions may contain words that are different from, but related to, the words in the existing questions. For example shown in (Zhang et al., 2014a), we find that for a queried question “how do I get knots out of my cats fur?”, there are good answers under an existing question “how can I remove a tangle in my cat’s fur?</context>
<context position="7362" citStr="Lee et al., 2008" startWordPosition="1152" endWordPosition="1155"> is more amenable to large-scale processing; (3) an empirical verification of the efficacy of the proposed framework on large-scale English and Chinese cQA data. The rest of this paper is organized as follows. Section 2 summarizes the related work. Section 3 describes our proposed framework for question retrieval. Section 4 reports the experimental results. Finally, we conclude the paper in Section 5. 2 Related Work 2.1 Question Retrieval in cQA Significant research efforts have been conducted over the years in attempt to improve question retrieval in cQA (Jeon et al., 2005; Xue et al., 2008; Lee et al., 2008; Duan et al., 2008; Bernhard and Gurevych, 2009; Cao et al., 2010; Zhou et al., 2011; Singh, 2012; Zhang et al., 2014a). Most of these works focus on finding similar questions for the user queried questions. The major challenge for question retrieval in cQA is the lexical gap problem. Jeon et al. (2005) proposed a wordbased translation model for automatically fixing the lexical gap problem. Xue et al. (2008) proposed a word-based translation language model for question retrieval. Lee et al. (2008) tried to further improve the translation probabilities based on question-answer pairs by selecti</context>
</contexts>
<marker>Lee, Kim, Song, Rim, 2008</marker>
<rawString>Jung-Tae Lee, Sang-Bum Kim, Young-In Song, and Hae-Chang Rim. 2008. Bridging lexical gaps between queries and questions on large online q&amp;a collections with compact translation models. In Proceedings of EMNLP, pages 410–418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew L Maas</author>
<author>Raymond E Daly</author>
<author>Peter T Pham</author>
<author>Dan Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Learning word vectors for sentiment analysis.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>142--150</pages>
<contexts>
<context position="10378" citStr="Maas et al., 2011" startWordPosition="1632" endWordPosition="1635"> a series of works applied deep learning techniques to learn high-quality word representations. Bengio et al. (2003) proposed a probabilistic neural network language model (NNLM) for word representations. Furthermore, Mikolov et al. (2013) proposed efficient neural network models for learning word representations, including the continuous skip-gram model and the continuous bag-ofword model (CBOW), both of which are unsupervised models learned from large-scale text corpora. Besides, there are also a large number of works addressing the task of learning word representations (Huang et al., 2012; Maas et al., 2011; Turian et al., 2010). Nevertheless, since most the existing works learned word representations mainly based on the word co-occurrence information, the obtained word embeddings cannot capture the relationship between two syntactically or semantically similar words if either of them yields very little context information. On the other hand, even though amount of context could be noisy or biased such that they cannot reflect the inherent relationship between words and further mislead the training process. Most recently, Yu et al. (2014) used semantic prior knowledge to improve word representati</context>
</contexts>
<marker>Maas, Daly, Pham, Huang, Ng, Potts, 2011</marker>
<rawString>Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings ofACL, pages 142–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="5406" citStr="Mikolov et al., 2013" startWordPosition="844" endWordPosition="847">ver, questions and answers are heterogeneous in many aspects, they do not share the same topic distribution in practice. Inspired by the recent success of continuous space word representations in capturing the semantic similarities in various natural language processing tasks, we propose to incorporate an embedding of words in a continuous space for question representations. Due to the ability of word embeddings, we firstly transform words in a question into continuous vector representations by looking up tables. These word embeddings are learned in advance using a continuous skip-gram model (Mikolov et al., 2013), or other continuous word representation learning methods. Once the words are embedded in a continuous space, one can view a question as a Bag-of-Embedded-Words (BoEW). Then, the variable-cardinality BoEW will be aggregated into a fixed-length vector by using the Fisher kernel (FK) framework of (Clinchant and Perronnin, 2013; Sanchez et al., 2013). Through the two steps, the proposed approach can map a question into a length invariable compact vector, which can be efficiently and effectively for large-scale question retrieval task in cQA. We test the proposed approach on large-scale Yahoo! An</context>
<context position="10000" citStr="Mikolov et al. (2013)" startWordPosition="1570" endWordPosition="1573">13) exploited the category metadata within cQA pages to further improve the performance. On the contrary, we focus on the representation learning for questions, with a different solution with those previous works. 2.2 Word Embedding Learning Representation of words as continuous vectors has attracted increasing attention in the area of natural language processing (NLP). Recently, a series of works applied deep learning techniques to learn high-quality word representations. Bengio et al. (2003) proposed a probabilistic neural network language model (NNLM) for word representations. Furthermore, Mikolov et al. (2013) proposed efficient neural network models for learning word representations, including the continuous skip-gram model and the continuous bag-ofword model (CBOW), both of which are unsupervised models learned from large-scale text corpora. Besides, there are also a large number of works addressing the task of learning word representations (Huang et al., 2012; Maas et al., 2011; Turian et al., 2010). Nevertheless, since most the existing works learned word representations mainly based on the word co-occurrence information, the obtained word embeddings cannot capture the relationship between two </context>
<context position="12750" citStr="Mikolov et al., 2013" startWordPosition="2003" endWordPosition="2006">alculating the similarity between the FVs of a queried question and an existing question in the archive. From the framework, we can see that although the word embedding learning computations and generative model estimation are time consuming, they can run only once in advance. Meanwhile, the computational requirements of FV generation and similarity calculation are limited. Hence, the proposed framework can efficiently achieve the largescale question retrieval task. 3.1 Word Embedding Learning In this paper, we consider a context-aware predicting model, more specifically, the Skip-gram model (Mikolov et al., 2013) for learning word embeddings, since it is much more efficient as well as memory-saving than other approaches.4 Skipgram is recently proposed for learning word representations using a neural network model, whose underlying idea is that similar words should have similar contexts. In the Skip-gram model (see Figure 1), a sliding window is employed on the input text stream to generate the training data, and l indicates the context window size to be 2l + 1. In each slide window, the model aims to use the central word wk as input to predict the context words. Let Md×N denote the learned embedding m</context>
<context position="14405" citStr="Mikolov et al. (2013)" startWordPosition="2302" endWordPosition="2305">lumn vector of M. The probability of its context word wk+j is then computed using a log-linear softmax function: 1 E7w 1 exp(eTwewk) where θ are the parameters we should learned, k = 1 · · · d, and j E [−l, l]. Then, the log-likelihood over the entire training data can be computed as: J(θ) = E logp(wk+j|wk; θ) (2) (wk,wk+j) To calculate the prediction errors for back propagation, we need to compute the derivative of p(wk+j|wk; θ), whose computation cost is proportional to the vocabulary size N. As N is often very large, it is difficult to directly compute the derivative. To deal this problem, Mikolov et al. (2013) proposed a simple negative sampling method, which generates r noise samples for each input word to estimate the target word, in which r is a very small number compared with N. Therefore, the training time yields linear scale to the number of noise samples and it becomes independent of the vocabulary size. Suppose the frequency of word w is u(w), then the probability of sampling w is usually set to p(w) a u(w)3/4 (Mikolov et al., 2013). 3.2 Metadata Powered Model After briefing the skip-gram model, we introduce how we equip it with the metadata information. In cQA sites, there are several meta</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Proceedings of NIPS, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Er Vasserman</author>
<author>Ioannis Tsochantaridis</author>
<author>Vibhu Mittal</author>
<author>Yi Liu</author>
</authors>
<title>Statistical machine translation for query expansion in answer retrieval.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="8308" citStr="Riezler et al. (2007)" startWordPosition="1306" endWordPosition="1309">d translation model for automatically fixing the lexical gap problem. Xue et al. (2008) proposed a word-based translation language model for question retrieval. Lee et al. (2008) tried to further improve the translation probabilities based on question-answer pairs by selecting the most important terms to build compact translation models. Bernhard and Gurevych (2009) proposed to use as a parallel training data set the definitions and glosses provided for the same term by different lexical semantic resources. In order to improve the word-based translation model with some contextual information, Riezler et al. (2007) and Zhou et al. (2011) proposed a phrase-based translation model for question and answer retrieval. The phrase-based translation model can capture some contextual information in modeling the translation of phrases as a whole, thus the more accurate translations can better improve the retrieval performance. Singh (2012) addressed the lexical gap issues by extending the lexical word-based translation model to incorporate semantic information (entities). In contrast to the works described above that assume question-answer pairs are “parallel text”, our paper deals with the lexical gap by learnin</context>
</contexts>
<marker>Riezler, Vasserman, Tsochantaridis, Mittal, Liu, 2007</marker>
<rawString>Stefan Riezler, Er Vasserman, Ioannis Tsochantaridis, Vibhu Mittal, and Yi Liu. 2007. Statistical machine translation for query expansion in answer retrieval. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Robertson</author>
<author>S Walker</author>
<author>S Jones</author>
<author>M HancockBeaulieu</author>
<author>M Gatford</author>
</authors>
<title>Okapi at trec-3.</title>
<date>1994</date>
<booktitle>In Proceedings of TREC,</booktitle>
<pages>109--126</pages>
<contexts>
<context position="3204" citStr="Robertson et al., 1994" startWordPosition="508" endWordPosition="511"> gap between the queried questions and the existing questions in the archives. Lexical gap means that the queried questions may contain words that are different from, but related to, the words in the existing questions. For example shown in (Zhang et al., 2014a), we find that for a queried question “how do I get knots out of my cats fur?”, there are good answers under an existing question “how can I remove a tangle in my cat’s fur?” in Yahoo! Answers. Although the two questions share few words in common, they have very similar meanings, it is hard for traditional retrieval models (e.g., BM25 (Robertson et al., 1994)) to determine their similarity. This lexical gap has become a major barricade preventing traditional IR models (e.g., BM25) from retrieving similar questions in cQA. To address the lexical gap problem in cQA, previous work in the literature can be divided into two groups. The first group is the translation models, which leverage the question-answer pairs to learn the semantically related words to improve traditional IR models (Jeon et al., 2005; Xue et al., 2008; Zhou et al., 2011). The basic assumption is that question-answer pairs are “parallel texts” and relationship of words (or phrases) </context>
</contexts>
<marker>Robertson, Walker, Jones, HancockBeaulieu, Gatford, 1994</marker>
<rawString>S. Robertson, S. Walker, S. Jones, M. HancockBeaulieu, and M. Gatford. 1994. Okapi at trec-3. In Proceedings of TREC, pages 109–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorge Sanchez</author>
<author>Florent Perronnin</author>
<author>Thomas Mensink</author>
<author>Jakob J Verbeek</author>
</authors>
<title>Image classification with the fisher vector: Theory and practice.</title>
<date>2013</date>
<journal>International Journal of Computer Vision,</journal>
<pages>222--245</pages>
<contexts>
<context position="5756" citStr="Sanchez et al., 2013" startWordPosition="897" endWordPosition="900">r question representations. Due to the ability of word embeddings, we firstly transform words in a question into continuous vector representations by looking up tables. These word embeddings are learned in advance using a continuous skip-gram model (Mikolov et al., 2013), or other continuous word representation learning methods. Once the words are embedded in a continuous space, one can view a question as a Bag-of-Embedded-Words (BoEW). Then, the variable-cardinality BoEW will be aggregated into a fixed-length vector by using the Fisher kernel (FK) framework of (Clinchant and Perronnin, 2013; Sanchez et al., 2013). Through the two steps, the proposed approach can map a question into a length invariable compact vector, which can be efficiently and effectively for large-scale question retrieval task in cQA. We test the proposed approach on large-scale Yahoo! Answers data and Baidu Zhidao data. Yahoo! Answers and Baidu Zhidao represent the largest and most popular cQA archives in English and Chinese, respectively. We conduct both quantitative and qualitative evaluations. Experimental results show that our approach can significantly outperform state-of-the-art translation models and topic-based models for </context>
<context position="19299" citStr="Sanchez et al., 2013" startWordPosition="3138" endWordPosition="3141"> field are not suitable for this kinds of issues, BoEWs cannot be directly used for large-scale question retrieval task. Given a cQA data collection Q = {qi,1 ≤ i ≤ |Q|}, where qi is the ith question and |Q |is the number of questions in the data collection. The ith question qi is composed by a sequence of words wi = {wij,1 ≤ j ≤ Ni}, where Ni denotes the length of qi. Through looking up table (word embedding matrix) of M, the ith question qi can be represented by Ewi = {ewij,1 ≤ j ≤ Ni}, where ewij is the word embedding of wij. According to the framework of FK (Clinchant and Perronnin, 2013; Sanchez et al., 2013; Zhang et al., 2014b), questions are modeled by a probability density function. In this work, we use Gaussian mixture model (GMM) to do it. We assume that the continuous word embedding Ewi for question qi have been generated by a “universal” (e.g., questionindependent) probability density function (pdf). As is a common practice, we choose this pdf to be a GMM since any continuous distribution can be approximated with arbitrary precision by a mixture of Gaussian. In what follows, the pdf is denoted uλ where λ = {θi, µi, Σi, i = 1 · · · K} is the set of parameters of the GMM. θi, µi and Σi deno</context>
<context position="20834" citStr="Sanchez et al., 2013" startWordPosition="3405" endWordPosition="3408">rd embeddings extracted from a representative set of questions. The parameters λ are estimated through the optimization of a Maximum Likelihood (ML) criterion using the Expectation-Maximization (EM) algorithm. In the following, we follow the notations used in (Sanchez et al., 2013). Given uλ, one can characterize the question qi using the following score function: Gqiλ = 5Ni λ loguλ(qi) (6) where Gqiλ is a vector whose size depends only on the number of parameters in λ. Assuming that the word embedding ewij is iid (a simplifying assumption), we get: 5λloguλ(ewij) (7) Following the literature (Sanchez et al., 2013), we propose to measure the similarity between two questions qi and qj using the FK: K(qi, qj) = Ga Fλ1Ga (g) where Fλ is the Fisher Information Matrix (FIM) of uλ: (9) Since Fλ is symmetric and positive definite, F−1 λ can be transformed to LT λ Lλ based on the Cholesky decomposition. Hence, KFK(qi, qj) can rewritten as follows: KFK(qi, qj) = GqT λ Gqj i (10) λ ... ... ... ... ... Ni L j=1 Gqi λ = � Fλ = Eqi∼uλ LGqiλ GqTiλ 254 where Gqi λ =LλGqiλ = Lλ 5λ loguλ(qi) (11) In (Sanchez et al., 2013), Gqi λ refers to as the Fisher Vector (FV) of qi. The dot product between FVs can be used to calcul</context>
</contexts>
<marker>Sanchez, Perronnin, Mensink, Verbeek, 2013</marker>
<rawString>Jorge Sanchez, Florent Perronnin, Thomas Mensink, and Jakob J. Verbeek. 2013. Image classification with the fisher vector: Theory and practice. International Journal of Computer Vision, pages 222–245.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Singh</author>
</authors>
<title>Entity based q&amp;a retrieval.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1266--1277</pages>
<contexts>
<context position="2495" citStr="Singh, 2012" startWordPosition="384" endWordPosition="385">A archives valuable resources for various tasks like question-answering (Jeon et al., 2005; Xue et al., 2008) and knowledge mining (Adamic et al., 2008), etc. One fundamental task for reusing content in cQA is finding similar questions for queried questions, as questions are the keys to accessing the knowledge in cQA. Then the best answers of these similar questions will be used to answer the queried questions. Many studies have been done along this line (Jeon et al., 2005; Xue et al., 2008; Duan et al., 2008; Lee et al., 2008; Bernhard and Gurevych, 2009; Cao et al., 2010; Zhou et al., 2011; Singh, 2012; Zhang et al., 2014a). One big challenge for question retrieval in cQA is the lexical gap between the queried questions and the existing questions in the archives. Lexical gap means that the queried questions may contain words that are different from, but related to, the words in the existing questions. For example shown in (Zhang et al., 2014a), we find that for a queried question “how do I get knots out of my cats fur?”, there are good answers under an existing question “how can I remove a tangle in my cat’s fur?” in Yahoo! Answers. Although the two questions share few words in common, they</context>
<context position="7460" citStr="Singh, 2012" startWordPosition="1172" endWordPosition="1173">sed framework on large-scale English and Chinese cQA data. The rest of this paper is organized as follows. Section 2 summarizes the related work. Section 3 describes our proposed framework for question retrieval. Section 4 reports the experimental results. Finally, we conclude the paper in Section 5. 2 Related Work 2.1 Question Retrieval in cQA Significant research efforts have been conducted over the years in attempt to improve question retrieval in cQA (Jeon et al., 2005; Xue et al., 2008; Lee et al., 2008; Duan et al., 2008; Bernhard and Gurevych, 2009; Cao et al., 2010; Zhou et al., 2011; Singh, 2012; Zhang et al., 2014a). Most of these works focus on finding similar questions for the user queried questions. The major challenge for question retrieval in cQA is the lexical gap problem. Jeon et al. (2005) proposed a wordbased translation model for automatically fixing the lexical gap problem. Xue et al. (2008) proposed a word-based translation language model for question retrieval. Lee et al. (2008) tried to further improve the translation probabilities based on question-answer pairs by selecting the most important terms to build compact translation models. Bernhard and Gurevych (2009) prop</context>
</contexts>
<marker>Singh, 2012</marker>
<rawString>A. Singh. 2012. Entity based q&amp;a retrieval. In Proceedings of EMNLP, pages 1266–1277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Surdeanu</author>
<author>M Ciaramita</author>
<author>H Zaragoza</author>
</authors>
<title>Learning to rank answers on large online qa collections.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>719--727</pages>
<contexts>
<context position="9308" citStr="Surdeanu et al., 2008" startWordPosition="1463" endWordPosition="1466"> word-based translation model to incorporate semantic information (entities). In contrast to the works described above that assume question-answer pairs are “parallel text”, our paper deals with the lexical gap by learning con251 tinuous word embeddings in capturing the similarities without any assumptions, which is much more reasonable in practice. Besides, some other studies model the semantic relationship between questions and answers with deep linguistic analysis (Duan et al., 2008; Wang et al., 2009; Wang et al., 2010; Ji et al., 2012; Zhang et al., 2014a) or a learning to rank strategy (Surdeanu et al., 2008; Carmel et al., 2014). Recently, Cao et al. (2010) and Zhou et al. (2013) exploited the category metadata within cQA pages to further improve the performance. On the contrary, we focus on the representation learning for questions, with a different solution with those previous works. 2.2 Word Embedding Learning Representation of words as continuous vectors has attracted increasing attention in the area of natural language processing (NLP). Recently, a series of works applied deep learning techniques to learn high-quality word representations. Bengio et al. (2003) proposed a probabilistic neura</context>
</contexts>
<marker>Surdeanu, Ciaramita, Zaragoza, 2008</marker>
<rawString>M. Surdeanu, M. Ciaramita, and H. Zaragoza. 2008. Learning to rank answers on large online qa collections. In Proceedings ofACL, pages 719–727.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev-Arie Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: A simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="10400" citStr="Turian et al., 2010" startWordPosition="1636" endWordPosition="1639">applied deep learning techniques to learn high-quality word representations. Bengio et al. (2003) proposed a probabilistic neural network language model (NNLM) for word representations. Furthermore, Mikolov et al. (2013) proposed efficient neural network models for learning word representations, including the continuous skip-gram model and the continuous bag-ofword model (CBOW), both of which are unsupervised models learned from large-scale text corpora. Besides, there are also a large number of works addressing the task of learning word representations (Huang et al., 2012; Maas et al., 2011; Turian et al., 2010). Nevertheless, since most the existing works learned word representations mainly based on the word co-occurrence information, the obtained word embeddings cannot capture the relationship between two syntactically or semantically similar words if either of them yields very little context information. On the other hand, even though amount of context could be noisy or biased such that they cannot reflect the inherent relationship between words and further mislead the training process. Most recently, Yu et al. (2014) used semantic prior knowledge to improve word representations. Xu et al. (2014) </context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai Wang</author>
<author>Zhaoyan Ming</author>
<author>Tat-Seng Chua</author>
</authors>
<title>A syntactic tree matching approach to finding similar questions in community-based qa services.</title>
<date>2009</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>187--194</pages>
<contexts>
<context position="9196" citStr="Wang et al., 2009" startWordPosition="1440" endWordPosition="1443">er improve the retrieval performance. Singh (2012) addressed the lexical gap issues by extending the lexical word-based translation model to incorporate semantic information (entities). In contrast to the works described above that assume question-answer pairs are “parallel text”, our paper deals with the lexical gap by learning con251 tinuous word embeddings in capturing the similarities without any assumptions, which is much more reasonable in practice. Besides, some other studies model the semantic relationship between questions and answers with deep linguistic analysis (Duan et al., 2008; Wang et al., 2009; Wang et al., 2010; Ji et al., 2012; Zhang et al., 2014a) or a learning to rank strategy (Surdeanu et al., 2008; Carmel et al., 2014). Recently, Cao et al. (2010) and Zhou et al. (2013) exploited the category metadata within cQA pages to further improve the performance. On the contrary, we focus on the representation learning for questions, with a different solution with those previous works. 2.2 Word Embedding Learning Representation of words as continuous vectors has attracted increasing attention in the area of natural language processing (NLP). Recently, a series of works applied deep lea</context>
</contexts>
<marker>Wang, Ming, Chua, 2009</marker>
<rawString>Kai Wang, Zhaoyan Ming, and Tat-Seng Chua. 2009. A syntactic tree matching approach to finding similar questions in community-based qa services. In Proceedings of SIGIR, pages 187–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Wang</author>
<author>X Wang</author>
<author>C Sun</author>
<author>B Liu</author>
<author>L Sun</author>
</authors>
<title>Modeling semantic relevance for question-answer pairs in web social communities.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="9215" citStr="Wang et al., 2010" startWordPosition="1444" endWordPosition="1447">ieval performance. Singh (2012) addressed the lexical gap issues by extending the lexical word-based translation model to incorporate semantic information (entities). In contrast to the works described above that assume question-answer pairs are “parallel text”, our paper deals with the lexical gap by learning con251 tinuous word embeddings in capturing the similarities without any assumptions, which is much more reasonable in practice. Besides, some other studies model the semantic relationship between questions and answers with deep linguistic analysis (Duan et al., 2008; Wang et al., 2009; Wang et al., 2010; Ji et al., 2012; Zhang et al., 2014a) or a learning to rank strategy (Surdeanu et al., 2008; Carmel et al., 2014). Recently, Cao et al. (2010) and Zhou et al. (2013) exploited the category metadata within cQA pages to further improve the performance. On the contrary, we focus on the representation learning for questions, with a different solution with those previous works. 2.2 Word Embedding Learning Representation of words as continuous vectors has attracted increasing attention in the area of natural language processing (NLP). Recently, a series of works applied deep learning techniques to</context>
</contexts>
<marker>Wang, Wang, Sun, Liu, Sun, 2010</marker>
<rawString>B. Wang, X. Wang, C. Sun, B. Liu, and L. Sun. 2010. Modeling semantic relevance for question-answer pairs in web social communities. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chang Xu</author>
<author>Yalong Bai</author>
<author>Jiang Bian</author>
<author>Bin Gao</author>
<author>Gang Wang</author>
<author>Xiaoguang Liu</author>
<author>Tie-Yan Liu</author>
</authors>
<title>Rcnet: A general framework for incorporating knowledge into word representations.</title>
<date>2014</date>
<booktitle>In Proceedings of CIKM,</booktitle>
<pages>1219--1228</pages>
<contexts>
<context position="10999" citStr="Xu et al. (2014)" startWordPosition="1725" endWordPosition="1728">ian et al., 2010). Nevertheless, since most the existing works learned word representations mainly based on the word co-occurrence information, the obtained word embeddings cannot capture the relationship between two syntactically or semantically similar words if either of them yields very little context information. On the other hand, even though amount of context could be noisy or biased such that they cannot reflect the inherent relationship between words and further mislead the training process. Most recently, Yu et al. (2014) used semantic prior knowledge to improve word representations. Xu et al. (2014) used the knowledge graph to advance the learning of word embeddings. In contrast to all the aforementioned works, in this paper, we present a general method to leverage the metadata of category information within cQA pages to further improve the word embedding representations. To our knowledge, it is the first work to learn word embeddings with metadata on cQA data set. 3 Our Approach In this Section, we describe the proposed approach: learning continuous word embedding with metadata for question retrieval in cQA. The proposed framework consists of two steps: (1) word embedding learning step:</context>
</contexts>
<marker>Xu, Bai, Bian, Gao, Wang, Liu, Liu, 2014</marker>
<rawString>Chang Xu, Yalong Bai, Jiang Bian, Bin Gao, Gang Wang, Xiaoguang Liu, and Tie-Yan Liu. 2014. Rcnet: A general framework for incorporating knowledge into word representations. In Proceedings of CIKM, pages 1219–1228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaobing Xue</author>
<author>Jiwoon Jeon</author>
<author>W Bruce Croft</author>
</authors>
<title>Retrieval models for question and answer archives.</title>
<date>2008</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>475--482</pages>
<contexts>
<context position="1993" citStr="Xue et al., 2008" startWordPosition="293" endWordPosition="296">t information resource on the web. These include the traditional Frequently Asked Questions (FAQ) archives and the emerging community question answering (cQA) services, such as Yahoo! Answers1, Live QnA2, and Baidu Zhidao3. The content in these web sites is usually organized as questions and lists of answers associated with metadata like user chosen categories to questions and askers’ awards to the best answers. This data made 1http://answers.yahoo.com/ 2http://qna.live.com/ 3http://zhidao.baidu.com/ cQA archives valuable resources for various tasks like question-answering (Jeon et al., 2005; Xue et al., 2008) and knowledge mining (Adamic et al., 2008), etc. One fundamental task for reusing content in cQA is finding similar questions for queried questions, as questions are the keys to accessing the knowledge in cQA. Then the best answers of these similar questions will be used to answer the queried questions. Many studies have been done along this line (Jeon et al., 2005; Xue et al., 2008; Duan et al., 2008; Lee et al., 2008; Bernhard and Gurevych, 2009; Cao et al., 2010; Zhou et al., 2011; Singh, 2012; Zhang et al., 2014a). One big challenge for question retrieval in cQA is the lexical gap between</context>
<context position="3671" citStr="Xue et al., 2008" startWordPosition="584" endWordPosition="587">uestions share few words in common, they have very similar meanings, it is hard for traditional retrieval models (e.g., BM25 (Robertson et al., 1994)) to determine their similarity. This lexical gap has become a major barricade preventing traditional IR models (e.g., BM25) from retrieving similar questions in cQA. To address the lexical gap problem in cQA, previous work in the literature can be divided into two groups. The first group is the translation models, which leverage the question-answer pairs to learn the semantically related words to improve traditional IR models (Jeon et al., 2005; Xue et al., 2008; Zhou et al., 2011). The basic assumption is that question-answer pairs are “parallel texts” and relationship of words (or phrases) can be established through word-to-word (or phrase-to-phrase) 250 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 250–259, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics translation probabilities (Jeon et al., 2005; Xue et al., 2008; Zhou et al., 2011). Experimental results show that translation models obtain</context>
<context position="7344" citStr="Xue et al., 2008" startWordPosition="1148" endWordPosition="1151">ength vector which is more amenable to large-scale processing; (3) an empirical verification of the efficacy of the proposed framework on large-scale English and Chinese cQA data. The rest of this paper is organized as follows. Section 2 summarizes the related work. Section 3 describes our proposed framework for question retrieval. Section 4 reports the experimental results. Finally, we conclude the paper in Section 5. 2 Related Work 2.1 Question Retrieval in cQA Significant research efforts have been conducted over the years in attempt to improve question retrieval in cQA (Jeon et al., 2005; Xue et al., 2008; Lee et al., 2008; Duan et al., 2008; Bernhard and Gurevych, 2009; Cao et al., 2010; Zhou et al., 2011; Singh, 2012; Zhang et al., 2014a). Most of these works focus on finding similar questions for the user queried questions. The major challenge for question retrieval in cQA is the lexical gap problem. Jeon et al. (2005) proposed a wordbased translation model for automatically fixing the lexical gap problem. Xue et al. (2008) proposed a word-based translation language model for question retrieval. Lee et al. (2008) tried to further improve the translation probabilities based on question-answe</context>
<context position="28763" citStr="Xue et al., 2008" startWordPosition="4757" endWordPosition="4760">we present the experimental results on the test sets of Yahoo data and Baidu data. We compare the baseline word embedding trained by Skip-gram against this trained by M-NET. The dimension of word embedding is set as 50,100 and 300. Since the motivation of this paper attempts to tackle the lexical gap problem for queried questions and questions in the archive, we also compare them with the two groups of methods which also address the lexical gap in the literature. The first group is the translation models: word-based translation model (Jeon et al., 2005), word-based translation language model (Xue et al., 2008), and phrase-based translation model (Zhou et al., 2011). We implement those three translation models based on the original papers and train those models with (question, best answer) pairs from the Yahoo! Webscope dataset Yahoo answers and the 1 billion web pages of Baidu Zhidao for English and Chinese, respectively. Training the translation models with different pairs (e.g., question-best answer, question-description, question-answer) may achieve inconsistent performance on Yahoo data and Baidu data, but its comparison and analysis are beyond the scope of this paper. The second group is the t</context>
<context position="30305" citStr="Xue et al., 2008" startWordPosition="4995" endWordPosition="4998"> comparison. Table 2 shows the question retrieval performance by using different evaluation metrics. From this table, we can see that learning continuous word embedding representations (Skip-gram + FV, M-NET + FV) for question retrieval can outperform the translation-based approaches and topic-based approaches on all evaluation metrics. We conduct a statistical test (t-test), the results 256 Model dim Yahoo data Baidu data MAP MRR R-Prec P@5 MAP MRR R-Prec P@5 LM (baseline) - 0.435 0.472 0.381 0.305 0.392 0.413 0.325 0.247 (Jeon et al., 2005) - 0.463 0.495 0.396 0.332 0.414 0.428 0.341 0.256 (Xue et al., 2008) - 0.518 0.560 0.423 0.346 0.431 0.435 0.352 0.264 (Zhou et al., 2011) - 0.536 0.587 0.439 0.361 0.448 0.450 0.367 0.273 (Ji et al., 2012) - 0.508 0.544 0.405 0.324 0.425 0.431 0.349 0.258 (Zhang et al., 2014a) - 0.527 0.572 0.433 0.350 0.443 0.446 0.358 0.265 50 0.532 0.583 0.437 0.358 0.447 0.450 0.366 0.272 Skip-gram + FV 100 0.544 0.6051 0.440 0.363 0.454 0.457 0.373 0.274 300 0.5501 0.6191 0.444 0.365 0.4601 0.4641 0.374 0.277 50 0.5481 0.6121 0.441 0.363 0.4591 0.4621 0.374 0.276 M-NET + FV 100 0.562$ 0.628$ 0.4521 0.367$ 0.468$ 0.471 0.3781 0.2801 300 0.571$ 0.643$ 0.455$ 0.374$ 0.475$ </context>
</contexts>
<marker>Xue, Jeon, Croft, 2008</marker>
<rawString>Xiaobing Xue, Jiwoon Jeon, and W. Bruce Croft. 2008. Retrieval models for question and answer archives. In Proceedings of SIGIR, pages 475–482.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mo Yu</author>
<author>Mark Dredze</author>
</authors>
<title>Improving lexical embeddings with semantic knowledge.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>545--550</pages>
<marker>Yu, Dredze, 2014</marker>
<rawString>Mo Yu and Mark Dredze. 2014. Improving lexical embeddings with semantic knowledge. In Proceedings of ACL, pages 545–550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chengxiang Zhai</author>
<author>John Lafferty</author>
</authors>
<title>A study of smoothing methods for language models applied to ad hoc information retrieval.</title>
<date>2001</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>334--342</pages>
<contexts>
<context position="29684" citStr="Zhai and Lafferty, 2001" startWordPosition="4895" endWordPosition="4898">hinese, respectively. Training the translation models with different pairs (e.g., question-best answer, question-description, question-answer) may achieve inconsistent performance on Yahoo data and Baidu data, but its comparison and analysis are beyond the scope of this paper. The second group is the topic-based methods: unsupervised question-answer topic model (Ji et al., 2012) and supervised question-answer topic model (Zhang et al., 2014a). We re-implement these two topicbased models and tune the parameter settings on our data set. Besides, we also introduce a baseline language model (LM) (Zhai and Lafferty, 2001) for comparison. Table 2 shows the question retrieval performance by using different evaluation metrics. From this table, we can see that learning continuous word embedding representations (Skip-gram + FV, M-NET + FV) for question retrieval can outperform the translation-based approaches and topic-based approaches on all evaluation metrics. We conduct a statistical test (t-test), the results 256 Model dim Yahoo data Baidu data MAP MRR R-Prec P@5 MAP MRR R-Prec P@5 LM (baseline) - 0.435 0.472 0.381 0.305 0.392 0.413 0.325 0.247 (Jeon et al., 2005) - 0.463 0.495 0.396 0.332 0.414 0.428 0.341 0.2</context>
</contexts>
<marker>Zhai, Lafferty, 2001</marker>
<rawString>Chengxiang Zhai and John Lafferty. 2001. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proceedings of SIGIR, pages 334–342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai Zhang</author>
<author>Wei Wu</author>
<author>Haocheng Wu</author>
<author>Zhoujun Li</author>
<author>Ming Zhou</author>
</authors>
<title>Question retrieval with high quality answers in community question answering.</title>
<date>2014</date>
<booktitle>In Proceedings of CIKM,</booktitle>
<pages>371--380</pages>
<contexts>
<context position="2515" citStr="Zhang et al., 2014" startWordPosition="386" endWordPosition="389">luable resources for various tasks like question-answering (Jeon et al., 2005; Xue et al., 2008) and knowledge mining (Adamic et al., 2008), etc. One fundamental task for reusing content in cQA is finding similar questions for queried questions, as questions are the keys to accessing the knowledge in cQA. Then the best answers of these similar questions will be used to answer the queried questions. Many studies have been done along this line (Jeon et al., 2005; Xue et al., 2008; Duan et al., 2008; Lee et al., 2008; Bernhard and Gurevych, 2009; Cao et al., 2010; Zhou et al., 2011; Singh, 2012; Zhang et al., 2014a). One big challenge for question retrieval in cQA is the lexical gap between the queried questions and the existing questions in the archives. Lexical gap means that the queried questions may contain words that are different from, but related to, the words in the existing questions. For example shown in (Zhang et al., 2014a), we find that for a queried question “how do I get knots out of my cats fur?”, there are good answers under an existing question “how can I remove a tangle in my cat’s fur?” in Yahoo! Answers. Although the two questions share few words in common, they have very similar m</context>
<context position="4494" citStr="Zhang et al., 2014" startWordPosition="702" endWordPosition="705">dings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 250–259, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics translation probabilities (Jeon et al., 2005; Xue et al., 2008; Zhou et al., 2011). Experimental results show that translation models obtain stateof-the-art performance for question retrieval in cQA. However, questions and answers are far from “parallel” in practice, questions and answers are highly asymmetric on the information they contain (Zhang et al., 2014a). The second group is the topic-based models (Cai et al., 2011; Ji et al., 2012), which learn the latent topics aligned across the question-answer pairs to alleviate the lexical gap problem, with the assumption that a question and its paired answers share the same topic distribution. However, questions and answers are heterogeneous in many aspects, they do not share the same topic distribution in practice. Inspired by the recent success of continuous space word representations in capturing the semantic similarities in various natural language processing tasks, we propose to incorporate an em</context>
<context position="7480" citStr="Zhang et al., 2014" startWordPosition="1174" endWordPosition="1177"> on large-scale English and Chinese cQA data. The rest of this paper is organized as follows. Section 2 summarizes the related work. Section 3 describes our proposed framework for question retrieval. Section 4 reports the experimental results. Finally, we conclude the paper in Section 5. 2 Related Work 2.1 Question Retrieval in cQA Significant research efforts have been conducted over the years in attempt to improve question retrieval in cQA (Jeon et al., 2005; Xue et al., 2008; Lee et al., 2008; Duan et al., 2008; Bernhard and Gurevych, 2009; Cao et al., 2010; Zhou et al., 2011; Singh, 2012; Zhang et al., 2014a). Most of these works focus on finding similar questions for the user queried questions. The major challenge for question retrieval in cQA is the lexical gap problem. Jeon et al. (2005) proposed a wordbased translation model for automatically fixing the lexical gap problem. Xue et al. (2008) proposed a word-based translation language model for question retrieval. Lee et al. (2008) tried to further improve the translation probabilities based on question-answer pairs by selecting the most important terms to build compact translation models. Bernhard and Gurevych (2009) proposed to use as a par</context>
<context position="9252" citStr="Zhang et al., 2014" startWordPosition="1452" endWordPosition="1455">essed the lexical gap issues by extending the lexical word-based translation model to incorporate semantic information (entities). In contrast to the works described above that assume question-answer pairs are “parallel text”, our paper deals with the lexical gap by learning con251 tinuous word embeddings in capturing the similarities without any assumptions, which is much more reasonable in practice. Besides, some other studies model the semantic relationship between questions and answers with deep linguistic analysis (Duan et al., 2008; Wang et al., 2009; Wang et al., 2010; Ji et al., 2012; Zhang et al., 2014a) or a learning to rank strategy (Surdeanu et al., 2008; Carmel et al., 2014). Recently, Cao et al. (2010) and Zhou et al. (2013) exploited the category metadata within cQA pages to further improve the performance. On the contrary, we focus on the representation learning for questions, with a different solution with those previous works. 2.2 Word Embedding Learning Representation of words as continuous vectors has attracted increasing attention in the area of natural language processing (NLP). Recently, a series of works applied deep learning techniques to learn high-quality word representati</context>
<context position="19319" citStr="Zhang et al., 2014" startWordPosition="3142" endWordPosition="3145">e for this kinds of issues, BoEWs cannot be directly used for large-scale question retrieval task. Given a cQA data collection Q = {qi,1 ≤ i ≤ |Q|}, where qi is the ith question and |Q |is the number of questions in the data collection. The ith question qi is composed by a sequence of words wi = {wij,1 ≤ j ≤ Ni}, where Ni denotes the length of qi. Through looking up table (word embedding matrix) of M, the ith question qi can be represented by Ewi = {ewij,1 ≤ j ≤ Ni}, where ewij is the word embedding of wij. According to the framework of FK (Clinchant and Perronnin, 2013; Sanchez et al., 2013; Zhang et al., 2014b), questions are modeled by a probability density function. In this work, we use Gaussian mixture model (GMM) to do it. We assume that the continuous word embedding Ewi for question qi have been generated by a “universal” (e.g., questionindependent) probability density function (pdf). As is a common practice, we choose this pdf to be a GMM since any continuous distribution can be approximated with arbitrary precision by a mixture of Gaussian. In what follows, the pdf is denoted uλ where λ = {θi, µi, Σi, i = 1 · · · K} is the set of parameters of the GMM. θi, µi and Σi denote respectively the </context>
<context position="29504" citStr="Zhang et al., 2014" startWordPosition="4866" endWordPosition="4869"> papers and train those models with (question, best answer) pairs from the Yahoo! Webscope dataset Yahoo answers and the 1 billion web pages of Baidu Zhidao for English and Chinese, respectively. Training the translation models with different pairs (e.g., question-best answer, question-description, question-answer) may achieve inconsistent performance on Yahoo data and Baidu data, but its comparison and analysis are beyond the scope of this paper. The second group is the topic-based methods: unsupervised question-answer topic model (Ji et al., 2012) and supervised question-answer topic model (Zhang et al., 2014a). We re-implement these two topicbased models and tune the parameter settings on our data set. Besides, we also introduce a baseline language model (LM) (Zhai and Lafferty, 2001) for comparison. Table 2 shows the question retrieval performance by using different evaluation metrics. From this table, we can see that learning continuous word embedding representations (Skip-gram + FV, M-NET + FV) for question retrieval can outperform the translation-based approaches and topic-based approaches on all evaluation metrics. We conduct a statistical test (t-test), the results 256 Model dim Yahoo data </context>
</contexts>
<marker>Zhang, Wu, Wu, Li, Zhou, 2014</marker>
<rawString>Kai Zhang, Wei Wu, Haocheng Wu, Zhoujun Li, and Ming Zhou. 2014a. Question retrieval with high quality answers in community question answering. In Proceedings of CIKM, pages 371–380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Zhang</author>
<author>Jihua Kang</author>
<author>Jin Qian</author>
<author>Xuanjing Huang</author>
</authors>
<title>Continuous word embeddings for detecting local text reuses at the semantic level.</title>
<date>2014</date>
<booktitle>In Proceedings of the 37th International ACM SIGIR Conference on Research &amp; Development in Information Retrieval, SIGIR ’14,</booktitle>
<pages>797--806</pages>
<contexts>
<context position="2515" citStr="Zhang et al., 2014" startWordPosition="386" endWordPosition="389">luable resources for various tasks like question-answering (Jeon et al., 2005; Xue et al., 2008) and knowledge mining (Adamic et al., 2008), etc. One fundamental task for reusing content in cQA is finding similar questions for queried questions, as questions are the keys to accessing the knowledge in cQA. Then the best answers of these similar questions will be used to answer the queried questions. Many studies have been done along this line (Jeon et al., 2005; Xue et al., 2008; Duan et al., 2008; Lee et al., 2008; Bernhard and Gurevych, 2009; Cao et al., 2010; Zhou et al., 2011; Singh, 2012; Zhang et al., 2014a). One big challenge for question retrieval in cQA is the lexical gap between the queried questions and the existing questions in the archives. Lexical gap means that the queried questions may contain words that are different from, but related to, the words in the existing questions. For example shown in (Zhang et al., 2014a), we find that for a queried question “how do I get knots out of my cats fur?”, there are good answers under an existing question “how can I remove a tangle in my cat’s fur?” in Yahoo! Answers. Although the two questions share few words in common, they have very similar m</context>
<context position="4494" citStr="Zhang et al., 2014" startWordPosition="702" endWordPosition="705">dings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 250–259, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics translation probabilities (Jeon et al., 2005; Xue et al., 2008; Zhou et al., 2011). Experimental results show that translation models obtain stateof-the-art performance for question retrieval in cQA. However, questions and answers are far from “parallel” in practice, questions and answers are highly asymmetric on the information they contain (Zhang et al., 2014a). The second group is the topic-based models (Cai et al., 2011; Ji et al., 2012), which learn the latent topics aligned across the question-answer pairs to alleviate the lexical gap problem, with the assumption that a question and its paired answers share the same topic distribution. However, questions and answers are heterogeneous in many aspects, they do not share the same topic distribution in practice. Inspired by the recent success of continuous space word representations in capturing the semantic similarities in various natural language processing tasks, we propose to incorporate an em</context>
<context position="7480" citStr="Zhang et al., 2014" startWordPosition="1174" endWordPosition="1177"> on large-scale English and Chinese cQA data. The rest of this paper is organized as follows. Section 2 summarizes the related work. Section 3 describes our proposed framework for question retrieval. Section 4 reports the experimental results. Finally, we conclude the paper in Section 5. 2 Related Work 2.1 Question Retrieval in cQA Significant research efforts have been conducted over the years in attempt to improve question retrieval in cQA (Jeon et al., 2005; Xue et al., 2008; Lee et al., 2008; Duan et al., 2008; Bernhard and Gurevych, 2009; Cao et al., 2010; Zhou et al., 2011; Singh, 2012; Zhang et al., 2014a). Most of these works focus on finding similar questions for the user queried questions. The major challenge for question retrieval in cQA is the lexical gap problem. Jeon et al. (2005) proposed a wordbased translation model for automatically fixing the lexical gap problem. Xue et al. (2008) proposed a word-based translation language model for question retrieval. Lee et al. (2008) tried to further improve the translation probabilities based on question-answer pairs by selecting the most important terms to build compact translation models. Bernhard and Gurevych (2009) proposed to use as a par</context>
<context position="9252" citStr="Zhang et al., 2014" startWordPosition="1452" endWordPosition="1455">essed the lexical gap issues by extending the lexical word-based translation model to incorporate semantic information (entities). In contrast to the works described above that assume question-answer pairs are “parallel text”, our paper deals with the lexical gap by learning con251 tinuous word embeddings in capturing the similarities without any assumptions, which is much more reasonable in practice. Besides, some other studies model the semantic relationship between questions and answers with deep linguistic analysis (Duan et al., 2008; Wang et al., 2009; Wang et al., 2010; Ji et al., 2012; Zhang et al., 2014a) or a learning to rank strategy (Surdeanu et al., 2008; Carmel et al., 2014). Recently, Cao et al. (2010) and Zhou et al. (2013) exploited the category metadata within cQA pages to further improve the performance. On the contrary, we focus on the representation learning for questions, with a different solution with those previous works. 2.2 Word Embedding Learning Representation of words as continuous vectors has attracted increasing attention in the area of natural language processing (NLP). Recently, a series of works applied deep learning techniques to learn high-quality word representati</context>
<context position="19319" citStr="Zhang et al., 2014" startWordPosition="3142" endWordPosition="3145">e for this kinds of issues, BoEWs cannot be directly used for large-scale question retrieval task. Given a cQA data collection Q = {qi,1 ≤ i ≤ |Q|}, where qi is the ith question and |Q |is the number of questions in the data collection. The ith question qi is composed by a sequence of words wi = {wij,1 ≤ j ≤ Ni}, where Ni denotes the length of qi. Through looking up table (word embedding matrix) of M, the ith question qi can be represented by Ewi = {ewij,1 ≤ j ≤ Ni}, where ewij is the word embedding of wij. According to the framework of FK (Clinchant and Perronnin, 2013; Sanchez et al., 2013; Zhang et al., 2014b), questions are modeled by a probability density function. In this work, we use Gaussian mixture model (GMM) to do it. We assume that the continuous word embedding Ewi for question qi have been generated by a “universal” (e.g., questionindependent) probability density function (pdf). As is a common practice, we choose this pdf to be a GMM since any continuous distribution can be approximated with arbitrary precision by a mixture of Gaussian. In what follows, the pdf is denoted uλ where λ = {θi, µi, Σi, i = 1 · · · K} is the set of parameters of the GMM. θi, µi and Σi denote respectively the </context>
<context position="29504" citStr="Zhang et al., 2014" startWordPosition="4866" endWordPosition="4869"> papers and train those models with (question, best answer) pairs from the Yahoo! Webscope dataset Yahoo answers and the 1 billion web pages of Baidu Zhidao for English and Chinese, respectively. Training the translation models with different pairs (e.g., question-best answer, question-description, question-answer) may achieve inconsistent performance on Yahoo data and Baidu data, but its comparison and analysis are beyond the scope of this paper. The second group is the topic-based methods: unsupervised question-answer topic model (Ji et al., 2012) and supervised question-answer topic model (Zhang et al., 2014a). We re-implement these two topicbased models and tune the parameter settings on our data set. Besides, we also introduce a baseline language model (LM) (Zhai and Lafferty, 2001) for comparison. Table 2 shows the question retrieval performance by using different evaluation metrics. From this table, we can see that learning continuous word embedding representations (Skip-gram + FV, M-NET + FV) for question retrieval can outperform the translation-based approaches and topic-based approaches on all evaluation metrics. We conduct a statistical test (t-test), the results 256 Model dim Yahoo data </context>
</contexts>
<marker>Zhang, Kang, Qian, Huang, 2014</marker>
<rawString>Qi Zhang, Jihua Kang, Jin Qian, and Xuanjing Huang. 2014b. Continuous word embeddings for detecting local text reuses at the semantic level. In Proceedings of the 37th International ACM SIGIR Conference on Research &amp; Development in Information Retrieval, SIGIR ’14, pages 797–806.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guangyou Zhou</author>
<author>Li Cai</author>
<author>Jun Zhao</author>
<author>Kang Liu</author>
</authors>
<title>Phrase-based translation model for question retrieval in community question answer archives.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>653--662</pages>
<contexts>
<context position="2482" citStr="Zhou et al., 2011" startWordPosition="380" endWordPosition="383">hidao.baidu.com/ cQA archives valuable resources for various tasks like question-answering (Jeon et al., 2005; Xue et al., 2008) and knowledge mining (Adamic et al., 2008), etc. One fundamental task for reusing content in cQA is finding similar questions for queried questions, as questions are the keys to accessing the knowledge in cQA. Then the best answers of these similar questions will be used to answer the queried questions. Many studies have been done along this line (Jeon et al., 2005; Xue et al., 2008; Duan et al., 2008; Lee et al., 2008; Bernhard and Gurevych, 2009; Cao et al., 2010; Zhou et al., 2011; Singh, 2012; Zhang et al., 2014a). One big challenge for question retrieval in cQA is the lexical gap between the queried questions and the existing questions in the archives. Lexical gap means that the queried questions may contain words that are different from, but related to, the words in the existing questions. For example shown in (Zhang et al., 2014a), we find that for a queried question “how do I get knots out of my cats fur?”, there are good answers under an existing question “how can I remove a tangle in my cat’s fur?” in Yahoo! Answers. Although the two questions share few words in</context>
<context position="4213" citStr="Zhou et al., 2011" startWordPosition="661" endWordPosition="664"> words to improve traditional IR models (Jeon et al., 2005; Xue et al., 2008; Zhou et al., 2011). The basic assumption is that question-answer pairs are “parallel texts” and relationship of words (or phrases) can be established through word-to-word (or phrase-to-phrase) 250 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 250–259, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics translation probabilities (Jeon et al., 2005; Xue et al., 2008; Zhou et al., 2011). Experimental results show that translation models obtain stateof-the-art performance for question retrieval in cQA. However, questions and answers are far from “parallel” in practice, questions and answers are highly asymmetric on the information they contain (Zhang et al., 2014a). The second group is the topic-based models (Cai et al., 2011; Ji et al., 2012), which learn the latent topics aligned across the question-answer pairs to alleviate the lexical gap problem, with the assumption that a question and its paired answers share the same topic distribution. However, questions and answers a</context>
<context position="7447" citStr="Zhou et al., 2011" startWordPosition="1168" endWordPosition="1171">ficacy of the proposed framework on large-scale English and Chinese cQA data. The rest of this paper is organized as follows. Section 2 summarizes the related work. Section 3 describes our proposed framework for question retrieval. Section 4 reports the experimental results. Finally, we conclude the paper in Section 5. 2 Related Work 2.1 Question Retrieval in cQA Significant research efforts have been conducted over the years in attempt to improve question retrieval in cQA (Jeon et al., 2005; Xue et al., 2008; Lee et al., 2008; Duan et al., 2008; Bernhard and Gurevych, 2009; Cao et al., 2010; Zhou et al., 2011; Singh, 2012; Zhang et al., 2014a). Most of these works focus on finding similar questions for the user queried questions. The major challenge for question retrieval in cQA is the lexical gap problem. Jeon et al. (2005) proposed a wordbased translation model for automatically fixing the lexical gap problem. Xue et al. (2008) proposed a word-based translation language model for question retrieval. Lee et al. (2008) tried to further improve the translation probabilities based on question-answer pairs by selecting the most important terms to build compact translation models. Bernhard and Gurevyc</context>
<context position="28819" citStr="Zhou et al., 2011" startWordPosition="4765" endWordPosition="4768"> Yahoo data and Baidu data. We compare the baseline word embedding trained by Skip-gram against this trained by M-NET. The dimension of word embedding is set as 50,100 and 300. Since the motivation of this paper attempts to tackle the lexical gap problem for queried questions and questions in the archive, we also compare them with the two groups of methods which also address the lexical gap in the literature. The first group is the translation models: word-based translation model (Jeon et al., 2005), word-based translation language model (Xue et al., 2008), and phrase-based translation model (Zhou et al., 2011). We implement those three translation models based on the original papers and train those models with (question, best answer) pairs from the Yahoo! Webscope dataset Yahoo answers and the 1 billion web pages of Baidu Zhidao for English and Chinese, respectively. Training the translation models with different pairs (e.g., question-best answer, question-description, question-answer) may achieve inconsistent performance on Yahoo data and Baidu data, but its comparison and analysis are beyond the scope of this paper. The second group is the topic-based methods: unsupervised question-answer topic m</context>
<context position="30375" citStr="Zhou et al., 2011" startWordPosition="5008" endWordPosition="5011">g different evaluation metrics. From this table, we can see that learning continuous word embedding representations (Skip-gram + FV, M-NET + FV) for question retrieval can outperform the translation-based approaches and topic-based approaches on all evaluation metrics. We conduct a statistical test (t-test), the results 256 Model dim Yahoo data Baidu data MAP MRR R-Prec P@5 MAP MRR R-Prec P@5 LM (baseline) - 0.435 0.472 0.381 0.305 0.392 0.413 0.325 0.247 (Jeon et al., 2005) - 0.463 0.495 0.396 0.332 0.414 0.428 0.341 0.256 (Xue et al., 2008) - 0.518 0.560 0.423 0.346 0.431 0.435 0.352 0.264 (Zhou et al., 2011) - 0.536 0.587 0.439 0.361 0.448 0.450 0.367 0.273 (Ji et al., 2012) - 0.508 0.544 0.405 0.324 0.425 0.431 0.349 0.258 (Zhang et al., 2014a) - 0.527 0.572 0.433 0.350 0.443 0.446 0.358 0.265 50 0.532 0.583 0.437 0.358 0.447 0.450 0.366 0.272 Skip-gram + FV 100 0.544 0.6051 0.440 0.363 0.454 0.457 0.373 0.274 300 0.5501 0.6191 0.444 0.365 0.4601 0.4641 0.374 0.277 50 0.5481 0.6121 0.441 0.363 0.4591 0.4621 0.374 0.276 M-NET + FV 100 0.562$ 0.628$ 0.4521 0.367$ 0.468$ 0.471 0.3781 0.2801 300 0.571$ 0.643$ 0.455$ 0.374$ 0.475$ 0.477$ 0.385$ 0.283$ Table 2: Evaluation results on Yahoo data and Bai</context>
<context position="32667" citStr="Zhou et al., 2011" startWordPosition="5370" endWordPosition="5373"> brings more improvements for question retrieval task. Translation-based methods significantly outperform LM, which demonstrate that matching questions with the semantically related translation words or phrases from question-answer pairs can effectively address the word lexical gap problem. Besides, we also note that phrase-based translation model is more effective because it captures some contextual information in modeling the translation of phrases as a whole. More precise translation can be determined for phrases than for words. Similar observation has also been found in the previous work (Zhou et al., 2011). On both data sets, topic-based models achieve comparable performance with the translationbased models and but they perform better than LM. The results demonstrate that learning the latent topics aligned across the question-answer pairs can be an alternative for bridging lexical gap problem for question retrieval. 5 Conclusion This paper proposes to learn continuous vector representations for question retrieval in cQA. We firstly introduce a new metadata powered word embedding method, called M-NET, to leverage the category information within cQA pages to obtain word representations. Once the </context>
</contexts>
<marker>Zhou, Cai, Zhao, Liu, 2011</marker>
<rawString>Guangyou Zhou, Li Cai, Jun Zhao, and Kang Liu. 2011. Phrase-based translation model for question retrieval in community question answer archives. In Proceedings of ACL, pages 653–662.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guangyou Zhou</author>
<author>Yubo Chen</author>
<author>Daojian Zeng</author>
<author>Jun Zhao</author>
</authors>
<title>Towards faster and better retrieval models for question search.</title>
<date>2013</date>
<booktitle>In Proceedings of CIKM,</booktitle>
<pages>2139--2148</pages>
<contexts>
<context position="9382" citStr="Zhou et al. (2013)" startWordPosition="1477" endWordPosition="1480"> In contrast to the works described above that assume question-answer pairs are “parallel text”, our paper deals with the lexical gap by learning con251 tinuous word embeddings in capturing the similarities without any assumptions, which is much more reasonable in practice. Besides, some other studies model the semantic relationship between questions and answers with deep linguistic analysis (Duan et al., 2008; Wang et al., 2009; Wang et al., 2010; Ji et al., 2012; Zhang et al., 2014a) or a learning to rank strategy (Surdeanu et al., 2008; Carmel et al., 2014). Recently, Cao et al. (2010) and Zhou et al. (2013) exploited the category metadata within cQA pages to further improve the performance. On the contrary, we focus on the representation learning for questions, with a different solution with those previous works. 2.2 Word Embedding Learning Representation of words as continuous vectors has attracted increasing attention in the area of natural language processing (NLP). Recently, a series of works applied deep learning techniques to learn high-quality word representations. Bengio et al. (2003) proposed a probabilistic neural network language model (NNLM) for word representations. Furthermore, Mik</context>
<context position="15404" citStr="Zhou et al., 2013" startWordPosition="2474" endWordPosition="2477"> w is usually set to p(w) a u(w)3/4 (Mikolov et al., 2013). 3.2 Metadata Powered Model After briefing the skip-gram model, we introduce how we equip it with the metadata information. In cQA sites, there are several metadata, such as “category”,“voting” and so on. In this paper, we only consider the metadata of category information for word embedding learning. All questions in cQA are usually organized into a hierarchy of categories. When an user asks a question, the user typically required to choose a category label for the question from a predefined hierarchy of categories (Cao et al., 2010; Zhou et al., 2013). Previous work in the literature has demonstrated the effectiveness of the category information for question retrieval (Cao et al., 2010; Zhou et al., 2013). On the contrary, we argue that the category information benefits the word embedding learning in this work. The basic idea is that category information encodes the attributes or properties of words, from which we can group similar words according to their categories. Here, a word’s category is assigned based on the questions it appeared in. For example, a question “What are the security issues with java?” is under the category of “Compute</context>
</contexts>
<marker>Zhou, Chen, Zeng, Zhao, 2013</marker>
<rawString>Guangyou Zhou, Yubo Chen, Daojian Zeng, and Jun Zhao. 2013. Towards faster and better retrieval models for question search. In Proceedings of CIKM, pages 2139–2148.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>