<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001364">
<title confidence="0.946012">
SRIUBC: Simple Similarity Features for Semantic Textual Similarity
</title>
<author confidence="0.994739">
Eric Yeh Eneko Agirre
</author>
<affiliation confidence="0.7764405">
SRI International University of the Basque Country
Menlo Park, CA USA Donostia, Basque Country
</affiliation>
<email confidence="0.997789">
yeh@ai.sri.com e.agirre@ehu.es
</email>
<sectionHeader confidence="0.995619" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999977423076923">
We describe the systems submitted by SRI In-
ternational and the University of the Basque
Country for the Semantic Textual Similarity
(STS) SemEval-2012 task. Our systems fo-
cused on using a simple set of features, fea-
turing a mix of semantic similarity resources,
lexical match heuristics, and part of speech
(POS) information. We also incorporate pre-
cision focused scores over lexical and POS in-
formation derived from the BLEU measure,
and lexical and POS features computed over
split-bigrams from the ROUGE-S measure.
These were used to train support vector re-
gressors over the pairs in the training data.
From the three systems we submitted, two per-
formed well in the overall ranking, with split-
bigrams improving performance over pairs
drawn from the MSR Research Video De-
scription Corpus. Our third system maintained
three separate regressors, each trained specif-
ically for the STS dataset they were drawn
from. It used a multinomial classifier to pre-
dict which dataset regressor would be most ap-
propriate to score a given pair, and used it to
score that pair. This system underperformed,
primarily due to errors in the dataset predictor.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996279583333333">
Previous semantic similarity tasks, such as para-
phrase identification or recognizing textual entail-
ment, have focused on performing binary decisions.
These problems are usually framed in terms of iden-
tifying whether a pair of texts exhibit the needed
similarity or entailment relationship or not. In many
cases, such as producing a ranking over similarity
scores, a soft measure of similarity between a pair
of texts would be more desirable.
We contributed three systems for the 2012 Se-
mantic Textual Similarity (STS) task (Agirre et al.,
2012). These are:
</bodyText>
<listItem confidence="0.896237916666667">
1. System 1, which used a combination of seman-
tic similarity, lexical similarity, and precision
focused part-of-speech (POS) features.
2. System 2, which used features from System
1, with the addition of skip-bigram features
derived from the ROUGE-S (Lin, 2004) mea-
sure. POS variants of skip-bigrams were incor-
porated as well.
3. System 3, used the features from above to first
classify the dataset the pair was drawn from,
and then applied regressors trained for that
dataset.
</listItem>
<bodyText confidence="0.999402">
Our systems characterize sentence pairs as feature
vectors, populated by a variety of scorers that will be
described below. During training, we used support
vector regression (SVR) to train regressors against
these vectors and their associated similarity scores.
The STS training data is divided into three
datasets, reflecting their origin: Microsoft Research
Paraphrase Corpus (MSRpar), MSR Research Video
Description Corpus (MSRvid), and WMT2008 De-
velopment dataset (SMTeuroparl). We trained indi-
vidual regressors for each of these datasets, and ap-
plied them to their counterparts in the testing set.
Both Systems 1 and 2 used the following types of
features:
</bodyText>
<page confidence="0.959533">
617
</page>
<note confidence="0.9600165">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 617–623,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<listItem confidence="0.9903646">
1. Resource based word to word semantic similar-
ities.
2. Cosine-based lexical similarity measure.
3. Bilingual Evaluation Understudy (BLEU) (Pa-
pineni et al., 2002) lexical overlap.
4. Precision focused Part of Speech (POS) fea-
tures.
System 2 added the following features:
1. Lexically motivated skip-bigram overlap.
2. Precision focused skip-bigram POS features.
</listItem>
<bodyText confidence="0.9821045">
One of the primary motivations for our the choice
of features was to use relatively simple and fast fea-
tures, which can be scaled up to large datasets, given
appropriate caching and pre-generated lookups. As
the test phase included surprise datasets, whose ori-
gin was not disclosed, we also trained a fourth model
using all of the training data from all three datasets.
Systems 1 and 2 employed this strategy for the sur-
prise data.
Since the statistics for each of the training datasets
varied, directly pooling them together may not be
the best strategy when scoring the surprise data,
whose origins were unknown. To account for this,
System 3 treated this as a gated regression problem,
where pairs are considered to originate strictly from
one dataset, and to score using a model specifically
tailored for that dataset. We first trained regressors
on each of the datasets separately. Then we trained
a classifier to predict which dataset a given pair is
likeliest to have been drawn from, and then applied
the matching trained regressor to obtain its score.
This team included one of the organizers. We
want to stress that we took all measures to make our
participation on the same conditions as the rest of
participants. In particular, the organizer did not al-
low the other member of the team to access any data
or information which was not already available for
the rest of participants.
For the rest of this system description, we first
outline the scorers used to populate the feature vec-
tors used for Systems 1 and 2. We then describe
the setup for performing the regression. We follow
with an explanation of our strategies for dealing with
the surprise data, including a description of System
3. We then summarize performance over the the
datasets, and discuss future avenues of investigation.
</bodyText>
<sectionHeader confidence="0.916726" genericHeader="introduction">
2 Resource Based Similarity
</sectionHeader>
<bodyText confidence="0.999886538461538">
Our system uses several resources for assessing the
word to word similarity between a pair of sentences.
In order to pool together the similarity scores for a
given pair, we employed the Semantic Matrix (Fer-
nando and Stevenson, 2008) framework. To gen-
erate the scores, we used several resources, princi-
pally those derived from the relation graph of Word-
Net (Fellbaum, 1998), and those derived from distri-
butional resources, namely Explicit Semantic Anal-
ysis (Gabrilovich and Markovitch, 2009), and the
Dekang Lin Proximity-based Thesaurus 1. We now
describe the Semantic Matrix method, and follow
with descriptions of each of the resources used.
</bodyText>
<subsectionHeader confidence="0.990691">
2.1 Semantic Matrix
</subsectionHeader>
<bodyText confidence="0.999910818181818">
The Semantic Matrix is a method for pooling all
of the pairwise similarity scores between the to-
kens found in two input strings. In order to score
the similarity between a pair of strings s1 and s2
we first identify all of the unique vocabulary words
from these strings to derive their corresponding oc-
currence vectors v1 and v2. Each dimension of
these vectors corresponds to a unique vocabulary
word, and binary values were used, corresponding
to whether that word was observed. The similarity
score for pair, sim(s1, s2), is given by Formula 1.
</bodyText>
<equation confidence="0.960793">
sim(s1, s2) = lv1 wv2 l (1)
</equation>
<bodyText confidence="0.999652545454546">
with W being the symmetric matrix marking the
similarity between pairs of words in the vocabulary.
We note that this is similar to the Mahalanobis dis-
tance, except adjusted to produce a similarity. For
this experiment, we normalized matrix entries so all
values lay in the 0-1 range.
As named entities and other words encountered
may not appear in one or more of the resources used,
we applied the identity to W. This is equivalent to
adding a strict lexical match fallback on top of the
similarity measure.
</bodyText>
<footnote confidence="0.99848">
1http://webdocs.cs.ualberta.ca/ lindek/downloads.htm
</footnote>
<page confidence="0.989368">
618
</page>
<bodyText confidence="0.999874">
Per (Fernando and Stevenson, 2008), a filter was
applied over the values of W. Any entries that fell
below a given threshold value were flattened to zero,
in order to prevent low scoring similarities from
overwhelming the score. From previous studies over
MSRpar, we applied a threshold of 0.9.
For our experiments, each of the word to word
similarity scorers described below were used to gen-
erate a corresponding word similarity matrix W,
with scores generated using the Semantic Matrix.
</bodyText>
<subsectionHeader confidence="0.999433">
2.2 WordNet Similarity
</subsectionHeader>
<bodyText confidence="0.999991666666667">
We used several methods to obtain word to word
similarities from WordNet. WordNet is a lexical-
semantic resource that describes typed relationships
between synsets, semantic categories a word may
belong to. Similarity scoring methods identify the
synsets associated with a pair of words, and then use
this relationship graph to generate a score.
The first set of scorers were generated from the
Leacock-Chodorow, Lin, and Wu-Palmer measures
from the WordNet Similarity package (Pedersen et
al., 2004). For each of these measures, we averaged
across all of the possible synsets between a given
pair of words.
Another scorer we used was Personalized PageR-
ank (PPR) (Agirre et al., 2010), a topic sensitive
variant of the PageRank algorithm (Page et al.,
1999) that uses a random walk process to identify
the significant nodes of a graph given its link struc-
ture. We first derived a graph G from WordNet,
treating synsets as the vertices and the relationships
between synsets as the edges. To obtain a signature
for a given word, we apply topic sensitive PageRank
(Haveliwala, 2002) over G, using the synsets asso-
ciated with the word as the initial distribution. At
convergence, we convert the stationary distribution
into a vector. The similarity between two words is
the cosine similarity between their vectors.
</bodyText>
<subsectionHeader confidence="0.997394">
2.3 Distributional Resources
</subsectionHeader>
<bodyText confidence="0.999946346153846">
In contrast with the structure based WordNet based
methods, distributional methods use statistical prop-
erties of corpora to derive similarity scores. We gen-
erated two scorers, one based on Explicit Seman-
tic Analysis (ESA), and the other on the Dekang
Lin Proximity-based Thesaurus. For a given word,
ESA generates a concept vector, where the con-
cepts are Wikipedia articles, and the score measures
how closely associated that word is with the textual
content of the article. To score the similarity be-
tween two words, we computed the cosine similar-
ity of their concept vectors. This method proved to
give state-of-the-art performance on the WordSim-
353 word pair relatedness dataset (Finkelstein et al.,
2002).
The Lin Proximity-based Thesaurus identifies
the neighborhood around words encountered in the
Reuters and Text Retrieval Conference (TREC). For
a given word, the Thesaurus identifies the top 200
words with the most similar neighborhoods, listing
the score based on these matches. For our experi-
ments, we treated these as feature vectors, with the
intuition being similar words should share similar
neighbors. Again, the similarity score between two
words was scored using the cosine similarity of their
vectors.
</bodyText>
<sectionHeader confidence="0.987984" genericHeader="method">
3 Cosine Similarity
</sectionHeader>
<bodyText confidence="0.9999608">
Another scorer we used was the cosine similarity
over the lemmas found in the sentences in a pair.
For generating the vectors used in the cosine simi-
larity computation, we used the term frequency of
the lemmas.
</bodyText>
<sectionHeader confidence="0.998459" genericHeader="method">
4 BLEU Features
</sectionHeader>
<bodyText confidence="0.9978068">
BLEU is a measure developed to automatically as-
sess how closely sentences generated by machine
translation systems match reference human gener-
ated texts. BLEU is a directional measurement, and
works on the assumption that the more lexically sim-
ilar a system generated sentence is to a reference sen-
tence, a human generated translation, the better the
system sentence is. This can also be seen as a stand-
in for the semantic similarity of the pairs, as was
shown when BLEU was applied to the paraphrase
identification identification problem in (Finch et al.,
2005).
The BLEU score for a given system sentence and
reference sentence of order N is computed using
Formula 2.
</bodyText>
<equation confidence="0.924923166666667">
BLEU(sys, ref) = B · exp
1
N log(pn) (2)
N
E
n��
</equation>
<page confidence="0.990959">
619
</page>
<bodyText confidence="0.966672625">
B is a brevity penalty used to prevent degenerate
translations. Given this has little bearing on our ex-
periments, we set its value to 1 for our experiments.
Following (Papineni et al., 2002), we give each order
n equal weight in the geometric mean. The proba-
bility of an order n-gram from the system sentence
being found in the reference, pn, is given in Formula
3.
</bodyText>
<equation confidence="0.6457455">
pn = EngramEsys countsysnref(ngram) (3)
EngramEsys countsys(ngram)
</equation>
<bodyText confidence="0.9738791">
countsys(ngram) is frequency of oc-
currence for the given n-gram in the sys-
tem sentence. The numerator term is
computed as countsysnref(ngram) =
min(countsys(ngram), countref(ngram)) where
countref(ngram) is the frequency of occurrence
of that n-gram in the reference sentence. This
is equivalent to having each n-gram have a 1-1
mapping with a matching n-gram in the reference
(if any), and counting the number of mappings.
As there is a risk of higher order system n-grams
having no matches in the reference, we apply Lapla-
cian smoothing to the n-gram counts.
BLEU is considered to be a precision focused
measure, as it only measures how much of the sys-
tem sentence matches a reference sentence. Follow-
ing (Finch et al., 2005), we obtain a modified BLEU
score for strings s1 and s2 of a pair by averaging the
BLEU scores where each takes a turn as the system
sentence, as given in Formula 4.
</bodyText>
<equation confidence="0.988737">
1
Score(s1, s2) = �BLEU(s1, s2) · BLEU(s2, s1)
(4)
</equation>
<bodyText confidence="0.9996775">
For our experiments, we used BLEU scores of or-
der N = 1..�, over n-grams formed over the sen-
tence lemmas, and used these as features for charac-
terizing a given pair.
</bodyText>
<subsectionHeader confidence="0.99351">
4.1 Precision Focused POS Features
</subsectionHeader>
<bodyText confidence="0.9996748">
From past experiments with paraphrase identifica-
tion over the MSR Paraphrase Corpus, we have
found including POS information to be beneficial.
To this capture this kind of information, we gen-
erated precision focused POS features, which mea-
</bodyText>
<listItem confidence="0.82838525">
sures the following between the sentences in a prob-
lem pair:
1. The overlap in POS tags.
2. The mismatch in POS tags.
</listItem>
<bodyText confidence="0.999905">
We follow the formulation for POS vectors given
in (Finch et al., 2005). For a given sentence pair,
we identify the set of words whose lemmas were
matched in both the system and reference sentences,
Wmateh and those with no matches, Wmiss. Using
the directional notion of system and reference sen-
tences from BLEU, for each word w ∈ Wmateh,
</bodyText>
<equation confidence="0.955511">
E countt(w)
POSMatch(t, sys, ref) = wEWmatch
(5)
</equation>
<bodyText confidence="0.999875857142857">
where countt is 1 if word w has the matching POS
tag, and 0 otherwise. |sys |is the token count of the
system sentence. This is deemed to be precision-
focused, as this computation is done over candidates
found in the system sentence.
To generate the score for missing POS tags, we
perform a similar computation,
</bodyText>
<equation confidence="0.9891585">
Ecountt(w)
wEWmiss (6)
</equation>
<bodyText confidence="0.9999562">
To score the POS match and misses between a
pair, we follow Formula 4 and average the scores
for each POS tag, where the sentences in a given
pair swap positions as the system and reference sen-
tences.
</bodyText>
<sectionHeader confidence="0.997296" genericHeader="method">
5 Split-Bigram Features
</sectionHeader>
<bodyText confidence="0.9999266">
System 2 added split-bigram features, which were
derived from the ROUGE-S measure. Like bigrams,
split-bigrams consist of an ordered pair of distinct
tokens drawn from a source sentence. Unlike bi-
grams, split-bigrams allow for a number of inter-
vening tokens to appear between the split-bigram to-
kens. For example, “The cat ate fish.” would gen-
erate the following split-bigrams the→cat, the→ate,
the→fish, cat→ate, cat→fish, and ate→fish. The in-
tent of split-bigrams is to quickly capture long range
</bodyText>
<equation confidence="0.966728333333333">
|sys|
POSMiss(t, sys, ref) =
|sys|
</equation>
<page confidence="0.956046">
620
</page>
<bodyText confidence="0.99984825">
dependencies, without requiring a parse of the sen-
tence.
Similar to ROUGE-S, we used lexical overlap
of the split-bigrams as an approximation of seman-
tic similarity. As our pairs are bidirectional, we
used the same framework (Formula 2) for obtain-
ing BLEU scores to generate split-bigram overlap
scores for our pairs. Here, counts are obtained over
split-bigrams found in the system and reference sen-
tences, and the order was set to 1.
For generating the skip-bigram overlap score for
a pair, we used a maximum distance of three.
</bodyText>
<subsectionHeader confidence="0.994995">
5.1 Skip-Bigram POS Features
</subsectionHeader>
<bodyText confidence="0.999554714285714">
In the same vein as the precision focused POS
features, we used the POS tags of matched split-
bigrams as features, where the frequency of the
POS tags in split-bigrams, t —* t&apos;, were used.
Here, Bmatch represents the split-bigrams which
were found in both the system and reference sen-
tences, matched on lexical content.
</bodyText>
<equation confidence="0.99334">
E countt,t/(b)
SBMatch(t —* t&apos;, sys, ref) = bEBmatch
(7)
</equation>
<bodyText confidence="0.9999195">
Due to sparsity, we only considered scores from
split-bigram matches between the system and ref-
erence sentences, and do not model split-bigram
misses. As before, we generate scores for each split-
bigram tag sequence by averaging the scores where
both sentences in a pair have swapped positions. For
our experiments, we only considered split-bigram
POS features of up to distance 3. In our initial exper-
iments we found split-bigram POS features helped
only in the case of shorter sentence pairs, so we only
generated features if both the sentences in a given
pair contained ten tokens or less.
</bodyText>
<sectionHeader confidence="0.998949" genericHeader="method">
6 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999867857142857">
For all three systems, we used the Stanford
CoreNLP (Toutanova et al., 2003) package to per-
form lemmatization and POS tagging of the in-
put sentences. For regressors, we used LibSVM’s
(Chang and Lin, 2011) support vector regression ca-
pability, using radial basis kernels. Based off of tun-
ing on the training set, we set -y = 1 and the default
</bodyText>
<table confidence="0.99762725">
Dataset Mean Std.Dev
MSRpar 3.322 0.9294
MSRvid 2.135 1.595
SMTeur 4.307 0.7114
</table>
<tableCaption confidence="0.9913065">
Table 1: Means and standard deviations of similarity
scores for each of the training datasets.
</tableCaption>
<bodyText confidence="0.88954325">
slack value.
From previous experience with paraphrase iden-
tification over the MSR Paraphrase Corpus, we re-
tained stop words in all of our experiments.
</bodyText>
<sectionHeader confidence="0.670485" genericHeader="method">
7 Dealing with Surprise Data
</sectionHeader>
<bodyText confidence="0.988523625">
As the STS training data was broken into three sep-
arate datasets, each with their own distinct statistics,
we developed three regressors trained individually
on each of these datasets. This presented a problem
when dealing with surprise datasets, whose statistics
were not known.
The approach taken by Systems 1 and 2 was sim-
ply to pool together all three training datasets into a
single dataset and train a single regressor on that uni-
fied model. We then applied that regressor against
the two surprise datasets, OnWN and SMTnews.
Analysis of the similarity score statistics showed
that they varied greatly between each of the train-
ing sets, as given in Table 1. Thus combining the
datasets blindly, as with Systems 1 and 2, may prove
to be a suboptimal strategy. The approach taken by
System 3 was to consider the feature vectors them-
selves as capturing information about which dataset
they were drawn from, and to use a classifier to pre-
dict that dataset. We then emit the score from the
regressor trained on just that matching dataset. We
used the Stanford Classifier’s (Manning and Klein,
2003) multinomial logistic regression as our dataset
predictor, using the feature vectors from System 2.
Five-fold cross validation over the training data
showed the dataset predictor to have an overall ac-
curacy of 91.75%.
In order to assess performance over the known
datasets at test time, System 3 also applied the same
strategy for the MSRpar, MSRvid, and SMTeuroparl
test sets.
|sys|
</bodyText>
<page confidence="0.991203">
621
</page>
<table confidence="0.99773175">
Sys All Allnorm Mean MSRpar MSRvid SMTeur OnWN SMTnews
1 0.7513 / 11 0.8017 / 40 0.5997 / 22 0.6084 0.7458 0.4688 0.6315 0.3994
2 0.7562 / 10 0.8111 / 24 0.5858 / 33 0.6050 0.7939 0.4294 0.5871 0.3366
3 0.6876 / 21 0.7812 / 54 0.4668 / 68 0.4791 0.7901 0.2159 0.3843 0.2801
</table>
<tableCaption confidence="0.921046">
Table 2: Pearson correlation of described systems against test data, by dataset. Overall measures are All indicates the
</tableCaption>
<table confidence="0.931236333333333">
combined Pearson, Allnorm the normalized variant, and Mean the macro average of Pearson correlations. Rank for
the system in the overall measure is given after the slash.
Guess/Gold MSRpar MSRvid SMTeur
MSRpar 664 7 75
MSRvid 7 737 10
SMTeur 79 6 649
</table>
<tableCaption confidence="0.74027975">
Table 3: Confusion for the dataset predictor, used to pre-
dict which dataset a pair was drawn from. This was
ddrawn using five-fold cross validation over the training
set, with columns representing golds and guesses as rows.
</tableCaption>
<sectionHeader confidence="0.996063" genericHeader="evaluation">
8 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999837454545455">
Results on the test data for each of the systems
against the individual datasets, are given in Table
2, given in Pearson linear correlation with the gold
standard scores. Overall measures for the systems
are given, along with their overall ranking.
The split-bigram features in System 2 contributed
primarily to performance over the MSRvid dataset,
while degrading performance on the other datasets
slightly. This is likely a result of increasing spar-
sity in the feature space, but overall this system per-
formed well. System 3 underperformed on most
datasets, asides from its performance on MSRvid.
The confusion generated over five-fold cross vali-
dation over the training set is given in Table 3, and
precision, recall, and F1 scores by dataset label from
five-fold cross validation over the training set are
given in Table 4. As these show, predictor errors lay
primarily in confusing MSRpar for SMTeuroparl,
and vice versa. This error was significant enough to
reduce performance on both the MSRpar and SM-
Teuroparl test sets. This proved to be enough to re-
duce the scores between these two datasets.
</bodyText>
<sectionHeader confidence="0.987502" genericHeader="conclusions">
9 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.996285">
Our STS systems have shown that relatively sim-
ple syntax free methods can be employed to the
STS task. Future avenues of investigation would
</bodyText>
<table confidence="0.9987775">
Dataset Prec Rec F1
MSRpar 0.8901 0.8853 0.8877
MSRvid 0.9775 0.9827 0.9801
SMTeur 0.8842 0.8842 0.8842
</table>
<tableCaption confidence="0.998018">
Table 4: Results on classifying pairs by source dataset,
using five-fold cross validation over training data.
</tableCaption>
<bodyText confidence="0.999940615384615">
be to include the use of syntactic information, in
order to obtain better predicate-argument informa-
tion. Syntactic information has proven useful for
the paraphrase identification task over MSRpar, as
demonstrated in studies such as (Das and Smith,
2009) and (Socher et al., 2011). Furthermore, a
qualitative assessment of the pairs across different
datasets showed relatively significant differences,
which would strengthen the argument for develop-
ing features and methods specific to each dataset.
Another improvement would be to develop a bet-
ter dataset predictor for System 3. Also recognizing
there may be ways to normalize and rescale scores
across datasets so the regression models used do not
have to account for differing means and standard de-
viations.
Finally, there are other bodies of source data that
may be adapted for use with the STS task, such as
the paraphrasing pairs of the Recognizing Textual
Entailment challenges, human generated reference
translations for machine translation evaluation, and
human generated summaries used for summariza-
tion evaluations. Although these are gold decisions,
at the very least they could provide a source of high
similarity pairs, from which one could manufacture
lower scoring variants.
</bodyText>
<sectionHeader confidence="0.998319" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9806325">
The authors would like to thank the Semantic Tex-
tual Similarity organizers for all of their hard work
</bodyText>
<page confidence="0.994916">
622
</page>
<bodyText confidence="0.999419421052632">
and effort.
One of the authors was supported by the
Intelligence Advanced Research Projects Activ-
ity (IARPA) via Air Force Research Laboratory
(AFRL) contract number FA8650-10-C-7058. The
U.S. Government is authorized to reproduce and dis-
tribute reprints for Governmental purposes notwith-
standing any copyright annotation thereon. The
views and conclusions contained herein are those
of the authors and should not be interpreted as
necessarily representing the official policies or en-
dorsements, either expressed or implied, of IARPA,
AFRL, or the U.S. Government.
Eneko Agirre was partially funded by the
European Communitys Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agreement
no. 270082 (PATHS project) and the Ministry
of Economy under grant TIN2009-14715-C04-01
(KNOW2 project)
</bodyText>
<sectionHeader confidence="0.998096" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999733746987952">
Eneko Agirre, Montse Cuadros, German Rigau, and Aitor
Soroa. 2010. Exploring knowledge bases for similar-
ity. In Proceedings of the International Conference on
Language Resources and Evaluation 2010.
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot
on semantic textual similarity. In Proceedings of the
6th International Workshop on Semantic Evaluation
(SemEval 2012), in conjunction with the First Joint
Conference on Lexical and Computational Semantics
(*SEM 2012).
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1–
27:27.
Dipanjan Das and Noah A. Smith. 2009. Paraphrase
identification as probabilistic quasi-synchronous
recognition. In In Proceedings of the Joint Confer-
ence of the Annual Meeting of the Association for
Computational Linguistics and the International Joint
Conference on Natural Language Processing(ACL
2009), pages 468–476, Singapore.
Christine Fellbaum. 1998. WordNet - An Electronic Lex-
ical Database. MIT Press.
Samuel Fernando and Mark Stevenson. 2008. A se-
mantic similarity approach to paraphrase detection. In
Computational Linguistics UK (CLUK 2008) 11th An-
nual Research Colloqium.
Andrew Finch, Young-Sook Hwang, and Eiichio Sumita.
2005. Using machine translation evaluation tech-
niques to determine sentence-level semantic equiva-
lence. In Proceedings of the Third International Work-
shop on Paraphrasing (IWP 2005), pages 17–24, Jeju
Island, South Korea.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2002. Placing search in context: The concept
revisited. ACM Transactions on Information Systems,
20(1):116–131.
Evgeniy Gabrilovich and Shaul Markovitch. 2009.
Wikipedia-based semantic interpretation. Journal of
Artificial Intelligence Research, 34:443–498.
Taher H. Haveliwala. 2002. Topic-sensitive pagerank.
In WWW ’02, pages 517–526, New York, NY, USA.
ACM.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. pages 74–81, Barcelona,
Spain, jul. Association for Computational Linguistics.
Christopher Manning and Dan Klein. 2003. Optimiza-
tion, maxent models, and conditional estimation with-
out magic. In Proceedings of the 2003 Conference
of the North American Chapter of the Association for
Computational Linguistics on Human Language Tech-
nology: Tutorials - Volume 5, NAACL-Tutorials ’03,
pages 8–8, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1999. The pagerank citation ranking:
Bringing order to the web. Technical Report 1999-
66, Stanford InfoLab, November. Previous number =
SIDL-WP-1999-0120.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ’02, pages 311–318, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity - Measuring the Re-
latedness of Concepts. In Proceedings of the Nine-
teenth National Conference on Artificial Intelligence
(Intelligent Systems Demonstrations), pages 1024–
1025, San Jose, CA, July.
Richard Socher, Eric H. Huang, Jeffrey Pennington, An-
drew Y. Ng, and Christopher D. Manning. 2011. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. In Advances in Neural Infor-
mation Processing Systems 24.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of HLT-NAACL 2003, pages 252–259.
</reference>
<page confidence="0.999157">
623
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.649772">
<title confidence="0.999388">SRIUBC: Simple Similarity Features for Semantic Textual Similarity</title>
<author confidence="0.993749">Eric Yeh Eneko Agirre</author>
<affiliation confidence="0.969253">SRI International University of the Basque Country</affiliation>
<address confidence="0.76338">Menlo Park, CA USA Donostia, Basque Country</address>
<email confidence="0.833227">yeh@ai.sri.come.agirre@ehu.es</email>
<abstract confidence="0.998592407407408">We describe the systems submitted by SRI International and the University of the Basque Country for the Semantic Textual Similarity (STS) SemEval-2012 task. Our systems focused on using a simple set of features, featuring a mix of semantic similarity resources, lexical match heuristics, and part of speech (POS) information. We also incorporate precision focused scores over lexical and POS information derived from the BLEU measure, and lexical and POS features computed over split-bigrams from the ROUGE-S measure. These were used to train support vector regressors over the pairs in the training data. From the three systems we submitted, two performed well in the overall ranking, with splitbigrams improving performance over pairs drawn from the MSR Research Video Description Corpus. Our third system maintained three separate regressors, each trained specifically for the STS dataset they were drawn from. It used a multinomial classifier to predict which dataset regressor would be most appropriate to score a given pair, and used it to score that pair. This system underperformed, primarily due to errors in the dataset predictor.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Montse Cuadros</author>
<author>German Rigau</author>
<author>Aitor Soroa</author>
</authors>
<title>Exploring knowledge bases for similarity.</title>
<date>2010</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation</booktitle>
<contexts>
<context position="8447" citStr="Agirre et al., 2010" startWordPosition="1348" endWordPosition="1351">m WordNet. WordNet is a lexicalsemantic resource that describes typed relationships between synsets, semantic categories a word may belong to. Similarity scoring methods identify the synsets associated with a pair of words, and then use this relationship graph to generate a score. The first set of scorers were generated from the Leacock-Chodorow, Lin, and Wu-Palmer measures from the WordNet Similarity package (Pedersen et al., 2004). For each of these measures, we averaged across all of the possible synsets between a given pair of words. Another scorer we used was Personalized PageRank (PPR) (Agirre et al., 2010), a topic sensitive variant of the PageRank algorithm (Page et al., 1999) that uses a random walk process to identify the significant nodes of a graph given its link structure. We first derived a graph G from WordNet, treating synsets as the vertices and the relationships between synsets as the edges. To obtain a signature for a given word, we apply topic sensitive PageRank (Haveliwala, 2002) over G, using the synsets associated with the word as the initial distribution. At convergence, we convert the stationary distribution into a vector. The similarity between two words is the cosine similar</context>
</contexts>
<marker>Agirre, Cuadros, Rigau, Soroa, 2010</marker>
<rawString>Eneko Agirre, Montse Cuadros, German Rigau, and Aitor Soroa. 2010. Exploring knowledge bases for similarity. In Proceedings of the International Conference on Language Resources and Evaluation 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<title>Semeval-2012 task 6: A pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics (*SEM</booktitle>
<contexts>
<context position="1924" citStr="Agirre et al., 2012" startWordPosition="299" endWordPosition="302">performed, primarily due to errors in the dataset predictor. 1 Introduction Previous semantic similarity tasks, such as paraphrase identification or recognizing textual entailment, have focused on performing binary decisions. These problems are usually framed in terms of identifying whether a pair of texts exhibit the needed similarity or entailment relationship or not. In many cases, such as producing a ranking over similarity scores, a soft measure of similarity between a pair of texts would be more desirable. We contributed three systems for the 2012 Semantic Textual Similarity (STS) task (Agirre et al., 2012). These are: 1. System 1, which used a combination of semantic similarity, lexical similarity, and precision focused part-of-speech (POS) features. 2. System 2, which used features from System 1, with the addition of skip-bigram features derived from the ROUGE-S (Lin, 2004) measure. POS variants of skip-bigrams were incorporated as well. 3. System 3, used the features from above to first classify the dataset the pair was drawn from, and then applied regressors trained for that dataset. Our systems characterize sentence pairs as feature vectors, populated by a variety of scorers that will be de</context>
</contexts>
<marker>Agirre, Cer, Diab, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot on semantic textual similarity. In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics (*SEM 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<volume>2</volume>
<pages>27--27</pages>
<contexts>
<context position="16468" citStr="Chang and Lin, 2011" startWordPosition="2691" endWordPosition="2694">gram tag sequence by averaging the scores where both sentences in a pair have swapped positions. For our experiments, we only considered split-bigram POS features of up to distance 3. In our initial experiments we found split-bigram POS features helped only in the case of shorter sentence pairs, so we only generated features if both the sentences in a given pair contained ten tokens or less. 6 Experimental Setup For all three systems, we used the Stanford CoreNLP (Toutanova et al., 2003) package to perform lemmatization and POS tagging of the input sentences. For regressors, we used LibSVM’s (Chang and Lin, 2011) support vector regression capability, using radial basis kernels. Based off of tuning on the training set, we set -y = 1 and the default Dataset Mean Std.Dev MSRpar 3.322 0.9294 MSRvid 2.135 1.595 SMTeur 4.307 0.7114 Table 1: Means and standard deviations of similarity scores for each of the training datasets. slack value. From previous experience with paraphrase identification over the MSR Paraphrase Corpus, we retained stop words in all of our experiments. 7 Dealing with Surprise Data As the STS training data was broken into three separate datasets, each with their own distinct statistics, </context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1– 27:27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
</authors>
<title>Paraphrase identification as probabilistic quasi-synchronous recognition. In</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing(ACL</booktitle>
<pages>468--476</pages>
<contexts>
<context position="21083" citStr="Das and Smith, 2009" startWordPosition="3456" endWordPosition="3459">uture Work Our STS systems have shown that relatively simple syntax free methods can be employed to the STS task. Future avenues of investigation would Dataset Prec Rec F1 MSRpar 0.8901 0.8853 0.8877 MSRvid 0.9775 0.9827 0.9801 SMTeur 0.8842 0.8842 0.8842 Table 4: Results on classifying pairs by source dataset, using five-fold cross validation over training data. be to include the use of syntactic information, in order to obtain better predicate-argument information. Syntactic information has proven useful for the paraphrase identification task over MSRpar, as demonstrated in studies such as (Das and Smith, 2009) and (Socher et al., 2011). Furthermore, a qualitative assessment of the pairs across different datasets showed relatively significant differences, which would strengthen the argument for developing features and methods specific to each dataset. Another improvement would be to develop a better dataset predictor for System 3. Also recognizing there may be ways to normalize and rescale scores across datasets so the regression models used do not have to account for differing means and standard deviations. Finally, there are other bodies of source data that may be adapted for use with the STS task</context>
</contexts>
<marker>Das, Smith, 2009</marker>
<rawString>Dipanjan Das and Noah A. Smith. 2009. Paraphrase identification as probabilistic quasi-synchronous recognition. In In Proceedings of the Joint Conference of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing(ACL 2009), pages 468–476, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christine Fellbaum</author>
</authors>
<title>WordNet - An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="5818" citStr="Fellbaum, 1998" startWordPosition="924" endWordPosition="925">llow with an explanation of our strategies for dealing with the surprise data, including a description of System 3. We then summarize performance over the the datasets, and discuss future avenues of investigation. 2 Resource Based Similarity Our system uses several resources for assessing the word to word similarity between a pair of sentences. In order to pool together the similarity scores for a given pair, we employed the Semantic Matrix (Fernando and Stevenson, 2008) framework. To generate the scores, we used several resources, principally those derived from the relation graph of WordNet (Fellbaum, 1998), and those derived from distributional resources, namely Explicit Semantic Analysis (Gabrilovich and Markovitch, 2009), and the Dekang Lin Proximity-based Thesaurus 1. We now describe the Semantic Matrix method, and follow with descriptions of each of the resources used. 2.1 Semantic Matrix The Semantic Matrix is a method for pooling all of the pairwise similarity scores between the tokens found in two input strings. In order to score the similarity between a pair of strings s1 and s2 we first identify all of the unique vocabulary words from these strings to derive their corresponding occurre</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christine Fellbaum. 1998. WordNet - An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Fernando</author>
<author>Mark Stevenson</author>
</authors>
<title>A semantic similarity approach to paraphrase detection.</title>
<date>2008</date>
<booktitle>In Computational Linguistics UK (CLUK 2008) 11th Annual Research Colloqium.</booktitle>
<contexts>
<context position="5678" citStr="Fernando and Stevenson, 2008" startWordPosition="898" endWordPosition="902">e first outline the scorers used to populate the feature vectors used for Systems 1 and 2. We then describe the setup for performing the regression. We follow with an explanation of our strategies for dealing with the surprise data, including a description of System 3. We then summarize performance over the the datasets, and discuss future avenues of investigation. 2 Resource Based Similarity Our system uses several resources for assessing the word to word similarity between a pair of sentences. In order to pool together the similarity scores for a given pair, we employed the Semantic Matrix (Fernando and Stevenson, 2008) framework. To generate the scores, we used several resources, principally those derived from the relation graph of WordNet (Fellbaum, 1998), and those derived from distributional resources, namely Explicit Semantic Analysis (Gabrilovich and Markovitch, 2009), and the Dekang Lin Proximity-based Thesaurus 1. We now describe the Semantic Matrix method, and follow with descriptions of each of the resources used. 2.1 Semantic Matrix The Semantic Matrix is a method for pooling all of the pairwise similarity scores between the tokens found in two input strings. In order to score the similarity betwe</context>
<context position="7286" citStr="Fernando and Stevenson, 2008" startWordPosition="1162" endWordPosition="1165">. sim(s1, s2) = lv1 wv2 l (1) with W being the symmetric matrix marking the similarity between pairs of words in the vocabulary. We note that this is similar to the Mahalanobis distance, except adjusted to produce a similarity. For this experiment, we normalized matrix entries so all values lay in the 0-1 range. As named entities and other words encountered may not appear in one or more of the resources used, we applied the identity to W. This is equivalent to adding a strict lexical match fallback on top of the similarity measure. 1http://webdocs.cs.ualberta.ca/ lindek/downloads.htm 618 Per (Fernando and Stevenson, 2008), a filter was applied over the values of W. Any entries that fell below a given threshold value were flattened to zero, in order to prevent low scoring similarities from overwhelming the score. From previous studies over MSRpar, we applied a threshold of 0.9. For our experiments, each of the word to word similarity scorers described below were used to generate a corresponding word similarity matrix W, with scores generated using the Semantic Matrix. 2.2 WordNet Similarity We used several methods to obtain word to word similarities from WordNet. WordNet is a lexicalsemantic resource that descr</context>
</contexts>
<marker>Fernando, Stevenson, 2008</marker>
<rawString>Samuel Fernando and Mark Stevenson. 2008. A semantic similarity approach to paraphrase detection. In Computational Linguistics UK (CLUK 2008) 11th Annual Research Colloqium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Finch</author>
<author>Young-Sook Hwang</author>
<author>Eiichio Sumita</author>
</authors>
<title>Using machine translation evaluation techniques to determine sentence-level semantic equivalence.</title>
<date>2005</date>
<booktitle>In Proceedings of the Third International Workshop on Paraphrasing (IWP</booktitle>
<pages>17--24</pages>
<location>Jeju Island, South</location>
<contexts>
<context position="11134" citStr="Finch et al., 2005" startWordPosition="1780" endWordPosition="1783">e used the term frequency of the lemmas. 4 BLEU Features BLEU is a measure developed to automatically assess how closely sentences generated by machine translation systems match reference human generated texts. BLEU is a directional measurement, and works on the assumption that the more lexically similar a system generated sentence is to a reference sentence, a human generated translation, the better the system sentence is. This can also be seen as a standin for the semantic similarity of the pairs, as was shown when BLEU was applied to the paraphrase identification identification problem in (Finch et al., 2005). The BLEU score for a given system sentence and reference sentence of order N is computed using Formula 2. BLEU(sys, ref) = B · exp 1 N log(pn) (2) N E n�� 619 B is a brevity penalty used to prevent degenerate translations. Given this has little bearing on our experiments, we set its value to 1 for our experiments. Following (Papineni et al., 2002), we give each order n equal weight in the geometric mean. The probability of an order n-gram from the system sentence being found in the reference, pn, is given in Formula 3. pn = EngramEsys countsysnref(ngram) (3) EngramEsys countsys(ngram) counts</context>
<context position="12455" citStr="Finch et al., 2005" startWordPosition="2006" endWordPosition="2009">s computed as countsysnref(ngram) = min(countsys(ngram), countref(ngram)) where countref(ngram) is the frequency of occurrence of that n-gram in the reference sentence. This is equivalent to having each n-gram have a 1-1 mapping with a matching n-gram in the reference (if any), and counting the number of mappings. As there is a risk of higher order system n-grams having no matches in the reference, we apply Laplacian smoothing to the n-gram counts. BLEU is considered to be a precision focused measure, as it only measures how much of the system sentence matches a reference sentence. Following (Finch et al., 2005), we obtain a modified BLEU score for strings s1 and s2 of a pair by averaging the BLEU scores where each takes a turn as the system sentence, as given in Formula 4. 1 Score(s1, s2) = �BLEU(s1, s2) · BLEU(s2, s1) (4) For our experiments, we used BLEU scores of order N = 1..�, over n-grams formed over the sentence lemmas, and used these as features for characterizing a given pair. 4.1 Precision Focused POS Features From past experiments with paraphrase identification over the MSR Paraphrase Corpus, we have found including POS information to be beneficial. To this capture this kind of informatio</context>
</contexts>
<marker>Finch, Hwang, Sumita, 2005</marker>
<rawString>Andrew Finch, Young-Sook Hwang, and Eiichio Sumita. 2005. Using machine translation evaluation techniques to determine sentence-level semantic equivalence. In Proceedings of the Third International Workshop on Paraphrasing (IWP 2005), pages 17–24, Jeju Island, South Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="9811" citStr="Finkelstein et al., 2002" startWordPosition="1567" endWordPosition="1570">use statistical properties of corpora to derive similarity scores. We generated two scorers, one based on Explicit Semantic Analysis (ESA), and the other on the Dekang Lin Proximity-based Thesaurus. For a given word, ESA generates a concept vector, where the concepts are Wikipedia articles, and the score measures how closely associated that word is with the textual content of the article. To score the similarity between two words, we computed the cosine similarity of their concept vectors. This method proved to give state-of-the-art performance on the WordSim353 word pair relatedness dataset (Finkelstein et al., 2002). The Lin Proximity-based Thesaurus identifies the neighborhood around words encountered in the Reuters and Text Retrieval Conference (TREC). For a given word, the Thesaurus identifies the top 200 words with the most similar neighborhoods, listing the score based on these matches. For our experiments, we treated these as feature vectors, with the intuition being similar words should share similar neighbors. Again, the similarity score between two words was scored using the cosine similarity of their vectors. 3 Cosine Similarity Another scorer we used was the cosine similarity over the lemmas f</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2002</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2002. Placing search in context: The concept revisited. ACM Transactions on Information Systems, 20(1):116–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Wikipedia-based semantic interpretation.</title>
<date>2009</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>34--443</pages>
<contexts>
<context position="5937" citStr="Gabrilovich and Markovitch, 2009" startWordPosition="938" endWordPosition="941">n of System 3. We then summarize performance over the the datasets, and discuss future avenues of investigation. 2 Resource Based Similarity Our system uses several resources for assessing the word to word similarity between a pair of sentences. In order to pool together the similarity scores for a given pair, we employed the Semantic Matrix (Fernando and Stevenson, 2008) framework. To generate the scores, we used several resources, principally those derived from the relation graph of WordNet (Fellbaum, 1998), and those derived from distributional resources, namely Explicit Semantic Analysis (Gabrilovich and Markovitch, 2009), and the Dekang Lin Proximity-based Thesaurus 1. We now describe the Semantic Matrix method, and follow with descriptions of each of the resources used. 2.1 Semantic Matrix The Semantic Matrix is a method for pooling all of the pairwise similarity scores between the tokens found in two input strings. In order to score the similarity between a pair of strings s1 and s2 we first identify all of the unique vocabulary words from these strings to derive their corresponding occurrence vectors v1 and v2. Each dimension of these vectors corresponds to a unique vocabulary word, and binary values were </context>
</contexts>
<marker>Gabrilovich, Markovitch, 2009</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2009. Wikipedia-based semantic interpretation. Journal of Artificial Intelligence Research, 34:443–498.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taher H Haveliwala</author>
</authors>
<title>Topic-sensitive pagerank.</title>
<date>2002</date>
<booktitle>In WWW ’02,</booktitle>
<pages>517--526</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="8842" citStr="Haveliwala, 2002" startWordPosition="1417" endWordPosition="1418">larity package (Pedersen et al., 2004). For each of these measures, we averaged across all of the possible synsets between a given pair of words. Another scorer we used was Personalized PageRank (PPR) (Agirre et al., 2010), a topic sensitive variant of the PageRank algorithm (Page et al., 1999) that uses a random walk process to identify the significant nodes of a graph given its link structure. We first derived a graph G from WordNet, treating synsets as the vertices and the relationships between synsets as the edges. To obtain a signature for a given word, we apply topic sensitive PageRank (Haveliwala, 2002) over G, using the synsets associated with the word as the initial distribution. At convergence, we convert the stationary distribution into a vector. The similarity between two words is the cosine similarity between their vectors. 2.3 Distributional Resources In contrast with the structure based WordNet based methods, distributional methods use statistical properties of corpora to derive similarity scores. We generated two scorers, one based on Explicit Semantic Analysis (ESA), and the other on the Dekang Lin Proximity-based Thesaurus. For a given word, ESA generates a concept vector, where t</context>
</contexts>
<marker>Haveliwala, 2002</marker>
<rawString>Taher H. Haveliwala. 2002. Topic-sensitive pagerank. In WWW ’02, pages 517–526, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<pages>74--81</pages>
<institution>Association for Computational Linguistics.</institution>
<location>Barcelona, Spain,</location>
<contexts>
<context position="2198" citStr="Lin, 2004" startWordPosition="343" endWordPosition="344">whether a pair of texts exhibit the needed similarity or entailment relationship or not. In many cases, such as producing a ranking over similarity scores, a soft measure of similarity between a pair of texts would be more desirable. We contributed three systems for the 2012 Semantic Textual Similarity (STS) task (Agirre et al., 2012). These are: 1. System 1, which used a combination of semantic similarity, lexical similarity, and precision focused part-of-speech (POS) features. 2. System 2, which used features from System 1, with the addition of skip-bigram features derived from the ROUGE-S (Lin, 2004) measure. POS variants of skip-bigrams were incorporated as well. 3. System 3, used the features from above to first classify the dataset the pair was drawn from, and then applied regressors trained for that dataset. Our systems characterize sentence pairs as feature vectors, populated by a variety of scorers that will be described below. During training, we used support vector regression (SVR) to train regressors against these vectors and their associated similarity scores. The STS training data is divided into three datasets, reflecting their origin: Microsoft Research Paraphrase Corpus (MSR</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. pages 74–81, Barcelona, Spain, jul. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Manning</author>
<author>Dan Klein</author>
</authors>
<title>Optimization, maxent models, and conditional estimation without magic.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology: Tutorials - Volume 5, NAACL-Tutorials ’03,</booktitle>
<pages>8--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="18060" citStr="Manning and Klein, 2003" startWordPosition="2959" endWordPosition="2962">sor against the two surprise datasets, OnWN and SMTnews. Analysis of the similarity score statistics showed that they varied greatly between each of the training sets, as given in Table 1. Thus combining the datasets blindly, as with Systems 1 and 2, may prove to be a suboptimal strategy. The approach taken by System 3 was to consider the feature vectors themselves as capturing information about which dataset they were drawn from, and to use a classifier to predict that dataset. We then emit the score from the regressor trained on just that matching dataset. We used the Stanford Classifier’s (Manning and Klein, 2003) multinomial logistic regression as our dataset predictor, using the feature vectors from System 2. Five-fold cross validation over the training data showed the dataset predictor to have an overall accuracy of 91.75%. In order to assess performance over the known datasets at test time, System 3 also applied the same strategy for the MSRpar, MSRvid, and SMTeuroparl test sets. |sys| 621 Sys All Allnorm Mean MSRpar MSRvid SMTeur OnWN SMTnews 1 0.7513 / 11 0.8017 / 40 0.5997 / 22 0.6084 0.7458 0.4688 0.6315 0.3994 2 0.7562 / 10 0.8111 / 24 0.5858 / 33 0.6050 0.7939 0.4294 0.5871 0.3366 3 0.6876 / </context>
</contexts>
<marker>Manning, Klein, 2003</marker>
<rawString>Christopher Manning and Dan Klein. 2003. Optimization, maxent models, and conditional estimation without magic. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology: Tutorials - Volume 5, NAACL-Tutorials ’03, pages 8–8, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Page</author>
<author>Sergey Brin</author>
<author>Rajeev Motwani</author>
<author>Terry Winograd</author>
</authors>
<title>The pagerank citation ranking: Bringing order to the web.</title>
<date>1999</date>
<booktitle>Previous number =</booktitle>
<tech>Technical Report 1999-66,</tech>
<pages>1999--0120</pages>
<institution>Stanford InfoLab,</institution>
<contexts>
<context position="8520" citStr="Page et al., 1999" startWordPosition="1360" endWordPosition="1363">ionships between synsets, semantic categories a word may belong to. Similarity scoring methods identify the synsets associated with a pair of words, and then use this relationship graph to generate a score. The first set of scorers were generated from the Leacock-Chodorow, Lin, and Wu-Palmer measures from the WordNet Similarity package (Pedersen et al., 2004). For each of these measures, we averaged across all of the possible synsets between a given pair of words. Another scorer we used was Personalized PageRank (PPR) (Agirre et al., 2010), a topic sensitive variant of the PageRank algorithm (Page et al., 1999) that uses a random walk process to identify the significant nodes of a graph given its link structure. We first derived a graph G from WordNet, treating synsets as the vertices and the relationships between synsets as the edges. To obtain a signature for a given word, we apply topic sensitive PageRank (Haveliwala, 2002) over G, using the synsets associated with the word as the initial distribution. At convergence, we convert the stationary distribution into a vector. The similarity between two words is the cosine similarity between their vectors. 2.3 Distributional Resources In contrast with </context>
</contexts>
<marker>Page, Brin, Motwani, Winograd, 1999</marker>
<rawString>Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. The pagerank citation ranking: Bringing order to the web. Technical Report 1999-66, Stanford InfoLab, November. Previous number = SIDL-WP-1999-0120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3414" citStr="Papineni et al., 2002" startWordPosition="522" endWordPosition="526">s (MSRpar), MSR Research Video Description Corpus (MSRvid), and WMT2008 Development dataset (SMTeuroparl). We trained individual regressors for each of these datasets, and applied them to their counterparts in the testing set. Both Systems 1 and 2 used the following types of features: 617 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 617–623, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics 1. Resource based word to word semantic similarities. 2. Cosine-based lexical similarity measure. 3. Bilingual Evaluation Understudy (BLEU) (Papineni et al., 2002) lexical overlap. 4. Precision focused Part of Speech (POS) features. System 2 added the following features: 1. Lexically motivated skip-bigram overlap. 2. Precision focused skip-bigram POS features. One of the primary motivations for our the choice of features was to use relatively simple and fast features, which can be scaled up to large datasets, given appropriate caching and pre-generated lookups. As the test phase included surprise datasets, whose origin was not disclosed, we also trained a fourth model using all of the training data from all three datasets. Systems 1 and 2 employed this </context>
<context position="11485" citStr="Papineni et al., 2002" startWordPosition="1846" endWordPosition="1849">nce sentence, a human generated translation, the better the system sentence is. This can also be seen as a standin for the semantic similarity of the pairs, as was shown when BLEU was applied to the paraphrase identification identification problem in (Finch et al., 2005). The BLEU score for a given system sentence and reference sentence of order N is computed using Formula 2. BLEU(sys, ref) = B · exp 1 N log(pn) (2) N E n�� 619 B is a brevity penalty used to prevent degenerate translations. Given this has little bearing on our experiments, we set its value to 1 for our experiments. Following (Papineni et al., 2002), we give each order n equal weight in the geometric mean. The probability of an order n-gram from the system sentence being found in the reference, pn, is given in Formula 3. pn = EngramEsys countsysnref(ngram) (3) EngramEsys countsys(ngram) countsys(ngram) is frequency of occurrence for the given n-gram in the system sentence. The numerator term is computed as countsysnref(ngram) = min(countsys(ngram), countref(ngram)) where countref(ngram) is the frequency of occurrence of that n-gram in the reference sentence. This is equivalent to having each n-gram have a 1-1 mapping with a matching n-gr</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 311–318, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>WordNet::Similarity - Measuring the Relatedness of Concepts.</title>
<date>2004</date>
<booktitle>In Proceedings of the Nineteenth National Conference on Artificial Intelligence (Intelligent Systems Demonstrations),</booktitle>
<pages>1024--1025</pages>
<location>San Jose, CA,</location>
<contexts>
<context position="8263" citStr="Pedersen et al., 2004" startWordPosition="1316" endWordPosition="1319">generate a corresponding word similarity matrix W, with scores generated using the Semantic Matrix. 2.2 WordNet Similarity We used several methods to obtain word to word similarities from WordNet. WordNet is a lexicalsemantic resource that describes typed relationships between synsets, semantic categories a word may belong to. Similarity scoring methods identify the synsets associated with a pair of words, and then use this relationship graph to generate a score. The first set of scorers were generated from the Leacock-Chodorow, Lin, and Wu-Palmer measures from the WordNet Similarity package (Pedersen et al., 2004). For each of these measures, we averaged across all of the possible synsets between a given pair of words. Another scorer we used was Personalized PageRank (PPR) (Agirre et al., 2010), a topic sensitive variant of the PageRank algorithm (Page et al., 1999) that uses a random walk process to identify the significant nodes of a graph given its link structure. We first derived a graph G from WordNet, treating synsets as the vertices and the relationships between synsets as the edges. To obtain a signature for a given word, we apply topic sensitive PageRank (Haveliwala, 2002) over G, using the sy</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. WordNet::Similarity - Measuring the Relatedness of Concepts. In Proceedings of the Nineteenth National Conference on Artificial Intelligence (Intelligent Systems Demonstrations), pages 1024– 1025, San Jose, CA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eric H Huang</author>
<author>Jeffrey Pennington</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Dynamic pooling and unfolding recursive autoencoders for paraphrase detection.</title>
<date>2011</date>
<booktitle>In Advances in Neural Information Processing Systems 24.</booktitle>
<contexts>
<context position="21109" citStr="Socher et al., 2011" startWordPosition="3461" endWordPosition="3464"> have shown that relatively simple syntax free methods can be employed to the STS task. Future avenues of investigation would Dataset Prec Rec F1 MSRpar 0.8901 0.8853 0.8877 MSRvid 0.9775 0.9827 0.9801 SMTeur 0.8842 0.8842 0.8842 Table 4: Results on classifying pairs by source dataset, using five-fold cross validation over training data. be to include the use of syntactic information, in order to obtain better predicate-argument information. Syntactic information has proven useful for the paraphrase identification task over MSRpar, as demonstrated in studies such as (Das and Smith, 2009) and (Socher et al., 2011). Furthermore, a qualitative assessment of the pairs across different datasets showed relatively significant differences, which would strengthen the argument for developing features and methods specific to each dataset. Another improvement would be to develop a better dataset predictor for System 3. Also recognizing there may be ways to normalize and rescale scores across datasets so the regression models used do not have to account for differing means and standard deviations. Finally, there are other bodies of source data that may be adapted for use with the STS task, such as the paraphrasing</context>
</contexts>
<marker>Socher, Huang, Pennington, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Eric H. Huang, Jeffrey Pennington, Andrew Y. Ng, and Christopher D. Manning. 2011. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In Advances in Neural Information Processing Systems 24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<pages>252--259</pages>
<contexts>
<context position="16340" citStr="Toutanova et al., 2003" startWordPosition="2669" endWordPosition="2672">es between the system and reference sentences, and do not model split-bigram misses. As before, we generate scores for each splitbigram tag sequence by averaging the scores where both sentences in a pair have swapped positions. For our experiments, we only considered split-bigram POS features of up to distance 3. In our initial experiments we found split-bigram POS features helped only in the case of shorter sentence pairs, so we only generated features if both the sentences in a given pair contained ten tokens or less. 6 Experimental Setup For all three systems, we used the Stanford CoreNLP (Toutanova et al., 2003) package to perform lemmatization and POS tagging of the input sentences. For regressors, we used LibSVM’s (Chang and Lin, 2011) support vector regression capability, using radial basis kernels. Based off of tuning on the training set, we set -y = 1 and the default Dataset Mean Std.Dev MSRpar 3.322 0.9294 MSRvid 2.135 1.595 SMTeur 4.307 0.7114 Table 1: Means and standard deviations of similarity scores for each of the training datasets. slack value. From previous experience with paraphrase identification over the MSR Paraphrase Corpus, we retained stop words in all of our experiments. 7 Dealin</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceedings of HLT-NAACL 2003, pages 252–259.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>