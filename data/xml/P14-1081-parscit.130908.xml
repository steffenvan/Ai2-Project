<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001035">
<title confidence="0.991349">
Adaptive HTER Estimation for Document-Specific MT Post-Editing
</title>
<author confidence="0.624185666666667">
Fei Huang ∗
Facebook Inc.
Menlo Park, CA
</author>
<email confidence="0.982069">
feihuang@fb.com
</email>
<note confidence="0.75536">
Jian-Ming Xu Abraham Ittycheriah Salim Roukos
IBM T.J. Watson Research Center
</note>
<author confidence="0.482019">
Yorktown Heights, NY
</author>
<email confidence="0.979863">
{jianxu, abei, roukos}@us.ibm.com
</email>
<sectionHeader confidence="0.993824" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999879090909091">
We present an adaptive translation qual-
ity estimation (QE) method to predict
the human-targeted translation error rate
(HTER) for a document-specific machine
translation model. We first introduce fea-
tures derived internal to the translation de-
coding process as well as externally from
the source sentence analysis. We show
the effectiveness of such features in both
classification and regression of MT qual-
ity. By dynamically training the QE model
for the document-specific MT model, we
are able to achieve consistency and pre-
diction quality across multiple documents,
demonstrated by the higher correlation co-
efficient and F-scores in finding Good sen-
tences. Additionally, the proposed method
is applied to IBM English-to-Japanese MT
post editing field study and we observe
strong correlation with human preference,
with a 10% increase in human translators’
productivity.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9985336">
Machine translation (MT) systems suffer from an
inconsistent and unstable translation quality. De-
pending on the difficulty of the input sentences
(sentence length, OOV words, complex sentence
structures and the coverage of the MT system’s
training data), some translation outputs can be per-
fect, while others are ungrammatical, missing im-
portant words or even totally garbled. As a result,
users do not know whether they can trust the trans-
lation output unless they spend time to analyze
∗This work was done when the author was with IBM Re-
search.
the MT output. This shortcoming is one of the
main obstacles for the adoption of MT systems,
especially in machine assisted human translation:
MT post-editing, where human translators have
an option to edit MT proposals or translate from
scratch. It has been observed that human trans-
lators often discard MT proposals even if some
are very accurate. If MT proposals are used prop-
erly, post-editing can increase translators produc-
tivity and lead to significant cost savings. There-
fore, it is beneficial to provide MT confidence es-
timation, to help the translators to decide whether
to accept MT proposals, making minor modifica-
tions on MT proposals when the quality is high
or translating from scratching when the quality is
low. This will save the time of reading and parsing
low quality MT and improve user experience.
In this paper we propose an adaptive qual-
ity estimation that predicts sentence-level human-
targeted translation error rate (HTER) (Snover et
al., 2006) for a document-specific MT post-editing
system. HTER is an ideal quality measurement
for MT post editing since the reference is ob-
tained from human correction of the MT output.
Document-specific MT model is an MT model that
is specifically built for the given input document.
It is demonstrated in (Roukos et al., 2012) that
document-specific MT models significantly im-
prove the translation quality. However, this raises
two issues for quality estimation. First, existing
approaches to MT quality estimation rely on lex-
ical and syntactical features defined over parallel
sentence pairs, which includes source sentences,
MT outputs and references, and translation models
(Blatz et al., 2004; Ueffing and Ney, 2007; Spe-
cia et al., 2009a; Xiong et al., 2010; Soricut and
Echihabi, 2010a; Bach et al., 2011). Therefore,
when the MT quality estimation model is trained,
</bodyText>
<page confidence="0.973981">
861
</page>
<note confidence="0.831066">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 861–870,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.99983816">
it can not be adapted to provide accurate estimates
on the outputs of document-specific MT models.
Second, the MT quality estimation might be in-
consistent across different document-specific MT
models, thus the confidence score is unreliable and
not very helpful to users.
In contrast to traditional static MT quality es-
timation methods, our approach not only trains
the MT quality estimator dynamically for each
document-specific MT model to obtain higher pre-
diction accuracy, but also achieves consistency
over different document-specific MT models. The
experiments show that our MT quality estima-
tion is highly correlated with human judgment
and helps translators to increase the MT proposal
adoption rate in post-editing.
We will review related work on MT quality es-
timation in section 2. In section 3 we will intro-
duce the document-specific MT system built for
post-editing. We describe the static quality estima-
tion method in section 4, and propose the adaptive
quality estimation method in section 5. In section
6 we demonstrate the improvement of MT quality
estimation with our method, followed by discus-
sion and conclusion in section 7.
</bodyText>
<sectionHeader confidence="0.999692" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999906794871795">
There has been a long history of study in con-
fidence estimation of machine translation. The
work of (Blatz et al., 2004) is among the best
known study of sentence and word level features
for translation error prediction. Along this line of
research, improvements can be obtained by incor-
porating more features as shown in (Quirk, 2004;
Sanchis et al., 2007; Raybaud et al., 2009; Specia
et al., 2009b). Soricut and Echihabi (2010b) pro-
posed various regression models to predict the ex-
pected BLEU score of a given sentence translation
hypothesis. Ueffing and Hey (2007) introduced
word posterior probabilities (WPP) features and
applied them in the n-best list reranking. Target
part-of-speech and null dependency link are ex-
ploited in a MaxEnt classifier to improve the MT
quality estimation (Xiong et al., 2010).
Quality estimation focusing on MT post-editing
has been an active research topic, especially after
the WMT 2012 (Callison-Burch et al., 2012) and
WMT2013 (Bojar et al., 2013) workshops with
the “Quality Estimation” shared task. Bic¸ici et
al. (2013) proposes a number of features mea-
suring the similarity of the source sentence to the
source side of the MT training corpus, which,
combined with features from translation output,
achieved significantly superior performance in the
MT QE evaluation. Felice and Specia (2012) in-
vestigates the impact of a large set of linguisti-
cally inspired features on quality estimation accu-
racy, which are not able to outperform the shal-
lower features based on word statistics. Gonz´alez-
Rubio et al. (2013) proposed a principled method
for performing regression for quality estimation
using dimensionality reduction techniques based
on partial least squares regression. Given the fea-
ture redundancy in MT QE, their approach is able
to improve prediction accuracy while significantly
reducing the size of the feature sets.
</bodyText>
<sectionHeader confidence="0.993298" genericHeader="method">
3 Document-specific MT System
</sectionHeader>
<bodyText confidence="0.998742970588235">
In our MT post-editing setup, we are given docu-
ments in the domain of software manuals, techni-
cal outlook or customer support materials. Each
translation request comes as a document with sev-
eral thousand sentences, focusing on a specific
topic, such as the user manual of some software.
The input documents are automatically seg-
mented into sentences, which are also called seg-
ments. Thus in the rest of the paper we will use
sentences and segments interchangeably. Our par-
allel corpora includes tens of millions of sentence
pairs covering a wide range of topics. Building
a general MT system using all the parallel data
not only produces a huge translation model (unless
with very aggressive pruning), the performance on
the given input document is suboptimal due to the
unwanted dominance of out-of-domain data. Past
research suggests using weighted sentences or cor-
pora for domain adaptation (Lu et al., 2007; Mat-
soukas et al., 2009; Foster et al., 2010). Here
we adopt the same strategy, building a document-
specific translation model for each input docu-
ment.
The document-specific system is built based on
sub-sampling: from the parallel corpora we se-
lect sentence pairs that are the most similar to
the sentences from the input document, then build
the MT system with the sub-sampled sentence
pairs. The similarity is defined as the number of
n-grams that appear in both source sentences, di-
vided by the input sentence’s length, with higher
weights assigned to longer n-grams. From the
extracted sentence pairs, we utilize the standard
pipeline in SMT system building: word align-
</bodyText>
<page confidence="0.997876">
862
</page>
<figureCaption confidence="0.999714">
Figure 1: Adaptive QE for document-specific MT system.
</figureCaption>
<bodyText confidence="0.999922333333333">
ment (HMM (Vogel et al., 1996) and MaxEnt (It-
tycheriah and Roukos, 2005) alignment models,
phrase pair extraction, MT model training (Itty-
cheriah and Roukos, 2007) and LM model train-
ing. The top region within the dashed line in Fig-
ure 1 shows the overall system built pipeline.
</bodyText>
<subsectionHeader confidence="0.999149">
3.1 MT Decoder
</subsectionHeader>
<bodyText confidence="0.999551230769231">
The MT decoder (Ittycheriah and Roukos, 2007)
employed in our study extracts various features
(source words, morphemes and POS tags, target
words and POS tags, etc.) with their weights
trained in a maximum entropy framework. These
features are combined with other features used in
a typical phrase-based translation system. Alto-
gether the decoder incorporates 17 features with
weights estimated by PRO (Hopkins and May,
2011) in the decoding process, and achieves
state-of-the-art translation performance in vari-
ous Arabic-English translation evaluations (NIST
MT2008, GALE and BOLT projects).
</bodyText>
<sectionHeader confidence="0.995633" genericHeader="method">
4 Static MT Quality Estimation
</sectionHeader>
<bodyText confidence="0.999750461538461">
MT quality estimation is typically formulated as
a prediction problem: estimating the confidence
score or translation error rate of the translated sen-
tences or documents based on a set of features. In
this work, we adopt HTER in (Snover et al., 2006)
as our prediction output. HTER measures the per-
centage of insertions, deletions, substitutions and
shifts needed to correct the MT outputs. In the
rest of the paper, we use TER and HTER inter-
changably.
In this section we will first introduce the set of
features, and then discuss MT QE problem from
classification and regression point of views.
</bodyText>
<subsectionHeader confidence="0.744718">
4.1 Features for MT QE
</subsectionHeader>
<bodyText confidence="0.973569818181818">
The features for quality estimation should reflect
the complexity of the source sentence and the de-
coding process. Therefore we conduct syntactic
analysis on the source sentences, extract features
from the decoding process and select the follow-
ing 26 features:
• 17 decoding features, including phrase
translation probabilities (source-to-target and
target-to-source), word translation probabil-
ities (also in both directions), maxent prob-
abilities1, word count, phrase count, distor-
</bodyText>
<footnote confidence="0.912631">
1The maxent probability is the translation probability
</footnote>
<page confidence="0.989856">
863
</page>
<bodyText confidence="0.877973868421053">
tion probabilities, as well as a set of language Configuration Training set Test set
model scores.
• Sentence length, i.e., the number of words in
the source sentence.
• Source sentence syntactic features, including
the number of noun phrases, verb phrases,
adjective phrases, adverb phrases, as in-
spired by (Green et al., 2013).
• The length of verb phrases, because verbs are
typically the roots in dependency structure
and they have more varieties during transla-
tion.
• The maximum length of source phrases in
the final translation, since longer matching
source phrase indicates better coverage of the
input sentence with possibly better transla-
tions.
• The number of phrase pairs with high fuzzy
match (FM) score. The high FM phrases are
selected from sentence pairs which are clos-
est in terms of n-gram overlap to the input
sentence. These sentences are often found in
previous translations of the software manual,
and thus are very helpful for translating the
current sentence.
• The average translation probability of the
phrase translation pairs in the final transla-
tion, which provides the overall translation
quality on the phrase level.
The first 17 features come from the decod-
ing process, which are called “decoding features”.
The remaining 9 features not related to the de-
coder are called “external features”. To evaluate
the effectiveness of the proposed features, we train
various classifiers with different feature configura-
tions to predict whether a translation output is use-
ful (with lower TER) as described in the following
section.
</bodyText>
<table confidence="0.899536222222222">
4.2 MT QE as Classification
Predicting TER with various input features can
be treated as a regression problem. However for
the post-editing task, we argue that it could also
be cast as a classification problem: MT system
Baseline (All negative) 80% 77%
17 decoding features only 89% 79%
9 external features only 85% 81%
total 26 features 92% 83%
</table>
<tableCaption confidence="0.890823">
Table 1: QE classification accuracy with different
</tableCaption>
<bodyText confidence="0.998148089285714">
feature configurations
users (including the translators) are often inter-
ested to know whether a given translation is rea-
sonably good or not. If useful, they can quickly
look through the translation and make minor mod-
ifications. On the other hand, they will just skip
reading and parsing the bad translation, and prefer
to translate by themselves from scratch. Therefore
we also develop algorithms that classify the trans-
lation at different levels, depending on whether the
TER is less than a given threshold. In our experi-
ments, we set TER=0.1 as the threshold.
We randomly select one input document with
2067 sentences for the experiment. We build
a document-specific MT system to translate this
document, then ask human translator to correct
the translation output. We compute TER for each
sentence using the human correction as the refer-
ence. The TER of the whole document is 0.31,
which means about 30% errors should be cor-
rected. In the classification task, our goal is to pre-
dict whether a sentence is a Good translation (with
TER G 0.1), and label them for human correction.
We adopt a decision tree-based classifier, experi-
menting with different feature configurations. We
select the top 1867 sentences for training and the
bottom 200 sentences for test. In the test set, there
are 46 sentences with TER G 0.1. Table 1 shows
the classification accuracy.
First we can see that as the overall TER is
around 0.3, predicting all the sentences being neg-
ative already has a strong baseline: 77%. How-
ever this is not helpful for the human translators,
because that means they have to translate every
sentence from scratch, and consequently there is
no productivity gain from MT post-editing. If we
only use the 17 decoding features, it improves the
classification accuracy by 9% on the training set,
but only 2% on the test set. This is probably due to
the overfitting when training the decision tree clas-
sifier. While using the 7 external features, the gain
on training set is less but the gain on the test set
derived from a Maximum Entropy translation model (Itty- 864
cheriah and Roukos, 2005).
is greater (4% improvement), because the trans-
lation output is generated based on the log-linear
combination of these decoding features, which are
biased towards the final translations. The exter-
nal features capture the syntactic structure of the
source sentence, as well as the coverage of the
training data with regard to the input sentence,
which are good indicators of the translation qual-
ity. Combining both the decoding features and the
external features, we observed the best accuracy
on both the training and test set. We will use the
combined 26 features in the following work.
</bodyText>
<subsectionHeader confidence="0.989982">
4.3 MT QE as Regression
</subsectionHeader>
<bodyText confidence="0.999980714285714">
For the QE regression task, we predict the TER for
each sentence translation using the above 26 fea-
tures. We experiment with several classifiers: lin-
ear regression model, decision tree based regres-
sion model and SVM model. With the same train-
ing and test data set up, we predict the TER for
each sentence in the test set, and compute the cor-
relation coefficient (r) and root mean square error
(RMSE). Our experiments show that the decision
tree-based regression model obtains the highest
correlation coefficients (0.53) and lowest RMSE
(0.23) in both the training and test sets. We will
use this model for the adaptive MT QE in the fol-
lowing work.
</bodyText>
<sectionHeader confidence="0.997874" genericHeader="method">
5 Adaptive MT Quality Estimation
</sectionHeader>
<bodyText confidence="0.968666742857143">
The above QE regression model is trained on a
portion of the sentences from the input document,
and evaluated on the remaining sentences from the
same document. One would like to know whether
the trained model can achieve consistent TER pre-
diction accuracy on other documents. When we
use the cross-document models for prediction, the
correlation is significantly worse (the details are
discussed in section 6.1). Therefore it is neces-
sary to build a QE regression model that’s robust
to different document-specific translation models.
To deal with this problem, we propose this adap-
tive MT QE method described below.
Our proposed method is as follows: we select a
fixed set of sentence pairs (5Q, RQ) to train the QE
model. The source side of the QE training data
5Q is combined with the input document 5d for
MT system training data subsampling. Once the
document-specific MT system is trained, we use it
to translate both the input document and the source
QE training data, obtaining the translation Td and
Figure 2: Correlation coefficient r between pre-
dicted TER (x-axis) and true TER (y-axis) for QE
models trained from the same document (top fig-
ure) or different document (bottom figure).
TQ . We compute the TER of TQ using RQ as the
reference, and train a QE regression model with
the 26 features proposed in section 4.1. Then we
use this document-specific QE model to predict the
TER of the document translation Td. As the QE
model is adaptively re-trained for each document-
specific MT system, its prediction is more accurate
and consistent. Figure 1 shows the flow of our MT
system with the adaptive QE training integrated as
part of the built.
</bodyText>
<sectionHeader confidence="0.999708" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999932071428572">
In this section, we first discuss experiments that
compare adaptive QE method and static QE
method on a few documents, and then present
results we obtained after deploying the adaptive
QE method in an English-to-Japanese MT Post-
Editing project. As mentioned before, the main
motivation for us to develop MT QE classification
scheme is that translators often discard good MT
proposals and translate the segments from scratch.
We would like to provide translators with some
guidance on reasonably good MT proposals–the
sentences with low TERs–to help them increase
the leverage on MT proposals to achieve improved
productivity.
</bodyText>
<page confidence="0.996317">
865
</page>
<subsectionHeader confidence="0.987866">
6.1 Evaluation on Test Set
</subsectionHeader>
<bodyText confidence="0.999978435897436">
Our experiment and evaluation is conducted over
three documents, each with about 2000 segments.
We first build document-specific MT model for
each document, then ask human translators to cor-
rect the MT outputs and obtain the reference trans-
lation. In a typical MT QE scenario, the QE model
is pre-trained and applied to various MT outputs,
even though the QE training data and MT out-
puts are generated from different translation mod-
els. To evaluate whether such model mismatch
matters, we compare the cross-model QE with the
same-model QE, where the QE training data and
the MT outputs are generated from the same MT
model.
We select one document LZA with 2067 sen-
tences. We use the first 1867 sentences to train the
static QE model and the remaining 200 sentences
are used as test set for TER prediction. We com-
pute the correlation coefficient (r) between each
predicted TER and true TER, as shown in Figure
2. We find that the TER predictions are reason-
ably correct when the training and test sentences
are from the same MT model (the top figure), with
correlation coefficients around 0.5. For the cross-
model QE, we train a static QE model with 1867
sentences from another document RTW, and use it
to predict the TER of the same 200 sentences from
document LZA (the bottom figure). We observe
significant degradation of correlation coefficient,
dropping from 0.5 to 0.1. This degradation and
unstable nature is the prime motivation to develop
a more robust MT quality estimation model.
We select 1700 sentences from multiple pre-
viously translated documents as the QE training
data, which are independent of the test documents.
We train the static QE model with this training set,
including the source sentences, references and MT
outputs (from multiple translation models). To
train the adaptive QE model for each test docu-
ment, we build a translation model whose subsam-
pling data includes source sentences from both the
test document and the QE training data. We trans-
late the QE source sentences with this newly built
MT model, and the translation output is used to
train the QE model specific to each test document.
We compare these two QE models on three doc-
uments, LZA, RTW and WC7, measuring r and
RMSE for each QE model. The result is shown
in Table 2. We find that the adaptive QE model
demonstrates higher r and lower RMSE than the
static QE model for all the test documents.
Besides the general correlation with human
judgment, we particularly focus on those reason-
ably good translations, i.e., the sentences with low
TERs which can help improve the translator’s pro-
ductivity most. Here we report the precision, re-
call and F-score of finding such “Good” sentences
(with TER G 0.1) on the three documents in Ta-
ble 3. Again, the adaptive QE model produces
higher recall, mostly higher precision, and signif-
icantly improved F-score. The overall F-score of
the adaptive QE model is 0.282. Compared with
the static QE model’s 0.17 F-score, this is rela-
tively 64% improvement.
In the adaptive QE model, the source side QE
training data is included in the subsampling pro-
cess to build the document-specific MT model. It
would be interesting to know whether this process
will negatively affect the MT quality. We evaluate
the TER of MT outputs with and without the adap-
tive QE training on the same three documents. As
seen in Table 4, we do not notice translation qual-
ity degradation. Instead, we observe slightly im-
provement on two document, with TERs reduction
by 0.1-0.4 pt. As our MT model training data in-
clude proprietary data, the MT performance is sig-
nificantly better than publicly available MT soft-
ware.
</bodyText>
<subsectionHeader confidence="0.999217">
6.2 Impact on Human Translators
</subsectionHeader>
<bodyText confidence="0.99994825">
We apply the proposed adaptive QE model to
large scale English-to-Japanese MT Post-Editing
project on 36 documents with 562K words. Each
English sentence can be categorized into 3 classes:
</bodyText>
<listItem confidence="0.992502538461538">
• Exact Match (EM): the source sentence is
completely covered in the bilingual training
corpora thus the corresponding target sen-
tence is returned as the translation;
• Fuzzy Match (FM): the source sentence is
similar to some sentence in the training data
(similarity measured by string editing dis-
tance), the corresponding fuzzy match target
sentence (FM proposal) as well as the MT
translation output (MT proposal) are returned
for human translators to select and correct;
• No Proposal (NP): there is no close match
source sentences in the training data (the FM
</listItem>
<footnote confidence="0.8730255">
2The adaptive QE model obtains much higher F-score
(80%) on the rest of the sentences (with TER &gt; 0.1).
</footnote>
<page confidence="0.991649">
866
</page>
<table confidence="0.9999032">
Document LZA RTW WC7
Num. of Sents 2067 2003 2405
r T RMSE 1 r T RMSE 1 r T RMSE 1
Static QE 0.10 0.38 0.40 0.32 0.13 0.36
Adaptive QE 0.58 0.23 0.61 0.22 0.47 0.20
</table>
<tableCaption confidence="0.589956">
Table 2: QE regression with static and adaptive models
</tableCaption>
<table confidence="0.9999754">
Document LZA RTW WC7
Num. of Sents 2067 2003 2405
P/R/F-score P/R/F-score P/R/F-score
Static QE 0.73/0.08/0.14 0.69/ 0.11/ 0.19 0.74/ 0.10/ 0.18
Adaptive QE 0.69/0.14/0.24 0.84/ 0.16/ 0.26 0.80/ 0.23/ 0.35
</table>
<tableCaption confidence="0.999953">
Table 3: Performance on predicting Good sentences with static and adaptive models
</tableCaption>
<bodyText confidence="0.999092848484849">
similarity score of 70% is used as the thresh-
old), therefore only the MT output is re-
turned.
EM sentences are excluded from the study be-
cause in general they do not require editing. We
focus on the FM and NP sentences3. In Table 5
we present the precision, recall and F-score of the
“Good” sentences in the FM and NP categories,
similar to those shown in Table 3. We consistently
observe higher performance on the FM sentences,
in terms of precision, recall and F-score. This is
expected because these sentences are well covered
in the training data. The overall F-score is in line
with the test set results shown in Table 3.
We are also interested to know whether the pro-
posed adaptive QE method is helpful to human
translators in the MT post-editing task. Based on
the TERs predicted by the adaptive QE model, we
assign each MT proposal with a confidence label:
High (0 &lt; TER &lt; 0.2), Medium (0.2 &lt; TER &lt;
0.3), or Low (TER &gt; 0.3). We present the MT pro-
posals with confidence labels to human translators,
then measure the percentage of sentences whose
MT proposals are used. From Table 6 and 7,
we can see that sentences with High and Medium
confidence labels are more frequently used by the
translators than those with Low labels, for both the
FM and NP categories. The MT usage for the FM
category is less than that for the NP category be-
cause translators can choose FM proposals instead
of the MT proposals for correction.
We also measure the translator’s productivity
gain for MT proposals with different confidence
</bodyText>
<footnote confidence="0.928483">
3The word count distribution of EM, FM and NP is 21%,
38% and 41%, respectively.
</footnote>
<table confidence="0.999611666666667">
Document LZA RTW WC7
TER-Baseline 30.81 30.74 29.96
TER-with Adaptive QE 30.69 30.78 29.56
</table>
<tableCaption confidence="0.9422055">
Table 4: MT Quality with and without Adaptive
QE measured by TER
</tableCaption>
<bodyText confidence="0.999873888888889">
labels. The productivity of a translator is defined
as the number of source words translated per unit
time. The post editing tool, IBM TranslationMan-
ager, records the time that a translator spends on
a segment and computes the number of characters
that a translator types on the segment so that we
can compute how many words the translator has
finished in a given time.
We choose the overall productivity of NP0 as
the base unit 1, where there is no proposal presents
and the translator has to translate the segments
from scratch. Measured with this unit, for exam-
ple, the overall productivity of FM0 being 1.14
implies a relative gain of 14% over that of NP0,
which demonstrates the effectiveness of FM pro-
posals.
Table 6 and 7 also show the productivity gain
on sentences with High, Medium and Low labels
from FM and NP categories. Again, the produc-
tivity gain is consistent with the confidence labels
from the adaptive QE model’s prediction. The
overall productivity gain with confidence-labeled
MT proposals is about 10% (comparing FM1 vs.
FM0 and NP1 vs. NP0). These results clearly
demonstrate the effectiveness of the adaptive QE
model in aiding the translators to make use of MT
proposals and improve productivity.
</bodyText>
<page confidence="0.992983">
867
</page>
<table confidence="0.999877">
Category Class FM usage MT usage Productivity
High 33% 34% 1.35
FM1 Medium 47% 18% 1.21
Low 60% 8% 1.20
Overall 45% 21% 1.26
High 53% - 1.12
FM0 Medium 64% - 1.14
Low 67% - 1.16
Overall 59% - 1.14
</table>
<tableCaption confidence="0.998643">
Table 6: MT proposal usage and productivity gain in FM category.
</tableCaption>
<bodyText confidence="0.992229">
In FM1, both Fuzzy Match and MT proposals present. In control class FM0, only Fuzzy Match proposals
present, and therefore, MT usage is not available for FM0. Strong correlation is observed between
predicted “High” , “Medium” and “Low” sentences with MT usage and post editing productivity.
</bodyText>
<table confidence="0.999209666666667">
Category Class MT usage Productivity
High 50% 1.25
NP1 Medium 42% 1.08
Low 27% 1.00
Overall 38% 1.09
High - 1.08
NP0 Medium - 1.00
Low - 0.96
Overall - 1.00
</table>
<tableCaption confidence="0.999394">
Table 7: MT proposal usage and productivity gain in NP category.
</tableCaption>
<bodyText confidence="0.998161">
In NP1, MT is the only proposal available, while in control NP0, there presents no proposal at all and
the translator has to translate from scratch. Strong correlation is observed between predicted “High” ,
“Medium” and “Low” sentences with MT usage and post editing productivity
</bodyText>
<page confidence="0.993002">
868
</page>
<table confidence="0.99987975">
Type Precision Recall F-score
FM 0.71 0.23 0.35
NP 0.67 0.18 0.29
Overall 0.69 0.21 0.32
</table>
<tableCaption confidence="0.9664145">
Table 5: Performance on predicting Good sen-
tences (TER ≤ 0.1) by adaptive QE model
</tableCaption>
<sectionHeader confidence="0.990882" genericHeader="conclusions">
7 Discussion and Conclusion
</sectionHeader>
<bodyText confidence="0.99956728125">
In this paper we proposed a method to adaptively
train a quality estimation model for document-
specific MT post editing. With the 26 pro-
posed features derived from decoding process and
source sentence syntactic analysis, the proposed
QE model achieved better TER prediction, higher
correlation with human correction of MT output
and higher F-score in finding good translations.
The proposed adaptive QE model is deployed to
a large scale English-to-Japanese MT post edit-
ing project, showing strong correlation with hu-
man preference and leading to about 10% gain in
human translator productivity.
The training data for QE model can be selected
independent of the input document. With such
fixed QE training data, it is possible to measure the
consistency of the trained QE models, and to al-
low the sanity check of the document-specific MT
models. However, adding such data in the sub-
sampling process extracts more bilingual data for
building the MT models, which slightly increase
the model building time but increased the transla-
tion quality. Another option is to select the sen-
tence pairs from the MT system subsampled train-
ing data, which is more similar to the input docu-
ment thus the trained QE model could be a better
match to the input document. However, the QE
model training data is no longer constant. The
model consistency is no longer guaranteed, and
the QE training data must be removed from the
MT system training data to avoid data contamina-
tion.
</bodyText>
<sectionHeader confidence="0.998984" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999617387096774">
Nguyen Bach, Fei Huang, and Yaser Al-Onaizan.
2011. Goodness: A method for measuring machine
translation confidence. In ACL, pages 211–219.
Ergun Bic¸ici, Declan Groves, and Josef van Genabith.
2013. Predicting sentence translation quality using
extrinsic and language independent features. Ma-
chine Translation.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2004. Confidence es-
timation for machine translation. In Proceedings of
the 20th international conference on Computational
Linguistics, COLING ’04, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Ondrej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut,
and Lucia Specia. 2013. Findings of the 2013
Workshop on Statistical Machine Translation. In
Eighth Workshop on Statistical Machine Transla-
tion, WMT-2013, pages 1–44, Sofia, Bulgaria.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Seventh Workshop on Statis-
tical Machine Translation, pages 10–51, Montr´eal,
Canada.
Mariano Felice and Lucia Specia. 2012. Linguistic
features for quality estimation. In Seventh Workshop
on Statistical Machine Translation, pages 96–103,
Montr´eal, Canada.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adap-
tation in statistical machine translation. In Proceed-
ings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ’10,
pages 451–459, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Jes´us Gonz´alez-Rubio, Jose Ram´on Navarro-Cerd´an,
and Francisco Casacuberta. 2013. Dimensionality
reduction methods for machine translation quality
estimation. Machine Translation, 27(3-4):281–301.
Spence Green, Jeffrey Heer, and Christopher D. Man-
ning. 2013. The efficacy of human post-editing for
language translation. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Sys-
tems, CHI ’13, pages 439–448, New York, NY,
USA. ACM.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352–1362, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Abraham Ittycheriah and Salim Roukos. 2005. A
maximum entropy word aligner for arabic-english
machine translation. In In Proceedings of HLT-
EMNLP, pages 89–96.
Abraham Ittycheriah and Salim Roukos. 2007. Direct
translation model 2. In In HLT-NAACL 2007: Main
Conference, pages 57–64.
Yajuan Lu, Jin Huang, and Qun Liu. 2007. Improv-
ing statistical machine translation performance by
</reference>
<page confidence="0.987817">
869
</page>
<reference confidence="0.99909375">
training data selection and optimization. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 343–350, Prague, Czech Republic,
June. Association for Computational Linguistics.
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing
Zhang. 2009. Discriminative corpus weight es-
timation for machine translation. In Proceedings
of the 2009 Conference on Empirical Methods in
Natural Language Processing: Volume 2 - Volume
2, EMNLP ’09, pages 708–717, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Christopher B. Quirk. 2004. Training a sentence-level
machine translation confidence measure. In In Pro-
ceedings of LREC.
Sylvain Raybaud, Caroline Lavecchia, David Langlois,
and Kamel Smaili. 2009. New confidence mea-
sures for statistical machine translation. CoRR,
abs/0902.1033.
Salim Roukos, Abraham Ittycheriah, and Jian-Ming
Xu. 2012. Document-specific statistical machine
translation for improving human translation produc-
tivity. In Proceedings of the 13th international con-
ference on Computational Linguistics and Intelli-
gent Text Processing - Volume Part II, CICLing’12,
pages 25–39, Berlin, Heidelberg. Springer-Verlag.
Alberto Sanchis, Alfons Juan, Enrique Vidal, and De-
partament De Sistemes Informtics. 2007. Estima-
tion of confidence measures for machine translation.
In In Procedings of Machine Translation Summit XI.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human annota-
tion. In In Proceedings of Association for Machine
Translation in the Americas, pages 223–231.
Radu Soricut and Abdessamad Echihabi. 2010a.
Trustrank: inducing trust in automatic translations
via ranking. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, ACL ’10, pages 612–621, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Radu Soricut and Abdessamad Echihabi. 2010b.
Trustrank: Inducing trust in automatic translations
via ranking. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 612–621. Association for Computa-
tional Linguistics.
Lucia Specia, Craig Saunders, Marco Turchi, Zhuoran
Wang, and John Shawe-taylor. 2009a. Improving
the confidence of machine translation quality esti-
mates. In In Proceedings of MT Summit XII.
Lucia Specia, Marco Turchi, Zhuoran Wang, John
Shawe-Taylor, and Craig Saunders. 2009b. Improv-
ing the confidence of machine translation quality es-
timates.
Nicola Ueffing and Hermann Ney. 2007. Word-
level confidence estimation for machine translation.
Computational Linguistics, 33(1):9–40.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical
translation. In Proceedings of the 16th Conference
on Computational Linguistics - Volume 2, COLING
’96, pages 836–841, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Deyi Xiong, Min Zhang, and Haizhou Li. 2010. Er-
ror detection for statistical machine translation using
linguistic features. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ’10, pages 604–611, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
</reference>
<page confidence="0.997753">
870
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.343790">
<title confidence="0.966952666666667">Adaptive HTER Estimation for Document-Specific MT Post-Editing Huang Facebook</title>
<author confidence="0.966752">Menlo Park</author>
<email confidence="0.999913">feihuang@fb.com</email>
<author confidence="0.996168">Jian-Ming Xu Abraham Ittycheriah Salim Roukos</author>
<affiliation confidence="0.7505045">IBM T.J. Watson Research Yorktown Heights, NY</affiliation>
<email confidence="0.897465">abei,</email>
<abstract confidence="0.99558552173913">We present an adaptive translation quality estimation (QE) method to predict the human-targeted translation error rate (HTER) for a document-specific machine translation model. We first introduce features derived internal to the translation decoding process as well as externally from the source sentence analysis. We show the effectiveness of such features in both classification and regression of MT quality. By dynamically training the QE model for the document-specific MT model, we are able to achieve consistency and prediction quality across multiple documents, demonstrated by the higher correlation coand F-scores in finding sentences. Additionally, the proposed method is applied to IBM English-to-Japanese MT post editing field study and we observe strong correlation with human preference, with a 10% increase in human translators’ productivity.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nguyen Bach</author>
<author>Fei Huang</author>
<author>Yaser Al-Onaizan</author>
</authors>
<title>Goodness: A method for measuring machine translation confidence.</title>
<date>2011</date>
<booktitle>In ACL,</booktitle>
<pages>211--219</pages>
<contexts>
<context position="3471" citStr="Bach et al., 2011" startWordPosition="535" endWordPosition="538">-specific MT model is an MT model that is specifically built for the given input document. It is demonstrated in (Roukos et al., 2012) that document-specific MT models significantly improve the translation quality. However, this raises two issues for quality estimation. First, existing approaches to MT quality estimation rely on lexical and syntactical features defined over parallel sentence pairs, which includes source sentences, MT outputs and references, and translation models (Blatz et al., 2004; Ueffing and Ney, 2007; Specia et al., 2009a; Xiong et al., 2010; Soricut and Echihabi, 2010a; Bach et al., 2011). Therefore, when the MT quality estimation model is trained, 861 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 861–870, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics it can not be adapted to provide accurate estimates on the outputs of document-specific MT models. Second, the MT quality estimation might be inconsistent across different document-specific MT models, thus the confidence score is unreliable and not very helpful to users. In contrast to traditional static MT quality estimation methods, o</context>
</contexts>
<marker>Bach, Huang, Al-Onaizan, 2011</marker>
<rawString>Nguyen Bach, Fei Huang, and Yaser Al-Onaizan. 2011. Goodness: A method for measuring machine translation confidence. In ACL, pages 211–219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
<author>Declan Groves</author>
<author>Josef van Genabith</author>
</authors>
<title>Predicting sentence translation quality using extrinsic and language independent features. Machine Translation.</title>
<date>2013</date>
<marker>Bic¸ici, Groves, van Genabith, 2013</marker>
<rawString>Ergun Bic¸ici, Declan Groves, and Josef van Genabith. 2013. Predicting sentence translation quality using extrinsic and language independent features. Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blatz</author>
<author>Erin Fitzgerald</author>
<author>George Foster</author>
<author>Simona Gandrabur</author>
<author>Cyril Goutte</author>
<author>Alex Kulesza</author>
<author>Alberto Sanchis</author>
<author>Nicola Ueffing</author>
</authors>
<title>Confidence estimation for machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics, COLING ’04,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3357" citStr="Blatz et al., 2004" startWordPosition="514" endWordPosition="517">y measurement for MT post editing since the reference is obtained from human correction of the MT output. Document-specific MT model is an MT model that is specifically built for the given input document. It is demonstrated in (Roukos et al., 2012) that document-specific MT models significantly improve the translation quality. However, this raises two issues for quality estimation. First, existing approaches to MT quality estimation rely on lexical and syntactical features defined over parallel sentence pairs, which includes source sentences, MT outputs and references, and translation models (Blatz et al., 2004; Ueffing and Ney, 2007; Specia et al., 2009a; Xiong et al., 2010; Soricut and Echihabi, 2010a; Bach et al., 2011). Therefore, when the MT quality estimation model is trained, 861 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 861–870, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics it can not be adapted to provide accurate estimates on the outputs of document-specific MT models. Second, the MT quality estimation might be inconsistent across different document-specific MT models, thus the confidence sco</context>
<context position="5013" citStr="Blatz et al., 2004" startWordPosition="775" endWordPosition="778">tors to increase the MT proposal adoption rate in post-editing. We will review related work on MT quality estimation in section 2. In section 3 we will introduce the document-specific MT system built for post-editing. We describe the static quality estimation method in section 4, and propose the adaptive quality estimation method in section 5. In section 6 we demonstrate the improvement of MT quality estimation with our method, followed by discussion and conclusion in section 7. 2 Related Work There has been a long history of study in confidence estimation of machine translation. The work of (Blatz et al., 2004) is among the best known study of sentence and word level features for translation error prediction. Along this line of research, improvements can be obtained by incorporating more features as shown in (Quirk, 2004; Sanchis et al., 2007; Raybaud et al., 2009; Specia et al., 2009b). Soricut and Echihabi (2010b) proposed various regression models to predict the expected BLEU score of a given sentence translation hypothesis. Ueffing and Hey (2007) introduced word posterior probabilities (WPP) features and applied them in the n-best list reranking. Target part-of-speech and null dependency link ar</context>
</contexts>
<marker>Blatz, Fitzgerald, Foster, Gandrabur, Goutte, Kulesza, Sanchis, Ueffing, 2004</marker>
<rawString>John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola Ueffing. 2004. Confidence estimation for machine translation. In Proceedings of the 20th international conference on Computational Linguistics, COLING ’04, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondrej Bojar</author>
<author>Christian Buck</author>
<author>Chris Callison-Burch</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2013</date>
<booktitle>Findings of the 2013 Workshop on Statistical Machine Translation. In Eighth Workshop on Statistical Machine Translation, WMT-2013,</booktitle>
<pages>1--44</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="5881" citStr="Bojar et al., 2013" startWordPosition="912" endWordPosition="915">al., 2009; Specia et al., 2009b). Soricut and Echihabi (2010b) proposed various regression models to predict the expected BLEU score of a given sentence translation hypothesis. Ueffing and Hey (2007) introduced word posterior probabilities (WPP) features and applied them in the n-best list reranking. Target part-of-speech and null dependency link are exploited in a MaxEnt classifier to improve the MT quality estimation (Xiong et al., 2010). Quality estimation focusing on MT post-editing has been an active research topic, especially after the WMT 2012 (Callison-Burch et al., 2012) and WMT2013 (Bojar et al., 2013) workshops with the “Quality Estimation” shared task. Bic¸ici et al. (2013) proposes a number of features measuring the similarity of the source sentence to the source side of the MT training corpus, which, combined with features from translation output, achieved significantly superior performance in the MT QE evaluation. Felice and Specia (2012) investigates the impact of a large set of linguistically inspired features on quality estimation accuracy, which are not able to outperform the shallower features based on word statistics. Gonz´alezRubio et al. (2013) proposed a principled method for </context>
</contexts>
<marker>Bojar, Buck, Callison-Burch, Federmann, Haddow, Koehn, Monz, Post, Soricut, Specia, 2013</marker>
<rawString>Ondrej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine Translation. In Eighth Workshop on Statistical Machine Translation, WMT-2013, pages 1–44, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<title>Findings of the 2012 workshop on statistical machine translation.</title>
<date>2012</date>
<booktitle>In Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>10--51</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="5848" citStr="Callison-Burch et al., 2012" startWordPosition="906" endWordPosition="909">k, 2004; Sanchis et al., 2007; Raybaud et al., 2009; Specia et al., 2009b). Soricut and Echihabi (2010b) proposed various regression models to predict the expected BLEU score of a given sentence translation hypothesis. Ueffing and Hey (2007) introduced word posterior probabilities (WPP) features and applied them in the n-best list reranking. Target part-of-speech and null dependency link are exploited in a MaxEnt classifier to improve the MT quality estimation (Xiong et al., 2010). Quality estimation focusing on MT post-editing has been an active research topic, especially after the WMT 2012 (Callison-Burch et al., 2012) and WMT2013 (Bojar et al., 2013) workshops with the “Quality Estimation” shared task. Bic¸ici et al. (2013) proposes a number of features measuring the similarity of the source sentence to the source side of the MT training corpus, which, combined with features from translation output, achieved significantly superior performance in the MT QE evaluation. Felice and Specia (2012) investigates the impact of a large set of linguistically inspired features on quality estimation accuracy, which are not able to outperform the shallower features based on word statistics. Gonz´alezRubio et al. (2013) </context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 workshop on statistical machine translation. In Seventh Workshop on Statistical Machine Translation, pages 10–51, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mariano Felice</author>
<author>Lucia Specia</author>
</authors>
<title>Linguistic features for quality estimation.</title>
<date>2012</date>
<booktitle>In Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>96--103</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="6229" citStr="Felice and Specia (2012)" startWordPosition="965" endWordPosition="968"> link are exploited in a MaxEnt classifier to improve the MT quality estimation (Xiong et al., 2010). Quality estimation focusing on MT post-editing has been an active research topic, especially after the WMT 2012 (Callison-Burch et al., 2012) and WMT2013 (Bojar et al., 2013) workshops with the “Quality Estimation” shared task. Bic¸ici et al. (2013) proposes a number of features measuring the similarity of the source sentence to the source side of the MT training corpus, which, combined with features from translation output, achieved significantly superior performance in the MT QE evaluation. Felice and Specia (2012) investigates the impact of a large set of linguistically inspired features on quality estimation accuracy, which are not able to outperform the shallower features based on word statistics. Gonz´alezRubio et al. (2013) proposed a principled method for performing regression for quality estimation using dimensionality reduction techniques based on partial least squares regression. Given the feature redundancy in MT QE, their approach is able to improve prediction accuracy while significantly reducing the size of the feature sets. 3 Document-specific MT System In our MT post-editing setup, we are</context>
</contexts>
<marker>Felice, Specia, 2012</marker>
<rawString>Mariano Felice and Lucia Specia. 2012. Linguistic features for quality estimation. In Seventh Workshop on Statistical Machine Translation, pages 96–103, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Cyril Goutte</author>
<author>Roland Kuhn</author>
</authors>
<title>Discriminative instance weighting for domain adaptation in statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>451--459</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7749" citStr="Foster et al., 2010" startWordPosition="1209" endWordPosition="1212">nto sentences, which are also called segments. Thus in the rest of the paper we will use sentences and segments interchangeably. Our parallel corpora includes tens of millions of sentence pairs covering a wide range of topics. Building a general MT system using all the parallel data not only produces a huge translation model (unless with very aggressive pruning), the performance on the given input document is suboptimal due to the unwanted dominance of out-of-domain data. Past research suggests using weighted sentences or corpora for domain adaptation (Lu et al., 2007; Matsoukas et al., 2009; Foster et al., 2010). Here we adopt the same strategy, building a documentspecific translation model for each input document. The document-specific system is built based on sub-sampling: from the parallel corpora we select sentence pairs that are the most similar to the sentences from the input document, then build the MT system with the sub-sampled sentence pairs. The similarity is defined as the number of n-grams that appear in both source sentences, divided by the input sentence’s length, with higher weights assigned to longer n-grams. From the extracted sentence pairs, we utilize the standard pipeline in SMT </context>
</contexts>
<marker>Foster, Goutte, Kuhn, 2010</marker>
<rawString>George Foster, Cyril Goutte, and Roland Kuhn. 2010. Discriminative instance weighting for domain adaptation in statistical machine translation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 451–459, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gonz´alez-Rubio</author>
<author>Jose Ram´on Navarro-Cerd´an</author>
<author>Francisco Casacuberta</author>
</authors>
<title>Dimensionality reduction methods for machine translation quality estimation.</title>
<date>2013</date>
<journal>Machine Translation,</journal>
<pages>27--3</pages>
<marker>Gonz´alez-Rubio, Navarro-Cerd´an, Casacuberta, 2013</marker>
<rawString>Jes´us Gonz´alez-Rubio, Jose Ram´on Navarro-Cerd´an, and Francisco Casacuberta. 2013. Dimensionality reduction methods for machine translation quality estimation. Machine Translation, 27(3-4):281–301.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spence Green</author>
<author>Jeffrey Heer</author>
<author>Christopher D Manning</author>
</authors>
<title>The efficacy of human post-editing for language translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’13,</booktitle>
<pages>439--448</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="10841" citStr="Green et al., 2013" startWordPosition="1694" endWordPosition="1697"> features: • 17 decoding features, including phrase translation probabilities (source-to-target and target-to-source), word translation probabilities (also in both directions), maxent probabilities1, word count, phrase count, distor1The maxent probability is the translation probability 863 tion probabilities, as well as a set of language Configuration Training set Test set model scores. • Sentence length, i.e., the number of words in the source sentence. • Source sentence syntactic features, including the number of noun phrases, verb phrases, adjective phrases, adverb phrases, as inspired by (Green et al., 2013). • The length of verb phrases, because verbs are typically the roots in dependency structure and they have more varieties during translation. • The maximum length of source phrases in the final translation, since longer matching source phrase indicates better coverage of the input sentence with possibly better translations. • The number of phrase pairs with high fuzzy match (FM) score. The high FM phrases are selected from sentence pairs which are closest in terms of n-gram overlap to the input sentence. These sentences are often found in previous translations of the software manual, and thus</context>
</contexts>
<marker>Green, Heer, Manning, 2013</marker>
<rawString>Spence Green, Jeffrey Heer, and Christopher D. Manning. 2013. The efficacy of human post-editing for language translation. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’13, pages 439–448, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonathan May</author>
</authors>
<title>Tuning as ranking.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1352--1362</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="9153" citStr="Hopkins and May, 2011" startWordPosition="1435" endWordPosition="1438">e pair extraction, MT model training (Ittycheriah and Roukos, 2007) and LM model training. The top region within the dashed line in Figure 1 shows the overall system built pipeline. 3.1 MT Decoder The MT decoder (Ittycheriah and Roukos, 2007) employed in our study extracts various features (source words, morphemes and POS tags, target words and POS tags, etc.) with their weights trained in a maximum entropy framework. These features are combined with other features used in a typical phrase-based translation system. Altogether the decoder incorporates 17 features with weights estimated by PRO (Hopkins and May, 2011) in the decoding process, and achieves state-of-the-art translation performance in various Arabic-English translation evaluations (NIST MT2008, GALE and BOLT projects). 4 Static MT Quality Estimation MT quality estimation is typically formulated as a prediction problem: estimating the confidence score or translation error rate of the translated sentences or documents based on a set of features. In this work, we adopt HTER in (Snover et al., 2006) as our prediction output. HTER measures the percentage of insertions, deletions, substitutions and shifts needed to correct the MT outputs. In the re</context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>Mark Hopkins and Jonathan May. 2011. Tuning as ranking. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1352–1362, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abraham Ittycheriah</author>
<author>Salim Roukos</author>
</authors>
<title>A maximum entropy word aligner for arabic-english machine translation. In</title>
<date>2005</date>
<booktitle>In Proceedings of HLTEMNLP,</booktitle>
<pages>89--96</pages>
<contexts>
<context position="8507" citStr="Ittycheriah and Roukos, 2005" startWordPosition="1331" endWordPosition="1335">ystem is built based on sub-sampling: from the parallel corpora we select sentence pairs that are the most similar to the sentences from the input document, then build the MT system with the sub-sampled sentence pairs. The similarity is defined as the number of n-grams that appear in both source sentences, divided by the input sentence’s length, with higher weights assigned to longer n-grams. From the extracted sentence pairs, we utilize the standard pipeline in SMT system building: word align862 Figure 1: Adaptive QE for document-specific MT system. ment (HMM (Vogel et al., 1996) and MaxEnt (Ittycheriah and Roukos, 2005) alignment models, phrase pair extraction, MT model training (Ittycheriah and Roukos, 2007) and LM model training. The top region within the dashed line in Figure 1 shows the overall system built pipeline. 3.1 MT Decoder The MT decoder (Ittycheriah and Roukos, 2007) employed in our study extracts various features (source words, morphemes and POS tags, target words and POS tags, etc.) with their weights trained in a maximum entropy framework. These features are combined with other features used in a typical phrase-based translation system. Altogether the decoder incorporates 17 features with we</context>
</contexts>
<marker>Ittycheriah, Roukos, 2005</marker>
<rawString>Abraham Ittycheriah and Salim Roukos. 2005. A maximum entropy word aligner for arabic-english machine translation. In In Proceedings of HLTEMNLP, pages 89–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abraham Ittycheriah</author>
<author>Salim Roukos</author>
</authors>
<title>Direct translation model 2. In</title>
<date>2007</date>
<booktitle>In HLT-NAACL 2007: Main Conference,</booktitle>
<pages>57--64</pages>
<contexts>
<context position="8598" citStr="Ittycheriah and Roukos, 2007" startWordPosition="1344" endWordPosition="1348">at are the most similar to the sentences from the input document, then build the MT system with the sub-sampled sentence pairs. The similarity is defined as the number of n-grams that appear in both source sentences, divided by the input sentence’s length, with higher weights assigned to longer n-grams. From the extracted sentence pairs, we utilize the standard pipeline in SMT system building: word align862 Figure 1: Adaptive QE for document-specific MT system. ment (HMM (Vogel et al., 1996) and MaxEnt (Ittycheriah and Roukos, 2005) alignment models, phrase pair extraction, MT model training (Ittycheriah and Roukos, 2007) and LM model training. The top region within the dashed line in Figure 1 shows the overall system built pipeline. 3.1 MT Decoder The MT decoder (Ittycheriah and Roukos, 2007) employed in our study extracts various features (source words, morphemes and POS tags, target words and POS tags, etc.) with their weights trained in a maximum entropy framework. These features are combined with other features used in a typical phrase-based translation system. Altogether the decoder incorporates 17 features with weights estimated by PRO (Hopkins and May, 2011) in the decoding process, and achieves state-</context>
</contexts>
<marker>Ittycheriah, Roukos, 2007</marker>
<rawString>Abraham Ittycheriah and Salim Roukos. 2007. Direct translation model 2. In In HLT-NAACL 2007: Main Conference, pages 57–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yajuan Lu</author>
<author>Jin Huang</author>
<author>Qun Liu</author>
</authors>
<title>Improving statistical machine translation performance by training data selection and optimization.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>343--350</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="7703" citStr="Lu et al., 2007" startWordPosition="1200" endWordPosition="1203">t documents are automatically segmented into sentences, which are also called segments. Thus in the rest of the paper we will use sentences and segments interchangeably. Our parallel corpora includes tens of millions of sentence pairs covering a wide range of topics. Building a general MT system using all the parallel data not only produces a huge translation model (unless with very aggressive pruning), the performance on the given input document is suboptimal due to the unwanted dominance of out-of-domain data. Past research suggests using weighted sentences or corpora for domain adaptation (Lu et al., 2007; Matsoukas et al., 2009; Foster et al., 2010). Here we adopt the same strategy, building a documentspecific translation model for each input document. The document-specific system is built based on sub-sampling: from the parallel corpora we select sentence pairs that are the most similar to the sentences from the input document, then build the MT system with the sub-sampled sentence pairs. The similarity is defined as the number of n-grams that appear in both source sentences, divided by the input sentence’s length, with higher weights assigned to longer n-grams. From the extracted sentence p</context>
</contexts>
<marker>Lu, Huang, Liu, 2007</marker>
<rawString>Yajuan Lu, Jin Huang, and Qun Liu. 2007. Improving statistical machine translation performance by training data selection and optimization. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 343–350, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spyros Matsoukas</author>
<author>Antti-Veikko I Rosti</author>
<author>Bing Zhang</author>
</authors>
<title>Discriminative corpus weight estimation for machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2, EMNLP ’09,</booktitle>
<pages>708--717</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7727" citStr="Matsoukas et al., 2009" startWordPosition="1204" endWordPosition="1208">utomatically segmented into sentences, which are also called segments. Thus in the rest of the paper we will use sentences and segments interchangeably. Our parallel corpora includes tens of millions of sentence pairs covering a wide range of topics. Building a general MT system using all the parallel data not only produces a huge translation model (unless with very aggressive pruning), the performance on the given input document is suboptimal due to the unwanted dominance of out-of-domain data. Past research suggests using weighted sentences or corpora for domain adaptation (Lu et al., 2007; Matsoukas et al., 2009; Foster et al., 2010). Here we adopt the same strategy, building a documentspecific translation model for each input document. The document-specific system is built based on sub-sampling: from the parallel corpora we select sentence pairs that are the most similar to the sentences from the input document, then build the MT system with the sub-sampled sentence pairs. The similarity is defined as the number of n-grams that appear in both source sentences, divided by the input sentence’s length, with higher weights assigned to longer n-grams. From the extracted sentence pairs, we utilize the sta</context>
</contexts>
<marker>Matsoukas, Rosti, Zhang, 2009</marker>
<rawString>Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing Zhang. 2009. Discriminative corpus weight estimation for machine translation. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2, EMNLP ’09, pages 708–717, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher B Quirk</author>
</authors>
<title>Training a sentence-level machine translation confidence measure. In</title>
<date>2004</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="5227" citStr="Quirk, 2004" startWordPosition="812" endWordPosition="813">escribe the static quality estimation method in section 4, and propose the adaptive quality estimation method in section 5. In section 6 we demonstrate the improvement of MT quality estimation with our method, followed by discussion and conclusion in section 7. 2 Related Work There has been a long history of study in confidence estimation of machine translation. The work of (Blatz et al., 2004) is among the best known study of sentence and word level features for translation error prediction. Along this line of research, improvements can be obtained by incorporating more features as shown in (Quirk, 2004; Sanchis et al., 2007; Raybaud et al., 2009; Specia et al., 2009b). Soricut and Echihabi (2010b) proposed various regression models to predict the expected BLEU score of a given sentence translation hypothesis. Ueffing and Hey (2007) introduced word posterior probabilities (WPP) features and applied them in the n-best list reranking. Target part-of-speech and null dependency link are exploited in a MaxEnt classifier to improve the MT quality estimation (Xiong et al., 2010). Quality estimation focusing on MT post-editing has been an active research topic, especially after the WMT 2012 (Calliso</context>
</contexts>
<marker>Quirk, 2004</marker>
<rawString>Christopher B. Quirk. 2004. Training a sentence-level machine translation confidence measure. In In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylvain Raybaud</author>
<author>Caroline Lavecchia</author>
<author>David Langlois</author>
<author>Kamel Smaili</author>
</authors>
<title>New confidence measures for statistical machine translation.</title>
<date>2009</date>
<location>CoRR, abs/0902.1033.</location>
<contexts>
<context position="5271" citStr="Raybaud et al., 2009" startWordPosition="818" endWordPosition="821">on method in section 4, and propose the adaptive quality estimation method in section 5. In section 6 we demonstrate the improvement of MT quality estimation with our method, followed by discussion and conclusion in section 7. 2 Related Work There has been a long history of study in confidence estimation of machine translation. The work of (Blatz et al., 2004) is among the best known study of sentence and word level features for translation error prediction. Along this line of research, improvements can be obtained by incorporating more features as shown in (Quirk, 2004; Sanchis et al., 2007; Raybaud et al., 2009; Specia et al., 2009b). Soricut and Echihabi (2010b) proposed various regression models to predict the expected BLEU score of a given sentence translation hypothesis. Ueffing and Hey (2007) introduced word posterior probabilities (WPP) features and applied them in the n-best list reranking. Target part-of-speech and null dependency link are exploited in a MaxEnt classifier to improve the MT quality estimation (Xiong et al., 2010). Quality estimation focusing on MT post-editing has been an active research topic, especially after the WMT 2012 (Callison-Burch et al., 2012) and WMT2013 (Bojar et </context>
</contexts>
<marker>Raybaud, Lavecchia, Langlois, Smaili, 2009</marker>
<rawString>Sylvain Raybaud, Caroline Lavecchia, David Langlois, and Kamel Smaili. 2009. New confidence measures for statistical machine translation. CoRR, abs/0902.1033.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Salim Roukos</author>
<author>Abraham Ittycheriah</author>
<author>Jian-Ming Xu</author>
</authors>
<title>Document-specific statistical machine translation for improving human translation productivity.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th international conference on Computational Linguistics and Intelligent Text Processing - Volume Part II, CICLing’12,</booktitle>
<pages>25--39</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="2987" citStr="Roukos et al., 2012" startWordPosition="462" endWordPosition="465">gh or translating from scratching when the quality is low. This will save the time of reading and parsing low quality MT and improve user experience. In this paper we propose an adaptive quality estimation that predicts sentence-level humantargeted translation error rate (HTER) (Snover et al., 2006) for a document-specific MT post-editing system. HTER is an ideal quality measurement for MT post editing since the reference is obtained from human correction of the MT output. Document-specific MT model is an MT model that is specifically built for the given input document. It is demonstrated in (Roukos et al., 2012) that document-specific MT models significantly improve the translation quality. However, this raises two issues for quality estimation. First, existing approaches to MT quality estimation rely on lexical and syntactical features defined over parallel sentence pairs, which includes source sentences, MT outputs and references, and translation models (Blatz et al., 2004; Ueffing and Ney, 2007; Specia et al., 2009a; Xiong et al., 2010; Soricut and Echihabi, 2010a; Bach et al., 2011). Therefore, when the MT quality estimation model is trained, 861 Proceedings of the 52nd Annual Meeting of the Asso</context>
</contexts>
<marker>Roukos, Ittycheriah, Xu, 2012</marker>
<rawString>Salim Roukos, Abraham Ittycheriah, and Jian-Ming Xu. 2012. Document-specific statistical machine translation for improving human translation productivity. In Proceedings of the 13th international conference on Computational Linguistics and Intelligent Text Processing - Volume Part II, CICLing’12, pages 25–39, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alberto Sanchis</author>
<author>Alfons Juan</author>
</authors>
<title>Enrique Vidal, and Departament De Sistemes Informtics.</title>
<date>2007</date>
<booktitle>In Procedings of Machine Translation</booktitle>
<location>Summit XI.</location>
<marker>Sanchis, Juan, 2007</marker>
<rawString>Alberto Sanchis, Alfons Juan, Enrique Vidal, and Departament De Sistemes Informtics. 2007. Estimation of confidence measures for machine translation. In In Procedings of Machine Translation Summit XI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation. In</title>
<date>2006</date>
<booktitle>In Proceedings of Association for Machine Translation in the Americas,</booktitle>
<pages>223--231</pages>
<contexts>
<context position="2667" citStr="Snover et al., 2006" startWordPosition="409" endWordPosition="412">If MT proposals are used properly, post-editing can increase translators productivity and lead to significant cost savings. Therefore, it is beneficial to provide MT confidence estimation, to help the translators to decide whether to accept MT proposals, making minor modifications on MT proposals when the quality is high or translating from scratching when the quality is low. This will save the time of reading and parsing low quality MT and improve user experience. In this paper we propose an adaptive quality estimation that predicts sentence-level humantargeted translation error rate (HTER) (Snover et al., 2006) for a document-specific MT post-editing system. HTER is an ideal quality measurement for MT post editing since the reference is obtained from human correction of the MT output. Document-specific MT model is an MT model that is specifically built for the given input document. It is demonstrated in (Roukos et al., 2012) that document-specific MT models significantly improve the translation quality. However, this raises two issues for quality estimation. First, existing approaches to MT quality estimation rely on lexical and syntactical features defined over parallel sentence pairs, which includ</context>
<context position="9603" citStr="Snover et al., 2006" startWordPosition="1503" endWordPosition="1506">th other features used in a typical phrase-based translation system. Altogether the decoder incorporates 17 features with weights estimated by PRO (Hopkins and May, 2011) in the decoding process, and achieves state-of-the-art translation performance in various Arabic-English translation evaluations (NIST MT2008, GALE and BOLT projects). 4 Static MT Quality Estimation MT quality estimation is typically formulated as a prediction problem: estimating the confidence score or translation error rate of the translated sentences or documents based on a set of features. In this work, we adopt HTER in (Snover et al., 2006) as our prediction output. HTER measures the percentage of insertions, deletions, substitutions and shifts needed to correct the MT outputs. In the rest of the paper, we use TER and HTER interchangably. In this section we will first introduce the set of features, and then discuss MT QE problem from classification and regression point of views. 4.1 Features for MT QE The features for quality estimation should reflect the complexity of the source sentence and the decoding process. Therefore we conduct syntactic analysis on the source sentences, extract features from the decoding process and sele</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In In Proceedings of Association for Machine Translation in the Americas, pages 223–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>Abdessamad Echihabi</author>
</authors>
<title>Trustrank: inducing trust in automatic translations via ranking.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>612--621</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3450" citStr="Soricut and Echihabi, 2010" startWordPosition="531" endWordPosition="534">on of the MT output. Document-specific MT model is an MT model that is specifically built for the given input document. It is demonstrated in (Roukos et al., 2012) that document-specific MT models significantly improve the translation quality. However, this raises two issues for quality estimation. First, existing approaches to MT quality estimation rely on lexical and syntactical features defined over parallel sentence pairs, which includes source sentences, MT outputs and references, and translation models (Blatz et al., 2004; Ueffing and Ney, 2007; Specia et al., 2009a; Xiong et al., 2010; Soricut and Echihabi, 2010a; Bach et al., 2011). Therefore, when the MT quality estimation model is trained, 861 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 861–870, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics it can not be adapted to provide accurate estimates on the outputs of document-specific MT models. Second, the MT quality estimation might be inconsistent across different document-specific MT models, thus the confidence score is unreliable and not very helpful to users. In contrast to traditional static MT quality </context>
<context position="5322" citStr="Soricut and Echihabi (2010" startWordPosition="826" endWordPosition="829">ive quality estimation method in section 5. In section 6 we demonstrate the improvement of MT quality estimation with our method, followed by discussion and conclusion in section 7. 2 Related Work There has been a long history of study in confidence estimation of machine translation. The work of (Blatz et al., 2004) is among the best known study of sentence and word level features for translation error prediction. Along this line of research, improvements can be obtained by incorporating more features as shown in (Quirk, 2004; Sanchis et al., 2007; Raybaud et al., 2009; Specia et al., 2009b). Soricut and Echihabi (2010b) proposed various regression models to predict the expected BLEU score of a given sentence translation hypothesis. Ueffing and Hey (2007) introduced word posterior probabilities (WPP) features and applied them in the n-best list reranking. Target part-of-speech and null dependency link are exploited in a MaxEnt classifier to improve the MT quality estimation (Xiong et al., 2010). Quality estimation focusing on MT post-editing has been an active research topic, especially after the WMT 2012 (Callison-Burch et al., 2012) and WMT2013 (Bojar et al., 2013) workshops with the “Quality Estimation” </context>
</contexts>
<marker>Soricut, Echihabi, 2010</marker>
<rawString>Radu Soricut and Abdessamad Echihabi. 2010a. Trustrank: inducing trust in automatic translations via ranking. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 612–621, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>Abdessamad Echihabi</author>
</authors>
<title>Trustrank: Inducing trust in automatic translations via ranking.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>612--621</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3450" citStr="Soricut and Echihabi, 2010" startWordPosition="531" endWordPosition="534">on of the MT output. Document-specific MT model is an MT model that is specifically built for the given input document. It is demonstrated in (Roukos et al., 2012) that document-specific MT models significantly improve the translation quality. However, this raises two issues for quality estimation. First, existing approaches to MT quality estimation rely on lexical and syntactical features defined over parallel sentence pairs, which includes source sentences, MT outputs and references, and translation models (Blatz et al., 2004; Ueffing and Ney, 2007; Specia et al., 2009a; Xiong et al., 2010; Soricut and Echihabi, 2010a; Bach et al., 2011). Therefore, when the MT quality estimation model is trained, 861 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 861–870, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics it can not be adapted to provide accurate estimates on the outputs of document-specific MT models. Second, the MT quality estimation might be inconsistent across different document-specific MT models, thus the confidence score is unreliable and not very helpful to users. In contrast to traditional static MT quality </context>
<context position="5322" citStr="Soricut and Echihabi (2010" startWordPosition="826" endWordPosition="829">ive quality estimation method in section 5. In section 6 we demonstrate the improvement of MT quality estimation with our method, followed by discussion and conclusion in section 7. 2 Related Work There has been a long history of study in confidence estimation of machine translation. The work of (Blatz et al., 2004) is among the best known study of sentence and word level features for translation error prediction. Along this line of research, improvements can be obtained by incorporating more features as shown in (Quirk, 2004; Sanchis et al., 2007; Raybaud et al., 2009; Specia et al., 2009b). Soricut and Echihabi (2010b) proposed various regression models to predict the expected BLEU score of a given sentence translation hypothesis. Ueffing and Hey (2007) introduced word posterior probabilities (WPP) features and applied them in the n-best list reranking. Target part-of-speech and null dependency link are exploited in a MaxEnt classifier to improve the MT quality estimation (Xiong et al., 2010). Quality estimation focusing on MT post-editing has been an active research topic, especially after the WMT 2012 (Callison-Burch et al., 2012) and WMT2013 (Bojar et al., 2013) workshops with the “Quality Estimation” </context>
</contexts>
<marker>Soricut, Echihabi, 2010</marker>
<rawString>Radu Soricut and Abdessamad Echihabi. 2010b. Trustrank: Inducing trust in automatic translations via ranking. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 612–621. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Craig Saunders</author>
<author>Marco Turchi</author>
<author>Zhuoran Wang</author>
<author>John Shawe-taylor</author>
</authors>
<title>Improving the confidence of machine translation quality estimates. In</title>
<date>2009</date>
<booktitle>In Proceedings of MT Summit XII.</booktitle>
<contexts>
<context position="3401" citStr="Specia et al., 2009" startWordPosition="522" endWordPosition="526"> reference is obtained from human correction of the MT output. Document-specific MT model is an MT model that is specifically built for the given input document. It is demonstrated in (Roukos et al., 2012) that document-specific MT models significantly improve the translation quality. However, this raises two issues for quality estimation. First, existing approaches to MT quality estimation rely on lexical and syntactical features defined over parallel sentence pairs, which includes source sentences, MT outputs and references, and translation models (Blatz et al., 2004; Ueffing and Ney, 2007; Specia et al., 2009a; Xiong et al., 2010; Soricut and Echihabi, 2010a; Bach et al., 2011). Therefore, when the MT quality estimation model is trained, 861 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 861–870, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics it can not be adapted to provide accurate estimates on the outputs of document-specific MT models. Second, the MT quality estimation might be inconsistent across different document-specific MT models, thus the confidence score is unreliable and not very helpful to use</context>
<context position="5292" citStr="Specia et al., 2009" startWordPosition="822" endWordPosition="825">, and propose the adaptive quality estimation method in section 5. In section 6 we demonstrate the improvement of MT quality estimation with our method, followed by discussion and conclusion in section 7. 2 Related Work There has been a long history of study in confidence estimation of machine translation. The work of (Blatz et al., 2004) is among the best known study of sentence and word level features for translation error prediction. Along this line of research, improvements can be obtained by incorporating more features as shown in (Quirk, 2004; Sanchis et al., 2007; Raybaud et al., 2009; Specia et al., 2009b). Soricut and Echihabi (2010b) proposed various regression models to predict the expected BLEU score of a given sentence translation hypothesis. Ueffing and Hey (2007) introduced word posterior probabilities (WPP) features and applied them in the n-best list reranking. Target part-of-speech and null dependency link are exploited in a MaxEnt classifier to improve the MT quality estimation (Xiong et al., 2010). Quality estimation focusing on MT post-editing has been an active research topic, especially after the WMT 2012 (Callison-Burch et al., 2012) and WMT2013 (Bojar et al., 2013) workshops </context>
</contexts>
<marker>Specia, Saunders, Turchi, Wang, Shawe-taylor, 2009</marker>
<rawString>Lucia Specia, Craig Saunders, Marco Turchi, Zhuoran Wang, and John Shawe-taylor. 2009a. Improving the confidence of machine translation quality estimates. In In Proceedings of MT Summit XII.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Marco Turchi</author>
<author>Zhuoran Wang</author>
<author>John Shawe-Taylor</author>
<author>Craig Saunders</author>
</authors>
<title>Improving the confidence of machine translation quality estimates.</title>
<date>2009</date>
<contexts>
<context position="3401" citStr="Specia et al., 2009" startWordPosition="522" endWordPosition="526"> reference is obtained from human correction of the MT output. Document-specific MT model is an MT model that is specifically built for the given input document. It is demonstrated in (Roukos et al., 2012) that document-specific MT models significantly improve the translation quality. However, this raises two issues for quality estimation. First, existing approaches to MT quality estimation rely on lexical and syntactical features defined over parallel sentence pairs, which includes source sentences, MT outputs and references, and translation models (Blatz et al., 2004; Ueffing and Ney, 2007; Specia et al., 2009a; Xiong et al., 2010; Soricut and Echihabi, 2010a; Bach et al., 2011). Therefore, when the MT quality estimation model is trained, 861 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 861–870, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics it can not be adapted to provide accurate estimates on the outputs of document-specific MT models. Second, the MT quality estimation might be inconsistent across different document-specific MT models, thus the confidence score is unreliable and not very helpful to use</context>
<context position="5292" citStr="Specia et al., 2009" startWordPosition="822" endWordPosition="825">, and propose the adaptive quality estimation method in section 5. In section 6 we demonstrate the improvement of MT quality estimation with our method, followed by discussion and conclusion in section 7. 2 Related Work There has been a long history of study in confidence estimation of machine translation. The work of (Blatz et al., 2004) is among the best known study of sentence and word level features for translation error prediction. Along this line of research, improvements can be obtained by incorporating more features as shown in (Quirk, 2004; Sanchis et al., 2007; Raybaud et al., 2009; Specia et al., 2009b). Soricut and Echihabi (2010b) proposed various regression models to predict the expected BLEU score of a given sentence translation hypothesis. Ueffing and Hey (2007) introduced word posterior probabilities (WPP) features and applied them in the n-best list reranking. Target part-of-speech and null dependency link are exploited in a MaxEnt classifier to improve the MT quality estimation (Xiong et al., 2010). Quality estimation focusing on MT post-editing has been an active research topic, especially after the WMT 2012 (Callison-Burch et al., 2012) and WMT2013 (Bojar et al., 2013) workshops </context>
</contexts>
<marker>Specia, Turchi, Wang, Shawe-Taylor, Saunders, 2009</marker>
<rawString>Lucia Specia, Marco Turchi, Zhuoran Wang, John Shawe-Taylor, and Craig Saunders. 2009b. Improving the confidence of machine translation quality estimates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>Wordlevel confidence estimation for machine translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="3380" citStr="Ueffing and Ney, 2007" startWordPosition="518" endWordPosition="521"> post editing since the reference is obtained from human correction of the MT output. Document-specific MT model is an MT model that is specifically built for the given input document. It is demonstrated in (Roukos et al., 2012) that document-specific MT models significantly improve the translation quality. However, this raises two issues for quality estimation. First, existing approaches to MT quality estimation rely on lexical and syntactical features defined over parallel sentence pairs, which includes source sentences, MT outputs and references, and translation models (Blatz et al., 2004; Ueffing and Ney, 2007; Specia et al., 2009a; Xiong et al., 2010; Soricut and Echihabi, 2010a; Bach et al., 2011). Therefore, when the MT quality estimation model is trained, 861 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 861–870, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics it can not be adapted to provide accurate estimates on the outputs of document-specific MT models. Second, the MT quality estimation might be inconsistent across different document-specific MT models, thus the confidence score is unreliable and no</context>
</contexts>
<marker>Ueffing, Ney, 2007</marker>
<rawString>Nicola Ueffing and Hermann Ney. 2007. Wordlevel confidence estimation for machine translation. Computational Linguistics, 33(1):9–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>Hmm-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th Conference on Computational Linguistics - Volume 2, COLING ’96,</booktitle>
<pages>836--841</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8465" citStr="Vogel et al., 1996" startWordPosition="1325" endWordPosition="1328">ocument. The document-specific system is built based on sub-sampling: from the parallel corpora we select sentence pairs that are the most similar to the sentences from the input document, then build the MT system with the sub-sampled sentence pairs. The similarity is defined as the number of n-grams that appear in both source sentences, divided by the input sentence’s length, with higher weights assigned to longer n-grams. From the extracted sentence pairs, we utilize the standard pipeline in SMT system building: word align862 Figure 1: Adaptive QE for document-specific MT system. ment (HMM (Vogel et al., 1996) and MaxEnt (Ittycheriah and Roukos, 2005) alignment models, phrase pair extraction, MT model training (Ittycheriah and Roukos, 2007) and LM model training. The top region within the dashed line in Figure 1 shows the overall system built pipeline. 3.1 MT Decoder The MT decoder (Ittycheriah and Roukos, 2007) employed in our study extracts various features (source words, morphemes and POS tags, target words and POS tags, etc.) with their weights trained in a maximum entropy framework. These features are combined with other features used in a typical phrase-based translation system. Altogether th</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. Hmm-based word alignment in statistical translation. In Proceedings of the 16th Conference on Computational Linguistics - Volume 2, COLING ’96, pages 836–841, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Min Zhang</author>
<author>Haizhou Li</author>
</authors>
<title>Error detection for statistical machine translation using linguistic features.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>604--611</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3422" citStr="Xiong et al., 2010" startWordPosition="527" endWordPosition="530"> from human correction of the MT output. Document-specific MT model is an MT model that is specifically built for the given input document. It is demonstrated in (Roukos et al., 2012) that document-specific MT models significantly improve the translation quality. However, this raises two issues for quality estimation. First, existing approaches to MT quality estimation rely on lexical and syntactical features defined over parallel sentence pairs, which includes source sentences, MT outputs and references, and translation models (Blatz et al., 2004; Ueffing and Ney, 2007; Specia et al., 2009a; Xiong et al., 2010; Soricut and Echihabi, 2010a; Bach et al., 2011). Therefore, when the MT quality estimation model is trained, 861 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 861–870, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics it can not be adapted to provide accurate estimates on the outputs of document-specific MT models. Second, the MT quality estimation might be inconsistent across different document-specific MT models, thus the confidence score is unreliable and not very helpful to users. In contrast to tr</context>
<context position="5705" citStr="Xiong et al., 2010" startWordPosition="885" endWordPosition="888">anslation error prediction. Along this line of research, improvements can be obtained by incorporating more features as shown in (Quirk, 2004; Sanchis et al., 2007; Raybaud et al., 2009; Specia et al., 2009b). Soricut and Echihabi (2010b) proposed various regression models to predict the expected BLEU score of a given sentence translation hypothesis. Ueffing and Hey (2007) introduced word posterior probabilities (WPP) features and applied them in the n-best list reranking. Target part-of-speech and null dependency link are exploited in a MaxEnt classifier to improve the MT quality estimation (Xiong et al., 2010). Quality estimation focusing on MT post-editing has been an active research topic, especially after the WMT 2012 (Callison-Burch et al., 2012) and WMT2013 (Bojar et al., 2013) workshops with the “Quality Estimation” shared task. Bic¸ici et al. (2013) proposes a number of features measuring the similarity of the source sentence to the source side of the MT training corpus, which, combined with features from translation output, achieved significantly superior performance in the MT QE evaluation. Felice and Specia (2012) investigates the impact of a large set of linguistically inspired features </context>
</contexts>
<marker>Xiong, Zhang, Li, 2010</marker>
<rawString>Deyi Xiong, Min Zhang, and Haizhou Li. 2010. Error detection for statistical machine translation using linguistic features. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 604–611, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>