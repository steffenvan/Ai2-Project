<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004686">
<title confidence="0.982337">
LIMSI’s statistical translation systems for WMT’08
</title>
<author confidence="0.9697555">
Daniel Déchelotte, Gilles Adda, Alexandre Allauzen, Hélène Bonneau-Maynard,
Olivier Galibert, Jean-Luc Gauvain, Philippe Langlais* and François Yvon
</author>
<affiliation confidence="0.675028">
LIMSI/CNRS
</affiliation>
<email confidence="0.993031">
firstname.lastname@limsi.fr
</email>
<sectionHeader confidence="0.99551" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999976909090909">
This paper describes our statistical machine
translation systems based on the Moses toolkit
for the WMT08 shared task. We address the
Europarl and News conditions for the follow-
ing language pairs: English with French, Ger-
man and Spanish. For Europarl, n-best rescor-
ing is performed using an enhanced n-gram
or a neuronal language model; for the News
condition, language models incorporate extra
training data. We also report unconvincing re-
sults of experiments with factored models.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999709">
This paper describes our statistical machine trans-
lation systems based on the Moses toolkit for the
WMT 08 shared task. We address the Europarl and
News conditions for the following language pairs:
English with French, German and Spanish. For Eu-
roparl, n-best rescoring is performed using an en-
hanced n-gram or a neuronal language model, and
for the News condition, language models are trained
with extra training data. We also report unconvinc-
ing results of experiments with factored models.
</bodyText>
<sectionHeader confidence="0.877107" genericHeader="method">
2 Base System architecture
</sectionHeader>
<bodyText confidence="0.9989875">
LIMSI took part in the evaluations on Europarl data
and on News data, translating French, German and
Spanish from and to English, amounting a total
of twelve evaluation conditions. Figure 1 presents
the generic overall architecture of LIMSI’s transla-
tion systems. They are fairly standard phrase-based
</bodyText>
<footnote confidence="0.671197">
tniv. Montréal, felipe@iro.umontreal.ca
</footnote>
<figureCaption confidence="0.996857">
Figure 1: Generic architecture of LIMSI’s SMT systems.
</figureCaption>
<bodyText confidence="0.9121026">
Depending on the condition, the decoder generates ei-
ther the final output or n-best lists. In the latter case,
the rescoring incorporates the same translation features,
except for a better target language model (see text).
translation systems (Och and Ney, 2004; Koehn et
al., 2003) and use Moses (Koehn et al., 2007) to
search for the best target sentence. The search uses
the following models: a phrase table, providing 4
scores and a phrase penalty, a lexicalized reordering
model (7 scores), a language model score and a word
penalty. These fourteen scores are weighted and lin-
early combined (Och and Ney, 2002; Och, 2003);
their respective weights are learned on development
data so as to maximize the BLEU score. In the fol-
lowing, we detail several aspects of our systems.
</bodyText>
<subsectionHeader confidence="0.991847">
2.1 Translation models
</subsectionHeader>
<bodyText confidence="0.99966425">
The translation models deployed in our systems for
the europarl condition were trained on the provided
Europarl parallel data only. For the news condition,
they were trained on the Europarl data merged with
</bodyText>
<figure confidence="0.9979705">
Source
text
Europarl Europarl Other News Co. Europarl
+ News Co. sources
Translation model 4g language model 4g language model
Phrase pair
extraction
or
Moses
LM Interpolation
or
$n$−best
translations
Neural network
Rescoring
and extraction
or
Targtextet
Target
text
</figure>
<page confidence="0.935926">
107
</page>
<note confidence="0.4092">
Proceedings of the Third Workshop on Statistical Machine Translation, pages 107–110,
</note>
<page confidence="0.461003">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.96696835">
the news-commentary parallel data, as depicted on
Figure 1. This setup was found to be more favor-
able than training on Europarl data only (for obvious
mismatching domain reasons) and than training on
news-commentary data only, most probably because
of a lack of coverage. Another, alternative way of
benefitting from the coverage of the Europarl corpus
and the relevance of the news-commentary corpus
is to use two phrase-tables in parallel, an interest-
ing feature of Moses. (Koehn and Schroeder, 2007)
found that this was the best way to “adapt” a transla-
tion system to the news-commentary task. These re-
sults are corroborated in (Déchelotte, 2007)1 , which
adapts a “European Parliament” system using a “Eu-
ropean and Spanish Parliaments” development set.
However, we were not able to reproduce those find-
ings for this evaluation. This might be caused by the
increase of the number of feature functions, from 14
to 26, due to the duplication of the phrase table and
the lexicalized reordering model.
</bodyText>
<subsectionHeader confidence="0.933955">
2.2 Language Models
2.2.1 Europarl language models
</subsectionHeader>
<bodyText confidence="0.999981307692308">
The training of Europarl language models (LMs)
was rather conventional: for all languages used in
our systems, we used a 4-gram LM based on the
entire Europarl vocabulary and trained only on the
available Europarl training data. For French, for
instance, this yielded a model with a 0.2 out-of-
vocabulary (OOV) rate on our LM development set,
and a perplexity of 44.9 on the development data.
For French also, a more accurate n-gram LM was
used to rescore the first pass translation; this larger
model includes both Europarl and giga word corpus
of newswire text, lowering the perplexity to 41.9 on
the development data.
</bodyText>
<subsectionHeader confidence="0.547347">
2.2.2 News language models
</subsectionHeader>
<bodyText confidence="0.99932180952381">
For this condition, we took advantage of the a
priori information that the test text would be of
newspaper/newswire genre and from the November-
december 2007 period. We consequently built much
larger LMs for translating both to French and to En-
glish, and optimized their combination on appropri-
1(Déchelotte, 2007) further found that giving an increased
weight to the small in-domain data could out-perform the setup
with two phrase-tables in parallel. We haven’t evaluated this
idea for this evaluation.
ate source of data. For French, we interpolated five
different LMs trained on corpus containing respec-
tively newspapers, newswire, news commentary and
Europarl data, and tuned their combination with text
downloaded from the Internet. Our best LM had an
OOV rate of about 2.1% and a perplexity of 111.26
on the testset. English LMs were built in a similar
manner, our largest model combining 4 LMs from
various sources, which, altogether, represent about
850M words. Its perplexity on the 2008 test set was
approximately 160, with an OOV rate of 2.7%.
</bodyText>
<subsubsectionHeader confidence="0.727637">
2.2.3 Neural network language models
</subsubsectionHeader>
<bodyText confidence="0.999587">
Neural-Network (NN) based continuous space
LMs similar to the ones in (Schwenk, 2007) were
also trained on Europarl data. These networks com-
pute the probabilities of all the words in a 8192 word
output vocabulary given a context in a larger, 65000-
word vocabulary. Each word in the context is first
associated with a numerical vector of dimension 500
by the input layer. The activity of the 500 neurons in
the hidden layer is computed as the hyperbolic tan-
gent of the weighted sum of these vectors, projecting
the context into a [−1, 1] hypercube of dimension
500. Final projection on a set of 8192 output neurons
yields the final probabilities through a softmax-ed,
weighted sum of the coordinates in the hypercube.
The final NN-based model is interpolated with the
main LM model in a 0.4-0.6 ratio, and yields a per-
plexity reduction of 9% relative with respect to the
n-gram LM on development data.
</bodyText>
<subsectionHeader confidence="0.999545">
2.3 Tuning procedure
</subsectionHeader>
<bodyText confidence="0.999995777777778">
We use MERT, distributed with the Moses decoder,
to tune the first pass of the system. The weights
were adjusted to maximize BLEU on the develop-
ment data. For the baseline system, a dozen Moses
runs are necessary for each MERT optimization, and
several optimization runs were started and compared
during the system’s development. Tuning was per-
formed using dev2006 for the Europarl task and on
News commentary dev2007 for the news task.
</bodyText>
<subsectionHeader confidence="0.993496">
2.4 Rescoring and post processing
</subsectionHeader>
<bodyText confidence="0.99987125">
For the Europarl condition, distinct 100 best trans-
lations from Moses were rescored with improved
LMs: when translating to French, we used the
French model described in section 2.2.1; when
</bodyText>
<page confidence="0.984312">
108
</page>
<table confidence="0.999701">
Es-En En-Es Fr-En En-Fr
baseline 32.21 31.62 32.41 29.31
Limsi 32.49 31.23 32.62 30.27
</table>
<tableCaption confidence="0.999059">
Table 1: Comparison of two tokenization policies
</tableCaption>
<table confidence="0.8323285">
All results on Europarl test2007
CI system CS system
En—*Fr 27.23 27.55
Fr—*En 30.96 30.98
</table>
<tableCaption confidence="0.986629333333333">
Table 2: Effect of training on true case texts, for English
to French (case INsensitive BLEU scores, untuned sys-
tems, results on test2006 dataset)
</tableCaption>
<bodyText confidence="0.993041090909091">
translating to English, we used the neuronal LM de-
scribed in section 2.2.3.
For all the “lowcase” systems (see below), recase-
ing was finally performed using our own recaseing
tool. Case is restored by creating a word graph al-
lowing all possible forms of caseing for each word
and each component of a compound word. This
word graph is then decoded using a cased 4-gram
LM to obtain the most likely form. In a final step,
OOV words (with respect to the source language
word list) are recased to match their original form.
</bodyText>
<sectionHeader confidence="0.961028" genericHeader="method">
3 Experiments with the base system
</sectionHeader>
<subsectionHeader confidence="0.999642">
3.1 Word tokenization and case
</subsectionHeader>
<bodyText confidence="0.999888375">
We developed our own tokenizer for English, French
and Spanish, and used the baseline tokenizer for
German. Experiments on the 2007 test dataset for
Europarl task show the impact of the tokenization
on the BLEU scores, with 3-gram LMs. Results are
always improved with our own tokenizer, except for
English to Spanish (Table 1).
Our systems were initially trained on lowercase
texts, similarly to the proposed baseline system.
However, training on true case texts proved bene-
ficial when translating from English to French, even
when scoring in a case insensitive manner. Table 2
shows an approximate gain of 0.3 BLEU for that di-
rection, and no impact on French to English perfor-
mance. Our English-French systems are therefore
case sensitive.
</bodyText>
<subsectionHeader confidence="0.998268">
3.2 Language Models
</subsectionHeader>
<bodyText confidence="0.999991846153846">
For Europarl, we experimented with LMs of increas-
ing orders: we found that using a 5-gram LM only
yields an insignificant improvement over a 4-gram
LM. As a result, we used 4-gram LMs for all our
first pass decodings. For the second pass, the use
of the Neural Network LMs, if used with an appro-
priate (tuned) weight, yields a small, yet consistent
improvement of BLEU for all pairs.
Performance on the news task are harder to ana-
lyze, due to the lack of development data. Throwing
in large set of in-domain data was obviously helpful,
even though we are currently unable to adequately
measure this effect.
</bodyText>
<sectionHeader confidence="0.980814" genericHeader="method">
4 Experiments with factored models
</sectionHeader>
<bodyText confidence="0.999865666666667">
Even though these models were not used in our sub-
missions, we feel it useful to comment here our (neg-
ative) experiments with factored models.
</bodyText>
<subsectionHeader confidence="0.987523">
4.1 Overview
</subsectionHeader>
<bodyText confidence="0.999974933333333">
In this work, factored models (Koehn and Hoang,
2007) are experimented with three factors : the sur-
face form, the lemma and the part of speech (POS).
The translation process is composed of different
mapping steps, which either translate input factors
into output factors, or generate additional output fac-
tors from existing output factors. In this work, four
mapping steps are used with two decoding paths.
The first path corresponds to the standard and di-
rect mapping of surface forms. The second decod-
ing path consists in two translation steps for respec-
tively POS tag and the lemmas, followed by a gener-
ation step which produces the surface form given the
POS-lemma couple. The system also includes three
reordering models.
</bodyText>
<subsectionHeader confidence="0.994249">
4.2 Training
</subsectionHeader>
<bodyText confidence="0.997899375">
Factored models have been built to translate from
English to French for the news task. To estimate the
phrase and generation tables, the training texts are
first processed in order to compute the lemmas and
POS information. The English texts are tagged and
lemmatized using the English version of the Tree-
tagger2. For French, POS-tagging is carried out
with a French version of the Brill’s tagger trained
</bodyText>
<footnote confidence="0.983293">
2http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger
</footnote>
<page confidence="0.998482">
109
</page>
<bodyText confidence="0.999506615384615">
on the MULTITAG corpus (Allauzen and Bonneau-
Maynard, 2008). Lemmatization is performed with
a French version of the Treetagger.
Three phrase tables are estimated with the Moses
utilities, one per factor. For the surface forms, the
parallel corpus is the concatenation of the official
training data for the tasks Europarl and News com-
mentary, whereas only the parallel data of news
commentary are used for lemmas and POS. For the
generation step, the table built on the parallel texts of
news commentary is augmented with a French dic-
tionary of 280 000 forms. The LM is the largest LM
available for French (see section 2.2.2).
</bodyText>
<subsectionHeader confidence="0.846222">
4.3 Results and lessons learned
</subsectionHeader>
<bodyText confidence="0.999988185185185">
On the news test set of 2008, this system obtains a
BLEU score of 20.2, which is worse than our “stan-
dard” system (20.9). A similar experiment on the
Europarl task proved equally unsuccessful.
Using only models which ignore the surface form
of input words yields a poor system. Therefore, in-
cluding a model based on surface forms, as sug-
gested (Koehn and Hoang, 2007), is also neces-
sary. This indeed improved (+1.6 BLEU for Eu-
roparl) over using one single decoding path, but not
enough to match our baseline system performance.
These results may be explained by the use of auto-
matic tools (POS tagger and lemmatizer) that are not
entirely error free, and also, to a lesser extend, by the
noise in the test data. We also think that more effort
has to be put into the generation step.
Tuning is also a major issue for factored trans-
lation models. Dealing with 38 weights is an op-
timization challenge, which took MERT 129 itera-
tions to converge. The necessary tradeoff between
the huge memory requirements of these techniques
and computation time is also detrimental to their use.
Although quantitative results were unsatisfactory,
it is finally worth mentioning that a manual exami-
nation of the output revealed that the explicit usage
of gender and number in our models (via POS tags)
may actually be helpful when translating to French.
</bodyText>
<sectionHeader confidence="0.999259" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999978647058824">
In this paper, we presented our statistical MT sys-
tems developed for the WMT 08 shared task. As ex-
pected, regarding the Europarl condition, our BLEU
improvements over the best 2007 results are limited:
paying attention to tokenization and caseing issues
brought us a small pay-off; rescoring with better
language models gave also some reward. The news
condition was new, and more challenging: our satis-
factory results can be attributed to the use of large,
well tuned, language models. In comparison, our ex-
periments with factored models proved disappoint-
ing, for reasons that remain to be clarified. On a
more general note, we feel that the performance of
MT systems for these tasks are somewhat shadowed
by normalization issues (tokenization errors, incon-
sistent use of caseing, typos, etc), making it difficult
to clearly analyze our systems’ performance.
</bodyText>
<sectionHeader confidence="0.999439" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999702612903226">
A. Allauzen and H. Bonneau-Maynard. 2008. Training
and evaluation of POS taggers on the French multitag
corpus. In Proc. LREC’08, To appear.
D. Déchelotte. 2007. Traduction automatique de la pa-
role par méthodes statistiques. Ph.D. thesis, Univ.
Paris XI, December.
P. Koehn and H. Hoang. 2007. Factored translation mod-
els. In Proc. EMNLP-CoNLL, pages 868–876.
P. Koehn and J. Schroeder. 2007. Experiments in domain
adaptation for statistical machine translation. In Proc.
of the Workshop on Statistical Machine Translation,
pages 224–227, Prague, Czech Republic.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. HLT-NAACL, pages
127–133, Edmonton, Canada, May.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In ACL, demonstration
session, Prague, Czech Republic.
F.J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In Proc. ACL, pages 295–302.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417–449.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. ACL, Sapporo, Japan.
H. Schwenk. 2007. Continuous space language models.
Computer Speech and Language, 21:492–518.
</reference>
<page confidence="0.998378">
110
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.678371">
<title confidence="0.863067">statistical translation systems for WMT’08</title>
<author confidence="0.840319">Daniel Déchelotte</author>
<author confidence="0.840319">Gilles Adda</author>
<author confidence="0.840319">Alexandre Allauzen</author>
<author confidence="0.840319">Hélène Galibert</author>
<author confidence="0.840319">Jean-Luc Gauvain</author>
<author confidence="0.840319">Philippe</author>
<email confidence="0.959097">firstname.lastname@limsi.fr</email>
<abstract confidence="0.9984445">This paper describes our statistical machine translation systems based on the Moses toolkit for the WMT08 shared task. We address the Europarl and News conditions for the following language pairs: English with French, Gerand Spanish. For Europarl, rescoris performed using an enhanced or a neuronal language model; for the News condition, language models incorporate extra training data. We also report unconvincing results of experiments with factored models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Allauzen</author>
<author>H Bonneau-Maynard</author>
</authors>
<title>Training and evaluation of POS taggers on the French multitag corpus.</title>
<date>2008</date>
<booktitle>In Proc. LREC’08,</booktitle>
<note>To appear.</note>
<marker>Allauzen, Bonneau-Maynard, 2008</marker>
<rawString>A. Allauzen and H. Bonneau-Maynard. 2008. Training and evaluation of POS taggers on the French multitag corpus. In Proc. LREC’08, To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Déchelotte</author>
</authors>
<title>Traduction automatique de la parole par méthodes statistiques.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>Univ. Paris XI,</institution>
<contexts>
<context position="3758" citStr="Déchelotte, 2007" startWordPosition="573" endWordPosition="574">a, as depicted on Figure 1. This setup was found to be more favorable than training on Europarl data only (for obvious mismatching domain reasons) and than training on news-commentary data only, most probably because of a lack of coverage. Another, alternative way of benefitting from the coverage of the Europarl corpus and the relevance of the news-commentary corpus is to use two phrase-tables in parallel, an interesting feature of Moses. (Koehn and Schroeder, 2007) found that this was the best way to “adapt” a translation system to the news-commentary task. These results are corroborated in (Déchelotte, 2007)1 , which adapts a “European Parliament” system using a “European and Spanish Parliaments” development set. However, we were not able to reproduce those findings for this evaluation. This might be caused by the increase of the number of feature functions, from 14 to 26, due to the duplication of the phrase table and the lexicalized reordering model. 2.2 Language Models 2.2.1 Europarl language models The training of Europarl language models (LMs) was rather conventional: for all languages used in our systems, we used a 4-gram LM based on the entire Europarl vocabulary and trained only on the av</context>
<context position="5120" citStr="Déchelotte, 2007" startWordPosition="796" endWordPosition="798">and a perplexity of 44.9 on the development data. For French also, a more accurate n-gram LM was used to rescore the first pass translation; this larger model includes both Europarl and giga word corpus of newswire text, lowering the perplexity to 41.9 on the development data. 2.2.2 News language models For this condition, we took advantage of the a priori information that the test text would be of newspaper/newswire genre and from the Novemberdecember 2007 period. We consequently built much larger LMs for translating both to French and to English, and optimized their combination on appropri1(Déchelotte, 2007) further found that giving an increased weight to the small in-domain data could out-perform the setup with two phrase-tables in parallel. We haven’t evaluated this idea for this evaluation. ate source of data. For French, we interpolated five different LMs trained on corpus containing respectively newspapers, newswire, news commentary and Europarl data, and tuned their combination with text downloaded from the Internet. Our best LM had an OOV rate of about 2.1% and a perplexity of 111.26 on the testset. English LMs were built in a similar manner, our largest model combining 4 LMs from various</context>
</contexts>
<marker>Déchelotte, 2007</marker>
<rawString>D. Déchelotte. 2007. Traduction automatique de la parole par méthodes statistiques. Ph.D. thesis, Univ. Paris XI, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
</authors>
<title>Factored translation models.</title>
<date>2007</date>
<booktitle>In Proc. EMNLP-CoNLL,</booktitle>
<pages>868--876</pages>
<contexts>
<context position="10058" citStr="Koehn and Hoang, 2007" startWordPosition="1622" endWordPosition="1625">cond pass, the use of the Neural Network LMs, if used with an appropriate (tuned) weight, yields a small, yet consistent improvement of BLEU for all pairs. Performance on the news task are harder to analyze, due to the lack of development data. Throwing in large set of in-domain data was obviously helpful, even though we are currently unable to adequately measure this effect. 4 Experiments with factored models Even though these models were not used in our submissions, we feel it useful to comment here our (negative) experiments with factored models. 4.1 Overview In this work, factored models (Koehn and Hoang, 2007) are experimented with three factors : the surface form, the lemma and the part of speech (POS). The translation process is composed of different mapping steps, which either translate input factors into output factors, or generate additional output factors from existing output factors. In this work, four mapping steps are used with two decoding paths. The first path corresponds to the standard and direct mapping of surface forms. The second decoding path consists in two translation steps for respectively POS tag and the lemmas, followed by a generation step which produces the surface form give</context>
<context position="12240" citStr="Koehn and Hoang, 2007" startWordPosition="1979" endWordPosition="1982">r lemmas and POS. For the generation step, the table built on the parallel texts of news commentary is augmented with a French dictionary of 280 000 forms. The LM is the largest LM available for French (see section 2.2.2). 4.3 Results and lessons learned On the news test set of 2008, this system obtains a BLEU score of 20.2, which is worse than our “standard” system (20.9). A similar experiment on the Europarl task proved equally unsuccessful. Using only models which ignore the surface form of input words yields a poor system. Therefore, including a model based on surface forms, as suggested (Koehn and Hoang, 2007), is also necessary. This indeed improved (+1.6 BLEU for Europarl) over using one single decoding path, but not enough to match our baseline system performance. These results may be explained by the use of automatic tools (POS tagger and lemmatizer) that are not entirely error free, and also, to a lesser extend, by the noise in the test data. We also think that more effort has to be put into the generation step. Tuning is also a major issue for factored translation models. Dealing with 38 weights is an optimization challenge, which took MERT 129 iterations to converge. The necessary tradeoff b</context>
</contexts>
<marker>Koehn, Hoang, 2007</marker>
<rawString>P. Koehn and H. Hoang. 2007. Factored translation models. In Proc. EMNLP-CoNLL, pages 868–876.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>J Schroeder</author>
</authors>
<title>Experiments in domain adaptation for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of the Workshop on Statistical Machine Translation,</booktitle>
<pages>224--227</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3611" citStr="Koehn and Schroeder, 2007" startWordPosition="546" endWordPosition="549">stical Machine Translation, pages 107–110, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics the news-commentary parallel data, as depicted on Figure 1. This setup was found to be more favorable than training on Europarl data only (for obvious mismatching domain reasons) and than training on news-commentary data only, most probably because of a lack of coverage. Another, alternative way of benefitting from the coverage of the Europarl corpus and the relevance of the news-commentary corpus is to use two phrase-tables in parallel, an interesting feature of Moses. (Koehn and Schroeder, 2007) found that this was the best way to “adapt” a translation system to the news-commentary task. These results are corroborated in (Déchelotte, 2007)1 , which adapts a “European Parliament” system using a “European and Spanish Parliaments” development set. However, we were not able to reproduce those findings for this evaluation. This might be caused by the increase of the number of feature functions, from 14 to 26, due to the duplication of the phrase table and the lexicalized reordering model. 2.2 Language Models 2.2.1 Europarl language models The training of Europarl language models (LMs) was</context>
</contexts>
<marker>Koehn, Schroeder, 2007</marker>
<rawString>P. Koehn and J. Schroeder. 2007. Experiments in domain adaptation for statistical machine translation. In Proc. of the Workshop on Statistical Machine Translation, pages 224–227, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. HLT-NAACL,</booktitle>
<pages>127--133</pages>
<location>Edmonton, Canada,</location>
<contexts>
<context position="1945" citStr="Koehn et al., 2003" startWordPosition="284" endWordPosition="287">ews data, translating French, German and Spanish from and to English, amounting a total of twelve evaluation conditions. Figure 1 presents the generic overall architecture of LIMSI’s translation systems. They are fairly standard phrase-based tniv. Montréal, felipe@iro.umontreal.ca Figure 1: Generic architecture of LIMSI’s SMT systems. Depending on the condition, the decoder generates either the final output or n-best lists. In the latter case, the rescoring incorporates the same translation features, except for a better target language model (see text). translation systems (Och and Ney, 2004; Koehn et al., 2003) and use Moses (Koehn et al., 2007) to search for the best target sentence. The search uses the following models: a phrase table, providing 4 scores and a phrase penalty, a lexicalized reordering model (7 scores), a language model score and a word penalty. These fourteen scores are weighted and linearly combined (Och and Ney, 2002; Och, 2003); their respective weights are learned on development data so as to maximize the BLEU score. In the following, we detail several aspects of our systems. 2.1 Translation models The translation models deployed in our systems for the europarl condition were t</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-based translation. In Proc. HLT-NAACL, pages 127–133, Edmonton, Canada, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In ACL, demonstration session,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1980" citStr="Koehn et al., 2007" startWordPosition="291" endWordPosition="294">n and Spanish from and to English, amounting a total of twelve evaluation conditions. Figure 1 presents the generic overall architecture of LIMSI’s translation systems. They are fairly standard phrase-based tniv. Montréal, felipe@iro.umontreal.ca Figure 1: Generic architecture of LIMSI’s SMT systems. Depending on the condition, the decoder generates either the final output or n-best lists. In the latter case, the rescoring incorporates the same translation features, except for a better target language model (see text). translation systems (Och and Ney, 2004; Koehn et al., 2003) and use Moses (Koehn et al., 2007) to search for the best target sentence. The search uses the following models: a phrase table, providing 4 scores and a phrase penalty, a lexicalized reordering model (7 scores), a language model score and a word penalty. These fourteen scores are weighted and linearly combined (Och and Ney, 2002; Och, 2003); their respective weights are learned on development data so as to maximize the BLEU score. In the following, we detail several aspects of our systems. 2.1 Translation models The translation models deployed in our systems for the europarl condition were trained on the provided Europarl par</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In ACL, demonstration session, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>295--302</pages>
<contexts>
<context position="2277" citStr="Och and Ney, 2002" startWordPosition="341" endWordPosition="344">s. Depending on the condition, the decoder generates either the final output or n-best lists. In the latter case, the rescoring incorporates the same translation features, except for a better target language model (see text). translation systems (Och and Ney, 2004; Koehn et al., 2003) and use Moses (Koehn et al., 2007) to search for the best target sentence. The search uses the following models: a phrase table, providing 4 scores and a phrase penalty, a lexicalized reordering model (7 scores), a language model score and a word penalty. These fourteen scores are weighted and linearly combined (Och and Ney, 2002; Och, 2003); their respective weights are learned on development data so as to maximize the BLEU score. In the following, we detail several aspects of our systems. 2.1 Translation models The translation models deployed in our systems for the europarl condition were trained on the provided Europarl parallel data only. For the news condition, they were trained on the Europarl data merged with Source text Europarl Europarl Other News Co. Europarl + News Co. sources Translation model 4g language model 4g language model Phrase pair extraction or Moses LM Interpolation or $n$−best translations Neur</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>F.J. Och and H. Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proc. ACL, pages 295–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="1924" citStr="Och and Ney, 2004" startWordPosition="280" endWordPosition="283">oparl data and on News data, translating French, German and Spanish from and to English, amounting a total of twelve evaluation conditions. Figure 1 presents the generic overall architecture of LIMSI’s translation systems. They are fairly standard phrase-based tniv. Montréal, felipe@iro.umontreal.ca Figure 1: Generic architecture of LIMSI’s SMT systems. Depending on the condition, the decoder generates either the final output or n-best lists. In the latter case, the rescoring incorporates the same translation features, except for a better target language model (see text). translation systems (Och and Ney, 2004; Koehn et al., 2003) and use Moses (Koehn et al., 2007) to search for the best target sentence. The search uses the following models: a phrase table, providing 4 scores and a phrase penalty, a lexicalized reordering model (7 scores), a language model score and a word penalty. These fourteen scores are weighted and linearly combined (Och and Ney, 2002; Och, 2003); their respective weights are learned on development data so as to maximize the BLEU score. In the following, we detail several aspects of our systems. 2.1 Translation models The translation models deployed in our systems for the euro</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz J. Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>21--492</pages>
<location>Sapporo,</location>
<contexts>
<context position="2289" citStr="Och, 2003" startWordPosition="345" endWordPosition="346"> condition, the decoder generates either the final output or n-best lists. In the latter case, the rescoring incorporates the same translation features, except for a better target language model (see text). translation systems (Och and Ney, 2004; Koehn et al., 2003) and use Moses (Koehn et al., 2007) to search for the best target sentence. The search uses the following models: a phrase table, providing 4 scores and a phrase penalty, a lexicalized reordering model (7 scores), a language model score and a word penalty. These fourteen scores are weighted and linearly combined (Och and Ney, 2002; Och, 2003); their respective weights are learned on development data so as to maximize the BLEU score. In the following, we detail several aspects of our systems. 2.1 Translation models The translation models deployed in our systems for the europarl condition were trained on the provided Europarl parallel data only. For the news condition, they were trained on the Europarl data merged with Source text Europarl Europarl Other News Co. Europarl + News Co. sources Translation model 4g language model 4g language model Phrase pair extraction or Moses LM Interpolation or $n$−best translations Neural network R</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. J. Och. 2003. Minimum error rate training in statistical machine translation. In Proc. ACL, Sapporo, Japan. H. Schwenk. 2007. Continuous space language models. Computer Speech and Language, 21:492–518.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>