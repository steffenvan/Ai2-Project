<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000493">
<title confidence="0.991394">
Learning Structures of Negations from Flat Annotations
</title>
<author confidence="0.9963">
Vinodkumar Prabhakaran
</author>
<affiliation confidence="0.9967375">
Department of Computer Science
Columbia University
</affiliation>
<address confidence="0.912549">
New York, NY, USA
</address>
<email confidence="0.998924">
vinod@cs.columbia.edu
</email>
<note confidence="0.372137333333333">
Branimir Boguraev
IBM Watson
Thomas J. Watson Research Center
</note>
<author confidence="0.391029">
Yorktown Heights, NY, USA
</author>
<email confidence="0.990557">
bran@us.ibm.com
</email>
<sectionHeader confidence="0.993739" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999916875">
We propose a novel method to learn negation
expressions in a specialized (medical) domain.
In our corpus, negations are annotated as ‘flat’
text spans. This allows for some infelicities
in the mark-up of the ground truth, making it
less than perfectly aligned with the underly-
ing syntactic structure. Nonetheless, the nega-
tions thus captured are correct in intent, and
thus potentially valuable. We succeed in train-
ing a model for detecting the negated pred-
icates corresponding to the annotated nega-
tions, by re-mapping the corpus to anchor its
‘flat’ annotation spans into the predicate argu-
ment structure. Our key idea—re-mapping the
negation instance spans to more uniform syn-
tactic nodes—makes it possible to re-frame
the learning task as a simpler one, and to lever-
age an imperfect resource in a way which en-
ables us to learn a high performance model.
We achieve high accuracy for negation detec-
tion overall, 87%. Our re-mapping scheme
can be constructively applied to existing flatly
annotated resources for other tasks where syn-
tactic context is vital.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999792333333333">
Accounting for extra-propositional aspects of mean-
ing in text is a very active NLP research area in
recent years, exploring different aspects of mean-
ing such as factivity (Saur´ı and Pustejovsky, 2009),
uncertainty/hedging (Farkas et al., 2010), commit-
ted belief (Prabhakaran et al., 2010), and modalities
(Prabhakaran et al., 2012a). Among these, negation
detection has generated special interest because of
demonstrated needs for negation detection capabil-
</bodyText>
<page confidence="0.979583">
71
</page>
<bodyText confidence="0.999946657142857">
ity in practical applications such as information re-
trieval (Averbuch et al., 2004), information extrac-
tion (Meystre et al., 2008), sentiment analysis (Wie-
gand et al., 2010; Councill et al., 2010), and relation
detection (Chowdhury and Lavelli, 2013).
Accurately detecting negations is especially im-
portant in systems processing medical/clinical text.
Consider the segment “Mild hyperinflation without
focal pneumonia”, taken from a patient’s clinical
record. It indicates the absence of focal pneumonia
in the patient. Not capturing this extra-propositional
aspect of negation concerning focal pneumonia will
lead to wrong—and harmful—inferences in down-
stream processing, e.g. by a clinical decision sup-
port system. The need for sophisticated negation de-
tection capabilities in clinical text is even more ur-
gent given the broadening spectrum of applications
in this domain: clinical question answering (Lee
et al., 2006), clinical decision support (Demner-
Fushman et al., 2009), medical information extrac-
tion (Uzuner et al., 2010), medical entity relation
mining (Tymoshenko et al., 2012), patient history
tracking (Raghavan et al., 2012), etc. Our moti-
vation for detecting negations in medical texts also
stems from practical concerns of an operational
medical question answering (QA) system (Ferrucci
et al., 2013).
Most recent approaches to negation detection
adopt supervised machine learning techniques to
learn the phraseology of negation-containing ex-
pressions. They often follow a two step process—
detection of negation cues (“no”, “without”, ...),
followed by detection of their associated scopes.
Cue detection is a relatively simple task, since the set
of cue words is not large. Determining the scope of
</bodyText>
<note confidence="0.9323825">
Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 71–81,
Denver, Colorado, June 4–5, 2015.
</note>
<bodyText confidence="0.999839012820513">
a negation cue, on the other hand, is more challeng-
ing. Negation constructs do not necessarily apply
to entire sentences: in the earlier example, Mild hy-
perinflation is not negated. The scope detection task
is to identify the part(s) of the sentence that come
under the scope of a negation cue. Scope detection
is crucial for interpreting negations, and to that end,
the BioScope corpus (Vincze et al., 2008) was re-
leased, with annotations of both negation cues and
their associated scopes.
The fact that these scopes are represented only as
text-spans is a drawback of BioScope. Without be-
ing anchored to a syntactic analysis of the sentences
in which they occur, BioScope’s scope annotations
suffer from a variety of inconsistencies of mark-up.
They also may, and occasionally do, fail to align
with the underlying syntactic structures (Vincze et
al., 2011; Stenetorp et al., 2012). Such inconsisten-
cies make it hard for a system to learn the actual
syntactic patterns connecting negation cues and their
scopes—which are, after all, the real object of nega-
tion interpretation.
The insight that we develop in this paper is that
a scope span can be associated with one or more
nodes in the syntactic analysis of a negated expres-
sion, and that these will be further connected—in a
systematic way—to the negation cue node. Map-
ping loosely and/or inconsistently bounded spans to
unique syntactic nodes (and configurations thereof)
reduces the noise inherent in BioScope. The learn-
ing task for scope detection would now be the easier
one of learning negation scoping patterns from syn-
tactic representations.
To elaborate on this, we look at BioScope’s is-
sues in some detail (Section 3.1). Our intent here,
however, is not to offer a review or criticism of the
corpus, nor to suggest how to correct those issues.
Given that we do want to use BioScope (we moti-
vate our choice of BioScope separately in Section 2),
we propose a new method for learning how to de-
tect negated constructs which are rooted in syntac-
tic structure elements, and therefore directly usable
by downstream components, many of which typi-
cally assume awareness of syntax. Our method is
to re-map BioScope’s scope span annotations onto
the syntactic space and then to use those annota-
tions’ corresponding node structure(s) to train a sys-
tem to automatically detect negated syntactic nodes.
As outlined earlier, due to the re-mapping, many
syntactic inconsistencies would not be seen by the
learner, which now is trained on cleaner data and
consequently, faces a simpler learning problem.
We verify that our re-mapping process identifies
the correct negated syntactic node with high accu-
racy (93%); this validates the approach we propose
here. Our supervised learning system, trained us-
ing re-mapped scope nodes to detect them automat-
ically, obtains an overall accuracy of 87%, using
automatically tagged cues. In the light of state-of-
the-art performance figures, ours is a novel, con-
structive and pragmatic approach which allows us to
leverage effectively an important resource, despite
its representational imperfections, and to utilize the
essential ‘nuggets’ it captures and exposes—namely
the expressions of negated predicates. This strategy
can also be applied to other tasks where syntactic
context is important but resources are annotated by
text spans only (e.g. hedge detection (Farkas et al.,
2010)).
The rest of the paper is grounded in discussion
of related work, and of BioScope and its annotations
(Section 2), highlighting some relevant details of the
issues with these (Section 3). We then outline the
syntactic framework we use in Section 4. Section 5
presents our re-mapping of BioScope, and Section 6
offers experiments and results. In Section 7, we
compare our performance with previously published
studies. Section 8 concludes the paper.
</bodyText>
<sectionHeader confidence="0.971074" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999780066666667">
Early approaches in negation detection were lim-
ited in the nature of negation they were concerned
with. The prime example here, NegEx (Chapman et
al., 2001), took a view of negation interpretation to
be “determining whether a finding or disease ... is
present or absent”. From such a standpoint, the no-
tion of scope is limited, since the scope is always the
finding or disease that follows a negation cue. While
this works well for simpler expressions of negations,
it tends to fail for more complex negation constructs.
More recent approaches attempt to tackle the vari-
ability in scopes encountered in broader data by us-
ing statistical learning methods grounded in publicly
available corpora with cue and scope annotations.
The first such corpus was BioScope (Vincze et
</bodyText>
<page confidence="0.99679">
72
</page>
<bodyText confidence="0.999932872340426">
al., 2008), which annotates negation cues and asso-
ciated scopes in 3 genres—medical abstracts, sci-
entific papers and clinical records. The BioNLP
Event Extraction (EE) shared task corpus (Kim et
al., 2009) also marks negation in the event annota-
tions on sentences from molecular biology literature.
Most recently, the *SEM 2012 shared task corpus
(Morante and Blanco, 2012) marks negations, their
foci, and scopes in sentences from Conan Doyle sto-
ries in an attempt to extend the research on negation
to the general domain. Both the BioNLP-EE and
*SEM corpora capture negations within—and there-
fore aligned with—syntactic analyses. Thus they de-
ploy annotation schemes which assume downstream
consumers of some granular negation representa-
tion, learnable from the annotated resource(s). How-
ever, the language in both of them differs greatly
from the language encountered in clinical text, mak-
ing them unsuitable for our QA system require-
ments. In contrast, BioScope matches our genre of
clinical text. As an additional plus, it captures nega-
tion in a task-independent, linguistically motivated
framework, which enables the building of systems
applicable to a wider range of domains.
BioScope’s negation-scope-as-span annotation
framework, however, limits th corpus utility. Vari-
ous approaches have used it to train negation scope
span detection systems, and many have shown the
importance of deep syntactic features in that task
(e.g., (Ballesteros et al., 2012; Velldal et al., 2012;
Zou et al., 2013)). They share a drawback: they are
optimized for predicting the spans as they are anno-
tated in BioScope—despite its various syntactic in-
consistencies. For example, Ballesteros et al. (2012)
use manual rules to detect the voice (passive or ac-
tive) of a verb phrase; this is motivated by an an-
notation guideline for whether to include verb sub-
jects in the span or not. In reality, what matters in
the end is whether a detection system can capture
the underlying phenomenon of negation that the an-
notations stand to represent, and not whether it can
accurately replicate the representational choices the
annotations follow. In light of this, our approach dif-
fers from the conventional ones, in that it mitigates
the effects of inconsistencies in BioScope’s original
annotations by re-mapping it, as we explain in Sec-
tion 5 below.
</bodyText>
<sectionHeader confidence="0.985483" genericHeader="method">
3 BioScope Corpus
</sectionHeader>
<bodyText confidence="0.999911">
The BioScope corpus (Vincze et al., 2008) is an-
notated for hedges and negations in sentences from
biomedical domain; in this work, we use only the
negation annotations. A negation (or hedge) annota-
tion comprises a cue and a corresponding scope. The
scope (hereafter BioScopeScopeSpan) is marked as
a contiguous text-span including the associated cue
annotation (BioScopeCue). BioScope contains sen-
tences from three sub-genres—abstracts, full papers,
and clinical records. We use all three sub-corpora.
We divide each sub-corpus into ‘Train’ (70%), ‘Dev’
(15%) and ‘Test’ (15%) sets through random sam-
pling. We use sentences in the Train and Dev sets
to build and select best models and report the results
obtained by our best models on Dev and Test sets.
</bodyText>
<subsectionHeader confidence="0.990146">
3.1 Issues Challenging the Use of BioScope
</subsectionHeader>
<bodyText confidence="0.999804482758621">
BioScope is an important resource that has helped
deeper understanding of various linguistic aspects
of negation in a task independent manner. But, as
we saw in the preceding sections, while demonstrat-
ing the importance of syntactic context for negation
detection, recent efforts share the frustration aris-
ing from the fact that BioScopeScopeSpan annota-
tions do not align with underlying syntactic struc-
ture. This problem is further exacerbated by in-
consistencies in the corpus annotation. From a
performance-driven point of view alone, negation
detection systems trained over BioScope annota-
tions are optimized to match the annotated spans in
the corpus (as discussed in Section 2). However,
for a negation detection system followed by down-
stream components implementing negation-driven
inference, spans alone are not sufficient—especially
spans which do not align with syntax. Negated ex-
pressions need to be captured within their syntactic
context, and for this, we need the uniformity of syn-
tax structures.
The misalignment issues of BioScopeScopeSpan
annotations with respect to the underlying syntac-
tic structures have already been extensively studied
(Vincze et al., 2011; Stenetorp et al., 2012). Vincze
et al. (2011) point out infelicities and mismatches,
comparing BioScope annotations with the more syn-
tactically oriented negated event annotations in the
BioNLP-EE corpus (Kim et al., 2009). Inconsis-
</bodyText>
<page confidence="0.993465">
73
</page>
<bodyText confidence="0.99990509375">
tencies are largely due to ‘loose’ annotation guide-
lines for BioScope, which are not rigorous enough in
ensuring that annotation spans align with syntactic
analyses. Given our position in this work—utilize
BioScope, despite its shortcomings, in an alterna-
tive framework of analysis and training (see Sec-
tion 2)—we explain some of the commonly occur-
ring inconsistencies in this section. For this pur-
pose, we use example annotations e1-e5 from Bio-
Scope. (Boldface denotes BioScopeCue annotations
and italics denotes corresponding BioScopeScopeS-
pan annotations as present in the BioScope corpus.)
One of the main source of inconsistencies within
the syntactic space is with regard to the inclusion
or exclusion of subjects of propositions. For exam-
ple, in e1, the annotations identify the negation span
to be the entire clause following the word but, in-
cluding its subject and object. However, in e2, only
the object of the predicate is marked as the negation
scope (Figure 1). Vincze et al. (2011) state that “the
treatment of subjects [in BioScope] remains prob-
lematic since in BioScope it is only the complements
that are usually included within the scope of a key-
word (that is, subjects are not with the exception of
passive constructions and raising verbs)”. Leaving
aside the rationale for such a guideline, we note that
such an inconsistency is harmful: proper interpreta-
tion of negated propositions does require a subject,
and making annotations consistent by ignoring sub-
jects, if present, does not help downstream compo-
nents. Additionally, it makes the learning of con-
texts of negated propositions difficult.
</bodyText>
<listItem confidence="0.427596333333333">
e1: The cDNA hybridized to multiple transcripts in
pre-B and B-cell lines, but transcripts were not
detected at significant levels in plasmacytoma,
T-cell, and nonlymphoid cell lines.
e2: Moreover, cAMP activators did not activate
NF-kappa B in Jurkat cells.
</listItem>
<bodyText confidence="0.99380388">
Another problem with BioScopeScopeSpan anno-
tations stems from the requirement that such anno-
tations should have contiguous spans. For exam-
ple, since sentence e3 is a passive construction, the
corresponding BioScopeScopeSpan annotation cap-
tures the subject (mechanism) as well. The conti-
guity requirement then forces the proposition IFNs
mediate this inhibition—which modifies the subject
but is itself not negated (Figure 2)—to be included
within the BioScopeScopeSpan and therefore to be
interpretable as negated. Clearly, there may be arbi-
trary intervening text in such, and similar, construc-
tions, again making the learning task difficult.
e3: However, the mechanism by which IFNs mediate
this inhibition has not been defined.
Sometimes, the BioScopeScopeSpan annotation
boundaries do not align with syntactic constituents.
For example, in e4, the BioScopeScopeSpan anno-
tation excludes the determiner the from the scope
while in e5, the determiner the is part of the scope.
This might be due to the guideline that the scope
should include the cue as well, causing to extend
the scope annotation leftward until it covers the cue
word (absence). Still, we are left with a span bound-
ary which crosses, partially, a noun phrase boundary.
</bodyText>
<listItem confidence="0.737401">
e4: Tal-1 transcription was shown to be monoallelic in
Jurkat, a T-cell line that expresses tal-1 in the
absence of apparent genomic alteration of the locus.
e5: The effects of selenium were specific for NF-kappa
B, since the activity of the transcription factor AP-1
was not suppressed.
</listItem>
<bodyText confidence="0.999922375">
A system trained and optimized on how well it
predicts the BioScopeScopeSpan boundaries suffers
from also being forced to learn such syntactic in-
consistencies along with the syntactic patterns that
truly capture negation. In addition to learning the
actual negation patterns, such a system is also forced
to learn artifacts of annotation guidelines like: when
to include or exclude subjects and when to include or
exclude determiners. In order to circumvent this, we
propose an approach in which we first re-map Bio-
Scope annotations onto nodes in the syntactic tree,
and then train a system using features derived from
the nodes, and node configurations, providing the
context for the negation cue and scope nodes. We
next describe the syntactic framework we use and
then explain our approach in detail.
</bodyText>
<sectionHeader confidence="0.998979" genericHeader="method">
4 Syntactic Framework
</sectionHeader>
<bodyText confidence="0.999873333333333">
Negation, as a language device, is naturally concep-
tualized as applying to fully instantiated predicate-
argument clusters. We therefore use predicate ar-
gument graphs as structural abstractions of syntax
trees. Additional advantages of these abstractions
include their affinity for having extra-propositional
</bodyText>
<page confidence="0.996611">
74
</page>
<figureCaption confidence="0.99983">
Figure 1: PAS for e2: “Moreover, cAMP activators did not activate NF-kappa B in Jurkat cells”
</figureCaption>
<bodyText confidence="0.9875284">
aspects of meaning ‘layered’ onto the representa-
tion (precedents in prior studies can be found in e.g.
(Saur´ı and Pustejovsky, 2009; Diab et al., 2009)),
and their pervasive use in a state-of-the-art QA
system—for question analysis, candidate genera-
tion, and analysis of passage evidence (Ferrucci et
al., 2010; Ferrucci, 2012)—which is at the heart of
our medical adaptation (Ferrucci et al., 2013).
We use predicate-argument structure (PAS) (Mc-
Cord et al., 2012) derived from dependency parses
produced by the English Slot Grammar parser (Mc-
Cord, 1990). In addition to normalizing across dif-
ferent tree structures expressing essentially the same
meaning, PAS provides a simplified view over ‘raw’
syntactic trees, gathering all arguments to a predi-
cate from local, and distant, parse tree nodes (see
(McCord et al., 2012) for details). Figures 1 and 2
show the PASes for examples e2 and e3. By localiz-
ing the logical arguments to a proposition, predicate-
based representation provides direct access to all ar-
guments of e.g. a verb frame: an important require-
ment for extracting context-denoting syntactic fea-
tures.
PAS-based view into sentences offers unambigu-
ously uniform treatment of some of the issues high-
lighted in the previous section. For example, going
back to e2, and the rationale for including or exclud-
ing subjects in the scope of a negation, we observe
that verb nodes in the PAS always have fully instan-
tiated frames, with subject arguments bound to the
</bodyText>
<figureCaption confidence="0.967122">
Figure 2: PAS for e3: “However, the mechanism by which IFNs mediate this inhibition has not been defined.”
</figureCaption>
<page confidence="0.992738">
75
</page>
<bodyText confidence="0.999966454545455">
predicate nodes corresponding to the deep syntac-
tic subject: observe how activator is ‘subj’ both to
do and activate. Thus whether to include a subject
into a verb scope (e.g. not activate) becomes largely
irrelevant, and a PAS-based scope rendering can al-
ways include subjects. As another example, for the
PAS for e3, the granular analysis of the arguments to
the predicate for define can be leveraged to designate
the predicate node for mechanism as the scope of the
negation not (defined), while excluding the IFN me-
diate inhibition subtree from the same scope.
</bodyText>
<sectionHeader confidence="0.9645625" genericHeader="method">
5 Learning Negations from Re-mapped
BioScope
</sectionHeader>
<bodyText confidence="0.9866066">
Our goal is a system for automatic identification of
negations and their scopes within the PAS of a sen-
tence. Our resource for this is BioScope, with its
text-based span annotations. We propose a novel ap-
proach, realized as a two-step process:
</bodyText>
<listItem confidence="0.999512777777778">
(1) BioScope-to-PAS mapping: map BioScope’s
text-span cue and scope annotations to PAS
nodes (CuePredicate and NegatedPredicate) by
identifying the predicate nodes in the PAS of the
sentence that best capture the annotations.
(2) NegatedPredicate learning: train a statistical
model to automatically identify the scope predi-
cate using features from the PAS context of cue
and scope predicates.
</listItem>
<subsectionHeader confidence="0.950633">
5.1 BioScopeScopeSpan-to-NegatedPredicate
Mapping
</subsectionHeader>
<bodyText confidence="0.999975217391304">
Having obtained PASes for sentences in the corpus,
we mark the PAS node with the minimal span that
contains the entire BioScopeScopeSpan annotation
as the NegatedPredicate. We define the ‘span’ of a
PAS node to be the span of text covered by the sub-
tree rooted at that node, which includes the spans of
all of its descendants. Similarly, we mark the PAS
node with the minimal span that contains the Bio-
ScopeCue annotation as the CuePredicate.
For example, in Figure 1, the predicate labeled not
was marked as the CuePredicate and the predicate
labeled do was marked as the corresponding Negat-
edPredicate. In order to perform a sanity check
on our re-mapping, we judged whether the predi-
cate nodes that we mark as NegatedPredicate in sen-
tences from our Dev set are in fact the ones being
negated. Of the 470 sentences containing negations,
13 (2.8%) failed to parse, breaking the mapping. In
other words, our mapping strategy has coverage of
about 97.2%. Of the sentences where a Negated-
Predicate was obtained, our mapping achieved an
accuracy of 92.8% in finding the correct negated
predicate.
</bodyText>
<subsectionHeader confidence="0.998079">
5.2 NegatedPredicate Learning
</subsectionHeader>
<bodyText confidence="0.99998582051282">
We now build a supervised learning system which,
given a CuePredicate in a sentence, will identify its
corresponding NegatedPredicate. For every predi-
cate p in a sentence PAS with a CuePredicate, we
create an instance &lt;CuePredicate, p&gt;. The instance
&lt;CuePredicate, p&gt; is assigned true if p is the cor-
responding NegatedPredicate. For all other p in the
PAS, &lt;CuePredicate, p&gt; is assigned false.
We extract three types of features for each in-
stance &lt;CuePredicate, p&gt;: 1) token features (word
lemma and POS tag) of CuePredicate and p, 2) syn-
tactic contextfeatures (token features of parent pred-
icates and all argument predicates) of CuePredicate
and p, and 3) predicate pair features (is CuePredi-
cate argument of p or vice versa?; distance between
CuePredicate and p; relative position of CuePredi-
cate and p).
We use the ClearTk (Ogren et al., 2008) frame-
work to build our system and perform experi-
ments. We use quadratic kernel SVMs in all our
experiments. The ClearTK wrapper for SVM-
Light (Joachims, 2006) internally shifts the predic-
tion threshold using sigmoid fitting to deal with the
highly skewed class imbalance (around 5% of posi-
tive cases) in our data. Prior studies (Prabhakaran et
al., 2012b) have shown this approach to be effective
in addressing the class imbalance problem.
During prediction, given an unseen sentence PAS
and a CuePredicate (either GOLD or automatically
predicted) in it, we need to find the correspond-
ing NegatedPredicate. We iterate over all candi-
date predicates c in the sentence PAS and apply
our trained model to assign a true or false value
to &lt;CuePredicate, c&gt;. For any CuePredicate in a
sentence there must be one and only one Negat-
edPredicate, since BioScope corpus marks a single
BioScopeScopeSpan for every BioScopeCue. We
choose the c for which &lt;CuePredicate, c&gt; is as-
signed a true value with the highest confidence as
</bodyText>
<page confidence="0.973142">
76
</page>
<table confidence="0.999940833333333">
Precision On Dev Precision On Test
Recall F-measure Recall F-measure
Clinical 95.68 95.68 95.68 96.15 96.9 96.53
Abstracts 94.4 94.4 94.4 95.42 96.9 96.15
Papers 79.22 96.83 87.14 85.29 98.31 91.34
Overall 92.36 95.11 93.71 94.13 97.09 95.58
</table>
<tableCaption confidence="0.999945">
Table 1: Performance of our CuePredicate detection on Dev and Test sets
</tableCaption>
<bodyText confidence="0.9947315">
the NegatedPredicate. If &lt;CuePredicate, c&gt; is as-
signed a false value for all c, we choose the c with
the least confident false assignment as the Negated-
Predicate.
</bodyText>
<sectionHeader confidence="0.99838" genericHeader="method">
6 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999989785714286">
The most commonly used metric to evaluate nega-
tion scope span detection is Percentage of Correct
Scopes (PCS). PCS measures the percentage of ex-
act matches between predicted and actual scope
spans. Since our task is different—negated predicate
detection as opposed to negated span detection—
we report the Percentage of Correct Scope Predi-
cates (PCSP) obtained in our experiments. Mod-
els built from the composite training corpus com-
prising training corpora of all three genres (see Sec-
tion 3) perform better than models built separately
over each sub-corpus. We report results separately
for each sub-corpus, as well as for the entire corpus,
and compare them with a strong baseline.
</bodyText>
<subsectionHeader confidence="0.995689">
6.1 Gold vs. Predicted CuePredicates
</subsectionHeader>
<bodyText confidence="0.9998723125">
We report results for the NegatedPredicate detection
task obtained using GOLD CuePredicates as well as
predicted CuePredicates. In order to measure the
performance on predicted CuePredicates, we built a
CuePredicate detector using linear kernel SVM to
detect whether a predicate is a negation cue or not.
We use three types of features: 1) token features
(lemma and POS) of the predicate, 2) linear con-
text (token features of the token after the predicate in
the sentence; features of tokens before the predicate
turned out to be not useful), and 3) syntactic context
(token features of parent and argument predicates).
As shown in Table 1, our CuePredicate tagger ob-
tained F-measures in the range of state-of-the-art re-
sults on negation cue detection using the BioScope
(90-96% F-measure (Velldal et al., 2012)).
</bodyText>
<subsectionHeader confidence="0.998468">
6.2 Baseline NegatedPredicate Predictor
</subsectionHeader>
<bodyText confidence="0.999962142857143">
Since this formulation of the task is new, we built
a strong baseline system appropriate for it. In our
baseline, we predict the NegatedPredicate to be the
parent predicate of the CuePredicate, if the CuePred-
icate is a terminal node in the PAS (this will cover
the most common cues such as no and not). If the
CuePredicate is not a terminal node (which covers
the cases of verbal negation cues such as failed),
we choose the CuePredicate itself as the Negated-
Predicate. Columns 1 and 3 of Table 2 show PCSP
obtained by the baseline algorithm on our Dev and
Test sets respectively using GOLD CuePredicates.
Columns 5 and 7 show corresponding results using
predicted CuePredicates.
</bodyText>
<subsectionHeader confidence="0.997172">
6.3 Our NegatedPredicate Predictor
</subsectionHeader>
<bodyText confidence="0.999941736842106">
The results obtained by our NegatedPredicate de-
tection system (Section 5.2) on Dev and Test sets
using GOLD CuePredicates is shown in Columns
2 and 4 of Table 2. Our system outperforms the
baseline by a large margin in all cases, with espe-
cially high performance in clinical records. We ob-
tain an overall PCSP of 90.2% and 89.2% on Dev
and Test sets respectively. The results we obtain in
Test set are in the range of what we obtain using
Dev set, which shows that our system does not over-
fit to our Dev set. On applying our system on pre-
dicted CuePredicates, the overall results (columns 6
and 8) decrease by around 3-5% from using GOLD
CuePredicates. The overall PCSP value of 86.8%
obtained on the Test set reflects the accuracy of our
end-to-end system on a blind test. Note that this is a
conservative estimate since we penalize our system
for failed parses where the mapping step could not
find a GOLD NegatedPredicate to compare against.
</bodyText>
<page confidence="0.995825">
77
</page>
<table confidence="0.995266714285714">
Gold Cues (On Dev) Gold Cues (On Test) Predicted Cues (On Dev) Predicted Cues (On Test)
Baseline System Baseline System Baseline System Baseline System
(1) (2) (3) (4) (5) (6) (7) (8)
Clinical 83.45 97.12 88.37 100.00 82.01 93.53 87.60 96.90
Abstracts 81.34 89.18 79.07 84.50 76.49 83.96 77.52 82.56
Papers 73.02 79.37 81.36 86.44 66.67 71.43 77.97 83.05
Overall 80.85 90.21 82.06 89.23 76.81 85.11 80.49 86.77
</table>
<tableCaption confidence="0.999176">
Table 2: Percentage of Correct NegatedPredicate (PCSP) on Dev and Test sets
</tableCaption>
<sectionHeader confidence="0.543304" genericHeader="method">
7 Comparison with Previous Approaches
</sectionHeader>
<bodyText confidence="0.999561416666667">
Comparing our system with previously published
approaches to negation scope detection is not
straightforward, essentially because our and their
tasks are different: negated predicate detection vs.
negated scope span detection. The resulting differ-
ence in evaluation metrics makes PCS numbers re-
ported elsewhere not directly comparable with our
PCSP results presented in Table 2. To make such
a comparison meaningful, we transform (reverse
map) the NegatedPredicates we identify back into
text spans and use those to derive PCS values better
aligned with previously published ones. (Note that
these PCS numbers are still not directly comparable,
due to differences in experiment setup, e.g. cross
validation vs. held out test set.
Transforming the NegatedPredicates back to Bio-
ScopeScopeSpan annotations is not trivial. As dis-
cussed in Section 5.1, we choose NegatedPredicate
to be the predicate node that minimally covers Bio-
ScopeScopeSpan. Hence, the span of a Negated-
Predicate may include text spans that were originally
not part of the corresponding BioScopeScopeSpan
annotation. Therefore, we built a statistically trained
system to predict whether the span of a descendant
node of a NegatedPredicate should, or should not,
be included in reverse mapping that NegatedPredi-
cate to the corresponding BioScopeScopeSpan.
We use the same set of features and learning con-
figuration as we used for NegatedPredicate learning
(Section 5.2). Our transformation obtained high ac-
curacy (94.9%) for the clinical records. However, it
was a harder task for abstracts (66.1%) and papers
(73.1%) which contain more complex sentences.
We applied this transform on the predicate nodes
identified by our end-to-end system (Section 6.3) in
order to derive PCS values. In Table 3, we com-
pare these PCS values against four previous studies
above (due to lack of space, we do not discuss their
techniques here), as well as with a baseline of our
own where we use the covered text of the predicate
node and all of its descendants as scopes used in the
comparison. Our system (with transform) obtains
higher PCS values than all other reported studies on
the clinical records. The PCS values obtained for the
abstracts and papers sub-corpora are lower, but still
in comparable range to the other studies. It is im-
portant to note that the main source of error here is
the NegatedPredicate-to-BioScopeScopeSpan trans-
</bodyText>
<table confidence="0.802403428571429">
Morante09 Ballestros12Velldal12 Ours (With Covered Text) Ours (With Transform) Zou13
(1) (2) (3) On Dev On Test On Dev On Test (8)
(4) (5) (6) (7)
Clinical 70.75 89.06 89.41 88.49 89.92 91.37 92.25 85.31
Abstracts 66.07 68.92 72.89 35.45 35.27 61.94 58.53 76.90
Papers 41.00 61.43 68.09 33.33 23.73 53.97 47.46 61.19
Overall - - - 50.85 49.55 69.57 66.82 -
</table>
<tableCaption confidence="0.94961125">
Table 3: PCS measures from previous BioScope span detection approaches and our end-to-end system.
Col. 1-3: end-to-end systems (Morante and Daelemans, 2009), (Ballesteros et al., 2012), and (Velldal et al., 2012);
Col. 4-7: our end-to-end system with different ways of obtaining the spans in our Dev and Test sets;
Col. 8: (Zou et al., 2013) system using GOLD cues (often 5-10% higher than using predicted cues)
</tableCaption>
<page confidence="0.997924">
78
</page>
<bodyText confidence="0.999979">
form step, with its inherent lower accuracies for
these two corpora, as reported above. We emphasize
that for practical applications this transformation is
of little use: what matters more, certainly for a nega-
tion detection system feeding downstream compo-
nents, are the PCSP values presented in Section 6.
</bodyText>
<sectionHeader confidence="0.967535" genericHeader="discussions">
8 Discussion and Conclusion
</sectionHeader>
<bodyText confidence="0.999993421052632">
The results for our system, with reverse map-
ping, offer indirect evidence for our observation
in Section 3.1: training a system to predict Bio-
ScopeScopeSpan boundaries would require it also
to learn inconsistencies in BioScope annotations.
This is a hard learning task, given the noise dis-
cussed in Section 3.1. Indeed, our results for learn-
ing the reverse-mapping transformation show that it
is harder to learn the specific annotation criteria in
BioScope than to learn the structural patterns ex-
pressing negations (which, as we saw in Section 6,
obtained close to 90% accuracy). While we had to
build a system to transform nodes back to spans for
the purposes of comparative analysis, such a system
has no role in our quest for practical negation detec-
tion and representation.
This substantiates our strategy of using BioScope,
as is, to learn not scope spans of negation expres-
sions, but negated predicates within the predicate-
argument structure (Section 5). The re-mapping
route takes us where we want to be, from the point
of view of a practical application of negation-based
inference: with access to negated predicate nodes.
The end-to-end accuracy (overall, across three dif-
ferent genres) of 87% on blind test validates the cre-
ative way we propose to make use of a valuable and
unique resource—despite its imperfections—by ex-
tracting the real value in it, while mitigating the ef-
fects of its various inconsistencies.
The results in Tables 2 and 3 show that we have
achieved our primary objective: using BioScope to
train a system which detects structured negation ex-
pressions in clinical text. Our approach to nega-
tion scope learning in the syntactic space is a two-
step one—first, re-mapping the text-span annota-
tions for negation scopes in BioScope to the syn-
tactic space and then training a scope predicate pre-
dictor. We show that our transformation introduces
only a small percentage of error and also that our
predicted nodes can be transformed back to original
span annotations with performance comparable to
other negation scope span prediction systems trained
on the same dataset. Notably, in clinical records, our
system outperforms reported state-of-the-art results
(column 8 of Table 3).
In a broader context, the work we report here indi-
rectly argues that the method we propose to circum-
vent certain limitations of a corpus like BioScope
can be applied to similar tasks (such as hedging, sen-
timent analysis, and variety of modalities, cf. Sec-
tion 1), for which current annotation resources offer
flat, and possibly inconsistent, annotations. In addi-
tion, we chose PAS as our syntactic framework for
the reasons listed in Section 4, but our approach is
not limited to PAS. Indeed, the claims, and methods,
are presented to be applicable, and workable, in a
more general syntactic framework.
</bodyText>
<sectionHeader confidence="0.99856" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9997992">
This work was conducted while the first author
was doing an internship at IBM Research at the TJ
Watson Research Center in Yorktown Heights, NY,
USA. We thank several anonymous reviewers for
their constructive feedback.
</bodyText>
<sectionHeader confidence="0.998535" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.710438590909091">
Mordechai Averbuch, Tom Karson, Benjamin Ben-Ami,
Oded Maimon, and Lior Rokach. 2004. Context-
sensitive medical information retrieval. In Proc.
of the 11th World Congress on Medical Informatics
(MEDINFO-2004), pages 1–8. Citeseer.
Miguel Ballesteros, Virginia Francisco, Alberto D´ıaz,
Jes´us Herrera, and Pablo Gerv´as. 2012. Inferring the
scope of negation in biomedical documents. In 13th
International Conference on Intelligent Text Process-
ing and Computational Linguistics (CICLING 2012),
New Delhi, 2012. Springer, Springer.
Wendy W Chapman, Will Bridewell, Paul Hanbury, Gre-
gory F Cooper, and Bruce G Buchanan. 2001. A sim-
ple algorithm for identifying negated findings and dis-
eases in discharge summaries. Journal of biomedical
informatics, 34(5):301–310.
Md. Faisal Mahbub Chowdhury and Alberto Lavelli.
2013. Exploiting the scope of negations and hetero-
geneous features for relation extraction: A case study
for drug-drug interaction extraction. In Proceedings
of the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
</reference>
<page confidence="0.995329">
79
</page>
<reference confidence="0.998562150943396">
Human Language Technologies, pages 765–771, At-
lanta, Georgia, June. Association for Computational
Linguistics.
Isaac G Councill, Ryan McDonald, and Leonid Ve-
likovich. 2010. What’s great and what’s not: learn-
ing to classify the scope of negation for improved sen-
timent analysis. In Proceedings of the workshop on
negation and speculation in natural language process-
ing, pages 51–59. Association for Computational Lin-
guistics.
Dina Demner-Fushman, Wendy W Chapman, and
Clement J McDonald. 2009. What can natural lan-
guage processing do for clinical decision support?
Journal of biomedical informatics, 42(5):760–772.
Mona Diab, Lori Levin, Teruko Mitamura, Owen Ram-
bow, Vinodkumar Prabhakaran, and Weiwei Guo.
2009. Committed belief annotation and tagging. In
Proceedings of the Third Linguistic Annotation Work-
shop, pages 68–73, Suntec, Singapore, August. Asso-
ciation for Computational Linguistics.
Rich´ard Farkas, Veronika Vincze, Gy¨orgy Szarvas,
Gy¨orgy M´ora, and J´anos Csirik, editors. 2010. Pro-
ceedings of the Fourteenth Conference on Computa-
tional Natural Language Learning. Association for
Computational Linguistics, Uppsala, Sweden, July.
David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James
Fan, David Gondek, Aditya A Kalyanpur, Adam Lally,
J William Murdock, Eric Nyberg, John Prager, et al.
2010. Building watson: An overview of the deepqa
project. AI magazine, 31(3):59–79.
David A Ferrucci, Anthony Levas, Sugato Bagchi, David
Gondek, and Erik T Mueller. 2013. Watson: Beyond
jeopardy! Artif. Intell., 199:93–105.
David A Ferrucci. 2012. Introduction to ”this is wat-
son”. IBM Journal of Research and Development,
56(3.4):1:1–1:15.
Thorsten Joachims. 2006. Training Linear SVMs in Lin-
ear Time. In Proceedings of the 12th ACM SIGKDD
international conference on knowledge discovery and
data mining, pages 217–226. ACM.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun’ichi Tsujii. 2009. Overview of
bionlp’09 shared task on event extraction. In Proceed-
ings of the Workshop on Current Trends in Biomedical
Natural Language Processing: Shared Task, BioNLP
’09, pages 1–9, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Minsuk Lee, James Cimino, Hai Ran Zhu, Carl Sable,
Vijay Shanker, John Ely, and Hong Yu. 2006. Beyond
information retrievalmedical question answering. In
AMIA Annual Symposium Proceedings, volume 2006,
page 469. American Medical Informatics Association.
Michael C. McCord, J. William Murdock, and Branimir
Boguraev. 2012. Deep parsing in Watson. IBM Jour-
nal of Research and Development, 56(3):3.
Michael C. McCord. 1990. Slot grammar: A sys-
tem for simpler construction of practical natural lan-
guage grammars. In R. Studer, editor, Natural Lan-
guage and Logic: Proc. of the International Scientific
Symposium, Hamburg, FRG, pages 118–145. Springer,
Berlin, Heidelberg.
St´ephane M Meystre, Guergana K Savova, Karin C
Kipper-Schuler, and John F Hurdle. 2008. Extracting
information from textual documents in the electronic
health record: a review of recent research. Yearbook
of medical informatics, pages 128–144.
Roser Morante and Eduardo Blanco. 2012. * SEM 2012
shared task: Resolving the scope and focus of nega-
tion. In Proceedings of the First Joint Conference on
Lexical and Computational Semantics-Volume 1: Pro-
ceedings of the main conference and the shared task,
and Volume 2: Proceedings of the Sixth International
Workshop on Semantic Evaluation, pages 265–274.
Association for Computational Linguistics.
Roser Morante and Walter Daelemans. 2009. A met-
alearning approach to processing the scope of nega-
tion. In Proceedings of the Thirteenth Conference
on Computational Natural Language Learning, pages
21–29. Association for Computational Linguistics.
Philip V. Ogren, Philipp G. Wetzler, and Steven Bethard.
2008. ClearTK: A UIMA toolkit for statistical natural
language processing. In Towards Enhanced Interoper-
ability for Large HLT Systems: UIMA for NLP work-
shop at Language Resources and Evaluation Confer-
ence (LREC).
Vinodkumar Prabhakaran, Owen Rambow, and Mona
Diab. 2010. Automatic committed belief tagging. In
COLING 2010: Posters, pages 1014–1022, Beijing,
China, August. COLING 2010 Organizing Commit-
tee.
Vinodkumar Prabhakaran, Michael Bloodgood, Mona
Diab, Bonnie Dorr, Lori Levin, Christine D. Piatko,
Owen Rambow, and Benjamin Van Durme. 2012a.
Statistical Modality Tagging from Rule-based Annota-
tions and Crowdsourcing. In Proceedings of the Work-
shop on Extra-Propositional Aspects of Meaning in
Computational Linguistics, pages 57–64, Jeju, Repub-
lic of Korea, July. Association for Computational Lin-
guistics.
Vinodkumar Prabhakaran, Owen Rambow, and Mona
Diab. 2012b. Predicting Overt Display of Power in
Written Dialogs. In Human Language Technologies:
The 2012 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, Montreal, Canada, June. Association for Compu-
tational Linguistics.
</reference>
<page confidence="0.970226">
80
</page>
<reference confidence="0.993280054545454">
976, Seattle, Washington, USA, October. Association
for Computational Linguistics.
Preethi Raghavan, Albert Lai, and Eric Fosler-Lussier.
2012. Learning to temporally order medical events in
clinical text. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 2: Short Papers), pages 70–74, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.
Roser Saur´ı and James Pustejovsky. 2009. Fact-
bank: a corpus annotated with event factuality.
Language Resources and Evaluation, 43:227–268.
10.1007/s10579-009-9089-9.
Pontus Stenetorp, Sampo Pyysalo, Tomoko Ohta, Sophia
Ananiadou, and Jun’ichi Tsujii. 2012. Bridging
the gap between scope-based and event-based nega-
tion/speculation annotations: a bridge not too far. In
Proceedings of the Workshop on Extra-Propositional
Aspects of Meaning in Computational Linguistics,
pages 47–56. Association for Computational Linguis-
tics.
Kateryna Tymoshenko, Swapna Somasundaran, Vinod-
kumar Prabhakaran, and Vinay Shet. 2012. Relation
mining in the biomedical domain using entity-level se-
mantics. In ECAI, pages 780–785.
¨Ozlem Uzuner, Imre Solti, and Eithon Cadag. 2010.
Extracting medication information from clinical text.
Journal of the American Medical Informatics Associa-
tion, 17(5):514–518.
Erik Velldal, Lilja Øvrelid, Jonathon Read, and Stephan
Oepen. 2012. Speculation and negation: Rules,
rankers, and the role of syntax. Computational lin-
guistics, 38(2):369–410.
Veronika Vincze, Gy¨orgy Szarvas, Rich´ard Farkas,
Gy¨orgy M´ora, and J´anos Csirik. 2008. The Bio-
Scope corpus: biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC bioinformatics,
9(Suppl 11):S9.
Veronika Vincze, Gyorgy Szarvas, Gyorgy Mora,
Tomoko Ohta, Rich´ard Farkas, et al. 2011. Lin-
guistic scope-based and biological event-based spec-
ulation and negation annotations in the BioScope and
Genia Event corpora. Journal of Biomedical Seman-
tics, 2(Suppl 5):S8.
Michael Wiegand, Alexandra Balahur, Benjamin Roth,
Dietrich Klakow, and Andr´es Montoyo. 2010. A sur-
vey on the role of negation in sentiment analysis. In
Proceedings of the workshop on negation and specu-
lation in natural language processing, pages 60–68.
Association for Computational Linguistics.
Bowei Zou, Guodong Zhou, and Qiaoming Zhu. 2013.
Tree kernel-based negation and speculation scope de-
tection with structured syntactic parse features. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, pages 968–
</reference>
<page confidence="0.999252">
81
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.103722">
<title confidence="0.999929">Learning Structures of Negations from Flat Annotations</title>
<author confidence="0.961063">Vinodkumar</author>
<affiliation confidence="0.999809">Department of Computer</affiliation>
<address confidence="0.869194">Columbia</address>
<author confidence="0.523501">New York</author>
<author confidence="0.523501">NY</author>
<email confidence="0.997332">vinod@cs.columbia.edu</email>
<author confidence="0.622484">Branimir</author>
<affiliation confidence="0.463488">IBM</affiliation>
<author confidence="0.940828">Thomas J Watson Research</author>
<affiliation confidence="0.671341">Yorktown Heights, NY,</affiliation>
<email confidence="0.999642">bran@us.ibm.com</email>
<abstract confidence="0.99824608">We propose a novel method to learn negation expressions in a specialized (medical) domain. In our corpus, negations are annotated as ‘flat’ text spans. This allows for some infelicities in the mark-up of the ground truth, making it less than perfectly aligned with the underlying syntactic structure. Nonetheless, the negations thus captured are correct in intent, and thus potentially valuable. We succeed in training a model for detecting the negated predicates corresponding to the annotated negations, by re-mapping the corpus to anchor its ‘flat’ annotation spans into the predicate argument structure. Our key idea—re-mapping the negation instance spans to more uniform syntactic nodes—makes it possible to re-frame the learning task as a simpler one, and to leverage an imperfect resource in a way which enables us to learn a high performance model. We achieve high accuracy for negation detection overall, 87%. Our re-mapping scheme can be constructively applied to existing flatly annotated resources for other tasks where syntactic context is vital.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mordechai Averbuch</author>
<author>Tom Karson</author>
<author>Benjamin Ben-Ami</author>
<author>Oded Maimon</author>
<author>Lior Rokach</author>
</authors>
<title>Contextsensitive medical information retrieval.</title>
<date>2004</date>
<booktitle>In Proc. of the 11th World Congress on Medical Informatics (MEDINFO-2004),</booktitle>
<pages>1--8</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="1901" citStr="Averbuch et al., 2004" startWordPosition="285" endWordPosition="288">ed resources for other tasks where syntactic context is vital. 1 Introduction Accounting for extra-propositional aspects of meaning in text is a very active NLP research area in recent years, exploring different aspects of meaning such as factivity (Saur´ı and Pustejovsky, 2009), uncertainty/hedging (Farkas et al., 2010), committed belief (Prabhakaran et al., 2010), and modalities (Prabhakaran et al., 2012a). Among these, negation detection has generated special interest because of demonstrated needs for negation detection capabil71 ity in practical applications such as information retrieval (Averbuch et al., 2004), information extraction (Meystre et al., 2008), sentiment analysis (Wiegand et al., 2010; Councill et al., 2010), and relation detection (Chowdhury and Lavelli, 2013). Accurately detecting negations is especially important in systems processing medical/clinical text. Consider the segment “Mild hyperinflation without focal pneumonia”, taken from a patient’s clinical record. It indicates the absence of focal pneumonia in the patient. Not capturing this extra-propositional aspect of negation concerning focal pneumonia will lead to wrong—and harmful—inferences in downstream processing, e.g. by a </context>
</contexts>
<marker>Averbuch, Karson, Ben-Ami, Maimon, Rokach, 2004</marker>
<rawString>Mordechai Averbuch, Tom Karson, Benjamin Ben-Ami, Oded Maimon, and Lior Rokach. 2004. Contextsensitive medical information retrieval. In Proc. of the 11th World Congress on Medical Informatics (MEDINFO-2004), pages 1–8. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miguel Ballesteros</author>
<author>Virginia Francisco</author>
<author>Alberto D´ıaz</author>
<author>Jes´us Herrera</author>
<author>Pablo Gerv´as</author>
</authors>
<title>Inferring the scope of negation in biomedical documents.</title>
<date>2012</date>
<booktitle>In 13th International Conference on Intelligent Text Processing and Computational Linguistics (CICLING 2012),</booktitle>
<publisher>Springer, Springer.</publisher>
<location>New Delhi,</location>
<marker>Ballesteros, Francisco, D´ıaz, Herrera, Gerv´as, 2012</marker>
<rawString>Miguel Ballesteros, Virginia Francisco, Alberto D´ıaz, Jes´us Herrera, and Pablo Gerv´as. 2012. Inferring the scope of negation in biomedical documents. In 13th International Conference on Intelligent Text Processing and Computational Linguistics (CICLING 2012), New Delhi, 2012. Springer, Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wendy W Chapman</author>
<author>Will Bridewell</author>
<author>Paul Hanbury</author>
<author>Gregory F Cooper</author>
<author>Bruce G Buchanan</author>
</authors>
<title>A simple algorithm for identifying negated findings and diseases in discharge summaries.</title>
<date>2001</date>
<journal>Journal of biomedical informatics,</journal>
<volume>34</volume>
<issue>5</issue>
<contexts>
<context position="7669" citStr="Chapman et al., 2001" startWordPosition="1187" endWordPosition="1190">)). The rest of the paper is grounded in discussion of related work, and of BioScope and its annotations (Section 2), highlighting some relevant details of the issues with these (Section 3). We then outline the syntactic framework we use in Section 4. Section 5 presents our re-mapping of BioScope, and Section 6 offers experiments and results. In Section 7, we compare our performance with previously published studies. Section 8 concludes the paper. 2 Background Early approaches in negation detection were limited in the nature of negation they were concerned with. The prime example here, NegEx (Chapman et al., 2001), took a view of negation interpretation to be “determining whether a finding or disease ... is present or absent”. From such a standpoint, the notion of scope is limited, since the scope is always the finding or disease that follows a negation cue. While this works well for simpler expressions of negations, it tends to fail for more complex negation constructs. More recent approaches attempt to tackle the variability in scopes encountered in broader data by using statistical learning methods grounded in publicly available corpora with cue and scope annotations. The first such corpus was BioSc</context>
</contexts>
<marker>Chapman, Bridewell, Hanbury, Cooper, Buchanan, 2001</marker>
<rawString>Wendy W Chapman, Will Bridewell, Paul Hanbury, Gregory F Cooper, and Bruce G Buchanan. 2001. A simple algorithm for identifying negated findings and diseases in discharge summaries. Journal of biomedical informatics, 34(5):301–310.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Faisal Mahbub Chowdhury</author>
<author>Alberto Lavelli</author>
</authors>
<title>Exploiting the scope of negations and heterogeneous features for relation extraction: A case study for drug-drug interaction extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>765--771</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="2068" citStr="Chowdhury and Lavelli, 2013" startWordPosition="310" endWordPosition="313">research area in recent years, exploring different aspects of meaning such as factivity (Saur´ı and Pustejovsky, 2009), uncertainty/hedging (Farkas et al., 2010), committed belief (Prabhakaran et al., 2010), and modalities (Prabhakaran et al., 2012a). Among these, negation detection has generated special interest because of demonstrated needs for negation detection capabil71 ity in practical applications such as information retrieval (Averbuch et al., 2004), information extraction (Meystre et al., 2008), sentiment analysis (Wiegand et al., 2010; Councill et al., 2010), and relation detection (Chowdhury and Lavelli, 2013). Accurately detecting negations is especially important in systems processing medical/clinical text. Consider the segment “Mild hyperinflation without focal pneumonia”, taken from a patient’s clinical record. It indicates the absence of focal pneumonia in the patient. Not capturing this extra-propositional aspect of negation concerning focal pneumonia will lead to wrong—and harmful—inferences in downstream processing, e.g. by a clinical decision support system. The need for sophisticated negation detection capabilities in clinical text is even more urgent given the broadening spectrum of appl</context>
</contexts>
<marker>Chowdhury, Lavelli, 2013</marker>
<rawString>Md. Faisal Mahbub Chowdhury and Alberto Lavelli. 2013. Exploiting the scope of negations and heterogeneous features for relation extraction: A case study for drug-drug interaction extraction. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 765–771, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isaac G Councill</author>
<author>Ryan McDonald</author>
<author>Leonid Velikovich</author>
</authors>
<title>What’s great and what’s not: learning to classify the scope of negation for improved sentiment analysis.</title>
<date>2010</date>
<booktitle>In Proceedings of the workshop on negation and speculation in natural language processing,</booktitle>
<pages>51--59</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2014" citStr="Councill et al., 2010" startWordPosition="303" endWordPosition="306">aspects of meaning in text is a very active NLP research area in recent years, exploring different aspects of meaning such as factivity (Saur´ı and Pustejovsky, 2009), uncertainty/hedging (Farkas et al., 2010), committed belief (Prabhakaran et al., 2010), and modalities (Prabhakaran et al., 2012a). Among these, negation detection has generated special interest because of demonstrated needs for negation detection capabil71 ity in practical applications such as information retrieval (Averbuch et al., 2004), information extraction (Meystre et al., 2008), sentiment analysis (Wiegand et al., 2010; Councill et al., 2010), and relation detection (Chowdhury and Lavelli, 2013). Accurately detecting negations is especially important in systems processing medical/clinical text. Consider the segment “Mild hyperinflation without focal pneumonia”, taken from a patient’s clinical record. It indicates the absence of focal pneumonia in the patient. Not capturing this extra-propositional aspect of negation concerning focal pneumonia will lead to wrong—and harmful—inferences in downstream processing, e.g. by a clinical decision support system. The need for sophisticated negation detection capabilities in clinical text is </context>
</contexts>
<marker>Councill, McDonald, Velikovich, 2010</marker>
<rawString>Isaac G Councill, Ryan McDonald, and Leonid Velikovich. 2010. What’s great and what’s not: learning to classify the scope of negation for improved sentiment analysis. In Proceedings of the workshop on negation and speculation in natural language processing, pages 51–59. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dina Demner-Fushman</author>
<author>Wendy W Chapman</author>
<author>Clement J McDonald</author>
</authors>
<title>What can natural language processing do for clinical decision support?</title>
<date>2009</date>
<journal>Journal of biomedical informatics,</journal>
<volume>42</volume>
<issue>5</issue>
<marker>Demner-Fushman, Chapman, McDonald, 2009</marker>
<rawString>Dina Demner-Fushman, Wendy W Chapman, and Clement J McDonald. 2009. What can natural language processing do for clinical decision support? Journal of biomedical informatics, 42(5):760–772.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona Diab</author>
<author>Lori Levin</author>
<author>Teruko Mitamura</author>
<author>Owen Rambow</author>
<author>Vinodkumar Prabhakaran</author>
<author>Weiwei Guo</author>
</authors>
<title>Committed belief annotation and tagging.</title>
<date>2009</date>
<booktitle>In Proceedings of the Third Linguistic Annotation Workshop,</booktitle>
<pages>68--73</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="17626" citStr="Diab et al., 2009" startWordPosition="2744" endWordPosition="2747">nd then explain our approach in detail. 4 Syntactic Framework Negation, as a language device, is naturally conceptualized as applying to fully instantiated predicateargument clusters. We therefore use predicate argument graphs as structural abstractions of syntax trees. Additional advantages of these abstractions include their affinity for having extra-propositional 74 Figure 1: PAS for e2: “Moreover, cAMP activators did not activate NF-kappa B in Jurkat cells” aspects of meaning ‘layered’ onto the representation (precedents in prior studies can be found in e.g. (Saur´ı and Pustejovsky, 2009; Diab et al., 2009)), and their pervasive use in a state-of-the-art QA system—for question analysis, candidate generation, and analysis of passage evidence (Ferrucci et al., 2010; Ferrucci, 2012)—which is at the heart of our medical adaptation (Ferrucci et al., 2013). We use predicate-argument structure (PAS) (McCord et al., 2012) derived from dependency parses produced by the English Slot Grammar parser (McCord, 1990). In addition to normalizing across different tree structures expressing essentially the same meaning, PAS provides a simplified view over ‘raw’ syntactic trees, gathering all arguments to a predic</context>
</contexts>
<marker>Diab, Levin, Mitamura, Rambow, Prabhakaran, Guo, 2009</marker>
<rawString>Mona Diab, Lori Levin, Teruko Mitamura, Owen Rambow, Vinodkumar Prabhakaran, and Weiwei Guo. 2009. Committed belief annotation and tagging. In Proceedings of the Third Linguistic Annotation Workshop, pages 68–73, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rich´ard Farkas</author>
</authors>
<title>Veronika Vincze, Gy¨orgy Szarvas, Gy¨orgy M´ora,</title>
<date>2010</date>
<booktitle>Proceedings of the Fourteenth Conference on Computational Natural Language Learning. Association for Computational Linguistics,</booktitle>
<editor>and J´anos Csirik, editors.</editor>
<location>Uppsala, Sweden,</location>
<marker>Farkas, 2010</marker>
<rawString>Rich´ard Farkas, Veronika Vincze, Gy¨orgy Szarvas, Gy¨orgy M´ora, and J´anos Csirik, editors. 2010. Proceedings of the Fourteenth Conference on Computational Natural Language Learning. Association for Computational Linguistics, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Ferrucci</author>
<author>Eric Brown</author>
<author>Jennifer Chu-Carroll</author>
<author>James Fan</author>
<author>David Gondek</author>
<author>Aditya A Kalyanpur</author>
<author>Adam Lally</author>
<author>J William Murdock</author>
<author>Eric Nyberg</author>
<author>John Prager</author>
</authors>
<title>Building watson: An overview of the deepqa project.</title>
<date>2010</date>
<journal>AI magazine,</journal>
<volume>31</volume>
<issue>3</issue>
<contexts>
<context position="17785" citStr="Ferrucci et al., 2010" startWordPosition="2767" endWordPosition="2770">predicateargument clusters. We therefore use predicate argument graphs as structural abstractions of syntax trees. Additional advantages of these abstractions include their affinity for having extra-propositional 74 Figure 1: PAS for e2: “Moreover, cAMP activators did not activate NF-kappa B in Jurkat cells” aspects of meaning ‘layered’ onto the representation (precedents in prior studies can be found in e.g. (Saur´ı and Pustejovsky, 2009; Diab et al., 2009)), and their pervasive use in a state-of-the-art QA system—for question analysis, candidate generation, and analysis of passage evidence (Ferrucci et al., 2010; Ferrucci, 2012)—which is at the heart of our medical adaptation (Ferrucci et al., 2013). We use predicate-argument structure (PAS) (McCord et al., 2012) derived from dependency parses produced by the English Slot Grammar parser (McCord, 1990). In addition to normalizing across different tree structures expressing essentially the same meaning, PAS provides a simplified view over ‘raw’ syntactic trees, gathering all arguments to a predicate from local, and distant, parse tree nodes (see (McCord et al., 2012) for details). Figures 1 and 2 show the PASes for examples e2 and e3. By localizing the</context>
</contexts>
<marker>Ferrucci, Brown, Chu-Carroll, Fan, Gondek, Kalyanpur, Lally, Murdock, Nyberg, Prager, 2010</marker>
<rawString>David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya A Kalyanpur, Adam Lally, J William Murdock, Eric Nyberg, John Prager, et al. 2010. Building watson: An overview of the deepqa project. AI magazine, 31(3):59–79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Ferrucci</author>
<author>Anthony Levas</author>
<author>Sugato Bagchi</author>
<author>David Gondek</author>
<author>Erik T Mueller</author>
</authors>
<title>Watson: Beyond jeopardy!</title>
<date>2013</date>
<journal>Artif. Intell.,</journal>
<contexts>
<context position="3135" citStr="Ferrucci et al., 2013" startWordPosition="463" endWordPosition="466">ision support system. The need for sophisticated negation detection capabilities in clinical text is even more urgent given the broadening spectrum of applications in this domain: clinical question answering (Lee et al., 2006), clinical decision support (DemnerFushman et al., 2009), medical information extraction (Uzuner et al., 2010), medical entity relation mining (Tymoshenko et al., 2012), patient history tracking (Raghavan et al., 2012), etc. Our motivation for detecting negations in medical texts also stems from practical concerns of an operational medical question answering (QA) system (Ferrucci et al., 2013). Most recent approaches to negation detection adopt supervised machine learning techniques to learn the phraseology of negation-containing expressions. They often follow a two step process— detection of negation cues (“no”, “without”, ...), followed by detection of their associated scopes. Cue detection is a relatively simple task, since the set of cue words is not large. Determining the scope of Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 71–81, Denver, Colorado, June 4–5, 2015. a negation cue, on the other hand, is more challenging. N</context>
<context position="17874" citStr="Ferrucci et al., 2013" startWordPosition="2781" endWordPosition="2784">tractions of syntax trees. Additional advantages of these abstractions include their affinity for having extra-propositional 74 Figure 1: PAS for e2: “Moreover, cAMP activators did not activate NF-kappa B in Jurkat cells” aspects of meaning ‘layered’ onto the representation (precedents in prior studies can be found in e.g. (Saur´ı and Pustejovsky, 2009; Diab et al., 2009)), and their pervasive use in a state-of-the-art QA system—for question analysis, candidate generation, and analysis of passage evidence (Ferrucci et al., 2010; Ferrucci, 2012)—which is at the heart of our medical adaptation (Ferrucci et al., 2013). We use predicate-argument structure (PAS) (McCord et al., 2012) derived from dependency parses produced by the English Slot Grammar parser (McCord, 1990). In addition to normalizing across different tree structures expressing essentially the same meaning, PAS provides a simplified view over ‘raw’ syntactic trees, gathering all arguments to a predicate from local, and distant, parse tree nodes (see (McCord et al., 2012) for details). Figures 1 and 2 show the PASes for examples e2 and e3. By localizing the logical arguments to a proposition, predicatebased representation provides direct access</context>
</contexts>
<marker>Ferrucci, Levas, Bagchi, Gondek, Mueller, 2013</marker>
<rawString>David A Ferrucci, Anthony Levas, Sugato Bagchi, David Gondek, and Erik T Mueller. 2013. Watson: Beyond jeopardy! Artif. Intell., 199:93–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Ferrucci</author>
</authors>
<title>Introduction to ”this is watson”.</title>
<date>2012</date>
<journal>IBM Journal of Research and Development,</journal>
<pages>56--3</pages>
<contexts>
<context position="17802" citStr="Ferrucci, 2012" startWordPosition="2771" endWordPosition="2772">ers. We therefore use predicate argument graphs as structural abstractions of syntax trees. Additional advantages of these abstractions include their affinity for having extra-propositional 74 Figure 1: PAS for e2: “Moreover, cAMP activators did not activate NF-kappa B in Jurkat cells” aspects of meaning ‘layered’ onto the representation (precedents in prior studies can be found in e.g. (Saur´ı and Pustejovsky, 2009; Diab et al., 2009)), and their pervasive use in a state-of-the-art QA system—for question analysis, candidate generation, and analysis of passage evidence (Ferrucci et al., 2010; Ferrucci, 2012)—which is at the heart of our medical adaptation (Ferrucci et al., 2013). We use predicate-argument structure (PAS) (McCord et al., 2012) derived from dependency parses produced by the English Slot Grammar parser (McCord, 1990). In addition to normalizing across different tree structures expressing essentially the same meaning, PAS provides a simplified view over ‘raw’ syntactic trees, gathering all arguments to a predicate from local, and distant, parse tree nodes (see (McCord et al., 2012) for details). Figures 1 and 2 show the PASes for examples e2 and e3. By localizing the logical argument</context>
</contexts>
<marker>Ferrucci, 2012</marker>
<rawString>David A Ferrucci. 2012. Introduction to ”this is watson”. IBM Journal of Research and Development, 56(3.4):1:1–1:15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Training Linear SVMs in Linear Time.</title>
<date>2006</date>
<booktitle>In Proceedings of the 12th ACM SIGKDD international conference on knowledge discovery and data mining,</booktitle>
<pages>217--226</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="22481" citStr="Joachims, 2006" startWordPosition="3534" endWordPosition="3535">lse. We extract three types of features for each instance &lt;CuePredicate, p&gt;: 1) token features (word lemma and POS tag) of CuePredicate and p, 2) syntactic contextfeatures (token features of parent predicates and all argument predicates) of CuePredicate and p, and 3) predicate pair features (is CuePredicate argument of p or vice versa?; distance between CuePredicate and p; relative position of CuePredicate and p). We use the ClearTk (Ogren et al., 2008) framework to build our system and perform experiments. We use quadratic kernel SVMs in all our experiments. The ClearTK wrapper for SVMLight (Joachims, 2006) internally shifts the prediction threshold using sigmoid fitting to deal with the highly skewed class imbalance (around 5% of positive cases) in our data. Prior studies (Prabhakaran et al., 2012b) have shown this approach to be effective in addressing the class imbalance problem. During prediction, given an unseen sentence PAS and a CuePredicate (either GOLD or automatically predicted) in it, we need to find the corresponding NegatedPredicate. We iterate over all candidate predicates c in the sentence PAS and apply our trained model to assign a true or false value to &lt;CuePredicate, c&gt;. For an</context>
</contexts>
<marker>Joachims, 2006</marker>
<rawString>Thorsten Joachims. 2006. Training Linear SVMs in Linear Time. In Proceedings of the 12th ACM SIGKDD international conference on knowledge discovery and data mining, pages 217–226. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Dong Kim</author>
<author>Tomoko Ohta</author>
<author>Sampo Pyysalo</author>
<author>Yoshinobu Kano</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Overview of bionlp’09 shared task on event extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing: Shared Task, BioNLP ’09,</booktitle>
<pages>1--9</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8492" citStr="Kim et al., 2009" startWordPosition="1321" endWordPosition="1324">ing or disease that follows a negation cue. While this works well for simpler expressions of negations, it tends to fail for more complex negation constructs. More recent approaches attempt to tackle the variability in scopes encountered in broader data by using statistical learning methods grounded in publicly available corpora with cue and scope annotations. The first such corpus was BioScope (Vincze et 72 al., 2008), which annotates negation cues and associated scopes in 3 genres—medical abstracts, scientific papers and clinical records. The BioNLP Event Extraction (EE) shared task corpus (Kim et al., 2009) also marks negation in the event annotations on sentences from molecular biology literature. Most recently, the *SEM 2012 shared task corpus (Morante and Blanco, 2012) marks negations, their foci, and scopes in sentences from Conan Doyle stories in an attempt to extend the research on negation to the general domain. Both the BioNLP-EE and *SEM corpora capture negations within—and therefore aligned with—syntactic analyses. Thus they deploy annotation schemes which assume downstream consumers of some granular negation representation, learnable from the annotated resource(s). However, the langua</context>
<context position="12831" citStr="Kim et al., 2009" startWordPosition="1995" endWordPosition="1998">iven inference, spans alone are not sufficient—especially spans which do not align with syntax. Negated expressions need to be captured within their syntactic context, and for this, we need the uniformity of syntax structures. The misalignment issues of BioScopeScopeSpan annotations with respect to the underlying syntactic structures have already been extensively studied (Vincze et al., 2011; Stenetorp et al., 2012). Vincze et al. (2011) point out infelicities and mismatches, comparing BioScope annotations with the more syntactically oriented negated event annotations in the BioNLP-EE corpus (Kim et al., 2009). Inconsis73 tencies are largely due to ‘loose’ annotation guidelines for BioScope, which are not rigorous enough in ensuring that annotation spans align with syntactic analyses. Given our position in this work—utilize BioScope, despite its shortcomings, in an alternative framework of analysis and training (see Section 2)—we explain some of the commonly occurring inconsistencies in this section. For this purpose, we use example annotations e1-e5 from BioScope. (Boldface denotes BioScopeCue annotations and italics denotes corresponding BioScopeScopeSpan annotations as present in the BioScope co</context>
</contexts>
<marker>Kim, Ohta, Pyysalo, Kano, Tsujii, 2009</marker>
<rawString>Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshinobu Kano, and Jun’ichi Tsujii. 2009. Overview of bionlp’09 shared task on event extraction. In Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing: Shared Task, BioNLP ’09, pages 1–9, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minsuk Lee</author>
<author>James Cimino</author>
<author>Hai Ran Zhu</author>
<author>Carl Sable</author>
<author>Vijay Shanker</author>
<author>John Ely</author>
<author>Hong Yu</author>
</authors>
<title>Beyond information retrievalmedical question answering.</title>
<date>2006</date>
<booktitle>In AMIA Annual Symposium Proceedings,</booktitle>
<volume>volume</volume>
<pages>469</pages>
<publisher>American Medical Informatics Association.</publisher>
<contexts>
<context position="2739" citStr="Lee et al., 2006" startWordPosition="405" endWordPosition="408">nt in systems processing medical/clinical text. Consider the segment “Mild hyperinflation without focal pneumonia”, taken from a patient’s clinical record. It indicates the absence of focal pneumonia in the patient. Not capturing this extra-propositional aspect of negation concerning focal pneumonia will lead to wrong—and harmful—inferences in downstream processing, e.g. by a clinical decision support system. The need for sophisticated negation detection capabilities in clinical text is even more urgent given the broadening spectrum of applications in this domain: clinical question answering (Lee et al., 2006), clinical decision support (DemnerFushman et al., 2009), medical information extraction (Uzuner et al., 2010), medical entity relation mining (Tymoshenko et al., 2012), patient history tracking (Raghavan et al., 2012), etc. Our motivation for detecting negations in medical texts also stems from practical concerns of an operational medical question answering (QA) system (Ferrucci et al., 2013). Most recent approaches to negation detection adopt supervised machine learning techniques to learn the phraseology of negation-containing expressions. They often follow a two step process— detection of </context>
</contexts>
<marker>Lee, Cimino, Zhu, Sable, Shanker, Ely, Yu, 2006</marker>
<rawString>Minsuk Lee, James Cimino, Hai Ran Zhu, Carl Sable, Vijay Shanker, John Ely, and Hong Yu. 2006. Beyond information retrievalmedical question answering. In AMIA Annual Symposium Proceedings, volume 2006, page 469. American Medical Informatics Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael C McCord</author>
<author>J William Murdock</author>
<author>Branimir Boguraev</author>
</authors>
<title>Deep parsing in Watson.</title>
<date>2012</date>
<journal>IBM Journal of Research and Development,</journal>
<volume>56</volume>
<issue>3</issue>
<contexts>
<context position="17939" citStr="McCord et al., 2012" startWordPosition="2790" endWordPosition="2794">ons include their affinity for having extra-propositional 74 Figure 1: PAS for e2: “Moreover, cAMP activators did not activate NF-kappa B in Jurkat cells” aspects of meaning ‘layered’ onto the representation (precedents in prior studies can be found in e.g. (Saur´ı and Pustejovsky, 2009; Diab et al., 2009)), and their pervasive use in a state-of-the-art QA system—for question analysis, candidate generation, and analysis of passage evidence (Ferrucci et al., 2010; Ferrucci, 2012)—which is at the heart of our medical adaptation (Ferrucci et al., 2013). We use predicate-argument structure (PAS) (McCord et al., 2012) derived from dependency parses produced by the English Slot Grammar parser (McCord, 1990). In addition to normalizing across different tree structures expressing essentially the same meaning, PAS provides a simplified view over ‘raw’ syntactic trees, gathering all arguments to a predicate from local, and distant, parse tree nodes (see (McCord et al., 2012) for details). Figures 1 and 2 show the PASes for examples e2 and e3. By localizing the logical arguments to a proposition, predicatebased representation provides direct access to all arguments of e.g. a verb frame: an important requirement </context>
</contexts>
<marker>McCord, Murdock, Boguraev, 2012</marker>
<rawString>Michael C. McCord, J. William Murdock, and Branimir Boguraev. 2012. Deep parsing in Watson. IBM Journal of Research and Development, 56(3):3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael C McCord</author>
</authors>
<title>Slot grammar: A system for simpler construction of practical natural language grammars. In</title>
<date>1990</date>
<booktitle>Natural Language and Logic: Proc. of the International Scientific Symposium,</booktitle>
<pages>118--145</pages>
<editor>R. Studer, editor,</editor>
<publisher>Springer,</publisher>
<location>Hamburg, FRG,</location>
<contexts>
<context position="18029" citStr="McCord, 1990" startWordPosition="2806" endWordPosition="2808">P activators did not activate NF-kappa B in Jurkat cells” aspects of meaning ‘layered’ onto the representation (precedents in prior studies can be found in e.g. (Saur´ı and Pustejovsky, 2009; Diab et al., 2009)), and their pervasive use in a state-of-the-art QA system—for question analysis, candidate generation, and analysis of passage evidence (Ferrucci et al., 2010; Ferrucci, 2012)—which is at the heart of our medical adaptation (Ferrucci et al., 2013). We use predicate-argument structure (PAS) (McCord et al., 2012) derived from dependency parses produced by the English Slot Grammar parser (McCord, 1990). In addition to normalizing across different tree structures expressing essentially the same meaning, PAS provides a simplified view over ‘raw’ syntactic trees, gathering all arguments to a predicate from local, and distant, parse tree nodes (see (McCord et al., 2012) for details). Figures 1 and 2 show the PASes for examples e2 and e3. By localizing the logical arguments to a proposition, predicatebased representation provides direct access to all arguments of e.g. a verb frame: an important requirement for extracting context-denoting syntactic features. PAS-based view into sentences offers u</context>
</contexts>
<marker>McCord, 1990</marker>
<rawString>Michael C. McCord. 1990. Slot grammar: A system for simpler construction of practical natural language grammars. In R. Studer, editor, Natural Language and Logic: Proc. of the International Scientific Symposium, Hamburg, FRG, pages 118–145. Springer, Berlin, Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>St´ephane M Meystre</author>
<author>Guergana K Savova</author>
<author>Karin C Kipper-Schuler</author>
<author>John F Hurdle</author>
</authors>
<title>Extracting information from textual documents in the electronic health record: a review of recent research. Yearbook of medical informatics,</title>
<date>2008</date>
<pages>128--144</pages>
<contexts>
<context position="1948" citStr="Meystre et al., 2008" startWordPosition="292" endWordPosition="295">text is vital. 1 Introduction Accounting for extra-propositional aspects of meaning in text is a very active NLP research area in recent years, exploring different aspects of meaning such as factivity (Saur´ı and Pustejovsky, 2009), uncertainty/hedging (Farkas et al., 2010), committed belief (Prabhakaran et al., 2010), and modalities (Prabhakaran et al., 2012a). Among these, negation detection has generated special interest because of demonstrated needs for negation detection capabil71 ity in practical applications such as information retrieval (Averbuch et al., 2004), information extraction (Meystre et al., 2008), sentiment analysis (Wiegand et al., 2010; Councill et al., 2010), and relation detection (Chowdhury and Lavelli, 2013). Accurately detecting negations is especially important in systems processing medical/clinical text. Consider the segment “Mild hyperinflation without focal pneumonia”, taken from a patient’s clinical record. It indicates the absence of focal pneumonia in the patient. Not capturing this extra-propositional aspect of negation concerning focal pneumonia will lead to wrong—and harmful—inferences in downstream processing, e.g. by a clinical decision support system. The need for </context>
</contexts>
<marker>Meystre, Savova, Kipper-Schuler, Hurdle, 2008</marker>
<rawString>St´ephane M Meystre, Guergana K Savova, Karin C Kipper-Schuler, and John F Hurdle. 2008. Extracting information from textual documents in the electronic health record: a review of recent research. Yearbook of medical informatics, pages 128–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Morante</author>
<author>Eduardo Blanco</author>
</authors>
<title>SEM 2012 shared task: Resolving the scope and focus of negation.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation,</booktitle>
<pages>265--274</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8660" citStr="Morante and Blanco, 2012" startWordPosition="1347" endWordPosition="1350">ore recent approaches attempt to tackle the variability in scopes encountered in broader data by using statistical learning methods grounded in publicly available corpora with cue and scope annotations. The first such corpus was BioScope (Vincze et 72 al., 2008), which annotates negation cues and associated scopes in 3 genres—medical abstracts, scientific papers and clinical records. The BioNLP Event Extraction (EE) shared task corpus (Kim et al., 2009) also marks negation in the event annotations on sentences from molecular biology literature. Most recently, the *SEM 2012 shared task corpus (Morante and Blanco, 2012) marks negations, their foci, and scopes in sentences from Conan Doyle stories in an attempt to extend the research on negation to the general domain. Both the BioNLP-EE and *SEM corpora capture negations within—and therefore aligned with—syntactic analyses. Thus they deploy annotation schemes which assume downstream consumers of some granular negation representation, learnable from the annotated resource(s). However, the language in both of them differs greatly from the language encountered in clinical text, making them unsuitable for our QA system requirements. In contrast, BioScope matches </context>
</contexts>
<marker>Morante, Blanco, 2012</marker>
<rawString>Roser Morante and Eduardo Blanco. 2012. * SEM 2012 shared task: Resolving the scope and focus of negation. In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, pages 265–274. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Morante</author>
<author>Walter Daelemans</author>
</authors>
<title>A metalearning approach to processing the scope of negation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>21--29</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="30510" citStr="Morante and Daelemans, 2009" startWordPosition="4838" endWordPosition="4841">dies. It is important to note that the main source of error here is the NegatedPredicate-to-BioScopeScopeSpan transMorante09 Ballestros12Velldal12 Ours (With Covered Text) Ours (With Transform) Zou13 (1) (2) (3) On Dev On Test On Dev On Test (8) (4) (5) (6) (7) Clinical 70.75 89.06 89.41 88.49 89.92 91.37 92.25 85.31 Abstracts 66.07 68.92 72.89 35.45 35.27 61.94 58.53 76.90 Papers 41.00 61.43 68.09 33.33 23.73 53.97 47.46 61.19 Overall - - - 50.85 49.55 69.57 66.82 - Table 3: PCS measures from previous BioScope span detection approaches and our end-to-end system. Col. 1-3: end-to-end systems (Morante and Daelemans, 2009), (Ballesteros et al., 2012), and (Velldal et al., 2012); Col. 4-7: our end-to-end system with different ways of obtaining the spans in our Dev and Test sets; Col. 8: (Zou et al., 2013) system using GOLD cues (often 5-10% higher than using predicted cues) 78 form step, with its inherent lower accuracies for these two corpora, as reported above. We emphasize that for practical applications this transformation is of little use: what matters more, certainly for a negation detection system feeding downstream components, are the PCSP values presented in Section 6. 8 Discussion and Conclusion The re</context>
</contexts>
<marker>Morante, Daelemans, 2009</marker>
<rawString>Roser Morante and Walter Daelemans. 2009. A metalearning approach to processing the scope of negation. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning, pages 21–29. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip V Ogren</author>
<author>Philipp G Wetzler</author>
<author>Steven Bethard</author>
</authors>
<title>ClearTK: A UIMA toolkit for statistical natural language processing.</title>
<date>2008</date>
<booktitle>In Towards Enhanced Interoperability for Large HLT Systems: UIMA for NLP workshop at Language Resources and Evaluation Conference (LREC).</booktitle>
<contexts>
<context position="22323" citStr="Ogren et al., 2008" startWordPosition="3505" endWordPosition="3508">cate, p&gt;. The instance &lt;CuePredicate, p&gt; is assigned true if p is the corresponding NegatedPredicate. For all other p in the PAS, &lt;CuePredicate, p&gt; is assigned false. We extract three types of features for each instance &lt;CuePredicate, p&gt;: 1) token features (word lemma and POS tag) of CuePredicate and p, 2) syntactic contextfeatures (token features of parent predicates and all argument predicates) of CuePredicate and p, and 3) predicate pair features (is CuePredicate argument of p or vice versa?; distance between CuePredicate and p; relative position of CuePredicate and p). We use the ClearTk (Ogren et al., 2008) framework to build our system and perform experiments. We use quadratic kernel SVMs in all our experiments. The ClearTK wrapper for SVMLight (Joachims, 2006) internally shifts the prediction threshold using sigmoid fitting to deal with the highly skewed class imbalance (around 5% of positive cases) in our data. Prior studies (Prabhakaran et al., 2012b) have shown this approach to be effective in addressing the class imbalance problem. During prediction, given an unseen sentence PAS and a CuePredicate (either GOLD or automatically predicted) in it, we need to find the corresponding NegatedPred</context>
</contexts>
<marker>Ogren, Wetzler, Bethard, 2008</marker>
<rawString>Philip V. Ogren, Philipp G. Wetzler, and Steven Bethard. 2008. ClearTK: A UIMA toolkit for statistical natural language processing. In Towards Enhanced Interoperability for Large HLT Systems: UIMA for NLP workshop at Language Resources and Evaluation Conference (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vinodkumar Prabhakaran</author>
<author>Owen Rambow</author>
<author>Mona Diab</author>
</authors>
<title>Automatic committed belief tagging.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In COLING 2010: Posters,</booktitle>
<pages>1014--1022</pages>
<location>Beijing, China, August. COLING</location>
<contexts>
<context position="1646" citStr="Prabhakaran et al., 2010" startWordPosition="249" endWordPosition="252"> as a simpler one, and to leverage an imperfect resource in a way which enables us to learn a high performance model. We achieve high accuracy for negation detection overall, 87%. Our re-mapping scheme can be constructively applied to existing flatly annotated resources for other tasks where syntactic context is vital. 1 Introduction Accounting for extra-propositional aspects of meaning in text is a very active NLP research area in recent years, exploring different aspects of meaning such as factivity (Saur´ı and Pustejovsky, 2009), uncertainty/hedging (Farkas et al., 2010), committed belief (Prabhakaran et al., 2010), and modalities (Prabhakaran et al., 2012a). Among these, negation detection has generated special interest because of demonstrated needs for negation detection capabil71 ity in practical applications such as information retrieval (Averbuch et al., 2004), information extraction (Meystre et al., 2008), sentiment analysis (Wiegand et al., 2010; Councill et al., 2010), and relation detection (Chowdhury and Lavelli, 2013). Accurately detecting negations is especially important in systems processing medical/clinical text. Consider the segment “Mild hyperinflation without focal pneumonia”, taken fr</context>
</contexts>
<marker>Prabhakaran, Rambow, Diab, 2010</marker>
<rawString>Vinodkumar Prabhakaran, Owen Rambow, and Mona Diab. 2010. Automatic committed belief tagging. In COLING 2010: Posters, pages 1014–1022, Beijing, China, August. COLING 2010 Organizing Committee.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Vinodkumar Prabhakaran</author>
<author>Michael Bloodgood</author>
<author>Mona Diab</author>
<author>Bonnie Dorr</author>
<author>Lori Levin</author>
<author>Christine D Piatko</author>
<author>Owen Rambow</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Statistical Modality Tagging from Rule-based Annotations and Crowdsourcing.</title>
<date>2012</date>
<booktitle>In Proceedings of the Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics,</booktitle>
<pages>57--64</pages>
<institution>of Korea, July. Association for Computational Linguistics.</institution>
<location>Jeju, Republic</location>
<marker>Prabhakaran, Bloodgood, Diab, Dorr, Levin, Piatko, Rambow, Van Durme, 2012</marker>
<rawString>Vinodkumar Prabhakaran, Michael Bloodgood, Mona Diab, Bonnie Dorr, Lori Levin, Christine D. Piatko, Owen Rambow, and Benjamin Van Durme. 2012a. Statistical Modality Tagging from Rule-based Annotations and Crowdsourcing. In Proceedings of the Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics, pages 57–64, Jeju, Republic of Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vinodkumar Prabhakaran</author>
<author>Owen Rambow</author>
<author>Mona Diab</author>
</authors>
<title>Predicting Overt Display of Power in Written Dialogs.</title>
<date>2012</date>
<booktitle>In Human Language Technologies: The 2012 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montreal, Canada,</location>
<contexts>
<context position="1688" citStr="Prabhakaran et al., 2012" startWordPosition="255" endWordPosition="258">fect resource in a way which enables us to learn a high performance model. We achieve high accuracy for negation detection overall, 87%. Our re-mapping scheme can be constructively applied to existing flatly annotated resources for other tasks where syntactic context is vital. 1 Introduction Accounting for extra-propositional aspects of meaning in text is a very active NLP research area in recent years, exploring different aspects of meaning such as factivity (Saur´ı and Pustejovsky, 2009), uncertainty/hedging (Farkas et al., 2010), committed belief (Prabhakaran et al., 2010), and modalities (Prabhakaran et al., 2012a). Among these, negation detection has generated special interest because of demonstrated needs for negation detection capabil71 ity in practical applications such as information retrieval (Averbuch et al., 2004), information extraction (Meystre et al., 2008), sentiment analysis (Wiegand et al., 2010; Councill et al., 2010), and relation detection (Chowdhury and Lavelli, 2013). Accurately detecting negations is especially important in systems processing medical/clinical text. Consider the segment “Mild hyperinflation without focal pneumonia”, taken from a patient’s clinical record. It indicat</context>
<context position="22676" citStr="Prabhakaran et al., 2012" startWordPosition="3564" endWordPosition="3567">s of parent predicates and all argument predicates) of CuePredicate and p, and 3) predicate pair features (is CuePredicate argument of p or vice versa?; distance between CuePredicate and p; relative position of CuePredicate and p). We use the ClearTk (Ogren et al., 2008) framework to build our system and perform experiments. We use quadratic kernel SVMs in all our experiments. The ClearTK wrapper for SVMLight (Joachims, 2006) internally shifts the prediction threshold using sigmoid fitting to deal with the highly skewed class imbalance (around 5% of positive cases) in our data. Prior studies (Prabhakaran et al., 2012b) have shown this approach to be effective in addressing the class imbalance problem. During prediction, given an unseen sentence PAS and a CuePredicate (either GOLD or automatically predicted) in it, we need to find the corresponding NegatedPredicate. We iterate over all candidate predicates c in the sentence PAS and apply our trained model to assign a true or false value to &lt;CuePredicate, c&gt;. For any CuePredicate in a sentence there must be one and only one NegatedPredicate, since BioScope corpus marks a single BioScopeScopeSpan for every BioScopeCue. We choose the c for which &lt;CuePredicate</context>
</contexts>
<marker>Prabhakaran, Rambow, Diab, 2012</marker>
<rawString>Vinodkumar Prabhakaran, Owen Rambow, and Mona Diab. 2012b. Predicting Overt Display of Power in Written Dialogs. In Human Language Technologies: The 2012 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Montreal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Seattle</author>
</authors>
<date></date>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Washington, USA,</location>
<marker>Seattle, </marker>
<rawString>976, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preethi Raghavan</author>
<author>Albert Lai</author>
<author>Eric Fosler-Lussier</author>
</authors>
<title>Learning to temporally order medical events in clinical text.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>70--74</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="2957" citStr="Raghavan et al., 2012" startWordPosition="436" endWordPosition="439">t. Not capturing this extra-propositional aspect of negation concerning focal pneumonia will lead to wrong—and harmful—inferences in downstream processing, e.g. by a clinical decision support system. The need for sophisticated negation detection capabilities in clinical text is even more urgent given the broadening spectrum of applications in this domain: clinical question answering (Lee et al., 2006), clinical decision support (DemnerFushman et al., 2009), medical information extraction (Uzuner et al., 2010), medical entity relation mining (Tymoshenko et al., 2012), patient history tracking (Raghavan et al., 2012), etc. Our motivation for detecting negations in medical texts also stems from practical concerns of an operational medical question answering (QA) system (Ferrucci et al., 2013). Most recent approaches to negation detection adopt supervised machine learning techniques to learn the phraseology of negation-containing expressions. They often follow a two step process— detection of negation cues (“no”, “without”, ...), followed by detection of their associated scopes. Cue detection is a relatively simple task, since the set of cue words is not large. Determining the scope of Proceedings of the Fo</context>
</contexts>
<marker>Raghavan, Lai, Fosler-Lussier, 2012</marker>
<rawString>Preethi Raghavan, Albert Lai, and Eric Fosler-Lussier. 2012. Learning to temporally order medical events in clinical text. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 70–74, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Saur´ı</author>
<author>James Pustejovsky</author>
</authors>
<title>Factbank: a corpus annotated with event factuality. Language Resources and Evaluation,</title>
<date>2009</date>
<volume>43</volume>
<pages>10--1007</pages>
<marker>Saur´ı, Pustejovsky, 2009</marker>
<rawString>Roser Saur´ı and James Pustejovsky. 2009. Factbank: a corpus annotated with event factuality. Language Resources and Evaluation, 43:227–268. 10.1007/s10579-009-9089-9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pontus Stenetorp</author>
<author>Sampo Pyysalo</author>
<author>Tomoko Ohta</author>
<author>Sophia Ananiadou</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Bridging the gap between scope-based and event-based negation/speculation annotations: a bridge not too far.</title>
<date>2012</date>
<booktitle>In Proceedings of the Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics,</booktitle>
<pages>47--56</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4561" citStr="Stenetorp et al., 2012" startWordPosition="688" endWordPosition="691">under the scope of a negation cue. Scope detection is crucial for interpreting negations, and to that end, the BioScope corpus (Vincze et al., 2008) was released, with annotations of both negation cues and their associated scopes. The fact that these scopes are represented only as text-spans is a drawback of BioScope. Without being anchored to a syntactic analysis of the sentences in which they occur, BioScope’s scope annotations suffer from a variety of inconsistencies of mark-up. They also may, and occasionally do, fail to align with the underlying syntactic structures (Vincze et al., 2011; Stenetorp et al., 2012). Such inconsistencies make it hard for a system to learn the actual syntactic patterns connecting negation cues and their scopes—which are, after all, the real object of negation interpretation. The insight that we develop in this paper is that a scope span can be associated with one or more nodes in the syntactic analysis of a negated expression, and that these will be further connected—in a systematic way—to the negation cue node. Mapping loosely and/or inconsistently bounded spans to unique syntactic nodes (and configurations thereof) reduces the noise inherent in BioScope. The learning ta</context>
<context position="12633" citStr="Stenetorp et al., 2012" startWordPosition="1966" endWordPosition="1969">oScope annotations are optimized to match the annotated spans in the corpus (as discussed in Section 2). However, for a negation detection system followed by downstream components implementing negation-driven inference, spans alone are not sufficient—especially spans which do not align with syntax. Negated expressions need to be captured within their syntactic context, and for this, we need the uniformity of syntax structures. The misalignment issues of BioScopeScopeSpan annotations with respect to the underlying syntactic structures have already been extensively studied (Vincze et al., 2011; Stenetorp et al., 2012). Vincze et al. (2011) point out infelicities and mismatches, comparing BioScope annotations with the more syntactically oriented negated event annotations in the BioNLP-EE corpus (Kim et al., 2009). Inconsis73 tencies are largely due to ‘loose’ annotation guidelines for BioScope, which are not rigorous enough in ensuring that annotation spans align with syntactic analyses. Given our position in this work—utilize BioScope, despite its shortcomings, in an alternative framework of analysis and training (see Section 2)—we explain some of the commonly occurring inconsistencies in this section. For</context>
</contexts>
<marker>Stenetorp, Pyysalo, Ohta, Ananiadou, Tsujii, 2012</marker>
<rawString>Pontus Stenetorp, Sampo Pyysalo, Tomoko Ohta, Sophia Ananiadou, and Jun’ichi Tsujii. 2012. Bridging the gap between scope-based and event-based negation/speculation annotations: a bridge not too far. In Proceedings of the Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics, pages 47–56. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kateryna Tymoshenko</author>
<author>Swapna Somasundaran</author>
<author>Vinodkumar Prabhakaran</author>
<author>Vinay Shet</author>
</authors>
<title>Relation mining in the biomedical domain using entity-level semantics.</title>
<date>2012</date>
<booktitle>In ECAI,</booktitle>
<pages>780--785</pages>
<contexts>
<context position="2907" citStr="Tymoshenko et al., 2012" startWordPosition="429" endWordPosition="432">dicates the absence of focal pneumonia in the patient. Not capturing this extra-propositional aspect of negation concerning focal pneumonia will lead to wrong—and harmful—inferences in downstream processing, e.g. by a clinical decision support system. The need for sophisticated negation detection capabilities in clinical text is even more urgent given the broadening spectrum of applications in this domain: clinical question answering (Lee et al., 2006), clinical decision support (DemnerFushman et al., 2009), medical information extraction (Uzuner et al., 2010), medical entity relation mining (Tymoshenko et al., 2012), patient history tracking (Raghavan et al., 2012), etc. Our motivation for detecting negations in medical texts also stems from practical concerns of an operational medical question answering (QA) system (Ferrucci et al., 2013). Most recent approaches to negation detection adopt supervised machine learning techniques to learn the phraseology of negation-containing expressions. They often follow a two step process— detection of negation cues (“no”, “without”, ...), followed by detection of their associated scopes. Cue detection is a relatively simple task, since the set of cue words is not lar</context>
</contexts>
<marker>Tymoshenko, Somasundaran, Prabhakaran, Shet, 2012</marker>
<rawString>Kateryna Tymoshenko, Swapna Somasundaran, Vinodkumar Prabhakaran, and Vinay Shet. 2012. Relation mining in the biomedical domain using entity-level semantics. In ECAI, pages 780–785.</rawString>
</citation>
<citation valid="true">
<authors>
<author>¨Ozlem Uzuner</author>
</authors>
<title>Imre Solti, and Eithon Cadag.</title>
<date>2010</date>
<journal>Journal of the American Medical Informatics Association,</journal>
<volume>17</volume>
<issue>5</issue>
<marker>Uzuner, 2010</marker>
<rawString>¨Ozlem Uzuner, Imre Solti, and Eithon Cadag. 2010. Extracting medication information from clinical text. Journal of the American Medical Informatics Association, 17(5):514–518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Velldal</author>
<author>Lilja Øvrelid</author>
<author>Jonathon Read</author>
<author>Stephan Oepen</author>
</authors>
<title>Speculation and negation: Rules, rankers, and the role of syntax.</title>
<date>2012</date>
<journal>Computational linguistics,</journal>
<pages>38--2</pages>
<contexts>
<context position="9769" citStr="Velldal et al., 2012" startWordPosition="1515" endWordPosition="1518">untered in clinical text, making them unsuitable for our QA system requirements. In contrast, BioScope matches our genre of clinical text. As an additional plus, it captures negation in a task-independent, linguistically motivated framework, which enables the building of systems applicable to a wider range of domains. BioScope’s negation-scope-as-span annotation framework, however, limits th corpus utility. Various approaches have used it to train negation scope span detection systems, and many have shown the importance of deep syntactic features in that task (e.g., (Ballesteros et al., 2012; Velldal et al., 2012; Zou et al., 2013)). They share a drawback: they are optimized for predicting the spans as they are annotated in BioScope—despite its various syntactic inconsistencies. For example, Ballesteros et al. (2012) use manual rules to detect the voice (passive or active) of a verb phrase; this is motivated by an annotation guideline for whether to include verb subjects in the span or not. In reality, what matters in the end is whether a detection system can capture the underlying phenomenon of negation that the annotations stand to represent, and not whether it can accurately replicate the represent</context>
<context position="25375" citStr="Velldal et al., 2012" startWordPosition="4000" endWordPosition="4003">, we built a CuePredicate detector using linear kernel SVM to detect whether a predicate is a negation cue or not. We use three types of features: 1) token features (lemma and POS) of the predicate, 2) linear context (token features of the token after the predicate in the sentence; features of tokens before the predicate turned out to be not useful), and 3) syntactic context (token features of parent and argument predicates). As shown in Table 1, our CuePredicate tagger obtained F-measures in the range of state-of-the-art results on negation cue detection using the BioScope (90-96% F-measure (Velldal et al., 2012)). 6.2 Baseline NegatedPredicate Predictor Since this formulation of the task is new, we built a strong baseline system appropriate for it. In our baseline, we predict the NegatedPredicate to be the parent predicate of the CuePredicate, if the CuePredicate is a terminal node in the PAS (this will cover the most common cues such as no and not). If the CuePredicate is not a terminal node (which covers the cases of verbal negation cues such as failed), we choose the CuePredicate itself as the NegatedPredicate. Columns 1 and 3 of Table 2 show PCSP obtained by the baseline algorithm on our Dev and </context>
<context position="30566" citStr="Velldal et al., 2012" startWordPosition="4847" endWordPosition="4850">e is the NegatedPredicate-to-BioScopeScopeSpan transMorante09 Ballestros12Velldal12 Ours (With Covered Text) Ours (With Transform) Zou13 (1) (2) (3) On Dev On Test On Dev On Test (8) (4) (5) (6) (7) Clinical 70.75 89.06 89.41 88.49 89.92 91.37 92.25 85.31 Abstracts 66.07 68.92 72.89 35.45 35.27 61.94 58.53 76.90 Papers 41.00 61.43 68.09 33.33 23.73 53.97 47.46 61.19 Overall - - - 50.85 49.55 69.57 66.82 - Table 3: PCS measures from previous BioScope span detection approaches and our end-to-end system. Col. 1-3: end-to-end systems (Morante and Daelemans, 2009), (Ballesteros et al., 2012), and (Velldal et al., 2012); Col. 4-7: our end-to-end system with different ways of obtaining the spans in our Dev and Test sets; Col. 8: (Zou et al., 2013) system using GOLD cues (often 5-10% higher than using predicted cues) 78 form step, with its inherent lower accuracies for these two corpora, as reported above. We emphasize that for practical applications this transformation is of little use: what matters more, certainly for a negation detection system feeding downstream components, are the PCSP values presented in Section 6. 8 Discussion and Conclusion The results for our system, with reverse mapping, offer indire</context>
</contexts>
<marker>Velldal, Øvrelid, Read, Oepen, 2012</marker>
<rawString>Erik Velldal, Lilja Øvrelid, Jonathon Read, and Stephan Oepen. 2012. Speculation and negation: Rules, rankers, and the role of syntax. Computational linguistics, 38(2):369–410.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veronika Vincze</author>
<author>Gy¨orgy Szarvas</author>
<author>Rich´ard Farkas</author>
<author>Gy¨orgy M´ora</author>
<author>J´anos Csirik</author>
</authors>
<title>The BioScope corpus: biomedical texts annotated for uncertainty, negation and their scopes. BMC bioinformatics, 9(Suppl 11):S9.</title>
<date>2008</date>
<marker>Vincze, Szarvas, Farkas, M´ora, Csirik, 2008</marker>
<rawString>Veronika Vincze, Gy¨orgy Szarvas, Rich´ard Farkas, Gy¨orgy M´ora, and J´anos Csirik. 2008. The BioScope corpus: biomedical texts annotated for uncertainty, negation and their scopes. BMC bioinformatics, 9(Suppl 11):S9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veronika Vincze</author>
<author>Gyorgy Szarvas</author>
<author>Gyorgy Mora</author>
<author>Tomoko Ohta</author>
<author>Rich´ard Farkas</author>
</authors>
<title>Linguistic scope-based and biological event-based speculation and negation annotations in the BioScope and Genia Event corpora.</title>
<date>2011</date>
<journal>Journal of Biomedical Semantics,</journal>
<volume>2</volume>
<pages>5--8</pages>
<contexts>
<context position="4536" citStr="Vincze et al., 2011" startWordPosition="684" endWordPosition="687">e sentence that come under the scope of a negation cue. Scope detection is crucial for interpreting negations, and to that end, the BioScope corpus (Vincze et al., 2008) was released, with annotations of both negation cues and their associated scopes. The fact that these scopes are represented only as text-spans is a drawback of BioScope. Without being anchored to a syntactic analysis of the sentences in which they occur, BioScope’s scope annotations suffer from a variety of inconsistencies of mark-up. They also may, and occasionally do, fail to align with the underlying syntactic structures (Vincze et al., 2011; Stenetorp et al., 2012). Such inconsistencies make it hard for a system to learn the actual syntactic patterns connecting negation cues and their scopes—which are, after all, the real object of negation interpretation. The insight that we develop in this paper is that a scope span can be associated with one or more nodes in the syntactic analysis of a negated expression, and that these will be further connected—in a systematic way—to the negation cue node. Mapping loosely and/or inconsistently bounded spans to unique syntactic nodes (and configurations thereof) reduces the noise inherent in </context>
<context position="12608" citStr="Vincze et al., 2011" startWordPosition="1962" endWordPosition="1965">stems trained over BioScope annotations are optimized to match the annotated spans in the corpus (as discussed in Section 2). However, for a negation detection system followed by downstream components implementing negation-driven inference, spans alone are not sufficient—especially spans which do not align with syntax. Negated expressions need to be captured within their syntactic context, and for this, we need the uniformity of syntax structures. The misalignment issues of BioScopeScopeSpan annotations with respect to the underlying syntactic structures have already been extensively studied (Vincze et al., 2011; Stenetorp et al., 2012). Vincze et al. (2011) point out infelicities and mismatches, comparing BioScope annotations with the more syntactically oriented negated event annotations in the BioNLP-EE corpus (Kim et al., 2009). Inconsis73 tencies are largely due to ‘loose’ annotation guidelines for BioScope, which are not rigorous enough in ensuring that annotation spans align with syntactic analyses. Given our position in this work—utilize BioScope, despite its shortcomings, in an alternative framework of analysis and training (see Section 2)—we explain some of the commonly occurring inconsisten</context>
<context position="13839" citStr="Vincze et al. (2011)" startWordPosition="2155" endWordPosition="2158">s section. For this purpose, we use example annotations e1-e5 from BioScope. (Boldface denotes BioScopeCue annotations and italics denotes corresponding BioScopeScopeSpan annotations as present in the BioScope corpus.) One of the main source of inconsistencies within the syntactic space is with regard to the inclusion or exclusion of subjects of propositions. For example, in e1, the annotations identify the negation span to be the entire clause following the word but, including its subject and object. However, in e2, only the object of the predicate is marked as the negation scope (Figure 1). Vincze et al. (2011) state that “the treatment of subjects [in BioScope] remains problematic since in BioScope it is only the complements that are usually included within the scope of a keyword (that is, subjects are not with the exception of passive constructions and raising verbs)”. Leaving aside the rationale for such a guideline, we note that such an inconsistency is harmful: proper interpretation of negated propositions does require a subject, and making annotations consistent by ignoring subjects, if present, does not help downstream components. Additionally, it makes the learning of contexts of negated pro</context>
</contexts>
<marker>Vincze, Szarvas, Mora, Ohta, Farkas, 2011</marker>
<rawString>Veronika Vincze, Gyorgy Szarvas, Gyorgy Mora, Tomoko Ohta, Rich´ard Farkas, et al. 2011. Linguistic scope-based and biological event-based speculation and negation annotations in the BioScope and Genia Event corpora. Journal of Biomedical Semantics, 2(Suppl 5):S8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Wiegand</author>
<author>Alexandra Balahur</author>
<author>Benjamin Roth</author>
<author>Dietrich Klakow</author>
<author>Andr´es Montoyo</author>
</authors>
<title>A survey on the role of negation in sentiment analysis.</title>
<date>2010</date>
<booktitle>In Proceedings of the workshop on negation and speculation in natural language processing,</booktitle>
<pages>60--68</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1990" citStr="Wiegand et al., 2010" startWordPosition="298" endWordPosition="302">r extra-propositional aspects of meaning in text is a very active NLP research area in recent years, exploring different aspects of meaning such as factivity (Saur´ı and Pustejovsky, 2009), uncertainty/hedging (Farkas et al., 2010), committed belief (Prabhakaran et al., 2010), and modalities (Prabhakaran et al., 2012a). Among these, negation detection has generated special interest because of demonstrated needs for negation detection capabil71 ity in practical applications such as information retrieval (Averbuch et al., 2004), information extraction (Meystre et al., 2008), sentiment analysis (Wiegand et al., 2010; Councill et al., 2010), and relation detection (Chowdhury and Lavelli, 2013). Accurately detecting negations is especially important in systems processing medical/clinical text. Consider the segment “Mild hyperinflation without focal pneumonia”, taken from a patient’s clinical record. It indicates the absence of focal pneumonia in the patient. Not capturing this extra-propositional aspect of negation concerning focal pneumonia will lead to wrong—and harmful—inferences in downstream processing, e.g. by a clinical decision support system. The need for sophisticated negation detection capabilit</context>
</contexts>
<marker>Wiegand, Balahur, Roth, Klakow, Montoyo, 2010</marker>
<rawString>Michael Wiegand, Alexandra Balahur, Benjamin Roth, Dietrich Klakow, and Andr´es Montoyo. 2010. A survey on the role of negation in sentiment analysis. In Proceedings of the workshop on negation and speculation in natural language processing, pages 60–68. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bowei Zou</author>
<author>Guodong Zhou</author>
<author>Qiaoming Zhu</author>
</authors>
<title>Tree kernel-based negation and speculation scope detection with structured syntactic parse features.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>968</pages>
<contexts>
<context position="9788" citStr="Zou et al., 2013" startWordPosition="1519" endWordPosition="1522">xt, making them unsuitable for our QA system requirements. In contrast, BioScope matches our genre of clinical text. As an additional plus, it captures negation in a task-independent, linguistically motivated framework, which enables the building of systems applicable to a wider range of domains. BioScope’s negation-scope-as-span annotation framework, however, limits th corpus utility. Various approaches have used it to train negation scope span detection systems, and many have shown the importance of deep syntactic features in that task (e.g., (Ballesteros et al., 2012; Velldal et al., 2012; Zou et al., 2013)). They share a drawback: they are optimized for predicting the spans as they are annotated in BioScope—despite its various syntactic inconsistencies. For example, Ballesteros et al. (2012) use manual rules to detect the voice (passive or active) of a verb phrase; this is motivated by an annotation guideline for whether to include verb subjects in the span or not. In reality, what matters in the end is whether a detection system can capture the underlying phenomenon of negation that the annotations stand to represent, and not whether it can accurately replicate the representational choices the</context>
<context position="30695" citStr="Zou et al., 2013" startWordPosition="4871" endWordPosition="4874">u13 (1) (2) (3) On Dev On Test On Dev On Test (8) (4) (5) (6) (7) Clinical 70.75 89.06 89.41 88.49 89.92 91.37 92.25 85.31 Abstracts 66.07 68.92 72.89 35.45 35.27 61.94 58.53 76.90 Papers 41.00 61.43 68.09 33.33 23.73 53.97 47.46 61.19 Overall - - - 50.85 49.55 69.57 66.82 - Table 3: PCS measures from previous BioScope span detection approaches and our end-to-end system. Col. 1-3: end-to-end systems (Morante and Daelemans, 2009), (Ballesteros et al., 2012), and (Velldal et al., 2012); Col. 4-7: our end-to-end system with different ways of obtaining the spans in our Dev and Test sets; Col. 8: (Zou et al., 2013) system using GOLD cues (often 5-10% higher than using predicted cues) 78 form step, with its inherent lower accuracies for these two corpora, as reported above. We emphasize that for practical applications this transformation is of little use: what matters more, certainly for a negation detection system feeding downstream components, are the PCSP values presented in Section 6. 8 Discussion and Conclusion The results for our system, with reverse mapping, offer indirect evidence for our observation in Section 3.1: training a system to predict BioScopeScopeSpan boundaries would require it also t</context>
</contexts>
<marker>Zou, Zhou, Zhu, 2013</marker>
<rawString>Bowei Zou, Guodong Zhou, and Qiaoming Zhu. 2013. Tree kernel-based negation and speculation scope detection with structured syntactic parse features. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 968–</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>