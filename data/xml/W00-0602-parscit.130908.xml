<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007286">
<title confidence="0.9926785">
Some Challenges of Developing Fully-Automated Systems for
Taking Audio Comprehension Exams
</title>
<note confidence="0.55534">
David D. Palmer
The MITRE Corporation
</note>
<address confidence="0.832294">
202 Burlington Road
Bedford, MA 01730
</address>
<email confidence="0.996139">
palmer@mitre.org
</email>
<sectionHeader confidence="0.987469" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997163">
Audio comprehension tests are designed to help eval-
uate a listener&apos;s understanding of a spoken passage
and are frequently a key component of language
competency exams. Just as reading comprehension
exams are proving useful in evaluating text-based
language processing technology, audio comprehen-
sion exams can be used to evaluate spoken language
processing systems. In this paper we discuss some of
the challenges of developing automated systems for
taking audio comprehension exams.
</bodyText>
<sectionHeader confidence="0.99379" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999417641509435">
There is currently interest in using reading compre-
hension exams to evaluate natural language process-
ing (NLP) systems. Reading comprehension tests
are designed to help evaluate a reader&apos;s understand-
ing of a written passage and are thus an example of
a text-based language processing task. Audio com-
prehension tests, on the other hand, are designed to
help evaluate a listener&apos;s understanding of a spoken
passage and are an example of a spoken language
processing task. These tests are frequently a key
component of language competency exams, such as
the Test of English as a Foreign Language (TOEFL)
in the United States.
In this paper, we focus on some of the future
challenges of developing fully-automated techniques
for audio comprehension, in which the system de-
veloped processes the exam passages (and possibly
questions) from the original audio source. Audio
comprehension provides an excellent example of an
understanding-based evaluation paradigm for speech
systems, in which the emphasis is not solely on
&amp;quot;getting all the words right&amp;quot; but rather on using
speech recognition technology to automatically ac-
complish a task with a human benchmark: answer-
ing questions about a natural language story. The
traditional paradigm for spoken language processing
tasks, such as audio comprehension, has consisted
largely of applying an existing text-based system to
the hypothesis words output by an automatic speech
recognition (ASR) system, ignoring the fact that in-
formation is lost due to recognition errors when mov-
ing from text to speech and the possibility that it can
be regained in part via word confidence prediction.
We believe that successful approaches to audio
comprehension will tackle the speech problem di-
rectly, by avoiding the use of features that are char-
acteristic of written text and by explicitly addressing
the problem of speech recognition errors through the
use of smoothing techniques and word confidence in-
formation. Preliminary research in fully-automated
techniques for reading comprehension, such as the
Deep Read system developed by Hirschman et al.
(1999), has included many standard NLP com-
ponents, such as part-of-speech tagging, corefer-
ence/pronoun resolution, proper name finding, and
morphological analysis (stemming). While the tech-
niques that are being developed for reading compre-
hension provide a starting point, these techniques
cannot be effectively applied to audio comprehension
exams directly, because of the nature of differences
between written and spoken language data. In this
paper we address three specific challenges in devel-
oping audio comprehension system:
</bodyText>
<listItem confidence="0.997643">
• Fundamental differences between text-based
data and spoken language data (Section 2)
• Identifying proper names in &amp;quot;noisy&amp;quot; data (Sec-
tions 3)
• Dealing with out-of-vocabulary words (Sections
4)
</listItem>
<bodyText confidence="0.999851285714286">
In our discussion we will use examples taken
from television and radio broadcast news, a &amp;quot;found&amp;quot;
source of audio passages with a virtually unlimited
vocabulary and a wide range of opportunities for
audio comprehension evaluation. All ASR transcrip-
tions we use will be actual output from a broadcast
news ASR system with a word error rate of 30%.
</bodyText>
<sectionHeader confidence="0.973169" genericHeader="introduction">
2 Fundamental Data Differences
</sectionHeader>
<bodyText confidence="0.9982914">
Beyond the obvious difference between a raw audio
signal and a written text, the type of data output
by a speech recognizer is fundamentally different
from text-based data, even though they both con-
sist primarily of words. On one level, there are im-
</bodyText>
<page confidence="0.998097">
6
</page>
<bodyText confidence="0.999957923076923">
portant orthographic differences, since spoken lan-
guage transcriptions lack many features present in
written language. In addition, there is an inher-
ent uncertainty in spoken language transcriptions,
which almost always contain word errors. The de-
gree of uncertainty is variable, since the word error
rate (WER) of state-of-the-art speech recognizers
can range from very low (1-5%) to very high (40-
50%), depending on the domain (e.g., digit recogni-
tion vs. travel dialog vs. broadcast news vs. tele-
phone conversations).
To illustrate some of the important differences be-
tween text-based and spoken language data, Figure 1
shows three versions of a sentence from a 1997 CNN
news broadcast. The first version is the sentence
as it would typically be written. The second ver-
sion is the sentence as it would look as output from
a &amp;quot;perfect&amp;quot; ASR system in which all spoken words
are correctly transcribed (0% WER); we will discuss
characteristics of this version in Section 2.1. The
third version shows the actual output of a speech
recognizer with a word error rate of 30%; in addi-
tion to the output hypothesis words, this version also
shows word-level confidence scores. We will discuss
the problems created by the word errors in Section
2.2.
</bodyText>
<subsectionHeader confidence="0.961164">
2.1 Orthographic and Lexical Features
</subsectionHeader>
<bodyText confidence="0.995529490566038">
Consider the following (written) questions that may
be asked about a spoken passage containing the ex-
ample sentence in Figure 1, all of which can be easily
answered by humans directly from the written pas-
sage:
Who has been seeking Mr. Reineck?
Whom have German authorities been seeking?
How long have German authorities been seeking
Mr. Reineck?
In comparing the &amp;quot;clean&amp;quot; transcription in Figure
1 and the written questions above to the perfect
(0% WER) transcription, there are several differ-
ences that are immediately evident. These differ-
ences impact the tokenization of the data as well as
the lexical representation of words, which will af-
fect the ability of an audio comprehension system
to relate words in a written questionl to ASR tran-
scriptions.
Lack of capitalization and punctuation: In
many languages, including English, capitalization
and punctuation, such as periods, commas, and quo-
tation marks, provide important information about
sentence/utterance boundaries and the presence of
proper names (which we will discuss in Section 3).
However, ASR output is usually caseless, such that
&apos;Spoken questions would first need to be transcribed by
the ASR system and will be addressed in Section 2.2.
boundaries and names (e.g., &amp;quot;REINECK&amp;quot;) are not
as easy to identify as in written language.
Most abbreviations are spelled out: Related to
the lack of punctuation, ASR output does not usu-
ally contain abbreviations ( &amp;quot;MISTER&amp;quot; vs. &amp;quot;Mr.&amp;quot;).
Numbers are spelled out: Types of numbers in
ASR data are not as easy to recognize (&amp;quot;NINETEEN
NINETY TWO&amp;quot; vs. 1992). Tokenization of num-
bers is very different in ASR output, as a single
written token like &amp;quot;$163.75&amp;quot; that is easily recogniz-
able as a monetary amount can result in a large
number of ASR output words &amp;quot;ONE HUNDRED
SIXTY THREE DOLLARS AND SEVENTY FIVE
CENTS,&amp;quot; which is not immediately identified as a
single quantity.
Presence of Disfluencies: Though not present
in this example, spoken language frequently con-
tains disfluencies, such as pause fillers (&amp;quot;UH&amp;quot;,
&amp;quot;UM&amp;quot;), word fragments, and repetitions, that are
not present in written language. For example, the
person reading the passage may actually say &amp;quot;MIS-
TER REIN- UH REINECK,&amp;quot; making successful pro-
cessing of the output more difficult.
Effective audio comprehension systems will need
to normalize all text-based and spoken language
data to address orthographic differences.
</bodyText>
<subsectionHeader confidence="0.984612">
2.2 Uncertainty in Speech Transcriptions
</subsectionHeader>
<bodyText confidence="0.999995344827586">
One of the primary factors that distinguishes text-
based language processing tasks, such as read-
ing comprehension, from spoken-language process-
ing tasks, such as audio comprehension, is the uncer-
tainty inherent in the word sequence output by the
speech recognizer. The sequence of output words is
rarely the same as the actual spoken word sequence,
due to word substitution, insertion, and deletion
errors. This uncertainty is clear in the third ver-
sion of the sentence in Figure 1; of the eleven spo-
ken words, three (27.3%) of the corresponding ASR
output words are incorrect (SINKING, IS, ARRIV-
ING). Audio comprehension systems that process
this &amp;quot;noisy&amp;quot; third version as if it contained the actual
spoken words could not possibly answer any of the
sample questions above correctly, since the most im-
portant words (MISTER REINECK) are not present
in the output.
Hirschman et al. (1999) report initial results in
developing a reading comprehension system using a
&amp;quot;bag of words&amp;quot; approach, in which the sentences in a
passage that are deemed most likely to contain the
answer are those with the maximum lexical over-
lap with the question, without regard for word or-
der within the sentence. Recognition word errors
would obviously adversely affect such an approach
applied to audio comprehension; in cases where the
words in the answer to the question were misrecog-
nized, the system would be incapable of answering
</bodyText>
<page confidence="0.997119">
7
</page>
<bodyText confidence="0.999515448275863">
correctly. In the case of spoken questions, an addi-
tional layer of uncertainty is present since the recog-
nizer may output different hypothesis words for the
same word in a question and in a spoken passage;
for example, &amp;quot;Reineck&amp;quot; was also misrecognized else-
where in the same news story as &amp;quot;RIGHT AT, &amp;quot;
&amp;quot;RYAN AND,&amp;quot; &amp;quot;RYAN EIGHT,&amp;quot; &amp;quot;REINER,&amp;quot; and
&amp;quot;RUNNING AND.&amp;quot;
One of the possible ways to address this lexical
overlap problem is to expand the set of candidate
words: rather than restricting processing to the sin-
gle best recognizer hypothesis sequence, we can al-
low the top N hypothesis sequences (known as the
&amp;quot;N-best list&amp;quot;). In the example of Figure 1, if (SEEK-
ING, MISTER, and REINECK) are alternative hy-
potheses for the incorrect (SINKING, IS, and AR-
RIVING) somewhere in the N-best list, the bag of
words approach would at least have a chance of an-
swering the question correctly.
While the bag of words approach is a simple tech-
nique providing an initial baseline result, &amp;quot;deeper&amp;quot;
understanding of reading (and audio) comprehen-
sion passages will require modeling of the sequential
nature of the language. Statistical language mod-
eling, an essential component of most state-of-the-
art speech recognition systems, seeks to estimate
the probability of the sequence of L spoken words,
P(wi...wL). The language modeling within the ASR
system contributes to the output word sequence, but
the actual recognizer output is usually not the orig-
inal sequence w1...WL , but instead a sequence of M
words hi...hm, where M may not necessarily be the
same as L and where P(hi.••hm) P(wi—wL)•
Systems processing ASR output data must there-
fore effectively model the difference between the
actual sequence and the hypothesized se-
quence hi ...hm.
One way to account for word errors in the ASR
output sequence h1...hm is by integrating word-level
confidence scores into the model of the word se-
quence. This word-level confidence score, which is a
number between 0 and 1 produced by many current
automatic speech recognition systems, is an estimate
of the posterior probability that the word output
by an ASR system is actually correct. As such,
it provides us with important information about
the output transcription that can assist error detec-
tion. The third version of the sentence in Figure 1
also includes the word confidence scores that were
produced with the output word sequence. In this
particular example, the word confidence scores are
an excellent indication of the presence of word er-
rors, since the three word errors (SINKING, IS, and
ARRIVING) also have the three lowest confidence
scores (.14, .09, and .21). Unfortunately, though
confidence scores are a good indication of correct-
ness, it is not always this straightforward to distin-
guish the errors from correctly transcribed words.
</bodyText>
<sectionHeader confidence="0.957102" genericHeader="method">
3 Robust Name Finding
</sectionHeader>
<bodyText confidence="0.999706604166667">
Extracting entities such as proper names is an im-
portant first step in many systems aimed at auto-
matic language understanding, and identifying these
types of phrases is useful in many language un-
derstanding tasks, such as coreference resolution,
sentence chunking and parsing, and summariza-
tion/gisting. The targets of proper name find-
ing, names of persons, locations, and organizations,
are very often the answers to the common &amp;quot;W-
questions&amp;quot; Who? and Where? A common definition
of the extended name finding task, known as the
&amp;quot;named entity&amp;quot; task, also includes numeric phrases,
such as dates, times, monetary amounts, and per-
cents, which are often the answers to other com-
mon questions When? and How Much? Identify-
ing named entities in passages should thus help in
reading/audio comprehension. In fact, Hirschman et
al. (1999) report that identifying named entities in
reading comprehension passages and questions con-
sistently improves the performance of their system,
even when the name recognition has an accuracy as
low as 76.5%. We would expect name recognition
to also be a very important component of any audio
comprehension system.
Figure 2 shows an example of the importance of
names in a news story. This example again shows
three versions of a sentence from the news. The first
version shows the ASR output for a sentence. Due
to the word errors &amp;quot;OUR STRAWS YEAR BEHIND
IT&amp;quot;, an audio comprehension system would be un-
able to answer most simple questions such as:
Who is shown on a T-shirt with a sledgeham-
mer?
Where is Jorg Haider from?
However, the second version in Figure 2 shows
that if we know that &amp;quot;OUR STRAWS&amp;quot; is a location
phrase and that &amp;quot;YEAR BEHIND IT&amp;quot; is a person
phrase (albeit incorrectly transcribed), we could at
least know where in the passage to find the answer
to the Who? and Where? questions, since the other
words in the sentence are correctly transcribed. This
information could be used, for example, to consult
other corresponding word sequences in the N-best
list or word lattice in which the words &amp;quot;Austria&apos;s
JOrg Haider&amp;quot; may have been correctly transcribed.
In this case &amp;quot;Haider&amp;quot; is an out-of-vocabulary word
and would not be present elsewhere in the N-best
list; we will discuss this problem in Section 4.
</bodyText>
<subsectionHeader confidence="0.978742">
3.1 Name finding techniques
</subsectionHeader>
<bodyText confidence="0.999346333333333">
Finding names in text-based sources such as news-
paper and newswire documents has been a focus of
research for many years, and some systems have
</bodyText>
<page confidence="0.989557">
8
</page>
<bodyText confidence="0.999977739583334">
reported performance approaching human perfor-
mance (96-98%) on the named entity task. Find-
ing names in speech data is a very new topic of re-
search, and most previous work has consisted of the
direct application of text-based systems to speech
data, with some minor adaptations.
For the range of word error rates common for
most large vocabulary ASR systems (&lt; 30%), all
the named entity models we will describe in this sec-
tion produce performance between 70-90%. This is
comparable to or better than the accuracy (76.5%)
of the named entity system that Hirschman et al.
(1999) report improves their reading comprehension
system. However, there is significant room for im-
provement of the speech data NE systems. Previous
work has found that the absence of capitalization
and punctuation information in speech transcrip-
tions results in a 2-3% decrease in name finding per-
formance(Miller et al., 1999), and this degradation is
greater in the presence of word errors. The decline in
NE performance for text-based systems applied di-
rectly to errorful speech data is roughly linear with
increase in WER, although the NE performance de-
grades more slowly than the WER, i.e. each recog-
nition error does not result in an NE error. One of
the goals of work directly on speech understanding
models should be to improve this linear degradation.
One example of a trainable text-based system
that has been applied successfully to speech rec-
ognizer output is described by Bikel et a/.(1999).
Each type of entity (person, location, etc.) to be
recognized is represented as a separate state in a
finite-state machine. A bigram language model is
trained for each phrase type (i.e., for each state),
and Viterbi-style decoding is then used to pro-
duce the most likely sequence of phrase labels in
a test utterance. This model incorporates non-
overlapping features about the words, such as punc-
tuation and capitalization, in a bigram back-off to
handle infrequent or unobserved words. Specifically,
each word is deterministically assigned one of 14
non-overlapping features (such as two-digit-number,
contains-digit-and-period, capitalized-word, and all-
capital-letters), and the back-off distribution de-
pends on the assigned feature. The approach has
resulted in high performance on many text-based
tasks, including English and Spanish newswire texts.
Despite the fact that the original model relied heav-
ily on text-based features such as punctuation and
capitalization in the language model back-off, it
gives good results on speech data without modify-
ing anything but the training material (Miller et al.,
1999).
A closely related statistical approach to named en-
tity tagging specifically targeted at speech data was
developed at Sheffield by Gotoh and Renals (2000).
In their model, named entity tags are treated as cat-
egories associated with words, effectively expanding
the vocabulary, e.g. a word that might be both a per-
son and a place name would be represented with two
different lexical items. An n-gram language model
is trained on these augmented words, using a sin-
gle model for joint word/tag dependence on the his-
tory rather than the two components used in the
Bikel model and thus representing the class-to-class
transitions implicitly rather than explicitly. A key
difference between the approaches is in the back-off
mechanism, which resembles a class grammar for the
Sheffield system. In addition, the Sheffield approach
uses a causal decoding algorithm, unlike the Viterbi
algorithm which delays decisions until an entire sen-
tence has been observed, though this is not a restric-
tion of the model. The extended-vocabulary n-gram
approach has the advantage that it is well-suited to
using directly in the ASR search process.
Palmer, Ostendorf, and Burger (1999; 2000) use
a model similar to other probabilistic name finding
models, with several important differences in the
model topology and the language modeling tech-
nique used. A key difference in their approach is
that infrequent data is handled using the class-based
smoothing technique described in (Iyer and Osten-
dorf, 1997) that, unlike the orthographic-feature-
dependent back-off, allows for ambiguity of word
classes. They describe methods for incorporating
information from place and name word lists, as well
as simple part-of-speech labels, and thus account for
the fact that some words can be used in multiple
classes. Their results for high error rates (28.2) are
slightly better than the simple back-off, suggesting
that the POS smoothing technique is more robust to
ASR errors. In addition to the robustness provided
by the class-based smoothing, they also report ini-
tial success in integrating word confidence scores into
their model to further improve the robustness of the
system to speech recognition errors.
</bodyText>
<sectionHeader confidence="0.998947" genericHeader="method">
4 Out-of-Vocabulary Words
</sectionHeader>
<bodyText confidence="0.998967375">
Historically, the goal of automatic speech recogni-
tion (ASR) has been to transcribe the sequence of
words contained in an audio stream. State-of-the-
art speech recognition systems model this problem
using a probabilistic formulation in which the most
likely sequence of words is produced given a sequence
of acoustic features derived from the raw utterance
audio signal. While this approach has been very
successful, the model has a serious limitation: it can
only produce output hypotheses from a finite list of
words that the recognizer explicitly models. This
list of possible output words is known as the system
vocabulary, and any spoken word not contained in
the vocabulary is referred to as an out-of-vocabulary
(00V) word. Every 00V word in the input utter-
ance is guaranteed to result in one or more output
</bodyText>
<page confidence="0.995142">
9
</page>
<bodyText confidence="0.996282290322581">
errors.
As we discussed in Section 2.2, ASR output word
errors, especially from spoken names, will adversely
affect audio comprehension performance. However,
the methods for dealing with errors that we dis-
cussed in previous sections, such as using N-best list
output, can only compensate for misrecognitions of
known words. Since 00V words will never appear in
the hypothesized N-best output, other methods are
necessary for accounting for their presence in the in-
put audio stream. Figures 1 and 2 both have exam-
ples of words that were out-of-vocabulary (Reineck,
Haider) for the particular ASR system. Figure 3
shows another example, in which several names are
out-of-vocabulary (Brill, Salif, Keta, Nusa, Fateh,
Ali-han)._
Some examples of questions that might be asked
about this passage are:
Which two musicians did Wally Brill discover?
Where is vocalist Salif Keta from?
Who got turned onto Keta and Ali-han&apos;s music?
Clearly, these questions could not be answered di-
rectly from the actual output due to the word errors.
In fact, identifying likely proper names in the out-
put, as we discussed in Section 3, would also be inad-
equate, because the output word error &amp;quot;DECATUR&amp;quot;
might be mistaken for the answer to a Where? ques-
tion, and &amp;quot;MISTER FUNG&amp;quot; might be mistaken for
the answer to a Who? question. An effective method
for dealing with out-of-vocabulary words is thus nec-
essary.
</bodyText>
<subsectionHeader confidence="0.995601">
4.1 Increasing ASR Vocabulary
</subsectionHeader>
<bodyText confidence="0.999997246153847">
One approach to the 00V problem might be to
increase the vocabulary size of the ASR system.
Speech recognition systems can have a range of vo-
cabulary sizes, depending on the target domain, the
generality required, as well as the availability of com-
putational resources. For example, many research
systems designed for constrained environments, such
as real-time travel information dialog, use a vo-
cabulary size as small as 1,000-5,000 words. On
the other hand, current research systems for uncon-
strained tasks such as the transcription of broadcast
news programs frequently have vocabularies between
25,000 and 64,000 words. Increasing the vocabu-
lary size of a speech recognition system can result
in lower error rates, in part by decreasing the per-
centage of 00V words in the input utterance. How-
ever, systems with larger vocabularies require more
memory and run slower than those with smaller
vocabularies. Since practical ASR systems cannot
have unlimited memory and computational require-
ments, they naturally cannot have unlimited vocab-
ulary sizes.
In addition to increased computational cost,
adding words to a vocabulary increases the poten-
tial confusability with other vocabulary words. In
fact, Rosenfeld (1995) reports that a vocabulary size
around 64,000 is nearly optimal for processing read
North American Business news, and that increas-
ing the vocabulary size beyond this yields negligible
recognition improvement at best. The optimal vo-
cabulary size is also domain dependent: a 64,000
word vocabulary may not be necessary for travel di-
alog but may be inadequate for directory assistance.
Rosenfeld&apos;s analysis shows that increasing the sys-
tem vocabulary size can help recognition rates for
many common words while hurting for less common
words. Yet the less common words, such as new
names introduced as a result of national and inter-
national events, usually contain more semantic in-
formation about the utterance, and these errors are
much more costly for language understanding ap-
plications. Since new words are constantly being in-
troduced into common usage, it is impossible to ever
have a complete vocabulary of all spoken words, and
the treatment of new lexical items is thus an essen-
tial element of any system aiming to process natural
language.
Hetherington (1995) conducts an extensive empir-
ical study of the out-of-vocabulary problem in his
PhD thesis. He presents a demonstration of the
magnitude of the 00V problem for a wide range
of multilingual natural language corpora and shows
that some tasks can require vocabularies larger than
100,000 words to reduce the 00V rate below 1%.
He shows that even an 00V rate of 1% results in
15-20% of all utterances containing unknown words.
He also produces experimental results of the effect
that unknown words have on speech recognition out-
put, showing that, on the average, each 00V input
word results in 1.5 actual word errors. Of the errors
resulting from 00V words, 20% of these word errors
result from in-vocabulary words being misrecognized
due to their proximity to an unknown word. This
work demonstrates the need for 00V word handling
in any speech recognition system.
</bodyText>
<subsectionHeader confidence="0.978759">
4.2 Dynamic Vocabularies
</subsectionHeader>
<bodyText confidence="0.999512230769231">
The need for unlimited spoken language vocabulary
despite a limited ASR vocabulary suggests an al-
ternative view of large-vocabulary spoken language
processing, in which rather than trying to include
all possible words in the ASR vocabulary we in-
stead develop techniques for dynamically adapting
the overall audio comprehension system vocabulary
using lexical resources, without requiring a larger
ASR vocabulary and the problems this entails.
Geutner et al. (1998) describe a multi-pass de-
coding approach targeted at reducing the out-of-
vocabulary rates for heavily inflected language, such
as Serbo-Croatian, German, and Turkish. Their
</bodyText>
<page confidence="0.99438">
10
</page>
<bodyText confidence="0.999981041666667">
work attempts to dynamically expand the effective
vocabulary size by adapting the recognition dictio-
nary to each utterance. In the first recognition pass,
an utterance-specific vocabulary list is constructed
from the word lattice. They then use a technique
they call &amp;quot;Hypothesis Driven Lexical Adaptation&amp;quot;
to expand the vocabulary list by adding all words
in a full dictionary that are sufficiently similar to
those in the utterance list, where &amp;quot;similarity&amp;quot; is de-
termined by the morphology and phonetics of the
words. An automatic process then creates a new ut-
terance recognition vocabulary and language model
from the expanded vocabulary list , and a second
recognition pass is performed using the expanded
models. Geutner et al. report that the lexical adap-
tation methods result in a significant decrease of up
to 55% in 00V rates for the inflected languages,
and that this improvement in 00V rate results in
an improvement in the recognition rate of 3-6% (ab-
solute).
Geutner&apos;s multi-pass approach requires vocabu-
lary adaptation and re-recognition of each com-
plete utterance. The importance of name finding
in audio comprehension suggests an alternative to
this approach that would allow more targeted re-
recognition of partial utterances. As the example
in Figure 2 showed, it is possible to determine the
data regions that contain the potential answers, even
when the words themselves are misrecognized. For
written questions, phonetic information from the
question and hypothesis words can be used to help
repair key misrecognitions. For example, using pho-
netic information, it is possible to relate &amp;quot;vocal-
ist Salif Keta&amp;quot; in a question to &amp;quot;VOCALIST SELL
THE DECATUR&amp;quot; in the ASR output. This infor-
mation can be supplemented with external lexical re-
sources, such as word lists for the appropriate type of
proper name, to expand the set of possible hypothe-
ses within the region. Large lists of names are avail-
able publicly that could be used for this purpose;
for example, the U.S. Census publishes a ranked list
of the most common surnames and first names in
the United States, most of which are 00V words
for current ASR systems. Once a region in the ASR
output is identified as an 00V person, the Census
data could be used to correct the 00V errors. This
would then allow the audio comprehension system
to answer more Who? questions correctly.
</bodyText>
<sectionHeader confidence="0.999743" genericHeader="method">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999944909090909">
Just as research in reading comprehension can help
evaluate text-based NLP systems against a hu-
man benchmark, audio comprehension can provide
a useful task for evaluating speech understand-
ing systems. Audio comprehension provides an
understanding-based evaluation paradigm for speech
systems that encourages research on a useful spoken
language understanding application rather than on
&amp;quot;getting all the words right.&amp;quot; The techniques devel-
oped for audio comprehension promise to be widely
useful in many language understanding area.
</bodyText>
<sectionHeader confidence="0.996958" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.999813921568628">
D. Bikel, R. Schwartz, R. Weischedel, &amp;quot;An Algo-
rithm that Learns What&apos;s in a Name,&amp;quot; Machine
Learning, 34(1/3):211-231, 1999.
P. Geutner, M. Finke, P. Scheytt, &amp;quot;Adaptive Vocab-
ularies for Transcribing Multilingual Broadcast
News,&amp;quot; Proc. International Conference on Acous-
tic, Speech and Signal Processing, 1998.
Y. Gotoh, S. Renals, &amp;quot;Information Extraction From
Broadcast News,&amp;quot; Philosophical Transactions of
the Royal Society, series A: Mathematical, Physi-
cal and Engineering Sciences, vol.358, issue 1769,
April 2000.
I.L. Hetherington, &amp;quot;A Characterization of the
Problem of New, Out-of-Vocabulary Words in
Continuous-Speech Recognition and Understand-
ing,&amp;quot; PhD Thesis, Massachusetts Institute of
Technology, 1995.
L. Hirschman, M. Light, E. Breck, J. Burger,
&amp;quot;Deep Read: A Reading Comprehension System,&amp;quot;
Proc. 37th Annual Meeting fo the Association for
Computational Linguistics (ACL99), pp. 325-332,
1999.
R. Iyer and M. Ostendorf, &amp;quot;Transforming Out-of-
Domain Estimates to Improve In-Domain Lan-
guage Models,&amp;quot; Proc. European Conference on
Speech Comm. and Tech., Vol. 4, pp. 1975-1978,
1997.
D. Miller, R. Schwartz, R. Weischedel, R.
Stone, &amp;quot;Named Entity Extraction from Broadcast
News,&amp;quot; Proc. DARPA Broadcast News Workshop,
pp. 37-40, 1999.
D. Palmer, M. Ostendorf, and J. Burger, &amp;quot;Robust
Information Extraction from Spoken Language
Data,&amp;quot; Proc. European Conference on Speech
Comm. and Tech., pp. 1035-1038, 1999.
D. Palmer, M. Ostendorf, and J. Burger, &amp;quot;Robust
Information Extraction from Automatically Gen-
erated Speech Transcriptions,&amp;quot; Speech Communi-
cation, in press, 2000.
P. Robinson, E. Brown, J. Burger, N. Chinchor, A.
Douthat, L. Ferro, and L. Hirschman, &amp;quot;Overview:
Information extraction from broadcast news,&amp;quot;
Proc. DARPA Broadcast News Workshop, pp. 27-
30, 1999.
R. Rosenfeld, &amp;quot;Optimizing Lexical and N-gram Cov-
erage Via Judicious Use of Linguistic Data.&amp;quot;
Proc. European Conference on Speech Comm. and
Tech., volume 2, pages 1763-1766, 1995.
M. Siu and H. Gish, &amp;quot;Evaluation of word confidence
for speech recognition systems,&amp;quot; Computer Speech
ey Language, Vol. 13, No. 4, Oct 1999, pp. 299-319
</reference>
<page confidence="0.998453">
11
</page>
<bodyText confidence="0.670775">
German authorities have been seeking Mr. Reineck since 1992.
</bodyText>
<sectionHeader confidence="0.5943675" genericHeader="method">
GERMAN AUTHORITIES HAVE BEEN SEEKING MISTER REINECK
SINCE NINETEEN NINETY TWO
</sectionHeader>
<equation confidence="0.548922">
GERMAN(.74) AUTHORITIES(.90) HAVE(.79) BEEN(.82) SINKING(.14) IS(.09) ARRIVING(.21)
SINCE(.60) NINETEEN(. 90) NINETY(.95) TWO (.94)
</equation>
<figureCaption confidence="0.9876075">
Figure 1: Example of text-based vs. spoken language differences: a written sentence and its ASR transcrip-
tions (WER 0% and 30% with word confidence scores).
</figureCaption>
<sectionHeader confidence="0.4268315" genericHeader="method">
THE SHIRTS SHOW OUR STRAWS YEAR BEHIND IT WITH A SLEDGEHAMMER AND A RACIST
CAPTION
</sectionHeader>
<bodyText confidence="0.6256275">
THE SHIRTS SHOW [location] [person] WITH A SLEDGEHAMMER AND A RACIST CAPTION
The T-shirts showed Austria&apos;s Jorg Haider with a sledgehammer and a racist caption
</bodyText>
<figureCaption confidence="0.949478">
Figure 2: Example showing the importance of names: ASR output (30% WER) for a sentence, the same
</figureCaption>
<bodyText confidence="0.853248">
ASR sentence with locations of proper names labeled, and the correct transcription.
Then a few years ago, Wally Brill got turned onto the music of West African vocalist Salif Keta and the
haunting sounds of the late Nusa Fateh Ali-han.
</bodyText>
<sectionHeader confidence="0.996174333333333" genericHeader="method">
IN A FEW YEARS AGO ALWAYS REAL GOT TURNED ON TO THE MUSIC OF WEST AFRICAN
VOCALIST SELL THE DECATUR AND THE HAUNTING SOUNDS OF THE LATE MISTER FUNG&apos;S
ALLEY
</sectionHeader>
<figureCaption confidence="0.992454">
Figure 3: Example of ASR output showing numerous 00V name errors.
</figureCaption>
<page confidence="0.992959">
12
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.886646">
<title confidence="0.999597">Some Challenges of Developing Fully-Automated Systems Taking Audio Comprehension Exams</title>
<author confidence="0.999962">David D Palmer</author>
<affiliation confidence="0.907984">The MITRE</affiliation>
<address confidence="0.99532">202 Burlington Bedford, MA 01730</address>
<email confidence="0.990249">palmer@mitre.org</email>
<abstract confidence="0.999439090909091">Audio comprehension tests are designed to help evaluate a listener&apos;s understanding of a spoken passage and are frequently a key component of language competency exams. Just as reading comprehension exams are proving useful in evaluating text-based language processing technology, audio comprehension exams can be used to evaluate spoken language processing systems. In this paper we discuss some of the challenges of developing automated systems for taking audio comprehension exams.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Bikel</author>
<author>R Schwartz</author>
<author>R Weischedel</author>
</authors>
<title>An Algorithm that Learns What&apos;s in a Name,&amp;quot;</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<marker>Bikel, Schwartz, Weischedel, 1999</marker>
<rawString>D. Bikel, R. Schwartz, R. Weischedel, &amp;quot;An Algorithm that Learns What&apos;s in a Name,&amp;quot; Machine Learning, 34(1/3):211-231, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Geutner</author>
<author>M Finke</author>
<author>P Scheytt</author>
</authors>
<title>Adaptive Vocabularies for Transcribing Multilingual Broadcast News,&amp;quot;</title>
<date>1998</date>
<booktitle>Proc. International Conference on Acoustic, Speech and Signal Processing,</booktitle>
<contexts>
<context position="25120" citStr="Geutner et al. (1998)" startWordPosition="4008" endWordPosition="4011"> due to their proximity to an unknown word. This work demonstrates the need for 00V word handling in any speech recognition system. 4.2 Dynamic Vocabularies The need for unlimited spoken language vocabulary despite a limited ASR vocabulary suggests an alternative view of large-vocabulary spoken language processing, in which rather than trying to include all possible words in the ASR vocabulary we instead develop techniques for dynamically adapting the overall audio comprehension system vocabulary using lexical resources, without requiring a larger ASR vocabulary and the problems this entails. Geutner et al. (1998) describe a multi-pass decoding approach targeted at reducing the out-ofvocabulary rates for heavily inflected language, such as Serbo-Croatian, German, and Turkish. Their 10 work attempts to dynamically expand the effective vocabulary size by adapting the recognition dictionary to each utterance. In the first recognition pass, an utterance-specific vocabulary list is constructed from the word lattice. They then use a technique they call &amp;quot;Hypothesis Driven Lexical Adaptation&amp;quot; to expand the vocabulary list by adding all words in a full dictionary that are sufficiently similar to those in the ut</context>
</contexts>
<marker>Geutner, Finke, Scheytt, 1998</marker>
<rawString>P. Geutner, M. Finke, P. Scheytt, &amp;quot;Adaptive Vocabularies for Transcribing Multilingual Broadcast News,&amp;quot; Proc. International Conference on Acoustic, Speech and Signal Processing, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Gotoh</author>
<author>S Renals</author>
</authors>
<title>Information Extraction From Broadcast News,&amp;quot;</title>
<date>2000</date>
<journal>Philosophical Transactions of the Royal Society, series A: Mathematical, Physical and Engineering Sciences,</journal>
<volume>358</volume>
<pages>1769</pages>
<contexts>
<context position="17204" citStr="Gotoh and Renals (2000)" startWordPosition="2749" endWordPosition="2752"> and allcapital-letters), and the back-off distribution depends on the assigned feature. The approach has resulted in high performance on many text-based tasks, including English and Spanish newswire texts. Despite the fact that the original model relied heavily on text-based features such as punctuation and capitalization in the language model back-off, it gives good results on speech data without modifying anything but the training material (Miller et al., 1999). A closely related statistical approach to named entity tagging specifically targeted at speech data was developed at Sheffield by Gotoh and Renals (2000). In their model, named entity tags are treated as categories associated with words, effectively expanding the vocabulary, e.g. a word that might be both a person and a place name would be represented with two different lexical items. An n-gram language model is trained on these augmented words, using a single model for joint word/tag dependence on the history rather than the two components used in the Bikel model and thus representing the class-to-class transitions implicitly rather than explicitly. A key difference between the approaches is in the back-off mechanism, which resembles a class </context>
</contexts>
<marker>Gotoh, Renals, 2000</marker>
<rawString>Y. Gotoh, S. Renals, &amp;quot;Information Extraction From Broadcast News,&amp;quot; Philosophical Transactions of the Royal Society, series A: Mathematical, Physical and Engineering Sciences, vol.358, issue 1769, April 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I L Hetherington</author>
</authors>
<title>A Characterization of the Problem of New, Out-of-Vocabulary Words in Continuous-Speech Recognition and Understanding,&amp;quot;</title>
<date>1995</date>
<tech>PhD Thesis,</tech>
<institution>Massachusetts Institute of Technology,</institution>
<contexts>
<context position="23770" citStr="Hetherington (1995)" startWordPosition="3796" endWordPosition="3797">ry size can help recognition rates for many common words while hurting for less common words. Yet the less common words, such as new names introduced as a result of national and international events, usually contain more semantic information about the utterance, and these errors are much more costly for language understanding applications. Since new words are constantly being introduced into common usage, it is impossible to ever have a complete vocabulary of all spoken words, and the treatment of new lexical items is thus an essential element of any system aiming to process natural language. Hetherington (1995) conducts an extensive empirical study of the out-of-vocabulary problem in his PhD thesis. He presents a demonstration of the magnitude of the 00V problem for a wide range of multilingual natural language corpora and shows that some tasks can require vocabularies larger than 100,000 words to reduce the 00V rate below 1%. He shows that even an 00V rate of 1% results in 15-20% of all utterances containing unknown words. He also produces experimental results of the effect that unknown words have on speech recognition output, showing that, on the average, each 00V input word results in 1.5 actual </context>
</contexts>
<marker>Hetherington, 1995</marker>
<rawString>I.L. Hetherington, &amp;quot;A Characterization of the Problem of New, Out-of-Vocabulary Words in Continuous-Speech Recognition and Understanding,&amp;quot; PhD Thesis, Massachusetts Institute of Technology, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hirschman</author>
<author>M Light</author>
<author>E Breck</author>
<author>J Burger</author>
</authors>
<title>Deep Read: A Reading Comprehension System,&amp;quot;</title>
<date>1999</date>
<booktitle>Proc. 37th Annual Meeting fo the Association for Computational Linguistics (ACL99),</booktitle>
<pages>325--332</pages>
<contexts>
<context position="2780" citStr="Hirschman et al. (1999)" startWordPosition="418" endWordPosition="421"> information is lost due to recognition errors when moving from text to speech and the possibility that it can be regained in part via word confidence prediction. We believe that successful approaches to audio comprehension will tackle the speech problem directly, by avoiding the use of features that are characteristic of written text and by explicitly addressing the problem of speech recognition errors through the use of smoothing techniques and word confidence information. Preliminary research in fully-automated techniques for reading comprehension, such as the Deep Read system developed by Hirschman et al. (1999), has included many standard NLP components, such as part-of-speech tagging, coreference/pronoun resolution, proper name finding, and morphological analysis (stemming). While the techniques that are being developed for reading comprehension provide a starting point, these techniques cannot be effectively applied to audio comprehension exams directly, because of the nature of differences between written and spoken language data. In this paper we address three specific challenges in developing audio comprehension system: • Fundamental differences between text-based data and spoken language data </context>
<context position="8714" citStr="Hirschman et al. (1999)" startWordPosition="1362" endWordPosition="1365">. The sequence of output words is rarely the same as the actual spoken word sequence, due to word substitution, insertion, and deletion errors. This uncertainty is clear in the third version of the sentence in Figure 1; of the eleven spoken words, three (27.3%) of the corresponding ASR output words are incorrect (SINKING, IS, ARRIVING). Audio comprehension systems that process this &amp;quot;noisy&amp;quot; third version as if it contained the actual spoken words could not possibly answer any of the sample questions above correctly, since the most important words (MISTER REINECK) are not present in the output. Hirschman et al. (1999) report initial results in developing a reading comprehension system using a &amp;quot;bag of words&amp;quot; approach, in which the sentences in a passage that are deemed most likely to contain the answer are those with the maximum lexical overlap with the question, without regard for word order within the sentence. Recognition word errors would obviously adversely affect such an approach applied to audio comprehension; in cases where the words in the answer to the question were misrecognized, the system would be incapable of answering 7 correctly. In the case of spoken questions, an additional layer of uncert</context>
<context position="12868" citStr="Hirschman et al. (1999)" startWordPosition="2042" endWordPosition="2045">ing tasks, such as coreference resolution, sentence chunking and parsing, and summarization/gisting. The targets of proper name finding, names of persons, locations, and organizations, are very often the answers to the common &amp;quot;Wquestions&amp;quot; Who? and Where? A common definition of the extended name finding task, known as the &amp;quot;named entity&amp;quot; task, also includes numeric phrases, such as dates, times, monetary amounts, and percents, which are often the answers to other common questions When? and How Much? Identifying named entities in passages should thus help in reading/audio comprehension. In fact, Hirschman et al. (1999) report that identifying named entities in reading comprehension passages and questions consistently improves the performance of their system, even when the name recognition has an accuracy as low as 76.5%. We would expect name recognition to also be a very important component of any audio comprehension system. Figure 2 shows an example of the importance of names in a news story. This example again shows three versions of a sentence from the news. The first version shows the ASR output for a sentence. Due to the word errors &amp;quot;OUR STRAWS YEAR BEHIND IT&amp;quot;, an audio comprehension system would be un</context>
<context position="15035" citStr="Hirschman et al. (1999)" startWordPosition="2412" endWordPosition="2415">search for many years, and some systems have 8 reported performance approaching human performance (96-98%) on the named entity task. Finding names in speech data is a very new topic of research, and most previous work has consisted of the direct application of text-based systems to speech data, with some minor adaptations. For the range of word error rates common for most large vocabulary ASR systems (&lt; 30%), all the named entity models we will describe in this section produce performance between 70-90%. This is comparable to or better than the accuracy (76.5%) of the named entity system that Hirschman et al. (1999) report improves their reading comprehension system. However, there is significant room for improvement of the speech data NE systems. Previous work has found that the absence of capitalization and punctuation information in speech transcriptions results in a 2-3% decrease in name finding performance(Miller et al., 1999), and this degradation is greater in the presence of word errors. The decline in NE performance for text-based systems applied directly to errorful speech data is roughly linear with increase in WER, although the NE performance degrades more slowly than the WER, i.e. each recog</context>
</contexts>
<marker>Hirschman, Light, Breck, Burger, 1999</marker>
<rawString>L. Hirschman, M. Light, E. Breck, J. Burger, &amp;quot;Deep Read: A Reading Comprehension System,&amp;quot; Proc. 37th Annual Meeting fo the Association for Computational Linguistics (ACL99), pp. 325-332, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Iyer</author>
<author>M Ostendorf</author>
</authors>
<title>Transforming Out-ofDomain Estimates to Improve In-Domain Language Models,&amp;quot;</title>
<date>1997</date>
<booktitle>Proc. European Conference on Speech Comm. and Tech.,</booktitle>
<volume>4</volume>
<pages>1975--1978</pages>
<contexts>
<context position="18534" citStr="Iyer and Ostendorf, 1997" startWordPosition="2960" endWordPosition="2964">nlike the Viterbi algorithm which delays decisions until an entire sentence has been observed, though this is not a restriction of the model. The extended-vocabulary n-gram approach has the advantage that it is well-suited to using directly in the ASR search process. Palmer, Ostendorf, and Burger (1999; 2000) use a model similar to other probabilistic name finding models, with several important differences in the model topology and the language modeling technique used. A key difference in their approach is that infrequent data is handled using the class-based smoothing technique described in (Iyer and Ostendorf, 1997) that, unlike the orthographic-featuredependent back-off, allows for ambiguity of word classes. They describe methods for incorporating information from place and name word lists, as well as simple part-of-speech labels, and thus account for the fact that some words can be used in multiple classes. Their results for high error rates (28.2) are slightly better than the simple back-off, suggesting that the POS smoothing technique is more robust to ASR errors. In addition to the robustness provided by the class-based smoothing, they also report initial success in integrating word confidence score</context>
</contexts>
<marker>Iyer, Ostendorf, 1997</marker>
<rawString>R. Iyer and M. Ostendorf, &amp;quot;Transforming Out-ofDomain Estimates to Improve In-Domain Language Models,&amp;quot; Proc. European Conference on Speech Comm. and Tech., Vol. 4, pp. 1975-1978, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Miller</author>
<author>R Schwartz</author>
<author>R Weischedel</author>
<author>R Stone</author>
</authors>
<title>Named Entity Extraction from Broadcast News,&amp;quot;</title>
<date>1999</date>
<booktitle>Proc. DARPA Broadcast News Workshop,</booktitle>
<pages>37--40</pages>
<contexts>
<context position="15357" citStr="Miller et al., 1999" startWordPosition="2460" endWordPosition="2464">For the range of word error rates common for most large vocabulary ASR systems (&lt; 30%), all the named entity models we will describe in this section produce performance between 70-90%. This is comparable to or better than the accuracy (76.5%) of the named entity system that Hirschman et al. (1999) report improves their reading comprehension system. However, there is significant room for improvement of the speech data NE systems. Previous work has found that the absence of capitalization and punctuation information in speech transcriptions results in a 2-3% decrease in name finding performance(Miller et al., 1999), and this degradation is greater in the presence of word errors. The decline in NE performance for text-based systems applied directly to errorful speech data is roughly linear with increase in WER, although the NE performance degrades more slowly than the WER, i.e. each recognition error does not result in an NE error. One of the goals of work directly on speech understanding models should be to improve this linear degradation. One example of a trainable text-based system that has been applied successfully to speech recognizer output is described by Bikel et a/.(1999). Each type of entity (p</context>
<context position="17049" citStr="Miller et al., 1999" startWordPosition="2725" endWordPosition="2728">ally, each word is deterministically assigned one of 14 non-overlapping features (such as two-digit-number, contains-digit-and-period, capitalized-word, and allcapital-letters), and the back-off distribution depends on the assigned feature. The approach has resulted in high performance on many text-based tasks, including English and Spanish newswire texts. Despite the fact that the original model relied heavily on text-based features such as punctuation and capitalization in the language model back-off, it gives good results on speech data without modifying anything but the training material (Miller et al., 1999). A closely related statistical approach to named entity tagging specifically targeted at speech data was developed at Sheffield by Gotoh and Renals (2000). In their model, named entity tags are treated as categories associated with words, effectively expanding the vocabulary, e.g. a word that might be both a person and a place name would be represented with two different lexical items. An n-gram language model is trained on these augmented words, using a single model for joint word/tag dependence on the history rather than the two components used in the Bikel model and thus representing the c</context>
</contexts>
<marker>Miller, Schwartz, Weischedel, Stone, 1999</marker>
<rawString>D. Miller, R. Schwartz, R. Weischedel, R. Stone, &amp;quot;Named Entity Extraction from Broadcast News,&amp;quot; Proc. DARPA Broadcast News Workshop, pp. 37-40, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Palmer</author>
<author>M Ostendorf</author>
<author>J Burger</author>
</authors>
<title>Robust Information Extraction from Spoken Language Data,&amp;quot;</title>
<date>1999</date>
<booktitle>Proc. European Conference on Speech Comm. and Tech.,</booktitle>
<pages>1035--1038</pages>
<marker>Palmer, Ostendorf, Burger, 1999</marker>
<rawString>D. Palmer, M. Ostendorf, and J. Burger, &amp;quot;Robust Information Extraction from Spoken Language Data,&amp;quot; Proc. European Conference on Speech Comm. and Tech., pp. 1035-1038, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Palmer</author>
<author>M Ostendorf</author>
<author>J Burger</author>
</authors>
<title>Robust Information Extraction from Automatically Generated Speech Transcriptions,&amp;quot; Speech Communication, in press,</title>
<date>2000</date>
<marker>Palmer, Ostendorf, Burger, 2000</marker>
<rawString>D. Palmer, M. Ostendorf, and J. Burger, &amp;quot;Robust Information Extraction from Automatically Generated Speech Transcriptions,&amp;quot; Speech Communication, in press, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Robinson</author>
<author>E Brown</author>
<author>J Burger</author>
<author>N Chinchor</author>
<author>A Douthat</author>
<author>L Ferro</author>
<author>L Hirschman</author>
</authors>
<title>Overview: Information extraction from broadcast news,&amp;quot;</title>
<date>1999</date>
<booktitle>Proc. DARPA Broadcast News Workshop,</booktitle>
<pages>27--30</pages>
<marker>Robinson, Brown, Burger, Chinchor, Douthat, Ferro, Hirschman, 1999</marker>
<rawString>P. Robinson, E. Brown, J. Burger, N. Chinchor, A. Douthat, L. Ferro, and L. Hirschman, &amp;quot;Overview: Information extraction from broadcast news,&amp;quot; Proc. DARPA Broadcast News Workshop, pp. 27-30, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rosenfeld</author>
</authors>
<title>Optimizing Lexical and N-gram Coverage Via Judicious Use of Linguistic Data.&amp;quot;</title>
<date>1995</date>
<booktitle>Proc. European Conference on Speech Comm. and Tech.,</booktitle>
<volume>2</volume>
<pages>1763--1766</pages>
<contexts>
<context position="22706" citStr="Rosenfeld (1995)" startWordPosition="3625" endWordPosition="3626">000 and 64,000 words. Increasing the vocabulary size of a speech recognition system can result in lower error rates, in part by decreasing the percentage of 00V words in the input utterance. However, systems with larger vocabularies require more memory and run slower than those with smaller vocabularies. Since practical ASR systems cannot have unlimited memory and computational requirements, they naturally cannot have unlimited vocabulary sizes. In addition to increased computational cost, adding words to a vocabulary increases the potential confusability with other vocabulary words. In fact, Rosenfeld (1995) reports that a vocabulary size around 64,000 is nearly optimal for processing read North American Business news, and that increasing the vocabulary size beyond this yields negligible recognition improvement at best. The optimal vocabulary size is also domain dependent: a 64,000 word vocabulary may not be necessary for travel dialog but may be inadequate for directory assistance. Rosenfeld&apos;s analysis shows that increasing the system vocabulary size can help recognition rates for many common words while hurting for less common words. Yet the less common words, such as new names introduced as a </context>
</contexts>
<marker>Rosenfeld, 1995</marker>
<rawString>R. Rosenfeld, &amp;quot;Optimizing Lexical and N-gram Coverage Via Judicious Use of Linguistic Data.&amp;quot; Proc. European Conference on Speech Comm. and Tech., volume 2, pages 1763-1766, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Siu</author>
<author>H Gish</author>
</authors>
<title>Evaluation of word confidence for speech recognition systems,&amp;quot;</title>
<date>1999</date>
<journal>Computer Speech ey Language,</journal>
<volume>13</volume>
<pages>299--319</pages>
<marker>Siu, Gish, 1999</marker>
<rawString>M. Siu and H. Gish, &amp;quot;Evaluation of word confidence for speech recognition systems,&amp;quot; Computer Speech ey Language, Vol. 13, No. 4, Oct 1999, pp. 299-319</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>