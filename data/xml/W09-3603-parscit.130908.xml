<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002854">
<title confidence="0.99522">
Accurate Argumentative Zoning with Maximum Entropy models
</title>
<author confidence="0.998818">
Stephen Merity and Tara Murphy and James R. Curran
</author>
<affiliation confidence="0.996061">
School of Information Technologies
University of Sydney
</affiliation>
<address confidence="0.437485">
NSW 2006, Australia
</address>
<email confidence="0.997812">
{smerity,tm,james}@it.usyd.edu.au
</email>
<sectionHeader confidence="0.997372" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996579875">
We present a maximum entropy classifier
that significantly improves the accuracy of
Argumentative Zoning in scientific litera-
ture. We examine the features used to
achieve this result and experiment with
Argumentative Zoning as a sequence tag-
ging task, decoded with Viterbi using up
to four previous classification decisions.
The result is a 23% F-score increase on the
Computational Linguistics conference pa-
pers marked up by Teufel (1999).
Finally, we demonstrate the performance
of our system in different scientific do-
mains by applying it to a corpus of As-
tronomy journal articles annotated using a
modified Argumentative Zoning scheme.
</bodyText>
<sectionHeader confidence="0.999525" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999915258064516">
The task of generating automatic summarizations
of one or more texts is a central problem in Natu-
ral Language Processing (NLP). Summarization is
a fundamental component for future information
retrieval and question answering systems, incor-
porating both natural language understanding and
natural language generation.
Comprehension-based summarization, e.g.
Kintsch and Van Dijk (1978) and Brown et al.
(1983), is the most ambitious model of automatic
summarization, requiring a complete understand-
ing of the text. Due to the failure of rule-based
NLP and knowledge representation, other less
knowledge-intensive methods now dominate.
Sentence extraction, e.g. Brandow et al. (1995)
and Kupiec et al. (1995), selects a small number
of abstract worthy sentences from a larger text.
The resulting sentences form a collection of ex-
cerpt sentences meant to capture the essence of the
text. The next stage is information fusion (Barzi-
lay et al., 1999; Knight and Marcu, 2000) which
attempts to combine the excerpts into a more cohe-
sive text. These methods can create inflexible and
incoherent extracts that result in under-informative
results (Teufel et al., 1999).
Argumentative Zoning (Teufel, 1999; Teufel
and Moens, 2002) attempts to solve this prob-
lem by representing the structure of a text us-
ing a rhetorically-based schema. Sentences are
classified into one of a small number of non-
hierarchical argumentative roles, which can then
be used in both the sentence extraction and text
generation/fusion phase of automatic summariza-
tion. Argumentative Zoning can enable tailored
summarizations depending on the needs of the
user, e.g. a layperson versus a domain expert.
The first experiments in Argumentative Zon-
ing used Naive Bayes (NB) classifiers (Kupiec et
al., 1995; Teufel, 1999) which assume conditional
independence of the features. However, this as-
sumption is rarely true for the kinds of rich feature
representations we want to use for most NLP tasks.
Maximum entropy (ME) models have become
popular in NLP because they can incorporate evi-
dence from the complex, diverse and overlapping
features needed to represent language. Some ex-
ample applications include part-of-speech (POS)
tagging (Ratnaparkhi, 1996), parsing (Johnson et
al., 1999), language modelling (Rosenfeld, 1996),
and text categorisation (Nigam et al., 1999).
We have developed an Argumentative Zoning
(zone) classifier using a ME model. We compare
our zone classifier to a reimplementation of Teufel
and Moens (2002)’s NB classifier and features on
their original Computational Linguistics corpus.
Like Teufel (1999), we model zone classification
as a sequence tagging task. Our zone classifier
achieves an F-score of 96.88%, a 20% improve-
ment. We also show how Argumentative Zoning
can be applied to other domains by evaluating our
system on a corpus of Astronomy journal articles,
achieving an F-measure of 97.9%.
</bodyText>
<page confidence="0.992394">
19
</page>
<note confidence="0.999816">
Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, ACL-IJCNLP 2009, pages 19–26,
Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP
</note>
<figure confidence="0.880306909090909">
Category Abbr.
Background BKG
Other OTH
Own OWN
Description
general scientific background
neutral descriptions of other researcher’s work
neutral descriptions of the authors’ new work
Aim AIM
Textual TXT
Contrast CTR
</figure>
<subsectionHeader confidence="0.682939">
Basis BAS
</subsectionHeader>
<bodyText confidence="0.9959332">
statements of the particular aim of the current paper
statements of textual organisation of the current paper
contrastive or comparative statements about other work
explicit mention of weaknesses of other work
statements that own work is based on other work
</bodyText>
<tableCaption confidence="0.995673">
Table 1: Teufel’s (1999) Argumentative Zones
</tableCaption>
<sectionHeader confidence="0.952869" genericHeader="method">
2 Argumentative Zoning
</sectionHeader>
<bodyText confidence="0.99997275">
Teufel (1999) introduced a new rhetorical analy-
sis for scientific texts called Argumentative Zon-
ing. Each sentence of an article from the scien-
tific literature is classified into one of seven basic
rhetorical structures shown in Table 1.
The first three: Background, Other, and Own,
are part of the basic schema and represent attribu-
tion of intellectual ownership. The four additional
categories: aim, textual, contrast, and basis, are
based upon Swales (1990)’s Creating A Research
Space (CARS) model, and provide pointed infor-
mation about the author’s stance and the paper it-
self. Teufel assumes that each sentence only re-
quires a single classification and that all sentences
clearly fit into the above structure. The assump-
tion is clearly not always correct, but is a useful
approximation nevertheless.
Due to the specific nature of these classifica-
tions it is hoped that this will allow for much more
robust automatic abstraction generation. Sum-
maries of a paper could be created specifically for
the user, either focusing on the aim of the work,
the work’s stance in the field (what other works it
is based upon or compared with) and so on.
Teufel used Argumentative Zoning to determine
the author’s use and opinion of other authors they
cite in their work and also to create Rhetorical
Document Profiles (RDP), a type of summariza-
tion used to provide typical information that a new
reader may need in a systematic manner.
For the use of Argumentative Zoning in RDPs
Teufel (1999) points out that due to the redun-
dancy in language that near perfect accuracy is not
required as important pieces of information will be
repeated in the paper. Recognising these salient
points once is enough for them to be included in
the RDP. In further tasks, such as the analysis of
the function of citations (Teufel et al., 2006) and
automatic summarization, higher levels of accu-
racy are more critical.
</bodyText>
<sectionHeader confidence="0.979238" genericHeader="method">
3 Maximum Entropy models
</sectionHeader>
<bodyText confidence="0.982789">
Maximum entropy (ME) or log-linear models are
statistical models that can incorporate evidence
from a diverse range of complex and potentially
overlapping features. Unlike Naive Bayes (NB),
the features can be conditionally dependent given
the class, which is important since feature sets in
NLP rarely satisfy this independence constraint.
The ME classifier uses models of the form:
</bodyText>
<equation confidence="0.994433666666667">
n
p(y|x) = Z(x) exp(i=1
� λifi(x, y) (1)
</equation>
<bodyText confidence="0.982916666666667">
where y is the zone label, x is the context (the sen-
tence) and the fi(x, y) are the features with asso-
ciated weights λi.
The probability of a sequence of zone labels
y1 . . . yn given a sequence of sentences is s1 ... sn
is approximated as follows:
</bodyText>
<equation confidence="0.985479666666667">
n
p(y1 ... yn|s1 ... sn) � p(yi|xi) (2)
i=1
</equation>
<bodyText confidence="0.999528181818182">
where xi is the context for sentence si. In our ex-
periments that treat argumentative zoning as a se-
quence labelling task, the context xi incorporates
history information – i.e. the previous labelling
decisions of the classifier. Optimal decoding of
this sequence uses the Viterbi algorithm, which we
compare against the Oracle case of knowing the
correct label for the previous sentence.
The features are binary valued functions which
pair a zone label with various elements of the sen-
tential context; for example:
</bodyText>
<equation confidence="0.9398395">
x _( 1 if goal E x &amp; y = AIM 3
fj (X1 y) Sll 0 otherwise ( )
</equation>
<bodyText confidence="0.9807598">
goal E x, that is, the word goal is part of the
context of the sentence, is a contextual predicate.
The central idea in maximum entropy mod-
elling is that the model chosen should satisfy all of
the constraints imposed by the training data (in the
</bodyText>
<page confidence="0.96766">
20
</page>
<bodyText confidence="0.999990357142857">
form of empirical feature counts from the train-
ing data) whilst remaining as unbiased as possi-
ble. This is achieved by selecting the model with
the maximum entropy, i.e. the most uniform dis-
tribution, given the constraints.
Our classifier uses the maximum entropy imple-
mentation described in Curran and Clark (2003).
Generalised Iterative Scaling (GIS) is used to esti-
mate the values of the weights and we use a Gaus-
sian prior over the weights (Chen and Rosenfeld,
1999) which allows many rare, but informative,
features to be used without overfitting. This will
be an important property when we use sparse fea-
tures like bigrams in the models below.
</bodyText>
<sectionHeader confidence="0.905493" genericHeader="method">
4 Modelling Argumentative Zones
</sectionHeader>
<subsectionHeader confidence="0.986899">
4.1 Our Features
</subsectionHeader>
<bodyText confidence="0.995904754716981">
The two primary sources of features for our zone
classifier were the words in the sentences and the
position of the sentence relative to the rest of the
paper. A number of feature types use additional
external resources (e.g. semantic lists of agents or
common rhetorical patterns) or annotations (e.g.
named entities). Where feasible we have reimple-
mented the features described in Teufel (1999). In
other cases, our features are somewhat simpler.
Since the Curran and Clark (2003) classifier
only accepts binary features, any numerical fea-
tures had to be bucketed into smaller sets of alter-
natives to reduce sparseness, either by integer di-
vision or through reducing the number by scaling
to a small integer range. The features we imple-
mented are described below.
Unigrams, bigrams and n-grams
A sub-sequence of n words from a given sentence.
We include unigram and bigram features and re-
port them individually and together (as n-grams).
These features include all of the unigrams and
bigrams above the feature cutoff, unlike Teufel’s
cont-1 features below. Also, both the Compu-
tational Linguistics and Astronomy corpora con-
tain marked up citations, cross-references to ta-
bles, figures, and sections and mathematical ex-
pressions. In the Computational Linguistics cor-
pus self citations are distinguished from other ci-
tations. These structured elements have been nor-
malised to a single token each, e.g. __CITE__.
These tokens have been retained in the unigram
and bigram features.
first The first four words of a sentence, added in-
dividually.
Sections, positions, and lengths
section A section counter which increments on
each heading to measure the distance into the doc-
ument. It does not take into consideration whether
they are sub-headings or similar. There are two
versions of this feature. The first is a straight
counter (1 to n) and the second is grouped into two
buckets representing each half of the paper (break-
ing at the middle section).
location The position of a sentence between two
headings (representing a section). There are two
versions of this feature, one counts to a maxi-
mum of 10 and the other represents a percentage
through the section bucketed into 20% intervals.
paragraph The position of the sentence within a
paragraph. Again there are two features – either
straight counts (with a maximum of 10) or buck-
eted into thirds of a paragraph.
length of sentence grouped into multiples of 3.
</bodyText>
<subsectionHeader confidence="0.759393">
Named entity features
</subsectionHeader>
<bodyText confidence="0.999996625">
Our astronomy corpus has been manually anno-
tated with domain-specific named entity informa-
tion (Murphy et al., 2006). There are 12 coarse-
grained categories and 43 fine-grained categories
including star, galaxy, telescope, as well as a num-
ber of the usual categories including person, or-
ganisation and location. Both the coarse-grained
and fine-grained categories were used as features.
</bodyText>
<subsectionHeader confidence="0.99473">
4.2 Teufel (1999)’s features
</subsectionHeader>
<bodyText confidence="0.999735705882353">
To compare with previous work, we also im-
plemented most of the features that gave Teufel
(1999) the best performance. We list all of the fea-
ture types in Table 2, indicating which ones have
and have not been implemented.
Teufel’s unigram features (cont-1) are filtered
using TF-IDF to select the top scoring 10 words in
each document, and then these are used to mark
the top 40 sentences in each document containing
those filtered words.
TLoc marks the position of the sentence over
the entire paper, using 10 unevenly sized segments
(larger segments are in the middle of the paper).
Struct-1 marks where a sentence appears in a
section. It divides each section into three equally
sized segments; singles out the first and the last
sentence as separate segments; the second and
</bodyText>
<page confidence="0.995946">
21
</page>
<table confidence="0.999928866666667">
Name Impl? Description
Cont-1 yes An application of TF-IDF over the words and sentences
Cont-2 partial Does the sentence contain words in the title or heading (excluding stop words)
TLoc yes Position of the sentence in relation to 10 segments (A-J)
Struct-1 yes Position within a section
Struct-2 yes Relative position of sentence within a paragraph
Struct-3 partial Type of headline of the current section
TLength yes Is the sentence longer than 15 words?
Syn-1 no Voice of the first finite verb in the sentence
Syn-2 no Tense of the first finite verb in the sentence
Syn-3 no Is the first finite verb modified by a modal auxiliary
Cit-1 yes Does the sentence contain a citation or name of author?
Formu yes Does a formulaic expression occur in the sentence
Ag-1 yes Type of agent
Ag-2 yes Type of action (with or without negation)
</table>
<tableCaption confidence="0.998938">
Table 2: Teufel (1999)’s set of features
</tableCaption>
<bodyText confidence="0.999757444444444">
third sentence as a sixth segment; and the second-
last plus third-last sentence as a seventh segment.
Struct-3 the type of section heading for the cur-
rent section. In our case, we have not mapped
these down to the reduced set used by Teufel.
Formu uses pattern matching rules to iden-
tify formulaic expressions. Ag-1 and Ag-2 iden-
tify agent and action expressions from gazetteers.
Teufel (1999) provides these in the appendices.
</bodyText>
<subsectionHeader confidence="0.998355">
4.3 Feature Cutoff
</subsectionHeader>
<bodyText confidence="0.999978538461539">
Features that occur rarely in the training set are
problematic because the statistics extracted for
these features are not reliable. They may still con-
tribute positively to the ME model because we use
Gaussian smoothing (Chen and Rosenfeld, 1999)
help avoid overfitting.
Instead of including every possible feature, we
used a cutoff to remove features that occur less
than four times. This primarily applies to the
n-gram features, especially bigrams, which were
quite sparse given the small quantity of training
data. Due to the speed of the ME implementation
it is possible to have quite a low cut-off.
</bodyText>
<subsectionHeader confidence="0.999366">
4.4 History features and Viterbi
</subsectionHeader>
<bodyText confidence="0.999984769230769">
In order to take advantage of the predictabil-
ity of tags given prior sequences (for example,
AIM commonly following itself) we used history
features and treated Argumentative Zoning as a
sequence labelling task. Since each prediction
now relies on the previous decisions we used the
Viterbi algorithm to find the optimal sequence.
Given the small number of labelling alter-
natives, we experimented with several history
lengths ranging from previous label to the previ-
ous four labels. To determine the impact of this
feature in an ideal situation, we also experimented
with using an Oracle set of history features.
</bodyText>
<sectionHeader confidence="0.99998" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999965692307692">
Our results are produced using ten-fold cross val-
idation and are reported in terms of precision, re-
call and f-score for each of the zone classes, and a
weighted average over all classes. We have inves-
tigated the impact of each feature type using sub-
tractive analysis, where we have also calculated
paired t-test confidence intervals (the error values
reported are the 95% confidence interval).
The baselines for both sets were already quite
high (at least 70%) due to the common tag of
OWN, representing the author’s own work, but our
results show significant improvements over this
baseline.
</bodyText>
<subsectionHeader confidence="0.671173">
5.1 CMP-LG Corpus
</subsectionHeader>
<bodyText confidence="0.9920893125">
The CMP-LG corpus is a collection of 80 con-
ference papers collected by Teufel (1999) from
the Computation and Language E-Print Archive 1.
The LATEX source was converted to HTML with La-
tex2HTML then transformed into XML with cus-
tom PERL scripts. This text was then tokenized us-
ing the TTT (Text Tokenization) System into Penn
Treebank format. The result is a corpus of 12,000
annotated sentences, containing 333,000 word to-
kens, in XML format.
We attempted to recreate Teufel’s original ex-
periments by emulating the features she used with
the same type of classifier. We used Weka’s (Frank
et al., 2005) implementation of the NB classifier.
Table 3 reproduces the results from Teufel and
Moens (2002) alongside our reimplementation of
</bodyText>
<footnote confidence="0.992012">
1http://xxx.lanl.gov/cmp-lg/
</footnote>
<page confidence="0.996959">
22
</page>
<table confidence="0.9995989">
Tag original reproduced
P R F P R F
AIM 44 65 52 45.8 57.8 51.1
BAS 37 40 38 23.8 37.0 28.9
CTR 34 20 26 33.1 19.2 24.3
BKG 40 50 45 46.9 53.6 50.1
OTH 52 39 44 70.6 55.0 61.8
TXT 57 66 61 66.3 47.6 55.4
OWN 84 88 86 86.7 90.8 88.7
Weighted 72 73 72 76.8 76.8 76.8
</table>
<tableCaption confidence="0.9137405">
Table 3: Teufel and Moens (2002)’s and our NB
performance on CMP-LG
</tableCaption>
<table confidence="0.999869555555555">
History Type Order Performance
Baseline None 93.16
Viterbi First 1.77 ± 0.49%
Viterbi Second 1.97 ± 0.42%
Viterbi Third 2.08 ± 0.45%
Viterbi Fourth 2.1 ± 0.46%
Viterbi Fifth 2.13 ± 0.46%
Oracle First 3.67 ± 0.68%
Oracle Second 4.06 ± 0.70%
</table>
<tableCaption confidence="0.959644">
Table 4: History features on the CMP-LG corpus
with ME model of unigram/bigram features only
</tableCaption>
<table confidence="0.999844727272727">
Feature Classifier Viterbi
Ngrams -21.39±2.35% -23.23±3.24%
Unigram -8.00±1.02% -7.53±1.14%
Bigram -7.89±1.20% -6.87±1.44%
Concept -0.06±0.24% -0.06±0.16%
First -1.24±0.44% -1.14±0.39%
Length -0.34±0.24% -0.40±0.25%
Section -0.42±0.27% -0.27±0.33%
Location 0.03±0.20% 0.04±0.07%
Paragraph 0.10±0.15% 0.01±0.08%
All 95.69% 96.88%
</table>
<tableCaption confidence="0.999768">
Table 5: Subtractive analysis CMP-LG ME model
</tableCaption>
<bodyText confidence="0.999937166666666">
the features using Weka’s NB classifier. We have
been able to replicate their results to a reasonable
extent – gaining higher overall performance using
most of their original features. Notably, our Other
class is significantly more accurate whilst the orig-
inal Basis class did better.
Our next experiment investigated the value of
treating Argumentative Zoning as a sequence la-
belling task, i.e. the impact of the Markov history
features and Viterbi decoding on performance. For
these experiments we only used the unigram and
bigram features with the maximum entropy clas-
sifier. Table 4 presents the results: the baseline is
already much higher than the NB classifier which
is a result of both the unigram/bigram features and
the ME classifier itself.
The improvement using longer Markov win-
dows (up to 2.13%) is also shown – and longer
windows are better, although there is diminishing
returns. We chose a Markov history of the four
previous decisions for the rest of our experiments.
Table 4 also shows that knowing the previous label
perfectly (with the Oracle experiment) can make a
large difference to classification accuracy.
</bodyText>
<table confidence="0.99893175">
Feature Change
TLength -2.09±9.96%
Struct-1 0.38±6.08%
TLoc 0.96±7.25%
Struct-3 -1.65±6.76%
Cont-2 -1.10±6.39%
Struct-2 1.59±7.99%
Ag-1/2 -0.39±8.97%
Formu 0.14±8.46%
Cit-1 -1.88±5.19%
Cont-1 -0.38±5.85%
All 70.25%
</table>
<tableCaption confidence="0.996309">
Table 6: Teufel’s Subtractive analysis CMP-LG ME
</tableCaption>
<bodyText confidence="0.999356857142857">
Table 5 presents the subtractive analysis to de-
termine the impact of different feature types. From
this we can see that the n-grams (unigrams and bi-
grams) have by far the largest impact – and neither
of these feature types was directly implemented by
Teufel and Moens (2002). The next most impor-
tant features are the first few words (again a uni-
gram type feature), length and the section number.
The Markov history features also have an impact
of just over 1%.
Table 6 shows a different story for Teufel’s fea-
tures using the maximum entropy model. It seems
that none of the feature types alone are making an
enormous contribution and that the impact of them
varies enormously between folds (the confidence
intervals are far bigger than the differences).
Finally, Table 7 gives the results of using the
maximum entropy model with Markov history
length four and all of the features. Overall, we
improve Teufel and Moens’ performance by just
under 20% on our reproduced experiments.
</bodyText>
<subsectionHeader confidence="0.999354">
5.2 Astronomical Corpus
</subsectionHeader>
<bodyText confidence="0.9996189">
The astronomical corpus was created by Mur-
phy et al. (2006) and consists of papers obtained
from arXiv (2005)’s astrophysics section (astro-
ph). The papers were converted from LATEX to
Unicode by a custom script which attempted to re-
tain as much of the paper’s special characters and
formatting as possible.
The resulting text was then processed using
MXTerminator (Reynar and Ratnaparkhi, 1997)
with an additional Python script to find sentence
</bodyText>
<page confidence="0.99529">
23
</page>
<bodyText confidence="0.949119636363636">
Description
As has been noted in prior studies , Abell|GXYC 2255|GXYC has an unusually large number
of galaxies with extended radio emission.
This is consistent with the findings of Hogg|P Fruchter|P ( 1999|DAT ) who found that GRB
hosts are in general subluminous galaxies .
We scanned the data of about 1.8|DUR year|DUR ( TJDs|DUR 11000-11699|DUR ) and found
30 new GRB-like events.
In Fig . REF we present the 1.4|FRQ GHz|FRQ radio images of the cluster A2744|GXYC ,
at different angular resolutions . (subclassed from OWN)
Smith|P et al. ( 2001|DAT ) reported no detection of transient emission at sub-mm ( 850|WAV
um|WAV ) wavelengths . (subclassed from OTH)
</bodyText>
<figure confidence="0.930079777777778">
Reduction of the NIR images was performed with the IRAF|CODE and STSDAS|CODE pack-
ages. (subclassed from OWN)
Category Abbr.
Background BKG
Other OTH
Own OWN
Data DAT
Observation OBS
Technique TEC
</figure>
<figureCaption confidence="0.99929">
Figure 1: Examples of sentences with the given tags in the astronomical corpus
</figureCaption>
<table confidence="0.999902888888889">
Tag P R F
AIM 96.5 88.2 92.2
BAS 86.7 89.8 88.2
CTR 92.1 89.0 90.5
BKG 86.0 96.3 90.9
OTH 96.3 91.7 93.9
TXT 98.2 93.8 95.9
OWN 98.6 99.2 98.9
Weighted 96.88 96.88 96.88
</table>
<tableCaption confidence="0.849811">
Table 7: Final CMP-LG ME performance
</tableCaption>
<table confidence="0.999932666666667">
Feature Classifier Viterbi
Ngrams -18.83±3.74% -16.03±2.99%
Unigram -5.51±1.37% -5.25±2.00%
Bigram -2.04±0.78% -1.79±0.87%
Concept -0.18±0.29% -0.05±0.12%
Entity -0.18±0.39% -0.31±0.23%
First -0.02±0.29% -0.86±0.79%
Length -0.06±0.16% -0.08±0.10%
Paragraph -0.04±0.20% 0.07±0.19%
Section -0.29±0.24% -0.40±0.57%
Location -0.09±0.25% 0.06±0.15%
All 98.15% 96.68%
</table>
<tableCaption confidence="0.999816">
Table 8: Subtractive analysis ASTRO ME model
</tableCaption>
<bodyText confidence="0.998144125">
boundaries, and then tokenized using the Penn
Treebank (Marcus et al., 1993) sed script, with an-
other Python script fixing common errors. The
LATEX, which the tokenizer split off incorrectly,
was then reattached.
Each sentence of the corpus was then anno-
tated using a modified version of the Argumen-
tative Zoning schema. While the original three
zones: Background, Own, Other are used, we have
replaced the CARS labels with content labels de-
scribing aspects of the work: DAT for data used
in the analysis, OBS for observations performed,
and TEC for techniques applied. Only Own and
Other are subclassed with the extended schema of
Data, Observation and Techniques. Examples of
each zone classification are shown in Figure 1.
</bodyText>
<table confidence="0.999875727272727">
Tag P R F
BKG 92.1 97.1 94.5
OTH 95.0 97.1 96.1
OTH-DAT 100.0 92.3 96.0
OTH-OBS 91.3 93.3 92.3
OTH-TEC 100.0 100.0 100.0
OWN 99.9 99.3 99.6
OWN-DAT 95.9 86.6 91.0
OWN-OBS 98.2 89.4 93.6
OWN-TEC 90.4 100.0 94.9
Weighted 97.9 97.9 97.9
</table>
<tableCaption confidence="0.998995">
Table 9: Final ASTRO ME model performance
</tableCaption>
<bodyText confidence="0.99840976923077">
Table 8 shows the impact of different feature
types on classification accuracy for the Astron-
omy corpus. Again, the most important features
are the n-grams (although to a slightly lesser ex-
tent than for the Computational Linguistics cor-
pus). The other features make very little contri-
bution at all. Disappointingly, the (gold-standard)
named entity features contribute very little addi-
tional information – which is surprising given that
the content categories (data and observation) are
directly connected with some of the entity types
(like telescope).
In the Astronomy corpus, the Markov history
features actually have a detrimental effect, which
suggests the history is misleading. This warrants
further exploration, but we suspect there may be
more changing backwards and forwards between
argumentative zones in the Astronomy corpus.
Overall, we can see that the two tasks are of a sim-
ilar level of difficulty of around 96% F-score.
Table 9 shows the distribution over zones and
content labels for the Astronomy corpus. The
Background label is the hardest to reproduce even
though it is not split into content sub-types. The
sub-types are relatively rare for Other, so the re-
sults should not be considered as reliable.
</bodyText>
<page confidence="0.997683">
24
</page>
<table confidence="0.999941307692308">
Tag P R F
BKG NB CMP-LG 51.5 % 61.1 % 55.9 %
OTH NB CMP-LG 73.0 % 64.2 % 68.3 %
OWN NB CMP-LG 91.9 % 93.1 % 92.5 %
BKG NB ASTRO 63.1 % 63.5 % 63.3 %
OTH NB ASTRO 53.9 % 39.7 % 45.7 %
OWN NB ASTRO 88.5 % 93.0 % 90.7 %
BKG ME CMP-LG 53.6 % 27.5 % 36.3 %
OTH ME CMP-LG 63.0 % 24.4 % 35.2 %
OWN ME CMP-LG 81.7 % 96.8 % 88.6 %
BKG ME ASTRO 61.2 % 29.5 % 39.8 %
OTH ME ASTRO 50.4 % 20.0 % 28.6 %
OWN ME ASTRO 81.2 % 96.7 % 88.2 %
</table>
<tableCaption confidence="0.808790666666667">
Table 10: Comparing CMP-LG and ASTRO directly
on the basic annotation scheme
Table 10 compares the performance of our
</tableCaption>
<bodyText confidence="0.996573111111111">
Naive Bayes and Maximum Entropy classifiers
on the two corpora for just the basic annotation
scheme: Background, Own and Other. The fea-
tures used are the set of Teufel features we have
implemented (so it does not include unigram or
bigram features).
The results show that classifiers for both cor-
pora behave in quite similar ways on the basic
scheme. Own is by far the most frequent category,
and not surprisingly, it is most accurately classi-
fied in both domains. Background seems to be eas-
ier to distinguish in Astronomy, but Other is more
distinct in Computational Linguistics.
Further, we see no advantage to using maximum
entropy models over Naive Bayes when the fea-
ture set is not sophisticated/overlapping enough,
and the dataset large enough, to warrant the extra
power (and cost).
</bodyText>
<sectionHeader confidence="0.999541" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999988225806452">
This paper has presented new models of Argumen-
tative Zoning using Maximum Entropy (ME) mod-
els. We have demonstrated that using ME models
with standard word features, such as unigrams and
bigrams, significantly outperforms Naive Bayes
models incorporating task-specific features. Fur-
ther, these task-specific features had very little ad-
ditional impact on the ME model.
Our ME model has raised the state-of-the-art
in automatic Argumentative Zoning classification
from 76% to 96.88% F-score on Teufel’s Compu-
tational Linguistics conference paper corpus.
To test the wider applicability of Argumentative
Zoning, we have annotated a corpus of Astronomy
journal articles with a modified zone and content
scheme, and achieved a similar level of perfor-
mance using our maximum entropy classifier. We
found that more sophisticated semantic features,
e.g. gold-standard named entities, also had little
impact on the accuracy of our classifier.
Now that we have a very accurate Argumenta-
tive Zone classifier, we would like to investigate
the impact of Argumentative Zones in information
retrieval, question answering, and summarization
tasks, particularly in the astronomy domain, where
we have additional tools such as the named entity
recognizer.
In summary, using a maximum entropy classi-
fier with simple unigram and bigram features re-
sults in a very accurate classifier for Argumenta-
tive Zones across multiple domains.
</bodyText>
<sectionHeader confidence="0.998379" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999986857142857">
We would like to thank Sophie Liang and the
anonymous reviewers for their helpful feedback
on this paper. This work has been supported by
the Australian Research Council under Discovery
project DP0665973. The first author was sup-
ported by the Microsoft Research Asia Scholar-
ship in IT at the University of Sydney.
</bodyText>
<sectionHeader confidence="0.999383" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99921916">
arXiv. 2005. arxiv.org archive. http://arxiv.org.
R. Barzilay, K.R. McKeown, and M. Elhadad. 1999.
Information fusion in the context of multi-document
summarization. In Proceedings of the 37th an-
nual meeting of the Association for Computational
Linguistics on Computational Linguistics, pages
550–557. Association for Computational Linguistics
Morristown, NJ, USA.
R. Brandow, K. Mitze, and L.F. Rau. 1995. Automatic
condensation of electronic publications by sentence
selection. Information Processing and management,
31(5):675–685.
A.L. Brown, J.D. Day, and R.S. Jones. 1983. The
development of plans for summarizing texts. Child
Development, pages 968–979.
Stanley Chen and Ronald Rosenfeld. 1999. A Gaus-
sian prior for smoothing maximum entropy models.
Technical report, Carnegie Mellon University, Pitts-
burgh, PA.
James R. Curran and Stephen Clark. 2003. Investigat-
ing GIS and smoothing for maximum entropy tag-
gers. In Proceedings of the 10th Conference of the
European Chapter of the Association for Computa-
tional Linguistics, pages 91–98, Budapest, Hungary,
12–17 April.
</reference>
<page confidence="0.96453">
25
</page>
<reference confidence="0.999321338709678">
E. Frank, M.A. Hall, G. Holmes, R. Kirkby,
B. Pfahringer, I.H. Witten, and L. Trigg. 2005.
Weka-a machine learning workbench for data min-
ing. The Data Mining and Knowledge Discovery
Handbook, pages 1305–1314.
M. Johnson, S. Geman, S. Canon, Z. Chi, and S. Rie-
zler. 1999. Estimators for stochastic ‘unification-
based’ grammars. In Proceedings of the 37th Meet-
ing of the ACL, pages 535–541, University of Mary-
land, MD.
W. Kintsch and T.A. Van Dijk. 1978. Toward a model
of text comprehension and production. Psychologi-
cal review, 85(5):363–94.
K. Knight and D. Marcu. 2000. Statistics-based
summarization-step one: Sentence compression. In
Proceedings of the National Conference on Artifi-
cial Intelligence, pages 703–710. Menlo Park, CA;
Cambridge, MA; London; AAAI Press; MIT Press;
1999.
J. Kupiec, J. Pedersen, and F. Chen. 1995. A train-
able document summarizer. In Proceedings of the
18th annual international ACM SIGIR conference
on Research and development in information re-
trieval, pages 68–73. ACM New York, NY, USA.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of english: The
penn treebank.
T. Murphy, T. McIntosh, and J.R. Curran. 2006.
Named entity recognition for astronomy literature.
In Proceedings of the 2006 Australasian Language
Technology Workshop (ALTW).
K. Nigam, J. Lafferty, and A. McCallum. 1999. Us-
ing maximum entropy for text classification. In
Proceedings of the IJCAI-99 Workshop on Machine
Learning for Information Filtering, pages 61–67,
Stockholm, Sweden.
Adwait Ratnaparkhi. 1996. A maximum entropy part-
of-speech tagger. In Proceedings of the EMNLP
Conference, pages 133–142, Philadelphia, PA.
J.C. Reynar and A. Ratnaparkhi. 1997. A maximum
entropy approach to identifying sentence bound-
aries. In Proceedings of the fifth conference on Ap-
plied natural language processing, pages 16–19.
R. Rosenfeld. 1996. A maximum entropy approach to
adaptive statistical language modeling. Computer,
Speech and Language, 10:187–228.
J.M. Swales. 1990. Genre analysis: English in aca-
demic and research settings. Cambridge University
Press.
S. Teufel and M. Moens. 2002. Summarising scientific
articles — experiments with relevance and rhetorical
status. Computational Linguistics, 28(4):409–445.
S. Teufel, J. Carletta, and M. Moens. 1999. An anno-
tation scheme for discourse-level argumentation in
research articles. In Proceedings of EACL 1999.
S. Teufel, A. Siddharthan, and D. Tidhar. 2006. Auto-
matic classification of citation function. In Proceed-
ings of the 2006 Conference on Empirical Methods
in Natural Language Processing, pages 103–110.
S. Teufel. 1999. Argumentative zoning: Information
extraction from scientific text. Ph.D. thesis, Univer-
sity of Edinburgh, Edinburgh, UK.
</reference>
<page confidence="0.998092">
26
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.917653">
<title confidence="0.995143">Accurate Argumentative Zoning with Maximum Entropy models</title>
<author confidence="0.9628">Merity Murphy R</author>
<affiliation confidence="0.998231">School of Information University of</affiliation>
<address confidence="0.971607">NSW 2006, Australia</address>
<abstract confidence="0.99891">We present a maximum entropy classifier that significantly improves the accuracy of Zoning scientific literature. We examine the features used to achieve this result and experiment with Argumentative Zoning as a sequence tagging task, decoded with Viterbi using up to four previous classification decisions. The result is a 23% F-score increase on the Computational Linguistics conference papers marked up by Teufel (1999). Finally, we demonstrate the performance of our system in different scientific domains by applying it to a corpus of Astronomy journal articles annotated using a modified Argumentative Zoning scheme.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>arXiv</author>
</authors>
<date>2005</date>
<note>arxiv.org archive. http://arxiv.org.</note>
<contexts>
<context position="19810" citStr="arXiv (2005)" startWordPosition="3210" endWordPosition="3211">el’s features using the maximum entropy model. It seems that none of the feature types alone are making an enormous contribution and that the impact of them varies enormously between folds (the confidence intervals are far bigger than the differences). Finally, Table 7 gives the results of using the maximum entropy model with Markov history length four and all of the features. Overall, we improve Teufel and Moens’ performance by just under 20% on our reproduced experiments. 5.2 Astronomical Corpus The astronomical corpus was created by Murphy et al. (2006) and consists of papers obtained from arXiv (2005)’s astrophysics section (astroph). The papers were converted from LATEX to Unicode by a custom script which attempted to retain as much of the paper’s special characters and formatting as possible. The resulting text was then processed using MXTerminator (Reynar and Ratnaparkhi, 1997) with an additional Python script to find sentence 23 Description As has been noted in prior studies , Abell|GXYC 2255|GXYC has an unusually large number of galaxies with extended radio emission. This is consistent with the findings of Hogg|P Fruchter|P ( 1999|DAT ) who found that GRB hosts are in general sublumin</context>
</contexts>
<marker>arXiv, 2005</marker>
<rawString>arXiv. 2005. arxiv.org archive. http://arxiv.org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>K R McKeown</author>
<author>M Elhadad</author>
</authors>
<title>Information fusion in the context of multi-document summarization.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics,</booktitle>
<pages>550--557</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1827" citStr="Barzilay et al., 1999" startWordPosition="265" endWordPosition="269">hension-based summarization, e.g. Kintsch and Van Dijk (1978) and Brown et al. (1983), is the most ambitious model of automatic summarization, requiring a complete understanding of the text. Due to the failure of rule-based NLP and knowledge representation, other less knowledge-intensive methods now dominate. Sentence extraction, e.g. Brandow et al. (1995) and Kupiec et al. (1995), selects a small number of abstract worthy sentences from a larger text. The resulting sentences form a collection of excerpt sentences meant to capture the essence of the text. The next stage is information fusion (Barzilay et al., 1999; Knight and Marcu, 2000) which attempts to combine the excerpts into a more cohesive text. These methods can create inflexible and incoherent extracts that result in under-informative results (Teufel et al., 1999). Argumentative Zoning (Teufel, 1999; Teufel and Moens, 2002) attempts to solve this problem by representing the structure of a text using a rhetorically-based schema. Sentences are classified into one of a small number of nonhierarchical argumentative roles, which can then be used in both the sentence extraction and text generation/fusion phase of automatic summarization. Argumentat</context>
</contexts>
<marker>Barzilay, McKeown, Elhadad, 1999</marker>
<rawString>R. Barzilay, K.R. McKeown, and M. Elhadad. 1999. Information fusion in the context of multi-document summarization. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, pages 550–557. Association for Computational Linguistics Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Brandow</author>
<author>K Mitze</author>
<author>L F Rau</author>
</authors>
<title>Automatic condensation of electronic publications by sentence selection.</title>
<date>1995</date>
<booktitle>Information Processing and management,</booktitle>
<pages>31--5</pages>
<contexts>
<context position="1564" citStr="Brandow et al. (1995)" startWordPosition="220" endWordPosition="223">re texts is a central problem in Natural Language Processing (NLP). Summarization is a fundamental component for future information retrieval and question answering systems, incorporating both natural language understanding and natural language generation. Comprehension-based summarization, e.g. Kintsch and Van Dijk (1978) and Brown et al. (1983), is the most ambitious model of automatic summarization, requiring a complete understanding of the text. Due to the failure of rule-based NLP and knowledge representation, other less knowledge-intensive methods now dominate. Sentence extraction, e.g. Brandow et al. (1995) and Kupiec et al. (1995), selects a small number of abstract worthy sentences from a larger text. The resulting sentences form a collection of excerpt sentences meant to capture the essence of the text. The next stage is information fusion (Barzilay et al., 1999; Knight and Marcu, 2000) which attempts to combine the excerpts into a more cohesive text. These methods can create inflexible and incoherent extracts that result in under-informative results (Teufel et al., 1999). Argumentative Zoning (Teufel, 1999; Teufel and Moens, 2002) attempts to solve this problem by representing the structure </context>
</contexts>
<marker>Brandow, Mitze, Rau, 1995</marker>
<rawString>R. Brandow, K. Mitze, and L.F. Rau. 1995. Automatic condensation of electronic publications by sentence selection. Information Processing and management, 31(5):675–685.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Brown</author>
<author>J D Day</author>
<author>R S Jones</author>
</authors>
<title>The development of plans for summarizing texts. Child Development,</title>
<date>1983</date>
<pages>968--979</pages>
<contexts>
<context position="1291" citStr="Brown et al. (1983)" startWordPosition="181" endWordPosition="184">ally, we demonstrate the performance of our system in different scientific domains by applying it to a corpus of Astronomy journal articles annotated using a modified Argumentative Zoning scheme. 1 Introduction The task of generating automatic summarizations of one or more texts is a central problem in Natural Language Processing (NLP). Summarization is a fundamental component for future information retrieval and question answering systems, incorporating both natural language understanding and natural language generation. Comprehension-based summarization, e.g. Kintsch and Van Dijk (1978) and Brown et al. (1983), is the most ambitious model of automatic summarization, requiring a complete understanding of the text. Due to the failure of rule-based NLP and knowledge representation, other less knowledge-intensive methods now dominate. Sentence extraction, e.g. Brandow et al. (1995) and Kupiec et al. (1995), selects a small number of abstract worthy sentences from a larger text. The resulting sentences form a collection of excerpt sentences meant to capture the essence of the text. The next stage is information fusion (Barzilay et al., 1999; Knight and Marcu, 2000) which attempts to combine the excerpts</context>
</contexts>
<marker>Brown, Day, Jones, 1983</marker>
<rawString>A.L. Brown, J.D. Day, and R.S. Jones. 1983. The development of plans for summarizing texts. Child Development, pages 968–979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Chen</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>A Gaussian prior for smoothing maximum entropy models.</title>
<date>1999</date>
<tech>Technical report,</tech>
<institution>Carnegie Mellon University,</institution>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="8441" citStr="Chen and Rosenfeld, 1999" startWordPosition="1349" endWordPosition="1352">cate. The central idea in maximum entropy modelling is that the model chosen should satisfy all of the constraints imposed by the training data (in the 20 form of empirical feature counts from the training data) whilst remaining as unbiased as possible. This is achieved by selecting the model with the maximum entropy, i.e. the most uniform distribution, given the constraints. Our classifier uses the maximum entropy implementation described in Curran and Clark (2003). Generalised Iterative Scaling (GIS) is used to estimate the values of the weights and we use a Gaussian prior over the weights (Chen and Rosenfeld, 1999) which allows many rare, but informative, features to be used without overfitting. This will be an important property when we use sparse features like bigrams in the models below. 4 Modelling Argumentative Zones 4.1 Our Features The two primary sources of features for our zone classifier were the words in the sentences and the position of the sentence relative to the rest of the paper. A number of feature types use additional external resources (e.g. semantic lists of agents or common rhetorical patterns) or annotations (e.g. named entities). Where feasible we have reimplemented the features d</context>
<context position="13868" citStr="Chen and Rosenfeld, 1999" startWordPosition="2244" endWordPosition="2247">st sentence as a seventh segment. Struct-3 the type of section heading for the current section. In our case, we have not mapped these down to the reduced set used by Teufel. Formu uses pattern matching rules to identify formulaic expressions. Ag-1 and Ag-2 identify agent and action expressions from gazetteers. Teufel (1999) provides these in the appendices. 4.3 Feature Cutoff Features that occur rarely in the training set are problematic because the statistics extracted for these features are not reliable. They may still contribute positively to the ME model because we use Gaussian smoothing (Chen and Rosenfeld, 1999) help avoid overfitting. Instead of including every possible feature, we used a cutoff to remove features that occur less than four times. This primarily applies to the n-gram features, especially bigrams, which were quite sparse given the small quantity of training data. Due to the speed of the ME implementation it is possible to have quite a low cut-off. 4.4 History features and Viterbi In order to take advantage of the predictability of tags given prior sequences (for example, AIM commonly following itself) we used history features and treated Argumentative Zoning as a sequence labelling ta</context>
</contexts>
<marker>Chen, Rosenfeld, 1999</marker>
<rawString>Stanley Chen and Ronald Rosenfeld. 1999. A Gaussian prior for smoothing maximum entropy models. Technical report, Carnegie Mellon University, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Stephen Clark</author>
</authors>
<title>Investigating GIS and smoothing for maximum entropy taggers.</title>
<date>2003</date>
<booktitle>In Proceedings of the 10th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>91--98</pages>
<location>Budapest,</location>
<contexts>
<context position="8286" citStr="Curran and Clark (2003)" startWordPosition="1321" endWordPosition="1324">_( 1 if goal E x &amp; y = AIM 3 fj (X1 y) Sll 0 otherwise ( ) goal E x, that is, the word goal is part of the context of the sentence, is a contextual predicate. The central idea in maximum entropy modelling is that the model chosen should satisfy all of the constraints imposed by the training data (in the 20 form of empirical feature counts from the training data) whilst remaining as unbiased as possible. This is achieved by selecting the model with the maximum entropy, i.e. the most uniform distribution, given the constraints. Our classifier uses the maximum entropy implementation described in Curran and Clark (2003). Generalised Iterative Scaling (GIS) is used to estimate the values of the weights and we use a Gaussian prior over the weights (Chen and Rosenfeld, 1999) which allows many rare, but informative, features to be used without overfitting. This will be an important property when we use sparse features like bigrams in the models below. 4 Modelling Argumentative Zones 4.1 Our Features The two primary sources of features for our zone classifier were the words in the sentences and the position of the sentence relative to the rest of the paper. A number of feature types use additional external resour</context>
</contexts>
<marker>Curran, Clark, 2003</marker>
<rawString>James R. Curran and Stephen Clark. 2003. Investigating GIS and smoothing for maximum entropy taggers. In Proceedings of the 10th Conference of the European Chapter of the Association for Computational Linguistics, pages 91–98, Budapest, Hungary, 12–17 April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Frank</author>
<author>M A Hall</author>
<author>G Holmes</author>
<author>R Kirkby</author>
<author>B Pfahringer</author>
<author>I H Witten</author>
<author>L Trigg</author>
</authors>
<title>Weka-a machine learning workbench for data mining. The Data Mining and Knowledge Discovery Handbook,</title>
<date>2005</date>
<pages>1305--1314</pages>
<contexts>
<context position="16096" citStr="Frank et al., 2005" startWordPosition="2611" endWordPosition="2614">.1 CMP-LG Corpus The CMP-LG corpus is a collection of 80 conference papers collected by Teufel (1999) from the Computation and Language E-Print Archive 1. The LATEX source was converted to HTML with Latex2HTML then transformed into XML with custom PERL scripts. This text was then tokenized using the TTT (Text Tokenization) System into Penn Treebank format. The result is a corpus of 12,000 annotated sentences, containing 333,000 word tokens, in XML format. We attempted to recreate Teufel’s original experiments by emulating the features she used with the same type of classifier. We used Weka’s (Frank et al., 2005) implementation of the NB classifier. Table 3 reproduces the results from Teufel and Moens (2002) alongside our reimplementation of 1http://xxx.lanl.gov/cmp-lg/ 22 Tag original reproduced P R F P R F AIM 44 65 52 45.8 57.8 51.1 BAS 37 40 38 23.8 37.0 28.9 CTR 34 20 26 33.1 19.2 24.3 BKG 40 50 45 46.9 53.6 50.1 OTH 52 39 44 70.6 55.0 61.8 TXT 57 66 61 66.3 47.6 55.4 OWN 84 88 86 86.7 90.8 88.7 Weighted 72 73 72 76.8 76.8 76.8 Table 3: Teufel and Moens (2002)’s and our NB performance on CMP-LG History Type Order Performance Baseline None 93.16 Viterbi First 1.77 ± 0.49% Viterbi Second 1.97 ± 0.4</context>
</contexts>
<marker>Frank, Hall, Holmes, Kirkby, Pfahringer, Witten, Trigg, 2005</marker>
<rawString>E. Frank, M.A. Hall, G. Holmes, R. Kirkby, B. Pfahringer, I.H. Witten, and L. Trigg. 2005. Weka-a machine learning workbench for data mining. The Data Mining and Knowledge Discovery Handbook, pages 1305–1314.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
<author>S Geman</author>
<author>S Canon</author>
<author>Z Chi</author>
<author>S Riezler</author>
</authors>
<title>Estimators for stochastic ‘unificationbased’ grammars.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Meeting of the ACL,</booktitle>
<pages>535--541</pages>
<institution>University of Maryland, MD.</institution>
<contexts>
<context position="3130" citStr="Johnson et al., 1999" startWordPosition="465" endWordPosition="468">e.g. a layperson versus a domain expert. The first experiments in Argumentative Zoning used Naive Bayes (NB) classifiers (Kupiec et al., 1995; Teufel, 1999) which assume conditional independence of the features. However, this assumption is rarely true for the kinds of rich feature representations we want to use for most NLP tasks. Maximum entropy (ME) models have become popular in NLP because they can incorporate evidence from the complex, diverse and overlapping features needed to represent language. Some example applications include part-of-speech (POS) tagging (Ratnaparkhi, 1996), parsing (Johnson et al., 1999), language modelling (Rosenfeld, 1996), and text categorisation (Nigam et al., 1999). We have developed an Argumentative Zoning (zone) classifier using a ME model. We compare our zone classifier to a reimplementation of Teufel and Moens (2002)’s NB classifier and features on their original Computational Linguistics corpus. Like Teufel (1999), we model zone classification as a sequence tagging task. Our zone classifier achieves an F-score of 96.88%, a 20% improvement. We also show how Argumentative Zoning can be applied to other domains by evaluating our system on a corpus of Astronomy journal </context>
</contexts>
<marker>Johnson, Geman, Canon, Chi, Riezler, 1999</marker>
<rawString>M. Johnson, S. Geman, S. Canon, Z. Chi, and S. Riezler. 1999. Estimators for stochastic ‘unificationbased’ grammars. In Proceedings of the 37th Meeting of the ACL, pages 535–541, University of Maryland, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Kintsch</author>
<author>T A Van Dijk</author>
</authors>
<title>Toward a model of text comprehension and production. Psychological review,</title>
<date>1978</date>
<pages>85--5</pages>
<marker>Kintsch, Van Dijk, 1978</marker>
<rawString>W. Kintsch and T.A. Van Dijk. 1978. Toward a model of text comprehension and production. Psychological review, 85(5):363–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>D Marcu</author>
</authors>
<title>Statistics-based summarization-step one: Sentence compression.</title>
<date>2000</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence,</booktitle>
<pages>703--710</pages>
<publisher>AAAI Press; MIT Press;</publisher>
<location>Menlo Park, CA; Cambridge, MA; London;</location>
<contexts>
<context position="1852" citStr="Knight and Marcu, 2000" startWordPosition="270" endWordPosition="273">tion, e.g. Kintsch and Van Dijk (1978) and Brown et al. (1983), is the most ambitious model of automatic summarization, requiring a complete understanding of the text. Due to the failure of rule-based NLP and knowledge representation, other less knowledge-intensive methods now dominate. Sentence extraction, e.g. Brandow et al. (1995) and Kupiec et al. (1995), selects a small number of abstract worthy sentences from a larger text. The resulting sentences form a collection of excerpt sentences meant to capture the essence of the text. The next stage is information fusion (Barzilay et al., 1999; Knight and Marcu, 2000) which attempts to combine the excerpts into a more cohesive text. These methods can create inflexible and incoherent extracts that result in under-informative results (Teufel et al., 1999). Argumentative Zoning (Teufel, 1999; Teufel and Moens, 2002) attempts to solve this problem by representing the structure of a text using a rhetorically-based schema. Sentences are classified into one of a small number of nonhierarchical argumentative roles, which can then be used in both the sentence extraction and text generation/fusion phase of automatic summarization. Argumentative Zoning can enable tai</context>
</contexts>
<marker>Knight, Marcu, 2000</marker>
<rawString>K. Knight and D. Marcu. 2000. Statistics-based summarization-step one: Sentence compression. In Proceedings of the National Conference on Artificial Intelligence, pages 703–710. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kupiec</author>
<author>J Pedersen</author>
<author>F Chen</author>
</authors>
<title>A trainable document summarizer.</title>
<date>1995</date>
<booktitle>In Proceedings of the 18th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>68--73</pages>
<publisher>ACM</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1589" citStr="Kupiec et al. (1995)" startWordPosition="225" endWordPosition="228">lem in Natural Language Processing (NLP). Summarization is a fundamental component for future information retrieval and question answering systems, incorporating both natural language understanding and natural language generation. Comprehension-based summarization, e.g. Kintsch and Van Dijk (1978) and Brown et al. (1983), is the most ambitious model of automatic summarization, requiring a complete understanding of the text. Due to the failure of rule-based NLP and knowledge representation, other less knowledge-intensive methods now dominate. Sentence extraction, e.g. Brandow et al. (1995) and Kupiec et al. (1995), selects a small number of abstract worthy sentences from a larger text. The resulting sentences form a collection of excerpt sentences meant to capture the essence of the text. The next stage is information fusion (Barzilay et al., 1999; Knight and Marcu, 2000) which attempts to combine the excerpts into a more cohesive text. These methods can create inflexible and incoherent extracts that result in under-informative results (Teufel et al., 1999). Argumentative Zoning (Teufel, 1999; Teufel and Moens, 2002) attempts to solve this problem by representing the structure of a text using a rhetori</context>
</contexts>
<marker>Kupiec, Pedersen, Chen, 1995</marker>
<rawString>J. Kupiec, J. Pedersen, and F. Chen. 1995. A trainable document summarizer. In Proceedings of the 18th annual international ACM SIGIR conference on Research and development in information retrieval, pages 68–73. ACM New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<contexts>
<context position="21778" citStr="Marcus et al., 1993" startWordPosition="3512" endWordPosition="3515"> 86.0 96.3 90.9 OTH 96.3 91.7 93.9 TXT 98.2 93.8 95.9 OWN 98.6 99.2 98.9 Weighted 96.88 96.88 96.88 Table 7: Final CMP-LG ME performance Feature Classifier Viterbi Ngrams -18.83±3.74% -16.03±2.99% Unigram -5.51±1.37% -5.25±2.00% Bigram -2.04±0.78% -1.79±0.87% Concept -0.18±0.29% -0.05±0.12% Entity -0.18±0.39% -0.31±0.23% First -0.02±0.29% -0.86±0.79% Length -0.06±0.16% -0.08±0.10% Paragraph -0.04±0.20% 0.07±0.19% Section -0.29±0.24% -0.40±0.57% Location -0.09±0.25% 0.06±0.15% All 98.15% 96.68% Table 8: Subtractive analysis ASTRO ME model boundaries, and then tokenized using the Penn Treebank (Marcus et al., 1993) sed script, with another Python script fixing common errors. The LATEX, which the tokenizer split off incorrectly, was then reattached. Each sentence of the corpus was then annotated using a modified version of the Argumentative Zoning schema. While the original three zones: Background, Own, Other are used, we have replaced the CARS labels with content labels describing aspects of the work: DAT for data used in the analysis, OBS for observations performed, and TEC for techniques applied. Only Own and Other are subclassed with the extended schema of Data, Observation and Techniques. Examples o</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Building a large annotated corpus of english: The penn treebank.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Murphy</author>
<author>T McIntosh</author>
<author>J R Curran</author>
</authors>
<title>Named entity recognition for astronomy literature.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Australasian Language Technology Workshop (ALTW).</booktitle>
<contexts>
<context position="11222" citStr="Murphy et al., 2006" startWordPosition="1801" endWordPosition="1804"> middle section). location The position of a sentence between two headings (representing a section). There are two versions of this feature, one counts to a maximum of 10 and the other represents a percentage through the section bucketed into 20% intervals. paragraph The position of the sentence within a paragraph. Again there are two features – either straight counts (with a maximum of 10) or bucketed into thirds of a paragraph. length of sentence grouped into multiples of 3. Named entity features Our astronomy corpus has been manually annotated with domain-specific named entity information (Murphy et al., 2006). There are 12 coarsegrained categories and 43 fine-grained categories including star, galaxy, telescope, as well as a number of the usual categories including person, organisation and location. Both the coarse-grained and fine-grained categories were used as features. 4.2 Teufel (1999)’s features To compare with previous work, we also implemented most of the features that gave Teufel (1999) the best performance. We list all of the feature types in Table 2, indicating which ones have and have not been implemented. Teufel’s unigram features (cont-1) are filtered using TF-IDF to select the top s</context>
<context position="19760" citStr="Murphy et al. (2006)" startWordPosition="3199" endWordPosition="3203"> of just over 1%. Table 6 shows a different story for Teufel’s features using the maximum entropy model. It seems that none of the feature types alone are making an enormous contribution and that the impact of them varies enormously between folds (the confidence intervals are far bigger than the differences). Finally, Table 7 gives the results of using the maximum entropy model with Markov history length four and all of the features. Overall, we improve Teufel and Moens’ performance by just under 20% on our reproduced experiments. 5.2 Astronomical Corpus The astronomical corpus was created by Murphy et al. (2006) and consists of papers obtained from arXiv (2005)’s astrophysics section (astroph). The papers were converted from LATEX to Unicode by a custom script which attempted to retain as much of the paper’s special characters and formatting as possible. The resulting text was then processed using MXTerminator (Reynar and Ratnaparkhi, 1997) with an additional Python script to find sentence 23 Description As has been noted in prior studies , Abell|GXYC 2255|GXYC has an unusually large number of galaxies with extended radio emission. This is consistent with the findings of Hogg|P Fruchter|P ( 1999|DAT </context>
</contexts>
<marker>Murphy, McIntosh, Curran, 2006</marker>
<rawString>T. Murphy, T. McIntosh, and J.R. Curran. 2006. Named entity recognition for astronomy literature. In Proceedings of the 2006 Australasian Language Technology Workshop (ALTW).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Nigam</author>
<author>J Lafferty</author>
<author>A McCallum</author>
</authors>
<title>Using maximum entropy for text classification.</title>
<date>1999</date>
<booktitle>In Proceedings of the IJCAI-99 Workshop on Machine Learning for Information Filtering,</booktitle>
<pages>61--67</pages>
<location>Stockholm,</location>
<contexts>
<context position="3214" citStr="Nigam et al., 1999" startWordPosition="476" endWordPosition="479"> used Naive Bayes (NB) classifiers (Kupiec et al., 1995; Teufel, 1999) which assume conditional independence of the features. However, this assumption is rarely true for the kinds of rich feature representations we want to use for most NLP tasks. Maximum entropy (ME) models have become popular in NLP because they can incorporate evidence from the complex, diverse and overlapping features needed to represent language. Some example applications include part-of-speech (POS) tagging (Ratnaparkhi, 1996), parsing (Johnson et al., 1999), language modelling (Rosenfeld, 1996), and text categorisation (Nigam et al., 1999). We have developed an Argumentative Zoning (zone) classifier using a ME model. We compare our zone classifier to a reimplementation of Teufel and Moens (2002)’s NB classifier and features on their original Computational Linguistics corpus. Like Teufel (1999), we model zone classification as a sequence tagging task. Our zone classifier achieves an F-score of 96.88%, a 20% improvement. We also show how Argumentative Zoning can be applied to other domains by evaluating our system on a corpus of Astronomy journal articles, achieving an F-measure of 97.9%. 19 Proceedings of the 2009 Workshop on Te</context>
</contexts>
<marker>Nigam, Lafferty, McCallum, 1999</marker>
<rawString>K. Nigam, J. Lafferty, and A. McCallum. 1999. Using maximum entropy for text classification. In Proceedings of the IJCAI-99 Workshop on Machine Learning for Information Filtering, pages 61–67, Stockholm, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy partof-speech tagger.</title>
<date>1996</date>
<booktitle>In Proceedings of the EMNLP Conference,</booktitle>
<pages>133--142</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="3098" citStr="Ratnaparkhi, 1996" startWordPosition="462" endWordPosition="463">ng on the needs of the user, e.g. a layperson versus a domain expert. The first experiments in Argumentative Zoning used Naive Bayes (NB) classifiers (Kupiec et al., 1995; Teufel, 1999) which assume conditional independence of the features. However, this assumption is rarely true for the kinds of rich feature representations we want to use for most NLP tasks. Maximum entropy (ME) models have become popular in NLP because they can incorporate evidence from the complex, diverse and overlapping features needed to represent language. Some example applications include part-of-speech (POS) tagging (Ratnaparkhi, 1996), parsing (Johnson et al., 1999), language modelling (Rosenfeld, 1996), and text categorisation (Nigam et al., 1999). We have developed an Argumentative Zoning (zone) classifier using a ME model. We compare our zone classifier to a reimplementation of Teufel and Moens (2002)’s NB classifier and features on their original Computational Linguistics corpus. Like Teufel (1999), we model zone classification as a sequence tagging task. Our zone classifier achieves an F-score of 96.88%, a 20% improvement. We also show how Argumentative Zoning can be applied to other domains by evaluating our system o</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy partof-speech tagger. In Proceedings of the EMNLP Conference, pages 133–142, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Reynar</author>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy approach to identifying sentence boundaries.</title>
<date>1997</date>
<booktitle>In Proceedings of the fifth conference on Applied natural language processing,</booktitle>
<pages>16--19</pages>
<contexts>
<context position="20095" citStr="Reynar and Ratnaparkhi, 1997" startWordPosition="3252" endWordPosition="3255">7 gives the results of using the maximum entropy model with Markov history length four and all of the features. Overall, we improve Teufel and Moens’ performance by just under 20% on our reproduced experiments. 5.2 Astronomical Corpus The astronomical corpus was created by Murphy et al. (2006) and consists of papers obtained from arXiv (2005)’s astrophysics section (astroph). The papers were converted from LATEX to Unicode by a custom script which attempted to retain as much of the paper’s special characters and formatting as possible. The resulting text was then processed using MXTerminator (Reynar and Ratnaparkhi, 1997) with an additional Python script to find sentence 23 Description As has been noted in prior studies , Abell|GXYC 2255|GXYC has an unusually large number of galaxies with extended radio emission. This is consistent with the findings of Hogg|P Fruchter|P ( 1999|DAT ) who found that GRB hosts are in general subluminous galaxies . We scanned the data of about 1.8|DUR year|DUR ( TJDs|DUR 11000-11699|DUR ) and found 30 new GRB-like events. In Fig . REF we present the 1.4|FRQ GHz|FRQ radio images of the cluster A2744|GXYC , at different angular resolutions . (subclassed from OWN) Smith|P et al. ( 20</context>
</contexts>
<marker>Reynar, Ratnaparkhi, 1997</marker>
<rawString>J.C. Reynar and A. Ratnaparkhi. 1997. A maximum entropy approach to identifying sentence boundaries. In Proceedings of the fifth conference on Applied natural language processing, pages 16–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rosenfeld</author>
</authors>
<title>A maximum entropy approach to adaptive statistical language modeling.</title>
<date>1996</date>
<journal>Computer, Speech and Language,</journal>
<pages>10--187</pages>
<contexts>
<context position="3168" citStr="Rosenfeld, 1996" startWordPosition="471" endWordPosition="472">e first experiments in Argumentative Zoning used Naive Bayes (NB) classifiers (Kupiec et al., 1995; Teufel, 1999) which assume conditional independence of the features. However, this assumption is rarely true for the kinds of rich feature representations we want to use for most NLP tasks. Maximum entropy (ME) models have become popular in NLP because they can incorporate evidence from the complex, diverse and overlapping features needed to represent language. Some example applications include part-of-speech (POS) tagging (Ratnaparkhi, 1996), parsing (Johnson et al., 1999), language modelling (Rosenfeld, 1996), and text categorisation (Nigam et al., 1999). We have developed an Argumentative Zoning (zone) classifier using a ME model. We compare our zone classifier to a reimplementation of Teufel and Moens (2002)’s NB classifier and features on their original Computational Linguistics corpus. Like Teufel (1999), we model zone classification as a sequence tagging task. Our zone classifier achieves an F-score of 96.88%, a 20% improvement. We also show how Argumentative Zoning can be applied to other domains by evaluating our system on a corpus of Astronomy journal articles, achieving an F-measure of 97</context>
</contexts>
<marker>Rosenfeld, 1996</marker>
<rawString>R. Rosenfeld. 1996. A maximum entropy approach to adaptive statistical language modeling. Computer, Speech and Language, 10:187–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Swales</author>
</authors>
<title>Genre analysis: English in academic and research settings.</title>
<date>1990</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="4970" citStr="Swales (1990)" startWordPosition="747" endWordPosition="748"> mention of weaknesses of other work statements that own work is based on other work Table 1: Teufel’s (1999) Argumentative Zones 2 Argumentative Zoning Teufel (1999) introduced a new rhetorical analysis for scientific texts called Argumentative Zoning. Each sentence of an article from the scientific literature is classified into one of seven basic rhetorical structures shown in Table 1. The first three: Background, Other, and Own, are part of the basic schema and represent attribution of intellectual ownership. The four additional categories: aim, textual, contrast, and basis, are based upon Swales (1990)’s Creating A Research Space (CARS) model, and provide pointed information about the author’s stance and the paper itself. Teufel assumes that each sentence only requires a single classification and that all sentences clearly fit into the above structure. The assumption is clearly not always correct, but is a useful approximation nevertheless. Due to the specific nature of these classifications it is hoped that this will allow for much more robust automatic abstraction generation. Summaries of a paper could be created specifically for the user, either focusing on the aim of the work, the work’</context>
</contexts>
<marker>Swales, 1990</marker>
<rawString>J.M. Swales. 1990. Genre analysis: English in academic and research settings. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Teufel</author>
<author>M Moens</author>
</authors>
<title>Summarising scientific articles — experiments with relevance and rhetorical status.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>4</issue>
<contexts>
<context position="2102" citStr="Teufel and Moens, 2002" startWordPosition="307" endWordPosition="310">ledge-intensive methods now dominate. Sentence extraction, e.g. Brandow et al. (1995) and Kupiec et al. (1995), selects a small number of abstract worthy sentences from a larger text. The resulting sentences form a collection of excerpt sentences meant to capture the essence of the text. The next stage is information fusion (Barzilay et al., 1999; Knight and Marcu, 2000) which attempts to combine the excerpts into a more cohesive text. These methods can create inflexible and incoherent extracts that result in under-informative results (Teufel et al., 1999). Argumentative Zoning (Teufel, 1999; Teufel and Moens, 2002) attempts to solve this problem by representing the structure of a text using a rhetorically-based schema. Sentences are classified into one of a small number of nonhierarchical argumentative roles, which can then be used in both the sentence extraction and text generation/fusion phase of automatic summarization. Argumentative Zoning can enable tailored summarizations depending on the needs of the user, e.g. a layperson versus a domain expert. The first experiments in Argumentative Zoning used Naive Bayes (NB) classifiers (Kupiec et al., 1995; Teufel, 1999) which assume conditional independenc</context>
<context position="3373" citStr="Teufel and Moens (2002)" startWordPosition="501" endWordPosition="504">arely true for the kinds of rich feature representations we want to use for most NLP tasks. Maximum entropy (ME) models have become popular in NLP because they can incorporate evidence from the complex, diverse and overlapping features needed to represent language. Some example applications include part-of-speech (POS) tagging (Ratnaparkhi, 1996), parsing (Johnson et al., 1999), language modelling (Rosenfeld, 1996), and text categorisation (Nigam et al., 1999). We have developed an Argumentative Zoning (zone) classifier using a ME model. We compare our zone classifier to a reimplementation of Teufel and Moens (2002)’s NB classifier and features on their original Computational Linguistics corpus. Like Teufel (1999), we model zone classification as a sequence tagging task. Our zone classifier achieves an F-score of 96.88%, a 20% improvement. We also show how Argumentative Zoning can be applied to other domains by evaluating our system on a corpus of Astronomy journal articles, achieving an F-measure of 97.9%. 19 Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, ACL-IJCNLP 2009, pages 19–26, Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP Category Abbr. B</context>
<context position="16193" citStr="Teufel and Moens (2002)" startWordPosition="2626" endWordPosition="2629">l (1999) from the Computation and Language E-Print Archive 1. The LATEX source was converted to HTML with Latex2HTML then transformed into XML with custom PERL scripts. This text was then tokenized using the TTT (Text Tokenization) System into Penn Treebank format. The result is a corpus of 12,000 annotated sentences, containing 333,000 word tokens, in XML format. We attempted to recreate Teufel’s original experiments by emulating the features she used with the same type of classifier. We used Weka’s (Frank et al., 2005) implementation of the NB classifier. Table 3 reproduces the results from Teufel and Moens (2002) alongside our reimplementation of 1http://xxx.lanl.gov/cmp-lg/ 22 Tag original reproduced P R F P R F AIM 44 65 52 45.8 57.8 51.1 BAS 37 40 38 23.8 37.0 28.9 CTR 34 20 26 33.1 19.2 24.3 BKG 40 50 45 46.9 53.6 50.1 OTH 52 39 44 70.6 55.0 61.8 TXT 57 66 61 66.3 47.6 55.4 OWN 84 88 86 86.7 90.8 88.7 Weighted 72 73 72 76.8 76.8 76.8 Table 3: Teufel and Moens (2002)’s and our NB performance on CMP-LG History Type Order Performance Baseline None 93.16 Viterbi First 1.77 ± 0.49% Viterbi Second 1.97 ± 0.42% Viterbi Third 2.08 ± 0.45% Viterbi Fourth 2.1 ± 0.46% Viterbi Fifth 2.13 ± 0.46% Oracle First </context>
<context position="18971" citStr="Teufel and Moens (2002)" startWordPosition="3067" endWordPosition="3070">(with the Oracle experiment) can make a large difference to classification accuracy. Feature Change TLength -2.09±9.96% Struct-1 0.38±6.08% TLoc 0.96±7.25% Struct-3 -1.65±6.76% Cont-2 -1.10±6.39% Struct-2 1.59±7.99% Ag-1/2 -0.39±8.97% Formu 0.14±8.46% Cit-1 -1.88±5.19% Cont-1 -0.38±5.85% All 70.25% Table 6: Teufel’s Subtractive analysis CMP-LG ME Table 5 presents the subtractive analysis to determine the impact of different feature types. From this we can see that the n-grams (unigrams and bigrams) have by far the largest impact – and neither of these feature types was directly implemented by Teufel and Moens (2002). The next most important features are the first few words (again a unigram type feature), length and the section number. The Markov history features also have an impact of just over 1%. Table 6 shows a different story for Teufel’s features using the maximum entropy model. It seems that none of the feature types alone are making an enormous contribution and that the impact of them varies enormously between folds (the confidence intervals are far bigger than the differences). Finally, Table 7 gives the results of using the maximum entropy model with Markov history length four and all of the fea</context>
</contexts>
<marker>Teufel, Moens, 2002</marker>
<rawString>S. Teufel and M. Moens. 2002. Summarising scientific articles — experiments with relevance and rhetorical status. Computational Linguistics, 28(4):409–445.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Teufel</author>
<author>J Carletta</author>
<author>M Moens</author>
</authors>
<title>An annotation scheme for discourse-level argumentation in research articles.</title>
<date>1999</date>
<booktitle>In Proceedings of EACL</booktitle>
<contexts>
<context position="2041" citStr="Teufel et al., 1999" startWordPosition="299" endWordPosition="302">le-based NLP and knowledge representation, other less knowledge-intensive methods now dominate. Sentence extraction, e.g. Brandow et al. (1995) and Kupiec et al. (1995), selects a small number of abstract worthy sentences from a larger text. The resulting sentences form a collection of excerpt sentences meant to capture the essence of the text. The next stage is information fusion (Barzilay et al., 1999; Knight and Marcu, 2000) which attempts to combine the excerpts into a more cohesive text. These methods can create inflexible and incoherent extracts that result in under-informative results (Teufel et al., 1999). Argumentative Zoning (Teufel, 1999; Teufel and Moens, 2002) attempts to solve this problem by representing the structure of a text using a rhetorically-based schema. Sentences are classified into one of a small number of nonhierarchical argumentative roles, which can then be used in both the sentence extraction and text generation/fusion phase of automatic summarization. Argumentative Zoning can enable tailored summarizations depending on the needs of the user, e.g. a layperson versus a domain expert. The first experiments in Argumentative Zoning used Naive Bayes (NB) classifiers (Kupiec et </context>
</contexts>
<marker>Teufel, Carletta, Moens, 1999</marker>
<rawString>S. Teufel, J. Carletta, and M. Moens. 1999. An annotation scheme for discourse-level argumentation in research articles. In Proceedings of EACL 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Teufel</author>
<author>A Siddharthan</author>
<author>D Tidhar</author>
</authors>
<title>Automatic classification of citation function.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>103--110</pages>
<contexts>
<context position="6328" citStr="Teufel et al., 2006" startWordPosition="978" endWordPosition="981">thor’s use and opinion of other authors they cite in their work and also to create Rhetorical Document Profiles (RDP), a type of summarization used to provide typical information that a new reader may need in a systematic manner. For the use of Argumentative Zoning in RDPs Teufel (1999) points out that due to the redundancy in language that near perfect accuracy is not required as important pieces of information will be repeated in the paper. Recognising these salient points once is enough for them to be included in the RDP. In further tasks, such as the analysis of the function of citations (Teufel et al., 2006) and automatic summarization, higher levels of accuracy are more critical. 3 Maximum Entropy models Maximum entropy (ME) or log-linear models are statistical models that can incorporate evidence from a diverse range of complex and potentially overlapping features. Unlike Naive Bayes (NB), the features can be conditionally dependent given the class, which is important since feature sets in NLP rarely satisfy this independence constraint. The ME classifier uses models of the form: n p(y|x) = Z(x) exp(i=1 � λifi(x, y) (1) where y is the zone label, x is the context (the sentence) and the fi(x, y)</context>
</contexts>
<marker>Teufel, Siddharthan, Tidhar, 2006</marker>
<rawString>S. Teufel, A. Siddharthan, and D. Tidhar. 2006. Automatic classification of citation function. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 103–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Teufel</author>
</authors>
<title>Argumentative zoning: Information extraction from scientific text.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh,</institution>
<location>Edinburgh, UK.</location>
<contexts>
<context position="667" citStr="Teufel (1999)" startWordPosition="93" endWordPosition="94">els Stephen Merity and Tara Murphy and James R. Curran School of Information Technologies University of Sydney NSW 2006, Australia {smerity,tm,james}@it.usyd.edu.au Abstract We present a maximum entropy classifier that significantly improves the accuracy of Argumentative Zoning in scientific literature. We examine the features used to achieve this result and experiment with Argumentative Zoning as a sequence tagging task, decoded with Viterbi using up to four previous classification decisions. The result is a 23% F-score increase on the Computational Linguistics conference papers marked up by Teufel (1999). Finally, we demonstrate the performance of our system in different scientific domains by applying it to a corpus of Astronomy journal articles annotated using a modified Argumentative Zoning scheme. 1 Introduction The task of generating automatic summarizations of one or more texts is a central problem in Natural Language Processing (NLP). Summarization is a fundamental component for future information retrieval and question answering systems, incorporating both natural language understanding and natural language generation. Comprehension-based summarization, e.g. Kintsch and Van Dijk (1978)</context>
<context position="2077" citStr="Teufel, 1999" startWordPosition="305" endWordPosition="306">ther less knowledge-intensive methods now dominate. Sentence extraction, e.g. Brandow et al. (1995) and Kupiec et al. (1995), selects a small number of abstract worthy sentences from a larger text. The resulting sentences form a collection of excerpt sentences meant to capture the essence of the text. The next stage is information fusion (Barzilay et al., 1999; Knight and Marcu, 2000) which attempts to combine the excerpts into a more cohesive text. These methods can create inflexible and incoherent extracts that result in under-informative results (Teufel et al., 1999). Argumentative Zoning (Teufel, 1999; Teufel and Moens, 2002) attempts to solve this problem by representing the structure of a text using a rhetorically-based schema. Sentences are classified into one of a small number of nonhierarchical argumentative roles, which can then be used in both the sentence extraction and text generation/fusion phase of automatic summarization. Argumentative Zoning can enable tailored summarizations depending on the needs of the user, e.g. a layperson versus a domain expert. The first experiments in Argumentative Zoning used Naive Bayes (NB) classifiers (Kupiec et al., 1995; Teufel, 1999) which assum</context>
<context position="3473" citStr="Teufel (1999)" startWordPosition="516" endWordPosition="517">E) models have become popular in NLP because they can incorporate evidence from the complex, diverse and overlapping features needed to represent language. Some example applications include part-of-speech (POS) tagging (Ratnaparkhi, 1996), parsing (Johnson et al., 1999), language modelling (Rosenfeld, 1996), and text categorisation (Nigam et al., 1999). We have developed an Argumentative Zoning (zone) classifier using a ME model. We compare our zone classifier to a reimplementation of Teufel and Moens (2002)’s NB classifier and features on their original Computational Linguistics corpus. Like Teufel (1999), we model zone classification as a sequence tagging task. Our zone classifier achieves an F-score of 96.88%, a 20% improvement. We also show how Argumentative Zoning can be applied to other domains by evaluating our system on a corpus of Astronomy journal articles, achieving an F-measure of 97.9%. 19 Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, ACL-IJCNLP 2009, pages 19–26, Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP Category Abbr. Background BKG Other OTH Own OWN Description general scientific background neutral descriptions of ot</context>
<context position="5995" citStr="Teufel (1999)" startWordPosition="921" endWordPosition="922">at this will allow for much more robust automatic abstraction generation. Summaries of a paper could be created specifically for the user, either focusing on the aim of the work, the work’s stance in the field (what other works it is based upon or compared with) and so on. Teufel used Argumentative Zoning to determine the author’s use and opinion of other authors they cite in their work and also to create Rhetorical Document Profiles (RDP), a type of summarization used to provide typical information that a new reader may need in a systematic manner. For the use of Argumentative Zoning in RDPs Teufel (1999) points out that due to the redundancy in language that near perfect accuracy is not required as important pieces of information will be repeated in the paper. Recognising these salient points once is enough for them to be included in the RDP. In further tasks, such as the analysis of the function of citations (Teufel et al., 2006) and automatic summarization, higher levels of accuracy are more critical. 3 Maximum Entropy models Maximum entropy (ME) or log-linear models are statistical models that can incorporate evidence from a diverse range of complex and potentially overlapping features. Un</context>
<context position="9066" citStr="Teufel (1999)" startWordPosition="1452" endWordPosition="1453">s many rare, but informative, features to be used without overfitting. This will be an important property when we use sparse features like bigrams in the models below. 4 Modelling Argumentative Zones 4.1 Our Features The two primary sources of features for our zone classifier were the words in the sentences and the position of the sentence relative to the rest of the paper. A number of feature types use additional external resources (e.g. semantic lists of agents or common rhetorical patterns) or annotations (e.g. named entities). Where feasible we have reimplemented the features described in Teufel (1999). In other cases, our features are somewhat simpler. Since the Curran and Clark (2003) classifier only accepts binary features, any numerical features had to be bucketed into smaller sets of alternatives to reduce sparseness, either by integer division or through reducing the number by scaling to a small integer range. The features we implemented are described below. Unigrams, bigrams and n-grams A sub-sequence of n words from a given sentence. We include unigram and bigram features and report them individually and together (as n-grams). These features include all of the unigrams and bigrams a</context>
<context position="11509" citStr="Teufel (1999)" startWordPosition="1846" endWordPosition="1847">ce within a paragraph. Again there are two features – either straight counts (with a maximum of 10) or bucketed into thirds of a paragraph. length of sentence grouped into multiples of 3. Named entity features Our astronomy corpus has been manually annotated with domain-specific named entity information (Murphy et al., 2006). There are 12 coarsegrained categories and 43 fine-grained categories including star, galaxy, telescope, as well as a number of the usual categories including person, organisation and location. Both the coarse-grained and fine-grained categories were used as features. 4.2 Teufel (1999)’s features To compare with previous work, we also implemented most of the features that gave Teufel (1999) the best performance. We list all of the feature types in Table 2, indicating which ones have and have not been implemented. Teufel’s unigram features (cont-1) are filtered using TF-IDF to select the top scoring 10 words in each document, and then these are used to mark the top 40 sentences in each document containing those filtered words. TLoc marks the position of the sentence over the entire paper, using 10 unevenly sized segments (larger segments are in the middle of the paper). Stru</context>
<context position="13157" citStr="Teufel (1999)" startWordPosition="2129" endWordPosition="2130"> (A-J) Struct-1 yes Position within a section Struct-2 yes Relative position of sentence within a paragraph Struct-3 partial Type of headline of the current section TLength yes Is the sentence longer than 15 words? Syn-1 no Voice of the first finite verb in the sentence Syn-2 no Tense of the first finite verb in the sentence Syn-3 no Is the first finite verb modified by a modal auxiliary Cit-1 yes Does the sentence contain a citation or name of author? Formu yes Does a formulaic expression occur in the sentence Ag-1 yes Type of agent Ag-2 yes Type of action (with or without negation) Table 2: Teufel (1999)’s set of features third sentence as a sixth segment; and the secondlast plus third-last sentence as a seventh segment. Struct-3 the type of section heading for the current section. In our case, we have not mapped these down to the reduced set used by Teufel. Formu uses pattern matching rules to identify formulaic expressions. Ag-1 and Ag-2 identify agent and action expressions from gazetteers. Teufel (1999) provides these in the appendices. 4.3 Feature Cutoff Features that occur rarely in the training set are problematic because the statistics extracted for these features are not reliable. Th</context>
<context position="15578" citStr="Teufel (1999)" startWordPosition="2526" endWordPosition="2527"> of precision, recall and f-score for each of the zone classes, and a weighted average over all classes. We have investigated the impact of each feature type using subtractive analysis, where we have also calculated paired t-test confidence intervals (the error values reported are the 95% confidence interval). The baselines for both sets were already quite high (at least 70%) due to the common tag of OWN, representing the author’s own work, but our results show significant improvements over this baseline. 5.1 CMP-LG Corpus The CMP-LG corpus is a collection of 80 conference papers collected by Teufel (1999) from the Computation and Language E-Print Archive 1. The LATEX source was converted to HTML with Latex2HTML then transformed into XML with custom PERL scripts. This text was then tokenized using the TTT (Text Tokenization) System into Penn Treebank format. The result is a corpus of 12,000 annotated sentences, containing 333,000 word tokens, in XML format. We attempted to recreate Teufel’s original experiments by emulating the features she used with the same type of classifier. We used Weka’s (Frank et al., 2005) implementation of the NB classifier. Table 3 reproduces the results from Teufel a</context>
</contexts>
<marker>Teufel, 1999</marker>
<rawString>S. Teufel. 1999. Argumentative zoning: Information extraction from scientific text. Ph.D. thesis, University of Edinburgh, Edinburgh, UK.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>