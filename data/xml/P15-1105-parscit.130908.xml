<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000610">
<title confidence="0.894356">
Automatic Spontaneous Speech Grading: A Novel Feature Derivation
Technique using the Crowd
Vinay Shashidhar Nishant Pandey Varun Aggarwal
</title>
<author confidence="0.557809">
Aspiring Minds Aspiring Minds Aspiring Minds
</author>
<email confidence="0.945753">
vinay.shashidhar@aspiringminds.com nishant.pandey@aspiringminds.com varun@aspiringminds.com
</email>
<sectionHeader confidence="0.992808" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999968628571428">
In this paper, we address the problem
of evaluating spontaneous speech us-
ing a combination of machine learning
and crowdsourcing. Machine learning
techniques inadequately solve the stated
problem because automatic speaker-
independent speech transcription is
inaccurate. The features derived from it
are also inaccurate and so is the machine
learning model developed for speech
evaluation. To address this, we post the
task of speech transcription to a large
community of online workers (crowd).
We also get spoken English grades from
the crowd. We achieve 95% transcription
accuracy by combining transcriptions
from multiple crowd workers. Speech
and prosody features are derived by force
aligning the speech samples on these
highly accurate transcriptions. Addi-
tionally, we derive surface and semantic
level features directly from the transcrip-
tion. To demonstrate the efficacy of our
approach we performed experiments on
an expert–graded speech sample of 319
adult non–native speakers. Using these
features in a regression model, we are
able achieve a Pearson correlation of
0.76 with expert grades, an accuracy
much higher than any previously reported
machine learning approach. Our approach
has an accuracy that rivals that of expert
agreement. This work is timely given
the huge requirement of spoken English
training and assessment.
</bodyText>
<sectionHeader confidence="0.999305" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998840068181818">
Automatic evaluation of spoken English has been
of keen interest for more than two decades (Zech-
ner et al., 2007; Neumeyer et al., 1996; Franco
et al., 2000; Cucchiarini et al., 1997). It can
help learners get feedback in a scalable manner,
help build better English training software and
also help companies and institutions filter and se-
lect prospective employees more effectively. The
problem acquires significance given the evidence
that better English leads to better employment out-
come, wages and promotions (Guven and Islam,
2013).
There has been a considerable success in auto-
matically scoring spoken English, when the spo-
ken text is known a priori (Cucchiarini et al.,
2000; Franco et al., 2000). In these cases, the
candidate is asked to either read a given text or
listen to some speech and repeat it. For these
tasks, the scores generated by an automatic sys-
tem on parameters such as pronunciation and flu-
ency closely mimic those given by human ex-
perts. The primary approach behind a majority of
these systems is to force align the speech sample
on the known text using an HMM–based acous-
tic model. Features such as likelihood, posterior
probability and fluency related features are derived
from the aligned speech and a machine learning
model is used to predict expert grades (Neumeyer
et al., 1996; Franco et al., 2000; Cucchiarini et al.,
1997). Some approaches additionally use prosody
and energy related features (Dong et al., 2004).
More recently, this research has moved towards
the assessment of higher granularity metrics like
the mispronunciation of particular phonemes (Li
et al., 2009; Ito et al., 2006; Koniaris and Engwall,
2011).
In spontaneous speech evaluation, the candidate
is asked to speak on a topic or answer a question
and what he/she speaks isn’t known priori. Evalu-
ation of spontaneous speech is the ultimate test of
a candidate’s proficiency in speaking a language
(Hagley, 2010; Halleck, 1995). While scores from
the evaluation of read/repeat speech do correlate
with spontaneous speech evaluation, there remains
</bodyText>
<page confidence="0.959359">
1085
</page>
<note confidence="0.977117333333333">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1085–1094,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.99991602173913">
an unexplained variance in the spontaneous speech
scores (see Section 5). Generally, candidates who
score high on spontaneous speech also score high
on read speech and not vice versa.
Given the primacy of spontaneous speech eval-
uation in judging a person’s language capabil-
ity, there is considerable interest in doing it au-
tomatically (Cucchiarini et al., 1997; Dong et
al., 2004). Automated approaches for the same
have not worked well (Powers et al., 2002; Cuc-
chiarini et al., 2000) primarily because speaker-
independent speech recognition is a tough com-
puter science problem. This is exacerbated when
the speakers are not proficient in the language or
are non-natives (Powers et al., 2002). Given that
speech to text conversion for such candidates has a
low accuracy, force alignment of the speech on this
inaccurate text makes the features and the model
inaccurate.
We present a semi-automated approach to grade
short duration (45 seconds) spontaneous speech.
We accurately predict a holistic score which is
based on the pronunciation, fluency, content char-
acteristics and grammar of the speech sample, as
determined by experts. Multiple previous studies
in language acquisition and second language re-
search conclusively show that proficiency in a sec-
ond language can be characterized by these factors
(Bhat et al., 2014). Being able to provide a holistic
score is of high interest in both educational test-
ing (Zechner et al., 2009) and job related testing
(Streeter et al., 2011). Institutions and firms look
for a holistic score, say based on CEFR, a stan-
dard to describe spoken English assessment (Lit-
tle, 2006; Little, 2007), to make an accept or reject
decision on candidates. Currently, an expert based
assessment is used for these purposes.
Our method involves combining machine learn-
ing with a crowdsourcing layer. Crowdsourcing
(Estell´es-Arolas and Gonz´alez-Ladr´on-de Gue-
vara, 2012) is the process of getting human in-
telligence tasks performed by a large community
of online workers (crowd) as opposed to tradi-
tional employees.1 The responses from the hu-
man intelligence tasks are then used to create rel-
evant features for machine learning. Human in-
</bodyText>
<footnote confidence="0.836484666666667">
1Our approach is different from peer grading (Lejk
and Wyvill, 2001) or crowd grading (Van Houdnos, 2011;
Tetreault et al., 2010; Madnani et al., 2011) approaches.
These approaches directly ask the crowd to grade the re-
sponse. The primary feature of our technique is using the
crowd in the feature extraction step of machine learning.
</footnote>
<bodyText confidence="0.976349355555556">
telligence tasks are defined as those which most
humans find easy, but are hard for machines. For
instance, a classic example is the task of finding
a particular object in an image. There is a large
research community that uses crowdsourcing and
has demonstrated that it can help perform tasks in-
expensively, in large volumes and within reason-
able time (Howe, 2006; Whitla, 2009).
Our system design for evaluation of sponta-
neous speech is illustrated in Figure 1. We post
the task2 of speech transcription to the crowd. We
get a final accurate transcription by combining the
transcriptions from more than one crowd worker
for the same speech sample. Once we have this
accurate transcription, we force-align (Erling and
Seargeant, 2013; Sj¨olander, 2003) the speech of
the candidate on this text to derive various features
which go into a machine learning engine. We also
collect spoken English grades of the speech from
the crowd (Lejk and Wyvill, 2001), which are used
as additional features. With these accurately iden-
tified features and crowd grades, machine learning
is able to grade spontaneous speech with high ac-
curacy. We found that this approach does much
better than a pure machine learning approach.
Crowdsourcing has been used for almost a
decade in various problems in speech analysis,
grading and language learning (Kunath and Wein-
berger, 2010; Peabody, 2011; Wang et al., 2014).
Within assessment of speech, currently all such
approaches use the crowd to directly grade cer-
tain parts of the speech (Wang and Meng, 2012).
Our work is uniquely positioned where we use the
crowd to do accurate transcription, a human intel-
ligence task, and use it in a machine learning based
algorithm.3 We show that such a system provides
an accuracy rivaling that of experts.
In this paper, we solve a hitherto unsolved prob-
lem of spontaneous speech evaluation (Zechner et
al., 2009). The paper makes the following contri-
butions:
• We show that spoken English can be graded
with accuracy by combining machine learn-
ing and crowdsourcing higher than a pure
machine learning approach.
</bodyText>
<footnote confidence="0.6129255">
2Even though speaker-independent speech recognition is
a hard problem for machines, it is fairly easy for a native
speaker or anyone with reasonable command over the lan-
guage.
3Again, speech transcription has been done previously us-
ing crowdsourcing (Zaidan and Callison-Burch, 2011), but
not used for a grading purpose or combined with machine
learning.
</footnote>
<page confidence="0.99633">
1086
</page>
<figureCaption confidence="0.999216">
Figure 1: System Design
</figureCaption>
<listItem confidence="0.811650888888889">
• We show that the features derived from
crowdsourced transcriptions perform as well
as crowd grades in predicting expert grades.
However, crowd grades add additional pre-
dictive value.
• We propose a scalable and accurate way to
perform evaluation of spontaneous speech, a
huge requirement in the industry and else-
where.
</listItem>
<bodyText confidence="0.9959701">
The paper is organized as follows– Section 2
describes the procedure and aim of the speech
assessment task; Section 3 describes the feature
classes used in the prediction algorithm; Section
4 describes the crowdsourcing framework which
is used as an input to machine learning meth-
ods; Section 5 demonstrates how this framework
is used with machine learning techniques to pre-
dict a composite spoken English score; Section 6
discusses the future work and concludes the paper.
</bodyText>
<sectionHeader confidence="0.871132" genericHeader="method">
2 Grading Task
</sectionHeader>
<bodyText confidence="0.999994105263158">
We want to assess the quality of spoken English
of candidates based on their spontaneous speech
samples. The speech samples of the candidates
were collected using Aspiring Minds’ automated
speech assessment tool– SVAR (SVAR, 2014).
SVAR is conducted over phone as well as on a
computer. The test has multiple sections where
the candidate is required to: read sentences aloud,
listen and repeat sentences, listen to a passage or
conversation and answer multiple choice questions
and finally spontaneously speak on a given topic.
In the spontaneous speech section, the candidates4
are provided with a topic and given 30 seconds5 to
think, take notes and then speak on the topic for
45 seconds. The topic is repeated to ensure task
clarity. The complete test takes 16-20 minutes to
complete, depending on the test version.
Currently, SVAR evaluates speech samples
from the read and repeat sections with high accu-
racy (SVAR, 2014). Our goal in this paper is to
evaluate the spontaneous speech of the candidate
and provide a composite score based on it.
A 5 point rubric for the composite score, similar
to CEFR (Examinations, 2011), was prepared with
the help of experts. This score is a function of the
pronunciation, fluency, content organization and
grammar quality of the speech sample. Broadly
speaking, Pronunciation (Dobson, 1957) refers to
the correctness in the utterance of the phonemes
of a word by the students as per neutral accent.
Fluency (Brumfit and Brumfit, 1984) refers to a
desired rate of speech along with the absence of
hesitations, false starts and stops etc. Content or-
ganization (Stalnaker, 1999) measures the candi-
date’s ability to structure the information disposi-
tion and present it coherently. Grammar (Brazil,
1995) measures how well the syntax of the lan-
guage was followed by the candidate.
</bodyText>
<footnote confidence="0.976883">
4The subjects of our study use English as their second lan-
guage and hail from various backgrounds, dialects and edu-
cational qualifications.
5This is as per global standards of spoken English assess-
ment. High stake tests such as TOEFL provide the candidate
15-30 seconds to think before responding to a spontaneous
speech task.
</footnote>
<page confidence="0.994478">
1087
</page>
<figureCaption confidence="0.9698185">
Figure 2: Our intuition of how different features
predict the holistic score.
</figureCaption>
<bodyText confidence="0.992087">
In the next section we discuss the features which
are used in the prediction algorithm.
</bodyText>
<sectionHeader confidence="0.999613" genericHeader="method">
3 Features
</sectionHeader>
<bodyText confidence="0.999914307692307">
We use three classes of features– Crowd Grades
(CG), Force Alignment features (FA) and Natu-
ral Language Processing features (NLP). The spo-
ken English samples are posted to the crowd to get
the transcription and spoken English grades (Fig-
ure 1). Each task was completed by three workers.
The crowd grades become one set of features. A
second set, i.e., FA features, are derived by align-
ing (Erling and Seargeant, 2013; Sj¨olander, 2003)
the speech sample on the crowdsourced transcrip-
tions. A third set, i.e., NLP features, are also de-
rived from the crowdsourced text. These are ex-
plained in the succeeding paragraphs.
</bodyText>
<listItem confidence="0.826416333333333">
• Crowd Grades: The crowd transcribes the
speech in addition to providing scores on
each of the following– pronunciation, flu-
ency, content organization and grammar.
These grades are combined to form a com-
posite score per worker per candidate. These
are further averaged across workers to give a
final score.6
• FA features: The speech sample is forced
aligned (Erling and Seargeant, 2013;
Sj¨olander, 2003) on the crowdsourced tran-
scription using the HTK speech recognizer
(Young et al., 2006). We used an acoustic
model based on TIMIT (Garofolo et al.,
1993) for our experiments. TIMIT is a
</listItem>
<footnote confidence="0.8802902">
6Advanced Expectation-Maximization techniques (Hos-
seini et al., 2012) may also be used for an aggregation strat-
egy, once the number of tasks done by every individual
worker increases. In our current experiments, this number
wasn’t very high.
</footnote>
<bodyText confidence="0.980872658536585">
corpus of phonemically and lexically tran-
scribed speech of American English speakers
of different sexes and dialects.
A number of speech quality features are de-
rived, which include– rate of speech, posi-
tion and length of pauses, log likelihood of
recognition, posterior probability, hesitations
and repetitions etc. These features are well
known in literature and may be referred from
(Neumeyer et al., 1996; Zechner et al., 2009;
Cucchiarini et al., 2000). These features are
predictive of the pronunciation and fluency of
the candidate.
• NLP features: These features predict the con-
tent quality and grammar of the spoken con-
tent7. They were derived using standard NLP
packages (LightSide, 2013; AfterTheDead-
line, 2014) on the crowdsourced transcrip-
tion. The package calculates surface level
features such as the number of words, com-
plexity or difficulty of words and the num-
ber of common words used. It also calculates
semantic features like the coherency in text,
context of the words spoken, sentiment of the
text and grammar correctness. In the current
system, we do not use any prompt specific
features such as occurrence of specific words
or phrases. These features are predictive of
the grammar and content organization of the
sample.
All the features described above were obtained
for the spontaneous speech sample. We also
derived features similar to FA features for the
candidate’s read and repeat speech samples col-
lected during his/her SVAR test. The speech and
prosody features are calculated by force aligning
the speech on the known text. One of the mod-
els (RS/LR) in our experiments is based on these
features and has been included for comparison.
These features do not have any bearing on our final
model for spontaneous speech evaluation.
</bodyText>
<sectionHeader confidence="0.990854" genericHeader="method">
4 Crowdsourcing
</sectionHeader>
<bodyText confidence="0.99652025">
The spoken English sample was given to the
crowd to transcribe and provide grades. The task
was posted on a popular crowdsourcing platform–
Amazon Mechanical Turk (AMT) (Paolacci et al.,
</bodyText>
<footnote confidence="0.948017">
7We were looking at prompt independent features only, at
this point.
</footnote>
<page confidence="0.994785">
1088
</page>
<bodyText confidence="0.988352106666667">
2010). AMT is a popular crowdsourcing market-
place. It is inspired by the famous 18th century au-
tomated chess playing machine, running on the in-
telligence of a hidden human operator. It has more
than 500, 000 online workers from 190 countries
(Turk, 2014). One can post tasks on the platform
online and offer fixed remuneration for their com-
pletion.
A clean and simple interface was provided to
the worker with standard features needed for tran-
scription. Additionally, an advanced audio player
was embedded with the ability to play the speech
sample in repeat mode, rewind and forward, apart
from standard play/pause functionality to help the
worker. The different transcriptions were com-
bined using the ROVER algorithm (Fiscus, 1997).
ROVER is a sophisticated voting algorithm to
combine multiple transcriptions with errors, to ob-
tain the best estimate of the correct transcription.
It is reported to lead to an error reduction of 20-
25%. ROVER proceeds in two stages: first the
outputs are aligned and a single word transcription
network (WTN) is built. The second stage consists
of selecting the best scoring word (with the highest
number of votes) at each node.
Several methods have been used in the past for
increasing the reliability of the grades given by the
crowd by identifying and correcting any biases and
removing non-serious/low quality workers (Aker
et al., 2012). One of the key techniques for this
involves inserting gold standard tasks with known
answers to get an estimate of the worker’s abil-
ity (Nguyen et al., 2013). The gold standard tasks
are similar to real tasks and the workers have no
way to distinguish between the two. Our tasks
took workers a reasonable amount of time (8-10
minutes). It wasn’t hence feasible to insert a gold
standard task, as done typically, with every task to
be completed.
To overcome this problem, we propose an in-
novative approach where a risk is assigned to a
worker based on his/her performance on the gold
standard tasks. We conceptualized this system as
a state machine that determines the risk level of
a worker and proposes actions based on it (Re-
fer to Figure 3). All workers started with an ini-
tial risk level of 0.2. Gold standard tasks were
probabilistically inserted among real tasks based
on the worker’s risk level. Workers with a higher
risk level saw more gold standard tasks. Also,
the risk level of the worker was updated based on
Figure 3: Risk Level State Diagram: In the above
figure, each node corresponds to a risk level asso-
ciated with a worker. The values range between
0 (min) - 1 (max). The worker is either assigned
a gold standard task (G) or a normal task (N) on
the basis of his/her present risk level. The risk
level changes every time a task is Accepted (A) or
Rejected (R). Additionally worker may be warned
(W) or blocked (B) in case of rejection.
his/her performance on the gold standard tasks.
Workers who consistently performed poorly on
gold standard tasks were allocated a higher risk
level and a notification was sent to them with
a corrective course of action. Beyond a certain
level, the worker was barred from attempting fu-
ture work. We did not do any retrospective correc-
tion of the barred worker’s completed tasks and
simply stopped him/her from attempting newer
tasks. This approach allowed us to control for the
quality of workers, provide feedback, remove un-
suitable workers and also adaptively control the
balance between real and gold standard tasks.8
We describe the experimental setup and the re-
sults in the next section.
</bodyText>
<sectionHeader confidence="0.999596" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9861985">
We conducted the experiments to answer the fol-
lowing questions:
</bodyText>
<listItem confidence="0.999152857142857">
• Can read/repeat features predict spontaneous
speech grades accurately?
• How accurate is a pure machine learning ap-
proach (without crowdsourced transcription)
in predicting grades as compared to grades
given by human experts?
• How much better is the ML-CS approach in
</listItem>
<footnote confidence="0.933392">
8Specific details of the implementation are beyond the
scope of the paper.
</footnote>
<page confidence="0.996304">
1089
</page>
<bodyText confidence="0.9824905">
predicting grades as compared to a pure ML
approach and to using Crowd Grades only?
</bodyText>
<listItem confidence="0.97246">
• Do Crowd Grades add additional value in
</listItem>
<bodyText confidence="0.961045230769231">
predicting grades over and above the features
derived from the crowdsourced transcription?
We conducted the experiments on 319 sponta-
neous speech samples which were graded by ex-
pert assessors. To answer the questions stated
above, we used different sets of features to develop
models and compared their accuracy. The mod-
els were built against expert grades using super-
vised learning techniques. We experimented with
three machine learning techniques– Ridge Regres-
sion, SVMs and Neural Networks with different
features selection algorithms. The data set used in
the experiments is discussed in the next section.
</bodyText>
<subsectionHeader confidence="0.9804">
5.1 Data Set
</subsectionHeader>
<bodyText confidence="0.999996933333333">
Our data set contains 319 spontaneous speech re-
sponses. The speech samples were from seniors
(non–native English speakers in final year of un-
dergraduate education) pursuing bachelor’s degree
in India. The candidates were asked to describe
one of the following scenes: a hospital, flood, a
crowded market and a school playground. The
candidates were given 30 seconds to think and take
notes and were then asked to speak for the next
45 seconds. The responses were collected on the
phone during the SVAR test (SVAR, 2014). Apart
from the spontaneous speech response, each can-
didate was asked to read 12 given sentences and
repeat 9 given sentences immediately after listen-
ing to each of them. Empty or very noisy re-
sponses (not humanly discernible) were not in-
cluded in the final 319 sample set.
These responses were graded by two experts
who had more than fifteen years of experience in
grading spoken English responses. There were
two set of scores. The first was a holistic score
on the spontaneous speech samples based on its
pronunciation, fluency, content characteristics and
grammar. The second was a score on the pronun-
ciation and fluency quality of the read/repeat sen-
tences. The correlation between grades given by
the two experts was 0.86 and 0.83 respectively for
the two cases. For each of the two scores, the av-
erage of the scores by the two expert grades was
used for further purposes.
The correlation between the expert scores on
spontaneous speech and read/repeat speech was
0.54. This shows that there is a considerable unex-
plained variance (70%) in the spontaneous speech
score, not addressed by the read/repeat scores.
This could be due to a difference in the pronun-
ciation quality and fluency of the candidates in
reading/repeating text vs. speaking spontaneously
and also due to the additional parameters of gram-
mar and content characteristics in the spontaneous
speech score. Thus, an automatic score mimick-
ing the read/repeat expert grades, which is a solved
problem, is inadequate for our task.
The first score is used for all subsequent discus-
sion and development of models.
</bodyText>
<subsectionHeader confidence="0.998272">
5.2 Crowdsourced Tasks
</subsectionHeader>
<bodyText confidence="0.999995352941177">
The 319 speech sample assessment task was
posted on Amazon Mechanical Turk (AMT). Each
task was completed by three workers. In total, 71
unique workers completed the tasks. The majority
of workers (90%) belonged to USA and India.
The task took on an average 8–9 minutes to
complete and a worker was paid between 6–10
cents per task including a bonus which was paid
on completion of every 4 tasks. We also got the
speech transcribed by experts to find the accuracy
we could get from turks. The average transcrip-
tion accuracy for a worker was 82.4%9. This sig-
nificantly improved to 95.4% when the transcrip-
tions of the three workers were combined using
the ROVER algorithm. In comparison, the aver-
age automatic transcription of a speech recogni-
tion engine was 59.8%.
</bodyText>
<subsectionHeader confidence="0.987431">
5.3 Regression Modeling
</subsectionHeader>
<bodyText confidence="0.994126625">
The data set was split into two sets: train and vali-
dation. The train-set had 75% of the sample points
whereas the validation set had 25%. The split was
done randomly making sure that the grade distri-
bution in both the sets was similar. While learning
the model, a 4-fold cross validation was performed
on the train sample.
Linear ridge regression, Neural Networks and
SVM regression with different kernels were used
to build the models. The least cross-validation er-
ror was used to select the models. We used some
simple techniques for feature selection including
forward feature selection and the algorithm which
removes all but the k highest correlating features.
Regression parameters: For linear regression
with regularization, optimal ridge coefficient A,
</bodyText>
<footnote confidence="0.923239">
9PBP similar text function was used as similarity metric.
</footnote>
<page confidence="0.996467">
1090
</page>
<tableCaption confidence="0.99946">
Table 1: Regression Results
</tableCaption>
<table confidence="0.999847625">
Technique Model Code Feature Type Train r Validation r
Ridge Regression RR-1 RS/LR 0.51 0.47
RR-2 Pure ML 0.54 0.47
RR-3 Crowd Grades 0.63 0.57
RR-4 ML-CS 0.55 0.60
RR-5 All 0.76 0.76
SVM SVM-1 RS/LR 0.50 0.46
SVM-2 Pure ML 0.53 0.46
SVM-3 Crowd Grades 0.62 0.57
SVM-4 ML-CS 0.60 0.61
SVM-5 All 0.75 0.74
Neural Networks NN-1 RS/LR 0.56 0.51
NN-2 Pure ML 0.60 0.44
NN-3 Crowd Grades 0.63 0.57
NN-4 ML-CS 0.66 0.57
NN-5 All 0.80 0.76
</table>
<bodyText confidence="0.999787181818182">
between 1 and 1000, was selected based on the the
least RMS error in cross-validation. For support
vector machines we tested two kernels: linear and
radial basis function. In order to select the optimal
SVM model, we varied the penalty factor C, pa-
rameters γ and E, the SVM kernel and the selected
set of values that gave us the lowest RMS error in
cross-validation. The Neural Networks model had
one hidden layer and 5 to 10 neurons.
Feature sets used: The experiments were car-
ried out on five sets of features:
</bodyText>
<listItem confidence="0.994094916666667">
• RS/LR: A set of features generated by force
aligning read/repeated by candidates.
• Pure ML: Features generated by automatic
speech transcription of spontaneous speech
using a speech recognizer.
• Crowd Grades: A set of features pertaining to
grades given by the crowd.
• ML–CS: NLP and FA features generated by
force aligning free speech on crowdsourced
transcription.
• All: NLP and FA features from crowd-
sourced transcription and Crowd Grades.
</listItem>
<bodyText confidence="0.999982238095238">
Here, the first set, RS/LR, helps us to know how
well we can predict spontaneous speech grades by
simply using the read/speak speech of the candi-
date and without using his/her spontaneous speech
sample. This provides a comparison baseline. The
second approach evaluates how well we can grade
spontaneous speech of the candidate using ma-
chine learning approaches only. The third feature
shows the efficacy of directly using grades given
by crowd, while the fourth finds how well machine
learning can do if it has a fairly accurate transcrip-
tion of the speech by the crowd. The final fifth
set tests what happens if we combine the third and
fourth set of features, i.e. make use of both the
crowdsourced transcription and the crowd grades.
In the following subsection, the features per-
taining to ML-CS approach are referred to as ML-
CS, those pertaining to natural language process-
ing on crowdsourced transcription are referred to
as NLP features while the one pertaining to crowd
grades are referred to as Crowd Grades.
</bodyText>
<subsectionHeader confidence="0.993236">
5.4 Observations
</subsectionHeader>
<bodyText confidence="0.99979425">
The results of the experiments are tabulated in Ta-
ble 1. We report the Pearson coefficient of corre-
lation (r) for the different models against the ex-
pert grades. These are the results for the models
selected according to least cross-validation error.
The best cross-validation error in case of SVMs
was obtained for the linear kernel.
All the following observations are based on the
validation error. All three techniques perform sim-
ilarly with Neural Networks doing slightly worse
in some cases. The broad trends across fea-
ture–sets remain similar across different modeling
</bodyText>
<page confidence="0.985674">
1091
</page>
<bodyText confidence="0.999952028571429">
techniques. We will be referring to the ridge re-
gression results for further discussion.
Firstly, it is observed that the read/repeat fea-
tures predict the spontaneous speech score with
low accuracy (r = 0.47). This implies that read-
/repeat speech and derived features are inadequate
to grade a person’s spontaneous speech, the ulti-
mate test of a person’s spoken language skills.
The second observation is that the ML-only ap-
proach using spontaneous speech features (Model
RR-2) is also inadequate to grade spontaneous
speech and does worse than approaches that uses
features from crowdsourced transcription (Model
RR-4). This clearly shows the value of getting ac-
curate transcription from workers towards better
features and model.
Further, among the crowdsourcing approaches,
we find that the crowd-grades (Model RR-3) does
equivalently well (and sometimes worse) than the
model using features derived from the crowd-
sourced speech (Model RR-4). However, when
we combine all the features from crowdsourcing
including the crowd grades, we find much better
prediction accuracy (r = 0.76). This shows that
the crowd grades feature provides some orthogo-
nal information as compared to the features from
the crowdsourced transcription, towards predict-
ing the grade given by experts.
The validation r for Model RR-5 is 0.76. We
find that the expert agreement on the validation
sample is 0.78. Thus, our predicted score rivals the
agreement of experts. This shows great promise
for the technique to be used in a high-stake test
setting.
In summary, we show the following:
</bodyText>
<listItem confidence="0.989192222222222">
• Read/repeat speech features are inadequate to
predict spontaneous speech scores.
• ML only approach based on spontaneous
speech samples is also inadequate for the pur-
pose.
• Features derived from crowdsourced tran-
scription (or even crowd grades) do better
than a ML only approach.
• When considering features from crowd-
</listItem>
<bodyText confidence="0.840607666666667">
sourced transcription and crowd grades to-
gether, we can predict spontaneous speech
scores as well as those done by experts.
</bodyText>
<sectionHeader confidence="0.99456" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999967">
We addressed the problem of evaluating spon-
taneous speech using a combination of machine
learning and crowdsourcing. To achieve this, we
post the task of speech transcription to the crowd.
Additionally, we also get spoken English grades
from the crowd. We are able to derive accurate
features by force aligning the speech sample on
the crowdsourced text. We experimented our tech-
nique on expert–graded speech samples of adult
non–native speakers. Using these features in a
regression model, we are able to predict expert
grades with much higher accuracy than a machine
learning only approach. These features also pre-
dict equivalent or better than crowd grades and a
combination of these two outperforms all other ap-
proaches. Our approach shows an accuracy that
rivals that of expert agreement.
Our technique has a promise of higher accuracy
but has some trade-offs compared to fully auto-
mated approaches. First, there is a cost for ev-
ery assessment done and the scalability depends
on the number of non-expert workers available.
Though these drawbacks exist, we were able get
tasks done inexpensively. We recently had the
crowd rate a hundred samples in a day without any
challenge. Second, our approach doesn’t provide
instant grades. This works fine in many scenarios,
but doesn’t cater well to providing real-time feed-
back. Real time crowdsourcing has been an active
area of research (Bernstein et al., 2011; Lasecki et
al., 2013) and is an area for future work for us as
well.
</bodyText>
<sectionHeader confidence="0.998482" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9236178">
AfterTheDeadline. 2014. www.afterthedeadline.com.
Ahmet Aker, Mahmoud El-Haj, M-Dyaa Albakour, and
Udo Kruschwitz. 2012. Assessing crowdsourcing
quality through objective tasks. In LREC, pages
1456–1461. Citeseer.
Michael S Bernstein, Joel Brandt, Robert C Miller, and
David R Karger. 2011. Crowds in two seconds: En-
abling realtime crowd-powered interfaces. In Pro-
ceedings of the 24th annual ACM symposium on
User interface software and technology, pages 33–
42. ACM.
Suma Bhat, Huichao Xue, and Su-Youn Yoon. 2014.
Shallow analysis based assessment of syntactic com-
plexity for automated speech scoring. In Proceed-
ings of the 52nd Annual Meeting of the Association
</reference>
<page confidence="0.995895">
1092
</page>
<bodyText confidence="0.9957865">
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 1305–1315. Association for Computa-
tional Linguistics.
Cahit Guven and Asadul Islam. 2013. Age at migra-
tion, language proficiency and socio-economic out-
comes: Evidence from australia. Technical report.
</bodyText>
<reference confidence="0.998653588235294">
David Brazil. 1995. A grammar of speech. Oxford
University Press, USA.
Christopher Brumfit and Christopher J Brumfit. 1984.
Communicative methodology in language teaching:
The roles of fluency and accuracy, volume 129.
Cambridge University Press Cambridge.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 1997.
Automatic evaluation of dutch pronunciation by us-
ing speech recognition technology. In Automatic
Speech Recognition and Understanding, 1997. Pro-
ceedings., 1997 IEEE Workshop on, pages 622–629.
IEEE.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2000.
Quantitative assessment of second language learn-
ers’ fluency by means of automatic speech recogni-
tion technology. The Journal of the Acoustical Soci-
ety ofAmerica, 107(2):989–999.
Eric John Dobson. 1957. English Pronunciation,
1500-1700: Phonology, volume 2. Clarendon Press.
Bin Dong, Qingwei Zhao, Jianping Zhang, and
Yonghong Yan. 2004. Automatic assessment of
pronunciation quality. In Chinese Spoken Lan-
guage Processing, 2004 International Symposium
on, pages 137–140. IEEE.
Elizabeth J Erling and Philip Seargeant. 2013. English
and development: Policy, pedagogy and globaliza-
tion, volume 17. Multilingual Matters.
Enrique Estell´es-Arolas and Fernando Gonz´alez-
Ladr´on-de Guevara. 2012. Towards an integrated
crowdsourcing definition. Journal of Information
science, 38(2):189–200.
Cambridge EOCL Examinations. 2011. Using the
CEFR: Principles of good practice. at University of
Cambridge.
Jonathan G Fiscus. 1997. A post-processing system
to yield reduced word error rates: Recognizer out-
put voting error reduction (rover). In Automatic
Speech Recognition and Understanding, 1997. Pro-
ceedings., 1997 IEEE Workshop on, pages 347–354.
IEEE.
Horacio Franco, Victor Abrash, Kristin Precoda, Harry
Bratt, Ramana Rao, John Butzberger, Romain
Rossier, and Federico Cesari. 2000. The sri
eduspeaktm system: Recognition and pronunciation
scoring for language learning. Proceedings of In-
STILL 2000, pages 123–128.
John S Garofolo, Lori F Lamel, William M Fisher,
Jonathon G Fiscus, and David S Pallett. 1993.
Darpa timit acoustic-phonetic continous speech cor-
pus cd-rom. nist speech disc 1-1.1. NASA STI/Recon
Technical Report N, 93:27403.
Eric Hagley. 2010. Creation of speaking tests for efl
communication classes. ł, (8):33–41.
Gene B Halleck. 1995. Assessing oral proficiency: A
comparison of holistic and objective measures. The
Modern Language Journal, 79(2):223–234.
Mehdi Hosseini, Ingemar J Cox, Nataˇsa Mili´c-
Frayling, Gabriella Kazai, and Vishwa Vinay. 2012.
On aggregating labels from multiple crowd workers
to infer relevance of documents. In Advances in in-
formation retrieval, pages 182–194. Springer.
Jeff Howe. 2006. The rise of crowdsourcing. Wired
magazine, 14(6):1–4.
Akinori Ito, Tadao Nagasawa, Hirokazu Ogasawara,
Motoyuki Suzuki, and Shozo Makino. 2006. Au-
tomatic detection of english mispronunciation using
speaker adaptation and automatic assessment of en-
glish intonation and rhythm. Educational technol-
ogy research, 29(1):13–23.
Christos Koniaris and Olov Engwall. 2011. Percep-
tual differentiation modeling explains phoneme mis-
pronunciation by non-native speakers. In Acous-
tics, Speech and Signal Processing (ICASSP), 2011
IEEE International Conference on, pages 5704–
5707. IEEE.
Stephen A Kunath and Steven H Weinberger. 2010.
The wisdom of the crowd’s ear: speech accent rat-
ing and annotation with amazon mechanical turk. In
Proceedings of the NAACL HLT 2010 Workshop on
Creating Speech and Language Data with Amazon’s
Mechanical Turk, pages 168–171. Association for
Computational Linguistics.
Walter S Lasecki, Christopher D Miller, and Jeffrey P
Bigham. 2013. Warping time for more effec-
tive real-time crowdsourcing. In Proceedings of the
SIGCHI Conference on Human Factors in Comput-
ing Systems, pages 2033–2036. ACM.
Mark Lejk and Michael Wyvill. 2001. The effect of
the inclusion of selfassessment with peer assessment
of contributions to a group project: A quantitative
study of secret and agreed assessments. Assessment
&amp; Evaluation in Higher Education, 26(6):551–561.
Hongyan Li, Shijin Wang, Jiaen Liang, Shen Huang,
and Bo Xu. 2009. High performance automatic mis-
pronunciation detection method based on neural net-
work and trap features. In INTERSPEECH, pages
1911–1914.
LightSide. 2013. http://lightsidelabs.com/.
David Little. 2006. The common european framework
of reference for languages: Content, purpose, origin,
reception and impact. Language Teaching, 39:167–
190, 7.
</reference>
<page confidence="0.521686">
1093
</page>
<reference confidence="0.999766432989691">
David Little. 2007. The common european framework
of reference for languages: Perspectives on the mak-
ing of supranational language education policy. The
Modern Language Journal, 91(4):645–655.
Nitin Madnani, Joel Tetreault, Martin Chodorow, and
Alla Rozovskaya. 2011. They can help: using
crowdsourcing to improve the evaluation of gram-
matical error detection systems. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies: short papers-Volume 2, pages 508–513. Asso-
ciation for Computational Linguistics.
Leonardo Neumeyer, Horacio Franco, Mitchel Wein-
traub, and Patti Price. 1996. Automatic text-
independent pronunciation scoring of foreign lan-
guage student speech. In Spoken Language, 1996.
ICSLP 96. Proceedings., Fourth International Con-
ference on, volume 3, pages 1457–1460. IEEE.
Quoc Viet Hung Nguyen, Tam Nguyen Thanh, Tran
Lam Ngoc, and Karl Aberer. 2013. An evaluation
of aggregation techniques in crowdsourcing. In The
14th International Conference on Web Information
System Engineering (WISE), 2013, number EPFL-
CONF-187456.
Gabriele Paolacci, Jesse Chandler, and Panagiotis G
Ipeirotis. 2010. Running experiments on amazon
mechanical turk. Judgment and Decision making,
5(5):411–419.
Mitchell Aaron Peabody. 2011. Methods for pronunci-
ation assessment in computer aided language learn-
ing. Ph.D. thesis, Massachusetts Institute of Tech-
nology.
Donald E Powers, Jill C Burstein, Martin Chodorow,
Mary E Fowles, and Karen Kukich. 2002. Stumping
e-rater: challenging the validity of automated essay
scoring. Computers in Human Behavior, 18(2):103–
134.
K˚are Sj¨olander. 2003. An hmm-based system for au-
tomatic segmentation and alignment of speech. In
Proceedings of Fonetik, volume 2003, pages 93–96.
Citeseer.
Robert Stalnaker. 1999. The problem of logical omni-
science, ii. context and content: Essays on intention-
ality in speech and thought (pp. 255–273).
Lynn Streeter, Jared Bernstein, Peter Foltz, and Don-
ald DeLand. 2011. Pearsons automated scoring of
writing, speaking, and mathematics.
SVAR. 2014. http://www.aspiringminds.in/talent-
evaluation/spoken-english-SVAR.html.
Joel R Tetreault, Elena Filatova, and Martin Chodorow.
2010. Rethinking grammatical error annotation and
evaluation with the amazon mechanical turk. In Pro-
ceedings of the NAACL HLT 2010 Fifth Workshop
on Innovative Use of NLP for Building Educational
Applications, pages 45–48. Association for Compu-
tational Linguistics.
Amazon Mechanical Turk. 2014.
https://requester.mturk.com/tour.
Nathan Van Houdnos. 2011. Can the internet grade
math? crowdsourcing a complex scoring task and
picking the optimal crowd size. Dietrich College of
Humanities and Social Sciences at Research Show-
case @ CMU.
Hao Wang and Helen Meng. 2012. Deriving percep-
tual gradation of l2 english mispronunciations using
crowdsourcing and the workerrank algorithm. Proc.
of the 15th Oriental COCOSDA, Macau, China,
pages 9–12.
Hao Wang, Xiaojun Qian, and Helen Meng. 2014.
Phonological modeling of mispronunciation gra-
dations in l2 english speech of l1 chinese learn-
ers. In Acoustics, Speech and Signal Processing
(ICASSP), 2014 IEEE International Conference on,
pages 7714–7718. IEEE.
Paul Whitla. 2009. Crowdsourcing and its application
in marketing activities. Contemporary Management
Research, 5(1).
Steve Young, Gunnar Evermann, Mark Gales, Thomas
Hain, Dan Kershaw, Xunying Liu, Gareth Moore,
Julian Odell, Dave Ollason, Dan Povey, et al. 2006.
The htk book (for htk version 3.4). Cambridge uni-
versity engineering department, 2(2):2–3.
Omar F Zaidan and Chris Callison-Burch. 2011.
Crowdsourcing translation: Professional quality
from non-professionals. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies-
Volume 1, pages 1220–1229. Association for Com-
putational Linguistics.
Klaus Zechner, Derrick Higgins, and Xiaoming Xi.
2007. Speechrater: A construct-driven approach
to scoring spontaneous non-native speech. Proc.
SLaTE.
Klaus Zechner, Derrick Higgins, Xiaoming Xi, and
David M Williamson. 2009. Automatic scoring of
non-native spontaneous speech in tests of spoken en-
glish. Speech Communication, 51(10):883–895.
</reference>
<page confidence="0.994587">
1094
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.414301">
<title confidence="0.965947">Automatic Spontaneous Speech Grading: A Novel Feature Technique using the Crowd</title>
<author confidence="0.610336">Vinay Shashidhar Nishant Pandey Varun Aggarwal</author>
<affiliation confidence="0.504319">Aspiring Minds Aspiring Minds Aspiring Minds</affiliation>
<email confidence="0.972404">vinay.shashidhar@aspiringminds.comnishant.pandey@aspiringminds.comvarun@aspiringminds.com</email>
<abstract confidence="0.998313222222222">In this paper, we address the problem of evaluating spontaneous speech using a combination of machine learning and crowdsourcing. Machine learning techniques inadequately solve the stated problem because automatic speakerindependent speech transcription is inaccurate. The features derived from it are also inaccurate and so is the machine learning model developed for speech evaluation. To address this, we post the task of speech transcription to a large community of online workers (crowd). We also get spoken English grades from crowd. We achieve transcription accuracy by combining transcriptions from multiple crowd workers. Speech and prosody features are derived by force aligning the speech samples on these accurate transcriptions. tionally, we derive surface and semantic level features directly from the transcription. To demonstrate the efficacy of our approach we performed experiments on expert–graded speech sample of adult non–native speakers. Using these features in a regression model, we are able achieve a Pearson correlation of expert grades, an accuracy much higher than any previously reported machine learning approach. Our approach has an accuracy that rivals that of expert agreement. This work is timely given the huge requirement of spoken English training and assessment.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>AfterTheDeadline</author>
</authors>
<date>2014</date>
<note>www.afterthedeadline.com.</note>
<contexts>
<context position="14231" citStr="AfterTheDeadline, 2014" startWordPosition="2252" endWordPosition="2254">sexes and dialects. A number of speech quality features are derived, which include– rate of speech, position and length of pauses, log likelihood of recognition, posterior probability, hesitations and repetitions etc. These features are well known in literature and may be referred from (Neumeyer et al., 1996; Zechner et al., 2009; Cucchiarini et al., 2000). These features are predictive of the pronunciation and fluency of the candidate. • NLP features: These features predict the content quality and grammar of the spoken content7. They were derived using standard NLP packages (LightSide, 2013; AfterTheDeadline, 2014) on the crowdsourced transcription. The package calculates surface level features such as the number of words, complexity or difficulty of words and the number of common words used. It also calculates semantic features like the coherency in text, context of the words spoken, sentiment of the text and grammar correctness. In the current system, we do not use any prompt specific features such as occurrence of specific words or phrases. These features are predictive of the grammar and content organization of the sample. All the features described above were obtained for the spontaneous speech sam</context>
</contexts>
<marker>AfterTheDeadline, 2014</marker>
<rawString>AfterTheDeadline. 2014. www.afterthedeadline.com.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmet Aker</author>
<author>Mahmoud El-Haj</author>
<author>M-Dyaa Albakour</author>
<author>Udo Kruschwitz</author>
</authors>
<title>Assessing crowdsourcing quality through objective tasks.</title>
<date>2012</date>
<booktitle>In LREC,</booktitle>
<pages>1456--1461</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="16924" citStr="Aker et al., 2012" startWordPosition="2691" endWordPosition="2694">voting algorithm to combine multiple transcriptions with errors, to obtain the best estimate of the correct transcription. It is reported to lead to an error reduction of 20- 25%. ROVER proceeds in two stages: first the outputs are aligned and a single word transcription network (WTN) is built. The second stage consists of selecting the best scoring word (with the highest number of votes) at each node. Several methods have been used in the past for increasing the reliability of the grades given by the crowd by identifying and correcting any biases and removing non-serious/low quality workers (Aker et al., 2012). One of the key techniques for this involves inserting gold standard tasks with known answers to get an estimate of the worker’s ability (Nguyen et al., 2013). The gold standard tasks are similar to real tasks and the workers have no way to distinguish between the two. Our tasks took workers a reasonable amount of time (8-10 minutes). It wasn’t hence feasible to insert a gold standard task, as done typically, with every task to be completed. To overcome this problem, we propose an innovative approach where a risk is assigned to a worker based on his/her performance on the gold standard tasks.</context>
</contexts>
<marker>Aker, El-Haj, Albakour, Kruschwitz, 2012</marker>
<rawString>Ahmet Aker, Mahmoud El-Haj, M-Dyaa Albakour, and Udo Kruschwitz. 2012. Assessing crowdsourcing quality through objective tasks. In LREC, pages 1456–1461. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael S Bernstein</author>
<author>Joel Brandt</author>
<author>Robert C Miller</author>
<author>David R Karger</author>
</authors>
<title>Crowds in two seconds: Enabling realtime crowd-powered interfaces.</title>
<date>2011</date>
<booktitle>In Proceedings of the 24th annual ACM symposium on User interface software and technology,</booktitle>
<pages>33--42</pages>
<publisher>ACM.</publisher>
<marker>Bernstein, Brandt, Miller, Karger, 2011</marker>
<rawString>Michael S Bernstein, Joel Brandt, Robert C Miller, and David R Karger. 2011. Crowds in two seconds: Enabling realtime crowd-powered interfaces. In Proceedings of the 24th annual ACM symposium on User interface software and technology, pages 33– 42. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suma Bhat</author>
<author>Huichao Xue</author>
<author>Su-Youn Yoon</author>
</authors>
<title>Shallow analysis based assessment of syntactic complexity for automated speech scoring.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association</booktitle>
<contexts>
<context position="5282" citStr="Bhat et al., 2014" startWordPosition="802" endWordPosition="805">eech to text conversion for such candidates has a low accuracy, force alignment of the speech on this inaccurate text makes the features and the model inaccurate. We present a semi-automated approach to grade short duration (45 seconds) spontaneous speech. We accurately predict a holistic score which is based on the pronunciation, fluency, content characteristics and grammar of the speech sample, as determined by experts. Multiple previous studies in language acquisition and second language research conclusively show that proficiency in a second language can be characterized by these factors (Bhat et al., 2014). Being able to provide a holistic score is of high interest in both educational testing (Zechner et al., 2009) and job related testing (Streeter et al., 2011). Institutions and firms look for a holistic score, say based on CEFR, a standard to describe spoken English assessment (Little, 2006; Little, 2007), to make an accept or reject decision on candidates. Currently, an expert based assessment is used for these purposes. Our method involves combining machine learning with a crowdsourcing layer. Crowdsourcing (Estell´es-Arolas and Gonz´alez-Ladr´on-de Guevara, 2012) is the process of getting </context>
</contexts>
<marker>Bhat, Xue, Yoon, 2014</marker>
<rawString>Suma Bhat, Huichao Xue, and Su-Youn Yoon. 2014. Shallow analysis based assessment of syntactic complexity for automated speech scoring. In Proceedings of the 52nd Annual Meeting of the Association</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Brazil</author>
</authors>
<title>A grammar of speech.</title>
<date>1995</date>
<publisher>Oxford University Press, USA.</publisher>
<contexts>
<context position="11472" citStr="Brazil, 1995" startWordPosition="1807" endWordPosition="1808">was prepared with the help of experts. This score is a function of the pronunciation, fluency, content organization and grammar quality of the speech sample. Broadly speaking, Pronunciation (Dobson, 1957) refers to the correctness in the utterance of the phonemes of a word by the students as per neutral accent. Fluency (Brumfit and Brumfit, 1984) refers to a desired rate of speech along with the absence of hesitations, false starts and stops etc. Content organization (Stalnaker, 1999) measures the candidate’s ability to structure the information disposition and present it coherently. Grammar (Brazil, 1995) measures how well the syntax of the language was followed by the candidate. 4The subjects of our study use English as their second language and hail from various backgrounds, dialects and educational qualifications. 5This is as per global standards of spoken English assessment. High stake tests such as TOEFL provide the candidate 15-30 seconds to think before responding to a spontaneous speech task. 1087 Figure 2: Our intuition of how different features predict the holistic score. In the next section we discuss the features which are used in the prediction algorithm. 3 Features We use three c</context>
</contexts>
<marker>Brazil, 1995</marker>
<rawString>David Brazil. 1995. A grammar of speech. Oxford University Press, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Brumfit</author>
<author>Christopher J Brumfit</author>
</authors>
<title>Communicative methodology in language teaching: The roles of fluency and accuracy, volume 129.</title>
<date>1984</date>
<publisher>Cambridge University Press Cambridge.</publisher>
<contexts>
<context position="11207" citStr="Brumfit and Brumfit, 1984" startWordPosition="1764" endWordPosition="1767">h samples from the read and repeat sections with high accuracy (SVAR, 2014). Our goal in this paper is to evaluate the spontaneous speech of the candidate and provide a composite score based on it. A 5 point rubric for the composite score, similar to CEFR (Examinations, 2011), was prepared with the help of experts. This score is a function of the pronunciation, fluency, content organization and grammar quality of the speech sample. Broadly speaking, Pronunciation (Dobson, 1957) refers to the correctness in the utterance of the phonemes of a word by the students as per neutral accent. Fluency (Brumfit and Brumfit, 1984) refers to a desired rate of speech along with the absence of hesitations, false starts and stops etc. Content organization (Stalnaker, 1999) measures the candidate’s ability to structure the information disposition and present it coherently. Grammar (Brazil, 1995) measures how well the syntax of the language was followed by the candidate. 4The subjects of our study use English as their second language and hail from various backgrounds, dialects and educational qualifications. 5This is as per global standards of spoken English assessment. High stake tests such as TOEFL provide the candidate 15</context>
</contexts>
<marker>Brumfit, Brumfit, 1984</marker>
<rawString>Christopher Brumfit and Christopher J Brumfit. 1984. Communicative methodology in language teaching: The roles of fluency and accuracy, volume 129. Cambridge University Press Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catia Cucchiarini</author>
<author>Helmer Strik</author>
<author>Lou Boves</author>
</authors>
<title>Automatic evaluation of dutch pronunciation by using speech recognition technology.</title>
<date>1997</date>
<booktitle>In Automatic Speech Recognition and Understanding,</booktitle>
<pages>622--629</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="1820" citStr="Cucchiarini et al., 1997" startWordPosition="258" endWordPosition="261">riments on an expert–graded speech sample of 319 adult non–native speakers. Using these features in a regression model, we are able achieve a Pearson correlation of 0.76 with expert grades, an accuracy much higher than any previously reported machine learning approach. Our approach has an accuracy that rivals that of expert agreement. This work is timely given the huge requirement of spoken English training and assessment. 1 Introduction Automatic evaluation of spoken English has been of keen interest for more than two decades (Zechner et al., 2007; Neumeyer et al., 1996; Franco et al., 2000; Cucchiarini et al., 1997). It can help learners get feedback in a scalable manner, help build better English training software and also help companies and institutions filter and select prospective employees more effectively. The problem acquires significance given the evidence that better English leads to better employment outcome, wages and promotions (Guven and Islam, 2013). There has been a considerable success in automatically scoring spoken English, when the spoken text is known a priori (Cucchiarini et al., 2000; Franco et al., 2000). In these cases, the candidate is asked to either read a given text or listen </context>
<context position="4318" citStr="Cucchiarini et al., 1997" startWordPosition="650" endWordPosition="653"> Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1085–1094, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics an unexplained variance in the spontaneous speech scores (see Section 5). Generally, candidates who score high on spontaneous speech also score high on read speech and not vice versa. Given the primacy of spontaneous speech evaluation in judging a person’s language capability, there is considerable interest in doing it automatically (Cucchiarini et al., 1997; Dong et al., 2004). Automated approaches for the same have not worked well (Powers et al., 2002; Cucchiarini et al., 2000) primarily because speakerindependent speech recognition is a tough computer science problem. This is exacerbated when the speakers are not proficient in the language or are non-natives (Powers et al., 2002). Given that speech to text conversion for such candidates has a low accuracy, force alignment of the speech on this inaccurate text makes the features and the model inaccurate. We present a semi-automated approach to grade short duration (45 seconds) spontaneous speec</context>
</contexts>
<marker>Cucchiarini, Strik, Boves, 1997</marker>
<rawString>Catia Cucchiarini, Helmer Strik, and Lou Boves. 1997. Automatic evaluation of dutch pronunciation by using speech recognition technology. In Automatic Speech Recognition and Understanding, 1997. Proceedings., 1997 IEEE Workshop on, pages 622–629. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catia Cucchiarini</author>
<author>Helmer Strik</author>
<author>Lou Boves</author>
</authors>
<title>Quantitative assessment of second language learners’ fluency by means of automatic speech recognition technology.</title>
<date>2000</date>
<journal>The Journal of the Acoustical Society ofAmerica,</journal>
<volume>107</volume>
<issue>2</issue>
<contexts>
<context position="2319" citStr="Cucchiarini et al., 2000" startWordPosition="336" endWordPosition="339">terest for more than two decades (Zechner et al., 2007; Neumeyer et al., 1996; Franco et al., 2000; Cucchiarini et al., 1997). It can help learners get feedback in a scalable manner, help build better English training software and also help companies and institutions filter and select prospective employees more effectively. The problem acquires significance given the evidence that better English leads to better employment outcome, wages and promotions (Guven and Islam, 2013). There has been a considerable success in automatically scoring spoken English, when the spoken text is known a priori (Cucchiarini et al., 2000; Franco et al., 2000). In these cases, the candidate is asked to either read a given text or listen to some speech and repeat it. For these tasks, the scores generated by an automatic system on parameters such as pronunciation and fluency closely mimic those given by human experts. The primary approach behind a majority of these systems is to force align the speech sample on the known text using an HMM–based acoustic model. Features such as likelihood, posterior probability and fluency related features are derived from the aligned speech and a machine learning model is used to predict expert </context>
<context position="4442" citStr="Cucchiarini et al., 2000" startWordPosition="671" endWordPosition="675">nference on Natural Language Processing, pages 1085–1094, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics an unexplained variance in the spontaneous speech scores (see Section 5). Generally, candidates who score high on spontaneous speech also score high on read speech and not vice versa. Given the primacy of spontaneous speech evaluation in judging a person’s language capability, there is considerable interest in doing it automatically (Cucchiarini et al., 1997; Dong et al., 2004). Automated approaches for the same have not worked well (Powers et al., 2002; Cucchiarini et al., 2000) primarily because speakerindependent speech recognition is a tough computer science problem. This is exacerbated when the speakers are not proficient in the language or are non-natives (Powers et al., 2002). Given that speech to text conversion for such candidates has a low accuracy, force alignment of the speech on this inaccurate text makes the features and the model inaccurate. We present a semi-automated approach to grade short duration (45 seconds) spontaneous speech. We accurately predict a holistic score which is based on the pronunciation, fluency, content characteristics and grammar </context>
<context position="13966" citStr="Cucchiarini et al., 2000" startWordPosition="2210" endWordPosition="2213"> also be used for an aggregation strategy, once the number of tasks done by every individual worker increases. In our current experiments, this number wasn’t very high. corpus of phonemically and lexically transcribed speech of American English speakers of different sexes and dialects. A number of speech quality features are derived, which include– rate of speech, position and length of pauses, log likelihood of recognition, posterior probability, hesitations and repetitions etc. These features are well known in literature and may be referred from (Neumeyer et al., 1996; Zechner et al., 2009; Cucchiarini et al., 2000). These features are predictive of the pronunciation and fluency of the candidate. • NLP features: These features predict the content quality and grammar of the spoken content7. They were derived using standard NLP packages (LightSide, 2013; AfterTheDeadline, 2014) on the crowdsourced transcription. The package calculates surface level features such as the number of words, complexity or difficulty of words and the number of common words used. It also calculates semantic features like the coherency in text, context of the words spoken, sentiment of the text and grammar correctness. In the curre</context>
</contexts>
<marker>Cucchiarini, Strik, Boves, 2000</marker>
<rawString>Catia Cucchiarini, Helmer Strik, and Lou Boves. 2000. Quantitative assessment of second language learners’ fluency by means of automatic speech recognition technology. The Journal of the Acoustical Society ofAmerica, 107(2):989–999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric John Dobson</author>
</authors>
<date>1957</date>
<booktitle>English Pronunciation, 1500-1700: Phonology,</booktitle>
<volume>2</volume>
<publisher>Clarendon Press.</publisher>
<contexts>
<context position="11063" citStr="Dobson, 1957" startWordPosition="1741" endWordPosition="1742">ure task clarity. The complete test takes 16-20 minutes to complete, depending on the test version. Currently, SVAR evaluates speech samples from the read and repeat sections with high accuracy (SVAR, 2014). Our goal in this paper is to evaluate the spontaneous speech of the candidate and provide a composite score based on it. A 5 point rubric for the composite score, similar to CEFR (Examinations, 2011), was prepared with the help of experts. This score is a function of the pronunciation, fluency, content organization and grammar quality of the speech sample. Broadly speaking, Pronunciation (Dobson, 1957) refers to the correctness in the utterance of the phonemes of a word by the students as per neutral accent. Fluency (Brumfit and Brumfit, 1984) refers to a desired rate of speech along with the absence of hesitations, false starts and stops etc. Content organization (Stalnaker, 1999) measures the candidate’s ability to structure the information disposition and present it coherently. Grammar (Brazil, 1995) measures how well the syntax of the language was followed by the candidate. 4The subjects of our study use English as their second language and hail from various backgrounds, dialects and ed</context>
</contexts>
<marker>Dobson, 1957</marker>
<rawString>Eric John Dobson. 1957. English Pronunciation, 1500-1700: Phonology, volume 2. Clarendon Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bin Dong</author>
<author>Qingwei Zhao</author>
<author>Jianping Zhang</author>
<author>Yonghong Yan</author>
</authors>
<title>Automatic assessment of pronunciation quality.</title>
<date>2004</date>
<booktitle>In Chinese Spoken Language Processing, 2004 International Symposium on,</booktitle>
<pages>137--140</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="3086" citStr="Dong et al., 2004" startWordPosition="466" endWordPosition="469">he scores generated by an automatic system on parameters such as pronunciation and fluency closely mimic those given by human experts. The primary approach behind a majority of these systems is to force align the speech sample on the known text using an HMM–based acoustic model. Features such as likelihood, posterior probability and fluency related features are derived from the aligned speech and a machine learning model is used to predict expert grades (Neumeyer et al., 1996; Franco et al., 2000; Cucchiarini et al., 1997). Some approaches additionally use prosody and energy related features (Dong et al., 2004). More recently, this research has moved towards the assessment of higher granularity metrics like the mispronunciation of particular phonemes (Li et al., 2009; Ito et al., 2006; Koniaris and Engwall, 2011). In spontaneous speech evaluation, the candidate is asked to speak on a topic or answer a question and what he/she speaks isn’t known priori. Evaluation of spontaneous speech is the ultimate test of a candidate’s proficiency in speaking a language (Hagley, 2010; Halleck, 1995). While scores from the evaluation of read/repeat speech do correlate with spontaneous speech evaluation, there rema</context>
<context position="4338" citStr="Dong et al., 2004" startWordPosition="654" endWordPosition="657">nnual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1085–1094, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics an unexplained variance in the spontaneous speech scores (see Section 5). Generally, candidates who score high on spontaneous speech also score high on read speech and not vice versa. Given the primacy of spontaneous speech evaluation in judging a person’s language capability, there is considerable interest in doing it automatically (Cucchiarini et al., 1997; Dong et al., 2004). Automated approaches for the same have not worked well (Powers et al., 2002; Cucchiarini et al., 2000) primarily because speakerindependent speech recognition is a tough computer science problem. This is exacerbated when the speakers are not proficient in the language or are non-natives (Powers et al., 2002). Given that speech to text conversion for such candidates has a low accuracy, force alignment of the speech on this inaccurate text makes the features and the model inaccurate. We present a semi-automated approach to grade short duration (45 seconds) spontaneous speech. We accurately pre</context>
</contexts>
<marker>Dong, Zhao, Zhang, Yan, 2004</marker>
<rawString>Bin Dong, Qingwei Zhao, Jianping Zhang, and Yonghong Yan. 2004. Automatic assessment of pronunciation quality. In Chinese Spoken Language Processing, 2004 International Symposium on, pages 137–140. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth J Erling</author>
<author>Philip Seargeant</author>
</authors>
<title>English and development: Policy, pedagogy and globalization, volume 17. Multilingual Matters.</title>
<date>2013</date>
<contexts>
<context position="7191" citStr="Erling and Seargeant, 2013" startWordPosition="1113" endWordPosition="1116">a classic example is the task of finding a particular object in an image. There is a large research community that uses crowdsourcing and has demonstrated that it can help perform tasks inexpensively, in large volumes and within reasonable time (Howe, 2006; Whitla, 2009). Our system design for evaluation of spontaneous speech is illustrated in Figure 1. We post the task2 of speech transcription to the crowd. We get a final accurate transcription by combining the transcriptions from more than one crowd worker for the same speech sample. Once we have this accurate transcription, we force-align (Erling and Seargeant, 2013; Sj¨olander, 2003) the speech of the candidate on this text to derive various features which go into a machine learning engine. We also collect spoken English grades of the speech from the crowd (Lejk and Wyvill, 2001), which are used as additional features. With these accurately identified features and crowd grades, machine learning is able to grade spontaneous speech with high accuracy. We found that this approach does much better than a pure machine learning approach. Crowdsourcing has been used for almost a decade in various problems in speech analysis, grading and language learning (Kuna</context>
<context position="12474" citStr="Erling and Seargeant, 2013" startWordPosition="1973" endWordPosition="1976"> a spontaneous speech task. 1087 Figure 2: Our intuition of how different features predict the holistic score. In the next section we discuss the features which are used in the prediction algorithm. 3 Features We use three classes of features– Crowd Grades (CG), Force Alignment features (FA) and Natural Language Processing features (NLP). The spoken English samples are posted to the crowd to get the transcription and spoken English grades (Figure 1). Each task was completed by three workers. The crowd grades become one set of features. A second set, i.e., FA features, are derived by aligning (Erling and Seargeant, 2013; Sj¨olander, 2003) the speech sample on the crowdsourced transcriptions. A third set, i.e., NLP features, are also derived from the crowdsourced text. These are explained in the succeeding paragraphs. • Crowd Grades: The crowd transcribes the speech in addition to providing scores on each of the following– pronunciation, fluency, content organization and grammar. These grades are combined to form a composite score per worker per candidate. These are further averaged across workers to give a final score.6 • FA features: The speech sample is forced aligned (Erling and Seargeant, 2013; Sj¨olande</context>
</contexts>
<marker>Erling, Seargeant, 2013</marker>
<rawString>Elizabeth J Erling and Philip Seargeant. 2013. English and development: Policy, pedagogy and globalization, volume 17. Multilingual Matters.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Enrique Estell´es-Arolas</author>
<author>Fernando Gonz´alezLadr´on-de Guevara</author>
</authors>
<title>Towards an integrated crowdsourcing definition.</title>
<date>2012</date>
<journal>Journal of Information science,</journal>
<volume>38</volume>
<issue>2</issue>
<marker>Estell´es-Arolas, Gonz´alezLadr´on-de Guevara, 2012</marker>
<rawString>Enrique Estell´es-Arolas and Fernando Gonz´alezLadr´on-de Guevara. 2012. Towards an integrated crowdsourcing definition. Journal of Information science, 38(2):189–200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cambridge EOCL Examinations</author>
</authors>
<title>Using the CEFR: Principles of good practice. at</title>
<date>2011</date>
<institution>University of Cambridge.</institution>
<contexts>
<context position="10857" citStr="Examinations, 2011" startWordPosition="1711" endWordPosition="1712">k on a given topic. In the spontaneous speech section, the candidates4 are provided with a topic and given 30 seconds5 to think, take notes and then speak on the topic for 45 seconds. The topic is repeated to ensure task clarity. The complete test takes 16-20 minutes to complete, depending on the test version. Currently, SVAR evaluates speech samples from the read and repeat sections with high accuracy (SVAR, 2014). Our goal in this paper is to evaluate the spontaneous speech of the candidate and provide a composite score based on it. A 5 point rubric for the composite score, similar to CEFR (Examinations, 2011), was prepared with the help of experts. This score is a function of the pronunciation, fluency, content organization and grammar quality of the speech sample. Broadly speaking, Pronunciation (Dobson, 1957) refers to the correctness in the utterance of the phonemes of a word by the students as per neutral accent. Fluency (Brumfit and Brumfit, 1984) refers to a desired rate of speech along with the absence of hesitations, false starts and stops etc. Content organization (Stalnaker, 1999) measures the candidate’s ability to structure the information disposition and present it coherently. Grammar</context>
</contexts>
<marker>Examinations, 2011</marker>
<rawString>Cambridge EOCL Examinations. 2011. Using the CEFR: Principles of good practice. at University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan G Fiscus</author>
</authors>
<title>A post-processing system to yield reduced word error rates: Recognizer output voting error reduction (rover). In Automatic Speech Recognition and Understanding,</title>
<date>1997</date>
<booktitle>Proceedings.,</booktitle>
<pages>347--354</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="16279" citStr="Fiscus, 1997" startWordPosition="2587" endWordPosition="2588">hine, running on the intelligence of a hidden human operator. It has more than 500, 000 online workers from 190 countries (Turk, 2014). One can post tasks on the platform online and offer fixed remuneration for their completion. A clean and simple interface was provided to the worker with standard features needed for transcription. Additionally, an advanced audio player was embedded with the ability to play the speech sample in repeat mode, rewind and forward, apart from standard play/pause functionality to help the worker. The different transcriptions were combined using the ROVER algorithm (Fiscus, 1997). ROVER is a sophisticated voting algorithm to combine multiple transcriptions with errors, to obtain the best estimate of the correct transcription. It is reported to lead to an error reduction of 20- 25%. ROVER proceeds in two stages: first the outputs are aligned and a single word transcription network (WTN) is built. The second stage consists of selecting the best scoring word (with the highest number of votes) at each node. Several methods have been used in the past for increasing the reliability of the grades given by the crowd by identifying and correcting any biases and removing non-se</context>
</contexts>
<marker>Fiscus, 1997</marker>
<rawString>Jonathan G Fiscus. 1997. A post-processing system to yield reduced word error rates: Recognizer output voting error reduction (rover). In Automatic Speech Recognition and Understanding, 1997. Proceedings., 1997 IEEE Workshop on, pages 347–354. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Horacio Franco</author>
<author>Victor Abrash</author>
<author>Kristin Precoda</author>
<author>Harry Bratt</author>
<author>Ramana Rao</author>
<author>John Butzberger</author>
<author>Romain Rossier</author>
<author>Federico Cesari</author>
</authors>
<title>The sri eduspeaktm system: Recognition and pronunciation scoring for language learning.</title>
<date>2000</date>
<booktitle>Proceedings of InSTILL</booktitle>
<pages>123--128</pages>
<contexts>
<context position="1793" citStr="Franco et al., 2000" startWordPosition="254" endWordPosition="257">ach we performed experiments on an expert–graded speech sample of 319 adult non–native speakers. Using these features in a regression model, we are able achieve a Pearson correlation of 0.76 with expert grades, an accuracy much higher than any previously reported machine learning approach. Our approach has an accuracy that rivals that of expert agreement. This work is timely given the huge requirement of spoken English training and assessment. 1 Introduction Automatic evaluation of spoken English has been of keen interest for more than two decades (Zechner et al., 2007; Neumeyer et al., 1996; Franco et al., 2000; Cucchiarini et al., 1997). It can help learners get feedback in a scalable manner, help build better English training software and also help companies and institutions filter and select prospective employees more effectively. The problem acquires significance given the evidence that better English leads to better employment outcome, wages and promotions (Guven and Islam, 2013). There has been a considerable success in automatically scoring spoken English, when the spoken text is known a priori (Cucchiarini et al., 2000; Franco et al., 2000). In these cases, the candidate is asked to either r</context>
</contexts>
<marker>Franco, Abrash, Precoda, Bratt, Rao, Butzberger, Rossier, Cesari, 2000</marker>
<rawString>Horacio Franco, Victor Abrash, Kristin Precoda, Harry Bratt, Ramana Rao, John Butzberger, Romain Rossier, and Federico Cesari. 2000. The sri eduspeaktm system: Recognition and pronunciation scoring for language learning. Proceedings of InSTILL 2000, pages 123–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John S Garofolo</author>
<author>Lori F Lamel</author>
<author>William M Fisher</author>
<author>Jonathon G Fiscus</author>
<author>David S Pallett</author>
</authors>
<title>Darpa timit acoustic-phonetic continous speech corpus cd-rom. nist speech disc 1-1.1. NASA STI/Recon</title>
<date>1993</date>
<tech>Technical Report N, 93:27403.</tech>
<contexts>
<context position="13235" citStr="Garofolo et al., 1993" startWordPosition="2096" endWordPosition="2099">ourced text. These are explained in the succeeding paragraphs. • Crowd Grades: The crowd transcribes the speech in addition to providing scores on each of the following– pronunciation, fluency, content organization and grammar. These grades are combined to form a composite score per worker per candidate. These are further averaged across workers to give a final score.6 • FA features: The speech sample is forced aligned (Erling and Seargeant, 2013; Sj¨olander, 2003) on the crowdsourced transcription using the HTK speech recognizer (Young et al., 2006). We used an acoustic model based on TIMIT (Garofolo et al., 1993) for our experiments. TIMIT is a 6Advanced Expectation-Maximization techniques (Hosseini et al., 2012) may also be used for an aggregation strategy, once the number of tasks done by every individual worker increases. In our current experiments, this number wasn’t very high. corpus of phonemically and lexically transcribed speech of American English speakers of different sexes and dialects. A number of speech quality features are derived, which include– rate of speech, position and length of pauses, log likelihood of recognition, posterior probability, hesitations and repetitions etc. These fea</context>
</contexts>
<marker>Garofolo, Lamel, Fisher, Fiscus, Pallett, 1993</marker>
<rawString>John S Garofolo, Lori F Lamel, William M Fisher, Jonathon G Fiscus, and David S Pallett. 1993. Darpa timit acoustic-phonetic continous speech corpus cd-rom. nist speech disc 1-1.1. NASA STI/Recon Technical Report N, 93:27403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Hagley</author>
</authors>
<title>Creation of speaking tests for efl communication classes. ł,</title>
<date>2010</date>
<pages>8--33</pages>
<contexts>
<context position="3554" citStr="Hagley, 2010" startWordPosition="542" endWordPosition="543">, 1996; Franco et al., 2000; Cucchiarini et al., 1997). Some approaches additionally use prosody and energy related features (Dong et al., 2004). More recently, this research has moved towards the assessment of higher granularity metrics like the mispronunciation of particular phonemes (Li et al., 2009; Ito et al., 2006; Koniaris and Engwall, 2011). In spontaneous speech evaluation, the candidate is asked to speak on a topic or answer a question and what he/she speaks isn’t known priori. Evaluation of spontaneous speech is the ultimate test of a candidate’s proficiency in speaking a language (Hagley, 2010; Halleck, 1995). While scores from the evaluation of read/repeat speech do correlate with spontaneous speech evaluation, there remains 1085 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1085–1094, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics an unexplained variance in the spontaneous speech scores (see Section 5). Generally, candidates who score high on spontaneous speech also score high on read speech and not vice versa. Given the pr</context>
</contexts>
<marker>Hagley, 2010</marker>
<rawString>Eric Hagley. 2010. Creation of speaking tests for efl communication classes. ł, (8):33–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gene B Halleck</author>
</authors>
<title>Assessing oral proficiency: A comparison of holistic and objective measures.</title>
<date>1995</date>
<journal>The Modern Language Journal,</journal>
<volume>79</volume>
<issue>2</issue>
<contexts>
<context position="3570" citStr="Halleck, 1995" startWordPosition="544" endWordPosition="545"> et al., 2000; Cucchiarini et al., 1997). Some approaches additionally use prosody and energy related features (Dong et al., 2004). More recently, this research has moved towards the assessment of higher granularity metrics like the mispronunciation of particular phonemes (Li et al., 2009; Ito et al., 2006; Koniaris and Engwall, 2011). In spontaneous speech evaluation, the candidate is asked to speak on a topic or answer a question and what he/she speaks isn’t known priori. Evaluation of spontaneous speech is the ultimate test of a candidate’s proficiency in speaking a language (Hagley, 2010; Halleck, 1995). While scores from the evaluation of read/repeat speech do correlate with spontaneous speech evaluation, there remains 1085 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1085–1094, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics an unexplained variance in the spontaneous speech scores (see Section 5). Generally, candidates who score high on spontaneous speech also score high on read speech and not vice versa. Given the primacy of spontan</context>
</contexts>
<marker>Halleck, 1995</marker>
<rawString>Gene B Halleck. 1995. Assessing oral proficiency: A comparison of holistic and objective measures. The Modern Language Journal, 79(2):223–234.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehdi Hosseini</author>
<author>Ingemar J Cox</author>
<author>Nataˇsa Mili´cFrayling</author>
<author>Gabriella Kazai</author>
<author>Vishwa Vinay</author>
</authors>
<title>On aggregating labels from multiple crowd workers to infer relevance of documents.</title>
<date>2012</date>
<booktitle>In Advances in information retrieval,</booktitle>
<pages>182--194</pages>
<publisher>Springer.</publisher>
<marker>Hosseini, Cox, Mili´cFrayling, Kazai, Vinay, 2012</marker>
<rawString>Mehdi Hosseini, Ingemar J Cox, Nataˇsa Mili´cFrayling, Gabriella Kazai, and Vishwa Vinay. 2012. On aggregating labels from multiple crowd workers to infer relevance of documents. In Advances in information retrieval, pages 182–194. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Howe</author>
</authors>
<title>The rise of crowdsourcing.</title>
<date>2006</date>
<pages>14--6</pages>
<note>Wired magazine,</note>
<contexts>
<context position="6821" citStr="Howe, 2006" startWordPosition="1056" endWordPosition="1057">n Houdnos, 2011; Tetreault et al., 2010; Madnani et al., 2011) approaches. These approaches directly ask the crowd to grade the response. The primary feature of our technique is using the crowd in the feature extraction step of machine learning. telligence tasks are defined as those which most humans find easy, but are hard for machines. For instance, a classic example is the task of finding a particular object in an image. There is a large research community that uses crowdsourcing and has demonstrated that it can help perform tasks inexpensively, in large volumes and within reasonable time (Howe, 2006; Whitla, 2009). Our system design for evaluation of spontaneous speech is illustrated in Figure 1. We post the task2 of speech transcription to the crowd. We get a final accurate transcription by combining the transcriptions from more than one crowd worker for the same speech sample. Once we have this accurate transcription, we force-align (Erling and Seargeant, 2013; Sj¨olander, 2003) the speech of the candidate on this text to derive various features which go into a machine learning engine. We also collect spoken English grades of the speech from the crowd (Lejk and Wyvill, 2001), which are</context>
</contexts>
<marker>Howe, 2006</marker>
<rawString>Jeff Howe. 2006. The rise of crowdsourcing. Wired magazine, 14(6):1–4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akinori Ito</author>
<author>Tadao Nagasawa</author>
<author>Hirokazu Ogasawara</author>
<author>Motoyuki Suzuki</author>
<author>Shozo Makino</author>
</authors>
<title>Automatic detection of english mispronunciation using speaker adaptation and automatic assessment of english intonation and rhythm.</title>
<date>2006</date>
<booktitle>Educational technology research,</booktitle>
<pages>29--1</pages>
<contexts>
<context position="3263" citStr="Ito et al., 2006" startWordPosition="493" endWordPosition="496">ese systems is to force align the speech sample on the known text using an HMM–based acoustic model. Features such as likelihood, posterior probability and fluency related features are derived from the aligned speech and a machine learning model is used to predict expert grades (Neumeyer et al., 1996; Franco et al., 2000; Cucchiarini et al., 1997). Some approaches additionally use prosody and energy related features (Dong et al., 2004). More recently, this research has moved towards the assessment of higher granularity metrics like the mispronunciation of particular phonemes (Li et al., 2009; Ito et al., 2006; Koniaris and Engwall, 2011). In spontaneous speech evaluation, the candidate is asked to speak on a topic or answer a question and what he/she speaks isn’t known priori. Evaluation of spontaneous speech is the ultimate test of a candidate’s proficiency in speaking a language (Hagley, 2010; Halleck, 1995). While scores from the evaluation of read/repeat speech do correlate with spontaneous speech evaluation, there remains 1085 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages</context>
</contexts>
<marker>Ito, Nagasawa, Ogasawara, Suzuki, Makino, 2006</marker>
<rawString>Akinori Ito, Tadao Nagasawa, Hirokazu Ogasawara, Motoyuki Suzuki, and Shozo Makino. 2006. Automatic detection of english mispronunciation using speaker adaptation and automatic assessment of english intonation and rhythm. Educational technology research, 29(1):13–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christos Koniaris</author>
<author>Olov Engwall</author>
</authors>
<title>Perceptual differentiation modeling explains phoneme mispronunciation by non-native speakers.</title>
<date>2011</date>
<booktitle>In Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on,</booktitle>
<pages>5704--5707</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="3292" citStr="Koniaris and Engwall, 2011" startWordPosition="497" endWordPosition="500">force align the speech sample on the known text using an HMM–based acoustic model. Features such as likelihood, posterior probability and fluency related features are derived from the aligned speech and a machine learning model is used to predict expert grades (Neumeyer et al., 1996; Franco et al., 2000; Cucchiarini et al., 1997). Some approaches additionally use prosody and energy related features (Dong et al., 2004). More recently, this research has moved towards the assessment of higher granularity metrics like the mispronunciation of particular phonemes (Li et al., 2009; Ito et al., 2006; Koniaris and Engwall, 2011). In spontaneous speech evaluation, the candidate is asked to speak on a topic or answer a question and what he/she speaks isn’t known priori. Evaluation of spontaneous speech is the ultimate test of a candidate’s proficiency in speaking a language (Hagley, 2010; Halleck, 1995). While scores from the evaluation of read/repeat speech do correlate with spontaneous speech evaluation, there remains 1085 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1085–1094, Beijing, China, J</context>
</contexts>
<marker>Koniaris, Engwall, 2011</marker>
<rawString>Christos Koniaris and Olov Engwall. 2011. Perceptual differentiation modeling explains phoneme mispronunciation by non-native speakers. In Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on, pages 5704– 5707. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen A Kunath</author>
<author>Steven H Weinberger</author>
</authors>
<title>The wisdom of the crowd’s ear: speech accent rating and annotation with amazon mechanical turk.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk,</booktitle>
<pages>168--171</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7814" citStr="Kunath and Weinberger, 2010" startWordPosition="1213" endWordPosition="1217">2013; Sj¨olander, 2003) the speech of the candidate on this text to derive various features which go into a machine learning engine. We also collect spoken English grades of the speech from the crowd (Lejk and Wyvill, 2001), which are used as additional features. With these accurately identified features and crowd grades, machine learning is able to grade spontaneous speech with high accuracy. We found that this approach does much better than a pure machine learning approach. Crowdsourcing has been used for almost a decade in various problems in speech analysis, grading and language learning (Kunath and Weinberger, 2010; Peabody, 2011; Wang et al., 2014). Within assessment of speech, currently all such approaches use the crowd to directly grade certain parts of the speech (Wang and Meng, 2012). Our work is uniquely positioned where we use the crowd to do accurate transcription, a human intelligence task, and use it in a machine learning based algorithm.3 We show that such a system provides an accuracy rivaling that of experts. In this paper, we solve a hitherto unsolved problem of spontaneous speech evaluation (Zechner et al., 2009). The paper makes the following contributions: • We show that spoken English </context>
</contexts>
<marker>Kunath, Weinberger, 2010</marker>
<rawString>Stephen A Kunath and Steven H Weinberger. 2010. The wisdom of the crowd’s ear: speech accent rating and annotation with amazon mechanical turk. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 168–171. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter S Lasecki</author>
<author>Christopher D Miller</author>
<author>Jeffrey P Bigham</author>
</authors>
<title>Warping time for more effective real-time crowdsourcing.</title>
<date>2013</date>
<booktitle>In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,</booktitle>
<pages>2033--2036</pages>
<publisher>ACM.</publisher>
<marker>Lasecki, Miller, Bigham, 2013</marker>
<rawString>Walter S Lasecki, Christopher D Miller, and Jeffrey P Bigham. 2013. Warping time for more effective real-time crowdsourcing. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 2033–2036. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Lejk</author>
<author>Michael Wyvill</author>
</authors>
<title>The effect of the inclusion of selfassessment with peer assessment of contributions to a group project: A quantitative study of secret and agreed assessments.</title>
<date>2001</date>
<booktitle>Assessment &amp; Evaluation in Higher Education,</booktitle>
<volume>26</volume>
<issue>6</issue>
<contexts>
<context position="6190" citStr="Lejk and Wyvill, 2001" startWordPosition="948" endWordPosition="951">, 2006; Little, 2007), to make an accept or reject decision on candidates. Currently, an expert based assessment is used for these purposes. Our method involves combining machine learning with a crowdsourcing layer. Crowdsourcing (Estell´es-Arolas and Gonz´alez-Ladr´on-de Guevara, 2012) is the process of getting human intelligence tasks performed by a large community of online workers (crowd) as opposed to traditional employees.1 The responses from the human intelligence tasks are then used to create relevant features for machine learning. Human in1Our approach is different from peer grading (Lejk and Wyvill, 2001) or crowd grading (Van Houdnos, 2011; Tetreault et al., 2010; Madnani et al., 2011) approaches. These approaches directly ask the crowd to grade the response. The primary feature of our technique is using the crowd in the feature extraction step of machine learning. telligence tasks are defined as those which most humans find easy, but are hard for machines. For instance, a classic example is the task of finding a particular object in an image. There is a large research community that uses crowdsourcing and has demonstrated that it can help perform tasks inexpensively, in large volumes and wit</context>
</contexts>
<marker>Lejk, Wyvill, 2001</marker>
<rawString>Mark Lejk and Michael Wyvill. 2001. The effect of the inclusion of selfassessment with peer assessment of contributions to a group project: A quantitative study of secret and agreed assessments. Assessment &amp; Evaluation in Higher Education, 26(6):551–561.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Li</author>
<author>Shijin Wang</author>
<author>Jiaen Liang</author>
<author>Shen Huang</author>
<author>Bo Xu</author>
</authors>
<title>High performance automatic mispronunciation detection method based on neural network and trap features.</title>
<date>2009</date>
<booktitle>In INTERSPEECH,</booktitle>
<pages>1911--1914</pages>
<contexts>
<context position="3245" citStr="Li et al., 2009" startWordPosition="489" endWordPosition="492"> a majority of these systems is to force align the speech sample on the known text using an HMM–based acoustic model. Features such as likelihood, posterior probability and fluency related features are derived from the aligned speech and a machine learning model is used to predict expert grades (Neumeyer et al., 1996; Franco et al., 2000; Cucchiarini et al., 1997). Some approaches additionally use prosody and energy related features (Dong et al., 2004). More recently, this research has moved towards the assessment of higher granularity metrics like the mispronunciation of particular phonemes (Li et al., 2009; Ito et al., 2006; Koniaris and Engwall, 2011). In spontaneous speech evaluation, the candidate is asked to speak on a topic or answer a question and what he/she speaks isn’t known priori. Evaluation of spontaneous speech is the ultimate test of a candidate’s proficiency in speaking a language (Hagley, 2010; Halleck, 1995). While scores from the evaluation of read/repeat speech do correlate with spontaneous speech evaluation, there remains 1085 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language</context>
</contexts>
<marker>Li, Wang, Liang, Huang, Xu, 2009</marker>
<rawString>Hongyan Li, Shijin Wang, Jiaen Liang, Shen Huang, and Bo Xu. 2009. High performance automatic mispronunciation detection method based on neural network and trap features. In INTERSPEECH, pages 1911–1914.</rawString>
</citation>
<citation valid="true">
<authors>
<author>LightSide</author>
</authors>
<date>2013</date>
<note>http://lightsidelabs.com/.</note>
<contexts>
<context position="14206" citStr="LightSide, 2013" startWordPosition="2250" endWordPosition="2251">ers of different sexes and dialects. A number of speech quality features are derived, which include– rate of speech, position and length of pauses, log likelihood of recognition, posterior probability, hesitations and repetitions etc. These features are well known in literature and may be referred from (Neumeyer et al., 1996; Zechner et al., 2009; Cucchiarini et al., 2000). These features are predictive of the pronunciation and fluency of the candidate. • NLP features: These features predict the content quality and grammar of the spoken content7. They were derived using standard NLP packages (LightSide, 2013; AfterTheDeadline, 2014) on the crowdsourced transcription. The package calculates surface level features such as the number of words, complexity or difficulty of words and the number of common words used. It also calculates semantic features like the coherency in text, context of the words spoken, sentiment of the text and grammar correctness. In the current system, we do not use any prompt specific features such as occurrence of specific words or phrases. These features are predictive of the grammar and content organization of the sample. All the features described above were obtained for t</context>
</contexts>
<marker>LightSide, 2013</marker>
<rawString>LightSide. 2013. http://lightsidelabs.com/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Little</author>
</authors>
<title>The common european framework of reference for languages: Content, purpose, origin, reception and impact.</title>
<date>2006</date>
<journal>Language Teaching,</journal>
<volume>39</volume>
<pages>7</pages>
<contexts>
<context position="5574" citStr="Little, 2006" startWordPosition="854" endWordPosition="856">ich is based on the pronunciation, fluency, content characteristics and grammar of the speech sample, as determined by experts. Multiple previous studies in language acquisition and second language research conclusively show that proficiency in a second language can be characterized by these factors (Bhat et al., 2014). Being able to provide a holistic score is of high interest in both educational testing (Zechner et al., 2009) and job related testing (Streeter et al., 2011). Institutions and firms look for a holistic score, say based on CEFR, a standard to describe spoken English assessment (Little, 2006; Little, 2007), to make an accept or reject decision on candidates. Currently, an expert based assessment is used for these purposes. Our method involves combining machine learning with a crowdsourcing layer. Crowdsourcing (Estell´es-Arolas and Gonz´alez-Ladr´on-de Guevara, 2012) is the process of getting human intelligence tasks performed by a large community of online workers (crowd) as opposed to traditional employees.1 The responses from the human intelligence tasks are then used to create relevant features for machine learning. Human in1Our approach is different from peer grading (Lejk a</context>
</contexts>
<marker>Little, 2006</marker>
<rawString>David Little. 2006. The common european framework of reference for languages: Content, purpose, origin, reception and impact. Language Teaching, 39:167– 190, 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Little</author>
</authors>
<title>The common european framework of reference for languages: Perspectives on the making of supranational language education policy.</title>
<date>2007</date>
<journal>The Modern Language Journal,</journal>
<volume>91</volume>
<issue>4</issue>
<contexts>
<context position="5589" citStr="Little, 2007" startWordPosition="857" endWordPosition="858">n the pronunciation, fluency, content characteristics and grammar of the speech sample, as determined by experts. Multiple previous studies in language acquisition and second language research conclusively show that proficiency in a second language can be characterized by these factors (Bhat et al., 2014). Being able to provide a holistic score is of high interest in both educational testing (Zechner et al., 2009) and job related testing (Streeter et al., 2011). Institutions and firms look for a holistic score, say based on CEFR, a standard to describe spoken English assessment (Little, 2006; Little, 2007), to make an accept or reject decision on candidates. Currently, an expert based assessment is used for these purposes. Our method involves combining machine learning with a crowdsourcing layer. Crowdsourcing (Estell´es-Arolas and Gonz´alez-Ladr´on-de Guevara, 2012) is the process of getting human intelligence tasks performed by a large community of online workers (crowd) as opposed to traditional employees.1 The responses from the human intelligence tasks are then used to create relevant features for machine learning. Human in1Our approach is different from peer grading (Lejk and Wyvill, 2001</context>
</contexts>
<marker>Little, 2007</marker>
<rawString>David Little. 2007. The common european framework of reference for languages: Perspectives on the making of supranational language education policy. The Modern Language Journal, 91(4):645–655.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Joel Tetreault</author>
<author>Martin Chodorow</author>
<author>Alla Rozovskaya</author>
</authors>
<title>They can help: using crowdsourcing to improve the evaluation of grammatical error detection systems.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short</booktitle>
<volume>2</volume>
<pages>508--513</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6273" citStr="Madnani et al., 2011" startWordPosition="962" endWordPosition="965">, an expert based assessment is used for these purposes. Our method involves combining machine learning with a crowdsourcing layer. Crowdsourcing (Estell´es-Arolas and Gonz´alez-Ladr´on-de Guevara, 2012) is the process of getting human intelligence tasks performed by a large community of online workers (crowd) as opposed to traditional employees.1 The responses from the human intelligence tasks are then used to create relevant features for machine learning. Human in1Our approach is different from peer grading (Lejk and Wyvill, 2001) or crowd grading (Van Houdnos, 2011; Tetreault et al., 2010; Madnani et al., 2011) approaches. These approaches directly ask the crowd to grade the response. The primary feature of our technique is using the crowd in the feature extraction step of machine learning. telligence tasks are defined as those which most humans find easy, but are hard for machines. For instance, a classic example is the task of finding a particular object in an image. There is a large research community that uses crowdsourcing and has demonstrated that it can help perform tasks inexpensively, in large volumes and within reasonable time (Howe, 2006; Whitla, 2009). Our system design for evaluation of</context>
</contexts>
<marker>Madnani, Tetreault, Chodorow, Rozovskaya, 2011</marker>
<rawString>Nitin Madnani, Joel Tetreault, Martin Chodorow, and Alla Rozovskaya. 2011. They can help: using crowdsourcing to improve the evaluation of grammatical error detection systems. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2, pages 508–513. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonardo Neumeyer</author>
<author>Horacio Franco</author>
<author>Mitchel Weintraub</author>
<author>Patti Price</author>
</authors>
<title>Automatic textindependent pronunciation scoring of foreign language student speech.</title>
<date>1996</date>
<booktitle>In Spoken Language,</booktitle>
<volume>3</volume>
<pages>1457--1460</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="1772" citStr="Neumeyer et al., 1996" startWordPosition="250" endWordPosition="253">e efficacy of our approach we performed experiments on an expert–graded speech sample of 319 adult non–native speakers. Using these features in a regression model, we are able achieve a Pearson correlation of 0.76 with expert grades, an accuracy much higher than any previously reported machine learning approach. Our approach has an accuracy that rivals that of expert agreement. This work is timely given the huge requirement of spoken English training and assessment. 1 Introduction Automatic evaluation of spoken English has been of keen interest for more than two decades (Zechner et al., 2007; Neumeyer et al., 1996; Franco et al., 2000; Cucchiarini et al., 1997). It can help learners get feedback in a scalable manner, help build better English training software and also help companies and institutions filter and select prospective employees more effectively. The problem acquires significance given the evidence that better English leads to better employment outcome, wages and promotions (Guven and Islam, 2013). There has been a considerable success in automatically scoring spoken English, when the spoken text is known a priori (Cucchiarini et al., 2000; Franco et al., 2000). In these cases, the candidate</context>
<context position="13917" citStr="Neumeyer et al., 1996" startWordPosition="2202" endWordPosition="2205">zation techniques (Hosseini et al., 2012) may also be used for an aggregation strategy, once the number of tasks done by every individual worker increases. In our current experiments, this number wasn’t very high. corpus of phonemically and lexically transcribed speech of American English speakers of different sexes and dialects. A number of speech quality features are derived, which include– rate of speech, position and length of pauses, log likelihood of recognition, posterior probability, hesitations and repetitions etc. These features are well known in literature and may be referred from (Neumeyer et al., 1996; Zechner et al., 2009; Cucchiarini et al., 2000). These features are predictive of the pronunciation and fluency of the candidate. • NLP features: These features predict the content quality and grammar of the spoken content7. They were derived using standard NLP packages (LightSide, 2013; AfterTheDeadline, 2014) on the crowdsourced transcription. The package calculates surface level features such as the number of words, complexity or difficulty of words and the number of common words used. It also calculates semantic features like the coherency in text, context of the words spoken, sentiment </context>
</contexts>
<marker>Neumeyer, Franco, Weintraub, Price, 1996</marker>
<rawString>Leonardo Neumeyer, Horacio Franco, Mitchel Weintraub, and Patti Price. 1996. Automatic textindependent pronunciation scoring of foreign language student speech. In Spoken Language, 1996. ICSLP 96. Proceedings., Fourth International Conference on, volume 3, pages 1457–1460. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quoc Viet Hung Nguyen</author>
<author>Tam Nguyen Thanh</author>
<author>Tran Lam Ngoc</author>
<author>Karl Aberer</author>
</authors>
<title>An evaluation of aggregation techniques in crowdsourcing.</title>
<date>2013</date>
<booktitle>In The 14th International Conference on Web Information System Engineering (WISE),</booktitle>
<pages>187456</pages>
<contexts>
<context position="17083" citStr="Nguyen et al., 2013" startWordPosition="2719" endWordPosition="2722"> reduction of 20- 25%. ROVER proceeds in two stages: first the outputs are aligned and a single word transcription network (WTN) is built. The second stage consists of selecting the best scoring word (with the highest number of votes) at each node. Several methods have been used in the past for increasing the reliability of the grades given by the crowd by identifying and correcting any biases and removing non-serious/low quality workers (Aker et al., 2012). One of the key techniques for this involves inserting gold standard tasks with known answers to get an estimate of the worker’s ability (Nguyen et al., 2013). The gold standard tasks are similar to real tasks and the workers have no way to distinguish between the two. Our tasks took workers a reasonable amount of time (8-10 minutes). It wasn’t hence feasible to insert a gold standard task, as done typically, with every task to be completed. To overcome this problem, we propose an innovative approach where a risk is assigned to a worker based on his/her performance on the gold standard tasks. We conceptualized this system as a state machine that determines the risk level of a worker and proposes actions based on it (Refer to Figure 3). All workers </context>
</contexts>
<marker>Nguyen, Thanh, Ngoc, Aberer, 2013</marker>
<rawString>Quoc Viet Hung Nguyen, Tam Nguyen Thanh, Tran Lam Ngoc, and Karl Aberer. 2013. An evaluation of aggregation techniques in crowdsourcing. In The 14th International Conference on Web Information System Engineering (WISE), 2013, number EPFLCONF-187456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriele Paolacci</author>
<author>Jesse Chandler</author>
<author>Panagiotis G Ipeirotis</author>
</authors>
<title>Running experiments on amazon mechanical turk. Judgment and Decision making,</title>
<date>2010</date>
<pages>5--5</pages>
<marker>Paolacci, Chandler, Ipeirotis, 2010</marker>
<rawString>Gabriele Paolacci, Jesse Chandler, and Panagiotis G Ipeirotis. 2010. Running experiments on amazon mechanical turk. Judgment and Decision making, 5(5):411–419.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Aaron Peabody</author>
</authors>
<title>Methods for pronunciation assessment in computer aided language learning.</title>
<date>2011</date>
<tech>Ph.D. thesis,</tech>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="7829" citStr="Peabody, 2011" startWordPosition="1218" endWordPosition="1219">peech of the candidate on this text to derive various features which go into a machine learning engine. We also collect spoken English grades of the speech from the crowd (Lejk and Wyvill, 2001), which are used as additional features. With these accurately identified features and crowd grades, machine learning is able to grade spontaneous speech with high accuracy. We found that this approach does much better than a pure machine learning approach. Crowdsourcing has been used for almost a decade in various problems in speech analysis, grading and language learning (Kunath and Weinberger, 2010; Peabody, 2011; Wang et al., 2014). Within assessment of speech, currently all such approaches use the crowd to directly grade certain parts of the speech (Wang and Meng, 2012). Our work is uniquely positioned where we use the crowd to do accurate transcription, a human intelligence task, and use it in a machine learning based algorithm.3 We show that such a system provides an accuracy rivaling that of experts. In this paper, we solve a hitherto unsolved problem of spontaneous speech evaluation (Zechner et al., 2009). The paper makes the following contributions: • We show that spoken English can be graded w</context>
</contexts>
<marker>Peabody, 2011</marker>
<rawString>Mitchell Aaron Peabody. 2011. Methods for pronunciation assessment in computer aided language learning. Ph.D. thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald E Powers</author>
<author>Jill C Burstein</author>
<author>Martin Chodorow</author>
<author>Mary E Fowles</author>
<author>Karen Kukich</author>
</authors>
<title>Stumping e-rater: challenging the validity of automated essay scoring.</title>
<date>2002</date>
<journal>Computers in Human Behavior,</journal>
<volume>18</volume>
<issue>2</issue>
<pages>134</pages>
<contexts>
<context position="4415" citStr="Powers et al., 2002" startWordPosition="667" endWordPosition="670">nternational Joint Conference on Natural Language Processing, pages 1085–1094, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics an unexplained variance in the spontaneous speech scores (see Section 5). Generally, candidates who score high on spontaneous speech also score high on read speech and not vice versa. Given the primacy of spontaneous speech evaluation in judging a person’s language capability, there is considerable interest in doing it automatically (Cucchiarini et al., 1997; Dong et al., 2004). Automated approaches for the same have not worked well (Powers et al., 2002; Cucchiarini et al., 2000) primarily because speakerindependent speech recognition is a tough computer science problem. This is exacerbated when the speakers are not proficient in the language or are non-natives (Powers et al., 2002). Given that speech to text conversion for such candidates has a low accuracy, force alignment of the speech on this inaccurate text makes the features and the model inaccurate. We present a semi-automated approach to grade short duration (45 seconds) spontaneous speech. We accurately predict a holistic score which is based on the pronunciation, fluency, content c</context>
</contexts>
<marker>Powers, Burstein, Chodorow, Fowles, Kukich, 2002</marker>
<rawString>Donald E Powers, Jill C Burstein, Martin Chodorow, Mary E Fowles, and Karen Kukich. 2002. Stumping e-rater: challenging the validity of automated essay scoring. Computers in Human Behavior, 18(2):103– 134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K˚are Sj¨olander</author>
</authors>
<title>An hmm-based system for automatic segmentation and alignment of speech.</title>
<date>2003</date>
<booktitle>In Proceedings of Fonetik,</booktitle>
<volume>volume</volume>
<pages>93--96</pages>
<publisher>Citeseer.</publisher>
<marker>Sj¨olander, 2003</marker>
<rawString>K˚are Sj¨olander. 2003. An hmm-based system for automatic segmentation and alignment of speech. In Proceedings of Fonetik, volume 2003, pages 93–96. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Stalnaker</author>
</authors>
<title>The problem of logical omniscience, ii. context and content: Essays on intentionality in speech and thought</title>
<date>1999</date>
<pages>255--273</pages>
<contexts>
<context position="11348" citStr="Stalnaker, 1999" startWordPosition="1789" endWordPosition="1790">ate and provide a composite score based on it. A 5 point rubric for the composite score, similar to CEFR (Examinations, 2011), was prepared with the help of experts. This score is a function of the pronunciation, fluency, content organization and grammar quality of the speech sample. Broadly speaking, Pronunciation (Dobson, 1957) refers to the correctness in the utterance of the phonemes of a word by the students as per neutral accent. Fluency (Brumfit and Brumfit, 1984) refers to a desired rate of speech along with the absence of hesitations, false starts and stops etc. Content organization (Stalnaker, 1999) measures the candidate’s ability to structure the information disposition and present it coherently. Grammar (Brazil, 1995) measures how well the syntax of the language was followed by the candidate. 4The subjects of our study use English as their second language and hail from various backgrounds, dialects and educational qualifications. 5This is as per global standards of spoken English assessment. High stake tests such as TOEFL provide the candidate 15-30 seconds to think before responding to a spontaneous speech task. 1087 Figure 2: Our intuition of how different features predict the holis</context>
</contexts>
<marker>Stalnaker, 1999</marker>
<rawString>Robert Stalnaker. 1999. The problem of logical omniscience, ii. context and content: Essays on intentionality in speech and thought (pp. 255–273).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynn Streeter</author>
<author>Jared Bernstein</author>
<author>Peter Foltz</author>
<author>Donald DeLand</author>
</authors>
<title>Pearsons automated scoring of writing, speaking, and mathematics.</title>
<date>2011</date>
<contexts>
<context position="5441" citStr="Streeter et al., 2011" startWordPosition="830" endWordPosition="833">curate. We present a semi-automated approach to grade short duration (45 seconds) spontaneous speech. We accurately predict a holistic score which is based on the pronunciation, fluency, content characteristics and grammar of the speech sample, as determined by experts. Multiple previous studies in language acquisition and second language research conclusively show that proficiency in a second language can be characterized by these factors (Bhat et al., 2014). Being able to provide a holistic score is of high interest in both educational testing (Zechner et al., 2009) and job related testing (Streeter et al., 2011). Institutions and firms look for a holistic score, say based on CEFR, a standard to describe spoken English assessment (Little, 2006; Little, 2007), to make an accept or reject decision on candidates. Currently, an expert based assessment is used for these purposes. Our method involves combining machine learning with a crowdsourcing layer. Crowdsourcing (Estell´es-Arolas and Gonz´alez-Ladr´on-de Guevara, 2012) is the process of getting human intelligence tasks performed by a large community of online workers (crowd) as opposed to traditional employees.1 The responses from the human intelligen</context>
</contexts>
<marker>Streeter, Bernstein, Foltz, DeLand, 2011</marker>
<rawString>Lynn Streeter, Jared Bernstein, Peter Foltz, and Donald DeLand. 2011. Pearsons automated scoring of writing, speaking, and mathematics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>SVAR</author>
</authors>
<date>2014</date>
<note>http://www.aspiringminds.in/talentevaluation/spoken-english-SVAR.html.</note>
<contexts>
<context position="9960" citStr="SVAR, 2014" startWordPosition="1561" endWordPosition="1562">Section 3 describes the feature classes used in the prediction algorithm; Section 4 describes the crowdsourcing framework which is used as an input to machine learning methods; Section 5 demonstrates how this framework is used with machine learning techniques to predict a composite spoken English score; Section 6 discusses the future work and concludes the paper. 2 Grading Task We want to assess the quality of spoken English of candidates based on their spontaneous speech samples. The speech samples of the candidates were collected using Aspiring Minds’ automated speech assessment tool– SVAR (SVAR, 2014). SVAR is conducted over phone as well as on a computer. The test has multiple sections where the candidate is required to: read sentences aloud, listen and repeat sentences, listen to a passage or conversation and answer multiple choice questions and finally spontaneously speak on a given topic. In the spontaneous speech section, the candidates4 are provided with a topic and given 30 seconds5 to think, take notes and then speak on the topic for 45 seconds. The topic is repeated to ensure task clarity. The complete test takes 16-20 minutes to complete, depending on the test version. Currently,</context>
<context position="20772" citStr="SVAR, 2014" startWordPosition="3338" endWordPosition="3339">rithms. The data set used in the experiments is discussed in the next section. 5.1 Data Set Our data set contains 319 spontaneous speech responses. The speech samples were from seniors (non–native English speakers in final year of undergraduate education) pursuing bachelor’s degree in India. The candidates were asked to describe one of the following scenes: a hospital, flood, a crowded market and a school playground. The candidates were given 30 seconds to think and take notes and were then asked to speak for the next 45 seconds. The responses were collected on the phone during the SVAR test (SVAR, 2014). Apart from the spontaneous speech response, each candidate was asked to read 12 given sentences and repeat 9 given sentences immediately after listening to each of them. Empty or very noisy responses (not humanly discernible) were not included in the final 319 sample set. These responses were graded by two experts who had more than fifteen years of experience in grading spoken English responses. There were two set of scores. The first was a holistic score on the spontaneous speech samples based on its pronunciation, fluency, content characteristics and grammar. The second was a score on the </context>
</contexts>
<marker>SVAR, 2014</marker>
<rawString>SVAR. 2014. http://www.aspiringminds.in/talentevaluation/spoken-english-SVAR.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel R Tetreault</author>
<author>Elena Filatova</author>
<author>Martin Chodorow</author>
</authors>
<title>Rethinking grammatical error annotation and evaluation with the amazon mechanical turk.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>45--48</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6250" citStr="Tetreault et al., 2010" startWordPosition="958" endWordPosition="961">on candidates. Currently, an expert based assessment is used for these purposes. Our method involves combining machine learning with a crowdsourcing layer. Crowdsourcing (Estell´es-Arolas and Gonz´alez-Ladr´on-de Guevara, 2012) is the process of getting human intelligence tasks performed by a large community of online workers (crowd) as opposed to traditional employees.1 The responses from the human intelligence tasks are then used to create relevant features for machine learning. Human in1Our approach is different from peer grading (Lejk and Wyvill, 2001) or crowd grading (Van Houdnos, 2011; Tetreault et al., 2010; Madnani et al., 2011) approaches. These approaches directly ask the crowd to grade the response. The primary feature of our technique is using the crowd in the feature extraction step of machine learning. telligence tasks are defined as those which most humans find easy, but are hard for machines. For instance, a classic example is the task of finding a particular object in an image. There is a large research community that uses crowdsourcing and has demonstrated that it can help perform tasks inexpensively, in large volumes and within reasonable time (Howe, 2006; Whitla, 2009). Our system d</context>
</contexts>
<marker>Tetreault, Filatova, Chodorow, 2010</marker>
<rawString>Joel R Tetreault, Elena Filatova, and Martin Chodorow. 2010. Rethinking grammatical error annotation and evaluation with the amazon mechanical turk. In Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 45–48. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amazon Mechanical Turk</author>
</authors>
<date>2014</date>
<note>https://requester.mturk.com/tour.</note>
<contexts>
<context position="15800" citStr="Turk, 2014" startWordPosition="2512" endWordPosition="2513">s do not have any bearing on our final model for spontaneous speech evaluation. 4 Crowdsourcing The spoken English sample was given to the crowd to transcribe and provide grades. The task was posted on a popular crowdsourcing platform– Amazon Mechanical Turk (AMT) (Paolacci et al., 7We were looking at prompt independent features only, at this point. 1088 2010). AMT is a popular crowdsourcing marketplace. It is inspired by the famous 18th century automated chess playing machine, running on the intelligence of a hidden human operator. It has more than 500, 000 online workers from 190 countries (Turk, 2014). One can post tasks on the platform online and offer fixed remuneration for their completion. A clean and simple interface was provided to the worker with standard features needed for transcription. Additionally, an advanced audio player was embedded with the ability to play the speech sample in repeat mode, rewind and forward, apart from standard play/pause functionality to help the worker. The different transcriptions were combined using the ROVER algorithm (Fiscus, 1997). ROVER is a sophisticated voting algorithm to combine multiple transcriptions with errors, to obtain the best estimate o</context>
</contexts>
<marker>Turk, 2014</marker>
<rawString>Amazon Mechanical Turk. 2014. https://requester.mturk.com/tour.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathan Van Houdnos</author>
</authors>
<title>Can the internet grade math? crowdsourcing a complex scoring task and picking the optimal crowd size.</title>
<date>2011</date>
<booktitle>Dietrich College of Humanities and Social Sciences at Research Showcase @ CMU.</booktitle>
<marker>Van Houdnos, 2011</marker>
<rawString>Nathan Van Houdnos. 2011. Can the internet grade math? crowdsourcing a complex scoring task and picking the optimal crowd size. Dietrich College of Humanities and Social Sciences at Research Showcase @ CMU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Wang</author>
<author>Helen Meng</author>
</authors>
<title>Deriving perceptual gradation of l2 english mispronunciations using crowdsourcing and the workerrank algorithm.</title>
<date>2012</date>
<booktitle>Proc. of the 15th Oriental COCOSDA,</booktitle>
<pages>9--12</pages>
<location>Macau, China,</location>
<contexts>
<context position="7991" citStr="Wang and Meng, 2012" startWordPosition="1244" endWordPosition="1247"> from the crowd (Lejk and Wyvill, 2001), which are used as additional features. With these accurately identified features and crowd grades, machine learning is able to grade spontaneous speech with high accuracy. We found that this approach does much better than a pure machine learning approach. Crowdsourcing has been used for almost a decade in various problems in speech analysis, grading and language learning (Kunath and Weinberger, 2010; Peabody, 2011; Wang et al., 2014). Within assessment of speech, currently all such approaches use the crowd to directly grade certain parts of the speech (Wang and Meng, 2012). Our work is uniquely positioned where we use the crowd to do accurate transcription, a human intelligence task, and use it in a machine learning based algorithm.3 We show that such a system provides an accuracy rivaling that of experts. In this paper, we solve a hitherto unsolved problem of spontaneous speech evaluation (Zechner et al., 2009). The paper makes the following contributions: • We show that spoken English can be graded with accuracy by combining machine learning and crowdsourcing higher than a pure machine learning approach. 2Even though speaker-independent speech recognition is </context>
</contexts>
<marker>Wang, Meng, 2012</marker>
<rawString>Hao Wang and Helen Meng. 2012. Deriving perceptual gradation of l2 english mispronunciations using crowdsourcing and the workerrank algorithm. Proc. of the 15th Oriental COCOSDA, Macau, China, pages 9–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Wang</author>
<author>Xiaojun Qian</author>
<author>Helen Meng</author>
</authors>
<title>Phonological modeling of mispronunciation gradations in l2 english speech of l1 chinese learners.</title>
<date>2014</date>
<booktitle>In Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on,</booktitle>
<pages>7714--7718</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="7849" citStr="Wang et al., 2014" startWordPosition="1220" endWordPosition="1223">ndidate on this text to derive various features which go into a machine learning engine. We also collect spoken English grades of the speech from the crowd (Lejk and Wyvill, 2001), which are used as additional features. With these accurately identified features and crowd grades, machine learning is able to grade spontaneous speech with high accuracy. We found that this approach does much better than a pure machine learning approach. Crowdsourcing has been used for almost a decade in various problems in speech analysis, grading and language learning (Kunath and Weinberger, 2010; Peabody, 2011; Wang et al., 2014). Within assessment of speech, currently all such approaches use the crowd to directly grade certain parts of the speech (Wang and Meng, 2012). Our work is uniquely positioned where we use the crowd to do accurate transcription, a human intelligence task, and use it in a machine learning based algorithm.3 We show that such a system provides an accuracy rivaling that of experts. In this paper, we solve a hitherto unsolved problem of spontaneous speech evaluation (Zechner et al., 2009). The paper makes the following contributions: • We show that spoken English can be graded with accuracy by comb</context>
</contexts>
<marker>Wang, Qian, Meng, 2014</marker>
<rawString>Hao Wang, Xiaojun Qian, and Helen Meng. 2014. Phonological modeling of mispronunciation gradations in l2 english speech of l1 chinese learners. In Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on, pages 7714–7718. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Whitla</author>
</authors>
<title>Crowdsourcing and its application in marketing activities.</title>
<date>2009</date>
<journal>Contemporary Management Research,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="6836" citStr="Whitla, 2009" startWordPosition="1058" endWordPosition="1059">011; Tetreault et al., 2010; Madnani et al., 2011) approaches. These approaches directly ask the crowd to grade the response. The primary feature of our technique is using the crowd in the feature extraction step of machine learning. telligence tasks are defined as those which most humans find easy, but are hard for machines. For instance, a classic example is the task of finding a particular object in an image. There is a large research community that uses crowdsourcing and has demonstrated that it can help perform tasks inexpensively, in large volumes and within reasonable time (Howe, 2006; Whitla, 2009). Our system design for evaluation of spontaneous speech is illustrated in Figure 1. We post the task2 of speech transcription to the crowd. We get a final accurate transcription by combining the transcriptions from more than one crowd worker for the same speech sample. Once we have this accurate transcription, we force-align (Erling and Seargeant, 2013; Sj¨olander, 2003) the speech of the candidate on this text to derive various features which go into a machine learning engine. We also collect spoken English grades of the speech from the crowd (Lejk and Wyvill, 2001), which are used as additi</context>
</contexts>
<marker>Whitla, 2009</marker>
<rawString>Paul Whitla. 2009. Crowdsourcing and its application in marketing activities. Contemporary Management Research, 5(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Young</author>
<author>Gunnar Evermann</author>
<author>Mark Gales</author>
<author>Thomas Hain</author>
<author>Dan Kershaw</author>
<author>Xunying Liu</author>
<author>Gareth Moore</author>
<author>Julian Odell</author>
<author>Dave Ollason</author>
<author>Dan Povey</author>
</authors>
<title>The htk book (for htk version 3.4). Cambridge university engineering department,</title>
<date>2006</date>
<pages>2--2</pages>
<contexts>
<context position="13169" citStr="Young et al., 2006" startWordPosition="2084" endWordPosition="2087">third set, i.e., NLP features, are also derived from the crowdsourced text. These are explained in the succeeding paragraphs. • Crowd Grades: The crowd transcribes the speech in addition to providing scores on each of the following– pronunciation, fluency, content organization and grammar. These grades are combined to form a composite score per worker per candidate. These are further averaged across workers to give a final score.6 • FA features: The speech sample is forced aligned (Erling and Seargeant, 2013; Sj¨olander, 2003) on the crowdsourced transcription using the HTK speech recognizer (Young et al., 2006). We used an acoustic model based on TIMIT (Garofolo et al., 1993) for our experiments. TIMIT is a 6Advanced Expectation-Maximization techniques (Hosseini et al., 2012) may also be used for an aggregation strategy, once the number of tasks done by every individual worker increases. In our current experiments, this number wasn’t very high. corpus of phonemically and lexically transcribed speech of American English speakers of different sexes and dialects. A number of speech quality features are derived, which include– rate of speech, position and length of pauses, log likelihood of recognition,</context>
</contexts>
<marker>Young, Evermann, Gales, Hain, Kershaw, Liu, Moore, Odell, Ollason, Povey, 2006</marker>
<rawString>Steve Young, Gunnar Evermann, Mark Gales, Thomas Hain, Dan Kershaw, Xunying Liu, Gareth Moore, Julian Odell, Dave Ollason, Dan Povey, et al. 2006. The htk book (for htk version 3.4). Cambridge university engineering department, 2(2):2–3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar F Zaidan</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Crowdsourcing translation: Professional quality from non-professionals.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language TechnologiesVolume 1,</booktitle>
<pages>1220--1229</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8819" citStr="Zaidan and Callison-Burch, 2011" startWordPosition="1377" endWordPosition="1380">ides an accuracy rivaling that of experts. In this paper, we solve a hitherto unsolved problem of spontaneous speech evaluation (Zechner et al., 2009). The paper makes the following contributions: • We show that spoken English can be graded with accuracy by combining machine learning and crowdsourcing higher than a pure machine learning approach. 2Even though speaker-independent speech recognition is a hard problem for machines, it is fairly easy for a native speaker or anyone with reasonable command over the language. 3Again, speech transcription has been done previously using crowdsourcing (Zaidan and Callison-Burch, 2011), but not used for a grading purpose or combined with machine learning. 1086 Figure 1: System Design • We show that the features derived from crowdsourced transcriptions perform as well as crowd grades in predicting expert grades. However, crowd grades add additional predictive value. • We propose a scalable and accurate way to perform evaluation of spontaneous speech, a huge requirement in the industry and elsewhere. The paper is organized as follows– Section 2 describes the procedure and aim of the speech assessment task; Section 3 describes the feature classes used in the prediction algorit</context>
</contexts>
<marker>Zaidan, Callison-Burch, 2011</marker>
<rawString>Omar F Zaidan and Chris Callison-Burch. 2011. Crowdsourcing translation: Professional quality from non-professionals. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language TechnologiesVolume 1, pages 1220–1229. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Zechner</author>
<author>Derrick Higgins</author>
<author>Xiaoming Xi</author>
</authors>
<title>Speechrater: A construct-driven approach to scoring spontaneous non-native speech.</title>
<date>2007</date>
<booktitle>Proc. SLaTE.</booktitle>
<contexts>
<context position="1749" citStr="Zechner et al., 2007" startWordPosition="245" endWordPosition="249">ion. To demonstrate the efficacy of our approach we performed experiments on an expert–graded speech sample of 319 adult non–native speakers. Using these features in a regression model, we are able achieve a Pearson correlation of 0.76 with expert grades, an accuracy much higher than any previously reported machine learning approach. Our approach has an accuracy that rivals that of expert agreement. This work is timely given the huge requirement of spoken English training and assessment. 1 Introduction Automatic evaluation of spoken English has been of keen interest for more than two decades (Zechner et al., 2007; Neumeyer et al., 1996; Franco et al., 2000; Cucchiarini et al., 1997). It can help learners get feedback in a scalable manner, help build better English training software and also help companies and institutions filter and select prospective employees more effectively. The problem acquires significance given the evidence that better English leads to better employment outcome, wages and promotions (Guven and Islam, 2013). There has been a considerable success in automatically scoring spoken English, when the spoken text is known a priori (Cucchiarini et al., 2000; Franco et al., 2000). In the</context>
</contexts>
<marker>Zechner, Higgins, Xi, 2007</marker>
<rawString>Klaus Zechner, Derrick Higgins, and Xiaoming Xi. 2007. Speechrater: A construct-driven approach to scoring spontaneous non-native speech. Proc. SLaTE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Zechner</author>
<author>Derrick Higgins</author>
<author>Xiaoming Xi</author>
<author>David M Williamson</author>
</authors>
<title>Automatic scoring of non-native spontaneous speech in tests of spoken english.</title>
<date>2009</date>
<journal>Speech Communication,</journal>
<volume>51</volume>
<issue>10</issue>
<contexts>
<context position="5393" citStr="Zechner et al., 2009" startWordPosition="822" endWordPosition="825">rate text makes the features and the model inaccurate. We present a semi-automated approach to grade short duration (45 seconds) spontaneous speech. We accurately predict a holistic score which is based on the pronunciation, fluency, content characteristics and grammar of the speech sample, as determined by experts. Multiple previous studies in language acquisition and second language research conclusively show that proficiency in a second language can be characterized by these factors (Bhat et al., 2014). Being able to provide a holistic score is of high interest in both educational testing (Zechner et al., 2009) and job related testing (Streeter et al., 2011). Institutions and firms look for a holistic score, say based on CEFR, a standard to describe spoken English assessment (Little, 2006; Little, 2007), to make an accept or reject decision on candidates. Currently, an expert based assessment is used for these purposes. Our method involves combining machine learning with a crowdsourcing layer. Crowdsourcing (Estell´es-Arolas and Gonz´alez-Ladr´on-de Guevara, 2012) is the process of getting human intelligence tasks performed by a large community of online workers (crowd) as opposed to traditional emp</context>
<context position="8337" citStr="Zechner et al., 2009" startWordPosition="1303" endWordPosition="1306">in various problems in speech analysis, grading and language learning (Kunath and Weinberger, 2010; Peabody, 2011; Wang et al., 2014). Within assessment of speech, currently all such approaches use the crowd to directly grade certain parts of the speech (Wang and Meng, 2012). Our work is uniquely positioned where we use the crowd to do accurate transcription, a human intelligence task, and use it in a machine learning based algorithm.3 We show that such a system provides an accuracy rivaling that of experts. In this paper, we solve a hitherto unsolved problem of spontaneous speech evaluation (Zechner et al., 2009). The paper makes the following contributions: • We show that spoken English can be graded with accuracy by combining machine learning and crowdsourcing higher than a pure machine learning approach. 2Even though speaker-independent speech recognition is a hard problem for machines, it is fairly easy for a native speaker or anyone with reasonable command over the language. 3Again, speech transcription has been done previously using crowdsourcing (Zaidan and Callison-Burch, 2011), but not used for a grading purpose or combined with machine learning. 1086 Figure 1: System Design • We show that th</context>
<context position="13939" citStr="Zechner et al., 2009" startWordPosition="2206" endWordPosition="2209">eini et al., 2012) may also be used for an aggregation strategy, once the number of tasks done by every individual worker increases. In our current experiments, this number wasn’t very high. corpus of phonemically and lexically transcribed speech of American English speakers of different sexes and dialects. A number of speech quality features are derived, which include– rate of speech, position and length of pauses, log likelihood of recognition, posterior probability, hesitations and repetitions etc. These features are well known in literature and may be referred from (Neumeyer et al., 1996; Zechner et al., 2009; Cucchiarini et al., 2000). These features are predictive of the pronunciation and fluency of the candidate. • NLP features: These features predict the content quality and grammar of the spoken content7. They were derived using standard NLP packages (LightSide, 2013; AfterTheDeadline, 2014) on the crowdsourced transcription. The package calculates surface level features such as the number of words, complexity or difficulty of words and the number of common words used. It also calculates semantic features like the coherency in text, context of the words spoken, sentiment of the text and gramma</context>
</contexts>
<marker>Zechner, Higgins, Xi, Williamson, 2009</marker>
<rawString>Klaus Zechner, Derrick Higgins, Xiaoming Xi, and David M Williamson. 2009. Automatic scoring of non-native spontaneous speech in tests of spoken english. Speech Communication, 51(10):883–895.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>