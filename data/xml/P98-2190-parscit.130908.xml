<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.996691">
Conditions on Consistency of
Probabilistic Tree Adjoining Grammars*
</title>
<author confidence="0.992247">
Anoop Sarkar
</author>
<affiliation confidence="0.998105">
Dept. of Computer and Information Science
University of Pennsylvania
</affiliation>
<address confidence="0.96898">
200 South 33rd Street,
Philadelphia, PA 19104-6389 USA
</address>
<email confidence="0.978121">
anoopOlinc.cis.upenn.edu
</email>
<sectionHeader confidence="0.994122" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999969863636364">
Much of the power of probabilistic methods in
modelling language comes from their ability to
compare several derivations for the same string
in the language. An important starting point
for the study of such cross-derivational proper-
ties is the notion of consistency. The probabil-
ity model defined by a probabilistic grammar is
said to be consistent if the probabilities assigned
to all the strings in the language sum to one.
From the literature on probabilistic context-free
grammars (CFGs), we know precisely the con-
ditions which ensure that consistency is true for
a given CFG. This paper derives the conditions
under which a given probabilistic Tree Adjoin-
ing Grammar (TAG) can be shown to be con-
sistent. It gives a simple algorithm for checking
consistency and gives the formal justification
for its correctness. The conditions derived here
can be used to ensure that probability models
that use TAGs can be checked for deficiency
(i.e. whether any probability mass is assigned
to strings that cannot be generated).
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999971">
Much of the power of probabilistic methods
in modelling language comes from their abil-
ity to compare several derivations for the same
string in the language. This cross-derivational
power arises naturally from comparison of vari-
ous derivational paths, each of which is a prod-
uct of the probabilities associated with each step
in each derivation. A common approach used
to assign structure to language is to use a prob-
abilistic grammar where each elementary rule
</bodyText>
<listItem confidence="0.6833585">
• This research was partially supported by NSF grant
SBR8920230 and ARO grant DAAH0404-94-G-0426.
</listItem>
<bodyText confidence="0.97892228">
The author would like to thank Aravind Joshi, Jeff Rey-
nal., Giorgio Satta, B. Srinivas, Fei Xia and the two
anonymous reviewers for their valuable comments.
or production is associated with a probability.
Using such a grammar, a probability for each
string in the language is computed. Assum-
ing that the probability of each derivation of a
sentence is well-defined, the probability of each
string in the language is simply the sum of the
probabilities of all derivations of the string. In
general, for a probabilistic grammar G the lan-
guage of G is denoted by L(G). Then if a string
v is in the language L(G) the probabilistic gram-
mar assigns v some non-zero probability.
There are several cross-derivational proper-
ties that can be studied for a given probabilis-
tic grammar formalism. An important starting
point for such studies is the notion of consis-
tency. The probability model defined by a prob-
abilistic grammar is said to be consistent if the
probabilities assigned to all the strings in the
language sum to 1. That is, if Pr defined by a
probabilistic grammar, assigns a probability to
each string v E E*, where Pr(v) = 0 if v
then
</bodyText>
<equation confidence="0.9914605">
E Pr(v) = 1 (1)
vEL(G)
</equation>
<bodyText confidence="0.999897733333334">
From the literature on probabilistic context-
free grammars (CFGs) we know precisely the
conditions which ensure that (1) is true for a
given CFG. This paper derives the conditions
under which a given probabilistic TAG can be
shown to be consistent.
TAGs are important in the modelling of nat-
ural language since they can be easily lexical-
ized; moreover the trees associated with words
can be used to encode argument and adjunct re-
lations in various syntactic environments. This
paper assumes some familiarity with the TAG
formalism. (Joshi, 1988) and (Joshi and Sch-
abes, 1992) are good introductions to the for-
malism and its linguistic relevance. TAGs have
</bodyText>
<page confidence="0.99243">
1164
</page>
<bodyText confidence="0.956279333333333">
been shown to have relations with both phrase-
structure grammars and dependency grammars
(Rambow and Joshi, 1995) and can handle
(non-projective) long distance dependencies.
Consistency of probabilistic TAGs has prac-
tical significance for the following reasons:
</bodyText>
<listItem confidence="0.9952471875">
• The conditions derived here can be used
to ensure that probability models that use
TAGs can be checked for deficiency.
• Existing EM based estimation algorithms
for probabilistic TAGs assume that the
property of consistency holds (Schabes,
1992). EM based algorithms begin with an
initial (usually random) value for each pa-
rameter. If the initial assignment causes
the grammar to be inconsistent, then it-
erative re-estimation might converge to an
inconsistent grammar&apos;.
• Techniques used in this paper can be used
to determine consistency for other proba-
bility models based on TAGs (Carroll and
Weir, 1997).
</listItem>
<sectionHeader confidence="0.938166" genericHeader="introduction">
2 Notation
</sectionHeader>
<bodyText confidence="0.887658775">
In this section we establish some notational con-
ventions and definitions that we use in this pa-
per. Those familiar with the TAG formalism
only need to give a cursory glance through this
section.
A probabilistic TAG is represented by
(N, E, I, A, S,0) where N, E are, respectively,
non-terminal and terminal symbols. I U A is a
set of trees termed as elementary trees. We take
V to be the set of all nodes in all the elementary
trees. For each leaf A E V, label(A) is an ele-
ment from E U {€}, and for each other node A,
label(A) is an element from N. S is an element
from N which is a distinguished start symbol.
The root node A of every initial tree which can
start a derivation must have label(A) = S.
I are termed initial trees and A are auxil-
iary trees which can rewrite a tree node A E V.
This rewrite step is called adjunction. (/) is a
function which assigns each adjunction with a
probability and denotes the set of parameters
&apos;Note that for CFGs it has been shown in (Chaud-
hari et al., 1983; Sanchez and Benedc, 1997) that inside-
outside reestimation can be used to avoid inconsistency.
We will show later in the paper that the method used to
show consistency in this paper precludes a straightfor-
ward extension of that result for TAGs.
in the model. In practice, TAGs also allow a
leaf nodes A such that label(A) is an element
from N. Such nodes A are rewritten with ini-
tial trees from I using the rewrite step called
substitution. Except in one special case, we
will not need to treat substitution as being dis-
tinct from adjunction.
For t e I U A, A(t) are the nodes in tree
t that can be modified by adjunction. For
label(A) E N we denote Adj(label(A)) as the
set of trees that can adjoin at node A E V.
The adjunction of t into N E V is denoted by
N t. No adjunction at N E V is denoted
</bodyText>
<listItem confidence="0.949720157894737">
by N nil. We assume the following proper-
ties hold for every probabilistic TAG G that we
consider:
1. G is lexicalized. There is at least one
leaf node a that lexicalizes each elementary
tree, i.e. a E E.
2. G is proper. For each N E V,
cb(N nil) + E 1
3. Adjunction is prohibited on the foot node
of every auxiliary tree. This condition is
imposed to avoid unnecessary ambiguity
and can be easily relaxed.
4. There is a distinguished non-lexicalized ini-
tial tree T such that each initial tree rooted
by a node A with label(A) = S substitutes
into 7- to complete the derivation. This en-
sures that probabilities assigned to the in-
put string at the start of the derivation are
well-formed.
</listItem>
<bodyText confidence="0.9868568">
We use symbols S, A, B,... to range over V,
symbols a, b, c, . . . to range over E. We use
t1, t2,... to range over I U A and c to denote
the empty string. We use Xi to range over all i
nodes in the grammar.
</bodyText>
<sectionHeader confidence="0.712041" genericHeader="method">
3 Applying probability measures to
</sectionHeader>
<subsectionHeader confidence="0.719274">
Tree Adjoining Languages
</subsectionHeader>
<bodyText confidence="0.998134">
To gain some intuition about probability assign-
ments to languages, let us take for example, a
language well known to be a tree adjoining lan-
guage:
</bodyText>
<equation confidence="0.953666">
L(G) = {ebncne &gt; 1}
</equation>
<page confidence="0.861424">
1165
</page>
<bodyText confidence="0.9998735">
It seems that we should be able to use a func-
tion &apos;0 to assign any probability distribution to
the strings in L(G) and then expect that we can
assign appropriate probabilites to the adjunc-
tions in G such that the language generated by
G has the same distribution as that given by
. However a function 0 that grows smaller
by repeated multiplication as the inverse of an
exponential function cannot be matched by any
TAG because of the constant growth property of
TAGs (see (Vijay-Shanker, 1987), P. 104). An
example of such a function &apos;0 is a simple Pois-
son distribution (2), which in fact was also used
as the counterexample in (Booth and Thomp-
son, 1973) for CFGs, since CFGs also have the
constant growth property.
</bodyText>
<equation confidence="0.999525">
1 (2)
0(anbncncr) = e • n!
</equation>
<bodyText confidence="0.98760603030303">
This shows that probabilistic TAGs, like CFGs,
are constrained in the probabilistic languages
that they can recognize or learn. As shown
above, a probabilistic language can fail to have
a generating probabilistic TAG.
The reverse is also true: some probabilis-
tic TAGs, like some CFGs, fail to have a
corresponding probabilistic language, i.e. they
are not consistent. There are two reasons
why a probabilistic TAG could be inconsistent:
&amp;quot;dirty&amp;quot; grammars, and destructive or incorrect
probability assignments.
&amp;quot;Dirty&amp;quot; grammars. Usually, when applied
to language, TAGs are lexicalized and so prob-
abilities assigned to trees are used only when
the words anchoring the trees are used in a
derivation. However, if the TAG allows non-
lexicalized trees, or more precisely, auxiliary
trees with no yield, then looping adjunctions
which never generate a string are possible. How-
ever, this can be detected and corrected by a
simple search over the grammar. Even in lexi-
calized grammars, there could be some auxiliary
trees that are assigned some probability mass
but which can never adjoin into another tree.
Such auxiliary trees are termed unreachable and
techniques similar to the ones used in detecting
unreachable productions in CFGs can be used
here to detect and eliminate such trees.
Destructive probability assignments.
This problem is a more serious one, and is the
main subject of this paper. Consider the prob-
abilistic TAG shown in (3)2.
</bodyText>
<equation confidence="0.963511125">
52
IS3
E
95(Si t--&gt; t2) = 1.0 s* a
0 S2 H t2) = 0.99
0 52 H nil) = 0.01
cb S3 IH t2) = 0.98
0 53 IH nil) = 0.02
</equation>
<bodyText confidence="0.999931791666667">
Consider a derivation in this TAG as a genera-
tive process. It proceeds as follows: node S1 in
t1 is rewritten as t2 with probability 1.0. Node
S2 in t2 is 99 times more likely than not to be
rewritten as t2 itself, and similarly node S3 is 49
times more likely than not to be rewritten as t2.
This however, creates two more instances of 52
and S3 with same probabilities. This continues,
creating multiple instances of t2 at each level of
the derivation process with each instance of t2
creating two more instances of itself. The gram-
mar itself is not malicious; the probability as-
signments are to blame. It is important to note
that inconsistency is a problem even though for
any given string there are only a finite number
of derivations, all halting. Consider the prob-
ability mass function (pmf) over the set of all
derivations for this grammar. An inconsistent
grammar would have a pmf which assigns a large
portion of probability mass to derivations that
are non-terminating. This means there is a fi-
nite probability the generative process can enter
a generation sequence which has a finite proba-
bility of non-termination.
</bodyText>
<sectionHeader confidence="0.999384" genericHeader="method">
4 Conditions for Consistency
</sectionHeader>
<bodyText confidence="0.9948805">
A probabilistic TAG G is consistent if and only
if:
</bodyText>
<equation confidence="0.987548">
E Pr(v) = 1 (4)
veL(G)
</equation>
<bodyText confidence="0.9991345">
where Pr(v) is the probability assigned to a
string in the language. If a grammar G does
not satisfy this condition, G is said to be incon-
sistent.
To explain the conditions under which a prob-
abilistic TAG is consistent we will use the TAG
</bodyText>
<footnote confidence="0.982802">
2The subscripts are used as a simple notation to
uniquely refer to the nodes in each elementary tree. They
are not part of the node label for purposes of adjunction.
</footnote>
<equation confidence="0.889588315789474">
tl Si t2
(3)
1166
in (5) as an example.
ti
t2
Sb(A1 1-4 t2) = 0.8
0(A1 nil) = 0.2
Al
al
A3
a2
t3
CA2 t2) = 0.2
002 nil) = 0.8
0 B11-4 t3) = 0.2
0 B1 nil) = 0.8
0 A3 H t2) = 0.4
0 A3 H nil) = 0.6
</equation>
<bodyText confidence="0.999298076923077">
From this grammar, we compute a square ma-
trix M which of size 1V1, where V is the set
of nodes in the grammar that can be rewrit-
ten by adjunction. Each Mij contains the ex-
pected value of obtaining node Xi when node
X, is rewritten by adjunction at each level of a
TAG derivation. We call M the stochastic ex-
pectation matrix associated with a probabilistic
TAG.
To get M for a grammar we first write a ma-
trix P which has 1V1 rows and 1/ U Al columns.
An element Pij corresponds to the probability
of adjoining tree tj at node X„ i.e. 0(X -4 ti)3.
</bodyText>
<table confidence="0.795267">
ti t2 t3
A1 - 0 0.8 0
A2 0 0.2 0
P= B1 0 0 0.2
A3 0 0.4 0
B2 0 0 0.1
</table>
<bodyText confidence="0.960449666666667">
We then write a matrix N which has 1/ U Al
rows and 1V1 columns. An element Nij is 1.0 if
node Xi is a node in tree ti.
</bodyText>
<equation confidence="0.88331875">
A1 A2 Bi A3 B2
ti 1.0 0 0 0 0
N= t2 0 1.0 1.0 1.0 0
t3 0 0 0 0 1.0
</equation>
<bodyText confidence="0.9702545">
Then the stochastic expectation matrix M is
simply the product of these two matrices.
</bodyText>
<footnote confidence="0.760831">
3Note that P is not a row stochastic matrix. This
is an important difference in the construction of M for
TAGs when compared to CFGs. We will return to this
point in §5.
</footnote>
<table confidence="0.797899833333333">
A1 A2 B1 A3 B2
A1 0 0.8 0.8 0.8 0
A2 0 0.2 0.2 0.2 0
M = P • N = B1 0 0 0 0 0.2
A3 0 0.4 0.4 0.4 0
B2 0 0 0 0 0.1
</table>
<bodyText confidence="0.997867615384615">
By inspecting the values of M in terms of the
grammar probabilities indicates that Mij con-
tains the values we wanted, i.e. expectation of
obtaining node Ai when node Ai is rewritten by
adjunction at each level of the TAG derivation
process.
By construction we have ensured that the
following theorem from (Booth and Thomp-
son, 1973) applies to probabilistic TAGs. A
formal justification for this claim is given in
the next section by showing a reduction of the
TAG derivation process to a multitype Galton-
Watson branching process (Harris, 1963).
</bodyText>
<construct confidence="0.5413278">
Theorem 4.1 A probabilistic grammar is con-
sistent if the spectral radius p(M) &lt; 1, where
M is the stochastic expectation matrix com-
puted from the grammar. (Booth and Thomp-
son, 1973; Soule, 1974)
</construct>
<bodyText confidence="0.994287">
This theorem provides a way to determine
whether a grammar is consistent. All we need to
do is compute the spectral radius of the square
matrix M which is equal to the modulus of the
largest eigenvalue of M. If this value is less than
one then the grammar is consistent4. Comput-
ing consistency can bypass the computation of
the eigenvalues for M by using the following
theorem by GerSgorin (see (Horn and Johnson,
1985; Wetherell, 1980)).
</bodyText>
<construct confidence="0.780762428571429">
Theorem 4.2 For any square matrix M,
p(M) &lt; 1 if and only if there is an n &gt; 1
such that the sum of the absolute values of
the elements of each row of Mn is less than
one. Moreover, any n&apos; &gt; n also has this prop-
erty. (Gerigorin, see (Horn and Johnson, 1985;
Wetherell, 1980))
</construct>
<footnote confidence="0.679202833333333">
4The grammar may be consistent when the spectral
radius is exactly one, but this case involves many special
considerations and is not considered in this paper. In
practice, these complicated tests are probably not worth
the effort. See (Harris, 1963) for details on how this
special case can be solved.
</footnote>
<equation confidence="0.969796666666667">
0(B2 1-4 t3) -= 0.1
0(B2 IH nil) = 0.9
(5)
</equation>
<page confidence="0.937579">
1167
</page>
<bodyText confidence="0.999544">
This makes for a very simple algorithm to
check consistency of a grammar. We sum the
values of the elements of each row of the stochas-
tic expectation matrix M computed from the
grammar. If any of the row sums are greater
than one then we compute A42, repeat the test
and compute M22 if the test fails, and so on un-
til the test succeeds5. The algorithm does not
halt if p(M) &gt; 1. In practice, such an algorithm
works better in the average case since compu-
tation of eigenvalues is more expensive for very
large matrices. An upper bound can be set on
the number of iterations in this algorithm. Once
the bound is passed, the exact eigenvalues can
be computed.
For the grammar in (5) we computed the fol-
lowing stochastic expectation matrix:
</bodyText>
<equation confidence="0.9332416">
-o 0.8 0.8 0.8 0 -
0 0.2 0.2 0.2 0
M= 0 0 0 0 0.2
0 0.4 0.4 0.4 0
0 0 0 0 0.1 _
</equation>
<bodyText confidence="0.953223428571429">
The first row sum is 2.4. Since the sum of
each row must be less than one, we compute the
power matrix M2. However, the sum of one of
the rows is still greater than 1. Continuing we
compute M22.
- 0 0.1728 0.1728 0.1728 0.0688 -
0 0.0432 0.0432 0.0432 0.0172
</bodyText>
<equation confidence="0.958117666666667">
m22 0 0 0 0 0.0002
0 0.0864 0.0864 0.0864 0.0344
0 0 0 0 0.0001
</equation>
<bodyText confidence="0.975630571428572">
This time all the row sums are less than one,
hence p(M) &lt;1. So we can say that the gram-
mar defined in (5) is consistent. We can confirm
this by computing the eigenvalues for M which
are 0,0,0.6,0 and 0.1, all less than 1.
Now consider the grammar (3) we had con-
sidered in Section 3. The value of M for that
grammar is computed to be:
Si 52 S3
Si 0 1.0 1.0
M(3) = S2 0 0.99 0.99
53 0 0.98 0.98
5We compute M22 and subsequently only successive
powers of 2 because Theorem 4.2 holds for any n&apos; &gt; n.
This permits us to use a single matrix at each step in
the algorithm.
The eigenvalues for the expectation matrix
M computed for the grammar (3) are 0, 1.97
and 0. The largest eigenvalue is greater than
1 and this confirms (3) to be an inconsistent
grammar.
</bodyText>
<sectionHeader confidence="0.827889" genericHeader="method">
5 TAG Derivations and Branching
</sectionHeader>
<subsectionHeader confidence="0.59809">
Processes
</subsectionHeader>
<bodyText confidence="0.9997272">
To show that Theorem 4.1 in Section 4 holds
for any probabilistic TAG, it is sufficient to show
that the derivation process in TAGs is a Galton-
Watson branching process.
A Galton-Watson branching process (Harris,
1963) is simply a model of processes that have
objects that can produce additional objects of
the same kind, i.e. recursive processes, with cer-
tain properties. There is an initial set of ob-
jects in the 0-th generation which produces with
some probability a first generation which in turn
with some probability generates a second, and
so on. We will denote by vectors Zo, Zi, Z2,
the 0-th, first, second, ... generations. There
are two assumptions made about Zo, Z1, Z2,
</bodyText>
<listItem confidence="0.875220125">
1. The size of the n-th generation does not
influence the probability with which any of
the objects in the (n + 1)-th generation is
produced. In other words, Zo, Zi, Z2, ...
form a Markov chain.
2. The number of objects born to a parent
object does not depend on how many other
objects are present at the same level.
</listItem>
<bodyText confidence="0.999892333333333">
We can associate a generating function for
each level Zi. The value for the vector Zn, is the
value assigned by the n-th iterate of this gen-
erating function. The expectation matrix M is
defined using this generating function.
The theorem attributed to Galton and Wat-
son specifies the conditions for the probability
of extinction of a family starting from its 0-th
generation, assuming the branching process rep-
resents a family tree (i.e, respecting the condi-
tions outlined above). The theorem states that
p(M) &lt; 1 when the probability of extinction is
</bodyText>
<page confidence="0.988049">
1168
</page>
<table confidence="0.97482894117647">
1.0.
ti level 0
t2 (0) level 1
t2 (0) t3(1) t2(1.1) level 2
t2 (1.1)t3 (0) level 3
level 4 (6)
Bi A
A2 B2 A
/\ /\
Bl A B a3 al
/\ (7)
A3 az B a3
a2 A2
/\
Bi A
A3 a2
a2
</table>
<bodyText confidence="0.998747176470588">
The assumptions made about the generating
process intuitively holds for probabilistic TAGs.
(6), for example, depicts a derivation of the
string a2a2a2a2a3a3a1 by a sequence of adjunc-
tions in the grammar given in (5)6. The parse
tree derived from such a sequence is shown in
Fig. 7. In the derivation tree (6), nodes in the
trees at each level i are rewritten by adjunction
to produce a level i + 1. There is a final level 4
in (6) since we also consider the probability that
a node is not rewritten further, i.e. Pr(A nil)
for each node A.
We give a precise statement of a TAG deriva-
tion process by defining a generating function
for the levels in a derivation tree. Each level
i in the TAG derivation tree then corresponds
to Zi in the Markov chain of branching pro-
</bodyText>
<footnote confidence="0.92854325">
6The numbers in parentheses next to the tree names
are node addresses where each tree has adjoined into
its parent. Recall the definition of node addresses in
Section 2.
</footnote>
<bodyText confidence="0.997863384615385">
cesses. This is sufficient to justify the use of
Theorem 4.1 in Section 4. The conditions on
the probability of extinction then relates to the
probability that TAG derivations for a proba-
bilistic TAG will not recurse infinitely. Hence
the probability of extinction is the same as the
probability that a probabilistic TAG is consis-
tent.
For each Xj E V, where V is the set of nodes
in the grammar where adjunction can occur,
we define the k-argument adjunction generating
function over variables Si,., sk corresponding
to the k nodes in V.
</bodyText>
<equation confidence="0.964558">
gj(si , • -•,sk) =
5k(t)
cx; t)
,EAdi(x.ouinio
</equation>
<bodyText confidence="0.998388666666667">
where, r3 (t) = 1 if node Xj is in tree t, rj(t) = 0
otherwise.
For example, for the grammar in (5) we get
the following adjunction generating functions
taking the variable Si, s2, s3, 34, s5 to represent
the nodes A1, A2, B1, A3, B2 respectively.
</bodyText>
<equation confidence="0.949496705882353">
gi(si, • - • ,ss) =
0(Al. t2) • s2 • s3 s4 + 4)(A1 1--.&gt; nil)
92(.91, • • • s5) =
(1)(A2 1-+ t2) &apos; S2 • S3S4 ± 4(A2 1-4 nil)
g3(si, • • • ,55) =
o(Bi t3) • s5 + q5(B1 1-4 nil)
94(si, s5) =
CA3 t2) • S2 &apos; 83 &apos; ± 4)(A3 nil)
95(51, s5) =
0(B2 1-4 t3) • 85 + (1)(B2 nil)
The n-th level generating function
Gn(si, ,$) is defined recursively as fol-
lows.
Go (Si • • • , Sk) = Si
GI (Si, • • • , Sk) gl (S1, • • • , Sk)
Gn(Si, • • • , Sk) = (Si, • • Sk), • • • ,
gk (S1, • • • ,Sk)]
</equation>
<bodyText confidence="0.999912">
For the grammar in (5) we get the following
level generating functions.
</bodyText>
<equation confidence="0.9487468">
GO(Si, • • • = Si
1169
(si , ..• ,s5) gl(S1). • • Ss)
tfi(Al t2) • S2 • 53 • S4 + q5(Al H nil)
= 0.8 • .92 • S3 • S4 ± 0.2
G2(81, • • ,s5) =
0(A2H tz ) [92 (si , s5)][g3(51, • • •
[g4(si, • • • ,s5)] + 0(A2 nil)
0.08s34sis5+ 0.034s3si+ 0.0482838485 +
0.18s2s3s4 + 0.04s5 + 0.196
</equation>
<bodyText confidence="0.999662166666667">
Examining this example, we can express
Gi(si,...,sk) as a sum Di(si,... , sk) +
where Ci is a constant and Di(.) is a polyno-
mial with no constant terms. A probabilistic
TAG will be consistent if these recursive equa-
tions terminate, i.e. if
</bodyText>
<equation confidence="0.746517">
, sk) 0
</equation>
<bodyText confidence="0.999971090909091">
We can rewrite the level generation functions in
terms of the stochastic expectation matrix M,
where each element m2,3 of M is computed as
follows (cf. (Booth and Thompson, 1973)).
The limit condition above translates to the con-
dition that the spectral radius of M must be
less than 1 for the grammar to be consistent.
This shows that Theorem 4.1 used in Sec-
tion 4 to give an algorithm to detect inconsis-
tency in a probabilistic holds for any given TAG,
hence demonstrating the correctness of the al-
gorithm.
Note that the formulation of the adjunction
generating function means that the values for
O(X 1-4 nil) for all X E V do not appear in
the expectation matrix. This is a crucial differ-
ence between the test for consistency in TAGs
as compared to CFGs. For CFGs, the expecta-
tion matrix for a grammar G can be interpreted
as the contribution of each non-terminal to the
derivations for a sample set of strings drawn
from L(G). Using this it was shown in (Chaud-
hari et al., 1983) and (Sanchez and Benedi,
1997) that a single step of the inside-outside
algorithm implies consistency for a probabilis-
tic CFG. However, in the TAG case, the inclu-
sion of values for .0(X 1--+ nil) (which is essen-
tial if we are to interpret the expectation ma-
trix in terms of derivations over a sample set of
strings) means that we cannot use the method
used in (8) to compute the expectation matrix
and furthermore the limit condition will not be
convergent.
</bodyText>
<sectionHeader confidence="0.999362" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999990142857143">
We have shown in this paper the conditions
under which a given probabilistic TAG can be
shown to be consistent. We gave a simple al-
gorithm for checking consistency and gave the
formal justification for its correctness. The re-
sult is practically significant for its applications
in checking for deficiency in probabilistic TAGs.
</bodyText>
<sectionHeader confidence="0.999185" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999256372093023">
T. L. Booth and R. A. Thompson. 1973. Applying prob-
ability measures to abstract languages. IEEE Trans-
actions on Computers, C-22(5):442-450, May.
J. Carroll and D. Weir. 1997. Encoding frequency in-
formation in lexicalized grammars. In Proc. 5th Int&apos;l
Workshop on Parsing Technologies IWPT-97, Cam-
bridge, Mass.
R. Chaudhari, S. Pham, and 0. N. Garcia. 1983. Solu-
tion of an open problem on probabilistic grammars.
IEEE Transactions on Computers, C-32(8):748-750,
August.
T. E. Harris. 1963. The Theory of Branching Processes.
Springer-Verlag, Berlin.
R. A. Horn and C. R. Johnson. 1985. Matrix Analysis.
Cambridge University Press, Cambridge.
A. K. Joshi and Y. Schabes. 1992. Tree-adjoining gram-
mar and lexicalized grammars. In M. Nivat and
A. Podelski, editors, Tree automata and languages,
pages 409-431. Elsevier Science.
A. K. Joshi. 1988. An introduction to tree adjoining
grammars. In A. Manaster-Ramer, editor, Mathemat-
ics of Language. John Benjamins, Amsterdam.
0. Rambow and A. Joshi. 1995. A formal look at de-
pendency grammars and phrase-structure grammars,
with special consideration of word-order phenomena.
In Leo Wanner, editor, Current Issues in Meaning-
Text Theory. Pinter, London.
J.-A. Sanchez and J.-M. Benedi. 1997. Consistency of
stochastic context-free grammars from probabilistic
estimation based on growth transformations. IEEE
Transactions on Pattern Analysis and Machine Intel-
ligence, 19(9):1052-1055, September.
Y. Schabes. 1992. Stochastic lexicalized tree-adjoining
grammars. In Proc. of COLING &apos;92, volume 2, pages
426-432, Nantes, France.
S. Soule. 1974. Entropies of probabilistic grammars. Inf.
Control, 25:55-74.
K. Vijay-Shanker. 1987. A Study of Tree Adjoining
Grammars. Ph.D. thesis, Department of Computer
and Information Science, University of Pennsylvania.
C. S. Wetherell. 1980. Probabilistic languages: A re-
view and some open questions. Computing Surveys,
12(4):361-379.
</reference>
<figure confidence="0.8906515">
ayi (si , • • • , sk)
Mi =
(8)
as;
</figure>
<page confidence="0.870749">
1170
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.883022">
<title confidence="0.9976755">Conditions on Consistency of Probabilistic Tree Adjoining Grammars*</title>
<author confidence="0.994784">Anoop Sarkar</author>
<affiliation confidence="0.999629">Dept. of Computer and Information Science University of Pennsylvania</affiliation>
<address confidence="0.9980505">200 South 33rd Street, Philadelphia, PA 19104-6389 USA</address>
<email confidence="0.999596">anoopOlinc.cis.upenn.edu</email>
<abstract confidence="0.995020956521739">Much of the power of probabilistic methods in modelling language comes from their ability to compare several derivations for the same string in the language. An important starting point for the study of such cross-derivational properis the notion of probability model defined by a probabilistic grammar is to be the probabilities assigned to all the strings in the language sum to one. From the literature on probabilistic context-free grammars (CFGs), we know precisely the conditions which ensure that consistency is true for a given CFG. This paper derives the conditions under which a given probabilistic Tree Adjoining Grammar (TAG) can be shown to be consistent. It gives a simple algorithm for checking consistency and gives the formal justification for its correctness. The conditions derived here can be used to ensure that probability models use TAGs can be checked for (i.e. whether any probability mass is assigned to strings that cannot be generated).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T L Booth</author>
<author>R A Thompson</author>
</authors>
<title>Applying probability measures to abstract languages.</title>
<date>1973</date>
<journal>IEEE Transactions on Computers,</journal>
<pages>22--5</pages>
<contexts>
<context position="8120" citStr="Booth and Thompson, 1973" startWordPosition="1417" endWordPosition="1421">e to use a function &apos;0 to assign any probability distribution to the strings in L(G) and then expect that we can assign appropriate probabilites to the adjunctions in G such that the language generated by G has the same distribution as that given by . However a function 0 that grows smaller by repeated multiplication as the inverse of an exponential function cannot be matched by any TAG because of the constant growth property of TAGs (see (Vijay-Shanker, 1987), P. 104). An example of such a function &apos;0 is a simple Poisson distribution (2), which in fact was also used as the counterexample in (Booth and Thompson, 1973) for CFGs, since CFGs also have the constant growth property. 1 (2) 0(anbncncr) = e • n! This shows that probabilistic TAGs, like CFGs, are constrained in the probabilistic languages that they can recognize or learn. As shown above, a probabilistic language can fail to have a generating probabilistic TAG. The reverse is also true: some probabilistic TAGs, like some CFGs, fail to have a corresponding probabilistic language, i.e. they are not consistent. There are two reasons why a probabilistic TAG could be inconsistent: &amp;quot;dirty&amp;quot; grammars, and destructive or incorrect probability assignments. &amp;quot;D</context>
<context position="13094" citStr="Booth and Thompson, 1973" startWordPosition="2376" endWordPosition="2380">matrices. 3Note that P is not a row stochastic matrix. This is an important difference in the construction of M for TAGs when compared to CFGs. We will return to this point in §5. A1 A2 B1 A3 B2 A1 0 0.8 0.8 0.8 0 A2 0 0.2 0.2 0.2 0 M = P • N = B1 0 0 0 0 0.2 A3 0 0.4 0.4 0.4 0 B2 0 0 0 0 0.1 By inspecting the values of M in terms of the grammar probabilities indicates that Mij contains the values we wanted, i.e. expectation of obtaining node Ai when node Ai is rewritten by adjunction at each level of the TAG derivation process. By construction we have ensured that the following theorem from (Booth and Thompson, 1973) applies to probabilistic TAGs. A formal justification for this claim is given in the next section by showing a reduction of the TAG derivation process to a multitype GaltonWatson branching process (Harris, 1963). Theorem 4.1 A probabilistic grammar is consistent if the spectral radius p(M) &lt; 1, where M is the stochastic expectation matrix computed from the grammar. (Booth and Thompson, 1973; Soule, 1974) This theorem provides a way to determine whether a grammar is consistent. All we need to do is compute the spectral radius of the square matrix M which is equal to the modulus of the largest </context>
<context position="21252" citStr="Booth and Thompson, 1973" startWordPosition="3955" endWordPosition="3958">+ q5(Al H nil) = 0.8 • .92 • S3 • S4 ± 0.2 G2(81, • • ,s5) = 0(A2H tz ) [92 (si , s5)][g3(51, • • • [g4(si, • • • ,s5)] + 0(A2 nil) 0.08s34sis5+ 0.034s3si+ 0.0482838485 + 0.18s2s3s4 + 0.04s5 + 0.196 Examining this example, we can express Gi(si,...,sk) as a sum Di(si,... , sk) + where Ci is a constant and Di(.) is a polynomial with no constant terms. A probabilistic TAG will be consistent if these recursive equations terminate, i.e. if , sk) 0 We can rewrite the level generation functions in terms of the stochastic expectation matrix M, where each element m2,3 of M is computed as follows (cf. (Booth and Thompson, 1973)). The limit condition above translates to the condition that the spectral radius of M must be less than 1 for the grammar to be consistent. This shows that Theorem 4.1 used in Section 4 to give an algorithm to detect inconsistency in a probabilistic holds for any given TAG, hence demonstrating the correctness of the algorithm. Note that the formulation of the adjunction generating function means that the values for O(X 1-4 nil) for all X E V do not appear in the expectation matrix. This is a crucial difference between the test for consistency in TAGs as compared to CFGs. For CFGs, the expecta</context>
</contexts>
<marker>Booth, Thompson, 1973</marker>
<rawString>T. L. Booth and R. A. Thompson. 1973. Applying probability measures to abstract languages. IEEE Transactions on Computers, C-22(5):442-450, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>D Weir</author>
</authors>
<title>Encoding frequency information in lexicalized grammars.</title>
<date>1997</date>
<booktitle>In Proc. 5th Int&apos;l Workshop on Parsing Technologies IWPT-97,</booktitle>
<location>Cambridge, Mass.</location>
<contexts>
<context position="4527" citStr="Carroll and Weir, 1997" startWordPosition="728" endWordPosition="731">owing reasons: • The conditions derived here can be used to ensure that probability models that use TAGs can be checked for deficiency. • Existing EM based estimation algorithms for probabilistic TAGs assume that the property of consistency holds (Schabes, 1992). EM based algorithms begin with an initial (usually random) value for each parameter. If the initial assignment causes the grammar to be inconsistent, then iterative re-estimation might converge to an inconsistent grammar&apos;. • Techniques used in this paper can be used to determine consistency for other probability models based on TAGs (Carroll and Weir, 1997). 2 Notation In this section we establish some notational conventions and definitions that we use in this paper. Those familiar with the TAG formalism only need to give a cursory glance through this section. A probabilistic TAG is represented by (N, E, I, A, S,0) where N, E are, respectively, non-terminal and terminal symbols. I U A is a set of trees termed as elementary trees. We take V to be the set of all nodes in all the elementary trees. For each leaf A E V, label(A) is an element from E U {€}, and for each other node A, label(A) is an element from N. S is an element from N which is a dis</context>
</contexts>
<marker>Carroll, Weir, 1997</marker>
<rawString>J. Carroll and D. Weir. 1997. Encoding frequency information in lexicalized grammars. In Proc. 5th Int&apos;l Workshop on Parsing Technologies IWPT-97, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Chaudhari</author>
<author>S Pham</author>
</authors>
<title>Solution of an open problem on probabilistic grammars.</title>
<date>1983</date>
<journal>IEEE Transactions on Computers,</journal>
<pages>32--8</pages>
<marker>Chaudhari, Pham, 1983</marker>
<rawString>R. Chaudhari, S. Pham, and 0. N. Garcia. 1983. Solution of an open problem on probabilistic grammars. IEEE Transactions on Computers, C-32(8):748-750, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T E Harris</author>
</authors>
<title>The Theory of Branching Processes.</title>
<date>1963</date>
<publisher>Springer-Verlag,</publisher>
<location>Berlin.</location>
<contexts>
<context position="13306" citStr="Harris, 1963" startWordPosition="2413" endWordPosition="2414">.2 0.2 0 M = P • N = B1 0 0 0 0 0.2 A3 0 0.4 0.4 0.4 0 B2 0 0 0 0 0.1 By inspecting the values of M in terms of the grammar probabilities indicates that Mij contains the values we wanted, i.e. expectation of obtaining node Ai when node Ai is rewritten by adjunction at each level of the TAG derivation process. By construction we have ensured that the following theorem from (Booth and Thompson, 1973) applies to probabilistic TAGs. A formal justification for this claim is given in the next section by showing a reduction of the TAG derivation process to a multitype GaltonWatson branching process (Harris, 1963). Theorem 4.1 A probabilistic grammar is consistent if the spectral radius p(M) &lt; 1, where M is the stochastic expectation matrix computed from the grammar. (Booth and Thompson, 1973; Soule, 1974) This theorem provides a way to determine whether a grammar is consistent. All we need to do is compute the spectral radius of the square matrix M which is equal to the modulus of the largest eigenvalue of M. If this value is less than one then the grammar is consistent4. Computing consistency can bypass the computation of the eigenvalues for M by using the following theorem by GerSgorin (see (Horn an</context>
<context position="16722" citStr="Harris, 1963" startWordPosition="3069" endWordPosition="3070">mpute M22 and subsequently only successive powers of 2 because Theorem 4.2 holds for any n&apos; &gt; n. This permits us to use a single matrix at each step in the algorithm. The eigenvalues for the expectation matrix M computed for the grammar (3) are 0, 1.97 and 0. The largest eigenvalue is greater than 1 and this confirms (3) to be an inconsistent grammar. 5 TAG Derivations and Branching Processes To show that Theorem 4.1 in Section 4 holds for any probabilistic TAG, it is sufficient to show that the derivation process in TAGs is a GaltonWatson branching process. A Galton-Watson branching process (Harris, 1963) is simply a model of processes that have objects that can produce additional objects of the same kind, i.e. recursive processes, with certain properties. There is an initial set of objects in the 0-th generation which produces with some probability a first generation which in turn with some probability generates a second, and so on. We will denote by vectors Zo, Zi, Z2, the 0-th, first, second, ... generations. There are two assumptions made about Zo, Z1, Z2, 1. The size of the n-th generation does not influence the probability with which any of the objects in the (n + 1)-th generation is pro</context>
</contexts>
<marker>Harris, 1963</marker>
<rawString>T. E. Harris. 1963. The Theory of Branching Processes. Springer-Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R A Horn</author>
<author>C R Johnson</author>
</authors>
<title>Matrix Analysis.</title>
<date>1985</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="13921" citStr="Horn and Johnson, 1985" startWordPosition="2520" endWordPosition="2523">, 1963). Theorem 4.1 A probabilistic grammar is consistent if the spectral radius p(M) &lt; 1, where M is the stochastic expectation matrix computed from the grammar. (Booth and Thompson, 1973; Soule, 1974) This theorem provides a way to determine whether a grammar is consistent. All we need to do is compute the spectral radius of the square matrix M which is equal to the modulus of the largest eigenvalue of M. If this value is less than one then the grammar is consistent4. Computing consistency can bypass the computation of the eigenvalues for M by using the following theorem by GerSgorin (see (Horn and Johnson, 1985; Wetherell, 1980)). Theorem 4.2 For any square matrix M, p(M) &lt; 1 if and only if there is an n &gt; 1 such that the sum of the absolute values of the elements of each row of Mn is less than one. Moreover, any n&apos; &gt; n also has this property. (Gerigorin, see (Horn and Johnson, 1985; Wetherell, 1980)) 4The grammar may be consistent when the spectral radius is exactly one, but this case involves many special considerations and is not considered in this paper. In practice, these complicated tests are probably not worth the effort. See (Harris, 1963) for details on how this special case can be solved. </context>
</contexts>
<marker>Horn, Johnson, 1985</marker>
<rawString>R. A. Horn and C. R. Johnson. 1985. Matrix Analysis. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
<author>Y Schabes</author>
</authors>
<title>Tree-adjoining grammar and lexicalized grammars.</title>
<date>1992</date>
<booktitle>Tree automata and languages,</booktitle>
<pages>409--431</pages>
<editor>In M. Nivat and A. Podelski, editors,</editor>
<publisher>Elsevier Science.</publisher>
<contexts>
<context position="3572" citStr="Joshi and Schabes, 1992" startWordPosition="580" endWordPosition="584"> where Pr(v) = 0 if v then E Pr(v) = 1 (1) vEL(G) From the literature on probabilistic contextfree grammars (CFGs) we know precisely the conditions which ensure that (1) is true for a given CFG. This paper derives the conditions under which a given probabilistic TAG can be shown to be consistent. TAGs are important in the modelling of natural language since they can be easily lexicalized; moreover the trees associated with words can be used to encode argument and adjunct relations in various syntactic environments. This paper assumes some familiarity with the TAG formalism. (Joshi, 1988) and (Joshi and Schabes, 1992) are good introductions to the formalism and its linguistic relevance. TAGs have 1164 been shown to have relations with both phrasestructure grammars and dependency grammars (Rambow and Joshi, 1995) and can handle (non-projective) long distance dependencies. Consistency of probabilistic TAGs has practical significance for the following reasons: • The conditions derived here can be used to ensure that probability models that use TAGs can be checked for deficiency. • Existing EM based estimation algorithms for probabilistic TAGs assume that the property of consistency holds (Schabes, 1992). EM b</context>
</contexts>
<marker>Joshi, Schabes, 1992</marker>
<rawString>A. K. Joshi and Y. Schabes. 1992. Tree-adjoining grammar and lexicalized grammars. In M. Nivat and A. Podelski, editors, Tree automata and languages, pages 409-431. Elsevier Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
</authors>
<title>An introduction to tree adjoining grammars.</title>
<date>1988</date>
<booktitle>Mathematics of Language. John Benjamins,</booktitle>
<editor>In A. Manaster-Ramer, editor,</editor>
<location>Amsterdam.</location>
<contexts>
<context position="3542" citStr="Joshi, 1988" startWordPosition="577" endWordPosition="578">ach string v E E*, where Pr(v) = 0 if v then E Pr(v) = 1 (1) vEL(G) From the literature on probabilistic contextfree grammars (CFGs) we know precisely the conditions which ensure that (1) is true for a given CFG. This paper derives the conditions under which a given probabilistic TAG can be shown to be consistent. TAGs are important in the modelling of natural language since they can be easily lexicalized; moreover the trees associated with words can be used to encode argument and adjunct relations in various syntactic environments. This paper assumes some familiarity with the TAG formalism. (Joshi, 1988) and (Joshi and Schabes, 1992) are good introductions to the formalism and its linguistic relevance. TAGs have 1164 been shown to have relations with both phrasestructure grammars and dependency grammars (Rambow and Joshi, 1995) and can handle (non-projective) long distance dependencies. Consistency of probabilistic TAGs has practical significance for the following reasons: • The conditions derived here can be used to ensure that probability models that use TAGs can be checked for deficiency. • Existing EM based estimation algorithms for probabilistic TAGs assume that the property of consisten</context>
</contexts>
<marker>Joshi, 1988</marker>
<rawString>A. K. Joshi. 1988. An introduction to tree adjoining grammars. In A. Manaster-Ramer, editor, Mathematics of Language. John Benjamins, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rambow</author>
<author>A Joshi</author>
</authors>
<title>A formal look at dependency grammars and phrase-structure grammars, with special consideration of word-order phenomena.</title>
<date>1995</date>
<booktitle>Current Issues in MeaningText Theory.</booktitle>
<editor>In Leo Wanner, editor,</editor>
<publisher>Pinter,</publisher>
<location>London.</location>
<contexts>
<context position="3770" citStr="Rambow and Joshi, 1995" startWordPosition="612" endWordPosition="615"> paper derives the conditions under which a given probabilistic TAG can be shown to be consistent. TAGs are important in the modelling of natural language since they can be easily lexicalized; moreover the trees associated with words can be used to encode argument and adjunct relations in various syntactic environments. This paper assumes some familiarity with the TAG formalism. (Joshi, 1988) and (Joshi and Schabes, 1992) are good introductions to the formalism and its linguistic relevance. TAGs have 1164 been shown to have relations with both phrasestructure grammars and dependency grammars (Rambow and Joshi, 1995) and can handle (non-projective) long distance dependencies. Consistency of probabilistic TAGs has practical significance for the following reasons: • The conditions derived here can be used to ensure that probability models that use TAGs can be checked for deficiency. • Existing EM based estimation algorithms for probabilistic TAGs assume that the property of consistency holds (Schabes, 1992). EM based algorithms begin with an initial (usually random) value for each parameter. If the initial assignment causes the grammar to be inconsistent, then iterative re-estimation might converge to an in</context>
</contexts>
<marker>Rambow, Joshi, 1995</marker>
<rawString>0. Rambow and A. Joshi. 1995. A formal look at dependency grammars and phrase-structure grammars, with special consideration of word-order phenomena. In Leo Wanner, editor, Current Issues in MeaningText Theory. Pinter, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-A Sanchez</author>
<author>J-M Benedi</author>
</authors>
<title>Consistency of stochastic context-free grammars from probabilistic estimation based on growth transformations.</title>
<date>1997</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>19--9</pages>
<contexts>
<context position="22086" citStr="Sanchez and Benedi, 1997" startWordPosition="4108" endWordPosition="4111">o detect inconsistency in a probabilistic holds for any given TAG, hence demonstrating the correctness of the algorithm. Note that the formulation of the adjunction generating function means that the values for O(X 1-4 nil) for all X E V do not appear in the expectation matrix. This is a crucial difference between the test for consistency in TAGs as compared to CFGs. For CFGs, the expectation matrix for a grammar G can be interpreted as the contribution of each non-terminal to the derivations for a sample set of strings drawn from L(G). Using this it was shown in (Chaudhari et al., 1983) and (Sanchez and Benedi, 1997) that a single step of the inside-outside algorithm implies consistency for a probabilistic CFG. However, in the TAG case, the inclusion of values for .0(X 1--+ nil) (which is essential if we are to interpret the expectation matrix in terms of derivations over a sample set of strings) means that we cannot use the method used in (8) to compute the expectation matrix and furthermore the limit condition will not be convergent. 6 Conclusion We have shown in this paper the conditions under which a given probabilistic TAG can be shown to be consistent. We gave a simple algorithm for checking consist</context>
</contexts>
<marker>Sanchez, Benedi, 1997</marker>
<rawString>J.-A. Sanchez and J.-M. Benedi. 1997. Consistency of stochastic context-free grammars from probabilistic estimation based on growth transformations. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(9):1052-1055, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Schabes</author>
</authors>
<title>Stochastic lexicalized tree-adjoining grammars.</title>
<date>1992</date>
<booktitle>In Proc. of COLING &apos;92,</booktitle>
<volume>2</volume>
<pages>426--432</pages>
<location>Nantes, France.</location>
<contexts>
<context position="3572" citStr="Schabes, 1992" startWordPosition="582" endWordPosition="584">v) = 0 if v then E Pr(v) = 1 (1) vEL(G) From the literature on probabilistic contextfree grammars (CFGs) we know precisely the conditions which ensure that (1) is true for a given CFG. This paper derives the conditions under which a given probabilistic TAG can be shown to be consistent. TAGs are important in the modelling of natural language since they can be easily lexicalized; moreover the trees associated with words can be used to encode argument and adjunct relations in various syntactic environments. This paper assumes some familiarity with the TAG formalism. (Joshi, 1988) and (Joshi and Schabes, 1992) are good introductions to the formalism and its linguistic relevance. TAGs have 1164 been shown to have relations with both phrasestructure grammars and dependency grammars (Rambow and Joshi, 1995) and can handle (non-projective) long distance dependencies. Consistency of probabilistic TAGs has practical significance for the following reasons: • The conditions derived here can be used to ensure that probability models that use TAGs can be checked for deficiency. • Existing EM based estimation algorithms for probabilistic TAGs assume that the property of consistency holds (Schabes, 1992). EM b</context>
</contexts>
<marker>Schabes, 1992</marker>
<rawString>Y. Schabes. 1992. Stochastic lexicalized tree-adjoining grammars. In Proc. of COLING &apos;92, volume 2, pages 426-432, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Soule</author>
</authors>
<title>Entropies of probabilistic grammars.</title>
<date>1974</date>
<journal>Inf. Control,</journal>
<pages>25--55</pages>
<contexts>
<context position="13502" citStr="Soule, 1974" startWordPosition="2447" endWordPosition="2448">ctation of obtaining node Ai when node Ai is rewritten by adjunction at each level of the TAG derivation process. By construction we have ensured that the following theorem from (Booth and Thompson, 1973) applies to probabilistic TAGs. A formal justification for this claim is given in the next section by showing a reduction of the TAG derivation process to a multitype GaltonWatson branching process (Harris, 1963). Theorem 4.1 A probabilistic grammar is consistent if the spectral radius p(M) &lt; 1, where M is the stochastic expectation matrix computed from the grammar. (Booth and Thompson, 1973; Soule, 1974) This theorem provides a way to determine whether a grammar is consistent. All we need to do is compute the spectral radius of the square matrix M which is equal to the modulus of the largest eigenvalue of M. If this value is less than one then the grammar is consistent4. Computing consistency can bypass the computation of the eigenvalues for M by using the following theorem by GerSgorin (see (Horn and Johnson, 1985; Wetherell, 1980)). Theorem 4.2 For any square matrix M, p(M) &lt; 1 if and only if there is an n &gt; 1 such that the sum of the absolute values of the elements of each row of Mn is les</context>
</contexts>
<marker>Soule, 1974</marker>
<rawString>S. Soule. 1974. Entropies of probabilistic grammars. Inf. Control, 25:55-74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
</authors>
<title>A Study of Tree Adjoining Grammars.</title>
<date>1987</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer and Information Science, University of Pennsylvania.</institution>
<contexts>
<context position="7959" citStr="Vijay-Shanker, 1987" startWordPosition="1389" endWordPosition="1390">nments to languages, let us take for example, a language well known to be a tree adjoining language: L(G) = {ebncne &gt; 1} 1165 It seems that we should be able to use a function &apos;0 to assign any probability distribution to the strings in L(G) and then expect that we can assign appropriate probabilites to the adjunctions in G such that the language generated by G has the same distribution as that given by . However a function 0 that grows smaller by repeated multiplication as the inverse of an exponential function cannot be matched by any TAG because of the constant growth property of TAGs (see (Vijay-Shanker, 1987), P. 104). An example of such a function &apos;0 is a simple Poisson distribution (2), which in fact was also used as the counterexample in (Booth and Thompson, 1973) for CFGs, since CFGs also have the constant growth property. 1 (2) 0(anbncncr) = e • n! This shows that probabilistic TAGs, like CFGs, are constrained in the probabilistic languages that they can recognize or learn. As shown above, a probabilistic language can fail to have a generating probabilistic TAG. The reverse is also true: some probabilistic TAGs, like some CFGs, fail to have a corresponding probabilistic language, i.e. they ar</context>
</contexts>
<marker>Vijay-Shanker, 1987</marker>
<rawString>K. Vijay-Shanker. 1987. A Study of Tree Adjoining Grammars. Ph.D. thesis, Department of Computer and Information Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C S Wetherell</author>
</authors>
<title>Probabilistic languages: A review and some open questions. Computing Surveys,</title>
<date>1980</date>
<pages>12--4</pages>
<contexts>
<context position="13939" citStr="Wetherell, 1980" startWordPosition="2524" endWordPosition="2525">robabilistic grammar is consistent if the spectral radius p(M) &lt; 1, where M is the stochastic expectation matrix computed from the grammar. (Booth and Thompson, 1973; Soule, 1974) This theorem provides a way to determine whether a grammar is consistent. All we need to do is compute the spectral radius of the square matrix M which is equal to the modulus of the largest eigenvalue of M. If this value is less than one then the grammar is consistent4. Computing consistency can bypass the computation of the eigenvalues for M by using the following theorem by GerSgorin (see (Horn and Johnson, 1985; Wetherell, 1980)). Theorem 4.2 For any square matrix M, p(M) &lt; 1 if and only if there is an n &gt; 1 such that the sum of the absolute values of the elements of each row of Mn is less than one. Moreover, any n&apos; &gt; n also has this property. (Gerigorin, see (Horn and Johnson, 1985; Wetherell, 1980)) 4The grammar may be consistent when the spectral radius is exactly one, but this case involves many special considerations and is not considered in this paper. In practice, these complicated tests are probably not worth the effort. See (Harris, 1963) for details on how this special case can be solved. 0(B2 1-4 t3) -= 0.</context>
</contexts>
<marker>Wetherell, 1980</marker>
<rawString>C. S. Wetherell. 1980. Probabilistic languages: A review and some open questions. Computing Surveys, 12(4):361-379.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>