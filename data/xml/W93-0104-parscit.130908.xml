<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000506">
<title confidence="0.991272">
Internal and External Evidence in the Identification and
Semantic Categorization of Proper Names
</title>
<author confidence="0.945158">
David D. McDonald
</author>
<email confidence="0.990442">
mcdonald@cs.brandeis.edu
</email>
<sectionHeader confidence="0.992273" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9991112">
We describe the proper name recognition and classification facility (&amp;quot;PNF&amp;quot;) of the SPARSER
natural language understanding system. PNF has been used very successfully in the analysis of
unrestricted texts in several sublanguages taken from online news sources. It makes its categoriza-
tions on the basis of &apos;external&apos; evidence from the context of the phrases adjacent to the name as well
as &apos;internal&apos; evidence within the sequence of words and characters. A semantic model of each name
and its components is maintained and used for subsequent reference.
We describe PNF&apos;s operations of delimiting, classifying, and semantically recording the
structure of a name; we situate PNF with respect to the related parsing mechanisms within Sparser;
and finally we work through an extended example that is typical of the sorts of text we have applied
PNF to.
</bodyText>
<sectionHeader confidence="0.965602" genericHeader="method">
1 Introduction: Internal versus External Evidence
</sectionHeader>
<bodyText confidence="0.9999810625">
It has long been appreciated by people working with proper names in unrestricted written texts
that any adequate treatment requires the use of a grammar. We know at this point that the correct
identification and semantic categorization of names requires an analysis based on the patterning
of orthographic and lexical classes of elements that is analogous to what one finds in the other
content-rich, syntactic structure-poor phrase types in the &apos;periphery&apos; of the language such as
numbers, dates, citations, etc. The point of this paper will be to argue that this grammar must be
context sensitive, and that it should incorporate a rich semantic model of names and their
relationships to individuals.
The context-sensitivity requirement derives from the fact that the classification of a proper
name involves two complementary kinds of evidence, which we will term &apos;internal&apos; and
&apos;external&apos;. Internal evidence is derived from within the sequence of words that comprise the
name. This can be definitive criteria, such as the presence of known &apos;incorporation terms&apos;
(&amp;quot;Ltd.&amp;quot;, &amp;quot;G.m.b.H.&amp;quot;) that indicate companies; or heuristic criteria such as abbreviations or
known first names often indicating people. Name-internal evidence is the only criteria consid-
ered in virtually all of the name recognition systems that are reported as part of state of the art
information extraction systems (see e.g. Rau 1991, Alshawi 1992, DARPA 1992), most of
</bodyText>
<page confidence="0.998187">
32
</page>
<bodyText confidence="0.999659155555555">
which depend on large (-20,000 word) gazetteers and lists of known names for their relatively
high performance.
By contrast, external evidence is the classificatory criteria provided by the context in which
a name appears. The basis for this evidence is the obvious observation that names are just ways
to refer to individuals of specific types (people, churches, rock groups, etc.), and that these
types have characteristic properties and participate in characteristic events. The presence of these
properties or events in the immediate context of a proper name can be used to provide confirm-
ing or criterial evidence for a name&apos;s category. External evidence is analyzed in PNF in terms of
substitution contexts and operationalized in terms of context-sensitive rewrite rules.
External evidence is a necessity for high accuracy performance. One obvious reason is that
predefined word lists can never be complete. Another is that in many instances, especially those
involving subsequent references, external evidence will override internal evidence. In the final
consideration it is always the way a phrase is used—the attributions and predications it is part
of—that make it a proper name of a given sort; without the consideration of external evidence
this definitive criteria is missed, resulting in mistakes and confusion in the state of the parser.
(Relying solely on name lists has led to some funny errors, for example mistaking the food
company Sara Lee for a person. Even some external evidence such as a title can be inadequate,
if considered apart from the wider context of use, as in General Mills—both actual mistakes
made by an otherwise quite reasonable program some years ago (Masand &amp; Duffey 1985).)
An additional reason, and one with considerable engineering utility from the point of view
of the grammar writer, is that the inclusion of external evidence into the mix of available analy-
sis tools reduces the demands on the judgements one requires of internal evidence. It can
provide a weaker (less specific) categorization about which it can be more certain, which can
then be refined as external evidence becomes available. Lacking definitive internal evidence one
can initially label a segment simply as a &apos;name&apos;, and then later strengthen the judgement when,
e.g., the segment is found to be adjacent to an age phrase or a title and context-sensitive rewrite
rules are triggered to re-label it as a person and to initiate the appropriate semantic processes.
This kind of staged analysis is a requirement when the conclusions from internal evidence
are ambiguous. It is not uncommon, for example, to have the names of a person and a company
in the same news article both share a word, i.e. when the company is named after its founder. A
subsequent reference using just that word cannot be definitively categorized on internal evidence
alone, and must wait for the application of external evidence from the context. (In the event that
the context is inadequate, as when it involves a predication not in the grammar, such &apos;name&apos;
segments can be left to default judgements by statistical heuristics operating after a first pass by
the parser, and the stronger categorizations then tested for coherency as the parse is resumed. In
the case of a company and a person with the same name, a well edited publication is unlikely to
use the ambiguous word to refer to the founder without prefixing it with &amp;quot;Mr.&amp;quot; or &amp;quot;Ms. as
needs be, so a word with both person and company denotations but without external evidence
can be assumed with some assurance to be referring to the company.)
In this paper we will describe in some detail the proper name facility of the SPARSER
natural language understanding system: &amp;quot;PNF&amp;quot;, with particular attention to how it uses external
evidence and deploys its semantic model of names and their referents to handle ambiguities such
as the one noted just above.
In a blind test of an earlier implementation of PNF in the context of &amp;quot;Who&apos;s News&amp;quot; articles
from the the Wall Street Journal, it performed at nearing 100% in the scored sentences in the
</bodyText>
<page confidence="0.998457">
33
</page>
<bodyText confidence="0.999899">
sublanguage for which a full grammar had been prepared. We are currently testing a new
implementation on a more diverse set of texts.
Space will not permit a comparison of this algorithm with other approaches to proper
names beyond occasional remarks and references. As far as we know this is the only treatment
of proper names that makes essential use of context-sensitive rewrite rules, however the FUNES
system of Sam Coates-Stephens (1992) is very similar to this work in making essential use of
external evidence, and Coates-Stephens&apos;s extensive research into proper names is an important
contribution to the field; we have adopted some of his terminology as noted below.
</bodyText>
<sectionHeader confidence="0.468661" genericHeader="method">
2 An overview of the procedure: Delimit, Classify, Record
</sectionHeader>
<bodyText confidence="0.999857727272727">
The goal of the proper name facility in Sparser (PNF) is to form and interpret full phrasal
constituents—noun phrases—that fit into the rest of a text&apos;s parse and contribute to the analysis
of the entire text just like any other kind of constituent. That is, PNF is operating as a compon-
ent in a larger natural language comprehension system, and not as a standalone facility intended
for name spotting, indexing, or other tasks based on skimming. This integration is essential to
the way the PNF makes its decisions; it would not operate with anything like the same level of
performance if it were independent, since there would then be no source of external evidence.
To form full constituents for use in a language comprehension system we must (1) delimit
the sequence of words that make up each name, i.e. identify its boundaries; (2) classify or cate-
gorize the resulting constituent based on the kind of individual it names; and (3) record the
name and the individual it denotes in the discourse model as our interpretation of the constit-
uent&apos;s meaning.
For other parts of Sparser&apos;s grammar, these three actions are done with one integrated
mechanism much as they would be in any other system. Constituents are initiated bottom up by
the terminal rules of Sparser&apos;s lexicalized grammar, and then compositions of adjacent constit-
uents are checked for and nonterminal nodes introduced in accordance with Sparser&apos;s moderate-
ly complex control structure that permits a deterministic parse and monotonic semantic interpre-
tation. The rules these operations deploy (essentially standard productions, though their mode
of operation is more like that of a categorial grammar) both delimit and classify (label) constit-
uents in one action: the categories are given by the production&apos;s lefthand sides, and the new
constituents&apos; boundaries by the sequence of (typically binary) daughter constituents on the
rules&apos; righthand sides, with the new constituent&apos;s denotation given by an interpretation function
included directly with the rule and applied as the rule completes.
This normal mode of operations has not proved workable for proper names, and the reason
has to do with the central problem with names from the point of view of a grammar, namely that
in unrestricted texts the set of words that names can be comprised of cannot be completely
known in advance. The set is unbounded, growing at an apparently constant rate with the size
of one&apos;s corpus, while the growth of other classes of content words tapers off asymptotically
(Liberman 1989). This means that we cannot have a lexicalized grammar for proper names since
the bulk of the names we will encounter will be based on words that are undefined at the time
the grammar is written.
Complicating the picture is the fact that virtually any normal word can do double duty as
part of a name (&amp;quot; ... Her name was equally preposterous. April Wednesday, she called herself,
</bodyText>
<page confidence="0.823493">
34
1
</page>
<bodyText confidence="0.999987105263158">
and her press card bore this out. &amp;quot;MacLean 1976 pg.68). This means that one either introduces
a massive and arbitrary ambiguity into one&apos;s normal vocabulary, allowing any word to be part
of a name, or one looks for another means of parsing proper names, which is the course that
was taken with SPARSER&apos;s PNF, where we separate out the three actions into distinct and
largely independent operations. In the remainder of this section we will sketch the procedures
for delimiting, classifying, and recording proper names; then in the following section we will
go into detail with an example once the parsing machinery that the procedures draw on has been
introduced.
Looking first at the kind of formal mechanisms used, the delimit operation is based on a
simple state machine, rather than the application of context free rewrite rules as done in the rest
of the grammar. This reflects that fact that the internal constituent structure of a proper name is
far more flat than hierarchical, and consequently should be treated as a Kleene Star structure in a
regular grammar (&lt;name-word&gt;+). The binary branching tree that one would get with context-
free rules would be an artifact of the rule application machinery rather than reflect the grammar
of names.
In essence, the delimitation algorithm simply treats as a group any contiguous sequence of
capitalized words (including `sequences&apos; of length one). This is virtually always the correct
thing to do as the example below illustrates, though the exceptions have to be treated carefully
as discussed later.
</bodyText>
<subsubsectionHeader confidence="0.535628">
&amp;quot;The Del Fuegos, 0 Positive, and We Saw the Wolf will peiform acoustic sets in
Amnesty International USA Group 133&apos;s Seventh Annual Benefit Concert at 8
</subsubsectionHeader>
<bodyText confidence="0.9138465">
p.m. on Friday, March 19, at the First Parish Unitarian Universalist Church in
Arlington Center.&amp;quot; (Arlington Advocate, 3/18/93)
A sequence is terminated at the first non-capitalized wordl or comma; other punctuation is
handled case by case, e.g. &amp;quot;&amp;&amp;quot; is taken to extend sequences and periods are terminators unless
they are part of an abbreviation.
Classifying a proper name is a two-step process. First, Sparser&apos;s regular parsing routines
are applied within the delimited word sequence. This introduces any information the grammar
has about words or phrases it knows. This information supplies the basis for the bulk of the
structure within a proper name, and provides the name-internal evidence on which the classifi-
cation will be based. For Sparser it includes:
</bodyText>
<listItem confidence="0.9646294">
• embedded references to cities or countries, e.g. &amp;quot;Cambridge Savings Bank&amp;quot;.
• open class &apos;keywords&apos; like &amp;quot;Church&amp;quot; or &amp;quot;Bank&amp;quot; (following Coates-Stephens term-
inology), and the incorporation-terms used by companies of various countries when
giving their full legal names (&amp;quot;Inc.&amp;quot; in the U.S.A., &amp;quot;P.T.&amp;quot; in Indonesia,
&amp;quot;G.m.b.H.&amp;quot; in Germany, etc.).
</listItem>
<bodyText confidence="0.999878571428572">
It is reasonable to depend upon the existence of mixed-case text, since the number of online sources that
supply uppercase only is rapidly diminishing and will probably disappear once all of the Model-33 Teletypes
and other 6-bit data entry terminals in the world are fmally junked. In any event, to handle all-uppercase texts
within Sparser&apos;s design it is only the delimitation algorithm that must be changed, and a good approximation
of the needed segmentation is independently available from the distribution of function words and punctuation
in any text. In the example above these are the commas, the appostrophe-s, &amp;quot;in&amp;quot;, &amp;quot;at&amp;quot;, and &amp;quot;on&amp;quot;; a mistake
would be made in &amp;quot;We Saw the Wolf&apos; [sic] which will be problematic without external context in any event.
</bodyText>
<page confidence="0.988154">
35
</page>
<listItem confidence="0.976287">
• the relatively closed class of stylized modifiers used with people like &amp;quot;Jr.&amp;quot;, &amp;quot;Sr.&amp;quot;,
&amp;quot;Mr.&amp;quot;, &amp;quot;Dr.&amp;quot;.
• items used for heuristic classification judgements (the items above are definitive)
</listItem>
<bodyText confidence="0.968195190476191">
such as abbreviations (a strong indicator that the name refers to a person or a com-
pany based on a person&apos;s name), or punctuation like &amp;quot;&amp;&amp;quot; or ambiguous modifiers
like &amp;quot;II&amp;quot; (which invariably means &apos;the second&apos;, but may be used with Limited
Partnerships as well as people).
The parsing stage will reveal when the capitalized word -based delimitation has exceeded its
proper scope. One such case is of course when a proper name appears just after the capitalized
word at the start of a sentence: &amp;quot;An Abitibi spokesman said ...&amp;quot;. This is handled by defining all
the closed-class grammatical functional words as such so that they will be seen during this
embedded parse, and resegmenting the word sequence to exclude them.
Another, more interesting case is where we have a sequence of modifiers prefixed to a
proper name that are themselves proper names, e.g. &amp;quot;Giant Group said ... seeking to block a
group led by Giant Chairman Burt Sugarman from acquiring ...&amp;quot;. In this situation there is no
hope for correctly separating the names unless the grammar includes rules for such companies
and titles, in which case they will appear to the classifier as successive edges with the appro-
priate labels so that it can blow to appreciate them for what they are and to leave them out. (It is
perhaps a matter of judgement to hold that a person&apos;s title is not a part of their name, but that
policy appears to be the most consistent overall since it permits the capitalized premodifier ver-
sion of a title of employment (e.g. &amp;quot;Chairman&amp;quot;) and its predicative lowercase version (as in an
appositive) to be understood as the same kind of relationship semantically—a different one than
the relationship between a person and their conventional title such as &amp;quot;Mr.&amp;quot;.) It is important to
appreciate that all of these considerations only make sense when one is analyzing proper names
in the context of a larger system that already has grammars and semantic models for titles and
employment status and such; and they are hard to justify in an application that is simply name
spotting.
In practice, the operations of delimiting and classifying are often interleaved, since the
classification of an initially delimited segment can aid in the determination of whether the
segment needs to be extended, as when distinguishing between a list of names and a compound
name incorporating commas, e.g. &amp;quot;... a string of companies – including Bosch, Continental and
Varta – have announced co-operative agreements ...&amp;quot; (The Financial Times, 5/16/90); v.s.
&amp;quot;HEALTH-CARE FIRM FOLDS: Wood, Lucksinger &amp; Epstein is dissolving its practice.&amp;quot; (Wall
Street Journal 2/26/91). We will describe this process in the extended example at the end of the
paper.
Once the words of the sequence have been parsed and edges introduced into the chart
reflecting the grammar&apos;s analysis, the second part of the classification process is initiated as a
state machine is passed over that region of the chart to arrive at the most certain classification
possible given just this name-internal evidence. If no specific conclusion can be reached, then
the sequence will be covered with an edge that is simply given the category &apos;name&apos;, and it will
be up to external evidence to improve on that judgement as will be described later. If a conclu-
sion is made as to the kind of entity being named then the edge will be labeled with the appro-
priate semantic category such as &apos;person&apos;, &apos;company&apos;, &apos;newspaper&apos;, etc.
The recording process now takes over to provide a denotation for the edge in the discourse
model. Before this denotation is established, the representation of the name is just a label and a
</bodyText>
<page confidence="0.991641">
36
</page>
<bodyText confidence="0.999366189189189">
designated sequence of words and edges internal to the name (e.g., edges over an embedded
reference to a city or region). What we are providing now is a structured representation of the
name qua name—a unique instance of one of the defined classes of names that reifies that
specific pattern of words and embedded references.
Including names as actual entities in the semantic model, rather than just treating them as
ephemeral pointers to the individuals they name and only using them momentarily during the
interpretation process, provides us with an elegant treatment of the ambiguity that is intrinsic to
names as representational entities. Real names, unlike the hypothetical &apos;rigid designators&apos; enter-
tained by philosophers, may refer to any number of individuals according to the contingent facts
of the actual world. We capture this by making the denotation of the lexico-syntactic name—the
edge in the chart—be a semantic individual of type &apos;name&apos; rather than (the representation of) a
concrete individual. The name object in turn may be then associated in the discourse model with
any number of particular individuals of various types: people, companies, places, etc. according
to the facts in the world. Thus the ambiguity of names is taken not to be a linguistic fact but a
pragmatic fact involving different individuals having the same name.
The structure that the semantic model imposes on names is designed to facilitate under-
standing subsequent references to the individuals that the names name. The type of name struc-
ture used predicts the kinds of reduced forms of the name that one can expect to be used. This
design criteria was adopted because, again, the overarching purpose of PTF is to contribute to
the thorough understanding of extended unrestricted texts, and this means that it is not enough
just to notice that a given name has occurred somewhere in an article (which is easy to do by
just attending to the cases where the full company name is given with the &apos;incorporation term&apos;
that well edited newspapers will always provide when a company is introduced into a text, e.g.
&amp;quot;Sumitomo Electric Industries, Lg.&amp;quot;). Instead, PTF must be able to recognize that the same
individual is being talked about later when it sees, e.g., &amp;quot;Sumitomo Electric&amp;quot; (or &amp;quot;the
company&amp;quot;), as well as to distinguish it from subsequent references to other companies that
share part of the name: &amp;quot;Sumitomo Wiring Systems&amp;quot;; or to correctly deduce a subsidiary rela-
tionship &amp;quot;Sumitomo Electric International (Singapore)&amp;quot;. Similarly, people and companies or
locations that share name elements should be appreciated as such: &amp;quot;the Suzuki Motors Company
... Osamu Suzuki, the president of the company&amp;quot;.
To facilitate subsequent reference, not only does each proper name receive a denotation as
an entirety, but the words that comprise it are also given denotations which are related, seman-
tically, to the roles the words have each played in that name and in the names of other particular
individuals. Thus the word &amp;quot;Suzuki&amp;quot;, for example, is taken to always denote the same semantic
object, prosaically printed as #&lt;name-word &amp;quot;suzuld&amp;quot;&gt;. In turn this individual is related to (at
least) two other individuals—to the car company by way of the relation &apos;first-word-in-name&apos;,
and to its president by the relation &apos;family-name&apos;.
</bodyText>
<page confidence="0.999225">
37
</page>
<sectionHeader confidence="0.551854" genericHeader="method">
3 The setting for the process
</sectionHeader>
<bodyText confidence="0.74115504">
In order to supply the external evidence needed to accurately categorize proper names and
understand them semantically, a language understanding system must include grammars (and
their attendant semantic models) for properties and event-types that are characteristically associ-
ated with them, and they should have as broad a coverage as possible.
SPARSER has been applied to understanding news articles about people changing their jobs
(particularly the Wall Street Journal&apos;s &amp;quot;Who&apos;s News&amp;quot; column), and with a lesser competence to
articles on corporate joint ventures and quarterly earnings. As a result, it has quite strong
semantic grammars for some of the very most frequent properties of companies and people in
business news texts: the parent—subsidiary relationship between companies, age, titles, and for
a few of the more common event-types (via its primary grammars).
A complementary consideration is such relatively mundane things as what approach will be
taken to punctuation, capitalization, or abbreviations. For SPARSER, since it is designed to
work with well-edited news text written by professional journalists, punctuation is retained and
there are grammar rules that appreciate the (sometimes heuristic) information that it can provide.
The whitespace between words is also noted (newlines, tabs, different numbers of spaces) since
it provides relatively reliable evidence for paragraphs, tables, header fields, etc., which in turn
can provide useful external evidence.
Additionally, SPARSER is designed to handle a constant, unrestricted stream of text, day
after day, and this has led to a way to treat unknown words that allows it to look at their proper-
ties, and from that possibly form them into proper names, without being required to give them a
long-term representation which would eventually cause the program to run out of memory.
To illustrate how these aspects work, and at the same time establish the implementation
setting in which proper name processing takes place, we will now describe the lower levels of
SPARSER&apos;s operation, starting with its tokenizer and populating the terminal positions of the
chart.
</bodyText>
<subsectionHeader confidence="0.997509">
3.1 Tokenizing
</subsectionHeader>
<bodyText confidence="0.997230428571429">
The tokenizer transduces characters to objects representing words, punctuation, digit sequences,
or numbers of spaces. It is conservatively designed, just grouping contiguous sequences of
alphabetic characters or digits and punctuation and passing them them all through to be the
terminals of the chart, where even the simplest compounds are assembled by sets of rules that
are easily changed and experimented with. For example, rather than conclude inside the
tokenizer that the character sequence &amp;quot;$47.2 million&amp;quot; is an instance of money, it just passes
through six tokens, including the space.
A word is &apos;known&apos; if it is mentioned in any of the rules of the grammar.2 A known word
has a permanent representation, and the tokenizer finds and returns this object when it delimits
the designated sequence of characters. The &apos;token-hood&apos; of this word type is represented by the
word object filling particular places in the chart.
2 Note that since this is a lexicalized semantic grammar, words have preterminal categories like &apos;title&apos; or &apos;head-
of-CO-phrase&apos; (e.g. &amp;quot;company&amp;quot;, &apos;firm&amp;quot;, &amp;quot;enterprise&amp;quot;) or are often treated just as literals, e.g. all the
prepositions or words like &amp;quot;based&amp;quot;.
</bodyText>
<page confidence="0.998162">
38
</page>
<bodyText confidence="0.997805571428571">
The tokenizer separates the identity of a word from its capitalization. A word is defined by
the identity of its characters. The pattern of upper and lowercase letters that happens to occur in
a given instance is a separate matter, and is represented in the chart rather than with the word.3
Thus when the chart is populated with terminals, each position records the word that starts
there, its capitalization, and the kind of whitespace that preceded it, all given as separate fields
of the position object. The scan is done incrementally in step with the rest of the SPARSER&apos;s
actions.
</bodyText>
<subsectionHeader confidence="0.997698">
3.2 Word-triggered operations
</subsectionHeader>
<bodyText confidence="0.99866015625">
Sparser&apos;s processing is organized into layers. Tokenizing and populating the terminals of the
chart is the first level, then comes a set of special operations that are triggered by words or their
properties (e.g. ending in &amp;quot;ed&amp;quot; or consisting solely of digits). The application of phrase
structure rules is the next layer, and finally there is the application of heuristics to try spanning
gaps caused by unknown words. Semantic interpretation is done continuously as part of what
happens when a rule completes and an edge is formed. We will not describe the last two layers
(see McDonald 1992 for a description of the phrase structure algorithm), but will briefly
describe the word-level operations since it includes triggering PNF.
Operations triggered by the identify of a word include forming initials and known abbre-
viations, and particularly the recognition of multi-word fixed phrases which we call
&amp;quot;polywords&amp;quot; following Becker (1975). Polywords are immutable sequences of words that are
not subject to syntactically imposed morphological changes (plurals, tense) and that can only be
defined as a group. Polywords are a natural way of predefining entities that have fixed, multi-
word names such as the countries of the world, the states of the US, major cities, etc. Instances
of this relatively closed class of individuals are a valuable kind of evidence in the classification
of proper names.
When PNF finishes the recognition and classification of a new name, it adds to the gram-
mar a polyword rule for the sequence of words in the name, with the recorded name-object as
the polyword&apos;s denotation, so that the process can be short-circuited the next time the name is
seen. Note that this does not stop PNF from triggering and running its delimiting operation the
next time that sequence is seen; it only speeds up the classification and recording. If we allowed
the polyword operation to take precedence, we would never see the longer word sequences that
embed known names (&amp;quot;New York Port Authority&amp;quot;), or we would have to resort to a more
complex algorithm.
We have a particularly fast algorithm for checking for polywords when the first word of the
sequence has been seen. There are also special rules that allow paired punctuation to be grouped
(parentheses, quotations, etc.) even if the words separating them are not all known. This is
particularly useful for picking up nicknames embedded within a person&apos;s name since it will
often be an unknown word in quotations inside parentheses (&amp;quot;Richard M. (&amp;quot;tricky Dick&amp;quot;)
Nixon&amp;quot;). Subsidiaries of companies are often marked for their geographical area in the same
way, e.g. &amp;quot;manufactured by UNIVERSAL FLUID HEADS (Aust.) PTY. LTD.&amp;quot; (from the name
plate on a camera tripod).
</bodyText>
<tableCaption confidence="0.494527333333333">
3 One can deliberately define a capitalization-sensitive version of a word, e.g. to syntactically distinguish titles
in pre-head position from those in appositives or elsewhere. In such cases there is a distinct word object with
a link to the case-neutral version of the word.
</tableCaption>
<page confidence="0.998986">
39
</page>
<bodyText confidence="0.999904142857143">
The first check at the word-level is for actions triggered by a word&apos;s properties, particularly
here the properties of its characters. This is how compound numbers are formed (42,357.25)
triggering off words that are sequences of digits, and it is how PNF is triggered. Every time a
chart position is reached that indicates that the following word is capitalized, PNF is called.
PNF then takes over the process of scanning the successive terminals of the chart, until it scans
a word that is not capitalized, deliberately calling other SPARSER mechanisms like polyword
recognition or phrase structure rewrite rules as needed.
When PNF is finished, its results are given in an edge it constructs over the sequence of
capitalized words and selected punctuation, with the label on the edge dictating how it will fit
into SPARSER&apos;s later processing layers and the referent field of the edge pointing to the name
object that records it in the discourse history. Since Sparser uses a semantic grammar, the label
is the constituent&apos;s classification—a semantic category like &apos;person&apos;. There is also conventional
label (always NP for a name) included with the edge for default or heuristic rules of phrasal
formation; see McDonald (submitted) for the details of this two label system.
</bodyText>
<sectionHeader confidence="0.72054" genericHeader="method">
4 Walking through an example
</sectionHeader>
<bodyText confidence="0.999102">
In this final section of this paper we will look at the processing of the following paragraph-
initial noun phrase from the Wall Street Journal of 10/27/89, article #34:
</bodyText>
<subsectionHeader confidence="0.6953245">
&amp;quot;An industry analyst, Robert B. Morris III in Goldman, Sachs &amp; Co.&apos;s
San Francisco office, said...&amp;quot;
</subsectionHeader>
<bodyText confidence="0.996352571428571">
The capitalization of the very first word &amp;quot;An&amp;quot; triggers PNF, whose delimitation process
stops immediately with the next word since it is lowercase. The classification pass through the
(one word) sequence then shows it to be a grammatical function word, and classification applies
the rule &apos;single word sequences consisting solely of a non-preposition function word are not to
be treated as names&apos;. PNF is then finished; the article reading of &amp;quot;An&amp;quot; will have been introduced
into the chart during classification; and the scan moves on.4
As the parse moves forward, the title phrase &amp;quot;an industrial analyst&amp;quot; is recognized and the
comma after it is marked as possibly indicating an appositive (or also a list of titles, though this
is less likely).
PNF is triggered again by the capitalization of &amp;quot;Robert&amp;quot;, and the delimitation process takes
it up to the word &amp;quot;in&amp;quot;. Running the regular rules of the grammar within that sequence uncovers
the abbreviation and the generation-indicator &amp;quot;III&amp;quot; for &apos;the third&apos;. We do not maintain any lists
of the common first names of people or such, so consequently both &amp;quot;Robert&amp;quot; and &amp;quot;Morris&amp;quot; are
4 Fortunately we have yet to see a company whose name was &amp;quot;The&amp;quot;—one wonders how journalists would deal
with it. Of course there are companies like Next Inc. and On Technology, which, like the names of race
horses or boats, add spice to the grammarian&apos;s life by overloading the interpretations of closed-class words.
The only consistent treatment we have arrived at for these (&amp;quot;On&amp;quot; referring to the company does occur in
sentence-initial position) is to treat the words as ambiguous and to introduce two edges into the chart, one for
each reading. We only do this if the full name of the company appeared earlier in the article, however, when
the preposition will have received its denotation as an element of a name and the basis of the ambiguity been
established.
</bodyText>
<page confidence="0.994443">
40
</page>
<bodyText confidence="0.999966813953489">
seen as unknown words. The abbreviation and generation-indicator are enough, however, to
allow the sequence to be reliably classified as the name of a person.
Given that classification, an edge is constructed over the sequence and given the label
&apos;person&apos;, and the recording process constructs a name object for the edge&apos;s denotation. The
pattern given by the classifier is &apos;name – initial – name – generation-indicator&apos;, which is clear
enough for the name subtype &apos;person&apos;s name with generation&apos; to be instantiated. This object
takes a sequence of first names or initials, a last name (the word at the end before &amp;quot;III&amp;quot;), and
then the &amp;quot;III&amp;quot; in a slot that also gets words like &amp;quot;Junior&amp;quot;. Let us call this new name-individual
Name-i.
Part of the recording is the creation of denotations for the words &amp;quot;Robert&amp;quot; and &amp;quot;Morris&amp;quot;.
Individuals are created for them of type &apos;single word element of a name&apos;, and rules are added to
the grammar so that the next time we see them in the grammar we will be taken directly to those
same individuals. By warrant of forming the interpretation of the whole sequence as Name-1,
we can now attribute properties to the names (semantic objects) &apos;Robert&apos; and &apos;Morris&apos;—
&apos;Robert&apos; is the first name of Name-1 and &apos;Morris&apos; is the last name. The policy of letting words
like &amp;quot;Morris&amp;quot; denote name objects with semantic links to the name they are part of (with that
name in turn linked to the person whose name it is) provides a very direct way to understand
subsequent references involving just part of the original name (e.g. &amp;quot;Mr. Morris&amp;quot;) as we can
trace it directly to the person just by following those links. (Of course the links will also take us
to anyone else the world model knows of who has that same last name, hence the need for a
good discourse model that appreciates the context set up by the article being processed.)
Moving on, the next point where PNF comes into play is at the word &amp;quot;Goldman&amp;quot;. It is seen
as a one word sequence because of the comma just after it, and is an unknown word. Not being
a function word, it is spanned with an edge labeled just &apos;name&apos; and recorded with just a new
single-word name individual as its denotation. Given the significance of commas for name
patterns, we also at this point make a note (set a variable) that this comma is preceded by a
name.
PNF immediately resumes with &amp;quot;Sachs &amp; Co.&amp;quot;, stopping the delimitation process when it
recognizes the &amp;quot; &amp;quot; and &amp;quot;s&amp;quot; tokens as constituting an appostrophe-s, which is a definitive marker
for the end of a noun phrase. During the delimitation process, the abbreviation &amp;quot;Co.&amp;quot; will also
have been recognized and expanded, and the &amp;quot;&amp;&amp;quot; noted and appreciated as being a punctuation
mark that can appear in names. Punctuation is always handled during the course of delimitation
for just these reasons.
The presence of the &amp;quot;&amp;&amp;quot; and the word &amp;quot;Company&amp;quot; are definitive markers of companies and
the classification process will start the assembly of a pattern to send off to be recorded. In this
case however, as noted earlier, there is what amounts to an interaction between classification
and delimitation. Part of what the classifier knows about companies is the profusion of cases
where the name of the company is a sequence of words separated by commas (law firms,
advertising agencies, any sort of partnership tends to use this name pattern). Appreciating this,
the process looks for the contextual note about the preceding comma. Finding it, it observes that
the name in front of the comma is not itself classified as a company (which would have
indicated a list of companies rather than a single name), and it proceeds to assimilate the edge
over &amp;quot;Goldman&amp;quot; and the comma into the name it is already assembling. Had there been still
</bodyText>
<footnote confidence="0.674736">
5 There is no interesting limit on the number of &apos;first names&apos; a person can have, so we have not yet found it
profitable to have any more structure in that field than simply an ordered sequence; consider &amp;quot;M.A.K.
Halliday&amp;quot; ,&amp;quot;(Prince) Charles Philip Arthur George&amp;quot;.
</footnote>
<page confidence="0.999086">
41
</page>
<bodyText confidence="0.971788512820513">
more &apos;stranded&apos; elements of the name followed by commas, this would have been noted as well
and those elements added in.
Occasionally the name of a company like this is given with &amp;quot;and&amp;quot; instead of the special
punctuation character. Had that happened here, the fact that the &amp;quot;and&amp;quot; preceded the word
&amp;quot;company&amp;quot; would have been sufficient evidence to take the whole sequence as the name of a
single company, however if there had been no such internal evidence within any of the elements
of the conjunction, they would have been grouped together as unconnected names spanned by a
single edge labeled &apos;name&apos;, leaving it to external evidence from the context to supply a stronger
categorization (both as category and whether they were one name or several), as we can see
with the next capitalized word sequence that PNF delimits, &amp;quot;San Francisco&amp;quot;.
With access to a good gazetteer we could have already defined San Francisco as the name
of a city using a polyword. Alternatively, without needing any word list we can conclude that it
is a location, and probably a city, just by looking at its context: the word &amp;quot;office&amp;quot;.
As said earlier, the availability of mechanisms that use external evidence like this allows
PNF to make a weak analysis that can be strengthened later. In this case it will see &amp;quot;San
Francisco&amp;quot; as a sequence of two unknown words. Without any internal evidence to base its
judgement on, it can only (1) accept the sequence as a phrase and span it with an edge, indicat-
ing that the words have more relationship to each other than either individually has to its neigh-
bors, and (2) give this edge the neutral label &apos;name&apos;.
After PNF is done, the phrase structure component of Sparser takes over. Sparser&apos;s rewrite
rule facility includes context-sensitive as well as context-free productions, including for this
case the rule
name -&gt; location / &apos;office&amp;quot;
That is, an edge labeled &apos;name&apos; can be respanned with a new edge with the label &apos;location&apos; when
the name edge appears just in front of the word &amp;quot;office&amp;quot;. Context sensitive rules are handled
with the same machinery as context free rules, e.g. the trigger pattern here is the same as that of
a CF rule with the righthand side &apos;name&apos; + &amp;quot;office&amp;quot;. The difference is simply that instead of
covering the whole righthand side with a new edge we just respan the one indicated constituent.
Similarly, if the person in this example had the name &amp;quot;Robert Morris&amp;quot; (rather than &amp;quot;Robert
B. Morris HI&amp;quot;), where there would have been no available internal evidence to indicate its
classification during the operations of PNF, we would later have applied either of two context
free rules: one working forwards from the definitively recognized title, the other backwards
from the pp &apos;in-company&apos;.
name -&gt; person / title &amp;quot;,&amp;quot;
name -&gt; person / in-company
A repertoire of such context-sensitive rules or their equivalent is needed if a proper name
classification facility is expected to work well with the open-ended set of name words found in
actual texts; Sparser used a set of roughly 30 rules to handle the names in the blind test on the
Who&apos;s News column mentioned earlier.
</bodyText>
<page confidence="0.998818">
42
</page>
<sectionHeader confidence="0.997225" genericHeader="method">
5 References
</sectionHeader>
<reference confidence="0.99743385">
Alshawi, Hiyan (ed.) (1992) The Core Language Engine, MIT Press.
Becker, Joe (1975) &amp;quot;The Phrasal Lexicon&amp;quot;, in Schank &amp; Webber (eds.) Proc. TINLAP-1,
ACM, 60-63.
Coates-Stephens, Sam (1992) &amp;quot;The Analysis and Acquisition of Proper Names for the
Understanding of Free Text&amp;quot;, Computers in the Humanities.
(DARPA) Defence Advanced Research Projects Agency (1992) Proceedings of the Fourth
Message Understanding Conference: MUC-4 , June 1992, Morgan Kaufmann.
Liberman, Mark (1989) Panel presentation at the 27th Annual Meeting of the ACL, Vancouver.
MacLean, Alistair (1976) The Golden Gate, Fawcett Publications, Greenwich Connecticut.
Masand, Brij M. &amp; Roger D. Duffey (1985) &amp;quot;A Rapid Prototype of an Information Extractor
and its Application to Database Table Generation&amp;quot;, working paper, Brattle Research
Corporation, Cambridge, MA.
McDonald, David D. (1992) &amp;quot;An Efficient Chart-based Algorithm for Partial-Parsing of
Unrestricted Texts&amp;quot;, proceedings of the 3d Conference on Applied Natural Language
Processing (ACL), Trento, Italy, April 1992, pp. 193-200.
(submitted) &amp;quot;The Interplay of Syntactic and Semantic Node Labels in Partial Parsing&amp;quot;,
manuscript submitted to the International Workshop on Parsing Technologies, August
1993.
Rau, Lisa F. (1991) &amp;quot;Extracting Company Names from Text&amp;quot;, proc. Seventh Conference on
Artificial Intelligence Applications, February 24-28, 1992, IEEE, 189-194.
</reference>
<page confidence="0.999833">
43
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.822606">
<title confidence="0.998996">Internal and External Evidence in the Identification Semantic Categorization of Proper Names</title>
<author confidence="0.999995">David D McDonald</author>
<email confidence="0.99947">mcdonald@cs.brandeis.edu</email>
<abstract confidence="0.982103909090909">We describe the proper name recognition and classification facility (&amp;quot;PNF&amp;quot;) of the SPARSER natural language understanding system. PNF has been used very successfully in the analysis of unrestricted texts in several sublanguages taken from online news sources. It makes its categorizations on the basis of &apos;external&apos; evidence from the context of the phrases adjacent to the name as well as &apos;internal&apos; evidence within the sequence of words and characters. A semantic model of each name and its components is maintained and used for subsequent reference. We describe PNF&apos;s operations of delimiting, classifying, and semantically recording the structure of a name; we situate PNF with respect to the related parsing mechanisms within Sparser; and finally we work through an extended example that is typical of the sorts of text we have applied PNF to.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>The Core Language Engine,</title>
<date>1992</date>
<editor>Alshawi, Hiyan (ed.)</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="7020" citStr="(1992)" startWordPosition="1125" endWordPosition="1125">ier implementation of PNF in the context of &amp;quot;Who&apos;s News&amp;quot; articles from the the Wall Street Journal, it performed at nearing 100% in the scored sentences in the 33 sublanguage for which a full grammar had been prepared. We are currently testing a new implementation on a more diverse set of texts. Space will not permit a comparison of this algorithm with other approaches to proper names beyond occasional remarks and references. As far as we know this is the only treatment of proper names that makes essential use of context-sensitive rewrite rules, however the FUNES system of Sam Coates-Stephens (1992) is very similar to this work in making essential use of external evidence, and Coates-Stephens&apos;s extensive research into proper names is an important contribution to the field; we have adopted some of his terminology as noted below. 2 An overview of the procedure: Delimit, Classify, Record The goal of the proper name facility in Sparser (PNF) is to form and interpret full phrasal constituents—noun phrases—that fit into the rest of a text&apos;s parse and contribute to the analysis of the entire text just like any other kind of constituent. That is, PNF is operating as a component in a larger natur</context>
</contexts>
<marker>1992</marker>
<rawString>Alshawi, Hiyan (ed.) (1992) The Core Language Engine, MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joe Becker</author>
</authors>
<title>The Phrasal Lexicon&amp;quot;,</title>
<date>1975</date>
<booktitle>Proc. TINLAP-1, ACM,</booktitle>
<pages>60--63</pages>
<editor>in Schank &amp; Webber (eds.)</editor>
<contexts>
<context position="25881" citStr="Becker (1975)" startWordPosition="4164" endWordPosition="4165">inally there is the application of heuristics to try spanning gaps caused by unknown words. Semantic interpretation is done continuously as part of what happens when a rule completes and an edge is formed. We will not describe the last two layers (see McDonald 1992 for a description of the phrase structure algorithm), but will briefly describe the word-level operations since it includes triggering PNF. Operations triggered by the identify of a word include forming initials and known abbreviations, and particularly the recognition of multi-word fixed phrases which we call &amp;quot;polywords&amp;quot; following Becker (1975). Polywords are immutable sequences of words that are not subject to syntactically imposed morphological changes (plurals, tense) and that can only be defined as a group. Polywords are a natural way of predefining entities that have fixed, multiword names such as the countries of the world, the states of the US, major cities, etc. Instances of this relatively closed class of individuals are a valuable kind of evidence in the classification of proper names. When PNF finishes the recognition and classification of a new name, it adds to the grammar a polyword rule for the sequence of words in the</context>
</contexts>
<marker>Becker, 1975</marker>
<rawString>Becker, Joe (1975) &amp;quot;The Phrasal Lexicon&amp;quot;, in Schank &amp; Webber (eds.) Proc. TINLAP-1, ACM, 60-63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sam Coates-Stephens</author>
</authors>
<title>The Analysis and Acquisition of Proper Names for the Understanding of Free Text&amp;quot;, Computers in the Humanities.</title>
<date>1992</date>
<contexts>
<context position="7020" citStr="Coates-Stephens (1992)" startWordPosition="1124" endWordPosition="1125"> test of an earlier implementation of PNF in the context of &amp;quot;Who&apos;s News&amp;quot; articles from the the Wall Street Journal, it performed at nearing 100% in the scored sentences in the 33 sublanguage for which a full grammar had been prepared. We are currently testing a new implementation on a more diverse set of texts. Space will not permit a comparison of this algorithm with other approaches to proper names beyond occasional remarks and references. As far as we know this is the only treatment of proper names that makes essential use of context-sensitive rewrite rules, however the FUNES system of Sam Coates-Stephens (1992) is very similar to this work in making essential use of external evidence, and Coates-Stephens&apos;s extensive research into proper names is an important contribution to the field; we have adopted some of his terminology as noted below. 2 An overview of the procedure: Delimit, Classify, Record The goal of the proper name facility in Sparser (PNF) is to form and interpret full phrasal constituents—noun phrases—that fit into the rest of a text&apos;s parse and contribute to the analysis of the entire text just like any other kind of constituent. That is, PNF is operating as a component in a larger natur</context>
</contexts>
<marker>Coates-Stephens, 1992</marker>
<rawString>Coates-Stephens, Sam (1992) &amp;quot;The Analysis and Acquisition of Proper Names for the Understanding of Free Text&amp;quot;, Computers in the Humanities.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Defence</author>
</authors>
<title>Advanced Research Projects Agency</title>
<date>1992</date>
<booktitle>Proceedings of the Fourth Message Understanding Conference: MUC-4 ,</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<marker>Defence, 1992</marker>
<rawString>(DARPA) Defence Advanced Research Projects Agency (1992) Proceedings of the Fourth Message Understanding Conference: MUC-4 , June 1992, Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Liberman</author>
</authors>
<title>Panel presentation at the 27th Annual Meeting of the ACL,</title>
<date>1989</date>
<location>Vancouver. MacLean, Alistair</location>
<contexts>
<context position="9850" citStr="Liberman 1989" startWordPosition="1582" endWordPosition="1583">h the new constituent&apos;s denotation given by an interpretation function included directly with the rule and applied as the rule completes. This normal mode of operations has not proved workable for proper names, and the reason has to do with the central problem with names from the point of view of a grammar, namely that in unrestricted texts the set of words that names can be comprised of cannot be completely known in advance. The set is unbounded, growing at an apparently constant rate with the size of one&apos;s corpus, while the growth of other classes of content words tapers off asymptotically (Liberman 1989). This means that we cannot have a lexicalized grammar for proper names since the bulk of the names we will encounter will be based on words that are undefined at the time the grammar is written. Complicating the picture is the fact that virtually any normal word can do double duty as part of a name (&amp;quot; ... Her name was equally preposterous. April Wednesday, she called herself, 34 1 and her press card bore this out. &amp;quot;MacLean 1976 pg.68). This means that one either introduces a massive and arbitrary ambiguity into one&apos;s normal vocabulary, allowing any word to be part of a name, or one looks for </context>
</contexts>
<marker>Liberman, 1989</marker>
<rawString>Liberman, Mark (1989) Panel presentation at the 27th Annual Meeting of the ACL, Vancouver. MacLean, Alistair (1976) The Golden Gate, Fawcett Publications, Greenwich Connecticut.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brij M Masand</author>
<author>Roger D Duffey</author>
</authors>
<title>A Rapid Prototype of an Information Extractor and its Application to Database Table Generation&amp;quot;, working paper, Brattle Research Corporation,</title>
<date>1985</date>
<location>Cambridge, MA.</location>
<contexts>
<context position="4201" citStr="Masand &amp; Duffey 1985" startWordPosition="649" endWordPosition="652"> way a phrase is used—the attributions and predications it is part of—that make it a proper name of a given sort; without the consideration of external evidence this definitive criteria is missed, resulting in mistakes and confusion in the state of the parser. (Relying solely on name lists has led to some funny errors, for example mistaking the food company Sara Lee for a person. Even some external evidence such as a title can be inadequate, if considered apart from the wider context of use, as in General Mills—both actual mistakes made by an otherwise quite reasonable program some years ago (Masand &amp; Duffey 1985).) An additional reason, and one with considerable engineering utility from the point of view of the grammar writer, is that the inclusion of external evidence into the mix of available analysis tools reduces the demands on the judgements one requires of internal evidence. It can provide a weaker (less specific) categorization about which it can be more certain, which can then be refined as external evidence becomes available. Lacking definitive internal evidence one can initially label a segment simply as a &apos;name&apos;, and then later strengthen the judgement when, e.g., the segment is found to be</context>
</contexts>
<marker>Masand, Duffey, 1985</marker>
<rawString>Masand, Brij M. &amp; Roger D. Duffey (1985) &amp;quot;A Rapid Prototype of an Information Extractor and its Application to Database Table Generation&amp;quot;, working paper, Brattle Research Corporation, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D McDonald</author>
</authors>
<title>An Efficient Chart-based Algorithm for Partial-Parsing of Unrestricted Texts&amp;quot;,</title>
<date>1992</date>
<booktitle>proceedings of the 3d Conference on Applied Natural Language Processing (ACL),</booktitle>
<pages>193--200</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="25533" citStr="McDonald 1992" startWordPosition="4114" endWordPosition="4115">triggered operations Sparser&apos;s processing is organized into layers. Tokenizing and populating the terminals of the chart is the first level, then comes a set of special operations that are triggered by words or their properties (e.g. ending in &amp;quot;ed&amp;quot; or consisting solely of digits). The application of phrase structure rules is the next layer, and finally there is the application of heuristics to try spanning gaps caused by unknown words. Semantic interpretation is done continuously as part of what happens when a rule completes and an edge is formed. We will not describe the last two layers (see McDonald 1992 for a description of the phrase structure algorithm), but will briefly describe the word-level operations since it includes triggering PNF. Operations triggered by the identify of a word include forming initials and known abbreviations, and particularly the recognition of multi-word fixed phrases which we call &amp;quot;polywords&amp;quot; following Becker (1975). Polywords are immutable sequences of words that are not subject to syntactically imposed morphological changes (plurals, tense) and that can only be defined as a group. Polywords are a natural way of predefining entities that have fixed, multiword na</context>
</contexts>
<marker>McDonald, 1992</marker>
<rawString>McDonald, David D. (1992) &amp;quot;An Efficient Chart-based Algorithm for Partial-Parsing of Unrestricted Texts&amp;quot;, proceedings of the 3d Conference on Applied Natural Language Processing (ACL), Trento, Italy, April 1992, pp. 193-200.</rawString>
</citation>
<citation valid="true">
<title>(submitted) &amp;quot;The Interplay of Syntactic and Semantic Node Labels in Partial Parsing&amp;quot;, manuscript submitted to the International Workshop on Parsing Technologies,</title>
<date>1993</date>
<marker>1993</marker>
<rawString>(submitted) &amp;quot;The Interplay of Syntactic and Semantic Node Labels in Partial Parsing&amp;quot;, manuscript submitted to the International Workshop on Parsing Technologies, August 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lisa F Rau</author>
</authors>
<title>Extracting Company Names from Text&amp;quot;,</title>
<date>1991</date>
<booktitle>proc. Seventh Conference on Artificial Intelligence Applications,</booktitle>
<pages>189--194</pages>
<publisher>IEEE,</publisher>
<contexts>
<context position="2447" citStr="Rau 1991" startWordPosition="373" endWordPosition="374">per name involves two complementary kinds of evidence, which we will term &apos;internal&apos; and &apos;external&apos;. Internal evidence is derived from within the sequence of words that comprise the name. This can be definitive criteria, such as the presence of known &apos;incorporation terms&apos; (&amp;quot;Ltd.&amp;quot;, &amp;quot;G.m.b.H.&amp;quot;) that indicate companies; or heuristic criteria such as abbreviations or known first names often indicating people. Name-internal evidence is the only criteria considered in virtually all of the name recognition systems that are reported as part of state of the art information extraction systems (see e.g. Rau 1991, Alshawi 1992, DARPA 1992), most of 32 which depend on large (-20,000 word) gazetteers and lists of known names for their relatively high performance. By contrast, external evidence is the classificatory criteria provided by the context in which a name appears. The basis for this evidence is the obvious observation that names are just ways to refer to individuals of specific types (people, churches, rock groups, etc.), and that these types have characteristic properties and participate in characteristic events. The presence of these properties or events in the immediate context of a proper na</context>
</contexts>
<marker>Rau, 1991</marker>
<rawString>Rau, Lisa F. (1991) &amp;quot;Extracting Company Names from Text&amp;quot;, proc. Seventh Conference on Artificial Intelligence Applications, February 24-28, 1992, IEEE, 189-194.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>