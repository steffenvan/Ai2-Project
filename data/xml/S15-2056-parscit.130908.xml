<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000019">
<title confidence="0.996168">
DFKI: Multi-objective Optimization for the Joint Disambiguation of Entities
and Nouns &amp; Deep Verb Sense Disambiguation
</title>
<author confidence="0.964417">
Dirk Weissenborn
</author>
<affiliation confidence="0.566751">
LT, DFKI
</affiliation>
<address confidence="0.875117">
Alt-Moabit 91c
Berlin, Germany
</address>
<email confidence="0.992983">
dirk.weissenborn@dfki.de
</email>
<author confidence="0.569843">
Feiyu Xu
</author>
<affiliation confidence="0.336275">
LT, DFKI
</affiliation>
<address confidence="0.867339">
Alt-Moabit 91c
Berlin, Germany
</address>
<email confidence="0.991441">
feiyu@dfki.de
</email>
<author confidence="0.710228">
Hans Uszkoreit
</author>
<affiliation confidence="0.419097">
LT, DFKI
</affiliation>
<address confidence="0.868526">
Alt-Moabit 91c
Berlin, Germany
</address>
<email confidence="0.995666">
uszkoreit@dfki.de
</email>
<sectionHeader confidence="0.99556" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9993705">
We introduce an approach to word sense dis-
ambiguation and entity linking that combines
a set of complementary objectives in an exten-
sible multi-objective formalism. During dis-
ambiguation the system performs continuous
optimization to find optimal probability dis-
tributions over candidate senses. Verb senses
are disambiguated using a separate neural net-
work model. Our results on noun and verb
sense disambiguation as well as entity linking
outperform all other submissions on the Se-
mEval 2015 Task 13 for English.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999976652173913">
The task of assigning the correct meaning to a given
word or entity mention in a document is called word
sense disambiguation (WSD) (Navigli, 2009) or en-
tity linking (EL) (Bunescu and Pasca, 2006), respec-
tively. Successful disambiguation requires not only
an understanding of the topic or domain a document
is dealing with (global), but also an analysis of how
an individual word is used within its local context.
E.g., the meanings of the word “newspaper” as the
company or the physical product, often cannot be
distinguished by the topic, but by recognizing which
type of meaning fits best into the local context of its
occurrence. On the other hand, for an ambiguous
entity mention such as “Michael Jordan” it is impor-
tant to recognize the topic of the wider context to
distinguish, e.g., between the basketball player and
the machine learning expert.
The combination of the two most commonly used
reference knowledge bases for WSD and EL, e.g.,
WordNet (Fellbaum, 1998) and Wikipedia, by Ba-
belNet (Navigli and Ponzetto, 2012) has enabled a
new line of research towards the joint disambigua-
tion of words and named entities. Babelfy (Moro
et al., 2014) has shown the potential of combining
these two tasks in a purely knowledge-driven ap-
proach that jointly finds connections between po-
tential word senses in the global context. On the
other hand, typical supervised methods (Zhong and
Ng, 2010) trained on sense-annotated corpora are
usually quite successful in dealing with individual
words in a local context. Hoffart et al. (2011) rec-
ognize the importance of combining both local con-
text and global context for robust disambiguation.
However, their approach is limited to EL, where op-
timization is performed in a discrete setting.
We present a system that combines disambigua-
tion objectives for both global and local contexts
into a single multi-objective function. In contrast
to prior work we model the problem in a continuous
setting based on probability distributions over can-
didate meanings. Our approach exploits lexical and
encyclopedic knowledge, local context information
and statistics of the mapping from text to candidate
meanings. Furthermore, we introduce a deep learn-
ing approach to verb sense disambiguation based on
semantic role labeling.
</bodyText>
<sectionHeader confidence="0.987935" genericHeader="introduction">
2 Approach
</sectionHeader>
<bodyText confidence="0.999278">
The SemEval-2015 task 13 (Moro and Navigli,
2015) requires a system to jointly detect and dis-
ambiguate word and entity mentions given a refer-
ence knowledge base. The provided input to the sys-
tem are tokenized, lemmatized and POS-tagged doc-
</bodyText>
<page confidence="0.991144">
335
</page>
<bodyText confidence="0.80011">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 335–339,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
uments; the output are sense-annotated mentions.
Our system employs BabelNet 1.1.1 as reference
knowledge base (KB). BabelNet is a multilingual se-
mantic graph of concepts and named entities that are
represented by synonym sets, called Babel synsets.
</bodyText>
<subsectionHeader confidence="0.995672">
2.1 Mention Extraction &amp; Entity Detection
</subsectionHeader>
<bodyText confidence="0.999924714285714">
We define a mention to be a sequence of tokens in
a given document for which there exists at least one
candidate meaning in the KB. The system considers
all content words (nouns, verbs, adjectives, adverbs)
as mentions including also multi-token words of up
to 5 tokens that contain at least one noun. In ad-
dition, we apply a pre-trained stacked linear-chain
CRF (Lafferty et al., 2001) using the FACTORIE
toolkit of version 1.1 (McCallum et al., 2009) to
identify named entity (NE) mentions. In our ap-
proach, we distinguish NEs from common nouns
and treat them as two different classes because there
are many common nouns also referring to NEs mak-
ing disambiguation unnecessarily complicated.
</bodyText>
<subsectionHeader confidence="0.997709">
2.2 Candidate Search
</subsectionHeader>
<bodyText confidence="0.9999767">
After potential mentions are extracted the system
tries to identify their candidate meanings, i.e., the
appropriate synsets. Mentions without such can-
didates are discarded. The mapping of candi-
date mentions to synsets is based on similarities
of their surface strings or lemmas. If the surface
string or lemma of a mention matches the lemma
of a synonym in a synset that has the same part
of speech, the synset will be considered a candi-
date meaning. We allow partial matches for Ba-
belNet synonyms derived from Wikipedia titles or
redirections. A partial match allows the surface
string of a mention to differ by up to two tokens
from the Wikipedia title (excluding everything in
parentheses) if the partial string was used at least
once as an anchor for the corresponding Wikipedia
page. For example, for the Wikipedia title Arm-
strong School District (Pennsylvania), the follow-
ing surface strings would be considered matches:
“Armstrong School District (Pennsylvania)”, “Arm-
strong School District”, “Armstrong”, but not
“School”, since “School” was never used as an an-
chor. If there is no match we try the same procedure
applied to the lowercased text or lemma.
Because of the distinction between nouns and
named entities we treat NE as a separate POS tag.
Candidate synsets for NEs are Babel synsets con-
sidered NEs in BabelNet, and additionally Babel
synsets of all Wikipedia senses that are not consid-
ered NEs. Similarly, candidate synsets for nouns are
noun synsets that are not considered NEs in addi-
tion to all synsets of WordNet senses in BabelNet.
We add synsets of Wikipedia senses and WordNet
senses, respectively, because the distinction of NEs
and simple concepts is not always clear in BabelNet.
For example the synset for “UN” (United Nations) is
considered a concept whereas it could also be con-
sidered a NE. Finally, if there is no candidate for a
potential noun mention we try to find NE candidates
for it and vice versa.
</bodyText>
<subsectionHeader confidence="0.993643">
2.3 Disambiguation of Nouns and Named
Entities
</subsectionHeader>
<bodyText confidence="0.999788315789474">
We formulate the disambiguation problem in a con-
tinuous setting by using probability distributions
over candidates. This has several advantages over
a discrete setting. First, we can exploit well estab-
lished continuous optimization algorithms, such as
conjugate gradient or LBFGS, which guarantee to
converge to a local optimum. Second, by optimiz-
ing upon probability distributions we are optimizing
the actually desired result in contrast to densest sub-
graph algorithms where such probabilities need to
be calculated artificially afterwards, e.g., Moro et al.
(2014). Third, discrete optimization usually works
on a single candidate per iteration whereas in a con-
tinuous setting, probabilities are adjusted for each
candidate, which is computationally advantageous
for highly ambiguous documents.
Given a set of objectives D the overall objective
function O is defined as the sum of all normalized
objectives O ∈ D given a set of mentions M:
</bodyText>
<equation confidence="0.998053">
O(M) (1)
O... (M) − O�ni. (M) .
</equation>
<bodyText confidence="0.999959">
We normalize each objective using the difference
of their maximum and minimum value for the given
document. For disambiguation we optimize the
multi-objective function using Conjugate Gradient
(Hestenes and Stiefel, 1952) with up to 1000 iter-
ations per document.
</bodyText>
<equation confidence="0.95096">
O(M) = �
O∈O
</equation>
<page confidence="0.979823">
336
</page>
<bodyText confidence="0.998868333333333">
Coherence Jointly disambiguating all mentions
within a document has been shown to have a large
impact on disambiguation quality. We adopt the idea
of semantic signatures and the idea of maximizing
the semantic agreement among selected candidate
senses from Moro et al. (2014). We define the con-
tinuous objective function based on probability dis-
tributions pm(c) over the candidate set Cm of each
mention m E M in a document as follows:
</bodyText>
<equation confidence="0.985605">
eλm,c
pm(c) =
λE(2)
c0ECm e ,
</equation>
<bodyText confidence="0.999775111111111">
where S denotes the semantic interpretation graph,
✶ the indicator function and pm(c) is a softmax
function. The only free, optimizable parameters are
the softmax weights λm,c. This objective can be
interpreted as finding the densest subgraph of the
semantic interpretation graph where each node is
weighted by its probability and therefore each edge
is weighted by the product of its adjacent vertex
probabilities.
</bodyText>
<equation confidence="0.436521">
1http://wordnet.princeton.edu/man/lexnames.5WN.html
(Zhong
owing form:
qm(tc)·pm(c) (3)
</equation>
<bodyText confidence="0.943880228070175">
of surrounding words
and Ng, 2010), sur-
rounding POS tags and possible lexnames. We used
pre-trained embeddings from Mikolov et al. (2013).
Type classification is included in the overall ob-
jective in the foll
gration of prior information. E.g., the word
without further context has a strong prior on its
meaning as a city instead of a person. Our approach
utilizes prior information in form of frequency
statistics over candidate synsets for a
sur-
face string. These priors are derived from annota-
tion frequencies provided by WordNet for Babel-
synsets containing the respective WordNet sense
and from occurrence frequencies in Wikipedia ex-
tracted by DBpedia Spotlight(Daiber et al., 2013) for
synsets containing only Wikipedia senses. Laplace-
smoothing is applied to all prior frequencies. This
prior is used to initialize the probability distribution
over candidate synsets. Note that the priors are used
i.e., as actual priors and not during con-
text based optimization itself.
Furthermore, because candidate priors for NE
mentions can be very high we add an additional
L2-regularization objective for NE mentions with
= 0.001, which we found to work best on de-
velopment data. Finally, named entities were fil-
tered out if they were included in another NE, had
no connection in the semantic interpretation graph
with another candidate sense of the input document
or were overlapping with an
“Paris”
mention’s
“naturally”,
λ
other NE but were con-
nectedworse.
s(m, c, m&apos;, c&apos;) = pm(c) · pm0(c&apos;) · ✶((c, c&apos;) E S)
Type Classification One of the biggest problems
of supervised approaches to WSD is the size and
synset coverage of training corpora such as Sem-
Cor (Miller et al., 1993). One way to circum-
vent this problem is to use a coarser set of seman-
tic classes that groups synsets together. Previous
studies on using semantic classes for disambigua-
tion showed promising results (Izquierdo-Bevi´a et
al., 2006). WordNet provides a mapping, called lex-
names, of synsets into 45 types based on the syntac-
tic categories of synsets and their logical groupings1.
A multi-class logistic (softmax) regression model
was trained that calculates a probability distribution
qm(t) over lexnames t given a potential WordNet
mention m. The features used as input to the model
are the following: embedding of the mention’s text,
sum of embeddings of all sentence words, embed-
ding of the dependency parse parent, collocations
</bodyText>
<equation confidence="0.4719505">
mEM
cECm
</equation>
<bodyText confidence="0.5082855">
Priors Another advantage of working with proba-
bility distributions over candidates is the easy inte-
</bodyText>
<subsectionHeader confidence="0.999299">
2.4 Disambiguation of Verbs
</subsectionHeader>
<bodyText confidence="0.9890037">
The disambiguation of verbs requires an approach
that focuses more on the local context and especially
the usage of a verb within a sentence. Therefore, we
train a neural network based on semantic role label-
ing (SRL) and sentence words. Figure 1 illustrates
an example network. The input is composed of the
word embeddings (Turian et al., 2010) for each fea-
ture (word itself, its lemma, SRLs and bag of sen-
tence words). All
individual input embeddings are
</bodyText>
<figure confidence="0.548233833333333">
� E s(m, c, m&apos;, c&apos;)
Ocoh(M) = m0EM
mEM m0:Am
cECm c0ECm0
�
Otyp(M) =
</figure>
<page confidence="0.899265">
337
</page>
<figureCaption confidence="0.884741">
Figure 1: Disambiguation neural network for “won”
in the sentence “Obama won the Nobel Prize.”
</figureCaption>
<bodyText confidence="0.845595666666667">
50-dimensional and connected to a 100-dimensional
hidden layer. The output layer consists of all can-
didate synsets of the verb. The individual output
weights Wc are candidate specific. To ensure bet-
ter generalization and to deal with the sparseness of
training corpora, Wc is defined as the following sum:
</bodyText>
<equation confidence="0.9973555">
Wc = Ws(c) + � �Wsp + Wse, (4)
sp∈Ps(c) se∈Es(c)
</equation>
<bodyText confidence="0.99985625">
where s(c) is the respective synset of c, Ps is the set
of all hypernyms of s (transitive closure) and Es are
the synsets entailed by s. We used ClearNLP2(Choi,
2012) for extracting SRLs.
</bodyText>
<sectionHeader confidence="0.999884" genericHeader="method">
3 Results
</sectionHeader>
<bodyText confidence="0.9999552">
The results of our system are shown in Table 1.
Our approaches to the disambiguation of English
nouns, named entities and verbs generally outper-
formed all other submissions across different do-
mains as well as the strong baseline provided by
the most-frequent-sense (MFS). This demonstrates
the system’s capability to adapt to different domains.
However, results on the math and computer domain
also reveal that performance strongly depends on
the document topic. The results for this domain are
worse compared to the other domains for almost all
participating systems, which may indicate that exist-
ing resources do not cover this domain as well as the
others. Another potential explanation is that enforc-
ing only pairwise coherence does not take the hidden
</bodyText>
<footnote confidence="0.989445">
2http://clearnlp.wikispaces.com
</footnote>
<table confidence="0.991322466666666">
bio math gen all
MFS 75.3 43.6 69.2 66.7
best other 76.5 51.4 63.7 64.8
DFKI 79.1 44.9 73.4 70.3
(a) Nouns
bio math gen all
MFS 98.9 57.1 77.4 85.7
best other 98.9 74.3 89.7 87.0
DFKI 100.0 57.1 90.3 88.9
(b) Named Entities
bio math gen all
MFS 52.5 55.7 61.4 55.1
best other 53.8 60.6 70.6 57.1
DFKI 58.3 52.3 66.7 57.7
(c) Verb
</table>
<tableCaption confidence="0.997474">
Table 1: F1 scores of our system, the best other sys-
</tableCaption>
<bodyText confidence="0.893732428571429">
tem and an MFS baseline on the disambiguation of
English nouns, named entities and verbs for all do-
mains of the SemEval 2015 task 13. bio- biomedi-
cal; math- math &amp; computer; gen- general
topics computer and maths into account that connect
all concepts in the specific document. This might be
an interesting point for further research.
</bodyText>
<sectionHeader confidence="0.999297" genericHeader="method">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999995571428572">
We have presented a robust approach for disam-
biguating nouns and named entities as well as a neu-
ral network for verb sense disambiguation that we
used in the SemEval 2015 task 13. Our system
achieved an overall F1 score of 70.3 for nouns, 88.9
for NEs and 57.7 for verbs across different domains,
outperforming all other submissions for these cate-
gories of English. The disambiguation of nouns and
named entities performs especially well compared
to other systems and can still be extended through
the introduction of additional, complementary ob-
jectives. Disambiguating verbs remains a very chal-
lenging task and the promising results of our model
still leave much room for improvement.
</bodyText>
<sectionHeader confidence="0.959506" genericHeader="conclusions">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.98041275">
This research was partially supported by the
German Federal Ministry of Education and Re-
search (BMBF) through the projects Deepen-
dance (01IW11003), ALL SIDES (01IW14002) and
</bodyText>
<figure confidence="0.996599666666667">
Input Hidden Output
layer layer layer
won
win
Obama
Prize
BoW
WA0 Wc3
...
</figure>
<page confidence="0.982899">
338
</page>
<bodyText confidence="0.6651105">
BBDC (01IS14013E) and by Google through a Fo-
cused Research Award granted in July 2013.
</bodyText>
<sectionHeader confidence="0.980401" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995749723684211">
[Bunescu and Pasca2006] Razvan C Bunescu and Mar-
ius Pasca. 2006. Using encyclopedic knowledge for
named entity disambiguation. In EACL, volume 6,
pages 9–16.
[Choi2012] Jinho D Choi. 2012. Optimization of nat-
ural language processing components for robustness
and scalability.
[Daiber et al.2013] Joachim Daiber, Max Jakob, Chris
Hokamp, and Pablo N Mendes. 2013. Improving effi-
ciency and accuracy in multilingual entity extraction.
[Fellbaum1998] Christiane Fellbaum. 1998. WordNet.
Wiley Online Library.
[Hestenes and Stiefel1952] Magnus Rudolph Hestenes
and Eduard Stiefel. 1952. Methods of conjugate gra-
dients for solving linear systems, volume 49. National
Bureau of Standards Washington, DC.
[Hoffart et al.2011] Johannes Hoffart, Mohamed Amir
Yosef, Ilaria Bordino, Hagen F¨urstenau, Manfred
Pinkal, Marc Spaniol, Bilyana Taneva, Stefan Thater,
and Gerhard Weikum. 2011. Robust disambiguation
of named entities in text. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 782–792. Association for Compu-
tational Linguistics.
[Izquierdo-Bevi´a et al.2006] Rub´en Izquierdo-Bevi´a,
Lorenza Moreno-Monteagudo, Borja Navarro, and
Armando Su´arez. 2006. Spanish all-words semantic
class disambiguation using cast3lb corpus. In MICAI
2006: Advances in Artificial Intelligence, pages
879–888. Springer.
[Lafferty et al.2001] John Lafferty, Andrew McCallum,
and Fernando CN Pereira. 2001. Conditional random
fields: Probabilistic models for segmenting and label-
ing sequence data.
[McCallum et al.2009] Andrew McCallum, Karl Schultz,
and Sameer Singh. 2009. FACTORIE: Probabilistic
programming via imperatively defined factor graphs.
In Neural Information Processing Systems (NIPS).
[Mikolov et al.2013] Tomas Mikolov, Ilya Sutskever, Kai
Chen, Greg S Corrado, and Jeff Dean. 2013. Dis-
tributed representations of words and phrases and their
compositionality. In Advances in Neural Information
Processing Systems, pages 3111–3119.
[Miller et al.1993] George A Miller, Claudia Leacock,
Randee Tengi, and Ross T Bunker. 1993. A seman-
tic concordance. In Proceedings of the workshop on
Human Language Technology, pages 303–308. Asso-
ciation for Computational Linguistics.
[Moro and Navigli2015] Andrea Moro and Roberto Nav-
igli. 2015. SemEval-2015 Task 13: Multilingual All-
Words Sense Disambiguation and Entity Linking. In
Proc. of SemEval-2015.
[Moro et al.2014] Andrea Moro, Alessandro Raganato,
and Roberto Navigli. 2014. Entity linking meets word
sense disambiguation: A unified approach. Transac-
tions of the Association for Computational Linguistics,
2.
[Navigli and Ponzetto2012] Roberto Navigli and Si-
mone Paolo Ponzetto. 2012. Babelnet: The automatic
construction, evaluation and application of a wide-
coverage multilingual semantic network. Artificial
Intelligence, 193:217–250.
[Navigli2009] Roberto Navigli. 2009. Word sense dis-
ambiguation: A survey. ACM Computing Surveys
(CSUR), 41(2):10.
[Turian et al.2010] Joseph Turian, Lev Ratinov, and
Yoshua Bengio. 2010. Word representations: a simple
and general method for semi-supervised learning. In
Proceedings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 384–394.
Association for Computational Linguistics.
[Zhong and Ng2010] Zhi Zhong and Hwee Tou Ng.
2010. It makes sense: A wide-coverage word sense
disambiguation system for free text. In Proceedings of
the ACL 2010 System Demonstrations, pages 78–83.
Association for Computational Linguistics.
</reference>
<page confidence="0.999215">
339
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.106680">
<title confidence="0.998793">DFKI: Multi-objective Optimization for the Joint Disambiguation of Entities and Nouns &amp; Deep Verb Sense Disambiguation</title>
<author confidence="0.991544">Dirk</author>
<affiliation confidence="0.907004">LT, Alt-Moabit</affiliation>
<address confidence="0.539977">Berlin,</address>
<email confidence="0.992557">dirk.weissenborn@dfki.de</email>
<author confidence="0.984874">Feiyu Xu</author>
<affiliation confidence="0.853267">LT, Alt-Moabit Berlin,</affiliation>
<email confidence="0.993127">feiyu@dfki.de</email>
<author confidence="0.990995">Hans Uszkoreit</author>
<affiliation confidence="0.8669135">LT, Alt-Moabit</affiliation>
<address confidence="0.96351">Berlin, Germany</address>
<email confidence="0.999374">uszkoreit@dfki.de</email>
<abstract confidence="0.964654461538462">We introduce an approach to word sense disambiguation and entity linking that combines a set of complementary objectives in an extensible multi-objective formalism. During disambiguation the system performs continuous optimization to find optimal probability distributions over candidate senses. Verb senses are disambiguated using a separate neural network model. Our results on noun and verb sense disambiguation as well as entity linking outperform all other submissions on the SemEval 2015 Task 13 for English.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
<author>Marius Pasca</author>
</authors>
<title>Using encyclopedic knowledge for named entity disambiguation.</title>
<date>2006</date>
<booktitle>In EACL,</booktitle>
<volume>6</volume>
<pages>9--16</pages>
<marker>[Bunescu and Pasca2006]</marker>
<rawString>Razvan C Bunescu and Marius Pasca. 2006. Using encyclopedic knowledge for named entity disambiguation. In EACL, volume 6, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinho D Choi</author>
</authors>
<title>Optimization of natural language processing components for robustness and scalability.</title>
<date>2012</date>
<marker>[Choi2012]</marker>
<rawString>Jinho D Choi. 2012. Optimization of natural language processing components for robustness and scalability.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joachim Daiber</author>
<author>Max Jakob</author>
<author>Chris Hokamp</author>
<author>Pablo N Mendes</author>
</authors>
<title>Improving efficiency and accuracy in multilingual entity extraction.</title>
<date>2013</date>
<marker>[Daiber et al.2013]</marker>
<rawString>Joachim Daiber, Max Jakob, Chris Hokamp, and Pablo N Mendes. 2013. Improving efficiency and accuracy in multilingual entity extraction.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<date>1998</date>
<publisher>WordNet. Wiley Online Library.</publisher>
<marker>[Fellbaum1998]</marker>
<rawString>Christiane Fellbaum. 1998. WordNet. Wiley Online Library.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Rudolph Hestenes</author>
<author>Eduard Stiefel</author>
</authors>
<title>Methods of conjugate gradients for solving linear systems, volume 49. National Bureau of Standards</title>
<date>1952</date>
<location>Washington, DC.</location>
<marker>[Hestenes and Stiefel1952]</marker>
<rawString>Magnus Rudolph Hestenes and Eduard Stiefel. 1952. Methods of conjugate gradients for solving linear systems, volume 49. National Bureau of Standards Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Hoffart</author>
<author>Mohamed Amir Yosef</author>
<author>Ilaria Bordino</author>
<author>Hagen F¨urstenau</author>
<author>Manfred Pinkal</author>
<author>Marc Spaniol</author>
<author>Bilyana Taneva</author>
<author>Stefan Thater</author>
<author>Gerhard Weikum</author>
</authors>
<title>Robust disambiguation of named entities in text.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>782--792</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>[Hoffart et al.2011]</marker>
<rawString>Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino, Hagen F¨urstenau, Manfred Pinkal, Marc Spaniol, Bilyana Taneva, Stefan Thater, and Gerhard Weikum. 2011. Robust disambiguation of named entities in text. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 782–792. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rub´en Izquierdo-Bevi´a</author>
<author>Lorenza Moreno-Monteagudo</author>
<author>Borja Navarro</author>
<author>Armando Su´arez</author>
</authors>
<title>Spanish all-words semantic class disambiguation using cast3lb corpus.</title>
<date>2006</date>
<booktitle>In MICAI 2006: Advances in Artificial Intelligence,</booktitle>
<pages>879--888</pages>
<publisher>Springer.</publisher>
<marker>[Izquierdo-Bevi´a et al.2006]</marker>
<rawString>Rub´en Izquierdo-Bevi´a, Lorenza Moreno-Monteagudo, Borja Navarro, and Armando Su´arez. 2006. Spanish all-words semantic class disambiguation using cast3lb corpus. In MICAI 2006: Advances in Artificial Intelligence, pages 879–888. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando CN Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<marker>[Lafferty et al.2001]</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando CN Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Karl Schultz</author>
<author>Sameer Singh</author>
</authors>
<title>FACTORIE: Probabilistic programming via imperatively defined factor graphs.</title>
<date>2009</date>
<booktitle>In Neural Information Processing Systems (NIPS).</booktitle>
<marker>[McCallum et al.2009]</marker>
<rawString>Andrew McCallum, Karl Schultz, and Sameer Singh. 2009. FACTORIE: Probabilistic programming via imperatively defined factor graphs. In Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<marker>[Mikolov et al.2013]</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Claudia Leacock</author>
<author>Randee Tengi</author>
<author>Ross T Bunker</author>
</authors>
<title>A semantic concordance.</title>
<date>1993</date>
<booktitle>In Proceedings of the workshop on Human Language Technology,</booktitle>
<pages>303--308</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>[Miller et al.1993]</marker>
<rawString>George A Miller, Claudia Leacock, Randee Tengi, and Ross T Bunker. 1993. A semantic concordance. In Proceedings of the workshop on Human Language Technology, pages 303–308. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Moro</author>
<author>Roberto Navigli</author>
</authors>
<date>2015</date>
<booktitle>SemEval-2015 Task 13: Multilingual AllWords Sense Disambiguation and Entity Linking. In Proc. of SemEval-2015.</booktitle>
<marker>[Moro and Navigli2015]</marker>
<rawString>Andrea Moro and Roberto Navigli. 2015. SemEval-2015 Task 13: Multilingual AllWords Sense Disambiguation and Entity Linking. In Proc. of SemEval-2015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Moro</author>
<author>Alessandro Raganato</author>
<author>Roberto Navigli</author>
</authors>
<title>Entity linking meets word sense disambiguation: A unified approach.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<volume>2</volume>
<marker>[Moro et al.2014]</marker>
<rawString>Andrea Moro, Alessandro Raganato, and Roberto Navigli. 2014. Entity linking meets word sense disambiguation: A unified approach. Transactions of the Association for Computational Linguistics, 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>Babelnet: The automatic construction, evaluation and application of a widecoverage multilingual semantic network.</title>
<date>2012</date>
<journal>Artificial Intelligence,</journal>
<pages>193--217</pages>
<marker>[Navigli and Ponzetto2012]</marker>
<rawString>Roberto Navigli and Simone Paolo Ponzetto. 2012. Babelnet: The automatic construction, evaluation and application of a widecoverage multilingual semantic network. Artificial Intelligence, 193:217–250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Word sense disambiguation: A survey.</title>
<date>2009</date>
<journal>ACM Computing Surveys (CSUR),</journal>
<volume>41</volume>
<issue>2</issue>
<marker>[Navigli2009]</marker>
<rawString>Roberto Navigli. 2009. Word sense disambiguation: A survey. ACM Computing Surveys (CSUR), 41(2):10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>384--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>[Turian et al.2010]</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhi Zhong</author>
<author>Hwee Tou Ng</author>
</authors>
<title>It makes sense: A wide-coverage word sense disambiguation system for free text.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 System Demonstrations,</booktitle>
<pages>78--83</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>[Zhong and Ng2010]</marker>
<rawString>Zhi Zhong and Hwee Tou Ng. 2010. It makes sense: A wide-coverage word sense disambiguation system for free text. In Proceedings of the ACL 2010 System Demonstrations, pages 78–83. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>