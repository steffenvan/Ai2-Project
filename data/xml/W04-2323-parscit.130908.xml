<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001605">
<title confidence="0.992814">
Unifying Annotated Discourse Hierarchies to Create a Gold Standard
</title>
<author confidence="0.999261">
Marco Carbone, Ya’akov Gal, Stuart Shieber, and Barbara Grosz
</author>
<affiliation confidence="0.977605">
Division of Engineering and Applied Sciences
Harvard University
</affiliation>
<address confidence="0.974788">
33 Oxford St.
Cambridge, MA 02138
</address>
<email confidence="0.999613">
mcarbone,gal,shieber,grosz@eecs.harvard.edu
</email>
<sectionHeader confidence="0.995648" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999891142857143">
Human annotation of discourse corpora typi-
cally results in segmentation hierarchies that
vary in their degree of agreement. This paper
presents several techniques for unifying multi-
ple discourse annotations into a single hierar-
chy, deemed a “gold standard” — the segmen-
tation that best captures the underlying linguis-
tic structure of the discourse. It proposes and
analyzes methods that consider the level of em-
beddedness of a segmentation as well as meth-
ods that do not. A corpus containing annotated
hierarchical discourses, the Boston Directions
Corpus, was used to evaluate the “goodness”
of each technique, by comparing the similar-
ity of the segmentation it derives to the original
annotations in the corpus. Several metrics of
similarity between hierarchical segmentations
are computed: precision/recall of matching ut-
terances, pairwise inter-reliability scores ( ),
and non-crossing-brackets. A novel method for
unification that minimizes conflicts among an-
notators outperforms methods that require con-
sensus among a majority for the and precision
metrics, while capturing much of the structure
of the discourse. When high recall is preferred,
methods requiring a majority are preferable to
those that demand full consensus among anno-
tators.
</bodyText>
<sectionHeader confidence="0.998905" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999947063157895">
The linguistic structure of a discourse is composed of
utterances that exhibit meaningful hierarchical relation-
ships (Grosz and Sidner, 1986). Automatic segmentation
of discourse forms the basis for many applications, from
information retrieval and text summarization to anaphora
resolution (Hearst, 1997). These automatic methods, usu-
ally based on supervised machine learning techniques,
require a manually annotated corpus of data for train-
ing. The creation of these corpora often involves multiple
judges annotating the same discourses, so as to avoid bias
from using a single judge’s annotations as ground truth.
Usually, for a particular discourse, these multiple annota-
tions are unified into a single annotation, either manually
by the annotators’ discussions or automatically. How-
ever, annotation unification approaches have not been for-
mally evaluated, and although manual unification might
be the best approach, it can be time-consuming. In-
deed, much of the work on automatic recognition of dis-
course structure has focused on linear, rather than hi-
erarchical segmentation (Hearst, 1997; Hirschberg and
Nakatani, 1996), because of the difficulties of obtain-
ing consistent hierarchical annotations. In addition, those
approaches that do handle hierarchical segmentation do
not address automatic unification methods (Carlson et al.,
2001; Marcu, 2000).
There are several reasons for the prevailing emphasis
on linear annotation and the lack of work on automatic
methods for unifying hierarchical discourse annotations.
First, initial attempts to create annotated hierarchical cor-
pora of discourse structure using naive annotators have
met with difficulties. Rotondo (1984) reported that “hi-
erarchical segmentation is impractical for naive subjects
in discourses longer than 200 words.” Passonneau and
Litman (1993) conducted a pilot study in which subjects
found it “difficult and time-consuming” to identify hi-
erarchical relations in discourse. Other attempts have
had more success using improved annotation tools and
more precise instructions (Grosz and Hirschberg, 1992;
Hirschberg and Nakatani, 1996). Second, hierarchical
segmentation of discourse is subjective. While agreement
among annotators regarding linear segmentation has been
found to be higher than 80% (Hearst, 1997), with respect
to hierarchical segmentation it has been observed to be
as low as 60% (Flammia and Zue, 1995). Moreover, the
precise definition of “agreement” with respect to hierar-
chical segmentation is unclear, complicating evaluation.
It is natural to consider two segments in separate annota-
tions to agree if they both span precisely the same utter-
ances and agree on the level of embeddedness. However,
it is less clear how to handle segments that share the same
utterances but differ with respect to the level of embed-
dedness.
In this paper, we show that despite these difficulties it is
possible to automatically combine a set of multi-level dis-
course annotations together into a single gold standard, a
segmentation that best captures the underlying linguis-
tic structure of the discourse. We aspire to create cor-
pora analogous to the Penn Treebank in which a unique
parse tree exists for each sentence that is agreed upon by
all to convey the “correct” parse of the sentence. How-
ever, whereas the Penn Treebank parses are determined
through a time-consuming negotiation between labelers,
we aim to derive gold standard segmentations automati-
cally.
There are several potential benefits for having a unify-
ing standard for discourse corpora. First, it can constitute
a unique segmentation of the discourse that is deemed the
nearest approximation of the true objective structure, as-
suming one exists. Second, it can be used as a single uni-
fied version with which to train and evaluate algorithms
for automatic discourse segmentation. Third, it can be
used as a preprocessing step for computational tasks that
require discourse structure, such as anaphora resolution
and summarization.
In this work, we describe and evaluate several ap-
proaches for unifying multiple hierarchical discourse seg-
mentations into one gold standard. Some of our ap-
proaches measure the agreement between annotations by
taking into account the level of embeddedness and others
ignore the hierarchy. We also introduce a novel method,
called the Conflict-Free Union, that minimizes the num-
ber of conflicts between annotations. For our experi-
ments, we used the Boston Directions Corpus (BDC).1
Ideally, each technique would be evaluated with re-
spect to a single unified segmentation of the BDC that
was deemed “true” by annotators who are experts in dis-
course theory, but we did not have the resources to at-
tempt this task. Instead, we measure each technique by
comparing the average similarity between its gold stan-
dard and the original annotations used to create it. Our
similarity metrics measure both hierarchical and linear
segment agreement using precision/recall metrics, inter-
reliability similarities among annotations using the ( )
metric, and percentage of non-crossing-brackets.
We found that there is no single approach that does
</bodyText>
<footnote confidence="0.9158575">
1The Boston Directions Corpus was designed and collected
by Barbara Grosz, Julia Hirschberg, and Christine H. Nakatani.
</footnote>
<bodyText confidence="0.997404333333333">
well with respect to all of the similarity metrics. How-
ever, the Conflict-Free Union approach outperforms the
other methods for the and precision metrics. Also, tech-
niques that include majority agreements of annotators
have better recall than techniques which demanded full
consensus among annotators. We also uncovered some
flaws in each technique; for example, we found that gold
standards that include dense structure are over-penalized
by some of the metrics.
</bodyText>
<sectionHeader confidence="0.992398" genericHeader="method">
2 Methods for Creating a Gold Standard
</sectionHeader>
<bodyText confidence="0.99998075">
It is likely that there is no perfect way to find and evaluate
a gold standard, and in some cases there may be multiple
segmentations that are equally likely to serve as a gold
standard. In the BDC corpus, unlike the Penn Treebank,
there are multiple annotations for each discourse which
were not manually combined into one gold standard an-
notation. In this paper, we explore automatic methods to
create a gold standard for the BDC corpus. These meth-
ods could also be used on other corpora with non-unified
annotations. Next, we present several automatic methods
to combine multiple human-annotated discourse segmen-
tations into one gold standard.
</bodyText>
<subsectionHeader confidence="0.989389">
2.1 Flat vs. Hierarchical Approaches
</subsectionHeader>
<bodyText confidence="0.999986153846154">
Most previous work that has combined multiple an-
notations has used linear segmentations, i.e. dis-
course segmentations without hierarchies (Hirschberg
and Nakatani, 1996). In general, the hierarchical nature
of discourse structure has not been considered when com-
puting labeler inter-reliability and in evaluations of agree-
ment with automatic methods. Since computational dis-
course theory relies on the hierarchy of its segments, we
will consider it in this paper. For each approach that fol-
lows, we consider both a “flat” version, which does not
consider level of embeddedness, and a “full” approach,
which does. We analyze the differences between the flat
and full versions for each approach.
</bodyText>
<subsectionHeader confidence="0.999538">
2.2 Segment Definition
</subsectionHeader>
<bodyText confidence="0.9990755">
A discourse is made up of a sequence of utterances,
. In this paper, we define a segment as
a triple , where is the first utterance in the seg-
ment, is the last utterance in the segment, and is the
segment’s level of embeddedness.2 We will sometimes
refer to and as boundary utterances. Lastly, when
we are not interested in level of embeddedness, we will
sometimes refer to a segment as , without the value.
</bodyText>
<subsectionHeader confidence="0.99275">
2.3 The Consensus Approach
</subsectionHeader>
<bodyText confidence="0.723473">
A conservative way to combine segmentations into a gold
standard is the Consensus (CNS) (or raw agreement) ap-
2The levels are numbered from top to bottom; hence, 1 is the
level of the largest segment, 2 is the level below that, and so on.
</bodyText>
<equation confidence="0.39146">
1 2 3 GS
</equation>
<figureCaption confidence="0.9669925">
Figure 1: An example of the FullCNS approach. FlatCNS
would create the same gold standard. The segments in the
annotations that are marked in bold are those selected by
the gold standard.
</figureCaption>
<figure confidence="0.99806775">
A B E F H J K
C I L
D G M
1 2 3 Flat GS Full GS
</figure>
<figureCaption confidence="0.9810825">
Figure 2: An example where FullCNS and FlatCNS cre-
ate different gold standards..
</figureCaption>
<bodyText confidence="0.99964975">
proach (Hirschberg and Nakatani, 1996). CNS constructs
a gold standard by including only those segments which
have been included by every annotator. In the “full” ver-
sion of CNS (FullCNS), the annotators need to agree
upon the embedded level of the segment along with the
segment boundaries. The “flat” version (FlatCNS) ig-
nores hierarchy and considers only the segment bound-
aries when determining agreement.
Figure 1 shows an example of performing FullCNS
on three annotations. In the figure, all three annotators
agree on only the largest segment (segments A, E, and
H). Hence, the gold standard includes only that single
segment. FlatCNS gives the same gold standard in this
example as there are no two segments with the same
boundaries but with different levels of embeddedness.
In Figure 2, we see an example where the gold stan-
dards created by FlatCNS and FullCNS differ. Aside
from the largest segment, FullCNS contains only the seg-
ment representing the agreement of segments D, G, and
M. FlatCNS includes that segment as well, in addition
to two more from the agreement of segments B, H, and K
and segments C, I, and L. FullCNS does not include those
segments because the segments occur at differing levels
of embeddedness.
</bodyText>
<subsectionHeader confidence="0.990517">
2.4 Majority Consensus
</subsectionHeader>
<bodyText confidence="0.999928">
A straightforward extension to the CNS approach is
to relax the need for full agreement and include
those segments on which a majority of the annotators
agreed (Grosz and Hirschberg, 1992). Other thresholds
of agreement could be used as well, but in this paper we
</bodyText>
<figure confidence="0.507901">
1 2 3 GS
</figure>
<figureCaption confidence="0.998124">
Figure 3: An example of the FullMCNS approach.
</figureCaption>
<bodyText confidence="0.999904545454546">
only consider it an agreement when more than 50% of
annotators agree on a segment. We call this the Major-
ity Consensus (MCNS) approach. As with CNS, we can
have both a “full” version (FullMCNS) and a “flat” ver-
sion (FlatMCNS), which do and do not consider the level
of embeddedness, respectively.
Figure 3 shows an example of performing FullMCNS
on the same three annotations we saw in Figure 1. Here,
we again include the largest segment because it is agreed
upon by all, but now we also include the two segments
agreed upon by annotators 1 and 3 because two out of
three annotators, a majority, have selected them. These
two segments correspond to segments B and C for anno-
tator 1 and segments I and J for annotator 3.
MCNS is less strict than CNS as it includes segments
agreed upon by most annotators and does not require full
agreement, but both methods are affected by a potential
flaw. Note that in Figure 3, segment D could very well
be in some notion of agreement with annotation 3, but
MCNS does not capture this near-miss; D is left out of the
gold standard. The next approach we discuss can handle
this sort of situation.
</bodyText>
<subsectionHeader confidence="0.925081">
2.5 Conflict-Free Union
</subsectionHeader>
<bodyText confidence="0.9999339">
The Conflict-Free Union (CFU) approach combines the
annotations of all of the annotators and removes those
segments that conflict with each other to get a conflict-
free gold standard. There are usually multiple ways to
construct a conflict-free gold standard. The CFU ap-
proach finds the one with the fewest segments removed.
Figure 4 shows the use of CFU on the three example
annotations. Notice that the only segments not included
in the gold standard are F and G, which conflict with B,
C, D, I, and J. Resolving the conflicts here required re-
moving two segments; the other way to resolve the con-
flict would have been to remove C and J, which would
be equally as good. CFU captures as many conflict-free
segments from the annotations as possible without dis-
crimination. Even if only one annotator chose a segment,
CFU would include it if it did not create more conflicts.
Hence, it is likely that CFU could construct gold stan-
dards with too much structure. However, in our example
it is better at capturing the similarity of structure between
annotators 1 and 3. Due to its ability to capture structure,
</bodyText>
<figure confidence="0.999510275862069">
A B E F H I
C J
G
D
H I
J
F
G
A B
C
D
E
E
F
G
J
H I
A B
C
D
E
J
H I
F
G
A B
C
D
1 2 3 GS
</figure>
<figureCaption confidence="0.999825">
Figure 4: An example of the CFU approach.
</figureCaption>
<bodyText confidence="0.868017">
we expected that CFU would perform better in recall than
the previously mentioned approaches.
</bodyText>
<subsubsectionHeader confidence="0.517703">
2.5.1 CFU Algorithm
</subsubsectionHeader>
<bodyText confidence="0.998726095238095">
The consensus and majority approaches are straight-
forward to compute, but CFU presents an optimization
problem in which the greatest number of segments that
can be combined without any internal conflicts must be
found. Brute force methods, such as trying every possible
set of segments and picking the largest conflict-free set,
grow exponentially in the total number of segments con-
tained in the annotations. We present a dynamic program-
ming algorithm that computes the CFU in time,
where is the number of utterances in the discourse.
First, we say that segment straddles an utterance
when . Let represent the number
of segments between utterances and , inclusive, that
straddle . That is, represents the number of unique
segments with the form , where
and . We use to compute , the index repre-
senting the utterance that, if considered a new bound-
ary utterance, would minimize the number of conflicting
segments within and , and , that minimum num-
ber of segments. Then we can solve the following recur-
rence equations:
</bodyText>
<figure confidence="0.511987">
1 2 3 GS
</figure>
<figureCaption confidence="0.996896">
Figure 5: An example of the FullUNI approach.
</figureCaption>
<subsectionHeader confidence="0.954533">
2.6 Union
</subsectionHeader>
<bodyText confidence="0.999194388888889">
The methods for finding a gold standard in Sections 2.1-
2.4 produce segmentations that contain no internal con-
flicts.3 However, since we evaluate a gold standard by its
similarity with the original annotations, it makes sense to
define an approach that is capable of constructing a uni-
fied segmentation that includes conflicts, which we call
the Union approach (UNI). UNI simply includes every
segment from every annotator. The flat version ignores
hierarchies (FlatUNI), and the full version includes them
(FullUNI).
An example of an application of FullUNI is given in
Figure 5. We see that every segment chosen by annotators
1, 2, and 3 have been included in the gold standard, cre-
ating some internal conflicts. We certainly would not ex-
pect to use this construction as a prediction of the actual
gold standard, but we include it for comparison with CFU
in evaluating the importance of avoiding internal conflicts
with respect to our metrics.
</bodyText>
<subsectionHeader confidence="0.997299">
2.7 Best Annotator
</subsectionHeader>
<bodyText confidence="0.999707892857143">
The final approach we considered chooses the “best” an-
notation and considers it to be the gold standard. We se-
lect the annotation with the highest inter-labeler reliabil-
ity with all the other annotations to be the “best” annota-
tion, using the pairwise metric. We discuss this metric
and its uses in Section 3.
where 3 Measures of Evaluation
We can generate a binary tree with as the value
of the root node, representing the segment over all utter-
ances. The left child, then, has the value , and
the right child has value . We compute the
rest of the tree similarly, with as the left child
of the left child, and so on. For each segment included by
an annotator, we include it in the gold standard if it is
represented by a segment in the constructed tree. Note
that we only store the boundary utterances in the tree, so
the gold standard we construct will not include level of
embeddedness.
There are several ways of evaluating an algorithm for
creating a gold standard, just as there are several ways
of evaluating any segmentation algorithm. Ideally, we
would like to compare to some objectively true gold stan-
dard, but it is impossible to determine if there are one or
more true standards, or even if one exists. Instead, we
can compare a gold standard against each annotator’s in-
dividual structuring, or against that of several human an-
notators collectively. Also, we could compare gold stan-
dards with each other in terms of how they affect the out-
</bodyText>
<footnote confidence="0.5641334">
3MCNS avoids conflicts because any two segments that a
majority of annotators agree upon will always both be included
by at least one annotator, and we assume that individual anno-
tations are always internally consistent.
1 2
</footnote>
<figureCaption confidence="0.9950545">
Figure 6: The metric would consider these two segmen-
tations in agreement.
</figureCaption>
<bodyText confidence="0.999364615384615">
come of some computational task which considers dis-
course structure, such as anaphora resolution. This last
approach is probably the best when the purpose of the
gold standard is known in advance, but in this paper we
consider only task-independent metrics.
For the sake of scientific validity, we did not compare
a gold standard with a segmentation of our own. Instead,
we chose to evaluate gold standards by averaging their
similarity to the original segmentations made by human
annotators. For each approach we presented earlier, we
report an average similarity score over all original seg-
mentations and the gold standard, based on several dif-
ferent quantitative measures of inter-reliability.
</bodyText>
<subsectionHeader confidence="0.997983">
3.1 Pairwise Agreement Scores
</subsectionHeader>
<bodyText confidence="0.982330283018868">
For linear segmentation, pairwise agreement between an-
notators is computed by dividing the number of utter-
ances for which both annotators agree by the total number
of utterances. In contrast, a hierarchical segmentation for
a sequence of utterance in a discourse is analogous to a
parse tree for a sequence of words. It requires a different
metric for pairwise agreement that considers the hierar-
chy.
Following Flammia and Zue (1995), we define a gen-
eral symmetric metric for observed agreement be-
tween two segments that accounts both for deletions and
for insertions of segments in a hierarchy. Intuitively,
we want different sub-trees that vary only in hierarchical
structure but share the same boundaries to0 be considered
similar. For example, in Figure 6, there is good reason to
consider both annotations to be similar, even though no
segment pair in either spans the same utterances.
Formally, let and be two possible segmentations.
A segment in matches with segmentation if
there exists some segment or in and
there exists some segment or in . In
other words, a segment in matches a segmentation
if the utterances that constitute its boundaries also con-
stitute boundaries for some segment in . For example,
in Figure 5, we consider that the segments and
in annotation 3 match the segments and in
annotation 1.
Flammia and Zue then let be the number of seg-
ments in that match with segments in and let
be the number of segments in that match with seg-
ments in . and are the number of segments in
and respectively. Following Bakeman and Gottman
(1986), they define the observed agreement to be
For the metric to be valid, they also take into account
the probability of chance agreement between annotators.
For example, if the distribution underlying the segmenta-
tion is skewed such that the structure is very sparse, most
segmentations will include very few constituents, and
will be unnaturally deflated.
The kappa coefficient ( ) is used for correcting the ob-
served agreement by subtracting the probability that
two segments in and , chosen at random, happen to
agree. The coefficient is computed as follows:
Carletta (1996) reports that content analysis researchers
generally think of as “good reliability,” with
allowing “tentative conclusions to be
drawn.”
All that remains is to define the chance agreement
probability . Let and be the fraction of
utterances that begin or end one or more segments in seg-
mentation respectively. Flammia and Zue compute an
upper bound on as
where .
</bodyText>
<subsectionHeader confidence="0.999058">
3.2 Precision and Recall
</subsectionHeader>
<bodyText confidence="0.999845192307692">
We use standard evaluation metrics from information re-
trieval to measure pairwise agreement between gold stan-
dards and annotations. We say that segment
in some segmentation flatly agrees with segmentation
if there exists a segment in , which spans
exactly the same utterances as . We say that segment
in some segmentation fully agrees with seg-
mentation if flatly agrees with , and the segment
that fits it is also of the same depth as ; i.e., there exists
a segment in .
We define the number of relevant segments in a seg-
mentation to be the total number of segments in
that agree with a gold standard for that particular dis-
course. For gold standard types that consider embed-
dedness, such as Full Consensus and Full Majority Con-
sensus, we check for full agreement. For gold standard
types that do not, such as Flat Consensus and Conflict-
Free Union, we consider flat agreement.
We define recall as the number of relevant segments
in divided by the total number of segments in . We
define precision as the number of relevant segments in
divided by the total number of segments in the gold stan-
dard. Intuitively, if a gold standard has low agreement
with the original segmentation, recall will be low. If a
gold standard’s structure is more dense then the original
segmentation, precision will be low.
</bodyText>
<subsectionHeader confidence="0.998871">
3.3 Non-Crossing-Brackets
</subsectionHeader>
<bodyText confidence="0.999971076923077">
The non-crossing-bracket measure is a common perfor-
mance metric used in syntactic parsing for measuring
hierarchical structure similarity. A segment constituent
in some segmentation crosses brackets with
segmentation if spans at least one boundary utterance
in .
For each segmentation , we define the number of
non-crossing-brackets as the number of segments in
that do not exhibit crossing brackets with the appropri-
ate gold standard. For each segmentation, we compute a
non-crossing-bracket percentage by dividing the number
of non-crossing-brackets by the total number of bracket
pairs.
</bodyText>
<sectionHeader confidence="0.988243" genericHeader="method">
4 Empirical Methodology
</sectionHeader>
<subsectionHeader confidence="0.996787">
4.1 Boston Directions Corpus
</subsectionHeader>
<bodyText confidence="0.999970833333334">
For our empirical analysis of different gold standard ap-
proaches, we used the Boston Directions Corpus (BDC).
The BDC corpus contains transcribed monologues by
speakers who were instructed to perform a series of
direction-giving tasks. The monologues were subse-
quently annotated by a group of subjects according to the
Grosz and Sidner (1986) theory of discourse structure.
This theory provides a foundation for hierarchical seg-
mentation of discourses into constituent parts. Some of
the subjects were experts in discourse theory and others
were naive annotators. In our experiments here, we only
consider the annotations from experts.
</bodyText>
<subsectionHeader confidence="0.973014">
4.2 Experimental Design
</subsectionHeader>
<bodyText confidence="0.999021363636364">
Our experiments were run on 12 discourses in the sponta-
neous speech component of the BDC. The lengths of the
discourses ranged from 15 to 150 intonational phrases.
Each discourse was segmented by three different annota-
tors, resulting in 36 separate annotations. For each dis-
course, we combined the three annotations into a gold
standard according to each technique described in Sec-
tion 2. We then proceeded to compute the similarity be-
tween the gold standard and each of the original annota-
tions by using the pairwise evaluation metrics described
in Section 3.
</bodyText>
<figureCaption confidence="0.99414">
Figure 7: Results — Pairwise Agreement Scores
</figureCaption>
<subsectionHeader confidence="0.842694">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.999782470588235">
We report results for each gold standard averaged over
all 36 annotations. Table 1 presents precision/recall per-
centages for pairwise agreement scores, as well as val-
ues and non-crossing brackets (NCB) percentages. Fig-
ure 7 plots the pairwise agreement precision/recall values
on a graph, with error bars indicating one standard de-
viation from the mean. Recall that the gold standards
we are comparing are Full Consensus (FullCNS), Flat
Consensus (FlatCNS), Full Majority Consensus (FullM-
CNS), Flat Majority Consensus (FlatMCNS), Conflict
Free Union (CFU), Full Union (FullUNI), Flat Union
(FlatUNI) and Best Annotator.
Our results show that CFU, FullUNI and FlatUNI all
achieved high scores and low variance. Both Full
and Flat Consensus scored the worst. This pattern was
also apparent with regard to agreement between the gold
standard and the annotations. Again, CFU, FullUNI
and FlatUNI achieved the best recall, and FullCNS and
FlatCNS scored the worst recall. It is interesting to point
out that since any segmentation proposed by an evaluator
will always be included in the FullUNI gold standard, its
agreement recall will always be 1.
We see a change in trend with regard to precision be-
tween gold standard and the annotations. Here, FullCNS
and FlatCNS achieved very high precision, while Full-
UNI and FlatUNI achieved low precision. CFU’s preci-
sion was slightly better. With respect to the non-crossing-
brackets metrics, the gold standards based on consensus
(FullCNS, FlatCNS) did not clash at all with any anno-
tation, since any segment in the gold standard is present
in each of the annotations. Of the remaining methods,
FlatMCNS (0.84) and FullMCNS (0.81) had the high-
est percentage of non-crossing-brackets, while the union
based approaches, FullUNI (0.47) and FlatUNI (0.54)
</bodyText>
<table confidence="0.9974799">
Agreement Rec. Agreement Pre. NCB
ave. sd. ave. sd. ave. sd. ave. sd
FullCNS .25 .63 .15 .34 1 0 1 0
FlatCNS .48 .42 .27 .32 .81 .60 1 0
FullMCNS .67 .32 .43 .39 .75 .25 .81 .32
FlatMCNS .79 .21 .59 .20 .71 .89 .84 .12
CFU .84 .08 .91 .89 .50 .11 .78 .19
FullUNI .84 .08 1 0 .44 .16 .53 .20
FlatUNI .84 .09 .98 .01 .49 .06 .46 .09
BestAnnotator .85 .15 .55 .45 .52 .47 .62 .33
</table>
<tableCaption confidence="0.999933">
Table 1: Experimental results.
</tableCaption>
<bodyText confidence="0.999868076923077">
had the lowest, because their gold standards are densely
structured and internally include conflicts.
Looking at each gold standard separately, we do not
identify a single gold standard that does well across the
board. CFU, FullUNI and FlatUNI have high and
agreement recall values, but they all have low agree-
ment precision values. FullCNS and FlatCNS have low
and recall values, but better agreement precision values.
FullMCNS and FlatMCNS average out the best across all
metrics, but they do not achieve the best performance in
any of the metrics. Note that “full” type methods require
agreement in hierarchy; they are held to a higher standard
of evaluation than “flat” type methods.
</bodyText>
<sectionHeader confidence="0.99965" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999991445783133">
From the results, we see that generally the consensus-
type approaches (CNS and MCNS) perform very well
with the precision metric and the union approaches (CFU,
UNI) perform well with the recall and metrics. Preci-
sion measures the percentage of the gold standard that
was agreed upon by the annotators, and since the con-
sensus approaches tend to include only those segments
labeled by everyone, they have high precision. Specifi-
cally, FullCNS performs perfectly in precision because it
contains only those segments explicitly included by ev-
eryone, while the majority consensus methods perform
slightly worse because an annotator is occasionally in the
minority.
Recall measures the percentage of the annotator’s seg-
ments captured by the gold standard. Since the union
approaches include every or almost every segment, de-
pending on whether it is “flat” or “full,” respectively, an
annotator’s segment is almost always included in the gold
standard, yielding high recall for these methods. The dif-
ference between precision and recall highlights two dif-
ferent approaches: precision encourages a bottom-up ap-
proach where the most likely segments to be included in
the gold standard are added from scratch; recall encour-
ages a top-down approach where all possible segments
are added and the least likely segments to be included in
the gold standard are pruned. The metric attempts to
balance these two approaches by rewarding agreements
yet penalizing extra structure. Nevertheless, even the
naive union methods (UNI) performs well with , indi-
cating that it favors agreement far more than it punishes
extra structure.
Based on these observations, we believe that there is
good reason to prefer to use CFU as a gold standard over
FullUNI and FlatUNI. Although they all have the same
and similar precision/recall values, the CFU gold stan-
dard corresponds to a true segmentation — it does not
exhibit internal conflicts.
However, if a conservative but accurate gold standard
is desired, then the MCNS approaches are the best all-
around consensus approaches to use, as they perform
fairly well with as well as with precision and recall.
These approaches construct fairly conservative gold stan-
dards, but not nearly as strict as the full consensus ap-
proaches. Hence, as seen by the high precision value,
a gold standard constructed by an MCNS method will
contain mostly relevant segments but will be missing the
more controversial segments.
The Best Annotator approach performed very well
with , but not as well with respect to precision and re-
call. Its performance was completely dominated by the
MCNS approaches in all metrics, except for . In gen-
eral, is at its highest when minor boundary disagree-
ments are infrequent, because it is not sensitive to the
exact type of matching boundaries. This phenomenon
is shown in Figure 6. There, we see two segmentations
that are clearly different but are considered the same by
. Precision and recall, however, would not consider the
second level segments in agreement.
The consistently good results of the non-crossing-
brackets metric for MCNS and CFU indicate that there
are few cases in which the expert BDC annotators cre-
ate segments whose boundaries cross. Again, this effect
is probably a result of the well-structured nature of the
tasks in the BDC discourses. Since there are few cross-
ing boundaries, the metric performs well for the Union
and Best Annotator methods since almost every bound-
ary is represented. If annotations had exhibited more dis-
crepancy, the non-crossing-brackets and metrics would
probably differentiate more among these approaches.
Lastly, we note that the difference between “full” and
“flat” metrics of the same type were insignificant, but
with the consensus approaches, the “flat” approaches per-
formed slightly better than their “full” counterparts, most
likely because the “full” approaches were too conserva-
tive in demanding level agreement. Thus, if we care not to
have conflicts in our gold standard, the “full” approaches
should be used to find the gold standard, as they produce
more structured segmentations. In addition, a gold stan-
dard with labeled embeddedness might be necessary for
post-segmentation processing, such as anaphora resolu-
tion. However, if the gold standard is being used for
purely evaluative reasons, the “flat” approaches should
be used as they perform slightly better.
</bodyText>
<sectionHeader confidence="0.99985" genericHeader="discussions">
6 Future Work
</sectionHeader>
<bodyText confidence="0.999953608695652">
One problem with the measures of evaluation that we
have explored in this paper is that they tell us how similar
a gold standard is to the original annotations but say noth-
ing about how effective the gold standard would be when
used for further discourse processing. One suggestion
for future studies would be to evaluate the gold standards
with respect to possible post-segmentation tasks, such as
anaphora resolution or summarization. Such an approach
would be a better measure of the objective goodness of
the gold standard and could also be a way to monitor
the skills of the annotators. Specific metrics might also
be more relevant for a specific discourse task. For in-
stance, perhaps non-crossing-brackets is a more useful
metric to consider when segmenting as a preprocessing
step for anaphora resolution.
It would also be interesting to further explore the
Conflict-Free Union approach, as it performed well but
suffered from including extra structure. The top-down
processing could be enhanced by removing those seg-
ments which are deemed the least probable to be in the
gold standard, perhaps based on some features, such as
depth. For example, perhaps a segment that is at a deep
level and is only in a few annotations could get removed,
while larger segments would remain regardless, or vice
versa. With a few good features, it seems quite possi-
ble to increase the precision of CFU. A similar approach
could be taken to add new segments to those picked in the
Majority Consensus approach.
Finally, it is worth exploring whether it is a good idea
to have multiple annotations for a given corpus in the first
place. Some corpora, such as the Penn Treebank, require
its annotators to meet whenever there is a conflict so that
the conflict can be resolved before the corpus is publicly
released. Penn has now begun a Discourse Treebank as
well (Creswell et al., 2003). Wiebe et al. (1999) use sta-
tistical methods to automatically correct the biases in an-
notations of speaker subjectivity. The corrections are then
used as a basis for further conflict resolution. Carlson
et al. (2001) also used conflict resolution when creating
their discourse-tagged corpus. One interesting area of re-
search would be to compare how annotators choose to re-
solve their conflicts compared to the different automatic
approaches of finding a gold standard. It is possible that
the compromises made by the annotators cannot be cap-
tured by any computational method, in which case it may
be worth having all conflicts resolved manually.
</bodyText>
<sectionHeader confidence="0.998597" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998456">
We would like to thank Jill Nickerson for comments on
an earlier draft of this paper. This work is supported in
part by the National Science Foundation under Grant No.
IIS-0222892, the GK-12 Fellowship (NSF 04-533), and
NSF Career Award IIS-0091815.
</bodyText>
<sectionHeader confidence="0.999043" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998900170212766">
R. Bakeman and J.M. Gottman. 1986. Observing inter-
actions: an introduction to sequential analysis. Cam-
bridge University Press.
J. Carletta. 1996. Assessing agreement on classification
tasks: The kappa statistic. Computational Linguistics,
22(2).
L. Carlson, D. Marcu, and M. E. Okurowski. 2001.
Building a discourse-tagged corpus in the framework
of rhetorical structure theory. In Proc. of the 2nd
SIGDIAL Workshop on Discourse and Dialogue, Eu-
rospeech 2001, Denmark, September.
C. Creswell, K. Forbes, E. Miltsakaki, R. Prasad,
A. Joshi, and B. Webber. 2003. Penn discourse tree-
bank: Building a large scale annotated corpus encod-
ing dltag-based discourse structure and discourse rela-
tions. Manuscript [fix this].
G. Flammia and V. Zue. 1995. Empirical evalua-
tion of human performance and agreement in parsing
discourse constituents in spoken dialogue. In Proc.
Euroospeech-95, volume 3, pages 1965–1968, Madrid,
Spain.
B.J. Grosz and J. Hirschberg. 1992. Some intona-
tional characteristics of disourse structure. In Proc. of
ICSLP-92, volume 1.
B.J. Grosz and C.L. Sidner. 1986. Attention, intentions
and the structure of discourse. Computational Linguis-
tics, 12:175–204.
M. Hearst. 1997. Texttiling: Segmenting text into multi-
paragraph subtopic passages. Computational Linguis-
tics, 23:33–64.
J. Hirschberg and C. Nakatani. 1996. A prosodic anal-
ysis of discourse segments in direction-giving mono-
logues. In Proc. ofACL-1996, Santa Cruz.
D. Marcu. 2000. The Theory and Practice of Discourse
Parsing and Summarization. The MIT Press, Novem-
ber.
R.J. Passonneau and D.J. Litman. 1993. Intention-based
segmentation: Human reliability and correlation with
linguistic cues. In Meeting of the Association for Com-
putational Linguistics, pages 148–155.
J.A. Rotondo. 1984. Clustering analysis of subject parti-
tions of text. Discourse Processes, 7:69–88.
J. Wiebe, R. Bruce, and T. O’Hara. 1999. Development
and use of a gold-standard data set for subjectivity clas-
sifications. In Proc. 37th Annual Meeting of the Assoc.
for Computational Linguistics (ACL-99), pages 246–
253, University of Maryland, June.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.486279">
<title confidence="0.988237">Unifying Annotated Discourse Hierarchies to Create a Gold Standard</title>
<author confidence="0.904655">Ya’akov Gal Carbone</author>
<author confidence="0.904655">Stuart Shieber</author>
<affiliation confidence="0.824493">Division of Engineering and Applied Harvard</affiliation>
<address confidence="0.9791535">33 Oxford Cambridge, MA</address>
<email confidence="0.999494">mcarbone,gal,shieber,grosz@eecs.harvard.edu</email>
<abstract confidence="0.994900655172414">Human annotation of discourse corpora typically results in segmentation hierarchies that vary in their degree of agreement. This paper presents several techniques for unifying multiple discourse annotations into a single hierarchy, deemed a “gold standard” — the segmentation that best captures the underlying linguistic structure of the discourse. It proposes and analyzes methods that consider the level of embeddedness of a segmentation as well as methods that do not. A corpus containing annotated hierarchical discourses, the Boston Directions Corpus, was used to evaluate the “goodness” of each technique, by comparing the similarity of the segmentation it derives to the original annotations in the corpus. Several metrics of similarity between hierarchical segmentations are computed: precision/recall of matching utterances, pairwise inter-reliability scores ( ), and non-crossing-brackets. A novel method for unification that minimizes conflicts among annotators outperforms methods that require consensus among a majority for the and precision metrics, while capturing much of the structure of the discourse. When high recall is preferred, methods requiring a majority are preferable to those that demand full consensus among annotators.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Bakeman</author>
<author>J M Gottman</author>
</authors>
<title>Observing interactions: an introduction to sequential analysis.</title>
<date>1986</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="19836" citStr="Bakeman and Gottman (1986)" startWordPosition="3262" endWordPosition="3265">entations. A segment in matches with segmentation if there exists some segment or in and there exists some segment or in . In other words, a segment in matches a segmentation if the utterances that constitute its boundaries also constitute boundaries for some segment in . For example, in Figure 5, we consider that the segments and in annotation 3 match the segments and in annotation 1. Flammia and Zue then let be the number of segments in that match with segments in and let be the number of segments in that match with segments in . and are the number of segments in and respectively. Following Bakeman and Gottman (1986), they define the observed agreement to be For the metric to be valid, they also take into account the probability of chance agreement between annotators. For example, if the distribution underlying the segmentation is skewed such that the structure is very sparse, most segmentations will include very few constituents, and will be unnaturally deflated. The kappa coefficient ( ) is used for correcting the observed agreement by subtracting the probability that two segments in and , chosen at random, happen to agree. The coefficient is computed as follows: Carletta (1996) reports that content ana</context>
</contexts>
<marker>Bakeman, Gottman, 1986</marker>
<rawString>R. Bakeman and J.M. Gottman. 1986. Observing interactions: an introduction to sequential analysis. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: The kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="20411" citStr="Carletta (1996)" startWordPosition="3356" endWordPosition="3357">ly. Following Bakeman and Gottman (1986), they define the observed agreement to be For the metric to be valid, they also take into account the probability of chance agreement between annotators. For example, if the distribution underlying the segmentation is skewed such that the structure is very sparse, most segmentations will include very few constituents, and will be unnaturally deflated. The kappa coefficient ( ) is used for correcting the observed agreement by subtracting the probability that two segments in and , chosen at random, happen to agree. The coefficient is computed as follows: Carletta (1996) reports that content analysis researchers generally think of as “good reliability,” with allowing “tentative conclusions to be drawn.” All that remains is to define the chance agreement probability . Let and be the fraction of utterances that begin or end one or more segments in segmentation respectively. Flammia and Zue compute an upper bound on as where . 3.2 Precision and Recall We use standard evaluation metrics from information retrieval to measure pairwise agreement between gold standards and annotations. We say that segment in some segmentation flatly agrees with segmentation if there </context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>J. Carletta. 1996. Assessing agreement on classification tasks: The kappa statistic. Computational Linguistics, 22(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Carlson</author>
<author>D Marcu</author>
<author>M E Okurowski</author>
</authors>
<title>Building a discourse-tagged corpus in the framework of rhetorical structure theory.</title>
<date>2001</date>
<booktitle>In Proc. of the 2nd SIGDIAL Workshop on Discourse and Dialogue, Eurospeech</booktitle>
<contexts>
<context position="2887" citStr="Carlson et al., 2001" startWordPosition="412" endWordPosition="415">er manually by the annotators’ discussions or automatically. However, annotation unification approaches have not been formally evaluated, and although manual unification might be the best approach, it can be time-consuming. Indeed, much of the work on automatic recognition of discourse structure has focused on linear, rather than hierarchical segmentation (Hearst, 1997; Hirschberg and Nakatani, 1996), because of the difficulties of obtaining consistent hierarchical annotations. In addition, those approaches that do handle hierarchical segmentation do not address automatic unification methods (Carlson et al., 2001; Marcu, 2000). There are several reasons for the prevailing emphasis on linear annotation and the lack of work on automatic methods for unifying hierarchical discourse annotations. First, initial attempts to create annotated hierarchical corpora of discourse structure using naive annotators have met with difficulties. Rotondo (1984) reported that “hierarchical segmentation is impractical for naive subjects in discourses longer than 200 words.” Passonneau and Litman (1993) conducted a pilot study in which subjects found it “difficult and time-consuming” to identify hierarchical relations in di</context>
<context position="33409" citStr="Carlson et al. (2001)" startWordPosition="5473" endWordPosition="5476"> Majority Consensus approach. Finally, it is worth exploring whether it is a good idea to have multiple annotations for a given corpus in the first place. Some corpora, such as the Penn Treebank, require its annotators to meet whenever there is a conflict so that the conflict can be resolved before the corpus is publicly released. Penn has now begun a Discourse Treebank as well (Creswell et al., 2003). Wiebe et al. (1999) use statistical methods to automatically correct the biases in annotations of speaker subjectivity. The corrections are then used as a basis for further conflict resolution. Carlson et al. (2001) also used conflict resolution when creating their discourse-tagged corpus. One interesting area of research would be to compare how annotators choose to resolve their conflicts compared to the different automatic approaches of finding a gold standard. It is possible that the compromises made by the annotators cannot be captured by any computational method, in which case it may be worth having all conflicts resolved manually. Acknowledgments We would like to thank Jill Nickerson for comments on an earlier draft of this paper. This work is supported in part by the National Science Foundation un</context>
</contexts>
<marker>Carlson, Marcu, Okurowski, 2001</marker>
<rawString>L. Carlson, D. Marcu, and M. E. Okurowski. 2001. Building a discourse-tagged corpus in the framework of rhetorical structure theory. In Proc. of the 2nd SIGDIAL Workshop on Discourse and Dialogue, Eurospeech 2001, Denmark, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Creswell</author>
<author>K Forbes</author>
<author>E Miltsakaki</author>
<author>R Prasad</author>
<author>A Joshi</author>
<author>B Webber</author>
</authors>
<title>Penn discourse treebank: Building a large scale annotated corpus encoding dltag-based discourse structure and discourse relations. Manuscript [fix this].</title>
<date>2003</date>
<contexts>
<context position="33192" citStr="Creswell et al., 2003" startWordPosition="5438" endWordPosition="5441">e larger segments would remain regardless, or vice versa. With a few good features, it seems quite possible to increase the precision of CFU. A similar approach could be taken to add new segments to those picked in the Majority Consensus approach. Finally, it is worth exploring whether it is a good idea to have multiple annotations for a given corpus in the first place. Some corpora, such as the Penn Treebank, require its annotators to meet whenever there is a conflict so that the conflict can be resolved before the corpus is publicly released. Penn has now begun a Discourse Treebank as well (Creswell et al., 2003). Wiebe et al. (1999) use statistical methods to automatically correct the biases in annotations of speaker subjectivity. The corrections are then used as a basis for further conflict resolution. Carlson et al. (2001) also used conflict resolution when creating their discourse-tagged corpus. One interesting area of research would be to compare how annotators choose to resolve their conflicts compared to the different automatic approaches of finding a gold standard. It is possible that the compromises made by the annotators cannot be captured by any computational method, in which case it may be</context>
</contexts>
<marker>Creswell, Forbes, Miltsakaki, Prasad, Joshi, Webber, 2003</marker>
<rawString>C. Creswell, K. Forbes, E. Miltsakaki, R. Prasad, A. Joshi, and B. Webber. 2003. Penn discourse treebank: Building a large scale annotated corpus encoding dltag-based discourse structure and discourse relations. Manuscript [fix this].</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Flammia</author>
<author>V Zue</author>
</authors>
<title>Empirical evaluation of human performance and agreement in parsing discourse constituents in spoken dialogue.</title>
<date>1995</date>
<booktitle>In Proc. Euroospeech-95,</booktitle>
<volume>3</volume>
<pages>1965--1968</pages>
<location>Madrid,</location>
<contexts>
<context position="3940" citStr="Flammia and Zue, 1995" startWordPosition="563" endWordPosition="566"> than 200 words.” Passonneau and Litman (1993) conducted a pilot study in which subjects found it “difficult and time-consuming” to identify hierarchical relations in discourse. Other attempts have had more success using improved annotation tools and more precise instructions (Grosz and Hirschberg, 1992; Hirschberg and Nakatani, 1996). Second, hierarchical segmentation of discourse is subjective. While agreement among annotators regarding linear segmentation has been found to be higher than 80% (Hearst, 1997), with respect to hierarchical segmentation it has been observed to be as low as 60% (Flammia and Zue, 1995). Moreover, the precise definition of “agreement” with respect to hierarchical segmentation is unclear, complicating evaluation. It is natural to consider two segments in separate annotations to agree if they both span precisely the same utterances and agree on the level of embeddedness. However, it is less clear how to handle segments that share the same utterances but differ with respect to the level of embeddedness. In this paper, we show that despite these difficulties it is possible to automatically combine a set of multi-level discourse annotations together into a single gold standard, a</context>
<context position="18714" citStr="Flammia and Zue (1995)" startWordPosition="3068" endWordPosition="3071">port an average similarity score over all original segmentations and the gold standard, based on several different quantitative measures of inter-reliability. 3.1 Pairwise Agreement Scores For linear segmentation, pairwise agreement between annotators is computed by dividing the number of utterances for which both annotators agree by the total number of utterances. In contrast, a hierarchical segmentation for a sequence of utterance in a discourse is analogous to a parse tree for a sequence of words. It requires a different metric for pairwise agreement that considers the hierarchy. Following Flammia and Zue (1995), we define a general symmetric metric for observed agreement between two segments that accounts both for deletions and for insertions of segments in a hierarchy. Intuitively, we want different sub-trees that vary only in hierarchical structure but share the same boundaries to0 be considered similar. For example, in Figure 6, there is good reason to consider both annotations to be similar, even though no segment pair in either spans the same utterances. Formally, let and be two possible segmentations. A segment in matches with segmentation if there exists some segment or in and there exists so</context>
</contexts>
<marker>Flammia, Zue, 1995</marker>
<rawString>G. Flammia and V. Zue. 1995. Empirical evaluation of human performance and agreement in parsing discourse constituents in spoken dialogue. In Proc. Euroospeech-95, volume 3, pages 1965–1968, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
<author>J Hirschberg</author>
</authors>
<title>Some intonational characteristics of disourse structure.</title>
<date>1992</date>
<booktitle>In Proc. of ICSLP-92,</booktitle>
<volume>1</volume>
<contexts>
<context position="3622" citStr="Grosz and Hirschberg, 1992" startWordPosition="516" endWordPosition="519">rk on automatic methods for unifying hierarchical discourse annotations. First, initial attempts to create annotated hierarchical corpora of discourse structure using naive annotators have met with difficulties. Rotondo (1984) reported that “hierarchical segmentation is impractical for naive subjects in discourses longer than 200 words.” Passonneau and Litman (1993) conducted a pilot study in which subjects found it “difficult and time-consuming” to identify hierarchical relations in discourse. Other attempts have had more success using improved annotation tools and more precise instructions (Grosz and Hirschberg, 1992; Hirschberg and Nakatani, 1996). Second, hierarchical segmentation of discourse is subjective. While agreement among annotators regarding linear segmentation has been found to be higher than 80% (Hearst, 1997), with respect to hierarchical segmentation it has been observed to be as low as 60% (Flammia and Zue, 1995). Moreover, the precise definition of “agreement” with respect to hierarchical segmentation is unclear, complicating evaluation. It is natural to consider two segments in separate annotations to agree if they both span precisely the same utterances and agree on the level of embedde</context>
<context position="11114" citStr="Grosz and Hirschberg, 1992" startWordPosition="1738" endWordPosition="1741">ere the gold standards created by FlatCNS and FullCNS differ. Aside from the largest segment, FullCNS contains only the segment representing the agreement of segments D, G, and M. FlatCNS includes that segment as well, in addition to two more from the agreement of segments B, H, and K and segments C, I, and L. FullCNS does not include those segments because the segments occur at differing levels of embeddedness. 2.4 Majority Consensus A straightforward extension to the CNS approach is to relax the need for full agreement and include those segments on which a majority of the annotators agreed (Grosz and Hirschberg, 1992). Other thresholds of agreement could be used as well, but in this paper we 1 2 3 GS Figure 3: An example of the FullMCNS approach. only consider it an agreement when more than 50% of annotators agree on a segment. We call this the Majority Consensus (MCNS) approach. As with CNS, we can have both a “full” version (FullMCNS) and a “flat” version (FlatMCNS), which do and do not consider the level of embeddedness, respectively. Figure 3 shows an example of performing FullMCNS on the same three annotations we saw in Figure 1. Here, we again include the largest segment because it is agreed upon by </context>
</contexts>
<marker>Grosz, Hirschberg, 1992</marker>
<rawString>B.J. Grosz and J. Hirschberg. 1992. Some intonational characteristics of disourse structure. In Proc. of ICSLP-92, volume 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
<author>C L Sidner</author>
</authors>
<title>Attention, intentions and the structure of discourse.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<pages>12--175</pages>
<contexts>
<context position="1685" citStr="Grosz and Sidner, 1986" startWordPosition="239" endWordPosition="242"> computed: precision/recall of matching utterances, pairwise inter-reliability scores ( ), and non-crossing-brackets. A novel method for unification that minimizes conflicts among annotators outperforms methods that require consensus among a majority for the and precision metrics, while capturing much of the structure of the discourse. When high recall is preferred, methods requiring a majority are preferable to those that demand full consensus among annotators. 1 Introduction The linguistic structure of a discourse is composed of utterances that exhibit meaningful hierarchical relationships (Grosz and Sidner, 1986). Automatic segmentation of discourse forms the basis for many applications, from information retrieval and text summarization to anaphora resolution (Hearst, 1997). These automatic methods, usually based on supervised machine learning techniques, require a manually annotated corpus of data for training. The creation of these corpora often involves multiple judges annotating the same discourses, so as to avoid bias from using a single judge’s annotations as ground truth. Usually, for a particular discourse, these multiple annotations are unified into a single annotation, either manually by the</context>
<context position="23107" citStr="Grosz and Sidner (1986)" startWordPosition="3792" endWordPosition="3795">that do not exhibit crossing brackets with the appropriate gold standard. For each segmentation, we compute a non-crossing-bracket percentage by dividing the number of non-crossing-brackets by the total number of bracket pairs. 4 Empirical Methodology 4.1 Boston Directions Corpus For our empirical analysis of different gold standard approaches, we used the Boston Directions Corpus (BDC). The BDC corpus contains transcribed monologues by speakers who were instructed to perform a series of direction-giving tasks. The monologues were subsequently annotated by a group of subjects according to the Grosz and Sidner (1986) theory of discourse structure. This theory provides a foundation for hierarchical segmentation of discourses into constituent parts. Some of the subjects were experts in discourse theory and others were naive annotators. In our experiments here, we only consider the annotations from experts. 4.2 Experimental Design Our experiments were run on 12 discourses in the spontaneous speech component of the BDC. The lengths of the discourses ranged from 15 to 150 intonational phrases. Each discourse was segmented by three different annotators, resulting in 36 separate annotations. For each discourse, </context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>B.J. Grosz and C.L. Sidner. 1986. Attention, intentions and the structure of discourse. Computational Linguistics, 12:175–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Texttiling: Segmenting text into multiparagraph subtopic passages.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--33</pages>
<contexts>
<context position="1849" citStr="Hearst, 1997" startWordPosition="262" endWordPosition="263">among annotators outperforms methods that require consensus among a majority for the and precision metrics, while capturing much of the structure of the discourse. When high recall is preferred, methods requiring a majority are preferable to those that demand full consensus among annotators. 1 Introduction The linguistic structure of a discourse is composed of utterances that exhibit meaningful hierarchical relationships (Grosz and Sidner, 1986). Automatic segmentation of discourse forms the basis for many applications, from information retrieval and text summarization to anaphora resolution (Hearst, 1997). These automatic methods, usually based on supervised machine learning techniques, require a manually annotated corpus of data for training. The creation of these corpora often involves multiple judges annotating the same discourses, so as to avoid bias from using a single judge’s annotations as ground truth. Usually, for a particular discourse, these multiple annotations are unified into a single annotation, either manually by the annotators’ discussions or automatically. However, annotation unification approaches have not been formally evaluated, and although manual unification might be the</context>
<context position="3832" citStr="Hearst, 1997" startWordPosition="546" endWordPosition="547">84) reported that “hierarchical segmentation is impractical for naive subjects in discourses longer than 200 words.” Passonneau and Litman (1993) conducted a pilot study in which subjects found it “difficult and time-consuming” to identify hierarchical relations in discourse. Other attempts have had more success using improved annotation tools and more precise instructions (Grosz and Hirschberg, 1992; Hirschberg and Nakatani, 1996). Second, hierarchical segmentation of discourse is subjective. While agreement among annotators regarding linear segmentation has been found to be higher than 80% (Hearst, 1997), with respect to hierarchical segmentation it has been observed to be as low as 60% (Flammia and Zue, 1995). Moreover, the precise definition of “agreement” with respect to hierarchical segmentation is unclear, complicating evaluation. It is natural to consider two segments in separate annotations to agree if they both span precisely the same utterances and agree on the level of embeddedness. However, it is less clear how to handle segments that share the same utterances but differ with respect to the level of embeddedness. In this paper, we show that despite these difficulties it is possible</context>
</contexts>
<marker>Hearst, 1997</marker>
<rawString>M. Hearst. 1997. Texttiling: Segmenting text into multiparagraph subtopic passages. Computational Linguistics, 23:33–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hirschberg</author>
<author>C Nakatani</author>
</authors>
<title>A prosodic analysis of discourse segments in direction-giving monologues.</title>
<date>1996</date>
<booktitle>In Proc. ofACL-1996,</booktitle>
<location>Santa Cruz.</location>
<contexts>
<context position="2670" citStr="Hirschberg and Nakatani, 1996" startWordPosition="383" endWordPosition="386">iple judges annotating the same discourses, so as to avoid bias from using a single judge’s annotations as ground truth. Usually, for a particular discourse, these multiple annotations are unified into a single annotation, either manually by the annotators’ discussions or automatically. However, annotation unification approaches have not been formally evaluated, and although manual unification might be the best approach, it can be time-consuming. Indeed, much of the work on automatic recognition of discourse structure has focused on linear, rather than hierarchical segmentation (Hearst, 1997; Hirschberg and Nakatani, 1996), because of the difficulties of obtaining consistent hierarchical annotations. In addition, those approaches that do handle hierarchical segmentation do not address automatic unification methods (Carlson et al., 2001; Marcu, 2000). There are several reasons for the prevailing emphasis on linear annotation and the lack of work on automatic methods for unifying hierarchical discourse annotations. First, initial attempts to create annotated hierarchical corpora of discourse structure using naive annotators have met with difficulties. Rotondo (1984) reported that “hierarchical segmentation is imp</context>
<context position="8124" citStr="Hirschberg and Nakatani, 1996" startWordPosition="1220" endWordPosition="1223">reebank, there are multiple annotations for each discourse which were not manually combined into one gold standard annotation. In this paper, we explore automatic methods to create a gold standard for the BDC corpus. These methods could also be used on other corpora with non-unified annotations. Next, we present several automatic methods to combine multiple human-annotated discourse segmentations into one gold standard. 2.1 Flat vs. Hierarchical Approaches Most previous work that has combined multiple annotations has used linear segmentations, i.e. discourse segmentations without hierarchies (Hirschberg and Nakatani, 1996). In general, the hierarchical nature of discourse structure has not been considered when computing labeler inter-reliability and in evaluations of agreement with automatic methods. Since computational discourse theory relies on the hierarchy of its segments, we will consider it in this paper. For each approach that follows, we consider both a “flat” version, which does not consider level of embeddedness, and a “full” approach, which does. We analyze the differences between the flat and full versions for each approach. 2.2 Segment Definition A discourse is made up of a sequence of utterances, </context>
<context position="9709" citStr="Hirschberg and Nakatani, 1996" startWordPosition="1504" endWordPosition="1507"> 2.3 The Consensus Approach A conservative way to combine segmentations into a gold standard is the Consensus (CNS) (or raw agreement) ap2The levels are numbered from top to bottom; hence, 1 is the level of the largest segment, 2 is the level below that, and so on. 1 2 3 GS Figure 1: An example of the FullCNS approach. FlatCNS would create the same gold standard. The segments in the annotations that are marked in bold are those selected by the gold standard. A B E F H J K C I L D G M 1 2 3 Flat GS Full GS Figure 2: An example where FullCNS and FlatCNS create different gold standards.. proach (Hirschberg and Nakatani, 1996). CNS constructs a gold standard by including only those segments which have been included by every annotator. In the “full” version of CNS (FullCNS), the annotators need to agree upon the embedded level of the segment along with the segment boundaries. The “flat” version (FlatCNS) ignores hierarchy and considers only the segment boundaries when determining agreement. Figure 1 shows an example of performing FullCNS on three annotations. In the figure, all three annotators agree on only the largest segment (segments A, E, and H). Hence, the gold standard includes only that single segment. FlatC</context>
</contexts>
<marker>Hirschberg, Nakatani, 1996</marker>
<rawString>J. Hirschberg and C. Nakatani. 1996. A prosodic analysis of discourse segments in direction-giving monologues. In Proc. ofACL-1996, Santa Cruz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
</authors>
<title>The Theory and Practice of Discourse Parsing and Summarization.</title>
<date>2000</date>
<publisher>The MIT Press,</publisher>
<contexts>
<context position="2901" citStr="Marcu, 2000" startWordPosition="416" endWordPosition="417">otators’ discussions or automatically. However, annotation unification approaches have not been formally evaluated, and although manual unification might be the best approach, it can be time-consuming. Indeed, much of the work on automatic recognition of discourse structure has focused on linear, rather than hierarchical segmentation (Hearst, 1997; Hirschberg and Nakatani, 1996), because of the difficulties of obtaining consistent hierarchical annotations. In addition, those approaches that do handle hierarchical segmentation do not address automatic unification methods (Carlson et al., 2001; Marcu, 2000). There are several reasons for the prevailing emphasis on linear annotation and the lack of work on automatic methods for unifying hierarchical discourse annotations. First, initial attempts to create annotated hierarchical corpora of discourse structure using naive annotators have met with difficulties. Rotondo (1984) reported that “hierarchical segmentation is impractical for naive subjects in discourses longer than 200 words.” Passonneau and Litman (1993) conducted a pilot study in which subjects found it “difficult and time-consuming” to identify hierarchical relations in discourse. Other</context>
</contexts>
<marker>Marcu, 2000</marker>
<rawString>D. Marcu. 2000. The Theory and Practice of Discourse Parsing and Summarization. The MIT Press, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Passonneau</author>
<author>D J Litman</author>
</authors>
<title>Intention-based segmentation: Human reliability and correlation with linguistic cues.</title>
<date>1993</date>
<booktitle>In Meeting of the Association for Computational Linguistics,</booktitle>
<pages>148--155</pages>
<contexts>
<context position="3364" citStr="Passonneau and Litman (1993)" startWordPosition="479" endWordPosition="482"> annotations. In addition, those approaches that do handle hierarchical segmentation do not address automatic unification methods (Carlson et al., 2001; Marcu, 2000). There are several reasons for the prevailing emphasis on linear annotation and the lack of work on automatic methods for unifying hierarchical discourse annotations. First, initial attempts to create annotated hierarchical corpora of discourse structure using naive annotators have met with difficulties. Rotondo (1984) reported that “hierarchical segmentation is impractical for naive subjects in discourses longer than 200 words.” Passonneau and Litman (1993) conducted a pilot study in which subjects found it “difficult and time-consuming” to identify hierarchical relations in discourse. Other attempts have had more success using improved annotation tools and more precise instructions (Grosz and Hirschberg, 1992; Hirschberg and Nakatani, 1996). Second, hierarchical segmentation of discourse is subjective. While agreement among annotators regarding linear segmentation has been found to be higher than 80% (Hearst, 1997), with respect to hierarchical segmentation it has been observed to be as low as 60% (Flammia and Zue, 1995). Moreover, the precise </context>
</contexts>
<marker>Passonneau, Litman, 1993</marker>
<rawString>R.J. Passonneau and D.J. Litman. 1993. Intention-based segmentation: Human reliability and correlation with linguistic cues. In Meeting of the Association for Computational Linguistics, pages 148–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Rotondo</author>
</authors>
<title>Clustering analysis of subject partitions of text.</title>
<date>1984</date>
<booktitle>Discourse Processes,</booktitle>
<pages>7--69</pages>
<contexts>
<context position="3222" citStr="Rotondo (1984)" startWordPosition="461" endWordPosition="462">cal segmentation (Hearst, 1997; Hirschberg and Nakatani, 1996), because of the difficulties of obtaining consistent hierarchical annotations. In addition, those approaches that do handle hierarchical segmentation do not address automatic unification methods (Carlson et al., 2001; Marcu, 2000). There are several reasons for the prevailing emphasis on linear annotation and the lack of work on automatic methods for unifying hierarchical discourse annotations. First, initial attempts to create annotated hierarchical corpora of discourse structure using naive annotators have met with difficulties. Rotondo (1984) reported that “hierarchical segmentation is impractical for naive subjects in discourses longer than 200 words.” Passonneau and Litman (1993) conducted a pilot study in which subjects found it “difficult and time-consuming” to identify hierarchical relations in discourse. Other attempts have had more success using improved annotation tools and more precise instructions (Grosz and Hirschberg, 1992; Hirschberg and Nakatani, 1996). Second, hierarchical segmentation of discourse is subjective. While agreement among annotators regarding linear segmentation has been found to be higher than 80% (Hea</context>
</contexts>
<marker>Rotondo, 1984</marker>
<rawString>J.A. Rotondo. 1984. Clustering analysis of subject partitions of text. Discourse Processes, 7:69–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
<author>R Bruce</author>
<author>T O’Hara</author>
</authors>
<title>Development and use of a gold-standard data set for subjectivity classifications.</title>
<date>1999</date>
<booktitle>In Proc. 37th Annual Meeting of the Assoc. for Computational Linguistics (ACL-99),</booktitle>
<pages>246--253</pages>
<institution>University of Maryland,</institution>
<marker>Wiebe, Bruce, O’Hara, 1999</marker>
<rawString>J. Wiebe, R. Bruce, and T. O’Hara. 1999. Development and use of a gold-standard data set for subjectivity classifications. In Proc. 37th Annual Meeting of the Assoc. for Computational Linguistics (ACL-99), pages 246– 253, University of Maryland, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>