<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002828">
<title confidence="0.997996">
RoBox: CCG with Structured Perceptron for Supervised Semantic
Parsing of Robotic Spatial Commands
</title>
<author confidence="0.999369">
Kilian Evang Johan Bos
</author>
<affiliation confidence="0.999914">
University of Groningen University of Groningen
</affiliation>
<email confidence="0.986871">
k.evang@rug.nl johan.bos@rug.nl
</email>
<sectionHeader confidence="0.99721" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999919333333333">
We use a Combinatory Categorial Gram-
mar (CCG) parser with a structured per-
ceptron learner to address Shared Task 6
of SemEval-2014, Supervised Semantic
Parsing of Robotic Spatial Commands.
Our system reaches an accuracy of 79%
ignoring spatial context and 87% using
the spatial planner, showing that CCG can
successfully be applied to the task.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998871">
When interpreting utterances, humans use world
knowledge whereas most semantic parsers to date
rely purely on linguistic clues. Shared Task 6 in
the SemEval 2014 campaign for semantic evalua-
tion aims to integrate reasoning about microworlds
with semantic parsing. In this task, a system
is given an instruction for a robot and has to
produce an executable semantic representation in
Robot Control Language (Dukes, 2013a, RCL).
The Robot Commands Treebank (Dukes, 2013b)
is used for training and evaluation. We partici-
pated in this shared task with a system rooted in
Combinatory Categorial Grammar (CCG). In par-
ticular, we were interested in finding out whether
existing techniques for automatically deriving cat-
egorial grammars with semantics could be moved
easily to the new domain of robot commands
and integrated with the provided spatial reasoning
component. In this paper we outline our method
and present the results for this shared task. 1
</bodyText>
<sectionHeader confidence="0.865621" genericHeader="method">
2 Extracting a CCG from RCL
</sectionHeader>
<bodyText confidence="0.8629175">
CCGs (Steedman, 2001) use a small set of atomic
constituent categories such as 5 (sentence), NP
</bodyText>
<footnote confidence="0.883455166666667">
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
1Our code is available at http://www.let.rug.
nl/evang/RoBox.zip
</footnote>
<bodyText confidence="0.999868791666667">
(noun phrase) or PP (prepositional phrase). Con-
stituents that take other constituents as arguments
have complex categories describing their combi-
natory potential. For example, an intransitive En-
glish verb has category 5\NP, meaning that it
forms a sentence by combining with an NP to its
left. Similarly, modifiers also have complex cate-
gories. For example, a pre-sentential adverb might
have category 5/5 because it combines with a
sentence to its right to form a modified sentence.
The combinatory rules that license these exam-
ple combinations are called backward application
and forward application. They and other combi-
natory rules also allow for constituents to be asso-
ciated with semantic expressions, and specify how
to form a combined semantic expression for the
derived larger constituent.
In this section, we describe a process that takes
an RCL corpus as input and produces a set of CCG
lexical entries, i.e. natural-language words paired
with categories and semantic expressions. The
goal is for these lexical entries to produce the cor-
rect semantics under CCG combinatory rules also
for unseen robotic commands.
</bodyText>
<subsectionHeader confidence="0.997694">
2.1 Transforming the Trees
</subsectionHeader>
<bodyText confidence="0.999849733333333">
RCL expressions are rooted ordered trees whose
nodes are labeled with tags. We will write them in
the form (t:h) where t is the root tag and h is the
sequence of subtrees of the root’s children. Leaves
are abbreviated as just their tags. In each training
example, each pre-terminal (parent of a leaf) can
be aligned to one or more words in the correspond-
ing natural language expression. An example is
shown in Figure 1. Since the alignments to words
are not crossing, we can interpret the RCL tree as
a phrase structure tree for the sentence and use the
algorithm of (Hockenmaier and Steedman, 2007)
to translate it to CCG. We extend the algorithm
with a semantic step that makes sure the deriva-
tions would produce the original RCL expressions.
</bodyText>
<page confidence="0.980741">
482
</page>
<note confidence="0.7244995">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 482–486,
Dublin, Ireland, August 23-24, 2014.
</note>
<figure confidence="0.54566425">
(event:(action:move)(entity:(color:green)(type :prism))(destination:(spatial-relation :(relation:
within)(entity:(indicator:back)(indicator:left)(type:corner))))
event
event
</figure>
<figureCaption confidence="0.999899">
Figure 1: Top: an RCL expression. Middle: its representation as a tree diagram. Internal nodes are
annotated with constituent types. Pre-terminals are aligned to words in a corresponding natural-language
expression. Bottom: result of the CCG transformation.
</figureCaption>
<figure confidence="0.920217519230769">
type:h
prism
color:a
green
pyramid
green
entity:c
relation:h
within
in the
indicator:a
left
left
type:h
corner
corner
action:h
destination:c
spatial-relation:h
indicator:a
back
bottom
move
move the
entity:c
entity:
(entity : (type : prism))
entity/entity:
(color : green)
pyramid
move the
green
in the
bottom
left
corner
event/destination
(event/destination)/entity:
(event : (action : move))
entity
destination
spatial-relation
spatial-relation/entity:
(spatial-relation : (relation : within))
entity
entity/entity:
(indicator : back)
entity
entity/entity:
(indicator : left)
entity
(entity : (type : corner))
</figure>
<bodyText confidence="0.896963">
The procedure is as follows:
</bodyText>
<listItem confidence="0.997168">
1. Determine constituent types. We treat
action, relation and type constituents as heads,
entitys and destinations as complements (i.e.
arguments) and cardinals, colors, indicators,
measures and spatial-relations as adjuncts (i.e.
modifiers). For sequence nodes that have multiple
event children, we treat the first as head and the
rest as adjuncts. A corresponding constituent type
label h, a or c is added to the label of each internal
node (cf. Figure 1, middle).
2. Assign lexical semantics. To the label of
each pre-terminal, add an RCL expression which
is a copy of a connected subgraph of the tree itself
(without the constituent type labels). For a-type
and c-type pre-terminals, the subgraph includes
only the pre-terminal and its daughter. For h-type
pre-terminals the parent is also included, as well
as any subtrees with root tag id or reference-id
the parent may have.To illustrate, the label of
the action:h node in our example becomes ac-
tion:h:(event : (action: move)), and color:a be-
comes color:a:(color :green). The leaves are now
no longer needed, so we remove them.
3. Add sequence nodes. If the root is tagged
sequence, add an additional node tagged sequence
between each child and the root.
4. Binarize the tree. Each local tree with
</listItem>
<bodyText confidence="0.994372142857143">
more than two daughters is binarized by insert-
ing dummy nodes, provisionally labeled C : h
where C is the tag of the parent. Left adjuncts
(such as the first indicator in Figure 1) are split
off first, followed by right adjuncts (such as the
destination in Figure 1), left complements and
right complements.
</bodyText>
<listItem confidence="0.855863">
5. Assign CCG categories. Starting from the
root, the tag of each node is replaced by a CCG
category. For simplicity, we directly use RCL tags
as atomic categories rather than mapping them to
standard CCG categories:
</listItem>
<bodyText confidence="0.999186125">
The root gets its tag (event or sequence) as cat-
egory.
c-type nodes get their tag as category. Their sib-
ling gets category P/T if it is on the left and P\T
if it is on the right, where T is the tag of the c-
type node and P is the category of the parent. For
example, the destination node in Figure 1 gets
destination as category, and its left sibling there-
</bodyText>
<page confidence="0.997059">
483
</page>
<bodyText confidence="0.9996474">
fore gets event/destination because the parent’s
category is event.
a-type nodes such as the two indicators in Fig-
ure 1 get category P/P if they are on the left
of their sibling and P\P if they are on its right,
where P is the category of their parent. The sib-
ling gets category P.
Nodes without siblings get their tag as category.
Constituent type labels are dropped. The result
for our example is shown at the bottom of Figure 1.
</bodyText>
<subsectionHeader confidence="0.997581">
2.2 The Lexicon
</subsectionHeader>
<bodyText confidence="0.999683571428571">
For each leaf in the transformed corpus that is
aligned to one or more words, a lexical item is ex-
tracted containing the words, category and RCL.
For single-word items, we also add part-of-speech
tags, obtained using the C&amp;C POS tagger (Curran
and Clark, 2003), to reduce overgeneration. Ex-
amples of lexical items are:
</bodyText>
<listItem confidence="0.99911775">
• (block/NN) �- entity : (entity : (type :
(block)))
• (on, top, of) spatial-relation/entity :
(spatial-relation : (relation : above))
</listItem>
<subsectionHeader confidence="0.994308">
2.3 Combinatory Rules
</subsectionHeader>
<bodyText confidence="0.940848536585366">
Given the extracted lexical items, the corpus
derivations are licensed by standard CCG rules
(Steedman, 2001), using a modified semantics that
keeps things simple and ensures that the semantics
of (most) intermediate constituents are themselves
RCL subexpressions, which is important for inter-
facing with the spatial planner during parsing. The
most important two rules are forward and back-
ward application:
(X/Y ):f Y :g ⇒ X:FAPP(X/Y, f,g) (&gt;)
Y :g (X\Y ):f ⇒ X:BAPP(X\Y,g, f) (&lt;)
where FAPP and BAPP are defined as follows:
FAPP(X/Y, a, (t:h)) = (t:ah) if X = Y
FAPP(C, (t:h), c) = (t:hc) otherwise
BAPP(X\Y, (t:h), a) = (t:ha) if X = Y
BAPP(C, c, (t:h)) = (t:ch) otherwise
In words, the semantics of the adjunct or comple-
ment is added as a subtree under the root of the
semantics of the head.
We also use a restricted form of the CCG rule
forward composition to form chains of entity ad-
juncts:
(entity/entity):a (entity/entity):b
⇒ (entity/entity):ab (&gt;B)
This is motivated by our use of the spatial plan-
ner. Without forward composition, we would, e.g.,
not be able to build a constituent with the seman-
tics (entity : (color : green)(color : red)(type :
cube-group)) in the context of a stack consisting
of green and red cubes, but no stack consisting
exclusively of red cubes – the planner would fil-
ter out the intermediate constituent with semantics
(entity:(color:red)(type:cube-group)).
Finally, we use type-changing rules, which
is standard practice in CCG parsing (Clark and
Curran, 2007; Zettlemoyer and Collins, 2007).
They are automatically extracted from the training
data. Some of them account for unary productions
within RCL expressions by introducing an addi-
tional internal node, such as the destination node
in Figure 1. For example:
</bodyText>
<equation confidence="0.9461735">
sp-relation:h ⇒
destination:(destination:h) (*1)
</equation>
<bodyText confidence="0.999917888888889">
Others account for RCL leaves that are not linked
to any words. For example, the RCL expression
for the command take the light blue prism from the
blue cube renders the from-phrase as an adjunct
to the prism node: (spatial-relation : (relation:
above)(entity:(color:blue)(type:cube))), where
above is not linked. Rules like the following deal
with this by not only introducing an internal node,
but also a branch leading to the unlinked leaf:
</bodyText>
<equation confidence="0.9806975">
entity:h ⇒ entity/entity:
(sp-relation:(relation:above)h) (*2)
</equation>
<subsectionHeader confidence="0.967636">
2.4 Anaphora
</subsectionHeader>
<bodyText confidence="0.9999016">
Anaphora are marked in RCL entity expressions
by the subexpression (id : 1) for antecedent en-
tities and (reference-id : 1) for anaphoric enti-
ties. The latter have the special type reference,
in which case they are typically linked to the word
it, or type-reference, in which case they are typi-
cally linked to the word one, as in the yellow one.
More than one anaphoric relation in a command,
and thus, other IDs than 1, are possible, but ex-
tremely rare. We do not explicitly try to resolve
</bodyText>
<page confidence="0.998072">
484
</page>
<bodyText confidence="0.9999379375">
anaphora, but merely generate versions both with
and without the id subexpression for each entity
lexical item seen in training as an antecedent. We
then rely on the parser and spatial planner to find a
parse with the correct item marked as antecedent.
If the spatial planner rejects a subexpression be-
cause it contains an unknown reference ID, we ac-
cept it anyway because the expression can later
combine with another one that contains the an-
tecedent. However, at the level of complete parses,
those containing a reference-id expression but no
id expression – or vice versa – are rejected. As a
heuristic, we also reject parses where reference-id
precedes id because we found this to be a notice-
able source of errors, and no cataphora in the train-
ing data.
</bodyText>
<sectionHeader confidence="0.969078" genericHeader="method">
3 Training and Decoding
</sectionHeader>
<bodyText confidence="0.999919875">
Following (Zettlemoyer and Collins, 2007), we
use a CKY CCG parser in combination with sim-
ple perceptron updates: iterate over the training
corpus T times, for each sentence producing all
parses. Each parse is characterized by a num-
ber of features and scored using a global weight
vector. The weight vector is updated by sub-
tracting the feature vector of the highest-scoring
parse and adding the feature vector of the highest-
scoring correct parse. No update is performed if
the highest-scoring parse is correct, or no correct
parse was found. Since for the present task the
training data already induces a lexicon, we treat
the lexicon as fixed and perform no lexical update.
We parallelize training using iterative parameter
mixing (McDonald et al., 2010) with 12 shards.
</bodyText>
<subsectionHeader confidence="0.9967055">
3.1 Semantically Empty and Unknown
Words
</subsectionHeader>
<bodyText confidence="0.997991086956522">
The parser initially considers each contiguous sub-
sequence of words in the sentence and adds all
matching lexical items to the chart. In order to
allow for words that are not linked to the seman-
tics, we simply add two additional lexical items
to the chart for each word w in the sentence:
(w) �- X/X : nil and (w) �- X\X : nil where
X is a variable that can be bound to any category
during rule application. We modify the combina-
tory rules above to require that at least one of the
input items has non-nil semantics and to use that
as output semantics if the other is nil.
In decoding, the parser also has to deal with
words not seen in training. For one, there are the
nil items, so it is possible to treat the unknown
words as semantically empty. In addition, we look
at other single-word lexical items with the same
POS tag and generate corresponding lexical items
for the unknown word on the fly, hoping that fea-
tures and the spatial planner will guide the parser
to the right choice. To limit the search space, this
is currently only done for nouns since we found
the greatest lexical variance to occur with them.
</bodyText>
<subsectionHeader confidence="0.991769">
3.2 Features
</subsectionHeader>
<bodyText confidence="0.975648">
Each chart edge is characterized by the following
local features:
</bodyText>
<listItem confidence="0.885823">
• each lexical item w �- c:s used.
• each instance of a combinatory rule used, e.g.
&gt;.
• (p, c, s) for each lexical item used where p
is the POS tag (or empty for multiwords).
This allows to learn correlations between cat-
egory/semantics pairs and particular parts of
speech, primarily for unknown words.
• each instance of a type-changing rule used,
together with the semantic head word of the
constituent it roots, e.g. (∗1, in). This helps
to learn not to use type-changing rules where
they don’t make sense. E.g. the word
squares often heads entity descriptions that
type-change into measure phrases but the
word cube doesn’t.
• the root tag of the semantics of each con-
</listItem>
<bodyText confidence="0.996300555555556">
stituent, together with the word to its immedi-
ate left, e.g. (destination, from). This exam-
ple feature is indicative of typical erroneous
parses where spatial adjuncts corresponding
to from-phrases are misparsed as destination
complements. The word from provides a
strong clue against such a parse but would be
ignored without such a feature because it is
not aligned to any RCL node.
</bodyText>
<listItem confidence="0.531731666666667">
• the root tag of the semantics of each con-
stituent, together with the first word in it, e.g.
(spatial-relation, above).
</listItem>
<subsectionHeader confidence="0.998709">
3.3 The Spatial Planner
</subsectionHeader>
<bodyText confidence="0.99984275">
The spatial planner provided together with the
treebank provides access to the context in which
each command is to be interpreted, consisting of
a current arrangement of bodies on a board and in
</bodyText>
<page confidence="0.997368">
485
</page>
<bodyText confidence="0.999970655172414">
the gripper of the robot being instructed. It can tell
us for some RCL subexpressions, chiefly entity
descriptions, whether they “make sense” given
the context. For example, if the parser builds an
edge with semantics (entity :(type :cube)(color :
red)) but there is no red cube anywhere on the
board, we can immediately reject the edge (pro-
vided no negations or hypothetical descriptions
are used, which is the case for the commands
in this task) and thereby avoid errors and reduce
the search space. The planner also helps resolve
attachment ambiguities early: in the command
put the prism on the cube, a constituent with se-
mantics (entity :(type : prism)(spatial-relation :
(relation:above)(entity:(type:cube)))) is a pos-
sible but incorrect parse. If we are lucky enough
that no prism is actually sitting on a cube in the
microworld, the planner will weed it out.
We have not yet explored making the fullest
possible use of the spatial planner for checking the
validity of event or sequence expressions, which
would involve simulating changing the state of the
world as a sequence of event instructions is car-
ried out. Currently we only filter out initial event
instructions with action drop for scenes in which
there is nothing initially in the robot’s gripper to
be dropped. RCL requires the action move here
instead, a distinction which is often not made in
the natural language commands.
</bodyText>
<sectionHeader confidence="0.999465" genericHeader="conclusions">
4 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999992153846154">
We carried out two experiments, one using the spa-
tial planner and one not using it. In each case, we
trained on training examples shorter than 16 words
to speed up training and evaluated on the full test
set. In both training and decoding, a beam search
strategy keeps only the 60 highest-scoring edges
per chart cell. The weights of non-nil lexical items
were initialized to 1, those of nil items to 0.5, all
other feature weights to 0. The number of training
epochs T was set to 3. These values were chosen
experimentally using 80% of the training data and
another 10% for testing.
Of the 909 test sentences, 720 (79.21%) were
parsed exactly correctly when not using the plan-
ner, and 789 (86.80%) when using it, making third
place among the six participating systems. The
result shows that standard CCG-based techniques
for semantic parsing can be successfully applied to
the domain of robotic spatial commands and profit
from the integration of a spatial planner.
A preliminary analysis suggests most errors are
related to pronoun ellipsis, the ambiguous word
one, anaphora or attachment ambiguity. We be-
lieve some further careful feature engineering and
extended use of the spatial planner could go a great
length to improve accuracy further.
</bodyText>
<sectionHeader confidence="0.999403" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999659307692308">
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493–552.
James R. Curran and Stephen Clark. 2003. Inves-
tigating GIS and smoothing for maximum entropy
taggers. In Proceedings of the 11th Meeting of the
European Chapter of the Association for Compu-
tational Linguistics (EACL-03), pages 91–98, Bu-
dapest, Hungary.
Kais Dukes. 2013a. Semantic annotation of robotic
spatial commands. In Language and Technology
Conference (LTC), Poznan, Poland.
Kais Dukes. 2013b. Train robots: A dataset for natu-
ral language human-robot spatial interaction through
verbal commands. In International Conference on
Social Robotics (ICSR). Embodied Communication
of Goals and Intentions Workshop, Bristol, United
Kingdom.
J. Hockenmaier and M. Steedman. 2007. CCGbank:
a corpus of CCG derivations and dependency struc-
tures extracted from the Penn Treebank. Computa-
tional Linguistics, 33(3):355–396.
Ryan McDonald, Keith Hall, and Gideon Mann. 2010.
Distributed training strategies for the structured per-
ceptron. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ’10, pages 456–464, Stroudsburg, PA,
USA.
Mark Steedman. 2001. The Syntactic Process. The
MIT Press.
Luke S. Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for pars-
ing to logical form. In In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL-2007), pages
678–687.
</reference>
<page confidence="0.99904">
486
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.923403">
<title confidence="0.999872">RoBox: CCG with Structured Perceptron for Supervised Parsing of Robotic Spatial Commands</title>
<author confidence="0.999962">Kilian Evang Johan Bos</author>
<affiliation confidence="0.99946">University of Groningen University of Groningen</affiliation>
<email confidence="0.951982">k.evang@rug.nljohan.bos@rug.nl</email>
<abstract confidence="0.9938791">We use a Combinatory Categorial Grammar (CCG) parser with a structured perceptron learner to address Shared Task 6 of SemEval-2014, Supervised Semantic Parsing of Robotic Spatial Commands. Our system reaches an accuracy of 79% ignoring spatial context and 87% using the spatial planner, showing that CCG can successfully be applied to the task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Widecoverage efficient statistical parsing with CCG and log-linear models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="9588" citStr="Clark and Curran, 2007" startWordPosition="1517" endWordPosition="1520">ns of entity adjuncts: (entity/entity):a (entity/entity):b ⇒ (entity/entity):ab (&gt;B) This is motivated by our use of the spatial planner. Without forward composition, we would, e.g., not be able to build a constituent with the semantics (entity : (color : green)(color : red)(type : cube-group)) in the context of a stack consisting of green and red cubes, but no stack consisting exclusively of red cubes – the planner would filter out the intermediate constituent with semantics (entity:(color:red)(type:cube-group)). Finally, we use type-changing rules, which is standard practice in CCG parsing (Clark and Curran, 2007; Zettlemoyer and Collins, 2007). They are automatically extracted from the training data. Some of them account for unary productions within RCL expressions by introducing an additional internal node, such as the destination node in Figure 1. For example: sp-relation:h ⇒ destination:(destination:h) (*1) Others account for RCL leaves that are not linked to any words. For example, the RCL expression for the command take the light blue prism from the blue cube renders the from-phrase as an adjunct to the prism node: (spatial-relation : (relation: above)(entity:(color:blue)(type:cube))), where abo</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007. Widecoverage efficient statistical parsing with CCG and log-linear models. Computational Linguistics, 33(4):493–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Stephen Clark</author>
</authors>
<title>Investigating GIS and smoothing for maximum entropy taggers.</title>
<date>2003</date>
<booktitle>In Proceedings of the 11th Meeting of the European Chapter of the Association for Computational Linguistics (EACL-03),</booktitle>
<pages>91--98</pages>
<location>Budapest, Hungary.</location>
<contexts>
<context position="7876" citStr="Curran and Clark, 2003" startWordPosition="1236" endWordPosition="1239"> as the two indicators in Figure 1 get category P/P if they are on the left of their sibling and P\P if they are on its right, where P is the category of their parent. The sibling gets category P. Nodes without siblings get their tag as category. Constituent type labels are dropped. The result for our example is shown at the bottom of Figure 1. 2.2 The Lexicon For each leaf in the transformed corpus that is aligned to one or more words, a lexical item is extracted containing the words, category and RCL. For single-word items, we also add part-of-speech tags, obtained using the C&amp;C POS tagger (Curran and Clark, 2003), to reduce overgeneration. Examples of lexical items are: • (block/NN) �- entity : (entity : (type : (block))) • (on, top, of) spatial-relation/entity : (spatial-relation : (relation : above)) 2.3 Combinatory Rules Given the extracted lexical items, the corpus derivations are licensed by standard CCG rules (Steedman, 2001), using a modified semantics that keeps things simple and ensures that the semantics of (most) intermediate constituents are themselves RCL subexpressions, which is important for interfacing with the spatial planner during parsing. The most important two rules are forward an</context>
</contexts>
<marker>Curran, Clark, 2003</marker>
<rawString>James R. Curran and Stephen Clark. 2003. Investigating GIS and smoothing for maximum entropy taggers. In Proceedings of the 11th Meeting of the European Chapter of the Association for Computational Linguistics (EACL-03), pages 91–98, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kais Dukes</author>
</authors>
<title>Semantic annotation of robotic spatial commands.</title>
<date>2013</date>
<booktitle>In Language and Technology Conference (LTC),</booktitle>
<location>Poznan, Poland.</location>
<contexts>
<context position="988" citStr="Dukes, 2013" startWordPosition="146" endWordPosition="147">tic Spatial Commands. Our system reaches an accuracy of 79% ignoring spatial context and 87% using the spatial planner, showing that CCG can successfully be applied to the task. 1 Introduction When interpreting utterances, humans use world knowledge whereas most semantic parsers to date rely purely on linguistic clues. Shared Task 6 in the SemEval 2014 campaign for semantic evaluation aims to integrate reasoning about microworlds with semantic parsing. In this task, a system is given an instruction for a robot and has to produce an executable semantic representation in Robot Control Language (Dukes, 2013a, RCL). The Robot Commands Treebank (Dukes, 2013b) is used for training and evaluation. We participated in this shared task with a system rooted in Combinatory Categorial Grammar (CCG). In particular, we were interested in finding out whether existing techniques for automatically deriving categorial grammars with semantics could be moved easily to the new domain of robot commands and integrated with the provided spatial reasoning component. In this paper we outline our method and present the results for this shared task. 1 2 Extracting a CCG from RCL CCGs (Steedman, 2001) use a small set of a</context>
</contexts>
<marker>Dukes, 2013</marker>
<rawString>Kais Dukes. 2013a. Semantic annotation of robotic spatial commands. In Language and Technology Conference (LTC), Poznan, Poland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kais Dukes</author>
</authors>
<title>Train robots: A dataset for natural language human-robot spatial interaction through verbal commands.</title>
<date>2013</date>
<booktitle>In International Conference on Social Robotics (ICSR). Embodied Communication of Goals and Intentions Workshop,</booktitle>
<location>Bristol, United Kingdom.</location>
<contexts>
<context position="988" citStr="Dukes, 2013" startWordPosition="146" endWordPosition="147">tic Spatial Commands. Our system reaches an accuracy of 79% ignoring spatial context and 87% using the spatial planner, showing that CCG can successfully be applied to the task. 1 Introduction When interpreting utterances, humans use world knowledge whereas most semantic parsers to date rely purely on linguistic clues. Shared Task 6 in the SemEval 2014 campaign for semantic evaluation aims to integrate reasoning about microworlds with semantic parsing. In this task, a system is given an instruction for a robot and has to produce an executable semantic representation in Robot Control Language (Dukes, 2013a, RCL). The Robot Commands Treebank (Dukes, 2013b) is used for training and evaluation. We participated in this shared task with a system rooted in Combinatory Categorial Grammar (CCG). In particular, we were interested in finding out whether existing techniques for automatically deriving categorial grammars with semantics could be moved easily to the new domain of robot commands and integrated with the provided spatial reasoning component. In this paper we outline our method and present the results for this shared task. 1 2 Extracting a CCG from RCL CCGs (Steedman, 2001) use a small set of a</context>
</contexts>
<marker>Dukes, 2013</marker>
<rawString>Kais Dukes. 2013b. Train robots: A dataset for natural language human-robot spatial interaction through verbal commands. In International Conference on Social Robotics (ICSR). Embodied Communication of Goals and Intentions Workshop, Bristol, United Kingdom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hockenmaier</author>
<author>M Steedman</author>
</authors>
<title>CCGbank: a corpus of CCG derivations and dependency structures extracted from the Penn Treebank.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="3670" citStr="Hockenmaier and Steedman, 2007" startWordPosition="571" endWordPosition="574">nds. 2.1 Transforming the Trees RCL expressions are rooted ordered trees whose nodes are labeled with tags. We will write them in the form (t:h) where t is the root tag and h is the sequence of subtrees of the root’s children. Leaves are abbreviated as just their tags. In each training example, each pre-terminal (parent of a leaf) can be aligned to one or more words in the corresponding natural language expression. An example is shown in Figure 1. Since the alignments to words are not crossing, we can interpret the RCL tree as a phrase structure tree for the sentence and use the algorithm of (Hockenmaier and Steedman, 2007) to translate it to CCG. We extend the algorithm with a semantic step that makes sure the derivations would produce the original RCL expressions. 482 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 482–486, Dublin, Ireland, August 23-24, 2014. (event:(action:move)(entity:(color:green)(type :prism))(destination:(spatial-relation :(relation: within)(entity:(indicator:back)(indicator:left)(type:corner)))) event event Figure 1: Top: an RCL expression. Middle: its representation as a tree diagram. Internal nodes are annotated with constituent types. Pre-te</context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>J. Hockenmaier and M. Steedman. 2007. CCGbank: a corpus of CCG derivations and dependency structures extracted from the Penn Treebank. Computational Linguistics, 33(3):355–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Keith Hall</author>
<author>Gideon Mann</author>
</authors>
<title>Distributed training strategies for the structured perceptron.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>456--464</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="12444" citStr="McDonald et al., 2010" startWordPosition="1987" endWordPosition="1990">ining corpus T times, for each sentence producing all parses. Each parse is characterized by a number of features and scored using a global weight vector. The weight vector is updated by subtracting the feature vector of the highest-scoring parse and adding the feature vector of the highestscoring correct parse. No update is performed if the highest-scoring parse is correct, or no correct parse was found. Since for the present task the training data already induces a lexicon, we treat the lexicon as fixed and perform no lexical update. We parallelize training using iterative parameter mixing (McDonald et al., 2010) with 12 shards. 3.1 Semantically Empty and Unknown Words The parser initially considers each contiguous subsequence of words in the sentence and adds all matching lexical items to the chart. In order to allow for words that are not linked to the semantics, we simply add two additional lexical items to the chart for each word w in the sentence: (w) �- X/X : nil and (w) �- X\X : nil where X is a variable that can be bound to any category during rule application. We modify the combinatory rules above to require that at least one of the input items has non-nil semantics and to use that as output </context>
</contexts>
<marker>McDonald, Hall, Mann, 2010</marker>
<rawString>Ryan McDonald, Keith Hall, and Gideon Mann. 2010. Distributed training strategies for the structured perceptron. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 456–464, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2001</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="1567" citStr="Steedman, 2001" startWordPosition="239" endWordPosition="240"> Robot Control Language (Dukes, 2013a, RCL). The Robot Commands Treebank (Dukes, 2013b) is used for training and evaluation. We participated in this shared task with a system rooted in Combinatory Categorial Grammar (CCG). In particular, we were interested in finding out whether existing techniques for automatically deriving categorial grammars with semantics could be moved easily to the new domain of robot commands and integrated with the provided spatial reasoning component. In this paper we outline our method and present the results for this shared task. 1 2 Extracting a CCG from RCL CCGs (Steedman, 2001) use a small set of atomic constituent categories such as 5 (sentence), NP This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1Our code is available at http://www.let.rug. nl/evang/RoBox.zip (noun phrase) or PP (prepositional phrase). Constituents that take other constituents as arguments have complex categories describing their combinatory potential. For example, an intransitive English verb has category 5\NP, meaning that it forms a </context>
<context position="8201" citStr="Steedman, 2001" startWordPosition="1287" endWordPosition="1288"> of Figure 1. 2.2 The Lexicon For each leaf in the transformed corpus that is aligned to one or more words, a lexical item is extracted containing the words, category and RCL. For single-word items, we also add part-of-speech tags, obtained using the C&amp;C POS tagger (Curran and Clark, 2003), to reduce overgeneration. Examples of lexical items are: • (block/NN) �- entity : (entity : (type : (block))) • (on, top, of) spatial-relation/entity : (spatial-relation : (relation : above)) 2.3 Combinatory Rules Given the extracted lexical items, the corpus derivations are licensed by standard CCG rules (Steedman, 2001), using a modified semantics that keeps things simple and ensures that the semantics of (most) intermediate constituents are themselves RCL subexpressions, which is important for interfacing with the spatial planner during parsing. The most important two rules are forward and backward application: (X/Y ):f Y :g ⇒ X:FAPP(X/Y, f,g) (&gt;) Y :g (X\Y ):f ⇒ X:BAPP(X\Y,g, f) (&lt;) where FAPP and BAPP are defined as follows: FAPP(X/Y, a, (t:h)) = (t:ah) if X = Y FAPP(C, (t:h), c) = (t:hc) otherwise BAPP(X\Y, (t:h), a) = (t:ha) if X = Y BAPP(C, c, (t:h)) = (t:ch) otherwise In words, the semantics of the ad</context>
</contexts>
<marker>Steedman, 2001</marker>
<rawString>Mark Steedman. 2001. The Syntactic Process. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Online learning of relaxed CCG grammars for parsing to logical form. In</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL-2007),</booktitle>
<pages>678--687</pages>
<contexts>
<context position="9620" citStr="Zettlemoyer and Collins, 2007" startWordPosition="1521" endWordPosition="1524">entity/entity):a (entity/entity):b ⇒ (entity/entity):ab (&gt;B) This is motivated by our use of the spatial planner. Without forward composition, we would, e.g., not be able to build a constituent with the semantics (entity : (color : green)(color : red)(type : cube-group)) in the context of a stack consisting of green and red cubes, but no stack consisting exclusively of red cubes – the planner would filter out the intermediate constituent with semantics (entity:(color:red)(type:cube-group)). Finally, we use type-changing rules, which is standard practice in CCG parsing (Clark and Curran, 2007; Zettlemoyer and Collins, 2007). They are automatically extracted from the training data. Some of them account for unary productions within RCL expressions by introducing an additional internal node, such as the destination node in Figure 1. For example: sp-relation:h ⇒ destination:(destination:h) (*1) Others account for RCL leaves that are not linked to any words. For example, the RCL expression for the command take the light blue prism from the blue cube renders the from-phrase as an adjunct to the prism node: (spatial-relation : (relation: above)(entity:(color:blue)(type:cube))), where above is not linked. Rules like the</context>
<context position="11729" citStr="Zettlemoyer and Collins, 2007" startWordPosition="1868" endWordPosition="1871">to find a parse with the correct item marked as antecedent. If the spatial planner rejects a subexpression because it contains an unknown reference ID, we accept it anyway because the expression can later combine with another one that contains the antecedent. However, at the level of complete parses, those containing a reference-id expression but no id expression – or vice versa – are rejected. As a heuristic, we also reject parses where reference-id precedes id because we found this to be a noticeable source of errors, and no cataphora in the training data. 3 Training and Decoding Following (Zettlemoyer and Collins, 2007), we use a CKY CCG parser in combination with simple perceptron updates: iterate over the training corpus T times, for each sentence producing all parses. Each parse is characterized by a number of features and scored using a global weight vector. The weight vector is updated by subtracting the feature vector of the highest-scoring parse and adding the feature vector of the highestscoring correct parse. No update is performed if the highest-scoring parse is correct, or no correct parse was found. Since for the present task the training data already induces a lexicon, we treat the lexicon as fi</context>
</contexts>
<marker>Zettlemoyer, Collins, 2007</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2007. Online learning of relaxed CCG grammars for parsing to logical form. In In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL-2007), pages 678–687.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>