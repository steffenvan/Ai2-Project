<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015026">
<title confidence="0.99161">
Is sentence compression an NLG task?
</title>
<author confidence="0.99906">
Erwin Marsi, Emiel Krahmer
</author>
<affiliation confidence="0.971697">
Tilburg University
Tilburg, The Netherlands
</affiliation>
<email confidence="0.990463">
e.j.krahmer@uvt.nl
e.c.marsi@uvt.nl
</email>
<author confidence="0.994172">
Iris Hendrickx, Walter Daelemans
</author>
<affiliation confidence="0.8214135">
Antwerp University
Antwerpen, Belgium
</affiliation>
<email confidence="0.984623">
iris.hendrickx@ua.ac.be
walter.daelemans@ua.ac.be
</email>
<sectionHeader confidence="0.993716" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999711882352941">
Data-driven approaches to sentence com-
pression define the task as dropping any
subset of words from the input sentence
while retaining important information and
grammaticality. We show that only 16%
of the observed compressed sentences in
the domain of subtitling can be accounted
for in this way. We argue that part of this
is due to evaluation issues and estimate
that a deletion model is in fact compat-
ible with approximately 55% of the ob-
served data. We analyse the remaining
problems and conclude that in those cases
word order changes and paraphrasing are
crucial, and argue for more elaborate sen-
tence compression models which build on
NLG work.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999915678571429">
The task of sentence compression (or sentence re-
duction) can be defined as summarizing a single
sentence by removing information from it (Jing
and McKeown, 2000). The compressed sentence
should retain the most important information and
remain grammatical. One of the applications
is in automatic summarization in order to com-
press sentences extracted for the summary (Lin,
2003; Jing and McKeown, 2000). Other appli-
cations include automatic subtitling (Vandeghin-
ste and Tsjong Kim Sang, 2004; Vandeghinste and
Pan, 2004; Daelemans et al., 2004) and displaying
text on devices with very small screens (Corston-
Oliver, 2001).
A more restricted version defines sentence
compression as dropping any subset of words
from the input sentence while retaining impor-
tant information and grammaticality (Knight and
Marcu, 2002). This formulation of the task pro-
vided the basis for the noisy-channel en decision-
tree based algorithms presented in (Knight and
Marcu, 2002), and for virtually all follow-up work
on data-driven sentence compression (Le and
Horiguchi, 2003; Vandeghinste and Pan, 2004;
Turner and Charniak, 2005; Clarke and Lapata,
2006; Zajic et al., 2007; Clarke and Lapata, 2008)
It makes two important assumptions: (1) only
word deletions are allowed – no substitutions or
insertions – and therefore no paraphrases; (2) the
word order is fixed. In other words, the com-
pressed sentence must be a subsequence of the
source sentence. We will call this the subsequence
constraint, and refer to the corresponding com-
pression models as word deletion models. Another
implicit assumption in most work is that the scope
of sentence compression is limited to isolated sen-
tences and that the textual context is irrelevant.
Under this definition, sentence compression is
reduced to a word deletion task. Although one
may argue that even this counts as a form of
text-to-text generation, and consequently an NLG
task, the generation component is virtually non-
existent. One can thus seriously doubt whether it
really is an NLG task.
Things would become more interesting from an
NLG perspective if we could show that sentence
compression necessarily involves transformations
beyond mere deletion of words, and that this re-
quires linguistic knowledge and resources typical
to NLG. The aim of this paper is therefore to chal-
lenge the deletion model and the underlying subse-
quence constraint. To use an analogy, our aim is to
show that sentence compression is less like carv-
ing something out of wood - where material can
only be removed - and more like molding some-
thing out of clay - where the material can be thor-
</bodyText>
<note confidence="0.9159275">
Proceedings of the 12th European Workshop on Natural Language Generation, pages 25–32,
Athens, Greece, 30 – 31 March 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.998366">
25
</page>
<bodyText confidence="0.999794806451613">
oughly reshaped. In support of this claim we pro-
vide evidence that the coverage of deletion models
is in fact rather limited and that word reordering
and paraphrasing play an important role.
The remainder of this paper is structured as fol-
lows. In Section 2, we introduce our text material
which comes from the domain of subtitling. We
explain why not all material is equally well suited
for studying sentence compression and motivate
why we disregard certain parts of the data. We
also describe the manual alignment procedure and
the derivation of edit operations from it. In Sec-
tion 3, an analysis of the number of deletions, in-
sertions, substitutions, and reorderings in our data
is presented. We determine how many of the com-
pressed sentences actually satisfy the subsequence
constraint, and how many of them could in prin-
ciple be accounted for. That is, we consider al-
ternatives with the same compression ratio which
do not violate the subsequence constraint. Next
is an analysis of the remaining problematic cases
in which violation of the subsequence constraint
is crucial to accomplish the observed compression
ratio. We single out (1) reordering after deletion
and (2) paraphrasing as important factors. Given
the importance of paraphrases, Section 3.4 dis-
cusses the perspectives for automatic extraction of
paraphrase pairs from large text corpora, and tries
to estimate how much text is required to obtain a
reasonable coverage. We finish with a summary
and discussion in Section 4.
</bodyText>
<sectionHeader confidence="0.995923" genericHeader="introduction">
2 Material
</sectionHeader>
<bodyText confidence="0.999412838235294">
We study sentence compression in the context of
subtitling. The basic problem of subtitling is that
on average reading takes more time than listen-
ing, so subtitles can not be a verbatim transcrip-
tion of the speech without increasingly lagging be-
hind. Subtitles can be presented at a rate of 690
to 780 characters per minute, while the average
speech rate is considerably higher (Vandeghinste
and Tsjong Kim Sang, 2004). Subtitles are there-
fore often a compressed representation of the orig-
inal spoken text.
Our text material stems from the NOS Journaal,
the daily news broadcast of the Dutch public tele-
vision. It is parallel text with on one side the au-
tocue sentences (aut), i.e. the text the news reader
is reading, and on the other side the corresponding
subtitle sentences (sub). It was originally collected
and processed in two earlier research projects –
Atranos and Musa – on automatic subtitling (Van-
deghinste and Tsjong Kim Sang, 2004; Vandegh-
inste and Pan, 2004; Daelemans et al., 2004). All
text was automatically tokenized and aligned at
the sentence level, after which alignments were
manually checked.
The same material was further annotated in an
ongoing project called DAESO1, in which the gen-
eral goal is automatic detection of semantic over-
lap. All aligned sentences were first syntactically
parsed after which their parse trees were manually
aligned in more detail. Pairs of similar syntactic
nodes – either words or phrases – were aligned and
labeled according to a set of five semantic similar-
ity relations (Marsi and Krahmer, 2007). For cur-
rent purposes, only the alignment at the word level
is used, ignoring phrasal alignments and relation
labels.
Not all material in this corpus is equally well
suited for studying sentence compression as de-
fined in the introduction. As we will discuss in
more detail below, this prompted us to disregard
certain parts of the data.
Sentence deletion, splitting and merging For a
start, autocue and subtitle sentences are often not
in a one-to-one alignment relation. Table 1 speci-
fies the alignment degree (i.e. the number of other
sentences that a sentence is aligned to) for autocue
and subtitle sentences. The first thing to notice
is that there is a large number of unaligned sub-
titles. These correspond to non-anchor text from,
e.g., interviews or reporters abroad. More inter-
esting is that about one in five autocue sentences
is completely dropped. A small number of about
4 to 8 percent of the sentence pairs are not one-
to-one aligned. A long autocue sentence may be
split into several simpler subtitle sentences, each
containing only a part of the semantic content of
the autocue sentence. Conversely, one or more -
usually short - autocue sentences may be merged
into a single subtitle sentence.
These decisions of sentence deletion, splitting
and merging are worthy research topics in the con-
text of automatic subtitling, but they should not
be confused with sentence compression, the scope
of which is by definition limited to single sen-
tence. Accordingly we disregarded all sentence
pairs where autocue and subtitle are not in a one-
to-one relation with each other. This reduced the
data set from 15289 to 11034 sentence pairs.
</bodyText>
<footnote confidence="0.989211">
1http://daeso.uvt.nl
</footnote>
<page confidence="0.995628">
26
</page>
<table confidence="0.950239666666667">
Degree: Autocue: (%) Subtitle: (%)
0 3607 (20.74) 12542 (46.75)
1 12382 (71.19) 13340 (49.72)
2 1313 (7.55) 901 (3.36)
3 83 (0.48) 41 (0.15)
4 8 (0.05) 6 (0.02)
</table>
<tableCaption confidence="0.998863">
Table 1: Degree of sentence alignment
</tableCaption>
<bodyText confidence="0.9930379375">
Word compression A significant part of the re-
duction in subtitle characters is actually not ob-
tained by deleting words but by lexical substitution
of a shorter token. Examples of this include sub-
stitution by digits (“7” for “seven”), abbreviations
or acronyms (“US” for “United States”), symbols
(euro symbol for “Euro”), or reductions of com-
pound words (“elections” for “state-elections”).
We will call this word compression. Although an
important part of subtitling, we prefer to abstract
from word compression and focus here on sen-
tence compression proper. Removing all sentence
pairs containing a word compression has the dis-
advantage of further reducing the data set. Instead
we choose to measure compression ratio (CR) in
terms of tokens2 rather than characters.
</bodyText>
<equation confidence="0.799872">
CR = #toksub (1)
#tokaut
</equation>
<bodyText confidence="0.99984785">
This means that the majority of the word com-
pressions do not affect the sentence CR.
Variability in compression ratio The CR of
subtitles is not constant, but varies depending
(mainly) on the amount of provided autocue ma-
terial in a given time frame. The histogram in
Figure 1 shows the distribution of the CR (mea-
sured in words) for one-to-one aligned sentences.
In fact, autocue sentences are most likely not to
be compressed at all (thus belonging to the largest
bin, from 1.00 to 1.09 in the histogram).3 In order
to obtain a proper set of compression examples,
we retained only those sentence pairs where the
compression ratio is less than one.
Parsing failures As mentioned earlier detailed
alignment of autocue and subtitle sentences was
carried out on their syntactic trees. However,
for various reasons a small number of sentences
(0.2%) failed to pass the parser and received no
parse tree. As a consequence, their trees could not
</bodyText>
<footnote confidence="0.9971176">
2Throughout this study we ignore punctuation and letter
case.
3Some instances even show a CR larger than one, because
occasionally there is sufficient time/space to provide a clari-
fication, disambiguation, update, or stylistic enhancement.
</footnote>
<figureCaption confidence="0.99919">
Figure 1: Histogram of compression ratio
</figureCaption>
<table confidence="0.810583">
Min: Max: Sum: Mean: SD:
aut-tokens 2 43 80651 15.41 5.48
sub-tokens 1 29 53691 10.26 3.72
CR 0.07 0.96 nan 0.69 0.17
</table>
<tableCaption confidence="0.845702">
Table 2: Properties of the final data set of
</tableCaption>
<bodyText confidence="0.9696480625">
5233 pairs of autocue-subtitle sentences: mini-
mum value, maximal value, total sum, mean and
standard deviation for number of tokens per au-
tocue/subtitle sentence and Compression Ratio
be aligned and there is no alignment at the word
level available either. Variability in CR and pars-
ing failures are together responsible for a further
reduction down to 5233 sentence pairs, the final
size of our data set, with an overall CR of 0.69.
Other properties of this data set are summarized in
Table 2.4
Word deletions, insertions and substitutions
Having a manual alignment of similar words in
both sentences allows us to simply deduce word
deletions, substitutions and insertions, as well as
word order changes, in the following way:
</bodyText>
<listItem confidence="0.991311555555556">
• if an autocue word is not aligned to a subtitle
word, then it is was deleted
• if a subtitle word is not aligned to an autocue
word, then it was inserted
• if different autocue and subtitle words are
aligned, then the former was substituted by
the latter
• if alignments cross each other, then the word
order was changed
</listItem>
<bodyText confidence="0.9984245">
The remaining option is where the aligned
words are identical (ignoring differences in case).
</bodyText>
<footnote confidence="0.972418">
4We use the acronym nan (“not a number”) for unde-
fined/meaningless values.
</footnote>
<page confidence="0.998251">
27
</page>
<bodyText confidence="0.9999916875">
Without the word alignment, we would have
to resort to automatically calculating the edit dis-
tance, i.e. the sum of the minimal number of
insertions, deletions and substitutions required to
transform one sentence in to the other. However,
this would result in different and often counter-
intuitive sequences of edit operations. Our ap-
proach clearly distinguishes word order changes
from the edit operations; the conventional edit dis-
tance, by contrast, can only account for changes
in word order by sequences of the edit operations.
Another difference is that substitution can also be
accomplished as deletion followed by insertion,
which means edit operations need to have an as-
sociated weight. Global tuning of these weights
turns out to be hard.
</bodyText>
<sectionHeader confidence="0.998763" genericHeader="method">
3 Analysis
</sectionHeader>
<subsectionHeader confidence="0.999847">
3.1 Edit operations
</subsectionHeader>
<bodyText confidence="0.99991944">
The observed deletions, insertions, substitutions,
edit distances, and word order changes are shown
in Table 3. As expected, deletion is the most fre-
quent operation, with on average seven deletions
per sentence. Insertion and substitutions are far
less frequent. Note also that – even though the task
is compression – insertions are somewhat more
frequent than substitutions. Word order changes
occur in 1688 cases (32.26%). Here, reordering is
a binary variable – i.e. the word order is changed
or not – hence Min, Max and SD are undefined.
Another point of view is to look at the number
of sentence pairs containing a certain edit oper-
ation. Here we find 5233 pairs (100.00%) with
deletion, 2738 (52.32%) with substitution, 3263
(62.35%) with insertion, and 1688 (32.26%) with
reordering.
The average CR for subsequences is 0.68
(5D = 0.20) versus 0.69 (5D = 0.17)
for non-subsequences. A detailed inspection of
the relation between the subsequence/non −
subsequence ratio and CR revealed no clear cor-
relation, so we did not find indications that non-
subsequences occur more frequently at higher
compression ratios.
</bodyText>
<subsectionHeader confidence="0.999937">
3.2 Percentage of subsequences
</subsectionHeader>
<bodyText confidence="0.997631">
The subtitle is a subsequence of the autocue if
there are no insertions, no substitutions, and no
word order changes. In contrast, if any of these
do occur, the subtitle is not a subsequence. It turns
</bodyText>
<table confidence="0.959845333333333">
Min: Max: Sum: Mean: SD:
del 1 34 34728 6.64 4.57
sub 0 6 4116 0.79 0.94
ins 0 17 7768 1.48 1.78
dist 1 46 46612 8.91 5.78
reorder nan nan 1688 0.32 nan
</table>
<tableCaption confidence="0.968409">
Table 3: Observed word deletions, insertions, sub-
stitutions, and edit distances
</tableCaption>
<bodyText confidence="0.999394916666666">
out that only 843 (16.11%) subtitles are a subse-
quence, which is rather low.
At first sight, this appears to be bad news for
any deletion model, as it seems to imply that the
model cannot account for close to 84% the ob-
served data. However, the important thing to keep
in mind is that compression of a given sentence
is a problem for which there are usually multiple
solutions (Belz and Reiter, 2006). This is exactly
what makes it so hard to perform automatic evalu-
ation of NLG systems. There may very well exist
semantically equivalent alternatives with the same
CR which do satisfy the subsequence constraint.
For this reason, a substantial part of the observed
non-subsequences may have subsequence counter-
parts which can be accounted for by a deletion
model. The question is: how many?
In order to address this question, we took a
random sample of 200 non-subsequence sentence
pairs. In each case we tried to come up with
an alternative subsequence subtitle with the same
meaning and the same CR (or when opportune,
even a lower CR). Table 4 shows the distribu-
tion of the difference in tokens between the orig-
inal non-subsequence subtitle and the manually-
constructed equivalent subsequence subtitle. Ap-
parently 95 out of 200 (47%) subsequence sub-
titles have the same (or even fewer) tokens, and
thus the same (or an even lower) compression ra-
tio. This suggests that the subsequence constraint
is not as problematic as it seemed and that the cov-
erage of a deletion model is in fact far better than
it appeared to be. Recall that 16% of the original
subtitles were already subsequences, so our anal-
ysis suggests that a deletion model is compatible
with 55% (16% plus 47% of 84%).
</bodyText>
<subsectionHeader confidence="0.992953">
3.3 Problematic non-subsequences
</subsectionHeader>
<bodyText confidence="0.9999584">
Another result of this exercise in rewriting sub-
titles is that it allows us to identify those cases
where the attempt to create a proper subse-
quence fails. In (1), we show one representa-
tive example of a problematic subtitle, for which
</bodyText>
<page confidence="0.992652">
28
</page>
<figure confidence="0.870566956521739">
(1) Aut de bron was een geriatrische patient die zonder het zelf te merken uitzonderlijk veel larven bij zich
the source was a geriatric patient who without it self to notice exceptionally many larvae with him
verspreiding
spreading
veroorzaakte
caused
bleek te dragen en een grote
appeared to carry and a large
“the source was a geriatric patient who unknowingly carried exceptionally many larvae and caused a wide spreading”
Sub een geriatrische patient met larven heeft de verspreiding veroorzaakt
a geriatric patient with larvae has the spreading caused
Seq de bron was een geriatrische patient die veel larven bij zich bleek te dragen en een verspreiding veroorzaakte
(2) Aut in verband met de lawineramp in gal¨ur hebben de politieke partijen in tirol gezamenlijk besloten de
in relation to the avalanche-disaster in Galt¨ur have the political parties in Tirol together decided the
verkiezingscampagne voor het
election-campaign for the
regionale
regional
parlement
parliament
schorten
postpone
te
to
op
up
Sub de politieke partijen in tirol hebben besloten de verkiezingen op te schorten
the political parties in Tirol have decided the elections up to postpone
“Political parties in Tirol have decided to postpone the elections”
(3) Aut velen van hen worden door de servi¨ers in volgeladen treinen gedeporteerd
many of them are by the Serbs in crammed trains deported
Sub vluchtelingen worden per trein gedeporteerd
refugees are by train deported
token-diff: count: (%:)
-2 4 2.00
-1 18 9.00
0 73 36.50
1 42 21.00
2 32 16.00
3 11 5.50
4 9 4.50
5 5 2.50
7 2 1.00
8 2 1.00
9 1 0.50
11 1 0.50
</figure>
<figureCaption confidence="0.842467">
Table 4: Distribution of difference in tokens
between original non-subsequence subtitle and
equivalent subsequence subtitle
</figureCaption>
<bodyText confidence="0.99957437254902">
the best equivalent subsequence we could ob-
tain still has nine more tokens than the origi-
nal non-subsequence. These problematic non-
subsequences reveal where insertion, substitution
and/or word reordering are essential to obtain a
subtitle with a sufficient CR (i.e. the CR observed
in the real subtitles). At least three different types
of phenomena were observed.
Word order In some cases deletion of a con-
stituent necessitates a change in word order to ob-
tain a grammatical sentence. In example (2), the
autocue sentence has the PP modifier in verband
met de lawineramp in gal¨ur in its topic position
(first sentence position). Deleting this modifier, as
is done in the subtitle, results in a sentence that
starts with the verb hebben, which is interpreted as
a yes-no question. For a declarative interpretation,
we have to move the subject de politieke partijen
to the first position, as in the subtitle. Incidentally,
this indicates that it is instructive to apply sentence
compression models to multiple languages, as a
word order problem like this never arises in En-
glish.
Similar problems arise whenever an embedded
clause is promoted to a main clause, which re-
quires a change in the position of the finite verb
in Dutch. In total, a word order problem occurred
in 24 out 200 sentences.
Referring expressions Referring expressions
are on many occasions replaced by a shorter
one – usually a little less precise. For
example, de belgische overheid ‘the Belgian
authorities’ is replaced by belgie ‘Belgium’.
Extreme cases of this occur where a long
NP like deze tweede impeachment-procedure
in de amerikaanse geschiedenis ‘this second
impeachment-procedure in the American history’
is replaced by an anaphor like het ‘it’.
Since a referring expression or anaphor must be
appropriate in the given context, substitutions like
these transcend the domain of a single sentence
and require taking the preceding textual context
into account. This is especially clear in exam-
ples like (3) in which ‘many of them’ is replaced
the ‘refugees’. It is questionable whether these
types of substitutions belong to the task of sen-
tence compression. We prefer to regard it as one of
the additional tasks in automatic subtitling, apart
from compression. Incidentally, it is interesting
that the challenge of generating referring expres-
sions is also relevant for automatic subtitling.
</bodyText>
<page confidence="0.996966">
29
</page>
<bodyText confidence="0.996709388888889">
Paraphrasing Apart from the reduced referring
expressions, there are nominal paraphrases reduc-
ing a noun phrases like medewerkers van banken
‘employees of banks’ to a compound word like
bankmedewerkers ‘bank-employees’. Likewise,
there are adverbial paraphrases such as sinds een
paar jaar ‘since a few years’ to tegenwoordig
‘nowadays’, and van de afgelopen tijd ‘of the past
time’ to recent ‘recent’. However, the majority of
the paraphrasing concerns verbs as in the two ex-
amples below.
“Y refused to extradite him to Y”
Even though not all paraphrases are actually
shorter, it seems that at least some of them boost
compression beyond what can be accomplished
with only word deletion. In the next Section, we
look at the possibilities of automatic extraction of
such paraphrases.
</bodyText>
<sectionHeader confidence="0.973501" genericHeader="method">
3.4 Perspectives for automatic paraphrase
extraction
</sectionHeader>
<bodyText confidence="0.99996055882353">
There is a growing amount of work on automatic
extraction of paraphrases from text corpora (Lin
and Pantel, 2001; Barzilay and Lee, 2003; Ibrahim
et al., 2003; Dolan et al., 2004). One general pre-
requisite for learning a particular paraphrase pat-
tern is that it must occur in the text corpus with a
sufficiently high frequency, otherwise the chances
of learning the pattern are proportionally small. In
this section, we investigate to what extent the para-
phrases encountered in our random sample of 200
pairs can be retrieved from a reasonably large text
corpus.
In a first step, we manually extracted 106
paraphrase patterns. We filtered these pat-
terns and excluded anaphoric expressions, general
verb alternation patterns like active/passive and
continuous/non-continuous, as well as verbal pat-
terns involving more than two slots. After this fil-
tering step, 59 pairs of paraphrases remained, in-
cluding the examples shown in the preceding Sec-
tion.
The aim was to estimate how big our corpus
has to be to cover the majority of these para-
phrase pairs. We started with counting for each
of the paraphrase pairs in our sample how often
they occur in a corpus of Dutch news texts, the
Twente News Corpus5, which contains approxi-
mately 325M tokens and 20M sentences. We em-
ployed regular expressions to count the number of
paraphrase pattern matches. The corpus turned out
to contain 70% percent of all paraphrase pairs (i.e.
both patterns in the pair occur at least once). We
also counted how many pairs have a frequencies of
at least 10 and 100. To study the effect of corpus
size on the percentage of covered paraphrases, we
performed these counts on 1, 2, 5, 10, 25, 50 and
100% of the corpus. Figure 2 shows the percent-
age of covered paraphrases dependent on the cor-
pus size. The most strict threshold that only counts
pairs that occur at least 100 times in our corpus,
does not retrieve any counts on 1% of the corpus
(3M words). At 10% of the corpus size only 4%
of the paraphases is found, and on the full data set
25% of the pairs is found.
For 51% percent of the patterns (with a fre-
quency of at least 10) we find substantial evidence
in our corpus of 325M tokens. We fitted a curve
through our data points, and found a logarithmic
line fit with adjusted R2 value of .943. This sug-
gests that in order to get 75% of the patterns, we
would need a corpus that is 18 times bigger than
our current one, which amounts to roughly 6 bil-
lion words. Although this seems like a lot of text,
using the WWW as our corpus would easily give
us these numbers. Today’s estimate of the Index
Dutch World Wide Web is 688 million pages6. If
we assume that each page contains at least 100 to-
kens on average, this implies a corpus size of 68
billion tokens.
The patterns used here are word-based and in
many cases they express a particular verb tense or
verb form (e.g. 3rd person singular), and word
order. This implies that our estimations are the
minimum number of matches one can find. For
more abstract matching, we would need syntacti-
cally parsed data (Lin and Pantel, 2001). We ex-
pect that this would also positively affect the cov-
erage.
</bodyText>
<footnote confidence="0.9968885">
5http://www.vf.utwente.nl/˜druid/TwNC/
TwNC-main.html
6http://www.worldwidewebsize.com/
index.php?lang=NL, as measured in December
</footnote>
<page confidence="0.789893">
2008
</page>
<figure confidence="0.999813796296297">
(4) Aut X
X
Sub X
X
initiatief
initiative
tot oprichting
to raising
(5) Aut X
X
uitlevering
extradition
Sub Y
Y
hem
him
niet
not
wilde
wanted
uitleveren
extradite
aan
to
X
Y
het
the
neemt
takes
Y
Y
zet
sets
op
up
Y
Y
van
of
om
for
zijn
his
Y
Y
vroeg
asked
maar
but
die
that
weigerde
refused
</figure>
<page confidence="0.716828">
30
</page>
<figureCaption confidence="0.905944">
Figure 2: Percentage of covered paraphrases as a
function of the corpus size
</figureCaption>
<sectionHeader confidence="0.999614" genericHeader="conclusions">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999979752941177">
We found that only 16.11% of 5233 subtitle sen-
tences were proper subsequences of the corre-
sponding autocue sentence, and therefore 84% can
not be accounted for by a deletion model. One
consequence appears to be that the subsequence
constraint greatly reduces the amount of avail-
able training material for any word deletion model.
However, an attempt to rewrite non-subsequences
to semantically equivalent sequences with the
same CR suggests that a deletion model could in
principle be adequate for 55% of the data. More-
over, in those cases where an application can toler-
ate a little slack in the CR, a deletion model might
be sufficient. For instance, if we are willing to tol-
erate up to two more tokens, we can account for as
much as 169 (84%) of the 200 non-subsequences
in our sample, which amounts to 87% (16% plus
84% of 84%) of the total data.
It should be noted that we have been very strict
regarding what counts as a semantically equiva-
lent subtitle: every piece of information occurring
in the non-subsequence subtitle must reoccur in
the sequence subtitle. However, looking at our
original data, it is clear that considerable liberty
is taken as far as conserving semantic content is
concerned: subtitles often drop substantial pieces
of information. If we relax the notion of seman-
tic equivalence a little, an even larger part of the
non-subsequences can be rewritten as proper se-
quences.
The remaining problematic non-subsequences
are those where insertion, substitution and/or word
reordering are essential to obtain a sufficient CR.
One of the issues we identified is that deletion
of certain constituents must be accompanied by a
change in word order to prevent an ungrammati-
cal sentence. Since changes in word order appear
to require grammatical modeling or knowledge,
this brings sentence compression closer to being
an NLG task.
Nguyen and Horiguchi (2003) describe an ex-
tension of the decision tree-based compression
model (Knight and Marcu, 2002) which allows for
word order changes. The key to their approach
is that dropped constituents are temporarily stored
on a deletion stack, from which they can later be
re-inserted in the tree where required. Although
this provides an unlimited freedom for rearranging
constituents, it also complicates the task of learn-
ing the parsing steps, which might explain why
their evaluation results show marginal improve-
ments at best.
In our data, most of the word order changes ap-
pear to be minor though, often only moving the
verb to second position after deleting a constituent
in the topic position. We believe that unrestricted
word order changes are perhaps not necessary and
that the vast majority of the word order problems
can be solved by a fairly restricted way of reorder-
ing. In particular, we plan to implement a parser-
based model with an additional swap operation
that swaps the two topmost items on the stack. We
expect that this is more feasible as a learning task
than a model with a deletion stack.
Apart from reordering, other problems for word
deletion models are the insertions and substitu-
tions as a result of paraphrasing. Within a deci-
sion tree-based model, paraphrasing of words or
continuous phrases may be modeled by a combi-
nation of a paraphrase lexicon and an extra opera-
tion which replaces the n topmost elements on the
stack by the corresponding paraphrase. However,
paraphrases involving variable arguments, as typ-
ical for verbal paraphrases, cannot be accounted
for in this way. More powerful compression mod-
els may draw on existing NLG methods for text
revision (Inui et al., 1992) to accommodate full
paraphrasing.
We also looked at the perspectives for auto-
matic paraphrase extraction from large text cor-
pora. About a quarter of the required paraphrase
patterns was found at least a hundred times in our
corpus of 325M tokens. Extrapolation suggests
that using the web at its current size would give us
a coverage of approximately ten counts for three
</bodyText>
<page confidence="0.999582">
31
</page>
<bodyText confidence="0.999570133333333">
quarters of the paraphrases.
Incidentally, we identified two other tasks in
automatic subtitling which are closely related to
NLG. First, splitting and merging of sentences
(Jing and McKeown, 2000), which seems related
to content planning and aggregation. Second, gen-
eration of a shorter referring expression or an
anaphoric expression, which is currently one of
the main themes in data-driven NLG.
In conclusion, we have presented evidence that
deletion models for sentence compression are not
sufficient, and that more elaborate models in-
volving reordering and paraphrasing are required,
which puts sentence compression in the field of
NLG.
</bodyText>
<sectionHeader confidence="0.99893" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.995509285714286">
We would like to thank Nienke Eckhardt, Paul van Pelt, Han-
neke Schoormans and Jurry de Vos for the corpus annota-
tion work, and Erik Tsjong Kim Sang and colleagues for the
autocue-subtitle material from the ATRANOS project, and
Martijn Goudbeek for help with curve fitting. This work was
conducted within the DAESO project funded by the Stevin
program (De Nederlandse Taalunie).
</bodyText>
<sectionHeader confidence="0.998804" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99987048275862">
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: an unsupervised approach using multiple-
sequence alignment. In Proceedings of the 2003 Confer-
ence of the North American Chapter of the Association for
Computational Linguistics on Human Language Technol-
ogy, pages 16–23, Morristown, NJ, USA.
Anja Belz and Ehud Reiter. 2006. Comparing automatic and
human evaluation of NLG systems. In Proceedings of the
11th Conference of the European Chapter of the Associa-
tion for Computational Linguistics, pages 313–320.
James Clarke and Mirella Lapata. 2006. Models for sentence
compression: a comparison across domains, training re-
quirements and evaluation measures. In Proceedings of
the 21st International Conference on Computational Lin-
guistics and the 44th annual meeting of the Association for
Computational Linguistics, pages 377–384, Morristown,
NJ, USA.
James Clarke and Mirella Lapata. 2008. Global inference
for sentence compression an integer linear programming
approach. Journal of Artificial Intelligence Research,
31:399–429.
Simon Corston-Oliver. 2001. Text compaction for display
on very small screens. In Proceedings of the Workshop
on Automatic Summarization (WAS 2001), pages 89–98,
Pittsburgh, PA, USA.
Walter Daelemans, Anita H¨othker, and Erik Tjong Kim Sang.
2004. Automatic sentence simplification for subtitling in
Dutch and English. In Proceedings of the 4th Interna-
tional Conference on Language Resources and Evalua-
tion, pages 1045–1048.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsuper-
vised construction of large paraphrase corpora: Exploiting
massively parallel news sources. In Proceedings of the
20th International Conference on Computational Linguis-
tics, pages 350–356, Morristown, NJ, USA.
Ali Ibrahim, Boris Katz, and Jimmy Lin. 2003. Extract-
ing structural paraphrases from aligned monolingual cor-
pora. In Proceedings of the 2nd International Work-
shop on Paraphrasing, volume 16, pages 57–64, Sapporo,
Japan.
Kentaro Inui, Takenobu Tokunaga, and Hozumi Tanaka.
1992. Text Revision: A Model and Its Implementation.
In Proceedings of the 6th International Workshop on Nat-
ural Language Generation: Aspects of Automated Natural
Language Generation, pages 215–230. Springer-Verlag
London, UK.
Hongyan Jing and Kathleen McKeown. 2000. Cut and paste
based text summarization. In Proceedings of the 1st Con-
ference of the North American Chapter of the Association
for Computational Linguistics, pages 178–185, San Fran-
cisco, CA, USA.
Kevin Knight and Daniel Marcu. 2002. Summarization be-
yond sentence extraction: A probabilistic approach to sen-
tencecompression. Artificial Intelligence, 139(1):91–107.
Nguyen Minh Le and Susumu Horiguchi. 2003. A New Sen-
tence Reduction based on Decision Tree Model. In Pro-
ceedings of the 17th Pacific Asia Conference on Language,
Information and Computation, pages 290–297.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language En-
gineering, 7(4):343–360.
Chin-Yew Lin. 2003. Improving summarization perfor-
mance by sentence compression - A pilot study. In Pro-
ceedings of the Sixth International Workshop on Informa-
tion Retrieval with Asian Languages, volume 2003, pages
1–9.
Erwin Marsi and Emiel Krahmer. 2007. Annotating a par-
allel monolingual treebank with semantic similarity re-
lations. In Proceedings of the 6th International Work-
shop on Treebanks and Linguistic Theories, pages 85–96,
Bergen, Norway.
Jenine Turner and Eugene Charniak. 2005. Supervised and
unsupervised learning for sentence compression. In Pro-
ceedings of the 43rd Annual Meeting of the Association
for Computational Linguistics, pages 290–297, Ann Ar-
bor, Michigan, June.
Vincent Vandeghinste and Yi Pan. 2004. Sentence com-
pression for automated subtitling: A hybrid approach. In
Proceedings of the ACL Workshop on Text Summarization,
pages 89–95.
Vincent Vandeghinste and Erik Tsjong Kim Sang. 2004.
Using a Parallel Transcript/Subtitle Corpus for Sentence
Compression. In Proceedings of LREC 2004.
David Zajic, Bonnie J. Dorr, Jimmy Lin, and Richard
Schwartz. 2007. Multi-candidate reduction: Sentence
compression as a tool for document summarization tasks.
Information Processing Management, 43(6):1549–1570.
</reference>
<page confidence="0.999299">
32
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.440030">
<title confidence="0.973455">Is sentence compression an NLG task?</title>
<author confidence="0.95915">Erwin Marsi</author>
<author confidence="0.95915">Emiel</author>
<affiliation confidence="0.858325">Tilburg</affiliation>
<address confidence="0.95803">Tilburg, The</address>
<email confidence="0.985693">e.c.marsi@uvt.nl</email>
<author confidence="0.952358">Iris Hendrickx</author>
<author confidence="0.952358">Walter</author>
<affiliation confidence="0.736875">Antwerp</affiliation>
<address confidence="0.899795">Antwerpen,</address>
<email confidence="0.997729">walter.daelemans@ua.ac.be</email>
<abstract confidence="0.992360888888889">Data-driven approaches to sentence compression define the task as dropping any subset of words from the input sentence while retaining important information and We show that only of the observed compressed sentences in the domain of subtitling can be accounted for in this way. We argue that part of this is due to evaluation issues and estimate that a deletion model is in fact compatwith approximately the observed data. We analyse the remaining problems and conclude that in those cases word order changes and paraphrasing are crucial, and argue for more elaborate sentence compression models which build on NLG work.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Learning to paraphrase: an unsupervised approach using multiplesequence alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>16--23</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="21431" citStr="Barzilay and Lee, 2003" startWordPosition="3498" endWordPosition="3501">gelopen tijd ‘of the past time’ to recent ‘recent’. However, the majority of the paraphrasing concerns verbs as in the two examples below. “Y refused to extradite him to Y” Even though not all paraphrases are actually shorter, it seems that at least some of them boost compression beyond what can be accomplished with only word deletion. In the next Section, we look at the possibilities of automatic extraction of such paraphrases. 3.4 Perspectives for automatic paraphrase extraction There is a growing amount of work on automatic extraction of paraphrases from text corpora (Lin and Pantel, 2001; Barzilay and Lee, 2003; Ibrahim et al., 2003; Dolan et al., 2004). One general prerequisite for learning a particular paraphrase pattern is that it must occur in the text corpus with a sufficiently high frequency, otherwise the chances of learning the pattern are proportionally small. In this section, we investigate to what extent the paraphrases encountered in our random sample of 200 pairs can be retrieved from a reasonably large text corpus. In a first step, we manually extracted 106 paraphrase patterns. We filtered these patterns and excluded anaphoric expressions, general verb alternation patterns like active/</context>
</contexts>
<marker>Barzilay, Lee, 2003</marker>
<rawString>Regina Barzilay and Lillian Lee. 2003. Learning to paraphrase: an unsupervised approach using multiplesequence alignment. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 16–23, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
<author>Ehud Reiter</author>
</authors>
<title>Comparing automatic and human evaluation of NLG systems.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>313--320</pages>
<contexts>
<context position="14807" citStr="Belz and Reiter, 2006" startWordPosition="2418" endWordPosition="2421">um: Mean: SD: del 1 34 34728 6.64 4.57 sub 0 6 4116 0.79 0.94 ins 0 17 7768 1.48 1.78 dist 1 46 46612 8.91 5.78 reorder nan nan 1688 0.32 nan Table 3: Observed word deletions, insertions, substitutions, and edit distances out that only 843 (16.11%) subtitles are a subsequence, which is rather low. At first sight, this appears to be bad news for any deletion model, as it seems to imply that the model cannot account for close to 84% the observed data. However, the important thing to keep in mind is that compression of a given sentence is a problem for which there are usually multiple solutions (Belz and Reiter, 2006). This is exactly what makes it so hard to perform automatic evaluation of NLG systems. There may very well exist semantically equivalent alternatives with the same CR which do satisfy the subsequence constraint. For this reason, a substantial part of the observed non-subsequences may have subsequence counterparts which can be accounted for by a deletion model. The question is: how many? In order to address this question, we took a random sample of 200 non-subsequence sentence pairs. In each case we tried to come up with an alternative subsequence subtitle with the same meaning and the same CR</context>
</contexts>
<marker>Belz, Reiter, 2006</marker>
<rawString>Anja Belz and Ehud Reiter. 2006. Comparing automatic and human evaluation of NLG systems. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics, pages 313–320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Mirella Lapata</author>
</authors>
<title>Models for sentence compression: a comparison across domains, training requirements and evaluation measures.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>377--384</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2077" citStr="Clarke and Lapata, 2006" startWordPosition="307" endWordPosition="310">n, 2004; Daelemans et al., 2004) and displaying text on devices with very small screens (CorstonOliver, 2001). A more restricted version defines sentence compression as dropping any subset of words from the input sentence while retaining important information and grammaticality (Knight and Marcu, 2002). This formulation of the task provided the basis for the noisy-channel en decisiontree based algorithms presented in (Knight and Marcu, 2002), and for virtually all follow-up work on data-driven sentence compression (Le and Horiguchi, 2003; Vandeghinste and Pan, 2004; Turner and Charniak, 2005; Clarke and Lapata, 2006; Zajic et al., 2007; Clarke and Lapata, 2008) It makes two important assumptions: (1) only word deletions are allowed – no substitutions or insertions – and therefore no paraphrases; (2) the word order is fixed. In other words, the compressed sentence must be a subsequence of the source sentence. We will call this the subsequence constraint, and refer to the corresponding compression models as word deletion models. Another implicit assumption in most work is that the scope of sentence compression is limited to isolated sentences and that the textual context is irrelevant. Under this definitio</context>
</contexts>
<marker>Clarke, Lapata, 2006</marker>
<rawString>James Clarke and Mirella Lapata. 2006. Models for sentence compression: a comparison across domains, training requirements and evaluation measures. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 377–384, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Mirella Lapata</author>
</authors>
<title>Global inference for sentence compression an integer linear programming approach.</title>
<date>2008</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>31--399</pages>
<contexts>
<context position="2123" citStr="Clarke and Lapata, 2008" startWordPosition="315" endWordPosition="318">ng text on devices with very small screens (CorstonOliver, 2001). A more restricted version defines sentence compression as dropping any subset of words from the input sentence while retaining important information and grammaticality (Knight and Marcu, 2002). This formulation of the task provided the basis for the noisy-channel en decisiontree based algorithms presented in (Knight and Marcu, 2002), and for virtually all follow-up work on data-driven sentence compression (Le and Horiguchi, 2003; Vandeghinste and Pan, 2004; Turner and Charniak, 2005; Clarke and Lapata, 2006; Zajic et al., 2007; Clarke and Lapata, 2008) It makes two important assumptions: (1) only word deletions are allowed – no substitutions or insertions – and therefore no paraphrases; (2) the word order is fixed. In other words, the compressed sentence must be a subsequence of the source sentence. We will call this the subsequence constraint, and refer to the corresponding compression models as word deletion models. Another implicit assumption in most work is that the scope of sentence compression is limited to isolated sentences and that the textual context is irrelevant. Under this definition, sentence compression is reduced to a word d</context>
</contexts>
<marker>Clarke, Lapata, 2008</marker>
<rawString>James Clarke and Mirella Lapata. 2008. Global inference for sentence compression an integer linear programming approach. Journal of Artificial Intelligence Research, 31:399–429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Corston-Oliver</author>
</authors>
<title>Text compaction for display on very small screens.</title>
<date>2001</date>
<booktitle>In Proceedings of the Workshop on Automatic Summarization (WAS</booktitle>
<pages>89--98</pages>
<location>Pittsburgh, PA, USA.</location>
<marker>Corston-Oliver, 2001</marker>
<rawString>Simon Corston-Oliver. 2001. Text compaction for display on very small screens. In Proceedings of the Workshop on Automatic Summarization (WAS 2001), pages 89–98, Pittsburgh, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
</authors>
<title>Anita H¨othker, and Erik Tjong Kim Sang.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation,</booktitle>
<pages>1045--1048</pages>
<marker>Daelemans, 2004</marker>
<rawString>Walter Daelemans, Anita H¨othker, and Erik Tjong Kim Sang. 2004. Automatic sentence simplification for subtitling in Dutch and English. In Proceedings of the 4th International Conference on Language Resources and Evaluation, pages 1045–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<pages>350--356</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="21474" citStr="Dolan et al., 2004" startWordPosition="3506" endWordPosition="3509">ent’. However, the majority of the paraphrasing concerns verbs as in the two examples below. “Y refused to extradite him to Y” Even though not all paraphrases are actually shorter, it seems that at least some of them boost compression beyond what can be accomplished with only word deletion. In the next Section, we look at the possibilities of automatic extraction of such paraphrases. 3.4 Perspectives for automatic paraphrase extraction There is a growing amount of work on automatic extraction of paraphrases from text corpora (Lin and Pantel, 2001; Barzilay and Lee, 2003; Ibrahim et al., 2003; Dolan et al., 2004). One general prerequisite for learning a particular paraphrase pattern is that it must occur in the text corpus with a sufficiently high frequency, otherwise the chances of learning the pattern are proportionally small. In this section, we investigate to what extent the paraphrases encountered in our random sample of 200 pairs can be retrieved from a reasonably large text corpus. In a first step, we manually extracted 106 paraphrase patterns. We filtered these patterns and excluded anaphoric expressions, general verb alternation patterns like active/passive and continuous/non-continuous, as w</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. In Proceedings of the 20th International Conference on Computational Linguistics, pages 350–356, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ali Ibrahim</author>
<author>Boris Katz</author>
<author>Jimmy Lin</author>
</authors>
<title>Extracting structural paraphrases from aligned monolingual corpora.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2nd International Workshop on Paraphrasing,</booktitle>
<volume>16</volume>
<pages>57--64</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="21453" citStr="Ibrahim et al., 2003" startWordPosition="3502" endWordPosition="3505">t time’ to recent ‘recent’. However, the majority of the paraphrasing concerns verbs as in the two examples below. “Y refused to extradite him to Y” Even though not all paraphrases are actually shorter, it seems that at least some of them boost compression beyond what can be accomplished with only word deletion. In the next Section, we look at the possibilities of automatic extraction of such paraphrases. 3.4 Perspectives for automatic paraphrase extraction There is a growing amount of work on automatic extraction of paraphrases from text corpora (Lin and Pantel, 2001; Barzilay and Lee, 2003; Ibrahim et al., 2003; Dolan et al., 2004). One general prerequisite for learning a particular paraphrase pattern is that it must occur in the text corpus with a sufficiently high frequency, otherwise the chances of learning the pattern are proportionally small. In this section, we investigate to what extent the paraphrases encountered in our random sample of 200 pairs can be retrieved from a reasonably large text corpus. In a first step, we manually extracted 106 paraphrase patterns. We filtered these patterns and excluded anaphoric expressions, general verb alternation patterns like active/passive and continuous</context>
</contexts>
<marker>Ibrahim, Katz, Lin, 2003</marker>
<rawString>Ali Ibrahim, Boris Katz, and Jimmy Lin. 2003. Extracting structural paraphrases from aligned monolingual corpora. In Proceedings of the 2nd International Workshop on Paraphrasing, volume 16, pages 57–64, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kentaro Inui</author>
<author>Takenobu Tokunaga</author>
<author>Hozumi Tanaka</author>
</authors>
<title>Text Revision: A Model and Its Implementation.</title>
<date>1992</date>
<booktitle>In Proceedings of the 6th International Workshop on Natural Language Generation: Aspects of Automated Natural Language Generation,</booktitle>
<pages>215--230</pages>
<publisher>Springer-Verlag</publisher>
<location>London, UK.</location>
<contexts>
<context position="28514" citStr="Inui et al., 1992" startWordPosition="4713" endWordPosition="4716">h a deletion stack. Apart from reordering, other problems for word deletion models are the insertions and substitutions as a result of paraphrasing. Within a decision tree-based model, paraphrasing of words or continuous phrases may be modeled by a combination of a paraphrase lexicon and an extra operation which replaces the n topmost elements on the stack by the corresponding paraphrase. However, paraphrases involving variable arguments, as typical for verbal paraphrases, cannot be accounted for in this way. More powerful compression models may draw on existing NLG methods for text revision (Inui et al., 1992) to accommodate full paraphrasing. We also looked at the perspectives for automatic paraphrase extraction from large text corpora. About a quarter of the required paraphrase patterns was found at least a hundred times in our corpus of 325M tokens. Extrapolation suggests that using the web at its current size would give us a coverage of approximately ten counts for three 31 quarters of the paraphrases. Incidentally, we identified two other tasks in automatic subtitling which are closely related to NLG. First, splitting and merging of sentences (Jing and McKeown, 2000), which seems related to co</context>
</contexts>
<marker>Inui, Tokunaga, Tanaka, 1992</marker>
<rawString>Kentaro Inui, Takenobu Tokunaga, and Hozumi Tanaka. 1992. Text Revision: A Model and Its Implementation. In Proceedings of the 6th International Workshop on Natural Language Generation: Aspects of Automated Natural Language Generation, pages 215–230. Springer-Verlag London, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
<author>Kathleen McKeown</author>
</authors>
<title>Cut and paste based text summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>178--185</pages>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="1103" citStr="Jing and McKeown, 2000" startWordPosition="161" endWordPosition="164">observed compressed sentences in the domain of subtitling can be accounted for in this way. We argue that part of this is due to evaluation issues and estimate that a deletion model is in fact compatible with approximately 55% of the observed data. We analyse the remaining problems and conclude that in those cases word order changes and paraphrasing are crucial, and argue for more elaborate sentence compression models which build on NLG work. 1 Introduction The task of sentence compression (or sentence reduction) can be defined as summarizing a single sentence by removing information from it (Jing and McKeown, 2000). The compressed sentence should retain the most important information and remain grammatical. One of the applications is in automatic summarization in order to compress sentences extracted for the summary (Lin, 2003; Jing and McKeown, 2000). Other applications include automatic subtitling (Vandeghinste and Tsjong Kim Sang, 2004; Vandeghinste and Pan, 2004; Daelemans et al., 2004) and displaying text on devices with very small screens (CorstonOliver, 2001). A more restricted version defines sentence compression as dropping any subset of words from the input sentence while retaining important i</context>
<context position="29087" citStr="Jing and McKeown, 2000" startWordPosition="4805" endWordPosition="4808"> NLG methods for text revision (Inui et al., 1992) to accommodate full paraphrasing. We also looked at the perspectives for automatic paraphrase extraction from large text corpora. About a quarter of the required paraphrase patterns was found at least a hundred times in our corpus of 325M tokens. Extrapolation suggests that using the web at its current size would give us a coverage of approximately ten counts for three 31 quarters of the paraphrases. Incidentally, we identified two other tasks in automatic subtitling which are closely related to NLG. First, splitting and merging of sentences (Jing and McKeown, 2000), which seems related to content planning and aggregation. Second, generation of a shorter referring expression or an anaphoric expression, which is currently one of the main themes in data-driven NLG. In conclusion, we have presented evidence that deletion models for sentence compression are not sufficient, and that more elaborate models involving reordering and paraphrasing are required, which puts sentence compression in the field of NLG. Acknowledgments We would like to thank Nienke Eckhardt, Paul van Pelt, Hanneke Schoormans and Jurry de Vos for the corpus annotation work, and Erik Tsjong</context>
</contexts>
<marker>Jing, McKeown, 2000</marker>
<rawString>Hongyan Jing and Kathleen McKeown. 2000. Cut and paste based text summarization. In Proceedings of the 1st Conference of the North American Chapter of the Association for Computational Linguistics, pages 178–185, San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Summarization beyond sentence extraction: A probabilistic approach to sentencecompression.</title>
<date>2002</date>
<journal>Artificial Intelligence,</journal>
<volume>139</volume>
<issue>1</issue>
<contexts>
<context position="1757" citStr="Knight and Marcu, 2002" startWordPosition="258" endWordPosition="261">d retain the most important information and remain grammatical. One of the applications is in automatic summarization in order to compress sentences extracted for the summary (Lin, 2003; Jing and McKeown, 2000). Other applications include automatic subtitling (Vandeghinste and Tsjong Kim Sang, 2004; Vandeghinste and Pan, 2004; Daelemans et al., 2004) and displaying text on devices with very small screens (CorstonOliver, 2001). A more restricted version defines sentence compression as dropping any subset of words from the input sentence while retaining important information and grammaticality (Knight and Marcu, 2002). This formulation of the task provided the basis for the noisy-channel en decisiontree based algorithms presented in (Knight and Marcu, 2002), and for virtually all follow-up work on data-driven sentence compression (Le and Horiguchi, 2003; Vandeghinste and Pan, 2004; Turner and Charniak, 2005; Clarke and Lapata, 2006; Zajic et al., 2007; Clarke and Lapata, 2008) It makes two important assumptions: (1) only word deletions are allowed – no substitutions or insertions – and therefore no paraphrases; (2) the word order is fixed. In other words, the compressed sentence must be a subsequence of th</context>
<context position="26909" citStr="Knight and Marcu, 2002" startWordPosition="4444" endWordPosition="4447">e non-subsequences can be rewritten as proper sequences. The remaining problematic non-subsequences are those where insertion, substitution and/or word reordering are essential to obtain a sufficient CR. One of the issues we identified is that deletion of certain constituents must be accompanied by a change in word order to prevent an ungrammatical sentence. Since changes in word order appear to require grammatical modeling or knowledge, this brings sentence compression closer to being an NLG task. Nguyen and Horiguchi (2003) describe an extension of the decision tree-based compression model (Knight and Marcu, 2002) which allows for word order changes. The key to their approach is that dropped constituents are temporarily stored on a deletion stack, from which they can later be re-inserted in the tree where required. Although this provides an unlimited freedom for rearranging constituents, it also complicates the task of learning the parsing steps, which might explain why their evaluation results show marginal improvements at best. In our data, most of the word order changes appear to be minor though, often only moving the verb to second position after deleting a constituent in the topic position. We bel</context>
</contexts>
<marker>Knight, Marcu, 2002</marker>
<rawString>Kevin Knight and Daniel Marcu. 2002. Summarization beyond sentence extraction: A probabilistic approach to sentencecompression. Artificial Intelligence, 139(1):91–107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nguyen Minh Le and Susumu Horiguchi</author>
</authors>
<title>A New Sentence Reduction based on Decision Tree Model.</title>
<date>2003</date>
<booktitle>In Proceedings of the 17th Pacific Asia Conference on Language, Information and Computation,</booktitle>
<pages>290--297</pages>
<contexts>
<context position="1997" citStr="Horiguchi, 2003" startWordPosition="297" endWordPosition="298"> subtitling (Vandeghinste and Tsjong Kim Sang, 2004; Vandeghinste and Pan, 2004; Daelemans et al., 2004) and displaying text on devices with very small screens (CorstonOliver, 2001). A more restricted version defines sentence compression as dropping any subset of words from the input sentence while retaining important information and grammaticality (Knight and Marcu, 2002). This formulation of the task provided the basis for the noisy-channel en decisiontree based algorithms presented in (Knight and Marcu, 2002), and for virtually all follow-up work on data-driven sentence compression (Le and Horiguchi, 2003; Vandeghinste and Pan, 2004; Turner and Charniak, 2005; Clarke and Lapata, 2006; Zajic et al., 2007; Clarke and Lapata, 2008) It makes two important assumptions: (1) only word deletions are allowed – no substitutions or insertions – and therefore no paraphrases; (2) the word order is fixed. In other words, the compressed sentence must be a subsequence of the source sentence. We will call this the subsequence constraint, and refer to the corresponding compression models as word deletion models. Another implicit assumption in most work is that the scope of sentence compression is limited to iso</context>
<context position="26817" citStr="Horiguchi (2003)" startWordPosition="4432" endWordPosition="4433">n. If we relax the notion of semantic equivalence a little, an even larger part of the non-subsequences can be rewritten as proper sequences. The remaining problematic non-subsequences are those where insertion, substitution and/or word reordering are essential to obtain a sufficient CR. One of the issues we identified is that deletion of certain constituents must be accompanied by a change in word order to prevent an ungrammatical sentence. Since changes in word order appear to require grammatical modeling or knowledge, this brings sentence compression closer to being an NLG task. Nguyen and Horiguchi (2003) describe an extension of the decision tree-based compression model (Knight and Marcu, 2002) which allows for word order changes. The key to their approach is that dropped constituents are temporarily stored on a deletion stack, from which they can later be re-inserted in the tree where required. Although this provides an unlimited freedom for rearranging constituents, it also complicates the task of learning the parsing steps, which might explain why their evaluation results show marginal improvements at best. In our data, most of the word order changes appear to be minor though, often only m</context>
</contexts>
<marker>Horiguchi, 2003</marker>
<rawString>Nguyen Minh Le and Susumu Horiguchi. 2003. A New Sentence Reduction based on Decision Tree Model. In Proceedings of the 17th Pacific Asia Conference on Language, Information and Computation, pages 290–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Discovery of inference rules for question answering.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>4</issue>
<contexts>
<context position="21407" citStr="Lin and Pantel, 2001" startWordPosition="3494" endWordPosition="3497">wadays’, and van de afgelopen tijd ‘of the past time’ to recent ‘recent’. However, the majority of the paraphrasing concerns verbs as in the two examples below. “Y refused to extradite him to Y” Even though not all paraphrases are actually shorter, it seems that at least some of them boost compression beyond what can be accomplished with only word deletion. In the next Section, we look at the possibilities of automatic extraction of such paraphrases. 3.4 Perspectives for automatic paraphrase extraction There is a growing amount of work on automatic extraction of paraphrases from text corpora (Lin and Pantel, 2001; Barzilay and Lee, 2003; Ibrahim et al., 2003; Dolan et al., 2004). One general prerequisite for learning a particular paraphrase pattern is that it must occur in the text corpus with a sufficiently high frequency, otherwise the chances of learning the pattern are proportionally small. In this section, we investigate to what extent the paraphrases encountered in our random sample of 200 pairs can be retrieved from a reasonably large text corpus. In a first step, we manually extracted 106 paraphrase patterns. We filtered these patterns and excluded anaphoric expressions, general verb alternati</context>
<context position="24357" citStr="Lin and Pantel, 2001" startWordPosition="4022" endWordPosition="4025">s. Although this seems like a lot of text, using the WWW as our corpus would easily give us these numbers. Today’s estimate of the Index Dutch World Wide Web is 688 million pages6. If we assume that each page contains at least 100 tokens on average, this implies a corpus size of 68 billion tokens. The patterns used here are word-based and in many cases they express a particular verb tense or verb form (e.g. 3rd person singular), and word order. This implies that our estimations are the minimum number of matches one can find. For more abstract matching, we would need syntactically parsed data (Lin and Pantel, 2001). We expect that this would also positively affect the coverage. 5http://www.vf.utwente.nl/˜druid/TwNC/ TwNC-main.html 6http://www.worldwidewebsize.com/ index.php?lang=NL, as measured in December 2008 (4) Aut X X Sub X X initiatief initiative tot oprichting to raising (5) Aut X X uitlevering extradition Sub Y Y hem him niet not wilde wanted uitleveren extradite aan to X Y het the neemt takes Y Y zet sets op up Y Y van of om for zijn his Y Y vroeg asked maar but die that weigerde refused 30 Figure 2: Percentage of covered paraphrases as a function of the corpus size 4 Discussion We found that o</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Discovery of inference rules for question answering. Natural Language Engineering, 7(4):343–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Improving summarization performance by sentence compression - A pilot study.</title>
<date>2003</date>
<booktitle>In Proceedings of the Sixth International Workshop on Information Retrieval with Asian Languages,</booktitle>
<volume>volume</volume>
<pages>1--9</pages>
<contexts>
<context position="1319" citStr="Lin, 2003" startWordPosition="195" endWordPosition="196">the observed data. We analyse the remaining problems and conclude that in those cases word order changes and paraphrasing are crucial, and argue for more elaborate sentence compression models which build on NLG work. 1 Introduction The task of sentence compression (or sentence reduction) can be defined as summarizing a single sentence by removing information from it (Jing and McKeown, 2000). The compressed sentence should retain the most important information and remain grammatical. One of the applications is in automatic summarization in order to compress sentences extracted for the summary (Lin, 2003; Jing and McKeown, 2000). Other applications include automatic subtitling (Vandeghinste and Tsjong Kim Sang, 2004; Vandeghinste and Pan, 2004; Daelemans et al., 2004) and displaying text on devices with very small screens (CorstonOliver, 2001). A more restricted version defines sentence compression as dropping any subset of words from the input sentence while retaining important information and grammaticality (Knight and Marcu, 2002). This formulation of the task provided the basis for the noisy-channel en decisiontree based algorithms presented in (Knight and Marcu, 2002), and for virtually </context>
</contexts>
<marker>Lin, 2003</marker>
<rawString>Chin-Yew Lin. 2003. Improving summarization performance by sentence compression - A pilot study. In Proceedings of the Sixth International Workshop on Information Retrieval with Asian Languages, volume 2003, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erwin Marsi</author>
<author>Emiel Krahmer</author>
</authors>
<title>Annotating a parallel monolingual treebank with semantic similarity relations.</title>
<date>2007</date>
<booktitle>In Proceedings of the 6th International Workshop on Treebanks and Linguistic Theories,</booktitle>
<pages>85--96</pages>
<location>Bergen,</location>
<contexts>
<context position="6776" citStr="Marsi and Krahmer, 2007" startWordPosition="1079" endWordPosition="1082">Kim Sang, 2004; Vandeghinste and Pan, 2004; Daelemans et al., 2004). All text was automatically tokenized and aligned at the sentence level, after which alignments were manually checked. The same material was further annotated in an ongoing project called DAESO1, in which the general goal is automatic detection of semantic overlap. All aligned sentences were first syntactically parsed after which their parse trees were manually aligned in more detail. Pairs of similar syntactic nodes – either words or phrases – were aligned and labeled according to a set of five semantic similarity relations (Marsi and Krahmer, 2007). For current purposes, only the alignment at the word level is used, ignoring phrasal alignments and relation labels. Not all material in this corpus is equally well suited for studying sentence compression as defined in the introduction. As we will discuss in more detail below, this prompted us to disregard certain parts of the data. Sentence deletion, splitting and merging For a start, autocue and subtitle sentences are often not in a one-to-one alignment relation. Table 1 specifies the alignment degree (i.e. the number of other sentences that a sentence is aligned to) for autocue and subti</context>
</contexts>
<marker>Marsi, Krahmer, 2007</marker>
<rawString>Erwin Marsi and Emiel Krahmer. 2007. Annotating a parallel monolingual treebank with semantic similarity relations. In Proceedings of the 6th International Workshop on Treebanks and Linguistic Theories, pages 85–96, Bergen, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenine Turner</author>
<author>Eugene Charniak</author>
</authors>
<title>Supervised and unsupervised learning for sentence compression.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>290--297</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="2052" citStr="Turner and Charniak, 2005" startWordPosition="303" endWordPosition="306">, 2004; Vandeghinste and Pan, 2004; Daelemans et al., 2004) and displaying text on devices with very small screens (CorstonOliver, 2001). A more restricted version defines sentence compression as dropping any subset of words from the input sentence while retaining important information and grammaticality (Knight and Marcu, 2002). This formulation of the task provided the basis for the noisy-channel en decisiontree based algorithms presented in (Knight and Marcu, 2002), and for virtually all follow-up work on data-driven sentence compression (Le and Horiguchi, 2003; Vandeghinste and Pan, 2004; Turner and Charniak, 2005; Clarke and Lapata, 2006; Zajic et al., 2007; Clarke and Lapata, 2008) It makes two important assumptions: (1) only word deletions are allowed – no substitutions or insertions – and therefore no paraphrases; (2) the word order is fixed. In other words, the compressed sentence must be a subsequence of the source sentence. We will call this the subsequence constraint, and refer to the corresponding compression models as word deletion models. Another implicit assumption in most work is that the scope of sentence compression is limited to isolated sentences and that the textual context is irrelev</context>
</contexts>
<marker>Turner, Charniak, 2005</marker>
<rawString>Jenine Turner and Eugene Charniak. 2005. Supervised and unsupervised learning for sentence compression. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 290–297, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Vandeghinste</author>
<author>Yi Pan</author>
</authors>
<title>Sentence compression for automated subtitling: A hybrid approach.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL Workshop on Text Summarization,</booktitle>
<pages>89--95</pages>
<contexts>
<context position="1461" citStr="Vandeghinste and Pan, 2004" startWordPosition="214" endWordPosition="217"> crucial, and argue for more elaborate sentence compression models which build on NLG work. 1 Introduction The task of sentence compression (or sentence reduction) can be defined as summarizing a single sentence by removing information from it (Jing and McKeown, 2000). The compressed sentence should retain the most important information and remain grammatical. One of the applications is in automatic summarization in order to compress sentences extracted for the summary (Lin, 2003; Jing and McKeown, 2000). Other applications include automatic subtitling (Vandeghinste and Tsjong Kim Sang, 2004; Vandeghinste and Pan, 2004; Daelemans et al., 2004) and displaying text on devices with very small screens (CorstonOliver, 2001). A more restricted version defines sentence compression as dropping any subset of words from the input sentence while retaining important information and grammaticality (Knight and Marcu, 2002). This formulation of the task provided the basis for the noisy-channel en decisiontree based algorithms presented in (Knight and Marcu, 2002), and for virtually all follow-up work on data-driven sentence compression (Le and Horiguchi, 2003; Vandeghinste and Pan, 2004; Turner and Charniak, 2005; Clarke </context>
<context position="6194" citStr="Vandeghinste and Pan, 2004" startWordPosition="985" endWordPosition="989">ech rate is considerably higher (Vandeghinste and Tsjong Kim Sang, 2004). Subtitles are therefore often a compressed representation of the original spoken text. Our text material stems from the NOS Journaal, the daily news broadcast of the Dutch public television. It is parallel text with on one side the autocue sentences (aut), i.e. the text the news reader is reading, and on the other side the corresponding subtitle sentences (sub). It was originally collected and processed in two earlier research projects – Atranos and Musa – on automatic subtitling (Vandeghinste and Tsjong Kim Sang, 2004; Vandeghinste and Pan, 2004; Daelemans et al., 2004). All text was automatically tokenized and aligned at the sentence level, after which alignments were manually checked. The same material was further annotated in an ongoing project called DAESO1, in which the general goal is automatic detection of semantic overlap. All aligned sentences were first syntactically parsed after which their parse trees were manually aligned in more detail. Pairs of similar syntactic nodes – either words or phrases – were aligned and labeled according to a set of five semantic similarity relations (Marsi and Krahmer, 2007). For current purp</context>
</contexts>
<marker>Vandeghinste, Pan, 2004</marker>
<rawString>Vincent Vandeghinste and Yi Pan. 2004. Sentence compression for automated subtitling: A hybrid approach. In Proceedings of the ACL Workshop on Text Summarization, pages 89–95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent</author>
</authors>
<title>Vandeghinste and Erik Tsjong Kim Sang.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC</booktitle>
<marker>Vincent, 2004</marker>
<rawString>Vincent Vandeghinste and Erik Tsjong Kim Sang. 2004. Using a Parallel Transcript/Subtitle Corpus for Sentence Compression. In Proceedings of LREC 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Zajic</author>
<author>Bonnie J Dorr</author>
<author>Jimmy Lin</author>
<author>Richard Schwartz</author>
</authors>
<title>Multi-candidate reduction: Sentence compression as a tool for document summarization tasks.</title>
<date>2007</date>
<journal>Information Processing Management,</journal>
<volume>43</volume>
<issue>6</issue>
<contexts>
<context position="2097" citStr="Zajic et al., 2007" startWordPosition="311" endWordPosition="314">, 2004) and displaying text on devices with very small screens (CorstonOliver, 2001). A more restricted version defines sentence compression as dropping any subset of words from the input sentence while retaining important information and grammaticality (Knight and Marcu, 2002). This formulation of the task provided the basis for the noisy-channel en decisiontree based algorithms presented in (Knight and Marcu, 2002), and for virtually all follow-up work on data-driven sentence compression (Le and Horiguchi, 2003; Vandeghinste and Pan, 2004; Turner and Charniak, 2005; Clarke and Lapata, 2006; Zajic et al., 2007; Clarke and Lapata, 2008) It makes two important assumptions: (1) only word deletions are allowed – no substitutions or insertions – and therefore no paraphrases; (2) the word order is fixed. In other words, the compressed sentence must be a subsequence of the source sentence. We will call this the subsequence constraint, and refer to the corresponding compression models as word deletion models. Another implicit assumption in most work is that the scope of sentence compression is limited to isolated sentences and that the textual context is irrelevant. Under this definition, sentence compress</context>
</contexts>
<marker>Zajic, Dorr, Lin, Schwartz, 2007</marker>
<rawString>David Zajic, Bonnie J. Dorr, Jimmy Lin, and Richard Schwartz. 2007. Multi-candidate reduction: Sentence compression as a tool for document summarization tasks. Information Processing Management, 43(6):1549–1570.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>